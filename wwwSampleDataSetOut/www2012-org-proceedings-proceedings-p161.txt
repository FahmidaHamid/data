DOHA: Scalable Real-time Web Applications through

Adaptive Concurrent Execution

Aiman Erbad

University of British Columbia

201-2366 Main Mall
Vancouver, BC, Canada
aerbad@cs.ubc.ca

Norman C. Hutchinson
University of British Columbia

Vancouver, BC, Canada

201-2366 Main Mall
norm@cs.ubc.ca

Charles ’Buck’ Krasic

Google, Inc.

1600 Amphitheatre Parkway
Mountain View, CA, USA
ckrasic@google.com

ABSTRACT
Browsers have become mature execution platforms enabling
web applications to rival their desktop counterparts. An
important class of such applications is interactive multime-
dia: games, animations, and interactive visualizations. Un-
like many early web applications, these applications are la-
tency sensitive and processing (CPU and graphics) inten-
sive. When demands exceed available resources, application
quality (e.g., frame rate) diminishes because it is hard to
balance timeliness and utilization. The quality of ambitious
web applications is also limited by single-threaded execution
prevalent in the Web. Applications need to scale their qual-
ity, and thereby scale processing load, based on the resources
that are available. We refer to this as scalable quality.

DOHA is an execution layer written entirely in JavaScript
to enable scalable quality in web applications. DOHA favors
important computations with more inﬂuence over quality
based on hints from application-speciﬁc adaptation policies.
To utilize widely available multi-core resources, DOHA aug-
ments HTML5 web workers with mechanisms to facilitate
state management and load-balancing. We evaluate DOHA
with an award-winning web-based game. When resources
are limited, the modiﬁed game has better timing and over-
all quality. More importantly, quality scales linearly with a
small number of cores and the game is playable in challeng-
ing scenarios that are beyond the scope of the original game.

Categories and Subject Descriptors
C.4 [Performance of Systems]: Performance attributes;
D.1.3 [Programming Techniques]: Concurrent Program-
ming—parallel programming

General Terms
Design, Performance

Keywords
Quality of Service, HTML5 games, Web workers, JavaScript

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012 April 16-20, 2012, Lyon, France.
Copyright ACM 978-1-4503-1229-5/12/04.

1.

INTRODUCTION

The Web has evolved from a distributed document repos-
itory to become the de-facto distributed application plat-
form. As a platform its success is unparallelled. It supports
an enormous variety of complex and rich applications and
services, with the functionality of web applications often ri-
valling that of their desktop counterparts. The trend of
adopting the Web platform is expected to continue because
web applications have faster maintenance and deployment
cycles as well as good portability. As ambitious applications
migrate to the Web, they need to maintain rich user inter-
faces and support interactive scenarios making performance
a major concern in web-clients. Until recently, interactive
multimedia applications were limited by the lack of key tech-
nologies, such as rich graphics elements, bi-directional con-
tinuous network transport, and fast JavaScript engines. Browser
capabilities and the tools available now allow building com-
plex web applications, such as games using only HTML,
CSS, and JavaScript. Modern browsers are improving in
terms of performance and features at an exceptional pace
but web applications are becoming more ambitious and re-
source intensive.

Web-based games are great representatives of ambitious
(media-rich and time-sensitive) applications which are mov-
ing to the Web. Games and game developers are at the
edge of technology, often pushing the boundaries of what is
possible. Similar to desktop games [4], popular web games
[8, 24, 25] use most of the available processing power (be-
tween 80%-100% of a 2GHz core). Games and other am-
bitious web applications are shifting the performance opti-
mization focus from the download and parsing time of web
ﬁles to the run-time performance. According to the devel-
opers [8, 25], adding new features is limited by the available
processing capability. Developers spend signiﬁcant eﬀort op-
timizing after adding each feature and are forced to remove
or simplify some features to match the available capabil-
ity. This problem is exacerbated if developers target all exe-
cution platform combinations of browsers/browser versions
(Chrome, Firefox 4, Firefox 6, etc), operating systems (Win-
dows, Mac, Android and Linux), and hardware processing
capability (mobile to high-end PC).

Many web applications have adopted an event-based pro-
gramming model [22] in order to be responsive. When the
demand exceeds available CPU resources, however, it is not
feasible to execute all application events (callback functions)
in a timely fashion. The browser best-eﬀort execution model
does not provide any mechanism to balance between time-
liness and utilization. To port interactive web applications,

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France161developers hardcode the appropriate settings, such as the
game target frames per second. This approach cannot keep
up with the expanding number of platform combinations.
More importantly, it does not gracefully handle the dynamic
ﬂuctuations in application demands (common in multimedia
applications) or available resources (due to sharing the CPU
with other applications) over time. Without a general solu-
tion that scales demand to available resources, the perceived
quality of these applications becomes brittle and sensitive to
any change in the execution conditions.

The quality of ambitious applications is also limited by
single-threaded execution prevalent in web browsers. These
applications need more processing power than available in
one core especially in mobile platforms with low-end cores.
Multi-core processors are now available in most computing
platforms (desktops, laptops, tablets, and smart phones)
since hardware trends favor parallel architectures. To de-
liver the available CPU cycles to web applications, we need
to facilitate concurrent software development. HTML5 web
workers [10] introduce a shared-nothing concurrency model
as the ﬁrst step toward concurrent web applications, as we
see in Figure 1. Although this enables concurrent execution,
it does not help developers address challenging concurrency
issues, such as state management and load-balancing.

DOHA is an execution layer on top of JavaScript engines
that enhances the event-driven concurrency model. With
hints from applications, DOHA guarantees timely dispatch
for important events and scales application demands with
available resources. DOHA reduces the challenges of devel-
oping concurrent web applications unleashing the potential
of widely available multi-core processors. To be more spe-
ciﬁc, we make the following contributions:

(cid:129) We deﬁne scalable quality as a necessary requirement
to write web applications once and run them with con-
sistent quality everywhere. Scalable quality ensures
applications degrade quality gracefully when demands
exceed resources and scale quality up when more re-
sources are available.

(cid:129) We designed and implemented an execution layer which
includes HTML5 web workers to enable scalable qual-
ity in interactive multimedia web applications. To
utilize multi-core resources, DOHA augments HTML5
web workers with mechanisms to ease handling chal-
lenging concurrency issues, such as state management
and load-balancing.

(cid:129) While re-structuring the simulation engine of an award-
winning web-based game (RAPT [25]), we examine
the challenges and opportunities of using HTML5 web
workers and share our qualitative and quantitative ob-
servations.

The modiﬁed game (with DOHA) has better timing and
higher perceived quality when resources are scarce. More
importantly, the overall quality scales linearly as we use
more cores (up to 3 cores in RAPT). Our parallel game
is playable in larger game scenarios beyond the scope of the
original game. The remainder of the paper is structured
as follows: Section 2 discusses DOHA’s design and imple-
mentation details, Section 3 presents our evaluation results,
Section 4 describes our qualitative lessons learned, Section
5 highlights related work, and Section 6 concludes.

Figure 1: Web application with two HTML5 workers
running in a multi-core platform

function update (time) {

// Call update for all entities
for(entity in game_entities){

entity.update (time);

}

}

Figure 2: Game loop global update function

2. DESIGN AND IMPLEMENTATION

While studying the architecture of various web-based games

[1, 8, 24, 25], we observed that they have one or multiple ex-
ecution loops for the basic tasks, such as rendering and sim-
ulation. As we see in Figure 2, current game loops have a
global update that iterates over all game entities (e.g., play-
ers and enemies) in a pre-determined order (creation time
order in RAPT [25] and z-axis ordering in the Render Engine
[8]). The global update is called using a JavaScript timer 30
to 60 times a second depending on the target frame rate.
Current game loops attempt to update all entities at each
frame in a timely fashion. This architecture leads to brittle
application quality because adding one feature aﬀects frame
duration and can render the game not playable.

DOHA provides web applications with abstractions and
an execution layer to have better control over quality and
enable access to available multi-core resources. DOHA con-
sists of two major components: the event-loop which handles
prioritized execution locally in each thread, and the con-
current execution module, MultiProc, which simpliﬁes state
management and scheduling of events on worker threads.

2.1 Event-loop

An event-based architecture is a natural ﬁt for the asyn-
chronous browser execution environment. DOHA’s event-
driven programming model is inspired by the principles of
reactive programming [3] and aims to support the needs
of interactive web applications. Popular web applications
are event-driven with a large number of short callback func-
tions [22]. DOHA introduces explicit execution events that
specify the function to be executed and the call parame-
ters. Events in DOHA give the underlying scheduler per-
formance hints. Events also deﬁne the granularity at which
applications adapt (scale quality up and down). Inspired by

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France162priority-progress adaptation [15], elastic computationally in-
tensive parts of games are broken into explicit events.

Our key observation is that time-sensitive applications
have some computations that are time synchronous (e.g.,
sound and game loop updates) and others that are best-
eﬀort (e.g., AI logic and the particle engine) and can be
adapted. These two types of computations need to be clearly
identiﬁed so that their needs can be met independently.
The event-loop dispatches non-preemptively, prioritizing the
time synchronous computations over the best-eﬀort compu-
tations [14]. Events can be dispatched with low latency be-
cause our event-based model should ideally have short-lived
computations that avoid blocking.

The key primitives in the event-loop are: submit an event
for execution (and start the execution loop if it was not
active), run to start the execution loop, cancel to delete
a submitted event before it is executed, and stop to pause
the execution loop. Each event is given a type, a callback
specifying the function that will be called, and an array of
arguments. Explicit execution events have two types: timer
and best-eﬀort.
In timer events, the release time speciﬁes
the time an event becomes eligible to execute. Once eli-
gible, timer events take precedence over best-eﬀort events.
For best-eﬀort events, execution is ordered according to pri-
ority. When application demands exceed available resources,
it is not feasible to dispatch all events in a timely fashion.
Best-eﬀort events with more inﬂuence over perceived quality
are given high priority so they execute ﬁrst. Less important
events are cancelled when they become stale (based on time)
matching demands to available resources. Priority and re-
lease time are assigned by an application-speciﬁc adaptation
policy.

To order the execution of both event types, the event-loop
has two internal priority queues. Timer events have a min-
heap so events with earlier release times are closer to the
heap root. Best eﬀort events have a max-heap so higher
priority events are closer to the root. At each event-loop
iteration, we peek at the timers heap root to examine the
closest release-time. If it has been reached, we execute the
root event. If it has not been reached, we execute the best-
eﬀort heap root. If the best-eﬀort events heap is empty, we
yield to the underlying JavaScript engine until the closest
release-time. If the timers heap is empty, we yield execution
of the event-loop until a new event is submitted.

When an event is cancelled or executed, it is removed from
the associated heap (while maintaining the heap property).
Our event-loop is minimal and is designed to co-exist with
the underlying JavaScript engine. We see our event-loop
as an enhancement layer to add the essential adaptation
mechanisms: priority and cancellation. Heaps in our de-
sign allow applications to queue events improving utilization
while keeping full control over timing through prioritized ex-
ecution and cancellation. To avoid blocking the underlying
JavaScript engine, we can run DOHA’s event-loop in a timed
mode by setting a threshold (e.g., 200 ms) for the maximum
duration of an event-loop iteration.

2.1.1 RAPT: Events and Policies
As a case study for DOHA, we choose the game Robots
Are People Too (RAPT) [25]. RAPT won the most fun
game award in Mozilla’s Game On contest [21]. RAPT
is an HTML5 platform game ported from C++. Players
jump between moving platforms and coordinate their move-

function update (time) {

var evt;
// Delete pending events
for(evt in pending_events){

eventloop.cancel(evt);

}
// Add events to event-loop with a priority
for(entity in game_entities){

evt = new Event(entity.update, [time], BestEffort);
evt.priority = entity.getPriority();
eventloop.submit(evt);

}

}

Figure 3: Modiﬁed simulation loop update call

ments in order to pass game levels. The exit to each game
level is blocked by enemies that roll, jump, ﬂy, and shoot
to prevent escape. RAPT uses 100% of a single core CPU
(2GHz). To understand the time proﬁle of diﬀerent game
components, we used the internal browser proﬁler. The per-
formance of RAPT is impacted by two major components:
graphics and simulation (physics and collision detection). In
Chrome, 50% of the time is spent rendering, 30% on the sim-
ulation update, and around 20% is spent inside the browser.
The major components (graphics, simulation, and AI) are
similar to traditional desktop games [4]. We focused our ex-
periments on the simulation updates because it constitutes
a large performance concern especially after the rendering
in browsers becomes hardware-accelerated.

Our ﬁrst task was to split the large monolithic simulation
loop into small explicit update events. The main simula-
tion loop is now triggered by a timer event executing the
global update function at a rate of 30 frames per second
(33ms frame duration). Timer events triggering the global
simulation update take precedence over best-eﬀort events
submitted within each frame. As shown in Figure 3, the
modiﬁed global simulation update starts by cancelling the
pending best-eﬀort events from the previous frame. Then,
a separate update event per game entity is created and sub-
mitted to the underlying event-loop. Before an update event
is submitted, the getPriority policy method for each entity
is called to calculate the event importance.

Adaptation policies developed with the game deﬁne rela-
tive importance among diﬀerent game entities in each game
loop iteration (game frame). Relative importance (priority)
among game entities dictates the order of event execution.
Since players are at the heart of a game, their updates are
the most critical indicator of perceived game quality. Our
basic adaptation policy assigns priority based on distance
from active players. Priority is a number between 0.0 and
1.0. Players get priority 1.0. The priority assigned to the
update events of other entities is inversely proportional to
their distance from the closest player.

Using distance only can lead to starvation for distant en-
tities. These entities will not be updated if resources are
limited which causes ﬂaws in their physics updates. To min-
imize starvation and ensure correct simulation for all game
entities, we deﬁned a minimum update heuristic based on
the time since last update. As the time since last update
increases, the priority increases to reach 1.0 when we ex-
ceed a maximum time between updates threshold. Finally,
game entities have some non-linear behaviors, such as grav-
ity. These behaviors limit scalability because they require a
high and consistent update rate. Our policy needs to detect
and account for these behaviors while assigning priority.

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France163All our entities sub-class two base classes: enemy and
player. These base classes deﬁne the adaptation policies
other entities inherit. This current policy can be customized
at run-time with the appropriate thresholds, such as mini-
mum update threshold and distance ranges. We can also
override a policy to include other factors speciﬁc to an en-
tity type. For example, we can increase the priority of a
bullet proportional to its speed.

It is important to note that with simple modiﬁcations to
the main simulation update loop, it was possible to scale
quality using DOHA’s event-loop. Web-based games have
other places where scalability can help trade accuracy for
performance, such as the particle engine (visual eﬀects ac-
curacy) and AI logic (algorithm accuracy).
2.2 MultiProc: Concurrent Execution

HTML5 web workers are implemented using threads in
major browsers and utilize multi-core hardware if available.
Worker threads were envisioned to provide an API to run
scripts in the background without locking the user interface
[10]. Since their inception, web workers have been used in
computationally expensive demo applications to speed-up
highly parallel algorithms. For example, our parallel fac-
torial micro-benchmark gets around 10x speed-up with 16
cores for large numbers (3 * 109 in Chrome).

We believe web workers have a larger role in enhancing the
performance of interactive multimedia web applications es-
pecially in the mobile Web. Mobile platforms have low-end
multi-core processors (e.g., 600 MHz) and browsing perfor-
mance is the biggest barrier to entry for a large number of
ambitious web applications. Using one of the most chal-
lenging web application domains, HTML5 games, we show
that web workers with appropriate support can signiﬁcantly
improve performance and perceived quality.

2.2.1 MultiProc API
MultiProc provides mechanisms to write concurrent web
applications with diﬀerent architectures. As shown in Fig-
ure 4, we started with a central architecture that is tightly
coupled. This architecture works in applications with mini-
mal shared state. remote submit is used to submit a remote
event to the central scheduler. The scheduler (in the main
thread) decides where to execute each event based on worker
load statistics (orders workers based on load). Events can
be cancelled using remote cancel before they are assigned
to a worker. To inform the main thread that an event was
executed successfully, a worker calls done. This call updates
the worker load statistics (number of active events).

Concurrent web applications with expensive communica-
tion (as shown in Table 2) are similar to distributed sys-
tems. To share state between application components across
workers, MultiProc introduces a publish-subscribe commu-
nication API and RPC events. To send a direct RPC event
to a speciﬁc worker bypassing the central scheduler, we use
the remote direct submit call. Shared state can be published
using publish state. The state is transferred across worker
boundaries and the method that subscribed for the state
updates using the subscribe state API call is notiﬁed.

2.2.2 Central Hierarchical Design
MultiProc started with support for a centralized mas-
ter/slave web application architecture. The main browser
thread is the master dispatching events to slave workers.

//Central scheduler API
remote_submit(Event e);
remote_cancel(Event e);
done(Event e);

//RPC and state management API
remote_direct_submit(Event e);
publish_state(topic, msg);
subscribe_state(topic, worker_id, function_name);
unsubscribe_state(topic, worker_id);

Figure 4: MultiProc public API

The main thread and workers each run their own event-loop
to manage event execution. Worker creation, book-keeping,
and scheduling decisions happen in the centralized sched-
uler. This central design is based on the observation that the
main thread handles the Document Object Model (DOM)
and that workers can only communicate with their parents
(no direct communication between siblings). To extend our
adaptation model across workers, events are queued in the
main scheduler and sent according to their priority. DOHA’s
central scheduler allows a small ﬁxed window of events in-
ﬂight per worker. Upon notiﬁcation that an event was exe-
cuted successfully, another event is dispatched to the same
worker. Since the window size is small, the scheduler is agile
in responding to load imbalance among workers.

When an event reaches a worker, it is passed to the ap-
plication code. The application code at the worker adds
the event to the local event-loop which respects its prior-
ity. Each level of DOHA orders events according to their
priority to approximate a distributed adaptive event-loop
across workers. To balance load across workers, events are
assigned to the worker with the smallest load (number of
events in progress). MultiProc uses two heaps to manage
remote events and workers. Remote events are ordered ac-
cording to their priority while workers are ordered according
to the number of active events.

Assuming each event can be executed in any worker, the
centralized scheduler will have perfect load balancing. How-
ever, some events have dependencies (e.g., manipulate the
same data-structure). To handle these dependencies be-
tween events our framework uses event coloring [27]. Pro-
grammers color events and MultiProc adheres to the coloring
constraints. Events with the same color execute in the same
worker and events with diﬀerent colors can execute in paral-
lel (in diﬀerent workers). Workers can generate events and
assign them unique colors. Since our design is centralized,
a worker delegates the event to the main thread (master)
which assigns it to the appropriate worker. Coloring is an
easy to adopt [27] yet powerful concurrency control mecha-
nism. If all events have the default color, we have a serial
program. Having more colors reduces the scheduling con-
straints which leads to better load balancing across workers.
To test dynamic load balancing in the central design, we
used a 3D animation that renders a large frame using ray
tracing. Ray tracing is computationally intensive and has
minimal shared state. To simulate a loaded worker, we limit
the CPU share of one worker (out of 4 worker threads) to
be 25% of the CPU time. To simulate dependency between
events, we vary the percentage of events with unique colors.
100% means each event has a unique color (no dependency)
and 0% means each event is colored with one of the four

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France164Percentage Of Unique Colors

Delay (ms)

0

501

25
393

50
342

75
320

100
310

Table 1: Average delay to render a frame using ray
tracing

major colors (25% of the total events per color to distribute
work evenly across 4 workers). The original rendering appli-
cation which uses round robin scheduling takes 309ms to ren-
der a frame when all the workers have enough resources and
takes around 500ms when one worker is limited. In Table
1, we see that MultiProc central load balancing algorithm
assigned events to other less loaded workers reducing the
impact of the loaded worker and maintaining the same over-
all application execution time when each event has a unique
color (100%). As the percentage of events with unique colors
decreases (more dependencies between events), the render-
ing task gets delayed signiﬁcantly. The load balancing logic
was not able to run as many events in parallel and gets the
same results as the original round robin version.

Our initial design assumed web applications have a cen-
tral design where all execution events pass by the MultiProc
scheduler. Even though our results with a simple applica-
tion were positive, in the central design all application state
accessed during each computation and the generated re-
sults cross worker boundaries. The communication cost be-
come prohibitively expensive in complex applications, such
as games, with tightly coupled components sharing state.
Table 2 shows the high costs of a ping-pong message in
HTML5 web workers as we vary the message size. 3ms is a
relatively high cost considering the 33.3ms frame duration
(or 16.6ms with a rate of 60 frames per second).

Message Size (bytes)

Firefox (ms)

Chrome 15 (ms)

10
3
3.4

100

3
4

1K 10K 100K
2.3
4.5

4.5
46.9

3
6

Table 2: Average delay for a ping-pong message be-
tween workers
2.2.3 State Management and Publish-Subscribe
To address these high communication costs, we moved to
a less central design where the code in workers is more inde-
pendent. We re-structured the simulation loop of RAPT as
a network of components running in workers. As we see in
Figure 5, each worker has an event-loop to run local events.
Instead of sending all events and their related state across
worker boundaries, we send few direct events (synchroniza-
tion and control events) and necessary state updates be-
tween workers. For example, we send a game loop start
iteration event from the main thread with minimum data
parameters, such as the current time. Update events for en-
tities assigned to the worker are generated locally and added
to the local event-loop.

Without shared memory, workers can not access the browser

Document Object Model (DOM). The code for each game
entity in RAPT had to be split into two parts: one for sim-
ulation which runs in worker threads and another for ren-
dering which runs in the main thread. The modiﬁed game
loop performs rendering in the main thread while the loop in
each worker performs the simulation. To maintain the game
view, the rendering state of each entity is replicated. After
each simulation update, each entity communicates the state
needed for rendering back to the rendering replica in the

Figure 5: Web application using MultiProc with two
workers

main thread. This partial replication of the entity’s state
uses our publish-subscribe communication API, as we see in
Figure 6. Partial replication transfers the minimum amount
of state needed for rendering, such as the entity position (x,
y), and orientation (angle).

//Publish state in the worker
Entity.prototype.publishState = function()
{

var msg = [this.x, this.y, this.angle];
worker.publish_state(this.id, msg);

};
//Update state in the main thread
Entity.prototype.updateState = function(args)
{

this.x = args[0]; this.y = args[1];
this.angle = args[2];

};

Figure 6: Entity sharing rendering state in Concur-
rent RAPT using the Publish-Subscribe API

Few key entities in RAPT are global. For example, players
are accessed and modiﬁed by diﬀerent types of enemies in
multiple workers. Similarly, some entities at the boundary of
partitions need to have their state shared between two work-
ers. To perform correct simulation, the entire entity’s state
is replicated across multiple workers. One worker owns the
primary (authoritative) copy of the entity and other workers
have full replicas. We synchronize all replicas after each en-
tity update. The primary publishes its state to the entity’s
topic which all replicas (partial and full) subscribe to.

To allow modifying global objects, each identical replica
acts as a proxy. State mutation is only allowed in the au-
thoritative version of an entity. When a mutator method
in a replica is called, the call is published on the global
object mutation topic which the authoritative version sub-
scribes to. State management in entities heavily use the
publish-subscribe API for one-to-one (partial replication),
one-to-many (full replication), and many-to-one (proxy for-
warding) communication. These diﬀerent communication
patterns and the dynamic movements of entities to balance
load across workers are the main motivations for our publish-
subscribe communication API. Publish-subscribe provides a
loosely coupled communication API that supports various
communication patterns.

Our publish-subscribe logic is central. Web workers pass

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France165the communication API calls to the main browser thread.
Our main publish-subscribe unit maps topics to a subscribers
list. Each subscriber is a tuple of <worker ID, function
name>. When a message is received from the topic, it is
forwarded to a function with the given name on the speci-
ﬁed worker. In each worker, the application registers a list
of public functions that handle state-update messages.

The topics used for publish-subscribe communication need
to be unique. We built a distributed identity manager to
provide each game entity (in RAPT) with a unique identity
that is used as a topic for its communication. The primary
identity manager in the main browser thread assigns each
worker a limited range in the identity space. When the iden-
tity range in a worker runs out, the remote identity manager
asks the primary manager for a new range.
2.2.4 Load-Balancing
Our central scheduler implements load-balancing as we
saw in Section 2.2.2. However, DOHA’s state management
support is agnostic to the way application components are
distributed across workers. Building eﬃcient distributed al-
gorithms for games is an active area of research that is out-
side the scope of our work. We aim to provide the neces-
sary mechanisms so application developers can implement
their favorite distributed load-balancing algorithms on top
of DOHA.

In the concurrent version of RAPT, we partition the game
map geographically into a number of grids equal to the num-
ber of workers. Each worker handles a grid with all associ-
ated entities (enemies, and players). The state of each en-
tity is updated in a single worker. This design respects data
locality since each entity primarily interacts with other en-
tities in its vicinity. Local interactions avoid expensive state
transfer across worker boundaries. When entities move be-
tween grids, they migrate with all their state to a diﬀerent
worker.

Even though static geographical partitioning does not dis-
tribute work evenly across workers, our experience in few
popular state of the art web-based games suggests that de-
signers distribute game entities evenly across the game map.
To help developers implement the load-balancing algorithms,
DOHA provides:

(cid:129) load information so developers can use it to decide

when to migrate entities.

(cid:129) a distributed identity manager which names entities
uniquely, thus avoiding name conﬂicts upon migration.
(cid:129) A loosely-coupled communication API to easily set-up
and tear-down communication channels for frequent
entity migration.

Developers need to develop the load-balancing policy and
then use our communication layer to send the entity state.
DOHA aims to support applications with diﬀerent concur-
rency requirements, ranging from simple applications that
only need the computational beneﬁts of web workers to the
more demanding web-based games. Simple applications with-
out shared state can delegate load balancing and scheduling
of remote events to the central scheduler. For more ad-
vanced applications with shared state across workers, DOHA
provides a publish-subscribe communication layer to man-
age state.
In our eﬀorts to parallelize RAPT, we initially

tempted to isolate a major game component, such as the
physics engine in a worker. This would have been easier and
can probably enhance performance. However, it does not
scale with the number of cores. Even though current mo-
bile platforms have at most dual-core processors, RAPT and
other web applications should aim for scalable parallelism to
improve performance with more cores.

3. EVALUATION

We conducted a set of experiments with gaming scenarios
of various computational demands. In the basic test map for
RAPT, both players move inside a horizontal tunnel in one
direction and the enemies move in a parallel tunnel above
the players. We compare the following game versions: the
original RAPT (RAPT), the modiﬁed RAPT using adapta-
tion only (RAPT-A), and the modiﬁed concurrent RAPT
(RAPT-C) with 2 web workers.

Our evaluation takes two views on performance: the ﬁrst
based on lower-level event-loop execution metrics, and the
second based on higher-level application metrics. The low-
level metrics include: number of events submitted per sec-
ond, and the ratio of cancelled events. These low-level met-
rics show the throughput of the event-loop (events per sec-
ond). To understand how these low level metrics aﬀect game
quality, we analyze the quality of the gameplay experience
using high-level metrics, such as the simulation loop jit-
ter proﬁle (jitter median and jitter tail which is the 95th
percentile of the jitter distribution) to quantify the average
timeliness and the magnitude of execution glitches, and the
average frames per second (FPS) versus priority for all enti-
ties to quantify the average game quality (scalable quality).
We performed our experiments on an AMD Opteron with
16 2GHz cores. Multi-core hardware allowed web workers to
run on diﬀerent cores. The duration of each experiment is 80
seconds. To avoid start-up and shutdown eﬀects, we use the
middle 60 seconds. We used Google Chrome 15.0.874.102
beta in Ubuntu 10.04 LTS. The two changing experiment
parameters are the computational diﬃculty of the game sce-
nario (which is controlled by the number and type of ene-
mies) and the game version (RAPT, RAPT-A, and RAPT-
C). We have three game scenarios: an easy scenario where all
versions have reasonable quality; a medium scenario which is
the hardest playable scenario by RAPT and RAPT-A (with
the processing power of one core); and ﬁnally an extremely
challenging game scenario with processing requirements be-
yond the capacity of one core.
3.1 Adaptive Execution

In this section we discuss the eﬀects of our adaptation
model on game performance. We analyze the low-level event-
loop throughput, the timeliness of simulation loop updates,
and the average overall game quality (scalable quality).

3.1.1 Timeliness
Table 3 shows the simulation loop jitter proﬁle for all
RAPT versions running all scenarios. The median jitter
gives a measure of average timeliness and agility in respond-
ing to stimuli, such as input and collisions. To quantify
glitches which aﬀect quality negatively, we measure the jit-
ter tail. The expected inter-arrival time between frames is
33.3ms since the target frame rate is 30FPS (frame duration
1000ms/30). We measure the oﬀset for the expected arrival

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France166Jitter (ms)

Median

Tail

Table 3: Jitter Proﬁle

Scenario RAPT RAPT-A RAPT-C
Easy
22
Medium 47
219
Hard
Easy
26
Medium 55
Hard

1
0
0
7
17
33

0
0
0
7
8
8

291

time and report its median and 95th percentile to capture
the jitter distribution.

As seen in Table 3, the jitter median and tail in the origi-
nal RAPT increases with the diﬃculty of the game scenario.
The median jitter reaches 219ms which means RAPT exe-
cutes 1 out of 7 frames (219/33.3=6.6) yielding a frame rate
of around 4 FPS. This increasing jitter is due to RAPT’s
game loop iterating over all game entities in each frame lead-
ing to a large delay in processing each frame.

RAPT-A has low consistent jitter proﬁle (median=0ms
and tail=8ms) for all game scenarios. Our reactive event-
driven design gives timer events more importance than best-
eﬀort events in each game loop frame. When the timer event
running the game simulation loop (global update) ﬁres, we
stop execution of best-eﬀort events and delete all pending
events from the previous frame. Finally, RAPT-C has low
consistent median jitter. But the jitter tail increases with
the diﬃculty of the scenario to reach 33ms in the hard sce-
nario (95% of the jitter values are less than 33ms). This
increase in the jitter tail is mainly due to the communica-
tion and OS scheduling spikes for the two web workers.

3.1.2 Low-Level Event-loop Statistics

Table 4: Event Throughput Statistics

Statistics

Event Submission Rate

Cancellation Ratio (%)

7417

Scenario RAPT-A RAPT-C
Easy
Medium 11150
33110
Hard
Easy
2.2
Medium 17
Hard
88

7127
10749
31317
0.1
18
66

Our low-level event-loop statistics help us understand the
event throughput. As seen in Table 4, both RAPT-A and
RAPT-C submit more events as the diﬃculty increases be-
cause of the increase in the number of entities. The cancel-
lation ratio also increases because the frame duration is not
enough to update all entities as the diﬃculty increases.

RAPT-A submits slightly more events per second in all
scenarios than RAPT-C indicating a higher simulation rate.
The ratio of cancelled events in the easy and medium sce-
narios (RAPT-A and RAPT-C) is comparable. In the hard
scenario, RAPT-C cancels less events (66%) than RAPT-A
(88%). Moreover, RAPT-C has higher event rate due to
having 2 extra cores to execute the simulation updates.

3.1.3 Priority Vs Quality
The medium scenario (hardest playable scenario for RAPT-
A and RAPT) tests our capability to gracefully degrade
quality when resources are scarce. RAPT in the medium
scenario has an average quality of 12.3 FPS which is similar
to the simulation rate. When resources are scarce, RAPT-
A and RAPT-C cancel update events for stale low priority

entities. Thus, the overall average game quality (in FPS) is
not captured in the simulation rate (29 and 30 FPS).

To give more meaningful measure of the overall game qual-
ity (scalable quality), we measure the average frames per
second for all game entities. We correlate this quality in-
dicator with the priority assigned by our policy. As we see
in Figure 7, the FPS of game enemies in RAPT-A ranges
from 16 for low priority entities to 29 (maximum) for high
priority entities. Similarly, the average jitter for all entities
decreases from 17ms to 4ms as priority increases (average jit-
ter is inversely related to average FPS). We notice that low
priority entities never starve (have at least 16 FPS). This is
due to our minimum update threshold which ensures that
even low priority entities are updated at a lower frequency.
When CPU is limited, our adaptation model in RAPT-A im-
proves quality for important entities so quality has a strong
correlation with priority.

)
S
P
F
(
 

d
n
o
c
e
S
 
r
e
P
 
s
e
m
a
r
F
 
e
g
a
r
e
v
A

 30

 28

 26

 24

 22

 20

 18

 16

 14

 0

 0.2

 0.4

 0.6

 0.8

 1

Priority

RAPT-A

RAPT-C

Figure 7: Priority Versus Quality (Average FPS)

RAPT-C has relatively higher quality for all entities (24
FPS). The quality does not have a strong correlation with
priority because at each time instance game entities with
the highest priority are concentrated in one worker (due to
geographical partitioning of entities). Other workers at the
same time instance are processing events with low prior-
ity leading average FPS to lose correlation with priority.
In addition, the medium scenario with RAPT-C uses three
separate cores (2 workers and the browser thread) and our
adaptation scheme helps more when resources are limited.
3.1.4 Results Summary
RAPT-A and RAPT-C have better timing (lower jitter
mean and tail) than the original RAPT. By cancelling up-
dates of less important entities at the end of each simulation
loop, RAPT-A and RAPT-C can provide important enti-
ties (with more inﬂuence over quality) a higher update rate.
This translates to better overall game quality in RAPT-A
and RAPT-C.
3.2 Concurrent Execution

While playing the game, we noticed RAPT-A had much
worse perceived quality than RAPT-C in the hard scenario.
RAPT-A’s main thread was overwhelmed by the extremely
high load and it was not yielding execution to the browser
engine (to perform the rendering).
In this case, isolation
between the two tasks (simulation and rendering) in RAPT-
C provided much better perceived quality.

The hard scenario is not playable in either RAPT or RAPT-

A. RAPT-A was overwhelmed by the load and the simula-

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France167Table 5: Jitter Proﬁle in the Easy Scenario (with
one core)

Jitter (ms) RAPT RAPT-A RAPT-C
Median
Tail

25
28

5
8

17
28

tion rate in the original RAPT is extremely low (2 FPS).
The hard scenario is only playable in RAPT-C. Even though
the hard scenario had a large number of enemies, our design
scales the communication costs. RAPT-C only executes and
communicates state updates for as many events as the frame
duration allows.

To evaluate the eﬀects of adding more cores on game qual-
ity, we run RAPT-C using the hard scenario while varying
the number of cores. As we see in Figure 8, the average FPS
for all game entities (scalable quality) increases as we add
more cores. RAPT-C with 1 worker gets an average of 3.5
FPS. With 3 workers, the average FPS is between 12 and
14. Average jitter also drops from 260ms with 1 worker to
around 42ms with 3 workers. In the hard scenario, we get
linear improvement in quality with each worker added up
to 3 workers. As we see in Figure 8, using 4 workers does
not improve the game quality. With 4 workers, only 3% of
the execution events are cancelled which indicates the ap-
plication load in the hard scenario is small relative to the
available cores. In this case, RAPT-C pays additional con-
currency costs but does not beneﬁt from the extra core.

)
S
P
F
(
 

d
n
o
c
e
S
 
r
e
P
 
s
e
m
a
r
F
 
e
g
a
r
e
v
A

 14

 12

 10

 8

 6

 4

 2

 0

 0.2

 0.4

 0.6

 0.8

 1

Priority

3 Workers 
1 Workers 

2 Workers 
4 Workers 

Figure 8: RAPT FPS in HARD scenario with 1, 2,
3 and 4 workers

To look at timeliness as we add more cores, we measure
the jitter distribution of the simulation loop frames in the
hard scenario. RAPT has by far the worst jitter proﬁle. As
we see in Figure 9, only 20% of execution frames have jitter
less than 210ms. RAPT’s jitter tail extends to around 291ms
causing signiﬁcantly bigger execution glitches. RAPT-A and
RAPT-C (with 1 and 2 workers) have the same low average
jitter proﬁle and RAPT-C has a relatively worse jitter tail.
We also observe that latency increases as we add more
cores. With 3 workers, the mean jitter is 14ms and the
jitter tail is 64ms. The jitter increase is partly because the
simulation loop in worker threads is triggered by a periodic
update event sent from the main thread. When the number
of workers increase, the communication load on the main
thread increases and the loop update events are delayed.
3.2.1 Less Cores Than Workers
To test what will happen if we have more workers than
cores, we ran the medium scenario in a single core machine

1

.8

.6

.4

.2

0

 0

 50

 100

 150

 200

 250

 300

Jitter (ms)

RAPT
RAPT-A
2 Worker RAPT-C

4 Worker RAPT-C
3 Worker RAPT-C
1 Worker RAPT-C

Figure 9: Jitter Cumulative Distribution in Hard
Scenario

(a 2.80GHz Intel(R) Pentium 4). As we see in Table 5,
RAPT-C has lower FPS and higher jitter tail than RAPT-
A (but comparable quality to RAPT). We also noticed in
the low-level event-loop statistics that RAPT-C submitted
less events and cancelled more. This performance gap is due
to the overhead of communication between and scheduling
of the worker threads and the lack of any parallel speed-up
using the one core machine.

Ideally we should have one worker per core. Degradation
in performance is expected if we use more or less workers
than necessary. To help applications choose the appropriate
number of workers, browsers can have an API to expose
the number of cores (user agent information) or JavaScript
library developers can detect it (using micro tests).

3.2.2 Results Summary
DOHA demonstrates the potential to scale quality linearly
as we use more cores in a challenging game scenario. It is
essential to choose the appropriate number of workers for
the application execution and communication load, and the
underlying hardware. Using more workers than needed (4
workers case in Figure 9) or using more workers than cores
(Table 5) can reduce performance due to concurrency over-
heads (without getting any parallel speed-up).

4. LESSONS LEARNED

This section includes few of the subjective lessons learned
which can shed more light on DOHA and web-based game
development. We noticed that:
(cid:129) Performance engineering inside browsers is challeng-
ing. Browsers have primitive debugging and perfor-
mance monitoring tools. Web workers have even less
support. To conduct a rigorous experimental study
and quantify performance, we had to build a com-
plete performance analysis infrastructure. We built a
parallel performance monitor to capture performance
data from workers, and a visualization tool to display
performance signals in real-time for interactive perfor-
mance debugging. We are developing a tool to record
and reply performance data and some execution state
to enable interactive performance analysis oﬄine.

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France168(cid:129) Game adaptation in RAPT, required minimal code
changes. To introduce priority and cancellation of
events we only had to change the core game loop as
described in Section 2.1.1. Using the central sched-
uler was relatively easy having already used explicit
events because most of the distribution tasks are del-
egated to the central scheduler. However, using web
workers with a distributed application design requires
re-structuring existing code in a major way. For exam-
ple, RAPT was modiﬁed to have a distributed game
loop and we used replication to manage shared state
as described in section 2.2.3. Introducing the publish-
subscribe layer improved the abstraction but develop-
ers still need to write complex distributed algorithms.
Finally, parallel and adaptive execution are indepen-
dent and can be adopted separately even though they
are introduced together within the framework of scal-
able quality. Applications can use DOHA’s publish-
subscribe communication layer without adopting ex-
plicit events and the other way around.
(cid:129) HTML5 web workers expose an elegant shared-nothing
concurrency abstraction. Explicit message-passing is
a good ﬁt for asynchronous event-driven browser ex-
ecution. It also allows web developers to use familiar
distributed computing abstractions (from their expe-
rience with server components). The main limitation
is the high communication costs in implementations.
Current communication cost in browsers will enable
applications to scale quality in multi-core platforms.
To scale in many-core platforms, the communication
costs needs to be reduced signiﬁcantly. Browsers need
to optimize the communication channels and expose
optimization mechanisms (e.g., use immutable objects
with ownership transfer to pass large objects across
workers). JavaScript frameworks similar to DOHA
can also perform communication optimization (batch-
ing and pipelining) to reduce messaging costs.
(cid:129) DOHA is applicable in other web multimedia applica-
tions, such as video applications, visualizations, and
animations. We observed the same event-driven ar-
chitecture in the few animation and visualization plat-
forms we studied. To extend our support to server-
side game components, we ported DOHA to node.js
[12], a popular JavaScript server framework. Our fu-
ture work aims to use scalable quality in other appli-
cation domains and perform an in-depth study of the
adaptation and load-balancing policies required. Cur-
rently, our adaptation policies and the partitioning al-
gorithms are developed separately. We think develop-
ing a load-balancing algorithm that is quality-aware
will improve RAPT-C quality signiﬁcantly. For exam-
ple, it can distribute high priority entities evenly across
cores to maximize their chance of getting updated.

5. RELATED WORK

DOHA builds upon the event-driven nature of popular
web applications, which have a large number of short han-
dler functions [22]. Event-driven programming is a natu-
ral ﬁt for current JavaScript engines with single execution
thread and asynchronous DOM APIs. Event-driven reactiv-
ity in DOHA has its roots in the concepts of reactive pro-
gramming [3]. DOHA introduces event classes and speciﬁes

timing information at the event level similar to the applica-
tion model in Cooperative Polling [14].

Unlike conventional multimedia adaptation techniques [16],
our model does not require estimation of resource require-
ments, simplifying its usage drastically. DOHA application-
level adaptation is inspired by priority-progress adaptation
[15]. This adaptation technique was developed in multime-
dia video streaming with three main principles: incremental
quality, priority assignment based on the contribution to
perceived quality (temporal or spatial), and ﬁnally ordering
computations according to priority. In DOHA, we developed
CPU adaptation policies for web-based games and extended
the adaptation model across parallel threads. To improve
timing in networked applications, all resources (CPU, net-
work, and storage) need to be considered [13]. Paceline [9]
is a transport protocol above TCP that uses the same adap-
tation principles as DOHA to improve data communication
timing in media-streaming applications. DonneyBrook’s [6]
interest sets use distance, aim, and recency from the player’s
perspective to decide which entities are more important.
Similar to interest sets, our CPU adaptation policy uses dis-
tance from the player to determine the importance of game
entities, but our priority scheme has a continuous spectrum
(between 0.0 and 1.0) allowing smooth scalability instead of
the two priority levels in DonneyBrook.

Recently, developers used web workers to separate the
physics engine of a simple animation [17] improving the an-
imation’s frame rate. However, oﬄoading functional units
limits scalability to the number of independent units while
DOHA aims for scalable parallelism using web workers.

Parallel game servers use techniques, such as Synchroniza-
tion via scheduling [5] and Software transactional memory
[18] to manage state. These techniques assume shared mem-
ory while web workers have no sharing and use message-
passing. Similar to the Multikernel [2], we embrace the
network nature of concurrent systems and re-structure our
experimental web-based game as a network of distributed
components. We use replication to share state using ideas
from the distributed architecture of interactive multi-player
games in Colyseus [7]. To manage concurrency in DOHA’s
central scheduler, we use coloring [27] which is a coarse grain
technique that is easier to use than explicit dependencies be-
tween events in Grand Central Dispatch [11].

To improve the browser performance, the parallel browser
project [19] re-writes the bottlenecks (parsing and rule match-
ing) in a parallel fashion. Application-level concurrency is
equally important especially with the slower pace of change
in browsers. Native Client [26] allows web applications to
execute native code inside a browser sandbox and improve
performance with hand-coded assembler and native threads.
DOHA aims to improve the performance of applications
written in JavaScript, the de-facto language for web appli-
cations. Using native code in the browser is complementary
to our work especially since JavaScript engines are becom-
ing more mature. The exokernel browser architecture in
Atlantis [20] deﬁnes a narrow API for basic services and
allows web applications to extend their execution environ-
ments. Atlantis’ run-time language, Syphon, supports a full
threading model. Even though the performance of threads
is arguably superior to web workers with message-passing,
the performance gains come at the high cost of introducing
a concurrency model that causes most systems errors [23].

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France1696. CONCLUSIONS

Browsers are becoming mature platforms. Ambitious web
applications with high computational demands and low la-
tency interactions, such as games, animations, and interac-
tive visualizations are pushing the limits of available pro-
cessing resources. The best-eﬀort execution model of cur-
rent browsers lacks the necessary mechanisms to help ap-
plications control quality and balance between timeliness
and utilization in overload conditions. Even though HTML5
web workers provide a concurrency model to utilize multi-
core resources, web developers still need more programming
support to deal with hard concurrent software development
issues, such as state management and load balancing.

With hints from application-level adaptation policies, DOHA

favors important events that have more inﬂuence over ap-
plication quality. To scale web application quality with
widely available multi-core processors, DOHA simpliﬁes con-
current real-time web development. When CPU resources
are scarce, the modiﬁed game using DOHA had better tim-
ing and higher overall quality. More importantly, the qual-
ity scales linearly with a small number of cores. Scalable
quality enables ambitious web applications to explore more
challenging scenarios without the fear of brittle quality.

DOHA and the modiﬁed RAPT versions are open source

and may be downloaded from http://qstream.org

7. REFERENCES

[1] Ambiera. Copperlicht - fast webgl javascript 3d

engine. http://www.ambiera.com/copperlicht/.
[accessed 30-Oct-2011].

[2] A. Baumann, P. Barham, P.-E. Dagand, T. Harris,
R. Isaacs, S. Peter, T. Roscoe, A. Sch¨upbach, and
A. Singhania. The multikernel: a new os architecture
for scalable multicore systems. SOSP ’09, pages 29–44,
New York, NY, USA, 2009. ACM.

[3] G. Berry, G. Gonthier, A. B. G. Gonthier, and P. S.

Laltte. The esterel synchronous programming
language: Design, semantics, implementation, 1992.
[4] M. J. Best, A. Fedorova, R. Dickie, A. Tagliasacchi,

A. Couture-Beil, C. Mustard, S. Mottishaw,
A. Brown, Z. F. Huang, X. Xu, N. Ghazali, and
A. Brownsword. Searching for concurrent design
patterns in video games. Euro-Par ’09, pages 912–923,
Berlin, Heidelberg, 2009. Springer-Verlag.

[5] M. J. Best, S. Mottishaw, C. Mustard, M. Roth,

A. Fedorova, and A. Brownsword. Synchronization via
scheduling: techniques for eﬃciently managing shared
state. SIGPLAN Not., 46:640–652, June 2011.

[6] A. Bharambe, J. R. Douceur, J. R. Lorch,

T. Moscibroda, J. Pang, S. Seshan, and X. Zhuang.
Donnybrook: enabling large-scale, high-speed,
peer-to-peer games. SIGCOMM Comput. Commun.
Rev., 38(4):389–400, 2008.

[7] A. Bharambe, J. Pang, and S. Seshan. Colyseus: a

distributed architecture for online multiplayer games.
NSDI’06, pages 12–12. USENIX Association, 2006.

[8] M. Cook. Pistol slut. http://pistolslut.com, 2011.

[accessed 3-May-2011].

[9] A. Erbad, M. Tayarani Najaran, and C. Krasic.
Paceline: latency management through adaptive
output. In ACM MMSys ’10, pages 181–192, 2010.

[10] I. Hickson. Web workers.

http://dev.w3.org/html5/workers/, 2009.
[11] A. Inc. Concurrency programming guide.

http://developer.apple.com/, 2009. [accessed
9-Oct-2009].

[12] I. Joyent. Node.js: Evented i/o for v8 javascript.

http://nodejs.org/. [accessed 2-Nov-2011].
[13] C. Krasic and J.-S. L´egar´e. Interactivity and
scalability enhancements for quality-adaptive
streaming. In MM ’08: Proceeding of the 16th ACM
international conference on Multimedia, pages
753–756, New York, NY, USA, 2008. ACM.

[14] C. Krasic, M. Saubhasik, A. Sinha, and A. Goel. Fair

and timely scheduling via cooperative polling. In
EuroSys ’09, pages 103–116, New York, NY, USA,
2009. ACM.

[15] C. Krasic, J. Walpole, and W. Feng. Quality-adaptive
media streaming by priority drop. In NOSSDAV ’03,
pages 112–121, June 2003.

[16] R. Kuschnig, I. Koﬂer, and H. Hellwagner. An

evaluation of tcp-based rate-control algorithms for
adaptive internet streaming of h.264/svc. In ACM
MMSys ’10, pages 157–168, 2010.

[17] S. Ladd. Box2d, web workers, better performance.

http://blog.sethladd.com/, 2011.

[18] D. Lupei, B. Simion, D. Pinto, M. Misler, M. Burcea,

W. Krick, and C. Amza. Transactional memory
support for scalable and transparent parallelization of
multiplayer games. In EuroSys ’10, pages 41–54, New
York, NY, USA, 2010. ACM.

[19] L. A. Meyerovich and R. Bod´ık. Fast and parallel

webpage layout. In WWW’10, Raleigh NC, USA, 2010.

[20] J. Mickens and M. Dhawan. Atlantis: Robust,

extensible execution environments for web
applications. SOSP ’11. ACM, 2011.

[21] Mozilla. High scores.

https://gaming.mozillalabs.com/games/winners,
2011. [accessed 3-May-2011].

[22] B. Z. Paruj Ratanaworabhan, Ben Livshits. Jsmeter:

Comparing the behavior of javascript benchmarks
with real web applications. In WebApps’10, Boston
MA, USA, 2010.

[23] L. Ryzhyk, P. Chubb, I. Kuz, and G. Heiser. Dingo:

taming device drivers. In EuroSys ’09, pages 275–288.
ACM, 2009.

[24] D. Szablewski. Biolab disaster.

http://playbiolab.com, 2011. [accessed 3-May-2011].

[25] E. Wallace, J. Ardini, and K. Gishen. Robots are

people too. http://raptjs.com, 2011. [accessed
3-May-2011].

[26] B. Yee, D. Sehr, G. Dardyk, J. B. Chen, R. Muth,

T. Ormandy, S. Okasaka, N. Narula, and N. Fullagar.
Native client: a sandbox for portable, untrusted x86
native code. Commun. ACM, 53:91–99, Jan. 2010.

[27] N. Zeldovich, A. Yip, F. Dabek, R. T. Morris,

D. Mazi ˜A´lres, and F. Kaashoek. Multiprocessor
support for event-driven programs. In USENIX ATC
2003, pages 239–252. USENIX, 2003.

WWW 2012 – Session: Web PerformanceApril 16–20, 2012, Lyon, France170