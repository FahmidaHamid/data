Estimating the Prevalence of Deception

in Online Review Communities

Myle Ott

Dept. of Computer Science

Cornell University
Ithaca, NY 14850

myleott@cs.cornell.edu

Claire Cardie

Depts. of Computer Science

and Information Science

Cornell University
Ithaca, NY 14850

Jeff Hancock

Depts. of Communication and

Information Science
Cornell University
Ithaca, NY 14850

cardie@cs.cornell.edu

jeff.hancock@cornell.edu

ABSTRACT
Consumers’ purchase decisions are increasingly inﬂuenced
by user-generated online reviews [3]. Accordingly, there has
been growing concern about the potential for posting decep-
tive opinion spam—ﬁctitious reviews that have been deliber-
ately written to sound authentic, to deceive the reader [15].
But while this practice has received considerable public at-
tention and concern, relatively little is known about the ac-
tual prevalence, or rate, of deception in online review com-
munities, and less still about the factors that inﬂuence it.

We propose a generative model of deception which, in con-
junction with a deception classiﬁer [15], we use to explore the
prevalence of deception in six popular online review commu-
nities: Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor,
and Yelp. We additionally propose a theoretical model of
online reviews based on economic signaling theory [18], in
which consumer reviews diminish the inherent information
asymmetry between consumers and producers, by acting as a
signal to a product’s true, unknown quality. We ﬁnd that de-
ceptive opinion spam is a growing problem overall, but with
diﬀerent growth rates across communities. These rates, we
argue, are driven by the diﬀerent signaling costs associated
with deception for each review community, e.g., posting re-
quirements. When measures are taken to increase signaling
cost, e.g., ﬁltering reviews written by ﬁrst-time reviewers,
deception prevalence is eﬀectively reduced.

Categories and Subject Descriptors
I.2.7 [Artiﬁcial Intelligence]: Natural Language Process-
ing; J.4 [Computer Applications]: Social and Behavioral
Sciences—economics, psychology; K.4.1 [Computers and
Society]: Public Policy Issues—abuse and crime involving
computers; K.4.4 [Computers and Society]: Electronic
Commerce

General Terms
Algorithms, Experimentation, Measurement, Theory

Keywords
Deceptive opinion spam, Deception prevalence, Gibbs sam-
pling, Online reviews, Signaling theory

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

1.

INTRODUCTION

Consumers rely increasingly on user-generated online re-
views to make, or reverse, purchase decisions [3]. Accord-
ingly, there appears to be widespread and growing concern
among both businesses and the public [12, 14, 16, 19, 20,
21] regarding the potential for posting deceptive opinion
spam—ﬁctitious reviews that have been deliberately writ-
ten to sound authentic, to deceive the reader [15]. Perhaps
surprisingly, however, relatively little is known about the ac-
tual prevalence, or rate, of deception in online review com-
munities, and less still is known about the factors that can
inﬂuence it. On the one hand, the relative ease of producing
reviews, combined with the pressure for businesses, prod-
ucts, and services to be perceived in a positive light, might
lead one to expect that a preponderance of online reviews
are fake. One can argue, on the other hand, that a low rate
of deception is required for review sites to serve any value.1
The focus of spam research in the context of online reviews
has been primarily on detection. Jindal and Liu [8], for
example, train models using features based on the review
text, reviewer, and product to identify duplicate opinions.2
Yoo and Gretzel [23] gather 40 truthful and 42 deceptive
hotel reviews and, using a standard statistical test, manually
compare the psychologically relevant linguistic diﬀerences
between them. While useful, these approaches do not focus
on the prevalence of deception in online reviews.

Indeed, empirical, scholarly studies of the prevalence of
deceptive opinion spam have remained elusive. One reason
is the diﬃculty in obtaining reliable gold-standard annota-
tions for reviews, i.e., trusted labels that tag each review
as either truthful (real) or deceptive (fake). One option for
producing gold-standard labels, for example, would be to
rely on the judgements of human annotators. Recent stud-
ies, however, show that deceptive opinion spam is not eas-
ily identiﬁed by human readers [15]; this is especially the
case when considering the overtrusting nature of most hu-
man judges, a phenomenon referred to in the psychological

1It is worth pointing out that a review site containing de-
ceptive reviews might still serve value, for example, if there
remains enough truthful content to produce reasonable ag-
gregate comparisons between oﬀerings.
2Duplicate (or near-duplicate) opinions are opinions that
appear more than once in the corpus with the same (or sim-
ilar) text. However, simply because a review is duplicated
does not make it deceptive. Furthermore, it seems unlikely
that either duplication or plagiarism characterizes the ma-
jority of fake reviews. Moreover, such reviews are potentially
detectable via oﬀ-the-shelf plagiarism detection software.

WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France201deception literature as a truth bias [22]. To help illustrate
the non-trivial nature of identifying deceptive content, given
below are two positive reviews of the Hilton Chicago Hotel,
one of which is truthful, and the other of which is deceptive
opinion spam:

1. “My husband and I stayed in the Hilton Chicago and
had a very nice stay! The rooms were large and com-
fortable. The view of Lake Michigan from our room
was gorgeous. Room service was really good and quick,
eating in the room looking at that view, awesome! The
pool was really nice but we didnt get a chance to use it.
Great location for all of the downtown Chicago attrac-
tions such as theaters and museums. Very friendly staﬀ
and knowledgable, you cant go wrong staying here.”

2. “We loved the hotel. When I see other posts about
it being shabby I can’t for the life of me ﬁgure out
what they are talking about. Rooms were large with
TWO bathrooms, lobby was fabulous, pool was large
with two hot tubs and huge gym, staﬀ was courteous.
For us, the location was great–across the street from
Grant Park with a great view of Buckingham Fountain
and close to all the museums and theatres. I’m sure
others would rather be north of the river closer to the
Magniﬁcent Mile but we enjoyed the quieter and more
scenic location. Got it for $105 on Hotwire. What a
bargain for such a nice hotel.”

Answer: See footnote.3

The diﬃculty of detecting which of these reviews is fake
is consistent with recent large meta-analyses demonstrating
the inaccuracy of human judgments of deception, with accu-
racy rates typically near chance [1]. In particular, humans
have a diﬃcult time identifying deceptive messages from
cues alone, and as such, it is not surprising that research on
estimating the prevalence of deception (see Section 8.2) has
generally relied on self-report methods, even though such
reports are diﬃcult and expensive to obtain, especially in
large-scale settings, e.g., the web [5]. More importantly, self-
report methods, such as diaries and large-scale surveys, have
several methodological concerns, including social desirability
bias and self-deception [4]. Furthermore, there are consid-
erable disincentives to revealing one’s own deception in the
case of online reviews, such as being permanently banned
from a review portal, or harming a business’s reputation.

Recently, automated approaches (see Section 4.1) have
emerged to reliably label reviews as truthful vs. deceptive:
Ott et al. [15] train an n-gram–based text classiﬁer using a
corpus of truthful and deceptive reviews—the former culled
from online review communities and the latter generated
using Amazon Mechanical Turk (http://www.mturk.com).
Their resulting classiﬁer is nearly 90% accurate.

In this work, we present a general framework (see Sec-
tion 2) for estimating the prevalence of deception in online
review communities. Given a classiﬁer that distinguishes
truthful from deceptive reviews (like that described above),
and inspired by studies of disease prevalence [9, 10], we pro-
pose a generative model of deception (see Section 3) that
jointly models the classiﬁer’s uncertainty as well as the ground-
truth deceptiveness of each review. Inference for this model,

which we perform via Gibbs sampling, allows us to estimate
the prevalence of deception in the underlying review commu-
nity, without relying on either self-reports or gold-standard
annotations.

We further propose a theoretical component to the frame-
work based on signaling theory from economics [18] (see Sec-
tion 6) and use it to reason about the factors that inﬂuence
deception prevalence in online review communities. In our
context, signaling theory interprets each review as a signal
to the product’s true, unknown quality; thus, the goal of
consumer reviews is to diminish the inherent information
asymmetry between consumers and producer. Very brieﬂy,
according to a signaling theory approach, deception preva-
lence should be a function of the costs and beneﬁts that
accrue from producing a fake review. We hypothesize that
review communities with low signaling cost, such as commu-
nities that make it easy to post a review, and large beneﬁts,
such as highly traﬃcked sites, will exhibit more deceptive
opinion spam than those with higher signaling costs, such
as communities that establish additional requirements for
posting reviews, and lower beneﬁts, such as low site traﬃc.
We apply our approach to the domain of hotel reviews.
In particular, we examine hotels from the Chicago area, re-
stricting attention to positive reviews only, and instantiate
the framework for six online review communities (see Sec-
tion 5): Expedia (http://www.expedia.com), Hotels.com
(http://www.hotels.com), Orbitz (http://www.orbitz.com),
Priceline (http://www.priceline.com), TripAdvisor (http:
//www.tripadvisor.com), and Yelp (http://www.yelp.com).
We ﬁnd ﬁrst that the prevalence of deception indeed varies
by community. However, because it is not possible to vali-
date these estimates empirically (i.e., the gold-standard rate
of deception in each community is unknown), we focus our
discussion instead on the relative diﬀerences in the rate of
deception between communities. Here, the results conﬁrm
our hypotheses and suggest that deception is most prevalent
in communities with a low signal cost. Importantly, when
measures are taken to increase a community’s signal cost,
we ﬁnd dramatic reductions in our estimates of the rate of
deception in that community.

2. FRAMEWORK

In this section, we propose a framework to estimate the
prevalence, or rate, of deception among reviews in six on-
line review communities. Since reviews in these communi-
ties do not have gold-standard annotations of deceptiveness,
and neither human judgements nor self-reports of deception
are reliable in this setting (see discussion in Section 1), our
framework instead estimates the rates of deception in these
communities using the output of an imperfect, automated
deception classiﬁer.
In particular, we utilize a supervised
machine learning classiﬁer, which has been shown recently
by Ott et al. [15] to be nearly 90% accurate at detecting
deceptive opinion spam in a class-balanced dataset.

A similar framework has been used previously in stud-
ies of disease prevalence, in which gold-standard diagnostic
testing is either too expensive, or impossible to perform [9,
10]. In such cases, it is therefore necessary to estimate the
prevalence of disease in the population using a combination
of an imperfect diagnostic test, and estimates of the test’s
positive and negative recall rates.4

3The ﬁrst review is deceptive opinion spam.

4Recall rates of an imperfect diagnostic test are unlikely to

WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France202Figure 1: The Bayesian Prevalence Model in plate
notation. Shaded nodes represent observed vari-
ables, and arrows denote dependence. For example,
f (xi) is observed, and depends on η∗, θ∗, and yi.

4. Prevalence Models (Section 3):

E[πf ] = E

Our proposed framework is summarized here, with each
step discussed in greater detail in the corresponding section:

1. Data (Section 5):

i=1

Assume given a set of labeled training reviews, Dtrain =
{(xi, yi)}N train
, where, for each review i, yi ∈ {0, 1}
gives the review’s label (0 for truthful, 1 for deceptive),
and xi ∈ R|V | gives the review’s feature vector repre-
sentation, for some feature space of size |V |. Similarly,
assume given a set of labeled truthful development re-
views, Ddev = {(xi, 0)}N dev
i=1 , and a set of unlabeled test
reviews, Dtest = {xi}N test
i=1 .

2. Deception Classiﬁer (Section 4.1):

Using the labeled training reviews, Dtrain, learn a su-
pervised deception classiﬁer, f : R|V | → {0, 1}.

3. Classiﬁer Sensitivity and Speciﬁcity (Section 4.2):

By cross-validation on Dtrain, estimate the sensitivity
(deceptive recall) of the deception classiﬁer, f , as:

η = Pr(f (xi) = 1| yi = 1).

(1)
Then, use Ddev to estimate the speciﬁcity (truthful
recall) of the deception classiﬁer, f , as:
θ = Pr(f (xi) = 0| yi = 0).

(2)

Finally, use f , η, θ, and either the Na¨ıve Prevalence
Model (Section 3.1), or the generative Bayesian Preva-
lence Model (Section 3.2), to estimate the prevalence
of deception, denoted π, among reviews in Dtest. Note
that if we had gold-standard labels, {yi}N test
i=1 , the gold-
standard prevalence of deception would be:

N test(cid:88)

i=1

∗

π

=

1

N test

yi.

(3)

3. PREVALENCE MODELS

In Section 2, we propose a framework to estimate the
prevalence of deception in a group of reviews using only
the output of a noisy deception classiﬁer. Central to this
framework is the Prevalence Model, which models the un-
certainty of the deception classiﬁer, and ultimately produces
the desired prevalence estimate. In this section, we propose
two competing Prevalence Models, which can be used inter-
changeably in our framework.
3.1 Naïve Prevalence Model

The Na¨ıve Prevalence Model (na¨ıve) estimates the preva-
lence of deception in a corpus of reviews by correcting the
output of a noisy deception classiﬁer according to the clas-
siﬁer’s known performance characteristics.
Formally, for a given deception classiﬁer, f , let πf be the
number of reviews in Dtest for which f makes a positive
prediction, i.e., the number of reviews for which f predicts
deceptive. Also, let the sensitivity (deceptive recall) and

be known precisely. However, imprecise estimates can often
be obtained, especially in cases where it is feasible to perform
gold-standard testing on a small subpopulation.

speciﬁcity (truthful recall) of f be given by η and θ, respec-
tively. Then, we can write the expectation of πf as:



 1

(cid:88)

N test

x∈Dtest

(cid:88)

=

1

N test

δ[f (x) = 1]

E [δ [f (x) = 1]]

x∈Dtest

∗

+ (1 − θ)(1 − π

∗

),

= ηπ

(4)
where π∗ is the true (latent) rate of deception, and δ[a = b]
is the Kronecker delta function, which is equal to 1 when
a = b, and 0 otherwise.
If we rearrange Equation 4 in terms of π∗, and replace the
expectation of πf with the observed value, we get the Na¨ıve
Prevalence Model estimator:

πna¨ıve =

.

(5)

πf − (1 − θ)
η − (1 − θ)

Intuitively, Equation 5 corrects the raw classiﬁer output,
given by πf , by subtracting from it the false positive rate,
given by 1 − θ, and dividing the result by the diﬀerence
between the true and false positive rates, given by η−(1−θ).
Notice that when f is an oracle,5 i.e., when η = θ = 1, the
Na¨ıve Prevalence Model estimate correctly reduces to the
oracle rate given by f , i.e., πna¨ıve = πf = π∗.
3.2 Bayesian Prevalence Model

Unfortunately, the Na¨ıve Prevalence Model estimate, πna¨ıve,
is not restricted to the range [0, 1]. Speciﬁcally, it is negative
when πf < 1− θ, and greater than 1 when πf > η. Further-
more, the Na¨ıve Prevalence Model makes the unrealistic as-
sumption that the estimates of the classiﬁer’s sensitivity (η)
and speciﬁcity (θ), obtained using the procedure discussed
in Section 4.2 and Appendix B, are exact.

5An oracle is a classiﬁer that does not make mistakes, and
always predicts the true, gold-standard label.

    Ntest απ*yif(xi)η*θ*βγWWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France203The Bayesian Prevalence Model (bayes) addresses these
limitations by modeling the generative process through which
deception occurs, or, equivalently, the joint probability dis-
tribution of the observed and latent data.
In particular,
bayes models the observed classiﬁer output, the true (la-
tent) rate of deception (π∗), as well as the classiﬁer’s true
(latent) sensitivity (η∗) and speciﬁcity (θ∗). Formally, bayes
assumes that our data was generated according to the fol-
lowing generative story:

• Sample the true rate of deception: π∗ ∼ Beta(α)
• Sample the classiﬁer’s true sensitivity: η∗ ∼ Beta(β)
• Sample the classiﬁer’s true speciﬁcity: θ∗ ∼ Beta(γ)
• For each review i:

– Sample the ground-truth deception label:

yi ∼ Bernoulli(π

∗

)

(cid:26) Bernoulli(η∗)

Bernoulli(1 − θ∗)

– Sample the classiﬁer’s output:

f (xi) ∼

if yi = 1
if yi = 0

The corresponding graphical model is given in plate nota-
tion in Figure 1. Notice that by placing Beta prior distribu-
tions on π∗, η∗, and θ∗, bayes enables us to encode our prior
knowledge about the true rate of deception, as well as our
uncertainty about the estimates of the classiﬁer’s sensitivity
and speciﬁcity. This is discussed further in Section 4.2.

A similar model has been proposed by Joseph et al. [10]
for studies of disease prevalence, in which it is necessary to
estimate the prevalence of disease in a population given only
an imperfect diagnostic test. However, that model samples
the total number of true positives and false negatives, while
our model samples the yi individually. Accordingly, while
pilot experiments conﬁrm that the two models produce iden-
tical results, the generative story of our model, given above,
is comparatively much more intuitive.
3.2.1 Inference
While exact inference is intractable for the Bayesian Preva-
lence Model, a popular alternative way of approximating the
desired posterior distribution is with Markov Chain Monte
Carlo (MCMC) sampling, and more speciﬁcally Gibbs sam-
pling. Gibbs sampling works by sampling each variable, in
turn, from the conditional distribution of that variable given
all other variables in the model. After repeating this proce-
dure for a ﬁxed number of iterations, the desired posterior
distribution can be approximated from samples in the chain
by: (1) discarding a number of initial burn-in iterations, and
(2) since adjacent samples in the chain are often highly cor-
related, thinning the number of remaining samples according
to a sampling lag.

The conditional distributions of each variable given the
others can be derived from the joint distribution, which can
be read directly from the graph. Based on the graphical
representation of bayes, given in Figure 1, the joint distri-
bution of the observed and latent variables is just:

Pr(f (x), y, π

∗
, η
Pr(y | π
∗

∗

∗
, θ
) · Pr(π

; α, β, γ) = Pr(f (x)| y, η
)·
∗
∗
, θ
∗ | β) · Pr(θ
∗ | γ),

∗ | α) · Pr(η

(6)

Table 1: Reference 5-fold cross-validated perfor-
mance of an SVM deception detection classiﬁer in
a balanced dataset of TripAdvisor reviews, given by
Ott et al. [15]. F-score corresponds to the harmonic
mean of precision and recall.

metric

performance

Accuracy
Deceptive Precision
Deceptive Recall
Deceptive F-score
Truthful Precision
Truthful Recall
Truthful F-score
Baseline Accuracy

89.6%
89.1%
90.3%
89.7%
90.1%
89.0%
89.6%
50%

where each term is given according to the sampling distri-
butions speciﬁed in the generative story in Section 3.2.

A common technique to simplify the joint distribution,
and the sampling process, is to integrate out (collapse) vari-
ables that do not need to be sampled. If we integrate out π∗,
η∗, and θ∗ from Equation 6, we can derive a Gibbs sampler
that only needs to sample the yi’s at each iteration. The re-
sulting sampling equations, and the corresponding Bayesian
Prevalence Model estimate of the prevalence of deception,
πbayes, are given in greater detail in Appendix A.

4. DECEPTION DETECTION
4.1 Deception Classiﬁer

The next component of the framework given in Section 2
is the deception classiﬁer, which predicts whether each unla-
beled review is truthful (real) or deceptive (fake). Following
previous work [15], we assume given some amount of labeled
training reviews, so that we can train deception classiﬁers
using a supervised learning algorithm.

Previous work has shown that Support Vector Machines
(SVM) trained on n-gram features perform well in decep-
tion detection tasks [8, 13, 15]. Following Ott et al. [15],
we train linear SVM classiﬁers using the LIBSVM [2] soft-
ware package, and represent reviews using unigram and bi-
gram bag-of-words features. While more sophisticated and
purpose-built classiﬁers might achieve better performance,
pilot experiments suggest that the Prevalence Models (see
Section 3) are not heavily aﬀected by minor diﬀerences in
classiﬁer performance. Furthermore, the simple approach
just outlined has been previously evaluated to be nearly 90%
accurate at detecting deception in a balanced dataset [15].
Reference cross-validated classiﬁer performance appears in
Table 1.
4.2 Classiﬁer Sensitivity and Speciﬁcity

Both Prevalence Models introduced in Section 3 can uti-
lize knowledge of the underlying deception classiﬁer’s sensi-
tivity (η∗), i.e., deceptive recall rate, and speciﬁcity (θ∗), i.e.,
truthful recall rate. While it is not possible to obtain gold-
standard values for these parameters, we can obtain rough
estimates of their values (denoted η and θ, respectively)
through a combination of cross-validation, and evaluation
on a labeled development set. For the Na¨ıve Prevalence

WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France204Table 2: Corpus statistics for unlabeled test reviews
from six online review communities.

community

# hotels # reviews

Expedia
Hotels.com
Orbitz
Priceline
TripAdvisor
Yelp
Mechanical Turk

100
103
97
98
104
103
20

4,341
6,792
1,777
4,027
9,602
1,537
400

Model, the estimates are used directly, and are assumed to
be exact.

For the Bayesian Prevalence Model, we adopt an empiri-
cal Bayesian approach and use the estimates to inform the
corresponding Beta priors via their hyperparameters, β and
γ, respectively. The full procedure is given in Appendix B.

5. DATA

In this section, we brieﬂy discuss each of the three kinds of
data used by our framework introduced in Section 2. Corpus
statistics are given in Table 2. Following Ott et al. [15], we
excluded all reviews with fewer than 150 characters, as well
as all non-English reviews.6
5.1 Training Reviews (Dtrain)

Training a supervised deception classiﬁer requires labeled
training data. Following Ott et al. [15], we build a balanced
set of 800 training reviews, containing 400 truthful reviews
from six online review communities, and 400 gold-standard
deceptive reviews from Amazon Mechanical Turk.

Deceptive Reviews: In Section 1, we discuss some of the
diﬃculties associated with obtaining gold-standard labels of
deception, including the inaccuracy of human judgements,
and the problems with self-reports of deception. To avoid
these diﬃculties, Ott et al. [15] have recently created 400
gold-standard deceptive reviews using Amazon’s Mechan-
ical Turk service.
In particular, they paid one US dol-
lar ($1) to each of 400 unique Mechanical Turk workers
to write a fake positive (5-star) review for one of the 20
most heavily-reviewed Chicago hotels on TripAdvisor. Each
worker was given a link to the hotel’s website, and instructed
to write a convincing review from the perspective of a satis-
ﬁed customer. Any submission found to be plagiarized was
rejected. Any submission with fewer than 150 characters
was discarded. To date, this is the only publicly-available7
gold-standard deceptive opinion spam dataset. As such, we
choose it to be our sole source of labeled deceptive reviews
for training our supervised deception classiﬁers. Note that
these same reviews are used to estimate the resulting clas-
siﬁer sensitivity (deceptive recall), via the cross-validation
procedure given in Appendix B.

Truthful Reviews: Many of the same challenges that make
it diﬃcult to obtain gold-standard deceptive reviews, also
apply to obtaining truthful reviews. Related work [8, 11]
has hypothesized that the relative impact of spam reviews

6Language was identiﬁed by the Language Detection Li-
brary: http://code.google.com/p/language-detection/.
7http://www.cs.cornell.edu/~myleott/op_spam

Table 3: Signal costs associated with six online re-
view communities, sorted approximately from high-
est signal cost to lowest. Posting cost is High if users
are required to purchase a product before review-
ing it, and Low otherwise. Exposure beneﬁt is Low,
Medium, or High based on the number of reviews in
the community (see Table 2).

community
Orbitz
Priceline
Expedia
Hotels.com
Yelp
TripAdvisor

posting cost

exposure benefit

High
High
High
High
Low
Low

Low

Medium
Medium
Medium

Low
High

is smaller for heavily-reviewed products, and that therefore
spam should be less common among them. For consistency
with our labeled deceptive review data, we simply label as
truthful all positive (5-star) reviews of the 20 previously
chosen Chicago hotels. We then draw a random sample of
size 400, and take that to be our labeled truthful training
data.
5.2 Development Reviews (Ddev)

By training on deceptive and truthful reviews from the
same 20 hotels, we are eﬀectively controlling our classiﬁer
for topic. However, because this training data is not repre-
sentative of Chicago hotel reviews in general, it is important
that we do not use it to estimate the resulting classiﬁer’s
speciﬁcity (truthful recall). Accordingly, as speciﬁed in our
framework (Section 2), classiﬁer speciﬁcity is instead esti-
mated on a separate, labeled truthful development set, which
we draw uniformly at random from the unlabeled reviews in
each review community. For consistency with the sensitivity
estimate, the size of the draw is always 400 reviews.
5.3 Test Reviews (Dtest)

The last data component of our framework is the set of
test reviews, among which to estimate the prevalence of de-
ception. To avoid evaluating reviews that are too diﬀerent
from our training data in either sentiment (due to negative
reviews), or topic (due to reviews of hotels outside Chicago),
we constrain each community’s test set to contain only pos-
itive (5-star) Chicago hotel reviews. This unfortunately dis-
qualiﬁes our estimates of each community’s prevalence of
deception from being representative of all hotel reviews. No-
tably, estimates of the prevalence of deception among nega-
tive reviews might be very diﬀerent from our estimates, due
to the distinct motives of posting deceptive positive vs. neg-
ative reviews. We discuss this further in Section 9.

6. SIGNAL THEORY

In terms of economic theory, the role of review commu-
nities is to reduce the inherent information asymmetry [18]
between buyers and sellers in online marketplaces, by provid-
ing buyers with a priori knowledge of the underlying quality
of the products being sold [7]. It follows that if reviews regu-
larly failed to reduce this information asymmetry, or, worse,
convey false information, then they would cease to be of

WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France205(a) Orbitz

(b) Priceline

(c) Expedia

(d) Hotels.com

(e) Yelp

(f) TripAdvisor

Figure 2: Graph of Na¨ıve estimates of deception prevalence versus time, for six online review communities.
Blue (a–d) and red (e–f ) graphs correspond to high and low posting cost communities, respectively.

value to the user. Given that review communities are, in
fact, valued by users [3], it seems unlikely that the preva-
lence of deception among them is large.

Nonetheless, there is widespread concern about the preva-
lence of deception in online reviews, rightly or wrongly, and
further, deceptive reviews can be cause for concern even in
small quantities, e.g., if they are concentrated in a single
review community. We propose that by framing reviews as
signals—voluntary communications that serve to convey in-
formation about the signaler [18], we can reason about the
factors underlying deception by manipulating the distinct
signal costs associated with truthful vs. deceptive reviews.
Speciﬁcally, we claim that for a positive review to be
posted in a given review community, there must be an in-
curred signal cost, that is increased by:

1. The posting cost for posting the review in a given
review community, i.e., whether users are required to
purchase a product prior to reviewing it (high cost) or
not (low cost). Some sites, for example, allow anyone
to post reviews about any hotel, making the review
cost eﬀectively zero. Other sites, however, require the
purchase of the hotel room before a review can be writ-
ten, raising the cost from zero to the price of the room.

and decreased by:

2. The exposure beneﬁt of posting the review in that
review community, i.e., the beneﬁt derived from other
users reading the review, which is proportional to the
size of the review community’s audience. Review sites
with more traﬃc have greater exposure beneﬁt.

Observe that both the posting cost and the exposure beneﬁt
depend entirely on the review community. An overview of

these factors for each of the six review communities is given
in Table 3.

Based on the signal cost function just deﬁned, we propose

two hypotheses:

• Hypothesis 1 : Review communities that have low sig-
nal costs (low posting requirements, high exposure),
e.g., TripAdvisor and Yelp, will have more deception
than communities with high signal costs, e.g., Orbitz.
• Hypothesis 2 : Increasing the signal cost will decrease

the prevalence of deception.

7. EXPERIMENTAL SETUP

The framework described in Section 2 is instantiated for
the six review communities introduced in Section 5. In par-
ticular, we ﬁrst train our SVM deception classiﬁer following
the procedure outlined in Section 4.1. An important step
when training SVM classiﬁers is setting the cost parameter,
C. We set C using a nested 5-fold cross-validation pro-
cedure, and choose the value that gives the best average
balanced accuracy, deﬁned as 1

2 (sensitivity + speciﬁcity).

We then estimate the classiﬁer’s sensitivity, speciﬁcity,
and hyperparameters, using the procedure outlined in Sec-
tion 4.2 and Appendix B. Based on those estimates, we then
estimate the prevalence of deception among reviews in our
test set using the Na¨ıve and the Bayesian Prevalence Models.
Gibbs sampling for the Bayesian Prevalence Model is per-
formed using Equations 7 and 8 (given in Appendix A) for
70,000 iterations, with a burn-in of 20,000 iterations, and
a sampling lag of 50. We use an uninformative (uniform)
prior for π∗, i.e., α = (cid:104)1, 1(cid:105). Multiple runs are performed to
verify the stability of the results.

-­‐6%	  -­‐4%	  -­‐2%	  0%	  2%	  4%	  6%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  -­‐6%	  -­‐4%	  -­‐2%	  0%	  2%	  4%	  6%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  -­‐6%	  -­‐4%	  -­‐2%	  0%	  2%	  4%	  6%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  -­‐6%	  -­‐4%	  -­‐2%	  0%	  2%	  4%	  6%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  -­‐6%	  -­‐4%	  -­‐2%	  0%	  2%	  4%	  6%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  -­‐6%	  -­‐4%	  -­‐2%	  0%	  2%	  4%	  6%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France206(a) Orbitz

(b) Priceline

(c) Expedia

(d) Hotels.com

(e) Yelp

(f) TripAdvisor

Figure 3: Graph of Bayesian estimates of deception prevalence versus time, for six online review communities.
Blue (a–d) and red (e–f ) graphs correspond to high and low posting cost communities, respectively. Error
bars show Bayesian 95% credible intervals.

8. RESULTS AND DISCUSSION

Estimates of the prevalence of deception for six review
communities over time, given by the Na¨ıve Prevalence Model,
appear in Figure 2. Blue graphs (a–d) correspond to com-
munities with High posting cost (see Table 3), i.e., commu-
nities for which you are required to book a hotel room before
posting a review, while red graphs (e–f) correspond to com-
munities with Low posting cost, i.e., communities that allow
any user to post reviews for any hotel.

In agreement with Hypothesis 1 (given in Section 6), it
is clear from Figure 2 that deceptive opinion spam is de-
creasing or stationary over time for High posting cost re-
view communities (blue graphs, a–d).
In contrast, review
communities that allow any user to post reviews for any ho-
tel, i.e., Low posting cost communities (red graphs, e–f), are
seeing growth in their rate of deceptive opinion spam.

Unfortunately, as discussed in Section 3.1, we observe that
the prevalence estimates produced by the Na¨ıve Prevalence
Model are often negative. This occurs when the rate at
which the classiﬁer makes positive predictions is below the
classiﬁer’s estimated false positive rate, suggesting both that
the estimated false positive rate of the classiﬁer is perhaps
overestimated, and that the classiﬁer’s estimated speciﬁcity
(truthful recall rate, given by θ) is perhaps underestimated.
We address this further in Section 8.1.

The Bayesian Prevalence Model, on the other hand, en-
codes the uncertainty in the estimated values of the classi-
ﬁer’s sensitivity and speciﬁcity through two Beta priors, and
in particular their hyperparameters, β and γ. Estimates of
the prevalence of deception for the six review communities
over time, given by the Bayesian Prevalence Model, appear
in Figure 3. Blue (a–d) and red (e–f) graphs, as before, cor-

respond to communities with High and Low posting costs,
respectively.

In agreement with Hypothesis 1 (Section 6), we again ﬁnd
that Low signal cost communities, e.g., TripAdvisor, seem to
contain larger quantities and accelerated growth of deceptive
opinion spam when compared to High signal cost communi-
ties, e.g., Orbitz. Interestingly, communities with a blend of
signal costs appear to have medium rates of deception that
are neither growing nor declining, e.g., Hotels.com, which
has a rate of deception of ≈ 2%.

To test Hypothesis 2, i.e., that increasing the signal cost
will decrease the prevalence of deception, we need to increase
the signal cost, as we have deﬁned it in Section 6. Thus, it
is necessary to either increase the posting cost, or decrease
the exposure beneﬁt. And while we have no control over a
community’s exposure beneﬁt, we can increase the posting
cost by, for example, hiding all reviews written by users
who have not posted at least two reviews. Essentially, by
requiring users to post more than one review in order for
their review to be displayed, we are increasing the posting
cost and, accordingly, the signal cost as well.

Bayesian Prevalence Model estimates for TripAdvisor for
varying signal costs appear in Figure 4. In particular, we
give the estimated prevalence of deception over time af-
ter removing reviews written by ﬁrst-time review writers,
and after removing reviews written by ﬁrst- or second-time
review writers.
In agreement with Hypothesis 2, we see a
clear reduction in the prevalence of deception over time on
TripAdvisor after removing these reviews, with rates drop-
ping from ≈ 6%, to ≈ 5%, and ﬁnally to ≈ 4%, suggesting
that an increased signal cost may indeed help to reduce the
prevalence of deception in online review communities.

0%	  2%	  4%	  6%	  8%	  10%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France207(a) TripAdvisor. All reviews.

(b) TripAdvisor. First-time reviewers ex-
cluded.

(c) TripAdvisor. First-time and second-
time reviewers excluded.

Figure 4: Graph of Bayesian estimates of deception prevalence versus time, for TripAdvisor, with reviews
written by new users excluded. Excluding reviews written by ﬁrst- or second-time reviewers increases the
signal cost, and decreases the prevalence of deception.

8.1 Assumptions and Limitations

In this work we have made a number of assumptions, a
few of which we will now highlight and discuss.
First, we note that our unlabeled test set, Dtest, overlaps
with our labeled truthful training set, Dtrain. Consequently,
we will underestimate the prevalence of deception, because
the overlapping reviews will be more likely to be classiﬁed
at test time as truthful, having been seen in training as be-
ing truthful. Excluding these overlapping reviews from the
test set results in overestimating the prevalence of decep-
tion, based on the hypothesis that the overlapping reviews,
chosen from the 20 most highly-reviewed Chicago hotels, are
more likely to be truthful to begin with.
Second, we observe that our development set, Ddev, con-
taining labeled truthful reviews, is not gold-standard. Un-
fortunately, while it is necessary to obtain a uniform sample
of reviews in order to fairly estimate the classiﬁer’s truthful
recall rate (speciﬁcity), such review samples are inherently
unlabeled. This can be problematic if the underlying rate of
deception is high among the reviews from which the devel-
opment set is sampled, because the speciﬁcity will then be
underestimated. Indeed, our Na¨ıve Prevalence Model regu-
larly produces negative estimates, suggesting that the esti-
mated classiﬁer speciﬁcity may indeed be underestimated,
possibly due to deceptive reviews in the development set.

Third, our proposal for increasing the signal cost, by hid-
ing reviews written by ﬁrst- or second-time reviewers, is not
ideal. While our results conﬁrm that hiding these reviews
will cause an immediate reduction in deception prevalence,
the increase in signal cost might be insuﬃcient to discourage
new deception, once deceivers become aware of the increased
posting requirements.

Fourth, in this work we have only considered a limited
version of the deception prevalence problem. In particular,
we have only considered positive Chicago hotel reviews, and
our classiﬁer is trained on deceptive reviews coming only
from Amazon Mechanical Turk. Both negative reviews as
well as deceptive reviews obtained by other means are likely
to be diﬀerent in character than the data used in this study.
8.2 Implications for Psychological Research

The current research also represents a novel approach to a
long-standing and ongoing debate around deception preva-
lence in the psychological literature. In one of the ﬁrst large-

scale studies looking at how often people lie in everyday
communication, DePaulo et al. [4] used a diary method to
calculate the average number of lies told per day. At the
end of seven days participants told approximately one to
two lies per day, with more recent studies replicating this
general ﬁnding [6], suggesting that deception is frequent in
human communication. More recently, Serota et al. [17]
conducted a large scale representative survey of Americans
asking participants how often they lied in the last 24 hours.
While they found the same average deception rate as pre-
vious research (approximately 1.65 lies per day), they dis-
covered that the data was heavily skewed, with 60 percent
of the participants reporting no lies at all. They concluded
that rather than deception prevalence being spread evenly
across the population, there are instead a few proliﬁc liars.
Unfortunately, both sides of this debate have relied solely
on self-report data.

The current approach oﬀers a novel method for assessing
deception prevalence that does not require self-report, but
can provide insight into the prevalence of deception in hu-
man communication more generally. At the same time, the
question raised by the psychological research also mirrors
an important point regarding the prevalence of deception in
online reviews: are a few deceptive reviews posted by many
people, or are there many deceptive reviews told by only a
few? That is, do some hotels have many fake reviews while
others are primarily honest? Or, is there a little bit of cheat-
ing by most hotels? This kind of individualized modeling
represents an important next step in this line of research.

9. CONCLUSION

In this work, we have presented a general framework for
estimating the prevalence of deception in online review com-
munities, based on the output of a noisy deception classiﬁer.
Using this framework, we have explored the prevalence of
deception among positive reviews in six popular online re-
view communities, and provided the ﬁrst empirical study of
the magnitude, and inﬂuencing factors of deceptive opinion
spam.

We have additionally proposed a theoretical model of on-
line reviews as a signal to a product’s true (unknown) qual-
ity, based on economic signaling theory. Speciﬁcally, we have
deﬁned the signal cost of positive online reviews as a func-

0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  0%	  2%	  4%	  6%	  8%	  10%	  12%	  Jan-­‐09	  Jul-­‐09	  Jan-­‐10	  Jul-­‐10	  Jan-­‐11	  Jul-­‐11	  WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France208tion of the posting costs and exposure beneﬁts of the review
community in which it is posted. Based on this theory, we
have further suggested two hypotheses, both of which are
supported by our ﬁndings. In particular, we ﬁnd ﬁrst that
review communities with low signal costs (low posting re-
quirements, high exposure) have more deception than com-
munities with comparatively higher signal costs. Second, we
ﬁnd that by increasing the signal cost of a review community,
e.g., by excluding reviews written by ﬁrst- or second-time re-
viewers, we can eﬀectively reduce both the prevalence and
the growth rate of deception in that community.

Future work might explore other methods for manipu-
lating the signal costs associated with posting online re-
views, and the corresponding eﬀects on deception preva-
lence. For example, some sites, such as Angie’s List (http:
//www.angieslist.com/), charge a monthly access fee in or-
der to browse or post reviews, and future work might study
the eﬀectiveness of such techniques at deterring deception.

10. ACKNOWLEDGMENTS

This work was supported in part by National Science
Foundation Grant NSCC-0904913, and the Jack Kent Cooke
Foundation. We also thank, alphabetically, Cristian Danescu-
Niculescu-Mizil, Lillian Lee, Bin Lu, Karthik Raman, Lu
Wang, and Ainur Yessenalina, as well as members of the
Cornell NLP seminar group and the WWW reviewers for
their insightful comments, suggestions and advice on vari-
ous aspects of this work.

11. REFERENCES
[1] C. Bond and B. DePaulo. Accuracy of deception

judgments. Personality and Social Psychology Review,
10(3):214, 2006.

[2] C.-C. Chang and C.-J. Lin. LIBSVM: A library for

support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1–27:27,
2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.

revisited. American Journal of Epidemiology,
153(9):921, 2001.

[10] L. Joseph, T. Gyorkos, and L. Coupal. Bayesian

estimation of disease prevalence and the parameters of
diagnostic tests in the absence of a gold standard.
American Journal of Epidemiology, 141(3):263, 1995.
[11] E. Lim, V. Nguyen, N. Jindal, B. Liu, and H. Lauw.

Detecting product review spammers using rating
behaviors. In Proceedings of the 19th ACM
international conference on Information and
knowledge management, pages 939–948. ACM, 2010.

[12] D. Meyer. Fake reviews prompt belkin apology. http:

//news.cnet.com/8301-1001_3-10145399-92.html,
Jan. 2009.

[13] R. Mihalcea and C. Strapparava. The lie detector:

Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309–312. Association
for Computational Linguistics, 2009.

[14] C. Miller. Company settles case of reviews it faked.

http://www.nytimes.com/2009/07/15/technology/
internet/15lift.html, July 2009.

[15] M. Ott, Y. Choi, C. Cardie, and J. Hancock. Finding

deceptive opinion spam by any stretch of the
imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume 1,
pages 309–319. Association for Computational
Linguistics, 2011.

[16] B. Page. Amazon withdraws ebook explaining how to

manipulate its sales rankings.
http://www.guardian.co.uk/books/2011/jan/05/
amazon-ebook-manipulate-kindle-rankings, Jan.
2011.

[17] K. Serota, T. Levine, and F. Boster. The prevalence of

lying in America: Three studies of self-reported lies.
Human Communication Research, 36(1):2–25, 2010.

[18] M. Spence. Job market signaling. The quarterly

[3] Cone. 2011 Online Inﬂuence Trend Tracker. Online:

journal of Economics, 87(3):355, 1973.

http://www.coneinc.com/negative-reviews-
online-reverse-purchase-decisions, August 2011.

[4] B. DePaulo, D. Kashy, S. Kirkendol, M. Wyer, and

J. Epstein. Lying in everyday life. Journal of
personality and social psychology, 70(5):979, 1996.

[5] J. Hancock. Digital Deception: The Practice of Lying
in the Digital Age. Deception: Methods, Contexts and
Consequences, pages 109–120, 2009.

[6] J. Hancock, J. Thom-Santelli, and T. Ritchie.

Deception and design: The impact of communication
technology on lying behavior. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 129–134. ACM, 2004.

[19] D. Streitfeld. In a race to out-rave, 5-star web reviews

go for $5. http://www.nytimes.com/2011/08/20/
technology/finding-fake-reviews-online.html,
Aug. 2011.

[20] D. Streitfeld. For $2 a star, an online retailer gets

5-star product reviews. http:
//www.nytimes.com/2012/01/27/technology/for-2-
a-star-a-retailer-gets-5-star-reviews.html,
Jan. 2012.

[21] A. Topping. Historian Orlando Figes agrees to pay

damages for fake reviews.
http://www.guardian.co.uk/books/2010/jul/16/
orlando-figes-fake-amazon-reviews, July 2010.

[7] N. Hu, L. Liu, and J. Zhang. Do online reviews aﬀect

[22] A. Vrij. Detecting lies and deceit: Pitfalls and

product sales? The role of reviewer characteristics and
temporal eﬀects. Information Technology and
Management, 9(3):201–214, 2008.

[8] N. Jindal and B. Liu. Opinion spam and analysis. In

Proceedings of the international conference on Web
search and web data mining, pages 219–230. ACM,
2008.

[9] W. Johnson, J. Gastwirth, and L. Pearson. Screening
without a “gold standard”: The Hui-Walter paradigm

opportunities. Wiley-Interscience, 2008.

[23] K. Yoo and U. Gretzel. Comparison of Deceptive and

Truthful Travel Reviews. Information and
Communication Technologies in Tourism 2009, pages
37–47, 2009.

WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France209APPENDIX
A. GIBBS SAMPLER FOR BAYESIAN PREVA-

LENCE MODEL

Gibbs sampling of the Bayesian Prevalence Model, intro-
duced in Section 3.2, is performed according to the following
conditional distributions:

,

(7)

,

(8)

Pr(yi = 1| f (x), y(−i); α, β, γ)

∝ (α1 + N (−i)

1

(cid:80) β + N (−i)
) · βf (xi) + X (−i)

f (xi)

1

and,

Pr(yi = 0| f (x), y(−i); α, β, γ)

∝ (α0 + N (−i)

0

(cid:80) γ + N (−i)
) · γ1−f (xi) + Y (−i)

f (xi)

0

where,

X (−i)
k =

Y (−i)

k

=

σ[yj = 1] · σ[f (xj) = k],

σ[yj = 0] · σ[f (xj) = k],

j(cid:54)=i

(cid:88)
(cid:88)
j(cid:54)=i
0 + X (−i)
+ Y (−i)
.

1

= X (−i)
= Y (−i)

1

0

,

1

N (−i)
N (−i)

0

After sampling, we reconstruct the collapsed variables to
yield the Bayesian Prevalence Model estimate of the preva-
lence of deception:

Estimates of the classiﬁer’s sensitivity and speciﬁcity are
similarly given by:

α1 + N1

(cid:80) α + N test .
(cid:80) β + N1
(cid:80) γ + N0

β1 + X1

γ1 + Y0

,

.

πbayes =

ηbayes =

θbayes =

(9)

(10)

(11)

B. ESTIMATING CLASSIFIER SENSITIV-

ITY AND SPECIFICITY

We estimate the sensitivity and speciﬁcity of our deception

classiﬁer via the following procedure:

1. Assume given a labeled training set, Dtrain, containing
N train reviews of n hotels. Also assume given a devel-
opment set, Ddev, containing labeled truthful reviews.
, of sizes given
con-
j
(−j) con-

, respectively, such that Dtrain
by, N train
tains all (and only) reviews of hotel j. Let Dtrain
tain all reviews except those of hotel j.

2. Split Dtrain into n folds, Dtrain

, . . . ,Dtrain

, . . . , N train

n

n

1

1

3. Then, for each hotel j:

(a) Train a classiﬁer, fj, from reviews in Dtrain

use it to classify reviews in Dtrain

(−j) , and
(b) Let |T P|j correspond to the observed number of

.

j

true positives, i.e.:

(cid:88)

|T P|j =

σ[y = 1] · σ[fj(x) = 1].

(12)

(c) Similarly,

j

(x,y)∈Dtrain
let |F N|j correspond to the observed

number of false negatives.

4. Calculate the aggregate number of true positives (|T P|)
and false negatives (|F N|), and compute the sensitivity
(deceptive recall) as:

|T P|

|T P| + |F N| .

η =

(13)

5. Train a classiﬁer using all reviews in Dtrain, and use it

to classify reviews in Ddev.

6. Let the resulting number of true negative and false
positive predictions in Ddev be given by |T N|dev and
|F P|dev, respectively, and compute the speciﬁcity (truth-
ful recall) as:

θ =

|T N|dev

|T N|dev + |F P|dev

.

(14)

For the Bayesian Prevalence Model, we observe that the
posterior distribution of a variable with an uninformative
(uniform) Beta prior, after observing a successes and b fail-
ures, is just Beta(a+1, b+1), i.e., a and b are pseudo counts.
Based on this observation, we set the hyperparameters β and
γ, corresponding to the classiﬁer’s sensitivity (deceptive re-
call) and speciﬁcity (truthful recall), respectively, to:

β = (cid:104)|F N| + 1,|T P| + 1(cid:105) ,
γ = (cid:104)|F P|dev + 1,|T N|dev + 1(cid:105) .

WWW 2012 – Session: Fraud and Bias in User RatingsApril 16–20, 2012, Lyon, France210