Understanding Web Images by Object Relation Network

Na Chen

Dept. of Computer Science

University of Southern

California

Los Angeles, CA, USA
nchen@usc.edu

Qian-Yi Zhou

Dept. of Computer Science

University of Southern

California

Los Angeles, CA, USA
qianyizh@usc.edu

Viktor K. Prasanna

Ming Hsieh Dept. of Electrical

Engineering

University of Southern

California

Los Angeles, CA, USA
prasanna@usc.edu

BasketballPlayer1

Person 1

throw

Basketball1

hold

SoccerBall1

A Collection of 
SoccerPlayers

SoccerPlayer1

SoccerPlayer2

SoccerPlayer3

kick

kick

SoccerBall1

Figure 1: Images and their Object Relation Networks automatically generated by our system

ABSTRACT
This paper presents an automatic method for understand-
ing and interpreting the semantics of unannotated web im-
ages. We observe that the relations between objects in an
image carry important semantics about the image. To cap-
ture and describe such semantics, we propose Object Rela-
tion Network (ORN), a graph model representing the most
probable meaning of the objects and their relations in an im-
age. Guided and constrained by an ontology, ORN transfers
the rich semantics in the ontology to image objects and the
relations between them, while maintaining semantic consis-
tency (e.g., a soccer player can kick a soccer ball, but cannot
ride it). We present an automatic system which takes a raw
image as input and creates an ORN based on image visual
appearance and the guide ontology. We demonstrate various
useful web applications enabled by ORNs, such as automatic
image tagging, automatic image description generation, and
image search by image.

Categories and Subject Descriptors
I.4 [Image Processing and Computer Vision]: Image
Representation, Scene Analysis, Applications

Keywords
Image understanding, image semantics, ontology, detection

1.

INTRODUCTION

Understanding the semantics of web images has been a
critical component in many web applications, such as auto-
matic web image interpretation and web image search. Man-

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

ual annotation, particularly tagging, has been considered as
a reliable source of image semantics due to its human origins.
Manual annotation can yet be very time-consuming and ex-
pensive when dealing with web-scale image data. Advances
in Semantic Web have made ontology another useful source
for describing image semantics (e.g., [23]). Ontology builds
a formal and explicit representation of semantic hierarchies
for the concepts and their relationships in images, and al-
lows reasoning to derive implicit knowledge. However, the
gap between ontological semantics and image visual appear-
ance is still a hinderance towards automated ontology-driven
image annotation. With the rapid growth of image resources
on the world-wide-web, vast amount of images with no meta-
data have emerged. Thus automatically understanding raw
images solely based on their visual appearance becomes an
important yet challenging problem.

Advances in computer vision have oﬀered computers an
eye to see the objects in images. In particular, object detec-
tion [8] can automatically detect what is in the image and
where it is. For example, given Fig. 2(a) as the input im-
age to object detectors, Fig. 2(b) shows the detected objects
and their bounding boxes. However, current detection tech-
niques have two main limitations. First, detection is lim-
ited to isolated objects and cannot see through the relations
between them. Second, only generic objects are detected;
detection quality can drop signiﬁcantly when detectors at-
tempt to assign more speciﬁc meaning to these objects. For
instance, detectors successfully detected one person and one
ball in Fig. 2(b), but cannot further tell whether the person
is throwing or holding the ball, or whether the person is a
basketball player or a soccer player.

This paper presents an automatic system for understand-
ing web images with no metadata, by taking advantages
of both ontology and object detection. Given a raw image
as input, our system adopts object detection as an eye in
pre-processing to ﬁnd generic objects in the image; and em-

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France291ploys a guide ontology as a semantic source of background
knowledge. We propose Object Relation Network (ORN) to
transfer rich semantics in the guide ontology to the detected
objects and their relations. In particular, ORN is deﬁned as
a graph model representing the most probable ontological
class assignments for the objects and their relations. Our
method automatically generates ORN for an image by solv-
ing an energy optimization problem over a directed graphical
model. The output ORN can be regarded as an instantiation
of the guide ontology with respect to the input image. Fig. 1
illustrates three web images and their ORNs automatically
generated by our system.

Object Relation Networks can be applied to many web
applications that need automatic image understanding. In
particular, we demonstrate three typical applications:

Automatic image tagging: With a few simple inference
rules, ORNs can automatically produce informative tags de-
scribing entities, actions, and even scenes in images.

Automatic image description generation: Natural
language description of an image can be automatically gen-
erated based on its ORN, using a simple template based
approach.

Image search by image: Given a query image, the ob-
jective is to ﬁnd images semantically-similar to the query
image from a image library. We show that the distance
between ORN graphs is an eﬀective measurement of image
semantic similarity. Search results consist of images with
ORNs that are close to the query image’s ORN, ranked by
ORN distances.

The main contributions of this paper include:

1. We propose and exploit Object Relation Network to-
wards automatic web image understanding. ORN is
an intuitive and expressive graph model in represent-
ing the semantics of web images. It presents the most
probable meaning of image objects and their relations,
while maintaining the semantic consistency through
the network.

2. We combine ontological knowledge and image visual
features into a probabilistic graphical model. By solv-
ing an optimization problem on this graphical model,
our method automatically transfers rich semantics in
the ontology to a raw image, generating an ORN in
which isolated objects detected from the image are
connected through meaningful relations.

3. We propose and demonstrate three application scenar-
ios that can beneﬁt from ORNs: automatic image tag-
ging, automatic image description generation, and im-
age search by image.

2. RELATED WORK

Our work is related to the following research areas.
Image understanding with keywords and text: Some
research achievements have been made in the web commu-
nity towards understanding web image semantics with key-
words and text, such as tag recommendation, tag ranking
and transfer learning from text to images. Tag recommenda-
tion [24, 29] enriches the semantics carried by existing tags
by suggesting similar tags. Tag ranking [15, 28] identiﬁes
the most relevant semantics among existing tags. Transfer
learning from text to images [19] builds a semantic linkage
between text and image based on their co-occurrence. These

methods all require images to have meaningful initial tags
or relevant surrounding text, and thus do not work for un-
tagged images or images with irrelevant text surrounded.

Image understanding using visual appearance: Com-

puter vision community has made great progress in automat-
ically identifying static objects in images, also known as ob-
ject detection. The PASCAL visual object classes challenge
(VOC) is an annual competition to evaluate the performance
of detection approaches. For instance,
in the VOC2011
challenge [5], detectors are required to detect twenty ob-
ject classes from over 10,000 ﬂickr images. These eﬀorts
have made object detectors a robust and practical tool for
extracting generic objects from images (e.g., [8]). On the
other hand, detection and segmentation are usually local-
ized operations and thus lose information about the global
structure of an image. Therefore, contextual information is
introduced to connect localized operations and the global
structure. In particular, researchers implicitly or explicitly
introduce a probabilistic graphical model to organize pix-
els, regions, or detected objects. The probabilistic graphical
model can be a hierarchical structure [27, 14], a directed
graphical model [26], or a conditional random ﬁeld [10, 20,
11, 12]. These methods are similar in spirit to our method,
however, there are two key diﬀerences: (1) we introduce on-
tology to provide semantics for both relations and objects,
while previous research (even with an ontology [18]) focuses
on spatial relationships, such as on-top-of, beside, which are
insuﬃcient to satisfy the semantic demand of web applica-
tions; (2) previous research usually focuses on improving
local operations or providing a general description for the
entire scene, they do not explicitly reveal the semantic re-
lations between objects thus are less informative than our
Object Relation Network model.

Ontology-aided image annotation: A number of an-
notation ontologies (e.g., [23, 21]) have been proposed to
provide description templates for image and video annota-
tion. Concept ontology [6] characterizes the inter-concept
correlations to help image classiﬁcation. Lexical ontologies,
particularly the WordNet [7] ontology, describe the seman-
tic hierarchies of words. WordNet groups words into sets of
synonyms and records diﬀerent semantic relations between
them, such as antonymy, hypernymy and meronymy. The
WordNet ontology has been used to: (1) improve or ex-
tend existing annotations of an image [25, 3], (2) provide
knowledge about the relationships between object classes for
object category recognition [17], and (3) organize the struc-
ture of image database [4]. Diﬀerent from the prior work,
we exploit ontologies to provide background knowledge for
automatic image understanding. In particular, the key dif-
ference between our guide ontology and the ontology in [17]
is that our guide ontology contains semantic hierarchies of
both object classes and relation classes, and supports various
semantic constraints.

3. SYSTEM OVERVIEW

An overview of our system is illustrated in Fig. 2. Taking
an unannotated image (Fig. 2(a)) as input, our system ﬁrst
employs a number of object detectors to detect generic ob-
jects from the input image (Fig. 2(b)). The guide ontology
(Fig. 2(c), detailed in Section 4) contains useful background
knowledge such as semantic hierarchies and constraints re-
lated to the detected objects and their relations. Our sys-
tem then constructs a directed graphical model as a primi-

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France292(a) Input

(b) Detection

(d) Directed graphical model

P(o1 ⇝ BasketballPlayer) = 0.44
P(o1 ⇝ Person) = 0.37
P(o1 ⇝ SoccerPlayer) = 0.19

o1 (Person1)

object node
relation node

(e) Energy 
optimization 

towards the best 

labeling

o2 (Ball1)

P(o2 ⇝ Ball) = 0.52
P(o2 ⇝ Basketball) = 0.30
P(o2 ⇝ SoccerBall) = 0.18

r1,2
P(r1,2 ⇝ Throw) = 0.47
P(r1,2 ⇝ Head) = 0.21
P(r1,2 ⇝ Non-Interact) = 0.14
P(r1,2 ⇝ Hold) = 0.10
P(r1,2 ⇝ Kick) = 0.08
P(r1,2 ⇝ Interact) = 0.00

E(L) = Ec (L;Ont)
+ Ei (L;Ont)
+ Ev (L;Img)

(f) Output: Object 
Relation Network

BasketballPlayer1

throw

Basketball1

(c) Guide ontology

(g) Applications

Automatic tagging

Automatic description generation

basketball

basketball player 

ball throwing

A basketball player is 
throwing a basketball.

Image search by image

query image

search results (ranked by relevance)

Figure 2: System pipeline for an example image: (a) the input to our system is an image with no metadata;
(b) object detectors ﬁnd generic objects; (c) the guide ontology contains background knowledge related to
the detected objects and their relations; (d) a directed graphical model is constructed; (e) the best labeling of
the graph model is predicted using energy minimization; (f ) the output Object Relation Network represents
the most probable yet ontologically-consistent class assignments for the directed graph model; (g) typical
applications of ORNs.

tive relation network among the detected objects (Fig. 2(d)).
We deﬁne a set of energy functions to transfer two kinds of
knowledge to the graphical model: (1) background knowl-
edge from the guide ontology, and (2) probabilities of po-
tential ontological class assignments to each node, estimated
from visual appearance of the node. Deﬁnitions of these en-
ergy functions are detailed in Section 5. By solving an en-
ergy minimization problem (Fig. 2(e)), we achieve the best
labeling over the graphical model, i.e., the most probable
yet ontologically-consistent class assignments over the en-
tire node set of the graphical model. The Object Relation
Network (ORN) is generated by applying the best labeling
on the graphical model (Fig. 2(f)), as the output of our sys-
tem. The ORN can also be regarded as an instantiation of
the guide ontology. Finally, we propose and demonstrate
three application scenarios of ORNs, including automatic
image tagging, automatic image description generation, and
image search by image (Fig. 2(g)).

4. GUIDE ONTOLOGY

The source of semantics in our system is a guide ontol-
ogy. It provides useful background knowledge about image
objects and their relations. An example guide ontology is
shown in Fig. 2(c).

In general, guide ontologies should have three layers. The
root layer contains three general classes, Object, OO-Relation,
and Object Collection, denoting the class of image objects,
the class of binary relations between image objects, and the
class of image object collections, respectively. The detection
layer contains classes of the generic objects that can be de-

tected by the object detectors in our system. Each of these
class is an immediate subclass of Object, and corresponds
to a generic object (e.g., person and ball ). The semantic
knowledge layer contains background knowledge about the
semantic hierarchies of object classes and relation classes,
and the constraints on relation classes. Each object class
at this layer must have a superclass in the detection layer,
while each relation class must be a subclass of OO-Relation.
Conceptually, any ontology regarding the detectable ob-
jects and their relations can be adapted into our system as
part of the semantic knowledge layer, ranging from a tiny
ontology which contains only one relation class with a do-
main class and a range class, to large ontologies such as
WordNet [7]. However, ontologies with more hierarchical
information and more restrictions are always preferred since
they carry more semantic information. Our system supports
four typical types of background knowledge in the guide on-
tology:

• Subsumption is a relationship where one class is a
subclass of another, denoted as A (cid:2) B. E.g., Basket-
ball Player is a subclass of Athlete.

• Domain/range constraints assert the domain or

range object class of a relation class, denoted as
domain(C) and range(C). E.g., in Fig. 2(c), the do-
main and the range of Kick must be Soccer Player and
Soccer Ball respectively.

• Cardinality constraints limit the maximum number
of relations of a certain relation class that an object
can have, where the object’s class is a domain/range of

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France293the relation class. E.g., in Fig. 2(c), Basketball Player
≤1−−−−−→ Throw means that a Basketball Player can
have at most one Throw relation.

• Collection refers to a set of image objects belonging

to the same object class, denote as collection(C).

5. DIRECTED GRAPHICAL MODEL

The core of our system is a directed graphical model G =
(V, E).
It is a primitive relation network connecting the
detected objects through relations. In particular, given a set
of detected objects {Oi} in the input image, we create one
object node oi for each object Oi, and one relation node ri,j
for each object pair < Oi, Oj > that has a corresponding
relation class in the detection layer of the guide ontology
(e.g., object pair <person1,ball1 > corresponding to class P-
B Relation), indicating that the two objects have potential
relations. For each relation node ri,j, we create two directed
edges (oi, ri,j) and (ri,j , oj). These nodes and edges form
the basic structure of G.
We now consider a labeling problem over the node set V
of graph G: for each node v ∈ V , we label it with a class
assignment from the subclass tree rooted at the generic class
corresponding to v. In particular, we denote the potential
class assignments for object node oi as C(oi) = {Co|Co (cid:2)
Cg(Oi)}, where Cg(Oi) is the generic class of object Oi
(e.g., Person for object person1 ). Similarly, the set of po-
tential class assignments for relation node ri,j is deﬁned as
C(ri,j) = {Cr|Cr (cid:2) Cg(Oi, Oj )}, where Cg(Oi, Oj ) is the
corresponding relation class in the detection layer (e.g., P-B
Relation). A labeling L : {v (cid:2) C(v)} is feasible when we
have C(v) ∈ C(v) for each node v ∈ V .

The best feasible labeling Loptimal is required to (1) satisfy
the ontology constraints, (2) be as informative as possible,
and (3) maximize the probability of the class assignment on
each node regarding visual appearance. We predict Loptimal
by minimizing an energy function E over labeling L with
respect to an image Img and a guide ontology Ont:

E(L) = Ec(L; Ont) + Ei(L; Ont) + Ev(L; Img)

(1)

representing the sum of the constraint energy, the informa-
tive energy, and the visual energy; which are detailed in the
following subsections respectively.
5.1 Ontology based energy

We deﬁne energy functions based on background knowl-

edge in the guide ontology Ont.

Domain/range constraints restrict the potential class
assignments of a relation’s domain or range. Thus, we deﬁne
a domain-constraint energy for each edge e = (oi, ri,j ) and
a range-constraint energy for each edge e = (ri,j, oj ):
c (oi (cid:2) Co, ri,j (cid:2) Cr) =
ED

(cid:2)

if Co (cid:2) domain(Cr)
if Co (cid:2) range(Cr)

0
∞ otherwise
(cid:2)
0
∞ otherwise

c (ri,j (cid:2) Cr, oj (cid:2) Co) =
ER

Intuitively, they add strong penalty to the energy function
when any of the domain/range constraints is violated.

Cardinality constraints restrict the number of instances
of a certain relation class that an object can take. We are
particularly interested in cardinality constraint of 1 since it
is the most common case in practice. In order to handle this

o2
(Person2)

r2,3

o1
(Person1)

o2
(Person2)

r1,3

r2,3

o1
(Person1)

r1,3

o3
(Ball1)

o3
(Ball1)

Figure 3: Given an image with detected objects
shown in left, and the cardinality constraint Throw
≤1←−−−−−− Basketball, we add an edge between node
pair (r1,3, r2,3) and penalize the energy function if
both nodes are assigned as Throw.

type of constraints, we add additional edges as shown in
Fig. 3. In particular, if a relation class Cr has a cardinality
constraint being 1 with its domain (or range), we create an
edge between any relation node pair (ri,j, ri,k) (or (ri,j , rk,j)
when dealing with range) in which both nodes have a same
domain node oi (or range node oj) and both nodes have po-
tential of being labeled with relation class Cr. A cardinality
constraint energy is deﬁned on these additional edges:

ED Card

c

(ri,j (cid:2) C1, ri,k (cid:2) C2) =

ER Card

c

(ri,j (cid:2) C1, rk,j (cid:2) C2) =

(cid:2) ∞ if C1 = C2 = Cr
(cid:2) ∞ if C1 = C2 = Cr

otherwise

0

0

otherwise

Intuitively, they penalize the energy function when two re-
lations are assigned as Cr and have the same domain object
(or range object).

Depth information, deﬁned as the depth of a class as-
signment in the subclass tree rooted at its generic class. In-
tuitively, we prefer deep class assignments which are more
speciﬁc and more informative. In contrast, general class as-
signments with small depth should be penalized since they
are less informative and thus may be of less interest to the
user.
In the extremely general case where generic object
classes are assigned to the object nodes and OO-Relation
is assigned to all the relation nodes, the labeling is feasible
but should be avoided since little information is revealed.
Therefore, we add an energy function for each node oi or
ri,j concerning depth information:

i (oi (cid:2) Co) = −ωdep · depth(Co)
EO
i (ri,j (cid:2) Cr) = −ωdep · depth(Cr)
ER

Collection refers to a set of object nodes with the same
class assignment. Intuitively, we prefer collections with larger
size as they tend to group more objects in the same image to
the same class. For example, in Fig. 4, when the two persons
in the front are labeled with Soccer Player due to the strong
observation that they may Kick a Soccer Ball ( how to make
observations from visual features is detailed in Sec 5.2), it is
quite natural to label the third person with SoccerP layer
as well since the three of them will form a relatively larger
Soccer Players Collection. In addition, we bonus collections
that are deeper in the ontology, e.g., we prefer Soccer Play-
ers Collection to Person Collection. To integrate collection
information into our energy minimization framework is a bit
complicated since we do not explicitly have graph nodes for

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France294o1
(Person1)

r1,4

o2
(Person2)

o1
(Person1)

r2,4

r1,4

o3
(Person3)

r3,4

o2
(Person2)

r2,4

o3
(Person3)

r3,4

o4
(Ball1)

o4
(Ball1)

Figure 4: Edges are added between object nodes
that have the potential to form a collection. Energy
bonus is given when the labeling results in a large
and informative collection.

collections. Therefore, we add edges between object nodes
(oi, oj ) when they belong to the same generic object class
that has the potential to form a collection (e.g., Fig. 4 right),
and deﬁne an energy function for each such edge:

(oi (cid:2) C1, oj (cid:2) C2) =

ECol

(cid:2) −ωcol

i

0

2

N−1 depth(collection(Co))

if C1 = C2 = Co
otherwise

N−1 is a normalization factor
where ωcol is a weight, and
with N representing the number of object nodes that can be
potentially labeled with Co.

2

Finally, the ontology based constraint energy Ec(L; Ont)
and informative energy Ei(L; Ont) are the sum of these en-
ergy functions:

(cid:3)
(cid:3)
(cid:3)

(oi,ri,j )

(ri,j ,ri,k )

(cid:3)

ED

c +

(ri,j ,oj )

+

c

ED Card
(cid:3)

ER
c
(cid:3)
(cid:3)

(ri,j ,rk,j )

ER Card

c

(2)

Ec(L; Ont) =

+

Ei(L; Ont) =

EO

i +

ER

i +

ECol

i

(3)

oi

ri,j

(oi,oj )

5.2 Visual feature based energy

Besides background knowledge from the ontology, we be-
lieve that visual appearance of objects can give us additional
information in determining class assignments. E.g., aBall
with white color is more likely to be a Soccer Ball, while the
relation between two spatially close objects are more proba-
ble to be Interact than Non-interact. Thus, we deﬁne visual
feature based energy functions for object nodes and relation
nodes respectively.
Visual feature based energy on object nodes: for
each object node oi, we collect a set of visual features Fo(Oi)
of the detected object Oi in the input image, and calculate a
probability distribution over potential assignment set C(oi)
based on Fo(Oi).
Intuitively, the conditional probability
function P (oi (cid:2) Co|Fo(Oi)) denotes the probability of oi
assigned as class Co when Fo(Oi) is observed from the image.
Thus, we deﬁne the visual feature based energy on an object
node as:

v (oi (cid:2) Co) = −ωobjP (oi (cid:2) Co|Fo(Oi))
EO

We choose eight visual features of Oi to form feature set
Fo(Oi), including: width and height of Oi’s bounding box
(which is part of the output from detectors); the average of
H,S,V values from the HSV color space; and the standard
deviation of H,S,V.

Given these eight feature values on an object node oi,
a probability distribution over the potential assignment set
C(oi) is estimated, which satisﬁes:
(cid:3)
C∈C(oi)
where Cg(Oi) is the generic class of Oi, and oi (cid:2) Cg(Oi) is
the notation for “oi is assigned as a subclass of Cg(Oi)”.

P (oi (cid:2) C|Fo(Oi)) = P (oi (cid:2) Cg(Oi)|Fo(Oi)) = 1

We take advantage of the hierarchical structure of the sub-
class tree rooted at Cg(Oi), and compute the probability dis-
tribution in a top-down manner. Assume P (oi (cid:2) Co|Fo(Oi))
is known for certain object class Co; if Co is a leaf node in
the ontology (i.e., Co has no subclass), we have P (oi (cid:2)
C|Fo(Oi)) = P (oi (cid:2) Co|Fo(Oi)); otherwise, given Co’s im-
mediate subclass set I(Co), we have a propagation equation:

P (oi (cid:2) Co|Fo(Oi)) = P (oi (cid:2) Co|Fo(Oi))
P (oi (cid:2) Ck|Fo(Oi))

+

(cid:3)
Ck∈I(Co)

We can view the right-hand side of this equation from the
perspective of multi-class classiﬁcation: given conditions oi (cid:2)
Co and Fo(Oi), the assignment of oi falls into |I(Co)| + 1
categories: oi (cid:2) Co, or oi (cid:2) Ck where Ck ∈ I(Co), k =
1, . . . ,|I (Co)|. Thus we can train a multi-class classiﬁer
(based on object visual features) to assign a classiﬁcation
score for each category, and apply the calibration method
proposed in [30] to transform these scores into a probabil-
ity distribution over these |I(Co)| + 1 categories. Multiplied
by the prior P (oi (cid:2) Co|Fo(Oi)), this probability distribu-
tion determines the probability functions on the right-hand
side of Eqn.(4). Thus, the probabilities recursively propa-
gate from the root class down to the entire subclass tree, as
demonstrated in Fig. 5.
In order to train a classiﬁer for each non-leaf object class
Co, we collect a set of objects Otrain belonging to class
Co from our training images with ground truth labeling,
and calculate their feature sets. The training samples are
later split into |I(Co)| + 1 categories regarding their label-
ing: assigned as Co, or belonging to one of Co’s immedi-
ate subclasses. We follow the suggestions in [30] to train
one-against-all SVM classiﬁers using a radial basis function
as kernel [22] for each category, apply isotonic regression
(PAV [1]) to calibrate classiﬁer scores, and normalize the
probability estimates to make them sum to 1. This training
process is made for every non-leaf object class once and for
all.

Visual feature based energy on relation nodes can
be handled in a similar manner to that on object nodes.
The only diﬀerence is the feature set Fr(Oi, Oj ). As for
relations, the relative spatial information is most important.
Therefore, Fr(Oi, Oj ) contains eight features of object pair
< Oi, Oj >: the width, height and center (both x and y
coordinates) of Oi’s bounding box; and the width, height
and center of Oj ’s bounding box.

Similarly, training samples are collected and classiﬁers
are trained for each non-leaf relation class Cr in the on-
tology. With these classiﬁers, probabilities propagate from
each generic relation class to its entire subclass tree to form
a distribution over the potential assignment set C(ri,j). The
visual feature based energy on ri,j is deﬁned as:

v (ri,j (cid:2) Cr) = −ωrelP (ri,j (cid:2) Cr|Fr(Oi, Oj ))
ER

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France295P(o1 ⊑ Person) = 1

P(o1 ⊑ Person) = 1

P(o1 ⊑ Person) = 1

Person

Person

Person

P(o1 ⇝ Person) = 0.37

P(o1 ⇝ Person) = 0.37

P(o1 ⊑ Athlete) = 0.63

P(o1 ⊑ Athlete) = 0.63

Athlete

Athlete

P(o1 ⇝ Athlete) = 0.0

Soccer 
Player

Basketball 

Player

P(o1 ⇝ SoccerPlayer) = 0.19 P(o1 ⇝ BasketballPlayer) = 0.44

o1

(Person1)

P(o1 ⇝ BasketballPlayer) = 0.44
P(o1 ⇝ Person) = 0.37
P(o1 ⇝ SoccerPlayer) = 0.19

r1,2

P(r1,2 ⇝ Throw) = 0.47
P(r1,2 ⇝ Head) = 0.21
P(r1,2 ⇝ Non-Interact) = 0.14
………...

o2
(Ball1)

P(o2 ⇝ Ball) = 0.52
P(o2 ⇝ Basketball) = 0.30
P(o2 ⇝ SoccerBall) = 0.18

List of Labeling

(sorted by energy)

o1 ⇝ BasketballPlayer
r1,2 ⇝ Throw
o2 ⇝ Basketball

o1 ⇝ BasketballPlayer
r1,2 ⇝ Non-Interact
o2 ⇝ Ball
o1 ⇝ Person
r1,2 ⇝ Non-Interact
o2 ⇝ Ball
………...

E = - 1.68

E = - 1.24

E = - 1.17

……

Figure 5: The probability distribution over per-
son1 ’s potential class assignments is estimated in
a top-down manner.

Figure 6: The probability distributions are orga-
nized in a network to predict a most probable yet
consistent labeling, which may in return improve the
classiﬁcation result.

In summary, the visual feature based energy is deﬁned as:

(cid:3)

(cid:3)

Ev(L; Img) =

EO

v +

ER
v

(4)

5.3 Energy optimization

oi

ri,j

Finding the best labeling Loptimal to minimize the energy
function E(L) is an intractable problem since the search
space of labeling L is in the order of |C||V |
, where |C| is the
number of possible class assignments for a node and |V | is
the number of nodes in graph G. However, we observe that
this space can be greatly reduced by taking the ontology
constraint energies into account. The brute-force search is
pruned by the following rules:

1. For node v, when labeling v (cid:2) C(v) is to be searched,
we immediately check the constraint energies on edges
touching v, and cut oﬀ this search branch if any of
these energies is inﬁnite.

2. We want to use rule 1 as early as possible. Thus, to
pick the next search node, we always choose the unla-
beled node with the largest number of labeled neigh-
bors.

3. On each node, we sort the potential class assignments
by their visual feature based probabilities in descend-
ing order. Class assignments with large probabilities
are searched ﬁrst, and those with very small proba-
bilities (empirically, < 0.1) are only searched when no
appropriate labeling can be found in previous searches.

In our experiments, the graphical model constructed is
relatively small (usually contains a few object nodes and
no more than 10 relation nodes). The energy optimization
process executes in less than 1 second per image.
5.4 ORN generation

Given the best labeling Loptimal over graph G, an Object
Relation Network (ORN) is generated as the output of our
system in three steps:

1. We apply labeling Loptimal over graph G to produce
object and relation nodes with most probable yet se-
mantically consistent class assignments for ORN.

2. A collection of objects is detected by ﬁnding object
nodes with the same class assignment in Loptimal. A
collection node is created accordingly, which is linked
to its members by adding edges representing the is-
MemberOf relationship.

3. We ﬁnally drop some meaningless relation nodes (in

particular, Non-interact), together with the edges touch-
ing them.

After these steps, an intuitive and expressive ORN is auto-
matically created from our system to interpret the semantics
of the objects and their relations in the input image. Exam-
ples are shown in Fig. 1 and Fig. 9.

6. EXPERIMENTAL RESULTS

6.1 Energy function evaluation

We ﬁrst demonstrate how visual feature based energy func-
tions work together with ontology based energy functions,
using the example in Fig. 6. We observe that the proba-
bility distributions shown in the middle tend to give a good
estimation for each node, i.e., provide a relatively high prob-
ability for the true labeling. But there is no guarantee that
the probability of the true labeling is always the highest
(e.g., Ball1 has higher probability of being assigned as Ball
than Basketball, highlighted in red). By combining the en-
ergy functions together, the ontology constraints provide a
strict frame to restrict the possible labeling over the entire
graph by penalizing inappropriate assignments (e.g., Bas-
ketball Player Throw Ball, given that the range of relation
T hrow is limited to Basketball). Probabilities are organized
into a tightly interrelated network which in return improves
the prediction for each single node (e.g., in the labeling with
minimal energy, Ball1 is correctly assigned as Basketball ).
To quantitatively evaluate the energy functions, we col-
lect 1,200 images from ImageNet [4] from category soccer,
basketball and ball. A person detector [9] and a ball detector
using Hough Circle Transform in OpenCV [2] are applied
on the entire image set to detect persons and balls. The de-
tected objects and the relations between them are manually
labeled with classes from the guide ontology in Fig. 2(c).
We then randomly select 600 images as training data, and
use the rest as test data. Three diﬀerent scenarios are com-
pared: (1) using only visual feature based energy, (2) using
both visual feature based energy and ontology constraints,
and (3) using the complete energy function E(L) in Eqn. 1.
Our system minimizes the energy cost in each of the scenar-
ios, and calculates the error rate by comparing the system
output with ground truth. As Fig. 8 and Table 1 suggest,
ontology-based energy transfers background knowledge from
the guide ontology to the relation network, and thus signif-
icantly improves the quality of class assignments.

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France296Semantic knowledge layer

Basketball 

Player

Athlete

Rider

Soccer 
Player

Cyclist

Horse
Rider

Motorcyclist

Basketball

Detection layer

Horse

Motorbike

Ride

Lead

Person-Horse 

Relation

O-O Relation

Object 
Collection

Person-Ball 
Relation

Person-Motorbike 

Relation

Root layer

Object

Bicycle

Person

Chair

Ball

Soccer 
Ball

Person 
Collection

...

Chair 

Collection

Soccer Player 

Collection

Athelete 
Collection

Person-Bicycle 

Relation

Ride
Basketball Player 

Collection

Kick

Interact

Head

Throw

Hold

Ride

Stand by

Figure 7: The ontology we use for system evaluation. Constraints are only shown in the root layer for clarity.

Error rate of class assignment

 

Using only Ev(L;Img)
Combining Ev(L;Img) and Ec(L;Ont)
Using complete energy function E(L)

ORN score

Detection score

k = 1
3.69
4.31

k = 2
3.38
3.93

k = 3
3.77
3.10

k > 3
3.95
3.38

overall

3.65
3.69

Table 2: Human evaluation of ORN and detection:
possible score ranges from 5(perfect) to 1(failure).
k is the number of detected objects.

the ﬁrst four columns successfully interpret the semantics of
the objects and their relations. We also demonstrate some
“bad” examples in the last column. Note that the “bad”
results are usually caused by detection errors (e.g., the top
image has a false alarm from the person detector while the
rest two images both miss certain objects). Nevertheless,
the “bad” ORNs still interpret reasonable image semantics.
Human evaluation: We perform human judgement on
the entire test data set. Scores on a scale of 5 (perfect) to 1
(failure) are given by human judges to reﬂect the quality of
the ORNs, shown in Table 2. First, we notice that the ORNs
are quite satisfactory as the overall score is 3.65. Second,
ORN scores for images of a single object are relatively high
because the detection is reliable when k = 1. With the
number of objects increasing, the relation network becomes
larger and thus more ontology knowledge is brought into the
optimization process. The quality of ORN keeps improving
despite the quality drop of detection.

7. APPLICATIONS

We present three typical web applications based on the
rich semantic information carried by ORNs, detailed as fol-
lows.
7.1 Automatic image tagging

We develop an automatic image tagging approach by com-
bining ORNs and a set of inference rules. Given a raw im-
age as input, our system automatically generates its ORN
which contains semantic information for the objects, rela-
tions, and collections. Thus, we directly output the ontolog-
ical class assignments in the ORN as tags regarding entities,
actions, and entity groups. In addition, with a few simple
rules, implicit semantics about the global scene can also be
easily inferred from the ORN and translated into tags. Ta-
ble 3 shows some example inference rules. Results from our
method and a reference approach ALIPR [13] are illustrated
in the third row of Fig. 10. Note that even with imperfect
ORNs (the 5th and 6th image), our approach is still capable
of producing relevant tags.

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 

On Person nodes

On Ball nodes

On P−B Relation nodes

Figure 8: Error rate of class assignments under three
diﬀerent scenarios.

Generic

class

Person

Ball

P-B Rel.

Using

Ev

0.5188
0.2907
0.3222

Using
Ev +Ec
0.4644
0.2693
0.2887

Using
E(L)
0.3766
0.2640
0.2259

Gain

0.1423
0.0267
0.0962

Gain
(k>3)
0.1783
0.0571
0.0775

Table 1: Evaluation results of the energy functions.
The ﬁrst columns contains data for Fig. 8. The last
two columns show the gain in accuracy by using the
complete energy function E(L), wherek is the num-
ber of detected objects.

6.2 System evaluation

To further evaluate the robustness and generality of our
system, we adapt a more complicated guide ontology (Fig. 7)
into the system. The detection layer contains 6 generic ob-
ject classes: Person, Horse, Motorbike, Chair, Bicycle, and
Ball, while the semantic layer contains simpliﬁed semantic
hierarchies from the WordNet ontology [7]. Moreover, we
extend our image set with images from VOC2011 [5] con-
taining over 28,000 web images. We randomly choose 2,000
images that have at least one generic object, manually label
ground truth class assignments for objects and relations, and
use them to train the visual feature based classiﬁers and the
weight set (ωdep, ωcol, ωobj, ωrel). We adopt the detectors
in [9, 2] to perform object detection.

Time complexity: The most time-consuming operation
of our system is detection, which usually takes around one
minute per test image. After this pre-processing, our sys-
tem automatically creates an ORN for each image within a
second. All experiments are run on a laptop with Intel i-7
CPU 1.60GHz and 6GB memory.

Qualitative results: Most of our ORNs are quite good.
Example results are shown in Fig. 9. The “Good” ORNs in

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France297A Collection of SoccerPlayers

SoccerPlayer1

SoccerPlayer2

SoccerPlayer3

kick

kick

SoccerBall1

Person1

HorseRider1

Person1

Ball1

Ball3

Ball4

Ball2

Ball1

lead

ride

Horse1

hold

A Collection of Balls

Ball10

Ball9

Ball8

Ball7

Ball5

Ball6

A Collection of 
BasketballPlayers

Basketball
Player1

Basketball
Player2

Basketball
Player3

throw

Basketball1

A Collection 
of Persons

Person1

Person3

Person2

Motorcyclist1

Motorbike1

SoccerPlayer5

ride

SoccerPlayer4

SoccerPlayer1
SoccerPlayer2

SoccerPlayer3

A Collection of SoccerPlayers

A Collection of Horses

Horse1

Horse2

SoccerBall1

Person1

A Collection of Cyclists

Cyclist3
ride

Cyclist4
ride

Cyclist1
ride
Bicycle1 Bicycle2 Bicycle4 Bicycle3

Cyclist2
ride

A Collection of Bicycles

Person1

Person2

Person3

Person4

A 

Collection 
of Persons

A Collection 
of Cyclists

Cyclist2

Cyclist1

ride
Bicycle2

ride

Bicycle1

Bicycle3

A Collection of Bicycles

Figure 9: Object Relation Networks are automatically generated from our system. “Good” results are
illustrated in the ﬁrst four columns while the last column shows some “bad” results.

∃xSoccerP layerCollection(x) ∧ ∃ySoccerBall(y) ∧

∃zSoccerP layer(z) ∧ (kick(z, y) ∨ head(z, y)) ∧ T ag(t) →
∃xCyclistCollection(x) ∧ ∃yCyclist(y) ∧ ∃zBicycle(z) ∧
∃xBasketballP layerCollection((x) ∧ ∃yBasketball(y) ∧
∃zBasketballP layer(z) ∧ (throw(z, y) ∨ hold(z, y)) ∧

ride(y, z) ∧ T ag(t) → t =“bicycle race”

t=“soccer game”

T ag(t) → t=“basketball game”

1

2

3

Table 3: Example rules for inferencing implicit
knowledge from ORNs

7.2 Automatic image description generation

Natural language generation for images is an open re-
search problem. We propose to exploit ORNs to automati-
cally generate natural language descriptions for images. We
extend our automatic tagging approach by employing a sim-
ple template based model (inspired by [11]) to transform
tags into concise natural language sentences. In particular,
the image descriptions begin with a sentence regarding the
global scene, followed by another sentence enumerating the
entities (and entity groups if there is any) in the image. The
last few sentences are derived from relation nodes in the
ORN with domain and range information. Examples are
shown in the last row of Fig. 10.

7.3 Image search by image

The key in image search by image is the similarity mea-
surement between two images. Since ORN is a graph model
that carries informative semantics about an image, the graph
distance between ORNs can serve as an eﬀective measure-
ment of the semantic similarity between images. Given that
ORN is an ontology instantiation, we employ the ontology
distance measurement in [16] to compute ORN distances. In
particular, we ﬁrst pre-compute the ORNs for images in our
image library which contains over 30,000 images. Then for

each query image, we automatically generate its ORN, and
retrieve images with the most similar ORNs from the im-
age library. The result images are sorted by ORN distances.
Fig. 11 illustrates several search results of our approach.
Search results from Google Image Search by Image are also
included for reference.

8. CONCLUSION

We presented Object Relation Network (ORN) to carry
semantic information for web images. By solving an opti-
mization problem, ORN was automatically created from a
graphical model to represent the most probable and informa-
tive ontological class assignments for objects detected from
the image and their relations, while maintaining semantic
consistency. Beneﬁting from the strong semantic expres-
siveness of ORN, we proposed automatic solutions for three
typical yet challenging image understanding problems. Our
experiments showed the eﬀectiveness and robustness of our
system.

9. ACKNOWLEDGMENTS

We acknowledge support from NSF grant CCF-1048311.

10. REFERENCES
[1] M. Ayer, H. Brunk, G. Ewing, W. Reid, and

E. Silverman. An empirical distribution function for
sampling with incomplete information. Annals of
Mathematical Statistics, 1955.

[2] G. Bradski and A. Kaehler. Learning OpenCV:

Computer Vision with the OpenCV Library. O’Reilly,
Cambridge, MA, 2008.

[3] R. Datta, W. Ge, J. Li, and J. Wang. Toward bridging

the annotation-retrieval gap in image search.
Multimedia, IEEE, 2007.

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France298Input images:

Obejct relation networks (ORNs) generated from our system:

A Collection of Cyclists

A Collection of SoccerPlayers

Person 1

Cyclist1

Cyclist2

SoccerPlayer5

hold

ride

ride

SoccerPlayer4

SoccerPlayer1
SoccerPlayer2

SoccerPlayer3

SoccerBall1

Bicycle1

Bicycle2

A Collection of Bicycles

Automatic generated tags based on ORNs:
bicycle race
bicycle riding

ball holding
soccer ball
person 

cyclists
bicycles

kick

SoccerBall1

soccer game
ball kicking

soccer players
soccer ball

A Collection of Motorcyclists

A Collection of Chairs

Motorcyclist1

Motorcyclist2

ride

Chair1

Chair3

Motorbike1

motorbike riding

motorcyclists
motorbike

Chair2

chairs

SoccerBall1

Person1

soccer ball

person

(Reference) Top-10 automatic annotation results from ALIPR [13]:

indoor, man-made, 
flower, plant, grass, 
old, poster, horse, 
house, rural_England

man-made, indoor, 

people, grass, sky, car, 
steam, engine, food, 

royal_guard

grass, people, rural, 
guard, fight, battle, 
flower, landscape, 
plant, man-made

Automatic generated image descriptions based on ORNs:

There are one person 
and one soccer ball. 
The person is holding 

the soccer ball. 

This is a picture of a 
bicycle race. There are 
two cyclists and two 
bicycles. Cyclist 1 is 

riding Bicycle 1. Cyclist 
2 is riding Bicycle 2.  

This is a picture of a soccer 
game. There are five soccer 
players and  one soccer ball. 
Soccer player 3 is kicking the 

soccer ball.

building, man-made, 
sport, historical, car, 

people, sky, plane, race, 

motorcycle

There are two motorcyclists and 
one motorbike. Motorcyclist 1 and 

Motorcyclist 2 are riding 

Motorbike 1.  

building, historical, man-
made, landscape, car, 
beach, people, modern, 

city, work

indoor, man-made, 
decoration, decoy, 
people, drawing, 
thing, sport, cloth, 

art

There are three chairs.  

There are one 
person and one 

soccer ball.  

Figure 10: Tags and natural language descriptions automatically generated from our ORN-based approaches.
Annotation results from the ALIPR system (http://alipr.com/) are also showed for reference.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and

[11] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi,

L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. CVPR, 2009.

A. C. Berg, and T. L. Berg. Baby Talk: Understanding
and Generating Image Descriptions. In CVPR, 2011.

[5] M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual
Object Classes Challenge 2011 (VOC2011) Results.
http://www.pascal-
network.org/challenges/VOC/voc2011/workshop/index.html.

[6] J. Fan, Y. Gao, and H. Luo. Integrating concept
ontology and multitask learning to achieve more
eﬀective classiﬁer training for multilevel image
annotation. Image Processing, IEEE Transactions on,
2008.

[7] C. Fellbaum. WordNet An Electronic Lexical Database.

The MIT Press, Cambridge, MA ; London, 1998.
[8] P. Felzenszwalb, R. Girshick, D. McAllester, and

D. Ramanan. Object detection with discriminatively
trained part based models. IEEE TPAMI, 32(9), 2010.
[9] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.

Discriminatively trained deformable part models,
release 4.
http://www.cs.brown.edu/(cid:2)pﬀ/latent-release4/.

[10] X. He, R. S. Zemel, and M. A. Carreira-Perpinan.

Multiscale conditional random ﬁelds for image
labeling. In CVPR, 2004.

[12] L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and
P. H. Torr. What, where and how many? combining
object detectors and crfs. In ECCV, 2010.

[13] J. Li and J. Z. Wang. Real-time computerized

annotation of pictures. In Proceedings of the 14th
annual ACM international conference on Multimedia,
2006.

[14] L.-J. Li, R. Socher, and L. Fei-Fei. Towards total scene

understanding: Classiﬁcation, annotation and
segmentation in an automatic framework. In CVPR,
2009.

[15] D. Liu, X.-S. Hua, L. Yang, M. Wang, and H.-J.

Zhang. Tag ranking. In WWW, 2009.

[16] A. Maedche and S. Staab. Measuring similarity

between ontologies. In EKAW, 2002.

[17] M. Marszalek and C. Schmid. Semantic hierarchies for

visual object recognition. In CVPR, 2007.

[18] I. Nwogu, V. Govindaraju, and C. Brown. Syntactic

image parsing using ontology and semantic
descriptions. In CVPR, 2010.

[19] G.-J. Qi, C. Aggarwal, and T. Huang. Towards

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France299Query image

ORN

Top-4 search results

Our approach

HorseRider1

ride

Horse1

Google Image Search by Image

Our approach

A Collection of Chairs

Chair1

Chair4

Chair2

Chair3

Google Image Search by Image

Our approach

A Collection of 
SoccerPlayers

Soccer
Player2

kick

Soccer
Player1

Soccer
Player3

kick

SoccerBall1

Google Image Search by Image

Figure 11: Image search results of our approach and Google Image Search by Image (http://images.google.com/)

semantic knowledge propagation from text corpus to
web images. In WWW, 2011.

[20] A. Rabinovich, A. Vedaldi, C. Galleguillos,

E. Wiewiora, and S. Belongie. Objects in context. In
ICCV, 2007.

[21] C. Saathoﬀ and A. Scherp. Unlocking the semantics of

multimedia presentations in the web with the
multimedia metadata ontology. In WWW, 2010.

[22] B. Sch¨olkopf and A. J. Smola. Learning with Kernels:

Support Vector Machines, Regularization,
Optimization, and Beyond. The MIT Press,
Cambridge, MA, 2002.

[23] A. T. G. Schreiber, B. Dubbeldam, J. Wielemaker,
and B. Wielinga. Ontology-based photo annotation.
IEEE Intelligent Systems, 2001.

[24] B. Sigurbj¨ornsson and R. van Zwol. Flickr tag

recommendation based on collective knowledge. In
WWW, 2008.

[25] M. Srikanth, J. Varner, M. Bowden, and D. Moldovan.
Exploiting ontologies for automatic image annotation.
In SIGIR, 2005.

[26] A. Torralba, K. Murphy, and W. T. Freeman. Using

the forest to see the trees: exploiting context for visual
object detection and localization. In Commun. ACM,
2010.

[27] Z. Tu, X. Chen, A. Yuille, and S. Zhu. Image parsing:

Unifying segmentation, detection, and recognition.
IJCV, 2005.

[28] J. Weston, S. Bengio, and N. Usunier. Large scale

image annotation: Learning to rank with joint
word-image embeddings. European Conference on
Machine Learning, 2010.

[29] L. Wu, L. Yang, N. Yu, and X.-S. Hua. Learning to

tag. In WWW, 2009.

[30] B. Zadrozny and C. Elkan. Transforming classiﬁer

scores into accurate multiclass probability estimates.
In ACM SIGKDD, 2002.

WWW 2012 – Session: Creating and Using Links between Data ObjectsApril 16–20, 2012, Lyon, France300