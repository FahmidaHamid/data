Community Detection in Incomplete Information Networks

National University of Defense

University of Illinois at Chicago

National University of Defense

National University of Defense

linwangqun2005@gmail.com

Wangqun Lin

Technology

Changsha, China

Quanyuan Wu

Technology

Changsha, China

quanyuanwu@nudt.edu.cn

Xiangnan Kong
Chicago, Illinois

xkong4@uic.edu

Yan Jia
Technology

Changsha, China

yanjia@nudt.edu.cn

University of Illinois at Chicago

Philip S. Yu
Chicago, Illinois
psyu@uic.edu

Chuan Li

Sichuan University
Chengdu, China

lcharles@scu.edu.cn

ABSTRACT

With the recent advances in information networks, the prob-
lem of community detection has attracted much attention in
the last decade. While network community detection has
been ubiquitous, the task of collecting complete network
data remains challenging in many real-world applications.
Usually the collected network is incomplete with most of
the edges missing. Commonly, in such networks, all nodes
with attributes are available while only the edges within a
few local regions of the network can be observed.
In this
paper, we study the problem of detecting communities in
incomplete information networks with missing edges. We
ﬁrst learn a distance metric to reproduce the link-based dis-
tance between nodes from the observed edges in the local
information regions. We then use the learned distance met-
ric to estimate the distance between any pair of nodes in
the network. A hierarchical clustering approach is proposed
to detect communities within the incomplete information
networks. Empirical studies on real-world information net-
works demonstrate that our proposed method can eﬀectively
detect community structures within incomplete information
networks.

Categories and Subject Descriptors

H.2.8 [Database Management]: Database Application-
Data Mining

General Terms

Algorithms, Experimentation

Keywords

Community detection, incomplete information networks, dis-
tance metric learning

1.

INTRODUCTION

Information networks arise naturally in a wide range of
domains. Examples include biological networks, publica-
tion networks and social networks. In these networks, fea-
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

ture vectors are usually available which are associated with
nodes. Links represent relationships between the nodes.
Identifying communities in information networks is a crucial
step to understand the network structures. The community
is deﬁned as a group of nodes which are densely connected
inside the group, while loosely connected with the nodes
outside the group.

Community detection in network data has been exten-
sively studied in the literature [17, 19, 3, 18, 2, 21, 14]. Con-
ventional approaches focus on detecting communities based
upon linkage information. They assume that the complete
linkage information within the entire network is available.
However, in many real-world networks, such as terrorist-
attack information networks, the complete linkages are very
diﬃcult or even impossible to obtain. Instead, the complete
linkage information is only available within a few small lo-
cal regions. We notice that a similar problem has also been
studied in [13]. However, in this paper, we focus on incom-
plete information networks with local information regions.
For example, in work relation networks, it is usually impos-
sible to obtain the complete linkage information among all
the people. But usually we can aﬀord to obtain the work
relationships within a small number of local regions, such as
groups or organizations. These networks are called incom-
plete information networks in this paper. The local regions
with complete linkage information are called local informa-
tion regions. An incomplete information network with local
information regions is shown in the upper left level of Fig-
ure 1. Some real-world examples for community detection
in incomplete information networks are listed as follows:

• Terrorist-attack network. Let us consider a ter-
rorist attack activity networks within a period in a
certain country. Each node in the network represents
a terrorist activity. Terrorist attacks committed by
the same terrorist organization are linked with each
other. Investigating the community structures within
these networks is a challenging problem, since most of
the connections/links between attacks are not clearly
resolved. Detecting the communities in these incom-
plete information networks is crucial for analyzing the
structures of terrorist-attack activities.

• Food Web The food web of a large ecosystem is usu-
ally a highly complex network. Each node in the net-
work represents a living organism, while the links rep-

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France341Structure view

Node attribute view for n1~ n9

t
u
p
n

I

n1 

n3 

n2 

n5 

n4 

n6 
n9 

n8 

n7 

local information regions

l

 

g
n
i
r
e
t
s
u
c
d
e
s
a
b
_
e
t
u
b
i
r
t
t

A

d
o
h
t
e
m

 
r
u
O

n1 

n3 
n3 

n2 

n5 

n4 

n6 
n9 

n8 

n7 

n1 
n3 

n2 

n5 

n4 

n6 
n9 

n8 

n7 

n5 

n3 

n1 

n2 

n4 

n8 

n7 

n9 

n6 

Euclidean distance

n1 

n3 

n2 

n5 

n9 

n8 

n7 

n4 

n6 

Mahalanobis distance

n5 

n1 

n3 

n2 

n4 

n9 

n8 

n7 

n6 

Figure 1: Comparison of diﬀerent clustering methods on

incomplete information networks with missing edges.

resent the relations between them. Usually, it is very
diﬃcult to resolve all of the links within a food web.
However, it is relatively easier to ﬁgure out some local
regions within the food web. Discovering communities
in these incomplete food webs can help us identify mi-
cro ecosystems and the corresponding living organisms
of each micro ecosystem.

Finding communities in incomplete information networks
is a challenging task. Conventional graph-based clustering
methods can not be directly applied to it. The reason is
that traditional graph clustering methods, such as normal-
ized cut based methods [24] and modularity based methods
[19], mainly focus on the topological structure of the net-
work. Since most of the links are absent in incomplete infor-
mation networks, it is impossible to cluster the network with
this kind of methods. As shown in the middle level of Fig-
ure 1, if we cluster the nodes using the traditional attribute
based methods such as k-means, the most likely result is that
we place nodes with the most similar attributes in the same
cluster. However, the nodes which are densely connected
in structure may not necessarily mean they have the most
similar attributes, i.e., they may be only similar on a subset
of the attributes. For example, in the food web networks,
a community usually stands for a micro ecosystem and can
contain various kinds of living organisms, which can have
very diﬀerent attributes. Recently, some new algorithms
[31] which perform clustering based on both structures and
the attributes of the network are proposed. However, they
can not be applied on incomplete information networks due
to the absence of the complete linkage structure.

Given the assumption that the structure of the network

has a close relation with attributes of each object in the
information network, in this paper, we propose a novel ap-
proach for community detection in incomplete information
networks. To the best of our knowledge, this is the ﬁrst
attempt to formulate and address the incomplete informa-
tion network problem. The main idea of our approach is
that, since the structure of the network has a strong rela-
tion with the attributes of the objects in the network, we
can learn a global distance metric from the local informa-
tion regions with complete linkage information. Then, we
use the global metric to measure the distance between any
pair of nodes in the network. Because the metric is learned
from the structure of the network, the distance will reﬂect
the hidden linkage structure in the network. Finally, we
propose a distance-based clustering algorithm to cluster the
nodes in the incomplete information network. The diﬀerent
clustering results are shown in Figure 1. To summarize, this
work contributes on the following aspects:

• We identify and deﬁne the problem of community de-
tection in incomplete information networks with local
information regions,
i.e., an incomplete information
network that still has a few tiny local regions where
the complete linkage information is available.

• In order to ﬁnd a measurement, which can reﬂect the
structural relation between the nodes in incomplete in-
formation networks, we cast the side information of the
network into an optimization problem. Then a metric,
which can be used to measure the distance between
any pair of nodes, is learned.

• Based on the learned metric, we devise a distance-
based modularity function to evaluate the quality of
the communities.

• Finally, we propose a distance-based algorithm DSHRINK

which can discover the hierarchical and overlapped com-
munities. Moreover, in order to speedup the clustering
process, an eﬀective strategy is also taken.

This paper is organized as follows. We introduce the re-
lated work in Section 2. The formal deﬁnition of our problem
is presented in Section 3. In Section 4, we introduce how to
make use of the side information to learn a global metric.
In Section 5, we explain the distance-based clustering algo-
rithm. The experimental results are presented in Section 6.
Finally, we conclude in Section 7.

2. RELATED WORK

Community detection in networks and graphs has been
widely studied in recently years[16, 4]. Many approaches
mainly focused on the topological structures based on var-
ious criteria including modularity [19], normalized cut [24],
structural density [30] and partition density [3]. Given a
graph, which is clustered into k communities, the modular-
ity function Q is deﬁned as:

Q =

−# di

2L$2%

(1)

k

!i=1" li

L

where L is the number of edges in the graph, li is the number
of edges between nodes within community i, and di is the
sum of the degrees of the nodes in community i. The optimal

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France342clustering result is achieved by maximizing the modularity
value which ranges from 0 to 1. In general, maximizing Q is a
NP-hard problem. Hence, many heuristic approaches, which
try to approximate the optimal modularity value, were pro-
posed [10]. Such approaches include greedy agglomeration
[19, 28], mathematical programming [1], spectral methods
[25], simulated annealing [11], sampling techniques [22], etc.
However, modularity is not a scale-invariant measure, and
therefor, by relying on its maximization, can not detect com-
munities smaller than a certain size [8]. Besides, Palla et.
al.
[20] proposed a clique percolation method, which can
detect overlapped communities, but is not suitable for de-
tecting hierarchical structures. Huang et. al. [12] proposed
a parameter-free algorithm SHRINK, which can not only
discover overlapped and hierarchical communities but also
the hub nodes and outliers among them. Rosvall et. al. [21]
tried to compress the information of the graph by optimizing
the minimum description length of the random walk and pro-
posed a highly accurate algorithm namely Infomap. Ahn et
al. [3] insisted that link communities are fundamental build-
ing blocks, and the overlapped and hierarchical communities
in networks are two aspects of the same phenomenon. They
proposed a link-based approach which reveals the real world
communities eﬀectively. Other link-based methods were also
devised by [6].

There are also some graph clustering methods which based
on attributes. Tian et al. [26] proposed an OLAP-style ag-
gregation approach to summarize large graphs by grouping
nodes based on user-selected attributes and relationships.
This method achieves homogeneous attribute values within
clusters but ignores the intra-cluster topological structures.
Tsai et al.
[27] proposed a feature weight self-adjustment
mechanism for k-means clustering.
In that study, ﬁnding
the appropriate weight is modeled as an optimization prob-
lem which tries to minimize the separations within clusters
and maximize the separations between clusters. Since most
of the attributes based methods mainly focus on the homo-
geneity of the clusters, the cohesive internal structure of the
clusters can not be guaranteed.

Recently, some clustering methods based on both links
[9] introduced the con-
and attributes were also proposed.
nected k-center(CkC) problem, which checks whether an at-
tributed graph can be partitioned or not by considering both
attributes and the links. Since the CkC problem is NP-
complete, the authors proposed a constant factor approx-
imation algorithm and a heuristic algorithm for the large
data sets. [31] proposed SA-Cluster, which is based on both
structural and attribute similarities through a uniﬁed dis-
tance measure.
In that study, a graph is partitioned into
k clusters so that each cluster contains a densely connected
subgraph with homogeneous attribute values. Then, in or-
der to learn the degree of contributions of structural sim-
ilarity and attribute similarity automatically, an eﬀective
method was proposed.

3. PROBLEM DEFINITION

In this section, we formally deﬁne our problem and intro-

duce several related concepts.

Deﬁnition 1 (Information Network) An information
network is denoted as G = (V, E, A), where V is the set of

vertices, E ⊆ V ×V is the set of edges, and A =&a1, a2, ..., a|V |’

is the set of node attributes which describe the properties of
vertices in V .

For convenience, we use A(v) to denote the attribute vector
of node v and E(U) to represent the edges among nodes in
U(U ⊆ V ).

Deﬁnition 2 (IIN) Incomplete Information Networks with
Local Information Regions (IIN) are deﬁned as follows: given
an information network G = (V, E, A) and a network G! =
(V !, E!, A!), network G! is called an incomplete information
network with local information regions of G iﬀ (1) V ! =
V, E! ⊂ E (2) ∀v! ∈ V !, ∀v ∈ V, if v = v!, then A!(v!) =
A(v). (3) ∃V !! ⊆ V !, for ∀e ∈ E(V !!), then e ∈ E!(V !!).
Speciﬁcally, we call the local subnetwork g(V !!, E!!, A!!) as
the local information region denoted by L, where E!! =
E(V !!) and A!! = A(V !!).

From Deﬁnition 2, we know that an incomplete information
network with local information regions is a network G! =
(V !, E!, A!) with a small set of connected regions V !! ⊂ V !,
where the edges in E! − E(V !!) are missing. There can be
many diﬀerent types of incomplete information networks. In
this paper, we focus on the incomplete information network
that has some local information regions, where the linkage
structures are completely preserved as in Deﬁnition 2. For
the remaining of the paper, we will just refer to this type of
”incomplete information network with local information re-
gions” as incomplete information network. Obviously, there
can be more than one local information regions in an in-
complete information network G!. Furthermore, when we
are talking about an incomplete information network, we
assume that there is a corresponding information network,
potentially. A typical incomplete information network is
shown in the upper left level in Figure 1.

Deﬁnition 3 (Dissimilar Node Pair) Given an infor-
mation network G = (V, E, A) and its k clusters C1, C2, ..., Ck,

where (k

i=1 V (Ci) = V (G), any pair of nodes (vi, vj) is a
dissimilar node pair iﬀ (1) vi ∈ Cm ∧ vj ∈ Cn ∧ m )= n(1 ≤
m, n ≤ k); (2) E({vi, vj}) = ∅. We denote the dissimilar
node-pair set as D.

For an information network G, if C = C1, C2, ..., Ck is the
set of clusters which are based on the linkage structures of G,
we formalize our community detection problem as: given an
incomplete information network G! of G and the dissimilar
node-pair set D, based on some similar criteria, the objective
is to ﬁnd the set C ! which should be as similar as possible
to C.

4. OPTIMIZATION FRAMEWORK

In this section, we address the problem of how to learn a
global metric which is used to measure the distance between
any pair of nodes in an incomplete information network G!.
This goal is achieved by solving an optimization problem,
which makes use of the side information getting from the
link relations of G! and the dissimilar node-pair set D.

Deﬁnition 4 (Structure Similarity) Given a network
G = (V, E), for any pair of nodes vi, vj ∈ V , the structure
similarity between node vi and vj is deﬁned as

where Γ(v) is the set containing v and its neighbors.

|Γ(vi) ∩ Γ(vj)|

)|Γ(vi)-Γ(vj)|

s(vi, vj) =

(2)

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France343We compute the structure similarity between any pair of
nodes ui, vi in local information region Li by Equation (2).
Then, we deﬁne the similar node-pair set S as a 3-tuple set
about the structure similarity as follows:

Equations (5) similar to [29]:

g(M) = !(ui,vi)∈D

-ui, vi-M

(si-ui, vi-M)2 ≤ w

(7)

max
M

h(M) = !(ui,vi)∈S

M/ 0

S = {(ui, vi, si)|si = s(ui, vi), ui, vi ∈ V (Li)}

(3)

s.t.

Based on the similar node-pair set S and the dissimilar
node-pair set D, our objective is to ﬁnd a metric by which,
the similar nodes should be close together and the dissimilar
nodes should be far away from each other. Moreover, the
extent of closeness between any pair of similar nodes should
be based on the structure similarity between them. Inspired
by [29], this objective can be achieved by learning a distance
metric. Let the matrix M∈ Rm×m represent the distance
metric. Then, the distance between any two nodes ui, vi ∈ V
is deﬁned by

dM(ui, vi) = -ui − vi-M =)(ui − vi)T M(ui − vi)

(4)

In order to make sure the distance metric deﬁned by Equa-
tion (4) satisﬁes non-negativity and the triangle inequality,
we constraint M to be positive semi-deﬁnite. Now, we can
formalize our objective as an optimization problem as fol-
lows:

min
M

s.t.

!(ui,vi)∈S
!ui,vi∈D

(si-ui − vi-M)2

-ui − vi-M ≥ w

(5)

M/ 0

where w is a constant.

We notice that our objective function (5) is a linear func-
tion of M. Further more, both of the constraints given in
Equations (5) are convex. Hence, our optimization problem
is convex, which enables us to compute the global optimal
resolution.

Despite our optimization problem falls into the category of
convex programming, it does not fall into any special class
of convex programming, e.g., quadratic programming and
semi-deﬁnite programming. Hence, the global solution can
only be solved by a generic approach. We also notice that
the learned optimal M can appear in two forms, which are
diagonal matrix and full matrix.

In order to get the diagonal form of M, we give the equiv-

alent Equations (5) similar to [29]:

f(M) = f(M11, ..., Mnn)

= !ui,vi∈S

s2
i -ui − vi-2

M − log

 !(ui,vi)∈D

(6)

-ui, −vi-M


Minimizing Equation (6) can be resolved by using the Newton-
Raphson method. Furthermore, in order to keep the semi-
deﬁnite characteristics of M, we replace the Newton update
H −1 !f by αH −1 !f , where α is the a step-size parameter
optimized via a line-search which gives the largest downhill
steps subject to Mii ≥ 0 [29].

In order to get the full matrix of M, we give the equivalent

The reason for giving the transformation of the original opti-
mization problem is for eﬃciently ﬁnding the global optimal
full matrix M by using gradient descent and the idea of iter-
ative projections [5]. We ﬁrst use a gradient ascent on g(M)
to optimize (7). Then, we project the intermediate results
to hold the constraints (7). The similar tricks are also used
in [29].

Besides, we notice that in Equations (5) and (7), w is
a constant whose value is not important. This is because
the distance between any pair of nodes in network G! is a
relative variable. Changing the value of w only makes the
distance between any pair of nodes ui and vi change from
-ui − vi-M to w2-ui − vi-M. Hence, we choose w = 1 in
this paper. For the convenience of discussion, we denote the
incomplete information network as G = (V, E, A, M) in the
rest of the paper.

5. DISTANCE-BASED CLUSTERING

By optimizing our objective function, we have learned a
matrix M in section 4. In other words, we have gotten a
metric which can be used to measure the distance between
any two nodes in graph G.
Inspired by the density-based
clustering approaches, e.g.,
[30, 12], which cluster nodes
from the higher density to lower density, in this section,
we propose a distance-based clustering approach DSHRINK
which can detect the overlapped and hierarchical communi-
ties hidden in the graph.

5.1 Distanced-based Modularity

The distance-based clustering approach DSHRINK places
the nodes which have the shorter distance with each other
into the same cluster, and the nodes which have the longer
distance between them into diﬀerent clusters.
In order to
evaluate the quality of clusters, we deﬁne the distance-based
modularity as follows:

Deﬁnition 5 (Distance-based Modularity) Given an
incomplete information network G = (V, E, A, M) and its
cluster C = {C1, C2, ..., Ck}, the distance-based modularity
Qd is deﬁned as

(8)

Qd =

i

k

!i=1" DI

DT −# DC

i

DT $2%
i = .u,v∈Ci

where k is the number of clusters, DI
dM(u, v)
is the sum of distance between any pair of nodes within clus-
ter Ci, DC
between any node in cluster Ci and any node in the net-

i = .u∈Ci,v∈V dM(u, v) is the sum of distance
work G, and DT =.u,v∈V dM(u, v) is the sum of distance

between any two nodes in the network G.

Obviously, in contrast to the original modularity deﬁned
by Newman [19], the value range of Distance-based Modu-
larity is [−1, 0]. If Qd = 0, it means all the nodes are either

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France344placed into one cluster or placed into diﬀerent clusters ran-
domly. The smaller value of Qd means the better quality of
clustering.

Similar to [7, 12], if we combine any two modules Cs and
Ct, the distance-based modularity gain 0Qd achieved from
the combination can be computed by

0Qd = QCs∪Ct

d

− QCs − QCt =

2DU
st
DT −

2DC
s DC
t
(DT )2

(9)

C l u s t e r s 

1 C 
2 C 

.
 .
 .
 

i C 

.
 .
 .
 

N o d e   S e t 

1 i v 
2 i v 

.
 .
 .
 

j v 
i 

.
 .
 .
 

D i s t a n c e     M a p 
,  1 v 
, ) 
j v 
j d 
( 
( 
i 
) 
1 
,  2 v 
, ) 
j v 
j d 
( 
( 
i 
) 
2 

.
 .
 .
 

,  N v 
, ) 
jN d 
) 

j v 
( 
( 
i 

where DU
dM(u, v) is the sum of distance
between any two nodes in modules Cs and Ct respectively.

st = .u∈Cs,v∈Ct

According to Equation (9), we compute the gain of distance-

based modularity 0Qd for combing j clusters C1, C2, ..., Cj
into a new community by

0Qd(j) = .s,t∈{1,...,j},s&=t 2DU
5.2 Clustering Algorithm

DT

st

−.s,t∈{1,...,j},s&=t 2DC

(DT )2

s DC
t

(10)

Before addressing our algorithm in detail, we give the fol-

lowing deﬁnitions.

Deﬁnition 6 (Nearest Neighbor) Given an incomplete
information network G = (V, E, A, M), the nearest neighbor
set for ∀v ∈ V is deﬁned as

NN(v) = {y|y = arg min

x

dM(v, x), x ∈ V ∧ x )= v}.

(11)

Deﬁnition 7 (Mutual Nearest Neighbor) Given an
incomplete information network G = (V, E, A, M), any pair
of nodes u, v ∈ V is said to be mutual nearest neighbor,
denoted by u γ↔ v, iﬀ v ∈ NN(u) ∧ u ∈ NN(v) ∧ dM(u, v) =
γ, where γ ∈R +.

Deﬁnition 8 (Local Community) Given an incomplete
information network G = (V, E, A, M), we call the subgraph
C(v) = (V !, E!, A!, M,γ ) of G as a local community iﬀ (1)
v ∈ V !; (2) ∀u ∈ V !, ∃v ∈ V ! ∧ (u γ↔ v); (3) {u|u ∈ V ! ∧ u γ↔
v∧v /∈ V !} = ∅. γ ∈R + is the radius of the local community
C(v).

The distance-based shrinking approach DSHRINK is pre-
sented in Figure 3. Our approach can be divided into two
phases. At the ﬁrst phase, we compute the distance be-
tween any pair of nodes in graph G and store the distance
as a 3-tuple (vi, vj, dM(vi, vj)) into a map structure (see
Figure 2). For any vi ∈ V , the sum of the distance between
node vi and any other node vj ∈ V is saved in ST
i . Since
ST
i , computing ST
DC
i
in advance can speed up computing DC
s when computing Qd.
For the same purpose, the total distance DT between any
pair of node is also be computed.

s =.vi∈Cs,vj ∈V dM(vi, vj) =.vi∈Cs

At the second phase, (1) we ﬁrst begin at an arbitrary
node and span the node to a local community based on Def-
inition 8. All the nodes, which are in the local community,
will be tagged as ”visited”. Then, we choose the next un-
visited node in graph G and repeat the above step. This
process will not stop until all the nodes are visited.
(2)
Secondly, for each local community discovered by the ﬁrst
step, we view each single node in it as a community. Then
0Qd is computed according to Equation (10). If 0Qd < 0,

Figure 2: Data structure used in DSHRINK

which means the combination of the communities can de-
crease the total distance-based modularity Qd, we shrink the
local community as a super node. Otherwise, the local com-
munity will not be shrunk. (3) Thirdly, we tag all the nodes
including super nodes and the common nodes as ”unvisited”
and repeat the ﬁrst and second steps. The above steps will
be repeated many times until shrinking any local maximal
community can not decrease the Qd any more. Finally, the
nodes condensed in a super node form a community, and
diﬀerent super nodes stand for diﬀerent communities.

According to the deﬁnition of local community, we know
that the order of traversing the nodes in the incomplete in-
formation network G does not change the ﬁnal members of
the local community. Moreover, from the clustering process,
we know that the local community, in which nodes have a
shorter distance, will be shrunk at the prior or the same
iteration than the local community, in which nodes have a
longer distance. Single nodes, which have not been shrunk
to any other super nodes, are viewed as hubs or outliers de-
pending on how many communities they are close to. If we
want to form the overlapped communities, the hub nodes
will be placed into more than one communities. Otherwise,
each hub node will only be placed into the community which
makes the most decrease of Qd by adding the hub node. If
we view the distance between each pair of nodes as the struc-
ture similarity, the above shrinking process is similar to [12].

5.3 Speeding up the Clustering Process with

Approximation

It is possible to speed up the clustering process by allowing
some approximation in the determination of the local com-
munity. We deﬁne -approximate mutual nearest neighbor
and -approximate local community as follows:

Deﬁnition 9 (-approximation Mutual Nearest
Neighbor) Given an incomplete information network
G = (V, E, A, M), any pair of nodes u, v ∈ V is said to be
-approximation mutual nearest neighbor in G, denoted by
u ↔
v, iﬀ (v ∈ NN(u) ∧| dM(u, v) − dM(v, x)|≤ ) ∨ (u ∈

NN(v) ∧| dM(u, v) − dM(u, y)|≤ ), where x ∈ NN(v),
y ∈ NN(u), ∈R +.

Obviously, -approximate mutual nearest neighbor is an ex-
tended version of mutual nearest neighbor.

Deﬁnition 10 (-approximation Local Community)
Given an incomplete information network G = (V, E, A, M),
C(v) = (V !, E!, A!, M, ) is a subgraph of network G. C(v)
is said to be a -approximation local community of G iﬀ (1)

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France345DSHRINK(G = (V, E, A, M))

Input:

G = (V, E, A, M) : Incomplete information network.

Output:

C = {C1, C2, ..., Ck} : Cluster set.

HO : Hubs and outliers.

Process:

1
2
3
4

Initialize each vi ∈ V as a community and put it in C;
for each vi ∈ V do

for each vj ∈ V ∧ vi #= vj do

Compute dM(vi, vj ) according to Equation 4;
Store (key(vi, vj ), value(dM(vi, vj ))) with
ascending order into the distance map.
ST
i + = dM(vi, vj);
DT + = dM(vi, vj );

end

end

5
6
7
8
9 while true do
10
11
12

for each vi ∈ V do

if vi.visited then continue
Span a local community C(vi) according to
Deﬁnition 8;
for each vj ∈ V (C(vi)) do

vj .visited = ture;

end
C ← C ∪ C(vi);

end
Qd.descrease = f alse;
for each Cj ∈ C do

Compute &Qd according to Equation 10;
if &Qd < 0 then

13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31 Get single nodes from C and put them into HO
32

vs ← V (Cj );
C ← (C − Cj ) ∪ vs;
Qd+ = &Qd;
Qd.descrease = true;
vs.visited = f alse;

end
if !(Qd.descrease) then break;

return C, HO;

end

end

Figure 3: The Description of DSHRINK

v); (3) {u|u ∈ V ! ∧ u ↔
v ∈ V !; (2) ∀u ∈ V !, ∃v ∈ V ! ∧ (u ↔


v ∧ v /∈ V !} = ∅; (4) let f(r) = {r|r = dM(s, t), s ↔
t ∧ s ∈

V ! ∧ t ∈ V !}, |Max(f(r)) − Min(f(r))|≤ ; (5)when (3)
and (4) can not be held at the same time, (4) is prior to (3)
to be guaranteed. , r ∈R +.

We note that this relaxation of the deﬁnition of local com-
munity can greatly speed up the clustering process. In order
to take advantage of -approximation local community, the
only diﬀerence in DSHRINK is to span a -approximation
local community instead of a local community in step (12).
When we span the -approximation local community (> 0),
the ﬁnal clustering result may rely on the visiting sequence
of the nodes. In this paper, we give priority to the shorter
distance nodes among all of the -approximation neighbours
when spanning the -approximation local communities. Our
experimental results show that the ﬁnal clustering eﬀect is
almost not aﬀected by the order of the visiting sequence of
nodes by taking the above strategy. Furthermore, given an
appropriate parameter , we ﬁnd that this relaxation does
not aﬀect the practical quality of the communities obtained.

Table 1: Summary of experimental data sets

Dataset

# Nodes # Links # Attributes # Classes

DBLP-A
DBLP-B

4638
4559

16,447
14,407

102
102

6
6

6. EXPERIMENTS

In this section, we use two real-world data sets to validate
the eﬀectiveness and eﬃciency of our approach. All the ex-
periments are conducted on a machine with Intel 8-core 2.7
GHz processors and 28GB memory.

6.1 Data Sets

DBLP-A Dataset: DBLP-A is the data set extracted
from DBLP database1 which provides bibliographic infor-
mation on computer science journals and proceeding. We
extract paper information from 16 top conferences which
cover 6 research ﬁelds including Artiﬁcial Intelligence, In-
formation Retrieval, Computer Vision, etc. We create the
coauthor network by choosing authors, who published at
least 2 papers during 2000 − 2010, as the nodes of the net-
work. Any pair of authors who have coauthored are linked
in this coauthor network. This coauthor network contains
4638 nodes and 16447 links in total. Each node is attached
with a bag-of-words which extracted from the paper titles
published by him/her. We ﬁrst apply the standard text pre-
processing such as stemming, stop words removal. Then we
reduce the dimension of the bag-of-words to 100 by PCA
and use them as the features of the corresponding node. In
addition, the number of co-authors and publications are also
used as features of the nodes.

DBLP-B Dataset: We also extract paper information
from 16 top conferences of 6 research ﬁelds such as Algo-
rithms & Theory, Natural Language Processing, Bioinfor-
matics, etc. The same setups with DBLP-A are also used
here to build the coauthor network as our second data set,
called DBLP-B.

We summarize our data sets in Table 1.

6.2 Incomplete Information Network Gener-

ation

In order to simulate the incomplete information networks
with local information regions, we use the following experi-
ment setting. If we perform random sampling on the nodes,
the sampled network usually ends up being sparsely con-
nected, without local information regions. In this paper, we
use the snowball sampling [23] to sample a group of con-
nected local region at a time. We randomly sample one
node and use BFS to include its neighboring nodes into the
sampled region until a ﬁxed number of nodes are sampled.
We repeat this process until a number of local regions are
sampled. Then we assume the links within the local in-
formative regions are available to the algorithms, while the
remaining links in the network are removed.
In order to
control the total number of nodes being sampled, we intro-
duce a parameter p, called sample ratio, i.e., the ratio of the
nodes in the network being sampled into the local region.
In addition, we introduce another parameter q, called local
information region size, to control the size of each local in-
formation region. In detail, we ﬁrst randomly choose a node

1http://www.informatik.uni-trier.de/∼ley/db/

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France346kmeans
Md+DSHRINK
Mf+DSHRINK

0.9

0.8

0.7

y
t
i
r
u
P

 

1.0

0.9

0.8

y
t
i
r
u
P

kmeans
Md+DSHRINK
Mf+DSHRINK

 

0.6
 
1

2

3

6

4

5

sample size (p%)
(a) DBLP-A

7

8

9

0.7
 
1

2

3

4

5

6

7

8

9

sample size (p%)
(b) DBLP-B

Figure 4: Accuracy comparison between diﬀerent methods (q% = 0.3%).

kmeans
Md+DSHRINK
Mf+DSHRINK

1.0

0.9

0.8

0.7

y
t
i
r
u
P

 

1.00

0.95

0.90

0.85

0.80

0.75

y
t
i
r
u
P

kmeans
Md+DSHRINK
Mf+DSHRINK

 

0.6
 
0.1
1
local information region size (q%)

0.9

0.2

0.3

0.7

0.4

0.5

0.6

0.8

(a) DBLP-A

0.70

 
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
local information region size (q%)

(b) DBLP-B

Figure 5: Accuracy comparison between diﬀerent methods (p% = 10%).

in the network. We then include q% nodes from its neigh-
bors using BFS search. Common neighbors of any pair of
nodes in the sampled region are further included into the
sampled local region. The above sampling process contin-
ues until we sample p% of the nodes in the network.
In
addition to the local regions, we sample the same number
of nodes and use them to generate dissimilar pairwise con-
straints. In the sampled group, the pairs of nodes that are
in diﬀerent classes are then used as the dissimilar node-pair
set D. More concretely, for DBLP-A and DBLP-B datasets,
we choose the pair of authors, whose research ﬁelds are not
overlapped as the dissimilar node pair.

6.3 Evaluation Measures

In order to measure the eﬀectiveness of our approach, we
adopt Purity to evaluate the quality of the communities gen-
erated by diﬀerent approaches. The deﬁnition of purity is
as follows: each cluster is ﬁrst assigned with the most fre-
quent class in the cluster, and then the purity is measured
by computing the number of the instances assigned with the
same labels in all clusters. Formally:

max

j

|Ci ∩ lj|

(12)

where {C1, · · · , Ck} is the set of clusters, lj is the j-th class

Purity =

1
n

k

!i=1

label. The value of purity ranges from 0 to 1. The com-
munity structure generated by each compared method will
be evaluated using the true label of each node such that
the higher purity value means the higher accuracy of the
method. Since each author can have multiple research areas
as its class labels. We computed the purity of the cluster-
ing results based on each label separately, and the average
results over 6 labels are reported.

6.4 Compared Methods

In order to demonstrate the eﬀectiveness and eﬃciency of
our approach, we compare our approach with the following
methods:

• Kmeans: We use the default Euclidean metric to mea-
sure the distance between any node xi and the centroid
xk. The K value used in the dataset of DBLP-A and
DBLP-B is 6, which is the same number of clusters
with the ground truth.

• Md+ DSHRINK: We learn a diagonal Mahalanobis
matrix Md and use it as the input of M for DSHRINK.

• Mf + DSHRINK: We learn a full Mahalanobis matrix

Mf and use it as the input of M for DSHRINK.

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France3473500

)
e
s
(
 
e
m
T

i

3000

2500

 

DBLP−A
DBLP−B

3500

)
e
s
(
 
e
m
T

i

3000

2500

 

DBLP−A
DBLP−B

2000

 
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

approximation tolerance(b)
(a) Mf +DSHRINK

2000

 
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

approximation tolerance(b)
(b) Md+DSHRINK

Figure 6: The computation time with the diﬀerent values of b.

6.5 Effectiveness Results

The variation of purity scores under diﬀerent values of
p% is given in Figure 4. In this experiments, q% = 0.3% is
used. Since the accuracy of KMeans is not aﬀected by the
number of sampling nodes, the purity value of the KMeans
is a horizontal line in all cases. We notice that the pu-
rity scores of Mf +DSHRINK and Md+DSHRINK ascend
quickly with the increasing number of local information re-
gions sampled. Especially, when p% > 2%, the purity val-
ues of Mf +DSHRINK and Md+DSHRINK exceed kmeans
over all data sets. That is because, ﬁrstly, the learned Ma-
halanobis matrix M rescales all of the nodes into a new fea-
ture space, where the similar nodes are closer, and the dis-
similar nodes are further away than the original Euclidean
space. Secondly, DSHRINK can automatically detect the
most appropriate number of communities by minimizing the
distance-based modularity. Since the number of commu-
nities in the networks is unknown, Mf +DSHRINK and
Md+DSHRINK have more advantage for discovering the
most appropriate community structures than Kmeans. An-
other observation is that, in most of the cases, with the same
value of p%, the purity scores of Mf +DSHRINK are a little
higher than Md+DSHRINK in both DBLP-A and DBLP-
B data sets. This demonstrates that the full Mahalanobis
matrix performs better rescaling function for separating the
similar nodes from the dissimilar nodes than diagonal Ma-
halanobis matrix in DBLP-A and DBLP-B data sets. How-
ever, this principle dose not not always hold. For instance,
in DBLP-A data set, the purity score of Mf +DSHRINK is
less than Md+DSHRINK when p% = 8%.

In Figure 5, given a speciﬁed value of p% = 10%, we also
present the changes of purity scores with the diﬀerent value
of q%. We notice that, on the one hand, for a speciﬁed
p% = 10%, the larger value of q% makes more similar node
pairs be captured in each local information region, but fewer
local information regions get chosen in the whole incomplete
information network. On the other hand, with the smaller
value of q%, fewer similar node pairs can be captured in
each local information region, but more local information
regions can be sampled. Since both the number of local
information regions and the number of similar node pairs
can aﬀect the learning of the metric, ﬁnding the balance
point of q% is critical for achieving a better clustering result.

From Figures 5 (a) to (b), we know that the balance point
of q% can be gotten between 0.5% to 0.7%.

6.6 Efﬁciency Results

We notice that, computing the optimal Mahalanobis ma-
trix and the distance between any pair of nodes can be
accomplished in advance before clustering process. In this
part, we mainly focus on the clustering process and test how
-approximation local community speeds up the clustering
process and aﬀects the quality of clusters. The distance here
is relative and changeable according to diﬀerent values of w
in Equation (5). Hence, discussing the value of  is meaning-
less with a special value of w. Fortunately, we ﬁnd the top k
nearest nodes of each node is a good base for us to compute
the appropriate  value. In order to compute an appropriate
 value, we average the sum of distance between each node
and its corresponding top k nearest nodes as follows:

d = .N

i=1.j∈T opK(i) dM(vi, vj)

k|N|

(13)

where T opK(i) is the set of node index, whose distance to
vi ranks in the top k among all of the nodes to vi, and |N|
is the total number of nodes in the incomplete information
network. In this paper, we choose k = 10 and give the value
of  as  = d × b. For a speciﬁed incomplete information
network G and a metric M, the value of d is a constant.
Therefor, changing the value of b is equivalent to change the
value of .

In Figures 6(a) to (b), we have illustrated the variation in
the eﬃciency of diﬀerent b values for Mf +DSHRINK and
Md+DSHRINK. We observe that the computation time de-
creases quickly with the increasing value of b. It is because
relaxing the deﬁnition of local community to a certain ex-
tent can decrease the iteration times in the clustering pro-
cess. We also ﬁnd that the purity score are not changed
dramatically with diﬀerenct b values.

7. CONCLUSION

In this paper, we presented the ﬁrst approach for com-
munity detection in incomplete information networks with
local information regions. While the traditional community
detection algorithms make the assumption of the full knowl-
edge of linkage information, they can not solve the problem

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France348of community detection in incomplete information networks.
In order to resolve this problem, we explored the metric
learning idea and learned a global metric from the side in-
formation of the incomplete information network. Moreover,
we proposed the distance-based modularity function. Based
on this function, we further devised a distance-based cluster-
ing algorithm DSHRINK. In order to speed up the cluster-
ing process, some helpful approximation strategies were also
proposed. Experimental results illustrated the eﬀectiveness
and eﬃciency of our approach.

8. ACKNOWLEDGMENTS

The ﬁrst author is supported by National Natural Science
Foundation of China through grants 60933005, 60873204
and the National High-Tech Program through grant
2010AA012505. The third author is supported in part by US
NSF through grants IIS 0905215, DBI-0960443, and Google
Mobile 2014 Program.

9. REFERENCES

[1] G. Agarwal and D. Kempe. Modularity-maximizing
graph communities via mathematical programming.
European Physical Journal B, 66:409–418, 2008.

[2] C. Aggarwal, Y. Xie, and P. Yu. Towards community

detection in locally heterogeneous networks. SDM,
pages 391–402, 2011.

[3] Y. Ahn, J. Bagrow, and S. Lehmann. Link

communities reveal multiscale complexity in networks.
Nature, 466:761–764, 2010.

[4] H. Alani, S. Dasmahapatra, K. O’Hara, and

N. Shadbolt. Identifying communities of practice
through ontology network analysis. Intelligent
Systems, 18(2):18–25, 2003.

[5] S. Boyd and L. Vandenberghe. Convex Optimization.

Cambridge University Press, 2004.

[6] T. Evans and R. Lambiotte. Line graphs, link

partitions and overlapping communities. Physical
Review E, 80:016105, 2009.

[7] Z. Feng, X. Xu, N. Yuruk, and T. A. J. Schweiger. A
novel similarity-based modularity function for graph
partitioning. In DaWak, pages 385–396, 2007.

[8] S. Fortunato and M. Barthelemy. Resolution limit in

community detection. Proceedings of The National
Academy of Sciences, 104(1):36–41, 2007.
[9] R. Ge, M. Ester, B. J. Gao, Z. Hu, B. K.

Bhattacharya, and B. Ben-moshe. Joint cluster
analysis of attribute data and relationship data: The
connected k-center problem, algorithms and
applications. ACM Transactions on Knowledge
Discovery From Data, 2:1–35, 2008.

[10] B. H. Good, Y. A. de Montjoye, and A. Clauset. The
performance of modularity maximization in practical
contexts. Physical Review E, 81:046106, 2010.

[11] R. Guimera and L. N. Amaral. Functional cartography

of complex metabolic networks. Nature,
433(7028):895–900, 2005.

[12] J. H. H. D. Y. S. Y. L. J. Huang, H. Sun. SHRINK: a

structural clustering algorithm for detecting
hierarchical communities in networks. In CIKM, pages
219–228, 2010.

[13] M. Kim and J. Leskovec. The network completion

problem: Inferring missing nodes and edges in
networks. In SDM, pages 47–58, 2011.

[14] J. Z. Z. N. L. Tang, H. Liu. Community evolution in
dynamic multi-mode networks. KDD, pages 677–685,
2008.

[15] A. Lancichinetti and S. Fortunato. Community
detection algorithms: A comparative analysis.
Physical Review E, 80(5):056117, 2009.

[16] J. Leskovec, K. Lang, A. Dasgupta, and M. Mahoney.
Statistical properties of community structure in large
social and information networks. WWW, pages
695–704, 2008.

[17] J. Leskovec, K. Lang, and M. Mahoney. Empirical
comparison of algorithms for network community
detection. WWW, pages 631–640, 2010.

[18] Y. Lin, J. Sun, P. Castro, R. Konuru, H. Sundaram,
and A. Kelliher. Metafac: community discovery via
relational hypergraph factorization. KDD, pages
527–536, 2009.

[19] M. Newman and M. Girvan. Finding and evaluating

community structure in networks. Physical Review E,
69:026113, 2004.

[20] G. Palla, I. Derenyi, I. Farkas, and T. Vicsek.

Uncovering the overlapping community structure of
complex networks in nature and society. Nature,
435:814, 2005.

[21] M. Rosvall and C. Bergstrom. Maps of random walks

on complex networks reveal community structure.
Proceedings of the National Academy of Sciences,
105:1118, 2008.

[22] M. Sales-Pardo, R. Guimer`a, A. Moreira, and

L. Amaral. Extracting the hierarchical organization of
complex systems. Proceedings of the National
Academy of Sciences, 104(39):15224–15229, 2007.

[23] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher,
and T. Eliassi-Rad. Collective classiﬁcation in network
data. AI Magazine, 29(3):93–106, 2008.

[24] J. Shi and J. Malik. Normalized cuts and image

segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(8):888–905, 2000.

[25] M. Shiga, I. Takigawa, and H. Mamitsuka. A spectral
clustering approach to optimally combining numerical
vectors with a modular network. KDD, pages 647–656,
2007.

[26] Y. Tian, R. Hankins, and J. Patel. Eﬃcient

aggregation for graph summarization. In SIGMOD,
pages 567–580, 2008.

[27] C. Tsai and C. Chiu. Developing a feature weight

self-adjustment mechanism for a k-means clustering
algorithm. Computational Statistics Data Analysis,
52:4658–4672, 2008.

[28] K. Wakita and T. Tsurumi. Finding community

structure in mega-scale social networks. In WWW,
pages 1275–1276, 2007.

[29] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance

metric learning, with application to clustering with
side-information. In NIPS, pages 505–512, 2002.

[30] X. Xu, N. Yuruk, Z. Feng, and T. A. J. Schweiger.

Scan: A structural clustering algorithm for networks.
In KDD, pages 824–833, 2007.

[31] Y. Zhou, H. Cheng, and J. Yu. Graph clustering based

on structural/attribute similarities. VLDB
Endowment, 2:718–729, 2009.

WWW 2012 – Session: Community Detection in Social NetworksApril 16–20, 2012, Lyon, France349