Structured Query Suggestion for Specialization and

Parallel Movement: Effect on Search Behaviors
Makoto P. Kato†‡∗

Katsumi Tanaka‡

Tetsuya Sakai†

†Microsoft Research Asia, China

tetsuyasakai@acm.org

‡Kyoto University, Japan

{kato,tanaka}@dl.kuis.kyoto-u.ac.jp

ABSTRACT
Query suggestion, which enables the user to revise a query with a
single click, has become one of the most fundamental features of
Web search engines. However, it is often difﬁcult for the user to
choose from a list of query suggestions, and to understand the re-
lation between an input query and suggested ones. In this paper,
we propose a new method to present query suggestions to the user,
which has been designed to help two popular query reformulation
actions, namely, specialization (e.g. from “nikon” to “nikon cam-
era” ) and parallel movement (e.g. from “nikon camera” to “canon
camera”). Using a query log collected from a popular commercial
Web search engine, our prototype called SParQS classiﬁes query
suggestions into automatically generated categories and generates a
label for each category. Moreover, SParQS presents some new enti-
ties as alternatives to the original query (e.g. “canon” in response to
the query “nikon”), together with their query suggestions classiﬁed
in the same way as the original query’s suggestions. We conducted
a task-based user study to compare SParQS with a traditional “ﬂat
list” query suggestion interface. Our results show that the SParQS
interface enables subjects to search more successfully than the ﬂat
list case, even though query suggestions presented were exactly the
same in the two interfaces. In addition, the subjects found the query
suggestions more helpful when they were presented in the SParQS
interface rather than in a ﬂat list.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

General Terms
Algorithms, Experimentation

Keywords
Query suggestion, search user interface, query log mining, Web
search

1.

INTRODUCTION

Query suggestion has become one of the most fundamental fea-
tures of commercial Web search engines. Given a list of query sug-
gestions, the user can simply click on one of them to initiate a new
∗
This research was conducted while the ﬁrst author was an intern
at Microsoft Research Asia.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

search. Providing effective query suggestions to the user is very im-
portant for helping the user express his information need precisely
so that he can access the required information. Hence, many query
suggestion algorithms based on session and clickthrough data have
been proposed [2, 4, 5, 7, 12, 18, 19, 25]. On another front, how
to present query suggestions can greatly affect their usefulness and
search experience to the user. Even if individual query suggestions
are highly related to the user’s original query, he may ﬁnd it difﬁ-
cult to choose a good suggestion from a large list, or to understand
the relation between an input query and suggested ones.

Traditionally, query suggestions are provided to the user in a
ﬂat list, as in commercial Web search engines such as Google and
Bing. Recently, some richer methods of presenting query sugges-
tions have been explored. For example, Sadikov et al. [21] clus-
tered query suggestions using clickthrough and session data. As
mentioned in their work, clustering query suggestions can save the
user’s time and effort to locate a suggestion that serves his need.
Going one step further, Guo et al. [10] automatically labeled query
suggestion clusters based on social annotation. This may help the
user understand the meaning of each query suggestion more easily.
The ﬁrst contribution of the present study is that we demonstrate
the effect of query suggestion presentation methods on the user’s
search behavior. To be more speciﬁc, our user study shows that
subjects using a ﬂat list query suggestion interface and those using
a richer query suggestion interface behaved signiﬁcantly differently
even though the set of query suggestions presented was exactly the
same.

The second contribution is a new query suggestion presentation
method that aims to support different types of query reformulation
seamlessly. According to Boldi et al. [6], there are two major types
of query reformulation: specialization, in which a broad or am-
biguous query is modiﬁed to narrow down the search result; and
parallel movement, in which the user’s topic of interest shifts to an-
other with similar aspects. For example, if the user’s original query
is “nikon” and he selects “nikon camera” from the query sugges-
tion list, this action is a specialization. Moreover, if the same user
who selected the “nikon camera” query is interested in buying cam-
eras rather than in the Nikon brand, then alternative brands such as
“canon ixy” and “olympus camera” may be useful to him as query
suggestions. Thus, he may select “canon ixy” from the query sug-
gestion list and may end up buying a Canon camera, even if he may
not have known this company at the outset. This action is a paral-
lel movement. Boldi et al. reported that these two types of query
reformulation constituted 30-38% and 48-56% of a Yahoo! search
query log, respectively.

Although the aforementioned two query suggestion presentation
methods [10, 21] are effective for either specialization or parallel
movement, they do not necessarily support these two types of query

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France389on clicked URLs, and selects some clusters as query suggestion cat-
egories (e.g. photo and accessories in Figure 1). Finally, it classi-
ﬁes query suggestions into labeled categories that are shared across
entities. Although SParQS can help the user only when the current
query contains a named entity, previous research suggests that this
is not a major limitation: it has been reported that at least 20-30%
of queries input to Microsoft’s Bing are named entities [30], and
that over 70% of search queries contain named entities [11].

The main objective of the present study is to demonstrate the ef-
fect of query suggestion presentation methods and the advantages
of the SParQS interface over a ﬂat list query suggestion interface
in search tasks. To this end, we designed ten Information Gather-
ing tasks (e.g. “Find information about Bill Clinton”) and ten Entity
Comparison tasks (e.g. “Find cars made by Mazda and other manu-
facturers”), and conducted user experiments involving 20 subjects.
Our results show that the SParQS interface enables the subjects
to search more successfully than the ﬂat list case, and is particu-
larly advantageous for Entity Comparison tasks. In addition, the
subjects found the query suggestions more helpful when they were
presented in the SParQS interface rather than in a ﬂat list.

The remainder of this paper proceeds as follows. Section 2 brieﬂy
surveys prior art in query suggestion algorithms, its presentation,
and knowledge acquisition from query logs. Section 3 formalizes
the query suggestion classiﬁcation problem and Section 4 describes
our approach to solving the problem. Section 5 describes the data
we input and generated from the SParQS back-end algorithm for
the purpose of evaluating the SParQS interface. Section 6 reports
on the user study in which we compared the SParQS interface with
a ﬂat list one. Finally, Section 7 concludes this paper and discusses
future work.

2. RELATED WORK
2.1 Query Suggestion Algorithms

Various query suggestion algorithms have been proposed in the
literature. One popular approach is to cluster queries based on the
clicked URLs within the corresponding search engine result pages.
Given a query, its query cluster can be identiﬁed, and the other
members of that cluster can be presented as query suggestions [4,
7]. Another approach is to incorporate random walk or hitting time
in a query and clicked URL bipartite graph [18, 19, 25]. Other stud-
ies utilized query sequences to model the user behavior, in order to
predict queries that are likely to follow a given query [2, 5, 12]. A
recent study tackles the problem of diversifying query suggestions
by diversifying their top-returned search results [26].

Any of the above algorithms could be plugged into SParQS, as
our problem formulation assumes that query suggestions are al-
ready available as input. In the experiments reported in this paper,
we use the well-known hitting time algorithm [19].
2.2 Query Suggestion Presentation

A variety of search user interfaces have been studied from the
viewpoints of usability and the effect on the user’s search activ-
ities. For example, some interfaces present classiﬁed or faceted
search results [9, 29]. Recently, as query suggestion has become a
common feature of Web search engines, studies on query sugges-
tion presentation have begun to emerge.

Kelly et al.

[15] investigated the effect of presenting the us-
age statistics of each query suggestion to the user. Their exper-
iments used four topics, each with eight query suggestions: four
frequently-used suggestions and four rarely-used ones. Each query
suggestion was accompanied by information as to how many other
people used that suggestion. They found that the use frequency of

Figure 1: Screenshot of the SParQS interface.

reformulation at the same time. For example, given the current
query “nikon,” it is not clear how query suggestions like “canon
ixy” should be presented to the user, as Canon is an alternative
camera brand. If the user wants to select a query suggestion strictly
related to Nikon, then showing “nikon camera” and “canon ixy”
in the same cluster would be confusing for the user. On the other
hand, if the user is open to alternatives such as Canon cameras, then
showing both “nikon camera” and “canon ixy” together in some
way might help the user. Thus, it may be difﬁcult for simple clus-
tering approaches to support specialization and parallel movement
simultaneously.

Motivated by the above observations, we propose a new query
suggestion presentation method that aims to support the user’s spe-
cialization and parallel movement activities seamlessly. We call
our prototype system Specialization and Parallel movement Query
Suggestion (SParQS): a screenshot of the SParQS interface is shown
in Figure 1. In this example, the current query is “nikon,” and some
specialization suggestions for “nikon” are classiﬁed into several
categories such as photo and accessories in a way similar to the
presentation method proposed by Guo et al. [10]. Moreover, to
support parallel movement, alternative entities “canon” and “olym-
pus” are presented vertically, and their specialization suggestions
are optionally shown to the user. (In Figure 1, “canon” is selected
as the current alternative entity.) Note that the specialization sug-
gestions for the original query “nikon” and those for an alternative
entity “canon” are arranged in the same way, under common as-
pects such as photo and accessories. Our hope is that the SParQS
interface will encourage the user to compare across multiple en-
tities based on a certain aspect: for example, comparing different
camera makers in terms of camera lenses. Moreover, in Figure 1,
note that the user can go directly from the current query “nikon” to
“canon digital cameras”. This is a combination of parallel move-
ment and specialization, which we call diagonal movement.

By default, SParQS only shows the classiﬁed query suggestions
for the current query (e.g., information presented beneath “nikon”
in Figure 1), plus the alternative entity names (“canon” and “olym-
pus” shown vertically in Figure 1). The query suggestions for an
alternative entity is shown only when that entity is clicked by the
user. This is because presenting too much query suggestion infor-
mation to the user unconditionally may have a negative impact on
his search activity.

The SParQS back-end algorithm relies on a log of queries and
clicked URLs from Microsoft’s Bing1. First, SParQS clusters en-
tities (e.g. “nikon,” “canon” and “olympus” in Figure 1) based on
queries that contain these entities. Second, it clusters queries based

1http://www.bing.com/

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France390nikon cameras
nikon dslr
nikon accessories
nikon lens
nikon lens review

nikon cameras
nikon dslr

nikon accessories

nikon lens
nikon lens review

Photo
nikon cameras
nikon dslr
Accessories
nikon accessories
Lenses
nikon lens
nikon lens review

(a)

(b)

(c)

Figure 2: Examples of (a) ﬂat list, (b) clustering and (c)
clustering-with-labels interfaces.

suggestions did not affect the subjects. Kelly, Gyllstrom and Baily
[16] studied the difference between “term suggestion” and query
suggestion. A term suggestion system they developed enables the
user to add terms to his original query by clicking on each sugges-
tion term. In their interactive information retrieval study with 55
subjects and 20 topics, subjects preferred query suggestion to term
suggestion.

To clarify the differences between SParQS and other typical query
suggestion presentation methods, Figure 2 shows examples of (a)
ﬂat list, (b) clustering and (c) clustering-with-labels interfaces, which
should be compared with our SParQS interface shown in Figure
1. The ﬂat list represents the query suggestion method adopted
by traditional search engines. The clustering interface corresponds
to work on query suggestion clustering such as that of Sadikov et
[21], who used a random walk model in a query-query and
al.
query-document transition graph. The clustering-with-labels inter-
face represents a method proposed by Guo et al. [10], who utilized
social annotation data for labeling query suggestion clusters.

SParQS differs from these existing query suggestion methods in
the following two aspects. First, SParQS provides new entities as
alternatives to the current entity, and their query suggestions: see
“canon” and “olympus” shown in Figure 1. Second, in contrast to
these bottom-up clustering approaches, we ﬁrst generate categories
and then classify query suggestions into these categories. This is
because we want categories that apply not only to the current en-
tity but also to the alternative entities: see photo, accessories etc.
in Figure 1. Moreover, while previous work on query suggestion
focused mainly on intrinsic evaluation (e.g. accuracy), the main
objective of this study is to demonstrate the effect of query sugges-
tion presentation methods and the advantages of the SParQS inter-
face over the traditional ﬂat list one through task-based evaluation.
There are also existing studies on search functionalities that are
related to query suggestion but are different: namely, interactive
query expansion [3] and query completion [1, 14, 28]. Interactive
query expansion is basically the same as the aforementioned term
suggestion, but it appears to have been replaced by query sugges-
tion during the last decade.
In contrast, query completion is as
common a feature as query suggestion in current search engines
and they probably complement each other: while query suggestion
provides a static list of possible queries given a complete initial
query, query completion aims to provide a dynamic list of possibil-
ities given a preﬁx string of an initial query, often within the search
query box. Amin et al.
[1] conducted user studies that investi-
gate organization strategies of autocompletion such as alphabetical
ordering and grouping. However, their user study focused on au-
tocompleting an entity name rather than the search query (e.g. for
inputting a location name in a Web form).
2.3 Knowledge Acquisition from Query Logs
Query logs are often utilized for acquiring knowledge such as en-
tity names and search intents. For example, some studies [17, 23]

Table 1: Symbols with examples.

Symbol
E
Q
U
Sj
Qi
Qi = {Q(1)
E = {E1, . . .}
i = {Q
Q∗
∗(1)
i
, . . .}
Yi = {y(1)
S∗
j = {S
, . . .}
∗(1)
j

, . . .}

i

i

Example
E ={“canon”, “nikon”, “microsoft”, . . . }
Q ={“nikon photo”, “microsoft windows”, . . . }
U ={“http://www.bing.com/”, . . . }
S2 ={“nikon camera”, “nikon lens”, . . . }
Q1 ={“nikon photo”, “canon printer”, . . . }
Q(2)
E1 ={“olympus”, “nikon”, “canon”}

1 = {“olympus printer”, “canon printer”, . . . }

, . . .} Q
∗(3)
1 ={“nikon photo”, “canon camera”, . . . }
y(3)
1 =photo
∗(3)
S
2 ={“nikon camera”, “nikon dslr”, . . . }

extracted named entities from query log data by means of query
contexts, which are generated by replacing known entity names in
queries with a wildcard. The query contexts are then used to cap-
ture new entity names. In the present study, SParQS also utilizes
query contexts to ﬁnd alternative entities given a query containing
an entity.

Query logs are also used for mining search intents. For exam-
[27] proposed a query clustering method based
ple, Wen et al.
on content similarity, e.g.
term overlaps between the queries or
the similarity between clicked URLs, to organize common intents.
Yin and Shah [30] built a taxonomy of Web search intents from
clickthrough data, by extracting is-a relationships among queries
based on clicked URLs and constructing a tree whose nodes are
queries containing a named entity. In contrast to these approaches,
SParQS ﬁrst creates categories by means of query clustering, and
then classiﬁes query suggestions into these categories based on the
similarity between clicked URLs of queries and those of query sug-
gestions.

3. PROBLEM DEFINITION

As was mentioned earlier, the SParQS back-end ﬁrst clusters en-
tities, then clusters queries, and ﬁnally classiﬁes query suggestions
into categories. This section formalizes these three problems.

3.1 Input

We require the following as input:

1. E: a set of entities, which can be prepared, for example, by
extracting Wikipedia entry titles or leveraging named entity
dictionaries;

2. Q: a set of queries;

3. U: a set of URLs;

4. w(q, u): a click count function, which indicates how many
times a URL u ∈ U presented in response to a query q ∈ Q
has been clicked;

5. Sj: a set of query suggestions for each entity ej ∈ E; and
6. n: the number of query suggestion categories required.

Examples are shown in Table 1. The input items 2-4 are what we
call clickthrough data, which can be represented as a bipartite graph
with weighted edges. The sets of queries Q and URLs U are nodes,
and there is an edge between a query q ∈ Q and URL u ∈ U
in the bipartite graph iff the URL u presented in response to the
query q has been clicked. Each edge is weighted by the click count
function w(q, u). A set of query suggestions Sj for each entity
ej ∈ E is also an input item in this problem. Thus, the problem is
independent of how query suggestions are generated.

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France3913.2 Output

2. Q∗

We require the following as output (See also Table 1):
1. E = {E1, E2, . . .}: a set of entity clusters, obtained by clus-
tering entities in E based on queries that contain the entities;
i = {Q
}: a set of query suggestion cate-
gories for each entity cluster Ei, such that Q∗
i is a subset of
a query cluster class Qi = {Q(1)
, . . .}. The class of
query clusters Qi is obtained by clustering queries in Qi, the
set of queries that contain any entity from Ei;

, . . . , Q

∗(n)
i

∗(1)
i

, Q(2)

i

i

4. S∗

3. Yi = {y(1)

i

i

, . . . , y(n)
Ei, where a label y(k)
j = {S
, . . . , S
where S
ej ∈ Ei, which has been classiﬁed into a category Q
The query suggestion set S
the full set of suggestions for an entity ej.

i }: a set of labels for each entity cluster
is attached to a category Q
; and
}: a class of query suggestion sets,
represents the query suggestion set for an entity
∗(k)
.
i
is a subset of Sj, which is

∗(1)
j
∗(k)
j

∗(n)
j

∗(k)
j

∗(k)
i

∗(k)
i

∈ Q∗

i is a category for query suggestions in S

The SParQS interface utilizes the above output as follows:

in
response to a query containing an entity ej from an entity set Ei,
a set of query suggestions Sj is presented to the user. The query
suggestions in Sj are classiﬁed into categories Q∗
i , and each cat-
∗(k)
egory Q
.
j
For example, suppose that E1 = {“olympus”, “nikon”, “canon”},
and e2 = “nikon” as shown in Table 1. Given a query “nikon,” the
SParQS presents its query suggestions S2 such as “nikon camera”
and “nikon lens.” The query “nikon camera,” which is contained in
∗(3)
a set of query suggestions S
, is classiﬁed into a category Q
1
with a label y(3)
and their query suggestions classiﬁed into the categories Q∗
also presented to the user.

1 = photo. The other entities in the entity set E1,
i , are

∗(3)
2

Moreover, we require the above output items to satisfy the fol-

lowing criteria:

Evenness of Categories.

Categories should be chosen so that they are equally useful to
all the entities in the cluster. For example, given the entity cluster
{“nikon”, “canon”, “olympus”}, ixy is not a good category label
as it is a digital camera brand by Canon and does not apply to the
other two entities.

Speciﬁcity of Categories.

Categories should be chosen so that they are neither too broad
nor too narrow. For example, given the entity cluster {“nikon”,
“canon”, “olympus”}, a category product may not be speciﬁc enough
(even though this does satisfy the evenness requirement), and may
result in too many suggestions per category. On the other hand, if
a category is so speciﬁc that it only applies to one particular query
suggestion, then the very idea of classifying query suggestions be-
comes meaningless.

Accuracy of Suggestion Classiﬁcation.

Given a set of categories, query suggestions should be classiﬁed
appropriately. For example, “canon printer” should not be classi-
ﬁed into the photo category. This would only confuse the user.

Note that we have not stated any explicit requirements for entity
clustering. However, it follows from the above three requirements
that entities should be clustered so that we can obtain categories
that are evenly spread across the clustered entities. Subsequently,
at the query clustering stage, the evenness and speciﬁcity require-
ments determine the appropriate query cluster size.

4. SParQS BACKEND ALGORITHM

Given the requirements discussed in the previous section, this
section explains how the SParQS back-end algorithm works. Be-
cause it is difﬁcult to optimize in terms of the aforementioned even-
ness, speciﬁcity and the accuracy at the same time, our approach
decomposes the problem into three subproblems and tackles them
one by one. First, entities are clustered based on queries that con-
tain the entities. Second, queries are clustered based on their clicked
URLs, some of which are selected as query suggestion categories.
Finally, we classify query suggestions into the categories that are
shared across entities in an entity cluster.
4.1 Clustering Entities

Our ﬁrst step is to cluster a given set of entities using the method
utilized by Sekine and Suzuki [23] and later by Komachi and Suzuki
[17]. From a query log, query contexts are obtained for each entity
by replacing the occurrences of the entity in queries with a wild-
card. For example, given the entity “canon” and queries “canon
camera” and “price canon camera,” query contexts “∗ camera” and
“price ∗ camera” are obtained.
Formally, a context c is obtained by replacing an entity e in a
query “preﬁx e sufﬁx” with a wildcard, i.e. c = “preﬁx ∗ sufﬁx,”
where preﬁx and suﬃx are strings that may be empty. We denote
the original query “preﬁx e sufﬁx” by c(e) (obtained by substituting
e into c). Given the entity set E and the query set Q, we can deﬁne
a set of query contexts C = {c|c(e) ∈ Q ∧ e ∈ E}. Using the l-th
query context cl in C, we deﬁne an entity vector ve, whose l-th el-
ement is deﬁned as ve,l =
u∈U w(cl(e), u). Recall that w(q, u)
is the number of times a URL u has been clicked in response to the
query q. Thus, the l-th element is the number of times any URL
was clicked after query cl(e) was input.

P

The similarity between entities is simply deﬁned as the cosine
between entity vectors. For clustering the entity vectors, we con-
ducted some trial experiments and ﬁnally chose to use a standard
group-average method with cosine similarity. By applying a thresh-
old to hierarchical clusters, we obtain a set of entity clusters E =
{E1, E2, . . .}. For example, E1 = {“olympus”, “nikon”, “canon”}
as shown in Table 1.
4.2 Clustering Queries
Having clustered the entities from E, we obtain a query set Qi =
{c(e)|c(e) ∈ Q∧ e ∈ Ei} (see example in Table 1). That is, this is
the set of queries that contain any entity from Ei. We now need to
cluster the queries in Qi. Although some work on the query sugges-
tion and query clustering uses clickthrough data [4, 7, 18, 19, 25,
27], these methods are not directly applicable to our problem. To be
more speciﬁc, if we cluster queries simply based on clicked URLs,
we would likely obtain query clusters where each of them corre-
sponds to a single entity. For example, consider two queries “nikon
camera” and “canon camera.” The search results for these queries
would be completely different, and therefore these queries would
not have any clicked URLs in common. Building on such data,
traditional query clustering would result in a cluster speciﬁcally
for “nikon,” another cluster speciﬁcally for “canon,” and so on. In
contrast, as our evenness requirement dictates, we want query clus-
ters that apply to multiple entities. Thus, in order to bridge the
gaps across different entities, we utilize the idea of query context
described earlier, and regard queries that contain different entities
but share the same context as identical in the query clustering pro-
cess. In the above example, “nikon camera” and “canon camera”
are considered identical when summing up the click counts.

Using the m-th URL um in U, a query q is usually represented
as a vector whose m-th element is a URL click count w(q, um). In

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France392our approach, a query q = c(e) ∈ Qi is represented as a vector
whose element is the sum of click counts of queries that have the
same context c. Thus, the m-th element of a query vector vq for
the query q is deﬁned as vq,m =

w(c(ej ), um).

P

ej∈Ei

In the same way as the entity clustering, queries in Qi are clus-
tered based on query vectors, which result in a set of query clusters
Qi = {Q(1)
, . . .} for each entity cluster Ei (an example of
Qi is shown in Table 1). The categories for Ei will be selected
from these query clusters, as we shall describe below.

, Q(2)

i

i

i

i

P

, Q(2)

4.3 Classifying Query Suggestions
Now, we have obtained entity clusters E = {E1, E2, . . .}, and
query clusters Qi = {Q(1)
, . . .} for each entity cluster Ei.
In the next step, query suggestions of entities from Ei are classiﬁed
into each category selected from the query clusters Qi. For the sake
of simplicity, the index i of an entity cluster Ei and a set of query
clusters Qi for the entity cluster is omitted hereinafter. Thus, we
refer to Ei and Qi as E and Q, respectively.
A query suggestion can be classiﬁed into a query cluster if their
similarity is greater than a threshold θ. A query cluster Q(k) ∈ Q is
represented as a vector vQ(k) that is the sum of all the query vectors
in the query cluster, i.e. vQ(k) =
q∈Q(k) vq. We represent a
query suggestion s as a vector whose m-th element is the click
count w(s, um). We deﬁne the similarity between a query cluster
Q(k) and query suggestion s simply as the cosine between their
vectors, which we denote by Sim(Q(k), s). If Sim(Q(k), s) ≥ θ,
we can classify a query suggestion s into a query cluster Q(k). For
each query suggestion, we select exactly one category that satisﬁes
the similarity constraint.
We then choose n query clusters as categories to classify query
suggestions, which we denote by Q(cid:4)
(⊂ Q), where |Q(cid:4)| = n. Re-
call that the three criteria should be satisﬁed for this classiﬁcation,
i.e. evenness, speciﬁcity, and accuracy. The accuracy criterion cor-
responds to the similarity constraint, i.e. we can classify a query
suggestion s to a category Q(k) if Sim(Q(k), s) ≥ θ. Below, we
describe how to meet the other two criteria.

Evenness dictates that categories should be chosen so that query
suggestions classiﬁed into a category are spread evenly across en-
tities. We can express this criterion in the form of query sugges-
tion entropy over entities. For an entity cluster E and category
Q(k) ∈ Q(cid:4)

, the entropy is deﬁned as follows:

Hk(E) = − X
ej∈E

Pk(ej ) log Pk(ej),

(1)

where Pk(ej) represents the probability that a query suggestion
classiﬁed into a category Q(k) is of an entity ej. Letting S(k)
be
query suggestions of an entity ej classiﬁed into a category Q(k),
the probability Pk(ej) is deﬁned as follows:

j

Pk(ej) =

P

|S(k)
el∈E |S(k)

| + α
| + α|E| ,

j

l

(2)

where α is a smoothing factor to avoid zero for Pk(ej).

According to the nature of entropy, Hk(E) becomes higher as
query suggestions classiﬁed into a category Q(k) are distributed
more evenly across entities. Thus, we want to choose clusters that
achieve a high value of Hk(E), which represents high evenness.

Speciﬁcity dictates that categories should be chosen so that they
are neither too broad nor too narrow. We express this criterion as
query suggestion entropy over categories. For an entity ej ∈ E
and set of categories Q(cid:4)

, the entropy is deﬁned as follows:

Hj(Q(cid:4)

) = − X

Q(k)∈Q(cid:2)

Pj(Q(k)

) log Pj (Q(k)

),

(3)

where Pj (Q(k)) represents the probability that a query suggestion
of an entity ej is classiﬁed into a category Q(k). The probability
Pj (Q(k)) is deﬁned as follows:

Pj(Q(k)

) =

P

|S(k)

| + α
Q(l)∈Q(cid:2) |S(l)

j

j

| + α|Q(cid:4)| .

(4)

The entropy Hj(Q(cid:4)

) indicates appropriate speciﬁcity, since Hj(Q(cid:4)

) becomes higher as query suggestions of
an entity ej are distributed more evenly across categories. A high
Hj(Q(cid:4)
) achieves its
maximum when given query suggestions are exactly evenly classi-
ﬁed into categories, i.e. when we have the same number of sugges-
tions per category.

Combining the two entropies, we set an objective function f to

be maximized:

f (Q(cid:4)

) = λ

X

Q(k)∈Q(cid:2)

Hk(E) + (1 − λ)

X

ej∈E

Hj(Q(cid:4)

),

(5)

, and ﬁnding the set Q∗

where λ is a parameter that determines whether evenness or speci-
ﬁcity should be emphasized, and 0 ≤ λ ≤ 1.
Finally, we obtain the set of categories Q∗

that maximizes the
objective function f. The objective function f takes a set of cat-
egories Q(cid:4)
that maximizes the objective
function f is NP-hard. If the set function f were monotonic and
submodular, it would be guaranteed that a simple greedy algorithm
returns the (1 − 1/e)-approximation of the maximum [20]. In our
case, however, the function f is neither monotonic nor submodular
because of the difﬁculty of the classiﬁcation problem.

The major difﬁculty is that the classiﬁcation of query suggestions
is nondeterministic while selecting categories. As a query sugges-
tion can be classiﬁed into exactly one category while there are many
candidates of categories, we cannot determine which category a
query suggestion should be classiﬁed into until all the categories
are selected. On the other hand, categories cannot be selected from
query clusters without classiﬁed query suggestions, since the ob-
jective function f relies on how query suggestions are classiﬁed.
This is why the function f is neither monotonic nor submodular.
The objective function f can increase or decrease depending on
classiﬁed query suggestions.

We therefore propose a greedy algorithm to handle the selec-
tion of categories and classiﬁcation of query suggestions simultane-
ously. A query cluster is incrementally added to a set of categories
that achieves the maximum value of the objective function f when
the query cluster is added. The greedy algorithm is described in Al-
gorithm 1. This algorithm repeats two processes until the number
of selected categories reaches n: tentative classiﬁcation of query
suggestions (lines 4-13), and selection of the best query cluster as
a category (lines 15-22). First, all the query suggestions are tenta-
tively classiﬁed into query clusters. A query suggestion s in a set
of query suggestions Sj for an entity ej ∈ E is tentatively classi-
ﬁed into a query cluster Q(l) ∈ Q if their similarity Sim(Q(l), s)
is greater than a parameter θ (lines 8-10). Second, a query cluster
is chosen as a category that maximizes the objective function f in
Equation 5, using tentatively classiﬁed query suggestions (lines 15-
17). A selected query cluster Q(x) is removed from a whole query
cluster set Q (line 18). Query suggestions in S(x)
for each entity
ej are classiﬁed into the selected query cluster, or a category Q(x),
and also removed from a whole query suggestion set (lines 19-22).
Finally, a label y(k) for each category Q∗(k) is selected from
queries in the category. We select the query that maximizes the

j

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France393= {}

, . . .} for each entity ej ∈ E.

query suggestions Sj for each entity ej ∈ E.
gestion sets S∗

/* tentative classiﬁcation of query suggestions */
for each Q(l) ∈ Q do
for each ej ∈ E do
j = {}
S(l)
for each s ∈ Sj do
if Sim(Q(l), s) ≥ θ then

Algorithm 1 A greedy algorithm
Input: An entity cluster E, a set of query clusters Q, and a set of
= {Q∗(1), . . .}, and classiﬁed query sug-
Output: Categories Q∗
j = {S
∗(1)
1: Q(cid:4)
j
2: for k = 1 to n do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: Q(cid:4)
= Q(cid:4) ∪ Q(x)
17: Q∗(k) = Q(x)
18: Q = Q − {Q(x)}
for each ej ∈ E do
19:
∗(k)
j = S(x)
S
20:
Sj = Sj − S(x)
21:
22:
23: end for

end for
/* selection of the best query cluster as a category */
x = argmax

f (Q(cid:4) ∪ {Q(l)})

S(l)
j = S(l)
end if
end for

j ∪ {s}

end for

l

j

j

end for

similarity to any one of the query suggestions in the category. For-
mally, a label y(k) for a category Q∗(k) is deﬁned as follows:

y(k)

= argmax
q∈Q∗(k)

vT

q vs

(cid:9)vq(cid:9)(cid:9)vs(cid:9) ,

(6)

where the variable s represents any query suggestion classiﬁed to
the category Q∗(k), i.e. s ∈ S
. When presenting a

∗(k)
j

ej∈E S

query as a label to the user, the entity is removed from the query.

5. DATA, QUALITY AND ACCURACY

For conducting a task-based user study to evaluate the SParQS
interface, we applied the SParQS back-end algorithm to Bing’s
query log. This section describes the query log used, and the ap-
plication of the SParQS back-end algorithm to the data. To ensure
that the resultant data are of sufﬁcient quality that deserve a user
study, this section also reports on the quality of category labels as
well as the accuracy of query suggestion classiﬁcation.
5.1 Data

We used Microsoft Bing’s query log from April 25th to May 1st,
2010, where each record consists of a query and clicked URL. The
number of records is 3,503,469,327, which contains 76,462,963
unique queries, and 62,978,872 unique URLs. As the SParQS al-
gorithm also requires named entity lists as input, we also manually
gathered lists of entities from Web pages for ﬁve entity classes:
Company, Person, Landmark, City, and Product. These ﬁve en-
tity classes are the top categories from the Extended Named Entity
Hierarchy proposed by Sekine et al. [22]. The total number of enti-
ties is 5,156, which consists of 2,000 companies, 119 people, 1203
landmarks, 388 cities, and 1,446 products.

Table 2: Evaluations of category labels. Bold font indicates the
highest value in each row.

λ

Assessor 1
Assessor 2
Intersection

0.00
0.642
0.686
0.591

0.25
0.655
0.699
0.606

0.50
0.666
0.719
0.631

0.75
0.659
0.716
0.619

1.00
0.629
0.711
0.597

To generate query suggestions, we applied the hitting time algo-
rithm proposed by Mei et al. [19]. First, we constructed a click-
through bipartite graph, where queries and URLs are nodes and
each edge represents a URL click in a search engine result page
produced in response to a query. The click count w(q, u) is used
as the weight of each edge. For every query pair, we computed the
hitting time of a random walk, deﬁned as the expected number of
steps it takes from a query to another on the bipartite graph. Finally,
we selected 20 queries that are the “closest” to each query.

We clustered entities and queries based on the method described
in Section 4. Before the clustering, we ﬁltered out queries that oc-
cur less than 10 times in the query log. Entity and query vectors in
Section 4.1 and 4.2 were weighted by a standard TF-IDF method,
and clustered with a group-average clustering method as was men-
tioned in Section 4.1. Based on some preliminary experiments,
the thresholds for entity clustering and query clustering were set
to 0.25 and 0.20, respectively. Then, we selected categories and
classiﬁed query suggestions into the categories using the greedy al-
gorithm described in Algorithm 1. The number of categories n was
set to 5, and the parameter θ, the similarity threshold for query sug-
gestion classiﬁcation, was set to 0.30. As this data construction is
primarily for our user study rather than intrinsic evaluation, we did
not use any held-out data for parameter tuning.

For an algorithm evaluation, we manually chose 20 entity clus-
ters that had at least two entities from each of the ﬁve entity classes.
For example, entity clusters such as {“nikon”, “canon”, “olym-
pus”} and {“sharp”, “samsung”, “lg”, “sony”, “panasonic”} were
chosen from an entity class Company. We hired four assessors
for this evaluation, who are graduate students majoring in com-
puter science. Two of them evaluated categories of 100 entity clus-
ters with ﬁve types of values for a parameter λ, which determines
whether evenness or speciﬁcity should be emphasized in the objec-
tive function f in Equation 5. We asked the other two assessors to
evaluate classiﬁed query suggestions for 390 entities.

5.2 Quality of Categories

The two assessors evaluated the quality of the category labels
generated by SParQS. We showed the two assessors a list of cat-
egory labels, a set of entities, and their unclassiﬁed query sugges-
tions. They then were asked to rate each label on a 3-point scale:
highly relevant (appropriate for classifying the query suggestions);
somewhat relevant (somewhat awkward but may be useful for clas-
sifying the suggestions); and irrelevant (not appropriate).

For ﬁve parameter settings: λ = 0.0, 0.25, 0.5, 0.75, and 1.0,
the two assessors evaluated 495, 493, 492, 490, and 489 categories
of 100 entity clusters, respectively (2,459 categories in total). The
inter-assessor agreement was substantial: 0.616 in terms of quadratic-
weighted kappa [24]. The results for the ﬁve values of λ (see Eq. 5)
are summarized in Table 2, where rows “Assessor 1” and “Asses-
sor 2” show precisions computed based on evaluations by each as-
sessor, and a row “Intersection” shows precisions computed based
on only evaluations agreed among the two assessors. Here, preci-
sion is deﬁned as the number of highly or somewhat relevant cat-
egories divided by the total number of evaluated categories. Note
that λ = 0.0 means using the entropy over categories (speciﬁcity)

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France394only, and λ = 1.0 means using the entropy over entities (even-
ness) only. It can be observed in all the rows that λ = 0.5, which
combines the two entropy measures, is slightly more accurate than
when only one of them is used. Although the difference across
the ﬁve parameters is not signiﬁcant, this result shows that the two
criteria, i.e. speciﬁcity and evenness, are both useful for obtaining
good categories. Thus, we used λ = 0.5 for the evaluation of query
suggestion classiﬁcation as well as the user study.

It is difﬁcult to say whether the highest precision of 0.631 in
the row “Intersection” is “good” or not as previous work [10] does
not report on the accuracy of category labels. However, there is
clearly a lot of room for improvement. Our failure analysis shows
that many of our categories were good for only one particular en-
tity and not for all the entities even though we incorporated the
entropy measure for evenness. Note that if multiple entities sim-
ply lack common aspects, then it is simply impossible for SParQS
to generate labels that apply to all of the entities. That is, the en-
tity clustering stage may impose an upperbound on the quality of
category labels.
5.3 Accuracy of Suggestion Classiﬁcation

The other two assessors examined whether the query suggestions
are appropriately classiﬁed. They judged whether it makes sense to
classify each query suggestion into the given category. The assess-
ment was done on a 3-point scale: irrelevant, neutral, and relevant.
The assessors were asked to consider only the relationship between
the category and the query suggestion, and not the quality of the
category label or the suggestion.

The two assessors evaluated 2,836 classiﬁed query suggestions
of 390 entities (about seven query suggestions per entity), and the
inter-assessor agreement after excluding the neutral judgments was
substantial: 0.700 in kappa coefﬁcient. The precision computed
based on the same data, deﬁned as the number of suggestions judged
relevant divided by the total number of suggestions, was 0.630.
Again, we could not ﬁnd an appropriate baseline in related work on
query suggestion. While our task here is query suggestion classi-
ﬁcation given the categories and query suggestions, previous work
evaluated query cluster quality [21] or the quality of the query sug-
gestions themselves [10]. Here, we refer to a study on building a
taxonomy from clickthrough data by Yin and Shah [30]. They pro-
posed a method to extract is-a relationships among queries based on
clicked URLs, and evaluated this method in terms of precision and
recall. The precision of their method ranged from 0.125 to 0.195
(is-a relationships between queries), and from 0.333 to 0.810 (is-a
relationships between aggregated queries). The aggregated queries
in their study are similar to our query contexts. However, we can-
not directly compare our results with theirs, as our problem is to
classify a query into categories (i.e. groups of aggregated queries)
rather than to classify a query into queries, or an aggregated query
into aggregated queries.

While we see a lot of room for improvement in the current im-
plementation of our SParQS back-end algorithm, we observed that
the results are reasonable and are adequate for our user study to
investigate the advantages of the SParQS interface.

6. USER STUDY

To examine the effect of the SParQS interface on users’ search
behaviors, we conducted a user study similar to the Text Retrieval
Conference (TREC) Interactive Track [8]. We prepared 20 tasks,
hired 20 subjects and asked them to collect answers relevant to each
task within ﬁve minutes. For each task, each subject used either the
SParQS interface, or a ﬂat list interface as a baseline to complete
the task.

As SParQS can only handle queries containing a named entity,
we designed search tasks that involve such queries where query
suggestion might be useful. As we discussed in Section 1 using
the query “nikon” as an example, search tasks that are initiated by
a named entity query can generally be classiﬁed into two types:
ﬁnding information about the given entity (e.g. Nikon cameras),
and ﬁnding information about entities related to the given one in
terms of a particular aspect (e.g. competitors such as Canon and
Olympus). We thus devised 10 Information Gathering tasks and
10 Entity Comparison tasks: Information Gathering tasks require
the subject to gather information about a given entity from several
aspects (e.g. “Find information about Bill Clinton”), while Entity
Comparison tasks require the subject to gather information about
several different entities in terms of a particular aspect for compar-
ison (e.g. “Find cars made by Mazda and other manufacturers”). In
Information Gathering tasks, subjects may possibly utilize query
suggestions for specialization in order to focus on a certain aspect
of the input entity, while in Entity Comparison tasks, they may pos-
sibly use query suggestions for parallel movement to shift their at-
tention to another entity. While these tasks were speciﬁcally de-
signed to examine the usefulness of SParQS for specialization and
parallel movement and are not “real” search tasks obtained from
(say) session data, we argue that it is useful to clarify exactly for
what kinds of search tasks SParQS can effectively help the user
through our experiments.
6.1 System Design

To conduct our user study, we implemented a system that can
present the query suggestion data using either the SParQS interface
as shown in Figure 3 or the ﬂat list interface as shown in Figure 4.
Query suggestions, generated as was described in Section 5.1, are
exactly the same in the two interfaces: only how they are presented
is different. To further ensure that exactly the same information
is available on the two interfaces, our ﬂat list interface also shows
the alternative-entity suggestions obtained from SParQS under reg-
ular query suggestions2. In Figure 4, for example, alternatives to
the query “armani” such as “tom ford” and “hugo boss” are shown
under query suggestions for “armani.”

When a query suggestion is clicked, or the subject inputs a query
to the search form and clicks the search button, the system retrieves
Web pages using Bing API3. Subjects were required to collect an-
swers to each given task from either the Bing snippets or the actual
body of the retrieved Web pages. Subjects can enter an answer ei-
ther by typing or by copy-and-paste from a snippet or a Web page.
As shown near the top of Figure 3, we provided an answer input box
with an Add Answer button so that subjects can submit answers one
by one. For a quantitative analysis, we recorded collected answers,
queries input, suggestions utilized, documents viewed, and entities
to which one or more answers were given.
6.2 Experimental Design

Table 3 shows the 20 tasks we devised. (The actual questions
shown to the subjects were a little longer than ones shown in the
table, similar to the narratives of TREC topics (See Figure 3)).
For example, Task 5 “Find information about Bill Clinton” is a
Information Gathering task, and subjects start with the initial query
“bill clinton” and are expected to specialize the initial query such
as “bill clinton birthplace” and “bill clinton biography” to gather
some kinds of information about Bill Clinton. On the other hand,
for example, Task 17 “Find tourist attractions in famous European

2Yahoo! search engine (http://www.yahoo.com/) currently
presents query suggestions in a similar way as our ﬂat list interface.
3http://www.bing.com/developers/

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France395Figure 3: System for the user study.

Figure 4: Flat list query suggestion interface.

Table 3: Tasks used in our user study. Type G means Informa-
tion Gathering task, while C means Entity Comparison task.
Type

ID Shortened question
1
2
3
4
5
6
7
8
9
10
11
12
13

Find Kyocera products
Find Panasonic appliances
Find information about James Cameron
Find information about Bill Clinton
Find information about the White House
Find information about the Amazon River
Find travel information about Beijing
Find travel information about Budapest
Find Rolex watch brands
Find Armani products
Find cars made by Mazda and other manufacturers
Find cameras made by Nikon and its competitors
Find concert schedules of Norah Jones and other
musicians
Find albums by Lady Gaga and other female singers
Find accommodations for visiting U.S. National
Parks such as Yosemite
Find events held in Chicago’s tourist attractions
such as Grant Park
Find tourist attractions in famous European cities
such as Paris
Find weather information for Australian cities such
as Sydney
Find small laptops made by different makers includ-
ing Acer
Find prices for Steve Madden and other handbags

14
15

16

17

18

19

20

Initial query
kyocera
panasonic
james cameron
bill clinton
white house
amazon river
beijing
budapest
rolex
armani
mazda
nikon
norah jones

lady gaga
yosemite national
park
grant park

paris

sydney

acer

steve madden

G
G
G
G
G
G
G
G
G
G
C
C
C

C
C

C

C

C

C

C

cities such as Paris” is an Entity Comparison task, with the ini-
tial query “paris.” In this case, subjects are expected to examine
tourist attractions of different cities besides Paris, such as London
and Rome.

The initial queries shown in Table 3 were used to automatically
produce the initial search engine result page. The subject can then
use either the search box or the query suggestion interface to com-
plete the task. Note that we selected two tasks for each combination
of the task type (Information Gathering or Entity Comparison) and
the entity class (Company, Person, Landmark, City and Product):
for example, Tasks 1 and 2 are Information Gathering tasks for
Company (Kyocera and Panasonic, respectively).

Our 20 subjects were undergraduate or graduate students in com-
puter science in their twenties. Thus we believe that this is a sam-
ple of relatively advanced users who are familiar with Web search
and query suggestion. The subjects were asked to collect as many
answers relevant to each task as possible within ﬁve minutes. Al-
though the time limit was set, we asked the subjects to conduct
search tasks as usual so that we could observe natural search be-
haviors. Upon completion of each task, the subjects ﬁlled out a
questionnaire about that particular task. Upon completion of the 20
tasks, the subjects ﬁlled out a questionnaire about the entire exper-
iment and their preference of interfaces.

We assigned the tasks and the two interfaces to the subjects so
that (1) each task was completed with the SParQS and the ﬂat list
interface exactly ten times, respectively; and (2) the tasks and sub-
jects were as independent of each other as possible. We random-
ized the presentation order of tasks, query suggestions, entities and
categories (the latter for the SParQS interface only).
6.3 Results and Discussions

In our user study, we observed several signiﬁcant differences of
search behaviors between the SParQS and ﬂat list users. Table 4
summarizes the user study results of our user study. “#Answers”
is the number of answers found per question, “#Queries” is the
number of queries used (either a query input or a query suggestion
clicked), “#QS queries” is the number of query suggestions used,
and “#Documents” is the number of documents viewed. “#Enti-
ties” is the number of entities to which the subject found one or
more answers, and “Successful search rate” is the proportion of
searches where the subject found at least one answer. Whereas, Ta-
ble 5 summarizes the questionnaire results of our user study. “Fa-
miliarity” (Are you familiar with this topic?) “Easiness” (Was it
easy to complete this task?) “Satisfaction” (Are you satisﬁed with
the answers you collected?) “Enough Time” (Did you have enough
time to complete the task?) “Helpfulness” (Was the query sugges-
tion effective to complete the task?) scores are from the online

per-task questionnaire: subjects chose from scores 1 (Not at all), 2,
3 (Somewhat), 4, and 5 (Extremely). We performed an ANOVA for
two between subjects variables, i.e. task type (Information Gather-
ing vs. Entity Comparison) and interface type (SParQS vs. ﬂat list).
Statistical signiﬁcance at α = 0.05 is indicated at the Signiﬁcance
column in Tables 4 and 5. If signiﬁcant interaction was found in
ANOVA, we examined the difference between the two interfaces
for each task type with Bonferroni post-hoc test [13]. A dagger in
the Signiﬁcance column indicates signiﬁcant interaction between
the SParQS and ﬂat list interface in either Information Gathering
or Entity Comparison tasks.

Compared to the ﬂat list users, the SParQS users found more an-
swers in Entity Comparison tasks and fewer answers in Information
Gathering tasks [Table 4(a)]. But these are not statistically signif-
icant. Especially in Entity Comparison tasks, the SParQS users
found answers for signiﬁcantly more entities than the ﬂat list users
[Table 4(e)]. There was a signiﬁcant interaction between the two
interface types in Entity Comparison tasks, t(198) = 3.02, p <
0.005. Thus, the SParQS interface was helpful in Entity Compari-
son tasks in terms of ﬁnding answers, but underperformed the ﬂat
list interface in Information Gathering tasks. The effectiveness of
the SParQS interface might depend on the difﬁculty of search tasks:
according to results shown in Table 4(b) and (d), Entity Compar-
ison tasks needed signiﬁcantly more queries and documents than

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France396Table 4: User study results of the user study. The mean and standard deviation (SD) are shown as “<Mean> (<SD>)”. A dagger in
the Signiﬁcance column indicates signiﬁcant interaction between the two interfaces in Entity Comparison tasks.

(a)
(b)
(c)
(d)
(e)
(f)

#Answers
#Queries

#QS queries
#Documents

#Entities

Successful search rate

Information Gathering
SParQS

List

Entity Comparison
List

SParQS

9.58 (5.81)
3.45 (2.49)
2.01 (2.35)
2.50 (1.96)
1.14 (0.40)
0.19 (0.26)

8.14 (5.81)
3.99 (2.33)
2.60 (2.24)
2.23 (2.00)
1.20 (0.42)
0.27 (0.30)

7.58 (7.71)
5.40 (2.95)
2.14 (1.77)
3.45 (2.74)
1.65 (0.88)
0.14 (0.19)

9.63 (7.72)
4.83 (1.77)
3.13 (1.95)
4.27 (2.78)
2.09 (1.14)
0.18 (0.24)

Signiﬁcance

N/A
Task

Interface

Interface, Task, † (Entity Comparison)

Task

Interface, Task

Table 5: Questionnaire results of the user study. The mean and standard deviation (SD) are shown as “<Mean> (<SD>)”. A dagger
in the Signiﬁcance column indicates signiﬁcant interaction between the two interfaces in Information Gathering tasks.

Information Gathering
SParQS

List

Entity Comparison
List

SParQS

(a)
(b)
(c)
(d)
(e)

Familiarity
Easiness

Satisfaction
Enough time
Helpfulness

2.94 (1.24)
3.80 (1.08)
3.73 (1.05)
3.66 (1.08)
3.37 (1.23)

3.13 (1.27)
4.16 (0.88)
4.10 (0.95)
3.88 (0.96)
3.91 (0.94)

2.69 (1.43)
3.68 (1.13)
3.61 (1.27)
3.41 (1.28)
3.77 (1.12)

2.63 (1.23)
3.72 (1.08)
3.82 (0.94)
3.62 (0.92)
3.86 (1.01)

Signiﬁcance

Task
Task

Interface

Task, Interface

Interface, † (Information Gathering)

Information Gathering tasks, F (1, 396) = 32.71, p < 0.01 and
F (1, 396) = 38.16, p < 0.01, respectively.

The SParQS users utilized signiﬁcantly more query suggestions
than the ﬂat list users, F (1, 396) = 14.08, p < 0.01 [Table 4(c)].
Moreover, the SParQS users were signiﬁcantly more successful in
search than the ﬂat list users: successful search rate of the SParQS
users is signiﬁcantly higher than that of the ﬂat list users, F (1, 396)
= 6.18, p < 0.05 [Table 4(f)].

For a better understanding of user behavior in the both types of
tasks and interfaces, we visualize some representative search be-
haviors on the two interfaces in Figures 5(a) and (b): they show
transitions between query input (Q), query suggestion selection (S),
document viewing (D) and answer submission (A) for each sub-
ject. A horizontal sequence represents the behavior pattern of each
subject, e.g. a subject starts with an initial query (Q), views a doc-
ument (D), and submits an answer (A). Search behaviors in Infor-
mation Gathering tasks 3 and 4 are shown in Figures 5(a), where
the top two blocks present search behaviors of the ﬂat list users,
while the bottom two present ones of the SParQS users. In Informa-
tion Gathering tasks, the ﬂat list users generally input a few queries
(blue/green) and found many answers (red) from a few search re-
sults and documents, while the SParQS users utilized many query
suggestions (green) but failed to efﬁciently get answers. Thus, the
SParQS interface might have unnecessarily encouraged the subjects
to utilize many query suggestions, but in fact a few searches might
have been sufﬁcient for obtaining many answers. On the other
hand, the SParQS users achieved high efﬁciency in Entity Com-
parison tasks as seen in Figure 5(b). They utilized many query
suggestions (green) and thereby found many answers (red). Espe-
cially in Task 15, there is a drastic improvement (many more “A”s)
even though the SParQS and ﬂat list interfaces contain exactly the
same query suggestions.

In Table 5, we see some positive questionnaire results. The
SParQS users were more satisﬁed with their answers, F (1, 396) =
7.3476, p < 0.01 [Table 5(c)], they felt that they had enough time
to complete the task, F (1, 396) = 3.9557, p < 0.05 [Table 5(d)],
and they felt that query suggestions were more helpful than the ﬂat
list case, F (1, 396) = 8.3381, p < 0.01 [Table 5(e)]. It seems
that the structured query suggestions highly impacted the usability
of query suggestions in both types of tasks. Those positive effects
were caused probably by the higher successful search rate and ac-
cessibility to query suggestions. The SParQS users did in fact uti-

lize more query suggestions than the ﬂat list users as can be seen in
Table 4(c).

[21] and Guo et al.

In post-experiment questionnaires, six subjects remarked that the
SParQS interface helped them ﬁnd effective queries quickly. More-
over, three subjects remarked that SParQS helped them understand
the suggested queries better. These comments support claims by
Sadikov et al.
the categorization of
query suggestions can save the user’s time to locate a suggestion
that serves his need, and can help the user understand the meaning
of each query suggestion. Whereas, one subject preferred the ﬂat
list to the SParQS saying that the quality of the query suggestion
classiﬁcation was not good enough. Thus, our future work involves
improving the SParQS back-end algorithm.

[10]:

7. CONCLUSIONS

In this paper, we proposed a new method to present query sug-
gestions to the user, which has been designed to help two query
reformulation actions: specialization and parallel movement. Our
prototype, SParQS classiﬁes query suggestions into automatically
generated categories and generates a label for each category. More-
over, SParQS presents some new entities as alternatives to the orig-
inal query, together with their query suggestions classiﬁed in the
same way as the original query’s suggestions. We conducted a
task-based user study to compare SParQS with a traditional ﬂat list
query suggestion interface. Our results show that subjects using
the ﬂat list query suggestion interface and those using the SParQS
interface behaved signiﬁcantly differently even though the set of
query suggestions presented was exactly the same. Moreover, SParQS
users could ﬁnd answers for more entities in Entity Comparison
tasks, experienced greater search satisfaction and success, and the
SParQS interface was perceived to be more helpful than the ﬂat list.
As future work, we would like to improve the accuracy and entity
coverage of the SParQS back-end algorithm. Moreover, as our user
study did not directly measure the effect of presenting alternative
entities to the user (as our baseline ﬂat list interface also presented
alternative entities), we would like to examine this effect in another
user study. While our user experiments clearly showed the advan-
tages of SParQS over a ﬂat list, we should clarify exactly what
features of SParQS contribute to these differences, e.g. the choice
of aspects that apply to multiple entities, and the labels for each as-
pect. In particular, how users experience diagonal movement (See
Section 1) probably deserves an investigation.

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France3973(G)

List

4(G)

List

Q D A A A A A A A A A A A A A A A A A
Q D D D A A A A A A A A A S A
Q A A D S A A A A S D
Q S D A A A A A A
Q D D A A A A A A A A A A A A A A A A
Q S D A A A A A A A A A A A A A
Q S D A A A A
Q S D D A A A A A A
Q S D A A A A A A A A A A A A A A A A A A A A
Q S S S D D D A A A A A A
Q D A A A A A A A A A A A
Q A A A D A A A A A A
Q D S A A A A A A A A A A A A A
Q S D D A A A A A A A A A A A A
Q D A A A A A A A A A A
Q D A A A A A
Q D A A A
Q D A A A A A A A A
Q D A A A A A A A A A A A A A
Q D A A A A A A A

3(G) SParQS

4(G) SParQS

Q D S D A A A A A S D D A A A A A A A A A A A A A A S D A
Q D S A S A A A A A A A A A A A A A A
Q A S D D A A A A A A A A A S A S D A S
Q S A S A S A D A A A A A A A A
Q D A A A A A A A A A
Q A A A A A S A S S A
Q A A A A S A S D S D D D A S A S D D
Q S D A S D D S A S D D A A A A A
Q D A A A A A A A S S D A
Q D A A A A A A A A A A A A A A
Q D A A A A A A A A A A A A A A S D S D
Q D A A A A A A A A A A A
Q S A D A
Q D Q D A A A A A A
Q A A S A
Q A A A S S S S S S Q
Q S S A A S A A A S S
Q S S S D A A A A A A A A A
Q S A D S A A A S D S D A
Q D A A A A A

(a) Behavior patterns of Information Gathering tasks.

15(C)

List

19(C)

List

Q S D A A A A S D A A A A A A A A A A
Q S A S D A Q S S S Q
Q S D S D S A A A A A A A A A
Q S A
Q D S D D A A A A S D Q D S D D A
Q S D S D D D A A A A A A A A S
Q S D A A A S D
Q Q Q Q Q D Q Q D
Q Q D A
Q D S D A A A A A A
Q S A A A S S S S Q S A S Q S A A Q S
Q S D D D Q A A A A A D Q Q A A A A S D A A A
Q D S S A A Q D Q S D A A Q S D A A A
Q Q D D D D Q A S Q A Q S Q D A A
Q Q D D D Q D A A A A A A A A A A A A A
Q S D D Q S A A A Q S A A A A Q S A Q Q S A
Q S D Q D A D Q A D D A
Q Q D D D A A Q D A
Q D S D A D D A A A Q D S D D A A Q S D D A
Q S D D A A A A A A

15(C) SParQS

19(C) SParQS

Q S D S D D A A A A A S A D S A A A D A A S A D A
Q S D Q D A S D A A S D A S
Q A D S A D S A D S A A A A A A
Q S D D D A A A A A A A A A A A A A A A A S D A A A A A A A A A A A A A A A A
Q S D D D Q A A A A A A A A A A A A A A D D D A A A A A Q S D Q D D A A A A A A A
Q S D D D A A A A A S D A D A D S A A A D A A A S
Q S D A S D S D A A A
Q Q D D A A A A A
Q S D A A A A A S S D D A S D D D A S S S D D D A A
Q S D S S D S D D D S D D A A A A A A
Q S Q D A A A A A A A A A A Q D A A A A A A A S
Q S D A D S D A S A S A D S
Q S D S A A A A D S D S A A A A A A D S A A A D D A A A A A
Q S S S D D S A A A A A A A A A A D D A A A A A A A A A A
Q Q D D D A A A A A A A A A A A A A A D D Q D D D D D D A A
Q S D A A A A A A A A A A A Q A D A A A A A A A A A A A
Q S A S A S A S A S A
Q S D S D A A A A A
Q S A S A S A
Q Q S D S D S D S D A A A A A

(b) Behavior patterns of Entity Comparison tasks.

Figure 5: Behavior patterns of each task. A horizontal se-
quence represents the behavior pattern of each subject, which
consist of the transition between query input (Q) in blue, query
suggestion selection (S) in green, document viewing (D) in yel-
low, and answer submission (A) in red.
8. ACKNOWLEDGMENTS

This work was supported in part by KAKENHI (No. 10J04687)
and a Kyoto University GCOE Program entitled “Informatics Edu-
cation and Research for Knowledge-Circulating Society.”

9. REFERENCES
[1] A. Amin, M. Hildebrand, J. van Ossenbruggen, V. Evers, and

L. Hardman. Organizing suggestions in autocompletion
interfaces. In Proc. of ECIR, pages 521–529, 2009.
[2] A. Anagnostopoulos, L. Becchetti, C. Castillo, and

A. Gionis. An optimization framework for query
recommendation. In Proc. of WSDM, pages 161–170, 2010.

[3] P. Anick. Using terminological feedback for web search
reﬁnement: a log-based study. In Proc. of SIGIR, pages
88–95, 2003.

[4] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query

recommendation using query logs in search engines. In
Current Trends in Database Technology-EDBT 2004
Workshops, pages 588–596, 2004.

[5] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and
S. Vigna. The query-ﬂow graph: model and applications. In
Proc. of CIKM, pages 609–618, 2008.

[6] P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. From Dango to
Japanese Cakes: Query Reformulation Models and Patterns.
In Proc. of WI, pages 183–190, 2009.

[7] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li.

Context-aware query suggestion by mining click-through
and session data. In Proc. of KDD, pages 875–883, 2008.
[8] S. Dumais and N. J. Belkin. The TREC interactive tracks:

Putting the user into search. In E. Voorhees and D. Harman,
editors, TREC: Experiment and Evaluation in Information
Retrieval, pages 123–152. MIT Press, 2005.

[9] S. Dumais, E. Cutrell, and H. Chen. Optimizing search by

showing results in context. In Proc. of CHI, pages 277–284,
2001.

[10] J. Guo, X. Cheng, G. Xu, and H. Shen. A Structured

Approach to Query Recommendation With Social
Annotation Data. In Proc. of CIKM, pages 619–628, 2010.

[11] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity

recognition in query. In Proc. of SIGIR, pages 267–274,
2009.

[12] Q. He, D. Jiang, Z. Liao, S. Hoi, K. Chang, E. Lim, and
H. Li. Web query recommendation via sequential query
prediction. In Proc. of ICDE, pages 1443–1454, 2009.

[13] R. Johnson and D. Wichern. Applied multivariate statistical

analysis, 4th ed. Prentice Hall, 1998.

[14] M. Kamvar and S. Baluja. Query suggestions for mobile

search: understanding usage patterns. In Proc. of CHI, pages
1013–1016, 2008.

[15] D. Kelly, A. Cushing, M. Dostert, X. Niu, and K. Gyllstrom.

Effects of popularity and quality on the usage of query
suggestions during information search. In Proc. of CHI,
pages 45–54, 2010.

[16] D. Kelly, K. Gyllstrom, and E. Bailey. A comparison of

query and term suggestion features for interactive searching.
In Proc. of SIGIR, pages 371–378, 2009.

[17] M. Komachi and H. Suzuki. Minimally supervised learning

of semantic knowledge from query logs. In Proc. of IJCNLP,
pages 358–365, 2008.

[18] H. Ma, H. Yang, I. King, and M. Lyu. Learning latent

semantic relations from clickthrough data for query
suggestion. In Proc. of CIKM, pages 709–718, 2008.

[19] Q. Mei, D. Zhou, and K. Church. Query suggestion using

hitting time. In Proc. of CIKM, pages 469–478, 2008.

[20] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of

approximations for maximizing submodular set functions-I.
Mathematical Programming, 14(1):265–294, 1978.
[21] E. Sadikov, J. Madhavan, L. Wang, and A. Halevy.

Clustering query reﬁnements by user intent. In Proc. of
WWW, pages 841–850, 2010.

[22] S. Sekine, K. Sudo, and C. Nobata. Extended named entity
hierarchy. In Proc. of the Third International Conference on
Language Resources and Evaluation (LREC-2002), pages
1818–1824, 2002.

[23] S. Sekine and H. Suzuki. Acquiring ontological knowledge
from query logs. In Proc. of WWW, pages 1223–1224, 2007.

[24] J. Sim and C. Wright. The kappa statistic in reliability

studies: use, interpretation, and sample size requirements.
Physical therapy, 85(3):257–268, 2005.

[25] Y. Song and L. He. Optimal rare query suggestion with

implicit user feedback. In Proc. of WWW, pages 901–910,
2010.

[26] Y. Song, D. Zhou, and L.-w. He. Post-ranking query

suggestion by diversifying search results. In Proc. of SIGIR,
pages 815–824, 2011.

[27] J. Wen, J. Nie, and H. Zhang. Clustering user queries of a

search engine. In Proc. of WWW, pages 162–168, 2001.

[28] R. White and G. Marchionini. Examining the effectiveness of

real-time query expansion. Information Processing &
Management, 43(3):685–704, 2007.

[29] K. Yee, K. Swearingen, K. Li, and M. Hearst. Faceted

metadata for image search and browsing. In Proc. of CHI,
pages 401–408, 2003.

[30] X. Yin and S. Shah. Building taxonomy of web search

intents for name entity queries. In Proc. of WWW, pages
1001–1010, 2010.

WWW 2012 – Session: SearchApril 16–20, 2012, Lyon, France398