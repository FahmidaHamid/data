Recommendations for Commenting on News Stories

Care to Comment?

∗

Erez Shmueli

Dept. of Information Systems

Engineering

Ben-Gurion University, Israel

erezshmu@bgu.ac.il

Amit Kagian
Yahoo! Labs
Haifa, Israel

akagian@yahoo-inc.com

Yehuda Koren

Yahoo! Labs
Haifa, Israel

yehuda@yahoo-inc.com

Ronny Lempel

Yahoo! Labs
Haifa, Israel

rlempel@yahoo-inc.com

ABSTRACT
Many websites provide commenting facilities for users to ex-
press their opinions or sentiments with regards to content
items, such as, videos, news stories, blog posts, etc. Pre-
vious studies have shown that user comments contain valu-
able information that can provide insight on Web documents
and may be utilized for various tasks. This work presents a
model that predicts, for a given user, suitable news stories
for commenting. The model achieves encouraging results
regarding the ability to connect users with stories they are
likely to comment on. This provides grounds for personal-
ized recommendations of stories to users who may want to
take part in their discussion. We combine a content-based
approach with a collaborative-ﬁltering approach (utilizing
users’ co-commenting patterns) in a latent factor modeling
framework. We experiment with several variations of the
model’s loss function in order to adjust it to the problem
domain. We evaluate the results on two datasets and show
that employing co-commenting patterns improves upon us-
ing content features alone, even with as few as two available
comments per story. Finally, we try to incorporate avail-
able social network data into the model. Interestingly, the
social data does not lead to substantial performance gains,
suggesting that the value of social data for this task is quite
negligible.

Categories and Subject Descriptors
J.4 [Computer Applications]: Social and Behavioral Sci-
ences

General Terms
Human Factors
∗

Work done while interning at Yahoo! Labs, Haifa.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

 

Thank You! 
Thank You!
Your comment has been received and should be available shortly.
Your comment has been received and should be available shortly.
A preview is shown here.
A preview is shown here. 

Amit a second ago. 
Amit a second ago.
I'm still waiting to find intelligent life on this planet... 
I'm still waiting to find intelligent life on this planet...

 

You may care to comment on one of the following stories as well: 
You may care to comment on one of the following stories as well:

Volcano blast unleashes 
Volcano blast unleashes
eerie scenes 
eerie scenes

D-Day, before and after, 
D-Day, before and after, 
in color 
in color

Electronic Entertainment 
Electronic Entertainment 
Expo gadgets 
Expo gadgets

Figure 1: Demonstration of a potential application.
After a user submits a comment to a story, the ap-
plication recommends, along with the conﬁrmation
message, other stories to comment on.

Keywords
Personalization, recommendation system, user generated con-
tent, comment recommendation, latent factor models, col-
laborative ﬁltering

1.

INTRODUCTION

Much of today’s media is consumed online, and media
Web sites – whether online outlets of media companies with
traditional forms of distribution (newspapers, TV stations,
etc.) or portals of Web media companies – vie for the at-
tention of consumers. New engaging user experiences enable
sites to increase their user base as well as understand their
users better, making them more attractive to advertisers.

A common feature of many present-day media sites is their
attempt to enrich user engagement by soliciting and sharing
user generated content (UGC). One popular form of UGC-
centric engagement stems from comments on content that
media sites solicit from users. Users are empowered to write
comments and are able to rate others’ comments, reply to
comments (thereby generating discussion threads), etc.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France429This work focuses on news sites, and aims to present to
each user a personalized ranked list of stories (that the user
has yet to consume) that may engage the user to the point
of commenting on them. An illustration of the planned ex-
perience is presented in Figure 1. This is an extreme form
of content recommendation, as we go beyond engaging the
user to click on or to consume information - we hope the user
will be passionate enough about the presented stories to ac-
tually generate content. Furthermore, there is a time-critical
dimension to our task – stories should be recommended to
the users soon after they are published, as freshness is a
major ingredient in a story’s attractiveness.

Technically, we tap each user’s propensity to comment
on stories of certain topics, as well as collaborative ﬁltering
(CF) signals that rely on co-commenting patterns of users
(tendency of diﬀerent users to comment on the same sto-
ries). Note that when trying to recommend fresh stories, a
trade-oﬀ emerges. Fresh stories have few comments, leading
to lower contribution of the co-commenting signal. This is a
form of the item cold-start problem in recommendation sys-
tems. On the other hand, waiting for more commenters to
comment on the story helps in identifying additional users
who would ﬁnd the story engaging, but in the meantime the
story will have “aged”.

Another signal we tap in our experiments is social data.
One of our datasets sports a social network between its users,
prompting us to investigate whether and to what extent do
stories, on which a user’s friends have commented, engage
the user to the point of commenting.

We apply a memory based recommendation algorithm to
the signals above, as well as several ﬂavors of a latent factor
model (LFM). The LFM ﬂavors deviate from each other by
the loss function applied while learning them. Our exper-
iments, on two diﬀerent datasets with thousands of stories
and users, achieve encouraging results.

The contributions of this paper are along two axes. First,
with respect to the application domain, we measure the rela-
tive strengths of three signals (textual tags, co-commenters,
and friends) as predictors of a person’s tendency to comment
on stories. Our ﬁndings show that:

• Textual tags provide a signal that is already surpassed
by roughly 5 co-commenters in the tested datasets.
Furthermore, combining tags and co-commenter infor-
mation improves on what each signal can achieve alone.
• We study the tradeoﬀ between recommendation accu-
racy and the number of comments available per story.
• The contribution of social signals to commenting ac-
tivity prediction is lower than either of the other signal
types.

Second, with respect to the art of recommendation systems,
we ﬁnd that:

• One can improve the accuracy of latent factor models
(LFMs) by tailoring the loss function to the speciﬁc
application at hand rather than using the standard
squared error loss function.

• In cases where uniformly unattractive items can be
easily identiﬁed, i.e. stories on which almost no users
will comment, the AUC metric may fail to measure the
personalization capabilities of competing recommenda-
tion schemes. We therefore introduce a novel variant

metric, called stratiﬁed AUC, that measures the accu-
racy of ranking items in separate bins that correspond
to the items’ global popularity. This isolates the uni-
formly unattractive items and prevents them obscuring
the quality of the evaluated recommendation scheme.

The rest of this paper is organized as follows. Section 2
surveys related work. Section 3 presents the LFM and mem-
ory based recommendation algorithms we evaluated. Sec-
tion 4 describes the two datasets in our experiments, and
Section 5 reports our results. We conclude in Section 6.

2. RELATED WORK

Since the appearance of Web 2.0 and, with it, user gener-
ated comments, various studies explored diﬀerent aspects of
user comments, showing that comments can provide many
insights about Web documents. Mishne and Glance [16]
presented a study of weblog comments and their relation to
blog posts, for example, exploring the relation between the
weblog popularity and users’ commenting patterns. Siers-
dorfer at al. [26] studied user comments on YouTube videos
and gave an analysis of the dependencies between comments,
views, comment ratings and topic categories along with a
predictor of comments’ ratings. Since user comments often
express apparent feelings of users, the study of user com-
ments has been also associated with sentiment analysis (see
survey by Pang and Lee [17]). For example, a study by
Pavlou and Dimoka [19] that demonstrated the role of user
comments in building sellers’ benevolence and credibility in
online marketplaces.

Other studies utilized user comments for the completion of
various tasks: user comments on blog posts were utilized for
automatic blog summarization [9] and for clustering of blog
documents [11]. Yee et al. [31] demonstrated the potential
of Youtube user comments to annotate videos by incorpo-
rating comments into the search index, which yielded up to
a 15% increase in search accuracy. Studying the content
sharing website Digg, Rangwala and Jamali [21] deﬁned a
co-participation network between users based on Digg’s com-
ment information and used this data to predict the popu-
larity of online content linked at Digg. A recent study by
Chen et al. [3] focused on user reputation in a comment rat-
ing environment and showed that the quality of a comment
judged editorially is almost uncorrelated with the ratings
that it receives, but can be predicted using standard text
features.

Several past studies focused on user comments on news
articles. Li et al.
[12] employed user comments for dy-
namically suggesting related stories to a given news article,
showing that suggestions improve when the content of user
comment threads is considered. A recent study by Park
et al.
[18] utilized user comments in order to identify the
political orientation of news articles by uncovering user sen-
timents expressed in their comments. Other studies [28, 29]
explored the distributions of comments on news from various
news agents and tried to predict the comments’ volume for
news stories. Schuth et al. [25] examined relations between
news comments by extracting discussion structures (’replies-
to’ relations) from ﬂat comment threads. Hsu et al. [8] pre-
sented a machine learning approach for ranking comments
based on the expressed preferences of a social web commu-
nity, which can be used to promote high-quality comments
and ﬁlter out low-quality ones. A recent study by Agar-

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France430wal et al.
[2] added personalization to comment ranking
and used a latent factor model to rank comments accord-
ing to personalized preferences of speciﬁc users. The work
presented in this paper adds to the line of user generated
comments research by describing a model that predicts, for
a given user, the most probable news stories for commenting
and thus, provides personalized recommendations of stories
to comment on.

Our work lies within the ﬁeld of recommender systems
[23]. Broadly speaking, recommender systems are based on
two diﬀerent strategies. The content ﬁltering approach cre-
ates a proﬁle for each user or product to characterize its na-
ture. As an example, a news article proﬁle may include its
publisher, author, subject tokens, categories, tags, textual
attributes, etc. Similarly systems may employ user proﬁles
including demographic information or answers to a suitable
questionnaire. The resulting proﬁles allow programs to as-
sociate users with matching products. Much more details
can be found in Ricci et al. [23].

An alternative to content ﬁltering relies only on past user
behavior—e.g., previous transactions or product ratings—
without requiring the creation of explicit proﬁles. This ap-
proach is known as Collaborative Filtering (CF), a term
coined by the developers of the ﬁrst recommender system
- Tapestry [7]. CF analyzes relationships between users and
interdependencies among products, in order to identify new
user-item associations. A major appeal of CF is that it is
domain free, yet it can address aspects of the data that are
often elusive and diﬃcult to proﬁle using content ﬁltering.

A common division of CF is between a class of “memory-
based” methods, and a class of “model-based” methods [1].
Memory-based methods are essentially heuristics that gen-
erate recommendations by directly referring to the raw user
feedback data, or to values immediately derived from it. A
known example is the item-item recommendation method
[14, 24]. Model-based methods initially construct a compact
model for representing the inherent structure essential for re-
lating user with items. Consequently, recommendations are
made by referring to the created model. The models often
follow a latent factor representation (or, LFM), such as the
widely used matrix factorization method [10]. LFM tries to
explain the user behavior by characterizing both items and
users on factors automatically inferred from the patterns of
user-item interactions.

While generally being more accurate than content based
techniques, CF suﬀers from what is called the “cold start”
problem, due to the inability to address products or users
new to the system. This often leads to favoring hybrid meth-
ods [1] that combine content and collaborative ﬁltering to-
gether. The methods proposed in this paper are indeed such
a hybrid.

Most works in the recommenders ﬁeld deal with entertain-
ment and media product. More relevant to us are the works
dealing with recommending textual items. Such textual
items, like news articles, are characterized by high volatil-
ity and churn rate, while oﬀering an opportunity of content
analysis of their text. One such work [4] describes Google
news recommendation engine, where the authors use a blend
of three separate CF methods in a highly scalable fashion.
Moving closer to the UGC domain, several recent works [5,
13] addressed question recommendation in community ques-
tion answering sites.

Table 1: Summary of Notations

T (s) The set of textual tags associated with story s
C(s) The set of users that commented on story s
U
S+
u
−
S
u
Su
s
u
c
t
rus
¯x

The set of all users in the data
The set of stories a user u commented on
The set of stories a user u did not commented on
A set of positive-negative story pairs for user u
Index letter for a story
Index letter for a user receiving recommendations
Index letter for a user as a commenter
Index letter for a textual tag
An aﬃnity score for user u and story s
Vector representation of x in the K-dimensional
latent space (where x can be a user, story, tag)

3. RECOMMENDATION METHODS

We begin by presenting some notations used throughout
this section. For a given story s, we denote the set of textual
tags associated with it by T (s). Similarly, we denote the set
of users that commented on s by C(s). Let U denote the set
of all users in the data. For each user u ∈ U , let S+
u be the
set of stories that u commented on, also referred to as the
−
positive examples of u. Similarly, let S
u be the set of stories
that u did not comment on, also called the negative examples
of u. As part of our LFM learning procedure (section 3.2)
we train our models on the same number of positive and
negative examples for each user u. To this end, we pair each
positive example s+ ∈ S+
u with a sampled negative example
− ∈ S
−
s
u . Generally, negative examples are drawn uniformly
−
from S
u , unless otherwise stated. We denote by Su the
set of positive-negative story pairs (s+, s
) over which the
LFMs are trained with respect to a user u. As for the LFM
parameters, we use a bar notation to distinguish vectors
in the latent space from the scalars they represent. For
example, ¯u ∈ R
K is the vector representation of user u and
¯s ∈ R
K is the vector representation of story s. Table 1
summarizes these notations for easy referencing.

−

We treat the problem of commenting recommendation as
a ranking problem. Given a user u, the goal is to rank the
test stories so that if story s1 is ranked higher than s2 then
u is more likely to comment on s1 than on s2. The ranking
of test stories is accomplished by using a prediction model
that given a user u and a story s, generates a score ru,s
reﬂecting how likely is u to comment on s. We addressed this
problem with two approaches, a memory based approach
and a more complex latent factor model in several variations,
some of which are novel with respect to recommendation
system usage. The rest of this section describes the methods
that were implemented in the two approaches.
3.1 Memory Based Approach (MB)

The memory based approach directly leverages the no-
tion of co-occurrence. This is done by calculating a co-
occurrence similarity measure for a given user and all story
features (textual tags and given commenters) and averaging
this measure over all features of a given story.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France431Thus, the memory based formula for ru,s is given by

(cid:2)

ru,s =

+

1

|{t ∈ T (s)}|

t∈T (s)
|{c ∈ C(s), c (cid:5)= u}|

1

nu,t√
(cid:2)
nunt

c∈C(s),c(cid:3)=u

nu,c√
nunc

(1)

(cid:3)

K(cid:2)

(cid:4)

The variable nu,t is the number of stories with tag t that
user u had commented on, nu is the total number of stories
that u had commented on and nt is the total number of sto-
ries with tag t. Similarly, nu,c is the number of stories which
both users u and c have commented on. Implementation-
wise, we build an inverted index [15] over training period
stories, along with their tags and commenting users. This
results in posting lists per tag and commenter, whose post-
ing elements contain (and are sorted by) story ID. The index
allows us to quickly compute co-occurrence (intersection)
counts on the ﬂy, by running two-term conjunctive queries
against it.
3.2 Latent Factor Model

K and ¯s ∈ R

The basic idea behind latent factor models is to represent
each user u and story s in a K-dimensional latent space, so
that, if ¯u ∈ R
K are the representations of u and
s in the latent space, then the score ru,s is calculated by the
inner product of the representation vectors, plus an additive
user bias

ru,s = (cid:6)¯u, ¯s(cid:7) + bu =

¯uk ¯sk

(2)
where bu ∈ R is a user bias parameter. Note that while the
user biases have no eﬀect on the ranking of stories for a ﬁxed
user, we still ﬁnd them improving results as their inclusion
leads to a more accurate training of the latent factors.

+ bu

k=1

In many recommender system settings, items are directly
modeled based on user interactions with them (during the
training period). Our setting is diﬀerent because of the
ephemeral nature of news stories. Hence, we assume no user
interactions are available for test stories (see also the related
design of the train-test split in Section 4). Therefore latent
space representation for stories (¯s) would not be a distinct
parameter of the model. Rather, we assign latent space pa-
rameters to the story features (textual tags and commenters)
and construct the stories’ representations through their fea-
tures. This is feasible since the same features do appear
with other stories during training. Formally, each textual
tag t and commenting user c are represented by factor vec-
tors, ¯t ∈ R
K , respectively. The representation
of a story s is given by averaging the representations of its
features

K and ¯c ∈ R

(cid:2)

¯s =

1

|{t ∈ T (s)}|

¯t +

1

|{c ∈ C(s)}|

¯c

(3)

c∈C(s)

Note that we chose to use two diﬀerent latent representations
for each user: ¯u as a user receiving a recommendation (see
equation (2)) and ¯c as a commenter (being a feature of a
story). This is done in order to allow the model to address
the diﬀerent roles a user assumes. In our experiments, we
used a value of K = 10 for the latent space dimensionality.
Other values we experimented with (20,50), did not produce
any observable gain in performance.

(cid:2)

t∈T (s)

(cid:2)

u∈U
−

The parameters of the latent factor models were learned
by employing a stochastic gradient descent algorithm with
early stopping (see, e.g., [27]). The algorithm attempts to
minimize the cost function

(cid:2)

loss(u, s

+

, s

−

)

(4)

cost =

{s+,s−}∈Su

where loss(u, s+, s
the rest of this subsection.

) is one of the loss function described in

The training procedure is an iterative process and a typi-
cal number of iterations is 20. At each iteration, we iterate
over all users u ∈ U . For each user, we train the model over
all positive-negative story pairs in Su. Recall that the story
pairs in Su include all the positive examples in S+
u and for
each s+ ∈ S+
− ∈ S
−
u . Nega-
tive examples are sampled uniformly except for the case of
the last loss function (see ’Rank Loss’ below).

u a sampled negative example s

Following is a high level pseudocode description of the

training procedure:

while validation performance is improving do

for all u ∈ U do
for all {s+, s

−} ∈ Su do

calculate the gradient ∇(loss(u, s+, s
change model parameters by −η·∇(loss(u, s+, s

))

−

−

))

end for

end for
end while

We used a learning rate of η = 0.1. The model’s latent fac-
tor parameters are regularized by penalizing their squared
magnitude with a regularization coeﬃcient λ = 0.1 (also
known as Tikhonov regularization). Following is a descrip-
tion of the various loss functions used in our experiments.
Squared Error (SE)
The most common loss function in recommendation systems
latent factor models is the squared error (e.g.
[10]). Since
recommendation systems commonly try to predict a user-
item rating (e.g., movie ratings), the SE loss function aims
at minimizing the distance of predictions from target ratings;
in our case it aims at getting the score of positive examples
close to 1 and of negative examples close to 0

se loss(u, s

−

+

, s

) = (1 − ru,s+ )

2

+ (0 − ru,s− )

2

(5)

Classiﬁcation Error (CE)
While observing the SE loss function, one may argue that
it is unjustiﬁed to penalize positive examples with scores
higher than 1. The same holds for negative examples with
scores lower than 0. The ’Classiﬁcation Error’ deals with
that by using a hinge loss that better meshes with misclas-
siﬁcation. Thus, the loss term is

ce loss(u, s

, s

+

−

) =|1 − ru,s+|+ + |ru,s−|+

(6)

where |x|+ = x if x ≥ 0 and 0 otherwise.
Bayesian Personalized Ranking (BPR)
Rendle et al. [22] suggested a loss function that is designed
for ranking problems. The method treats pairs consisting of
a positive and a negative example. The goal is to score the

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France432positive example higher than the negative example’s score.
The loss term is

bpr loss(u, s

−

) =−log (σ(ru,s+ − ru,s− ))

+

, s

(7)

where σ(x) is the sigmoid function

1

1+e−x .

− ∈ S

Rank Loss (RL)
Bengio et al.
[30] suggested a loss function that utilizes a
diﬀerent sampling method for negative examples, which we
adopt here for recommendation purposes. For each posi-
tive example s+ ∈ S+
u , negative examples are drawn with
−
replacement until a negative example s
u that vio-
lates the desired ranking is encountered. A negative example
− ∈ S
−
s
u is considered violating, with respect to a positive
example s+ ∈ S+
u , if its score is within a margin of 1 from
the positive example’s score or higher, that is, if it satisﬁes
ru,s+ (cid:2) 1 + ru,s− . The model is then updated based on such
violating pair of examples. No update is performed if no vio-
lating example could be found after |S
u | draws. The number
−
of non-violating negative examples drawn while sampling is
used to estimate the total number of negative examples that
violate the desired ranking for a given positive example. We
denote this as rank in the following formulation

rank =

total number of negative examples
number of negative examples drawn

We used a binary rankloss function after other suggested
variations [30] we experimented with did not improve per-
formance.

rankloss(rank) = 1 if rank > 1, 0 otherwise

dataset

type

Newsvine

Yahoo!

Original

Train
Test

Original

Train
Test

stories
111,893
44,573
6,232
25,527
15,236
2,815

users
221,809

6,173
6,173

320,425

8,015
8,015

comments

3,009,247
485,247
177,304
1,664,917
355,762
96,947

Table 2: Data statistics for the Newsvine and Yahoo!
datasets before and after splitting and preprocess-
ing.

 
s
e
i
r
o
t
s
 
f
o
#

 

14000
12000
10000
8000
6000
4000
2000
0

# of comments 

Figure 2: The distribution of stories by their number
of comments in the Newsvine dataset (after prepro-
cessing).

Then the loss function for user u and positive-negative

−} ∈ Su becomes
story pair {s+, s
) = rankloss(rank)·(1−ru,s+ +ru,s− ) (8)
−
, s
rl loss(u, s

+

4. DATASETS & EXPERIMENTAL SETUP
In order to evaluate our methods, we examined two dif-
ferent datasets of news articles, comments and users. We
proceed to describe how each of the datasets was obtained
and preprocessed.

The ﬁrst dataset was crawled from the Newsvine news
site1. Newsvine is a community-powered, collaborative jour-
nalism news website, owned by msnbc.com. Users can write
articles, seed links to external content, and discuss news
items submitted by both users and professional journalists.
We have chosen to crawl the Newsvine site, among dozens of
other available news sites, since: (1) Newsvine is relatively
easy to crawl due to the static HTML nature of its content
pages; and (2) its registered users constitute a social network
that is publicly visible. Our crawler fetched all news articles
published from May 18, 2011 till September 27, 2011. Each
fetched article was parsed in order to extract its title, con-
tent, editorial tags, time of publication and set of associated
comments. From each comment we extracted its commenter
ID. Finally, the set of friend IDs of each commenter was ob-
tained. As shown in Table 2, the Newsvine dataset contains
111,893 articles with 3,009,247 comments and 221,809 users.
Furthermore, its social network contains 106,705 (symmet-
ric) friendship relations, i.e. undirected friendship edges.
1www.newsvine.com

 
s
e
i
r
o
t
s
 
f
o
#

 

3500
3000
2500
2000
1500
1000
500
0

# of comments 

Figure 3: The distribution of stories by their num-
ber of comments in the Yahoo! dataset (after pre-
processing).

The second dataset was a set of articles obtained from
Yahoo! News2. Yahoo! News is an Internet-based news ag-
gregator provided by Yahoo!. Articles in Yahoo! News come
from news services, namely Associated Press (AP), Reuters,
Fox News, CNN.com, BBC News, and others. Similarly to
Newsvine, Yahoo! News also allows users to comment on
news articles. Our set of articles was sampled so as to con-
tain articles with varying number of comments. As shown in
Table 2, this dataset contains 25,527 articles with 1,664,917
comments and 320,425 users.

Since the ﬁrst dataset was crawled from the Newsvine
website we could not obtain any click data that can validate
2news.yahoo.com

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France433which uncommented stories were actually viewed by a user.
In order to compare the two datasets properly we did not
use any click data on the Yahoo! dataset as well.

Each article (in both datasets) was later represented as a
set of tags and a list of commenters (ordered by their com-
menting time on the article). The set of tags contained both
editorial tags (only available for the Newsvine dataset) and
named entities which were extracted from the article’s title
and content. In order to keep the article’s representation as
compact as possible, we chose to only extract named entities,
such as names of people and places, and not its entire set
of terms. In order to identify named entities, we applied a
very simple heuristic that looked for sequences of words be-
ginning with a capital letter, except for sequences of length
one appearing at the beginning of a sentence. The results of
this simple yet fast heuristic suﬃced for our needs.

Then, each dataset was split into a training set, containing
60% of the articles, and a test set, containing the remaining
40%. The articles were split according to the chronological
order of their publish dates, where older articles constituted
the training set and newer ones the test set.

Finally, users and stories that did not meet certain condi-
tions were ﬁltered out, as hereby described. Users with less
than ten comments in the training set were removed, being
an inappropriate target of recommendation. Users without
comments in the test set were also removed, as they cannot
be evaluated properly. In addition, we ﬁltered out from the
training set stories without comments, and from the test set
stories with less than ﬁve comments. We required stories
in the test set to have at least ﬁve comments due to the
oﬄine nature of our evaluation, as will become clearer in
Section 5. Table 2 summarizes the number of stories, users
and comments for the training and test sets of each dataset.
Figures 2 and 3 illustrate the distribution of stories by their
number of comments in the Newsvine and Yahoo! datasets,
respectively, after the preprocessing phase.

5. RESULTS

5.1 Evaluation: Predicting Comments

We trained ﬁve diﬀerent models over the training set data:
the Memory Based model (MB) and Latent Factor Models
with four error function variants (see Section 3). Once all
models were trained, they were used to generate, for each
user, a score for all stories in the test set.3 Then, test stories
were ranked for each user according to the assigned scores.
We used the Area Under ROC Curve (AUC) measurement in
order to evaluate the rankings. In general, the AUC value is
equivalent to the probability that a randomly chosen positive
example is ranked higher than a randomly chosen negative
one [6]. Eventually, we averaged the achieved AUC values
over all users. Table 3 shows the obtained mean AUC values
for both datasets. Overall, all methods perform quite well on
the Newsvine dataset, with a minor advantage to BPR with
87% AUC. On the Yahoo! dataset, performance diﬀerences
are more signiﬁcant; RL is the best performing method with
72.2% AUC while BPR is lagging behind signiﬁcantly with
AUC=64.3%.

3Stories in which the user was among the ﬁrst ﬁve com-
menters were excluded from the test for that user, since
the ﬁrst 5 commenters per story are treated as known com-
menters and comprise the co-commenting features of a story.

Method Newsvine AUC Yahoo! AUC

MB
SE
CE
BPR
RL

0.845
0.867
0.859
0.870
0.863

0.661
0.699
0.696
0.643
0.722

Table 3: Obtained AUC values for the various meth-
ods over the Newsvine and Yahoo! datasets.

Stratiﬁed AUC
The AUC metric, while common in the literature, is suscep-
tible to easily-forecasted elements. Whenever a considerable
portion of the ranked elements are easily classiﬁed as posi-
tive or negative, the AUC value increases and its power of
discrimination between better and worse methods deterio-
rates.

In our case, many stories are predictably unpopular, i.e. it
is easy to identify many stories that are unlikely to be com-
mented on by any user. We call these easy negatives. As our
emphasis is on evaluating the methods by their ability to
personalize recommendations, we would like to deemphasize
the masking eﬀect that easy negatives have on the AUC met-
ric. In a broader sense, we would like to promote methods
that are good at personalization, and demote methods that
are merely able to predict the universal popularity of stories.
To demonstrate that the AUC metric is biased toward pop-
ularity prediction, we measured the success of a popularity
oracle that ranks (in hindsight) the test stories, uniformly
for all users, by the number of comments they eventually
received. Obviously this oracle does no personalization at
all, as it recommends to all users the same order of stories4.
We also reiterate that it is an oﬄine algorithm that “learns”
from the test data and thus naturally overﬁts. Indeed, the
ranking produced by the ”popularity oracle” achieved very
high AUC values, with 89.7% AUC on Newsvine and 85.8%
AUC on Yahoo!, handily beating all other methods (com-
pare with the values in Table 3).

In order to mitigate the inﬂuence of the popularity bias
in our evaluation and to refocus it on personalization, we
propose a novel stratiﬁed-AUC performance measurement
(sAUC). The sAUC metric calculates the probability of a
random positive example to be ranked higher than a random
negative example of (approximately) equal popularity. More
speciﬁcally, sAUC partitions the test stories into popularity
bins (see Figures 2 and 3), and computes AUC separately
within each bin. These bin-speciﬁc AUC values are then
averaged proportionally to the number of positive examples
in each bin.

In order to suit the sAUC measure better, we retrained
our latent factor models with an emphasis on discriminat-
ing between positive and negative stories within the same
popularity bin. To this end, when training the model on a
positive example and a negative example (see Section 3), we
sampled the latter from the same popularity bin as the pos-
itive one. Table 4 displays sAUC values for both datasets.
As evident, the ”popularity oracle” loses its power when eval-

4The popularity oracle is the optimal user-independent
ranking of test stories, i.e. no other such ranking of sto-
ries achieves a better AUC.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France434Method

Newsvine sAUC Yahoo! sAUC

MB
SE
CE
BPR
RL

”popularity oracle”

0.682
0.700
0.702
0.713
0.700
0.582

0.639
0.686
0.690
0.660
0.685
0.565

Table 4: Obtained stratiﬁed-AUC values for the
various methods over the Newsvine and Yahoo!
datasets. In contrast to the classic AUC measure-
ment, here the “popularity oracle” ranking achieves
the poorest results.

uated with sAUC, while the other methods still achieve fair
performance, with BPR attaining the best performance of
71.3% sAUC on Newsvine and CE reaching 69% sAUC on
Yahoo!. It is interesting to note that while all the latent fac-
tor models outperform the memory based approach, there
is still no clear winning method for both datasets. Namely,
while BPR achieves the best AUC and sAUC on Newsvine,
its performance on the Yahoo! dataset is inferior to the
other LFMs. This promotes a possible invstigation of the
diﬀerences between the two datasets and a research of bet-
ter adjusted LFM loss functions in future research.

5.2 Content vs. Collaborative Filtering

As described in section 3, our methods utilize two types
of data signals, textual tags and co-commenting patterns,
as predictors of a person’s tendency to comment on stories.
In this section we attempt to quantify the impact of each
signal alone, and of their combination on the recommenda-
tion accuracy. In order to achieve this goal, we created three
variations for each one of our prediction models:

Tags. This variation takes into account the textual tags sig-
nal alone while completely ignoring the co-commenting
signal.

Co-Commenting. This variation takes into account the
co-commenting signal solely while completely ignoring
the textual tags signal. Furthermore, we used this vari-
ation in order to investigate the tradeoﬀ between rec-
ommendation accuracy and the number of comments
available per story. In order to accomplish this task we
tried diﬀerent values (one to ﬁve) of given comments
for test stories.

 

C
U
A

0.880

0.860

0.840

0.820

0.800

0.780

0.760

Tags
Co-Commenting
Both

2 

1 
5 
Number of given commenters 

4 

3 

Figure 4: Evaluating the AUC metric on a varying
number of given commenters. The LFM model is
trained and tested using the BPR loss function over
the Newsvine dataset.

 

C
U
A

0.750

0.700

0.650

0.600

0.550

0.500

Tags
Co-Commenting
Both

2 

1 
5 
Number of given commenters 

4 

3 

Figure 5: Evaluating the AUC metric on a varying
number of given commenters. The LFM model is
trained and tested using the RL loss function over
the Yahoo! dataset.

 

C
U
A
s

0.720

0.700

0.680

0.660

0.640

0.620

0.600

Tags
Co-Commenting
Both

2

1
5
Number of given commenters 

3

4

Both. Taking into account both of the co-commenting and
textual tags signals. Similarly to the Co-Commenting
variation, we tried diﬀerent values (one to ﬁve) of given
comments for test stories.

Figure 6: Evaluating the stratiﬁed-AUC metric on
a varying number of given commenters. The LFM
model is trained and tested using the BPR loss func-
tion over the Newsvine dataset.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France435 

C
U
A
s

0.700
0.680
0.660
0.640
0.620
0.600
0.580
0.560
0.540

Tags
Co-Commenting
Both

2 

1 
5 
Number of given commenters 

4 

3 

Figure 7: Evaluating the stratiﬁed-AUC metric on
a varying number of given commenters. The LFM
model is trained and tested using the CE loss func-
tion over the Yahoo! dataset.

Figures 4-7 show the results of applying the three diﬀerent
variations over the prediction models that achieved the best
results in the previous subsection. More speciﬁcally, Figures
4 and 6 show results for BPR when evaluating the AUC and
sAUC measures on the Newsvine dataset. Figure 5 shows
results for RL when evaluating the AUC measure on the Ya-
hoo! dataset. Finally, Figure 7 shows results for CE when
evaluating the sAUC measure on the Yahoo! dataset. A
careful examination of the four ﬁgures implies that when us-
ing 5 given commenters, the co-commenting signal provides
a signal that is equivalent or higher than the textual tags
signal. Interestingly, this reaﬃrms the ﬁndings by Pil´aszy
and Tikk [20] in the very diﬀerent domain of movies. Fur-
thermore, combining tags and co-commenting information
improves on what each signal achieves alone, even for as
little as two given commenters.
5.3 Social Data

As mentioned in Section 4, the Newsvine site has a ded-
icated social network among its users. Furthermore, the
Newsvine friendship relations are publicly crawlable. We
thus examined whether tapping the co-commenting patterns
of a user’s friends can help improve our personalized recom-
mendation for the user. We created a subset of the Newsvine
dataset that includes only users with at least one friend (and
stories commented by such users, etc.). Data statistics on
the resulting dataset are shown in Table 5.

type
Train
Test

stories users
2,747
40,823
5,207
2,747

comments

friendships

272,357
177,304

36,284
36,284

Table 5: Social data statistics for the Newsvine
dataset.

We experimented with multiple approaches for represent-
ing the social data in the memory based and LFM models.
In all tested approaches, adding social data had a negligi-
ble eﬀect on our ability to predict users’ comments. We
therefore only report below the results of the best variation
among those tested.

The approach that obtained the maximal recommenda-
tion accuracy was an LFM based on modifying each user
representation to include information about his friends. We

denote the friends of user u by f riends(u), and represent
each friend f by latent factor vector ¯f ∈ R
K . The vector
representation of u, ¯u (see Section 3.2) is now replaced by
(cid:2)
the vector ¯uF

¯uF = ¯u +

1

|f riends(u)|

¯f

(9)

f∈f riends(u)

We also tried an alternative approach of enhancing the
story representation based on friendship relationships, which
resulted in inferior performance. Table 6 shows the obtained
results when using the tags, co-commenting and social sig-
nals, compared to using only the tags and co-commenting
signals. The results are reported for the BPR loss function,
which achieved the best results for the Newsvine dataset (in
accordance with the previous subsection). As can be seen
from the results, the improvement in prediction due to the
addition of the social signal is negligible: 0.1% when using
the AUC metric and 0.4% when using the stratiﬁed-AUC
metric.

Signals

Tags & Co-Commenting

Tags & Co-Commenting & Social

AUC sAUC
0.822
0.774
0.777
0.823

Table 6: Performance eﬀect of adding the social sig-
nal on the Newsvine dataset.

Obviously, our inability to extract strong signals from the
social data is not a proof that such signals does not exist.
Furthermore, the above experiment was done on a single
dataset. However, with those grains of salt in mind, we
posit the following:

• Commenting activity indicates very strong engagement
and passion about a story, and friendship (as reﬂected
in social networks) is not a strong indication of the
friends sharing that level of topical passion.

• Aﬃnity trumps friendship - collaborative ﬁltering tech-
niques that (implicitly) identify like-minded users from
the population at large, are better at representing one’s
passions than one’s (explicit) selection of friends. Note
that one’s selection of friends is not task-oriented, i.e.
people do not mark online friendships for information
ﬁltering purposes. Thus, the expressed friendships are
not tuned to compete against task-oriented CF algo-
rithms. Still, when unwillingly competing in the infor-
mation ﬁltering race, they lose.

• Related to the previous point, it could very well be
that the major attraction of a social networks in a
site like Newsvine lies in one’s ability to keep informed
of friends’ activities and interests, without necessar-
ily sharing those same interests (or at least not at the
same level of passion). In other words, one may appre-
ciate knowing that a friend has commented on a story,
even if that story is of low personal interest.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France4366. CONCLUSIONS & FUTURE WORK

7. ACKNOWLEDGMENTS

In this study, we propose a method for personalized recom-
mendation of news stories for commenting. This is achieved
by predicting the stories that are most probable for com-
menting by a given user. We combine content-analysis (uti-
lizing stories textual tags) together with collaborative ﬁlter-
ing (utilizing users’ co-commenting patterns) in a memory
based and latent factor model (LFM) approaches.

With respect to the application domain, it is shown that
when using 5 given commenters, co-commenting provides a
signal that is equivalent or higher than the textual tags sig-
nal. Furthermore, combining both signals improves on what
each signal achieves alone. Moreover, prediction accuracy
grows with the number of comments available per story. Fi-
nally, our attempts to combine a third signal, that of social
relationships among users, did not result in an observable
gain in prediction accuracy.

With respect to the art of recommendation systems, we
exemplify how one can improve the accuracy of latent fac-
tor models by tailoring the loss function to the speciﬁc ap-
plication at hand, rather than using the standard squared
error loss function. Moreover, we demonstrate the bias of
the AUC metric in cases where one can easily identity items
that are not suitable for most users. Therefore, we suggest
a variation of the AUC metric, to which we call stratiﬁed
AUC. The stratiﬁed-AUC metric compares the ranking of
items in bins corresponding to their global popularity.

We plan to deploy our model on live traﬃc data, which
will allow a more realistic evaluation of the methods de-
scribed in this paper. Such an online setting will bring us
new challenges and opportunities. It will necessitate the re-
placement of the static train-test split by a more realistic
incremental evaluation, where the amount of information
per story gradually increases over time. Such an evalua-
tion will incorporate the temporal popularity of stories and
the aging of old stories. No less important is the oppor-
tunity to better identify stories regarded as “negative” for
the user. As previously explained, we currently draw nega-
tive examples from those stories on which the user has not
commented. In the future we will be exposed to additional
signals allowing us a more reﬁned identiﬁcation of negative
stories. We would like to discard stories which were never
presented to the user (e.g., she was inactive during their
serving time). Then, we should distinguish between stories
presented to user but not viewed (clicked) by her and stories
viewed by the user without initiating commenting activity.
Such considerations impact not only the evaluation proce-
dure, but also the modeling methodology. Another aspect
that was not addressed in this work is the discrimination be-
tween “types” of commenters; while many users have a rich
commenting history, a large portion of the users rarely com-
ment and their commenting history is very poor. A future
research may want to examine the possibility of modeling
users diﬀerently according to their commenting habits. In
additions, our preliminary attempts to employ friendship
connections to the recommendation model promote further
investigation of the beneﬁt of using social signals vs. users’
behavioral similarity.

We would like to acknowledge Michal Aharon, Yoad Lustig
and Yoelle Maarek for their contribution to this work in
many fruitful discussions.

8. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the next

generation of recommender systems: A survey of the
state-of-the-art and possible extensions. IEEE Trans.
Knowl. Data Eng., 17(6):734–749, 2005.

[2] D. Agarwal, B.-C. Chen, and B. Pang. Personalized

recommendation of user comments via factor models.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
571–582, July 2011.

[3] B.-C. Chen, J. Guo, B. Tseng, and J. Yang. User
reputation in a comment rating environment. In
Proceedings of the 17th international conference on
Knowledge discovery and data mining (KDD 2011),
pages 159–167, August 2011.

[4] A. S. Das, M. Datar, A. Garg, and S. Rajaram.

Google news personalization: scalable online
collaborative ﬁltering. In Proceedings of the 16th
international conference on World Wide Web, WWW
’07, pages 271–280, New York, NY, USA, 2007. ACM.
[5] G. Dror, Y. Koren, Y. Maarek, and I. Szpektor. I want

to answer; who has a question?: Yahoo! answers
recommender system. In KDD, pages 1109–1117, 2011.

[6] T. Fawcett. An introduction to roc analysis. Pattern

Recognition Letters, 27(8):861–874, June 2006.

[7] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry.

Using collaborative ﬁltering to weave an information
tapestry. Commun. ACM, 35:61–70, December 1992.

[8] C.-F. Hsu, E. Khabiri, and J. Caverlee. Ranking

comments on the social web. In Proceedings of the
2009 International Conference on Computational
Science and Engineering-Volume 04, pages 90–97,
August 2009.

[9] M. Hu, A. Sun, and E.-P. Lim. Comments-oriented

document summarization: understanding documents
with readersˆa ˘A´Z feedback. In Proc. of ACM SIGIR
08, pages 291–298, July 2008.

[10] Y. Koren, R. M. Bell, and C. Volinsky. Matrix

factorization techniques for recommender systems.
IEEE Computer, 42(8):30–37, 2009.

[11] B. Li, S. Xu, and J. Zhang. Enhancing clustering blog
documents by utilizing author/reader comments. In In
Proceedings of the 45th Annual Southeast Regional
Conference, 2007, Winston-Salem, North Carolina,
pages 94–99, March 2007.

[12] Q. Li, J. Wang, Y. P. Chen, and Z. Lin. User

comments for news recommendation in forum-based
social media. Information Sciences,
180(24):4929–4939, December 2010.

[13] S. Li and S. Manandhar. Improving question

recommendation by exploiting information need. In
ACL, pages 1425–1434, 2011.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France437[14] G. Linden, B. Smith, and J. York. Amazon.com

recommendations: item-to-item collaborative ﬁltering.
IEEE Internet Computing, 7(1):76–80, 2003.

[23] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor,
editors. Recommender Systems Handbook. Springer,
2011.

[15] C. D. Manning, P. Raghavan, and H. Sch¨ı´c ¸Ttze.

[24] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl.

Introduction to Information Retrieval. Cambridge
University Press, 2008.

[16] G. Mishne and N. Glance. Leave a reply: An analysis

of weblog comments. In Proceedings of the 15th
international conference on World Wide Web, pages
625–632, May 2006.

[17] B. Pang and L. Lee. Opinion mining and sentiment

analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135, 2008.

[18] S. Park, M. Ko, J. Kim, Y. Liu, and J. Song. The

politics of comments: Predicting political orientation
of news stories with commenters‘ sentiment patterns.
In Proceedings of ACM Conference on Computer
Supported Cooperative Work (CSCW) 2011, pages
113–122, March 2011.

Item-based collaborative ﬁltering recommendation
algorithms. In Proceedings of the 10th international
conference on World Wide Web, WWW ’01, pages
285–295, New York, NY, USA, 2001. ACM.

[25] A. Schuth, M. Marx, and M. de Rijke. Extracting the
discussion structure in comments on news-articles. In
WIDM, pages 97–104, November 2007.

[26] S. Siersdorfer, S. Chelaru, W. Nejdl, and J. S. Pedro.

How useful are your comments? analyzing and
predicting youtube comments and comment ratings.
In roceedings of the 19th international conference on
World Wide Web, pages 891–900, April 2010.

[27] G. TakŒ´Scs, I. Pil´aszy, B. NŒ´Zmeth, and D. Tikk.

On the gravity recommendation system. In KDD Cup
Workshop at SIGKDD 07, pages 22–30, August 2007.

[19] P. A. Pavlou and A. Dimoka. The nature and role of

[28] M. Tsagkias, W. Weerkamp, and M. de Rijke.

feedback text comments in online marketplaces:
Implications for trust building, price premiums, and
seller diﬀerentiation. Information Systems Research,
17(4):392–414, December 2006.

[20] I. Pil´aszy and D. Tikk. Recommending new movies:
even a few ratings are more valuable than metadata.
In Proc. of the 3rd ACM Conf. on Recommender
Systems (RecSys’09), page 93100, New York, NY,
USA, October 23–25 2009. ACM, ACM.

Predicting the volume of comments on online news
stories. In Proceeding of the 18th ACM conference on
Information and knowledge management (CIKM),
pages 1765–1768, November 2009.

[29] M. Tsagkias, W. Weerkamp, and M. de Rijke. News

comments: Exploring, modeling, and online
prediction. In Advances in Information Retrieval,
32nd European Conference on IR Research, ECIR
2010. Proceedings, pages 191–203, March 2010.

[21] H. Rangwala and S. Jamali. Co-participation networks

[30] J. Weston, S. Bengio, and N. Usunier. Large scale

using comment information. In Proceedings of the
Fourth International AAAI Conference on Weblogs
and Social Media, pages 315–318, May 2010.
[22] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. In Proc. 25th
Conference Annual Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), pages 452–461, 2009.

image annotation: Learning to rank with joint
word-image embeddings. Machine Learning Journal,
81:21–35, 2010.

[31] W. G. Yee, A. Yates, S. Liu, and O. Frieder. Are web
user comments useful for search? In 7th Workshop on
Large-Scale Distributed Systems for Information
Retrieval (LSDS-IR) at SIGIR 09, pages 63–70, July
2009.

WWW 2012 – Session: Obtaining and Leveraging User CommentsApril 16–20, 2012, Lyon, France438