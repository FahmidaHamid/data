Joint Relevance and Freshness Learning From

Clickthroughs for News Search

Hongning Wang

Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana IL, 61801 USA

wang296@illinois.edu

Anlei Dong, Lihong Li, Yi Chang,

Evgeniy Gabrilovich

Yahoo! Labs

701 First Avenue, Sunnyvale, CA 94089
{anlei,lihong,yichang,gabr}@yahoo-

inc.com

ABSTRACT
In contrast to traditional Web search, where topical rel-
evance is often the main selection criterion, news search
is characterized by the increased importance of freshness.
However, the estimation of relevance and freshness, and es-
pecially the relative importance of these two aspects, are
highly speciﬁc to the query and the time when the query
was issued. In this work, we propose a uniﬁed framework
for modeling the topical relevance and freshness, as well as
their relative importance, based on click logs. We use click
statistics and content analysis techniques to deﬁne a set of
temporal features, which predict the right mix of freshness
and relevance for a given query. Experimental results on
both historical click data and editorial judgments demon-
strate the eﬀectiveness of the proposed approach.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Experimentation

Keywords
Relevance and freshness modeling, learning to rank, tempo-
ral features

1.

INTRODUCTION

When a user submits the query “Apple Company” into a
news search engine, she is expecting to ﬁnd a list of most re-
cent news reports that are topically relevant to the company.
The emphasis on recency is crucial, as even the seemingly
current events can quickly become outdated, dwarfed by the
importance of new developments. For example, the news ar-
ticles covering the release of iPhone 4S were quite relevant
on Oct 4, 2011; however, they became less relevant just one
day later, when Apple Inc.’s former CEO Steve Jobs passed
away. Such cases are common in news search and make it
diﬀerent from the traditional web search, where “relevance”
is typically narrowly deﬁned as topical relatedness. In web

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

search, a great amount of eﬀort has been devoted to de-
signing eﬀective retrieval features and models to improve
the estimation of topical relevance [22, 15, 4, 11, 20]; how-
ever, for news search much less work has been done to take
freshness into account. The importance of content recency
in news search suggests extending the conventional notion
of relevance beyond pure topical match, by incorporating
freshness as another important ranking criterion.1

Figure 1: CTR curves over 8 diﬀerent URLs over a
two-month period.

Incorporating freshness into news ranking is not a triv-
ial exercise of combining relevance and freshness scores, as
users’ search intents need to be taken into consideration.
For breaking news queries, as in our previous example, re-
turning most recent news would always improve users’ satis-
faction. In other cases, for newsworthy queries such as “bin
laden death”, preferring older but more relevant reports, in
terms of coverage and authority, makes more sense. Previous
studies have shown that the relationship between returned
documents and queries varies as users’ intent in web search
changes over time [17, 19]. To demonstrate this phenomenon
in news search scenario, we select 8 most frequently returned
URLs under 4 diﬀerent queries (2 URLs per query) from Ya-
hoo! news search engine2 in a period of two months (late
May to late July, 2011). Two of these queries have the high-
est search frequency within one day, namely, “toy story 4”

1In what follows, we use the term “relevance” to only refer
to topical relatedness to avoid ambiguity.
2http://news.search.yahoo.com/

05/27/201106/02/201106/08/201106/15/201106/21/201106/28/201107/04/201107/11/201100.10.20.30.40.50.60.70.80.91CTRCTR curve over time for different queriesURL1@abc the bachelorette 2011URL2@abc the bachelorette 2011URL1@immigration reform 2011URL2@immigration reform 2011URL1@casey anthonyURL2@casey anthonyURL1@toy story 4URL2@toy story 4WWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France579and “casey antony”, and the other two have the longest life-
time span over this period, i.e., “abc the bachelorette 2011”
and “immigration reform 2011”. We illustrate the Click-
Through Rate (CTR) curves for these 8 URLs during the
above period in Figure 1 and list the corresponding URLs
in Table 1.

Table 1: URL list for Figure 1

Entry

URL

URL1@ abc the
bachelorette 2011

URL2@ abc the
bachelorette 2011

URL1@ immigra-
tion reform 2011
URL2@ immigra-
tion reform 2011

URL1@ casey an-
thony

URL2@ casey an-
thony

URL1@ toy story
4

URL2@ toy story
4

http://blog.zap2it.com/frominsidethebox/2011
/06/tv-ratings-bachelorette-leads-abc-again-
monday-stanley-cup-numbers-rise.html
http://www.broadcastingcable.com/article
/470053-Primetime-Ratings-
Bachelorette-Drops-ABC-Still-Wins-
Monday.php?rssid=20065
http://biz.yahoo.com/prnews/110526/
la10138.html?.v=1
http://seattletimes.nwsource.com/html/
editorials/2015089638-
edit19dream.html?syndication=rss
http://www.cnn.com/2011/CRIME/06/04/
casey.anthony.weekly.wrap/index.html?section
=cnn-latest
http://www.cnn.com/2011/CRIME/06/03/(cid:13)ori
da.casey.anthony.trial/index.html?eref=rss-
us
http://theenvelope.latimes.com/news/la-
et-at-toy-story-4-oscar-
sl,0,1068280.storylink?track=rss
http://1019litefm.radio.com/2011/06/28/toy-
story-4-might-happen/

From the CTR curves, we can clearly observe the dif-
ference between these two types of queries. For the query
“casey antony”, the CTR for the returned URLs quickly di-
minished because new stories came out soon; on the other
hand, for the query “abc the bachelorette 2011”, users’ inter-
est was maintained over much longer period of time. In the
latter case, users found those URLs containing summary re-
port useful in a long time after this TV episode was shown.
These results imply that users’ preferences are highly depen-
dent on the query as well as its issuing time.

Given the highly dynamic nature of news events and the
sheer scale of news reported around the globe, it is often
impractical for human editors to constantly keep track of the
news and provide timely relevance and freshness judgments.
Since the machine learned ranking methods often depend on
editorial judgments [20], delayed and inaccurate annotations
can mislead the learning algorithms. To this end, Dong et
al. [8] designed a set of crawling mechanisms and editorial
guidance to annotate the relevance and freshness. Then they
used the freshness grade to demote the relevance grade when
computing the ﬁnal ranking score. However, we believe such
rule-based demotion is often sub-optimal. To validate this
conjecture, we asked editors to annotate the news query log
for the day of Aug. 9, 2011 immediately one day after,
and then demoted the news search results based on their
judgments using the method from Dong et al.
[8]. The
relation between observed CTR and the demoted grades is
visualized by a scatter plot in Figure 2.

From Figure 2, we observe that the clicks are not strictly
correlated with the demoted grades: the average Pearson
correlation between them across the queries is 0.5764 with
a standard deviation 0.6401. The main reason for this in-
consistency is the hard demotion rule: users might have dif-
ferent demotion preferences for diﬀerent queries, and it’s al-

Figure 2: Scatter plot of CTR versus editor’s rele-
vance judgments.

most impossible for an editor to predeﬁne the combination
rules given the plurality of possibilities. As a result, the un-
certainty from this heuristically-derived ranking grades will
limit the performance of subsequent learning-to-rank algo-
rithms.

In this work, we propose to model relevance and freshness,
and the query-speciﬁc relative mix of these two aspects si-
multaneously from the click logs. We assume the users’ click
behavior for the given query depends jointly on both the
relevance and freshness of a news article. To this end, we
introduce a latent factor model, Joint Relevance Freshness
Learning (JRFL), which captures the relevance and fresh-
ness aspects, as well as the latent preference between the
two, in a uniﬁed way. To capture the temporal sensitivity
of both news documents and queries, a set of eﬀective tem-
poral features, utilizing click statistics and content analysis
techniques, is proposed. We evaluate the proposed method
on both click data and editorially annotated sessions. Ex-
perimental evaluation conﬁrms that the proposed learning
method outperforms several standard learning-to-rank al-
gorithms, which cannot properly handle the query-speciﬁc
trade-oﬀ between relevance and freshness.

The contributions of our joint learning method are two

folds:

1. The relevance and freshness are jointly learned from
the click logs, which avoids deﬁning any hard combi-
nation rules for relevance and freshness ahead of time,
and such query-speciﬁc preference is directly estimated
from the data.

2. Our method does not require any manually annotated
data, making it applicable in a broad spectrum of re-
trieval tasks, where task-speciﬁc ranking criteria can
be easily incorporated.

2. RELATED WORK

Learning-to-rank algorithms have shown signiﬁcant and
consistent success in various applications [20, 13, 25, 5].
Such machine-learned ranking algorithms learn a ranking
mechanism by optimizing particular loss functions based on
editorial annotations. An important assumption in those
learning methods is that the “relevance” of documents for
a given query is generally stationary over time, so that, as
long as the coverage of the labeled data is broad enough, the
learned ranking functions would generalize well to future un-
seen data. Such assumption is often true in web search, but

BadFairGoodExcellentPerfect00.20.40.60.81CTR v.s. Editorial JudgmentsCTRWWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France580less likely to hold in news search because of the dynamic
nature of news event and lack of timely annotations, as we
have analyzed in Section 1.

Recency ranking is an emerging research topic to tackle
the time-sensitive ranking problems in web search. Li et al.
[19] and Efron et al. [10] came up with solutions from a con-
tent analysis perspective, by introducing document publica-
tion timestamp into language models. However, the tempo-
ral property for both queries and documents are not limited
to timestamps alone; various signals are available to depict
it. Dong et al. [8] designed a system to automatically detect
and response to recency sensitive queries. Later on, features
extracted from Twitter3 stream were incorporated to iden-
tify fresh URLs [9]. But their learning method depended
on a predeﬁned freshness-demotion strategy, which does not
necessarily lead to optimal generalization performance. In
another eﬀort by Moon et al. [21], user clicks were combined
with a baseline ranking system to capture temporal shifts of
a user’s information need in recency search. However, they
only used clicks for the highest ranked article to update their
ranking model, thus did not make full use of click data.

The closet work to ours is Dai et al.’s divide-and-conquer
learning strategy for recency ranking [7], although their work
was substantially diﬀerent from ours. First, they still relied
on the manual relevance/freshness annotations to train the
rankers, where a weighted harmonic mean was used to inte-
grate the relevance and freshness grades. In JRFL, we do not
require such manual annotations — relevance and freshness
are automatically learned from the clickthroughs, resulting
in greater ﬂexibility in our model. Second, they did not
model relevance and freshness separately, but instead used
one ranker on all the features. In our method, we learn the
relevance and freshness models separately from two diﬀer-
ent sets of features. The separation allows the two models
to focus better on diﬀerent aspects of a document (namely,
freshness and relevance). Third, the importance weights be-
tween relevance and freshness were manually tuned in their
harmonic mean, while our method uses a set of query-speciﬁc
features to adaptively combine freshness and relevance, and
the adaptation is automatically learned from real clicks.

Since we are learning to optimize diﬀerent ranking crite-
ria, our work is also related to multi-object ranking. Svore et
al. proposed to optimize multiple graded ranking measures,
e.g., NDCG and CTR, by combining the gradients from dif-
ferent object functions in the framework of LambdaMART
[23]. Agarwal et al. used a constrained optimization frame-
work to encode multiple objectives for clicks and post-click
downstream utilities in content recommendation systems [1].
The problem tackled by JRFL is somewhat diﬀerent: we still
try to optimize the information utility of a ranking list, al-
though the utility is not directly observed and is aﬀected
by two criteria (relevance and freshness) in an unknown and
query-speciﬁc way. Our goal is to simultaneously learn, from
click data, the two criteria as well as the best combination
of them.

3. METHOD

Suppose, when a user submits a query to a news search en-
gine and gets an according list of ranked news documents,
she would ﬁrst judge the usefulness of each document by
her underlining sense of relevance and freshness, and gives

3http://twitter.com/

it an overall impression grade by her preference over rele-
vance and freshness at that particular time. Once she has
such impressions in mind, she would deliberately click the
documents most interesting to her and skip all the others.

Inspired by this example, we proposed to model the users’
click behavior in news search as a direct consequence of ex-
amining the relevance and freshness aspects of the returned
documents. Besides, for diﬀerent queries, the relative em-
phasis the users put over these two aspects can vary sub-
stantially, reﬂecting the searching intention for the speciﬁc
news event. Therefore, a good ranking function should be
able to infer such a trade-oﬀ and return the “optimally com-
bined ” ranking results for individual queries. However, we
cannot explicitly obtain the users’ relevance/freshness judg-
ments and the preferences over these two aspects, since their
evaluation process is not directly observable from the search
engine. Fortunately, the users’ click patterns are recorded,
from which we can assume the clicked documents are more
meaningful to her than the non-clicked ones [14]. There-
fore, we model relevance and freshness as two latent factors
and assume a linear combination of these two, which is also
latent, generates the observed click preferences.

To better determine the temporal property of the news
documents and detect the recency preference imposed for the
query, we design a set of novel temporal features from click
statistics and content-analysis techniques. In the following
sections, we will introduce the proposed model and temporal
features in detail.
3.1 Joint Relevance and Freshness Learning
The basic assumption of our proposed Joint Relevance
Freshness Learning (JRFL) model is that a user’s overall
impression assessment by combining relevance and freshness
for the clicked URLs should be higher than the non-clicked
ones, and such a combination is speciﬁc to the issued query.
Therefore, our method falls into the pairwise learning-to-
rank framework.

ni and SF

ni and X F

Formally, we have N diﬀerent queries and for the n-th
query we observed M diﬀerent URL click preference pairs
(Uni ≻ Unj), in which Uni is clicked but Unj is not. We de-
note X R
ni as the relevance and freshness features for
Uni under Query Qn, and SR
ni are the corresponding
relevance and freshness scores for this URL given by the rel-
evance model gR(X R
ni), respec-
tively. In addition, we denote αQ
n as the relative emphasis
on freshness aspect estimated by the query model fQ(X Q
n ),
i.e., αQ
n describing
query Qn. To make relevance/freshness scores comparable
n ≤ 1. As a result,
across all the URLs, we require 0 ≤ αQ
the user’s latent assessment Yni about the URL Uni for a
particular query Qn is assumed to be a linear combination
of its relevance and freshness scores:

n ), based on the features X Q

ni) and freshness model gF (X F

n = fQ(X Q

n × SF

ni + (1 − αQ

n ) × SR

Yni = αQ

(1)
Since we have observed the click preference (Uni ≻ Unj), we
can safely conclude that Yni > Ynj.

ni

Based on the previous discussion, we characterize the ob-
served pairwise click preferences as the joint consequences
of examining the combined relevance and freshness aspects
of the URLs for the query. For a given collection of click
logs, we are looking for a set of optimal models (gR, gF ,
fQ), which can explain the observed pairwise preferences as
many as possible. As a result, we formalize this pairwise

WWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France581learning problem as an optimization task,

min

fQ; gR; gF ; (cid:24)

1
2

(∥fQ∥ + ∥gR∥ + ∥gF∥) +
s.t. ∀(n, i, j), U RLni ≻ U RLnj

C
N

Yni − Ynj > 1 − ξnij
0 ≤ fQ(X Q
ξnij ≥ 0,

n ) ≤ 1

N∑

∑

n=1

i;j

ξnij

(2)

(3)

merical approximation is unavoidable in general when opti-
mizing their model parameters.

Formally, we use three linear models:

gR(X R
gF (X F
fQ(X Q

ni) = wT
ni) = wT
n ) = wT

RX R
ni
F X F
ni
QX Q
n

(4)

(5)

(6)

where the bias factor b in linear functions is excluded by
introducing the dummy feature 1.

As a result, the proposed JRFL model deﬁned in Eq(2)

can be instantiated as:

min

wR;wF ;wQ;(cid:24)

1
2

(∥wQ∥2 + ∥wR∥2 + ∥wF∥2) +

C
N

N∑

∑

ξnij (7)

n=1

i;j

where Yni and Ynj are deﬁned in Eq (1), the non-negative
slack variables {ξnij} are introduced to account for noise in
the clicks, ∥ · ∥ is the functional norm (to be deﬁned later)
describing complexity of the models, and C is the trade-oﬀ
parameter between model complexity and training error.

Figure 3: Intuitive illustration of the proposed Joint
Relevance and Freshness Learning model. The user
issued query and the corresponding returned URLs
are represented by their features on the left part.
Dashed arrow lines in the middle indicate the as-
sumed user’s judging process before she clicks. The
check boxes on the right record the clicked URLs.

Figure 3 depicts the intuition behind the proposed JRFL.
From the ﬁgure, we can clearly notice the diﬀerence between
our proposed JRFL and other classic pairwise-learning-to-
rank algorithms, e.g., RankSVM [13] and GBRank [25].
A classic pairwise learning-to-rank algorithm only uses one
scoring function to account for all the observed click prefer-
ences, where diﬀerent ranking criteria cannot be easily in-
corporated. Besides, even though RankSVM model shares a
similar object function as JRFL, they are still quite diﬀerent:
JRFL simultaneously learns a relevance model and a fresh-
ness model, and utilizes a query-speciﬁc model to leverage
these two aspects to explain the observed click patterns. Nei-
ther RankSVM nor GBRank deal with such query-speciﬁc
multi-criterion ranking. Besides, as we have discussed in
Section 2, in previous studies [8, 7], SR
ni and SF
ni for each
URL were already known, so that they tuned αQ
n directly
for each type of query. In our problem, all those factors are
latent and estimated automatically from the click logs.

In our previous description, we didn’t specify the forms of
the relevance model gR(X R), freshness model gF (X F ), and
query model fQ(X Q). Although many alternatives exist, we
choose linear functions for all these models to simplify the
exposition and derive eﬃcient model estimation procedures.
Other types of functions can also be employed, although nu-

s.t. ∀(n, i, j), Uni ≻ Unj

ni − X F
nj)
n ) × wT
R(X R

n × wT
wT
QX Q
+ (1 − wT
0 ≤ wT
QX Q
ξnij ≥ 0.

F (X F
QX Q
n ≤ 1

ni − X R

nj) > 1 − ξnij

Thanks to the associative property of linear functions,
the optimization problem deﬁned in Eq (7) can be divided
into two sub-problems: relevance/freshness model estima-
tion and query model estimation, and each of them is a con-
vex programming problem (note that Eq (7) itself is non-
convex). Therefore, we can utilize the coordinate descent
algorithm to iteratively solve the two convex programs, as
shown in Figure 4.

The interaction between the relevance/freshness models
and query model is clearly stated in the optimization pro-
cess:
in relevance/freshness models estimation, the query-
speciﬁc preference weight acts as a tuning factor, which
increases or decreases the features in these two models to
represent the searching intention; once we have the rele-
vance/freshness models, we tune the query model to preserve
as many click preferences as possible based on the current
relevance/freshness predictions.

Since each update step is convex, our coordinate optimiza-
tion is guaranteed to decrease the object function in Eq (7)
monotonically and therefore converges to a local optimum.
3.2 Temporal Features

We use 95 basic text matching features, such as query
term matched in title, matched position in document, and
source authority score from a subset of features employed in
Yahoo! news search engine, as our URL relevance features.
To capture the temporal property of the news documents
and queries, we propose a set of novel time-sensitive features
as our URL freshness features and query features, which are
summarized in Table 2.
3.2.1 URL Freshness Features
Publication age agepubdate(URL|Query): the URL’s pub-

lication timestamp is used to identify the document’s fresh-
ness property.

However, for news search, the freshness of news content
is more important. Therefore, we propose to identify the
given news document’s freshness quality from content anal-
ysis perspective.

Story age agestory(URL|Query): we use the regular ex-

pressions deﬁned in [8] to extract the mentioned dates in

Query: QnX…...…....URLi:(,)RFniniXXURLj:(,)RFnjnjXX…...QnDQfiYjYRgFgFniSRniSRgFgFnjSRnjSNWWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France582Table 2: Temporal Features for URL freshness and Query model

Type

URL freshness

Query Model

Feature

max

max

max

log p(URL|d)

log p(URL|d)
log p(URL|d)

d2Corpus(qjt)[t(cid:0)1day;t]
d2Corpus(qjt)[t(cid:0)5days;t(cid:0)2days]
d2Corpus(qjt)[(cid:0)1;t(cid:0)6days]

agepubdate(URL|Query) = timestamp(Query) − pubdate(URL)
agestory(URL|Query) = timestamp(Query) − pubdateextracted(URL)
LM@1(URL|Query, t) =
LM@5(URL|Query, t) =
LM@ALL(URL|Query, t) =
agepubdate(URLjQuery)(cid:0)mean[agepubdate(URLjQuery)]
t-dist(URL|Query) =
∑
q prob(Query|t) = log Count(Queryjt)+(cid:14)q
∑
q Count(Queryjt)+(cid:14)
u prob(User|t) = log Count(U serjt)+(cid:21)u
q Count(U serjt)+(cid:21)
q ratio(Query|t) = q prob(Query|t) − q prob(Query|t-1)
]
u ratio(User|t) = u prob(User|t) − u prob(User|t-1)
Ent(Query|t) = −p(Query|t) log p(Query|t)
[
]
CTR(URL|Query, t)
CTR(Query|t) = mean
[
]
agepubdate(URL|Query)
pub mean(Query|d) = meanU RL2Corpus(Qjt)
agepubdate(URL|Query)
pub dev(Query|d) = devU RL2Corpus(Qjt)
pub frq(Query|t) = log
Count(U RLjd)+(cid:27)u
U RL Count(U RLjt)+(cid:27)

dev[agepubdate(URLjQuery)]

∑

[

(δq, δ), (λu, λ) and (σu, σ) are the smoothing parameters estimated from the query log.

)

(

{[

(

)]}

Algorithm: Coordinate Descent for JRFL

Input:
X Q
n ,

A collection of
ni, X F

ni) ≻ (X R

(X R

click preferences L =

nj, X F

nj)

, . . . ,

(X R

nk, X F

nk) ≻

;

nl, X F
nl)

(X R
Input: Trade-oﬀ parameter C;
Input: Maximum iteration step S;
Input: Relative convergency bound ϵ;
Output: Learned model parameters of (wR, wF , wQ);
Step 0 : Randomly initialize (wR, wF , wQ) and set i = 0;
Step 1 : Update Relevance/Freshness models:
(∥wR∥2+∥wF∥2)+

) ← arg min

N∑

, w(i+1)

(w(i+1)

∑

R

F

1
2

wR;wF ;(cid:24)

C
N

n=1

i;j

with respect to the constrains listed in Eq (7) by ﬁxing
the Query model to w(i)
Q ;
Step 2 : Update Query model:

w(i+1)

Q

← arg min

wQ;(cid:24)

1
2

∥wQ∥2 +

C
N

N∑

∑

ξnij

n=1

i;j

with respect to the constrains listed in Eq (7) by ﬁxing
the Relevance/Freshness models to (w(i+1)
Step 3 : Compute object function value deﬁned in
Eq (7)→ obj and increase i = i + 1;
Step 4 : If the relative change in obj is greater than
ϵ and i is smaller than S, go to Step 1, else return
(w(i)

, w(i+1)

R , w(i)

F , w(i)

Q ) .

);

R

F

Figure 4: Coordinate Descent for JRFL.

ξnij

the news content, calculate their distances to the given query
within the document, and select the one with the minimal
distance as the extracted story timestamp to infer the cor-
responding story age.
Story coverage LM@f1,5,ALLg(URL|Query, t): content
coverage is an important character of the freshness for a
news document. Newer stories should cover more content
that has not been mentioned by the previous reports. For a
given query with a list of candidate news articles at a par-
ticular time, we ﬁrst collect all the previous news articles
associated with this query in our query log, and build lan-
guage models [16, 24] for each of these documents. Then, we
separate those language models into three sets: models with
documents published one day before, two to ﬁve days before,
and all the rest, and treat the candidate URLs as query to
calculate the maximum generation probability given by all
the models in these three sets accordingly.

from a user’s per-
spective, since the news search engine has already dis-

Relative age t-dist(URL|Query):
played agepubdate(URL|Query) to her, the document’s rel-

ative freshness within the returned list is more meaning-
ful for her. To capture this signal, we shift each URL’s

agepubdate(URL|Query) value within the returned URL list

by the mean value in this list and scale the results by the
corresponding standard deviation.

3.2.2 Query Freshness Features
The query features are designed to capture the latent pref-
erence between relevance and freshness.
Query/User frequency q prob(Query|t) and u prob
(User|t): the frequency of a query within a ﬁxed time slot
is a good indicator for breaking news query. We calculate
the frequency of the query and unique users who issued this
query within in a time slot prior to the query time.
Frequency ratio q ratio(Query|t) and u ratio(User|t):
the relative frequency ratio of a query within two consecu-

WWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France583Distribution entropy Ent(Query|t):

tive time slots implies the change of users interest. A higher
ratio indicates an increasing user interest on this news event.
the distribution of
query’s issuing time is a sign of breaking news: a burst of
search occurs when particular news event happens. We uti-
lize the entropy of query issuing time distribution to capture
such burstiness. A multinomial distribution p(Query|t) with
ﬁxed bin size (e.g., 2 hours per bin) is employed to approx-
imate the query’s temporal distribution within the day.
Average CTR CTR(Query|t): CTR is another signal rep-
resenting the freshness preference of the query: when break-
ing news happens, people tend to click more returned URLs.
We calculate the average CTR over all the associated URLs
within a ﬁxed time slot in prior to the query time.
URL recency pub mean(Query|d), pub dev(Query|d)
and pub frq(Query|d):
the recency of URLs associated
with the query can be treated as a good proﬁle of this query’s
freshness tendency: when the URLs associated with one par-
ticular query in a ﬁxed period are mostly fresh, it indicates
the query itself is highly likely to be a breaking news query.
We calculate the mean and standard deviation of the asso-

ciated URLs’ agepubdate(URL|Query) features and the fre-

quency of the URLs created in that speciﬁc period.

4. EXPERIMENT RESULTS

This section validates our JRFL model empirically with
large-scale click data sets and editorial annotations. We be-
gin by describing the data sets used.
4.1 Data Sets
4.1.1 Click Data Sets
We collected real search sessions from Yahoo! news search
engine in a two months period, from late May to late July,
2011. And to unbiasedly compare diﬀerent ranking algo-
rithms, we also set up a random bucket to collect exploration
clicks from a small portion of traﬃc at the same time. In
this random bucket, the top four URLs were randomly shuf-
ﬂed and displayed to the real users. By doing such random
shuﬄing, we were able to collect user click feedback on each
document without positional bias, and such feedback can be
thought as a reliable proxy on information utility of docu-
ments [18]. As a result, we only collected the top 4 URLs
from this random bucket.

In addition, we also asked editors to annotate the rele-
vance and freshness in Aug. 9, 2011’s query log immediately
one day after, according to the editorial guidance given by
Dong et al. [8].

Simple preprocessing is applied on these click data sets: 1)
ﬁltering out the sessions without clicks, since they are useless
for either training or testing in our experiments; 2) discard-
ing the URLs whose publication time is after the query’s
issuing time (caused by errors from news sources); 3) dis-
carding sessions with less than 2 URLs.
4.1.2 Preference Pair Selection
We decide to train our model on the normal click data,
because such clicks are easier to collect without hurting the
search engine’s performance. However, this kind of clicks are
known to be heavily positional biased [2]. To reduce the bias
for training, we followed Joachims et al.’s method to extract
preferences from clicks [14]. In particular, we employed two
click heuristics:

}
}

{
1. “Click ≻ Skip Above”:
For a ranked URL list
U1, U2, . . . , Um
and a set C containing the clicked
URLs, extract a preference pair Ui ≻ Uj for all pairs
1 ≤ j < i with Ui ∈ C and Uj /∈ C.
{
2. “Click ≻ Skip Next”:
For a ranked URL list
U1, U2, . . . , Um
, and a set C containing the clicked
URLs, extract a preference pair Ui ≻ Ui+1 for all
Ui ∈ C and Ui+1 /∈ C.

In addition, to ﬁlter out noisy and conﬂicting preferences,
we deﬁned three rules: 1) ﬁlter out the preference pairs ap-
pearing less than 5 times; 2) calculate Pearson’s χ2 value [6]
on all the pairs, and order them according to their χ2 value;
3) if both Ui ≻ Uj and Uj ≻ Ui are extracted, discard the
one with smaller χ2 value.

After these selection steps, we were able to keep the top
150K preference pairs from some portion of normal clicks we
collected in Section 4.1.1. Besides, for testing purpose, we
randomly select 500k query-URL pairs from original normal
click set (not including the URLs used for generating the
click preference pairs) and 500k from the random bucket
clicks. In order to guarantee the quality of freshness anno-
tation, we asked the editors to ﬁnish the annotation in day.
As a result, we only have about 13k query-URL pairs anno-
tated out of that day’s query log. As a summary, we list the
data sets for our experiments in Table 3.

Table 3: Evaluation Corpus

#(Q,t) #(Q,U,t) #URL Pairs

Training preferences

Normal clicks
Random clicks

Editorial judgment

75,236
59,062
127,474

1,404

230,351
500,000
500,000
13,091

150,000

-
-
-

4.1.3 Temporal Feature Implementation
For most of our temporal features, we had to specify a time
slot for the implementation; for example, the Query/User
frequency q prob(Query|t) and u prob(User|t) are both
calculated within a predeﬁned time slot.
In the following
experiments, we set such a time slot to be 24 hours, and all
the necessary statistics were collected from this time window
accordingly. Once the features were generated, we linearly
scaled each of them into the range [-1, 1] to normalize them.
4.1.4 Baselines and Evaluation Metrics
Since the proposed JRFL model works in a pairwise
learning-to-rank manner, we employed two classic pairwise
learning-to-rank algorithms, RankSVM [13] and GBRank
[25], as our baseline methods. Because these two algorithms
do not explicitly model relevance and freshness aspects for
ranking, we fed them with the concatenation of all our URL
relevance/freshness and query features. Besides, to compare
the models trained on clicks with those trained on editorial
judgments, we also used Dong et al.’s freshness-demotion-
trained GBRank model [8] as our baseline and denoted it as
“FreshDem”.

To quantitatively compare diﬀerent ranking algorithms’
retrieval performance, we employed a set of standard eval-
uation metrics in information retrieval.
In click data, we
treat all the clicked URLs as relevant and calculate the corre-
sponding Precision at 1 (P@1), Precision at 2 (P@2), Mean

WWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France584(a) Object Function Value Update

(b) Pairwise Error Rate (PER) Update

(c) Query Weight αQ Update

Figure 5: Scatter plot of CTR versus JRFL’s prediction

Average Precision at 3 (MAP@3), Mean Average Precision
at 4 (MAP@4), and Mean Reciprocal Rank (MRR). Deﬁni-
tions of these metrics can be found in standard texts (e.g.,
[3]).
In the editorial annotation data set, we treated the
grade “Good” and above as relevant for precision-based met-
rics, and also used discounted cumulative gain (DCG) [12]
as an evaluation metric. 4
4.2 Analysis of JRFL

4.2.1 Convergency
We ﬁrst demonstrate the convergency of the coordinate
descent algorithm for JRFL model as described in Figure 4,
which is the necessary condition for applying the proposed
model in real ranking problems. We randomly divided the
training preference pairs into two sets, one with 90k pairs for
training and the rest 60k for testing. We ﬁxed the trade-oﬀ
parameter C in JRFL to be 5.0 (we also tried other settings
for this parameter, smaller C would render us less iterations
to converge, but the tendency of convergency is the same),
(cid:0)5 and maximum it-
relative convergency bound ϵ to be 10
eration step S to be 50 in the coordinate descent algorithm.
To study if the algorithm’s convergence is sensitive to the
initial state, we tried 3 starting points: 1) ﬁxing the initial
query weights αQ(0) to be 1.0 (freshness only); 2) ﬁxing αQ(0)
to be 0.0 (relevance only); 3) setting it uniformly between
0 and 1, by directly set all the query weights accordingly
at step 0. We visualize the training process by illustrat-
ing the updating trace of object function deﬁned in Eq(7),
pairwise error rate on both training and testing set, and
the mean/standard deviation of the updated query weights
during the iterative optimization in Figure 5.

As demonstrated in Figure 5 that the proposed JRFL
model converges during the coordinate descent optimization
process, and such convergency does not depend on the initial
state. From Figure 5(c), we can observe that the optimal
query weight setting for this training set is usually around
0.4546 ± 0.0914. Hence, the random initialization converges
fastest comparing with two other settings, since the initial
state given by random is closest to this optimal setting.

In addition, it should be emphasized that, although the
average of the converged value of αQ is less than 0.5, it
does not necessarily indicate freshness is less important than

4According to Yahoo!’s business rule, the reported metrics
are normalized accordingly; therefore only the relative im-
provement makes sense.

relevance in general for news search task, because the scales
of the outputs of our freshness and relevance models may
not be comparable. Therefore, only the order among the
queries given by such learned query weights represents their
relative emphasis on relevance and freshness aspects.

Another phenomenon we observed in Figure 5(a) and (b)
is that even though the object function decreased quickly af-
ter the ﬁrst several iterations, the pairwise error rate needed
more iterations to reach its optimal value. Furthermore,
during these updates, there were some inconsistent updates
between the object function value and pairwise error rate:
while the object function value always decreased with more
iterations, the pairwise error rate did not show the same
monotonic behavior. This inconsistence is expected, be-
cause our object function in Eq (7) is a relaxed one: we
do not directly minimize the pairwise error rate, which is
computationally intractable, but we are trying to reduce the
prediction gap between the mis-ordered pairs.

Table 4: Feature weights learned by JRFL

Feature

URL freshness

Query model

Type Top 3 features

Neg

agepubdate(URL|Query)
LM@5(URL|Query, t)
t-dist(URL|Query)
Pos q ratio(Query|t)
pub frq(Query|t)
q prob(Query|t)
pub dev(Query|d)
pub mean(Query|d)

Neg Ent(Query|t)

Table 4 gives the top 3 positive and negative features from
the newly proposed URL freshness features and query fea-
tures, ordered by the learned weights. The weights in the
linear model reﬂect the features’ relative contribution to the
ﬁnal ranking decision. Because we only have 6 URL fresh-
ness features, and the learned weights for them are all neg-
ative, we only list the top 3 negative ones in this table.

The weights

learned by the

corresponding mod-
reasonable and consistent with our design:
els are
smaller values of
for URL freshness
features,
cov-
Publication
erage LM@5(URL|Query, t)
t-
dist(URL|Query) are,
the news ar-
ticle is; and for the query features, the larger values

agepubdate(URL|Query),

the more recent

age

the

Story

and Relative

age

510152025303511.11.21.31.41.5x 105Iteration StepsObject Function Value of Eq (7)Object Function Value UpdateαQ(0)=0.0αQ(0)=1.0αQ(0)= random51015202530350.150.160.170.180.190.20.210.22Iteration StepsPairwise Error RatePairwise Error Rate Update on Training and Testing Set Training PER with αQ(0)=0Training PER with αQ(0)=1Training PER with αQ(0)=randomTesting PER with αQ(0)=0Testing PER with αQ(0)=1Testing PER with αQ(0)=random510152025303500.10.20.30.40.50.60.70.80.91Iteration StepsQuery Weight αQQuery Weight αQ UpdateαQ(0)=0.0αQ(0)=1.0αQ(0)=randomWWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France585of query frequency q prob(Query|t) and URL recency
pub frq(Query|d), and the smaller values of Distribution
entropy Ent(Query|t), URL recency pub mean(Query|d)
and pub dev(Query|d) are, the more users and news
reports start to focus on this event, and therefore the
freshness asepct becomes more important.

4.2.2 Relevance and Freshness Learning
Since our JRFL model does not rely on explicit rele-
vance/freshness annotations, it is important to evaluate how
well our relevance and freshness models can estimate each
aspect separately. We separately used the relevance and
freshness annotations on Aug. 9, 2011’s query log as the
test bed and utilized two GBRank models trained on Dong
et al,’s relevance and freshness annotation data set accord-
ingly (44,641 query-URL pairs) [8]. Because [8]’s data set
does not contain the corresponding click information, those
two GBrank models were trained without new query fea-
tures. Our JRFL was trained on all the extracted click pref-
erence pairs.

Table 5: Performance on individual relevance and
freshness estimation

P@1 MAP@3 DCG@5

Relevance GBRank
JRFL Relevance
Freshness GBRank
JRFL Freshness

0.9655
0.8273
0.9823
0.9365

0.3422
0.2291
0.4998
0.3106

14.6026
14.7962
18.8597
19.8228

We observe mixed results in Table 5: the relevance and
freshness modules inside JRFL have worse ranking perfor-
mance than the purely relevance/freshness trained GBRank
models at the top positions (lower P@1 and MAP@3), but
similar cumulative performance, i.e., DCG@5 for both as-
pects. The reason for this result is that JRFL model has
to account for the trade-oﬀ between relevance and freshness
imposed by the queries during training, the most relevant
or recent documents might not be treated as good training
examples if their another aspect was not desirable. How-
ever, the purely relevance/freshness trained GBRank mod-
els do not have such constraints, and can derive patterns to
put the most relevant/recent documents at the top positions
separately. As a result, those two GBRank models’ perfor-
mance can be interpreted as upper bounds for each individ-
ual ranking criterion (freshness/relevance) in this data set.
When we reach the lower positions, those two types of rank-
ing algorithms give users quite similar utilities, i.e., under
DCG@5 metric. Besides, we want to emphasize that such
result is already very encouraging since the JRFL model
successfully infers the relevance and freshness solely from
the clicks, which conﬁrms the soundness of our assumption
in this work that users’ click behavior is the joint conse-
quence of examining the relevance and freshness of a news
article for the given query; by properly modeling such rela-
tionships, we can estimate the relevance and freshness from
the clickthroughs to certain extend.

4.2.3 Query Weight Analysis
There is no direct way for us to evaluate the correctness
of the inferred query weights, since such information is not
observable in the search log. Therefore, in this experiment,

we investigate it in an indirect way. As we have discussed in
the previous discussion, the order among the queries given
by such weight reﬂects the query’s relative emphasis over
freshness aspect. Therefore, we ranked the queries in our
training set according to the learned weights and list the
top 10 (freshness-driven) and bottom 10 (relevance-driven)
queries in Table 6.

Table 6: Query intention analysis by the inferred
query weight

Freshness Driven
7-Jun-2011, china

6-Jul-2011, casey an-
thony trial
24-Jun-2011,
draft 2011
28-Jun-2011, libya

nba

9-Jun-2011, iran

6-Jun-2011, pakistan
13-Jun-2011,
lebron
james
29-Jun-2011, greece

27-May-2011,
missing
6-Jun-2011,
palin

joplin

sarah

Relevance Driven
5-Jul-2011, casey anthony trial
summary
9-Jul-2011, nascar qualifying re-
sults
8-Jul-2011, burbank 100 years
parade
10-Jul-2011 gas prices summer
2011
10-Jul-2011, bafta ﬁlm awards
2011
2-Jul-2011, green lantern cast
9-Jul-2011,
leaderboard
3-Jul-2011, lake mead water level
july 2011
5-Jul-2011, caylee anthony au-
topsy report
4-Jul-2011, aurora colorado ﬁre-
works 2011

2011 usga

open

At the ﬁrst glance, it may be surprising to notice that most
of the top ranked freshness-driven queries are the name of
some countries and celebrities, e.g., “iran”, “libya” and “le-
bron james”. But that is also quite reasonable: for those kind
of queries, they are actually ambiguous, since there would
be many candidate news reports related to diﬀerent aspects
of these queries. But when users issue such type of “am-
biguous” queries in news search engine, they should be most
interested in the recent updates about these countries and
celebrities. We went back to check the most clicked URLs of
these queries, and the clicks conﬁrmed our assumption: the
most clicked URLs for query “libya” were about the recent
progress of libya war; and news articles covering the latest
diplomatic aﬀairs between U.S. and Iran got most clicks for
the query “iran”.

Table 7: Query length distribution under diﬀerent
query categories

Freshness Driven Relevance Driven
1.446 ± 0.804
3.396 ± 1.309

ALL

2.563 ± 1.203

Another interesting ﬁnding from the learned query weights
is that the length of relevance-driven queries is much longer
than the freshness-driven queries. To validate this obser-
vation, we selected the top 500 relevance-driven and top
500 freshness-driven queries to calculate the corresponding
query length distributions comparing to the length distri-
bution of all the queries, and showed the results in Table
7. This result is consistent with our intuition: when users

WWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France586are seeking speciﬁc information, they tend to put more con-
straints (i.e., longer queries) to describe their information
need; in contrast, when users are making recency search,
they usually do not have a pre-determined mind about
the events, so they often issue broad queries (i.e., shorter
queries) about entities of their interest to see what is hap-
pening recently to those entities. Apparently, our query
weight model is consistent with this intuition, and is able
to distinguish these two typical searching scenarios. In ad-
dition, this also reminds us query length is a good feature
to indicate the preference over freshness aspect.
4.3 Ranking Performance

To validate the eﬀectiveness of the proposed JRFL model
in real news search tasks, we quantitatively compare it with
all our baseline methods on: random bucket clicks, normal
clicks, and editorial judgments. All the click-based learning
algorithms are trained on all 150K click preferences. Since
all these models have several parameters to be tuned (e.g.,
the trade-oﬀ parameter C in both RankSVM and JRFL), we
report their best performance on the corresponding testing
set according to MAP@4 metric in the following results and
perform t-test to validate the signiﬁcance of improvement
(against the second best performance accordingly).

First, we performed the comparison on the random bucket
clicks, because such clicks are more trustable than normal
clicks due to the removal of positional biases.

Table 8: Comparison On Random Bucket Clicks

Model FreshDem RankSVM GBRank

JRFL

P@1
P@2
MAP@3
MAP@4
MRR

0.3413
0.3140
0.5301
0.5859
0.5899

0.3706
0.3372
0.5601
0.6090
0.6135

0.3882
0.3477
0.5751
0.6218
0.6261

0.3969*
0.3614*
0.6012*
0.6584*
0.6335*

* indicates p-value<0.05.

From the results in Table 8, we can ﬁnd the proposed
JRFL model achieves encouraging improvement over the sec-
ond best GBRank model, especially on MAP@4 the relative
improvement is over 5.88%. This improvement conﬁrms that
properly integrating relevance and freshness can indeed im-
prove the user’s search satisfaction.

Table 9: Comparison On Normal Clicks

Model FreshDem RankSVM GBRank

JRFL

P@1
P@2
MAP@3
MAP@4
MRR

0.3886
0.2924
0.4991
0.5245
0.5781

0.5981
0.4166
0.7208
0.7383
0.7553

0.5896
0.4002
0.6849
0.7024
0.7355

0.6164*
0.4404*
0.7502*
0.7631*
0.7702*

* indicates p-value<0.05

model: JRFL’s improvement on P@1 against the freshness-
demotion-trained GBRank model is 16.2% and 58.6% on the
random bucket click set and normal click set respectively.
We have already analyzed the reason for this degenerated
results in Figure 1 that a static grade demotion strategy can
hardly guide the learning algorithm to achieve the optimal
ranking results.

On the editorial annotation data set, to compare diﬀerent
model’s performance, we mapped the separately annotated
relevance and freshness grades into one single grade by the
freshness demotion strategy in Dong et al.’s work [8].

Table 10: Comparison On Editorial Annotations

Model FreshDem RankSVM GBRank

JRFL

P@1
P@2
MAP@3
MAP@4
MRR
DCG@1
DCG@5

0.9184
0.9043
0.3055
0.4049
0.9433
6.8975
15.7175

0.9626
0.9649
0.3628
0.4701
0.9783
7.9245
17.2279

0.9870
0.9729
0.3731
0.4796
0.9920
8.1712*
17.7468

0.9508
0.9117
0.4137
0.4742
0.9745
7.2203
18.9397*

* indicates p-value<0.05.

From this result, we ﬁnd that the freshness-demotion-
trained GBRank model did not achieve the best performance
on such “grade demoted” testing set either. This might be
caused by the time gap between diﬀerent annotations: [8]’s
annotations were generated more than one year ago (Febru-
ary to May, 2009). The out-of-date annotation might con-
tain inconsistent relations between queries and document as
in the new annotations. Besides, we also notice that the
margin of improvement from JRFL becomes smaller com-
paring to the click-based evaluations. In the following, we
perform some case studies to ﬁnd out the reasons for the
diminished improvement.

In Table 11, we illustrate one case of inferior ranking re-
sult for the query “afghanistan” from JRFL. We list the top
4 ranked results from JRFL together with the editorial rel-
evance and freshness grades. (We have to truncate some of
the URLs because they are too long to be displayed.)

Table 11: Case Study: Degenerated ranking results
by JRFL for query “afghanistan”

URL

http://www.cbsnews.com/video/
watch/?id=7376057nXXX
http://news.yahoo.com/afghanistan-
helicopter-crash-why-army-used-
chinook-half-180000528.html
http://news.yahoo.com/whatever-
happened-civilian-surge-
afghanistan-035607164.html
http://www.msnbc.msn.com/id/
44055633/ns/world news-
south and central asia/XXX

Relevance Freshness

Good

Excellent

Excellent

Excellent

Excellent

Excellent

Perfect

Excellent

Now we perform the same comparison on the normal click
set, and we can observe similar improvement from JRFL
over other ranking methods as shown in Table 9. Besides,
from the results on both of these two click data sets, we can
clearly observe that the click-preference-trained models gen-
erally outperform the freshness-demotion-trained GBRank

The freshness weight inferred by JRFL for this query is
0.7694, which is biased to freshness aspect. However, all
those URLs’ freshness grades are “Excellent”, so that in the
demoted ﬁnal grades, the “ground-truth” ranking only de-
pends on the relevance aspect. The predicted relevance score

WWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France587diﬀerences get diminished by this biased freshness weight
in JRFL: the JRFL predicted relevance score diﬀerence be-
tween the best document in “ground-truth” (last row in the
table) versus JRFL ordering (ﬁrst row in the table) is 0.44
while the corresponding freshness score diﬀerence is -0.31.
As a result, JRFL gives an arguably “bad” ranking result
for this query.

In addition, we want to revisit the relationship between
the predicted orders given by JRFL and CTR as we have
done in Figure 2. This time, we draw the scatter plot be-
tween the JRFL predicted ranking scores and CTRs on the
same set of URLs as shown in Figure 2.

The monotonic relationship between the predicted rank-
ing and CTRs is much more evident than the one given by
the demoted grades: URLs with lower CTRs concentrate
more densely in the area with lower prediction scores, and
the average Pearson correlation between the predicted rank-
ing score and CTR across all the queries is 0.7163 with stan-
dard deviation 0.1673, comparing to the average of 0.5764
and standard deviation of 0.6401 in the the demoted grades.

Figure 6: Scatter plot of CTR versus JRFL’s pre-
diction.

5. CONCLUSIONS

In this work, we proposed a joint learning framework,
Joint Relevance Freshness Learning, for modeling the topical
relevance and freshness, and the query-speciﬁc relative pref-
erence between these two aspects based on the clickthroughs
for the news search task. Experiments on large-scale query
logs and editorial annotations validate the eﬀectiveness of
the proposed learning method.

In this paper, we only instantiate the proposed joint learn-
ing framework by linear models, but many alternatives ex-
ist. It would be meaningful to employ other types of non-
linear functions to enhance JRFL. For example, using Logis-
tic functions can naturally avoid the range constrains over
query weights in optimization.

Besides, in our current setting, the preference between rel-
evance and freshness is assumed to be only query-dependent.
It would be interesting to extend this to user-dependent, i.e.,
personalized search. By deﬁning a proper set of user-related
features, or proﬁles, the proposed JRFL can be easily ap-
plied in such user-centric retrieval environment. What is
more, the proposed model can also be ﬂexibly extended to
other retrieval scenarios, where usefulness judgment is be-
yond pure topical relevance, such as opinions in blog search
and distance in location search.

6. REFERENCES
[1] D. Agarwal, B.-C. Chen, P. Elango, and X. Wang. Click
shaping to optimize multiple objectives. In KDD, 2011.

[2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning

user interaction models for predicting web search result
preferences. In SIGIR, 2006.

[3] R. Baeza-Yates, B. Ribeiro-Neto, et al. Modern information

retrieval, volume 463. ACM press New York, 1999.
[4] S. Brin and L. Page. The anatomy of a large-scale

hypertextual web search engine. Computer networks and
ISDN systems, 30(1-7):107–117, 1998.

[5] Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. Learning to

rank: from pairwise approach to listwise approach. In
ICML, pages 129–136. ACM, 2007.

[6] H. Chernoﬀ and E. Lehmann. The use of maximum

likelihood estimates in (cid:31)2 tests for goodness of ﬁt. The
Annals of Mathematical Statistics, pages 579–586, 1954.

[7] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank
for freshness and relevance. In SIGIR, pages 95–104, 2011.

[8] A. Dong, Y. Chang, Z. Zheng, G. Mishne, J. Bai, R. Zhang,
K. Buchner, C. Liao, and F. Diaz. Towards recency ranking
in web search. In WSDM, pages 11–20, 2010.

[9] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz, Y. Chang,
Z. Zheng, and H. Zha. Time is of the essence: improving
recency ranking using twitter data. In WWW, 2010.

[10] M. Efron and G. Golovchinsky. Estimation methods for

ranking recent information. In SIGIR, pages 495–504, 2011.

[11] H. Fang, T. Tao, and C. Zhai. A formal study of
information retrieval heuristics. In SIGIR, 2004.

[12] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM TOIS, 20(4):422–446,
2002.

[13] T. Joachims. Optimizing search engines using clickthrough

data. In KDD, pages 133–142, 2002.

[14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In SIGIR, pages 154–161, 2005.

[15] K. Jones, S. Walker, and S. Robertson. A probabilistic

model of information retrieval: development and
comparative experiments. Information Processing and
Management, 36(6):779–808, 2000.

[16] N. Kanhabua and K. Nørv˚ag. Determining time of queries

for re-ranking search results. Research and Advanced
Technology for Digital Libraries, pages 261–272, 2010.

[17] A. Kulkarni, J. Teevan, K. Svore, and S. Dumais.

Understanding temporal query dynamics. In WSDM, 2011.
[18] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased oﬄine

evaluation of contextual-bandit-based news article
recommendation algorithms. In Proceedings of the fourth
ACM WSDM ’11, pages 297–306, 2011.

[19] X. Li and W. Croft. Time-based language models. In

CIKM, pages 469–475, 2003.

[20] T. Liu. Learning to rank for information retrieval.
Foundations and Trends in Information Retrieval,
3(3):225–331, 2009.

[21] T. Moon, L. Li, W. Chu, C. Liao, Z. Zheng, and Y. Chang.

Online learning for recency search ranking using real-time
user feedback. In CIKM, pages 1501–1504, 2010.

[22] G. Salton and M. McGill. Introduction to modern

information retrieval. McGraw-Hill, Inc., 1986.

[23] K. M. Svore, M. N. Volkovs, and C. J. Burges. Learning to

rank with multiple objective functions. In WWW, 2011.

[24] C. Zhai and J. Laﬀerty. A study of smoothing methods for
language models applied to ad hoc information retrieval. In
SIGIR, pages 334–342, 2001.

[25] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression

framework for learning ranking functions using relative
relevance judgments. In SIGIR, pages 287–294, 2007.

−1−0.500.5100.20.40.60.81JRFL Ranking ScoreCTRCTR v.s. JRFL Ranking ScoreWWW 2012 – Session: Leveraging User Actions in SearchApril 16–20, 2012, Lyon, France588