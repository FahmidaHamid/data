Serf and Turf: Crowdturﬁng for Fun and Proﬁt

Gang Wang†, Christo Wilson†, Xiaohan Zhao†, Yibo Zhu†, Manish Mohanlal†,

Haitao Zheng‡† and Ben Y. Zhao‡†

†Computer Science, UC Santa Barbara, Santa Barbara, CA, USA

‡Division of Web Science Technology, KAIST, Korea

{gangw, bowlin, xiaohanzhao, yibo, manish, htzheng, ravenben}@cs.ucsb.edu

ABSTRACT
Popular Internet services in recent years have shown that remark-
able things can be achieved by harnessing the power of the masses
using crowd-sourcing systems. However, crowd-sourcing systems
can also pose a real challenge to existing security mechanisms de-
ployed to protect Internet services. Many of these security tech-
niques rely on the assumption that malicious activity is generated
automatically by automated programs. Thus they would perform
poorly or be easily bypassed when attacks are generated by real
users working in a crowd-sourcing system. Through measurements,
we have found surprising evidence showing that not only do mali-
cious crowd-sourcing systems exist, but they are rapidly growing
in both user base and total revenue. We describe in this paper a sig-
niﬁcant effort to study and understand these crowdturﬁng systems
in today’s Internet. We use detailed crawls to extract data about the
size and operational structure of these crowdturﬁng systems. We
analyze details of campaigns offered and performed in these sites,
and evaluate their end-to-end effectiveness by running active, be-
nign campaigns of our own. Finally, we study and compare the
source of workers on crowdturﬁng sites in different countries. Our
results suggest that campaigns on these systems are highly effec-
tive at reaching users, and their continuing growth poses a concrete
threat to online communities both in the US and elsewhere.

Categories and Subject Descriptors
H.3.5 [Information Storage and Retrieval]: Online Information
Services-Web-based services; J.4 [Computer Applications]: So-
cial and Behavioral Sciences

General Terms
Measurement, Security, Economics

Keywords
Crowdturﬁng, Crowdsourcing, Spam, Sybils, Experimentation

1.

INTRODUCTION

Popular Internet services in recent years have shown that re-
markable things can be achieved by harnessing the power of the
masses. By distributing tasks or questions to large numbers of In-
ternet users, these “crowd-sourcing” systems have done everything
from answering user questions (Quora), to translating books, creat-
ing 3-D photo tours [29], and predicting the behavior of stock mar-

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

kets and movie grosses. Online services like Amazon’s Mechanical
Turk, Rent-a-Coder (vWorker), Freelancer, and Innocentive have
created open platforms to connect people with jobs and workers
willing to perform them for various levels of compensation.

On the other hand, crowd-sourcing systems could pose a serious
challenge to a number of security mechanisms deployed to pro-
tect Internet services against automated scripts. For example, elec-
tronic marketplaces want to prevent scripts from automating auc-
tion bids [23], and online social networks (OSNs) want to detect
and remove fake users (Sybils) that spread spam [32, 34]. Detec-
tion techniques include different types of CAPTCHAs, as well as
machine-learning that tries to detect abnormal user behavior [10],
e.g. near-instantaneous responses to messages or highly bursty user
events. Regardless of the speciﬁc technique used, they rely on a
common assumption, that the malicious tasks in question cannot
be performed by real humans en masse. This is an assumption that
is easily broken by crowd-sourcing systems dedicated to organizing
works to perform malicious tasks.

Through measurements, we have found surprising evidence show-
ing that not only do malicious crowd-sourcing systems exist, but
they are rapidly growing in both user base and revenue generated.
Because of their similarity with both traditional crowd-sourcing
systems and astroturﬁng behavior, we refer to them as crowdturf-
ing systems. More speciﬁcally, we deﬁne crowdturﬁng systems
as systems where customers initiate “campaigns,” and a signiﬁcant
number of users obtain ﬁnancial compensation in exchange for per-
forming simple “tasks” that go against accepted user policies.

In this paper, we describe a signiﬁcant effort to study and under-
stand crowdturﬁng systems in today’s Internet. We found signif-
icant evidence of these systems in a number of countries, includ-
ing the US and India, but focus our study on two of the largest
crowdturﬁng systems with readily available data, both of which are
hosted in and targeted users in China. From anecdotal evidence,
we learn that these systems are well-known to young Internet users
in China, and have persisted despite threats from law enforcement
agencies to shut them down [5, 9, 20].

Our study results in four key ﬁndings on the operation and effec-
tiveness of crowdturﬁng systems. First, we used detailed crawls to
extract data about the size and operational structure of these crowd-
turﬁng systems. We use readily available data to quantify both tasks
and revenue ﬂowing through these systems, and observe that these
sites are growing exponentially in both metrics. Second, we study
the types of tasks offered and performed in these sites, which in-
clude mass account creation, and posting of speciﬁc content on
OSNs, microblogs, blogs, and online forums. Tasks often ask users
to post advertisements and positive comments about websites along
with an URL. We perform detailed analysis of tasks trying to start

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France6791

2

Customer

4
Agent

3

Worker

5

6

Work Flow

Cash Flow

1. Input task & money
2. Distribute jobs
3. Finish jobs
4. Submit report
5. Evalute jobs
6. Make payment

Customer

Agent

Customer

Agent

Leader

Leader

Leader

Workers

Mailing List / IM Group
(a)Distributed Structure

Workers

Public Web Service

(b) Centralized Structure

Figure 1: Work and cash ﬂow of a crowdturﬁng campaign.

Figure 2: Two different crowdturﬁng system structures.

information cascades on microblogging sites, and study the effec-
tiveness of cascades as a function of the microblog social graph.

Third, we want to evaluate the end-to-end effectiveness of crowd-
turﬁng campaigns. To do so, we created accounts on one of our
target systems, and initiated a number of benign campaigns that
provide unsolicited advertisements for legitimate businesses. By
bouncing clicks through our redirection server, we log responses to
advertisements generated by our campaigns, allowing us to quan-
tify their effectiveness. Our data shows that crowdturﬁng cam-
paigns can be cost-effective at soliciting real user responses. Fi-
nally, we study and compare the source of workers on crowdturﬁng
sites in different countries. We ﬁnd that crowdturﬁng workers eas-
ily cross national borders, and workers in less-developed countries
often get paid through global payment services for performing tasks
affecting US-based networks. This suggests that the continuing
growth of crowdturﬁng systems poses a real threat to U.S.-based
online communities such as Facebook, Twitter, and Google+.

This study is the one of the ﬁrst to examine the organization and
effectiveness of large-scale crowdturﬁng systems on the Internet.
These systems have already established roots in other countries,
and are responsible for producing fake social network accounts that
look indistinguishable from those of real users [34]. A recent study
shows that similar types of behavior are also on the rise in the US-
based Freelancer site [24]. Understanding the operation of these
systems from both ﬁnancial and technical angles is the ﬁrst step to
developing effective defenses to protect today’s online social net-
works and online communities.

2. CROWDTURFING OVERVIEW

In this section, we introduce the core concepts related to crowd-
turﬁng. We start by deﬁning crowdturﬁng and the key players in
a crowdturﬁng campaign. Next, we present two different types of
systems that are used to effect crowdturﬁng campaigns on the In-
ternet: distributed and centralized. Measurements of a distributed
crowdturﬁng system show that it is signiﬁcantly less popular with
users than centralized systems. Thus we focus on understanding
centralized crowdturﬁng systems in the remainder of our paper.

2.1 Introduction to Crowdturﬁng

The term crowdturﬁng is a portmanteau of “crowd-sourcing”
and “astroturﬁng.” Astroturﬁng refers to information dissemina-
tion campaigns that are sponsored by an organization, but are ob-
fuscated so as to appear like spontaneous, decentralized “grass-
roots” movements. Astroturﬁng campaigns often involve spreading
legally grey, or even illegal, content, such as defamatory rumors,
false advertising, or suspect political messages. Although astro-
turﬁng predates the Internet, the ability to quickly mobilize large
groups via crowd-sourcing systems has drastically increased the
power of astroturﬁng. We refer to this combined threat as crowd-
turﬁng. Because of its use of real human users, crowdturﬁng poses

an immediate threat to existing security measures that protect on-
line communities by targeting automated scripts and bots.

Crowdturﬁng campaigns on the Internet involve three key actors:

1. Customers: Individuals or companies who initiate a crowd-
turﬁng campaign. The customer is responsible for paying for
the monetary costs, and are typically are either related to or
themselves the beneﬁciaries of the campaign.

2. Agents: Intermediaries who take charge of campaign plan-
ning and management. The agent is responsible for ﬁnding,
managing, and distributing funds to workers to accomplish
the goals of the campaign.

3. Workers: Internet users who answer calls by agents to per-

form speciﬁc tasks in exchange for a fee.

Each campaign is structured as a collection of tasks. For ex-
ample, a campaign might entail generating positive sentiment for
a new restaurant. In this case, each task would be “post a single
(fake) positive restaurant review online.” Workers who complete
tasks generate submissions that include evidence of their work. The
customer/agent can then verify that the work was done to their sat-
isfaction. In the case of the restaurant review campaign, submis-
sions are screenshots of or URLs pointing to the fake reviews. Ide-
ally, there is a one-to-one mapping between tasks and submissions.
However, not all tasks may be completed, and submissions may be
rejected due to lack of quality. In these cases, the number of sub-
missions will not match the number of tasks for a given campaign.
The process for a crowdturﬁng campaign is shown in Figure 1.
Initially, a customer brings the campaign to an agent and pays them
to carry it out (1). The agent distributes individual tasks among a
pool of workers (2), who complete the tasks and return submissions
back to the agent (3). The agent passes the submissions back to the
customer (4), who evaluates the work. If the customer is satisﬁed
they inform the agent (5), who then pays the workers (6).

2.2 Crowdturﬁng Systems

Crowdturﬁng systems are instances of infrastructure used to con-
nect customers, agents, and workers to enable crowdturﬁng cam-
paigns. These systems are generally created and maintained by
agents, and help to streamline the process of organizing workers,
verifying their work, and distributing payments.

We have observed two different types of crowdturﬁng systems
in the wild: distributed and centralized. We now describe the dif-
ferences between these two structures, highlighting their respec-
tive strengths and weaknesses. Crowdturﬁng systems are similar to
crowd-sourcing systems like Amazon’s Mechanical Turk, with the
exception that they accept tasks that are unethical or illegal, and
that they can utilize distributed infrastructures.
Distributed Architecture.
Distributed crowdturﬁng systems
are organized around small instant message (IM) groups, mailing
lists, or chat rooms hosted by group leaders. As illustrated in Fig-

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France680ure 2a, leaders act as middlemen between agents and workers, and
organizes the workers.

The advantage of distributed crowdturﬁng systems is that they
are resistant to external threats, like law-enforcement. Individual
forums and mailing lists are difﬁcult to locate, and they can be dis-
solved and reconstituted elsewhere at any time. Furthermore, sen-
sitive communications, such as payment transfers, occur via private
channels directly between leaders and workers, and thus cannot be
observed by third parties.

However, there are two disadvantages to distributed systems that
limit their popularity. The ﬁrst is lack of accountability. Dis-
tributed systems do not have robust reputation metrics, leaving cus-
tomers with little assurance that work will be performed satisfac-
torily, and workers with no guarantees of getting paid. The sec-
ond disadvantage stems from the fragmented nature of distributed
systems. Prospective workers must locate groups before they can
accept jobs, which acts as a barrier-of-entry for many users. To
test this, we located 14 crowdturﬁng groups in China hosted on
the popular Tencent QQ instant messaging network. Despite the
fact that these groups were well advertised on popular forums, they
only hosted ≈2K total users. Over the course of several days of
observation, each group only generated 28 messages per day on
average, most of which was idle chatter. The conclusion we can
draw from these measurements is that distributed crowdturﬁng sys-
tems are not very successful at attracting workers. As we will show
in Section 3, centralized systems attract orders of magnitude more
campaigns and workers.
Centralized Architecture.
Centralized crowdturﬁng systems,
illustrated in Figure 2b, are instantiated as websites that directly
connect customers and workers. Much like Amazon’s Mechanical
Turk, customers post campaigns and offer rewards, while workers
sign up to complete tasks and collect payments. Both customer and
workers register bank information associated with their accounts,
and all transactions are processed through the website. Centralized
crowdturﬁng websites use reputation and punishment systems to
incentivize customer and workers to behave properly. The primary
role of the agent in centralized architectures is simply to maintain
the website, although they may also perform veriﬁcation of sub-
missions at the behest of customers.

The advantage of centralized crowdturﬁng systems is their sim-
plicity. There are a small number of these large, public websites,
making them trivial to locate by customers and workers. Central-
ized software automates campaign management, payment distribu-
tion, and maintains per-worker reputation scores. These features
streamline centralized crowdturﬁng systems, and reduce uncertain-
ties for all involved parties.

The disadvantage of centralized crowdturﬁng systems is their
susceptibility to scrutiny by third parties. Since these public sites
allow anyone to sign up, they are easy targets for inﬁltration, which
may be problematic for crowdturﬁng sites that operate in legally
grey-areas. On the other hand, this disadvantage made it possible
for us to crawl and analyze several large crowdturﬁng websites.

3. CAMPAIGNS, TASKS, AND REVENUE

We begin our analysis of crowdturﬁng systems, by analyzing the
volume of campaigns, tasks, users, and total revenue processed by
the largest known systems. We ﬁrst describe the representative sys-
tems in our study along with our data gathering methodology. We
then present detailed results addressing these questions.
3.1 Data Collection and General Statistics

While a number of crowdsurﬁng systems operate across the global
Internet, the two largest and most representative systems are hosted

on Chinese networks. Their popularity is explained by the fact that
China has both the world’s largest Internet population (485M) [25]
and a moderately low per-capita income (≈$3,200/year) [21]. Crowd-
turﬁng sites in China connect dodgy PR ﬁrms to a large online user
population willing to act as crowd-sourced labor, and have been
used to spread false rumors and advertising [4, 20, 5]. This “Shui
Jun” (water army), as it is commonly known, has emerged as a
force on the Chinese Internet that authorities are only beginning to
grapple with [9, 2].

This conﬂuence of factors makes China an ideal place to study
crowdturﬁng. In this section, we measure and characterize the two
largest crowdturﬁng websites in China: Zhubajie (ZBJ, zhubajie.
com) and Sandaha (SDH, sandaha.com). All data on these sites
are public, and we were able to gather all data on their current and
past tasks via periodic crawls of their campaign histories.
Zhubajie and Sandaha.
The ﬁrst site we crawled is Zhubajie
(ZBJ), which is the largest crowd-sourcing website in China. As
shown in Table 1, ZBJ has been active for ﬁve years, and is well
established in the Chinese market. Customers post many different
legitimate types of jobs to ZBJ, including requests for freelance
design and programming, as well as Mechanical Turk-style “human
intelligence tasks.” However, there is a subsection of ZBJ called
“Internet Marketing” that is dedicated solely to crowdturﬁng. ZBJ
also has an English-language version hosted in Texas (witmart.
com), but its crowdturﬁng subsection only has 3 campaigns to date.
Unlike ZBJ, Sandaha (SDH) only provides crowdturﬁng services,
and is four years younger than ZBJ.
Crawling Methodology. We crawled ZBJ and SDH in Septem-
ber, 2011 to gather data for this study. We crawled SDH in its en-
tirety, but only crawled the crowdturﬁng section of ZBJ. Both sites
are structured similarly, starting with a main page that links to a
paginated list of campaigns, ordered reverse chronologically. Each
campaign has its own page that gives pertinent information, along
with links to another paginated list of completed submissions from
workers. All information on both sites is publicly available, and
neither site employs security measures to prevent crawling.

Our crawler recorded details of all campaigns and submissions
on ZBJ and SDH. Campaigns are characterized by a description,
start and end times, total number of tasks, total money available to
pay workers, whether the campaign is completed, and the number
of accepted and rejected submissions. It also includes details for
each submission entered by workers, including the worker user-
name and UID, a submission timestamp, one or more screenshots
and/or URLs pointing to content generated by the worker, and a ﬂag
marking the submission as either accepted or rejected after review.
Both ZBJ and SDH make the complete history of campaigns
available on their sites, which enables the crawler to collect data
dating back to each site’s inception. Table 1 lists the total number
of campaigns on each site, as well as the percentage that were us-
able for our study. Data on some campaigns is incomplete because
the customer deleted them or made them private. Other data could
be missing because either the campaign only provided partial infor-
mation (e.g. no task count or price per task), or the campaign was
still ongoing at the time of our crawl. Incomplete campaigns only
account for 8% of the total on ZBJ and 12% on SDH, and thus have
little impact on our overall results. For clarity, we convert all cur-
rency values on ZBJ and SDH (Chinese Yuan) to US Dollars using
an exchange rate of 0.1543 to 1.
General Statistics.
Table 1 shows the high-level results from
our crawls. ZBJ is older and more well-established than SDH,
hence it has attracted more campaigns, workers, and money. Cam-
paigns on both sites each include many individual tasks, and task

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France681Website

Active
Since

Zhubajie (ZBJ)
Nov. 2006
Sandaha (SDH) March 2010

Total

Campaigns (%)

76K (92%)
3K (88%)

Total

Workers
169K
11K

Total
Tasks
17.4M
1.1M

Total

Submissions (%)

6.3M (36%)
1.4M (130%)

Total

Accepted (%)
3.5M (56%)
751K (55%)

Total
Money
$3.0M
$161K

Money

for Workers

$2.4M
$129K

Money for
Website (%)
$595K (20%)
$32K (20%)

Table 1: General information for two large crowdturﬁng websites.

h
t
n
o
M

 
r
e
p
 
s
n
g
a
p
m
a
C

i

104

103

102

101

100

C(ZBJ)

$(ZBJ)

$(SDH)

C(SDH)

106
105
104
103
102
101

h
t
n
o
M

 
r
e
p
 
s
r
a

l
l

o
D

2

0

2

2

0

0

2

0

2

2

0

0

2

2

0

0

2

2

0

0

0

0

0

0

0

0

1

1

1

1

7
-
1

8
-
3

8
-
8

9
-
1

9
-
6

9
-
1

0
-
4

0
-
9

1
-
2

1
-
7

0

1

F
D
C

 100

 80

 60

 40

 20

 0

Acpt. ZBJ
Subm. ZBJ
Task ZBJ
Acpt. SDH
Subm. SDH
Task SDH

 1

 10

 100

 1000

Tasks/Submissions per Campaign

F
D
C

 100

 80

 60

 40

 20

 0

 1

SDH
ZBJ
 1000
 10
Submissions per Worker

 100

 10000

Figure 3: Campaigns, dollars per month.

Figure 4: Tasks, submissions per camp.

Figure 5: Submissions per worker.

count is almost three orders of magnitude greater than number of
campaigns. The number of submissions generated by workers in
response to tasks is highly variable: on ZBJ only 36% of tasks
receive submissions, whereas on SDH 130% of tasks receive sub-
missions (i.e. there is competition among workers to complete the
same tasks). Roughly 50% of all submissions are accepted.

Most importantly, more than $4 million dollars have been spent
on crowdturﬁng on ZBJ and SDH in the past ﬁve years. Both sites
take a 20% cut of campaign dollars as a fee, resulting in signif-
icant proﬁts for ZBJ, due to its high volume of campaigns. Fur-
thermore, Figure 3 shows that the number of campaigns and total
money spent are growing exponentially. The younger SDH has a
growth trend that mirrors ZBJ, suggesting that it will reach similar
levels of proﬁtability within the next year. These trends indicate
the rising popularity of crowdsurﬁng systems, and foreshadow the
potential impact these systems will have in the very near future.

3.2 Campaigns, Tasks, and Workers

Figure 4 illustrates the high level breakdown of tasks and sub-
missions on ZBJ and SDH. There are three lines corresponding to
each site: tasks per campaign, submissions per campaign, and ac-
cepted submissions per campaign. Campaigns on ZBJ tend to have
an order of magnitude fewer tasks than those on SDH. Although
both sites only accept ≈50% of submissions, the overabundance
of submissions on SDH means that the number of accepted sub-
missions closely tracks the required number of tasks, especially for
campaigns with >100 tasks.
Campaign Types.
Crowdturﬁng campaigns on ZBJ and SDH
can be divided into several categories, with the ﬁve most popular
listed in Table 2. These ﬁve campaign types account for 88% of all
campaigns on ZBJ, and 91% on SDH.

“Account registration” refers to the creation of user accounts on a
target website. Unlike what has been observed by prior work [24],
these accounts are almost never used to automate the process of
spamming. Instead, customers request this service to bolster the
popularity of ﬂedgling websites and online games, in order to make
them appear well trafﬁcked.

Four campaign types refer to spamming in speciﬁc contexts: QQ
instant-message groups, forums, blogs, and microblogs (e.g. Twit-
ter). Customers in China prefer to pay workers directly to gener-
ate content on popular websites, rather than purchasing accounts
from workers and spamming through them. Note, that QQ and
forums represent a larger percentage of campaigns because their
existence predates microblogs, which have only become popular

Campaign

Type

Account Reg.
Forum Post
QQ Group
Microblog
Blog Post
Forum Post
QQ Group

Q&A

Blog Post
Microblog

Num of

Campaigns
29,413 (39%)
17,753 (23%)
12, 969 (17%)

4,061 (5%)
3,067 (4%)
1,928 (57%)
473 (14%)
463 (14%)
113 (3%)
93 (3%)

$/Camp.

$71
$16
$15
$12
$12
$48
$48
$47
$49
$49

$/Task Monthly
Growth
16%
$0.35
19%
$0.27
$0.70
17%
47%
$0.18
20%
$0.23
40%
$0.19
31%
$0.13
30%
$0.21
$0.19
21%
42%
$0.27

ZBJ

SDH

Table 2: The top ﬁve campaign types on ZBJ and SDH.

in China in the last year [25]. The last column of Table 2 shows
the average monthly growth in number of campaigns, and shows
that microblogs campaigns are growing faster than all other top-5
categories in both ZBJ and SDH. As the popularity of social net-
works and microblogs continues to grow, we expect to see more
campaigns targeting them.

Finally, “Q&A” involves posting and answering questions on so-
cial Q&A sites like Quora (quora.com). Workers are expected
to answer product-related questions in a biased manner, and in
some cases post dummy questions that are immediately answered
by other colluding workers.
Worker Characteristics.
We now focus our discussion on
the behavior of workers on crowdturﬁng websites. Figure 5 shows
that the total number of submissions per worker (including both
accepted and rejected submissions) varies across the worker pop-
ulation, and even between ZBJ and SDH. Roughly 40% of SDH
workers only complete a single task, compared to 20% on ZBJ.
The average worker on both sites complete around 5-7 tasks each.
Figure 5 also reveals that a small percentage of extremely proliﬁc
workers (especially on SDH) generate hundreds, even thousands,
of submissions. Figure 6 plots the percentage of submissions from
top workers ordered from most to least proliﬁc. The distribution
is highly skewed in favor of these career crowdturfers, who are
responsible for generating ≈75% of submissions.

We now examine the temporal aspects of worker behavior. Fig-
ure 7 plots the time difference between a campaign getting posted
online, and the ﬁrst submission from a worker. On SDH, 50% of
campaigns become active within 24 hours, whereas on ZBJ (with
its larger worker population) 75% of campaigns become active within
24 hours. However, some campaigns take signiﬁcantly longer to
ramp up: up to 15 days on ZBJ, and 30 days on SDH. As we discuss

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France682i

s
n
o
s
s
m
b
u
S

i

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

SDH
ZBJ

 0  10  20  30  40  50  60  70  80  90  100

Top Workers (%) 

i

s
n
o
s
s
m
b
u
S

i

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

 0

 5

 10

 15

ZBJ
SDH
 20

 25

 30

Time (Days)

)

%

i

(
 
s
n
o
s
s
m
b
u
S

i

 10

 8

 6

 4

 2

 0

ZBJ
SDH

0

2

4

6

8 10 12 14 16 18 20 22
Hours in the Day

Figure 6: Submissions by top workers.

Figure 7: Time to ﬁrst response.

Figure 8: Daily submissions.

 100

 80

 60

 40

 20

F
D
C

 0
 0.01

SDH
ZBJ

 0.1

 1

 10

 100

 1000

Price per Submission ($)

 100

 80

 60

 40

 20

F
D
C

 0

 0.1

y
e
n
o
M

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

SDH
ZBJ

 0  10  20  30  40  50  60  70  80  90  100

Top Workers (%) 

SDH
ZBJ

 10

 1
Total Money per Worker ($)

 100

 1000  10000

Figure 9: Submission prices.

Figure 10: Money per worker.

Figure 11: Money earned by top workers.

in Section 3.3, these slow moving campaigns have very speciﬁc re-
quirements that cannot be met by the vast majority of workers.

Figure 8 shows the correlation between time of day and number
of submissions on ZBJ and SDH. Most submissions happen during
the workday and in the evening. Slight drops around lunch and
dinner are also visible. This pattern conﬁrms that submissions are
generated by human beings, and not automated bots.

3.3 Money

We now explore the monetary reward component of crowdturf-
ing systems. As is common on crowd-sourcing systems like Me-
chanical Turk, workers on ZBJ and SDH make a tiny fee for each
accepted submission. As shown in Figure 9, the vast majority of
workers on ZBJ and SDH earn $0.11 per submission, although
≈20% of submissions command higher prices than this. Workers
must complete many submissions in order to earn substantial pay,
leading to the proliﬁc submission habits of career crowdturfers seen
in Figure 6. Note that this is a very different model from bid-for-
tasks systems like the recent Freelancer study [24].

The total amount of money earned by most workers on ZBJ and
SDH is very small. As illustrated in Figure 10, close to 70% of
workers earn less than $1 for their efforts. The remaining 30% of
workers earn between $1 and $100, making crowdturﬁng a poten-
tially rewarding part-time job to supplement their core income. For
a very small group of workers (0.4%), crowdturﬁng is a full-time
job, earning rewards in the $1,000 dollar range. Not surprisingly,
the distribution of monetary rewards matches this distribution. As
seen in Figure 11, the top 5% of workers take home 80% of the
proceeds on ZBJ and SDH. Clearly, a hard-core contingent of ca-
reer crowdturfers is taking the bulk of the reward money by quickly
completing many submissions.
Task Pricing.
The goal and budget of each crowdturﬁng cam-
paign affects the number and price of tasks in that campaign. Fig-
ure 12 plots the correlation between the number of tasks in a cam-
paign, versus the price per submission the customer is willing to
pay. The vast majority of campaigns with 1K-10K tasks call for
generating numerous “tweets” on microblog sites. We examine
these tasks in more detail in Section 4.

Although the vast majority of campaigns call for many tasks with
low price per submission, Figure 12 reveals that there is a small

minority of well paying tasks.
In many cases, these campaigns
only include a single task that can earn an accepted submission
≥$100 dollars. We examined the 158 outlying tasks that earned
≥$10 and determined that they include a large range of very strange
campaigns, some prominent examples include:

ceive a percentage of the sales.

mid scheme to receive a large payment.

• Pyramid Schemes: Workers recruit their friends into a pyra-
• Commissioned Sales: Workers sell products in order to re-
• Dating Sites: Workers crawl OSNs and clone the proﬁles of
• Power-Users: These tasks call for a single worker who owns
a powerful social network account, well-read blog, or works
for a news service to generate a story endorsing the customer.

attractive men and women onto a dating site.

4. CROWDTURFING ON MICROBLOGS

In this section, we study the broader impact of crowdturﬁng by
measuring the spread of crowdturf content on microblogging sites.
We gather data from Sina Weibo, the most popular microblogging
social network in China that has the same look and feel as Twitter.
We study Weibo for two reasons. First, as shown in Table 2, mi-
croblogging sites and Weibo in particular are very popular targets
for crowdturﬁng campaigns. Second, the vast majority of informa-
tion on Weibo (i.e. “tweets” and user proﬁle information) is public,
making it an ideal target for measurement and analysis.

We begin by introducing Weibo and our data collection method-
ology. Next, we examine properties of crowdturﬁng tasks and work-
ers on Weibo. Finally, we gauge the success of campaigns across
the social network by analyzing the spread of crowdturﬁng content.
4.1 Weibo Background and Data Collection

Founded in August 2009, Sina Weibo is the most popular mi-
croblogging social network in China, with more than 250 million
users as of October 2011 [1]. Weibo has functionality identical
to Twitter: users generate 140 character “tweets,” which can be
replied to and “retweeted” by other users. Users may also create
directed relationships with other users by following them.

We focus our study of Weibo campaigns from ZBJ, because ZBJ
has the most microblogging campaigns by far. Of the 4,061 mi-

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France683ZBJ
SDH

 10000

 1000

 100

 10

s
k
s
a
T

 
f
o
 
r
e
b
m
u
N

 1
 0.01

 0.1

 1

 10

 100

 1000

Price Per Submission ($)

i

#
 
s
n
g
a
p
m
a
C
 
o
b
e
W

i

 1200
 1000
 800
 600
 400
 200
 0

933

577

404 433 433

F
D
C

192

March Apr May Jun

Jul Aug

Months in the year 2011

 100

 80

 60

 40

 20

 0

 1

Weibo Accounts
Submissions

 10

Number of … per Worker

 100

 1000

Figure 12: # of tasks vs. submission price.

Figure 13: Weibo campaigns in 2011.

Figure 14: Submissions and accounts.

All Users

Workers

Customers

 100

 80

 60

 40

 20

F
D
C

 100

 80

 60

 40

 20

F
D
C

Customers

Workers

All Msgs.

F
D
C

 0
0.01

0.1

1

10

100

1000

104

Ratio of Followers to Users Followed

 0
 100

 1000  10000  100000  1e+06  1e+07

Messages per Campaign

 100

 80

 60

 40

 20

 0

Pay per Tweet
Pay per Retweet
 6
 10

 8

 12

 14

 2

 4

Maximum Crowdturf Cascade Depth

Figure 15: Follow rates for Weibo users.

Figure 16: Messages per campaign.

Figure 17: Crowdturﬁng cascades.

croblogging campaigns on ZBJ, 3,145 target Weibo. As shown in
Figure 13, the number of Weibo campaigns on ZBJ mirrors Weibo’s
rapid growth in popularity in 2011.

The goal of crowdturﬁng campaigns on Weibo is to increase the
customer’s reach, and to spread their sponsored message through-
out the social network. These goals lead to three task types: “pay
per tweet,” “pay per retweet,” and “purchasing followers.” The
most common task type is retweeting, in which the customer posts
a tweet and then pays workers to retweet it. Alternatively, cus-
tomers may pay workers to generate their own tweets, laden with
speciﬁc keywords and URLs, or to have their accounts follow the
customer’s for future messages.

To increase the power of their campaigns, customers prefer work-
ers who use realistic, well-maintained Weibo accounts to complete
tasks. Customers may not accept submissions from poor quality,
e.g. easily detected or banned, Sybil accounts. Conversely, workers
who control popular accounts with many followers can earn more
per task than worker accounts with average popularity.
Data Collection.
Understanding the spread of crowdturﬁng
content on Weibo requires identifying information cascades [19].
Each cascade is characterized by an origin post that initiates the
cascade, and retweets that further propagate the information. Cas-
cades form a directed tree with the origin post at the root. In crowd-
turﬁng cascades, the origin post is always generated by a customer
or a worker, but retweets can be attributed to workers and normal
Weibo users. Each campaign is a forest of cascade trees.

We crawled Weibo in early September, 2011 to gather data on the
spread of crowdturf content. The crawler was initially seeded with
URLs that matched campaigns already found on ZBJ, and used
simple content analysis to determine if each worker submission was
an origin post or a retweet in order to differentiate between “pay to
tweet” and “pay to retweet” tasks. In the latter case, the crawler
fetched the origin post using information embedded in the retweet.
Our crawler targets the mobile version of the Weibo site because
it lists all retweets of a given origin post on a single page, includ-
ing the full path of multi-hop retweets. The crawler recorded the
total number of tweets, followers, and users followed by each user
involved in crowdturﬁng cascades. Unfortunately, Weibo only di-
vulges the ﬁrst 1K followers for each user, so we are unable to fully
reconstruct the social graph.

Overall, our crawler collected 2,869 campaigns involving 1,280
customers. These campaigns received submissions from more than
12,000 Weibo accounts, and reached more than 463,000 non-worker
users. Among these, 2% of worker accounts were inaccessible,
and were presumably banned by Weibo for spamming. 0.08% of
the non-worker user accounts were inaccessible, and all customer
accounts remained active. “Pay per tweet” campaigns initiated
25,000 cascades, while “pay per retweet” campaigns triggered 5,000
cascades. We ignore “purchase followers” campaigns, since they
do not generate crowdturﬁng cascades.

To get a baseline understanding of normal Weibo user accounts,
we performed a snowball crawl of Weibo’s social graph in October
2011. The result is proﬁle data for 6 million “normal” Weibo users.

4.2 Weibo Account Analysis

We begin by examining and comparing the characteristics of
Weibo accounts controlled by workers and customers to those of
normal Weibo users. As shown in Figure 14, the number of ac-
counts controlled by each worker follows the same trend as sub-
missions per worker. This is intuitive: workers need multiple ac-
counts in order to make multiple submissions to a single campaign.
Hence, professional crowdturfers who generate many submissions
need to control a commensurate number of accounts. In absolute
terms, we observe 14,151 accounts controlled by 5,364 ZBJ work-
ers. The top 1% of workers each control ≥100 accounts, but the
average worker controls only ≈6 accounts.
Comparison to Normal Accounts.
We now compare charac-
teristics of worker’s and customer’s accounts to normal users. We
ﬁnd that each account type tweets with the same frequency. This
suggests that workers and customers are both careful not to over-
whelm their followers with spam tweets.

Previous work on Sybil detection on OSNs showed that follow
rate is an effective metric for locating aberrant accounts [31]. A
user’s follow rate is deﬁned as the ratio of followers to users fol-
lowed. Sybils often attempt to gain followers by following many
other users and hoping they reciprocate. Thus Sybils have follow
rates <1, e.g. they follow more users than they have followers.

Figure 15 shows the follow rates for different Weibo account
types. Surprisingly, normal users have the lowest follow rates.
Most worker accounts have follow rates ≈1, allowing them to eas-

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France684r
u
o
H

i

 
r
e
p
 
n
g
a
p
m
a
C

 
r
e
p
 
.
g
s
M

 
.
g
v
A

 100000

 10000

 1000

 100

 10

 1

Top 25%
All
Bottom 25%

 0

 1

 2

 3

 4

 5

 6

 7

Time (Days)

 1e+07

 1e+06

 100000

 10000

s
e
g
a
s
s
e
M

 
f
o
 
#

 1000

 1

i

s
n
g
a
p
m
a
C

 
l
a
r
i

V

 
f
o
 
#

 35
 30
 25
 20
 15
 10
 5
 0

n tl y   V ir a l

s i s t e

n

o

C

 5

 10

 15

 20

 25

 30

 35

# of Campaigns

Viral
All
Median

 10

 100

Money per Campaign ($)

Figure 18: Message creation over time.

Figure 19: Cost of Weibo campaigns.

Figure 20: Viral campaigns per customer.

ily blend in. This may represent a conscious effort on the part of
workers to make their Weibo accounts appear “normal” so that they
will evade automatic Sybil detectors. Customers tend to have fol-
low rates >1. This makes sense, since customers tend to be com-
mercial entities, and are thus net information disseminators rather
than information consumers.

4.3 Information Dissemination on Weibo

Much work has studied how to optimize information dissemina-
tion on social networks. We analyze our data to evaluate the level
of success in crowdturﬁng cascades, and whether there are factors
that can predict the success of social crowdturﬁng campaigns.
Campaign Analysis.
We start by examining the number of
messages generated by crowdturﬁng campaigns on Weibo. We de-
ﬁne a message as a single entry in a Weibo timeline. A tweet from
a single user generates f messages, where f is their number of fol-
lowers. The number of messages in a campaign is equal to the num-
ber of messages generated by the customer, workers, and any nor-
mal users who retweet the content. Total messages per campaign
represents an upper bound on the audience size of that campaign.
Since we have an incomplete view of the Weibo social graph, we
cannot quantify the number of duplicate messages per user.
Figure 16 shows the CDF of messages generated by Weibo cam-
paigns. 50% of campaigns generate ≤146K messages, and 8%
manage to breach the 1M-message milestone. As expected, work-
ers are responsible for the vast majority of messages, i.e. there are
very few retweets. Considering the low cost of these campaigns,
however, these raw numbers are nonetheless impressive.

Next, we want to examine the depth of crowdturﬁng cascades.
Figure 17 plots the depth of cascades measured as the height of
each information cascade tree. Pay-per-tweet campaigns are very
shallow, i.e. worker’s tweets are rarely retweeted by normal users.
In contrast, pay-per-retweet campaigns are more successful at en-
gaging normal users: 50% reach depths >2, i.e.
they include at
least one retweet from a normal user. One possible explanation
for the success of pay per retweet is that normal users may place
greater trust in information that is retweeted from a popular cus-
tomer, rather than content authored by random worker accounts.

Next, we examine the temporal dynamics of crowdturﬁng cam-
paigns. Figure 18 shows the number of messages generated per
hour after each campaign is initiated. The “all” line is averaged
across all campaigns, while the top- and bottom-25% lines focus
on the largest and smallest campaigns (in terms of total messages).
Most messages are generated during a campaigns’ ﬁrst hour (10K
on average), which is bolstered by the high-degree of customers
(who tend to be super-nodes), and the quick responses of career
crowdturfers (see Figure 7). However, by the end of the ﬁrst day,
the message rate drops to ≈1K per hour. There is a two order
of magnitude difference between the effectiveness of the top- and
bottom-25% campaigns, although they both follow the same falloff
trend after day 1.

Factors Impacting Campaign Success. We now take a look at
factors that may affect the performance of crowdturﬁng cascades.
The high-level question we wish to answer is: are there speciﬁc
ways to improve the probability that a campaign goes viral?

The ﬁrst factor we examine is the cost of the campaign. Fig-
ure 19 illustrates the number of messages generated by Weibo cam-
paigns versus their cost. The median line, around which the bulk
of campaigns are clustered, reveals a linear relationship between
money and messages. This result is intuitive: more money buys
more workers, who in turn generate more messages. However, Fig-
ure 19 also reveals the presence of viral campaigns, which we de-
ﬁne as campaigns that generate at least two times more messages
than their cost would predict. There are 723 viral campaigns scat-
tered randomly throughout the upper portion of Figure 19. This
shows that viral popularity is independent of campaign budget.

We look at whether speciﬁc workers are better at generating viral
campaigns. We found that individual workers are not responsible
for the success of viral campaigns. The only workers consistently
involved in viral campaigns are career crowdturfers, who tend to be
involved in all campaigns, viral or not.

Surprisingly, a small number of customers exhibit a consistent
ability to start viral campaigns. Figure 20 plots the total number
of campaigns started by each customer vs.
the number that went
viral, for all customers who started at least 1 viral campaign. The
vast majority of customers initiate ≤3 campaigns, which makes it
difﬁcult to claim correlation when one or more go viral. However,
the 20 customers (1.5%) in the highlighted region do initiate a sig-
niﬁcant number of campaigns, and they go viral ≥50% of the time.
Since many of these customers do not actively participate in their
own campaigns, this suggests that campaigns go viral because their
content is of interest to Weibo users, perhaps because they are re-
lated to customers such as well-known actors or performers.

5. ACTIVE EXPERIMENTS

Our next step to understanding crowdsurﬁng systems involves a
look from the perspective of a paying customer on ZBJ. We initiate
a number of benign advertising campaigns on different platforms
and subjects. By redirecting the click trafﬁc through a measure-
ment server under our control, we are able to analyze the clicks
of workers and of users receiving crowdturf content in real-time.
We begin by describing our experimental setup before moving on
to our ﬁndings, and conclude with a discussion of practical lessons
we learned during this process.
5.1 Experimental Setup
Methodology.
Figure 21 depicts the procedure we use to collect
real-time data on crowdturﬁng clicks. The process begins when we
post a new campaign to ZBJ that contains a brief description of the
tasks, along with a URL (“Task Info” in Figure 21) that workers can
click on to ﬁnd details and to perform the tasks. The task details
page is hosted on our measurement server, and thus any worker

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France685Astroturfing Site
<a>Task Info</a>

Measurement

Server

Online Store

Redirection

Spam
Creation

<a>Visit my store!</a>

Target Networks

s
n
o
s
s
m
b
u
S

i

i

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

Weibo
Forums
QQ

 0

 1

 2

 3

 4

 5

 6

Time (Hours)

r
u
o
H

 
r
e
p
 
…

 
f
o
 
r
e
b
m
u
N

 20

 15

 10

 5

 0

Submissions
Clicks

Peak Submissions: 60

 0

 20

 60
 40
Time (Hours)

 80

 100

Figure 21: Crowdturﬁng data collection.

Figure 22: Response time of ZBJ workers.

Figure 23: Long campaign characteristics.

who wants to accept our tasks must ﬁrst visit our server, where
we collect their information (i.e.
IP, timestamp, etc). Referring
workers to task details on external sites is a common practice on
ZBJ, and does not raise suspicion among workers.

Workers that accept our tasks are directed to post spam messages
that advertise real online stores to one of three target networks:
Weibo, QQ instant message groups, and discussion forums. The
posted messages urge normal users to click embedded links (“Visit
my store!” in Figure 21) that take them to our measurement server.
The measurement server records some user data before transpar-
ently redirecting them to the real online store.

We took care to preserve the integrity of our experimental setup.
Because some Chinese Internet users have limited access to web-
sites hosted outside of mainland China, we placed our measure-
ment server in China, and only advertised legitimate Chinese e-
commerce sites. In addition, we also identiﬁed many search en-
gines and bots generating clicks on our links, and ﬁltered them out
before analyzing our logs.
Campaign Details.
In order to experiment with a variety of top-
ics and venues, we posted nine total campaigns to ZBJ in October
2011. As shown in Table 3, we created three different advertising
campaigns (iPhone4S, Maldives, and Rafﬂe), and targeted each at
three distinct networks. We discuss a fourth campaign, OceanPark,
later in the section.

The ﬁrst campaign promotes an unofﬁcial iPhone dealer who im-
ports iPhones from North America and sells them in China. We
launched this campaign on October 4, 2011, immediately after Ap-
ple ofﬁcially unveiled the iPhone 4S. In the task requirements, we
required workers to post messages advertising a discount price from
the dealer on the iPhone 4S ($970).

The second campaign tried to sell a tour package to the Maldives
(a popular tourist destination in China). The spam advertises a 30%
group-purchase discount offered by the seller that saves $600 on
the total trip price ($1542 after discount). The third campaign tells
users about an online rafﬂe hosted by a car company. Anyone could
participate in the rafﬂe for free, and the prizes were 200 pre-paid
calling cards worth $4.63 each.

All campaigns shared the same set of baseline requirements. Each
campaign had a budget of $15 on each target network, and workers
had a time limit of 7 days to perform tasks. The desired number
of tasks was set to either 50 or 100, depending on the campaign
type. Submissions were not accepted if the content generated by the
worker was deleted by spam detection systems within 24 hours of
creation. These baseline requirements closely match the expected
norms for campaigns on ZBJ (see Figure 4 and Table 2).

We applied additional requirements for campaigns on speciﬁc
networks. For campaigns on the QQ instant messaging network,
workers were required to generate content in groups with a mini-
mum of 300 members. For campaigns on user discussion forums,
workers were only allowed to post content on a predeﬁned list of
forums that receive at least 1,000 hits per day.

Each campaign type had additional, variable requirements. For
Maldives and Rafﬂe campaigns, the price per task was set to $0.154,
meaning 100 submissions would be accepted. However, the price
for iPhone4S tasks was doubled to $0.308 with an expectation of
50 submissions.
iPhone 4S tasks were more challenging for two
reasons. On Weibo, workers were required to tweet using accounts
with at least 3,000 followers. On QQ, workers needed to spam two
groups instead of one. Finally, on forums, the list of acceptable
sites was reduced to only include the most popular forums.

5.2 Results and Analysis

Table 3 lists the high level results of from our crowdturﬁng cam-
paigns, including 9 short campaigns and the “OceanPark” cam-
paign. Seven of the short campaigns received sufﬁcient submis-
sions, and six were completed within a few hours (Time column).
Interestingly, workers continued submitting to campaigns even af-
ter they were “full,” in the hopes that earlier submissions would
be rejected, and they would claim the reward. In total, the short
campaigns garnered 894 submissions from 224 distinct workers.

Figure 22 shows the response times of workers for campaigns
targeting different networks. We aggregate the data across cam-
paign types rather than networks because workers’ ability to com-
plete tasks is based on the number of accounts they control on each
network. More than 80% of submissions are generated within an
hour for Weibo and forum campaigns, and within six hours for QQ.
The “Msgs” column lists the number of messages generated by
each campaign. For Weibo campaigns, we calculate messages us-
ing the same methodology as in Section 4. For QQ campaigns,
messages are calculated as the number of users in all QQ groups
that received spam from our workers. We cannot estimate the num-
ber of messages for forums because we do not know how many
users browse these sites.

We can understand the effectiveness of different crowdturﬁng
strategies by comparing the number of messages generated to the
number of clicks (responses by normal users, “Clicks” column in
Table 3). We see that QQ campaigns are the most effective, and
generate more clicks than Weibo campaigns despite generating only
1/5 as many messages as Weibo. One possible reason is that QQ
messages pop-up directly on users’ desktops, leading to more views
and clicks. Tweets on Weibo, on the other hand, are not as invasive,
and may get lost in the ﬂood of tweets in each user’s timeline. Fo-
rums perform the worst of the three, most likely because admins on
popular forums are diligent about deleting spammy posts.

Finally, we try to detect the presence of Sybil accounts (multiple
accounts controlled by one user) on crowdturﬁng sites. Column
“W/IP” in Table 3 compares the number of distinct workers (W )
to the number of distinct IPs (IP ) that click on the “Task Info”
link (see Figure 21) in each campaign. If W >IP , then not all ZBJ
workers clicked the link to read the instructions. This suggests that
multiple ZBJ worker accounts are controlled by a single user, who
viewed the instructions once before completing tasks from multiple

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France686Campaign

iPhone4S

Maldives

Rafﬂe

Network
Weibo

QQ

Forums
Weibo

QQ

Forums
Weibo

QQ

Forums
OceanPark Weibo

Subm.

47
41
71
108
118
123
131
131
124
204

Time Msgs.
197K
45min
35K
N/A
220K
46K
N/A
311K
60K
N/A
369K

6hr
3day
3h
4h
4h
2h
6day
1day
4day

Clicks W/IP
24/54
204
34/36
244
43
40/22
35/30
28
24/29
187
18/11
3
67/38
47
78
29/33
28/9
0
63
204/99

Table 3: Results from our crowdturﬁng campaigns.

accounts. Our results show that W >IP for 66% of our campaigns.
Thus, not only do crowdturfers utilize multiple accounts on target
websites to complete tasks (Figure 14), but they also have multiple
accounts on crowdturﬁng sites themselves.
Long Campaigns.
The campaigns we have analyzed thus far
all required ≤100 tasks, and many were completed within about
an hour by workers (see Figure 22). These short campaigns favor
career crowdturfers, who control many accounts on target websites
and move rapidly to generate submissions.

To observe the actions of less proliﬁc workers, we experimented
with a longer campaign that required 300 tasks. This campaign in-
cluded an additional restriction to limit career crowdturfers: each
ZBJ worker account could only submit once. The goal of the cam-
paign was to advertise discount tickets to an ocean-themed amuse-
ment park in Hong Kong on Weibo. This campaign is listed as
OceanPark in Table 3.

Figure 23 plots the number of worker submissions and clicks
from Weibo users over time for the OceanPark campaign. Just as
in previous experiments, the ﬁrst 100 submissions were generated
within the ﬁrst few hours. Clicks from users on the advertised links
closely track worker submission patterns. Overall, 191 submissions
were received on day one, 11 more on day two, and 2 ﬁnal submis-
sions on day four, for a total of 204 submissions. This indicates that
there are ≈200 active Weibo workers on ZBJ: if there were more,
they would have submitted to claim one of the 97 incomplete tasks
in our campaign.
Discussion.
Our real-world experiments demonstrate the fea-
sibility of crowd-sourced spamming. The iPhone4S and Maldives
campaigns were able to generate 491 and 218 click-backs (respec-
tively) while only costing $45 each. Considering that the iPhone
4S sells for $970 in China, and the Maldives tour package costs
$1,542, just a single sale of either item would be more than enough
to recoup the entire crowdturﬁng fee. The cost per click (CPC) of
these campaigns are $0.21 and $0.09, respectively, which is more
expensive than observed CPC rates ($0.01) for traditional display
advertising on the web [30]. However, with improved targeting
(i.e. omitting underperformers like forum spam) the costs could be
reduced, bringing CPC more in line with display advertising.

Our Maldives campaign is a good indicator of the effectiveness
of crowdturﬁng. The tour website listed 4 Maldives trips sold to 2
people in the month before our campaign. However, the day our
Maldives campaign went live, 11 trips were sold to 2 people. In the
month after our campaign, no additional trips were sold. While we
cannot be sure, it is likely that the 218 clicks from our campaign
were responsible for these sales.

6. CROWDTURFING GOES GLOBAL

In previous sections, we focused on the crowdturﬁng market in
China. We now take a global view and survey the market for crowd-

Website

Amazon Turk (US)
ShortTask
(US)

∗

MinuteWorkers (US)
MyEasyTask (US)
Microworkers (US)

Paisalive (India)

Cam-
paigns.
41K
30K
710
166
267
107

% Crowd-

turﬁng
12%
95%
70%
83%
89%
N/A

Tasks

$ per
Subm.
2.9M $0.092
527K $0.096
$0.241
10K
$0.149
4K
$0.175
84K
N/A
$0.01

Table 4: Details of U.S. and Indian crowd-sourcing sites. Data encom-
passes one month of campaigns, except ShortTask which is one year.

turﬁng systems in the U.S. and India. Additional crawls conducted
by us, as well as prior work from other researchers, demonstrates
that crowdturﬁng systems in the U.S. are very active, and are sup-
ported by an international workforce.
Mechanical Turk.
Although prior work has found that 41% of
tasks on Mechanical Turk were spam related in 2010 [15], our mea-
surements indicate that this is no longer the case. We performed
hourly crawls of Mechanical Turk for one month in October 2011,
and used keyword analysis to classify tasks. As shown in Table 4,
crowdturﬁng now only accounts for only 12% of campaigns.
Other U.S. Based Sites.
However, the drop in crowdturﬁng
on Mechanical Turk does not mean this problem has gone away.
Instead, crowdturﬁng has just shifted to alternative websites. For
example, recent work has shown that 31% of the jobs on Freelancer
over the last seven years were related to search engine optimization
(SEO), Sybil account creation, and spam [24]. Many SEO products
are also available on eBay: trivial keyword searches turn up many
sellers offering bulk Facebook likes/fans and Twitter followers.

To conﬁrm this ﬁnding, we crawled four U.S. based crowd-sourcing

sites that have been active since 2009. Since they do not provide
information on past tasks, we crawled MinuteWorkers, MyEasy-
Task, and Microworkers once a day during the month of October
2011. ShortTask does provide historical data for tasks going back
one year, hence we only crawled them once. As shown in Table 4,
keyword classiﬁcation reveals that between 70-95% of campaigns
on these sites are crowdturﬁng. We manually veriﬁed that the re-
maining campaigns were not malicious. The types of campaigns on
these sites closely matches the types found on Freelancer, i.e. the
most prevalent campaign type is SEO [24].

Sites like ShortTask, Microworkers, and MyEasyTask ﬁll two
needs in the underground market. First, they do not enforce any
restrictions against crowdturﬁng. This contrasts with Mechanical
Turk, which actively enforces policies against spammy jobs [6].
Second, these sites enable a truly international workforce by sup-
porting a wide range of payment methods. Amazon requires work-
ers to have U.S. bank accounts, or to accept cheques in Indian
Rupees, and hence most “turkers” are located in the US (46.8%)
and India (34%) [14]. However, alternative crowd-sourcing sites
support payments through systems like Paypal and E-Gold, which
makes them accessible to non-U.S. and non-Indian workers. For
example, Microworkers come from Indonesia (18%), Bangladesh
(17%), Philippines (5%), and Romania (5%) [12]. Freelancers are
also located in the United Kingdom and Pakistan [24].
Paisalive.
We located one crowdturﬁng site in India called
Paisalive that takes globalization even further. As shown in Ta-
ble 4, Paisalive is very small and the wages are very low compared
to other services. However, the interesting feature of Paisalive is
that it is e-mail based: workers sign up on the website, and after-
wards all task requests and submissions are handled through e-mail.
This design is geared towards enabling workers in rural populations
constrained by low-bandwidth, intermittent Internet connectivity.

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France6877. RELATED WORK
Crowd-sourcing Research.
Since coming online in 2005,
Amazon’s Mechanical Turk has been scrutinized by the research
community. This includes studies of worker demographics [14,
28], task pricing [7, 13], and even meta-studies on how to use Me-
chanical Turk to conduct user studies [18]. The characteristics of
Micro Workers have also been thoroughly studied [12].
OSN Spam and Detection.
Researchers have identiﬁed copi-
ous amounts of fake accounts and spam campaigns on large OSNs
like Facebook [8], Twitter [11, 32], and Renren [34]. The grow-
ing threat posed by this malicious activity has spurred work that
aims to detect and stop OSN spam using machine learning tech-
niques [3, 33, 31]. This body of research has focused on analyzing
and defending against the outward manifestations of OSN spam. In
contrast, our work identiﬁes some of the underlying systems used
by attackers to generate spam and evade security measures.
Opinion Spam.
Spam that attempts to inﬂuence the opinions
and actions of normal people has become more prevalent in recent
years [16]. Researchers have been working on detecting and char-
acterizing fake product reviews [22, 17], fake comments on news
sites [4], and astroturf political campaigns on Twitter [27]. The
authors of [26] created a model to help classify deceptive reviews
generated by Mechanical Turk workers. These works reafﬁrm our
results, that crowdturﬁng is a growing, global threat on the web.

8. CONCLUSION

In this paper, we contribute to the growing pool of knowledge
about malicious crowd-sourcing systems. Our analysis of the two
largest crowdturﬁng sites in China reveals that $4 million dollars
have already been spent on these two sites alone. The number of
campaigns and dollars spent on ZBJ and SDH are growing expo-
nentially, meaning that the problems associated with crowdturﬁng
will continue to get worse in the future.

We measure the real-world ramiﬁcations of crowdturﬁng by look-
ing at spam dissemination on Weibo, and by becoming active cus-
tomers of ZBJ. Our results reveal the presence of career crowd-
turfers that control thousands of accounts on OSNs, and manage
them carefully by hand. We ﬁnd that these workers are capable of
generating large information cascades, while avoiding the security
systems that are designed to catch automated spam. We also ob-
serve that this spam is highly effective, driving hundreds of clicks
from normal users.

Finally, our survey of crowdturﬁng sites in the U.S. and else-
where demonstrates the global nature of this problem. Unscrupu-
lous crowd-sourcing sites, coupled with international payment sys-
tems, have enabled a burgeoning crowdturﬁng market that targets
U.S. websites, fueled by a global workforce. As part of ongoing
work, we are exploring the design and quantifying the effective-
ness of both passive and active defenses against these systems.

Acknowledgments
The authors wish to thank Zengbin Zhang for his help on ZBJ ex-
periments. This project is supported by NSF under IIS-0916307,
and also supported by WCU (World Class University) program un-
der the National Research Foundation of Korea and funded by the
Ministry of Eduation, Science and Technology of Korea (Project
No: R31-30007).

9. REFERENCES
[1] China’s internet users targeted in online rumour probes. BBC, 2011.
[2] Websites shut over illegal PR deals. China Daily, August 2011.

[3] BENEVENUTO, F., MAGNO, G., RODRIGUES, T., AND ALMEIDA,

V. Detecting spammers on twitter. In Proc. of CEAS (2010).

[4] CHEN, C., ET AL. Battling the internet water army: Detection of

hidden paid posters. CoRR abs/1111.4297 (2011).

[5] DUAN, Y. The invisible hands behind web postings. China Daily,

June 2010.

[6] EATON, K. Mechanical turk’s unsavory side effect: Massive spam

generation. Fast Company, December 2010.

[7] FARIDANI, S., HARTMANN, B., AND IPEIROTIS, P. G. What’s the

right price? pricing tasks for ﬁnishing on time. In Proc. of AAAI
Workshop on Human Computation (2011).

[8] GAO, H., HU, J., WILSON, C., LI, Z., CHEN, Y., AND ZHAO,

B. Y. Detecting and characterizing social spam campaigns. In Proc.
of IMC (2010).

[9] GE, L., AND LIU, T. ’water army’ whistleblower threatened. Global

Times, January 2011.

[10] GILES, J. Inside facebook’s massive cyber-security system. New

Scientist, October 2011.

[11] GRIER, C., THOMAS, K., PAXSON, V., AND ZHANG, M. @spam:
the underground on 140 characters or less. In Proc. of CCS (2010).

[12] HIRTH, M., HOSSFELD, T., AND TRAN-GIA, P. Anatomy of a

crowdsourcing platform - using the example of microworkers.com. In
Proc. of IMIS (2011).

[13] IPEIROTIS, P. G. Analyzing the amazon mechanical turk

marketplace. XRDS 17 (December 2010), 16–21.

[14] IPEIROTIS, P. G. Demographics of mechanical turk. NYU Working

Paper, 2010.

[15] IPEIROTIS, P. G. Mechanical turk: Now with 40.92% spam. Behind

Enemy Lines blog, December 2010.

[16] JINDAL, N., AND LI, B. Opinion spam and analysis. In Proc. of

WSDM (2008).

[17] JINDAL, N., LIU, B., AND LIM, E.-P. Finding unusual review

patterns using unexpected rules. In Proc. of CIKM (2010).

[18] KITTUR, A., CHI, H., AND SUH, B. Crowdsourcing user studies

with mechanical turk. In Proc. of CHI (2008).

[19] KWAK, H., LEE, C., PARK, H., AND MOON, S. What is twitter, a

social network or a news media? In Proc. of WWW (2010).

[20] LAU, A. Reputation management: PR vs. search vs. china’s water

army. Search Engine Watch, May 2011.

[21] LE, Z. Average salaries up 13-14% last year as income disparity

increases. Global Times, May 2011.

[22] LIM, E.-P., ET AL. Detecting product review spammers using rating

behaviors. In Proc. of CIKM (2010).

[23] MASNICK, M. Bot-on-bot ebay scamming. Techdirt, July 2006.
[24] MOTOYAMA, M., MCCOY, D., LEVCHENKO, K., SAVAGE, S.,
AND VOELKER, G. M. Dirty jobs: The role of freelance labor in
web service abuse. In Proc. of Usenix Security (2011).

[25] NI, V. China’s internet users hit 485 million, weibo users and group

buyers surge. China Brieﬁng, July 2011.

[26] OTT, M., CHOI, Y., CARDIE, C., AND HANCOCK, J. T. Finding

deceptive opinion spam by any stretch of the imagination. In Proc. of
ACL (2011).

[27] RATKIEWICZ, J., ET AL. Detecting and tracking political abuse in

social media. In Proc. of ICWSM (2011).

[28] ROSS, J., ET AL. Who are the crowdworkers?: Shifting

demographics in amazon mechanical turk. In Proc. of CHI (2010).

[29] SNAVELY, N., SEITZ, S. M., AND SZELISKI, R. Photo tourism:

Exploring photo collections in 3d. ACM ToG 25, 3 (2006).

[30] STONE-GROSS, B., ET AL. Understanding fraudulent activities in

online ad exchanges. In Proc. of IMC (2011).

[31] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G. Detecting

spammers on social networks. In Proc. of ACSAC (2010).

[32] THOMAS, K., ET AL. Suspended accounts in retrospect: An analysis

of twitter spam. In Proc. of IMC (2011).

[33] WANG, A. H. Don’t follow me: Spam detection on twitter. In Proc.

of SECRYPT (2010).

[34] YANG, Z., WILSON, C., WANG, X., GAO, T., ZHAO, B. Y., AND

DAI, Y. Uncovering social network sybils in the wild. In Proc. of
IMC (2011).

WWW 2012 – Session: Security 2April 16–20, 2012, Lyon, France688