Information Integration Over Time in Unreliable and

Uncertain Environments

Aditya Pal

Dept. of Computer Science

University of Minnesota
Minneapolis, MN, USA
apal@cs.umn.edu

Ashwin Machanavajjhala

Yahoo! Research

Santa Clara, CA, USA

mvnak@yahoo-inc.com

ABSTRACT
Often an interesting true value such as a stock price, sports
score, or current temperature is only available via the obser-
vations of noisy and potentially conﬂicting sources. Several
techniques have been proposed to reconcile these conﬂicts
by computing a weighted consensus based on source relia-
bilities, but these techniques focus on static values. When
the real-world entity evolves over time, the noisy sources
can delay, or even miss, reporting some of the real-world up-
dates. This temporal aspect introduces two key challenges
for consensus-based approaches: (i) due to delays, the map-
ping between a source’s noisy observation and the real-world
update it observes is unknown, and (ii) missed updates may
translate to missing values for the consensus problem, even
if the mapping is known.

To overcome these challenges, we propose a formal ap-
proach that models the history of updates of the real-world
entity as a hidden semi-Markovian process (HSMM). The
noisy sources are modeled as observations of the hidden
state, but the mapping between a hidden state (i.e. real-
world update) and the observation (i.e. source value) is un-
known. We propose algorithms based on Gibbs Sampling
and EM to jointly infer both the history of real-world up-
dates as well as the unknown mapping between them and the
source values. We demonstrate using experiments on real-
world datasets how our history-based techniques improve
upon history-agnostic consensus-based approaches.

Categories and Subject Descriptors
H.2.5 [Database Management]: Heterogeneous Databases;
H.2.8 [Database Management]: Database Applications—
Data mining

General Terms
Algorithms, Experimentation

Keywords
Information Integration, Semi-Markov, Probabilistic Model

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

Vibhor Rastogi
Yahoo! Research

Santa Clara, CA, USA

rvibhor@yahoo-inc.com

Philip Bohannon
Yahoo! Research

Santa Clara, CA, USA
plb@yahoo-inc.com

1.

INTRODUCTION

While integrating “opinions” from multiple sources, a sys-
tem is often required to resolve conﬂicts. This arises in a
variety of settings, but one common setting is that of an
information integration system, in which multiple sources
provide information about the same real-world entity.
If
diﬀerent, incompatible values are provided for the same at-
tribute, the sources are said to conﬂict, and the process of
resolving the conﬂict to assign a value is referred to as “cor-
roboration” or “truth ﬁnding”. This task is frequently chal-
lenging, as illustrated by the following example. Consider
a restaurant aggregator that seeks to compile a database of
restaurants from three feeds, s1, s2, and s3. Let e be phone
attribute of one business, “Truya Sushi”. For example, both
sources s1 and s2 might report that the current value of e
is “555-1234”, while source s3 reports the current value as
“555-4444”. However, suppose a single value must be chosen
to show ﬁrst for “Truya Sushi”.

There exist two broad classes of opinion aggregation tech-
niques – (a) meta-learning approaches [2, 14], which would
employ classiﬁcation/regression with the values provided by
each source as covariates, and (b) graph propagation ap-
proaches [6, 15], which propagate object properties to source
properties and vice-versa via incident edges. Most graph
propagation approaches assume source independence, but
recently, the source-independence assumption was relaxed
by [1] which considered copying between sources, and by [3]
which considered internal dependence between data items.
However, all work of which we are aware of has ignored an
exceptionally important factor in estimating the value of an
unknown variable from conﬂicting sources, the history of up-
dates, as we now illustrate.

Temporal Information Example: Consider two histories
for e shown in Table 1 and 2. Each scenario shows the values
given for e by s1 − s3 at two times in the past, t1 and t2.
Sources need not report values at the same time, this is for
simplicity of presentation. We now turn to the additional
inferences that can be made. The main idea is to change
from modeling a single opinion to try to ascertain the latest
true update to e. To illustrate this idea, consider the ﬁrst
scenario, in which source s3 has steadily asserted the “555-
4444” number, while source s1 started with “555-4444” and

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France789Source

t1

t2

s1
s2
s3

555-4444
555-8566
555-4444

555-8566
555-1234
555-4444

t3 = now
555-1234
555-1234
555-4444

Table 1: Scenario 1

Source

t1

t2

s1
s2
s3

555-8566
555-8566
555-8566

555-1233
555-1234
555-1234

t3 =now
555-1234
555-1234
555-4444

Table 2: Scenario 2

s2 started with “555-8566”, but then both s1 and s2 evolved
to “555-1234”. In this scenario, given the greater accuracy
scores for sources s1 and s2, it is reasonable to output “555-
1234” as the value of e. In addition, there is some chance
that “555-4444” is an old value and s3 has never updated.
However, consider scenario two. In this scenario, at previous
time slices, s3 agreed with s1 and s2. In this case, we argue
that much more weight should be given to s3’s opinion, since
it has either received a new update not yet seen by s1 and s2
or has accepted an update from a wildly inaccurate source.
This illustrates the need to consider what is the last true
update, rather than the majority alone.

Other uses of history integration include tracking mobile
units [17], determining the rate of change of a variable or at-
tribute, estimating the typical lag introduced by each source
s, etc. In this paper, however, we focus on the estimation of
Z, and leave these applications to future work.

Challenges: The history aggregation problem is hard due
to three main challenges:
1. Missing updates: Streams can miss updates about the
entities. E.g. a restaurant changes its phone number,
but a stream retained its old phone number.

2. Independent error: Streams can send noisy data. E.g.
a restaurant changes its street name to Picasso street, but
a stream’s update read Pic. st..

3. Arbitrary lag: Streams can send updates after an ar-
bitrary time. E.g. one stream sent the update within
minutes of actual change, while other stream sent an up-
date after a week.
The above three challenges occur in practice due to the
fact that the information streams are manually updated and
may be reformatted.

Contributions: To our knowledge, this paper is the ﬁrst to
consider the implications of a value update history for truth
corroboration.
1. Our ﬁrst contribution is a formal generative model of
corroboration as determining the value of an unknown
history of real-world updates Z, based on the observa-
tion streams Sk for several sources k = 1, . . . , n. In our
model, we assume that updates in a stream Sk are noisy
versions of an update in the real world from the past,
and this is modeled using a latent mapping vector Ok.
Furthermore, we model the fact that streams can miss or
repeat the same real world update by imposing a Marko-
vian structure on the mapping vectors.

2. We give algorithms to jointly infer the hidden variables:
(i) mappings Ok corresponding to stream Sk, and (ii)

true stream Z. We show that when Z is known, then
mappings can be optimally estimated using dynamic pro-
gramming approach. We also show that when Ok is
known then Z can be estimated using particle ﬁltering
method (and exact inference is possible under some nat-
ural functional assumptions). Based on this we give two
inference algorithms, (i) EMA that alternates inference
of the two hidden variables by ﬁxing one and optimizing
the other, converging to a local optima, and (ii) Gibbs
that samples a set of possible mapping vectors, and infers
the best Z for the sampled set.

3. Finally, we demonstrate, using three real world tasks (in-
cluding one of estimating NFL scores via Twitter), the
performance of our joint inference algorithms, and show
that our model outperforms existing consensus based ap-
proaches that are either history-agnostic, or use history
in a naive way.

Outline: Rest of the paper is organized as follows. We de-
scribe the history integration problem in Sec. 2. We present
our model in Sec. 3 and the inference algorithms in Sec. 4
and 5. Experimental evaluation is discussed in Sec. 6 and 7.
Finally we conclude by discussing related work in Sec. 8.
2. HISTORY INTEGRATION PROBLEM

The state changes that occur in any real-world entity can
be succinctly described through a temporal sequence. We
deﬁne a “temporal sequence” as follows.

Definition 1

(Temporal Sequence). A temporal se-
quence Φ is a sequence of pairs [(v1, t1), . . . ,( vm, tm)], such
that the following constraints hold:

∀i ∈ [1, m), vi (cid:4)= vi+1
∀i ∈ [1, m), ti < ti+1

(1)
(2)
We use the following notations: (i) ΦV (i) =v i is the ith
value in the sequence, (ii) ΦT (i) = ti is the time correspond-
ing to the ith value, (iii) |Φ| (= m) is the number of entries
in Φ, and (iv) Φ(1 : i) is all the entries of Φ starting from
the ﬁrst pair onwards to the ith pair.
Let Z = [(v1, t1), . . . ,( vm, tm)] be the temporal sequence
that represents the state changes of an entity. Note that
since Z captures the changes in the entity value, consecu-
tive values Z V (i) andZ V (i + 1) are diﬀerent. We call Z as
an entity sequence. Let n streams make independent obser-
vations of Z and publish their views in the form of temporal
sequences S1, . . . , Sn. We call them as stream sequences.
The stream sequence S can have errors w.r.t Z obtained by
applying one or more of the following four operations on Z.

OP1 Missing updates: Stream S may not report some of the

updates in Z.

OP2 Noisy/Erroneous observations: A stream S may report
a valueS (j) that is diﬀerent from the observed value
Z(i).

OP3 Duplicate observations: Stream S may report diﬀer-
ent noisy observations of the same value Z(i) multiple
times (as say, S(j) andS (j + 1)).

OP4 Lagged observations: Stream S may report an obser-
vation Z(i) that occurred in the past (i.e., Z T (i) ≤
ST (j)). Nevertheless, we assume that updates in Z
are reported in the same order byS .

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France790WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France7912. Modeling lags: Time of jth update in Sk is gener-
ated using the (j − 1)th update and the time of O[j]th
update of Z. Thus probability distribution over ST
k (j)
k (j − 1) and Z T (Ok[j]), and is de-
depends only on ST
k (j − 1), Z T (Ok[j])).
k (j)|ST
noted as P (ST

3. Modeling noise: Value of jth update in Sk is gen-
erated using the value of O[j]th update of Z. Thus
k (j) depends only on
probability distribution over SV
Z V (Ok[j]), and is denoted as P (SV

k (j)|Z V (Ok[j])).

Figure 1 illustrates this generative probabilistic model pic-
torially. Given this generative process for S, we can write
the joint probability distribution of P (S, Z) as follows.

(cid:2)

O
(cid:2)

P (S, Z) =

=

P (Z)P (Ok|Z)P (ST

k

|Ok, Z)P (SV

k

|Ok, Z)

(5)

[P (Z)

(domain knowledge)

O

× n(cid:3)

k=1

× n(cid:3)

k=1

× n(cid:3)

|Sk|(cid:3)

j=1
|Sk|(cid:3)

j=1
|Sk|(cid:3)

k=1

j=1

P (Ok[j]|Ok[j − 1], Z)

(miss/repeat)

P (ST

k (j)|ST

k (j − 1), ZT (Ok[j])) (lag)

P (SV

k (j)|ZV (Ok[j]))]

(noise)

The key assumptions made in our generative model and
equation 5 is that given the complete history of the entity
(Z) and the mapping of observed variables to Z (essentially
O), the stream update values (SV ) are independent of each
other. This is true in most, but not all, practical scenarios.
For eg., some sources might be copying each other (see [3]),
and hence be correlated, but we assume, for the model’s
simplicity, that copying sources have been detected, and re-
moved from the analysis.

4. GENERAL INFERENCE ALGORITHM
Recall that the history integration problem is to compute
Z (cid:2) = arg maxZ{P (S, Z)}, where P (S, Z) is given by Equa-
tion 5. Here we describe our inference techniques to com-
pute Z (cid:2). This involves ﬁnding the set of updates Z V as well
as the times when these updates occurred Z T . We start
this section by describing a special case, where the mapping
vector O is known. Then using the solution for the spe-
cial case, we develop two approximate inference algorithms
for the entire problem – Gibbs, an approach based on Gibbs
Sampling [4], and EMA, an approach based on Expectation-
Maximization. In all approaches, the overall idea is to it-
eratively ﬁnd a mapping vector O (that represents miss-
ing/repeat updates) based on the current estimates of Z,
and then conditioned on the current O optimize for Z V and
Z T (accounting for lags and noise).
4.1 Mapping Vector is Known
Assume that we know the set of mapping vectors O then
the problem is to maximize P (Z|S, O), where Z(i) forms the
hidden state. Furthermore, since each Ok maps the updates
in a stream Sk to the updates in Z, the hidden states in
Z form a HSMM with transitions dictated by the prior on
Z and the emission from Z(i) given by the corresponding

stream reports Sk(j), where Ok[j] =i . Note that though
this special case of problem with ﬁxed O is similar to the
one of HSMM inference, it has many distinguishing char-
acteristics. First, not every hidden state Z(i) emits an ob-
servation; we have to deal with missing values. Moreover,
some hidden states emit multiple observations (when multi-
ple stream updates are mapped to the same Z(i)). Second,
Z(i) values are not discrete – in many of our experiments the
Z V values are modeled using Gaussian distributions, and in
general the transition and emission probabilities can be any
distributions from the exponential family.

We address missing updates by allowing hidden states to
emit a special null observation that has equal probability
of being emitted by each of the hidden states. To address
multiple emissions, we think of the multiple observations as
a single observation emitted with probability equal to the
product of the emission probability of each observation (as
they are conditionally independent given the hidden state).
Once missing/multiple updates are addressed, in the dis-
crete case, Z can be inferred exactly using standard Viterbi
algorithm for HMM(HSMM) inference.
In the continuous
case, one can use the general technique of particle ﬁlter-
ing, also known as sequential monte carlo, for inferring the
hidden values of Z. However, it is not possible to perform
exact inference for general prior distributions of Z, and one
can use sequential importance sampling [5] to perform an
approximate inference of Z.
4.2 Gibbs Inference Algorithm

Here we consider the problem when the mapping vectors
O are unknown. In the Gibbs algorithm, we begin by sam-
pling a random O from the prior P (O|Z); this prior de-
pends on the length of Z alone. Since we do not know |Z|,
we generate sample O’s for various values of |Z| and we
pick the one with the maximum likelihood. Starting with
this initial O, we iteratively ﬁnd the Z, with the maximum
P (Z|S, O) using techniques from Sec. 4.1 for a ﬁxed O, and
then ﬁnd a new O by sampling from the posterior distribu-
tion of P (O|S, Z) as follows. For all streams k:

P (Ok|O−k, S, Z) ∝ P (Ok|Z) · P (Sk|Z, Ok)

(6)
The above sampling distribution can be used to sample Ok
for all streams k. Z can be recomputed from the sampled
O and the process can be repeated either until convergence
or until a maximum number of iterations are performed.
4.3 EMA Inference Algorithm

The Gibbs algorithm can take many steps to converge.
An alternative is to use the Expectation Maximization tech-
nique. When Z is known, we can estimate an optimal O,
such that P (O|S, Z) is maximized. This can be done inde-
pendently for all streams, as P (O|S, Z) is
k (j)|Z V (Ok[j])) · P (ST

k (j)|Z T (Ok[j]))

∝ n(cid:2)

|Sk|(cid:2)

k=1

j=1

P (SV
·P (Ok[j]|Ok[j − 1], Z)

(7)

where we used equation 5 to get above factorization. We can
use Viterbi algorithm [12] to get the best mapping (see algo-
rithm 1). EstimateMapping algorithm exploits the optimal
sub-structure property and runs in O(m · (cid:2)
2) time, where(cid:2)
is the length of Z and m denotes the number of updates in
a stream. Ok = EstimateMapping(Z, Sk) gives mapping for

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France792Algorithm 1 EstimateMapping(Z, S)

{π, σ model missing update and ψ models noise and lag}
Let π(i) = log P (O[1] = i|Z)
Let σ(j, i, k) = log P (O[j] = i|O[j − 1] = k, Z)
Let ψ(j, i) = log P (S(j)|Z(i))
l = |Z|, m = |S|
for i = 1 to l do

c[1, i] = π(i) +ψ (1, i)

end for
for j = 2 to m do
for i = 1 to l do

r = arg maxr{c[j − 1, r] + σ(j, i, r) :∀r ∈ [1, i]}
c[j, i] = c[j − 1, r] +σ (j, i, r) +ψ (j, i)
d[j, i] = r

end for

end for
O[m] = arg maxi{c[m, i] : 1 ≤ i ≤ l}
for j = m − 1 to 1 do

O[j] = d[j + 1, O[j + 1]]

end for
Return O

stream k. The EMA algorithm takes an initial Z and then
alternates between ﬁnding O and Z iteratively. These iter-
ations are repeated until a ﬁxed point is reached. The EMA
algorithm works with the intuition that in any iteration, the
perturbations in O would improve the likelihood of the sub-
sequent Z. Note that EMA algorithm aims at ﬁnding local
maxima Z, O for the probability distributions P (Z, S, O).
The key problem with the EMA algorithm is that it can get
stuck in a local optima, and its performance is critically de-
pendent on the quality of initial Z. One way to alleviate
this is to choose a good initial Zinit using Gibbs and then
run the EMA algorithm starting from Zinit; we denote this
hybrid algorithm as Gibbs + EMA.

5.

INFERENCE FOR A NATURAL INSTAN-
TIATION

In this section, we describe an instantiation of our gen-
eral model, where we assume natural functional forms for
the various probabilities. Our choices allow us to tractable
solve the history integration problem. We show in our ex-
periments that this speciﬁc model works well on a number of
datasets. We ﬁrst describe the distributions used to model
domain knowledge, missing/repeat updates, lags and noise
(from Equation 5). We then present tractable algorithms
for estimating the most likely Z (cid:2) given O as well as for
sampling from the posterior distribution of P (O|S, Z). Fi-
nally we conclude this section with a brief note on parameter
learning for our natural instantiation.
5.1
Domain Knowledge [P (Z)]: We assume that the prior
on Z factorizes such that Z V (i) only depends on Z V (i − 1),
the previous diﬀerent value in Z, and Z T (i) only depends
on the time of the previous update Z T (i − 1). We use the
Exponential distribution to model the intervals between con-
secutive updates: P (Z T (i)− Z T (i− 1) = τ) ∝ exp(−γZ · τ).
When Z V values are continuous, we use a Gaussian prior
Z V (i) ∼ N(Z V (i − 1), σZ). The ﬁrst element, Z(1), of Z is
considered to have a uniform prior.
Missing/Repeated Updates [P (Ok[j]|Ok[j−1], Z)]: The

Instantiation

Algorithm 2 Estimating Z T

k (j) :∀k, ∀j, Ok[j] = |Z|}

ZT (|Z|) =min{S T
{If no mapping for last element of |Z|, then set |Z| = |Z| −1
and re-run algorithm}
for i = |Z| −1 to 1 do

if ∀k, ∀j, Ok[j] (cid:5)= i then
{No mapping vector, cant estimate time of update}
ZT (i) = ZT (i + 1)− 
ZT (i) = min{ST
if ZT (i) ≥ ZT (i + 1) then

k (j) :∀k, ∀j, Ok[j] = i}

else

ZT (i) =Z T (i + 1)− 

end if

end if
end for
Return ZT

diﬀerence between consecutive indices of a mapping vector
Ok[j] andO k[j + 1] determine whether an update in Z is
repeated/missed. We model the distance between Ok[j] and
Ok[j + 1], independent of Z, using a Poisson distribution:
P (Ok[j + 1] − Ok[j] = x) ∝ e
−λλx/x!. This is to model the
fact that a stream typically gets one out of every λ update.
k (j − 1), Z T (Ok[j]))]: We assume that
Lags [P (ST
k (j − 1) and model the lag, i.e.
k (j) is independent of ST
ST
k (j)− Z T (Ok[j])|, using an Exponential distribution that
|ST
k (j) −
penalizes streams with large reporting delays: P (ST
Z T (Ok[j]) = τ) ∝ exp(−γk · τ).

k (j)|ST

k (j)|Z V (Ok[j]))]: There is no one model for
Noise [P (SV
the way values in the stream are mis-reported by streams.
this is very application speciﬁc and depends on the kind of
data being reported. We present the following two example
instantiations. When values being reported are continuous
(like in the weather readings or trading volume), one can
k (j) ∼ N(Z V (Ok[j]), σk). When the
use Guassian noise: SV
reported values are discrete (like phone numbers, or football
scores), one may use following simple noise model: SV (j)
is the same asZ V (O[j]) with probability pk, and diﬀerent
with probability 1 − pk.
5.2 Estimating Z given O

In this section, we provide eﬃcient algorithms to extend
Sec. 4.1 for ﬁnding Z given O on the speciﬁc distributions
instantiated in the previous section. Since the prior on Z
factorizes into independent terms containing only Z T and
Z V , we can optimize for the times and values independently.

Finding Z T : When the lag follows an Exponential distri-
k (j) − Z T (Ok[j]) = τ) ∝ exp(−γk · τ), we
bution, i.e. P (ST
can exactly compute the time of the ith update in Z, by
solving the following minimization
(cid:3)
min{γz[Z T (|Z|) − Z T (1)] +

k (j) − Z T (i)]} (8)

γk[ST

∀i,∀k,∀j
Ok[j]=i

Subject to following constraints

Z T (i − 1) < Z T (i) < Z T (i + 1)

Z T (i) ≤ min{ST

k (j) : ∀k,∀j, Ok[j] = i}

In general, Algorithm EstimatingZ T provides a valid so-
lution to Z T that satisﬁes the required constraints. Under

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France793certain conditions it is provably optimal, i.e. minimizes Eq. 8
as shown in the lemma below.

Lemma 5.1. Algorithm 2 always returns a feasible solu-
tion to Equation 8, and the solution is guaranteed to be op-

timal when  → 0 and γz ≤ γk, ∀k.
If λz > λk then we can use Z T as initial solution and sample
Z T1 (i) uniformly in the range [Z T1 (i− 1), Z T (i)] and pick the
solution that minimizes Equation 8.

Finding Z V for continuous values: When Z V follows
k (j) − Z V (Ok[j]) = τ) ∝
an Gaussian distribution, i.e. P (SV
exp(−βk · τ
2), we can exactly compute the values of the ith
update in Z, by solving the following equation.

(cid:3)

∀k,∀j
Ok[j]=i

βk · SV

k (j)

(9)

βz[Z V (i+1) + Z V (i-1)] +

Z V (i) =

(cid:4)

2βz +

∀k,∀j
Ok[j]=i

βk

2 is the precision of the Gaussian distribution.
where β = 1/σ
Above equation is obtained by taking the Equation 7 on log
scale and taking its derivative w.r.t Z V (i) and equating it
to 0. This is similar to Kalman Filter, and we can estimate
the values in Z using an iterative algorithm.

Finding Z V for discrete values: For discrete values, in
the absence of a prior on Z, estimating Z V (i) corresponds to
k (j) = v) is
ﬁnding v such that the probability
maximized. We can show that v is just the majority update
amongst all the stream updates that map to i

Ok[j]=i P (SV

(cid:5)

Parameter learning. Given Z and stream observation
Sk, we can compute the error incurred by the stream. The
standard deviation of the error is our new estimate of σk.
Similarly the lag for all the observations can be used to com-
pute most likely γk and the number of missing updates can
be used to compute most likely λk. This falls naturally from
our underlying assumption that stream’s noise, lag and miss-
ing characteristics are independent.

6. DATASET DESCRIPTION

We experimentally evaluate the performance of our model
on four datasets. These datasets from diverse domains ex-
hibit varied stream characteristics and thus validate the wide
applicability of our model.

Twitter Dataset. We used Twitter api to extract all the
tweets posted on Twitter regarding NFL1 games, such as,
Jets vs Charger, Raven vs Jaguar, Packers vs Vikings. These
games were played in October, 2011. From the collected
tweets, we removed all the tweets that did not mention the
game score in the format \d+ - \d+. We also discarded
tweets containing words such as “predict” as these tweets
were found to be speculative and were not a representative
of the actual game score. The resulting dataset consists of
tweets from 20 users for Jets vs Chargers game, 37 users
for Raven vs Jaguar game, 23 users for Packers vs Vikings
game. The extracted tweets were of the order of number of
users, as users rarely posted score more than once. Our goal
in the experiment is to construct the game score timeline
1http://www.nﬂ.com/

# updates per game
Num streams (users) per game
# tweets per update per game
Relevant tweets per game
Non-relevant tweets per game
Average stream lag (sec)
# Missing update per stream

7.3
26.67
2.9
20
8
1388.4
6.67 / 7.3

Table 3: Twitter dataset characteristics aggregated
for all games.

with this dataset. Even thought the dataset looks small,
it is extremely challenging due to fact that the underlying
streams (users in this case) are missing a lot of updates, they
have a large lag and many irrelevant tweets (see Table 3).2

◦

◦

Climate Dataset. We consider the temperature data pro-
vided by Earth System Research Laboratory, USA. The
data, which is available for public download3, consists of
monthly temperature means for all the locations on Earth
from 2001 to 2010. The dataset consists of 10512 gridded
lat-
locations (with a grid resolution of 2.5
itude) and a temperature series of length 120 (one entry per
month).

longitude x 2.5

Our goal here is to estimate the series of mean tempera-
tures for the 120 months between 2001 and 2010 of a given
location based on its neighboring locations. This is a rea-
sonable goal due to high spatial auto-correlation present in
the Earth science data. The neighboring locations can be
viewed as noisy streams with no lag and missing update. We
simiulate missing updates by deleting temperature reading
from all stream at random points in time. Random lags is
simulated by adding λk ∗ r to the time of the update, i.e.
k (i) = max{ST
k (i− 1), 20· i} + λk ∗ r, where r is a random
ST
number between [0, 1] and λk is set randomly to an integer
in the range [20, 100]. Note that this dataset is challenging
as the stream’s noise (unknown to our model) varies due to
several environmental factors such as topology, height, lati-
tude, longitude of the selected locations.

NASDAQ Dataset. Our third dataset is NASDAQ’s vol-
ume trading data.4 The dataset contains 5 minute updates
of the trading volume that occurred from Jan 2008 to Dec
2010. We generate synthetic stream observations for this
dataset, using Guassian noise, Exponential lag and Poisson
missing/repeated updates as described in Section 5. Stream
generation parameters are not known to our algorithms.

This dataset is interesting for two reasons: (1) It is very
hard to come up with a prior for the volume trading data. As
a result the output of the model is completely dependent on
the stream emissions. (2) Trading data has several changes
with small amplitude. Baseline models can be tempted to
combine several diﬀerent updates together due to the close
proximity of values. Our model does not suﬀer from this as
stream observations can have random lags and they are pe-
nalized if two seemingly diﬀerent updates (in terms of time)
are being grouped together.

2For instance, we saw tweets corresponding to NHL scores
involving a team which have the same name as a NFL team
(Winnipeg Jets and New York Jets).
3http://www.esrl.noaa.gov/psd/data/
4http://www.eoddata.com/products/historicaldata.aspx

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France794Synthetic Dataset. We also evaluate our model on a syn-
thetically generated data. We consider Z V = [1, 2, . . . , m],
where m is varied from 10 to 100 and consider 5-15 streams.
Unlike NASDAQ and Climate data, the time of updates
(Z T ) is randomly initialized to non-regular spaced intervals
(e.g. Z T = [1, 3, 10, 38, 39, . . .]). The stream generation pro-
cess is as follows: A stream would pick elements of the Z
vector sequentially and could perform the following three
operations: a) Simulate missing update: Ignore the picked
element and move to the next element with Bernouilli prob-
ability = pmissk, b) Simulate independent error: Add Gaus-
sian noise with precision βk > 1, c) Simulate Lag: Publish
the noisy update after lag governed by Uniform distribution
in the range [1 − 10]. Note that streams for synthetic data
diﬀers from NASDAQ data in terms of the lag and the miss-
ing update distributions.

Dataset Summary. The above four datasets present dif-
ferent challenges to the model. Twitter dataset presents
an interesting aspect of how the model performs for sparse
datasets. On the other hand, the climate dataset presents
challenges since the underlying noise is diﬃcult to model5.
NASDAQ dataset presents challenges due to the small vari-
ability between adjacent updates. Finally, with the syn-
thetic dataset we present more extensive analysis of our
model under several diﬀerent conditions.

7. RESULTS AND EXPERIMENTS

In this section, we present our results over the four datasets.
The main objective of our experiments is to show that our
model can be adapted to run quite nicely for several dif-
ferent domains and under diﬀerent conditions. The second
objective is to show that in comparison to an intuitive base-
line model it performs better as it models all the parameters
that a stream can exhibit while reporting the updates.
7.1 Algorithms
We compare our algorithm against three baseline approaches.
Our: We use the hybrid algorithm Gibbs + EMA that is
described in Section 4 which runs a few iterations of Gibbs
to ﬁnd a Z and then runs EMA.
B1 (PickLastUpdate): This baseline picks the last update
across all streams as the true state of entity at that time.
This algorithm performs a merge sort of updates based on
ascending time order.
B2 (PickMajorityUpdate): This algorithm picks the ma-
jority of the stream updates at any given time. In order to
compute the merge stream it uses the following procedure:
Consider that a stream published an update at time t. We
collect the last update for all streams that was published
at time t or before it. The majority value amongst these
updates is set in the merge sequence at time t. This is done
for all t.
B3 (OptimalAlignment): This algorithm is based on the
intuition presented in [17], where streams can have ﬁxed
lags. The algorithm considers the optimal sequence align-
ment strategy to evaluate all possible alignments and then
ﬁxes O to compute Z that maximizes the joint-likelihood.
5Climate models that are used to extrapolate climate data
over missing points on Earth such as over Oceans, etc are
extremly sophisticated and take into account tens of vari-
ables.

Our
B3
B2

0.93
0.77
0.49

3
1
0

Precision Recall F-measure # Last correct

0.68
0.71

1

0.79
0.74
0.66

Table 4: Precision recall of our model vs the best
baseline model averaged over three games.

Baseline 3 performs better than the other two baselines
for the selected datasets and hence for the sake of clarity we
only present results with this baseline.
7.2 Results on Twitter Dataset

We treat each user as an independent stream providing
updates about the game score. For two teams playing a
game, users can post score of team1 ﬁrst (7-0) or team2 ﬁrst
(0-7), hence while computing Z, we consider both permu-
tations of the scores and pick the conﬁguration with least
noise. We used P oisson(λ = 5) to model missing updates
and Exponential(γ = 10) to model stream lag. The merg-
ing strategy is to consider the majority element amongst the
stream updates that map to same index in Z (since scores
can be thought of as discrete values). Once the best Z is ob-
tained, model parameters are recomputed and streams with
relatively small likelihood P (S|Z, O) are discarded and Z is
recomputed. This is repeated for 2-3 rounds.

Figure 2 and 3 shows updates computed by our algorithm
along with the corresponding mapping tweets. Even though
dataset was very sparse, with many streams having one or
two updates, it was able to retain the key updates that con-
stitute the real timeline of the game along with an accurate
mapping of the tweets. We also discovered that the model
is able to correctly discard the set of tweets that were not
related to the game score.

Comparison with Baseline models. Next we compare
the timeline predicted by our model with baseline models
B2 and B3 (using the majority merging strategy). We deﬁne
precision as the fraction of updates output by the algorithm
that are correct. Similarly, we deﬁne recall as the fraction of
actual updates of true timeline that are output by the algo-
rithm. Additionally, we also check whether the last update
output by each algorithm is correct.

Table 4 shows the performance of our model in comparison
with the best baseline model aggregated for all the selected
game datasets. Overall we see that our model has higher
precision and F1 over the best baseline model. We also
observe that the last update of Z presented by our model is
actually correct whereas this is not the case for the baseline.
This happens because after the end of the game, some tweets
mention the number of wins and losses for a team, and the
baseline model would consider that as an update. On the
contrary, our model correctly discards such updates from Z.
7.3 Results on Climate Dataset

We selected several locations randomly for diﬀerent lati-
tudes and longitudes and considered their temperature se-
ries. For each selected location, we considered 8 adjacent
neighbors to it as 8 streams. Typically locations are roughly
reso-
250 kms apart despite being neighbors due to 2.5
lution. The mean of absolute temperature diﬀerence be-
tween 8 neighbors to a location is 1.8 (on average) with a
mean standard deviation of 0.93. Our model is run with

◦

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France795(13:09) UGH terrible
strip, 7-0 hole. Need
to score now.

#jets

(13:06): Lmaooooo
dammmmmn Keller
coughed up da ball
#Chargers score thanks
2 Butler 7-0 #chargers

(14:10): Mark
Sanchez to Plaxico
Burress for a 3yd
score to bring the
#Jets closer to the
#Chargers...14-10

(14:09): Sanchez
#Jets
to Burress.
score TD but trail
#Chargers 14-10.

(16:05): WHAT THE HELL
#Chargers!!! You’re up 21-
10 and now you’re losing 24-21
and the #Jets are ready to
score again?? SMH.

(16:45): 27-21 JetsRT
"@bonniblucakes: What
was the score #jets
???"

(14:26):
#Chargers
score 21-10

(15:47): Sanchez
to Burress again,
3-yard score,
#Jets in front
24-21, 8:41 left

(16:17): #Jets blank
#Chargers in second
half, score 17 unan-
swered to win, 27-21.
What a game.

#NYJ

Score

7-0

14-10

21-10

21-14

21-27

Figure 2: Jets vs Chargers game score output by our algorithm along with the corresponding tweets.

(21:16): How
the **** is the
score still 0-0 ?
C’Mon #ravens

(21:18): 0-0.
Ravens hvn’t
showed up though.
Defense got a
fumble tho.
0-0

Score

(21:23): #Jaguars
score the first
points of the game
with the field goal,
up 3-0 over #Ravens
with 1:42 left in
the first quarter

(21:23): #Jaguars
score first 3-0 over
#Ravens.

(23:39): **** score
is 9-0? What is
this baseball? Need
more offense #ravens
and #jaquars. This
is the #NFL -_-

(22:51): #Jaguars
up 9-0.
game, that’s a
three-score lead.

In this

(23:47):
Bal-
timore #Ravens
finally score a
TD. #Jaguars lead
cut to 9-7 with
2:02 left.

(23:52): 7-9

(01:58): Just checked
the #Ravens score en
12-7
route to work...
to JACKSONVILLE?! Are
you kidding me?!
Woe-
ful. Utter disgrace.

(00:59): #Jaguars
defeat the #Baltimore
#Ravens by a score of
12-7 on #MNF! . . .

0-3

0-9

7-9

7-12

Figure 3: Raven vs Jaguars game score output by our algorithm along with the corresponding tweets.

Gaussian(σ = 1) noise, Exponential(γk = 10) lag, Poisson
(λk = 0.1) missing update. We also consider a prior on
length of Z, as we aim to get a update vector of same length
as true Z. Once model computes Z, stream parameters are
re-estimated and few more such rounds of algorithm is run.
We ﬁrst compare the quality of a Z output by Gibbs +
EMA and baseline B3 with respect to the true tempera-
ture series Z (cid:2) as the number of missing updates per stream
increases. Quality is computed as the sum of (Z V (m) −
Z (cid:2)V (m))2 over all times (months) m for which Z outputs
an update. Figure 4(a) shows Gibbs+EMA outperforms the
baseline B3.
7.4 Results on NASDAQ Dataset

Figures 4(b) shows the performance of our model in com-
parison with the best baseline B3 over the NASDAQ. We
plot the log of negative log-likelihood due to scale of the
values, and so lower value implies that model has higher
likelihood. The likelihood of the true Z that generated the
data is also plotted for absolute comparisons. We see that
Gibbs + EMA performs statistically signiﬁcantly better than
the best baseline (paired ttest, p <0 .01, 99% CI). More-
over, the average absolute error for last element of Z (i.e.
current state of the entity) was at least 10 times lower for
Gibbs + EMA compared to the best baseline B3.
7.5 Results on Synthetic Dataset

Figure 4(c) shows the performance of Gibbs + EMA in
comparison to B3 over the synthetic dataset. Like in the
NASDAQ data, we observe that our model performs much
better than B3. We perform further controlled experiments
on the synthetic data to better evaluate our algorithm.

Comparison between Gibbs vs Gibbs+EMA: Figure 5
shows the model performance of the Gibbs and Gibbs+EMA
algorithms for various values of |Z (cid:2)|. We see that Gibbs +
EMA slightly improves the performance of the model in com-
parison to stand alone Gibbs.

(cid:4)

(cid:6)

i

(Z V (i) − Z (cid:2)V (i))2(cid:7) 1

Eﬀect of variation in length of Z and the number
of streams: Figure 6(a) plots the quality of the Z output
by Gibbs + EMA as the number of streams increases. Each
line corresponds to a true Z (cid:2) of diﬀerent lengths. Quality is
2 . We
measured over the values as
can clearly see that the absolute error in the prediction in-
creases as the length ofZ (cid:2) increases. This intuitively makes
sense as we expect the error to increase for higher length Z
(as more iterations might be required for mixing of O and
inference). We also see that as more streams are added, the
error sharply goes down. This also makes sense as adding
more streams decreases the probability of missing updates.
Additionally, we observe that even though the underlying
streams imperfect their aggregation is quite robust to noise,
lags and missing updates.

Eﬀect of One Good Stream: In several practical scenar-
ios, there is one (or a few) good stream(s); i.e. streams with
small lag, low error and small miss probability. We simulate
such a scenario by using a goodness criteria (g), such that,
−g, have
a few good streams miss updates with probability e
−g, and noise with standard deviation
lags with parameter e
−g. The rest of the streams are bad – miss updates with
e
probability picked uniformly between 0 and 1, have lags with
parameter 10 and noise with standard deviation 1. We call
−g and as g is increased, streams
stream goodness to be e

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France796Our
Best Baseline

5

4

3

2

1

r
o
r
r
e

 

d
e
r
a
u
q
s
 

n
a
e
m

 
t

o
o
r

0
 
0

1

2

3

Number of missing updates

 

 

 

)
d
o
o
h

i
l

e
k

i
l
 

g
o

l
 

−
(
g
o

l

15

10

5

0

 

50

4

5

True
Our
Best baseline

100

150

Length of Z vector

200

)
d
o
o
h

i
l

e
k

i
l
 

g
o

l
 

−
(
g
o

l

12

10

8

6

4

2

0

 

10

True
Our
Best baseline

20

30

Length of Z vector

40

50

(a) Variance of our model’s Z w.r.t.
true Z over the climate dataset.

(b) Model comparison over NASDAQ
dataset.

(c) Model comparison over Synthetic
dataset.

Figure 4: Performance on Climate, NASDAQ and Synthetic data

)
d
o
o
h

i
l

e
k

i
l
 

g
o

l
 

−
(
g
o

l

10

9

8

7

6

5

 

 

Gibbs + EMA
Gibbs

20

40

60

Length of Z

80

100

Figure 5: Performance of Gibbs sampling algorithm
vs Gibbs+EMA.

are even better. In this scenario, we run our algorithm and
compute the mean squared error of the output with the true
Z. Figure 6(b) shows the mean squared error on log scale.
For the sake of clarity, we do not plot the performance of the
baseline models as they either perform equivalent or worse
as compared to our model. We observe here that as stream
goodness is increases the error decreases. The result also
indicates that as more streams turns good then the error
further decreases (log-linear).

Eﬀect of One Good Parameter: Another special case
is when one stream is good on only one of the parameters
(noise, lags or missing/repeated updates). To simulate this,
we consider the stream goodness parameter (as discussed in
previous result) and make one stream miss update with a
very small probability, other with a small lag and a third
with small noise. Figure 6(c) shows the performance of the
model in comparison where one stream has all the goodness
parameters. We see that when goodness is low then the
errors are relatively close. But when goodness increases then
the gap between errors increase. This happens because, we
observe that if a stream which is less noisy but misses a
lot of updates, then algorithm relies more on the updates
presented by other stream, whereas a stream which is good
in all the criteria practically dictates the inference of Z.

8. RELATED WORK

The history integration problem is most related to the fol-
lowing three ﬁelds – temporal data integration, multiple se-
quence alignment and reconciliation of values in static data.
We review each of these ﬁelds.

Temporal Data Within temporal data integration, per-
haps the work most closely related to ours is [17] that studies
inference for the purpose of mobile location tracking. They
also model the hidden variable (user’s actual location), and
have multiple observations streams, but with ﬁxed (yet un-
known) lags. That model makes sense for the mobile lo-
cation setting, since the lag of sensor’s reading to the real
world would be ﬁxed, but might not be correct for scenarios
discussed in this paper, when a single source may have vary-
ing lags for diﬀerent observations . Due to the assumption
of ﬁxed lags, their optimization problem is technically sim-
pler, and can be solved by trying out all possible variations
of the ﬁxed lag (i.e. the Ok mapping). This strategy would
be too ineﬃcient for our problem as the number of possible
Ok mapping vectors is exponential in |Sk|, the number of
observations in the source stream. Apart from mobile do-
main, HSMM based models have been used in several major
applications such as speech recognition, handwriting recog-
nition, network traﬃc modeling, and functional MRI brain
mapping. Zheng et al. [16] presents in detail how inferencing
is done in HSMM (also its variants) and presents a summary
of its applications in several domains.

Temporal data is often aggregated in sensor networks with
two critical diﬀerences: (i) Lags for sensor reading are as-
sumed to be known (Ok is known), and (ii) data is often ag-
gregated (rather than integrated) as the domain for the hid-
den variable (e.g. temperature) is often continuous, e.g. [8].
Finally, a recent paper [7] considers the problem of dedu-
plicating entities from a temporal stream of updates. While
their techniques model the evolution of an entity’s attribute
value, their focus is to cluster the temporal updates corre-
sponding to the same entity, and not compute the correct
value of the entity’s attribute.

Multiple Sequence Alignment The goal of multiple se-
quence alignment [9, 10, 11, 13] is to construct an alignment
of symbols in n sequences by adding gaps between consecu-
tive characters in the sequence. There is a penalty for adding
gaps, and a penalty when two sequences have diﬀering char-
acters at some position; an optimal alignment minimized the
total pairwise penalty across all sequences. While the his-
tory integration problem seems very similar to the alignment
problem (since we are trying to align stream updates to real
world updates using a mapping vector), there are key dif-
ferences between the two. For instance, we explicitly model
time in our problem. One could think of modeling time im-

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France797 

r
o
r
r
e
d
e
r
a
u
q
s
 

n
a
e
m

 
t

o
o
r

3

2.5

2

1.5

1

0.5

0

 

20

 

Z−length = 3
Z−length = 5
Z−length = 10
Z−length = 50

40

60

80

100

Number of Streams

0

−5

−10

−15

−20

 

)
r
o
r
r
e
d
e
r
a
u
q
s
 
n
a
e
m

 
t

o
o
r
(
g
o

l

−25
 
1

1e−1

1e−2

1e−3

 

1e−5

2
4
Number of good streams

3

5

 

)
r
o
r
r
e
d
e
r
a
u
q
s
 
n
a
e
m

 
t

o
o
r
(
g
o

l

0

−2

−4

−6

−8

−10

−12
 
2

 

10

12

One good parameter
One good stream

4

6

8

log(goodness)

(a) Eﬀect of varying length of Z and
number of streams on absolute error.

(b) Eﬀect of good stream on the
squared error, where stream goodness
varies from 1e − 1 to 1e − 5.

(c) One good stream vs One good pa-
rameter per stream

Figure 6: Experiments on Synthetic data

plicitly in the alignment problem by discretizing times and
replaying the same value Z V (i) for all times between Z T (i)
and Z T (i + 1), but this no longer is ﬂexible enough to model
all kinds of missing updates and lags.

Reconciliation of Static Data We also mention here some
of the static techniques that do not look at historical updates
for integration. Thus they cannot directly be applied for his-
tory integration, but we mention them for the sake of com-
pleteness.
In his seminal paper, Kleinberg introduced the
hubs and authorities framework (HITS), where each node is
associated with an authority and a hub score [6]. Each di-
rected edge is deemed as an endorsement of authority in one
direction and of connectivity (“hubness”) in the opposite di-
rection. The graph structure is then used to propagate these
scores till an equilibrium is attained.

Recently, Yin et al. [15], proposed the TruthFinder algo-
rithm speciﬁcally focused on opinion aggregation for binary
opinions following an approach similar to HITS. However,
unlike HITS, the predicate truth scores are computed by
probabilistic aggregation of agent opinions assuming inde-
pendence as in [2]. This paper also proposes simple heuris-
tics for handling dependencies between predicates and was
shown to be more eﬀective than a simple voting strategy.
9. CONCLUSION

In this paper, we studied the problem of merging histor-
ical information from multiple sources. Unlike prior work,
which assumes explicit mapping between source values and
the real values they observe, we model the mapping as a hid-
den unknown variable. We then perform inference to com-
pute an estimate of the history of true updates, together
with their mapping to the source values. We presented two
approximation algorithms for this inference task, and evalu-
ate their performance against several baseline methods that
either ignore history, or use it in a naive way. These ex-
periments show that our techniques are able to approximate
both the unknown history and the ﬁnal value signiﬁcantly
more accurately than baseline techniques.
10. REFERENCES
[1] X. L. Dong, L. Berti-Equille, and D. Srivastava. Truth

discovery and copying detection in a dynamic world.
Proc. VLDB Endow., 2:562–573, August 2009.

[2] D. Freitag. Multistrategy learning for information

extraction. In ICML, pages 161–169, 1998.

[3] A. Galland, S. Abiteboul, A. Marian, and P. Senellart.
Corroborating information from disagreeing views. In
WSDM, pages 131–140. ACM, 2010.

[4] S. Geman and D. Geman. Stochastic relaxation, Gibbs

distributions and the Bayesian restoration of images.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6(6):721–741, Nov. 1984.

[5] N. J. Gordon, D. J. Salmond, and A. F. M. Smith.

Novel approach to nonlinear/non-Gaussian Bayesian
state estimation. Radar and Signal Processing, IEE
Proceedings F, 140(2):107–113, Apr. 1993.
[6] J. M. Kleinberg. Authoritative sources in a

hyperlinked environment. JACM, 46(5):604–632, 1999.

[7] P. Li, X. L. Dong, A. Maurino, and D. Srivastava.

Linking temporal records. VLDB 2011.

[8] D. Liu, P. Ning, A. Liu, C. Wang, and W. Du.

Attack-resistant location estimation in wireless sensor
networks. ACM Trans. Inf. Syst. Secur., 11(4), 2008.

[9] S. B. Needleman and C. D. Wunsch. A general

method applicable to the search for similarities in the
amino acid sequence of two proteins. Journal of
Molecular Biology, 48(3):443 – 453, 1970.

[10] D. Sankoﬀ. Matching sequences under

deletion/insertion constraints. PNAS, 69(1):4–6, 1972.

[11] P. H. Sellers. On the theory and computation of
evolutionary distances. SIAM Journal on Applied
Mathematics, 26(4):pp. 787–793, 1974.

[12] A. Viterbi. Error bounds for convolutional codes and

an asymptotically optimum decoding algorithm. IEEE
Trans. on Info. Theory, 13(2):260–269, Apr. 1967.

[13] R. A. Wagner and M. J. Fischer. The string-to-string

correction problem. JACM, 21:168–173, 1974.
[14] D. H. Wolpert. Stacked generalization. Neural

Networks, 5(2):241–259, 1992.

[15] X. Yin, J. Han, and P. S. Yu. Truth discovery with

multiple conﬂicting information providers on the web.
In KDD, pages 1048–1052, 2007.

[16] S. Z. Yu. Hidden semi-markov models. Artiﬁcial

Intelligence, 174(2):215–243, 2010.

[17] S.-Z. Yu and H. Kobayashi. A hidden semi-markov
model with missing data and multiple observation
sequences for mobility tracking. Signal Processing,
83(2):235–250, 2003.

WWW 2012 – Session: Data and Content Management 1April 16–20, 2012, Lyon, France798