A Uniﬁed Approach to Learning Task-Speciﬁc Bit Vector

Representations for Fast Nearest Neighbor Search

Vinod Nair

Yahoo! Labs Bangalore
vnair@yahoo-inc.com

Dhruv Mahajan

Yahoo! Labs Bangalore
dkm@yahoo-inc.com

S. Sundararajan

Yahoo! Labs Bangalore

ssrajan@yahoo-inc.com

ABSTRACT
Fast nearest neighbor search is necessary for a variety of large scale
web applications such as information retrieval, nearest neighbor
classiﬁcation and nearest neighbor regression. Recently a number
of machine learning algorithms have been proposed for represent-
ing the data to be searched as (short) bit vectors and then using
hashing to do rapid search. These algorithms have been limited in
their applicability in that they are suited for only one type of task –
e.g. Spectral Hashing learns bit vector representations for retrieval,
but not say, classiﬁcation. In this paper we present a uniﬁed ap-
proach to learning bit vector representations for many applications
that use nearest neighbor search. The main contribution is a sin-
gle learning algorithm that can be customized to learn a bit vector
representation suited for the task at hand. This broadens the useful-
ness of bit vector representations to tasks beyond just conventional
retrieval.

We propose a learning-to-rank formulation to learn the bit vec-
tor representation of the data. LambdaRank algorithm is used for
learning a function that computes a task-speciﬁc bit vector from an
input data vector. Our approach outperforms state-of-the-art near-
est neighbor methods on a number of real world text and image
classiﬁcation and retrieval datasets. It is scalable and learns a 32-
bit representation on 1.46 million training cases in two days.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval; I.2.6 [Artiﬁcial Intelligence]: Learning

Keywords
nearest neighbor search, hashing, learning to rank

1.

INTRODUCTION

Nearest neighbor (NN) methods are widely used in several web
applications such as web objects (e.g., documents, and images)
classiﬁcation, and retrieval. In retrieval applications, availability of
large volume of web data makes it possible to retrieve very similar
objects for almost any given query object, and these similar objects
are nearest neighbors under some suitably deﬁned similarity mea-
sure. Similarly, NN methods such as k-Nearest Neighbor (kNN)
classiﬁers achieve high accuracy as very similar objects often be-

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

long to the same category. These classiﬁers are attractive due to
their simplicity and speed independence in the number of classes1.
Since NN methods work well in very large data set scenarios,
the ability to handle large volume of data during training and test-
ing is a necessity. For example, in a naive implementation, linear
search over the entire training data is needed, and this is expen-
sive for web scale applications. Therefore, development of algo-
rithms, data structures (e.g., KD-Trees) and representations (e.g.,
bit vectors) that facilitate fast search are of paramount importance.
Furthermore, task-speciﬁc performance (e.g., accuracy in classiﬁ-
cation tasks, and ranking measures such as normalized discounted
cumulative gain (NDCG), precision@k in retrieval tasks) achieved
is dependent on distance metric or similarity measure that is used
in ﬁnding the neighbors. Although Euclidean distance is a useful
metric in some applications, it is often not optimal. Hence, learning
a task-speciﬁc distance metric is also necessary.

Keeping above requirements in mind, we propose a uniﬁed frame-
work in which task-speciﬁc fast NN methods that achieve high
performance can be developed. We demonstrate our method on
classiﬁcation and retrieval tasks, and experimental results show su-
perior performance over state-of-the-art NN classiﬁcation and re-
trieval methods.

1.1 Nearest Neighbor Approaches

In recent years, there has been a ﬂurry of research activity on
nearest neighbor methods for web classiﬁcation and information
retrieval applications. To motivate our approach, we brieﬂy dis-
cuss several popular methods and differentiate them along three key
aspects (capturing the above mentioned requirements): 1) speed
(training and testing), 2) learning (representations/similarity mea-
sure with or without additional information (e.g., labels of objects,
pairwise similar or dissimilar objects), and 3) optimizing a task-
speciﬁc performance measure during learning. In Table 1 we present
a summary of various methods, and Figure 1 shows different con-
ﬁgurations to which these methods adhere to. We observe that all
the existing methods fall short in covering at least one of the as-
pects, and we attempt to address this issue.

Hashing methods are approximate nearest neighbor search meth-
ods, and are popular due to their speed.
In these methods data
points (objects) are represented as bit vectors, and such a represen-
tation helps in ﬁnding the neighbors quickly2. Similarity between
two objects is measured by the Hamming distance, and nearest

1Although standard classiﬁers such as support vector machines or logistic regression
models have been quite successfully used in the web context, the dependence of model
and test time complexities on the number of classes makes them less attractive when
the number of classes is very large.
2An important advantage of using a bit vector representation is that the search time
can be made constant with respect to the size of the search set. For example, with 32
bit representation, one can build an inverted index with 232 = 4294967296 bins

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France929Bits

Real vector

Bits

Bits

Bits

Random 
projections

Learned 
projections

Random 
projections

Real vector

Learned 
projections

Learned 
projections

Task-specific

learned 
projections

Data vector

Data vector

Data vector

Data vector

Data vector

(a) LSH

(b) Metric learning

(c) Metric learning 

+ LSH

(d) Spectral  
Hashing

(e) This paper

Figure 1: Comparison of various approaches with respect to our algorithm.

neighbors are identiﬁed as the points that are within a user spec-
iﬁed Hamming distance from a given query.

Locality Sensitive Hashing (LSH) [1] is a simple method (ﬁgure
1(a)) in which bit vector representation for a data point (object) is
obtained from projecting the data vector on several random direc-
tions, and converting the projected values to {0, 1} by thresholding.
This method does not make use of data to learn the representation.
Learning is important because it is useful to ﬁnd compact and better
representations, which in turn results in improved speed and task-
speciﬁc performance. Hence, attempts have been made to develop
better hashing methods. Sophisticated hashing methods such as Se-
mantic Hashing [19] and Spectral Hashing [23] (ﬁgure 1(d)) learn
compact bit vector representations (codes) with the desirable prop-
erty that similar objects have similar codes. In Semantic Hashing,
each object in the training database is represented by a compact bi-
nary code, and the code is computed using a feed forward neural
network, with the network weights learned by minimizing data re-
construction error. In Spectral Hashing [23], k-bit compact binary
codes are obtained by minimizing the sum of weighted Hamming
distance between data points that are similar, and the weight repre-
sents a similarity score computed using the euclidean distance in a
suitable input feature space. A key disadvantage of these methods
is that they do not make use of any task-speciﬁc information such
as object labels, query-document pair relevancy score, etc. (when
available); and, they do not explicitly optimize task-speciﬁc perfor-
mance measures. So these methods suffer on actual task-speciﬁc
performance such as classiﬁcation accuracy, NDCG, etc.

There are several popular methods that learn a distance metric
(e.g., Mahalanobis distance). These methods make use of addi-
tional information and learn the distance metric by a optimizing
task-speciﬁc objective function. Methods such as Neighborhood
component analysis (NCA) [9] and large margin nearest neigh-
bor (LMNN) [22] (ﬁgure 1(b)) classiﬁcation learn the metric us-
ing class label information, keeping nearest neighbor classiﬁcation
as the goal. Metric learning has also been seen as a special case
of ranking problem, from the view point that good neighbors ap-
pear at the top of the list and bad neighbors appear at the bottom.
McFee et al. [16] propose a structural SVM learning framework
(ﬁgure 1(b)) for learning the metric where various ranking mea-
sures such as precision@k, NDCG, etc., can be optimized (referred
as MLR in Table 1). Chechik et al. [6] propose an online algorithm

where each bin contains the data points that were mapped to the corresponding bit
vector.

for scalable image similarity (OASIS) that learns a similarity mea-
sure (ﬁgure 1(b)) with the property of scoring the more relevant
pair of objects higher; it uses labeled information in the form of
relative similarity of different pairs of objects. A major drawback
with these methods is that they do not provide bit vector representa-
tions. Therefore, ﬁnding nearest neighbors is expensive unless the
input representation is sparse (which is not the case in many appli-
cations). Also, learning is computationally expensive with NCA,
LMNN and MLR approaches3. Therefore, they are not suitable for
very large-scale problems.

Jain et al. [12] propose a method (ﬁgure 1(c)) that applies LSH
on a learned metric (referred as M+LSH in Table 1). However, this
method does not use task-speciﬁc objective function for learning
the metric; more importantly, it does not learn the bit vector repre-
sentation directly. Although LSH can be applied on the projected
data using a metric learned via NCA or LMNN, any such indepen-
dent two stage method will be sub-optimal in getting a good bit
vector representation.
1.2 Our Contributions

We propose a uniﬁed framework (Section 3) to develop fast near-
est neighbor methods for various applications such as classiﬁcation,
retrieval, regression, and user recommendations. In our method we
learn the bit vector representation directly by explicitly optimizing
task-speciﬁc performance. This is done by solving a learning-to-
rank problem. Our approach has several advantages.

• Using bit vector representation helps in ﬁnding nearest neigh-

bors fast using Hamming distance as the measure.

• High performance is achieved by direct bit vector learning
and explicit optimization of task-speciﬁc performance mea-
sure.

• It is easy to adapt the method for different applications via
choices of performance measure and ranking. LambdaRank
[5] can be used to optimize various non-smooth task-speciﬁc
performance measures (e.g., precision@k and nearest neigh-
bor classiﬁcation accuracy) (Section 3).

3In LMNN and MLR, computationally complex matrix constraints such as positive
deﬁniteness (PD) are used. Chechik et al. [6] argue that PD constraints are not nec-
essary for large datasets, and they do not use such constraints. In NCA, the computa-
tional cost is quadratic in the number of training points; there are no positive deﬁnite
constraints since the projection matrix is learned directly.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France930Bit vector

Learned 
function

Query / Document

(a)

(b)

h

Weight 
matrix W

v

+

W2j

W1j

v1

v2

bj

WDj

vD

(c)

Figure 2: (a) The algorithm we are proposing learns a function that takes a query or a document as input and computes a bit vector as output.
(b) The function multiplies the input vector v by the matrix W , adds the biases bj to the result, and then applies the sigmoid function to
compute a vector h whose components are real values between 0 and 1. The elements of W and the biases bj are the learnable parameters
of the function. (c) The sigmoid is applied component-wise to the result of the linear projection.

Learning Representation Task-Speciﬁc
No

bit vector

No

Yes

Yes

Method
LSH
Fig. 1(a)
Spectral
Fig. 1(d)
Semantic Yes
Fig. 1(d)
NCA
Fig. 1(b)
LMNN
Fig. 1(b)
OASIS
Fig. 1(b)
MLR
Fig. 1(b)
M-LSH
Fig. 1(c)
Ours
Fig. 1(e)

Yes

Yes

Yes

bit vector

bit vector

No

No

real vector

Yes (C)

real vector

Yes (C)

real vector

Yes (IR)

real vector

Yes (IR)

bit vector

No

bit vector

Yes
(C, IR, R)

Yes

Yes

classiﬁcation and regression (Section 6). We also show how the
speed of our method can be signiﬁcantly improved further with a
multi-stage (cascaded) implementation; here, speed improvement
is achieved by allocating fewer bits in earlier stages, and reducing
the nearest neighbor search space progressively (Section 6).

2. OVERVIEW

Our algorithm learns a function that takes a query or document
vector as input and computes its corresponding bit vector represen-
tation as output (see ﬁgure 2a). Here we describe at a high level
how the function is learned, and after learning, how it is used for
nearest neighbor search.
Training data: As mentioned before, we adopt a learning-to-rank
approach. So each training example has the form (query, docu-
ment list), where document list speciﬁes how its elements should
be ordered with respect to that query. In this paper we assume that
both the query and the document are the same type of data, using
the same representation (e.g., both are images of the same size, or
both are text documents that use the same bag-of-words representa-
tion)4. Therefore the same learned function can be applied to both
a query and a document to compute the corresponding bit vector.
Parameterization of the function: The function applies linear
projections to the input vector, followed by the sigmoid function
to map the output of the linear projections into real numbers in the
range [0, 1] (see ﬁgure 2b and 2c). Making the function differen-
tiable allows for gradient descent learning. At test time, when a
proper bit vector is required for fast nearest neighbor search, the
output of the sigmoid function is thresholded at 0.5 to convert its
value from a real number in [0, 1] into 0 or 1. The linear projection
coefﬁcients are the learnable parameters of the function. They are
represented as a matrix of size (number of inputs) × (number of
bits). See section 3 for more details.
Learning: LambdaRank [5] is the algorithm we use for learn-
ing.
It is based on the RankNet algorithm [4], which learns a
pairwise ranking function. Given a triplet (query, document1, doc-
ument2), the ranking function computes the probability that doc-
ument 1 should be ranked higher than document 2 for that query.
The parameters of the function are learned iteratively using the gra-
dient of the log probability of the true pairwise ranking for a set of
training triplets.

While RankNet learning is limited to pairwise ranking, Lamb-

Table 1: Comparison of Methods (see text for details). The abbre-
viations C, IR and R stand for classiﬁcation, information retrieval
and regression tasks respectively, and PD stands for positive deﬁ-
nite matrix constraint. Compact bit vector representation is prefer-
able for faster nearest neighbor search during training and testing.
Constraints such as positive deﬁnite affect training speed. Task-
speciﬁc performance optimization is important to get improved per-
formance. Jain et al. [12] use information theoretic criterion for
distance metric learning. However, LSH can be combined with any
other task-speciﬁc distance metric learning methods such as NCA,
LMNN, etc.

• From an optimization view point, our formulation and im-
plementation (Section 4) are simple and scalable. We ob-
served that our algorithm, parallelized over 8 cores, com-
pletes learning in 2 days on a 1.46 million image dataset. At
test time on the same dataset, 100000 query searches over
1.46 million vectors with a 32-bit representation using an in-
verted index are completed in 1.37 seconds (13.7 microsec-
onds per query on average) on a single core of an Intel Xeon
2.33GHz machine with 4GB RAM.

We conduct detailed experimental study (Section 5) on several bench-
mark datasets for two applications: 1) classiﬁcation and 2) informa-
tion retrieval. Comparisons with state-of-the-art NN methods show
that our method performs signiﬁcantly better. We discuss appli-
cability of our framework to other problems such as hierarchical

daRank can optimize more general, listwise ranking evaluation scores.
Suppose LambdaRank is optimizing a ranking function for some

4It is easy to modify our algorithm to handle the case where the
query and the document are two distinct types of inputs, but we do
not consider it here.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France931evaluation score S (e.g. NDCG). Given a query, the basic version
of LambdaRank deﬁnes the gradient to be that function’s RankNet
gradient for a pair of documents, multiplied by the change in the
score’s value |ΔS| if those two documents swapped positions in
the full listwise ordering computed by the current ranking func-
tion. Optimizing with the LambdaRank gradient has been shown
empirically to converge to a local optimum of various non-smooth
IR evaluation scores [7].

Adapting LambdaRank to learn a bit vector representation re-
quires several modiﬁcations. These are 1) the parameterization of
the ranking function, 2) the procedure for ﬁnding pairwise swaps
that give nonzero |ΔS|, and 3) the deﬁnition of appropriate evalu-
ation scores for various tasks. In addition, we show how to apply
LambdaRank to non-retrieval tasks such as classiﬁcation and re-
gression with bit vectors. Details are in section 3.
2.1 Nearest neighbor search with bit vectors
After learning, the bit vector representation is used to ﬁnd near-
est neighbors for a query vector within a set of documents. The
query is ﬁrst converted into a bit vector using the learned function,
with the output of the sigmoid function thresholded at 0.5. The bit
vectors for the documents to be searched can be precomputed and
stored in memory. The query bit vector is then compared against
the document bit vectors in the search set with Hamming distance
as the metric. Documents with the lowest Hamming distances are
returned as neighbors.

Two important choices that need to be made in bit vector-based
nearest neighbor search are 1) the type of thresholding to use on
the Hamming distance, and 2) how to resolve ties in the ranking.
These choices are task-speciﬁc, so we discuss them in section 3.

3. PROBLEM FORMULATION

Now we explain how the training data for different tasks are con-
structed to ﬁt the learning-to-rank framework. Then we deﬁne the
function for computing the bit vector representation, how it is used
for ranking, and how it is learned from data using the RankNet and
LambdaRank algorithms.
3.1 Training data for different tasks

Learning requires triplets, each consisting of a query and two
documents, along with the desired pairwise ranking. Depending
on the task (e.g. retrieval, nearest neighbor classiﬁcation, nearest
neighbor regression, etc.), how these triplets are created differs.
NN Classiﬁcation: Given a classiﬁcation dataset consisting of (in-
put, label) pairs, we deﬁne triplets as follows: the input for which
the class label needs to be predicted is the query.
Inputs in the
training set that are used as candidate neighbors are the documents.
Given a query belonging to a particular class, we want documents
of the same class to get ranked higher than documents of all other
classes. We will denote input vectors by v and their labels by l.
So three class-labeled inputs (v1, l1), (v2, l2), and (v3, l3), where
l1 = l2 and l1 (cid:2)= l3 are turned into a triplet (v1, v2, v3) where v1
is the query, v2 and v3 form the document pair.
Retrieval: A retrieval dataset may already be in the desired query-
document format, where each query has a corresponding list of
documents ordered by a relevance label. In such a case, it is clear
how to construct triplets. In image retrieval applications (such as
the ones used in our experiments) we may simply be given for each
query a set of documents that are equally relevant (i.e. the relevance
label is either 0 or 1). Here we can follow the same strategy as in
the classiﬁcation setting and create triplets of the form (v1, v2, v3)
where v1 is the query, v2 is a relevant document for the query, and
v3 is an irrelevant document.

3.2 Computing the bit vector representation
Let v ∈ (cid:4)D be the D-dimensional input vector (either a query
or a document) for which we want to compute the bit vector. First
we deﬁne a function for computing a real-valued B-dimensional
vector h whose components have values between 0 and 1, h ∈
[0, 1]B, from v. The bit vector is subsequently computed from h
by rounding. The jth component of h is given by

hj =

1 + exp

“

“
−

1
bj +

P

D

i=1 Wijvi

”” ,

(1)

where vi is the ith component of h, Wij is a learnable weight pa-
rameter, bj is a learnable bias, and 1/(1+exp(−(·)) is the sigmoid
function. The function for computing the entire h vector is param-
eterized in terms of a D × B matrix W and the B bias parameters
(ﬁgure 2b). To simplify notation we omit bias terms from the equa-
tions in the rest of the paper without loss of generality5 The bit
vector b is computed by rounding each component of h to 0 or 1.
Note that h is differentiable with respect to W , but b is not because
of the non-smoothness of rounding.

3.3 Smooth ranking function for learning

We want to formulate an algorithm for learning W within a
learning-to-rank framework. To do this we ﬁrst need to deﬁne a
ranking function suited for learning a bit vector representation. It
takes a query and a document as inputs and computes a scalar score
with which the document can be ranked. Given a query vq and a
set of documents {vd1 , vd2 , ..., vdn} (all in (cid:4)D), the score can be
computed for each query-document pair, followed by a sort, to rank
the documents with respect to the query.
Let bq ∈ {0, 1}B be the bit vector for the query and bi ∈
{0, 1}B be the bit vector for the ith document. The ranking score
si for the ith document is the Hamming distance between bq and
bi. Documents are ranked in ascending order of the score.

For gradient-based learning we need the ranking score to be dif-
ferentiable with respect to W . The score si does not meet this re-
quirement because rounding is not a differentiable function. So we
compute an alternative score ˆsi using h (from equation 1) instead.
Since h is real-valued, Hamming distance is no longer appropriate.
One alternative is to use Euclidean distance, i.e. let the score for the
ith document be the Euclidean distance between hq and hi. When
the components of hq and hi are exactly binary, the score will be
the same as Hamming distance. One drawback is that the score can
also be zero even when hq and hi are not binary, but still identical.
For example, if the components of both vectors are all 0.5, then the
score will be zero.

A second alternative is the following:

ˆsi =

NX

j=1

hqj(1 − hij) + (1− hqj)hij,

(2)

where hqj and hij are the jth component of hq and hi, respec-
tively.
It ‘relaxes’ Hamming distance to non-binary values, and
becomes equal to Hamming distance for binary values. It is not a
proper distance metric – two identical vectors will not give a score
of zero unless they are binary. We use this relaxed Hamming score
in our experiments.

5The biases can be put into W by appending an extra constant el-
ement 1 to the input vector and deﬁning W to be a (D + 1) × B
matrix where the extra row corresponds to the biases.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France9323.4 Learning with RankNet

gradient as follows:

As mentioned before, LambdaRank is based on RankNet. Now
we describe RankNet in some detail because it is required for Lamb-
daRank. Also it serves as an important baseline in our experiments
to demonstrate the usefulness of task-speciﬁc optimization.

So far we have formulated a ranking function that incorporates
the smooth version of the bit vector representation and is differen-
tiable with respect to W . Now we plug this ranking function into
the RankNet algorithm to learn W .

Given a query vq and a pair of documents, vd1 and vd2, let
vd1  vd2 denote the event that vd1 is ranked higher than vd2.
RankNet adopts a probabilistic view that allows for uncertainty in
the pairwise ranking. The ranking scores of the two documents
do not deterministically imply an ordering. Instead they deﬁne a
probability that vd1 is ranked higher than vd2:

P (vd1  vd2 ) =

1

1 + exp(ˆs1 − ˆs2)

,

(3)

where ˆs1 and ˆs2 are the ranking scores for vd1 and vd2, respec-
tively (as computed by equations 1 and 2).

Note that if ˆs1 < ˆs2, then P (vd1  vd2 ) > 0.5. Intuitively if
one document is closer to the query than the other, it has a higher
probability of being ranked higher than the farther one. And if
ˆs1 << ˆs2 (document 1 is much closer to the query than document
2), then P (vd1  vd2 ) ≈ 1.

Consider the simple case where our training set consists of only
one triplet (vq, vd1 , vd2 ). Let T12 be the target probability that
vd1 is ranked higher than vd2 with respect to vq. For example,
T12 = 1 if vd1 should be ranked higher than vd2, and 0 if the
opposite is true. The cost function used by RankNet for learning is:
C = −T12 log P (vd1 vd2 )−(1−T12) log(1−P (vd1 vd2 )).
(4)
C is the cross entropy between the two Bernoulli distributions
P (vd1  vd2 ) and T12. The parameters of the ranking function
are learned via gradient descent on C. C is a smooth differentiable
function of W , and an expression for ∂C
∂W can be derived analyti-
cally. Equation 4 considers only a single training triplet, but it can
be applied to the case where multiple training triplets are available
by simply minimizing the average cost of all the triplets.

Note that with RankNet only the way in which the triplets are
deﬁned differs from task to task. The training cost function and
the optimization procedure stay the same across tasks. In contrast,
LambdaRank allows a different cost function for each task, which
makes it possible to customize the learning.
3.5 Learning with LambdaRank

We begin with a general description of LambdaRank, and then
describe the adaptations speciﬁc to learning a bit vector represen-
tation for classiﬁcation and retrieval in sections 3.5.1, 3.5.2, and
3.5.3.

We keep the parameterization of the ranking function and the
deﬁnition of the ranking score the same as before, but change the
learning algorithm to LambdaRank (ﬁgure 3). This affects two
things – 1) the objective function used for learning, and 2) the pro-
cedure for computing its gradient.

For simplicity we present LambdaRank here as a modiﬁed ver-
sion of RankNet, but the underlying ideas are more general. Con-
sider again a RankNet training triplet (vq, vd1 , vd2 ) where vq is
a query, and vd1 and vd2 are two documents to be ranked with
respect to vq. Suppose that we want the learning to optimize the
ranking function for an evaluation score S. S can be a listwise rank-
ing score, e.g. NDCG. Then LambdaRank modiﬁes the RankNet

∂C
∂W

|ΔS|

(5)

where C is the RankNet cost function (equation 4) and W is the
weight matrix to be learned. |ΔS| is the absolute difference in the
value of S due to swapping the positions of vd1 and vd2 in the
ordering of all documents, with respect to vq, computed by the
current ranking function.

Note that LambdaRank learns on triplets, as before, but now only
those triplets that produce a non-zero change in S by swapping
the positions of the documents contribute to the learning. Given a
query, ﬁrst all documents are ordered with respect to it using the
ranking function given by the current W . Evaluating this ordering
will give some score S1. Now pick any two documents and swap
their positions in the ordering. Evaluating this new ordering will
give some score S2. LambdaRank then computes the RankNet gra-
dient for those two documents and the query, and multiplies it by
|S1−S2|. Only the score function S needs to be changed from task
to task in LambdaRank.

We now describe aspects of LambdaRank training that are spe-

ciﬁc to our approach.

Score function for classiﬁcation:

3.5.1
Suppose that for a query vq, L documents {vd1 , vd2 , ..., vdL}
are selected as neighbors (as explained later). Let lq and ldi be the
class labels of the query and the ith document, respectively. The
score function SC for nearest neighbor classiﬁcation of a single
query is:

SC = [M AJORIT Y (ld1 , ld2 , ..., ldL ) =l q],

(6)
where the M AJORIT Y (·) function picks the most frequent label
in {ld1 , ld2 , ..., ldL} and [·] is Iverson notation denoting an output
1 if the argument is true and 0 otherwise.

One disadvantage of the above score is that it does not reﬂect
incremental progress towards the correct answer. For example, if
the correct label is three votes short of becoming the majority, then
a swap that increases its vote count by 1 will not change the above
score. But such a swap moves the result closer to the correct answer
and therefore should be used. To allow for such swaps, we use the
following the score:

LX

[ldi = lq].

SC =

(7)

i=1

Note that without the M AJORIT Y (·) function, even swaps that
would make all the neighbors have the same label as the query
would be allowed. This is more stringent than necessary for nearest
neighbor classiﬁcation. A score function with a looser requirement
may give better accuracy, but we have not yet explored that.
Thresholding for neighbor selection: There are two types of thresh-
olding that can be used on the Hamming distance. An absolute
threshold k selects all documents with Hamming distance ≤ k from
the query as neighbors. A relative threshold k selects only those
documents in the k nearest non-empty Hamming distance bins from
the query as neighbors. Consider an example where the nearest
documents from the query appear at say, distances 4, 7, 12, 17, 20,
etc. and k = 3. With an absolute threshold, no documents would
be selected, which makes nearest neighbor classiﬁcation ambigu-
ous. But with a relative threshold, documents at distances 4, 7, and
12 would be selected. Since a relative threshold guarantees that the
set of neighbors will always be non-empty, we use it for nearest
neighbor classiﬁcation.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France933Resolving ties: If we use a B-dimensional bit vector representa-
tion, then Hammming distance can take on only B + 1 possible
values (0 to B). If the number of documents being ranked is much
greater than B + 1, then there will be a lot of ties in the ranking.
For nearest neighbor classiﬁcation, we do not break ties. Instead
we use all the documents from the selected bins to vote on the class
label.

3.5.2 Score function for retrieval:

In section 5 we consider two retrieval tasks where the relevance
label of a document is binary (either 0 or 1), and accuracy is mea-
sured using precision (# of retrieved documents that are relevant
divided by # of retrieved documents). We deﬁne the score function
for this particular setting. Again consider a query vq, and L docu-
ments {vd1 , vd2 , ..., vdL} retrieved for that query with an absolute
threshold k. Let ldi be the relevance label for the ith document.
The score function SR for retrieval is:
LX

SR =

ldi .

(8)

i=1

We do not normalize the score by L in order to prevent queries
that have a small number of retrieved documents from contributing
disproportionately to the gradient.
Resolving ties: During learning, we do not break ties in the rank-
ing. But during testing, tie-breaking may be needed. For example,
consider a task that requires retrieving exactly 100 documents for
a given query. If the nearest Hamming distance bin to the query
contains 110 documents, then some re-ranking procedure would be
needed to select exactly 1006. In such an application a bit vector
representation is still useful for rapidly shortlisting a small set of
documents for the re-ranker. Note that tie-breaking is needed for
any approach that uses bit vectors, not just ours.
3.5.3 Procedure for ﬁnding swaps with |ΔS| > 0
A key step in the per-query gradient computation of LambdaRank
is to identify only those pairwise swaps that have nonzero |ΔS|
(steps 3 and 4 in ﬁgure 3). At ﬁrst glance, ﬁnding such swaps may
appear to be a very expensive computation that requires sorting all
the documents in the training set for every query. But in the case of
learning a bit vector representation, a fast approximation turns out
to be possible.

Given a query, there are only a ﬁxed number of Hamming dis-
tance bins that the documents can belong to. For efﬁciency, we use
swaps only among those documents that belong to a small subset
of bins closest to the query. Finding documents that belong to the
nearest bins can be done efﬁciently using a heap structure, without
having to compute the full Hamming distance and sorting over the
entire training set.

IMPLEMENTATION DETAILS

4.
Subsampling: To signiﬁcantly speed up the per-query gradient
computation, we subsample the set of documents from which doc-
ument pairs are selected for each query. A noisy estimate of the
gradient can be computed cheaply from a small subset of the full
document set. This is helpful when the training set is large and
computing the gradient from the full set is too expensive.

In the case of RankNet, for each query we use only a small subset
of the full training set to generate pairs for that query. Most of our
experiments (section 5) use only 100 randomly chosen documents
per query to generate triplets.

6One possibility is to sort by the real-valued score ˆsi (equation 2)
and select the top 100.

LambdaRank learning algorithm:
Training inputs:
- A ranking metric M (e.g. NDCG) to be optimized.
- A set of training examples where the ith example contains:

∈ (cid:3)D,

1. Query vi
q
2. List Li of N documents {vi

, ..., vi

dN

} all in (cid:3)D.

d1

- Number of bits B.
- Learning rate parameters: step-size η, momentum m.
Training outputs: D × B weight matrix W .
Initialization: Wij are sampled from zero-mean Gaussian with small
(10−3) variance, ΔW = zero matrix.

Weight update computed using the ith training case:

1. Compute real-valued representation hq for vi
2. For the kth document in Li compute:

q using equation 1.

(a) Compute real-valued representation hk for vi

dk using

equation 1.

(b) Compute score ˆsk using equation 2 from hq and hk.

3. Compute an ordering O of the documents in Li by sorting them
in ascending order of the scores ˆsk. Let SO be the value of the
evaluation score S for O. For an efﬁcient approximation, see
sections 3.5.3 and 4.

4. Find the set V of pairs of documents (vi
a
such that swapping the positions of (vi
a and vi
for which |SO − SO(cid:2)| > 0.
a new ordering O(cid:2)
b
) ∈ V compute P (vi

5. For each element (vi
a

, vi
b

, vi
b

equation 3.

) selected from Li
) in O results in

 vi
b

) using

a

with respect to W .

∂W of the cost function C (equation 4)

6. Compute the gradient ∂C
7. ΔW ← mΔW − η(|SO − SO(cid:2)| ∂C
8. W ← W + ΔW .
Figure 3: Summary of LambdaRank learning algorithm.

∂W

).

For LambdaRank, we restrict the number of bins from which
documents are considered for swaps to be one-third of the number
of bits. For the tasks considered here, swaps only happen between
a document in one of the top k bins and a document outside of
the top k bins, but still within the restricted set. We can make this
even more efﬁcient by considering only a subset of the full training
set (Li in step 2 of ﬁgure 3) to populate these bins. How much
subsampling can be done without degrading accuracy depends on
the dataset, but in our experiments we have seen that good results
can be achieved even with 10× subsampling.
Gradient descent: The weight matrix W is updated by averaging
gradient estimates given by a set of queries and taking a step along
the average gradient. Averaging can reduce the noise in the updates.
The average is typically computed over 100 queries. One can easily
parallelize this computation by splitting the set of queries across
multiple cores and then averaging together the gradients computed
at all cores.

−1, 10

We use a ﬁxed step size to update W . The best value depends on
the dataset – for the datasets in section 5 we have tried values in the
−4]. On some datasets we have observed signiﬁcant
range [10
sensitivity in the accuracy to the step size value. We set the step size
to the highest possible value that does not produce large oscillations
in the objective function value during optimization.

We have not yet tried any second-order optimization methods
like conjugate gradient to improve convergence speed. Such meth-
ods may help in RankNet training, but are unlikely to be useful

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France934for LambdaRank since the actual objective function is implicit and
cannot be directly evaluated.

As in [11], we maintain an exponentially decaying sum of the
previous gradients which is added to the current gradient to com-
pute the weight update. The decay factor is set to 0.8, so the effect
of the gradient computed at a particular step persists for several
steps afterwards.

5. EXPERIMENTAL EVALUATION

Performance is evaluated on two types of tasks: 1) nearest neigh-
bor classiﬁcation and 2) retrieval. We present results for four clas-
siﬁcation and two retrieval datasets.
5.1 Metrics

The evaluation metric for classiﬁcation is the number of incor-
rect label predictions on a test set. Given a test case, its bit vector is
compared against the bit vectors for all the training cases. Neigh-
bors are selected using a relative threshold k – those training cases
that fall within the nearest k non-empty Hamming distances to the
test case bit vector are returned. The predicted label is then picked
by a majority vote among the neighbors.

For retrieval we use the same precision metric as in Weiss et al.
[23]. Given a query, its bit vector is compared against the bit vec-
tors for all the documents in the search set. Documents are selected
using an absolute threshold – those documents that are less than a
pre-speciﬁed Hamming distance threshold from the query bit vec-
tor are retrieved. We consider binary relevance labels here7 – a
document is either relevant for a query or not. So precision for a
single query is computed as follows:

Precision =

# of relevant documents in the retrieved set

# of documents in the retrieved set

.

(9)

This quantity is then averaged over a held-out set of queries to get
a single precision value.

5.2 Datasets

Tables 2 and 3 summarize the datasets. The datasets span a range
of training set sizes (60K to 1.45 million), input dimensionality
(128 to 47K), and in the case of classiﬁcation, number of classes (7
to 101). They include a number of different types of data – images
(MNIST, INRIA SIFT 1M, Tiny Images Subset), text documents
(MCAT, RCV), and geospatial measurements (Covertype).

We brieﬂy describe each dataset:

MNIST8 is a set of handwritten images of the digits 0 to 9. The
images are grayscale and of size 28 × 28.
MCAT contains text documents that belong to a subtree of the
Reuters Corpus Volume 1 (RCV1) dataset [14]. A document is rep-
resented as a bag-of-words with a vocabulary size of 11429. Only
about 0.58% of the word counts are nonzero, so the representation
is sparse.
Covertype9 [3] is a dataset of geospatial measurements that are
used to predict the forest covertype at various locations in the US.
RCV1 Subset10 contains only those documents in the RCV1 dataset
that do not have multiple labels associated with them. As in MCAT,
a document is represented as a bag-of-words, but with a much

7The two retrieval datasets we use supply binary relevance labels.
8http://yann.lecun.com/exdb/mnist/
9http://archive.ics.uci.edu/ml/datasets/Covertype
10http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
binary.html\#rcv1.binary

Dataset

Train set Test set

MNIST
MCAT
Covertype
RCV1

size
60000
150344
522911
531742

size
10000
4362
58101
15913

Input Number of
dim.
784
11429

classes

10
7
7
101

54

47236

Table 2: Classiﬁcation datasets.

Dataset

INRIA
SIFT1M
Tiny Images
Subset

Train set

size

100000

# of test
queries
10000

# of test docs

to search
1000000

Input
dim.
128

1458356

100000

1458356

512

Table 3: Retrieval datasets.

larger vocabulary size of 47236 and 101 classes. The represen-
tation is 0.14% sparse.
INRIA SIFT1M11 is a web image dataset designed for evaluating
retrieval algorithms. It consists of 128-dimensional SIFT features
[15] computed for images collected from the web. Given a query
image’s SIFT feature vector, the relevant images to retrieve are de-
ﬁned to be its 50 nearest neighbors, according to Euclidean dis-
tance, in the SIFT feature space.
Tiny Images Subset is derived from the Tiny Images dataset [21],
which contains 80 million images collected from the web. Various
text queries were given to popular image search engines and the
results were downloaded. The subset we use here contains only the
top-ranked images for the query terms, so it is likely to be less noisy
than the full 80 million set. The images are represented using 512
dimensional GIST feature vectors [17]. The retrieval task is deﬁned
in the same way as in INRIA SIFT1M: the goal is to retrieve the
50 nearest neighbors, according to Euclidean distance, of a query
image’s GIST feature vector.

5.3 Baselines

We compare against other methods that compute a bit vector
representation. For retrieval, Spectral Hashing is a state-of-the-art
method. Binary LSH is a commonly used baseline in the literature,
so we compare against it as well. We use the Matlab implementa-
tion of Spectral Hashing by the authors of that paper12.

For classiﬁcation, we again use Spectral Hashing and LSH as
baselines. However these methods were not intended to be used for
classiﬁcation, so they cannot be taken as strong baselines. There-
fore we decided to compare also against nearest neighbor methods
that do not use bit vectors. The simplest one is kNN classiﬁcation
with L2 distance. State-of-the-art learning methods are NCA and
LMNN. LMNN does not scale to datasets with more than a few tens
of thousands of training cases [22], so we cannot run it on Cover-
type, MCAT, and RCV1 Subset. For MNIST we quote the LMNN
classiﬁcation error from [22]. For NCA, we use the implementation
in the Matlab Toolbox for Dimensionality Reduction13.

Note that one can always apply binary LSH on top of a metric
learning method like NCA or LMNN to construct bit vectors. But
such a two-stage approach will at best give the same accuracy as the

11http://corpus-texmex.irisa.fr/
12www.cs.huji.ac.il/~yweiss/SpectralHashing/
13http://homepage.tudelft.nl/19j49/Matlab\_Toolbox\_for\
_Dimensionality\_Reduction.html

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France935underlying real-valued metric since binary LSH is only an approxi-
mation of it. So we compare against the accuracy of the real-valued
metric directly, i.e. the best-case result for the two-stage approach.
On the RCV1 Subset, Spectral Hashing required doing an eigen-
value decomposition of the 47236 × 47236 data covariance matrix
and selecting the top eigenvectors. For 32 and 64 bits, Matlab was
not able to perform this computation due to excessive memory use,
even with 32GB of RAM. Those results are not given. In NCA,
the dimensionality of the output of the linear projection matrix can
be made less than the input dimensionality. The dimensionality we
chose is 60 for MNIST, 500 for MCAT, 54 for Covertype, and 500
for RCV. On MNIST 60 gave the same accuracy as bigger values.
On Covertype the input dimensionality is already small (54), so a
lower number was not tried. On MCAT and RCV, we chose 500 to
keep the training times reasonble.
5.4 Classiﬁcation Results

Table 4 shows the test set error rates for the four classiﬁcation
datasets. We use a relative threshold of k = 3 for all methods
that use bit vectors. LambdaRank achieves the lowest classiﬁca-
tion error on three out of four datasets. Among bit vector meth-
ods, LambdaRank is 48.9% better than others (averaged over four
datasets), with RankNet being the closest competitor. LambdaRank
is on average 26.8% better than the best RankNet result. Only on
the Covertype dataset does RankNet perform comparably to Lamb-
daRank. So task-speciﬁc optimization by LambdaRank improves
accuracy, and as MNIST and Covertype results show, the improve-
ment can be substantial (+54% and +42%, respectively).

Recall from section 3.5.1 that there can be ties in the Hamming
distance ranking. A relative threshold k only guarantees that the
number of neighbors used to classify a given test case is at least k.
The actual number can be much larger than k if there are many ties.
All the four bit vector methods compared in table 4 share this prop-
erty. The better accuracy of LambdaRank despite this commonality
implies that simply having a large number of neighbors to classify
a test case is not sufﬁcient for high accuracy, and that learning a
good bit vector representation is also crucial.

The beneﬁt of LambdaRank can also be measured in terms of
the bit “compression” it gives with respect to the other methods
while matching their best error. In many cases LambdaRank needs
signiﬁcantly fewer bits. To get a rough idea of how big the com-
pression factor is, we linearly interpolate between the datapoints in
table 4 to determine the number of bits needed by LambdaRank to
achieve a particular error rate. On average the ratio of the number
of bits needed by LSH, Spectral Hashing and RankNet to achieve
their best error rate divided by the number of bits needed by Lamb-
daRank to achieve the same error rate is 7.1×, 4.1×, and 2.0×,
respectively.

5.5 Retrieval Results

Table 5 shows the precision results computed on the test sets
of INRIA SIFT1M and Tiny Images Subset. Again, LambdaRank
gives the best results on both datasets. It gives 1.2× and 2.66×
better precision than Spectral hashing on the two datasets.

The improvements over RankNet are again substantial for both
datasets: +6.5% and +13.6%. This adds further evidence to the
usefulness of task-speciﬁc learning.
5.6 Training and Testing Times

The training time of our algorithm scales well to large datasets.
On Tiny Images Subset (1.46 million training cases), learning with
8-core parallelization converges in approximately 2 days on an In-
tel Xeon 2.50GHz machine. Figure 4 shows the convergence be-

No. of

bits

LSH Spectral Rank
Net

hashing

Lambda

Rank

Non-bit vector

methods

8
16
32
64
128
256

8
16
32

8
16
32
64
128
256

8
16
32
64

58.73
52.15
24.61
13.20
7.71
4.64

67.35
60.39
42.85

42.07
33.96
20.65
12.33
9.55
7.64

84.63
75.20
63.79
50.95

MNIST
12.64
8.50
5.54
4.20
4.01
3.55
MCAT
1.70
1.38
1.42

Covertype

29.00
26.54
21.62
18.50
15.40
9.58
RCV1
45.60
18.56
16.84
13.34

12.32
7.57
4.85
4.02
2.37
1.63

1.54
1.47
1.31

30.36
21.60
14.24
10.88
7.74
5.52

25.21
15.93
14.41
12.58

33.63
19.67
10.10
6.82
5.74
4.63

24.97
8.02
5.36

42.17
34.30
27.29
14.68
9.46
7.44

75.24
59.46

-
-

kNN, L2: 3.09

NCA: 2.45
LMNN: 1.72

kNN, L2: 3.67

NCA: 7.66

kNN, L2: 6.25

NCA: 4.01

kNN, L2: 29.67

NCA: 45.79

Table 4: Classiﬁcation error (%) on the test sets of MNIST, MCAT,
Covertype, and RCV1.

No. of

LSH

Spectral
hashing

Rank
Net
INRIA SIFT1M (Precision ×10
15.00
184.61
1687.21
Tiny Image Subset (Precision ×10

14.26
200.04
1462.76

6.11
44.70
476.29

bits

8
16
32

8
16
32

Lambda

Rank

−4)

17.16
266.63
1805.25
−5)

6.93
34.58
410.41

28.73
211.39
3396.62

36.85
430.98
7979.02

42.93
578.99
9065.89

Table 5: Precision at Hamming distance < 2 from the query on the
test sets of INRIA SIFT1M and Tiny Images Subset.

haviour. Even after one pass through the data, the precision on the
test set has already reached 90% of its ﬁnal value. The memory
needs of the learning algorithm are minimal. The weight matrix,
its gradient, and the bit vectors for the training set (computed us-
ing the current weight matrix) account for most of the memory use,
and all of these together takes up much less memory than the train-
ing data itself (e.g. for Tiny Images Subset these variables take up
0.2% of the memory occupied by the training data).

Table 6 shows the CPU times needed for ﬁnding the nearest
neighbors for the test set of different datasets by the different al-
gorithms. Note that nearest neighbor classiﬁcation with bit vectors
is substantially faster than with real-valued representations, even
without using an inverted index. This is because the distance cal-
culation for bit vectors is done using bitwise XOR on chunks of 8
bits, and the 8-bit result is converted into a Hamming distance with
a lookup table. The distances from the 8-bit chunks are then added
together to get the full distance. So computing the Hamming dis-

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France936 

 

y
r
e
u
q
m
o
r
f
 
2
<
 
e
c
n
a
t
s
i
d
g
n
i
m
m
a
H

 

 
t
a
 

n
o
i
s
i
c
e
r
P

14.5 hours

2 days

0.1

0.08

0.06

0.04

0.02

0
0

1

2

Epochs

3

4

5

Figure 4: Precision on the test set as a function of the number of
passes through the Tiny Images Subset training data. Note that
most of the improvement happens in the ﬁrst pass.

Dataset

Lambda

MNIST
MCAT

Covertype

RCV1 Subset

Rank
23.99
8.85
765.83
127.78

L2
kNN
756.81
385.13
1744.83
5457.93

NCA

121.19
707.84
1785.33
10706.11

Table 6: CPU time (seconds) required to ﬁnd the nearest neighbors
for the entire test set by different methods on the various datasets.

tance between two 256-bit vectors requires only 256 bitwise XORs,
32 accesses into a lookup table and 32 adds.

For INRIA SIFT1M and Tiny Images Subset, the number of bits
in the learned representation is small enough to build an inverted
index that ﬁts into 4GB RAM. We use subroutines from the Spec-
tral Hashing software (see footnote 11) to build the inverted index
and use it for search. The CPU times for ﬁnding the nearest neigh-
bors on the test sets of INRIA SIFT1M and Tiny Images Subset
with a 32-bit representation and an inverted index are 0.10s and
1.37s, respectively, when run on a single core of an Intel Xeon
2.33GHz machine with 4GB RAM. This translates to an average
search time per query of 10 microseconds for INRIA SIFT1M and
13.7 microseconds for Tiny Images Subset. Note that linear search
on the same 32-bit representation is much slower than the inverted
index. The total CPU times for ﬁnding the nearest neighbors on the
test sets of INRIA SIFT1M and Tiny Images Subset are 63.09s and
1674.40s, respectively.

6. EXTENSIONS AND FURTHER APPLICA-

TIONS

Learning nonlinear features: Currently the ranking function does
not contain any learnable nonlinear features of the input vector. In-
cluding such features is easy and can make the function more ﬂex-
ible and accurate. For example, one can use hidden units from the
neural network literature to learn nonlinear features.
Cascade architecture: A common way to speed up search is to use
a multi-stage, cascade architecture where a fast ﬁrst stage search
rules out a large fraction of the search set, followed by increas-
ingly slower stages that only need to search the set retrieved by the
previous stage. Such an architecture can also be used in our case
– different ranking functions can be cascaded in increasing order
of the number of bits, with each stage searching only among the
neighbors found by the previous stage. We have tried a two stage

classiﬁer on the MNIST dataset with an 8-bit ranking function ﬁrst
and then a 256-bit classiﬁer. The search set size for the 256-bit
classiﬁer reduces by an order of magnitude with almost no loss in
accuracy.

Note that with LambdaRank it is possible to modify the training
of the initial ﬁltering stages such that they are explicitly trained to
ﬁlter (and not classify). We have not yet explored this option.
Hierarchical classiﬁcation: If a hierarchy over classes is given
in a classiﬁcation task, then the score function for LambdaRank
can be modiﬁed to incorporate this information. Instead of binary
relevance (1 for a document with the same label as the query, 0 oth-
erwise), now we use multiple relevance levels to represent varying
degrees of similarity between two classes. The similarity between
two classes can be deﬁned in many ways, e.g. with a monotoni-
cally decreasing function of the height of the common ancestor of
the two classes in the hierarchy. For a query vq and L documents
{vd1 , vd2 , ..., vdL} retrieved from the top K non-empty Hamming
distance bins, we can modify the original classiﬁcation score func-
tion (equation ) as follows:

SHier =

F (lq, ldi ).

(10)

LX

i=1

where lq and ldi are the labels of the query and the ith document,
respectively, and F (a, b) is a function that speciﬁes the similarity
between classes a and b.
Regression: As mentioned before, a bit vector representation for
nearest neighbor regression can be learned using our approach. In
the case of RankNet, pairwise ranking of a document pair with re-
spect to a query can be done using the absolute difference between
the query target and a document target. The document with the
closer target to the query in the pair should get ranked higher.
For LambdaRank we need to deﬁne a score function. Consider a
query vq, and L documents {vd1 , vd2 , ..., vdL} retrieved from the
top K bins for that query. Let tq and tdi be the regression target
values for the query and the ith document. The score can be the
mean squared error between tq and the document targets:

SRegression =

1
L

(tq − tdi )
2.

LX

i=1

(11)

As a preliminary experiment, we have trained a 64-bit RankNet
model on the SARCOS dataset14 (see section 2.5 of [18]). It achieves
a better standardized mean squared error than a linear regression
model. Experiments with LambdaRank is left as future work.

One potential application of nearest neighbor regression is in
Collaborative Filtering. Neighborhood based models have been
shown to be useful for predicting the rating a user would give to
an item [2]. For example, in a user-based neighborhood model,
given a user-item pair for which to predict a rating, a bit vector-
based representation can be used to retrieve similar users who have
rated the same item. The predicted rating is then computed as a
weighted average of the ratings of the retrieved users.

7. OTHER RELATED WORK

We discussed several popular methods closely related to our work
in the introduction, and empirically compared with representative
methods. Here, we present other related work.
Hashing Methods Kernel LSH [13] is a recent scheme that gener-
alizes LSH, and is useful when similarity measure is given directly
via a kernel function (without explicit knowledge of the underly-
ing transformation), or the underlying transformation is inﬁnitely
14http://www.gaussianprocess.org/gpml/data/

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France937dimensional (hence, incomputable). He et al. [10] propose a joint
optimization method to optimize the codes for both preserving sim-
ilarity as well as minimizing search time.

The main drawback of these hashing approaches is that they can-
not be directly used in applications where we are not given a sim-
ilarity metric but rather class/relevance labels that indicate which
data points are similar or dissimilar to each other.
Metric Learning Methods There are other methods that learn the
distance metric by optimizing task speciﬁc performance measure.
Classiﬁcation task oriented methods such as NCA and LMNN dis-
cussed before fall under this category. Other approaches include
Fisher’s linear discriminant analysis (FLDA), maximally collaps-
ing metric learning algorithm (MCML) [8], relevance component
analysis (RCA) [20], etc. While FLDA and MCML use class la-
bel information, RCA assumes that a set of chunklets is available,
where each chunklet is a set of examples which belong to the same
class (but, the class information is unknown; hence, RCA can be
seen as a weaker form of supervised learning). Both FLDA and
RCA involve matrix inversion (in the dimension of input space),
and use projections on eigen vectors for nearest neighbor classi-
ﬁcation. Therefore, they are computationally expensive for high
dimensional data. MCML method tries to collapse all examples
belonging to the same class into a single point, and keep examples
belonging to other class far away. Like LMNN, MCML uses pos-
itive deﬁnite matrix constraints during training. All these methods
are not scalable, and are limited to classiﬁcation application.

To summarize, our work can be seen as merging two streams
of work, one for learning a task-speciﬁc metric from labeled data,
and the other for learning bit vector representations for doing fast
search. To our knowledge this is the ﬁrst attempt that combines the
two types of learning into a uniﬁed framework.

8. CONCLUSIONS

We have presented a uniﬁed approach to learning a bit vector
representation for nearest neighbor search in different tasks. The
key contribution is the ability to customize the learning algorithm
to the task at hand by modifying the score function used in Lamb-
daRank training. What makes the uniﬁed approach possible is that
the differences across the tasks are abstracted out of the core near-
est neighbor search problem and pushed into the score function in
LambdaRank. As a result the same algorithm can be easily adapted
to different tasks to learn accurate representations for each. Ex-
perimental results clearly demonstrate that our algorithm 1) out-
performs other methods for nearest neighbor classiﬁcation and re-
trieval, and 2) scales well to large text and image datasets, making
it particularly useful for web applications.

9. REFERENCES
[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for

approximate nearest neighbor in high dimensions. In In
FOCS 2006, pages 459–468. IEEE Computer Society, 2006.
[2] R. Bell, Y. Koren, and C. Volinsky. Modeling relationships at

multiple scales to improve accuracy of large recommender
systems. In SIGKDD, KDD ’07, pages 95–104, New York,
NY, USA, 2007. ACM.

[3] J. A. Blackard and D. J. Dean. Comparative accuracies of

artiﬁcial neural networks and discriminant analysis in
predicting forest cover types from cartographic variables.
Computers and Electronics in Agriculture, vol.24:131–151,
1999.

[4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using

gradient descent. In ICML, ICML ’05, pages 89–96, New
York, NY, USA, 2005. ACM.

[5] C. J. Burges, R. Ragno, and Q. V. Le. Learning to rank with

nonsmooth cost functions. In B. Schölkopf, J. Platt, and
T. Hoffman, editors, NIPS 19, pages 193–200. MIT Press,
Cambridge, MA, 2007.

[6] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale
online learning of image similarity through ranking. J. Mach.
Learn. Res., 11:1109–1135, March 2010.

[7] P. Donmez, K. M. Svore, and C. J. Burges. On the local

optimality of lambdarank. In SIGIR, pages 460–467, New
York, NY, USA, 2009. ACM.

[8] A. Globerson and S. T. Roweis. Metric learning by

collapsing classes. In NIPS, pages –1–1, 2005.

[9] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov.

Neighbourhood components analysis. In NIPS 17, pages
513–520. MIT Press, 2004.

[10] J. He, R. Radhakrishnan, S.-F. Chang, and C. Bauer.

Compact hashing with joint optimization of search accuracy
and time. In IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR), June
2011.

[11] G. Hinton and R. Salakhutdinov. Reducing the

dimensionality of data with neural networks. Science,
313(5786):504 – 507, 2006.

[12] P. Jain, B. Kulis, and K. Grauman. Fast image search for

learned metrics. CVPR, 0:1–8, 2008.

[13] B. Kulis and K. Grauman. Kernelized locality-sensitive

hashing for scalable image search. In ICCV, 2009.

[14] D. D. Lewis, Y. Yang, T. G. Rose, F. Li, G. Dietterich, and

F. Li. Rcv1: A new benchmark collection for text
categorization research. JMLR, 5:361–397, 2004.

[15] D. G. Lowe. Distinctive image features from scale-invariant

keypoints. Int. J. Comput. Vision, 60:91–110, November
2004.

[16] B. Mcfee and G. Lanckriet. Metric learning to rank. In

ICML, 2010.

[17] A. Oliva and A. Torralba. Modeling the shape of the scene:

A holistic representation of the spatial envelope. IJCV,
42:145–175, 2001.

[18] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes

for Machine Learning. MIT Press, 2006.

[19] R. Salakhutdinov and G. Hinton. Semantic Hashing. In

SIGIR workshop on Information Retrieval and applications
of Graphical Models, 2007.

[20] N. Shental, T. Hertz, D. Weinshall, and M. Pavel.

Adjustment learning and relevant component analysis. In
ECCV, ECCV ’02, pages 776–792, London, UK, UK, 2002.
Springer-Verlag.

[21] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny
images: A large data set for nonparametric object and scene
recognition. PAMI, 30:1958–1970, November 2008.

[22] K. Q. Weinberger and L. K. Saul. Distance metric learning

for large margin nearest neighbor classiﬁcation. JMLR,
10:207–244, June 2009.

[23] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In

NIPS, pages 1753–1760, 2008.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France938