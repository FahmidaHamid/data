Distributed Graph Pattern Matching

Shuai Ma

Yang Cao

Tianyu Wo
State Key Laboratory of Software Development Environment

Jinpeng Huai

Beihang University, Beijing 100191, China

{mashuai@act, caoyang@act, huaijp, woty@act}buaa.edu.cn

ABSTRACT
Graph simulation has been adopted for pattern matching
to reduce the complexity and capture the need of novel ap-
plications. With the rapid development of the Web and
social networks, data is typically distributed over multiple
machines. Hence a natural question raised is how to evalu-
ate graph simulation on distributed data. To our knowledge,
no such distributed algorithms are in place yet. This paper
settles this question by providing evaluation algorithms and
optimizations for graph simulation in a distributed setting.
(1) We study the impacts of components and data locali-
ty on the evaluation of graph simulation. (2) We give an
analysis of a large class of distributed algorithms, captured
by a message-passing model, for graph simulation. We also
identify three complexity measures: visit times, makespan
and data shipment, for analyzing the distributed algorithm-
s, and show that these measures are essentially controversial
with each other. (3) We propose distributed algorithms and
optimization techniques that exploit the properties of graph
simulation and the analyses of distributed algorithms. (4)
We experimentally verify the eﬀectiveness and eﬃciency of
these algorithms, using both real-life and synthetic data.

Categories and Subject Descriptors
H.2.8 [Database Management]: Database applications—
graph data, data mining

Keywords
Graph querying, graph simulation, distributed algorithms

1.

INTRODUCTION

Graph pattern matching is being increasingly used in vari-
ous applications, e.g., software plagiarism detection, protein
interaction networks, social networks and intelligence analy-
sis [18, 25, 26]. Graph matching is typically deﬁned in terms
of subgraph isomorphism (see, e.g., [15] for a survey). Hence
the problem is np-complete [27]. Furthermore, subgraph i-
somorphism is often too restrictive to catch sensible matches
in emerging applications such as social networks [5, 13].

To reduce the complexity and capture the need of nov-
el applications, graph simulation [16] has been adopted for
pattern matching [5, 13]. It is less restrictive than subgraph
isomorphism, and can be determined in quadratic time [16].
We say that a graph G matches a pattern Q, via graph sim-

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

Figure 1: Pattern query and fragmented data graph
ulation, if there exists a binary relation M ⊆ Vq × V , where
Vq and V are the two sets of nodes in Q and G, respectively,
such that (1) for each (u, v) ∈ M , u and v have the same
label; and (2) for each node u in Q, there existsv in G such
that (a) (u, v) ∈ M , and (b) for each edge (u, u(cid:2)) in Q, there
exists an edge (v, v (cid:2)) in G having (u(cid:2), v(cid:2)) ∈ M . Graph simu-
lation (and its extensions) play a critical role for the analysis
of social positions/roles in social networks [5, 13, 12].

When evaluating a query on a large dataset, one wants to
partition and distribute the data to multiple machines, so
that the query can be eﬃciently evaluated in parallel, as ad-
vocated by, e.g., MapReduce [11] and Pregel [21]. Moreover,
it is common to ﬁnd distributed real-life datasets, such as
Facebook, Yahoo Flickr, Twitter, and Apple AppStore, that
are stored at data centers, which typically host a cluster of
tens to thousands of machines [1]. Hence, a natural question
raised is how to evaluate graph pattern matching, in terms
of graph simulation, on distributed data. To our knowledge,
no such distributed algorithms are in place yet.

Example 1: Consider a real-life example for social match-
ing, taken from [24], that recruits people from a social sys-
tem to set up a team to develop a new software product.

Ideally such a team consists of members with the follow-
ing roles: (1) project manager (PM), (2) business analyst
(BA), (3) software architect (SA), (4) user interface designer
(UD), (5) software developer (SD), and (6) software tester
(ST). All candidates are stored in a social system in the for-
m of data graph G1 as shown in Fig. 1, where (1) a node is
a person labeled with her expertise (with subscripts to dis-
tinguish one from another), and (2) an edge (A, B) indicates
the relationship that B worked well under the supervision of
A in previous projects. The relationships of team member-
s are particularly important as the success of the software
product heavily relies on their collaboration.

To identify the proper candidates from G1, pattern query
Q1 is designed that requires (1) all SA, BA and UD worked
well under the same PM, (2) UD worked well under BA such
that the user interface could clearly reﬂect the idea of BA, (3)
SD worked well under SA and ST, and (4) ST further worked
well under SD as they are mostly related in the development.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France949Data graph G1 is partitioned into ﬁve fragments: F1, . . .,
F5 (separated by dotted cycles), and it is distributed over a
cluster of ﬁve machines, one fragment on each machine. Ob-
serve the following. (1) When graph simulation is used, a
sensible match containing all nodes in F3, F4 and F5 are iden-
tiﬁed. In contrast, when subgraph isomorphism is adopted,
it imposes too strict constraints such that no matches can be
found. (2) Furthermore, to build a well-organized team, one
has to query over all these fragments over diﬀerent machines
to avoid missing any potential candidates.

This highlights the need for developing distributed algo-
rithms for graph simulation. There are a few cheap solu-
tions.
(1) A naive one simply collects all parts of a data
graph into one machine, and calls a centralized algorithm,
e.g., [16]. (2) Another one is to build a MapReduce [11] or
Pregel [21] system, and to delegate most of the work to the
system. There are, however, several problems for these two
solutions. Obviously the ﬁrst solution does not make use
of the distributed facilities at all. Graph simulation needs
recursive computations, as illustrated by its recursive deﬁ-
nition given before. MapReduce is typically not ﬁt for this
kind of graph algorithms, which needs a series of chained
MapReduce invocations [7, 21]. Pregel utilizes a message-
passing model, which is typically ﬁt for graph algorithms.
However, if we simply delegate the computation tasks to
Pregel, it may involve too many rounds of computations.
As one knows, a good solution for graph simulation must
exploit the nature of graph simulation itself.

As observed in [21], graph algorithms often exhibit poor
data locality and hence, may incur prohibitive overhead on
network traﬃc. One may verify that to make G1 match
Q1, it essentially requires the complete information of the
subgraph consisting of fragments F3, F4 and F5 (, even the
entire data graph in the worst case). That is, graph sim-
ulation has poor data locality (more sophisticated analyses
are available in Section 3). This further brings challenge for
developing distributed algorithms for graph simulation. 2

Contributions & Roadmap. To this end, we develop
algorithms and optimization techniques for evaluating graph
simulation in a distributed setting.

Our contributions can be summarized as follows.

(1) We study fundamental properties of graph simulation
(Section 3). We show that connected components in data
graph can be treated separately, and the ﬁnal matches are
simply the union of those matches for all single components.
We also show that graph simulation has poor data locality.
Nevertheless, we identify cases when data locality could be
exploited to facilitate the evaluation of graph simulation.
(2) We give an analysis of a large class of distributed algo-
rithms for graph simulation (Section 4). These distributed
algorithms are captured by a message-passing model, which
is ﬂexible enough to express a broad class of algorithms [19],
and is typically ﬁt for the evaluation of graph algorithm-
s [21]. We also identify three complexity measures: vis-
it times, makespan and data shipment, for the analyses of
this class of distributed algorithms, and show that these
measures are controversial with each other. As eﬃciency
(makespan) remains the dominant factor, we make a deci-
sion to sacriﬁce visit times and data shipment for makespan
when designing distributed algorithms.
(3) We propose distributed algorithms which exploit the
properties of graph simulation and the analyses of distribut-

ed algorithms (Section 5). The algorithms guarantee the
following. (a) The total computation cost at all machines is
comparable to what is needed by the best-known centralized
algorithm [16]. Moreover, the number of rounds of computa-
tion is bounded by a constant 4. (b) The total data shipment
is at most |G| + 4|B| + |Q||G|+(k−1)|Q|, where |G| and |Q|
are the sizes of data graph and pattern graph, respectively,
and |B| is the total number of nodes with edges across diﬀer-
ent fragments (boundary nodes), and k is the total number
of machines. (c) Each machine except the coordinator is vis-
ited at most g + 2 times, where g is the maximum number
of machines at which a connected component resides. The
coordinator is visited with 2(k − 1) extra times due to the
need for scheduling data shipment and assembling the ﬁnal
result. We also develop eﬀective optimization techniques.
(4) Using both real-life data (Google and Amazon) and syn-
thetic data, we conduct an extensive experimental study
(Section 6). We ﬁnd that our distributed algorithms for
graph simulation scale well with large data graphs (e.g., with
108 nodes). We also ﬁnd that our optimization techniques
are eﬀective, reducing 1/5 of running time in average.

Related work. There has been a host of work on graph
pattern matching, via subgraph isomorphism (e.g., [18, 25,
26]; see [3, 15] for surveys) and via graph simulation [16]
and its extensions [13, 12]. Nevertheless, none of these in-
vestigates the problem in a distributed fashion.

Distributed query processing has been studied for rela-
tional data [17] and XML [8]. There has also been recent
work on distributed graph processing to manage large-scale
graphs [11, 21]. However, to the best of our knowledge, no
previous work has studied distributed computation of graph
simulation [16] and its extensions[13, 12].

Close to this work is strong simulation [20], in which the
locality property of strong simulation allows us to develop a
simple yet eﬀective algorithm to ﬁnd matches in distributed
graphs. In contrast, we show that graph simulation has poor
locality, and hence the simple algorithm does not work here.
Message-passing model has been recently adopted for var-
ious famous distributed systems, e.g., Pregel [21]. Our dis-
tributed algorithms follows this model, which is ﬂexible e-
nough to express a broad class of algorithms [19], and is
typically ﬁt for the evaluation of graph algorithms [21].

2. PRELIMINARIES

In this section, we ﬁrst present basic graph notations. We
then introduce the problems of graph pattern matching and
its distributed counterpart, in terms of graph simulation.

2.1 Basic Graph Notations

We specify both pattern graphs and data graphs as fol-

lows. Let Σ be a (possibly inﬁnite) set of labels.
Graphs. A node-labeled directed graph (or simply a graph)
is deﬁned as G(V , E, l), where (1) V is a ﬁnite set of nodes;
(2) E ⊆ V × V is a ﬁnite set of edges, in which (u, u(cid:2))
denotes an edge from nodes u to u(cid:2); and (3) l is a labeling
total function that maps each node u in V to a label l(u) in
Σ. The size of G, denoted as |G|, is the total number of its
nodes and edges, i.e., |V | + |E|. We also denote G as (V, E)
when it is clear from the context.

Intuitively, the function l() speciﬁes node attributes, e.g.,
keywords, blogs, comments, ratings, names, emails, compa-
nies [4]; and the label set Σ denotes all such attributes.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France950Input: Pattern graph Q(Vq, Eq) and data graph G(V, E).
Output: The maximum match M in G for Q.
1. for each node u ∈ Vq do sim(u) := {w | w ∈ V, lQ(w) = lG(u)};
2. while there exist u ∈ Q, w ∈ sim(u) and v ∈ postQ(u)

such that postG(w) ∩ sim(u) = ∅ do

sim(v) := sim(v) \ {w};

3.
4. M := {(v, w) | v ∈ Vq, w ∈ sim(v)};
5. return M .

Figure 2: Centralized algorithm HHK

Subgraphs. Graph H(Vs, Es, lH ) is a subgraph of graph
G(V, E, lG) if (1) for each node u ∈ Vs, u ∈ V and
lH (u) = lG(u), and (2) for each edge e ∈ Es, e ∈ E. That
is, subgraph H only contains a subset of nodes and a subset
of edges of G. We also denote subgraph H as G[Vs] if Es is
exactly the edges that appear in G over Vs.
Descendants. We say that node v is a descendant of node
u in G if v is reachable from u, i.e., there is a directed path
from u to v. We use desc(G, u) to denote the subgraph that
contains the set of all descendants of u in G, including u
itself, and the set of edges in G on those descendants exactly.

2.2 Graph Pattern Matching

We next review the notion of graph simulation [16]. Con-
sider a pattern graph Q(Vq, Eq) and a data graph G(V, E).
A binary relation R ⊆ Vq × V is said to be a match if
(1) for each (u, v) ∈ R, u and v have the same label, i.e.,
lQ(u) = lG(v); and (2) for each edge (u, u(cid:2)) ∈ Eq, there
exists an edge (v, v(cid:2)) in E such that (u(cid:2), v(cid:2)) ∈ R.
Note that an empty binary relation is a match.
Graph G matches pattern Q via graph simulation, denoted
by Q ≺ G, if there exists a total match relation M , i.e., for
each u ∈ Vq, there exists v ∈ V such that (u, v) ∈ M .

Intuitively, simulation preserves the labels and the child
relationship of a graph pattern in its match. Simulation was
proposed for the analyses of programs [16], and studied for
schema extraction from semi-structured data [2]. Simula-
tion and its extensions were recently introduced for social
networks [5] and for graph pattern matching [13, 12].

Graph pattern matching. The problem is to ﬁnd, given
any pattern graph Q and data graph G, the maximum match
in G for Q if P ≺ G. It was known that the following result
holds [16, 13], by which the problem is well deﬁned.

Proposition 1: Given any pattern graph Q and data graph
G, there exists a unique maximum match of G for Q no
matter whether Q ≺ G or not [16, 13].
2
Algorithm HHK. We next present an algorithm for graph
simulation in [16], denoted by HHK and shown in Fig. 2.

For each node u in Q, the set sim(u) contains candidate
nodes in G, initially all nodes in G with the same label as
u (line 1). By the deﬁnition, if (u, v) ∈ Eq (v ∈ postQ(u),
successors of u), but there exist no nodes w(cid:2) ∈ sim(v) such
that (w, w(cid:2)) ∈ E (w(cid:2) ∈ postG(v)), then w cannot be matched
to v, and hence is removed from sim(u). This process is
repeated until there are no more changes (lines 2–3). Finally,
the maximum match is assembled and returned (lines 4–5).

Example 2: Consider pattern graph Q1 and data graph G1
shown in Fig. 1. The maximum match computed by HHK is
{(PM, PM2), (SA, SA2), (BA, BA2), (UD, UD1), (SD, SD1), . . .,
(SD, SDh), (ST, ST1), . . . , (ST, STh)}. Note that here PM1
cannot match PM since no child of PM1 is labeled with UD,
and it is similar for the other false matches.
2

Remark. (1) Graph simulation is computable in quadratic
time [16]. Algorithm HHK does not run in quadratic time,
but it is simple and easy to be understood. Its further reﬁne-
ment leads to a quadratic algorithm [16], the best available
algorithm as long as time complexity is concerned [23].
(2) Algorithm HHK correctly computes the maximum match
M in data graph G for pattern graph Q, and Q ≺ G iﬀ for
each node v in Q, there exists a node u in G with (v, u) ∈ M .
Hence we focus on computing the maximum match.

2.3 Distributed Graph Pattern Matching

We now introduce graph fragmentation, followed by the

problem of distributed graph pattern matching.

(cid:2)k

iﬀ (1)

We say (G[V1], . . . , G[Vk]) is a partition of graph G(V , E)
i=1 Vi = V ; and (2) for any i (cid:5)= j ∈ [1, k], Vi ∩Vj = ∅.
We also say node u in subgraph G[Vi] (1 ≤ i ≤ k) is a
boundary node if there exists an edge (u, v) in G from u to
v in G[Vj ] such that i (cid:5)= j and 1 ≤ j ≤ k. To maintain the
completeness of G, for each boundary node u in a partition
G[Vi], we maintain locally a set Bu of labeled nodes v : j such
that there exists an edge from u to v in subgraph G[Vj ].
Fragmented graph. A fragmented graph F of data graph
G is denoted as (F1, . . . , Fk), where (1) for each i ∈ [1, k],
Fi = (G[Vi], Bi) is a fragment of G placed at a separate
machine Si, (2) (G[V1], . . ., G[Vk]) is a partition of G, and
(3) Bi (i ∈ [1, k]) is the union of the sets Bu of labeled nodes
of all boundary nodes u in subgraph G[Vi].

}), F4 = (G[V4], {BSA2

Example 3: Consider the fragmented data graph G1 in
Fig. 1 that consists of ﬁve fragments F1, . . . , F5. Formally,
F1 = (G[V1], {BPM1 , BSA1
}), F2 = (G[V2], ∅), F3 = (G[V3],
}), F5 = (G[V5], ∅), where (1) V1
{BPM2
= {PM1, BA1}, V2 = {SA1, ST1}, V3 = {PM2, BA2, UD1}, V4
= {SA2}, and V5 = {SD1, ST1, . . . ,SD h, STh}, respectively;
and (2) BPM1 = {BA1 : 2}, BSA1 = {SD1 : 2}, BPM2 = {SA2 :
4} and BSA2 = {SDh : 5}, respectively.
Remark.
(1) The fragmented graph has the same num-
ber of nodes and edges as the original graph except that
the children of boundary nodes are labeled with IDs of the
fragments in which they are located. (2) Graph partition
is np-complete in general [14], and is not the focus of this
work. Here we allow arbitrary fragmentation of data graphs.

2

Distributed graph pattern matching. We now deﬁne
the graph pattern matching problem in a distributed setting.
Given pattern graph Q, and fragmented graph F = (F1,
. . ., Fk) of data graph G,
in which each fragment Fi =
(G[Vi], Bi) (i ∈ [1, k]) is placed at a separate machine Si,
the distributed graph pattern matching problem is to ﬁnd the
maximum match in G for Q, via graph simulation.

3. PROPERTY OF GRAPH SIMULATION

In this section, we study fundamental properties of graph
simulation, which help us design distributed algorithms. We
consider pattern graph Q(Vq, Eq) and data graph G(V, E).

3.1 Connected Components

We ﬁrst show the impacts of connected components (CCs)

on the evaluation of graph simulation.

Proposition 2: Let pattern Q consist of h CCs Q1, . . . , Qh.
For any data graph G, if Mi is the maximum match in G for
Qi, then
2

i=1 Mi is the maximum match in G for Q.

(cid:2)h

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France951By Proposition 2, we assume w.l.o.g. that pattern graphs

are always connected in the sequel.

(cid:2)h

Proposition 3: Let data graph G consist of h CCs G1, . . .,
Gh. For any pattern Q, if Mi is the maximum match in Gi
for Q, then
i=1 Mi is the maximum match in G for Q. 2
Proposition 3 implies that when a CC Gi (i ∈ [1, h]) is
only located in a single fragment, we can simply compute
the maximum match Mi in Gi for Q locally. However, this
strategy does not work when a CC is across multiple frag-
ments at diﬀerent machines. We need a better solution.

To do this, we ﬁrst introduce the following notions.
Consider a binary relation R ⊆ Vq × V . We use R(G) to
denote the subgraph H(Vs, Es) of G, in which (1) v ∈ Vs iﬀ
there exists u ∈ Vq with (u, v) ∈ R, and (2) (v, v(cid:2)) ∈ Es iﬀ
(i) (v, v (cid:2)) ∈ E and (ii) there exist u, u(cid:2) ∈ Vq with (u, v) ∈ R,
(u(cid:2), v(cid:2)) ∈ R and (u, u(cid:2)) ∈ Eq. We also use R(Q) to denote
the subgraph Q[Vq,s] of Q in which u ∈ Vq,s iﬀ there exists
v ∈ V with (u, v) ∈ R. Intuitively, R(Q) and R(G) are the
subgraphs of Q and G, respectively, that play a role in R.
Theorem 4: Consider any binary relation R ⊆ Vq × V on
pattern graph Q(Vq, Eq) and data graph G(V, E) that con-
tains the maximum match M in G for Q. If Mi is the max-
imum match in R(G)i for Q, then
i=1 Mi is exactly M ,
where R(G) consists of h CCs R(G)1, . . . , R(G)h.
2

(cid:2)h

By Theorem 4, we can utilize subgraph R(G), instead of
the entire G, to compute the maximum match M if R is
guaranteed to contain M . Note that even if G is connect-
ed, R(G) might be highly disconnected, by removing useless
nodes and edges from G. This enhances the possibility of
treating each CC separately for evaluating graph simulation.

Remark. Note that ﬁnding all pairwise disconnected com-
ponents is linear-time equivalent to ﬁnding strongly connect-
ed components, which is in linear time [9].

3.2 Data Locality

We then study the impacts of data locality on graph sim-
ulation. For distributed algorithms, one way to maximize
parallelization is to explore “what can be computed local-
ly” [22]. However, as observed in [21], graph algorithms
often exhibit poor locality and hence, may incur prohibitive
overhead on network traﬃc. This is indeed rather challeng-
ing for graph simulation, illustrated by an example below.

Example 4: Consider pattern Q1 and data graph G1 of
Fig. 1. Let Gs be the CC of G1 containing node PM2. Then
to decide whether Q1 ≺ Gs, we have to ship all subgraphs of
Gs to a single site to re-assemble Gs. Indeed, (1) the match
graph of Q1 and Gs is the entire Gs; and (2) removing any
node or edge from Gs makes Q1 (cid:5)≺ Gs. This tells us that
graph simulation has poor data locality.
2

Theorem 5: For any binary relation R ⊆ Vq ×V on pattern
graph Q(Vq, Eq) and data graph G(V, E) that contains the
maximum match M in G for Q,
(1) match (u, v) ∈ R is in M iﬀ it is in the maximum match
in subgraph desc(R[G], v) for subgraph desc(R[Q], u); and
(2) if there exists a cycle in desc(R[Q], u), there must exist
a cycle in desc(R[G], v) as well.
2

By Theorem 5, whether node v in G can be mapped to
node u in Q can be reduced to the sub-problem of checking
whether it belongs to the maximum match in desc(R[G], v)
for desc(R[Q], u), in which desc(R[G], v) and desc(R[Q], u)

are connected subgraphs of G and Q, respectively. More-
over, a cycle in Q must match a cycle in G.
Indeed, the
poor data locality of simulation is caused by the cycles in Q.

Data locality. We next formally deﬁne data locality that
may avoid unnecessary data shipment when evaluating pat-
tern query Q on data graph G, via graph simulation.

We say that a node v in data graph G can be determined
locally if checking whether v matches any node u in Q in-
volves only the nodes v (cid:2) in G that have distance dist(v, v(cid:2))
bounded by a constant factor determined by Q only. Simi-
larly, we say that data graph G can be determined locally if
all nodes in G can be determined locally.

Theorem 6: Checking whether node v in data graph G
matches node u in pattern graph Q can be determined lo-
cally iﬀ subgraph desc(Q, u) is a dag.
2

Example 5: Consider again pattern graph Q1 and data
graph G1 of Fig. 1. Observe that the cycle SD/ST/SD in
Q1 matches the cycle SD1/ST1/ . . . /SDh/STh in G1. This
makes G1 impossible to be determined locally since h can
be arbitrarily large and cannot be bounded by Q1.

On the contrary, nodes BA and UD in G1 can be deter-
mined locally since the involved nodes in G1 have a distance
bounded by the longest shortest distance in Q1.
2
Now let us consider a fragmented graph F = (F1, . . . , Fk)
of data graph G, in which each fragment Fi = (G[Vi], Bi)
(i ∈ [1, k]) is placed at a separate machine Si.

Corollary 7: A fragment Fi can be determined locally if all
its boundary nodes can be determined locally.
2

Summary. (1) We can treat each connect component in a
data graph separately when evaluating graph simulation.
(2) Theorem 6 tells us how to check whether a node in data
graph G can be determined locally or not.
(3) Corollary 7 tells us that the key for distributed pattern
matching is to determine the matches of boundary nodes.

4. ANALYSES OF DISTRIBUTED ALGO-

RITHMS

In this section, we investigate the complexities of a large
class of distributed algorithms for graph simulation, which
guides us the design of distributed algorithms. We consider
pattern graph Q(Vq, Eq) and fragmented graph F = (F1,
. . ., Fk) of data graphG (V, E), where each fragment Fi =
(G[Vi], Bi) (i ∈ [1, k]) is placed at a separate machine Si.

4.1 Model of Computation

We ﬁrst present the computational model for a large class

of distributed algorithms for graph simulation.

We consider distributed algorithms in a pure message-
passing (sharing nothing) model. The model consists of a
cluster of identical machines, in which one machine can di-
rectly send arbitrary number of messages to another one,
and those machines co-work with each other by local com-
putations and message-passing. Note that this model is ﬂex-
ible enough to express a broad class of algorithms [19], and
is typically ﬁt for the evaluation of graph algorithms [21].

The class of distributed algorithms that we consider work
in the following fashion. A user initiates a pattern query
Q at an arbitrary machine, referred to as the coordinator,
in the a cluster of k identical machines. Then query Q is

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France952(possibly) broadcasted to all the rest machines in the clus-
ter, after which those machines cooperate with each other,
through message passing, to compute the maximum match
M in data graph G for Q, via graph simulation. Finally, the
maximum match M is collected and presented to the user at
the coordinator. Here the fragmented graph of G is placed
at k machines, one fragment at each machine, i.e., the num-
ber of fragments is exactly the number of machines. Note
that this should not be treated a restriction as an arbitrary
number of fragments can be merged into a single fragment.

Complexity measures. There are a variety of complexity
measures on the performance of distributed algorithms [19],
closely related to their computation models. We consider
three measures: (1) visit times, the maximum visiting times
of a machine in the cluster which indicates the complexity
of interactions, (2) makespan, the time cost measuring the
completion time, from the time when the query is initiat-
ed to the time when the maximum match M is complete-
ly assembled, and (3) data shipment, the size of the total
messages among distinct machines in the cluster during the
computation. Intuitively, one wants to minimize visit times,
makespan and data shipment in the same time. As will be
seen shortly, these three measures, however, are controver-
sial with each other, which advocates a well-balanced strategy
when designing distributed algorithms.

Speciﬁcations. To help analyze the complexity of the class
of distributed algorithms, we need to further specify the fol-
lowing: (1) the local information available at each machine,
(2) the messages exchanged among machines, and (3) the
local computations executed on single machines.
(1) The local information at each machine Si (i ∈ [1, k])
consists of (a) pattern graph Q, (b) subgraph Gs,i of data
graph G and (c) a marked binary relation Ri ⊆ Vq × V .

The pattern graph Q is broadcasted to all machines, and
kept unchanged during the computation. The subgraph Gs,i
is initially the local fragment Fi = (G[Vi], Bi), and is updat-
ed once receiving messages containing subgraphs from other
machines. For each node pair (u, v) ∈ Ri, it is marked as
true, false or unknown, denoting that (u, v) is (a) in the maxi-
mum match M , (b) not in M , or (c) undetermined. Relation
Ri can be updated by either messages or local computations.
(2) When machine Si sends a message to another machine
Sj , the message only consists of the local information (Q,
Gs,i, Ri) available at machine Si. Here we do not allow
information coding [10] since it is orthogonal to the analysis
of the data shipment of distributed algorithms.
(3) At each machine Si, local algorithms that given the local
information (Q, Gs,i, Ri), compute an updated Ri by utiliz-
ing the deﬁnition of graph simulation. We require that the
local algorithms execute only local computations without in-
volving message-passing during the computation, and they
run in time of a polynomial of |Q| and |Gs,i|. Note that this
is to help analyze the makespan problem [28] of distributed
algorithms, and should not be treated as a restriction at all.

Remark. Under the model and speciﬁcations, we can ex-
press a large class of distributed algorithms for graph simu-
lation, including all the ones that come into our mind.
4.2 Complexities of Distributed Algorithms

We next present our ﬁndings on the class of distributed al-
gorithms for evaluating graph simulation queries, which sat-
isfy the computation model and speciﬁcations given above.

Figure 3: Example for complexity analyses

Our ﬁrst set of ﬁndings are shown below.

Proposition 8: The optimal data shipment of the class of
distributed algorithms is |G| − 1, and the bound is tight. 2

Surprisingly, a simple distributed algorithm, referred to as
naiveMatchds, can achieve the optimal data shipment. Giv-
en pattern graph Q and fragmented data graph G, the al-
gorithm simply collects all fragments of the data graph to
the coordinator, and then it calls a standard centralized al-
gorithm of graph simulation, e.g., HHK [16], to compute the
maximum match in G for Q. Note that the total data ship-
ment in the process is bounded by |G|−1 since the subgraph
at the coordinator contains at least one node.

Proposition 9: The optimal visit times of the class of dis-
tributed algorithms are 1, and the bound is tight.
2

Again a simple distributed algorithm, referred to as
naiveMatchvt, achieves the optimal visit times. Given pat-
tern graph Q and fragmented data graph G, the algorithm
visits k machines sequentially, where the last visited one is
the coordinator. In the process, each time when the algorith-
m visits a machine, it collects the local fragment, and sends
all the fragments collected so far to the next machine. Final-
ly, all fragments of G reside at the coordinator, and it calls
a centralized algorithm of graph simulation, e.g., HHK [16],
to compute the maximum match in G for Q. Note that each
machine is visited one and only once in the process.

While there are eﬃcient optimal algorithms for data ship-
ment and visit times separately, the problem of ﬁnding a
minimum makespan is much harder, as shown below.

Proposition 10: The minimum makespan problem of the
class of distributed algorithms is np-complete [28].
2

We next present our second set of ﬁndings that these three
complexity measures are controversial with each other. We
illustrate this with the following example.

Example 6: Consider pattern graph Qo and the fragment-
ed data graph Go in Fig. 3, in which each fragment is a
CC without any boundary nodes. Here nodes in Qo are
only allowed to match nodes in Go with the same labels
by ignoring their subscripts. We also assume w.l.o.g. that
F1 = (G[V1], ∅) lies at the coordinator.

One can verify the following:

(1) For algorithm naiveMatchds, the (optimal) data shipment
is |G| − 1 = 8h, while the visit times are h.
(2) For algorithm naiveMatchvt, the (optimal) visit times are
1, while the data shipment is 4h(h + 1).

Observe that all the computational workload in these al-
gorithms is mostly laid at a single machine, the coordinator.
Essentially, no computing power is used in parallel at all.
(3) Ideally, a distributed algorithm would work as follows.
(i) After receiving Q on each machine, a local algorithm is
called to compute the local match in the fragment.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France953Input: Pattern graph Q and fragmented data graph F = (F1, . . .,
Fk) with Fi = (G[Vi], Bi) placed at machine Si (i ∈ [1, k]).

Output: The maximum match M in G for Q.

Coordinator SQ.
1. send pattern graph Q to all k participating machines;
case (1): upon receiving Mis and CCs from all k machines do
1. M := M1 ∪ . . . ∪ Mk;
2. call dSchedule to ﬁnd an assignment of those CCs;
3. send the assignment to all k participating machines;
case (2): upon receiving Mb,is from all k machines do
1. M := M ∪ Mb,1 ∪ . . . ∪ Mb,k;
2. return M .

Machine Si.
case (3): upon receiving pattern graph Q do
1. P Mi := localHHK(Q, Fi);
2. let Cb,i be the CCs in P Mi(G) containing nodes in Bi;
3. let Mi be the matches in P Mi containing no nodes in Cb,i;
4. send Mi and the sizes and boundary nodes of CCs in Cb,i to SQ;
case (4): upon receiving the assignment of data shipment do
1. send CCs in Cb,i to their corresponding machines;
case (5) upon receiving all the assigned CCs Ci do
1. Mb,i := reﬁneHHK(Q, Ci);
2. send Mb,i to coordinator SQ;

Figure 4: Distributed algorithm disHHK

(ii) Here since each fragment is a CC, all local match results
are simply sent to and assembled at the coordinator. Recall
Proposition 3 in Section 3.

In this way, the computation is maximally parallelized,
and the makespan is minimized, accordingly. However, one
may verify that the data shipment is 30h, and the visit times
are h. In contrast, the optimal data shipment and visit times
are 8h and 1, respectively.
2
Summary. We ﬁnd that data shipment, visit times and
makespan of a large class of distributed algorithms for graph
simulation are controversial with each other. As eﬃciency
remains the dominant factor, there needs a well-balanced
strategy between makespan and the other two measures.

5. DISTRIBUTED EVALUATION

In this section, we present the distributed algorithms that
exploit the properties of graph simulation (Section 3) and
the analyses of distributed algorithms (Section 4). We con-
sider pattern graph Q(Vq, Eq) and fragmented graph F =
(F1, . . ., Fk) of data graphG (V, E), where each fragment Fi
= (G[Vi], Bi) (i ∈ [1, k]) is placed at a separate machine Si.
The distributed algorithm, referred to as disHHK, follows
the computation model and speciﬁcations given in Section 4.
It is initiated at the coordinator SQ where the query Q is
issued, and it consists of ﬁve stages (shown in Fig. 4).
Stage 1: Coordinator SQ simply broadcasts pattern query
Q to all the k participating machines.
Stage 2: The main objective is to (partially) evaluate Q in
each fragment at local machines in parallel (case (3)). As a
by-product, the local evaluation ﬁlters out useless nodes and
edges in the fragment, and hence reduces the data shipmen-
t. It also breaks a large fragment into smaller CCs, which
further reduces the sizes of CCs across diﬀerent machines.
Stage 3: The objective is to ship those CCs across diﬀerent
machines to single machines (case (1) and case (4)). This
involves two important, but controversial, issues: minimiz-
ing data shipment and makespan. We provide a solution
that (a) both minimizes the makespan with performance

guarantees and (b) minimizes the data shipment with heuris-
tics in the same time.
Stage 4: The objective is to compute the maximum match-
es in those CCs originally across diﬀerent machines in par-
allel, by making use of those partial matches on these com-
ponents computed locally at Stage 1 (case (5)).
Stage 5: Finally, the match results on all machines are sent
to and assembled at the coordinator (case (5) and case (2)).
Correctness. The correctness of disHHK can be easily ver-
iﬁed by the analyses of graph simulation in Section 3.

Proposition 11: Given any pattern graph Q and fragment-
ed graph F of data graph G, algorithm disHHK computes the
maximum match in G for Q.
2

Performance. The algorithm guarantees the following.
(1) The total computation cost is comparable to the one of
the best-known centralized algorithm [16]. And it invokes
four rounds of message-passing and local evaluation only.
(2) The total data shipment is at most |G| + 4|B| + |Q||G|+
(k−1)|Q|, where |B| is the total number of boundary nodes.
(3) Each machine except coordinator SQ is visited at most
g + 2 times, where g is the maximum number of machines at
which a CC resides at the end of Stage 2. Coordinator SQ
is visited with 2(k − 1) extra times since it needs to schedule
the data shipment and assemble the ﬁnal result.

Remark. (1) We have decided to sacriﬁce the visit times
and data shipment for the beneﬁts of the makespan, a deci-
sion based on the analyses of Section 4.
(2) As one may notice, most stages are run in parallel except
for case (1) at Stage 3. As will be seen soon, its computation
cost is really low, and would not cause a bottleneck.

In what follows, we describe each stage in more detail.

5.1 Local Evaluation of Partial Match

As shown by the analyses of Section 3, special care needs
to be paid on boundary nodes. To do this, we introduce
a notion ofpartial match relation . We consider a fragment
Fi = (G[Vi], Bi) at machine Si (i ∈ [1, k]).
Partial match. A binary relation R ⊆ Vq × Vi is said to
be a partial match if (1) for each (u, v) ∈ R, u and v have
the same label; and (2) for each edge (u, u(cid:2)) in Eq, (a) there
exists a node v(cid:2) ∈ Bv in Bi having the same label as u(cid:2) if
v is a boundary node, or (b) there exists an edge (v, v(cid:2)) in
G[Vi] such that (u(cid:2), v(cid:2)) ∈ R, otherwise.

The diﬀerence between a partial match and a match given
in Section 2 is the latter deals with boundary nodes. When
there are no boundary nodes involved, they are equivalent.
Note that (u, v) in a partial match might not appear in the
maximum match in G for Q since the matches on boundary
nodes are checked partially only with their directed neigh-
bors. We illustrate this with an example below.

Example 7: Consider pattern graph Q1 and data graph
G1 in Example 1. Pair (SA, SA1) is in the maximum partial
match P M1 in fragment F1 for Q. However, it does not
belong to the maximum match M in G for Q.
2

Algorithm localHHK. To compute the partial match, we
propose algorithm localHHK, a revision of algorithm HHK
that further deals with boundary nodes. Given pattern
graph Q and fragment Fi = (G[Vi], Bi) (i ∈ [1, k]), it com-
putes the maximum partial match in the fragment for Q.
Due to space limitations, its detail is omitted here.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France954It is easy to get the following result, along the same lines

as Proposition 1 and algorithm HHK in Section 2.2.

Corollary 12: For any pattern graph Q and fragment Fi,
(1) there is a unique maximum partial match; and (2) algo-
rithm localHHK computes the maximum partial match. 2

We next illustrate with an example how local evaluation
also ﬁlters out useless nodes and edges in data graphs, which
reduces data shipment and computations.

Example 8: Consider again pattern graph Q1 and frag-
ments F1 and F2 in data graph G1 in Example 1. Recall
that the maximum partial matches P M1 = {(SA, SA1)} and
P M2 = ∅. Hence, instead of the connected component con-
sisting of fragments F1 and F2, only node SA1 in F1 will be
considered in the following stages. Thus a plenty of unnec-
essary nodes and edges are ﬁltered out at this stage.
2

5.2 Scheduling Data Shipment

After the local evaluation of partial match is done, we
have a partial match P Mi on each machine Si (i ∈ [1, k]).
Let P M be
i=1 P Mi, and M be the maximum match in
G for Q. It is easy to verify that M ⊆ P M .

(cid:2)k

Now let us consider subgraph P M (G) consisting of nodes
and edges of G that play a role in P M , as deﬁned in Sec-
tion 3.1. Theorem 4 tells us that those matches in P M
involved with those CCs residing at single machines must
belong to the maximum match M . Due to the poor data
quality of graph simulation, those CCs of P M residing at
multiple machines need to be gathered into single machines.
A challenging task here is how to schedule the data ship-
ment such that both the data shipment and the makespan
are minimized. Note that the visit times have been ﬁxed in
the algorithm, and hence are not involved.

To do this, we introduce the following problem.

The scheduling problem. Consider subgraph P Mi(G) at
machine Si. Each CC of P Mi(G) is identiﬁed by its bound-
ary nodes, its size (e.g., the number of nodes and edges) and
its location Si. These information of all the CCs involved
with boundary nodes are sent to the coordinator, at which
those CCs of P M (G) across diﬀerent machines are merged.
Hence, we have a set {C1, . . . , Cl} of CCs of P M (G). For
each j ∈ [1, l], Cj is associated with k + 1 costs: (1) for each
i ∈ [1, k], the data shipment Cj.di if Cj is shipped to Si; and
(2) the computation cost Cj.c. Recall that the computation
cost of Cj is a polynomial of |Q| and |Cj|.

Formally, the scheduling problem is deﬁned as follows.
Given l CCs C1, . . . , Cl, and an integer k, ﬁnd an assign-
ment of the CC to k identical machines, so that both the
makespan and the total data shipment are minimized.

Approximation hardness. To minimize the makespan,
the key is to distribute those connected components evenly
on those k machines. As the minimum makespan problem
is np-complete (Proposition 10), we focus on approximate
solutions here. We ﬁrst look for chances that minimize both
the makespan and the data shipment.

We say that the scheduling problem is approximatable
within (α, β) if there exists a ptime algorithm such that
given any instance of the problem, the algorithm produces a
scheduling solution such that the data shipment is bounded
by α times of the optimal data shipment and the makespan
is bounded by β times of the optimal makespan.

Input: Connected components C1, . . . , Cl and an integer k.
Output: An assignment of C1, . . . , Cl to k identical machines.
1. let avgc := (C1.c + . . . + Cl.c)/k;
2. for each connected component Ci (i ∈ [1, l]) do
3.
4.
5. for each of the rest connected components Cj do
6. Assign Cj to the machine that has the least amount of load.

if the total load of Ci and its optimal machine ≤ avgc then

Assign Ci to its optimal machine;

Figure 5: Scheduling algorithm dSchedule

The result is, however, negative due to the controversial

nature of these two measures as shown in Section 4.2.

Theorem 13: The scheduling problem is not approximable
within (ξ, max(k − 1, 2)) for any ξ > 1.
2

Approximation algorithm. This motivates us to look
for solutions that have performance guarantees for the
makespan only. We propose an approximation algorithm
dSchedule that has a heuristic for minimizing the data ship-
ment, and a performance guarantee for the makespan.

Theorem 14: Algorithm dSchedule produces an assignment
of the scheduling problem such that the makespan is within
a factor (2 − 1/k) of the optimal one.
2

We now present the details of dSchedule, shown in Fig. 5.
Given connected components C1, . . . , Cl and an integer k as
inputs, the algorithm ﬁnds an assignment for these CCs. It
ﬁrst computes the average computation avgc cost of all CCs
(line 1). Given a CC Cj, the optimal machine Sjo to which
Cj is assigned is the one on which the largest part of Cj
resides, i.e., Cj.djo is the largest among {Cj .1, . . . , Cj.k}.
If the total load of Ci and its optimal assigned machine
together is equal or less than the average computation cost
avgc, then Ci is assigned to its optimal machine (lines 2-4).
For the rest CCs Cj, schedule Cj to the machine that has
the least amount of load (lines 5-6).

Remark. (1) A heuristic is used to minimize the data ship-
ment, by shipping CCs to their corresponding optimal ma-
chines w.r.t. the data shipment (lines 1-4).
(2) A greedy approach (lines 5-6) is adopted to guarantee
the performance of the makesapn, along the same lines as
the one for the standard makespan problem [28].
(3) The algorithm runs in O(kl), and is very eﬃcient. Hence,
its evaluation could not cause a bottleneck.

Example 9: Consider again pattern graph Q1 and data
graph G1 in Example 1. Algorithm dSchedule ﬁnds an as-
signment for the two connected components C1 consisting of
a single node SA1, and C2 consisting of the entire fragments
F3, F4 and F5.
In this case, (1) C1 is simply assigned to
S1, and (2) C2 is assigned to S5. That is, these CCs are
assigned to the machines with optimal data shipment.
2

5.3 Reﬁning Partial Match

Similar to algorithm localHHK, algorithm reﬁneHHK is al-
so a revision of algorithm HHK. The key diﬀerences between
reﬁneHHK and HHK lie in that (1) partial matches evaluated
at stage 1 are treated as initial candidate matches, and (2)
we process the matches of boundary nodes ﬁrst, in which
way the total computation cost of reﬁneHHK and localHHK
is comparative to the one of HHK.

5.4 Optimization Techniques

We next present optimization techniques for algorithm
disHHK, by means of data locality and query minimization.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France955Determining boundary matches using data locality.
For each partial match P Mi (i ∈ [1, k]) in fragment Fi for Q,
we further determine whether those matches in P Mi with
boundary nodes, computed by localHHK, belong to the max-
imum match M in the entire data graph G for Q. It is based
on an application of Theorems 5 and 6 in Section 3. This
reduces both computations and data shipments. Consider
the matches (u, v) for a boundary node v in a partial match
P Mi in fragment Fi = (G[Vi], Bi) for Q.

To determine whether (u, v) belongs to the maximum
match M in G for Q, it suﬃces to determine whether for
each child u(cid:2) of u in Q, there is a child v (cid:2) of v such that
(u(cid:2), v(cid:2)) is in M . For each child node j : v(cid:2) in Bv of Bi, if
v(cid:2) matches a child u(cid:2) of u in P Mi, match (v(cid:2), u(cid:2)) is further
checked by lazy evaluation at machine Sj as follows.

Let Cv(cid:2) be the connected component of P Mj such that v(cid:2)
is in Cj , and let P Mi,v(cid:2) be the set of matches in P Mj that
involve nodes in Cj . We have two cases to consider:
Case 1: when there are no boundary nodes in Gj .

For this case, we simply check whether node u(cid:2) belongs
to subgraph P Mi,v(cid:2) (Q). If the answer is ‘yes’, then match
(v(cid:2), u(cid:2)) is a true match, i.e., (v(cid:2), u(cid:2)) belongs to the maximum
match M in G for Q. Otherwise, it is a false match. The
correctness of this approach is guaranteed by Theorem 5.
Case 2: when there are boundary nodes in Cj, but subgraph
desc(Q, u(cid:2)) of pattern graph Q is a dag.

For this case, we need to check whether all nodes in
desc(Q, u(cid:2)), including u(cid:2) itself, matches no boundary nodes
in Cj. If the answer is ‘yes’, then match (v(cid:2), u(cid:2)) is a true
match, i.e., (v(cid:2), u(cid:2)) belongs to the maximum match M in G
for Q. Otherwise, it is an unknown match. The correctness
of this approach is guaranteed by Theorems 5 and 6.

We next illustrate the beneﬁts of this optimization tech-

nique with the following example.

Example 10: Consider pattern graph Q1 and data graph
G1 in Fig. 1, and the partial match results in Example 7.
One can verify that (1) boundary nodes SA1 and SA2 can
be determined by this optimization technique, while nothing
can be done for boundary node PM2.
(1) For node SA1, its only child SD1 is located in fragment
F2. The partial match P M2 is empty. Hence, a false match
decision is sent back to machine S1, and this further helps
determine that (SA, SA1) is a false match.
(2) For node SA2, its only child SD1 is located in fragment
F5. The subgraph P M5(G) contains no boundary nodes,
and SD belongs to P M5(Q). Hence, a true match decision
is sent back to machine S4, and this further helps determine
that (SA, SA2) is a true match.

After these are done, fragment F3 is the only part of G
that needs to be further evaluated. To check the matches in
F3, we simply ship fragment F4 to machine S3, instead of
shipping F3 and F4 to machine S5 as shown in Example 9.
That is, our approach could potentially save a large amount
of unnecessary data shipments and computations.
2

Remark. (1) This approach exploits the partial match re-
sults at other machines, and the checking is simple and eﬃ-
cient. (2) Only a small amount of data shipment is incurred.
The only involved data shipment is the triggers of the lazy
evaluation and the decisions (true, false, or unknown). Note
that the evaluation is done at machine Sj, not Si. This is
why the data shipment incurred is small.

Minimizing pattern graphs. Given pattern graph Q, we
compute a minimized pattern graph Qm such that for any
data graph G, G matches Q iﬀ G matches Qm, via graph
simulation. The algorithm runs in quadratic time, and is
taken from [6]. Note that Q is typically small.

We next illustrate the beneﬁts of minimizing pattern

graphs with an example below.

Example 11: Consider pattern graph Qo in Fig.3. The
minimized equivalent pattern graph Qmo of Qo is a compact
representation of Qo, by merging (1) nodes A1, A2, (2) nodes
C1, C2, and (3) nodes D1, D2, D3, D4.
It only consists of
four nodes and four edges. Hence, Qmo is much smaller
than Qo. It is easy to see that both the data shipment and
computation cost of evaluating Qmo on Go are much smaller
that those of evaluating Qo on Go.
2

We have implemented a version of disHHK that supports
these optimizations, referred to as disHHK+. As will be seen
in Section 6, disHHK+ signiﬁcantly outperforms disHHK.

6. EXPERIMENTAL STUDY

We next present an experimental study of our algorithms
disHHK and disHHK+. Using both real-life and synthetic
data, we conducted four sets of tests to evaluate: (1) the
makespan, (2) the data shipment, (3) the visit times of our
algorithms, and (4) the eﬀectiveness of algorithm localHHK.
Experimental setting. We use the following datasets.
Real-life data. We used two real-life datasets1. (a) Google
records a Web graph with 875,713 nodes and 5,105,039 edges
where nodes are URLs and an edge from URLs x to y indi-
cates that there exists a hyperlink from x to y. (b) Amazon
contains a product co-purchasing network with 548,552 n-
odes and 1,788,725 edges in which nodes are products and
an edge from products x to y represents that people buy y
with high probability when they buy x.
Synthetic graph generator. We adopted the graph-tool li-
brary2 to produce both pattern and data graphs. It is con-
trolled by three parameters: the number n of nodes, the
number nα of edges, and the number l of node labels. Given
n, α and l, the generator produces a graph with n nodes,
nα edges, and the nodes are labeled from a set of l labels.
Algorithms. We implemented the following algorithms, all in
Python: (1) algorithms disHHK and disHHK+, and (2) opti-
mal algorithms naiveMatchds and naiveMatchvt (Section 4).
The experiments were run on a cluster of 16 machines,
all with 2 Intel Xeon E5620 CPUs and 64GB memory, that
are connected by kilomega network. Each test was repeated
over 5 times and the average is reported here.
Experimental results. In all the experiments, we ﬁxed l
= 200, k = 16, and set α (αq) = 1.20 by default. All dataset-
s are partitioned with a hashing function hash(ID) mod k,
and distributed over all participating machines. This parti-
tion approach has been commonly used in large-scale data
process systems, such as MapReduce [11] and Pregel [21].
Exp-1: Makespan. In the ﬁrst set of tests, we evaluat-
ed the performance of disHHK, disHHK+, naiveMatchds and
naiveMatchvt. We did not report naiveMatchvt here as it is
always much slower than naiveMatchds.

1http://snap.stanford.edu/data/index.html
2http://projects.skewed.de/graph-tool/

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France95612

)
s
0
0
1
x
(
e
m

i
t
 
d
e
s
p
a
l
E

9

6

3
1
0

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
naieMatchds(Amaon)
dis(oogle)
dis(oogle)
dis(Amaon)
dis(Amaon)

)
s
0
0
1
x
(
e
m

i
t
 
d
e
s
p
a
l
E

3

5

7

9

11

13

15

8

6

4

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
naieMatchds(Amaon)
dis(oogle)
dis(oogle)
dis(Amaon)
dis(Amaon)

2
1
0
1.05 1.075 1.10 1.125 1.15 1.175 1.20

(a) Vary |Vq|

(b) Vary αq

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
naieMatchds(Amaon)
dis(oogle)
dis(oogle)
dis(Amaon)
dis(Amaon)

104

103
40

102

)

4
0
1
x
(
t
n
e
m
p
i
h
s
 

a
t
a
d

 
f
o
#

 

dis(ynthetic)
dis(ynthetic)

naieMatchds(ynthetic)
dis(oogle)
dis(oogle)

naieMatchds(oogle)
dis(Amaon)
naieMatchds(Amaon)

dis(Amaon)

)

4
0
1
x
(
t
n
e
m
p
i
h
s
 

a
t
a
d

 
f
o
#

 

3

5

7

9

11

13

15

1.05 1.075 1.10 1.125 1.15 1.175 1.20

)

4
0
1
x
(
t
n
e
m
p
i
h
s
 

a
t
a
d

 
f
o
#

 

104

103

102
40

104
103

102
10

1

3
2.5

2
1.5
1
0.5
0

)
s
0
0
1
x
(
e
m

i
t
 
d
e
s
p
a
l
E

dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
dis(oogle)
dis(oogle)

naieMatchds(Amaon)
dis(Amaon)
dis(Amaon)

1

2

3

4

5

6

7

8

9

10

(c) Vary |V |(×κd)

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
dis(oogle)
dis(oogle)

naieMatchds(Amaon)
dis(Amaon)
dis(Amaon)

)
s
0
0
1
x
(
e
m

i
t
 
d
e
s
p
a
l
E

10

8

6

4

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

2
1
0
1.05 1.075 1.10 1.125 1.15 1.175 1.20

(d) Vary a

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

)

4
0
1
x
(
t
n
e
m
p
i
h
s
 

a
t
a
d

 
f
o
#

 

37x103
32x103
27x103
22x103
17x103

1

2

3

4

5

6

7

8

9

10

1.05 1.075 1.10 1.125 1.15 1.175 1.20

200
180
160
140
120
100
80
60

s
e
m

i
t
 
t
i
s
i
v
 
l
a
t
o
t
 
f
o
#

 

dis(ynthetic)
dis(ynthetic)
dis(oogle)
dis(oogle)
dis(Amaon)
dis(Amaon)
5

7

3

(f) Vary |Vq|

(h) Vary |V |(×κd)

200
180
160
140
120
100
80
60

s
e
m

i
t
 
t
i
s
i
v
 
l
a
t
o
t
 
f
o
#

 

(g) Vary aq

dis(ynthetic)
dis(oogle)
dis(Amaon)
dis(ynthetic)
dis(oogle)
dis(Amaon)

200
180
160
140
120
100
80
60

s
e
m

i
t
 
t
i
s
i
v
 
l
a
t
o
t
 
f
o
#

 

s
e
m

i
t
 
t
i
s
i
v
 
l
a
t
o
t
 
f
o
#

 

dis(ynthetic)
dis(oogle)
dis(Amaon)
dis(ynthetic)
dis(oogle)
dis(Amaon)
7
5

6

8

9

10

200
180
160
140
120
100
80
60

(i) Vary a

dis(ynthetic)
dis(ynthetic)

9

11

13

15

1.05 1.075 1.10 1.125 1.15 1.175 1.20

1

2

3

4

1.05 1.075 1.10 1.125 1.15 1.175 1.20

10
8
6
4
2

)
s
0
0
1
x
(
e
m

i
t
 
d
e
s
p
a
l
E

0

2

104

)

4
0
1
x
(
t
n
e
m
p
i
h
s
 

a
t
a
d

103

102

dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
dis(oogle)
dis(oogle)

naieMatchds(Amaon)
dis(Amaon)
dis(Amaon)
8

4

6

10

12

14

16

(e) Vary k

naieMatchds(ynthetic)
dis(ynthetic)
dis(ynthetic)

naieMatchds(oogle)
dis(oogle)
dis(oogle)

naieMatchds(Amaon)
dis(Amaon)
dis(Amaon)

 
f
o
#

 

15

2

s
e
m

i
t
 
t
i
s
i
v
 
l
a
t
o
t
 
f
o
#

 

150

120

90

60

30

0

4

6

8

10

12

14

16

(j) Vary k

dis(ynthetic)
dis(oogle)
dis(Amaon)
dis(ynthetic)
dis(oogle)
dis(Amaon)

2

4

6

8

10

12

14

16

(k) Vary |Vq|

(l) Vary aq

(m) Vary |V |(×κd)

(n) Vary a

(o) Vary k

Figure 6: Evaluation on makespan, data shipment and visit times

(1) To evaluate the impacts of pattern graphs Q, we ﬁxed
data graphs G, e.g., Google with 875,713 nodes, Amazon
with 548,552 nodes and synthetic data with 108 nodes, while
varying (a) the number |Vq| of nodes in Q from 3 to 15 and
(b) the density αq of Q from 1.05 to 1.20, respectively. The
results are reported in Figures 6(a) and 6(b), respectively.

One can ﬁnd the following.

(a) All these algorithm-
s scale well with Vq and αq on large data graphs, except
naiveMatchds. While it took disHHK and disHHK+ less than
300s in all cases, it took naiveMatchds over 500s in all cas-
es, and was much slower than disHHK and disHHK+. (b)
disHHK+ is faster than disHHK. Indeed, the running time of
disHHK+ is consistently about [3/4, 4/5] of the time taken
by disHHK, a signiﬁcant reduction. (c) Finally, the elapsed
time of all algorithms increases when |Vq| or αq increases.
(2) To evaluate the impacts of data graphs G, we ﬁxed pat-
tern graphs Q with |Vq| = 9, while varying the number |V | of
nodes of G (Google from 5×104 to 5×105, Amazon from 103
to 104 and synthetic data from 107 to 108), and the density
α of G on synthetic data from 1.05 to 1.20, respectively. The
results are reported in Figures 6(c) and 6(d), respectively,
where κd is a constant such that it is 5 × 104, 103 and 107
for Google, Amazon and synthetic data, respectively.

One can ﬁnd the following. (a) All algorithms scale well on
the large data graphs except naiveMatchds, which took over
300s even on the smallest synthetic graphs with 107 nodes.
Hence we did not report its running time on synthetic data
(b) disHHK+ is consistently faster
graphs in Figure 6(c).
than disHHK, e.g., it took disHHK 217s on synthetic data
graphs with |V | = 108 while it was only 176s for disHHK+.
Indeed, the running time of disHHK+ is consistently about
[3/4, 4/5] of the time taken by disHHK, the same as the case
above when varying pattern graphs. (c) Finally, the elapsed
time of all algorithms increases when |V | or α increases.
(3) To evaluate the impacts of the number k of participat-
ing machines, we ﬁxed both data graphs G (with the same
setting for data graphs as (1)) and pattern graphs Q (with

the setting for pattern graphs as (2)), while varying k from
2 to 16. The results are shown in Figure 6(e).

We ﬁnd the following. (a) The elapsed time of all algo-
rithms decreases when k increases.
(b) The elapsed time
of disHHK and disHHK+ decreases faster than the one of
naiveMatchds when k increases. The elapsed time of disHHK
on synthetic data was reduced from 1081s with k = 2 to
215s with k = 16, while the one of naiveMatchds was only
reduced from 1603s with k = 2 to 1521s with k = 16. And
(c) the running time of disHHK+ is consistently about [3/4,
4/5] of the time taken by disHHK, the same as the cases
when varying pattern or data graphs.
Exp-2: Data shipments. In the second set of tests, using
the same setting as Exp-1, we evaluated the total data ship-
ments of disHHK, disHHK+, naiveMatchds and naiveMatchvt.
We did not report naiveMatchvt here since it always triggered
much more data shipments than naiveMatchds.
(1) We tested the impacts of Q using the same setting as
Exp-1(1). The results are reported in Figs. 6(f) and 6(g).
(a) The total data shipments of all algorithms are not sensi-
tive to the size of pattern graphs, (b) although naiveMatchds
achieves the theoretical optimal data shipment, disHHK
and disHHK+ trigger similar amount of data shipments as
naiveMatchds, and (c) they even trigger less data shipments
than naiveMatchds on large and sparse data graphs, e.g.,
when |Vq| > 11 on synthetic data or αq ≤ 1.125 on Amazon.
(2) We tested the impacts of G using the same setting as
Exp-1(2). The results are reported in Figs. 6(h) and 6(i).
(a) It is obvious that the total data shipments of all algo-
rithms increase while |V | or α increases. (b) disHHK and
disHHK+ shipped less data than naiveMatchds on large and
sparse data graphs, e.g., when α ≤ 1.125 on synthetic data.
And (c) disHHK+ always shipped less data than disHHK.
(3) We tested the impacts of k using the same setting as
Exp-1(3). The results are reported in Figure 6(j).
(a) The total data shipments of all algorithms increase when
k increases. This is obvious since there are more boundary
nodes when k increases when ﬁxing data graphs. And (b)

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France957disHHK and disHHK+ again trigger similar amount of data
shipments as naiveMatchds.
Exp-3: Visit times. In the third set of tests, using the
same setting as Exp-1, we evaluated total visits times of
all algorithms. We did not report naiveMatchvt here as it
is always equal to the number k of participating machines.
The impacts of Q, G and k are reported in Figs. 6(k) and
6(l), Figs. 6(m) and 6(n), and Figure 6(o), respectively.
(a) The total visit times of all algorithms decrease when |Vq|
or αq increases. This is because when the size of pattern
graphs increases, there are less matches in data graphs. (b)
The visit times of all algorithms obviously increase when
|V |, α or k increases. (c) disHHK and disHHK+ have more
visit times than naiveMatchds, and disHHK+ has [30%, 53%]
more visit times than disHHK, as expected. Indeed, this is
a price that has to be paid in exchange for eﬃciency.
Exp-4: Eﬀectiveness of localHHK. Adopting the same
setting as Exp-1, we also evaluated the eﬀectiveness of
localHHK, measured by the number of boundary nodes that
are ﬁltered out by localHHK. The results are shown below:

4287
1916

2418
1305

# of total boundary nodes

# of ﬁltered boundary nodes

Google Amazon Synthetic
1343952
446192
It shows that localHHK eliminates a large portion of
boundary nodes for disHHK and disHHK+, e.g., it cuts of-
f 44%, 54% and 33% boundary nodes on Google, Amazon
and synthetic data, respectively. This means that localHHK
indeed plays a considerable role in our algorithms.
Summary. From these experiments, we ﬁnd the follow-
ing. (1) disHHK and disHHK+ are eﬃcient and scale well
on large and dense data graphs, and considerably out-
perform naiveMatchds (optimal data shipment alone) and
naiveMatchvt (optimal visit times alone). (2) Our optimiza-
tion techniques are eﬀective, reducing the running time by
20% to 25%. (3) We have intensionally sacriﬁced data ship-
ment and visit times for makespan. However, disHHK and
disHHK+ even ship less data than naiveMatchds when data
graphs are large and sparse. Recall that real-life graphs are
often large and sparse. disHHK and disHHK+ indeed have
more visit times than naiveMatchvs, a price that has to be
paid in exchange for improving eﬃciency and minimizing da-
ta shipments. (4) localHHK eﬀectively ﬁlters out [33%, 54%]
unnecessary boundary nodes for disHHK and disHHK+.

7. CONCLUSION

We have proposed evaluation algorithms for graph simu-
lation in a distributed setting. To our knowledge, we are
among the ﬁrst to settle this problem. We have also veri-
ﬁed, both analytically and experimentally, the eﬀectiveness
of our algorithms and optimization techniques.

Several topics are targeted for future work. First, we are
to extend our algorithms to deal with skewed graph par-
titions. Second, we are experimentally verifying our algo-
rithms using MapReduce and Pregel platforms [11, 21]). Fi-
nally, we are to explore indexing techniques and distributed
incremental methods to speed up the computation, in re-
sponse to the dynamic changes of real-life data graphs.

Acknowledgments. Shuai is supported in part by NGFR
973 grant 2011CB302602, NSFC grant 60903149, the Funda-
mental Research Funds for the Universities, and the Young
Faculty Program of MSRA. Tianyu is a contact author.

8. REFERENCES
[1] Data center. http://wikibon.org/blog/inside-ten-of-the-

worlds-largest-data-centers.

[2] S. Abiteboul, P. Buneman, and D. Suciu. Data on the Web:
From Relations to Semistructured Data and XML. Morgan
Kaufmann, 1999.

[3] C. C. Aggarwal and H. Wang. Managing and Mining

Graph Data. Springer, 2010.

[4] S. Amer-Yahia, M. Benedikt, and P. Bohannon. Challenges

in searching online communities. IEEE Data Eng. Bull.,
30(2):23–31, 2007.

[5] J. Brynielsson, J. Hogberg, L. Kaati, C. M ˙artenson, and

P. Svenson. Detecting social positions using simulation. In
ASONAM, 2010.

[6] D. Bustan and O. Grumberg. Simulation-based

minimization. TOCL, 4(2), 2003.

[7] J. Cohen. Graph twiddling in a mapreduce world.

Computing in Science and Engineering, 11(4):29–41, 2009.

[8] G. Cong, W. Fan, and A. Kementsietsidis. Distributed

query evaluation with performance guarantees. In
SIGMOD, 2007.

[9] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.

Introduction to Algorithms. The MIT Press, 2001.

[10] T. M. Cover and J. A. Thomas. Elements of information

theory (2. ed.). Wiley, 2006.

[11] J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data

processing on large clusters. In OSDI, 2004.

[12] W. Fan, J. Li, S. Ma, N. Tang, and Y. Wu. Adding regular

expressions to graph reachability and pattern queries. In
ICDE, 2011.

[13] W. Fan, J. Li, S. Ma, N. Tang, Y. Wu, and Y. Wu. Graph

pattern matching: From intractable to polynomial time.
PVLDB, 3(1), 2010.

[14] P.-O. Fj¨allstr¨om. Algorithms for graph partitioning: A
survey. Linkoping Electronic Articles in Computer and
Information Science, 3, 1998.

[15] B. Gallagher. Matching structure and semantics: A survey

on graph-based pattern matching. AAAI FS., 2006.

[16] M. R. Henzinger, T. A. Henzinger, and P. W. Kopke.

Computing simulations on ﬁnite and inﬁnite graphs. In
FOCS, 1995.

[17] D. Kossmann. The state of the art in distributed query
processing. ACM Comput. Surv., 32(4):422–469, 2000.

[18] C. Liu, C. Chen, J. Han, and P. S. Yu. Gplag: detection of
software plagiarism by program dependence graph analysis.
In KDD, 2006.

[19] N. A. Lynch. Distributed Algorithms. Morgan Kaufmann,

1996.

[20] S. Ma, Y. Cao, W. Fan, J. Huai, and T. Wo. Capturing

topology in graph pattern matching. In VLDB, 2012.

[21] G. Malewicz, M. H. Austern, A. J. C. Bik, J. C. Dehnert,

I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for
large-scale graph processing. In SIGMOD, 2010.

[22] M. Naor and L. J. Stockmeyer. What can be computed

locally? SIAM J. Comput., 24(6):1259–1277, 1995.
[23] F. Ranzato and F. Tapparo. An eﬃcient simulation

algorithm based on abstract interpretation. Inf. Comput.,
208(1):1–22, 2010.

[24] L. G. Terveen and D. W. McDonald. Social matching: A

framework and research agenda. In ACM Trans.
Comput.-Hum. Interact., pages 401–434, 2005.

[25] Y. Tian and J. M. Patel. Tale: A tool for approximate large

graph matching. In ICDE, 2008.

[26] H. Tong, C. Faloutsos, B. Gallagher, and T. Eliassi-Rad.

Fast best-eﬀort pattern matching in large attributed
graphs. In KDD, 2007.

[27] J. R. Ullmann. An algorithm for subgraph isomorphism. J.

ACM, 23(1):31–42, 1976.

[28] V. V. Vazirani. Approximation Algorithms. Springer, 2003.

WWW 2012 – Session: Data and Content Management 2April 16–20, 2012, Lyon, France958