Towards Network-aware Service Composition in the Cloud

Adrian Klein

The University of Tokyo

Japan

adrian@nii.ac.jp

Fuyuki Ishikawa

National Inst. of Informatics

Japan

f-ishikawa@nii.ac.jp

Shinichi Honiden
The University of Tokyo

National Inst. of Informatics

Japan

honiden@nii.ac.jp

ABSTRACT
Service-Oriented Computing (SOC) enables the composition
of loosely coupled services provided with varying Quality of
Service (QoS) levels. Selecting a (near-)optimal set of ser-
vices for a composition in terms of QoS is crucial when many
functionally equivalent services are available. With the ad-
vent of Cloud Computing, both the number of such services
and their distribution across the network are rising rapidly,
increasing the impact of the network on the QoS of such
compositions. Despite this, current approaches do not dif-
ferentiate between the QoS of services themselves and the
QoS of the network. Therefore, the computed latency diﬀers
substantially from the actual latency, resulting in subopti-
mal QoS for service compositions in the cloud. Thus, we
propose a network-aware approach that handles the QoS of
services and the QoS of the network independently. First,
we build a network model in order to estimate the network
latency between arbitrary services and potential users. Our
selection algorithm then leverages this model to ﬁnd com-
positions that will result in a low latency given an employed
execution policy. In our evaluation, we show that our ap-
proach eﬃciently computes compositions with much lower
latency than current approaches.

Categories and Subject Descriptors
H.3.5 [On-line Information Services]: Web-based ser-
vices; H.3.4 [Systems and Software]: Distributed systems

General Terms
Algorithms, Performance, Measurement, Management

Keywords
Web Services, Cloud, Network, QoS, Optimization, Service
Composition

1.

INTRODUCTION

Service-Oriented Computing (SOC) is a paradigm for de-
signing and developing software in the form of interoperable
services. Each service is a software component that encap-
sulates a well-deﬁned business functionality.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

1.1 Service Composition

SOC enables the composition of these services in a loosely
coupled way in order to achieve complex functionality by
combining basic services. The value of SOC is that it enables
rapid and easy composition at low cost [18].

Figure 1: Workﬂow

Such compositions result in workﬂows consisting of ab-
stract tasks, like in Fig. 1, that can either be created manu-
ally or computed automatically by planning approaches [23].
For each abstract task, concrete services have to be chosen
in order to execute the workﬂow.
1.2 QoS-aware Service Composition

For service compositions, functional and non-functional
requirements [17] have to be considered when choosing such
concrete services. The latter are speciﬁed by Quality of Ser-
vice (QoS) attributes (such as latency, price, or availability),
and are especially important when many functionally equiv-
alent services are available. The QoS of a composition is the
aggregated QoS of the individual services according to the
workﬂow patterns [9], assuming that each service speciﬁes
its own QoS in a Service Level Agreement (SLA). The user
can then specify his QoS preferences and constraints for the
composition as a whole, e.g. a user might prefer fast services,
but only if they are within his available budget. Choosing
concrete services that are optimal with regard to those pref-
erences and constraints then becomes an optimization prob-
lem which is NP-hard [19]. Thus, while the problem can be
solved optimally with Integer (Linear) Programming (IP)
[26], usually heuristic algorithms like genetic algorithms are
used to ﬁnd near-optimal solutions in polynomial time [6].
1.3 Service Composition in the Cloud

With the advent of Cloud Computing and Software as
a Service (SaaS), it is expected that more and more web
services will be oﬀered all over the world [5]. This has two
main impacts on the requirements of service compositions.

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France9591.3.1 Network-Awareness
First, the impact of the network on the QoS of the overall
service composition increases with the degree of distribution
of the services. A service oﬀered in Frankfurt would in many
cases not only be used in Germany but worldwide. Despite
this, current approaches do not diﬀerentiate between the
QoS of services themselves and the QoS of the network. The
common consensus is that the provider of a service has to in-
clude the network latency in the response time he publishes
in his SLA. This is not a trivial requirement, since latency
varies a lot depending on the user’s location [27]; e.g. users
in Frankfurt, New York and Tokyo may have quite diﬀerent
experiences with the same service.

(a) Deployed Services

(b) Service SLAs

Figure 2: Distributed Deployment

For example, consider the abstract workﬂow depicted in
Fig.1, and the corresponding concrete services (X1, X2, A1,
etc.), in which X1 executes task X, etc., in the times con-
forming to Fig. 2b. We can see the deployed services and the
network delays between the diﬀerent deployment locations
in Fig. 2a. In such a scenario, current approaches would se-
lect X2, A2 and B3, because their QoS are optimal, and this
results in a total execution time of 265 ms. Now, if a user in
France wants to execute the workﬂow, the round trip times
would add over 200 ms to that time. In comparison, execut-
ing X1, A1 and B1 would just take 300 ms and only incur
a minimal delay because of round trip times. On the other
hand, if providers added the maximum delay for any user
to the execution time in their respective SLAs, this would
guarantee a certain maximum response time to all users, but
it would discourage users from selecting local providers and
instead favor providers with the most homogeneous delays
for all users (e.g. providers in the center of Fig.1 in France).
1.3.2 Scalability
Secondly, as the number of services increases, the scala-
bility of current approaches becomes crucial. That is, as the
number of services grows, more functionally equivalent ser-
vices become available for each abstract task, and this causes
the complexity of the problem to increase exponentially.
One can argue that the number of services oﬀered by dif-
ferent providers with the same functionality might not grow
indeﬁnitely. Usually a small number of big providers and a
fair number of medium-sized providers would be enough to
saturate the market. The crucial point is that in traditional
service composition, most providers specify just one SLA for

the service being oﬀered. Only in some cases up to a hand-
ful of SLAs may be speciﬁed; for example, a provider might
oﬀer platinum, gold and silver SLAs to cater to diﬀerent
categories of users with diﬀerent QoS levels [22].

Figure 3: Deployed Services by Diﬀerent Providers

In contrast to this,

in a network-aware approach each
provider has to provide many SLAs, as in Fig. 3. Given
an abstract task T a provider that oﬀers a service Ti for T
might have to supply diﬀerent SLAs for each of his instances
I1, . . . , ITi . Each instance might run on a physical or virtual
machine with diﬀerent characteristics (CPU, memory, etc.).
Also, these instances might be executed at completely diﬀer-
ent locations in order to oﬀer the service in diﬀerent coun-
tries. As we want to diﬀerentiate between each instance, we
get many more choices for each individual task. For exam-
ple, whereas previous approaches might assume 50 diﬀerent
providers for each task [25], and, thus, consider 50 choices
per task, we might easily have to consider 2500 choices, if we
assume that each provider deploys 50 instances of his service
on average.
1.4 Contributions

Thus, we propose a new approach towards network-aware
service composition in the cloud, consisting of the following
three contributions:

1. Network Model. We adopt a generic network model
that can be fed in a scalable way by stand-of-the-art algo-
rithms from the network research community. We enhance
this model by adding scalable facilities to ﬁnd services which
are close to certain network locations or network paths. This
allows us to estimate the network latency between arbitrary
network locations of services or users and to ﬁnd services
that will result in low latency for certain communication
patterns.

2. Network-aware QoS Computation. We specify
a realistic QoS model that allows us to compute network
QoS, such as latency and transfer rate. Our network-aware
QoS computation can handle input-dependent QoS, as well.
(For example, a video compression service could specify that
the service’s execution time depends on the amount of input
data supplied.)

3. Network-aware Selection Algorithm. Our selec-
tion algorithm is based on a genetic algorithm. We redeﬁne
all generic operations including initial generation, mutation,
and crossover. By leveraging our network model, we tailor
those operations to the problem of service composition in
the cloud in order to improve the scalability and the solu-
tion quality of our algorithm.

In our evaluations, we show that the latency of service
compositions computed by our algorithm is near-optimal
and much lower than current approaches. Furthermore, we
show that our approach has the scalability needed for ser-

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France960vice composition in the cloud; it beats current approaches
not just in absolute numbers but also in runtime complexity.
The structure of this paper is as follows. Section 2 reviews
related work. Section 3 deﬁnes our approach. Section 4 eval-
uates the beneﬁts of our approach in a cloud environment.
Section 5 concludes the paper.

2. RELATED WORK

In this section, we survey related work from the following

ﬁve categories.
2.1 QoS-aware Service Composition

The foundation for our research is in [26]. That paper in-
troduces the QoS-aware composition problem (CP) by for-
malizing and solved it with (Linear) Integer Programming
(IP), which is still a common way to obtain optimal solutions
for the CP. A genetic algorithm is used in [6, 8]. Moreover,
many eﬃcient heuristic algorithms have been introduced [2,
15, 12, 25], the most recent being in [13, 20]. All these ap-
proaches share the same deﬁnition of the CP, which ignores
the QoS of the network connecting the services. Except for
IP which requires a linear function to compute the utility of
a workﬂow, most approaches can be easily augmented with
our network-aware QoS computation.

As for the recent skyline approach [3] that prunes QoS-
wise dominated services, this approach would have to be
modiﬁed in order to be applicable to our cloud scenario.
Even services with the same QoS can give quite diﬀerent
experiences to diﬀerent users depending on the location of
the services and the users; that means QoS-wise dominated
services can only be pruned per network location. In such
case, the beneﬁt in our scenario would be quite small when
there are many diﬀerent network locations.
2.2 Advanced QoS

The approaches described above simply aggregate static
QoS values deﬁned in SLAs. A QoS evaluation depending
on the execution time is described in [14]. In a similar way,
our algorithm computes the starting time of the execution
of each service, so it can be used to compute time-dependent
QoS, as well. SLAs with conditionally deﬁned QoS are de-
scribed in [11]; these can be considered to be a special case
of input-dependent QoS, and, thus, they can be handled by
our approach as well.

Constraints whereby certain services have to be executed
by the same provider are given in [16]. Placing such con-
straints on critical services could also reduce network delays
and transfer times. However, doing so would require signiﬁ-
cant eﬀort to introduce heuristic constraints and there is no
guarantee that this would lead to a near-optimal solution.
2.3 Network QoS

Many studies, such as [4, 10], deal with point-to-point
network QoS, but do not consider services and compositions
from SOC. One of the few examples that considers this is
[24], which looks at service compositions in cloud comput-
ing. The diﬀerence between this study and our approach is
that instead of a normal composition problem, a scheduling
problem is solved where services can be deployed on virtual
machines at will. However, it is not clear if that approach
can handle the computation of input-dependent QoS and
network transfer times, because the authors did not provide
a QoS algorithm.

2.4 Network Coordinate System

In the related ﬁeld of network research, there are many
algorithms that build network coordinate (NC) systems to
estimate the latency between any two points. Two state-of-
the-art algorithms, Vivaldi [1] and Phoenix [7], build reli-
able network coordinate systems in a scalable fashion. Our
approach does not depend on a speciﬁc network coordi-
nate system; it uses a generic network model based on two-
dimensional coordinates that can be fed by NC systems, as
in [1], that are based on the Euclidean distance model (which
is the most widely used model of NC systems).
2.5 Workﬂow Scheduling

In the related ﬁeld of workﬂow scheduling, a workﬂow
is mapped to heterogeneous resources (CPUs, virtual ma-
chines, etc.), and information about the network is some-
times considered, as well. The idea is to achieve a (near-
)optimal scheduling minimizing the execution time; this is
often done by using greedy heuristic approaches, like HEFT
[21]. The reason such greedy algorithms seem to suﬃce is
that only one QoS property (response time) is optimized,
and that no QoS constraints have to be adhered to, greatly
simplifying the problem. Thus, while the setting is similar
to ours, the complexity of the problem is quite diﬀerent, be-
cause we optimize multiple QoS properties under given QoS
constraints. In addition, algorithms, like HEFT [21], often
work like a customized Dijkstra, which does not scale well
in our problem setting, as our evaluation shows.

3. APPROACH

In this section we deﬁne our approach. We will present
our proposed network model ﬁrst and then describe our net-
work QoS computation that makes use of this model. After
that, we deﬁne our proposed network selection algorithm
that makes use of the network model, as well.
3.1 Network Model

Our network model consists of two parts: a network co-
ordinate system that forms the basis of our network-aware
approach and a locality-sensitive hashing scheme that allows
us to ﬁnd services that are close to certain network locations
or network paths.
3.1.1 Network Coordinate System
In our cloud scenario, we face the challenge of dealing
with a huge number of services, N . We use existing network
coordinate systems in order to compute the latency between
any two network locations of services or users. Probing all
pairwise link distances would require O(N 2) measurements,

Figure 4: Network Model

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France961task of our workﬂow, and, thus, support the two following
operations with our LSH, illustrated in Fig. 5.

1. Compute the Hull of a Location Given a location,
we can ﬁnd services which perform a certain task close to
that location by computing an outer hull of a certain range
around the desired location.

Algorithm 1: computeHullOf(loc, task, range)

foreach y ∈ [locY − range; locY + range] do

1 buckets := empty list
2 grid := lookup grid corresponding to task
3 locBucket := hash location loc to its bucket in grid
4 locX := locBucket.x and locY := locBucket.y
5 foreach x ∈ [locX − range; locX + range] do
6
7
8
9
10
11 end
12 return buckets

buckets := buckets(cid:83){grid(x, y)}

if (x, y) within grid then

end

end

2. Compute the Hull of a Network Path Given
a path, e.g. between two network locations, we can ﬁnd
services which perform a certain task and directly lie on the
path or its computed outer hull of a certain range.

Algorithm 2: computeHullBetween(loc1, loc2, task, range)

1 buckets := empty list
2 locations := compute trajectory between loc1 and loc2
3 foreach locx ∈ locations do
4

buckets := buckets(cid:83)

computeHullOf(locx, task, range)

5 end
6 return buckets

After calling computeHullOf or computeHullBetween,
we just need to fetch the services contained in the returned
buckets. The computation of these two hulls (that contain
both the inner and outer hulls depicted in Fig. 5) allows our
network-aware selection algorithm to minimize the latency
of service compositions, as we will show in detail later.
3.2 Network-aware QoS Computation

Our network-aware QoS computation consists of two phases.

The ﬁrst phase simulates the execution of the workﬂow in
order to evaluate input-dependent QoS and network QoS.
The second phase aggregates these QoS over the workﬂow
structure.

1. Simulated Execution We assume that a workﬂow is
either given as a directed graph as in Fig. 7a or is converted
into one (e.g. from a tree as in Fig. 7b), before we simulate
the execution of the workﬂow according to Alg. 3.

2. QoS Aggregation In the second phase, we take the
obtained QoS for each node and aggregate it in a hierarchical
manner (over the tree representation as in Fig. 7b) according

Figure 5: Computed Hulls

giving us exact values, but it would obviously not scale.
Also, while services deployed in the cloud might exist for
some time (allowing us to cache their latencies), users would
frequently show up at new locations, and, thus, they would
have to ping all existing services before they could use our
approach; this would be prohibitive.

Therefore, we use network coordinate (NC) systems that
give us an accurate estimate of the latency between any
two network locations; these systems require O(N ) measure-
ments in total and just O(1) measurements for adding a new
network location. We decided to base our generic NC sys-
tem on algorithms, like Vivaldi [1], that are based on the Eu-
clidean distance model. While such NC systems might use
any number of coordinates, we will consider here a system of
two coordinates for the sake of simplicity. Accordingly, each
network location (including services and users) is placed in
a two-dimensional space, as in Fig. 4, and the latency be-
tween two locations simply corresponds to their Euclidean
distance. This allows our network-aware QoS computation
to accurately estimate the QoS of service compositions in
the cloud.
3.1.2 Locality-sensitive Hashing Scheme
In order for our network-aware selection algorithm to work
eﬃciently, it is not enough to be able to compute the laten-
cies between arbitrary network locations. Additionally, we
need to be able to ﬁnd services that are close to certain net-
work locations or network paths, so that we can eﬃciently
search for service compositions with lower latency. We real-
ize this functionality with a simple locality-sensitive hashing
scheme (LSH) that relies on our generic NC system.

Figure 6: Network Model Grid

We build an overlay grid on top of the two-dimensional
coordinate space, as in Fig. 6. Each cell of the grid cor-
responds to a bucket that can contain services. Given this
grid, we hash every network location to the corresponding
bucket. Close network locations correspond to identical or
close buckets. We maintain a separate grid for each unique

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France962(a) Graph

(b) Tree

Figure 7: Diﬀerent Representations of a Workﬂow

to the commonly used aggregation rules from [9, 25] that
take into account workﬂow patterns, like parallel(AND) or
alternative(OR) executions and loops. Just for the runtime
of the workﬂow, we keep the computation from the ﬁrst
phase, because we cannot compute it with a hierarchical
aggregation.

Figure 8: QoS of a Workﬂow

3. Example If we annotate the services of our previous
workﬂow example with execution durations, and their net-
work links with network delays, as in Fig. 8, our algorithm
will produce the values of Fig. 9 for the execution times of
the nodes (start/end) in ﬁve steps1.

Figure 9: Simulated QoS

This simple example shows that hierarchical QoS aggre-
gation alone would not work, because A and B would be ag-
gregated together ﬁrst. This makes it impossible to compute
the correct network QoS, because the maximum of the delay
of the incoming and outgoing nodes of (A,B) each would be
aggregated as 20, adding up to 40. Actually, however, both
paths that go through the incoming and outgoing nodes of
(A,B) have a cumulative delay of 30, which is 10 less than
the aggregated value.

3.3 Network-aware Selection Algorithm

Our selection algorithm is based on a genetic algorithm
which can solve the service composition problem in polyno-
mial time. First, we give a basic overview of genetic algo-
rithms. Then, we introduce the customizations of our algo-
rithm that are tailored to the problem of service composition
in the cloud.

1the computational steps are denoted in the # column

v.execStart = 0
v.requiredIncoming = |v.incoming|

Algorithm 3: simulateExecution(graph)
1 foreach vertex v ∈ graph do
2
3
4 end
5 while ∃ v ∈ graph . v.requiredIncoming = 0 do
pick any v ∈ graph . v.requiredIncoming = 0
6
evaluate QoS of v
7
8
v.executionEnd = v.execStart + v.qos.runtime
foreach w ∈ v.outgoing do
9
10
11
12
13
14
15
16 end

netQoS = getNetworkQoS(v, w)
duration =
transEnd = v.execEnd + netQoS.delay + trans
w.execStart = max{w.execStart, transEnd}
w.requiredIncoming -= 1

v.resultSize

netQoS.transf erRate

end

3.3.1 Genetic Algorithm
In a genetic algorithm (GA), possible solutions are en-
coded with genomes which correspond to the possible choices
available in the problem. For a service composition, a genome,
as in Fig. 11, contains one variable for each task of the work-
ﬂow, with the possible values being the respective concrete
services that can fulﬁll the task.

Figure 11: Workﬂow with corresponding Genome

Initial Population

Given a ﬁtness function that evaluates how good a possi-
ble solution (individual) is, the GA iteratively ﬁnds a near-
optimal solution as follows. First, an initial population is
generated. Then, in every iteration, individuals are selected,
changed either by mutation or crossover operations, and in-
serted into the new population for the next iteration. This
procedure is repeated until a convergence criteria is met,
which checks if the ﬁtness of the population has a reached a
satisfactory level, or if the ﬁtness does not improve anymore.
3.3.2
The initial population is usually generated completely ran-
domly. While it is desirable to keep some randomness for the
GA to work properly, adding individuals that are expected
to be better than average, has beneﬁcial eﬀects both on the
speed of convergence and on the ﬁnal solution quality. Thus,
we generate about a quarter of the initial population with
our Localizer heuristic, which is illustrated in Fig. 10. Us-
ing our previously built network model, we build a possible
initial solution, task by task. In each step, we only select
randomly among those services that can fulﬁll the current
task and are close to the network location of their preceding
service. In the last step, we also consider that the ﬁnal result
has to be sent back to the user, and select randomly among
those services that are close to the network path between
the preceding location and the user’s location.

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France963Figure 10: Localizer Heuristic for a good Initial Solution

3.3.3 Selection
We select individuals that are to be reproduced via muta-
tion and crossover through a roulette-wheel selection. The
probability of an individual with ﬁtness fi to be chosen out
of a population of size N is:

This operator speeds up the convergence and improves the
solution quality, but the key is to make it not too aggressive,
and to only use it for a certain fraction of the mutations.
(If used intensively, it could cause the GA to converge too
quickly to a local optimum.)

fi(cid:80)N

j=1 fj

pi =

Thus, it is a ﬁtness-proportionate selection ensuring that
better individuals have a higher chance of being chosen.
There is no need to customize the selection operator to our
problem, as it is already reﬂected in the ﬁtness function.
3.3.4 Elitism
Elitism keeps the top-k individuals seen so far in order to
achieve convergence in a reasonable time. We keep the most
ﬁt percent of our population in each generation.
3.3.5 Mutation
The purpose of the mutation operator is to change individ-
uals slightly in a random way in order to randomly improve
their ﬁtness sometimes and to escape local optima. For this
part, we partly use the standard uniform mutation operator
which changes each element of the genome with equal prob-
ability, and on average changes about one element, as shown
in Fig. 12.

Figure 12: Uniform Mutation

In addition, we use our own Localizer mutation opera-
tor which does a combination of mutation and local search.
First, some parts of the genome are picked randomly. For
each picked part, we then try to exchange the chosen ser-
vice with a constant number of services that are randomly
chosen close to the network path between the preceding and
following service (graph-wise), as shown in Fig. 13. The
best replacement is kept.

3.3.6 Crossover
The crossover operation combines two individuals (par-
ents) to create improved individuals (oﬀspring) that can
draw from the good points of both parents. A standard
single-point crossover operator is depicted in Fig. 14: A sin-
gle point of the genome is chosen, and the new individuals
are recombined from both parents around that point.

Figure 14: Single-Point Crossover

In an analogous way, two-point crossover, three-point cross-
over, etc. can be implemented until one has a uniform cross-
over operator in which every part of the genome of the par-
ents is randomly distributed among the oﬀspring. Our Lo-
calizer crossover is a customized uniform crossover operator
which, for each part of the genome, chooses a value from its
parents with a certain probability. We build one ﬁnal oﬀ-
spring, task by task, by choosing between the parents par1
and par2. Let dprevi be the distance to the location of the
previous service (or the user at the start), and dnexti be
the distance to the location of the following service (or the
user at the end). If the following service is not chosen yet,
we take the average of the locations of both possible follow-
ing services from the two parents. The probability that we
choose a parent pari for the current task then is:

(cid:80)2

pi =

dprevi + dnexti
j=1 dprevj + dnextj

Figure 13: Localizer Mutation

The eﬀect is that the oﬀspring is a kind of smoothened
version of the parents with regard to the network path. This
quickly minimizes the latency of our oﬀspring. Note that
when the parents are spread in a similar fashion network-
wise, our Localizer crossover comes very close to standard
uniform crossover. We hence only use our crossover, as it
was shown to be eﬀective in our experiments.

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France964Figure 15: Localizer Crossover

4. EVALUATION

In this section we evaluate our approach. First, we de-
scribe the setup of our evaluation and the algorithms we
want to compare. Then, we evaluate the optimality and
scalability of our approach.
4.1 Setup

The evaluation was run on a machine with an Intel Core
2 Quad CPU with 3 GHz. All algorithms were evaluated
sequentially and given up to a maximum of 2 GB of mem-
ory if needed. We randomly generated a network containing
100,000 unique locations within our two-dimensional coordi-
nate space [0, 1] × [0, 1]. Then, we generated our workﬂows
with randomly inserted tasks and control structures. For
each task, we randomly chose a number of those network
locations and created services there. Fig. 16 depicts an
example of a generated workﬂow of length 10:

Figure 16: Example Workﬂow of Size 10

We generated workﬂows with sizes between 10 and 80 (in
steps of 10); these sizes are comparable to those of other
recent approaches with 10 [3], up to 50 [25] or 100 [2]. We
varied the number of services available per task between 500
and 4000 (in steps of 500), which is considerably more than
what most of the previous studies have used (50 [25], 250
[20] or 350 [15]). As mentioned, the reason that we consider
such a large number of services per task is that we want to
diﬀerentiate between instances of the same service that are
deployed in diﬀerent network locations. The QoS attributes
of each service were generated according to a uniform dis-
tribution.

The network latency l between two services or between a
user and a service was computed from the Euclidean distance
d of their network coordinates as follows:

(cid:26)

l(lc1, lc2) =

20 + 400 · d(lc1, lc2)

0 if d(lc1, lc2) < 

otherwise

Thus, the latency for two locations is either 0 if they are
in a local area network (closer than a small ), or between

20 and 420 ms depending on their distance so that a good
range of realistic values will be covered.

4.2 Algorithms

We chose several algorithms and compared their optimal-

ity and scalability.

Random. A simple random algorithm that picks all ser-
vices at random from the available choices.
It shows the
expected value of a randomly chosen solution and provides
a baseline that more sophisticated algorithms should be able
to outperform easily.

Dijkstra. An optimal algorithm for the shortest-path
problem.
It can be used to ﬁnd the service composition
with the lowest latency, against which we can conveniently
compare our algorithm. However, Dijkstra cannot be used
with QoS constraints or when we have several QoS.

GA 100. The normal GA that uses uniform crossover
and uniform mutation. The size of the population is 100.
We supply this GA with the standard QoS model, which
does not model network latency. Accordingly, each service
provides not just its execution time, but adds its expected
(maximum) latency on top of that. This is the current stan-
dard approach for the service composition problem.

GA* 100. The same settings as GA, except that we
provide GA* with our network model that allows it to ac-
curately predict the network latency. G* represents na¨ıve
adoption of the current standard approach to service com-
position in the cloud.

GA* 50. GA* with a population size of only 50. We
evaluated GA* 50, as well, but while it was slightly faster
than GA* 100, we decided to keep it out of the ﬁnal evalu-
ations results, because of the bad quality of the solutions it
found.

NetGA 100. Our network-aware approach as described

in Sect. 3. The size of the population is 100.

NetGA 50. Our network-aware approach with a pop-
ulation size of only 50. Its results diﬀer from NetGA 100
by only about one or two percentage points, but it requires
less runtime, as we will see later. Note that in the follow-
ing graphs the results of NetGA 50 might sometimes overlie
those of NetGA 100, because their results are so close.

The convergence criteria are identical for all GAs: If the
ﬁtness of the best individual does not improve by at least one
percentage point over the last 50 iterations, the algorithm
terminates.

4.3 Optimality

To evaluate the optimality of our approach we plotted the
latencies of the compositions found by all algorithms versus
an increasing problem size.

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France965(a) Lat. vs. Workﬂow Length
(500 Services per Task)

(b) Lat. vs. Services per Task

(Workﬂow Length 40)

Figure 17: Latency vs Increasing Problem Size

4.3.1 Workﬂow Length
Fig. 17a plots the latency against an increasing workﬂow
size with a ﬁxed number (500) of services available per task.
We randomly generated the QoS of the services, and ﬁxed
the expected runtime of the generated workﬂows as 1000
ms without considering network latency. That means for a
workﬂow size of 10, each task in a sequential workﬂow would
have an expected runtime of 100 ms; for 20 sequential tasks,
the expected runtime would be 50 ms, etc. Fig. 17a reveals
the following ﬁndings.

Impact of Network. Even though the expected runtime
of the workﬂow does not change, the optimal latency found
by Dijkstra increases linearly with the workﬂow size, as more
network communication is needed.

Impact of Network Model. The latency of the compo-
sitions found by GA 100 is pretty much the same as that of
the Random approach. Thus, the current standard approach
is not eﬀective at ﬁnding compositions with low latency at
all. This backs up our claims that services should spec-
ify their runtime in their SLAs and that approaches should
properly account for network latency in order to be eﬀective.
Impact of Network-aware Approach. GA* 100 ﬁnds
compositions with latencies that get farther from the opti-
mal latency with increasing workﬂow size: For a workﬂow
size of 10, the compositions are about 1.5 times slower on
average, and for a workﬂow size of 80, they are more than
twice as slow. On the other hand, our network-aware ap-
proach NetGA manages to keep a good approximation ratio
of the optimal solution regardless of the workﬂow size. The
results of NetGA 100 and NetGA 50 are almost identical.

Figure 18: Empiric Algorithmic Complexity (a nx)

(n := Workﬂow Length; 500 Services per Task)

Services per Task

4.3.2
Fig. 17b shows that the optimal latency decreases slightly
as the number of services per task increases, as there are
more choices available. Except for NetGA which also ﬁnds
compositions with slightly lower latency, the other GAs do
not show a decreasing tendency. Still, the latency does not
change much, once there are more than 2000 services avail-
able per task.
4.4 Scalability

We evaluated the scalability of our approach under the

same settings as for optimality.

4.4.1 Workﬂow Length
In Fig. 19a, we can see that GA runs considerably faster
than GA*. However, the qualities of the solutions found by
GA are not much better than those of Random, which takes
much less than 1 ms to compute its results. Therefore, it
seems that GA is only faster, because it fails to improve the
quality of its solutions signiﬁcantly, thus, converging more
quickly to a bad local optimum. GA*, on the other hand,
ﬁnds solutions with a somewhat reasonable quality (as we
saw earlier), but takes a considerable runtime to do so; it
takes more than double the runtime of NetGA for a workﬂow
size of 40, and more than six times for a workﬂow size of 80.
Algorithmic Complexity. In order to summarize the
diﬀerent runtimes from Fig. 19a, we analyzed our empirical
results and computed approximations for the algorithmic
complexities. We assumed runtimes of the form of a · nx
where a is a constant factor, n stands for the workﬂow size,
and x is a power. First, we found the best combination
of a and x that minimizes the square of the error of the
representation Θ(t). Then, we computed x for the tightest
upper and lower bound given the previously computed a.

Fig. 18 explains why at ﬁrst GA* is slightly faster than
Dijkstra, but then overtakes it at a certain workﬂow size:
GA* ’s algorithmic complexity is much higher, only its con-
stant factor is smaller than Dijkstra’s. In addition, we can
conclude that GA will also overtake Dijkstra for suﬃciently
big workﬂows, whereas NetGA will not. We can also see that
NetGA is not only much faster than GA* ; it also has a much
lower algorithmic complexity, which is roughly linear with

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France966(a) Runtime vs. Workﬂow Length

(500 Services per Task)

(b) Runtime vs. Services per Task

(Workﬂow Length 40)

Figure 19: Runtime vs. Increasing Problem Size

regard to the workﬂow size (as far as we can observe it in
our experiments). This is due to the good convergence be-
haviour of NetGA which eﬀectively ﬁnds compositions with
low latency. Furthermore, if we are willing to sacriﬁce one or
two percentage points of solution quality, we can use NetGA
50 instead of NetGA 100 and get even shorter runtime and
smaller algorithmic complexity.

Services per Task

4.4.2
The runtime of Dijkstra signiﬁcantly increases, as the num-
ber of services per task increases. Thus, Dijkstra quickly be-
comes infeasible for practical purposes; it takes 10 seconds
for 1000 services per task, and almost 100 seconds for 3000
services per task. Also note that the runtimes of all GAs
increase by only about 10 to 15%, so all of them scale well
in this regard in our scenario. For the same workﬂow size,
GA* 100, NetGA 100 and NetGA 50 maintain runtime ra-
tios of about 4:2:1 regardless of the number of services per
task.

5. CONCLUSION

In this paper we described a network-aware approach to
service composition in a cloud, consisting of a network model,
a network-aware QoS computation, and a network-aware se-
lection algorithm. We showed that our approach achieves
near-optimal solutions with low latency, and that it has
roughly linear algorithmic complexity with regard to the
problem size.
It beats current approaches for which the
approximation ratio of the optimal latency worsens with in-
creasing problem size, and which have algorithmic complex-
ity between n1.5 and n2. This means that our network-aware
approach scales comparatively well in situations where the
network has a signiﬁcant impact on the QoS of service com-
positions. Our approach would facilitate service composi-
tions in settings where dozens of services each get deployed
on dozens or even hundreds of instances in clouds worldwide.
As future work, we would like to investigate how our ap-
proach fares in scenarios with multiple QoS where low la-
tencies are beneﬁcial for the user, but not the top priority,
or might even conﬂict with other QoS. In some cases, some
ﬁne-tuning might be needed to always beat the standard
approaches. Furthermore, we plan to conduct real-world

experiments in a distributed setting in order to test how
well our network model can cope in practice, e.g. when the
network latency changes dynamically due to factors such
as congestion. Additionally, we would like to evaluate our
approach with other, more complex coordinate systems (es-
pecially, three-dimensional ones).

6. ACKNOWLEDGEMENTS

We would like to thank Florian Wagner for the fruitful
discussions and the detailed feedback that helped us improve
our approach. Adrian Klein is supported by a Research
Fellowship for Young Scientists from the Japan Society for
the Promotion of Science.

7. REFERENCES
[1] Vivaldi: a Decentralized Network Coordinate System.

Communication, 34(4):15–26, 2004.

[2] M. Alrifai and T. Risse. Combining Global

Optimization with Local Selection for Eﬃcient
QoS-aware Service Composition. In WWW ’09:
Proceedings of the 18th international conference on
World wide web, pages 881–890, 2009.

[3] M. Alrifai, D. Skoutas, and T. Risse. Selecting Skyline

Services for QoS-based Web Service Composition. In
WWW ’10: Proceedings of the 19th international
conference on World wide web, pages 11–20, 2010.

[4] R. Boutaba. QoS-aware service composition in large

scale multi-domain networks. In 2005 9th IFIP/IEEE
International Symposium on Integrated Network
Management, 2005. IM 2005., pages 397–410, 2005.

[5] K. Candan, W.-S. Li, T. Phan, and M. Zhou.

Frontiers in Information and Software as Services. In
ICDE ’09. IEEE 25th International Conference on
Data Engineering, 2009, pages 1761 –1768, 2009.
[6] G. Canfora, M. Di Penta, R. Esposito, and M. L.

Villani. An Approach for QoS-aware Service
Composition based on Genetic Algorithms. In
GECCO ’05: Proceedings of the 2005 Conference on
Genetic and Evolutionary Computation, 2005.

[7] Y. Chen, X. Wang, C. Shi, E. K. Lua, X. Fu, B. Deng,

and X. Li. Phoenix: A Weight-based Network

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France967Coordinate System Using Matrix Factorization. IEEE
Transactions on Network and Service Management,
pages 1–14, 2011.

[8] M. Jaeger and G. M¨uhl. QoS-Based Selection of

Services: The Implementation of a Genetic Algorithm.
In KiVS 2007 Workshop: Service-Oriented
Architectures und Service-Oriented Computing
(SOA/SOC), Bern, Switzerland, pages 359–370, 2007.

[9] M. Jaeger, G. Rojec-Goldmann, and G. M¨uhl. QoS

Aggregation for Web Service Composition using
Workﬂow Patterns. In EDOC ’04: Proceedings of the
Eighth IEEE International Enterprise Distributed
Object Computing Conference, pages 149–159, 2004.

[10] J. Jin, J. Liang, and K. Nahrstedt. Large-scale

QoS-Aware Service-Oriented Networking with a
Clustering-Based Approach. In Proceedings of 16th
International Conference on Computer
Communications and Networks, pages 522–528, 2007.
[11] A. Klein, F. Ishikawa, and B. Bauer. A Probabilistic

Approach to Service Selection with Conditional
Contracts and Usage Patterns. In
ICSOC-ServiceWave ’09: Proceedings of the 7th
International Joint Conference on Service-Oriented
Computing, pages 253–268, 2009.

[12] A. Klein, F. Ishikawa, and S. Honiden. Eﬃcient

QoS-Aware Service Composition with a Probabilistic
Service Selection Policy. In Service-Oriented
Computing, volume 6470 of Lecture Notes in
Computer Science, pages 182–196, 2010.

[13] A. Klein, F. Ishikawa, and S. Honiden. Eﬃcient

Heuristic Approach with Improved Time Complexity
for QoS-aware Service Composition. In IEEE
International Conference on Web Services (ICWS
2011), pages 436–443, 2011.

[14] B. Kloepper, F. Ishikawa, and S. Honiden. Service

Composition with Pareto-Optimality of
Time-Dependent QoS Attributes. In Service-Oriented
Computing, volume 6470 of Lecture Notes in
Computer Science, pages 635–640, 2010.

[15] F. Lecue and N. Mehandjiev. Towards Scalability of

Quality Driven Semantic Web Service Composition. In
ICWS ’09: IEEE International Conference on Web
Services, pages 469–476, 2009.

[16] D. A. Menasc´e, E. Casalicchio, and V. Dubey. On

Optimal Service Selection in Service Oriented
Architectures. Performance Evaluation, 67(8), 2010.

[17] J. O’Sullivan, D. Edmond, and A. Ter Hofstede.

What’s in a Service? Distributed and Parallel
Databases, 12(2–3):117–133, 2002.

[18] M. P. Papazoglou, P. Traverso, S. Dustdar,

F. Leymann, and B. J. Kr¨amer. Service-Oriented
Computing: A Research Roadmap. In Service
Oriented Computing (SOC), Dagstuhl Seminar
Proceedings, 2006.

[19] D. Pisinger. Algorithms for Knapsack Problems. PhD
thesis, University of Copenhagen, Dept. of Computer
Science, 1995.

[20] F. Rosenberg, M. B. M¨uller, P. Leitner, A. Michlmayr,

A. Bouguettaya, and S. Dustdar. Metaheuristic
Optimization of Large-Scale QoS-aware Service
Compositions. IEEE, International Conference on
Services Computing, pages 97–104, 2010.

[21] H. Topcuoglu, S. Hariri, and M. Wu.

Performance-Eﬀective and Low-Complexity Task
Scheduling for Heterogeneous Computing. IEEE
Transactions on Parallel and Distributed Systems,
13(3):260–274, 2002.

[22] H. Wada, J. Suzuki, and K. Oba. Queuing theoretic

and evolutionary deployment optimization with
probabilistic slas for service oriented clouds. In World
Conference on Services - I, pages 661–669, 2009.

[23] F. Wagner, F. Ishikawa, and S. Honiden. QoS-Aware

Automatic Service Composition by Applying
Functional Clustering. IEEE International Conference
on Web Services (ICWS), pages 89–96, 2011.
[24] Z. Ye, X. Zhou, and A. Bouguettaya. Genetic

Algorithm Based QoS-Aware Service Compositions in
Cloud Computing. In Database Systems for Advanced
Applications, pages 321–334, 2011.

[25] T. Yu, Y. Zhang, and K.-J. Lin. Eﬃcient Algorithms

for Web Services Selection with End-to-End QoS
Constraints. ACM Transactions on the Web, 1(1):6,
2007.

[26] L. Zeng, B. Benatallah, M. Dumas, J. Kalagnanam,

and Q. Z. Sheng. Quality Driven Web Services
Composition. In WWW ’03: Proceedings of the 12th
international conference on World Wide Web, 2003.

[27] Z. Zheng, Y. Zhang, and M. R. Lyu. Distributed QoS
Evaluation for Real-World Web Services. pages 83–90,
2010.

WWW 2012 – Session: Web Engineering  2April 16–20, 2012, Lyon, France968