Max Algorithms in Crowdsourcing Environments

Petros Venetis, Hector Garcia-Molina

Department of Computer Science

Stanford University

Stanford, CA, 94305, USA

{venetis, hector}@cs.stanford.edu

Kerui Huang, Neoklis Polyzotis

Department of Computer Science

UC Santa Cruz

Santa Cruz, CA, 95064, USA

{khuang, alkis}@cs.ucsc.edu

ABSTRACT
Our work investigates the problem of retrieving the maxi-
mum item from a set in crowdsourcing environments. We
ﬁrst develop parameterized families of max algorithms, that
take as input a set of items and output an item from the
set that is believed to be the maximum. Such max algo-
rithms could, for instance, select the best Facebook proﬁle
that matches a given person or the best photo that describes
a given restaurant. Then, we propose strategies that select
appropriate max algorithm parameters. Our framework sup-
ports various human error and cost models and we consider
many of them for our experiments. We evaluate under many
metrics, both analytically and via simulations, the tradeoﬀ
between three quantities: (1) quality, (2) monetary cost,
and (3) execution time. Also, we provide insights on the
eﬀectiveness of the strategies in selecting appropriate max
algorithm parameters and guidelines for choosing max algo-
rithms and strategies for each application.

Categories and Subject Descriptors
H.1.m [Information Systems]: Models and Principles—
Miscellaneous; H.5.m [Information Systems]:
Informa-
tion Interfaces and Presentation—Miscellaneous

Keywords
human computation, crowdsourcing, max algorithms, worker
models, vote aggregation, plurality voting

1.

INTRODUCTION

Humans are more eﬀective than computers for many tasks,
such as identifying concepts in images, translating natural
language, and evaluating the usefulness of products. Thus,
there has been a lot of recent interest in crowdsourcing,
where humans perform tasks for pay or for fun [1, 20].

Crowdsourced applications often require substantial pro-
gramming: A computer must post tasks for the humans,
collect results, and cleanse and aggregate the answers pro-
vided by humans.
In many cases, a computer program is
required to oversee the entire process. For instance, say we
have 1,000 photos of a restaurant, and we wish to ﬁnd the
one that people think is most appealing and best describes
the restaurant. (We wish to use the photo in an advertise-
ment, for example.) Since it is not practical to show one

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

human all 1,000 photos, we need to break up the problem.
For example, as a ﬁrst step, we may show 100 people 10
photos each, and ask each one of them to select the best one
in their set. Then we can select the 100 winners, partition
them into 10 new sets, and ask 10 humans to evaluate, and
so on. What we have described is a crowdsourcing algorithm,
analogous to traditional algorithms, except that the base op-
erations or comparisons (e.g., compare 10 photos) are done
by humans as opposed to by the computer. Note that we use
the term program to refer to the computer code that runs
the entire process (e.g., ﬁnding human workers, displaying
photos to the workers, paying workers, and so on). We re-
serve the term algorithm to refer to the logic that performs
a speciﬁc function of interest (e.g., ﬁnding the max).

In this paper we study an important crowdsourcing algo-
rithm, one that ﬁnds the best or maximum item in a set,
just as illustrated in the previous example. This max al-
gorithm is a fundamental one, needed in many applications.
For instance, the max algorithm can be used to ﬁnd the most
relevant URL for a given user query. (The results can sub-
sequently be used as a gold standard for evaluating search
engine results.) As another example, a max algorithm can
be used in an entity resolution application, e.g., to ﬁnd the
best Facebook proﬁle that matches a target person.

The main challenge in developing an eﬀective max algo-
rithm is in handling user mistakes or variability. Speciﬁ-
cally, humans may give diﬀerent answers in a comparison
task (i.e., pick a diﬀerent item as the best), either because
they make a mistake and pick the wrong item, or because the
comparison task is subjective and there is no true best item.
We can compensate by asking multiple humans to perform
each task (comparison in our setting), but these extra tasks
add cost and/or latency.
In this paper we address these
quality-cost-latency tradeoﬀs and explore diﬀerent types of
max algorithms, under diﬀerent evaluation criteria.

There is a substantial body of work in the theory com-
munity that explores algorithms with “faulty” comparisons
(e.g., [2, 3, 4, 7, 8, 9, 16, 21]), and that work is clearly
applicable here. However, those works use relatively simple
models that often do not capture all of the crowdsourcing is-
sues (such as the monetary cost required to execute a crowd-
sourcing algorithm). For example, these works assume that
operators (in our case humans) compare only two items at
a time, or have a ﬁxed probability of making an error. It
is not obvious how to extend the proposed techniques to
the common case in crowdsourcing where humans compare
more than two items at a time and the error is related to
how similar the items are.

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France989At the other extreme, there is recent work [15] that imple-
ments algorithms like max in a real crowdsourcing system
and considers all practical issues. Their algorithm is highly
specialized to a speciﬁc crowdsourcing scenario and it is not
obvious how to generalize it to handle other scenarios, e.g.,
diﬀerent types of item comparisons or diﬀerent types of hu-
man workers.

For our work, we have taken a middle-of-the-ground ap-
proach: We formulate a fairly detailed framework that cap-
tures, for instance, human tasks that involve sets of items
of diﬀerent sizes, and diﬀerent human error models. Within
this framework, we are able to study, not just a single algo-
rithm or approach, but a wide range of families of param-
eterized max algorithms, and we are able to study how to
tune these algorithms for best performance. Instead of just
asking the question “For one given error model (or for one
particular crowdsourcing system), what algorithm is best,”
we ask the more general question “For what type of workers
and tasks does each possible family of max algorithm work
best.” Instead of just asking the question “How does algo-
rithm X perform when humans can compare two items,” we
ask “What is the impact on cost and answer quality of the
number of items a human is given to compare each time.”
Instead of just showing that “Algorithm X works well in a
given setting,” we show how to tune each family of max algo-
rithms (e.g., how many repetitions of each tasks are needed
at each stage of the process). To answer these questions we
really need a model that is relatively rich and detailed, but
yet not as rigid as an actual implementation.

As we will see, for some of our studies we resort to simu-
lations, since the models are not amenable to detailed anal-
ysis. However, in some of our scenarios, we are able to
develop analytical expressions that describe performance.
Since both our simulations and analysis share the same un-
derlying framework, when we can obtain analytical expres-
sions, we are also able to validate them with simulations.
This redundancy gives us greater conﬁdence in both the sim-
ulations and the analysis.

Contributions
• We develop a framework for reasoning about max algo-

Our main contributions are:

rithms (Section 2).

• A max algorithm often asks multiple workers to perform
the same comparison (task), so it needs a rule to ag-
gregate the responses. We formalize the most popular
aggregation rule and explore some of its key properties
(Section 3.1).

• We present several families of parameterized max algo-
rithms. The parameters for each family include the num-
ber of items compared by each human task, and the num-
ber of repetitions for each comparison (Sections 3.2 and
3.3). Note that the parameters may vary during an ex-
ecution, e.g., we may initially perform fewer repetitions
and in later stages perform more.

• We study strategies for tuning a max algorithm, i.e., for
ﬁnding the parameter values that lead to lowest overall
error, given appropriate constraints (Section 4).

• As part of our framework, we develop a number of models
for worker error and user cost/payments (Sections 5.1 and
5.2).

• We experimentally evaluate our algorithms and strategies

under various error and cost models (Section 6).

2. PRELIMINARIES

Our problem’s input is a set of items E = {e1, e2, . . . , e|E|}.
There is a smaller than relation <, which forms a strict
ordering for the items in E. That is, for any two items
ei, ej ∈ E (with i 6= j) exactly one of the following is true:
either ei < ej or ej < ei. (We assume that ei 6= ej for i 6= j.)
The item ei ∈ E for which ej < ei, ∀j ∈ {1, 2, . . . , |E|} \ {i}
is called the maximum item in E. Our goal is to recover the
maximum item in E, given a set of constraints (our problem
is formally deﬁned in Section 2.4). (Note that the < relation
is only a formalism used to quantify eﬀectiveness. The max
algorithms themselves do not use the < relation.)

2.1 Comparisons

To determine the maximum item, an algorithm uses a
comparison operator termed Comp(). Operator Comp(S, r, R)
asks r humans to compare the items in set S ⊆ E and com-
bines the responses using aggregation rule R. (One simple
rule R for example is to pick the item with the most votes,
described in detail in Section 3.1.) Each of the r workers
selects the item from S that he believes to be the maximum
and R provides the overall winner.

We use a probabilistic model to describe a worker.

In
particular, our model gives a vector of probabilities ~p =
[p1, p2, . . . , p|S|], where p1 is the probability that the worker
returns the maximum item, p2 is the probability he returns
the second best, and so on. For example, if ~p = [0.8, 0.1, 0.1],
then a worker selects the maximum item in S with prob-
ability 0.8, and each of the non-maximum items with 0.1
probability. We study other error models in Section 5.1.

In the crowdsourcing platforms we examine, human work
is compensated. We denote the compensation of any hu-
man performing a comparison of |S| items by Cost(|S|). We
consider various types of cost functions in Section 5.2. The
total cost of Comp(S, r, R) is the sum of its compensations,
which is equal to r · Cost(|S|).

2.2 Steps

Crowdsourcing algorithms are executed in steps. A batch
of comparisons C1 is submitted during the ﬁrst step to the
marketplace and after some time the human answers are
returned. Then, depending on these answers, an appropriate
set C2 of comparisons is selected for the second step. For
the selection of the Ci comparisons all human answers from
previous steps (1, 2, . . . , i − 1) can be considered.

Since the operations in a step are likely to be executed
concurrently, the number of steps an algorithm requires is
a(n) indicator/proxy of the execution time of the algorithm.
The number of steps is an especially good indicator when
the number of human workers is very large. Thus, we want
the execution time proxy (number of steps) to be kept to a
minimum.

2.3 Max Algorithms

Our work focuses on parameterized families of max algo-
rithms, although many more algorithms can be considered.
The input of a max algorithm is a set of items E and the out-
put is an item from E that is believed to be the maximum in
E. To obtain a max algorithm A from a parameterized fam-
ily of max algorithms A, one needs to instantiate a number
of parameters: A = A(parameters). We examine in detail
two intuitive families of max algorithm in Section 3.

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France9902.4 Problem Deﬁnition

The quantities we are interested in are:

Time(A, E), the Execution Time: The number of steps
required for a max algorithm A to complete (and return
one item) for input E.

Cost(A, E), the (Monetary) Cost: The total amount of

money required for A to complete:

[r × Cost(|S|)].

PTime(A,E)

i=1

PComp(S,r,R)∈Ci

Quality(A, E), the Quality of the Result: The probabil-
ity that max algorithm A returns the maximum item from
input E.
Given a family of algorithms A, our goal is to ﬁnd param-
eters that “optimize performance.” The optimization can be
formulated in a variety of ways. Here, we focus on maxi-
mizing the quality of the result, for a given cost budget B
and a given time bound T . The human models are an input
to the problem, and thus, for example, Quality(A, E) and
Cost(A, E) take into account how humans act and how they
are compensated.

Problem 1

(Max Algorithm).

maximize

A=A(parameters)

Quality(A, E)

subject to

Cost(A, E) ≤ B
Time(A, E) ≤ T

(1)

Other possible formulations of the problem are:

• Given a desired quality of the result and a budget, opti-

mize the execution time, or

• Given a desired quality of the result and an execution

time bound, optimize the (monetary) cost.
The strategies we propose in Section 4 can be adjusted to
address these problems, but we do not consider them in our
current work.

Although we deﬁne our optimization problem using the
Quality(A, E) metric, we propose alternative metrics in Sec-
tion 5.3. These metrics describe how close the returned item
is to the maximum item in E, while Quality(A, E) describes
whether A returns the maximum item in E or not. For our
evaluations (Section 6) we use both Quality() and the addi-
tional metrics.

3. PARAMETERIZED FAMILIES OF MAX

ALGORITHMS

This section explores the two most natural families of pa-
rameterized max algorithms that can be used to solve the
max algorithm problem (Problem 1). The parameterized
families of max algorithms we propose operate in steps and
their parameters are the following:
ri: the number of human responses seeked at step i, and
si: the size of the sets compared by Comp() at step i (one
set can have fewer items). We assume a human cannot
compare more than m items, thus si ∈ {2, 3, . . . , m}.

We refer to the ri and si values used by one algorithm by
{ri} and {si} respectively.

First, we present an intuitive human responses aggrega-
tion rule (Section 3.1) that is a key component of every
max algorithm. Then, we describe the bubble max algo-
rithms (Section 3.2) and the tournament max algorithms
(Section 3.3).

3.1 Plurality Rule

This section describes the most common human responses
aggregation rule: the plurality rule (denoted by RP). In our
complete report [19] we present another popular rule (the
majority rule) and compare it to the plurality rule.

When comparison Comp(S, r, RP) is performed, one of the
items in S with the most votes is selected: If l items have
received n responses each and there is no other item with
more responses, then one of the l items (selected at random)
is considered to be the maximum item in S. The l items are
called winners.

To understand the impact of aggregation rules on Quality(),

we deﬁne AggrQuality(s, r; ~p, R):

Definition 1. AggrQuality(s, r; ~p, R) is the probability that

R returns the maximum of s items, assuming we collect r
human responses, and each response follows a probabilistic
distribution ~p.

To calculate AggrQuality(s, r; ~p, RP) (for R = RP), we as-
sume S = {e1, e2, . . . , es}, with |S| = s and es < es−1 <
. . . < e1. Thus, if ~p = [p1, p2, . . . , ps], then pi is the probabil-
ity that a human response selected item ei as the maximum
in S. We note that the number of received human responses
for items e1, e2, . . . , es follows a multinomial distribution
with parameter ~p. Thus, since r votes are provided, the
probability that ki votes have been issued for item ei (for
i = 1, 2, . . . , s) is
i . Based on these
probabilities, we can compute the overall quality of the ag-
gregation rule as follows:

k1!·k2!·...·ks! · Qs

i=1 pki

r!

AggrQuality(s, r; ~p, RP) =

s

X

l=1

1
l

r

X

n=1

X

L∈L




X

0≤ki≤n−1,i∈ ¯L
Pi∈ ¯L ki+ln=r

r!

(n!)l Qj∈ ¯L kj! Y

z∈L

pkw
w

z Y
pn

w∈ ¯L




,

where L is the set of all sets L that are subsets of {1, 2, . . . , s}
with |L| = l and 1 ∈ L. Also, ¯L = {1, 2, . . . , s} \ L. (The
derivation can be found in our complete report [19]).
In
the following, Comp(S, r) will be considered equivalent to
Comp(S, r, RP).

In all known cases, AggrQuality() is non-decreasing on r
(although there is no formal proof since 1785 [6], when the
problem was ﬁrst stated).

3.2 Bubble Max Algorithms

The family of Bubble max algorithms (denoted by AB) is
named after the bubble-sort algorithm, because of its simi-
larities to it. Algorithm A = AB({ri}, {si}) is described in
Algorithm 1.

In summary, A operates on E as follows. If E only has one
item, this item is returned by the max algorithm A. Oth-
erwise, a comparison with s1 random items from E is per-
formed and a winner w is declared. The s1 random items
are removed from E. If E is now empty, w is returned. Oth-
erwise, in every step i, si − 1 items from E are chosen and
unioned with {w} forming S′
i, ri, R) is “executed”
and a winner w is declared, while the items of S′
i are re-
moved from E. When E becomes empty, the last winner w
is returned by the max algorithm.

i. Comp(S′

To formally deﬁne the max algorithm problem (Problem 1)
we introduce a few new variables in this section: xi (the size
of the output of step i), si (the size of the smallest set that

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France991Algorithm 1: AB({ri}, {si}) operating on E

if E == {e} then

return e

1 ← random subset of E of size min(s1, |E|) ;

1 ;

S′
E ← E \ S′
w ← Comp(S′
i ← 2 ;
while E 6= ∅ do

1, r1) ;

Si ← random subset of E of size min(si − 1, |E|) ;
E ← E \ Si ;
S′
w ← Comp(S′
i ← i + 1 ;

i ← Si ∪ {w} ;

i, ri, R) ;

1

2

3

4

5

6

7

8

9

10

11

12

13

return w

is compared at step i), and xi (the number of items that are
compared within a set of size si at step i). For example, in
step i, we can have an input of xi−1 = 12 items and si = 5.
In this case, we will have 2 sets of size si = 5 compared.
Thus, xi = 10 items are compared within sets of size si.
The remaining xi−1 − xi = si = 2 items are included in a
smaller set of size si to be compared.

Now we are able to express the following three quanti-

ties:

x0

i=j

i=1

j=2

sj −1

i=1
s1
x0

i=1 si ≤ 1.

(ri × Cost(si)).

×QTime(A,E)

× QTime(A,E)

Time(A, E): the smallest k for which |E| + k −Pk
Cost(A, E): PTime(A,E)
Quality(A, E):
AggrQuality(si, ri; ~pi, R) +
+PTime(A,E)
AggrQuality(si, ri; ~pi, R).
For this family of max algorithms Quality(A, E) is the
summation of the probabilities of the maximum item tak-
ing part in the ith comparison for the ﬁrst time, multi-
plied by the probability that it is propagated to all next
steps.
Our goal is to select the appropriate sequences {ri} and
{si} in order to maximize Quality(), given constraints on
Cost() and Time(). We formalize the optimization problem
derived from Equation (1) for the bubble max algorithms in
our complete report [19].

3.3 Tournament Max Algorithms

The Tournament max algorithms (AT) are named after
tournament-like algorithms that declare best teams in sports.
Given the parameters described at the beginning of this sec-
tion (sequences {ri} and {si}), we obtain a max algorithm
A = AT({ri}, {si}). A operates on some input E and returns
some item from E, as described in Algorithm 2.

Algorithm 2: AT({ri}, {si}) operating on E

1

2

3

4

5

6

7

i ← 1 ;
Ei ← E ;
while |Ei| 6= 1 do

partition Ei in non-overlapping sets Sj, with
|Sj| = si (the last set can have fewer items);
i ← i + 1 ;
Ei ← Sj{C(Sj , ri, R)} ;

return e ∈ Ei

In summary, A operates on E as follows. Set Ei con-
tains the items that are compared at step i (E1 = E). At
step i, Ei
is partitioned in non-overlapping sets (Sj, for
j = 1, 2, . . . , ⌈ |Ei|
⌉) of size si (|Sj| = si) and the winners
si
of each comparison Comp(Sj , ri, R) form set Ei+1.
If Ei+1
has exactly one item, this item is the output of A. Other-
wise, the process is repeated for one more step.

To formally deﬁne the max algorithm problem (Problem 1)

we need to be able to express three quantities:
Time(A, E): the minimum k for which xk = 1 (more details

in our complete report [19]).

sj

sj

j=1

j=1

xj−1

(cid:0) xj

ri ·(cid:16)⌊ xj−1

Cost(A, E): PTime(A,E)
Quality(A, E): QTime(A,E)

⌋ × Cost(sj) + Cost(sj)(cid:17).
AggrQuality(sj , rj; ~pj , R)+
AggrQuality(sj , rj; ~pj , R)(cid:1), which is the product of
xj−1
the probabilities that the maximum item in each step is
propagated to the next step.
We again aim to select appropriate sequences {ri} and
{si} in order to maximize Quality(), given constraints on
Cost() and Time(). We formalize the optimization problem
derived from Equation (1) for the tournament max algo-
rithms in our complete report [19].

4. STRATEGIES FOR TUNING MAX AL-

GORITHMS

This section describes strategies that optimize max al-
gorithms with parameters {ri} and {si} that run in steps
(like the ones we described in Section 3). Our strategies are
not guaranteed to be optimal, but they heuristically deter-
mine parameter choices ({ri} and {si}) that improve per-
formance while satisfying the (budget and time) constraints.
We present the strategies by increasing complexity.

4.1 ConstantSequences

The ﬁrst strategy we examine is the one most commonly
used in practice today: The practitioner decides how big
the sets of items are and how many human responses he
seeks per set of items. ConstantSequences emulates this
behaviour and selects parameters {ri} and {si} among the
constant sequences search space: For all i we have ri = r
and si = s. This way, we only have two parameters (r and
s) to determine.

ConstantSequences operates as follows. It goes through
all acceptable s values (from 2 to m), determines if the exe-
cution time constraint is satisﬁed, ﬁnds the maximum pos-
sible r for each s that keeps the cost below the budget B,
and computes Quality(). Finally, ConstantSequences re-
turns the “optimal” probability of returning the maximum
item and the r and s that give this “optimal” probability
(ˆp, ˆr, ˆs).

Proposition 1. If AggrQuality(s, r; ~p, R) is non-decreasing

on r, ConstantSequences returns the optimal selection of r
and s for constant sequences (for the bubble and tournament
max algorithms).

The proof can be found in our complete report [19]. As
mentioned in Section 3.1, AggrQuality() is assumed to be
non-decreasing in the political philosophy literature for the
plurality rule [14], and thus, we are conﬁdent that Con-
stantSequences returns the optimal r and s values.

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France9924.2 RandomHillclimb

RandomHillclimb builds on top of ConstantSequences and
operates as follows.
It ﬁrst determines the optimal r and
s values using ConstantSequences; the s value does not
change after this point for any step. Then, it attempts to
adjust the r values across the steps, such that Quality()
increases. A random source step c is selected and rc (repe-
titions at step c) is decreased by 1. Thus, some additional
comparisons can be applied in other steps. We check each
possible other step t and we attempt to add all the addi-
tional comparisons to this target step. If by “moving” the
repetitions from c to t we manage to improve the quality of
the tournament we adjust the sequence {ri}. We perform
this greedily, i.e., we make the adjustment for the step t that
maximizes the improvement in Quality() every time. If at
some point we fail to improve the performance of the max
algorithm, we stop.

4.3 AllPairsHillclimb

AllPairsHillclimb is an extension of RandomHillclimb
with one important diﬀerence: This strategy considers all
possible steps as sources c (not just a random one). We again
greedily change the r’s by keeping each time the source c and
the target t that maximize the improvement in Quality(A, E).

4.4 AllSAllPairsHillclimb

AllSAllPairsHillclimb is a generalization of the All-
PairsHillclimb. The diﬀerence is that all possible s’s are
considered (not only the one that ConstantSequences re-
turns). This optimization can only lead to improvements
for Quality(). We observe the signiﬁcance of these improve-
ments in our experiments in Section 6.

4.5 VaryingS

Algorithm 3: VaryingS(x0, ~p, B, T , A)

// global optimal values
(ˆp, {ˆri}, {ˆsi}) ← (0.0, N U LL, N U LL) ;
(p, {ri}, {si}) ←
AllSAllPairsHillclimb(x0, ~p, B, T, A) ;
u ← 1 ;
ˆru ← r1 ;
ˆsu ← s1 ;
b1 ← budget consumed from the selection of ˆr1 and ˆs1
on an input of size x0 ;
while xu > 1 do

u ← u + 1 ;
(p, {ri}, {si}) ←
AllSAllPairsHillclimb(xu−1, ~p, B − bu−1, T − (u −
1), A) ;
ˆru ← r1 ;
ˆsu ← s1 ;
bu ← budget consumed from the selection of ˆr1, ˆr2,
. . . , ˆru and ˆs1, ˆs2, . . . , ˆsu on an input of size x0 ;

1

2

3

4

5

6

7

8

9

10

11

12

13

14

ˆp ← Quality(A({ˆri}, {ˆsi}), E) ;
return (ˆp, {ˆri}, {ˆsi}) ;

VaryingS is a generalization of AllSAllPairsHillclimb
and it is the only strategy we examine that allows diﬀerent
s values in diﬀerent steps. VaryingS operates as seen in
Algorithm 3. The parameters for step 1 are the same as the

parameters the AllSAllPairsHillclimb returns for step 1
If the parameters for steps 1, 2, . . . , u − 1
(lines 1–5).
have been decided, the parameters for step u are decided as
follows: ru and su are set equal to the parameters for step 1
of AllSAllPairsHillclimb with the non-consumed budget
and execution time and the appropriate number of input
items (lines 8–12). The process stops when the selection of
{si} returns exactly one item in some step (line 7). After
all the selections have occurred, Quality() is computed and
returned (lines 13–14).

5. EVALUATION FRAMEWORK

Max algorithms can be used by various applications and
each application gives rise to diﬀerent human behaviours.
Also, each application may focus on diﬀerent evaluation met-
rics. To understand the quality/time/cost tradeoﬀ, we ex-
plore various concrete error and compensation worker mod-
els (still a fraction of what our framework supports) in Sec-
tions 5.1 and 5.2, and propose various evaluation metrics
in Section 5.3 (extending the already deﬁned Quality(A, E)
metric). In our complete report [19], we also explain how
our proposed models can be applied in other domains.

5.1 Human Error Models

This section describes various models for human errors.
We present our models from the most detailed one to the
simplest one. As we have seen in Section 2.1, a set of items S
is given to a human and the error model assigns probabilities
to each (possible) response of a human. We assume there are
no malicious workers, and thus p1 ≥ 1
|S| (i.e., the probability
of returning the maximum item in S is at least as high as
the probability of returning a random item from S). Our
complete report [19] describes how an appropriate model
(and parameters) can be selected in practice.

Let the input set of items of each comparison be S =
{e1, e2, . . . e|S|}, with ei being the ith best item in E (or
equivalently, e|S| < e|S|−1 < . . . < e1). As a reminder,
a human response has probability pi of returning item ei.
The rank of an item is deﬁned as follows:

Definition 2

(Item’s Rank). The rank of item ei ∈
S ,denoted as rank(ei, S), is deﬁned as the number of items
in S that are better than ei plus 1.

For example, rank(e1, S) = 0 + 1 = 1 and rank(ei, S) = i.

The proximity error model
Proximity Error Model
takes one parameter p ∈ [ 1
|S| , 1] and assumes there is a dis-
tance function d(·, ·) that compares how diﬀerent two items
are. Formally, d(ei, ej) ∈ (0, 1) (since ei 6= ej for i 6= j).
When d(ei, ej) is close to 0, ei is similar to ej. On the con-
trary, when d(ei, ej) is close to 1, ei and ej are very diﬀerent.

A worker returns ei with probability:

p1 = p
pi = (1 − p) ·

1−d(ei,e1)

P

|S|
j=2[1−d(ej ,e1)]

, i ∈ {2, 3, . . . , |S|}.




The intuition is the following: Humans ﬁnd it hard to dis-
tinguish similar items. Thus, the closer one item is to the
maximum item in S, the higher its probability of being se-
lected by the human as the maximum.

Order-Based Error Model
This model is similar to the
previous model with one diﬀerence: The distance function

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France993is deﬁned as d(ei, ej) = | rank(ei,S)−rank(ej ,S)|
∈ (0, 1). A
worker returns e1 with probability p1 = p and returns ei
with probability pi = (1 − p) ·

|S|−| rank(ei,S)−rank(e1,S)|

|S|

|S|
P
j=2

[|S|−| rank(ej ,S)−rank(e1,S)|]

for i = 2, 3, . . . , |S|. Probabilities pi depend on the order of
item ei in this model.

Linear Error Model
This model assumes that the hu-
man worker is able to determine the maximum item in S
with a probability that depends on the number of items |S|
to be examined. Intuitively, the more items to examine, the
harder the task becomes.

The probability that a worker selects the maximum item
is p1 = 1 − pe − se · (|S| − 2), where pe, se are model pa-
rameters. Eﬀectively, p1 = 1 − pe when |S| = 2 and it
decreases by se for each additional item in S. When the
worker fails to return the maximum item (with probability
pe + se · (|S| − 2), he returns a random (but not the maxi-
mum) item from S. Thus, each item in {e2, e3, . . . , e|S|} has
probability 1−pe−se·(|S|−2)
to be selected. Values pe and se
take values in such a way that pe + se · (|S| − 2) ∈ [0, 1 − 1
|S| ]
for all values of |S| used in practice.

|S|−1

Constant Error Model
This model assumes that the
human worker is able to determine the maximum item from
S with probability p ∈ [ 1
|S| , 1], for any S (with |S| ≥ 2).
In the event that the worker is not able to determine the
maximum item from S (which occurs with probability 1−p),
he returns one random non-maximum item from S. Thus,
each item in {e2, e3, . . . , e|S|} has probability 1−p
|S|−1 to be
selected.

5.2 Human Cost Models

This section describes the models we explore for human
work cost. The function Cost(·) we deﬁned in Section 2.1
takes as input the number of items that are compared at a
time.

Constant Cost Model
ﬁxed price. That is, Cost(|S|) = c for some constant c.

In this model, each task has a

In this model, each task has a price
Linear Cost Model
that depends on |S|.
If the human worker works on two
items, he is compensated with c, if he works on three items,
he is compensated with c + sc, if he works on four items,
he is compensated with c + 2 × sc, and so on. Formally,
Cost(|S|) = c + sc × (|S| − 2), for some constants c and sc.

5.3 Evaluation Metrics

We deﬁned Quality(A, E) as the main optimization crite-
rion, but there are other metrics of interest. Quality(A, E)
describes how often A returns the maximum item in E.
Other metrics evaluate how often A returns one of the top-k
items in E, or how close the returned items are to the maxi-
mum item. This section describes some standard metrics we
have used for our evaluations. We note that we only optimize
for the Quality(A, E) metric, but evaluate max algorithms
using the metrics we deﬁne in this section also.

We deﬁne the metrics of interest in the context of simu-
lations. We simulate the application A(E) (max algorithm
A operates on input set E) E times and then obtain the
following values:
Top-k is the fraction of the E simulations for which A(E)
belonged in the top-k items of E (i.e., rank(A(E), E) ≤ k).

1

i=1

ranki

E PE

Mean Reciprocal Rank (MRR) is 1

, where
ranki is the rank of the returned item in the ith simula-
tion.
Both metrics yield values in [0, 1]: Values close to 0 indi-
cate that A returns items that are not close to the maximum
item, while values close to 1 indicate that A returns items
that are close to the maximum. Also, both metrics have a
probabilistic meaning: For the top-k metric for example, if
we have a signiﬁcantly large number of simulations, we con-
verge to the probability that A returns an item in the top-k
items. Finally, we experimentally observe exceptionally low
variance values for our metrics (lower than 10−3 in all cases),
and only report the expected metric values in Section 6.

6. EXPERIMENTS

This section summarizes various experiments we have per-
formed, organized by results. We ﬁrst explore which max
algorithms and which strategies perform best. Surprisingly,
we see that the same algorithm and strategy perform best
under all combinations of human models and parameters we
have tested (Section 6.1). Nevertheless, we explore in more
detail:
• the eﬀect of diﬀerent parameters (like B, T , m, p, pe,
se, c, and sc) on the performance of the max algorithms
(Section 6.1),

• characterizations of the “optimal” solutions (Section 6.2),
• the eﬀectiveness of our proposed optimization strategies,
by comparing our heuristics to exhaustive search (Sec-
tion 6.3), and

• the sensitivity of our algorithms to inaccurate estima-
tions of the crowdsourcing system error model parame-
ters (Section 6.4).
Additionally, in our complete report [19]:

• We show that even though obtaining the top-1 item is
hard, ﬁnding one of the top-2 or top-3 items is signiﬁ-
cantly easier in practice,

• We show that performing more repetitions in a single
tournament is more eﬀective than performing multiple
tournaments (same input) with fewer repetitions each
(and at the end combining results), and

• We show that relaxing the time bound constraint can
improve the quality of the result by 50% in some cases.
Most of our experimental results were analytically calcu-
lated (and conﬁrmed with simulations). In a few cases, we
resorted to simulations only, since analysis was not possible.
When we use simulations, we point out why analysis was
hard and how our simulations were run.

6.1 Max Algorithms and Strategies Performance

This section compares the eﬀect of various strategies to
the quality of the result Quality() for the two parameterized
max algorithms we have deﬁned in Section 3 (bubble and
tournament).

We experimented with various human models and param-
eters: budget (B), time bound (T ), error model parameters
(p, pe, se), and cost model parameters (c, sc). All param-
eters tested gave qualitatively similar results. For succinct-
ness, we present one selection of human models and parame-
ters. Figure 1 shows Quality() on the y-axis and the budget
B on the x-axis, for each of the ﬁve strategies we described
in Section 4 (ConstantSequences, RandomHillclimb, All-
PairsHillclimb, AllSAllPairsHillclimb, and VaryingS).

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France994Tournament

)
(
y
t
i
l
a
u
Q

0.75

0.5

0.25

Bubble

ConstantSequences

RandomHillclimb

AllPairsHillclimb

AllSAllPairsHillclimb

VaryingS

25

50

75

100

Budget B

25

50

75

100

Budget B

0.75 Q
u
a
l
i
t
y
(
)

0.25

0.5

Figure 1: The eﬀect of the strategies (middle) on the tournament (left) and bubble (right) max algorithms.

The graph on the left refers to the tournament algorithm
and the graph at the right to the bubble algorithm; the leg-
end in the middle is common for both graphs. To generate
Figure 1 we used |E| = 100, the order-based error model
with parameter p = 0.78 (one of the many values we tried
in the range [0.6, 1]), the linear cost model with parameters
c = 1 and sc = 0.1, and allowed sets S participating in some
Comp() to have size at most m = 10. Finally, we set the
time bound T to inﬁnity and varied the budget B between
20 and 100 with a step of 20.

We observe in Figure 1 that there are “discontinuities”
(jumps) for Quality() as budget B increases. These dis-
continuities exist because: (1) B needs to be increased by a
particular amount to increase ri for some step i (e.g., a tour-
nament algorithm performs r1 × ⌈ |E|
⌉ comparisons at step
1, thus to increase r1 by 1 requires an additional budget
for ⌈ |E|
⌉ new comparisons), and (2) the ri increase does not
s1
have the same Quality() impact all times (e.g., increasing
any ri from 1 to 2 has no impact to Quality() by the plu-
rality rule properties, but increasing ri from 2 to 3 increases
Quality()).

s1

From Figure 1, we obtain the following:

Result 1: As expected, as the budget B in-
creases, the quality of the result Quality() in-
creases (for all strategies).

Result 1 states the intuitive fact that the more money one
is willing to spend, the higher the quality of the result is.
The increase is almost linear for the range of budgets we
examined. Furthermore, we do not observe topping oﬀ of
gains for the values of budget B we present, but of course it
takes place at higher values.

When comparing the ﬁve strategies with each other we

obtain the following:

Result 2: For all budgets examined and for both
the bubble and the tournament max algorithms
there is a clear ordering of the strategies from best
to worst:
VaryingS, AllSAllPairsHillclimb,
AllPairsHillclimb, RandomHillclimb, and Con-
stantSequences. Thus, VaryingS is the strategy
that should be used in all cases in practice.

Most applications today use strategies like ConstantSe-
quences, i.e., ri = r and si = s for all steps i. Result 2 sug-
gests that the quality of the result improves when the num-
ber of repetitions and the set size are allowed to vary across
steps. Interestingly, comparing to ConstantSequences, Vary-
ingS increases Quality() by ∼100% for the same budget in
some cases (e.g., for the tournament algorithm for B = 40)!
Thus, applications can beneﬁt by our strategies in two ways:
either increase Quality() for a ﬁxed budget, or produce a de-
sired Quality() for a lower budget.

Comparing the two max algorithms (bubble and tourna-

ment) we obtain the following:

Result 3: Tournament max algorithms perform
better than bubble max algorithms for the same
budget.

The performance diﬀerence between the tournament and
the bubble max algorithms can be explained in the following
way. The maximum item has to “survive” (not erroneously
be dropped by a comparison Comp()) by all the comparisons
it participates in for it to be the output of the max algo-
rithm. In bubble max algorithms the worst case scenario is
that the maximum item survives a very long chain of com-
parisons (Θ(|E|)), while in the tournament max algorithms
the number of comparisons is small in all cases (Θ(log |E|)).
Finally, it is important to note that in no combination of
human models and parameters that we tested, the bubble
max algorithms performed better than the tournament max
algorithms.

Now, observing each of the families of max algorithms

separately, we observe that:

Result 4: For very small or very large budgets
B, the diﬀerences between the ﬁve strategies are
not signiﬁcant. For middle values of B though we
observe large diﬀerences between the strategies.
We generally observe large diﬀerences between the
strategies for B ≈ |E|
2 shifted appropriately as the
cost parameters change. Thus, it is “indiﬀerent”
which strategy is used for very small or very large
values of B, but for the middle values VaryingS
should be used when possible.

The diﬀerences between the Quality() values the strate-
gies achieve, can be explained if we think of each strategy as
a search in the parameter space of sequences {ri} and {si}.
For small values of B the diﬀerences between the resulting
Quality() values are not signiﬁcant, because there are not
many choices of {ri} and {si} permitted by budget B. As
B increases, there are more parameter choices and the dif-
ferences between the strategies’ performance increase: Some
strategies are able to determine good parameter choices and
some are not. For very high values of B the diﬀerences be-
tween the performance of the algorithms decreases again.
There are many {ri} and {si} choices and all strategies per-
form very well: If there is enough budget, one can increase
ri’s signiﬁcantly and observe high Quality() for any strat-
egy. Browsing through the resulting {ri} and {si} from the
various strategies for large B’s indicates that the actual se-
lected ri and si values may be signiﬁcantly diﬀerent, but
the Quality() values are very close. We also note that the
diﬀerences between strategies are not signiﬁcant when p1 is
close to 1.

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France9956.2 Distribution of r’s

This section explores how the ri values are distributed
across the steps, when all steps have the same si. We exper-
iment with the AllPairsHillclimb strategy and the tour-
nament max algorithm.

i
r

20

0

1

2

3

4

i

B = 1500
B = 4000
B = 5500
B = 10000

Figure 2: Exploring how AllPairsHillclimb algo-
rithm distributes repetitions (ri’s) across steps.

Figure 2 shows the ri values on the y-axis achieved for
each step i (x-axis). For simplicity, we used si = 10 for each
step i and observed the ri’s. We used |E| = 10,000 (thus,
there are exactly 4 steps for the tournament max algorithm),
inﬁnite time bound (T = ∞), the constant error model with
p = 0.8, and the constant human cost model with c = 1. We
used various budgets (from B = 1,500 up to B = 10,000).

From Figure 2 we observe that:

Result 5:
Balanced numbers of repetitions
across steps are beneﬁcial when the budget allows
it. Thus, when ri’s are unbalanced, it is because
the budget does not allow more repetitions to be
allocated in some steps.

We observe in Figure 2 that when the budget is low (B =
1,500) all 4 steps are assigned one repetition each (ri = 1
for all i’s). As budget increases (B = 4,000 or B = 5,500)
mainly the last steps are beneﬁted from more repetitions.
(The number of comparisons performed in the initial steps
is very high, and thus it costs a lot to allocate comparisons in
them.) As we increase the budget even more (B = 10,000),
we observe that the repetitions across steps are again bal-
anced (approximately 10 repetitions per step for all steps).
Interestingly, there no are clear patterns for the {si} se-

quence, and we explain why in our full report [19].

6.3 Comparison with Exhaustive Search

This section evaluates how eﬀective our strategies are with
respect to an exhaustive search of the parameter space. In
order to evaluate our strategies, we perform an exhaustive
search over the space of {ri} and {si}. The exhaustive search
is very expensive, and thus we focus on a small instance of
the problem: We consider |E| = 10, m = 4, and set a maxi-
mum value for ri to 5. We use the linear error model (with
pe = 0.2 and se = 0.04), and the linear cost model (with
c = 1 and sc = 0.2). We set the budget B to 10, and the
time bound T to ∞. For the exhaustive search, we ﬁnd all
{ri} and {si} that lead to Cost(A, E) below B and for each
A = AT({ri}, {si}) we simulate the value (averaging over
500,000 simulation runs) for each of the following metrics:
MRR and top-k for k = 1, 2, and 3. (As mentioned earlier,
Quality() and top-1 are the same: They both measure the
probability that an algorithm returns the maximum item.)
We only evaluate the VaryingS strategy for the tournament
max algorithms, which is the best strategy (Result 2).

e
u
l
a
v

c
i
r
t
e
m

1

0.8

0.6

0.4

top-1 top-2 top-3 MRR

metric

Figure 3: Comparing the VaryingS strategy for
the tournament max algorithms with an exhaustive
search over the parameter space.

Figure 3 shows the distribution of the metric values using
box/whiskers plots and evaluates the eﬀectiveness of Vary-
ingS. Each box/whiskers plot shows the range of metric val-
ues observed as we exhaustively examine all {ri} and {si}
values (that meet the budget constraint). The top bar rep-
resents the maximum value observed, the bottom bar is the
lowest value. The bottom of the box is the ﬁrst quartile
(minimum value above the 25% percent of the observed val-
ues), the top of the box is the third quartile (minimum value
above 75% percent of the observed values), and the line in
the middle of the box is the median value observed. The
x-axis presents the metrics we examined (top-k and MRR).
The y-axis presents the metric values. For each metric, we
present the achieved value by VaryingS using the × symbol.
By observing the range of metric values the exhaustive

search achieves, we see that:

Result 6: For all metrics, there is a wide range
of values that are achieved by diﬀerent parameter
selections. Thus, simple strategies are not likely
to work well.

Observing the performance of the VaryingS strategy, we

see that:

Result 7: Even though VaryingS optimized over
only one objective function (top-1 or Quality()),
it produces very good results in all metrics we ex-
amined. Thus, it is enough to only optimize over
Quality() using VaryingS, to obtain high quality
results.

Result 7 suggests that using VaryingS to tune algorithms

is enough for any metric of interest.

6.4 Error Model Parameters Sensitivity

Modeling humans is a very diﬃcult task, and thus it is
vital for an algorithm to be tolerant in erroneous estima-
tions of the error model parameters. This section explores
how sensitive our algorithms are to the error model param-
eters. For our study, we assume that a crowdsourcing mar-
ketplace has error model parameters ~P , but the algorithm
A = AT({ri}, {si}) that is run was generated by strategy
VaryingS with parameters ~P ′ 6= ~P .

We have experimented with all the error models we have
described in Section 5.1 obtaining similar results. We now
describe one of the many experiments we have performed.
For this experiment, we used |E| = 1,000 items with the
linear cost model (c = 1 and sc = 0.2), our budget was
B = 1,400, the time bound was T = ∞, and m was set to 10.
We used the linear error model with parameters pe = 0.15

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France996and se = 0.02. We generated various algorithms A using the
VaryingS strategy, assuming that the crowdsourcing mar-
ketplace had parameters p′
e = se = 0.02.
In Figure 4 we see the diﬀerence ∆pe = p′
e − pe (x-axis) and
the Quality() achieved by the algorithm A (y-axis) produced
with the corresponding ∆pe (i.e., the algorithm returned by
VaryingS when p′

e = pe + ∆pe = 0.15 + ∆pe).

e ∈ (0, 0.3) and s′

)
(
y
t
i
l
a
u
Q

0.915

0.91

0.905

−0.1

0.1

0

∆pe

Figure 4: Sensitivity of a tournament algorithm to
the linear error model parameter pe.

We see that:

Result 8:
As expected,the highest value for
Quality() is observed for ∆pe = 0. Negative and
positive values of ∆pe (i.e., p′
e 6= pe) give slightly
(but not signiﬁcantly) smaller values for Quality().
Thus, small parameter errors do not signiﬁcantly
impact the performance of the algorithms.

Because of Result 8, the estimations of the actual parame-
ters of the error models followed in practice do not need to be
perfectly accurate. Thus, the cost for estimating the model
parameters of a marketplace can be kept to a minimum with
small accuracy losses in the algorithm’s application.

7. RELATED WORK

A lot of work has been done in the context of our prob-
lem. In summary, comparing to previous work, our paper is
diﬀerent in at least three aspects:
Error/cost models: The error and cost models we con-
sider are more general than previously considered error
models.

Notion of Steps: Crowdsourcing environments force algo-
rithms to be executed in steps, making the design of al-
gorithms harder.

Number of items compared: We allow the ﬂexibility of
comparing “any” number of items per operation. All
other techniques we are aware of only perform binary
comparisons.

In the rest of this section we present work related to our

paper.

Resilient algorithms
Some related work to ours has
extensively examined so-called resilient algorithms. These
algorithms assume that some items from the input are cor-
rupted (e.g., because of memory faults). The goal is to pro-
duce the correct result (for computing the maximum item
in a set for example) for the uncorrupted items in the input.
One of the ﬁrst papers in this area is [9], where resilient
algorithms for sorting and the max algorithm problem are
provided. An overview paper on resilient algorithms is [13],
which presents work done in resilient counting, resilient sort-
ing, and the resilient max algorithm problem. Along these

lines [10, 11] attempt to understand for a given algorithm,
how many corruptions can be tolerated for it to run cor-
rectly.
In our work we assume that no item from the in-
put may be corrupted, but that humans (who are eﬀectively
“comparators”) can make mistakes. Also, only binary com-
parisons are considered in [9, 10, 11, 13]; we consider any
comparison of size 2 or more.

Sorting/Max algorithms with errors
Another line
of work similar to ours involves sorting networks, in which
some comparators can be faulty. One of the ﬁrst works in
this area [21] proposes sorting algorithms that deal with two
scenarios of faults: (1) up to k comparators are faulty and
(2) each comparator has a probability δ (δ is small) of fail-
ing and yet, the result is retrieved correctly with probability
greater than 1−ε (ε is small). In [4] sorting networks that re-
turn the correct result with high probability are constructed
from any network that uses unreliable comparators (each
comparator can be faulty with some probability smaller than
1
2 ). The depth of the resulting network is equal to a constant
multiplied by the depth of the original network.

3

In [16] the max algorithm problem is considered under two
error models: (1) up to e comparisons are wrong, and (2) all
“yes” answers are correct and up to e “no” answers are wrong.
The max (and the sorting) problem is also considered in [3]
under a diﬀerent error model: If the two items compared
have very similar values (their absolute diﬀerence is below
a threshold), then a random one is returned; otherwise, the
correct item is returned. The max algorithm problem is
solved using 2 · n
2 comparisons. An extension of the error
model used in [16] is given in [2] where up to a fraction of p of
the questions are answered erroneously at any point. In [2]
it is proved that if p ≥ 1
2 , then the maximum item may
not be retrieved; if p < 1
2 , then the maximum item can be
(1−p)n (cid:17) binary comparisons (where n is
retrieved with Θ(cid:16)
the size of the input). Even more elaborate error models are
considered in [2]: For example, the number of wrong answers
we have received may be at most p fraction of the number
of questions only at the end of the process (and for example
the ﬁrst 5 questions may all be answered incorrectly).
In
this model if p ≥ 1
n−1 , then the maximum item may not be
found; if p < 1
n−1 , then the maximum item can be found in
n − 1 questions.

1

The complexity of four diﬀerent problems is explored in [7,
8]. The problems considered are: (1) binary search, (2) sort-
ing, (3) merging, and (4) selecting the kth best item. The
error model considered is the following: For any comparison
of items there is a probability p that the correct result is
returned and 1 − p that the wrong result is returned.

Again, this line of work also does not have the notion of
steps, nor the complexity of the error models we are explor-
ing. Also, the only type of comparisons that has been used
is that of two items per comparison.

Crowdsourcing Marcus et al. [15] consider the problems
of sorting and joining datasets in real crowdsourcing system,
considering many practical issues. Their work implements
max algorithms (as instances of sorting algorithms) by split-
ting the input set of items E in non-overlapping sets of equal
size and asking workers to sort these sets. Our work on
the contrary allows us to answer more general questions re-
garding diﬀerent scenarios, such as diﬀerent types of human
workers and compensation schemes, diﬀerent algorithms and

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France997strategies. For instance, we can use our framework to tune
the tournament algorithm in the setting described in [15].

Tournament algorithms have been used in crowdsourcing
to determine the correct answer to a diﬃcult task [18]. An-
swers provided by n workers to a task (not necessarily a com-
parison) are compared (in pairs) by workers. The winners
of the pairwise comparisons are compared with each other
in the next step. This process continues for a ﬁxed number
of steps, after which the majority answer is returned. The
intuition is that workers are better in identifying the cor-
rect answer (when comparing it to other answers) than in
producing the correct answer.

Sorting in steps Work has also been done in sorting in
a particular number of rounds (or steps as we deﬁned them).
H¨aggvist and Hell prove bounds on the complexity of sorting
n items in r rounds [12]. In the same spirit, [5] answers the
following question: How many processing units are required
if one wants to sort n items in r rounds? In these two works
there is no uncertainty about the data nor the comparisons.

Voting systems
Voting systems have been used to de-
clare a winner (maximum item in our case) according to
votes (comparisons to other items in our case). One work in
this line of research is [17], which declares a winner such that
it creates the least disagreement with the votes of the crowd.
Furthermore, de Condorcet [6] and List and Goodin [14] pro-
vide useful insights for the plurality rule from the standpoint
of political philosophy.

8. CONCLUSIONS

We investigated methods for retrieving the maximum item
from a set in a crowdsourcing environment. We developed
parameterized families of algorithms to retrieve the maxi-
mum item and proposed strategies to tune these algorithms
under various human error and cost models. We concluded,
among other things, the following:
• The type of worker errors impact results accuracy, but

not what is the best algorithm/strategy.

• It pays oﬀ to vary the size of a task.
• It pays oﬀ to vary/optimize the number of repetitions.
• Finding the maximum item with high accuracy is expen-
sive, unless workers are very reliable. However, it is much
easier (costs less) to ﬁnd an item in the top-2 or top-3.
Our results explore multiple models and shed light on var-
ious aspects of realistic crowdsourced max algorithms. The
models we used assume uniform and independent worker er-
rors. Of course, in a real crowdsourcing environment these
assumptions may not hold. We believe though it is impor-
tant to ﬁrst understand the tradeoﬀs between quality/time/cost
in more tractable scenarios. Furthermore, our algorithms,
optimized for the simpliﬁed model, may very well be good
starting points in a more realistic setting, and can then be
experimentally ﬁne tuned.

Natural extensions of our work include the retrieval of
the top-k items, from a set and sorting sets of items both
from the modeling perspective and from the practitioners’
viewpoint.

9. ACKNOWLEDGMENTS

10. REFERENCES
[1] Amazon Mechanical Turk. http://www.mturk.com/,

sep 2011.

[2] M. Aigner. Finding the Maximum and Minimum.

Discrete Applied Mathematics, 74(1):1–12, 1997.

[3] M. Ajtai, V. Feldman, A. Hassidim, and J. Nelson.

Sorting and Selection with Imprecise Comparisons. In
Automata, Languages and Programming, pages 37–48.
2009.

[4] S. Assaf and E. Upfal. Fault Tolerant Sorting

Network. FOCS, pages 275–284, 1990.

[5] B. Bollob´as and A. Thomason. Parallel Sorting.
Discrete Applied Mathematics, 6(1):1–11, 1983.

[6] M. de Condorcet. Essai Sur L’ Application De L’

Analyse `A La Probabilit´e Des D´ecisions Rendues `A La
Pluralit´e Des Voix. 1785.

[7] U. Feige, D. Peleg, P. Raghavan, and E. Upfal.

Computing with Unreliable Information. In STOC,
pages 128–137, 1990.

[8] U. Feige, P. Raghavan, D. Peleg, and E. Upfal.

Computing with Noisy Information. SIAM J.
Comput., pages 1001–1018, 1994.

[9] I. Finocchi, F. Grandoni, and G. Italiano. Designing

Reliable Algorithms in Unreliable Memories. In
Algorithms — ESA, pages 1–8. 2005.

[10] I. Finocchi, F. Grandoni, and G. Italiano. Optimal
Resilient Sorting and Searching in the Presence of
Memory Faults. In Automata, Languages and
Programming, pages 286–298. 2006.

[11] I. Finocchi and G. F. Italiano. Sorting and Searching

in the Presence of Memory Faults (without
Redundancy). In STOC, pages 101–110, 2004.

[12] R. H¨aggkvist and P. Hell. Sorting and Merging in

Rounds. SIAM J. on Matrix Analysis and
Applications, 3:465–473, 1981.

[13] G. Italiano. Resilient Algorithms and Data Structures.

In Algorithms and Complexity, pages 13–24. 2010.

[14] C. List and R. E. Goodin. Epistemic democracy:

Generalizing the Condorcet Jury Theorem. Journal of
Political Philosophy, 9:277–306, 2001.

[15] A. Marcus, E. Wu, D. Karger, S. Madden, and

R. Miller. Human-Powered Sorts and Joins. PVLDB,
5:13–24, Sept. 2011.

[16] B. Ravikumar, K. Ganesan, and K. B. Lakshmanan.

On Selecting the Largest Element in spite of
Erroneous Information. In STACS, pages 88–99. 1987.

[17] Ronald L. Rivest and Emily Shen. An Optimal

Single-Winner Preferential Voting System based on
Game Theory. In COMSOC, pages 399–410, 2010.

[18] Y.-A. Sun, C. R. Dance, S. Roy, and G. Little. How to

Assure the Quality of Human Computation Tasks
When Majority Voting Fails? Workshop on
Computational Social Science and the Wisdom of
Crowds (NIPS), 2011.

[19] P. Venetis, H. Garcia-Molina, K. Huang, and

N. Polyzotis. Max algorithms in crowdsourcing
environments. Technical report, Stanford University.
http://ilpubs.stanford.edu:8090/1017/.

[20] L. von Ahn. Games with a purpose. Computer,

39:92–94, 2006.

[21] A. C. Yao and F. F. Yao. On Fault-Tolerant Networks

We thank Aris Anagnostopoulos and Aditya G. Parames-

waran for useful early discussions on this work.

for Sorting. Technical report, Stanford, CA, USA,
1979.

WWW 2012 – Session: CrowdsourcingApril 16–20, 2012, Lyon, France998