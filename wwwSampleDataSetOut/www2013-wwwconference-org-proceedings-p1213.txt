Evaluating and Predicting User Engagement Change with

Degraded Search Relevance

Yang Song

Microsoft Research
One Microsoft Way

Redmond, WA

yangsong@microsoft.com

Xiaolin Shi
Microsoft Bing

One Microsoft Way

Redmond, WA

xishi@microsoft.com

∗
Xin Fu

LinkedIn Corporation
2029 Stierlin Court
Mountain View, CA

xin.fu.2007@gmail.com

ABSTRACT
User engagement in search refers to the frequency for users
(re-)using the search engine to accomplish their tasks. A-
mong factors that aﬀected users’ visit frequency, relevance
of search results is believed to play a pivotal role. While
multiple work in the past has demonstrated the correlation
between search success and user engagement based on lon-
gitudinal analysis, we examine this problem from a diﬀerent
perspective in this work. Speciﬁcally, we carefully designed
a large-scale controlled experiment on users of a large com-
mercial Web search engine, in which users were separated
into control and treatment groups, where users in treatment
group were presented with search results which are delib-
erate degraded in relevance. We studied users’ responses
to the relevance degradation through tracking several be-
havioral metrics (such as query per user, click per session)
over an extended period of time both during and following
the experiment. By quantifying the relationship between
user engagement and search relevance, we observe signiﬁ-
cant diﬀerences between user’s short-term search behavior
and long-term engagement change. By leveraging some of
the key ﬁndings from the experiment, we developed a ma-
chine learning model to predict the long term impact of rel-
evance degradation on user engagement. Overall, our model
achieves over 67% of accuracy in predicting user engagement
drop. Besides, our model is also capable of predicting en-
gagement change for low-frequency users with very few user
signals. We believe that insights from this study can be
leveraged by search engine companies to detect and inter-
vene search relevance degradation and to prevent long term
user engagement drop.

Categories and Subject Descriptors
G.3 [Probability and Statistics/Experimental Design]:
controlled experiments, randomized experiments, A/B test-
ing; H.4.m [Information Systems]: Miscellaneous

General Terms
Measurement, Design, Experimentation, Prediction
∗

Work done at Microsoft Bing.

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

Keywords
search quality, search relevance, user engagement, longitu-
dinal analysis

1.

INTRODUCTION

As Web search engines have become a necessity in our
daily lives, the relevance of search engine results has un-
doubtedly become the deterministic factor for search engines
like Google, Bing, Yahoo! and etc.
to declare their suc-
cesses and compete for query market share. For the past
decade, researchers and practitioners have never stopped
working towards improving search engines’ relevance, by
leveraging state-of-the-art methods from communities like
machine learning, data mining, natural language processing
and so on. These restless eﬀorts have brought tremendous
success to search engine companies with noticeable advances
in search result relevance. According to a recent report
by Experian1, the search success rate for Yahoo!, Bing and
Google were 81%, 80% and 66%, respectively.

Although it seems to be common sense that the better a
search engine’s relevance is, the more likely that users will
engage with it (i.e., come back to search more often), it has
come to our attention that few research eﬀort has been spent
to study the relationship between search relevance and us-
er engagement. Until very recently, Hu et al.[6] proposed
to characterize the relationship between search success and
search engine reuse by measuring the correlation between
changes in search satisfaction ratio and the rate of return.
A positive correlation was identiﬁed between these two vari-
ables, which truly indicates that search success can lead to
higher user engagement rate.

In this paper, we propose to study search relevance and
user engagement from a ranking perspective, and in a more
controlled environment. Despite the existence of many fac-
tors that can potentially inﬂuence search relevance, such as
snippet quality, domain bias [7], the quality of the ranking
algorithm is by far the most dominant component that de-
termines a search engine’s relevance score. We therefore iso-
late ranking from all other factors to better understand the
change of user engagement from a longitudinal perspective.
On the other hand, our study tries to look at the correla-
tions between these two variables from a diﬀerent perspec-
tive than previous work [6]: how user engagement changes
when the ranking algorithm suddenly becomes worse than

1http://www.experian.com/hitwise/press-release-experian-
hitwise-reports-google-share-of-searche.html

1213before? While most existing research aims at improving the
ranking algorithm, and most user engagement analysis is
conducted in the environment where users succeeded their
search objective, we, however, believe that search failure
can potentially lead to engagement changes which is more
complicated to understand and therefore should not be ig-
nored. The momentum for this study is quite straightfor-
ward: search engine companies nowadays make changes to
their ranking algorithms on regular basis. Before releasing a
new ranking algorithm, it is common sense to ﬁrst test it on
a small portion of user basis, which is widely known as ran-
domized experiments or A/B tests [10]. Apparently, not all
changes can lead to an improved ranking algorithm in prac-
tice but may possibly hurt user experience and turn users
away. It is therefore essential to understand the user behav-
iors under this scenario and make proper adjustments, e.g.,
performing early-termination of a bad ranking experiment,
promptly.

Speciﬁcally, this paper makes the following contributions:
We present a longitudinal study on a large amount of users
from the logs of a widely-used commercial Web search en-
gine. These users are enrolled in a carefully designed exper-
iment in which the changes were only made to the ranking
algorithm such that the ranking results look worse than be-
fore, while all other elements on the search result page stay
the same. We then perform deep analysis on user engage-
ment changes by studying the user behavior data at session-
level, user-level as well as query-level to fully understand the
root causes of user behavior changes.

Given the user engagement numbers over time, we propose
a machine learning model to predict engagement changes,
i.e., whether a user will come back more/less often in the
future. To be concrete, we leverage a set of features such
as the average length of user queries, the portion of queries
that has no clicks and so on, and use these features to train
an SVM model to make binary predictions for each user’s
weekly engagement change.

The rest of the paper is organized as follows: Section 2
discusses Related Work; Section 3 presents the details of
our controlled experiments and the data collected; Section 4
performs deep analysis on the data; Section 5 proposes the
model to predict user behavior changes; we conclude our
paper and discuss potential future work in Section 6.

2. RELATED WORK

Our study is based on a large-scale controlled experimen-
t of a commercial search engine. In the industry of online
services, designing online experiments to test the impact of
changes of products or services with large amounts of real
users has been an extremely important problem [19]. A-
mong many approaches, online controlled experiments have
been widely adopted, as this type of experiments have the
advantages of having best scientiﬁc design for establishing a
causal relationship between changes and their inﬂuence on
user-observable behavior and providing the ﬁrst-hard feed-
back from large volumes of online users directly [10].

There has been extensive study on online user behavior
that is related to information seeking and navigation. Most
of such study uses recorded user search data [8], which can
provide us with rich signals about diﬀerent aspects of user
behavior. For example, by studying clickthrough data, we
are able to evaluate and monitor user satisfaction toward
the relevance of a search engine [16, 20]. We can also have

a good estimation on users intent [8] or how much they are
frustrated with their search [5]. Previous research has al-
so found that, by tracking user search behavior over time,
such behavior could be well modeled and predicted [3, 15].
Moreover, some longitudinal study on user search behavior
suggests that there are two classes of users: navigators and
explorers [21]. In this work, we show that user behavior in
the virtual world of information search, similar to behavior
in many other real-world social and ecology systems [18], is
also highly adaptive with regard to the change of this infor-
mation system.

The main focus of user behavior in our study is user en-
gagement in search, which directly ties to the market share
of a search engine. It is believed that in many other forms
of products and brands, customer satisfaction has a strong
inﬂuence on the loyalty and market share [1, 14]. However,
only very recently researchers started investigation on the
relationship between user engagement and their satisfaction
toward the service provided by search engines [6]. One sig-
niﬁcant diﬀerence between the research of user engagement
and loyalty in the use of search engines and other products
lies in that, it is a complicated problem of deﬁning the usage
of a search engine. This is because the frequency of issuing
queries is not equal to the frequency of accomplishing tasks
in information search, as users may issue multiple queries
to accomplish one task [13, 11]. Moreover, we should be
aware that, in real business, the long-term user behavior do
not always align with short-term behavior, and this prob-
lem is particularly prominent in user engagement in search
[10]. Thus, unlike [6], which only considers user usage on
the query level, we focus more on the task or session level
in terms of user engagement in this paper.

3. EXPERIMENT DESIGN AND DATA COL-

LECTION

In this work, we conducted an A/B testing user experi-
ment on a widely-used commercial Web search engine in the
US search market, from Jan 2011 to March 2011 for a total
of 47 days. Two randomized buckets of users, of approxi-
mately equal size, were chosen as the control and treatment
groups, whereas for the control group, the ranking algorith-
m remains the same as before. For the treatment group,
we deliberately released an inferior ranking algorithm which
has shown to have worse relevance scores in terms of NDCG
scores [9] – approximately 3-point NDCG loss. To be more
concrete, we used an old ranker with a less sophisticated
machine-learning model and a diﬀerent set of features. Al-
most 100% of queries were aﬀected, i.e., showing diﬀerent
or re-ordered top-10 results between control and treatment.
Figure 1 shows an example. For the query “yahoo email”,
we can clearly observe the diﬀerences between control and
treatment results. For control, the ideal ranking is preserved
where the #1 result is indeed what users look for. However,
for users in the treatment group, they suﬀered from a total-
ly weird list of ranked results: the oﬃcial yahoo homepage
was ranked ﬁrst, followed by an ehow page and a wikipedia
page, both of which are somewhat irrelevant. The fourth
result looks like the desired page at the ﬁrst glance, but un-
fortunately it turns out to be misleading too. Notice that
although here we say that 100% of queries have diﬀerent
top-10 results, what we really mean is that at least one re-
sult in top-10 are diﬀerent. Therefore, many of the queries,

1214especially top navigational ones, still show the same top-3 or
top-5 results in both control and treatment. In the next sec-
tion, we shall examine the impact of truly degraded queries
to users in more details.

Overall, a total of 2.2 million users were enrolled in this
experiment. After the ﬁnish of the experiment, we collected
user session data from the logs. By our deﬁnition [6], a
user session is a sequence of user search activities, including
user queries, clicks on the search results and so on. A session
ends when the user becomes inactive for 30 minutes or more.
Users are anonymized by given a randomly generated user
id for each user.

Let’s deﬁne a metric named percentage delta, which is
used throughout this paper to measure the diﬀerence of any
metric between treatment and control:

%Δ(f ) =

f (T reatment) − f (Control)

f (Control)

× 100% (1)

Here f can be any arbitrary metric, e.g., session per us-
er, click through rate and so on (details in next session).
In our study, we notice a signiﬁcant diﬀerence between the
number of active days of users in treatment (4.500 days) vs.
control (4.515 days) (whose % Δ is -0.335% with p-value
0.004), where a user is deﬁned as active if he/she issued one
or more queries on that day. However, there is no signiﬁ-
cant diﬀerence between the range of active days of users in
treatment and control (both are around 11 days on aver-
age), which is deﬁned as the date diﬀerence between user’s
ﬁrst active day and last active day. For example, two users
u1 and u2 are active on days {1, 2, 3, 4, 15} and {1, 15} of
the experiment, respectively. Both of them have 15 as their
range of active days, while u1 has 5 active days but u2 has
only 2. These facts reveal that, the quality change of search
engine has eﬀect on the users’ frequency of usage; however,
we don’t see there is a higher rate of abandonment when the
search quality gets worse.

4. USER ENGAGEMENT ANALYSIS

We focus on three aspects of user analysis in this sec-
tion: (1) treatment/control comparison in terms of several
key metrics, (2) aﬀected user analysis, i.e., how user engage-
ment changes before and after users were exposed to bad rel-
evance results, and (3) degree of aﬀect analysis: quantifying
engagement ratio changes across user groups characterized
by the number of bad search results experienced.

4.1 Overall Engagement Changes Over Time
If we consider the search engine as an ecosystem, it is
expected to see that users change their behavior accordingly
once this ecosystem, i.e. the quality of the search results,
changes. In this part, we investigate the overall user search
behavior change with regard to the change of the search
environment with a deliberate setback. In generally, search
related user behavior can be classiﬁed into three categories:

a. Types of queries users issue. This type of behavior
is performed before a user seeing the search result of
the current query. Queries can be classiﬁed according
to diﬀerent criteria, such as navigational queries vs.
informational queries, head queries vs. tail queries and
short queries vs. long queries [2, 12].

b. User’s satisfaction toward the search results. This
type of behavior shows users’ reaction toward the search
result of the current query. Behaviors such as how
quick the users click the search results and how soon
they issue the next queries are strong signals showing
users’ satisfaction.

c. User’s engagement in search. This type of behavior
indicates the usage frequency and how frequently users
using or reusing the search engine in order to accom-
plish their search tasks. From the following analysis,
we will see that the short-term and long-term engage-
ments are very diﬀerent.

By studying the three types of user search behaviors, we
aim to answer the following two questions: 1, what are the
temporal patterns of user search behavior when the search
quality changes? 2, how do diﬀerent search behaviors cor-
relate with each other? Especially, how do other behaviors
correlate with the long-term user engagement?

Therefore, we focus on examining the following key met-
rics in our study (the type in square bracket indicates the
type of user behavior mentioned above):

• [c] Average daily sessions per user (S/U):

(cid:2)

u S(u)
|u|

,

where S(u) indicates user u’s daily session number and
|u| is the total number of users on that day.

(cid:2)

s U Q(s)

• [b] Average unique queries per session (UQ/S):
, where U Q(s) represents the number of u-
nique queries within session s, and |s| the total number
of sessions on that day.

|s|

• [b] Average session length per user (SL/U): the
total number of queries within a session, averaged over
each user.

• [a] Percentage of navigational queries per user
(%-Nav-Q/U): there exists many methods for this
type of classiﬁcation [2, 12]. We propose a simple
method by looking at click positions: if over n% of all
clicks for a query is concentrated on top-3 ranked URL-
s, this query is considered to be navigational. Other-
wise it is treated as informational. Here we empirically
set n to be 80.

• [a] Average query length per user (QL/U): the
query length measures the number of words in a user
query.

• [b] Average query success rate per user (Q-
Success/U: a user query is said to be successful if
the user clicks one or more results and stays at any of
them for more than 30 seconds [6].

• [b] Average query Click Through Rate (CTR):
the CTR for a query is 1 if there is one or more clicks,
otherwise 0.

• [b] Average query interval per user (QI/U): the
average time diﬀerence between two consecutive user
queries within a user session.

While S/U indicates the user engagement ratio, the re-
maining metrics measure the user satisfaction as well as their
eﬀort spent to complete user’s search task, and therefore can
be categorized as relevance metrics.

1215(a) Control (original ranking)

(b) Treatment (worse result)

Figure 1: Example query of “yahoo email” and the ranking diﬀerences between control and treatment.

We ﬁrst examine the user engagement change by looking
at S/U and UQ/S. Rather than comparing treatment and
control groups on a daily basis, we propose to use accumu-
lated numbers to better capture the metric changes from a
longitudinal perspective. For example, to compare S/U on
the K’s day since the starting of the experiment, we aggre-
gate user sessions from day 1 to day K for each user, and
calculate the diﬀerence between treatment and control using
unpaired (two sample) t-test.

The accumulated results of these two metrics are shown
in Figure 2. With the treatment group having a worse rele-
vance algorithm, we initially expect to see a noticeable drop
of of S/U in treatment in the ﬁrst few days, resulting a dip
in t-stat that may reach signiﬁcance level (t-stat ≤ -1.96)
very quickly. However, in contrast with common sense, we
see that S/U in treatment actually went up quite a bit for
the ﬁrst three days, and then gradually decreased. The t-
stat for S/U did not reach signiﬁcance level until almost two
weeks after the experiment. We also observe diminishing
returns after two weeks for the rest of days.

On the other hand, UQ/S has shown signiﬁcance from the
very ﬁrst day of the experiment, with t-stat > 5 for the ﬁrst
few days. We have observed that UQ/S raised over 2% for
treatment comparing to control (3.66 vs. 3.57) on day 1, and
continue to stay at 1% for the ﬁrst two weeks. This shows
a clear sign that users in treatment group have spent more
eﬀort to complete their search tasks than users in control.

Next, we show the (non-accumulative) temporal change
of user behavior related to their satisfaction with the search
results and engagement in Figure 3. We measure the diﬀer-
ence between treatment and control using eq.(1). In Figure
3(a), we see that the average query success rate drops signif-
icantly as soon as the search quality degrades, which means
that user satisfaction toward their search results is an im-
mediate indication of the change of search quality. Figure
3(b) shows the change of average query intervals. The time
interval is also strongly correlated with user satisfaction due
to the fact that if users are not satisﬁed with their current
search results, they are very likely to reformulate the orig-
inal queries within a very short period of time. Similarly,
Figure 3(c) shows that after the search results get worse,

2

0

/

U
S
c

 

i
t
s

i
t
a
t
s
−
t

−2

−4

−6

 

5
5

10
10

Acumulated Daily Plot

 

8

UQ/S
S/U

7
S
Q
U
c

 

/

6

i
t
s

i
t
a
t
s
−
t

5

4

35
35

40
40

45
45

15
15
days since experiment started

20
20

25
25

30
30

Figure 2: Accumulated daily plots for S/U and
UQ/S. T-stats are shown on the left and right y-
axis respectively.

the average session length increases. This again tells us that
after the search quality gets worse, there are more queries
within a session as it requires more eﬀorts from the users to
accomplish their search tasks.
Therefore, with user engagement metric S/U going up
then down (↑ then ↓), relevance metric UQ/S going up (↑),
and average session length going up a little bit (↑), what
is really going on underneath these metric changes? There
can be a few possible reasons: (1) users indeed come back
less frequently, but issue the same type of queries day after
day. Due to the deteriorated relevance, users need to formu-
late more frequently to complete same tasks as before; or (2)
users still come back at the same rate as before, but however
do not trust the search engine any more, and hence give easi-
er tasks to it, e.g., by issuing more navigational queries. For
more diﬃcult tasks, users simply ﬁnd other workarounds for
example by switching to another search engine with better
relevance results.

To ﬁnd the exact answer for that, we perform a more ﬁne-

grained analysis on aﬀected users in the next section.
4.2 Affected User Analysis

What we have observed in the previous section is the s-

1216Average Query Success Rate

Query Interval

Average Session Length

0

.

2
−

5

.

2
−

0

.

3
−

5

.

3
−

p−value<=0.05
0.05<p−value<=0.2
p−value>0.2

l

o
r
t

n
o
c
 

d
n
a

 
t

t

n
e
m
a
e
r
t
 

n
e
e
w
e
b

t

 

a

t
l

e
D
%

l

o
r
t

n
o
c
 

d
n
a

 
t

t

n
e
m
a
e
r
t
 

n
e
e
w
e
b

t

 

a

t
l

e
D
%

0

1
−

2
−

3
−

4
−

p−value<=0.05
0.05<p−value<=0.2
p−value>0.2

l

o
r
t

n
o
c
 

d
n
a

 
t

t

n
e
m
a
e
r
t
 

n
e
e
w
e
b

t

 

a

t
l

e
D
%

p−value<=0.05
0.05<p−value<=0.2
p−value>0.2

0
3

.

5
2

.

0
2

.

5

.

1

.

0
1

1

5

9 13

18

23

28

33

38

43

1

5

9 13

18

23

28

33

38

43

1

5

9 13

18

23

28

33

38

43

Days since experiment started

Days since experiment started

Days since experiment started

(a)

(b)

(c)

Figure 3: Daily change of behavior related to user satisfaction toward their search results and user engagement
with search.

tatistics from overall users in both groups, which gives us
some hints in terms of which direction those metrics devel-
oped. However, in this experiment we performed (as well
as many other relevance-improvement experiments), though
we claim that almost 100% of queries had diﬀerent search
results, only a small fraction of queries are practically in-
volved, where their top-3 search results got re-ordered. S-
ince most of the time, users are indeed only examine and
click top-3 search results. As a result, many users were not
truly exposed to results with bad relevance at all and their
engagement changes are quite unnoticeable. Therefore, we
only aim at studying behavioral changes for the group of
aﬀected users in this section.

We deﬁne the set of aﬀected queries retrospectively. Af-
ter the experiment was ﬁnished, we collected a set of queries
that had been issued by users from both treatment and con-
trol groups. Within them, we isolated queries where the
rankings of the results were diﬀerent in treatment and con-
trol. We also ﬁltered out queries with very low frequencies
to make sure the diﬀerence was not caused by server insta-
bility. For each of these queries, we gathered top-10 results
for treatment and control respectively. We then asked hu-
man assessors to evaluate the relevance of each result with
respect to that query, in a 5-level Likert scale: Perfect (5),
Excellent (4), Good (3), Fair (2) and Bad (1). We employed
normalized discounted cumulative gain (NDCG) [9] to as-
sess the relevance of a ranking result, where higher NDCG
scores (between 0 and 1) indicate better relevance. With
that, the set of aﬀected queries is deﬁned to have at least k
diﬀerence (k ∈ (0, 1)) between control and treatment, with
control having higher score. The value of k balances the
trade-oﬀ between the size of the query set and the discrimi-
native power of the set, where a high value of k results in a
smaller set but queries with larger NDCG diﬀerences. Note
that since we are only interested in truly impacted users, we
use NDCG@3 as the metric here.

We then deﬁne the set of aﬀected users. A user is said
to be aﬀected if he/she issued at least one of the queries in
the aﬀected query set. To better interpret user behaviorial
changes before and after the user was aﬀected, we selec-
t users who were only aﬀected after the third week of the
experiment. We expect those users to behave similarly as

r
e
s
U

 
r
e
P
 
s
n
o
s
s
e
S

i

 
f

o

 
r
e
b
m
u
N
 
y

l
i

a
D

S/U (Engagement) of Affected Users

8
3

.

6
3

.

4
3

.

2
3

.

0
3

.

8
2

.

6
2

.

0

Control
Treatment

10
Days since experiment started

20

30

40

Figure 4: S/U change of the aﬀected users in treat-
ment and control. Users were aﬀected on and after
the 21st day of the experiment, indicated by the
vertical dash line.

other non-aﬀected users in the ﬁrst three weeks, while ex-
hibit diﬀerent usage patterns after they were aﬀected by bad
relevance queries.

In our study, we empirically tried a number of k’s for the
aﬀected query set but due to space limitation only k = 0.6
is reported here. We ended up having roughly 450 queries in
the set. After ﬁltering, 5,134 and 5,287 users were selected
from treatment and control groups, respectively.

Figure 4 illustrates the change of user engagement in terms
of S/U over time, where the vertical grey dotted line indi-
cates the aﬀected date2. During the ﬁrst three weeks, no
signiﬁcant diﬀerence can be observed for the treated users.
Nevertheless, once treated users were aﬀected, we immedi-
ately notice substantial drop of engagement rate. The value
of %Δ(S/U ) suggests a 5% drop during the ﬁrst few aﬀected
days, and raises up to as high as 20% at peak.

On the other hand, UQ/U shows a similar pattern as S/U
for the ﬁrst three weeks, as demonstrated in Figure 5. How-
ever, as soon as treated users were aﬀected, the number of

2Due to the sensitivity of data, all the absolute val-
ues reported in this paper have been linearly scaled.

1217r
e
s
U

 
r
e
P
 
s
e
i
r
e
u
Q
e
u
q
n
U
 
y

 

i

l
i

a
D

2
1

1
1

0
1

9

8

7

0

UQ/U of Affected Users

Average Query Length of Affected Users

Control
Treatment

h

t

g
n
e
L

 

 
y
r
e
u
Q
e
g
a
r
e
v
A
 
y

l
i

a
D

0
3

.

8
2

.

.

6
2

4
2

.

2
2

.

0
2

.

Control
Treatment

10
Days since experiment started

20

30

40

0

10
Days since experiment started

20

30

40

Figure 5: UQ/U change of the aﬀected users in
treatment and control. Users were aﬀected on and
after the 21st day of the experiment.

Figure 7: Average daily query length for aﬀected
users. Shorter queries are more likely navigational
while longer queries are tail/hard queries.

% of Navigational Queries of Affected Users

Click Through Rate (CTR) of Affected Users

Q
−
v
a
N
−
%
 
y

l
i

a
D

0
7
0

.

5
6
0

.

0
6
0

.

5
5
0

.

0
5
0

.

5
4
0

.

0
4
0

.

5
3
0

.

0

Control
Treatment

2
9
0

.

0
9
0

.

8
8
0

.

6
8
0

.

4
8
0

.

2
8
0

.

e

t

 

a
R
h
g
u
o
r
h
T
 
k
c

i
l

C
 
y

l
i

a
D

Control
Treatment

10
Days since experiment started

20

30

40

0

10
Days since experiment started

20

30

40

Figure 6: Query distribution change over time. Y-
axis indicates the percentage of navigational queries
users issued.

Figure 8: Change of CTR of impacted users. CTR
ﬁrst went up after the 21 day of experiment then
suddenly dropped signiﬁcantly.

unique queries they issued clearly went up for the next 6-
7 days, which gradually decreased in the following weeks as
UQ/U in treatment became less than that of controlled user-
s. This further conﬁrms the same ﬁndings as we found in
the previous section: after initially aﬀected by a relevance
engine, users spent more eﬀort on reﬁning their queries to
complete the same task, but gradually lost conﬁdence so
they came back less frequently.

Now that we know that user engagement decreases when
exposed to a bad relevance engine, we want to further un-
derstand how that inﬂuence users’ queries and clicks over
time. To start with, we classify user queries into either nav-
igational or informational [2, 12] as discussed in previous
section. The distribution of query types is depicted in Fig-
ure 6. Before aﬀected, both control and treatment users
have roughly the same % of navigational queries — around
40% daily. After the aﬀected point, we can clearly observe a
soar of navigational queries merely after three days for those
who got aﬀected, which raised much as 64% on day 40.

We then examine the change of user query length. Pre-
vious research has shown that there is a high correlation
between query length and its diﬃculty [17], i.e., navigation-
al queries are mostly short queries (1 or 2 words), while
tail queries are longer than head queries in general. With
the relevance getting worse in treatment, we expect user-

s to issue easier queries than before. This assumption is
conﬁrmed in Figure 7. After initially aﬀected, users started
to reformulate their queries and therefore a sudden increase
of query length is observed for the ﬁrst week after aﬀected.
Users then gradually became inpatient and only issued short
queries.

Last but certainly not the least, we quantify the changes in
user clicks in terms of click through rate (CTR). In general,
higher CTRs correspond to better user satisfaction. From
Figure 8, we notice that CTR dropped substantially dur-
ing the ﬁrst week after treated users were aﬀected, as they
began to issue more queries to ﬁght against bad relevance
but eventually failed. Later, as those users started to issue
more navigational queries, the CTRs gradually increased to
a comparable rate to those of the controlled users.

We summarize these ﬁve ﬁndings for aﬀected users in Ta-
ble 2. Statistical signiﬁcance test is conducted where the
changes of the ﬁve metrics are all signiﬁcant. Figure 10
also plots the %Δ changes for these ﬁve metrics on dai-
ly basis. Note that before the aﬀected date, most of the
date points are within [−0.1, 0.1] range, meaning there is
no big diﬀerences between treated and controlled users. On
the other hand, after aﬀected, we observe a lot of escalated
points: treated users have as much as 40% more navigational

1218Metric

Before Aﬀect After Aﬀect

S/U
UQ/U

% Nav Q
Avg Q Len

CTR

3.0347
9.2907
0.4237
2.3270
0.8438

2.8238
8.4752
0.5247
2.2908
0.8356

%Δ(f )
7.47%*
9.62%**
-19.25%**

1.58%*
0.98%*

Table 2: Summary of changes for the 5 metrics dis-
cussed before and after aﬀected. Numbers here are
for the aﬀected users in the treatment group. *: p-
val <0.05. **: p-val <0.01. Values are linearly scaled
due to sensitivity.

 

D
%

5
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0

.

0

1

.

0
-

2

.

0
-

3

.

0
-

4

.

0
-

0

% D of All Metrics for Affected Users

S/U
UQ/U
% Nav Q
Avg Q Len
CTR

10
Days since experiment started

20

30

40

Figure 10: Daily changes for the 5 metrics before
and after the aﬀected date. Most metrics are not
signiﬁcant before aﬀected date but turned out to be
signiﬁcant afterwards.

queries, 20% and 30% less daily sessions (S/U) and unique
queries (UQ/U), respectively.

Examples of sessions are shown in Table 1 between treat-
ed and controlled users, where users started with the same
query and ended up clicking on the same result. With these
constraints, we assume that users have the same search in-
tent. However, in these examples, we see that users in treat-
ment spent signiﬁcantly more eﬀort in completing the same
tasks, by issuing and reformulating more queries and click-
ing on more results. Figure 9 illustrates a randomly-chosen
individual user whose engagement dropped during the ex-
periment, where the y-axis shows the number of daily ses-
sions. It is obvious that before aﬀected, the user tended to
issue more complicated informational queries. After aﬀect-
ed, we observe that most of the queries were navigation-only
queries. The S/U for that user dropped from 4.3 to 2.2, sig-
niﬁcantly.

4.3 Degree of Impact Analysis

In this section, we want to further conﬁrm our ﬁndings in
the previous section by dividing treated users into two buck-
ets: heavily-aﬀected users and lightly-aﬀected users. Our
assumption is that users who are exposed more often to bad
relevance results should demonstrate stronger signals to the
metrics we discussed, than those gently aﬀected.

Figure 11 shows the cumulative distribution function (CD-
F) for users by aﬀected queries. Since over half of the users

 

s
r
e
s
U
d
e
t
c
e
f
f

A

 

 
f
o
F
D
C

1

0.8

0.6

0.4

0.2

0.59

0
1

4

Number of Affected Queries

Number of Affected Queries

10

13

7

16

19

Figure 11: Cumulative Distribution Function of
users in terms of aﬀected queries.

Metric

S/U

*UQ/U

**% Nav Q
*Avg Q Len

*CTR

Lightly-Aﬀected Heavily-Aﬀected

2.8408
8.4871
0.4929
2.3007
0.8407

2.8077
8.4421
0.5713
2.2863
0.8258

Table 3: Breakdown of metrics in two buckets of
users based on the degree of aﬀect. Except for S/U,
all other metrics show statistical signiﬁcance.

were only aﬀected by two or less queries, we use two as our
cut-oﬀ to separate users into heavily-aﬀected and lightly-
aﬀected users, respectively.

The comparative results are shown in Table 3. In general,
our assumption is conﬁrmed by the data: heavily-aﬀected
users exhibited stronger signals than lightly-aﬀected users,
with ﬁve metrics all pointing to the correct direction. Ex-
cept for the S/U metric, the rest are all statistically signiﬁ-
cant. Speciﬁcally, heavily-aﬀected users issued substantially
more navigational queries (57.13%) than the other group
(49.29%). The CTR of heavily-aﬀected users also dropped
from 0.84 to 0.82. Also, heavily-aﬀected users are less likely
to issue new queries as indicated by UQ/U (8.44 vs. 8.48).

5. PREDICT USER ENGAGEMENT CHANGE

For search engines, keep the users engaged is the key to
their success. Now that we understand how users behave
under the circumstance of bad relevance, we want to further
leverage these signals to predict the change of user engage-
ment in the future. To start with, we formulate this prob-
lem as a binary classiﬁcation task using machine learning
technique. Our primary objective is to quickly detect user
session decrease in practical large-scale online experiments
so that search engines can take actions properly.
5.1 Data Preparation

Speciﬁcally, we focus on predicting user weekly number
of sessions, whether decrease (-) or increase (+). Due to
the fact of session diﬀerences during weekdays and week-
ends (i.e., users issue more queries during weekdays than
weekends), we decide to aggregate data to weekly level so
that this weekday/weekend impact is minimized. However,
we believe that our work can be easily adapted to predict
daily session changes with the addition of some daily-based
user features.

There are various ways to formulate the prediction prob-

1219query
doc bao

bao

daily express

doc bao express
free credit report

free annual transunion credit report

federal free annual credit report

grammy awards 2011

grammy awards 2011 live

Treatment

click

docbao.com.vn

Control

query
doc bao

click

vnexpress.net

www.express.co.uk

vnexpress.net

www.annualcreditreport.com

/cra/index.jsp

www.ftc.gov/bcp/edu/
microsites/freereports/

www.ftc.gov/bcp/edu/
microsites/freereports/

free credit report

grammy awards 2011

www.grammy.com/news/

grammy-performers

performances at grammys 2011

idolator.com/5766501/grammy-

grammys 2011 performers

awards-2011-performances

www.grammy.com/news/

grammy-performers

Table 1: Examples of sessions in comparison. Users in treatment spent more eﬀort for the same query intent
than controlled users. Bolded URLs are desired pages.

Figure 9: An example of user engagement drop before and after the user was aﬀected in terms of daily
session number. It is also clear that the user starts to issue more navigational queries after exposed to a bad
relevance engine.

lem. Ideally, we would like to be aware of the engagement
change as soon as possible, i.e., during the ﬁrst week of the
experiment. However, from a machine learning perspective
of view, this approach is infeasible due to the lack of train-
ing data. Therefore, to make reasonable predictions, we are
required to have at least one week training data to start
with so that the best we can do is to predict the second
week’s engagement given the ﬁrst week’s signals. In what
follows, we experiment with diﬀerent amounts of training
data: one with three week’s training data and one with one
week’s training data. Since users who have three week’s en-
gagement are usually heavy users while users with one week
data are often low-frequency users, these two experiments
essentially measure the eﬀect of degraded search relevance
to heavy and low-frequency users, respectively.
5.2 Prediction for High-Frequency Users

From the experimental data, we ﬁlter users who have at
least one activity during week 1, 2, 3 and 4 of the experiment.
Data is aggregated to user level as the prediction is per use
base. Prediction is made on week 4 by leveraging user data
from week 1-3. A positive label is assigned if week 4’s total

session number is more than that of week 3. Otherwise a
negative label is assigned. Overall, 77,940 positive instances
and 54,470 negative instances are collected.

5.2.1 Classiﬁers

We experimented with a variety of classiﬁcation algorithm-
s, including logistic regression, boosted decision trees and
linear Support Vector Machines (SVM) [4]. Among them,
linear SVM demonstrated the best performance in terms of
both classiﬁcation accuracy and scalability. Therefore, we
only report the results from linear SVM in our experiments.

5.2.2 Features

Table 4 summarizes the set of basic features we used,
which covers both aspects of engagement and relevance. Some
of them are closely related to the features used in the previ-
ous section. For example, numSessions is the same as S/U
on a weekly basis; numUnqQueries equals weekly UQ/U;
numNoClickQueries is correlated with CTR and etc.

We then derive a set of Δ-features based on the basic
features. To be concrete, for each basic features, we calculate
the Δ diﬀerences between each two weeks of week 2 to week

12204. For example, for numClicks, it has three Δ-features for
each pair of weeks: ΔW3W2numClicks, ΔW3W1numClicks
and ΔW2W1numClicks, where

ΔWiWjnumClicks =

WinumClicks − WjnumClicks

WinumClicks

.

(2)

We further transform some of the count features into percentile-

based features. Features like maxSessionLength and Query-
TimeInterval are un-bounded so that directly apply them
to classiﬁers may not be an optimal choice. Consequently,
these features are transformed into percentile and included
along with the original count features. As a result, a total
of 60 features are used for training the classiﬁer.

5.2.3 Results

Since we have over 50% more training instances in the
positive class than the negative class, we perform subsam-
pling from both classes with certain ratio to address the
class imbalance issue. To be concrete, we randomly sample
20,000 instances from each class, resulting a total of 40,000
training examples, and then randomly split them on a 50/50
ratio to ﬁt a linear SVM model. This process is repeated 10
times and the average result is reported here. To ﬁnd the
best parameters for the model, we perform a grid search.
Overall, the best performance is reached with s = 2, i.e.,
L2-regularized L2-loss support vector classiﬁcation (primal),
and c = 2 the cost factor.

We compare to several baselines:
Baseline (Session): baseline that leverages only weekly

number of sessions as the predictor.
through rate (CTR). CT R = 1 − numNoClickQueries

Baseline (CTR): baseline that uses only weekly click

.

numQueries

Baseline (Basic): baseline that uses the basic features

only, as listed in Table 4.

Figure 12 summarizes the performance of all algorithm-
It can be observed that with only the
s in comparison.
session feature, the result is almost equal to random guess
around 50%, which demonstrates a very poor correlation be-
tween users weekly sessions. With the introduction of click
features (CTR), the result is substantially improved. This
indicates a strong connection between search relevance and
user engagement. The performance is further improved by
leveraging all basic features, except for a few cases at very
low recalls. Finally, by combining basic features with Delta
features as well as percentile features, the algorithm achieves
the best predictive performance among all. Overall, linear
SVM achieves 72.17% of accuracy when using all 60 features.
Noticeably, at low levels of recall, by adding the Δ-based fea-
tures and percentile-based features, the model is capable of
improving the precision by 10% to 12% over basic features.
We list the top-15 highest weighted features in Table 5
from the linear SVM model. Overall, numClicks, aver-
age avgSessionLength, and numN oClickQueries, as well
as their Δ-based features, play the most important roles in
the model. We also notice that numSessions-related fea-
tures did not make into this list, which again demonstrates
a poor correlation between users weekly session numbers.
5.3 Prediction for Low-frequency Users

Given the fact that user’s last weekly features (W3 in our
case) are the most important features as shown in Table 5,
here we build another model by leveraging only one week fea-
tures. A legitimate reason for doing this study is due to the

Basic Features

numSessions: total number of sessions
numQueries: total number of queries

numUnqQueries: total number of unique queries

numNavQueries: total number of Navigational queries

numNoClickQueries: total number of queries without clicks

numClicks: total number of clicked URLs

avgQueryLength: average length of user queries

avgClickPosition: average SERP position of clicks

avgSessionLength: average length of sessions (# of queries)

maxSessionLength: maximum length of sessions

QueryTimeInterval: average time gap between two queries

ClickEntropy: the entropy of the user’s all clicks

Table 4: The basic features used in this paper. All
features are weekly-based. These features are used
to form a total of 60 features used by the classiﬁer.

i

i

n
o
s
c
e
r
P

11

0.9

0.8

0.7

0.6

0.5

0.4

 

 

Baseline (Session)
Baseline (CTR)
Baseline (Basic)
All Features

10

20

30

40

50

Recall

60

70

80

90

100

Figure 12: Precision-Recall curve for all algorithms.

fact that lots of search engine users are low-frequency user-
s, whose activities cannot be tracked consistently by search
engines for many reasons (e.g., a user who clears its browser
cookies will have a new user id). Therefore, we would like
to see how our algorithm performs when the data is sparse.
To this end, we extract a list of users from our data who
have at least one activity in each of two consecutive weeks.
We then randomly sample 20,000 users from the list. Fea-
tures are extracted from the ﬁrst week to make prediction
for the second week. The data is split into 50/50 for training
and testing. Note that since we only have one week data, we
are unable to construct Δ-based features. Therefore, only
basic features and percentile features are used for training.
Figure 13 demonstrates the result of precision-recall curves
for using one-week feature, as well as the model that lever-
ages all features for comparison. Comparatively, our algo-
rithm is able achieve 68.59% of accuracy for low-frequency
users even with one-week data, which is slightly worse (∼4%)
than the model using all features.

6. DISCUSSION AND CONCLUSION

From our longitudinal analysis, we have observed that the
change of user engagement follows a complex trend that is
aﬀected by multiple factors, therefore sometimes counter-
intuitive. One would assume that under a deliberate setback
of search relevance, user’s engagement should immediately
drop, which is, however, diﬀerent from what we have ob-
served. Our analysis indicated that user engagement in a
short-term actually increased signiﬁcantly, due to the fact
that users tend to spend more eﬀort by issuing more refor-

1221Feature Name
ΔW3W2numClicks
W3 avgSessionLength
ΔW3W1numQueries
W3 numClicks
ΔW3W2numN avQueries
ΔW2W1numQueries
W2 avgQueryLength
W2 numClicks
ΔW3W1numN oClickQueries
ΔW2W1avgSessionLength
W1 avgSessionLength
W1 numN oClickQueries
ΔW3W1avgSessionLength
W3 avgQueryLength
ΔW2W1numN oClickQueries

Weight
0.041983
0.034985
0.025762
0.020579
0.019042
0.019013
0.015908
0.015897
0.014183
0.014054
0.011007
0.010058
0.008692
0.008319
0.003609

Table 5: Top 15 highest-weighted features.

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

 

0.5
0

All Features
One−Week Feature

 

20

40

Recall

60

80

100

Figure 13: Precision-Recall curve for low-frequency
users.

mulation, to accomplish their search tasks. As a result, dur-
ing the early stage of our experiment, engagement metrics
are indeed negatively correlated with search relevance. Nev-
ertheless, we did observe that engagement ﬁnally dropped
after a certain period of time, when users eventually gave
up trusting the current search engine. Our further deep-
dive analysis demonstrated that after users lost their mo-
mentum, they tend to issue more navigational-type queries
(e.g., “facebook”, “amazon”), and have less queries in a ses-
sion, as well as a substantial drop of the click through rate.
By isolating aﬀected users from normal users, our time
series study focused on a set of core metrics that deﬁnes
user behavioral change over time. With the help of these
user-level and session-level features, we proposed a machine
learning framework that predicts user’s weekly engagement
change. The model achieved over 72% of accuracy for high-
frequency users and 66% for low-frequency ones. In practice,
predicting user engagement is indeed a diﬃcult task. And
therefore, it is reasonable for one to assume that the number
of sessions a user is going to issue in the future week is highly
correlated to her current week’s number, as well as the trend
in the past few weeks. However, our model revealed that the
highest correlated feature with engagement was in fact the
number of clicks, which turns out to be a relevance metric,
while the number of sessions did not make into our Top-15
feature list. This ﬁnding supports the correlation between
search relevance and user engagement in a positive way.

We believe ourselves to be among the ﬁrst to study the

relationship between engagement and relevance under the
setting of a deliberate relevance setback. Commercial Web
search engine companies like Google and Bing often per-
form numerous online A/B experiments before they ﬁnally
decide to ship new features to customers. In terms of search
relevance, some new algorithms may indeed improve user
satisfaction while others may eventually fail. The ship or
no-ship decision is often depend on how well those online
metrics perform like the ones we studied in the paper. Our
study revealed that even algorithms with bad relevance may
still be able to see a positive signal during the early stage of
the experiment. Therefore, our advice is not to celebrate too
early even if the signals look very positive in the ﬁrst few
days. We should consider keeping the experiments running
for a fairly reasonable amount of time (e.g., in our case at
least for two weeks), until the signals become stable or a
clear trend is observed.

Our study in this paper mainly targeted the relevance do-
main. It would be interesting to see if the same methodology
applies to other domains, e.g., User Interface (UI) changes.
In the future, we also plan to further improve the accuracy
of our prediction model by incorporating more features.

7. ACKNOWLEDGEMENTS

We would like to thank Pavel Dmitriev, Ya Xu, Brian
Frasca, Fritz Behr, Bing Data Mining Team and Bing Rele-
vance Team for their numerous support for this project.

8. REFERENCES
[1] E. W. Anderson and M. W. Sullivan. The antecedents and
consequences of customer satisfaction for ﬁrms. Marketing
Science, 12(2):125–143, 1993.

[2] A. Broder. A taxonomy of web search. SIGIR FORUM,

36(2):3–10, 2002.

[3] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating

simple user behavior for system eﬀectiveness evaluation. In
CIKM ’11, pages 611–620, 2011.

[4] R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and

C. J. Lin. Liblinear: A library for large linear classiﬁcation.
The Journal of Machine Learning Research,
9(6/1/2008):1871–1874, 2008.

[5] H. A. Feild, J. Allan, and R. Jones. Predicting searcher
frustration. In SIGIR ’10, pages 34–41, New York, NY,
USA, 2010. ACM.

[6] V. Hu, M. Stone, J. Pedersen, and R. W. White. Eﬀects of
search success on search engine re-use. In CIKM ’11, pages
1841–1846. ACM, 2011.

[7] S. Ieong, N. Mishra, E. Sadikov, and L. Zhang. Domain
bias in web search. In WSDM ’12, pages 413–422, New
York, NY, USA, 2012. ACM.

[8] B. J. Jansen, D. L. Booth, and A. Spink. Determining the

user intent of web search engine queries. In WWW ’07,
pages 1149–1150. ACM, 2007.

[9] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, Oct. 2002.

[10] R. Kohavi, R. M. Henne, and D. Sommerﬁeld. Practical

guide to controlled experiments on the web: listen to your
customers not to the hippo. In KDD ’07, pages 959–967.

[11] Z. Liao, Y. Song, L.-w. He, and Y. Huang. Evaluating the

eﬀectiveness of search task trails. In WWW ’12, pages
489–498, New York, NY, USA, 2012. ACM.

[12] C. D. Manning, P. Raghavan, and H. Schtze. Introduction

to Information Retrieval. Cambridge University Press, New
York, NY, USA, 2008.

[13] M. Meiss, J. Duncan, B. Gon¸calves, J. J. Ramasco, and

F. Menczer. What’s in a session: tracking individual

1222behavior on the web. In HT ’09, pages 173–182, 2009.

[18] J. E. R. Staddon. Adaptive Behavior and Learning.

[14] B. Mittal and W. M. Lassar. Why do customers switch?

the dynamics of satisfaction versus loyalty. Journal of
Services Marketing, 12:177–194, 1998.

[15] K. Radinsky, K. Svore, S. Dumais, J. Teevan, A. Bocharov,

and E. Horvitz. Modeling and predicting behavioral
dynamics on the web. In WWW ’12, pages 599–608, 2012.

[16] F. Radlinski, M. Kurup, and T. Joachims. How does

clickthrough data reﬂect retrieval quality? In CIKM ’08,
pages 43–52, New York, NY, USA, 2008. ACM.

[17] Y. Song and L. He. Optimal rare query suggestion with

implicit user feedback. In WWW ’10, pages 901–910, New
York, NY, USA, 2010. ACM.

Cambridge University Press, 1983.

[19] D. Tang, A. Agarwal, D. O’Brien, and M. Meyer.

Overlapping experiment infrastructure: More, better, faster
experimentation. In CIKM 2010, pages 17–26.

[20] K. Wang, T. Walker, and Z. Zheng. Pskip: estimating
relevance ranking quality from web search clickthrough
data. In KDD ’09, pages 1355–1364, 2009.

[21] R. W. White and S. M. Drucker. Investigating behavioral
variability in web search. In WWW ’07, pages 21–30, New
York, NY, USA, 2007. ACM.

1223