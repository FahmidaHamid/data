Mining Acronym Expansions and Their Meanings

Using Query Click Log

Bilyana Taneva1

∗

, Tao Cheng2, Kaushik Chakrabarti2, Yeye He2

1Max-Planck Institute for Informatics, Saarbrücken, Germany

2Microsoft Research, Redmond, WA

1btaneva@mpi-inf.mpg.de

2{taocheng, kaushik, yeyehe}@microsoft.com

ABSTRACT
Acronyms are abbreviations formed from the initial com-
ponents of words or phrases. Acronym usage is becoming
more common in web searches, email, text messages, tweets,
blogs and posts. Acronyms are typically ambiguous and
often disambiguated by context words. Given either just
an acronym as a query or an acronym with a few context
words, it is immensely useful for a search engine to know the
most likely intended meanings, ranked by their likelihood.
To support such online scenarios, we study the oﬄine min-
ing of acronyms and their meanings in this paper. For each
acronym, our goal is to discover all distinct meanings and for
each meaning, compute the expanded string, its popularity
score and a set of context words that indicate this meaning.
Existing approaches are inadequate for this purpose. Our
main insight is to leverage “co-clicks” in search engine query
click log to mine expansions of acronyms. There are several
technical challenges such as ensuring 1:1 mapping between
expansions and meanings, handling of “tail meanings” and
extracting context words. We present a novel, end-to-end
solution that addresses the above challenges. We further
describe how web search engines can leverage the mined in-
formation for prediction of intended meaning for queries con-
taining acronyms. Our experiments show that our approach
(i) discovers the meanings of acronyms with high precision
and recall, (ii) signiﬁcantly complements existing meanings
in Wikipedia and (iii) accurately predicts intended meaning
for online queries with over 90% precision.

Categories and Subject Descriptors
H.2.8 [DATABASE MANAGEMENT]: Database Ap-
plications—Data mining

Keywords
acronym; acronym expansion; acronym meaning; click log

1.

INTRODUCTION

Acronyms are abbreviations formed from the initial com-
ponents of words or phrases. These components may be indi-
vidual letters (e.g., “CMU”from “Carnegie Mellon University”)

∗Part of the work was done during employment at Microsoft
Research

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

or parts of words (e.g., “HTTP” from “Hypertext Transfer
Protocol”). Acronyms are used very commonly in web searches
as well as in all forms of electronic communication like email,
text messages, tweets, blogs and posts. With the emergence
of mobile devices, the usage of acronyms is becoming even
more common because typing is diﬃcult in such devices and
acronyms provide a succinct way to express information.

One key characteristic of acronyms is that they are typ-
ically ambiguious, i.e., the same acronym has many diﬀer-
ent meanings. For example, “CMU” can refer to “Central
Michigan University”, “Carnegie Mellon University”, “Central
Methodist University”, and many other meanings. Consider
a web search scenario: given an acronym as a query, it is
immensely useful for the search engine to know all its pop-
ular meanings, ranked by their popularity. For example, for
“CMU”, “Central Michigan University” is the most popular
meaning followed by “Carnegie Mellon University” and oth-
ers. The search engine can either modify the original query
with these expansions and retrieve more relevant results [7]
or simply show them to users so that they can disambiguate
it themselves1.

A second characteristic of acronyms is that they are typi-
cally disambiguated by context, i.e., the intended meaning is
clear when the user provides a few context words. For exam-
ple, a user searching for “cmu football” is most likely refer-
ring to “Central Michigan University” while the one search-
ing for “cmu computer science” is most likely referring to
“Carnegie Mellon University”. Given an acronym and one
or more context words, it is useful for the web search engine
to know the most likely intended meaning (or a few most
likely intended meanings, ranked by the likelihood). The
search engine can then use query alteration techniques to
retrieve more relevant results [7].

To enable the above online scenarios, we study the oﬄine
mining of acronyms and their meanings in this paper. For
each acronym, we discover its various meanings; for each
meaning, we output:
• Expansion: The complete expanded string of the acronym
for the meaning.
• Popularity score: A score reﬂecting how often people in-
tend this meaning when they use the acronym (e.g., how of-
ten web searchers intend it when they use only the acronym
as the query).

1Google has recently started showing the diﬀerent meanings of
a query on the right hand side of search result page for limited
queries. The algorithm used by Google has not been published
and is hence not publicly known.

1261cmu 

central michigan university 

cmu football 

central mich univ 

carnegie mellon university 

… 

cs carnegie mellon 

Figure 1: Example illustrating insights.

• Context words: A set of words when used in context of
the acronym indicates this meaning. Each word has a score
reﬂecting how strongly it indicates this meaning.
For example, for “CMU”, we aim to discover the various
meanings “Central Michigan University”, “Carnegie Mellon
University”, “Central Methodist University” and so on. The
popularity scores should reﬂect that “Central Michigan Uni-
versity” is more popular compared with the other meanings.
Finally, we aim to ﬁnd context words like “pittsburgh”, “com-
puter science”, “research”, “computing”, etc. for the meaning
“Carnegie Mellon University”. The 1:1 mapping between the
output and the meanings is critical to enable the above on-
line scenarios.

There are several eﬀorts in mining expansions of acronyms.
We brieﬂy discuss them here; a more detailed discussion can
be found in Section 6.
• Wikipedia: Wikipedia covers acronyms through its man-
ually edited “disambiguation pages”. It has low recall with
many meanings not covered. We ﬁnd from our experiments
that roughly two thirds of the meanings of acronyms are
not covered in Wikipedia. Furthermore, it does not provide
popularity scores.
• Acronymﬁnder.com: Websites such as acronymfinder.com
list the possible acronym expansions; this is also manually
edited. As in Wikipedia, it does not provide popularity
scores. Furthermore, it does not provide any context words
for most of the expansions.
• Automatic Mining: There has been recent work towards
automatic mining of acronym expansions using the Web [6].
The main focus of this work is in ﬁnding legitimate expan-
sions of a given acronym. However, there is no 1:1 mapping
between the outputted expansions and meanings, no popu-
larity scores and no context words.

Due to the above limitations, it is diﬃcult for web search
engines to leverage the above approaches to support the on-
line scenarios discussed above.

Our main insight is that acronyms and their various ex-
pansions are captured in a search engine query click log.
While some people use acronyms as queries and click on rel-
evant documents, others use their expanded forms as queries
and click on the same documents. Thus, we can ﬁnd ex-
pansions by observing queries that “co-click” on the same
documents as the acronym. As shown in Figure 1, by ob-
serving other queries co-clicked with query “cmu”, we can
ﬁnd acronym expansions such as “central michigan univer-
sity”, “central mich univ” and “carnegie mellon university”.
There are several technical challenges in ﬁnding the dis-
tinct meanings from the co-clicked queries. First, not all
co-clicked queries are expansions (e.g., “cmu football” is not

an expansion of “cmu” in Figure 1). How do we identify
the ones that are expansions? Second, the co-clicked queries
that are expansions do not correspond to the distinct mean-
ings. There are several variants that correspond to the
same meaning (e.g., “central michigan university” and “cen-
tral mich univ” in Figure 1). How do we group them such
that there is a 1:1 mapping between groups and meanings?
Third, how do we identify context words for each mean-
ing? Fourth, co-clicked queries tend to cover the popu-
lar meanings of the acronym (e.g., “Massachusetts Insti-
tute of Technology” for “MIT”) but not the “tail meanings”
(e.g., “Mazandaran Institute of Technology”, “Maharashtra
Institute of Technology”, “Mahakal Institute of Technology”,
etc.). This is because the ﬁrst few pages of results returned
by a search engine for the query “MIT” do not represent the
tail meanings. How do we ﬁnd such tail meanings?

Our main contributions can be summarized as follows:

• We formulate the oﬄine acronym mining problem. The
novelty of our problem formulation is to ﬁnd the distinct
meanings, not just the expansions. This is critical to enable
the above online scenarios (Section 2).
• We present a novel, end-to-end solution that leverages the
query click log to identify expansions, group them into dis-
tinct meanings, compute popularity scores and discover con-
text words (Section 3). We leverage two key insights. First,
expansions of the same meaning click on the same set of
documents, whereas expansions of diﬀerent meanings click
on diﬀerent documents. We design similarity functions to
leverage this insight and perform clustering to group the ex-
pansions. Second, co-clicked queries shed light on the con-
text words of respective meanings. For instance, the fact
that “cmu football” and “central michigan university” click
on the same document hints the relevance of “football” to
“central michigan university”. We leverage this insight to
discover context words.
• We present a novel enhancement to discover tail meanings
in addition to the more popular meanings (Section 3).
• We describe how web search engines can leverage the
mined information for prediction of intended meaning for
queries containing acronyms (Section 4).
• We perform extensive experiments using a large-scale query
click log. Our experiments show that our approach (i) dis-
covers acronym meanings with high precision and recall, (ii)
signiﬁcantly complements existing meanings in Wikipedia
and (iii) accurately predicts intended meaning for online
queries with over 90% precision (Section 5).

To the best of our knowledge, this is the ﬁrst work on

automatic mining of distinct meanings of acronyms.

2. PROBLEM DEFINITION

In this section, we formally deﬁne the oﬄine acronym
meaning discovery problem and then present our solution
overview.
2.1 Problem Deﬁnition

We study the following oﬄine acronym meaning discovery

problem.

Deﬁnition 1. (Acronym Meaning Discovery Prob-
lem) Given an input acronym, ﬁnd the set {M1, . . . , Mn}
of distinct meanings associated to it. For each meaning
Mi = (e, p, C), ﬁnd the canonical expansion Mi.e, the pop-
ularity score Mi.p and the set Mi.C of context words along
with scores.

1262For any meaning, there can be many variants of the ex-
panded string in the query log. For example, for the mean-
ing “Carnegie Mellon University” of the acronym “CMU”,
the variants include “Carnegie Mellon University”, “Carnegie
Mellon Univ” as well as several misspellings. Mi.e is the
most representative variant; we refer to it as the canon-
ical expansion. The popularity score Mi.p measures how
often web searchers intend this meaning when they use the
acronym in a query. To easily leverage these scores for online
meaning prediction, we compute these scores as probabili-
ties. Finally, the set Mi.C of the context words are the
words which when used in context of the acronym in web
searches indicate this meaning. For example, for the mean-
ing “Carnegie Mellon University”, Mi.C = {“pittsburgh”,
“research”, “cs”, “science”, etc.}. We associate a score with
each context word in Mi.C which measures how strongly
the word indicates this meaning. Again, to easily leverage
these scores for online meaning prediction, we compute these
scores as probabilities.

Notice that our problem formulation assumes that the
acronym is given. We assume a separate module that iden-
tiﬁes the acronyms commonly used in web searches; this can
be used as input to the acronym meaning discovery problem.
For example, this module can extract all acronyms listed
in Wikipedia. Another approach is to treat all words that
are not common English words as acronyms. Our frame-
work will be able to ﬁnd out the meanings of true acronyms,
whereas words which are not actual acronyms will not likely
produce any meanings.

2.2 Solution Overview

To discover the diﬀerent meanings of an acronym, we
leverage the query click log of a web search engine. Our so-
lution is based on the following insight: while some searchers
use acronyms as queries and click on the relevant documents,
others use their expanded forms as queries and click on the
same documents. We compute the canonical expansions,
popularity scores as well as context words for the diﬀerent
meanings of an acronym by observing the set of queries that
click on the same documents as the acronym query. We refer
to them as “co-clicked” queries.

Query Click Log: The query click log collects the click
behavior of millions of web searchers over a long period of
time (say, two years). We assume the query log Q to contain
records of the form (q, d, f ) where q is a query string, d is a
web document, represented by its unique URI, and f is the
number of times d has been clicked by web searchers after
posing the query q to the search engine.

It is technically challenging to ﬁnd the distinct meanings
from the co-clicked queries. To address this challenge, we
develop a novel, end-to-end solution that consists of the fol-
lowing steps:
• Candidate Expansion Identiﬁcation: We ﬁrst collect
the co-clicked queries for the given acronym. Not all co-
clicked queries are valid expansions of the acronym. We
identify the valid expansions from the co-clicked queries; we
refer to them as candidate expansions. For this purpose, we
use an acronym-expansion checking function, which checks if
a query can be considered as the complete expanded string
of the acronym.
• Acronym Expansion Clustering: The candidate ex-
pansions do not correspond to the distinct meanings; there
are several variants that correspond to the same meaning.

We group the candidate expansions such that each group has
unique meaning, and no two groups have the same meaning.
• Enhancement for Tail Meanings: We observe that co-
clicked queries do not cover the tail meanings. To address
this problem, we present a novel extension that considers su-
persequence queries. This approach ﬁnds signiﬁcantly more
meanings, especially the tail ones. We refer to this algorithm
as Enhanced Acronym Expansion Clustering.
• Canonical Expansion, Popularity Score and Con-
text Words Computation: We select the canonical ex-
pansion for each discovered meaning. We compute the popu-
larity score for each meaning, such that more popular mean-
ings receive higher scores. Finally, we assign a set of context
words to each meaning. We also assign a score to each con-
text word.

We describe the above steps in details in Section 3.

In
Section 4, we describe how we can leverage the discovered
meanings to predict the intended meanings of online queries.

3. ACRONYM MEANING DISCOVERY

We explain the four steps in details. The output of each
step is the input to the subsequent step. We present the
input and output of each step followed by the algorithm.
3.1 Candidate Expansion Identiﬁcation
Input: The acronym a and the query click log Q.
Output: The set E(a) of candidate expansions of a.

Our main insight is that the expansions corresponding
to the diﬀerent meanings of a are included in the set of
co-clicked queries for a. We ﬁrst compute the co-clicked
queries for a. Let D(q) denote the set of documents which
users clicked when they searched with the query string q as
recorded in the query click log. Furthermore, let Q(d) denote
the set of queries for which users clicked on web document
d as recorded in the query click log. We ﬁrst compute from
the query click log the set of documents D(a) clicked for
acronym a. Then, for each document d ∈ D(a), we collect
the set of queries Q(d) for which d was clicked. We thus
obtain the set ∪d∈D(a)Q(d) of co-clicked queries for a.

Not all co-clicked queries are valid expansions for a. To
identify the valid expansions in ∪d∈D(a)Q(d), we propose an
acronym-expansion checking function.
Acronym-Expansion Checking Function: We present
a function that checks whether a given string can be consid-
ered as the expanded string of a given acronym.

Deﬁnition 2. (Acronym-Expansion Checking Func-
tion) Given a string q and an acronym a, the checking func-
tion IsExp : q × a → {true, f alse} returns true if q is a
valid expansion of a and f alse otherwise.

For example, IsExp(“carnegie mellon university”, “cmu”)
should be true while IsExp(“cmu football”, “cmu”) should
be false.

It is diﬃcult to develop a set of exact rules for match-
ing acronym letters in a query. For example, a common
rule is that acronym letters should be the initial letters
of the words in the expansion. However, this rule does
not always hold: “Hypertext Transfer Protocol” is expan-
sion for “HTTP”. On the other hand, stop words are of-
ten skipped when constructing acronyms (e.g. “Master of
Business Administration” is expansion for “MBA”). How-
ever, this does not always hold (e.g., “League of Legends” is
expansion of “LOL”).

1263We approach the problem by using heuristics based on
dynamic programming. We assign weights to the words and
letters of the query string, and modify the longest common
subsequence algorithm to ﬁnd the subsequence with high-
est score [17]. Pseudo-code is shown in Algorithm 1 in the
Appendix, along with explanations of the checking function.
Expansion Identiﬁcation from Co-clicked Queries: A
co-clicked query q ∈ ∪d∈D(a)Q(d) is a valid expansion of
acronym a, iﬀ IsExp(q, a) = true; we refer to it as a can-
didate expansion of a. We compute the set E(a) = {q|q ∈
∪d∈D(a)Q(d), IsExp(q, a) = true} of candidate expansions
of a by checking each co-clicked query using the acronym-
expansion checking function.
3.2 Acronym Expansion Clustering
Input: The set E(a) of candidate expansions of a and the
query click log Q.
Output: Grouping G(a) = {G1, . . . , Gn} of candidate expan-
sions E(a).

The set E(a) of candidate expansions output by the pre-
vious step does not correspond to the distinct meanings.
It contains several variants that correspond to the same
meaning. For example, for the meaning “Carnegie Mellon
University”, the variants include “Carnegie Mellon Univer-
sity”, “Carnegie Mellon Univ” as well as misspellings like
“Carnegie Melon University”. They all pass the acronym-
expansion checking function. Given the set E(a) of can-
didate expansions of a, this step clusters them into a set
G(a) = {G1, . . . , Gn} of groups such that each group has a
unique meaning, and no two groups have the same mean-
ing. These groups correspond the desired set {M1, . . . , Mn}
of distinct meanings. We ﬁrst discuss the distance metrics
between the candidate expansions and then the clustering
algorithm.

3.2.1 Distance Metric for Candidate Expansions

Candidate expansions that correspond to the same mean-
ing are typically minor spelling variations of each other (e.g.,
“Carnegie Mellon University” and “Carnegie Melon Univer-
sity”) while those that correspond to diﬀerent meanings are
often far in terms of string distance (e.g., “Carnegie Mellon
University” and “Central Michigan University”). One obvi-
ous approach is to cluster the candidate expansions based
on their string distance, say edit distance. However, there
are many cases where expansions corresponding to the same
meaning have large string distances. For example, expan-
sions like “Mass Inst Tech” and “Massachusetts Institute of
Technology” correspond to the same meaning, but their edit
distance is high enough to prevent them from being grouped
together. On the other hand, expansions like “Manukau In-
stitute of Technology” and “Manipal Institute of Technol-
ogy” refer to two diﬀerent meanings but may incorrectly be
grouped together due to their low edit distance.

Our key insight is that each document clicked by any
of the expansions in E(a) typically corresponds to a single
meaning; hence, the expansions that correspond to the same
meaning will click on the same set of documents, whereas
expansions corresponding to diﬀerent meanings will click on
diﬀerent sets of documents. We design distance metrics to
leverage this insight and perform clustering to group the
expansions.
Set Distance (Jaccard Distance): One way to measure
the distance between two expansions ei and ej in E(a) is

by the distance between the corresponding sets D(ei) and
D(ej) of clicked documents. A common way to measure set
distance is Jaccard distance: dist(ei, ej) = 1 − |D(ei)∩D(ej )|
|D(ei)∪D(ej )| .
However, Jaccard distance has a serious limitation. Click
logs are known to be noisy and contain many clicks that
users performed by mistake (referred to as “mis-clicks”). For
example, documents associated with “Massachusetts Insti-
tute of Technology” get signiﬁcant number of mis-clicks for
the query “Michigan Institute of Technology”. Jaccard dis-
tance is not robust to such mis-clicks.
Distributional Distance (Jensen-Shannon Divergence)
Our main insight is to leverage the frequency of clicks. The
frequency of mis-clicks is typically much lower compared
with frequency of clicks on documents that are consistent
with the meaning of the expansion. We consider the distri-
bution of documents clicked for a given query instead of the
set of documents. We use a distributional distance metric,
square root of Jensen-Shannon divergence, to evaluate dis-
tance between expansions. This metric is much more robust
to mis-clicks.

Denote by F (q, d) the frequency with which d is clicked by
q. The click distribution Ω(q) of a query q over all possible
documents is P r(Ω(q) = d) =

F (q,d)

Pd∈D(q) F (q,d) .

Given click distributions deﬁned by click frequencies, the
Jensen-Shanon divergence between two expansions ei and ej
in E(a) is:
JSD(Ω(ei)||Ω(ej)) = 1
where Ω(e) = 1
Leibler divergence between two distributions:

2 KL(Ω(ej)||Ω(e)),
2 (Ω(ei)+Ω(ej)) and KL(X||Y ) is the Kullback-

2 KL(Ω(ei)||Ω(e))+ 1

KL(X||Y ) =Xi

P r(X(i)) log

P r(X(i))
P r(Y (i))

.

Then, dist(ei, ej) =pJSD(Ω(ei)||Ω(ej)).

3.2.2 Clustering of Candidate Expansions

We cluster the candidate expansions in E(a) based on the
above distance metric. We use the bottom-up, average-link
hierarchical clustering [14, 5].
3.3 Enhancement for Tail Meanings

While the set of co-clicked queries for the acronym a cov-
ers the popular meanings of a, it does not cover many of the
less popular meanings (referred to as “tail meanings”). Con-
sider the acronym “MIT”. “Massachusetts Institute of Tech-
nology” is the dominating meaning for “MIT”; the ﬁrst few
pages of results returned by the search engine for the query
“MIT” are dominated by that meaning. Tail meanings for
that acronym (e.g., “Maharashtra Institute of Technology”,
“Mahakal Institute of Technology”, “Mazandaran Institute
of Technology” and so on) are not represented in the top
results. As a result, the co-clicked queries for “MIT” will
not cover these tail meanings. As shown in Figure 2, the co-
clicked queries for “MIT” (i.e., “massachusetts institute of
technology”, “mit boston” and “mass institute of tech”) all
correspond to the dominating meaning. Hence, the above
approach misses the tail meanings.

We leverage the following insight to address this problem.
Since users searching for tail meanings do not ﬁnd the de-
sired documents when they use only the acronym as a query,
they use additional words to disambiguate the query. For
example, a user searching for the meaning “Maharashtra In-
stitute of Technology” (which is located in Pune, India) will
issue the query “mit pune” while the one searching for the

1264mit 

mit pune 

mit ujjain 

massachusetts institute of technology 

mit boston  

mass institute of tech 

maharashtra institute of technology 
pune 

mahakal institute of technology ujjain 

… 

mahakal institute of technology 

Figure 2: Example illustrating enhancement for tail
meanings.

meaning “Mahakal Institute of Technology” (which is located
in Ujjain, India) will issue the query “mit ujjain”. Instead
of collecting co-clicked queries for the acronym a, we collect
co-clicked queries for acronym supersequence queries (ASQ).
Deﬁnition 3. (Acronym Supersequence Query) An

acronym supersequence query, denoted as a+s, for an acronym
a is a query in the query click log that contains the string a
and an additional sequence of words s either as preﬁx or as
suﬃx of a.

For example, “mit pune”, “mit ujjain” and “mit admission”
are ASQs for “mit”. Co-clicked queries of ASQs of a contain
many more tail meanings of a. As shown in Figure 2, co-
clicked of the above ASQs of “mit” cover the tail meanings
like “Maharashtra Institute of Technology” and “Mahakal
Institute of Technology”. We enhance the candidate expan-
sion identiﬁcation and expansion clustering steps based on
the above insight.
Candidate Expansion Identiﬁcation:
Input: The acronym a and the query click log Q.
Output: The set E(a) of candidate expansions of a.

The goal of this step is to identify the candidate expan-
sions among the co-clicked queries of ASQs of a. However,
there is a challenge: unlike in the case of co-clicked queries
of a, the candidate expansions may not themselves appear in
co-clicked queries of ASQs of a. For example, the co-clicked
queries for ASQ “mit pune” does not contain “maharashtra
institute of technology”. But it contains “maharashtra in-
stitute of technology pune”. This is because people tend to
use acronyms and their expansions interchangeably; so, ASQ
a + s may not have co-clicks with e where e is an expansion
of a but will have co-clicks with e + s. We refer to them as
“expansion supersequence queries”.

We identify the candidate expansions of a as follows:

1) We ﬁrst obtain the set ASQ(a) of all ASQs of a. We
compute this by scanning the query click log and identifying
queries which contain a and a preﬁx or suﬃx string. We
consider preﬁx and suﬃx strings containing zero, one and
two words.
2) For each ASQ a + s ∈ ASQ(a), we collect the set of co-
clicked queries ∪d∈D(a+s)Q(d). e is a candidate expansion
for a based on ASQ a + s iﬀ (i) the expansion supersequence
query has co-clicks with a+s, i.e., e+s ∈ ∪d∈D(a+s)Q(d) and
(ii) the acronym-expansion checking function returns true,
i.e., IsExp(e, a) = true. We formally deﬁne the candidate
expansion set Es(a) of a based on ASQ a + s:

Es(a) = {e|e + s ∈ ∪d∈D(a+s)Q(d) ∧ IsExp(e, a) = true}

3) We obtain the candidate expansion set E(a) of a by union-

ing the candidate expansion sets based on the ASQs. We
formally deﬁne the candidate expansion set E(a) of a by

E(a) =S∀s,a+s∈ASQ(a) Es(a).

Note that the preﬁx/suﬃx can be empty. So, ASQ(a) in-
cludes a and hence the above candidate expansion set sub-
sumes the previously deﬁned candidate expansion set. The
new candidate expansion set contains strictly more expan-
sions and hence improves coverage.
Acronym Expansion Clustering:
Input: The set E(a) of candidate expansions of a and the
query click log Q.
Output: Grouping G(a) = {G1, . . . , Gn} of candidate expan-
sions E(a).

The goal is to group the set E(a) of candidate expan-
sions into groups such that each group corresponds to a dis-
tinct meaning. The key insight for expansions also holds
for expansion supersequence queries: the expansion super-
sequence queries that correspond to the same meaning will
click on the same set of documents, whereas expansion su-
persequence queries corresponding to diﬀerent meanings will
click on diﬀerent sets of documents. For example, “mas-
sachusetts institute of technology admissions” and “mass inst
of tech admissions” will share clicks but “massachusetts in-
stitute of technology admissions” and “maharashtra institute
of technology admissions” will not. Hence, we can leverage
the same general clustering approach based on distributional
distance to perform the grouping. However, the same ex-
pansions can have multiple corresponding expansion super-
sequence queries; we need to compute the distance between
two expansions by aggregating the distances between the
corresponding expansion supersequence queries. There are
multiple ways to perform this aggregation; we present two
such options:
Distance Aggregation: One option is to compute the dis-
tance for each distinct expansion supersequence query (cor-
responding to a distinct preﬁx/suﬃx string) and then ag-
gregate the distances. Let ASQ(a, ei, ej) = {a + s|a + s ∈
ASQ(a), ei + s ∈ Q, ej + s ∈ Q} be the subset of ASQ
queries for which both ei + s and ej + s are valid queries in
the query log Q. For each a + s ∈ ASQ(a, ei, ej), denote by
dists(ei, ej) the distance between two candidate expansions
ei and ej measured over the same supersequence a + s, using
the distributional distance between queries ei + s and ej + s.
This can be deﬁned as dists(ei, ej) = dist(ei + s, ej + s).
We then aggregate dists(ei, ej) over all possible a + s ∈
ASQ(a, ei, ej) to obtain the overall distance dist(ei, ej) be-
tween candidate expansions ei and ej. That is dist(ei, ej) =

1

|ASQ(a,ei,ej )| Pa+s∈ASQ(a,ei,ej ) dists(ei, ej).

Click Frequency Aggregation: Another option is to ag-
gregate clicks instead of distance scores. For each pair of
candidate expansions, ei and ej, we compute the click dis-
tribution of expansion ei by aggregating over all possible
expansion supersequence queries in ASQ(a, ei, ej). The ag-
gregated click distribution, denoted by Ωij(ei), is:
Pa+s∈ASQ(a,ei ,ej ) F (ei+s,d)

P r(Ωij(ei) = d) =

Pa+s∈ASQ(a,ei ,ej ) Pd∈D(ei+s) F (ei+s,d) .

We then compute a distributional distance between ei and
ej based on the aggregated click distribution: dist(ei, ej) =

pJSD(Ωij(ei)||Ωij(ej)).

As we will show later in our experiments, we did not ob-
serve noticeable diﬀerence between the two aggregation ap-
proaches.

1265We refer to this approach as Enhanced Acronym Expan-
sion Clustering; we refer to the approach discussed in Sec-
tion 3.2 as Acronym Expansion Clustering.

3.4 Canonical Expansion, Popularity, Context
Input: Grouping G(a) = {G1, . . . , Gn} of candidate expan-
sions E(a) and the query click log Q.
Output: Meanings {M1, . . . , Mn} with Mi.e, Mi.p and Mi.C
for each meaning Mi.

The clustering step outputs a set of groups of expansions
G(a) = {G1, . . . , Gn}. These groups correspond to the de-
sired set {M1, . . . , Mn} of distinct meanings for the acronym
a. In this step we compute, for each meaning Mi, canonical
expansion Mi.e, popularity Mi.p, and context words Mi.C.
Canonical Expansion: We posit that the canonical ex-
pansion of Gi is the most “popular” expansion, because intu-
itively the canonical acronym expansion should occur more
frequently than non-canonical expansions, or expansions with
spelling mistakes.

In our click log data, popularity is measured by the num-
ber of clicks. If a document d is clicked by acronym a for
a total of F (a, d) times, we want to ﬁnd out how many of
those clicks are intended for each expansion ek ∈ Gi.

Since there is no way for us to know users’ real intent,
we approximately distribute clicks F (a, d) to each expan-
sion ek ∈ Gi ∈ G proportionally by the number of clicks
between ek and d, namely, F (ek, d). The intuition is that
if the document d is clicked by a particular expansion ek a
lot, then a signiﬁcant portion of the clicks F (a, d) should be
credited to ek.

Given a click between a and d ∈ D(a), the probability
that the click is intended for ek, denoted as P r(ek, d), is
computed by the total number of clicks between ek and d,
F (ek, d), divided by the total number of clicks between d and
all possible expansions in G. The probability that a click on
document d ∈ D(a) belongs to expansion ek is thus:

P r(ek, d) =

F (ek, d)

PGl∈GPej ∈Gl

F (ej, d)

If we only look at acronym a itself (without supersequence
tokens), then the probability of an expansion ek can be com-
puted by aggregating over all possible acronym-document
clicks F (a, d):

(1)

ek.p = Pd∈D(a) F (a, d)P r(ek, d)

Pd∈D(a) F (a, d)

However, the probability of an expansion should also in-
clude cases where the acronym is mentioned in conjunction
with supersequence tokens a + s, where the meaning of a
is intended for that expansion. Conceptually, the meaning
probability of a should be counted regardless of whether a is
mentioned alone, or with some other tokens. (If we do not
account for supersequence queries, on the other hand, then
for certain tail expansions discovered via ASQ that have no
co-clicks with a, these expansions would get zero-probability,
which is intuitively incorrect).

We deﬁne P rs(ek, d) for each a + s ∈ ASQ(a) by:

P rs(ek, d) =

Then the probability of an expansion ek, denoted as ek.p,
can be computed by aggregating clicks credited to ek, di-

F (ek + s, d)

F (ej + s, d)

PGl∈GPej ∈Gl

vided by the total number of query clicks containing a:

ek.p = Pa+s∈ASQ(a)Pd∈D(a+s) F (a + s, d)P rs(ek, d)

Pa+s∈ASQ(a)Pd∈D(a+s) F (a + s, d)

As in our previous notations, a ∈ ASQ(a) because s can
be empty. Notice, Equation (2) is a generalization of Equa-
tion (1). If supersequence queries are not considered, then
it essentially becomes Equation (1).

(2)

With that, the canonical expansion of Gi is simply the

expansion with the highest probability:

Mi.e = argmax

ek.p

ek∈Gi

where ek.p is computed in Equation (2).
Meaning Group Popularity: The second output is the
probability of each meaning group Mi.p. Since we have al-
ready computed ek.p in Equation (2), we can simply aggre-
gate for all ek ∈ Gi to obtain Mi.p:

Mi.p = Xek∈Gi

ek.p

Context Words: Let D(Gi) be the set of documents clicked
by expansions in group Gi for meaning Mi. We assign con-
text words to each meaning Mi:
Mi.C = {w | w is a word in q, q ∈ ∪d∈D(Gi)Q(d)}. We as-
sign to each word in Mi.C a probability score, which mea-
sures how strongly the word indicates the meaning. Let
F (w, Gi) be the frequency of a word w in group Gi, given by

F (w, Gi) = Pw∈q,q∈∪d∈D(Gi )Q(d),d∈D(Gi) F (q, d). We com-

pute the probability that a word w is indicative for Mi by:

P r(w|Mi) =

F (w, Gi)

.

Pw′∈Mi.C F (w′, Gi)

4. ONLINE MEANING PREDICTION

Acronym queries are very common in Web search. Of-
ten users provide some context, in addition to the acronym,
which can be one or more other words. In such cases, the
user experience can be greatly enhanced if the correct mean-
ing of the acronym can be predicted by the search engine.
Then the search results for the query will be more relevant
and focused.

We propose a solution to such prediction task: given an
acronym and a context, predict the correct meaning of the
acronym. We assume that we are given a set of meanings
{M1, M2, . . . , Mn} for an acronym, found using our oﬄine
approach from Section 3. Each meaning Mi is also associ-
ated with a set of context words as described in Section 3.4.
To predict the correct meaning of an acronym, given context
words, we leverage (1) the popularity of each meaning, and
(2) the relatedness between each meaning and the context
words.
In case there are no context words given, to pre-
dict the meaning of an acronym, we use only the popularity
scores of its meanings, Mi.p, as computed in Section 3.4.

For each meaning Mi and context word w, we compute the
probability P r(Mi|w). Applying Bayes’ theorem we obtain:

P r(Mi|w) =

P r(w|Mi)P r(Mi)

P r(w)

Here, P r(w|Mi) is computed as in Section 3.4, and P r(Mi)
is given by Mi.p. P r(w) can be any dictionary-based prob-
ability of the word w. Note that since P r(w) is the same for
all meanings Mi, it is suﬃcient to consider only the numer-
ator in the above formula.

1266We compute P r(Mi|w) for each meaning of the acronym.
To predict which is the correct meaning, we consider the
meaning with the highest probability score.

This prediction task can be further generalized in case we
have more than one context word in addition to the acronym:

P r(Mi|w1, . . . , wk) =

P r(w1, . . . , wk|Mi)P r(Mi)

P r(w1, . . . , wk)

where P r(w1, . . . , wk|Mi) = Qj P r(wj|Mi) by considering

that all words are independent and identically distributed.
P r(w1, . . . , wk) is computed analogously.

5. EXPERIMENTS

We present an experimental evaluation of the solution pro-

posed in the paper. The goals of the study are:
• To study the eﬀectiveness of our clustering algorithm and
our enhanced clustering algorithm in discovering expansions
and grouping expansions into meanings;
• To compare the above algorithms with clusterings based
on edit distance and Jaccard distance in terms of cluster
quality;
• To compare the meanings available in Wikipedia with
those discovered by our clustering algorithm;
• To study the eﬀectiveness of online meaning prediction
algorithm for acronym+context queries.
5.1 Experimental Setup

5.1.1 Data

To evaluate the proposed approach of clustering acronym
expansions, we randomly sampled 100 pages from Wikipedia
disambiguation pages. We ﬁltered out pages which do not
represent acronyms (e.g., the disambiguation page about
“Jim Gray”), and pages of unambiguous acronyms with a
single meaning. This resulted in a set of 64 acronyms, on
which we perform our experiments. To collect candidate ex-
pansions, we use the query log from Bing from 2010 and
20112.
5.1.2 Compared Methods

We compare the following methods, all based on stan-
dard bottom-up hierarchical clustering with average link and
threshold 0.8:
• Edit Distance based Clustering (EDC): Clustering,
which uses edit distance between candidate expansions.
• Jaccard Distance based Clustering (JDC): Cluster-
ing, which uses Jaccard distance between expansions.
• Acronym Expansion Clustering (AEC): Our approach
from Section 3.2, which uses only acronym queries to collect
candidate expansions (no supersequence queries). Square
root of Jensen-Shannon divergence is used for distance be-
tween expansions.
• Enhanced Acronym Expansion Clustering (EAEC):
Our enhanced approach from Section 3.3, which uses su-
persequence queries to collect candidate expansions, square
root of Jensen-Shannon divergence as distance, and click
frequency based aggregation.

5.1.3 Ground Truth Meanings of Acronyms

We use two sets of ground truth meanings for acronyms:
• Wikipedia Meanings: Meanings listed in the Wikipedia
disambiguation pages of the acronyms.

2Note that due to proprietary and privacy concerns we can-
not share more details about the query log.

By analyzing the results from our clustering approach on
a set of acronyms, we noticed that the meanings discovered
from the query log, and the meanings listed in Wikipedia
for the same acronyms, are very diﬀerent. That is why, in
addition to the Wikipedia meanings, we compile a second
set of ground truth meanings:
• Golden Standard Meanings: We consider for each
acronym all queries from the click log, which (1) are legit-
imate w.r.t the acronym-expansion check (see Section 3.1),
and (2) have co-clicks with acronym or acronym superse-
quence queries. Then, we manually label the diﬀerent mean-
ings/expansions of the acronym.
In the Golden Standard
we have one or more diﬀerent expansions for each distinct
meaning. For example, we can have two expansions: “cen-
tral michigan university” and “central mich univ” referring
to the same meaning.

Note that some acronym expansions are not meaning-
ful, even though they are legitimate with respect to our
acronym-expansion check. One such example is the ex-
pansion “computer processor upgrade” for “CPU”, since we
speculate people never mean “computer processor upgrade”
when they mention “CPU”. In our Golden Standard set we
do not consider such expansions. Since not all legitimate
expansions are included in the ground truth, not all groups
from our clustering approaches have speciﬁc ground truth
meaning. We measured the number of the groups which
have ground truth meanings, divided by the total number of
groups in the clustering, and on average, 82% of the groups
have ground truth meanings.

5.1.4 Evaluation Measures

We use the following measures: Purity, Normalized Mu-

tual Information (NMI), and Recall.

Our algorithms output a set of groups G(a) = {G1, . . . , Gn}
which maps to golden standard meanings M = {M1, . . . , Mk}
for a given acronym a. We map each group of expansions
Gi to one or more meanings from M using the top-5 expan-
sions from Gi, ranked by their probabilities. A group can be
mapped to one or more meanings, and multiple groups can
be mapped to one meaning. For example, a group with ex-
pansions, “carnegie mellon university” and “central michigan
university”, is mapped to 2 distinct meanings, while a group
with expansions, “central michigan university” and “central
mich univ”, is mapped only to one meaning. By a group-
meaning mapping we consider a meaning, to which a group
can be mapped.
• Purity: The Purity measures the accuracy of the group-
j=1 |Gi ∩ Mj| be the
total number of group-meaning mappings. The Purity mea-
sure counts the number of groups, which are mapped to some
meaning, and divides this number by N :

meaning mappings. Let N = Pn

i=1Pk

Purity(G, M) =

1
N

max

j

|Gi ∩ Mj|

n

Xi=1

Good clusterings have Purity close to 1, and bad ones – close
to 0. Since high Purity is easy to achieve, by simply having
all expansions in separate groups, in addition to Purity, we
use Normalized Mutual Information, described below.
• Normalized Mutual Information (NMI):

NMI(G, M) =

MI(G; M)

[H(G) + H(M)]/2

P r(Gi)P r(Mj ) is
the Mutual Information of the clusters and the Golden Stan-

Here MI(G; M) = PiPj P r(Gi ∩ Mj) log P r(Gi∩Mj )

1267Purity NMI
0.862
0.956
0.999
0.918
0.999
0.998

EDC
JDC
AEC

Table 1: Evaluation for EDC, JDC, and AEC.

AEC
EAEC

Purity NMI Recall
0.801
0.993
0.996
0.996

0.994
0.995

Table 2: Evaluation for AEC and EAEC.

dard meanings. H(G) = −Pi P r(Gi) log P r(Gi) is the en-

tropy of the clusters, and H(M), computed analogically, is
the meanings entropy. NMI is a number between 0 and 1,
where good clusterings have NMI close to 1, and bad ones
– close to 0. Since, the cluster entropy H(G) increases with
the number of groups, clusterings with many groups have
low NMI scores. This is why NMI considers the trade-oﬀ
between the quality of the clusters and their total number.
• Recall: We compute Recall only with respect to our
ground truth meanings.
In practice, it is very diﬃcult to
ﬁnd all possible meanings for a given acronym. The Re-
call w.r.t our Golden Standard, is the number of meanings,
which are found in the clustering, divided by the total num-
ber of meanings in the Golden Standard. Furthermore, if a
group is mapped to multiple meanings, we consider only one
of them, assuming only a single meaning per group.

5.2 Acronym Meaning Discovery Results

We ﬁrst compare the eﬀectiveness of the Edit Distance
based (EDC) and Jaccard Distance based (JDC) clusterings
with our Acronym Expansion clustering (AEC) using a sub-
set of 20 acronyms. The results are presented in Table 1.

We notice that both methods, EDC and JDC, have lower
cluster quality than AEC, especially in terms of NMI. EDC
often fails to cluster expansions by semantic meaning, since
expansions with the same meaning can have very large string
distance. In such cases, they are incorrectly assigned to dif-
ferent groups. For example “central michigan university”,
“central mich univ”, and “central mi univ” belong to diﬀer-
ent groups from the EDC clustering. In contrast, the AEC
method groups these expansions in a single group, since it
does not rely on string distance.

The JDC method has better quality than EDC, but it
is still inferior to the AEC method. Since JDC uses set
distance between expansions, it is very diﬃcult to ﬁnd a
threshold for similarity. If the threshold is high, then distinct
meanings can be easily grouped together due to mis-clicks; if
the threshold is low, then identical meanings are not grouped
together because sometimes there are not enough clicks. In
contrast, the AEC method addresses these problems by us-
ing distributional distance metric over click frequencies.

5.2.1 Comparison between AEC and EAEC

To discuss the eﬀectiveness of our clustering approaches,
AEC and EAEC, we ﬁrst present some intuitive examples.
In Table 3 we show the top-5 meanings, their probabilities,
and a few context words for “CMU”, “MBA”, and “RISC”,
using our enhanced clustering EAEC. Each of the three
acronyms has one or two dominant meanings, with very high
probabilities, and other meanings with much lower probabil-
ities. We also notice that the context words of each meaning

NW

NGS

|NW ∩ NGS|

15.859

15.781

4.922

|NW ∩NGS |

|NW ∩NGS |

NW
0.351

NGS
0.339

Table 4: Number of meanings in Wikipedia (NW ), in
the Golden Standard (NGS), and shared (NW ∩ NGS).

are very descriptive. For example, “concrete masonry unit”
has context words “cinder”, “cement”, “construction”, etc.

We systematically compare AEC and EAEC in Table 2,
using the Golden Standard meanings and our complete data
set. We ﬁrst notice that the quality of the two methods is
very good, in terms of both, Purity and NMI. The meth-
ods succeed in grouping together expansions referring to the
same semantic meaning, even if they have large string dis-
tances. Furthermore, due to the choice of distributional dis-
tance metric for the clustering, the mis-clicks do not inﬂu-
ence the clustering.

From the results we also notice that the Recall improves
signiﬁcantly for the enhanced clustering (EAEC), compared
to AEC. Using EAEC with supersequence queries, we dis-
cover signiﬁcantly more new tail meanings. Furthermore,
since the enhanced clustering EAEC achieves 0.996 Recall
w.r.t the Golden Standard, we succeed to output 99% of the
meanings in the Golden Standard to the end users.

In addition to using click frequency based aggregation,
we also tried out distance based aggregation as described
in Section 3. Distance based aggregation yields a purity of
0.996, NMI of 0.997 and recall of 0.994, which is similar to
that of click frequency based aggregation.

5.2.2 Wikipedia vs. Golden Standard Meanings

An important result from our study is the comparison
between the Wikipedia meanings and the Golden Standard
meanings. First, in Table 5 we present the meanings of
“CMU”, “RISC”and “MBA” which belong only to Wikipedia,
only to the Golden Standard, or are shared by both. We no-
tice, that the amount of shared meanings is relatively small,
and that both sets, Wikipedia and Golden Standard mean-
ings have meanings not covered by the other.

In Table 4 we present the average number of meanings in
Wikipedia, in the Golden Standard, and their shared mean-
ings. In Figure 3 all acronyms in our data set are presented
with their meaning counts from the three meaning sets.

From these results we see that only 35% from the mean-
ings in Wikipedia are found using our clustering aproach.
This means that a lot of meanings listed in Wikipedia are
typically not used in their abbreviated form. It can be be-
cause they are extremely tail meanings, or because they
are mostly encyclopedic or domain-speciﬁc, e.g., medical or
mathematical terms. In such cases there is not enough evi-
dence in the query log that these acronyms refer to the cor-
responding meanings. For example, for “MBA” our method
did not ﬁnd the meaning “main belt asteroid” (see Table
5), and for “LRA” our method did not ﬁnd the meaning
“leukotriene receptor antagonist”.

On the other hand, our acronym mining approach discov-
ers many meanings, currently not present in Wikipedia: in
the Golden Standard set only 34% of the meanings belong to
Wikipedia. More importantly, we discover acronym mean-
ings which are frequently used by common users. Typically,
we discover many company names, associations, universities,
events, etc. which are used together with their abbreviated
form. By ﬁnding such new, valid and widely used acronym

1268Meaning

Probability

Context Words

central michigan university
carnegie mellon university

CMU

concrete masonry unit

RISC

central methodist university

canton municipal utilities

reduced instruction set computer
rice insurance services company
rna induced silencing complex
reinventing schools coalition

recovery industry services company

master of business administration

mortgage bankers association

MBA

montgomery bell academy

metropolitan builders association

military beneﬁt association

0.615
0.312
0.045
0.017
0.004

0.737
0.143
0.046
0.037
0.022

0.868
0.069
0.022
0.015
0.006

michigan, university, athletics, campus, edu, football, chippewas

mellon, carnegie, pittsburgh, university, library, computer, engineering

block, concrete, cmu, masonry, cinder, cement, construction

methodist, university, fayette, central, missouri, baseball

canton, court, municipal, docket, case, clerk, records

risc, instruction, set, computer, processor, architecture

insurance, rice, risceo, services, real, estate

complex, rna, silencing, gene, protein

schools, coalition, inventing, alaska

recovery, certiﬁed, specialist, matrix, educational

mba, business, gmat, administration, harvard, programs, degree

mortgage, bank, implode, amerisave, bankers, rates

bell, montgomery, academy, nashville, mba, school, edu

builders, homes, association, wisconsin, milwaukee

military, armed, association, beneﬁts, insurance, veterans

Table 3: Top-5 meanings for CMU, RISC, and MBA, ranked by probability, and some of their context words.

Only Wikipedia Meanings

Only Golden Standard Meanings

Shared Meanings

CMU

caribbean medical university

chiang mai university

california miramar university

colorado mesa university
coﬀman memorial union

college music update

complete music update

communication management unit

central methodist university

canton municipal utilities

centrul medical unirea
case management unit

central mindanao university
central missouri university

carnegie mellon university
central michigan university

canadian mennonite university

concrete masonry unit

couverture maladie universelle

rural infrastructure service commons

rice insurance services company

reduced instruction set computing

RISC

research institute for symbolic computation

reinventing schools coalition

rna induced silencing complex

maldives basketball association

marine biological association

metropolitan basketball association

media bloggers association
milwaukee bar association

monterey bay aquarium

macbook air

main belt asteroid

market basket analysis

miss black america

misty’s big adventure

recovery industry services company

rhode island statewide coalition

metropolitan builders association

master of business administration

military beneﬁt association
master builders association

mississippi basketball association

mortgage bankers association

montgomery bell academy
mountain bothy association

mountain bike action

massachusetts bar association

mariana bracetti academy

missionary baptist association

morten beyer agnew
mind body awareness

memphis business academy

Table 5: Meanings from Wikipedia and Golden Standard sets.

Golden Standard Meanings 

Shared Meanings 

Wikipedia Meanings 

MBA

80 

70 

60 

50 

 

i

s
g
n
n
a
e
M

 
f
o
 
r
e
b
m
u
N

40 

30 

20 

10 

0 

 

a
b
a

 
l
c
a

 
c
d
a

 
f
d
a

 

p
g
a

 

u
a

l

 
i

s
n
a

 
i

p
a

 
s
p
a

 

p
r
a

 

p
s
a

 
a
t
a

 

m
t
a

 

c
c
b

 

s
o
b

i

 

p
m
b

 

d
a
c

 

d
c
c

 

s
c
c

 
i

g
c

 

c
s

i

c

 

a
m
c

 

u
m
c

 
s
o
m
c

 
a
p
c

 

c
p
c

 
l

p
c

 

m
p
c

 

u
p
c

 

m
r
c

 
t
r
c

 
s
s
c

 

p
t
c

 
r
t
c

 

c
a
d

 
l

d
d

 
r
d
d

 
a
h
d

 
r
f
c
e

 

m
c
e

 

l

c
p
f

 

a
c
g

 
j
s
g

 
c
r
i

 
a
f
j

 

a
r
l

 

a
b
m

 

g
c
m

 
t
i

m

 
a
b
s
m

 
r
s
m

 
a
v
m

 
i

c
o

 
f
e
p

Acronyms 

 

m
p

i

 
i

m
p

 
c
s
i
r

 
a
s
r

 
i

g
s

 

p
m
s

 
i

s
s

 
i

d
t

 
c
y
t

 
c
c
u

Figure 3: Number of meanings from Wikipedia, from the Golden Standard, and shared between the two sets.

1269Query

Label

cmu michigan

cmu robotics institute

cmu pittsburgh

cmu fayette missouri

central michigan university
carnegie mellon university
carnegie mellon university
central methodist university

Table 6: Examples for “acronym+context” queries
and their labels.

expansions, we can signiﬁcantly extend the meaning lists in
Wikipedia disambiguation pages.

5.3 Online Meaning Prediction Results

Here we consider the online application scenario from Sec-
tion 4: given acronym+context queries, predict the meaning
of the acronym considering the provided context.

We use a set of 7612 acronym+context queries, randomly
sampled from the query click log, which refer to the acronyms
in our data set. For example, for “CMU” we use queries like
“cmu football”, “cmu pittsburgh”, “cmu robotics institute”,
etc. Human users label these queries with the meaning they
consider most probable, by looking only at the query. For
example, “cmu michigan” is labeled by “central michigan uni-
versity” (see Table 6 for more examples). Queries, for which
the additional context does not disambiguate the meaning,
are not labeled. For example, “cmu university” can refer to
multiple meanings, and hence it is not labeled.

We apply the prediction approach from Section 4 to the
labeled queries, without considering their labels. The output
is the most probable meaning of the acronym, given the
context words in the query.

For each acronym, we compute the number of correctly
predicted meanings (by comparing to their labels), divided
by the total number of labeled queries for this acronym.
The average precision is 0.941. This means that the as-
signed context words to each acronym meaning are highly
indicative and can be used to predict meanings for online
acronym+context queries eﬀectively.

6. RELATED WORKS

While there have been many works and systems available
on acronyms, we believe our work has the following unique
distinctions compared with the state of art. First, we solve
the general acronym meaning discovery problem in a com-
prehensive way. This is diﬀerent from other works which
either look at domain speciﬁc acronyms (e.g., medical do-
main), or only focus on certain aspects of the problem (e.g.,
only interested in ﬁnding expansions). Second, to the best
of our knowledge, this is the ﬁrst work on acronym expan-
sion and meaning discovery leveraging query click log by
exploiting the acronym co-click behaviors. Third, due to
the nature of query click analysis, our method is language
agnostic. This is diﬀerent from pattern based discovery in
text for instance, where people look for NLP patterns (e.g.,
“Carnegie Mellon Univeristy, also referred to as CMU, is ...”)
and therefore the patterns and methods are very language
dependent. We now study related works in details.

Wikipedia covers many acronyms and their diﬀerent mean-
ings through its “disambiguation pages”. These pages are
manually edited by one or a few editors. First, our experi-
ments show that many meanings are not covered in Wikipedia
disambiguation pages; there are almost twice more meanings
used in web search queries but not covered in Wikipedia.

Second, it does not provide popularity scores. Furthermore,
the meanings in Wikipedia are not necessarily the most pop-
ular meanings; our experiments show that roughly 65% of
the meanings of acronyms on Wikipedia are rarely or never
expressed in Web search queries (Section 5). Finally, our
work heavily taps into the wisdom of crowds, to discover
acronym expansions, understand their meanings and popu-
larity, and mine their corresponding context. Tapping data
contributed by millions of end users is a signiﬁcant and nec-
essary step forward.

Websites such as acronymfinder.com list many possible
acronym expansions; this is also manually edited. As in
Wikipedia, it does not provide popularity scores. Further-
more, it does not provide any context words for most of the
acronym expansions. While it does oﬀer a large number of
meanings for a large number of acronyms, our study shows
that it suﬀers signiﬁcantly from the quality problem: (1)
many expansions listed are actually near duplicates (“Re-
duced Instruction Set Computer” and “Reduced Instruction
Set Computing” for “RISC”); (2) many expansions are ac-
tually meaningless, in the sense people rarely or never use
its acronym form to refer to it (e.g., “More Bad Advice” for
“MBA”).

Recently, there have been a few works on automatic min-
ing of acronym expansions by leveraging Web data [8, 9,
6]. While some aspects are complementary to ours (e.g.,
in [6] subsequent queries in query sessions are exploited),
our study covers many more aspects, including meaning dis-
covery through clustering analysis, popularity computation
and context words mining. Our study heavily relies on query
click log, and it is not clear how other works can be adapted
to support eﬀective clustering, popularity and context words
mining without the help of query click log.

Another line of related work is on mining synonyms [3,
4, 15, 11, 1, 2]. Existing studies on synonyms are mostly
focused on unambiguous synonyms. Acronym is a special
type of synonym, which is highly ambiguous and context
dependent. This work can be regarded as a ﬁrst attempt
at addressing the ambiguity problem in synonyms, with a
focus on acronyms only.

There have been many works on acronym expansion dis-
covery in vertical domains (mainly in medical), e.g., [12, 10,
16, 13]. These works mainly rely on text analysis to discover
acronym expansions, and tend to be optimized for their re-
spective domains. This is diﬀerent from both the general
acronym mining aspect, as well as the query click log anal-
ysis angle of this work.

7. CONCLUSION

In this paper, we introduce the problem of ﬁnding dis-
tinct meanings of each acronym, along with the canonical
expansion, popularity score and context words. We present
a novel, end-to-end solution to this problem. We describe
how web search engines can leverage the mined information
for online acronym and acronym+context queries.

Our work can be extended in multiple directions. There
are other ambiguous queries besides acronyms like people
and place name queries. For example, the query “Jim Gray”
can refer to the computer scientist, the sportscaster in ad-
dition to many other less famous Jim Grays. Can our tech-
niques be adapted to ﬁnd all the distinct meanings of such
queries? Furthermore, it will also be interesting to look into
data sources other than query click log for the mining task.

12708. REFERENCES
[1] S. Chaudhuri, V. Ganti, and D. Xin. Exploiting web

search to generate synonyms for entities. In WWW
Conference, 2009.

[2] S. Chaudhuri, V. Ganti, and D. Xin. Mining document

collections to facilitate accurate approximate entity
matching. PVLDB, 2(1), 2009.

[3] T. Cheng, H. W. Lauw, and S. Paparizos. Fuzzy

matching of web queries to structured data. In ICDE,
2010.

[4] T. Cheng, H. W. Lauw, and S. Paparizos. Entity

synonyms for structured web search. TKDE, 2011.

[5] D. Defays. An eﬃcient algorithm for a complete link
method. The Computer Journal, 20(4):364–366, 1977.

[6] A. Jain, S. Cucerzan, and S. Azzam.

Acronym-expansion recognition and ranking on the
web. In Information Reuse and Integration, 2007.

[7] R. Jones, B. Rey, O. Madani, and W. Greiner.

Generating query substitutions. In WWW, 2006.

[8] L. S. Larkey, P. Ogilvie, M. A. Price, and B. Tamilio.

Acrophile: an automated acronym extractor and
server. In Proceedings of the ﬁfth ACM conference on
Digital libraries, pages 205–214, 2000.

[9] D. Nadeau and P. D. Turney. A supervised learning

approach to acronym identiﬁcation. In Proceedings of
the 18th Canadian Society conference on Advances in
Artiﬁcial Intelligence, pages 319–329, 2005.

[10] S. Pakhomov. Semi-supervised maximum entropy

based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of the
40th Annual Meeting on Association for
Computational Linguistics, 2002.

[11] P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu,
and V. Vyas. Web-scale distributional similarity and
entity set expansion. In EMNLP, 2009.

[12] J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,

and M. Morrell. Automatic extraction of
acronym-meaning pairs from medline databases.
Studies in health technology and informatics,
84(1):371–375, 2001.

[13] J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,

M. Morrell, and A. Rumshisky. Extraction and
disambiguation of acronym-meaning pairs in medline.
2004.

[14] R. Sibson. Slink: an optimally eﬃcient algorithm for

the single-link cluster method. The Computer Journal,
16(1):30–34, 1973.

[15] P. D. Turney. Mining the web for synonyms: Pmi-ir

versus lsa on toeﬂ. CoRR, cs.LG/0212033, 2002.

[16] J. Wren, H. Garner, et al. Heuristics for identiﬁcation
of acronym-deﬁnition patterns within text: towards an
automated construction of comprehensive
acronym-deﬁnition dictionaries. Methods of
information in medicine, 41(5):426–434, 2002.

[17] M. Zahariev. Eﬃcient acronym-expansion matching
for automatic acronym acquisition. In International
Conference on Information and Knowledge
Engineering, 2003.

A. APPENDIX: ACRONYM-EXPANSION

CHECKING FUNCTION

Let a and e be an acronym and a query, respectively. The
acronym-expansion checking function returns true if e is an
expansion of a, and false otherwise. We modify the longest
common subsequence algorithm by assigning weights to the
words and letters of the query string. We ﬁnd the subse-
quence with the highest score and use heuristics to decide if
e is an expansion of a as follows.

To increase the chance of matching an acronym letter at
the beginning of a word, we assign weights wns = 2 to the
initial letters of normal non-stop words, ws = 1 to the initial
letters of stop words, and wni = 0.1 to all other letters (i.e.
the non-initial letters of all words). The score s of a match
is the sum of the scores of the participating letters (see Line
15). Then we check if s ≥ 0.68·|a|·wns, where |a| denotes the
number of letters in the acronym and 0.68 is an empirically
set threshold parameter.

A further requirement is that all words in the query con-
tain acronym letters. However, as mentioned earlier, often
stop words are not considered when acronyms are formed
(e.g., “Master of Business Administration” is an expansion
for “MBA”). To solve this we use weights for the non-stop
query words (wns = 2) and for the stop words (ws = 1).
We use another threshold to check if s ≥ 0.8 · T , where T is
deﬁned in Line 2.

If both conditions in Line 16 are satisﬁed, the acronym-
expansion check returns true, and otherwise f alse. For ex-
ample, the score of “master of business administration” for
the acronym “MBA” is 6 and both inequalities in Line 16 are
satisﬁed. The query “master of business administration ed-
ucation” is not expansion of “MBA” as the second inequality
is not satisﬁed (6 6≥ 0.8 · 9). Finally, two initial requirements
are that the acronym consists of one word only, and that the
query contains at least two words.

Algorithm 1 Acronym-Expansion Checking Function

Input: Acronym a; Query e; Weights {wns, ws, wni}
Output: True if e is expansion of a, false otherwise

1: function IsExpansion(a, e, wns, ws, wni)

T ←Pt∈e w(t), w(t) =(wns, t is non-stop word

Let R be (|a| + 1) × (|e| + 1) array of zeros
for i ← 1 to |a| do

t is stop word

ws,

for j ← 1 to |e| do

p ← max (R[i − 1, j], R[i, j − 1])
if a[i − 1] = e[j − 1] then

2:

3:
4:
5:
6:
7:

8:

end if

else

9:
10:
11:
12:
13:
14:
15:
16:
17: end function

end for

wns, non-stop word starts at j −1 in e
ws,
wni, else

stop word starts at j −1 in e

R[i, j] ← max (p, R[i − 1, j − 1] + w)

w ←


R[i, j] ← p

end for
s ← R[|a|, |e|]
return s ≥ 0.68 · |a| · wns and s ≥ 0.8 · T

⊲ Score of the best match

1271