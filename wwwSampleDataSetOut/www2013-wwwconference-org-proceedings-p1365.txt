Content-Aware Click Modeling

Hongning Wang, ChengXiang Zhai

Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana IL, 61801 USA

{wang296,czhai}@illinois.edu

Anlei Dong, Yi Chang

Yahoo! Labs

701 First Avenue, Sunnyvale, CA 94089
{anlei,yichang}@yahoo-inc.com

ABSTRACT
Click models aim at extracting intrinsic relevance of docu-
ments to queries from biased user clicks. One basic modeling
assumption made in existing work is to treat such intrinsic
relevance as an atomic query-document-speciﬁc parameter,
which is solely estimated from historical clicks without us-
ing any content information about a document or relation-
ship among the clicked/skipped documents under the same
query. Due to this overly simpliﬁed assumption, existing
click models can neither fully explore the information about
a document’s relevance quality nor make predictions of rel-
evance for any unseen documents.

In this work, we proposed a novel Bayesian Sequential
State model for modeling the user click behaviors, where
the document content and dependencies among the sequen-
tial click events within a query are characterized by a set of
descriptive features via a probabilistic graphical model. By
applying the posterior regularized Expectation Maximiza-
tion algorithm for parameter learning, we tailor the model to
meet speciﬁc ranking-oriented properties, e.g., pairwise click
preferences, so as to exploit richer information buried in the
user clicks. Experiment results on a large set of real click
logs demonstrate the eﬀectiveness of the proposed model
compared with several state-of-the-art click models.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models; H.3.5 [Information Storage and Retrieval]: On-
line Information Service

General Terms
Algorithms, Experimentation

Keywords
Click modeling, query log analysis, probabilistic graphical
model

1.

INTRODUCTION

User click logs provide rich and valuable implicit feed-
back information and can be used as a proxy for relevance
judgments [14] or signals for directly inﬂuencing ranking [1].

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

However, they are also known to be vulnerable to position-
bias – documents appearing at higher positions tend to re-
ceive more clicks even though they are not relevant to the
query [10]. Therefore, properly modeling and interpreting
the underlying mechanism that gives rise to user clicks is an
important yet challenging research problem.

To fulﬁll this goal, click models [4, 5, 8, 11, 20] have been
proposed for modeling user clicks and extracting intrinsic
relevance information from the biased click logs. One fun-
damental assumption made in click models is the so-called
examination hypothesis: a user clicks on a returned docu-
ment if and only if that document has been examined by the
user and it is relevant to the given query. Based on such an
assumption, click models aim to distinguish the relevance-
driven clicks from the position-driven clicks by postulating
diﬀerent dependency assumptions between the events of ex-
amining a document and clicking on it, e.g., position mod-
els [7, 8] and cascade models [4, 11]. Though deviating
in various dependency assumptions about the examine and
click events, all click models formalize a document’s rele-
vance quality to a given query as an atomic query-document-
speciﬁc parameter, e.g., Bernoulli random variable [4, 11],
which is solely estimated from multiple occurrences of such
speciﬁc query-document pair in the click logs.

However, this commonly used modeling approach totally

ignores the actual document content, which presumably would
directly inﬂuence a user’s click decision. As a result, the ex-
isting click models can neither take advantage of in-depth
knowledge about the relevance quality of a document to
the query buried in the document content, nor beneﬁt from
the semantic relation among the documents under the same
query. For example, diversity is an important criterion for
satisfying a user’s information need [6]. A user would be
less likely to click on a near-duplicate document if she has
already clicked on a previous one with similar content; but
in such a case, her “skip” decision does not necessarily mean
that the document is irrelevant to her query. In most of the
existing click models, we are only aware of which position is
clicked, but the underlying “semantic explanations” for the
clicking behavior, e.g., clicked content redundancy and click
distance, are completely discarded.

A serious consequence of such an overly simpliﬁed assump-
tion of a document’s relevance quality to a given query is
that the model’s generalization capability is limited: one
has to collect a large number of such query-document pairs
to obtain a conﬁdent estimate of relevance. As shown in the
previously reported results, only when there is a suﬃcient
number of observations for the given query-document pairs,

1365could the existing click models demonstrate their advantages
[4, 8, 11]. In the extreme case, when a new document comes
into the search engine, there would be no way for us to accu-
rately infer its relevance to the query immediately by a click
model. The situation gets even severer in time-sensitive re-
trieval tasks, such as news search, where new documents
keep emerging and we need timely estimation of their rele-
vance quality to the given query before we could gather large
number of user clicks.

In addition, existing click models only target at decom-
posing the relevance-driven clicks from the position-driven
clicks, which boils down to discounting the observed clicks
for each document in a pointwise manner. However, in a real
search scenario, when a user decides to skip one document,
it does not necessarily indicate the document is irrelevant
to the query, since it is also possible that the previous/next
clicked document is more relevant than it. Such property
of user behavior has been proved by many real user studies
[10, 14]. Therefore, existing click models are not optimized
for distinguishing the relative order among the inferred rel-
evance quality.

To the best of our knowledge, no existing work in click
modeling attempted to address these two deﬁciencies, i.e.,
lack of exploring content information and failing to cap-
ture relative relevance preference.
In this work, we pro-
pose to solve these limitations within a probabilistic gen-
erative framework, which naturally incorporates the docu-
ment content and relative preferences between documents
into click modeling.
In detail, following the assumptions
in cascade models, we propose a Bayesian Sequential State
(BSS) model to formalize the generation of the observed
clicks under a given query. First, to capture the rich seman-
tic of a document’s relevance quality to the query, we in-
troduced a set of descriptive features (e.g., query matching
in title and site authority) into query-document relevance
modeling.
Instead of hard coding the dependency among
the click/examine events within a query (e.g., clicked docu-
ments must be relevant) [4, 11], we give our model the free-
dom to learn such relation from data based on the designed
features, e.g., a click decision will be aﬀected by the content
redundancy between the current and previously clicked doc-
uments. Second, ranking-oriented knowledge, e.g., pairwise
click preference, is incorporated by regularizing the poste-
rior distribution of clicks, which helps us tailor the proposed
probabilistic model and avoid undesirable local maxima.

The proposed model is a general click modeling frame-
work, which covers most of existing models as special cases.
On a large set of real click logs, the proposed BSS model
outperformed several state-of-the-art click models in terms
of relevance estimation quality. Especially when we only
have limited size of training samples for a particular query-
document pair, BSS model demonstrated its advantage by
leveraging the information from ranking-oriented features
for accurate relevance estimation. The introduced pairwise
click preference renders BSS model better ranking capability
in distinguishing the relative order of relevance among the
candidate documents. Besides, BSS model provides a princi-
pled way of interpreting and modeling user’s click behaviors,
which is not available in existing click models.

2. BACKGROUND

The main purpose for modeling the user’s click behav-
iors in search engine logs is to ﬁght against the notorious

position-bias and extract the document’s intrinsic relevance
to the query. Richardson et al.
[19] attempted to combat
position-bias by imposing a multiplicative factor on docu-
ments in lower positions to infer their true relevance. This
idea was later formalized as the examination hypothesis and
adopted in the position models [7]. The key assumption in
position models is that the user clicks on a document if and
only if that document has been examined by the user and it
is relevant to the query. In addition, the examination event
only depends on the position. Formally, given a document
d displayed at position i, the probability of d being clicked
(i.e., C = 1) is determined by the latent examination event
(i.e., E = 1) as,
P (C = 1|d, i) =

P (C = 1|d, i, E = e)P (E = e|d, i)

(cid:2)

e∈{0,1}
=P (C = 1|d, E = 1)P (E = 1|i)

where P (C = 1|d, E = 1) is speciﬁed by a document-speciﬁc
parameter αd describing the document’s intrinsic relevance
quality to the query, and P (E = 1|i) is determined by a
position-speciﬁc parameter βi to capture position bias.

However, the pure position models deal with examination
event in an isolated manner, i.e., the examination proba-
bility P (E = 1|i) is assumed to be independent from the
click events. Cascade models are one typical extension to
conquer this limitation, which further assume the user will
examine the returned documents from top to bottom and
make click decisions over each examined document. Once
the user stops examining, all the following documents will
not be examined. Therefore, a click event in a query session
is modeled as,

P (Ci = 1) =P (Ri = 1)

i−1(cid:3)

(cid:4)

1 − P (Rj = 1)

(cid:5)

j=1

where Ri = 1 is the event that document d at position i is
relevant to the given query.

One drawback of the original cascade model is that it can
only deal with queries containing one click, later work gen-
eralizes it to queries with multiple clicks. Chapelle et al., [4]
solved this limitation by distinguishing the perceived and in-
trinsic relevance of a document: they assumed the perceived
relevance controls the click event and the intrinsic relevance
determines the user’s satisfaction with the current document
and her further examination of the following documents.

Our proposed BSS model falls into the category of cascade
models: we assume the users would sequentially examine
the returned documents from top to bottom for the given
query, and a clicked document must be examined before-
hand. In addition, by incorporating a set of ranking features,
we model a document’s relevance quality to a given query
in a more general way: we assume the relevance quality of a
document to the given query is not only an intrinsic prop-
erty of the document itself, but also inﬂuenced by the dis-
played document content (e.g., title and abstract). The de-
pendency relation between the examine and click events are
ﬂexibly learned from data, e.g., an examined and relevant
document may still be skipped. In addition, the proposed
method also explores the relationship among the clicked and
skipped documents under the same query, e.g., content re-
dundancy, which is not covered by existing click models. In
previous work, click decision is only determined by the doc-
ument’s own relevance quality; while in our proposed model,

1366ranking-oriented constraints, e.g., pairwise click preferences,
are also incorporated to improve the model’s capability of
distinguishing the relative order of relevant documents.

3. BAYESIAN SEQUENTIAL STATE MODEL

As discussed earlier, existing click models have two limi-
tations: 1) modeling the relevance of document to the given
query as an atomic query-document-speciﬁc parameter; 2)
failing to capture the relative order of estimated relevance
between the documents. To break these two limitations and
make click models applicable in more search scenarios, we
propose a novel Bayesian Sequential State (BSS) model, in
which the relevance quality of document to a given query
is parameterized by a set of document-speciﬁc features, and
the dependencies among the click and examine events within
the same session are explicitly captured and exploited.
3.1 Basic Generative Assumption

Following the basic modeling assumption in cascade mod-
els, in our proposed BSS model, we assume that when a user
submits a query to the search engine and gets a list of ranked
results, she would sequentially examine the returned docu-
ments from top to bottom; a document must be examined
before she clicks on it; and once she decides to stop exam-
ining at current position, she would leave this query session
without further interactions. In particular, we assume that
when she is examining a document, she would judge its rel-
evance according to the displayed document content, e.g.,
title and abstract, which can be characterized by a set of
features, e.g., query term matching in title and abstract;
in addition, the user remembers her previously examined
documents under this query, so that when she moves onto
lower positions, her previous click/skip decisions will aﬀect
her later choices, e.g., skipping the less relevant documents.
In other words, the click/skip events within the same query
session are assumed to be dependent with each other.

Formally, assume there are N queries in our collection
and for each query there are M ordered documents. Fol-
lowing the notations introduced in Section 2, we use binary
variables to denote the relevance status, examine and click
events of a document, i.e., R = {0, 1}, E = {0, 1} and
C = {0, 1}. To make the presentation concise, we will ig-
nore the symbol di representing the document displayed at
position i under a particular query, when no ambiguity is
caused. Hence, the generation process of the observed clicks
in a collection of query logs deﬁned by the proposed BSS
model can be formalized as follows:

- For each query q in the query log:

- For document d in position i:

1. Decide whether to examine the current po-
sition based on previous examination event
Ei−1 and previous document di−1’s relevance
status Ri−1, i.e., Ei ∼ P (Ei|Ei−1, Ri−1, q).
If i = 1, Ei = 1;

2. If Ei = 0, abandon further examination;
3. Judge di’s relevance against query q, i.e., Ri ∼

P (Ri|di, q);
vance quality, i.e., Ci ∼ P (Ci|Ei, Ri, q)

4. Decide whether to click di based on its rele-

Ri, Ci}M
formulated as:
P (E, R, C|q) =

As a result, the joint probability of random variables {Ei,
i=1 within a search result page for query q can be

M(cid:3)

i=1

P (Ci|Ri, Ei, q)P (Ei|Ri−1, Ei−1, q)P (Ri|di, q)

(1)
Diﬀerent from most of the existing click models, where
the dependency relation is hard-coded in their conditional
probabilities, e.g., an examined and relevant document must
be clicked: Ei = 1, Ri = 1 ⇔ Ci = 1 [4, 11], we relax such
hard requirement to accommodate noise in clicks [5]. We
assume that even an examined document is not relevant,
the user might still click on it because of her carelessness,
i.e., P (Ci = 1|Ei = 1, Ri = 0) > 0; and on the other hand,
even if an examined document is relevant, the user might
still skip it due to the redundancy or her satisfaction of per-
vious clicks, i.e., P (Ci = 0|Ei = 1, Ri = 1) > 0. In addition,
to fully explore the dependency between a click event and
the document’s relevance status, we assume user’s further
examination also depends on the current document’s rele-
vance quality, i.e., P (Ei|Ei−1, Ri−1) (cid:4)= P (Ei|Ei−1).
3.2 Conditional Probability Reﬁnement

The generation process introduced in Eq (1) depicts the
skeleton of dependencies among the random variables of
{Ei, Ri, Ci}M
i=1 within the search result page for a given
query. Next, we will discuss the details of how we can incor-
porate descriptive features to materialize those dependency
relations and exploit rich information conveyed in the users’
click behaviors.

To parameterize the dependency, we deﬁne the conditional

probabilities in BSS model via logistic functions:

1. Relevance probability:

P (Ri = 1|di, q) = σ(wRT

f R
q,di + wR

q,di )

(2)

2. Click probability:

P (Ci = 1|Ri, Ei, q) =

⎧⎪⎨
⎪⎩

0

σ(wC
σ(wC

R=0

R=1

Tf C
Tf C

q,di )
q,di )

if Ei = 0
if Ei = 1,Ri = 0
if Ei = 1,Ri = 1

(3)

3. Examine probability:

P (Ei = 1|Ri−1, Ei−1, q) =

⎧⎪⎨
⎪⎩

0

σ(wE
σ(wE

R=0

R=1

Tf E
Tf E

q,di )
q,di )

if Ei−1 = 0
if Ei−1 = 1,Ri−1 = 0
if Ei−1 = 1,Ri−1 = 1

1

1+exp(−x) , {f R

(4)
q,d} are the features
where σ(x) =
characterizing the conditional probabilities for relevance sta-
tus, click and examination events of document di under
query q; and Θ = {wR, wC
} are the
R=0
corresponding importance weights for the features.

q,d, f E

q,d, f C

, wC

, wE

R=0

, wE

R=0

R=1

In particular, to distinguish the intrinsic relevance and
perceived relevance, we assume a document’s latent rele-
vance status to a given query is determined by the mix-
ture of these two types of relevance, i.e., P (Ri = 1|q) =
σ(wRTf R
q,d is
a scaler factor reﬂecting the intrinsic relevance quality of a
document to the given query, which is assumed to be drawn
from a zero mean Normal distribution. And wR Tf R
q,d is an

q,d) as deﬁned in Eq (2). In particular, wR

q,d + wR

1367(
(cid:86)

w
E
R
i

1
(cid:16)

T

f

E
R
i
1
(cid:16)

,

d

i

1
(cid:16)

)

Ei-1

Ei

w f(cid:86)
T

(

E
R
i

Ri-1

(
(cid:86)

w
C
R
i

1
(cid:16)

T

f

C
R
i
1
(cid:16)

,

d

i

1
(cid:16)

)

w f(cid:86)
T

(

C
R
i

Ci-1

(
(cid:86)

R

w f
T

R
d
i
1
(cid:16)

+ 

w
P
d
i

)

C
R d
,
i

i

)

Ci

E
R d
,
i

i

)

Ri

Table 1: Features for materializing conditional prob-
abilities in BSS model.

Type Description
f R
q,di

65 text matching features
e.g., query matching in ti-
tle, query proximity in ab-
stract

Value

-

(
(cid:86)

R

w f
T

R
d
i

(cid:14)

w
P
d
i

)

N

f C
q,di

Figure 1: Factor graph representation for the pro-
posed BSS model. Circles denote the random vari-
ables and black squares denote the conditional prob-
abilities deﬁned in Eq(2)-(4). Random variable Ei
implies whether document di is examined, Ri rep-
resents di’s relevance status to the query, and Ci
indicates whether di is clicked by the user.

estimate of the perceived relevance quality, which is charac-
terized by a weighted combination of relevance-driven fea-
tures f R
q,d, e.g., site authority and query term matching in
document title. When we have suﬃcient observations of the
query-document pair (q, d), the estimation of wR
q,d will be
close to its true intrinsic relevance; but when we only have
limited observations, e.g., for a new document, relevance-
driven features f R
q,d will help to identify its perceived rele-
vance, which leads to user clicks.

Using the language of probabilistic graphical models, we
summarize the speciﬁcation of the proposed BSS model by
a factor graph representation in Figure 1.

3.3 Feature Instantiation for BSS

Table 1 lists the detailed deﬁnition of the proposed fea-
tures in BSS model, which aim at capturing diﬀerent factors
aﬀecting a user’s click decision.

Among the proposed features, f R

q,d is the set of features
describing the relevance quality of a document to the given
query. This is the core problem for modern information
retrieval study, and many eﬀective features have been pro-
posed for this purpose, such as BM25 and PageRank.
In
this work, we utilized 65 text matching features (e.g., query
term matching in document title and abstract) as our rele-
vance features. We should note that the proposed model is
general and can potentially accommodate any combination
of relevance-driven features.

Though we are aiming to distinguish diﬀerent eﬀects of
the current document’s relevance status in examination and
click events, it is impossible for us to pre-categorize which
set of features would only aﬀect user’s click (examine) deci-
sion when the current document is relevant and vice versa.
We decide to use the same set of features for these two sit-
uations, but give them diﬀerent weights, i.e., {wC
}
for click event and {wE
} for examine event, to por-
tray their distinct contributions. In detail, the click-event-
related features f C
q,d are used to indicate how the user would
behave when an examined document is judged to be rele-
vant (R = 1) or irrelevant (R = 0). For example, when the
document is irrelevant, a mis-click might be caused by the
position of the document (the user trusts more about the
top ranked documents); and when the document is relevant,

, wC

, wE

R=0

R=0

R=1

R=1

(cid:10)
i
position
# clicks
i − arg maxj<i[Cj = 1]
distance to last click
||q||
query length
AVGj<i,Cj =1sim(di, dj )
clicked content similarity
skipped content similarity AVGj<i,Cj =0[sim(di, dj)]

j<i 1[Cj = 1]

f E
q,di

(cid:5)
(cid:10)
i
position
(cid:4)
Cj = 1
# clicks
i − arg maxj<i
distance to last click
AVGj<i,k<i[sim(dj, dk)]
avg content similarity
variance content similarity VARj<i,k<i[sim(dj, dk)]
(All three types of features also include an additional bias

(cid:5)
Cj = 1

(cid:4)
j<i 1

term b accordingly.)

a skip decision may be due to the content redundancy of the
clicked documents or her satisfaction of current search result
(number of clicks). And the examine-event-related features
f E
q,d exploit the factors aﬀecting a user’s examine decision on
the next position. For example, when the current document
is irrelevant (R = 0) and the user has skipped several doc-
uments in a row (e.g, distance to the last click), she would
be more likely to give up further examining; and when the
current document is relevant (R = 1) and the clicked docu-
ments are quite similar to each other so far (average content
similarity), she might be more likely to stop.

3.4 Inference and Model Estimation

When applying the proposed BSS model in the testing
phase, we do not need to restrict ourself to the documents
ever occurred in the training set (i.e., wR
q,d exists). Since we
have formalized the perceived relevance by a set of relevance-
driven features, we can directly apply the model to any un-
seen document by calculating σ(wRTf R
q,d) as an estimate of
its relevance quality to the query (i.e., using mean value of
the intrinsic relevance wR
q,d from prior for all the new candi-
date documents). And for those documents occurred in our
training set, we can follow Eq (2) to incorporate the intrinsic
relevance of document to the given query learned from the
training set.

In model learning phase, because a document’s relevance
quality and examination status are not observed in the click
logs, we appeal to the Expectation Maximization algorithm
[18] to estimate the optimal parameter setting, which max-
imizes the lower bound of the log-likelihood of the observed
click events in the training set,

(cid:2)

(cid:2)

log

q,i

Ei,Ri

p(Ei, Ri, Ci|q, Θ)

L(C, q, Θ) =
(cid:2)

(cid:2)

≥

p(Ei, Ri|Ci, q,Θ) log p(Ei, Ri, Ci|q, Θ)

(5)

q,i

Ei,Ri

Particularly, in E-Step, we calculate the posterior distri-
bution of P (E, R|C, q, Θ(t)) for the latent variables (Ei, Ri)M

i=1

1368in a query session with respect to the current model Θ(t).
One advantage of the proposed model is that, in the model
training phase, since the clicked documents are already known,
we can ﬁx them and reduce the maximum clique size in the
induced graph structure to 3, i.e., {Ri−1, Ei−1, Ei}. As a re-
sult, exact inference is tractable and can be eﬃciently calcu-
lated via Belief Propagation [15]. And in M-Step, we obtain
the new model parameter Θ(t+1) by maximizing the expecta-
tion of the “complete” log-likelihood under P (E, R|C, q,Θ (t))
as deﬁned in Eq (5), which can be solved by any standard
optimization technique (in this work, we used L-BFGS [17]).
The E-Step and M-Step are alternatively executed until the
relative change of the righthand side of Eq (5) is smaller
than a threshold.
3.5 Discussion

There are close connections and clear diﬀerences between
the proposed BSS model and other existing click models.
First, BSS model explicitly encodes a document’s relevance
quality to a given query as a mix of intrinsic relevance and
perceived relevance, which makes it feasible to incorporate
richer information conveyed in document content for rele-
vance estimation. Second, BSS model generalizes the de-
pendency between a click event and the corresponding doc-
ument’s examine and relevance status. Most of previous
work puts hard constraint over the click event, i.e., Ci =
1 ⇔ Ei = 1, Ri = 1, which fails to recognize noisy clicks
and dependency among documents under the same query.
Third, the conditional probabilities deﬁned in BSS are no
longer simply treated as document- or position-speciﬁc pa-
rameters; instead, a set of descriptive features are designed
to capture rich semantics of users’ click behaviors.

R=0 = wE

If we resume the hard dependency setting and drop most
of the newly introduced features, the proposed BSS model
can be easily adopted to many existing click models: the ex-
amination model proposed in [19] can be treated as a special
case of our BSS model if we remove all the examine features
except position and assume it is independent of previous
relevance status, i.e., wE
R=1. And if we disable the
relevance features f R
q,d for each query-
document pair in the logistic function, we will go back to
the traditional setting for the click models. Based on this,
if we further remove the examination and click features, it
reduces to the CCM model proposed in [11]; if we only keep
the examine feature of distance to last click, it will reduce
to the UBM model proposed in [8]; and if we restrict the
examine probability to be Ei = 1 − Ri−1, it will reduce to
the original cascade model [7], since the user has to keep
examining until the ﬁrst click.

q,d and only keep wR

From the above discussion, we can clearly notice that the
proposed BSS model is a more general framework for model-
ing users’ click behaviors: through parameterizations, many
informative signals and dependency relation are introduced
to help the model explore a document’s in-depth relevance
quality to the given query from historic clicks.

4. POSTERIOR REGULARIZATION

One potential problem of the current BSS model setting
is that the designed structure is too ﬂexible for the learning
procedure to identify the “true” parameters, which depict the
underlying dependency among the latent variables. One ob-
vious deﬁciency is that a document’s relevance status, Ri =
1 or Ri = 0, is interchangeable. Since the click/examine

events are determined by the same set of features (weights
to be learned from data), if we switch the labels of Ri in the
whole collection, the model will ﬁnd another optimal weight
setting (switch the weights) to maximize the likelihood, but
that is undesirable.

The main reason for this unidentiﬁable problem is that
to capture noise within the click events we did not set hard
constraints on the conditional probability of click events, i.e.,
we allow P (Ci = 1|Ei = 1, Ri = 0) > 0 and P (Ci = 1|Ei =
1, Ri = 1) < 1; but it gives too much freedom to these
two conditional probabilities, such that they can freely ex-
change their roles and still maximize the likelihood of clicks.
Existing click models avoid this unidentiﬁable problem by
hard-coding the click events, i.e., Ci = 1 ⇔ Ei = 1, Ri = 1.
In our work, to keep the ﬂexibility of the modeling assump-
tions and handle the noisy clicks, we decide to regularize the
posterior distribution inferred by the model.

Another beneﬁt of posterior regularization is that we can
easily incorporate the ranking-oriented knowledge, i.e., pair-
wise preference, into click modeling, which is hard to be
directly encoded in the original conditional probabilities.
4.1 Posterior Regularized EM Algorithm

Posterior Regularization (PR) proposed by Ganchev et
al. [9] is a general framework for postulating structural con-
straints over the latent variable models. The method roots in
the block coordinate ascent EM framework [18], and it mod-
iﬁes the E-step of a standard EM algorithm to inject con-
straints over the posterior distribution of latent variables via
the form of expectations. And such regularization will not
aﬀect the convergency of original EM algorithm. Taking our
problem as an example, we should expect that the number of
relevance-driven clicks should be larger than mistaken clicks,
e.g., E[C = 1, E = 1, R = 1] > E[C = 1, E = 1, R = 0].

Formally, the regularized E-step in PR framework aims to

optimize:

min
q,ξ

KL(q(Y )||p(Y |X, Θ(t)))

s.t. Eq[φ(X,Y )] − b ≤ ξ

||ξ||β ≤ 

(6)

(7)

where p(Y |X, Θ(t)) is the original posterior distribution of
the latent variables Y given the current model Θ(t) and ob-
servation X, q(Y ) is the regularized posterior distribution
of Y , φ(X, Y ) is the constraint function deﬁned over (X, Y ),
and ξ is a slack variable to relax the constraints. In our case,
Y = {Ei, Ri}M
form: the primal solution q∗
terms of the dual solution λ∗

The convenience of PR framework comes from its dual
(Y ) is uniquely determined in
by,

i=1 and X = {Ci}M

i=1

(8)

q∗

(Y ) =

pΘ(Y |X) exp{−λ∗φ(X, Y )}

Z(λ∗)

and the dual problem is deﬁned as,

−b

T λ − log Z(λ) − ||λ||β∗

max
λ≥0

(9)
where Z(λ) is the partition function for Eq(8), and ||λ||β∗
is the dual norm of ||λ||β .

Eq (9) can be solved by the projected gradient algorithm
[3], and Eq (8) can be eﬀectively computed via Belief Propa-
gation algorithm by factorizing the constraints according to
the original factor graph. Intuitively, the PR framework can

1369be thought as regularizing the posterior inference in E-Step
of the original EM algorithm, such that the posterior dis-
tribution of the latent variables could satisfy some desired
properties speciﬁed in the expectations.
4.2 Constraints for Posterior Regularization
In this section, we discuss the constraint that we designed
to conquer the unidentiﬁable problem and that to incorpo-
rate the search-oriented pairwise constraints into our BSS
model. In detail, we choose to relax the posterior constraints
by setting  to be a small constant (0.01), and use L2-norm
to regularize the slack ξ.

4.2.1 Dampen noisy clicks

As we have discussed before, we need to restrict the inﬂu-
ence of the noisy clicks, and we hypothesize that most of the
clicks are driven by the relevance quality of the correspond-
ing document. To achieve this, we deﬁne the constraint over
the click events as:

(cid:2)
⎧⎨
⎩

i

φnoise(X,Y ) =
(cid:2)

=

i

φnoise(X, Yi)

(10)

−1 if Ei = 1 and Ri = Ci
if Ei = 1 and Ri (cid:4)= Ci
c
otherwise
0

and set the left-hand side constant b to be zero in Eq(6).

The meaning of this constraint is straightforward: we re-
quire the ratio between the expectation of relevance-driven
clicks (Ei = 1, Ri = Ci) and noisy clicks (Ei = 1, Ri (cid:4)= Ci)
under the same query to be below a constant c, i.e., E[Ei =
1, Ri = Ci] > cE[Ei = 1, Ri (cid:4)= Ci]. In other words, we re-
c
quire at least
c+1 clicks should be explained by the relevance
quality of the document rather than a mistake.

4.2.2 Reduce mis-ordered pairs

Pairwise click preference can be easily incorporated via
the PR framework. In this work, we encoded two frequently
employed click heuristics, i.e., skip above and skip next [14],
by the constraints deﬁned below:

φpair(X, Yi)

(11)

(cid:2)

i

φpair(X, Y ) =
(cid:2)

⎧⎨
⎩

=

i

0 if Ei = 0
0 if Ei = 1, Ci = 1 − Ci−1, Ri = Ci, Ri−1 = Ci−1
1 otherwise

and set the left-hand side constant b to be 0 in Eq(6).

The meaning of this constraint is: we only put constraint
over the examined documents (i.e., Ei = 1) where the user
makes diﬀerent decisions in the adjacent positions (i.e., skip
above and skip next).
If the inferred relevance is consis-
tent with the observed click preference (i.e., Ri = Ci and
Ri−1 = Ci−1), such a constraint is inactive; otherwise, if the
inferred relevance preference contradicts the observed click
preference, we need to penalize it.

5. EXPERIMENT RESULTS

As we have discussed most of existing click models treat
the relevance quality of a document to the given query as a
static property, and therefore the evaluation is mostly per-
formed in general web search logs, where the relevance qual-
ity of a document to a query is relatively stable.
In this
work, we are more interested in evaluating the eﬀectiveness

of the click models in a more dynamic search environment,
i.e., news search, where new documents keep emerging, and
existing documents quickly become out-of-date and fall out
of the top ranked results. In such a scenario, we cannot ex-
pect to collect a large number of clicks for each document
before we can make a conﬁdent relevance estimation.
5.1 Data Sets

We collected a large set of real user search logs from Ya-
hoo! news search engine1 in a two months period, from
late May to late July 2011. During this period, a subset of
queries are randomly selected and all the associated users’
search activities are collected, including the anonymized user
ID, query string, timestamp, top 10 returned URL sets and
the corresponding user clicks. In order to unbiasedly com-
pare the relevance estimation performance among diﬀerent
click modeling approaches, we also set up a random bucket
to collect exploration clicks from a small portion of traﬃc at
the same time. In this random bucket, the top four URLs
were randomly shuﬄed and displayed to the real users. By
doing such random shuﬄing, we were able to reduce the
noise from position-bias in the collected user click feedback,
and such feedback can be used as a reliable proxy on infor-
mation utility of documents [16]. Therefore, we only col-
lected the top 4 URLs from this random bucket. In addi-
tion, we also asked editors to annotate one day’s query log
on Aug 9, 2011, into ﬁve-level relevance labels, e.g., “Bad”,
“Fair”, “Good”, “Excellent” and “Perfect”, immediately one
day after to ensure the annotation quality.

Simple pre-processing is applied on these click data sets:
1) ﬁlter out the queries without clicks in the random bucket,
since they are useless for testing purpose; 2) discard the
queries only appearing once in the whole collection; 3) nor-
malizing the relevance features f R
q,d by their mean and vari-
ance estimated on normal click set, i.e., z-score [21]. After
these pre-processing steps, we collected 460k queries from
the normal click set and 378k queries from the random bucket
set. One thing we should note is that because of the way
we set up the random bucket, many queries and documents
might only appear in the random bucket. Existing click
models can hardly estimate the relevance quality of such
unseen documents. In order to make a comprehensive com-
parison, we split the normal click set into two subsets, and
ensure each query is evenly distributed in these two subsets.
We choose one of them for training purpose and another for
testing. The basic statistics of the four data sets used in our
experiment are listed in Table 2.

Table 2: Statistics of evaluation corpus.

# Unique Query # Query

Normal training clicks
Normal testing clicks
Random bucket clicks

Editorial judgment

11,701
6,264
33,762
1,404

234,149
225,452
378,403
13,091

In order to test the model’s generalization capacity, we
further split the queries in the normal click testing set and
random bucket click testing set into diﬀerent categories ac-
cording to their frequencies in the training set. The basic
statistics of those categories are shown in Figure 2. As can
be clearly noticed in the ﬁgure, a large portion of testing

1http://news.search.yahoo.com/

1370queries in the random bucket set belong to the less frequent
query category (62.92% queries are in the <25 category)
comparing to the normal click set (11.49%), which makes
the prediction more diﬃcult in the random bucket set.

n
o

i
t
r
o
p
o
r
P

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Summary of Testing Queries’ Distribution 

Normal Click Set
Random Bucket Click Set

<25

25−100 100−200 200−400 400−800800−1.2k1.2k−1.6k1.6k−3.2k3.2k−6.4k >6.4k

Query Frequency

Figure 2: Distribution of testing queries according
to their frequencies in training set.

5.2 Quality of Relevance Modeling

The main question to be answered in our experiments
is whether the proposed model is more accurate than the
existing click models in terms of relevance estimation. To
answer this question and evaluate the quality of relevance
modeling of the proposed BSS model, we compared it with a
set of state-of-the-art click models, including the counting-
based models of Dynamic Bayesian Model (DBM) [4] and
User Browsing Model (UBM) [8], and feature-based models
of Logistic Regression model and Examination Model [19].
Among them, Logistic Regression model and Examination
Model are trained on the same set of 65 relevance features
f R
q,d as our BSS model.

5.2.1 Evaluation metrics

In previous work [8, 11, 20], perplexity on the testing click
set was often used as the metric for comparing diﬀerent click
models, and it is deﬁned as,

− 1
2

N

(cid:2)N

i=1 δ(ci=1) log2 p(ci=1)+δ(ci=0) log2 p(ci=0)

where N is the number of observations in the testing set.
The lower perplexity a model can achieve, the closer its pre-
diction is to the observation in the testing set.

However, such evaluation metric is problematic for two
major reasons. First, clicks in the testing query log is still
position-biased: a less relevant document appears at a higher
position would still receive more clicks, such that a model
correctly downgrades such a document will even get penal-
ized by the perplexity metric. Second, since perplexity is
deﬁned based on the absolute value of the predicted proba-
bilities, it is inherently sensitive to scaling or normalization
of these probabilities, making it diﬃcult to interpret the re-
sults appropriately.

To examine whether these two concerns are empirically
supported, we included a simple baseline for click modeling,
Naive Click Model (NCM), which only uses the frequency
of clicks on a particular query-document pair observed in
the training set as its relevance estimation.
In Table 3,
we compared NCM’s perplexity against other sophisticated
click models on normal click testing set. And to compare
their relevance estimation quality, we also evaluated their
P@1 ranking performance on the random bucket click set,
which is proved to be an unbiased proxy of document’s rele-

Table 3: Comparison between perplexity metric and
ranking metric.

Examine UBM DBN

BSS

perplexity

P@1

3.9534
0.3807

1.5471
0.3603

1.5719
0.3494

1.8213
0.4033

NCM
1.2925
0.3578

vance quality [16]. Due to space limitation, we did not show
the result from Logistic Regress Model.

From Table 3, we can clearly notice that the naive baseline
outperforms all the other click models in perplexity on the
normal click testing set, but its ranking performance is not
the best on the unbiased random bucket click set. Besides,
we also observed that the perplexity of Examination Model
is signiﬁcantly larger than the other click models. We looked
into the detailed output of Examination Model and found
that its predicted click probabilities (with mean 0.78) are
much larger than the other models’ predictions. Since the
probability of a document being clicked in the normal click
testing set is generally small (with mean 0.14), Examination
Model get seriously penalized by perplexity. However, Ex-
amination Model’s ranking performance is much better than
NCM in the random bucket click set. We thus conclude that
the perplexity calculated based on position-biased clicks is
not a trustable metric for measuring a click model’s capacity
of recognizing relevant documents.

A potentially better measure than perplexity is to directly
compare diﬀerent click models’ ranking performance based
on the estimated relevance of documents. To evaluate rank-
ing performance in a click-based data set, we treat all the
clicked documents as relevant and calculate the correspond-
ing Precision at 1 (P@1), Precision at 2 (P@2), Mean Av-
erage Precision (MAP) and Mean Reciprocal Rank (MRR).
Deﬁnitions of these metrics can be found in standard text-
books in information retrieval (e.g., [2]). And in the editorial
annotation data set, we treated the grade “Good” and above
as relevant for precision-based metrics, and also included the
normalized discounted cumulative gain (NDCG) [12] as an
evaluation metric. Compared with perplexity metric, such
a ranking-based evaluation can better reﬂect the utility of a
click model in estimating the relevance of a document.

We evaluate the quality of relevance estimation of these
models from two diﬀerent perspectives: one is to directly use
the estimated relevance from the click models to rank the
documents; and another is to treat such relevance estimation
as signals for training a learning-to-rank algorithm.

5.2.2 Estimated relevance for ranking

In this approach of evaluation, we ranked the candidate
documents with respect to the estimated relevance given by
a click model, and compared the ranking result against the
logged user clicks. The higher position a click model can put
a clicked document on, the better ranking capability it has.
We performed the comparison on both random bucket click
set and normal testing click set.

We ﬁrst compared diﬀerent models’ P@1 performance on
the random bucket click set in Figure 3 (a), where we illus-
trated the detailed comparison results under each category
of diﬀerent query frequencies. Li et al.
[16] proved that
P@1 metric on this random bucket click set can be used as
an unbiased proxy to measure the relevance of a document
to the given query. And to make a comprehensive compar-
ison, we also performed the same evaluation on the normal
click testing set in Figure 3 (b).

13710.46

0.44

0.42

0.4

0.38

0.36

0.34

1
@
P

0.32

 

<25

 

 

1
@
P

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

 

<25

Logistic Regression
Examine Model
UBM
DBN
BSS

25−100 100−200 200−400 400−800 800−1.2k1.2k−1.6k1.6k−3.2k3.2k−6.4k >6.4k

Query Frequency

Logistic Regression
Examine Model
UBM
DBN
BSS

25−100 100−200 200−400 400−800 800−1.2k1.2k−1.6k1.6k−3.2k3.2k−6.4k >6.4k

Query Frequency

(a) P@1 ranking performance under diﬀerent query fre-
quency categories on the random bucket click set

(b) P@1 ranking performance under diﬀerent query fre-
quency categories on the normal click set

Figure 3: P@1 comparison between diﬀerent click models over random bucket click set and normal click set.

As shown in Figure 3 (a) and (b), in the low query fre-
quency category (query frequency <25), feature-based mod-
els outperformed the counting-based models on both ran-
dom click set and normal click set. For those less frequent
queries, counting-based models do not have enough observa-
tions to get a conﬁdent estimation of a document’s relevance
quality; while by leveraging the information across diﬀerent
observations via the same set of relevance-driven features,
the feature-based models get a more accurate estimation of
relevance for the documents in this category. With more ob-
servations available for a particular query, the relevance esti-
mation quality of counting-based methods improves quickly
and outperforms the simple feature-based methods on both
testing sets. The reason for this slow improvement of sim-
ple feature-based methods is also due to the feature shar-
ing:
in terms of model complexity, counting-based models
have more freedom to tune the parameters for each query-
document pair; while feature-based models have to adjust
the shared feature weights across all the training samples,
such that it cannot arbitrarily ﬁt all the observations. Our
BSS model takes advantages of both counting-based and
feature-based models by combining the perceived relevance,
which is deﬁned by the weighted sum of relevance-driven
features as wRTf R
q,d, and the intrinsic relevance, which is
modeled as query-document dependent parameters wR
q,d, in
In BSS model, wR
a principled optimization framework.
q,d
will be pushed close to zero for the less frequent queries,
since there are no suﬃcient observations to get conﬁdent
estimations for them, and therefore wRTf R
q,d plays a more
important role in estimating relevance. And when we get
more observations for a particular query, wR
q,d is adjusted to
further enhance the relevance estimation, which cannot be
correctly predicted by the shared relevance features.

In Table 4 and Table 5, we list the ranking performance
over all queries in the two testing sets, where a paired two-
samples t-test is performed to validate the signiﬁcance of
improvement from the best performing method against the
runner-up method under each performance metric. Since a
large portion of testing queries in the random bucket click set
belong to the less frequent category (only 29.3% queries ap-
peared more than 100 times in the training set) comparing to
the normal click set (76.7%), it becomes much more diﬃcult
for the purely counting-based methods to make accurate rel-
evance estimation in the random bucket set. As we can ob-

Table 4: Ranking performance on random bucket
click set.

LogisicReg Examine UBM DBN

P@1
P@2
MAP
MRR

0.3696
0.3118
0.5628
0.5754

∗

0.3807
0.3348
0.6094
0.6154

0.3603
0.3239
0.5951
0.6003

0.3494
0.3194
0.5883
0.5939

indicates p-value<0.01

BSS

0.4033∗
0.3509∗
0.6272∗
0.6330∗

Table 5: Ranking performance on normal click set.

LogisicReg Examine UBM DBN

BSS

P@1
P@2
MAP
MRR

0.3623
0.3284
0.5981
0.6037

0.3878
0.3058
0.5643
0.5786

0.5316
0.3703
0.6688
0.6838

0.4273
0.3166
0.5908
0.6047

0.5462+
0.3823+
0.6804+
0.6964+

+ indicates p-value<0.05

serve in the results, although the counting-based UBM and
DBN methods achieved better ranking performance than the
simple feature-based models in the normal click set, their
performance degraded on the random bucket set, due to the
lack of observations. And as we have discussed earlier, by
leveraging the feature-based and counting-based relevance
estimations, BSS model outperformed all the other baseline
methods on both data sets.

5.2.3 Estimated relevance as signals for learning-to-

rank algorithm training

In this evaluation, we used the estimated relevance given
by a click model as labels to extract ranking preference pairs
of documents for training a learning-to-rank algorithm. We
employed the pairwise RankSVM [13] as our basic learning-
to-rank algorithm.

To stimulate the situation where we have to make pre-
diction over some new documents before we can collect suf-
ﬁcient training clicks for each query, e.g., in news search,
we only sampled 30% query log from each day in the nor-
mal training click set in this experiment. We estimated the
click models on the new training set and generated the click
preference pairs according to the predictions of each click
model on this set. In particular, we ordered the documents
under a given query according to their predicted relevance
from a click model, and treated the top ranked document

1372Per−Query Log−likelihood Updating During EM Iterations

P@1 Ranking Performance Updating During EM Iteration

−4.2

−4.4

−4.6

−4.8

−5

−5.2

−5.4

−5.6

d
o
o
h

i
l

i

e
k
L
−
g
o
L

 

Zero Initialization
+Dampen Noise Constraint
+Pairwise Preference Constraint
Manual Initialization

 
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

Iteration Steps

0.7

0.6

0.5

0.4

1
@
P

0.3

0.2

0.1

 
1

 

Zero Initializaton
+Dampen Nosie Constraint
+Pairwise Preference Constraint
Manual Initialization

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

Iteration Steps

(a) Per-query Log-likelihood updates

(b) P@1 ranking performance on training set updates

Figure 4: EM algorithm updating traces with diﬀerent training settings.

Table 6: RankSVM performance on random bucket
click set with diﬀerent training signals.

ori. click UBM DBN

BSS

P@1
P@2
MAP
MRR

0.3275
0.3774
0.3081
0.3380
0.5724
0.6093
0.5779
0.6151
+ indicates p-value<0.05

0.3823
0.3411
0.6130
0.6187

0.3869+

0.3411

0.6146+
0.6206+

Table 7: RankSVM performance on editorial judg-
ments with diﬀerent training signals.

ori. click UBM DBN

BSS

P@1
P@2
MAP
MRR
NDCG@1
NDCG@5

0.5288
0.4808
0.6173
0.6658
0.4570
0.5731
+ indicates p-value<0.05

0.6346
0.5313
0.6944
0.7546
0.5902
0.6830

0.6250
0.5016
0.6905
0.7545
0.5774
0.6832

0.6442+
0.5913+
0.7212+
0.7719+
0.6016+
0.7181+

as positive and others as negative. The preference pairs are
extracted according to this predicted relevance labels under
each query and fed into a RankSVM model. In addition, we
also included a RankSVM trained on the preference pairs
generated by the original clicks with the skip-above and skip-
next click heuristics [14] in this training set as a baseline.

We compared the performance of RankSVM models trained
by diﬀerent relevance signals on both random bucket click
set and editorial annotation set. In this experiment, we only
included the UBM and DBN as the baseline click models
since they performed much better than the simple feature-
based Logistic Regression model and Examination Model on
the normal click set according to Table 5.

As shown in Table 6 and Table 7, the training signals ex-
tracted from click models’ output led to much better ranking
performance of RankSVM than those extracted based on the
simple click heuristics. In addition, though the RankSVM
models trained on purely counting-based click models’ out-
put have comparable P@1 and NDCG@1 performance as
that trained on BSS model’s output, their predictions on
the lower positions are much worse, e.g., lower MAP and
NDCG@5. The main reason is that traditional click mod-
els only work in a pointwise way, and they cannot directly
optimize the relative order of the predicted relevance; while
in the proposed BSS model, we incorporated such ranking-

R=1, wE

R=0, wE

oriented property via the pairwise preference constraint, which
renders BSS model better capability of distinguishing the
relative order among the candidate documents. As a result,
the training signals extracted from BSS model’s output are
more informative for learning to rank algorithm training.
5.3 Effectiveness of Posterior Regularization
We now examine the eﬀectiveness of a key component in
the proposed BSS model, i.e., posterior regularization. As
discussed in Section 4, there are two motivations for apply-
ing posterior regularization: one is to address the problem
of “unidentiﬁability” in the proposed BSS model, and the
other is to incorporate pairwise ranking preferences into click
modeling. Below we validate the eﬀectiveness of posterior
regularization in achieving these two goals.

We ﬁrst initialized all the model parameters, i.e., {wR,
R=0}, to be zero in our EM algorithm.
R=0, wC

wC
We refer to this baseline as “zero initialization”. And based
on this initialization, we sequentially added the noise damp-
ening constraint and pairwise preference constraint into the
model to obtain two runs of model estimation using posterior
regularized EM algorithm. An alternative way for solving
the “unidentiﬁability” problem is to set priors over the model
parameters such that we can guide the model to search in a
desirable region. However, the diﬃculty of this approach for
our BSS model is that it is unclear how to set proper pri-
ors on the model parameters for directly manipulating the
probability P (C|E, R), since this probability is deﬁned over
a set of diﬀerent features via a logistic function. Thus to
compare with such an approach, we manually set the initial
value for the bias term in wC
R=1 to be 1,
reﬂecting the assumption that most of the clicks should be
explained by the relevance quality of a document rather than
noise. We refer to this baseline as “manual initialization.”

R=0 to be -1 and in wC

We plotted the per-query log-likelihood update trace and
the corresponding P@1 ranking performance on the train-
ing set during EM iterations for these four diﬀerent ways of
learning our BSS model in Figure 4(a) and 4(b).
Eﬀect of dampen noise constraint: as we can clearly
observe from the EM update trace that without any spe-
ciﬁc parameter initialization or posterior regularization, EM
failed to ﬁnd a conﬁguration which could improve the log-
likelihood over the all-zero initialized model. Manual ini-
tialization helped the model identify better conﬁgurations.
However, it is not a principled way for achieving so, and the
scale of such hard-coded setting will directly bias the learned

1373model. The proposed dampen noise constraint serves for the
same purpose as manual initialization, but it gives model
the freedom to learn such scale from data. As we can ﬁnd
from the updating trace, such constraint successfully led the
model to a better conﬁguration for both log-likelihood and
P@1 ranking performance than the manual initialization.
Eﬀect of pairwise preference constraint: with the pair-
wise preference constraint, which aims to enforce ranking-
oriented requirement, the model’s ranking capability is fur-
ther improved, even though the log-likelihood did not gain
too much. This is expected: log-likelihood deﬁned in Eq (5)
only considers the pointwise relevance estimation of each
query-document pair, which does not count the relative or-
der of relevance among the documents under the same query.
The pairwise preference constraint explores such knowledge,
which eﬀectively improves ranking accuracy as shown in Fig-
ure 4(b). And we should note that such knowledge can
hardly be encoded by manual initialization. Therefore, with
the pairwise preference constraint, we solved the deﬁciency
of traditional click models that the dependency relation among
the clicked/skipped documents is discarded, and we are able
to leverage the knowledge about pairwise preferences to fur-
ther improve the relevance estimation accuracy.
5.4 Understanding User Behaviors with BSS

An interesting additional beneﬁt of the proposed BSS model

is that the learned feature weights reveal the inﬂuence of dif-
ferent factors on users’ click behaviors, which is not available
in existing click models. To explore this beneﬁt, we list a
subset of learned feature weights in Table 8.

Table 8: Feature weights learned by BSS model.

title match

abs. match

0.098

0.167

bias
-4.654
4.405

bias
5.325
3.266

f R
wR

f C

wC
wC

R=0
R=1
f E

wE
wE

R=0
R=1

age

-0.839

pos

-1.133
0.149

pos
1.807
-1.381

authority

0.017

dis. to last click

query length

-0.445
0.415

# click
-0.418
0.665

-3.659
3.707

avg cont. sim.

2.947
-2.237

The weights learned by our BSS model followed our intu-
ition about their eﬀects in inﬂuencing user’s click decisions.
For example, “age” is an important factor in news search:
the most recent document (shorter age) is always preferred.
Our BSS model correctly identiﬁed this negative correla-
tion and put a relatively large weight over the age feature.
The most interesting discovery by our model is the weights
learned for the position feature in the click and examine
events.
In the click event, when the current document is
irrelevant (i.e., R = 0), the weight for the position feature is
quite negative, which indicates that a user is very likely to
click on an irrelevant document when its displayed position
is on the top, i.e., position-bias. But when the document
is relevant (i.e., R = 1), the corresponding weight is closer
to zero, which means user’s click decision is not aﬀected by
the displayed positions of relevant documents. And in the
examine event, the weight for the position feature is largely
positive when the current document is irrelevant, which in-
dicates users tend to further examine lower positions since
they have not found satisfactory results. But when the cur-
rent document is relevant, the weight becomes largely neg-

ative, which means users are inclined to stop further exam-
ination since their information need has been met by the
relevant documents.

6. RELATED WORK

Richardson et al.

No previous work directly addressed the two deﬁciencies
of existing click models, i.e., ignoring rich information con-
veyed in the document content when modeling clicks, and
failing to exploit the relative order of relevance among the
clicked/skipped documents. But there are several studies
touched the problem of utilizing features in click modeling.
[19] were the ﬁrst to derive a content-
based logistic regression model for predicting click throught
rate by discounting the bias in lower positions via a position-
speciﬁc multiplicative factor. However, they treated such
discount factor as constant, which was only determined by
positions, and thus it was independently estimated without
considering the speciﬁc displayed documents and the related
clicks. In [22], the authors also considered to introduce ad-
ditional features to model click events; however, they used
empirically tuned linear interpolation to combine the esti-
mated relevance by a click model with external signals (e.g.,
BM25). Our method provides a more principled way for
introducing rich descriptive features to formalize the depen-
dency structure for both click and examine events within a
query, and learn the optimal combination of those features
from the data. Zhu et al. [23] realized the necessity of incor-
porating features into click models. However, they used the
same set of general features (e.g., time and length of URL)
to describe both click and examine events without distin-
guishing their speciﬁc eﬀect in these two diﬀerent events.

7. CONCLUSIONS

Click modeling is an important technique for exploiting
search log data and is a crucial component in modern Web
search engines. In this work, we proposed a general Bayesian
Sequential State (BSS) model for addressing two deﬁcien-
cies of existing click models, namely failing to utilize doc-
ument content information for modeling clicks and not be-
ing optimized for distinguishing the relative order of rele-
vance among the candidate documents. As our solution,
a set of descriptive features and ranking-oriented pairwise
preferences are encoded via a probabilistic graphical model,
where the dependency relations among a document’s rele-
vance quality, examine and click events under a query are
automatically captured from the data. Experiments on a
large set of news search logs validate the eﬀectiveness of the
proposed BSS model comparing to several state-of-the-art
click models, where content-based features help BSS model
leverage information across diﬀerent observations when the
training set is limited, and pairwise preference constraint
gives the model a more accurate estimate of relevance.

As we have shown in the experiment, the proposed BSS
model provides an interesting way of understanding user’s
click behaviors by analyzing the learned weights on diﬀer-
ent features. With appropriate feature design, our model
has the potential to help understand user behavior in var-
ious other aspects as well. As our future work, it would
be meaningful to incorporate user-related features into our
model, i.e., personalized BSS model, where diﬀerent users
will have their own weights over the designed features to
reﬂect their unique search intents.

13748. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno.

Learning user interaction models for predicting web
search result preferences. In Proceedings of the 29th
annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 3–10. ACM, 2006.

[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern

information retrieval, volume 463. ACM press New
York, 1999.

[3] D. Bertsekas. Nonlinear programming. Athena

Scientiﬁc, 1999.

[4] O. Chapelle and Y. Zhang. A dynamic bayesian
network click model for web search ranking. In
Proceedings of the 18th international conference on
World wide web, pages 1–10. ACM, 2009.

[5] W. Chen, D. Wang, Y. Zhang, Z. Chen, A. Singla, and
Q. Yang. A noise-aware click model for web search. In
Proceedings of the ﬁfth ACM international conference
on Web search and data mining, pages 313–322. ACM,
2012.

[6] C. L. Clarke, M. Kolla, G. V. Cormack,

O. Vechtomova, A. Ashkan, S. B¨uttcher, and
I. MacKinnon. Novelty and diversity in information
retrieval evaluation. In Proceedings of the 31st annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 659–666,
2008.

[7] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An
experimental comparison of click position-bias models.
In Proceedings of the international conference on Web
search and web data mining, pages 87–94. ACM, 2008.

[8] G. E. Dupret and B. Piwowarski. A user browsing

model to predict search engine click data from past
observations. In Proceedings of the 31st annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 331–338.
ACM, 2008.

[9] J. Gra¸ca, K. Ganchev, and B. Taskar. Expectation

maximization and posterior constraints. Advances in
Neural Information Processing Systems, 20:569–576,
2007.

[10] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking

analysis of user behavior in www search. In
Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 478–479. ACM, 2004.
[11] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor,

Y.-M. Wang, and C. Faloutsos. Click chain model in
web search. In Proceedings of the 18th international
conference on World wide web, pages 11–20. ACM,
2009.

[12] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Transactions on
Information Systems (TOIS), 20(4):422–446, 2002.

[13] T. Joachims. Optimizing search engines using

clickthrough data. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 133–142. ACM,
2002.

[14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 154–161.
ACM, 2005.

[15] D. Koller and N. Friedman. Probabilistic graphical
models: principles and techniques. The MIT Press,
2009.

[16] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased

oﬄine evaluation of contextual-bandit-based news
article recommendation algorithms. In Proceedings of
the fourth ACM international conference on Web
search and data mining, pages 297–306. ACM, 2011.

[17] D. C. Liu and J. Nocedal. On the limited memory bfgs

method for large scale optimization. Mathematical
programming, 45(1):503–528, 1989.

[18] R. Neal and G. Hinton. A view of the em algorithm
that justiﬁes incremental, sparse, and other variants.
Learning in Graphical Models, 89:355–368, 1998.
[19] M. Richardson, E. Dominowska, and R. Ragno.

Predicting clicks: estimating the click-through rate for
new ads. In Proceedings of the 16th international
conference on World Wide Web, pages 521–530. ACM,
2007.

[20] S. Shen, B. Hu, W. Chen, and Q. Yang. Personalized

click model through collaborative ﬁltering. In
Proceedings of the ﬁfth ACM international conference
on Web search and data mining, pages 323–332. ACM,
2012.

[21] Wikipedia. Standard score.

http://en.wikipedia.org/wiki/Standard_score.
[22] Y. Zhang, D. Wang, G. Wang, W. Chen, Z. Zhang,

B. Hu, and L. Zhang. Learning click models via probit
bayesian inference. In Proceedings of the 19th ACM
international conference on Information and
knowledge management, pages 439–448. ACM, 2010.

[23] Z. A. Zhu, W. Chen, T. Minka, C. Zhu, and Z. Chen.

A novel click model and its applications to online
advertising. In Proceedings of the third ACM
international conference on Web search and data
mining, pages 321–330. ACM, 2010.

1375