Questions about Questions: An Empirical Analysis of

Information Needs on Twitter

Zhe Zhao

Department of EECS
University of Michigan

zhezhao@umich.edu

ABSTRACT
Conventional studies of online information seeking behavior
usually focus on the use of search engines or question an-
swering (Q&A) websites. Recently, the fast growth of online
social platforms such as Twitter and Facebook has made it
possible for people to utilize them for information seeking by
asking questions to their friends or followers. We anticipate
a better understanding of Web users’ information needs by
investigating research questions about these questions. How
are they distinctive from daily tweeted conversations? How
are they related to search queries? Can users’ information
needs on one platform predict those on the other?

In this study, we take the initiative to extract and analyze
information needs from billions of online conversations col-
lected from Twitter. With an automatic text classiﬁer, we
can accurately detect real questions in tweets (i.e., tweets
conveying real information needs). We then present a com-
prehensive analysis of the large-scale collection of informa-
tion needs we extracted. We found that questions being
asked on Twitter are substantially diﬀerent from the top-
ics being tweeted in general. Information needs detected on
Twitter have a considerable power of predicting the trends
of Google queries. Many interesting signals emerge through
longitudinal analysis of the volume, spikes, and entropy of
questions on Twitter, which provide insights to the under-
standing of the impact of real world events and user behav-
ioral patterns in social platforms.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Text Mining

General Terms
Experimentation, Empirical Studies

Keywords
Information Need, Time Series Analysis, Twitter

1.

INTRODUCTION

Recent years have witnessed an explosion of user-generated
content in social media. Online social platforms such as
Facebook, Twitter, Google+, and YouTube have been com-
plementing and replacing traditional platforms in many daily

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

Qiaozhu Mei

School of Information
University of Michigan
qmei@umich.edu

tasks of Web users, including the creation, seeking, diﬀusion,
and consumption of information. Indeed, a very recent re-
search interest has been shown in understanding how people
seek for information through online social networks [26, 8,
29, 28, 21], how this “social information seeking” behavior
diﬀers from that through traditional channels such as search
engines or online question answering (Q&A) sites, and how
the social channel complements these channels [27, 16, 25].
Based on a survey conducted by Morris et al. in 2010, 50.6%
of the respondents1 reported having asked questions through
their status updates on social networking sites [26]. The
questions they ask involve various needs of recommenda-
tions, opinions, factual knowledge, invitations and favor, so-
cial connections, and oﬀers. They covered many topics such
as technology, entertainment, shopping, and professional af-
fairs [26]. An analysis later by Efron and Winget suggested
that 13% of a random sample of tweets (microblogs posted
on Twitter.com) were questions [8].

Why is it compelling to understand the questions asked on
social platforms? This emerging research interest largely at-
tributes to the importance of understanding the information
needs of Web users. Indeed, as the core problem in infor-
mation retrieval, a correct interpretation of the information
needs of the users is the premise of any automatic system
that delivers and disseminates relevant information to the
users. It is the common belief that the analysis of users’ in-
formation needs has played a crucial role behind the success
of all major Web search engines and other modern informa-
tion retrieval systems. Better understanding and prediction
of users’ information needs also provides great opportunities
to business providers and advertisers, leading to eﬀective
recommender systems and online advertising systems.

Long have Web search engines been the dominating chan-
nel of information seeking on the Web. According to re-
cent statistics 2, 4 billion of search queries are submitted
to Google every day. The rest of the territory is shared
by other channels such as online question answering (Q&A)
sites such as Yahoo! Answers. A statistic in 2010 reported
a daily volume of 823,966 questions and answers 3, in which
each question on average earned ﬁve to six answers according
to [30]. This is much smaller than the number of information
needs asked through search engines.

1Note for the selection bias as all were Microsoft employees.
2http://www.comscore.com/Insights/Press_Releases/
2012/4/comScore_Releases_March_2012_U.S._Search_
Engine_Rankings
3http://yanswersblog.com/index.php/archives/2010/
05/03/1-billion-answers-served/

1545The emergence of social platforms seems to be a game-
changer. If the ratio reported by Efron and Winget [8] still
holds today, there will be over 50 million questions asked
through Twitter according to a recent statistic of 400 mil-
lion tweets posted per day 4. This number, although still
far behind the number of search queries, has already over-
whelmed the number of questions in traditional Q&A sites.
Moreover, it has been found that people tend to ask diﬀer-
ent questions to their friends rather than to search engines
or to strangers on Q&A sites. In Figure 1, we can see peo-
ple asking questions in their tweets by either broadcasting
so that any of their followers can respond to them, or by
targeting the question to particular friends. The results of
the survey by Morris et al. suggested that respondents es-
pecially prefer social sites over search engines when asking
for opinions and recommendations, and they tend to trust
the opinions of their friends rather than strangers on Q&A
sites [26]. It is reported in [27] that users enjoy the beneﬁts
of asking their social networks when they need personalized
answers and relevant information that unlikely exists pub-
licly on the Web. It is also reported that information needs
through social platforms present a higher coverage of topics
related to human interest, entertainment, and technology,
compared to search engine queries [29].

Figure 1: Instances of tweets conveying an informa-
tion need, and those which don’t.

All evidence suggests that the questions being asked through

social networks present a completely new perspective of on-
line information seeking behaviors. By analyzing this emerg-
ing type of behavior, one anticipates to help users eﬀectively
fulﬁll their information needs, to develop a new paradigm of
search service that bridges search engines and social net-
works (e.g., social search [9, 25]), and to predict what the
users need in order to strategize information service provi-
sion, persuasion campaign, and Internet monetization.

The availability of large scale user-generated content in
social network sites has provided a decent platform for this
kind of analysis. This revives our memory about the early
explorations of analyzing search engine query logs (e.g., [31,
32, 23]).
Indeed, the analysis of information needs with
large-scale query logs has provided tremendous insights and
features to researchers and practitioners, and it has led to a
large number of novel and improved tasks including search
result ranking [1], query recommendation [2], personaliza-

4http://news.cnet.com/8301-1023
hits-400-million-tweets-per-day-mostly-mobile/

3-57448388-93/twitter-

tion [33], advertising [4], and various prediction tasks [14,
17]. We believe that a large-scale analysis of information
needs on online social platforms will reproduce and comple-
ment the success of query log analysis, the results of which
will provide valuable insights to the design of novel and bet-
ter social search and other online information systems.

In this paper, we take the initiative and present the ﬁrst
very large scale and longitudinal study of information needs
in Twitter, the leading microblogging site. Questions that
convey information needs are extracted from a collection
of billions of microblogs (i.e., tweets). This is achieved
by an automatic text classiﬁer that distinguishes real ques-
tions (i.e., tweets conveying real information needs) from
tweets with question marks. With this dataset, we are able
to present a comprehensive description of the information
needs with both the perspectives of content analysis and
trend analysis. We ﬁnd that questions being asked on Twit-
ter are substantially diﬀerent from the content being tweeted
in general. We prove that information needs detected on
Twitter have a considerable power of predicting the trends
of search engine queries. Through the in-depth analysis of
various types of time series, we ﬁnd many interesting pat-
terns related to the entropy of language and bursts of in-
formation needs. These patterns provide valuable insights
to the understanding of the impact of bursting events and
behavioral patterns in social information seeking.

The rest of the paper is organized as follows. We start
by introducing the related work. The setup and dataset of
our experiments is presented in Section 3, followed by the
description of an automatic classiﬁer of information needs
in Section 4. In Section 5, we describe the detailed results
and insights drawn from the analysis of the large collection
of information needs. We then conclude in Section 6.

2. RELATED WORK

To the best of our knowledge, this is the ﬁrst work to
detect and analyze information needs from billion level, lon-
gitudinal collection of tweets. Our work is generally related
to the qualitative and quantitative analysis of information
seeking through social platforms (e.g., [29, 5, 27]) and tem-
poral analysis of online user behaviors (e.g., [3, 14]).
2.1 Questions on Social Platforms

As described in Section 1, there is a very recent interest in
understanding how people ask questions in social networks
such as Facebook and Twitter [5, 26, 8, 29, 28, 21]. This
body of work, although generally based on surveys or small
scale data analysis, provides insights to our large-scale anal-
ysis of information needs in Twitter. For example, In [29],
the authors labeled 4,140 tweets using Mechanical Turks and
analyzed 1,351 of them which were labeled as real questions.
They presented a rich characterization of the types and top-
ics in these questions, the responses to these questions, and
the eﬀects of the underlying social network. In [26, 27, 36],
Morris et al. surveyed whether and how people ask ques-
tions through social networks, the diﬀerences between these
questions and questions asked through search engines, and
how diﬀerent cultures inﬂuence the behaviors. Efron and
Winget further conﬁrmed this diﬀerence with a study of
375,509 tweets. Using a few simple rules, they identiﬁed
13% of these tweets as questions. They also provided pre-
liminary ﬁndings on how people react to questions.

Tweets	  Conveying	  	  Informa3on	  Need	  Tweets	  not	  Conveying	  	  Informa3on	  Need	  Do	  you	  know	  whether	  there	  is	  a	  roadwork	  on	  I94	  Man	  so	  everybody	  a	  frank	  ocean	  fan	  now?	  Idc	  I	  was	  an	  original…	  Which	  restaurant	  nearby	  has	  a	  discount?	  Why	  do	  I	  always	  do	  this?	  #hesatool	  #fml	  @someuser	  u	  work	  today???	  @someuser	  how	  are	  you?	  Can	  anyone	  suggest	  some	  local	  restaurants	  in	  Beijing?	  They’re	  sFll	  together,	  why	  haven’t	  they	  broken	  up	  yet?!?!	  @someuser,	  do	  you	  what	  I	  am	  doing	  is	  good?	  Umm	  what?	  It’s	  already	  August?	  Hey	  Summer,	  #wheredygo?	  What’s	  your	  favorite	  summer	  album	  to	  throw	  on	  a	  car	  stereo?	  Im	  sFll	  gone	  smile!	  What	  are	  you	  thanking?!	  Em	  not	  Is	  my	  avi	  cute?	  Why	  won’t	  people	  understand	  that?!	  1546More sophisticated methods have been proposed to de-
tect questions in online forums and Q&A sites [6, 35]. A
recent work [19] studied the same problem in the context of
Twitter, which presented a classiﬁer that achieved 77.5% of
accuracy in detecting questions from tweets. A much more
accurate classiﬁer is needed, however, to analyze information
needs at a very large scale.

It is interesting to see the eﬀort of making use of the un-
derstandings of social information seeking. In [16], Morris et
al. proposed SearchBuddies, an automatic content recom-
mendation for information seeking behavior. The proposed
work ﬁnds relevant content based on the content and social
context of Facebook status asking for information. Such
eﬀort can also be found in work like [9, 27, 34], where a
new paradigm of search service, social search, is discussed.
These explorations provided good motivations to our eﬀort
of large-scale analysis of information needs on social plat-
forms.
2.2 Temporal Analysis of User Activities

The techniques of analysis used in our work is related to
the existing work of analyzing user behaviors in general. For
example, in [3], the authors proved that sentiment trends in
Twitter has a power of predicting the Daw Jones Industrial
Average. In their approach, the Granger Causality Test is
used to test this predictive power. In [14], the authors used
the Google trend related to inﬂuenza spread worldwide to
detect which stage the ﬂu was at and to predict the trend
of the ﬂu. Our analysis provides another important applica-
tion of these methods. Note that our analysis is also related
to the analysis of large scale search engine logs (e.g., [31,
32, 23]). Indeed, we do anticipate the analysis of informa-
tion needs in social platforms to complement the analysis
of information needs through search engines, and provide
a totally diﬀerent perspective and insights to search engine
practitioners.

3. EXPERIMENT SETUP

We analyze a longitudinal collection of microblogs (tweets)
collected through the Twitter stream API with Gardenhose
access, which collects roughly 10% of all public statuses on
Twitter. The collection covers a period of 358 days, from
July 10th, 2011 to June 31st 2012. A total number of
4,580,153,001 (12.8 million tweets per day) tweets are in-
cluded in this collection, all of which are self-reported as
tweets in English. Every tweet contains a short textual mes-
sage constrained by 140 characters, based on which we de-
termine whether it conveys an information need. For every
tweet, we keep the complete metadata such as the user who
posted the tweet, the time stamp at which it was posted,
and geographical locations of the user if provided. In the
analysis in this paper, we adopt only the time and user in-
formation but leave the richer metadata for future analysis.
Note that a tweet may be a retweet of an existing tweet,
may mention one or more users by “@” their usernames, and
may contain one or more hashtags (user-deﬁned keywords
starting with an “#”). In our analysis, we keep the original
form of all hashtags, but de-identify all usernames mentioned
in the tweets (e.g., substituting all of them with a token
“@someuser”).

To analyze information needs in these tweets, we focus on
tweets that appear to be questions. Speciﬁcally, we focus on
tweets that contain at least one question mark. Note that

this treatment could potentially miss information needs that
are presented as statements. According to statistics in [26],
81.5% of information needs asked through social platforms
were explicitly phrased as questions and included a question
mark. Questions phrased as statements were often preceded
by inquisitive phrases like “I wonder,” or “I need” [26]. Be-
cause there is little foreseeable selection bias, we choose to
focus on questions with explicit question marks instead of
enumerating these arbitrary patterns in an ad hoc manner.
In our collection of tweets, 10.45% of tweets contain explicit
appearance of question mark(s).
4. DETECTING INFORMATION NEEDS

Not all tweets with question marks are real questions. In
order to detect information needs from tweets collected in
Section 3, we need to distinguish tweets that convey a real
information need from many false positives such as rhetorical
questions, expressions of sentiments/mood, and many other
instances. Figure 1 presents examples of tweets that convey
real information needs and those which don’t.

In this section, we present the task of detecting informa-
tion needs from tweets which is casted as a text classiﬁcation
problem. Given a tweet that contains one or two question
marks, the task is to determine whether it expects an infor-
mational answer or not. In this section, we ﬁrst give a formal
deﬁnition of this problem and rubrics based on which human
annotators can accurately classify a tweet. A qualitative
content analysis is conducted in order to develop a code-
book of classiﬁcation and generate a set of labeled tweets
as training/testing examples. We then introduce a classiﬁer
trained with these examples, using the state-of-the-art ma-
chine learning algorithms and a comprehensive collection of
features. The performance of the text classiﬁer is evaluated
and presented in Section 4.5.
4.1 Deﬁnition and Rubrics

Given a tweet with question marks, our task is to deter-
mine whether this tweet conveys a real information need or
not (i.e., real questions). A formal deﬁnition is needed to de-
scribe what we mean by “a real information need.” Inspired
by the literature of how people ask questions on Twitter
and Facebook [29, 27], we provide the following deﬁnition
and rubrics of “real questions:”

A tweet conveys an information need, or is a real ques-
tion, if it expects an informational answer from either the
general audience or particular recipients.

Therefore, a tweet conveys an information need if
• it requests for a piece of factual knowledge,
or a conﬁrmation of a piece of factual knowl-
edge. A piece of factual knowledge can be phrased
as a claim that is objective and fact-checkable (e.g.,
“Barack Obama is the 44th president of the United
States”).

• it requests for an opinion, idea, preference, rec-
ommendation, or personal plan of the recipi-
ent(s), as well as a conﬁrmation of such infor-
mation. Here the information been requested is sub-
jective, which is not fact checkable at the present.

A tweet does not convey an information need if it doesn’t
expect an informational answer. This includes rhetorical

1547questions, expressions of greeting, summary of the content
(eye attractors), imperial requests (to be distinguished from
invitations), sarcasm, humor, expressions of emotion (com-
plaints, regrets, anger, etc), or conversation starters.

Figure 1 shows some examples of tweets conveying infor-
mation need and tweets which don’t. Using the description
we proposed above, a human annotator can easily classify
a tweet. In the following subsections, we introduce how we
extract features from the tweets, how we select features us-
ing the state-of-the-art feature selection techniques, and how
we train classiﬁers using a single type of feature and then
combine them using boosting.
4.2 Human Annotation

Based on the rubrics, we developed a codebook5 and re-
cruited two human annotators to label a random sample of
tweets. We sampled 5,000 tweets randomly from our col-
lection, each of which contains at least one question mark
and self-reported as English. Finally, 3,119 tweets are la-
beled as real tweets in English and have same labels by the
two coders. Among the 3,119 tweets, 1,595 are labeled as
conveying an information need and 1,524 are labeled not
conveying an information need. The inter-rater reliability
measured by Cohen’s kappa score is 0.8350, the proportion
of the agreements in all the results is 91.5%. The 3,119 la-
beled tweets will be used to train and evaluate the classiﬁer
of information needs.
4.3 Text Classiﬁcation
4.3.1 Feature Extraction
The classiﬁcation of tweets is a particularly challenging
because of the extremely short length of content (i.e., a tweet
has a limited length of 140 characters). This makes the tex-
tual features in an individual tweet extremely sparse. To
overcome this challenge, we not only utilize lexical features
from the content of the tweets, but also generalize them
using the semantic knowledge base WordNet [24, 10].
It
is also our intent to include syntactical features as well as
metadata features. We extracted four diﬀerent types of fea-
ture from each tweet, i.e., lexical ngrams, synonyms and
hypernyms of words (obtained from the WordNet), ngrams
of the part-of-speech (POS) tags, and light metadata and
statistical features such as the length of the tweet and cov-
erage of vocabulary( i.e., number of diﬀerent words used in a
tweet divided by the number of diﬀerent words in the whole
dataset ), etc..
Lexical Features

We included unigrams, bigrams, as well as trigrams. The
start and end of a tweet are also considered in the ngrams.
This gives us great ﬂexibility to capture features that reﬂects
the intuitions from qualitative analysis. For example tweets
beginning with the 5Ws (who, when, what, where, and why)
are more likely to be real questions. All lexical features are
lowercased and stemmed using the Krovetz Stemmer [18].
Hashtags are treated as unique keywords. To eliminate the
noise of low frequent words, a feature is dropped if it appears
less than 5 times. This resulted in 44,121 lexical features.
WordNet Features

To deal with the problem of data sparsity, we attempt to
generalize the lexical features using the synonyms and the

5The codebook is made available at http://www-personal.
umich.edu/~zhezhao/projects/IN/codebook.html

hypernyms of the words in tweets. We hope this approach
would connect diﬀerent features sharing relevant semantics
in diﬀerent tweets. By doing this, our algorithm can also
handle words that haven’t been seen in the training data,
thus is anticipated to achieve a higher performance with
limited training data.

In [22], the authors studied how diﬀerent types of relevant
words from WordNet inﬂuence the results of text classiﬁca-
tion. In most cases, using only synonyms and hypernyms can
improve classiﬁers such as Support Vector Machine (SVM)
the most. We explored diﬀerent WordNet features in our
task and drew the same conclusion. We therefore adopt only
synonyms and hypernyms of words in a tweet as additional
features. Note here we actually excluded this semantic gen-
eralization for nouns in a tweet. This is because our task is
to discover patterns of how people ask questions, instead of
what they ask. 23,277 WordNet features are extracted.
Part-of-Speech Features

Compared to a statement, questions present special pat-
terns of syntactic structure. Therefore we attempt to include
syntactic features into consideration. Syntactic parsing of
billions of tweets appears to be costly and probably unnec-
essary, since the quality of parsing is compromised given the
inaccurate use of language in social media. We thus seek for
features that capture light syntactic information. We ﬁrst
obtain part-of-speech of the words in a tweet, and then ex-
tract ngrams of these part-of-speech tags. That is, given a
tweet with n words, w1, w2, . . . , wn, we extract grams from
the part-of-speech sequence of the tweet, is t1, t2, . . . , tn, and
then extract unigrams, bigrams and trigrams from this part-
of-speech sequence as additional features of the tweet. 3,902
POS features are extracted in total.
Meta Features

We also include 6 metadata features and simple statistical
features of the tweet such as the length of the tweets, the
number of words, the coverage of vocabulary, the number of
capitalized words, whether or not the tweet contains a URL,
and whether or not it mentions other users. We believe these
features are possibly indicative of questions.

4.3.2 Feature Selection
The four types of extracted features represent each tweet
as a vector with a very large number of dimensions. This is
not surprising given the huge and open vocabulary in Twit-
ter. Even though we can reduce the number of features by
various heuristics of post-processing, the number of features
remaining is still far larger than the number of training ex-
amples. Therefore, it is essential to conduct feature selection
and further reduce the dimensionality of the data.

In this paper, we adopt the state-of-the-art feature selec-
tion method named Bi-Normal Separation (BNS) proposed
in [11]. In this work, the author proved that the proposed
metric for feature selection outperformed other well-known
metric such as Information Gain and Chi-distance. Specif-
ically, let tp and tn be the number of positive cases with
and without a given feature, f p and f n be the number of
negative cases with and without the feature. Let tpr be the
sample true positive ratio (i.e., tpr = tp/(tp + f n)) and f pr
be the sample false positive ratio (i.e., f pr = f p/(f p + tn)).

The BNS metric of a given feature can be calculated by

(cid:107)F

−1(tpr) − F

−1(f pr)(cid:107),

(1)

where F is the Normal cumulative distribution function.

15484.4 Training Classiﬁer

After feature selection, we move forward and train four
independent classiﬁers using the Support Vector Machine
(SVM) [7], based on each of the four types of features. We
then combine the four classiﬁers that represent four types
of features into one stronger classiﬁer using boosting. This
is done through the Adaptive Boosting method called Ad-
aboost [12].

Adaboost is an eﬀective algorithm that trains a strong
classiﬁer based on several groups of weak classiﬁers. Usu-
ally Adaboost can obtain one classiﬁer better than any of
the weak classiﬁers. However, when the performances of the
weak classiﬁers are higher than a certain level, it is hard
to use this algorithm to generate a better classiﬁer. This
situation seems to apply to our scenario, since the SVM
classiﬁers are suﬃciently strong. In [20], the authors indi-
cated that the reason why this problem occurs is that after
several iterations, when the combination of weak classiﬁers
starts to achieve a higher performance, the diversity inside
the combination is getting lower. That says, new weak clas-
siﬁers are likely to make same predictions as the old ones.
To solve this problem, they add a parameter to control for
the diversity of the weak learners in each iteration. We aslso
adopt this technique to combine the four SVM classiﬁers.

We deﬁne parameter div as the threshold of a minimum
diversity of a new weak classiﬁer to be added in each iter-
ation in the Adaboost. The diversity that a new classiﬁer
could add in iteration t is deﬁned as follows:

N(cid:88)

i=1

1
N

(2)

(3)

divt =

dt(xi)

(cid:26) 0 ∃k, fk(xi) = ft(xi)

1 ∀k, fk(xi) (cid:54)= ft(xi)

dt(xi) =

Here dt(xi) is the diversity of classiﬁer to be added in it-
eration t to data point xi. N is the size of the training set.
fk(xi) is the predicted result of the classiﬁer in iteration k
for data point xi. Our information need detection algorithm
uses this modiﬁed Adaboost named AdaboostDIV. The di-
versity of a classiﬁer represents how much new information
it could provide to a group of classiﬁers that have already
been trained in Adaboost. This value will be smaller and
smaller when there are more classiﬁers adopted. In each it-
eration of AdaboostDIV, we examine the diversity of a new
classiﬁer. If the diversity of this classiﬁer is higher than min-
imal threshold div, we accept this classiﬁer into the group
of classiﬁers. Otherwise we terminate the algorithm.
4.5 Evaluation of the Classiﬁer

We train and evaluate our algorithm using the manually
labeled set of 3,119 tweets. 10-fold cross validation and the
metric of classiﬁcation accuracy are adopted to evaluate each
candidate classiﬁer.

Before feature selection, there are 44,121 ngram lexical
features, 23,277 WordNet features, 3,902 Part-of-Speech fea-
tures, and 6 meta features. In Table 1, we compare the per-
formance of the four SVM classiﬁers using each of the four
types of features and various feature selection algorithms.
The ﬁndings are consistent with the conclusions in [11]. Fea-
ture selection using the BNS metric outperformed two other
metrics, namely accuracy (ACCU) and Information Gain,
both of which improved over the classiﬁers without feature

Feature Type

Raw

ACCU

Information Gain

BNS

Lexical WordNet
0.745
0.790
0.804
0.856

0.610
0.673
0.676
0.702

POS Meta
0.668
0.634
0.718
0.723
0.745

/
/
/

Table 1: Results of SVM classiﬁers. Lexical fea-
tures performed the best. Feature selection im-
proved classiﬁcation accuracy.

Figure 2: Feature selection using BNS

selection. Among the four types of features alone, ngram lex-
ical features appear to provide the best performance, while
the six meta features provide the weakest result which is also
far better than random.

Figure 2 shows a ﬁne tuning of the number of features
selected using BNS. Clearly, when too few or too many fea-
tures are selected, the classiﬁcation performance drops be-
cause of insuﬃcient discriminative power and overﬁtting, re-
spectively. Based on our experiment results, we select 3,795
top ranked lexical features, 3,119 top WordNet features, as
well as 505 top Part-of-Speech features.

At last, we combined the four SVM classiﬁers, represent-
ing four types of features, using AdaboostDIV. The accuracy
of the classiﬁer (with 10-fold cross validation) improved from
85.6% to 86.6%. The small margin suggests that the lexical
features are strong enough in detecting information needs,
while other types of features add little to the success. Using
Adaboost instead of AdaboostDIV compromised the perfor-
mance, which is consistent to the ﬁndings in [20].

Finally, the best performing classiﬁer (four SVM classiﬁers
combined with AdaboostDIV, with feature selection with
BNS) is adopted to classify all the tweets in our collection. In
our evaluation, the improvements made by feature selection
and AdaboostDIV passed the paired-sample t-test at the 5%
signiﬁcance level.

5. ANALYZING INFORMATION NEEDS

After applying the text classiﬁer above to the entire col-
lection of tweets, we detected 136,841,672 tweets conveying
information need between July 10th 2011 to June 31st 2012.
This is roughly a proportion of 3% of all tweets, and 28.6%
of tweets with question marks. With this large scale col-
lection of real questions on Twitter, we are able to conduct
a comprehensive descriptive analysis of user’s information

1549Figure 3: Questions and background tweets over time.

needs. Without ambiguity, we call all the tweets collected
as the Background tweets, whether they are questions or
not. We call tweets that convey information needs as In-
formation Needs (or short as IN), or simply Questions.
5.1 General Trend

Once we are able to accurately identify real questions (or
information needs), the ﬁrst thing to look at is how many
questions are being asked and how they are distributed. Be-
low we present the general trend of the volumes of ques-
tions being asked comparing to the total number of tweets
in the background. For plotting purposes, we choose to show
the trend of the ﬁrst 5 months from this entire time scope,
from July 10th 2011 to November 30th, 2011. Most of the
events occurred during this period of time, so plotting the
whole year’s time series would take more space and cannot
be shown distinctly. These 5 months contain a collection of
1,640,850,528 tweets, in which 51,263,378 conveyed an infor-
mation need. We use this time period for all visualization
and time-series analysis below.

Since there is a huge diﬀerence between the raw numbers
of information needs and the background tweets, we nor-
malize the time series so that the two curves are easier to be
aligned on the plot. Speciﬁcally, we normalized all the time
series using the Z-normalization. That is, for the ith data
point valued xi in the time series, we transform the value by
following equation:

(cid:48)
x
i =

xi − µ

σ

(4)

Where µ and σ are the mean and standard deviation of
all data points in this time series. This simple normal-
ization doesn’t change the trend of the time-series, but al-
lows two series of arbitrary values being aligned to the same
range. In the plot, a positive value means the daily count of
IN/background tweets is above the average count over time,
and a negative value means the count is below the mean. An
actual value x on one day indicates that the count of that
day is x standard deviations away from the average.

From Figure 3, we observe that both the number of tweets
and the number of questions are increasing over time. There
are observable but weak days-of-week patterns, which diﬀer
search engine logs which present signiﬁcant weekly patterns
(more queries on weekdays than weekends) [23]. The trend is
much more sensitive than that of query logs [23], with obvi-
ous and irregular spikes and valleys scattered along the time

line. This implies that user’s information seeking behaviors
on Twitter are more sensitive to particular events than the
behaviors on search engines. The subﬁgure presents a strong
daily pattern, where both the total number of tweets and
information needs peak in late morning and early evening,
leaves a valley after noon, and sinks soon after midnight.

In general, the trend of information needs correlates with
the trend of the background, which means the information
needs on Twitter are likely to be social driven but not in-
formation driven. This is not surprising since real world
events are likely to stimulate both the demand and sup-
ply of information. The more interesting signals in the plot
are the noticeable diﬀerences between the two curves. On
some days there is a signiﬁcantly overrepresented “demand”
of information (i.e., questions) than the “supply” (i.e., back-
ground), where there appears a noticeable gap between the
two curves. This oﬀers opportunities to analyze what people
want, provide better recommendations, and develop propa-
ganda. It is also an interesting observation from the hours-
of-day trend that information needs are always overrepre-
sented between the two peaks, before and after noon.

5.2 Keywords

With a sense of the general trend of how people ask, the
next question is what people ask. Previous literature has
provided insights on the categorization and distribution of
topics of questions [26, 29]. Here we are not repeating their
eﬀorts, but to provide a ﬁner granularity analysis of the
keywords. After removing stopwords from the tweets, we
extracted all unigrams and bigrams from tweets classiﬁed as
conveying information needs. We trim this list of keywords
by keeping those appeared every day of our time frame. We
believe these keywords that the most representative of the
everyday information needs of users in Twitter instead of
information needs only triggered by particular events. For
each of these keywords, we keep the daily count of the num-
ber of background tweets containing the keyword and the
number of information needs containing the keyword.

With these counts, we can distinguish keywords that ap-
peared frequently in information needs and those appeared
frequently in the background. Table 2 lists a subset of key-
words that are signiﬁcantly overrepresented in information
needs (i.e., have a much larger frequency in IN than in Back-
ground tweets, normalized by the maximum of the two fre-
quencies), compared to the keywords signiﬁcantly overrep-

1550(a) Trend of tweets conveying informa-
tion need with keyword “obama”

(b) Trend of tweets conveying informa-
tion need with keyword “nasa”

(c) Trend of tweets conveying informa-
tion need with keyword “scandal”

Figure 4: Trend of tweets conveying information need with diﬀerent keywords

Frequent in IN

noyoutube

butterﬂy fall
pocket camera

Monday

skype

any suggestion

waterproof phone
any recommend

Frequent in Background

http

user video
follow back

retweet
beautiful

photo

good night
god bless

Table 2: Overrepresented keywords in information
needs and background

resented in the background. One can observe from the table
that keywords about technology (e.g., “noyoutube,”“pocket
camera,”“skype,”“waterproof phone”) and recommendation
seeking (e.g., “any suggestion,” “any recommend”) have a
high presence in questions while URLs (e.g., “http”), greet-
ings (e.g., “good night,”“god bless”) and requests (e.g., “fol-
low back”) are more frequent in the background. This ﬁnd-
ing is consistent with the quantitative analysis in literature
[26, 29].

We further dropped the keywords that appeared less than
10 times a day in average, from which we obtained 11,813
keywords. For these keywords, we generated time series that
represent the demand of information about these keywords,
by counting the number of questions and general tweets con-
taining particular keywords everyday.

Figure 4 presents the trends of information needs and

background tweets containing three particular keywords, namely
“Obama,” “NASA,” and “scandal.” In Fig. 4(a), we can see
that the trend of information needs closely correlates with
the background, with several noticeable bursting patterns.
These patterns generally correspond to real world events.
For example, the largest spike around September 8th was
correlated with President Obama’s speech about the $450
billion plan to boost jobs. Such types of major events are
likely to trigger both questions and discussions in online
communities, thus have caused a correlated spike of both
information needs and the background.

The trends of the keyword “NASA” present a diﬀerent pat-
tern. The questions and the background align well around
the big spike, but disjoin in other time periods. In general,
the trend of information needs is more sensitive than the
background discussions, presenting more ﬂuctuation. These
smallish spikes are not triggered by major events, but rather
reﬂecting the regular demands of information. The trends
of the keyword “scandal” is even more interesting. Even
the major spikes don’t correlate with questions and with

Figure 5: Bursts detected from IN and background

the background. For example, the big spike in information
needs was triggered by a widespread cascade of tweets that
connects “Priest sex scandal” with “Notre Dame football,”
which is more like a cascade of persuasion, rumor, or propa-
ganda instead of an real event.
5.3 Burstiness

The anecdotal examples above presented interesting in-
sights in understanding the diﬀerent roles of bursting pat-
terns in the time series. This is done by comparing indi-
vidual spikes in information needs with the pattern in the
background in the same time period. A diﬀerent perspective
of investigating such bursting patterns is to compare them
longitudinally. How many spikes are like the spike caused
by Obama’s job speech? If a similar bursting pattern can be
found among the information needs of a diﬀerent keyword,
that means there is an event that have made a similar im-
pact with the president’s speech in terms of triggering the
users’ behaviors of information seeking.

Literature has thrown light on how to detect real events
based on burst detection in social media [38, 37].
In our
analysis, we adopt a straightforward solution to detect sim-
ilar burst events in the time series of information needs and
the background. Speciﬁcally, we select a signature burst-
ing pattern of a real event as a query (e.g., the spike corre-
sponding to Obama’s job speech in Figure 4(a)) and retrieve
all similar spikes in the time series of other keywords. The

1551Figure 6: Entropy of word distributions in questions and background

similarity measurement is the Euclidean distance between Z-
normalized time series. By doing this, we found 14,640 burst
patterns in the time series of information needs and 12,456
burst patterns in the background of all keywords. Figure 5
plots the number of burst events that have a similar impact
as the Obama speech, aggregated from the time series of all
diﬀerent keywords. Apparently, there are more such spikes
in the time series of information needs rather than in the
background, which reassures our ﬁnding that the behavior
of question asking is more sensitive than the narrative dis-
cussions of events. The number of bursting patterns tops
in late August and the month of October, which coincides
with the two series of events related to “Hurricane Irene”
and “Occupy D.C.”

5.4 Entropy

The investigation of bursting patterns provides insights
about understanding the impact of real events on Twitter
users’ information seeking behaviors. The impact is featured
by the sudden increase of information needs (or background
tweets, or both) containing certain keyword. Another way
to measure the impact of an event is to look at how it inﬂu-
ences the content of information people are tweeting about
and asking for. Shannon’s Entropy [13] is a powerful tool
to measure the level of uncertainty, or unpredictability of a
distribution. It is well suited for sizing challenges, compres-
sion tasks, as well as the measure of diversity of information.
We apply Shannon’s entropy to the information needs de-
tected, by measuring the entropy of the word distribution
(a.k.a., the language model) in all background tweets and in
all questions every day. Clearly, a lower entropy indicates
a concentration of discussions on certain topics/keywords,
and a higher entropy indicates a spread of discussions on
diﬀerent topics, or a diversiﬁed conversation.

Our intuition is that if a major event inﬂuences the dis-
cussion and information seeking behaviors, the topics in the
background or in the questions on that day will concentrate
on the topics about that event. Thus we are likely to ob-
serve a decreased entropy. Figure 6 plots the entropy of the
language models of all information needs, and of all tweets
in the background over time. We mark several points in
the time series where we observe a sudden drop of entropy
on the next day, which indicates a concentration of topics

being discussed/asked. We selected these points by the sig-
niﬁcance of the entropy drop and the diﬀerences between
the entropy of IN and the entropy of background. We then
extract the keywords that are signiﬁcantly overrepresented
in the day after each marked point, which give us a basic
idea about the topics that have triggered this concentration.
These keywords are good indicators of the actual events that
have triggered the concentration (e.g., “the hurricane Irene,”
“arsenal chelsea” and “the rumor about the release date of
iphone 5”).

It is especially interesting to notice that on some particu-
lar days, entropy drops in information needs but increases in
the background. We believe these are very indicative signals
for monitoring what the public needs. For example, on Octo-
ber 12th, 2011, there was a sudden drop of entropy in infor-
mation needs which didn’t occur in the background tweets.
The discussions concentrated on keywords like “ios,” “up-
date,” and “blackberry.” Indeed, on that day Apple released
the new operation system iOS 5, which triggered massive
questions about how to get the updates. During the same
time, there was a series of outages which caused a shut-
down of the Blackberry Internet Service. Such an event has
contributed in the concentrations of questions about Black-
berry. It is interesting to see that these events about tech-
nology indeed had a larger impact in questions instead of the
background tweets, which is again consistent with the statis-
tics in literature [26, 29]. Clearly, analyzing the entropy of
information needs provides insights on detecting events that
have triggered the concentration of information needs. Such
discoveries indicate compelling opportunities for search and
recommender services, advertising, and rumor detection.

Interestingly, we found entropy analysis not only a pow-
erful tool for macro-level analysis of the impact of events,
but also eﬀective in micro-level analysis of the information
seeking behaviors of individual users. Indeed, we can also
compute the entropy of the distribution of the number of
questions that a user asks among diﬀerent hours of a day.
Behaviors of users with a low entropy are more predictable
than behaviors of users with a high entropy. Below we show
the two behavior patterns from two speciﬁc users. One is
with high entropy and the other is with low entropy in Fig-
ure 7 and 8 respectively.
In these two ﬁgures, the x-axes
represent the 30 days in September, 2011, and the y-axes

1552Figure 7: Questions of a user of low entropy.

Figure 8: Questions from a user of high entropy.

represent the 24 hours in each day. The diﬀerent colors in
these two ﬁgures represent diﬀerent numbers of posts (the
legends are shown on the right side of the ﬁgures).

Clearly, the user with low entropy is fairly predictable: he
always asks questions at 7am. By looking into his tweets,
we found that this is an automatic account that retweets
open questions from Yahoo! Answers. The second user is
much less predictable, who seemed to be asking questions all
over the hours of a day except for the bed time. By looking
into his tweets, we found that this is a user who uses Twit-
ter as an instant message platform, who chats with friends
whenever he is awake. This user-level analysis on entropy of
information needs presents insights on characterizing diﬀer-
ent individual behaviors.
5.5 Predictive Power

Up to now, we have presented many interesting types of
analysis, mostly on the longitudinal patterns of information
needs. We see various insights about how to make use of the
analysis of information needs in Twitter. Previous literature
has visioned the diﬀerent and complementary roles of social
networks and search engines in information seeking. What
if we compare the information needs (questions) posted on
Twitter and the information needs (queries) submitted to
search engines? Is one diﬀerent from the other? Can one
predict the other? If interesting conclusions can be drawn,
it will provide insight to the search engine business.

To do this, we compare the trends of information need in
Twitter with the trends of Google search queries. Figure
9(a) shows the time series of the Twitter questions contain-
ing the keyword “Justin Bieber” and the Google trend of the
query “Justin Bieber”. We use this query as an example be-
cause it is one of the most frequent search queries in Google
2011 and is also contained in a large number of Twitter
questions. We can see that information needs in Twitter is
more sensitive to bursting events, while the same queries in
Google presents a more periodic pattern (e.g., days-of-week
pattern).

We then move forward to test whether the information
needs from one platform can predict those in the other, using
the Granger causality test. The Granger causality test is a

statistical hypothesis test for determining whether a time
series is useful in forecasting another [15]. In [3], it is used
to test whether the sentiment of Twitter users can predict
stock market.

Speciﬁcally, we selected a subset of keywords and manu-
ally downloaded the trends of these keywords as queries sub-
mitted to Google 6. The subset of keywords contains twenty
keywords that have a high frequency in background tweets,
twenty keywords that have a high frequency in the ques-
tions, and twenty keywords from the most popular search
queries in Google. To select this subset, we sorted all the
keywords by frequency from the three diﬀerent sources and
select the top 20 named entities and nouns. If there is an
overlapping keyword from multiple sources, we simply add
a new keyword from the source with lower frequency of the
overlapping keyword7. We then use the Granger causality
test to test whether the three trends (Twitter background,
Twitter information needs, and Google queries) of each key-
word can predict each other. By changing the parameters
in Granger causality test, we can test the prediction power
of one time series to the other for diﬀerent lags of time. In
this paper, we only show the results with lag of 5 days due
to the limitation of space. We obtained similar results with
other diﬀerent lags.

Results show that the trends of information needs in Twit-
ter have a good predictive power in predicting trends of
Google queries and are less likely to be predicted by the
Google trends. This is measured by “of how many key-
words, one type of time series can predict another type of
time series, given certain signiﬁcance level.” From Figure
9(b), we see that the information needs in Twitter have a
better predictive power than the background in predicting
Google trends. From Figure 9(c), we see that the infor-
mation needs in Twitter have a better predictive power in
predicting Google trends rather than the other way around.
Between information needs in Twitter and Google trends,
the questions in Twitter have a stronger predictive power
of Google queries, which successfully predicts the Google
trends of more than 60% of the keywords with a signiﬁcance
level of 0.05. Among these keywords, 9 of them are from
the popular Google queries. This is a promising insight for
search engine practitioners to closely watch the questions in
Twitter and improve the search results of targeted queries
whenever a bursting pattern is observed.
5.6 Implications and Discussion

In this section, we presented various investigations of the
questions, or information needs, in Twitter. Most analyses
presented are longitudinal, based on the time series of par-
ticular statistics and comparisons between the questions and
background tweets. The analysis provided interesting impli-
cations to social behavior observers, search engine practi-
tioners, and researchers of social search.

To summarize, we conﬁrmed that the behaviors of infor-
mation seeking (questions) are substantially diﬀerent from
the behaviors of narrative conversations (background) in
Twitter. We also ﬁnd that it diﬀers from behaviors in Web
search. Some of the ﬁndings reconﬁrmed the conclusions
in literature, such as the overrepresented topics in Twitter

6Since we don’t have access to real search logs, we used the
Google trend: http://www.google.com/trends/
7The List of the keywords can be found at http://
www-personal.umich.edu/~zhezhao/projects/IN/wlist

1553(a) Keyword: Justin Bieber

(b) Background v.s. information needs in
predicting Google trends. The higher the
better

(c) Information need v.s. Google trends
in predicting each other. The higher the
better

Figure 9: Twitter information needs can predict search queries.

questions. Interesting patterns emerge when comparing the
questions with the background, which implies opportunities
for providers of information service. This includes the pat-
terns when the demands (e.g., questions) are signiﬁcantly
higher than the supply (e.g., background), when the infor-
mation needs concentrate on particular topics, and when the
spikes in information needs do not agree with those in the
background.

We found that information needs in Twitter are sensitive
to real world events (more sensitive than search queries). By
comparing the patterns of individual keywords in questions
and in the background, we foresee a new and meaningful
taxonomy to discriminate the types of information needs.
The comparative analysis also provides new ways to diﬀer-
entiate real world events and cascades of persuasion. This
implies useful tools to detect propaganda and rumors from
social media.

Entropy analysis provided a good way to detect real events
and their impact in the topics being asked about in Twit-
ter. It also provided a unique perspective to understand and
discriminate the behaviors of individual users. Such analy-
sis implies new tools for business providers and advertisers,
which can help them to come up with a better social mon-
etization strategy such as targeted advertising or content
recommendation.

With a limited but representative set of keywords and
trend information extracted from the Google trend, we found
that the information needs in Twitter has a predictive power
of search queries in Google. Although this conclusion has to
be reevaluated when large-scale query log data is available
(which we don’t have access to), it implies interesting action
moments for search engines. When spikes of information
needs are observed in Twitter, the search engine practitioner
has time to strategically optimize the search results for the
corresponding topics.

Despite the interesting implications, we do see potential
limitations of this analysis. For example, all our analy-
sis is done on a random sample of tweets. This makes
it diﬃcult to answer questions like “how many questions
are answered,” “how many questions are distributed (i.e.,
retweeted),” or “consequential user behaviors after informa-
tion seeking.” These questions can only be answered with
the availability of the complete set of tweets, or subset of
tweets sampled in a diﬀerent way (e.g., all tweets of a sub-
network of users). We leave these questions about questions
for future work.

6. CONCLUSION

Information needs and information seeking behaviors through

social platforms attracted much interest because of its unique
properties and complementary role to Web search. In this
paper, we present the ﬁrst large-scale analysis of informa-
tion needs, or questions, in Twitter. We proposed an auto-
matic classiﬁcation algorithm that distinguishes real ques-
tions from tweets with question marks with an accuracy as
high as 86.6%. Our classiﬁer makes use of diﬀerent types
of features with the state-of-the-art feature selection and
boosting methods.

We then present a comprehensive analysis of the large-
scale collection of information needs we extracted. We found
that questions being asked on Twitter are substantially dif-
ferent from the topics being tweeted in general.
Informa-
tion needs detected on Twitter have a considerable power of
predicting the trends of Google queries. Many interesting
signals emerge through longitudinal analysis of the volume,
spikes, and entropy of questions on Twitter, which provide
valuable insights to the understanding of the impact of real
world events in user’s information seeking behaviors, as well
as the understanding of individual behavioral patterns in
social platforms.

Based on the insights from this analysis, we foresee many
potential applications that utilizes the better understanding
of what people want to know on Twitter. One possible fu-
ture work is to develop an eﬀective algorithm to detect and
predict what individual users want to know in the future. By
doing this one may be able to develop better recommender
systems on social network platforms. With the presump-
tion of accessing large scale search query logs, a promising
opportunity lies in a large-scale comparison of social and
search behaviors in information seeking. On the other hand,
improving the classiﬁer to detect tweets with implicit infor-
mation need such as tweets that is not an explicit question
or without a question mark is also a potential future work.
Furthermore, it is interesting to do some user-level analysis,
such as studying the predictive power of diﬀerent groups of
users to see whether there exists a speciﬁc group of users
that contributes to predicting the trend most.

Acknowledgement We thank Cliﬀ Lampe, Paul Resnick
and Rebecca Gray for the useful discussions. This work
is partially supported by the National Science Foundation
under grant numbers IIS-0968489, IIS-1054199, and CCF-
1048168, and partially supported by the DARPA under award
number W911NF-12-1-0037.

15547. REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais. Improving web

search ranking by incorporating user behavior information.
In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 19–26. ACM, 2006.

[2] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query
recommendation using query logs in search engines. In
Current Trends in Database Technology-EDBT 2004
Workshops, pages 395–397. Springer, 2005.

[3] J. Bollen, H. Mao, and X.-J. Zeng. Twitter mood predicts

the stock market. CoRR, abs/1010.3003, 2010.

[4] A. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski,

D. Metzler, L. Riedel, and J. Yuan. Online expansion of
rare queries for sponsored search. In Proceedings of the 18th
international conference on World wide web, pages
511–520. ACM, 2009.

[5] E. H. Chi. Information seeking can be social. Computer,

42(3):42–46, 2009.

[6] G. Cong, L. Wang, C.-Y. Lin, Y.-I. Song, and Y. Sun.

Finding question-answer pairs from online forums. pages
467–474, 2008.

[7] C. Cortes and V. Vapnik. Support-vector networks.

Machine Learning, 20:273–297, 1995.

[8] M. Efron and M. Winget. Questions are content: a

taxonomy of questions in a microblogging environment.
Proceedings of the American Society for Information
Science and Technology, 47(1):1–10, 2010.

[9] B. Evans and E. Chi. Towards a model of understanding
social search. In Proceedings of the 2008 ACM conference
on Computer supported cooperative work, pages 485–494.
ACM, 2008.

[10] C. Fellbaum. Wordnet: An electronic lexical database.

Cambridge, MA: MIT Press, 38(11):39–41, 1998.

[11] G. Forman. An extensive empirical study of feature

selection metrics for text classiﬁcation. J. Mach. Learn.
Res., 3:1289–1305, 2003.

[12] Y. Freund and R. E. Schapire. A decision-theoretic

generalization of on-line learning and an application to
boosting, 1995.

[13] R. Gallager. Claude e. shannon: A retrospective on his life,
work, and impact. Information Theory, IEEE Transactions
on, 47(7):2681–2695, 2001.

[14] J. Ginsberg, M. H. Mohebbi, R. S. Patel, L. Brammer,

M. S. Smolinski, and L. Brilliant. Detecting inﬂuenza
epidemics using search engine query data. Nature,
457:1012–1014, February 2009.

[15] C. W. J. Granger. Investigating causal relations by

econometric models and cross-spectral methods.
Econometrica, 37(3):424–38, July 1969.

[16] B. Hecht, J. Teevan, M. R. Morris, and D. Liebling.

Searchbuddies: Bringing search engines into the
conversation. ICWSM, pages 138–145, 2012.

[17] R. Jones, R. Kumar, B. Pang, and A. Tomkins. I know

what you did last summer: query logs and user privacy. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge management,
pages 909–914. ACM, 2007.

[18] R. Krovetz. Viewing morphology as an inference process.

16th ACM SIGIR Conference, pages 191–202, 1993.

[19] B. Li, X. Si, M. R. Lyu, I. King, and E. Y. Chang.

Question identiﬁcation on twitter. pages 2477–2480, 2011.

[20] X. Li, L. Wang, and E. Sung. Adaboost with svm-based

component classiﬁers. Eng. Appl. Artif. Intell.,
21(5):785–795, 2008.

[21] Z. Liu and B. Jansen. Almighty twitter, what are people

asking for? ASIST, 2012.

[22] T. N. Mansuy and R. J. Hilderman. Evaluating wordnet

features in text classiﬁcation models. In Proceedings of the
Nineteenth International Florida Artiﬁcial Intelligence
Research Society Conference, pages 568–573. AAAI Press,
2006.

[23] Q. Mei and K. Church. Entropy of search logs: how hard is
search? with personalization? with backoﬀ? In Proceedings
of the international conference on Web search and web
data mining, pages 45–54. ACM, 2008.

[24] G. A. Miller. Wordnet: A lexical database for english.

Communications of the ACM, 38(11):39–41, 1995.

[25] M. Morris and J. Teevan. Exploring the complementary

roles of social networks and search engines.
Human-Computer Interaction Consortium
Workshop(HCIC), 2012.

[26] M. Morris, J. Teevan, and K. Panovich. What do people

ask their social networks, and why?: a survey study of
status message q&a behavior. In Proceedings of the 28th
international conference on Human factors in computing
systems, pages 1739–1748. ACM, 2010.

[27] M. R. Morris, J. Teevan, and K. Panovich. A comparison of

information seeking using search engines and social
networks. Proceedings of 4th International AAAI
Conference on Weblogs and Social Media, 42(3):291–294,
2010.

[28] J. Nichols and J. Kang. Asking questions of targeted

strangers on social networks. In Proceedings of the ACM
2012 conference on Computer Supported Cooperative
Work, pages 999–1002. ACM, 2012.

[29] S. A. Paul, L. Hong, and E. H. Chi. Is twitter a good place
for asking questions? a characterization study. Proceedings
of the 5th International AAAI Conference on Weblogs and
Social Media, 18(11):578–581, 2011.

[30] C. Shah. Measuring eﬀectiveness and user satisfaction in

Yahoo! answers. First Monday, 16(2-7), 2011.

[31] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz.
Analysis of a very large web search engine query log. In
ACm SIGIR Forum, volume 33, pages 6–12. ACM, 1999.
[32] J. Teevan, E. Adar, R. Jones, and M. Potts. Information
re-retrieval: repeat queries in yahoo’s logs. In Proceedings
of the 30th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 151–158. ACM, 2007.

[33] J. Teevan, S. Dumais, and D. Liebling. To personalize or

not to personalize: modeling queries with variation in user
intent. In Proceedings of the 31st annual international
ACM SIGIR conference on Research and development in
information retrieval, pages 163–170. ACM, 2008.

[34] J. Teevan, D. Ramage, and M. Morris. # twittersearch: a

comparison of microblog search and web search. In
Proceedings of the fourth ACM international Conference
on Web search and Data Mining, pages 35–44. ACM, 2011.

[35] K. Wang and T.-S. Chua. Exploiting salient patterns for

question detection and question retrieval in
community-based question answering. pages 1155–1163,
2010.

[36] J. Yang, M. R. Morris, J. Teevan, L. A. Adamic, and M. S.

Ackerman. Culture matters: A survey study of social q&a
behavior. 2011.

[37] J. Yao, B. Cui, Y. Huang, and X. Jin. Temporal and social
context based burst detection from folksonomies. In AAAI.
AAAI Press, 2010.

[38] J. Yao, B. Cui, Y. Huang, and Y. Zhou. Bursty event
detection from collaborative tags. World Wide Web,
15(2):171–195, 2012.

1555