A Framework for Benchmarking Entity-Annotation

Systems

Marco Cornolti

Dipartimento di Informatica

University of Pisa, Italy
cornolti@di.unipi.it

Paolo Ferragina

Dipartimento di Informatica

University of Pisa, Italy
ferragina@di.unipi.it

Massimiliano Ciaramita

Google Research
Zürich, Switzerland

massi@google.com

ABSTRACT
In this paper we design and implement a benchmarking
framework for fair and exhaustive comparison of entity-an-
notation systems. The framework is based upon the deﬁ-
nition of a set of problems related to the entity-annotation
task, a set of measures to evaluate systems performance,
and a systematic comparative evaluation involving all pub-
licly available datasets, containing texts of various types
such as news, tweets and Web pages. Our framework is
easily-extensible with novel entity annotators, datasets and
evaluation measures for comparing systems, and it has been
released to the public as open source1. We use this frame-
work to perform the ﬁrst extensive comparison among all
available entity annotators over all available datasets, and
draw many interesting conclusions upon their eﬃciency and
eﬀectiveness. We also draw conclusions between academic
versus commercial annotators.

Categories and Subject Descriptors
D.2.8 [Software Engineering]: Metrics—performance mea-
sures; I.2.7 [Artiﬁcial Intelligence]: Natural Language
Processing—text analysis

Keywords
Benchmark Framework; Entity annotation; Wikipedia

1.

INTRODUCTION

Classic approaches to document indexing, clustering, clas-
siﬁcation and retrieval are based on the bag-of-words para-
digm. The limitations of this paradigm are well-known to
the IR community and in recent years a good deal of work
has attempted to move beyond by “grounding” the processed
texts with respect to an adequate semantic representation,
by designing so-called entity annotators. The key idea is to
identify, in the input text, short-and-meaningful sequences
of terms (also called mentions) and annotate them with un-
ambiguous identiﬁers (also called entities) drawn from a cat-
alog. Most recent work adopts anchor texts occurring in
Wikipedia as entity mentions and the respective Wikipedia
pages as the mentioned entity, because Wikipedia oﬀers to-
day the best trade-oﬀ between catalogs with a rigorous struc-
1See http://acube.di.unipi.it/

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

ture but low coverage (such as WordNet, CYC, TAP), and a
large text collection with wide coverage but unstructured
and noisy content (like the whole Web). The process of en-
tity annotation involves three main steps: (1) parsing of the
input text, which is the task to detect candidate entity men-
tions and link each of them to all possible entities they could
mention; (2) disambiguation of mentions, which is the task
of selecting the most pertinent Wikipedia page (i.e., entity)
that best describes each mention; (3) pruning of a mention,
which discards a detected mention and its annotated entity
if they are considered not interesting or pertinent to the
semantic interpretation of the input text.

The focus around entity annotators has increased signif-
icantly in the last few years, with several interesting and
eﬀective algorithmic approaches to solve the mention-entity
match problem, possibly using other knowledge bases such
as DBpedia, Freebase or Yago (see e.g. [2, 3, 7, 9, 13, 16,
18, 19, 23, 5, 15]). Unfortunately, the research has inves-
tigated some speciﬁc tasks using non-uniform terminology,
non-comparable evaluation metrics, and limited datasets and
systems. As a consequence, we only have a partial picture of
the eﬃciency and eﬀectiveness of known annotators which
makes it diﬃcult to compare them in a fair and complete
way. This is a particularly important issue because those
systems are being used as black-boxes by more and more IR
tools, built on top of them, such as [1, 8, 21, 22]. Motivated
by these considerations, [20] recently attempted to compare
entity-annotation systems mainly coming from the commer-
cial realm2. However, as the authors state in the concluding
section of their paper, their evaluation is limited to strict
metrics which account for “exact” matches over mentions
and entities (and, rather observe that “a NE type might be
not wrong but not precise enough.”), it does not consider
datasets fully annotated by humans (i.e. mentions are only
the ones derived by few parsers), and misses to consider the
best performing tools which have been published recently in
the scientiﬁc literature. This last issue is a crucial limita-
tion because, as [12] showed recently, the DBpedia Spotlight
system (the best according to [20]) achieves much worse per-
formance than some of the systems tested in this paper.

Given this scenario, we aim with this paper at deﬁning and
implementing a framework for comparing in a complete, fair
and meaningful way the most eﬃcient, eﬀective and pub-
licly available entity-annotation systems: namely, AIDA [7],
Illinois Wikiﬁer [19], TagMe [3], Wikipedia-miner [16], and

2AlchemyAPI, DBpedia Spotlight, Evri, Extractiv, Lupe-
dia, OpenCalais, saplo, Wikimeta, Yahoo! Content Analysis
and Zemanta.

249Problem
Disambiguate to Wikipedia (D2W)

Annotate to Wikipedia (A2W)

Input
Text,
Set
of
mentions
Text

Output
Set of
tionsw

Set of
tions

relevant annota-

relevant annota-

Scored-annotate to Wikipedia (Sa2W)

Text

Set of relevant and scored
annotations

Concepts to Wikipedia (C2W)

Text

Set of relevant tags

Scored concepts to Wikipedia (Sc2W)

Text

Ranked-concepts to Wikipedia (Rc2W) Text

Set of relevant and scored
tags.
Ranked list of
tags

relevant

Description
Assign to each input mention its pertinent entity (possibly
null). This problem has been introduced in [2].

Identify the relevant mentions in the input text and assign
to each of them the pertinent entities. This problem has
been introduced in [16].
As A2W, but here each annotation is assigned a score
representing the likelihood that the annotation is correct.
This problem has been introduced in [16].
Tags are taken as the set of relevant entities that are men-
tioned in the input text. This problem has been deﬁned
in [13].
As C2W, but here each tag is assigned a score representing
the likelihood that the annotation is correct.
Identify the entities mentioned in a text and rank them
in terms of their relevance for the topics dealt with in the
input text. This problem has been deﬁned in [13].

Table 1: A set of entity-annotation problems.

DBpedia Spotlight; which currently deﬁne the state-of-the-
art for the entity-annotation task. In order to achieve this
goal, we will introduce (1) a hierarchy of entity-annotation
problems, that cover the wide spectrum of annotation goals
such systems could address; (2) a set of novel measures to
ﬁnely evaluate the eﬀectiveness of these systems; (3) all pub-
lic datasets available for the entity-annotation task, allowing
us to explore the eﬃciency and eﬀectiveness performance of
the annotation process according to the introduced measures
and hierarchy of problems.

We have made this framework publicly available (http:
//acube.di.unipi.it/), with all datasets and software used
in this paper, to make our experiments reproducible and
provide a common ground for developing new and better
solutions for this challenging problem.

2. BACKGROUND AND TERMINOLOGY

which is uniquely identiﬁed by its page-ID.

We use the following terminology:
• An entity (or concept, topic) is a Wikipedia article
• A mention is the occurrence of a sequence of terms
located in a text.
It is encoded by the integer pair
(cid:2)p, l(cid:3), where p is the position of the occurrence and l
is the length of the mention.
• An annotation is the linking of a mention to an entity.
It is encoded by the pair (cid:2)m, e(cid:3) where m = (cid:2)p, l(cid:3) is the
mention and e is the page-ID of the mentioned entity.
• A tag is the annotation of a text with an entity which
captures a topic (explicitly mentioned) in the input
text. (Thus, a tag comes with no mention.)
• A score is a real value s ∈ [0, 1] that is assigned to an

annotation or a tag to indicate its correctness.

As an example, consider the following text fragment: “Oba-
ma issues Iran ultimatum”. “Obama”, “Iran” and “ultima-
tum” are mentions which occur as anchors in Wikipedia
and can thus be linked with the entities represented by the
Wikipedia pages of Barack Obama, the nation of Iran and
the threat to declare war, respectively.

Figure 1 and Table 1 exemplify and formalize, respec-
tively, a set of problems which cover the wide spectrum of
goals the entity-annotation systems could be asked to solve.

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)

(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)
(cid:14)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:6)(cid:4)(cid:5)(cid:3)(cid:4)(cid:5)

(cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:1)(cid:6)(cid:7)(cid:7)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)

(cid:35)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:24)(cid:31)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:34)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:24)(cid:36)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:32)(cid:36)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)

(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)

(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)

(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)

(cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12)
(cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21)
(cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9)

(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)

(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)

(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)

(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)

(cid:37)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:25)(cid:19)(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)(cid:25)(cid:19)(cid:20)(cid:2)(cid:21)(cid:21)(cid:10)(cid:18)(cid:25)
(cid:12)(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:25)(cid:19)(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)(cid:25)(cid:19)(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:38)

(cid:37)(cid:39)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:25)(cid:19)(cid:40)(cid:41)(cid:42)(cid:43)(cid:25)(cid:19)(cid:39)(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)(cid:25)(cid:19)(cid:40)(cid:41)(cid:44)(cid:43)(cid:25)
(cid:39)(cid:20)(cid:2)(cid:21)(cid:21)(cid:10)(cid:18)(cid:25)(cid:19)(cid:45)(cid:41)(cid:40)(cid:43)(cid:25)(cid:19)(cid:39)(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:25)(cid:19)(cid:45)(cid:41)(cid:40)(cid:43)(cid:25)
(cid:39)(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)(cid:25)(cid:19)(cid:40)(cid:41)(cid:44)(cid:43)(cid:25)(cid:19)(cid:39)(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:25)(cid:19)(cid:40)(cid:41)(cid:42)(cid:43)(cid:38)

(cid:29)(cid:30)(cid:12)(cid:20)(cid:2)(cid:21)(cid:21)(cid:10)(cid:18)
(cid:31)(cid:30)(cid:12)(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)
(cid:32)(cid:30)(cid:12)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)
(cid:33)(cid:30)(cid:12)(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)
(cid:34)(cid:30)(cid:12)(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)
(cid:35)(cid:30)(cid:12)(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)

Figure 1: Some instances of annotation tasks. Num-
bers in problems Sa2W and Sc2W denote the cor-
rectness likelihood of each annotation/tag.

Some of these problems have been introduced in the litera-
ture, others are introduced in this paper as reasonable vari-
ations which complete the picture. This will allow us to con-
textualize and compare the known systems, and assess their
relations and limitations. These problems can be casted in
two main classes: the ﬁrst consists of three problems which
address the identiﬁcation of (possibly scored) annotations,
and thus the identiﬁcation of mention-entity pairs; the sec-
ond consists of three further problems that involve ﬁnding
tags (possibly scored or ranked), thus accounting only for
the entities.

Given that these annotation problems are strictly related
to each other, it is possible to introduce a few simple reduc-
tions that induce a hierarchy among them. Such a hierarchy,
in the form of a DAG, will ease the deﬁnition of our frame-
work and the set-up of the experiments for comparing the
annotators. We adopt the notation A ≺ B to indicate that

250251length, but is quite slow because of the quadratic-
programming approach. Currently, this system can
not be accessed or downloaded.5

links. Wikipedia-Miner has been designed to deal with
English documents of arbitrary length, it oﬀers a pub-
licly available API (as of September 2012).9

Illinois Wikiﬁer searches the input text for mentions ex-
tracted from Wikipedia anchors and titles, using the
Illinois NER system [18]. Disambiguation is formu-
lated as an optimization problem which aims at global
coherence among all mentions. It uses a novel related-
ness measure between Wikipedia pages based on NGD
(Normalized Google similarity distance) and pointwise
mutual information. Wikiﬁer has been designed to
deal with English documents of arbitrary length, its
software can be downloaded (as of August 2012).6

DBpedia Spotlight searches the input text for mentions
extracted from Wikipedia anchors, titles and redirects;
the parser is the LingPipe Exact Dictionary-Based Ch-
unker. It then associates a set of candidate entities to
each mention using the DBpedia Lexicalization dataset.
Given a spotted mention and a set of candidate enti-
ties, both the context of the mention and all contexts
of each entity are cast to a Vector-Space Model (using
a BOW approach) and the candidate whose context
has the highest cosine similarity is chosen. Note that
no semantic coherence is estimated among the chosen
entities. Spotlight has been designed to deal with En-
glish documents of arbitrary length, it oﬀers a publicly
available API (as of November 2012).7

TagMe 2 searches the input text for mentions deﬁned by
the set of Wikipedia page titles, anchors and redi-
rects. Each mention is associated with a set of can-
didate entities. Disambiguation exploits the structure
of the Wikipedia graph, according to the relatedness
measure introduced in [17] which takes into account
the amount of common incoming links between two
pages. TagMe’s disambiguation is enriched with a vot-
ing scheme in which all possible bindings between men-
tions and entities are scored and then they express a
vote for each other binding. A proper mix of heuristics
is eventually adopted to select the best annotation for
each mention. TagMe has been designed to deal with
short texts, it oﬀers a publicly available API.8

Wikipedia Miner is an implementation of the Wikiﬁca-
tion algorithm presented in [16, 11], one of the ﬁrst ap-
proaches proposed to solve the entity-annotation prob-
lem. This system is based on a machine-learning ap-
proach that is trained with links and contents taken
from Wikipedia pages. Three features are then used
to train a classiﬁer that selects valid annotations dis-
carding irrelevant ones: (i) the prior probability that a
mention refers to a speciﬁc entity, (ii) the relatedness
with the context from which the entity is extracted,
given by the non-ambiguous spotted mentions, and
(iii) the context quality which takes into account the
number of terms involved, the extent they relate to
each other, and how often they are used as Wikipedia
5Although the software for CMNS and CSAW are not avail-
able, we used their datasets to compare the other annotators.
6
http://www.mpi-inf.mpg.de/yago-naga/aida/.
7
http://spotlight.dbpedia.org/rest/annotate.
8
http://acube.di.unipi.it/tagme.
been provided in August 2012.

Its release 2.0 has

Dataset

AIDA/CONLL
AQUAINT
MSNBC
IITB
Meij

Gold
Stan-
dard
A2W
A2W
A2W
A2W
C2W

Num
Docs

Avg
length

Num
Anns

Avg
Anns/Doc

231
50
20
103
502

1039
1415
3316
3879
80

4485
727
658
11245
812

19.4
14.5
32.9
109.1
1.6

Table 3: Gold standard indicates the type of the
dataset. Num Docs is the number of documents in
the dataset, Avg length is their average length, ex-
pressed in characters. Num Anns is the total num-
ber of annotations or tags. Avg Anns/Doc is the av-
erage number of annotations or tags per document.

4. DATASETS

For our experiments we collected all publicly available
datasets that have been evaluated in the literature and we
used them to test all reviewed entity-annotation systems.
Details about these datasets are given below and in Table 3.
We observe that they range from news to tweets, up to Web
pages, providing a wide spectrum of text sources.

AIDA/CoNLL builds on the CoNLL 2003 entity-recogni-
tion task. Documents are news taken from Reuters
Corpus V1. A large subset of mentions referring to
named entities are annotated, but the common names
are not. Entities are annotated at each occurrence of
a mention. The dataset was introduced in [7]. This
dataset is divided into three chunks: Training, Test
A and Test B. Since the AIDA system has been tuned
over the ﬁrst two chunks, we will use only Test B, made
up of 231 documents, for our experiments.

AQUAINT contains a subset of the original AQUAINT
corpus, consisting of English newswire texts. Not all
occurrences of the mentions are annotated: only the
ﬁrst mention of each entity, and only the most impor-
tant entities are retained. This reﬂects the Wikipedia-
style linking. The dataset was introduced in [16].

IITB contains over a hundred of manually annotated texts
drawn from popular Web pages about sport, entertain-
ment, science and technology, and health [9]. This is
the most detailed dataset since almost all mentions,
including those whose related concepts are not highly
relevant, are annotated.

Meij contains tweets annotated with all occurring entities.
Tweets are poorly composed and very short, less than
140 chars. The dataset was introduced in [13].

MSNBC contains newswire text from MSNBC news net-
work. It annotates only important entities and their
referring mentions. The dataset was introduced in [2].

Three datasets are from news but they have diﬀerent char-
acteristics that will be useful to provide a full comparison
among the tested systems. Some of those datasets include
9

http://wikipedia-miner.cms.waikato.ac.nz/.

252annotations to Wikipedia pages that no longer exist in the
current version of Wikipedia. This may happen whenever a
page has been deleted or renamed. In our experiments we
discarded annotations to no-longer existing entities.

5. EVALUATION MEASURES

The deﬁnition of a correct match between two annotations
is challenging because it involves two possible dimensions:
the “syntactic” one of the mentions, and the “semantic” one
of the entities. A match in each dimension can be more or
less“correct”depending on the nature of the match itself. As
an example, let us consider again the text “President Barack
Obama issues Iran ultimatum”. A system might detect the
mention “President Barack Obama” whereas the ground-
truth identiﬁes as the true mention “Barack Obama”. Also,
a system could predict for the corresponding entity the per-
sonal page of Barack Obama whereas the annotators could
have preferred the page of the President of the United States
– which also contains information about Barack Obama –
or vice versa. It seems intuitive that simply counting these
as errors is a sub-optimal choice as it transfers the human-
annotators bias into the evaluation. Furthermore, factoring
out such biases from the evaluation metrics could also alle-
viate the dangers of overﬁtting on small data samples that
only oﬀer a glimpse of the complexity of the total entity
space. Such issues arise more frequently than one would ex-
pect given that, often, the human-labeled instances in the
available datasets oﬀer a wide spectrum of entity/mention
possibilities, all of them pertinent to some extent. To over-
come these problems we introduce novel fuzzy measures to
evaluate entity/mention matching which account for slight,
but non-signiﬁcant, misalignments in the detected mentions,
as well as for diﬀerent, but yet pertinent, entities.

In practice, we propose to generalize standard evaluation
measures such as true/false positives, true/false negatives,
precision and recall by deﬁning them on top of a binary rela-
tion M which speciﬁes the notion of“correct match”between
either two tags or two annotations. Given an input text t,
let g be the correct items (being them mentions or entities
or full annotations) speciﬁed in the ground truth for t and
let s be the solution found by the tested system. We can
introduce the following deﬁnitions:
, x)}
(cid:2) ∈ g : M(x
tp(s, g, M) ={ x ∈ s | ∃x
(cid:2)
(cid:2) ∈ g : M(x
, x)}
(cid:7) ∃x
f p(s, g, M) = {x ∈ s |
(cid:2)
, x)}
(cid:2) ∈ g : M(x
(cid:7) ∃x
tn(s, g, M) = {x (cid:7)∈ s |
(cid:2)
(cid:2) ∈ s : M(x
, x)}
f n(s, g, M) = {x ∈ g |
(cid:7) ∃x
(cid:2)

(1)

Based on these we can generalize the deﬁnitions of precision,
recall and F1 [10] and their (micro- and macro-) dataset-wise
versions. The macro- measures are the average of the cor-
responding measure over each document in the dataset D,
while the micro- measures take into account all annotations
together thus giving more importance to documents with
more annotations. Here follow the micro- and macro- mea-
sures, where we uses t and gt to denote, respectively, the
solution found by an annotator and the gold standard for a
document t ∈ D.

|tp(s,g,M)|
|tp(s,g,M)|

P (s, g, M) =

|tp(s,g,M)|+|f p(s,g,M)|
R(s, g, M) =
|tp(s,g,M)|+|f n(s,g,M)|
F1(s, g, M) = 2·P (s,g,M)·R(s,g,M)
P (s,g,M)+R(s,g,M)

(2)

Pmic(S, G, M) =

(cid:2)

(cid:2)

t∈D |tp(st,gt,M)|

t∈D(|tp(st,gt,M)|+|f p(st,gt,M)|)

(cid:2)

t∈D |tp(st,gt,M)|

(cid:2)

t∈D(|tp(st,gt,M)|+|f n(st,gt,M)|)

Rmic(S, G, M) =
F 1mic(S, G, M) = 2·Pmic(S,G,M)·Rmic(S,G,M)
Pmic(S,G,M)+Rmic(S,G,M)
t∈D P (st,gt,M)
Pmac(S, G, M) =

(cid:2)

|D|

(3)

(cid:2)

t∈D R(st,gt,M)

Rmac(S, G, M) =
F 1mac(S, G, M) = 2·Pmac(S,G,M)·Rmac(S,G,M)
Pmac(S,G,M)+Rmac(S,G,M)

|D|

5.1 Two-side measures

Here we consider appropriate matching relations M which
deal with both mentions and annotations, thus addressing
the case in which the entity-annotation problem accounts
for both of them.
Me for the C2W problem. It is quite straightforward to
devise a proper M for C2W, given that it considers only
the match between entities. We start from the observa-
tion that pages of Wikipedia can be divided into two dis-
tinct subsets: R be the set of redirect pages, nR be the
set of non-redirect pages (thus R ∩ nR = ∅). For exam-
ple “Obama” and “Barrack Hussein Obama” are redirects to
“Barack Obama”. Redirects are meant to ease the ﬁnding
of pages by the Wikipedia users. Redirects can be seen as
many-to-one bindings from all synonyms (pages in R) to the
most common form of the same entity (page in nR). Since
some annotators’ output, as well as some datasets, may con-
tain annotations to redirect pages, we have to consider the
de-referenced concept. Let d : R ∪ nR (cid:11)→ nR be a deref-
erence function that implements the many-to-one binding
into nR. We say that there exists a strong entity match
Me between two entities e1 and e2 iﬀ d(e1) = d(e2). Me is
reﬂexive, symmetric, and transitive.
Ma for the D2W problem. The output for D2W con-
sists of a set of annotations, some of them involving a null
entity. Thus the match relation must deal with mention-
entity pairs. Say we have to compute Ma(a1, a2) where
a1 = (cid:2)(cid:2)p1, l1(cid:3), e1(cid:3) and a2 = (cid:2)(cid:2)p2, l2(cid:3), e2(cid:3). We deﬁne the
strong annotation match Ma as a binary relation that is
veriﬁed iﬀ p1 = p2 and l1 = l2 and d(e1) = d(e2), where
d is the de-reference function speciﬁed before. Note that,
though in D2W the mentions to be annotated are given in
the input, there can be false-negative annotations, namely
those that are assigned to a null concept.
Mw for the A2W problem. Like D2W, the output of the
A2W problem is a set of annotations; unlike D2W, A2W re-
quires the systems to discover the corresponding mentions.
So the strong annotation match proposed for D2W would
be too strict, imposing a perfect text-alignment between the
mentions speciﬁed in the ground-truth and the mentions
outputted by the tested system. Here it is crucial to be
fuzzy, yet meaningful, in the counted matches so we intro-
duce the notion of weak annotation match Mw(a1, a2) which
is true iﬀ the mentions (textually) overlap10 over the input
text and d(e1) = d(e2). Mw is reﬂexive and symmetric, but
it is not transitive nor anti-symmetric.
10Let e1 = p1 +l1−1 ande 2 = p2 +l2−1 be the last character
index of the two mentions. Two mentions overlap iﬀ p1 ≤
p2 ≤ e1 ∨ p1 ≤ e2 ≤ e1 ∨ p2 ≤ p1 ≤ e2 ∨ p2 ≤ e1 ≤ e2.

253Me for the Rc2W problem. As pointed out in [13], since
the output of Rc2W is a ranking of entities, common metrics
like P1, R-prec, Recall, MRR and MAP [10] should be used
only after Me is computed.
M for the Sc2W and Sa2W problems. These problems
ask annotation systems to produce a likelihood score for
each annotation. In practice, it does not make much sense
to compare the likelihood scores (reals) between the ground
truth and the system’s annotation, and indeed the literature
does not oﬀer such datasets. Nonetheless we have systems
that solve Sa2W, so in order to evaluate them, we adapt their
output to some easier problems in the DAG-hierarchy by
introducing a threshold on the annotation scores. After that,
it is enough to set M = Me for Sc2W and M ∈ {Mw, Ma} for
Sa2W, and then compute the generalized metrics discussed
at the beginning of this section and detailed in Section 6.
5.2 One-side measures

The measures presented above for Sa2W and A2W quan-
tify the ability of a system to ﬁnd the correct annotation,
which includes ﬁnding both the correct mention and the cor-
rect entity. Here we consider the two measures Me and the
new Mm to dissect annotators by testing in depth two main
features: their ability to recognize the mentions or their abil-
ity to disambiguate them. In particular, we will address two
questions.

1. How much of the error is determined by the lack of a
correct mention recognition? To answer this question,
we introduce a mention annotation match relation Mm
that only checks the overlap of mentions, ignoring its
associated entity. This will be useful to evaluate the
precision of the parsers used by the systems to detect
the mentions.

2. How much of the error is determined by a wrong asso-
ciation to an entity? That’s exactly what is evaluated
by the metric Me introduced for problem C2W. This
evaluation takes into account only the entities, and not
their mentions, thus it is crucial in all applications in-
terested in retrieving the concepts (tag/entity) a text
is about.

Finally, although the entity-matching relation considers
the weak dereference function d, we will complete our ex-
perimental evaluations by considering a ﬁner measure which
will account for the “closeness” of entities in Wikipedia based
on the Milne-Witten relatedness function [16] (see Figure 5).
5.3 Similarity between systems

The similarity between systems can be measured document-
wise, by comparing annotations over single documents, or
dataset-wise, by comparing annotations over all documents
in a dataset. This comparison can help asses in which phase
of the annotation process systems behave diﬀerently, pro-
viding insights that could help the design of new annotation
approaches. In the former case, given two sets of annota-
tions/tags At and Bt for an input text t and a matching
relation M, we deﬁne the similarity between the systems
that produced them as:

Sim(At, Bt, M) =

|{x ∈ At | ∃y ∈ Bt : M(x, y)}|+
|{x ∈ Bt | ∃y ∈ At : M(x, y)}|

|At| + |Bt|

(4)

This measure is inspired by the Jaccard coeﬃcient [10],
but takes into account the fuzziness of the matching relation
M. Unlike the Jaccard measure, triangle inequality does not
hold. Note that all matching relations introduced above are
reﬂexive and symmetric, thus Sim is symmetric too and
Sim(x, x, M) = 1. Sim ranges in [0, 1].

In order to measure the similarity of two annotators over
a dataset (call it D), we apply iteratively Sim over all doc-
uments and then “average” the values. The average can ei-
ther give the same importance to all annotations/documents
regardless of their size (Smacro) or can be computed as the
overall “intersection” divided by the overall size, giving more
importance to bigger sets (Smicro). Formally:
Smacro(D, M) = 1|D| · (cid:2)

t∈D Sim(At, Bt, M)
(|{x ∈ At | ∃y ∈ Bt : M(x, y)}|+
|{x ∈ Bt | ∃y ∈ At : M(x, y)}|)

(cid:2)

t∈D

Smicro(D, M) =

(cid:2)

t∈D(|At|+|Bt|)

(5)
The actual interpretation of this measures depends only
on the chosen relation M. For the tag-based problems (C2W,
Rc2W, Sc2W), the only available matching function is Me,
thus Sim measures the fraction of commonly annotated en-
tities. For all other problems (Sa2W, A2W, D2W) the use of
Ma gives the fraction of common annotations (having same
concept and same mention); using Mw gives the fraction
of common overlapping annotations (having same concept
and overlapping mention); using Mm gives the fraction of
common overlapping mentions.

6. EXPERIMENTAL RESULTS

We ran a robustness test for the ﬁve entity-annotation
systems listed in Table 2 over the ﬁve (publicly available)
datasets described in Section 4. The annotators were tested
as they are, without any ﬁne-tuning or training over the
datasets, using the most up-to-date APIs or software made
available by the authors of the tested systems, as explained
in Section 3. The following sections present our ﬁndings in
terms of eﬀectiveness (precision, recall and F1) and eﬃciency
(speed) of the annotation process by means of the evaluation
measures introduced above. We observe that the datasets
AIDA-CoNNL and AQUAINT annotate only a subset of the
mentions (See Section 4), therefore entity annotators are
penalized, as they found many reasonable annotations that
are not contained in the (limited) gold standard, and this
explains the not much large F1 ﬁgures achieved in the ex-
periments.

(cid:4)(cid:5)(cid:2)(cid:3)

(cid:1)(cid:2)(cid:3)

(cid:6)(cid:2)(cid:3)

(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:4)(cid:5)(cid:7)(cid:3)(cid:6)(cid:8)(cid:6)(cid:4)(cid:5)(cid:9)(cid:6)

(cid:10)(cid:5)(cid:11)(cid:6)(cid:3)(cid:1)(cid:10)(cid:7)(cid:3)(cid:11)(cid:5)(cid:12)(cid:3)(cid:7)(cid:1)(cid:4)(cid:1)(cid:6)(cid:5)(cid:4)(cid:6)

(cid:4)(cid:11)(cid:13)(cid:4)(cid:4)(cid:5)(cid:14)(cid:3)(cid:7)(cid:1)(cid:4)(cid:1)(cid:6)(cid:5)(cid:4)

Figure 3: DAG of problems involved in our experi-
ments.

Figure 3 shows the hierarchy of problems restricted to
those involved in our experiments: we notice that all tested
systems solve Sa2W natively, the three news datasets and
the Web-page dataset provide a gold standard for A2W,
whereas the Twitter dataset provides a gold standard for
C2W. Thus, when evaluating over news and Web-pages, we

254Experiment 1

Type

News

Datasets

Problem Match-

AIDA/CONLL
AQUAINT
MSNBC

A2W

C2W
A2W

relations
Ma Mw
Mm Me

Mt
Ma Mw
Mm Me

Experiment 2
Experiment 3 Web
pages

Tweets Meij
IITB

Table 4: Experimental setup.

will adapt the systems output to solve A2W, by discarding
annotations using a threshold; whereas when evaluating for
tweets, we will adapt systems to solve C2W, by applying
a threshold on the annotations score and by discarding all
mentions. Table 4 summarizes the setup.
6.1 Experiment 1: News

In this experiment the evaluation focuses on the A2W
problem, the hardest for which datasets are available, ac-
cording to the reductions of Figure 2. To gain a deeper un-
derstanding of systems performance, we used all three news
datasets, because of their diﬀerences (see Table 3).

Since the reduction from Sa2W (the problem all annota-
tors can solve) to A2W needs a threshold on the likelihood
score, our experiments will take into account all possible val-
ues for that threshold, ranging from 0 to 1. We run three
types of experiments to test the eﬀectiveness of the systems
in detecting correct (full) annotations, or just the mentions
(discarding the entities), or just the entities (discarding the
mentions).
6.1.1 Finding the annotations
We computed all evaluation measures presented in Sec-
tion 5. For lack of space, we show in Figure 4 the plot
of micro-F1 using the weak-annotation match Mw over the
AIDA/CONLL dataset (the largest with 231 news and 4485
annotations). The interesting data is given by the maximum
value reached by F 1micro, varying t ∈ [0, 1]. Each annota-
tor gives a diﬀerent meaning to the annotation-scores, thus
it does not make sense to compare the performance of two
annotators for the same value of t. Hence that maximum can
be seen as the best performance achievable by each system
(according to micro-F1). Overall TagMe reaches maximum
F1, followed by Illinois Wikiﬁer, Wikipedia miner, AIDA in
its three variants and the poorest performance is achieved
by DBpedia Spotlight. Detailed results for all news datasets
are reported in Table 5.11

AQUAINT and MSNBC have longer documents. Surpris-
ingly TagMe gives the best results, even if it was designed
to annotate short texts [3, 4]. For AQUAINT, Wikipedia
Miner is the second best followed by Illinois Wikiﬁer, DBPe-
dia Spotlight and then AIDA. For MSNBC, the dataset with
the longest documents, the second best annotator is AIDA
with the CocktailParty and Local algorithms, followed by
Wikipedia Miner, Illinois Wikiﬁer and DBPedia Spotlight.
We also compute the threshold that maximizes F 1micro
for each system and for each dataset using the strong anno-
tation match Ma.

Table 6 reports results only for AIDA/CONLL for lack of
space, the other two news datasets generate consistent re-
sults. Measurements based on Ma give slightly worse results
11We will provide full tables of results for all experiments in
a web appendix with the ﬁnal version of paper.

1
F
-
o
r
c
M

i

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

F1 aidaconlltestb - weakannotationmatch

AIDA - PriorityOnly
AIDA - Local
AIDA - CocktailParty
Wikipedia Miner
Illinois Wikifier
TagMe 2
DBPedia Spotlight

 0.2

 0.4

 0.6

 0.8

 1

Threshold

Figure 4: Micro-F1 measure for Sa2W systems on
the dataset AIDA/CONLL, varying t ∈ [0, 1].

than those based on Mw, especially for annotators such as
TagMe, Illinois Wikiﬁer and Wikipedia Miner. This shows
that these systems detect mentions which do not coincide
with those in the gold standard, even though they overlap
with the human-labeled ones.

Overall, TagMe is the best annotator in terms of micro-
F1, followed by Illinois Wikiﬁer and Wikipedia Miner. This
suggests which algorithm employed by these annotators is
the most performing: TagMe’s voting scheme, based on the
relatedness measure introduced in [16]. Searching for the
best global coherence as done by Illinois Wikiﬁer also seems
a good approach. Next comes Wikipedia Miner’s algorithm,
using machine-learning on Wikipedia’s links. Among the
three versions of AIDA, the best is CocktailParty, followed
by Local and then PriorityOnly. AIDA seems to consistently
trade Recall for Precision (see also Section 6.1.2). Moreover,
its performance is aﬀected by the poor mention recognition,
as inspected further.

Of course it is hard to judge an algorithm by the perfor-
mance of the system implementing it, since many variables
contribute: implementation (bugs, optimizations), conﬁgu-
ration, the Wikipedia version it is based upon, etc. Con-
sequently, our results show which system (according to its
conﬁguration available to the public) performs best, and so
they give only a clue of the pro/cons of its underlying algo-
rithm.

Let’s answer another question. If we consider the anno-
tated entities that diﬀer from the ground truth, how close
are they to the correct ones? In order to make our analy-
sis as complete as possible, we estimated, for each system,
the distribution of the Milne-Witten’s relatedness [16] be-
tween the false-positive entities found by the systems and
the corresponding (namely, those whose mentions overlap)
entities in the ground-truth. The closer to 1 is the relat-
edness, the better is the correspondence between the entity
annotated by the system and the one annotated by humans.
Notice that graph-based measures could be used directly to
deﬁne weak entity annotation relation (Me). We leave this
for future work.
Figure 5 gives, for r ∈ [0, 1], the cumulative percentage of
the number of false-output vs ground-truth entities whose

255Dataset

AIDA/CO-
NLL
(avg-len
1039 chars)

AQUAINT
(avg-len
1415 chars)

MSNBC
(avg-len
3316 chars)

Annotator
AIDA-Local
AIDA-
CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner
AIDA-Local
AIDA-
CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner
AIDA-Local
AIDA-
CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner

Best t F 1mic Pmic Rmic
33.4
0.000
0.000
34.0

72.8
74.1

45.8
46.7

0.000
0.086
0.523
0.258
0.570
0.000
0.000

0.000
0.078
0.523
0.188
0.570
0.000
0.000

0.000
0.070
0.477
0.188
0.758

39.8
35.2
54.0
58.3
49.7
22.2
21.2

22.0
27.6
34.2
50.7
47.2
47.2
47.4

35.2
33.5
40.8
51.6
46.0

63.3
31.2
56.0
61.4
46.9
37.0
35.4

36.8
20.1
29.0
45.9
37.8
74.3
74.6

55.1
31.8
34.1
48.5
54.9

29.1
40.4
52.1
55.5
52.8
15.8
15.1

15.7
44.0
41.7
56.7
62.9
34.6
34.8

25.8
35.2
50.9
55.0
39.5

s
n
o

i
t

t

a
o
n
n
A

 1

 0.8

 0.6

 0.4

 0.2

 0

 1

FP relatedness aidaconll

AIDA - PriorityOnly
AIDA - Local
AIDA - CocktailParty
Wikipedia Miner
Illinois Wikifier
TagMe 2
DBPedia Spotlight

 0.8

 0.6

 0.4

 0.2

 0

MW relatedness

Figure 5: Distribution of Milne-Witten’s relatedness
between gold/false-output entities whose mentions
overlap.

Table 5: Results based on the weak-annotation
match among all annotators over news datasets.
Column F 1mic
indicates the maximum micro-F1
. Columns
computed for the best score-threshold t
indicate the micro-precision and
Pmic and Rmic
∗
. Figures are given as a per-
micro-recall for that t
centage.

∗

Dataset

AIDA/CO-
NLL
(avg-len
1039 chars)

Annotator
AIDA-Local
AIDA-
CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner

Best t F 1mic Pmic Rmic
0.000
42.8
42.8
0.000

93.3
93.3

58.7
58.7

0.000
0.086
0.367
0.203
0.477

58.7
50.4
68.5
74.6
70.0

93.3
44.5
60.5
72.1
63.1

42.8
57.9
78.9
77.3
78.7

Dataset

AIDA/CO-
NLL
(avg-len
1039 chars)

Annotator
AIDA-Local
AIDA-
CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner

Best t F 1mic Pmic Rmic
33.2
0.000
0.000
33.9

72.4
73.7

45.6
46.4

0.000
0.086
0.523
0.289
0.586

39.6
33.9
50.7
56.7
46.0

62.9
30.0
48.7
62.1
40.1

28.9
38.9
52.9
52.1
53.9

Table 6: Results based on the strong-annotation
match over the AIDA/CONLL dataset.

A comment is in order at this point.

relatedness is above r. The highest a line is, the most enti-
ties close to the correct ones are found. For r = 0.5, TagMe
ﬁnds entities more closely related to the correct ones, while
Illinois Wikiﬁer is the worst, Wikipedia Miner and DBpe-
dia Spotlight are about the same, as are AIDA-PriorityOnly
and AIDA-CocktailParty. For r = 0.8, AIDA-CocktailParty
and Aida-Local achieve the best position showing their ef-
fectiveness in ﬁnding entities closer to the correct matches.
In [7], AIDA has
been evaluated considering its ability to solve the D2W prob-
lem, namely passing the mentions contained in the dataset
to the system and restricting its evaluation to only those
mentions. This way, mentions found by the tested system
but not contained in the dataset were discarded, and thus,
not counted as errors. This is clearly a more restricted
annotation challenge than the one investigated in this pa-
per. Moreover,
in that paper, AIDA’s parameters were
tuned over the CONLL dataset, thus achieving the inter-
esting ﬁgures of 0.816 macro-precision@1 and 0.825 micro-
precision@1. These results are clearly not comparable with
those showed in Table 5. We also tried to tune TagMe
over this speciﬁc dataset and achieved comparable perfor-

Table 7: Results based on the weak-mention match
over the AIDA/CONLL dataset.

mance.12 This shows that there is ample room for improve-
ment for those systems whenever the annotation problem is
restricted, dataset is ﬁxed and tuning is possible. Precise
results on TagMe’s performance in this speciﬁc scenario will
be posted on its website.
6.1.2 Finding just the mentions
For the sake of space we report in Table 7 the results
only for F 1micro and over the AIDA/CONLL dataset, by
using the (weak) mention-annotation match Mm. This ba-
sically shows the performance of the NER algorithms used
by the various annotation systems. TagMe oﬀers the best
parsing thanks to its very high precision. The second best is
Wikipedia Miner, Illinois Wikiﬁer is very close, then AIDA’s
variants which all use the Stanford NER tagger, and ﬁnally
DBpedia Spotlight. AIDA’s NER algorithm has a poor F1
because of a very low recall. This ranking holds also for the
other news datasets, i.e. AQUAINT and MSNBC. The good
result obtained by TagMe and Wikipedia Miner is proba-
bly due to the choice of the mention corpus: their use of
Wikipedia anchors and titles from the one hand does not let
those systems recognize all the mentions contained in a text,
but on the other hand it selects better candidates, given the
well-deﬁned mention corpus (drawn from Wikipedia), and
this leads to a better overall performance.

12The same training was not possible over the other systems,
since they do not provide an interface for training.

256Dataset

AIDA/CO-
NLL
(avg-len
1039 chars)

Annotator
AIDA-Local
AIDA-
CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner

Best t F 1mic Pmic Rmic
43.0
0.000
0.000
42.7

77.6
79.8

55.3
55.7

0.000
0.117
0.523
0.336
0.586

47.7
35.9
56.6
65.6
50.9

65.8
37.6
53.5
68.8
47.1

37.4
34.4
60.2
62.7
55.3

Table 8: Results based on entity-match for the
AIDA/CONLL dataset.

6.1.3 Finding just the entities
The performance for this task can be computed via the re-
lation Me. Table 8 reports the results for the AIDA/CONLL
dataset. TagMe outperforms the other annotators in terms
of F1 and recall, while the highest precision is achieved by
AIDA-CocktailParty, at the cost of very low recall. The
CocktailParty variant seems to be the best choice for AIDA’s
variants, resulting slightly better than Local and signiﬁ-
cantly better than PriorityOnly. By comparing entities match
against weak-mentions match, we notice that the smallest
drop in performance is for TagMe (9% in micro-F1), the
largest is for Wikipedia Miner (about 19%), Illinois Wiki-
ﬁer (about 12%) and then DBpedia Spotlight (about 15%).
AIDA’s drop depends on the variant (from about 3% to
11%): This suggests that the most robust disambiguator is
TagMe’s one, together with AIDA-CocktailParty’s; however,
the better NER parser makes TagMe the absolute best in the
whole annotation process. This ranking among the systems
holds also for the other news datasets, namely AQUAINT
and MSNBC.

Similarity between systems

6.1.4
Table 9 reports the values of the similarity measure Simmacro

using as match relation both the (weak) mention-annotation
match Mm and the entity-annotation match Me. Both simi-
larities are computed on the output produced by the systems
when the score-threshold t is set to its value maximizing the
annotator performance on either the mention-detection or
the entity-match (see Tables 7 and 8).

As expected, the highest similarities are between the three
variants of AIDA. A similarity around 65% is shown among
Illinois Wikiﬁer, Wikipedia Miner and TagMe, which are
rather dissimilar to the AIDA system.

We have seen the similarity on the whole output of two
systems, but how good are the dissimilar annotations? To
answer this question, we have inspected these annotations
by counting the amount of true and false positives. Most
correct dissimilarities fall in the range of 30-40%, meaning
that the annotations found by one system and not by the
other are mostly wrong. But there are some exceptions, the
most notable being that the dissimilarity of both AIDA-
Local and AIDA-CocktailParty against all other systems
has a true-positive rate of 53-62%.
In particular, the dis-
similarity against Wikipedia Miner is 61% correct, against
TagMe it is 62% correct and against Illinois Wikiﬁer it is 53%
correct. This demonstrates that AIDA-Local and AIDA-
CocktailParty ﬁnd a set of annotations that are not found
by the other annotators but still are mostly correct. This
suggests that an improved annotator could be designed with
a NER algorithm and a disambiguation algorithm combining
those of, e.g., TagMe and AIDA-CocktailParty.

DBPed.
Spotl.

AIDA-
AIDA-
Prior-
Cocktail-
Only
Party
87/91 41/41
93/96
100/100 85/89 42/41

Illinois
Wik.

TagMe
2

Wikip.
Miner

42/49
42/49

47/48
46/49

50/56
49/55

100/100 43/41

41/47

47/48

51/56

100/100 45/46

53/51

55/48

100/100 63/68 65/69

100/100 70/68

100/100

AIDA-Local
AIDA-
CocktailParty
AIDA-
PriorityOnly
DBPedia
Spotlight
Illinois Wiki-
ﬁer
TagMe 2
Wikipedia
Miner

Table 9: Simmacro similarity among systems over the
AIDA/CONLL dataset, computed both for recogni-
tion of mentions/entities.

Dataset

Meij
(avg-len
80 chars)

Annotator
AIDA-Local
AIDA-CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner

Best t. F 1mic Pmic Rmic
15.1
0.000
14.8
0.000
16.4
0.469
37.4
0.102
35.3
0.406
0.133
46.9
47.3
0.289

53.0
51.7
73.5
29.7
51.0
43.8
48.4

23.6
23.0
26.8
33.1
41.7
45.3
47.9

Table 10: Results based on strong tag match for the
Meij dataset.

6.2 Experiment 2: Tweets

This experiment aims at testing systems over very-short
and poorly composed texts, as tweets. This makes it hard
for NER algorithms to recognize the mentions and ﬁnd per-
tinent entities. As expected results on tweets are lower than
those on news stories. A similar study on Twitter’s posts
was conducted by E. Meij and was published on his private
blog13, but focused on diﬀerent measures and was conducted
on a few/diﬀerent set of annotators. Since Meij’s dataset is
for C2W, the output of known systems must be adapted
by taking only the annotations whose score is higher than
a threshold. Since only TagMe is speciﬁcally designed to
address the annotation of short texts (as CMNS), this ex-
periment is aimed also at testing the ﬂexibility of all other
systems.

Wikipedia Miner proves the best in F1 (because of its very
good recall), followed by TagMe. Illinois Wikiﬁer achieves
worse results, especially in terms of recall, while DBpe-
dia Spotlight loses signiﬁcantly in precision. All variants
of AIDA have a poor performance, although with highest
precision; this is likely due to the poor performance of the
Stanford parser on this type of text.
6.3 Experiment 3: Web pages

This experiment aims at testing systems over long texts
drawn from Web pages. This dataset is challenging for
TagMe which was designed for very-short texts only. The
key diﬃculty is that attempting to optimize global coherence
of the annotations, as many systems do, may be dangerous
because of concept drifts in the long input text. Moreover,
the text provided by the IITB dataset includes, without pro-
viding any structure, accessory textual elements taken from

13

http://edgar.meij.pro/comparison-semantic-

labeling-algorithms-twitter-data/

257Dataset

IITB
(avg-len
3879 chars)

Annotator
AIDA-Local
AIDA-CocktailParty
AIDA-PriorityOnly
DBPedia Spotlight
Illinois Wikiﬁer
TagMe 2
Wikipedia Miner

Best t F 1mic Pmic Rmic
0.000
0.000
0.000
0.023
0.438
0.102
0.219

66.3
65.7
63.6
46.2
58.0
45.2
56.8

4.1
4.1
4.0
50.0
35.9
42.0
48.2

7.7
7.6
7.5
48.0
44.3
43.6
52.2

Table 11: Results based on the weak-annotation
match over the IITB dataset.

System

Meij

AIDA-Local
2228
AIDA-CocktailParty 865
78
AIDA-PriorityOnly
1057
DBPedia Spotlight
102
Illinois Wikiﬁer
1
TagMe 2
17
Wikipedia Miner

AIDA/-
CO-
NLL
13003
16876
623
9045
1277
33
171

AQUA-
INT

MSNBC IITB

12030
13913
633
168
1303
42
126

22957
28335
1187
1289
3891
103
1282

21535
30402
1363
1236
2801
127
1245

Table 12: Avg. annotation time per document (ms).

the web page (like menus, footers, etc.) that are generally
not semantically linked to the main body.

Table 11 shows that Wikipedia Miner is the best annota-
tor, closely followed by DBpedia Spotlight, the third being
Illinois Wikiﬁer and then TagMe. However, unlike the news
datasets with long texts (i.e. AQUAINT, MBSC), AIDA is
worse because of the low recall of the Stanford NER Tagger.

6.4 Runtime efﬁciency

Time eﬃciency is a key feature of entity-annotation sys-
tems because it impacts onto their scalability. Of course,
network latency and the HW/SW features of the remote/-
local servers may aﬀect time measures. Nevertheless, we
adopted some care in order to make these evaluations un-
biased (see Table 12 for a summary). For Illinois Wikiﬁer,
which run locally, the runtime has been recorded as the dif-
ference between the system time after and before the library
call. For AIDA, the time has been recorded as the one re-
turned by the method getOverallRuntime() of the RMI
Service, and thus it does not include the network latency.
Similarly, as TagMe’s runtime, it has been considered that
included in the response returned by the RESTful API. For
Wikipedia Miner and DBpedia Spotlight, accessed through
a Web service, the time has been recorded as the diﬀerence
between the system time after and before the query, and
the timing has been calibrated subtracting the time needed
to process an empty query, which should account for the
network latency.

TagMe is deﬁnitely the fastest annotator, being about
10 times faster than Wikipedia Miner, which is the second
one. DBpedia Spotlight is also fast, except for a peak of
9s over AIDA/CONNL and 1s on Meij. Illinois Wikiﬁer is
about three times slower. AIDA-PriorityOnly is the fastest
one among all AIDA variants, due to the basic disambigua-
tion process, AIDA-Local and CocktailParty are slower, be-
ing about 300 times slower than TagMe. For all systems,
the runtime looks linear in the size of the text – which is
good. A special case is the dataset of Meij, for which texts
are very short; TagMe is extremely fast, due to the engi-
neering tuned on short texts, then comes Wikipedia Miner,

 0.8

 0.7

 0.6

 0.5

1
F

 0.4

 0.3

 0.2

 0.1

 0

 10

Runtime - Best F1

T

a

g

M

e

 

2

W

i
k
i

p

e

d

i

a

 

M

i

n

A

I

D

e
r

A

-

P

Illi

n

o

i
s
 

W

i
k
ifi

e
r

ri

o

rit

y

O

n

l
y

A

I

A

I

D

D

A

A

-

-
L

C

o

o

c

c

k
t

a

l

a

il

P

a
r
t
y

D

B

P

e

d

i

a

 

S

p

o

tli

g

h

t

 100

 1000

 10000

 100000

Runtime (msec)

Figure 6: Average runtime (in log-scale) for dataset
AIDA/CONLL and best achieved F1 measures
(metrics based on weak annotation match).

AIDA-PriorityOnly and Illinois Wikiﬁer, the others being
way slower.

We conclude by reporting in Figure 6 a 2d plot of the
runtime and the best F1 of each tested annotator over the
AIDA/CONLL dataset. For all other datasets the plotting
is similar. The net result is that TagMe dominates all other
systems in terms of runtime and F1, AIDA-priority is faster
but at the prize of lower eﬀectiveness. Despite the diﬃcul-
ties indicated before concerning the evaluation of systems’
eﬃciency, our ﬁgures diﬀer so much that they provide a rea-
sonable picture of systems’ speed.

7. CONCLUSIONS

We designed,

implemented and tested a benchmarking
framework to fairly and fully compare entity-annotation sys-
tems. Our goal was to help improving the understanding of
this challenging new problem. Our framework is easily ex-
tensible with new annotation systems, new datasets, new
matching relations and new evaluation measures. It is writ-
ten in Java and it has been released to the public as open
source code. Using the framework we provided the ﬁrst com-
plete comparison among all publicly available entity annota-
tors over all available datasets for the entity-annotation task,
showing that some of these annotators are very eﬀective and
ready to scale over large datasets and real applications. We
believe that our framework will spur new research on this
interesting topic, providing a common ground upon which
researchers will test the eﬃciency and eﬀectiveness of their
proposals.

8. ACKNOWLEDGMENTS

We thank all our colleagues who provided prompt sup-
port for their software, datasets and web services: Johannes
Hoﬀart, Edwin Lewis-Kelham, Edgar Meij, Lev Ratinov,
Mark Sammons, Ugo Scaiella, Daniele Vitale, Gerhard We-
ikum, Ian Witten. We also thank the Wikimedia Foun-
dation and the Wikipedia contributors, whose knowledge-
sharing philosophy made this work possible. This research
was supported by the PRIN ARS-technomedia grant and by
a Google research award 2013.

2589. REFERENCES
[1] I. Bordino, G. De Francisci Morales, I. Weber, and F.
Bonchi. From Machu Picchu to “rafting the urubamba
river”: Anticipating information needs via the
Entity-Query Graph. In Proc. WSDM, 2013 (to
appear).

[2] S. Cucerzan. Large-scale named entity disambiguation
based on wikipedia data. In Proc. EMNLP and CNLL,
708–716, 2007.

[3] P. Ferragina and U. Scaiella. Tagme: on-the-ﬂy

annotation of short text fragments (by Wikipedia
entities). In Proc. CIKM, 1625–1628, 2010.

[4] P. Ferragina and U. Scaiella. Fast and accurate

annotation of short texts with Wikipedia pages. IEEE
Software, 29(1): 70-75, 2012.

[5] E. Gabrilovich and S. Markovitch. Wikipedia-based

semantic interpretation for natural language
processing. J. Artif. Int. Res., 34(1):443–498, 2009.

[6] J. Hoﬀart, F. M. Suchanek, K. Berberich,

E. Lewis-Kelham, G. de Melo, and G. Weikum. Yago2:
exploring and querying world knowledge in time,
space, context, and many languages. In Proc. WWW,
229–232, 2011.

[7] J. Hoﬀart, M. A. Yosef, I. Bordino, H. F¨urstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. Robust disambiguation of named entities
in text. In Proc. EMNLP, 782–792, 2011.

[8] P. Jiang, H. Hou, L. Chen, S. Chen, C. Yao, C. Li,

and M. Wang. Wiki3C: Exploiting Wikipedia for
Context-aware Concept Categorization. In Proc.
WSDM, 2013 (to appear).

[9] S. Kulkarni, A. Singh, G. Ramakrishnan, and

S. Chakrabarti. Collective annotation of Wikipedia
entities in web text. In ACM KDD, 457–466, 2009.

[10] C.D. Manning, P. Raghavan, and H. Sch¨utze.

Introduction to information retrieval. Cambridge
University Press, 2008.

[11] O. Medelyan, D. Milne, C. Legg, and I. H. Witten.

Mining meaning from Wikipedia. Int. J.
Hum.-Comput. Stud., 67(9):716–754, 2009.

[12] E. Meij. A Comparison of ﬁve semantic linking

algorithms on tweets. Personal Blog:
http://alturl.com/aujuc, 2012.

[13] E. Meij, W. Weerkamp, and M. de Rijke. Adding

semantics to microblog posts. In Proc. WSDM,
563–572, 2012.

[14] P.N. Mendes, M. Jakob, A. Garc´ıa-Silva, and C. Bizer,

DBpedia spotlight: shedding light on the web of
documents. In Proc. I-SEMANTICS, 1–8, 2011.

[15] R. Mihalcea and A. Csomai. Wikify!: linking

documents to encyclopedic knowledge. In Proc. ACM
CIKM, 233–242, 2007.

[16] D. Milne and I. H. Witten. Learning to link with

wikipedia. In Proc. CIKM, 509–518, 2008.

[17] D. Milne and I. H. Witten. An eﬀective, low-cost

measure of semantic relatedness obtained from
Wikipedia links. AAAI Workshop on Wikipedia and
Artiﬁcial Intelligence, 2008.

[18] L. Ratinov and D. Roth. Design challenges and

misconceptions in named entity recognition. In Proc.
CoNLL, 147–155, 2009.

[19] L. Ratinov, D. Roth, D. Downey, and M. Anderson.

Local and global algorithms for disambiguation to
wikipedia. In Proc. ACL-HLT, 1375–1384, 2011.

[20] G. Rizzo, R. Troncy, S. Hellmann, and M. Bruemmer.

Nerd meets nif: Lifting nlp extraction results to the
linked data cloud. In Proc. LDOW, 2012.

[21] U. Scaiella, P. Ferragina, A. Marino, M. Ciaramita.

Topical Clustering of Search Results. In Proc. WSDM,
223-232, 2012.

[22] D. Vitale, P. Ferragina, U. Scaiella. Classiﬁcation of

short texts by deploying topical annotations. In Proc.
ECIR, 376-387, 2012.

[23] M. A. Yosef, J. Hoﬀart, I. Bordino, M. Spaniol, and

G. Weikum. AIDA: an online tool for accurate
disambiguation of named entities in text and tables.
PVLDB, 4(12):1450–1453, 2011.

259