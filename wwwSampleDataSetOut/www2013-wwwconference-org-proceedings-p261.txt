A Framework for Learning Web Wrappers from the Crowd

Valter Crescenzi, Paolo Merialdo, Disheng Qiu

Dipartimento di Ingegneria

Università degli Studi Roma Tre

Via della Vasca Navale, 79 – Rome, Italy

{crescenz, merialdo, disheng}@dia.uniroma3.it

ABSTRACT
The development of solutions to scale the extraction of data
from Web sources is still a challenging issue. High accuracy
can be achieved by supervised approaches but the costs of
training data, i.e., annotations over a set of sample pages,
limit their scalability. Crowd sourcing platforms are making
the manual annotation process more aﬀordable. However,
the tasks demanded to these platforms should be extremely
simple, to be performed by non-expert people, and their
number should be minimized, to contain the costs. We intro-
duce a framework to support a supervised wrapper inference
system with training data generated by the crowd. Training
data are labeled values generated by means of membership
queries, the simplest form of queries, posed to the crowd.
We show that the costs of producing the training data are
strongly aﬀected by the expressiveness of the wrapper for-
malism and by the choice of the training set. Traditional
supervised wrapper inference approaches use a statically de-
(cid:12)ned formalism, assuming it is able to express the wrapper.
Conversely, we present an inference algorithm that dynam-
ically chooses the expressiveness of the wrapper formalism
and actively selects the training set, while minimizing the
number of membership queries to the crowd. We report the
results of experiments on real web sources to con(cid:12)rm the
eﬀectiveness and the feasibility of the approach.

Categories and Subject Descriptors
H.3.5 [Information Storage and Retrieval]: On-line In-
formation Services |Web-based services

General Terms
Algorithms, Experimentation

Keywords
wrapper generation, crowdsourcing, active learning

1.

INTRODUCTION

Although many research eﬀorts concentrated on the de-
velopment of methods and tools to generate web wrappers,
large-scale data extraction is still a challenging issue.

Early proposals to infer web wrappers for data intensive
websites were based on supervised approaches. Wrappers

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

were generated starting from a set of training data, typi-
cally provided as labeled values, i.e., annotated pages. To
overcome the need of human intervention in the production
of training data, unsupervised approaches have been investi-
gated. They exploit the local regularities of script-generated
web pages to infer a wrapper. Unsupervised approaches
adopt sophisticated algorithms to generate the wrappers,
and represent an attempt to \scale-up" the wrapper gener-
ation process. Unfortunately, although they eliminate the
costs of training data, they have a limited applicability be-
cause of the low precision of the produced wrappers.

The recent advent of crowd sourcing platforms (such as,
for example, Amazon Mechanical Turk) can open new op-
portunities for supervised approaches. These platforms pro-
vide support for managing and assigning mini-tasks to peo-
ple. In the wrapper production process, crowd sourcing plat-
forms can be used to produce massive training data for su-
pervised wrapper inference systems. As they facilitate the
involvement of a large number of persons to produce the
training data, we may say that they represent a solution to
\scale-out" the wrapper generation process. However, to ob-
tain an eﬃcient and eﬀective process, two main issues need
to be addressed. First, since mini-tasks are performed by
non-expert people, they should be extremely simple. Sec-
ond, since the costs of producing wrappers become propor-
tional to the number of mini-tasks, the number of training
data produced by the crowd to infer a wrapper should be
minimized.

We are developing a system that relies on crowd sourc-
ing platforms to create accurate web wrappers. Our system
adopts a supervised approach to infer wrappers with train-
ing data generated by means of a crowd computing plat-
form. The mini-tasks submitted to the platform consist of
membership queries (MQ), which are the simplest form of
queries, since they admit only a yes/no answer (e.g., \Ob-
serve this page: is the string ‘Dean Martin’ a correct value
to extract?") [1]. To address the costs issue, our system is
able to select the queries that more quickly bring to infer an
accurate wrapper, thus minimizing the number of mini-tasks
assigned to the crowd platform.

The traditional approach to build wrappers for large web-
sites is to provide training data for the attributes of interest
on a set of pages, and then to apply an inference algorithm
to learn a wrapper. We observe that there are two hidden
assumptions behind the approach: 1) the formalism used by
the learning algorithm to specify the wrapper is suﬃciently
expressive, 2) the wrappers inferred from the sample set,
hopefully work also on the whole set of pages.

261As the following running example illustrates, these as-
sumptions can largely aﬀect the learning costs and the qual-
ity of the solution.

1.1 Running Example

Suppose we are interested in extracting data about ac-
tors from the pages of a large movie website. A wrapper
corresponds to a set of extraction rules, where each rule ex-
tracts the value of an attribute (e.g. the actor name). For
the sake of simplicity, we concentrate on a single rule, but
the discussion can be trivially extended to the more general
case where a wrapper is composed of a set of rules. To illus-
trate the example without digging into the technical details
of the formalisms used to express the extraction rules, we
represent pages as tables, where data is organized in rows
and columns, as shown in Figure 1(a). In such a simpli(cid:12)ed
abstraction, an extraction rule speci(cid:12)es the cell containing
the relevant data, and it can be expressed by absolute coordi-
nates (e.g., (cid:12)rst row, second column), denoted abs(row,col),
or by relative coordinates, that is, with respect to another
cell (e.g. the (cid:12)rst cell located right of/left of/above/under
the cell containing ‘Home’), denoted right-of(‘x’). For exam-
ple, according to Hopkins’s page, candidate extraction rules
for Name are abs(1,1) and above(‘Height’). Similarly, rules
for Home are abs(6,2) and right-of(‘Home’). More complex
relative rules can be speci(cid:12)ed with a richer (more expres-
sive) class, which admits longer paths from a pivot; e.g.,
below-right-of(‘Born’), to extract data from the cell contain-
ing the ‘Latest Film’.

Later in the paper, we shall refer to a concrete represen-
tation (the same used in the implementation of the system),
where pages are modeled with their DOM trees, and rules
are XPath expressions.

The Expressiveness Problem. As the following example
illustrates, the number of training data to correctly infer a
wrapper depends on the expressiveness of the language used
to specify the rules.

Let us concentrate on the extraction of actors’ Name. Sup-
pose that the learning algorithm adopts only absolute ex-
traction rules: a correct rule for Name is abs(1,1). Note
that only one training data would suﬃce to infer this rule.
Suppose now to adopt a more expressive language, which
also includes relative rules. Using just one labeled value, say
‘Antony Hopkins’ on page ph, several rules can be gener-
ated to extract the attribute Name: abs(1,1), above(‘Height’),
above(‘1.74m’). To determine the correct rule it is necessary
to carefully choose at least another annotation: only with
the help of the labeled value ‘Laura Wolf’ on page pw we
have evidence that above(‘Height’) does not work as an ex-
traction rule for Name.

This simple example illustrates a well known learning the-
ory results: the more expressive is the model, the larger is
the number of labeled examples that are necessary to infer
the correct rule [1, 6] (intuitively, the space of hypotheses
is larger and thus more examples are needed to discard the
incorrect ones).

Supervised wrapper inference approaches that ignore the
costs of training data tend to work with an overly expressive
class of rules to have enough expressiveness for all the at-
tributes. However, as our example emphasizes, this implies
more expensive training data.

The Sampling Problem. Suppose that Home is an attribute
to extract. If the sample set is composed only of awarded
actors (such as Hopkins), the inferred rule could not work
for the broader set of all actors, including those without
any award (such as Smith and Wolf). For example, the rule
abs(6,2) for Home might work for the awarded actors, but it
does not extract the required information for others.

The usual approach to address this issue is to require a
large set of labeled values that hopefully covers all the possi-
ble types of target pages. However, if the labeled values are
generated by submitting a task to a crowd sourcing plat-
form, a bigger set implies higher costs; also, the training
data should be representative for the whole collection of tar-
get pages and its choice is not trivial. For some domains a
set of labeled examples can be obtained automatically. For
instance, if we have a small database of popular actors, we
can annotate the pages when they oﬀer data matching with
those stored in the database [8]. However, the database
could be biased, e.g. it contains only famous (awarded) ac-
tors, leading to the generation of wrong extraction rules, as
discussed in the example.

1.2 Overview and Contributions

We propose a logical framework based on original solu-
tions for exploiting crowd platforms to infer wrappers around
large web sources. Since we aim at demanding to a crowd
platform the burden of generating labeled examples, our ap-
proach considers a cost takes into account the number of
membership queries submitted to a crowd platform.

Our framework includes a supervised active learning algo-
rithm that aims at minimizing the number of membership
queries to infer a wrapper. Since the costs of learning a
wrapper depends on the expressiveness of the class of rules,
unlike traditional approaches we do not work with a stat-
ically de(cid:12)ned class, but we propose an original approach
inspired to a statistical learning technique [14, 15], called
structural risk minimization (SRM), in which the expres-
siveness of the language is determined at runtime. To this
end, we organize the class of candidate rules into a hierarchy
of classes of increasing expressiveness:
initially the correct
rule is searched only within the less expressive class. The
class of rules is lazily expanded only if it is actually needed.
A fundamental decision is whether and when expanding
the set of candidate rules: we introduce a probabilistic model
that evaluates the quality of the wrapper by computing the
probability that the rules in the current class of candidate
rules are correct. Our learning algorithm exploits the prob-
abilistic model in order to trigger the expansion only if there
is enough evidence that the correct rule is not amongst the
current set of candidates.

The algorithm adopts active learning techniques [13] to
ask for additional labeled values: it selects a value and poses
a membership query to obtain a con(cid:12)rmation about the cor-
rectness of the extracted value. By accurately choosing this
query, the user interaction is minimized. Our experiments
prove that our learning algorithm can infer high quality
wrappers with a fraction of the queries required by a tra-
ditional approach.

Our algorithm infers the wrapper on a set of labeled val-
ues, and the quality model evaluates the wrapper on a larger
set, ideally on the whole set of target pages. However, in
many practical cases the evaluation on the whole set is unre-
alistic because of its size. To overcome this issue, our frame-

2621

2

1

2

1

2

Laura Wolf

FR
Latest Film One week (2007)

01/09/91

1
2 Born
3
4 Home
5

Ranking

laura.w.me

5500

pw

v+
0 = john.s.me

(d) initial ann. for Home

1
2 Height
3 Born
4
5 Awards
6 Home
7

Ranking

Latest Film

Anthony Hopkins
1.74m

31/12/37 UK

Noah (2014)

Oscar

anthony.h.me

34

ph

r1
r2
r3
r4
r5
r6

abs(5,2)
right-of(‘Home’)
above(‘1002’)
below(‘Waco (1966)’)
below-right-of(‘Latest Film’)
above-right-of(‘Ranking’)

(b) extraction rules

John Smith

1
2 Height
3 Born
4
5 Home
6

Ranking

Latest Film

1.88m

06/03/31 US

Waco (1966)

john.s.me

1002

ps

(a) set of pages

r1
Oscar
john.s.me
5500

r2, r6
anthony.h.me
john.s.me
laura.w.me

r3, r4
nil
john.s.me
nil

r5
Oscar
john.s.me
laura.w.me

ph
ps
pw

(c) extracted values

Figure 1: Running Example

work also includes an algorithm to compute a small set of
representative pages: the extraction rules inferred and eval-
uated against our representative set also work on the larger
set of target pages.

Our experiments show that our algorithm is able to se-
lect a representative set several orders of magnitude smaller
than the whole set of target pages, and that wrappers in-
ferred from our representative sample outperforms (in term
of precision and recall) wrappers generated from much larger
randomly selected sets.

In summary, we make the following contributions:
(cid:15) a framework that exploits crowd platforms to infer

wrappers around large web sources;

(cid:15) a cost model that takes into account both the process-
ing costs and the human intervention costs needed to
feed the crowd platform;

(cid:15) a probabilistic quality model for computing correctness
of a wrapper over the whole set of pages even if it is
inferred on a smaller set of pages chosen by a sampling
algorithm;

(cid:15) an active learning algorithm for generating high qual-

ity wrappers in a cost-eﬀective manner;

(cid:15) a sampling algorithm for selecting small yet represen-

tative sets of pages;

(cid:15) the results of an experimental evaluation of our frame-

work on real life websites.

The paper is organized as follows: Section 2 formalizes
our setting; Section 3 develops our probabilistic model to
characterize the correctness of extraction rules; based on the
model, Section 4 presents the active learning algorithm to in-
fer extraction rules; Section 5 introduces the sampling algo-
rithm; Section 6 discusses experiments with a set of sources
from the Web; (cid:12)nally, Section 7 discusses related work and
Section 8 concludes the paper.

2. PRELIMINARIES

Let U = fp1; p2 : : : png be a set of pages. Every page
publishes several attributes of interest (e.g., in our running
example, actors’ Ranking, Height, etc.). For simplicity, we
develop the discussion concentrating on one attribute, and
we assume that its values are either a textual leaf of the
DOM tree representation of the pages, or a distinguished
nil value. We write v 2 p to denote that v is a value of the
page p, and pv to denote the page in which the value v is
located.

We refer to a generic extraction rule (or simply rule) r
over the set of pages U as a concrete tool to build a vector of
values indexed by the pages in U such that r(p) 2 p[fnilg.
Every rule extracts one vector of values from U denoted
r(U ). Figure 1(c) shows the vectors extracted by the rules
r1, r2, r3, r4, r5, r6 in Figure 1(b). We denote by R(U )
the set of vectors obtained by applying a set of rules R over
U , and blur the distinction between a rule and the vector
it extracts from U . Note that jR(U )j (cid:20) jRj, with the strict
inequality holding whenever a vector is extracted by diﬀerent
rules.
We introduce the concept of labeled sample value (or sim-
ply labeled value) vl where v 2 pv is a value from a page pv,
and l 2 f+;(cid:0)g is either a positive or a negative label. In
the following v+ and v
denote a positively labeled value
(or annotation) and a negative labeled value, respectively,
i.e., the two possible answers to a MQ.

(cid:0)

A rule r is admissible wrt a set of labelled values L (de-

noted L(r)) iﬀ:

L(r) , 8vl 2 L;

l = + ! r(pv) = v
l = (cid:0) ! r(pv) ̸= v
that is, it is compliant with the labels in the set.
The concept can be trivially extended to a set of rules R.
We denote by RL = fr 2 R : L(r)g the subset of admissible
L (U ) all the values they extract

rules in R wrt L, and by bV R
from U : bV R

L (U ) = fv : v = r(p); r 2 RL; p 2 Ug.

Example 1. Let ph, ps and pw be the pages in Figure 1(a)
and let U = fph; ps; pwg. The attribute Home is extracted
by the rule r2 =right-of(‘Home’): two positive annotations

2630 ; v+

0 =’john.s.me’ and v+

are v+
1 =’laura.w.me’; a negative la-
(cid:0)
2 =’5500’. Observe that r2 is admissible wrt
belled value is v
L = fv+
g. Now consider another rule r1=abs(5,2)
(cid:0)
1 ; v
and the set of rules R = fr1; r2g. Then r1 is not admissible
2
wrt L since r1(pw) =’5500’ which is the negatively labelled
L (U ) = fv0; v1; v3g
value v
where v3 =’anthony.h.me’.

2 . Hence, RL = fr2g and bV R

(cid:0)

In the following, given a set of rules R, we will only con-
sider special ordered sets of labeled values, called training
sequences, which are formed by an initial set of positive an-
notations, and then by adding only new values which are still
admissible with respect to those already seen. Intuitively, a
training sequence lists the answers to the MQ posed to learn
a extraction rule.
A Training Sequence (t.s.) L wrt a set of rules R and a
set of pages U is speci(cid:12)ed by a sequence of labeled values
that de(cid:12)nes a sequence of (observed) sets Lk with Lk+1 =
Lk[fvkg = fv+
a(cid:0)1; va : : : ; vkg such that: (i) it begins
with sequence of a (cid:21) 1 annotations v+
̸= nil with
positive labels, and (ii) 8k (cid:21) a; vk 2 V R
Lk (U ) n
Lk.

Lk (U ) = bV R

0 ; : : : ; v+

0 ; : : : ; v+

a(cid:0)1

The constraint (i) on the (cid:12)rst annotations of the sequence
is useful to generate a (cid:12)nite set of admissable rules RLa ,
whereas the constraint (ii) on the remaining values entails
that the new value vk that forms Lk+1 from Lk leads to
smaller and smaller admissible sets: RLk+1 (cid:18) RLk .
It is
worth noting that RLk+1 plays the role of what the learning
communities call the version-space [13], i.e. the set of hy-
potheses still plausible after having considered an input set
of labeled values.

L2 (U ) = bV R

Example 2. Consider again the above Example 1 and
is
g and RL2 = fr2; r6; r5g. Possible candidate
L2 (U )nL2 = f’anthony.h.me’; ’Oscar’g.

our running example in Figure 1. Then a possible t.s.
L2 = fv+
0 ; v+
1
values are V R
A new MQ can be formed by choosing a new value v2 to
query from the elements in V R
L2 (U ). E.g., \ is ’anthony.h.me’
a correct value? ".

In the following we will uniformly refer to both L and one
of its observed subsets Lk blurring the diﬀerences between
the two concepts whenever the context clari(cid:12)es which one is
actually involved.

It can always be decided whether a rule extracting the
desired vector exists. However, since it is not known in ad-
vance whether that rule was in the set of all candidate rules,
the only certain way to be sure of its presence is by checking
every single page [1].

3. PROBABILISTIC MODEL TO

EVALUATE WRAPPER QUALITY

We now introduce a probabilistic model for evaluating the
quality of a wrapper expressed as an extraction rule r taken
from a class of candidate extraction rules R.
Our model computes: (i) the probability P (rjLk+1) that
a rule r is correct, observed a t.s. Lk+1; (ii) the probability
P (RjLk+1) that the correct rule is not in R, observed Lk+1,
i.e., that a correct rule has not been generated at all. Table 1
summarizes the notations used for the main events covered
by our analysis, and their probabilities. By applying Bayes’

Table 1: The main events of the bayesian analysis

Probability Notation

P(R) / P(R)
jr; Lk)
jR; Lk)

P (vl
k

P (vl
k

P (Lk+1) = P (vlk

k ; Lk)

Event
prior probability that a/none rule in R
is correct
likelihood of vl
observed Lk
likelihood of vl
is not in RLk , observed Lk
probability of a t.s. Lk+1

k if the correct rule

k if r is correct,

theorem:

P (rjLk+1) =

P (vl
k

jLk)

jr; Lk)P (rjLk)
P (vl
k
jR; Lk)P (RjLk)
P (vl
k

(1)

P (vl
k

P (RjLk+1) =
(2)
jLk) is a normalization factor that can be ex-

jLk)

where P (vl
k

pressed as:∑

kjri; Lk)P (rijLk) + P (vl

kjR; Lk)P (RjLk)

P (vl

Lk

r2R
For any k, P (rjLk+1) and P (RjLk+1) can be de(cid:12)ned iter-
jR; Lk), P (RjLk) and
atively by means of P (vl
jR; Lk) can
P (rjLk). The probabilities P (vl
k
be de(cid:12)ned by abstracting the actual process that leads to
the observed t.s.
into a simple generative model. This is
essentially equivalent to de(cid:12)ne a p.d.f. over every t.s.

jr; Lk) and P (vl

jr; Lk), P (vl

k

k

k

By repeatedly applying the bayesian updating rules ex-
pressed by equations 1 and 2, the model allows the com-
putation of P (RjLk+1) and P (rjLk+1) for any k, starting
from prior-probabilities P (RjLa) = P(R) of having gener-
ated a correct rule in R, and the probability P (rjR; La) of
r being a correct rule once the t.s. La has been observed.
The iteration continues until admissible rules exist, i.e., until
Lk ̸= ∅.
R
3.1 Bootstrapping Probabilities
For bootstrapping our probabilistic model, the following
probabilities are needed: (i) the prior probability P(R) that
a correct rule has been generated in the class of rules R,
and; (ii) the probability P (rjR; La) that the extraction rule
r 2 R does extract the correct vector of values from the
input set of pages U once its has been observed the initial
set of annotations La.
For the former prior P(R), we follow a standard approach,
and estimate it by measuring the frequency of the involved
events on a suﬃciently large set of attributes: P(R) has been
(cid:12)xed to nh
N where nh is the number of attributes extracted
by a rule in R and N is the number of attributes sampled
(we considered N = 290 attributes).
As regards the latter p.d.f. P (rjR; La), if jRLa (U )j > 0 we
redistribute P(R) according to a uniform p.d.f. over all the
vectors extracted by admissible rules r 2 RLa , as follows:

P (rjR; La) =
where: n = jfr

n

jRLa (U )j (cid:1) P(R);
′ 2 RLa (U ) : r

; r

′

′

(U ) = r(U )gj:

3.2 Generative Model for Training Sequences
We now describe the generative model for the training
it is used to derive the posterior probability

sequences:

264(cid:0)

R

P (vl

(Lk; r) = V

kjr; Lk) =

1
R
jV
Lk (U )j
0

P (rjLk) that r(U ) is the correct vector to extract once a
t.s. Lk has been observed. This vector is not known in ad-
vance, but the values forming the t.s. Lk will be labeled as
either positive or negative according to it.
Let P(R) be the prior probability that the correct vector
can be extracted by a rule belonging to R. We suppose that
the acquisition of a new labeled value vk to form Lk+1 from
Lk follows a uniform p.d.f. amongst all values still queryable,
Lk (U ) n Lk. Similarly, given a
R
i.e., the values in V
Lk (U )\ r(U ) denote the set
R
correct rule r, let V +(Lk; r) = V
of all and only the values that can form new positive values,
Lk (U ) n V +(Lk; r) the set of values that
R
and V
{
can form negative values. It follows:

Lk (U ) = bV

; iﬀ vk 2 V l(Lk; r)
; otherwise
jR; Lk) following an ap-
proach based on a uniform p.d.f. over all possible values.
R
These are essentially all the values in V
Lk (U ) but only the
Lk (U ) can be labeled either positive or
values in V
negative (and we assume with the same probability) while
Lk (U ) will surely be labeled nega-
the values in V
tive. Therefore, it follows that P (vl
k

jR; Lk) =
Lk (U ) \ R
Lk (U )j ; iﬀ vk 2 V
R
Lk (U ) n R
; iﬀ vk 2 V
R
Lk (U )j
; iﬀ vk ̸2 V
R
Lk (U )
Note that the exact computation of the set R
expensive, since given a value v 2 V
out whether v 2 R
very large number of vectors in R

Lk (U ) can be
R
Lk (U ), in order to (cid:12)gure
Lk (U ), we should enumerate a potentially

8>><>>: P (v+

Similarly, we can compute P (vl
k

R
Lk (U )\R
2(cid:1)jV
R
jV
Lk (U )nR

jR; Lk)=
jR; Lk)=

Lk (U )\R
R

Lk (U ) n R
R

k
(cid:0)
P (v
k

1

1

0

Lk (U ).

Lk (U )

Lk (U )

{

We adopt an approximate and eﬃcient solution based on
the assumption that the equivalences holding for k = 0:1
R
Lk (U ) = ∅, also hold for
any k > 0. Hence, it can be rewritten as:

R
Lk (U ) and V

Lk (U ) n R
R

Lk (U ) = V

kjR; Lk) ≃

P (vl

1
R
Lk (U )j

2(cid:1)jV
0

; iﬀ vk 2 V
; iﬀ vk ̸2 V

R
Lk (U )
R
Lk (U )

(3)

Actually, this is an oversimpli(cid:12)cation when k gets bigger
R
Lk (U ) and V
Lk (U ) gets smaller
Lk (U ) ̸= ∅. Since the algorithm

and approaches jUj: both R
Lk (U ) n R
R
and smaller and V
looks for the correct rule while minimizing k, in our setting
this sempli(cid:12)cation does not signi(cid:12)cantly aﬀect the results.

4. LEARNING EXTRACTION RULES

The probabilistic model developed in the previous section
aims at computing, observed a t.s. Lk+1, the probability
P (rjLk+1) that a given extraction rule r within a set of
candidate rules R is correct, and the probability P (R
Lk+1 )
that the correct rule is not inside R.

We now present an algorithm, called alf, that exploits the
model to infer a wrapper. alf aims at inferring extraction
rules with a high probability of correctness, while minimizing
the length of the t.s., i.e., the number of membership queries
to be posed to the crowd.

1Admitting that every value is extracted at least by one rule.

As we discussed in Section 1.1, the length of the t.s. de-
pends on the expressiveness of the class of rules. Rather
then relying on a statically designed (and possibly oversize)
class of extraction rules, alf organizes the class of candidate
rules R into a hierarchy of classes fRhg0(cid:20)h(cid:20)m of increasing
expressiveness.
The algorithm starts by looking for a rule within the class
R0 of the lowest expressiveness and computes the probabil-
ity of its correctness. If such a probability is not satisfactory,
the algorithm expands the class to the larger class R1, and
consequently poses more membership queries, thus enlarg-
ing the t.s. In order to choose the appropriate membership
queries, alf uses an active approach by selecting the best
queries to minimize its total number as further discussed
in Section 4.1. The process is repeated until either it (cid:12)nds
a rule of satisfactory probability, or it concludes that it is
unlikely that this rule exists within the considered hierarchy.
The approach is independent of the details of the formal-
ism used to express the extraction rules. In our implemen-
tation, we make use of XPath expressions; namely, we use
absolute and relative XPath expressions. The former specify
paths from the root to the leaf node that contains the value
to be extracted; the latter are paths starting from a generic
pivoting node.2 Relative XPath expressions are classi(cid:12)ed
based on the path length.3 Our hierarchy fRhg organizes
absolute and relative XPath expressions as follows: R0 is
the class of absolute XPath expressions, Rh, for 0 < h (cid:20) m,
is obtained by adding to Rh(cid:0)1 the class of relative XPath
rules with path length h.

g

a(cid:0)1

Lk+1 );

o ; : : : ; v+

Lk+1 , P (Rh

Listing 1 alf: An active learning algorithms for extraction
rules
Input: a set of pages U = fp1; : : : ; pjUjg
Input: a set of initial annotations La = fv+
Parameter: a hierarchy of rules fRhg over U
Output: P (rjLk+1) over r 2 Rh
1: let h   0;
2: let R   Rh
La ;
3: while (R ̸= ∅ and not halt(R; Lk)) do
vk   chooseQuestion(R; Lk);
4:
l   oracle(vk);
5:
Lk+1   Lk [ fvl
6:
7: R   Rh
compute P (rjLk+1); 8r 2 R according to eq. 1;
8:
compute P (RhjLk+1) according to eq. 2;
9:
h   h + expandRuleSet(R; Lk+1);
10:
k   k + 1;
11:
12: end while
13: if (R ̸= ∅) then
return Rh
14:
15: end if
16: return ?;

Lk+1 , P (rjLk+1) and P (RhjLk+1);

Lk+1 ;

g;

k

2In the current prototype only textual leaves are used as
candidate pivot.
3The distance from the pivot to the extracted node is mea-
sured according to the number of edges crossed in the DOM
representation of the HTML pages but considering contigu-
ous siblings at distance 1.

265Listing 1 contains the pseudo-code of the alf algorithm:
as input it takes a t.s. L built by actively [13] asking to an
oracle (here modeled by means of the subprogram oracle())
the label of a value chosen by the subprogram choose-
Question(); as output, it returns a p.d.f. describing the
probability of correctness over the rules still admissible, and
the possibility that a correct rules does not exist at all.
Initially, R0 is taken as initial set of candidate rules, and
the set of rules admissible wrt the initial annotations R0
La
is computed (lines 1-2).
In every iteration, the oracle is
asked to label a new value vk (lines 4-5) and the t.s. is up-
dated to obtain Lk+1 (line 6). Then, the set of admissible
rules is updated (line 7) (recall that RLk+1 (cid:18) RLk ), and the
probabilities P (rjLk+1) and P (R
Lk+1 ) are consequently up-
dated (lines 8-9). expandRuleSet() has to decide whether
the set of candidate rule should be expanded (line 10).

alf can be instantiated by appropriately choosing the se-
mantics of three subprograms: chooseQuestion(), which
composes the next membership query, i.e., it selects the next
value to be labeled by the user; halt(), which establishes
an exit criterion before the t.s. naturally expires (i.e., R
becomes empty); expandRuleSet(), which decides at run-
time whether Rh should be expanded with new candidate
rules by incrementing h. The latter decision is based on the
probability that the current class of rules cannot contain
a correct rule (P (Rh
Lk+1 )): the higher its value, the more
likely new candidate rules are needed, and thus the class of
rules needs to be expanded.

We now describes several strategies to instantiate the three

subprograms.
4.1 Asking the Right Questions

The chooseQuestion() procedure chooses the next mem-
bership query: it decides the next value to be labeled. We
propose three alternative strategies: Entropy, Greedy,
and Lucky, plus a baseline algorithm Random.
Random: It chooses a random admissible value:
chooseQuestion(R,L) f return a random v 2 V R
and it serves as a baseline against other strategies.
Entropy: It bases the choice on the p.d.f. of the extracted
values: a simple strategy is to choose the value on which
rules most disagree, appropriately weighted according to
their probability. This is equivalent to compute the vote

L (U ); g

entropy [13] for each v 2 RLk (U ):
H(v) = (cid:0)[P (v+jLk) log P (v+jLk) + P (v
∑
∑

P (v+jLk) =

r2fr2R

where:

(cid:0)jLk) =

P (v

and:

r2fr2R

(cid:0)jLk) log P (v
(cid:0)jLk)]
(4)
Lk :r(pv )=vg P (rjLk);
Lk :r(pv )̸=vg P (rjLk):

are the probabilities that v is either a value to extract or an
incorrect value, respectively. The next value is that maxi-
mizing the vote entropy:

chooseQuestion(R,L) f return argmaxv2V R

L (U ) H(v); g

This choice essentially removes the most uncertain value.
Greedy: The construction of the whole version-space is in-
eﬃcient, since it requires to enumerate all possible t.s.. How-
ever, the version-space can be exploited to (cid:12)nd the quickest

t.s. con(cid:12)rming that a given rule is a solution. Let us call
such a kind of sequences con(cid:12)rming t.s.: they aim more at
deciding as quickly as possible that a given rule is a solution,
rather than at (cid:12)nding which is the solution.

In every search step, Greedy \elects" the most likely rule
to play the role of the solution, and then it greedily builds
a con(cid:12)rming t.s. wrt that conjecture.
If, after a few la-
beled values, that rule is confuted and removed from the
version-space, the whole process is repeated by formulat-
ing another conjecture around the most likely rule in the
remaining version-space.

In this setting, the query is selected by greedily taking
the value extracted by the supposedly \correct" rule from
the page on which most other rules behaves diﬀerently:
if
that value is labeled positive as expected, the largest number
of rules is removed from the version-space.
(cid:3)

chooseQuestion(R,L) f return r
where:

(cid:3)
= argmaxr2RL(U ) P (rjL);
= argmaxp2U

) g
jfr(p) : r(p) ̸= r

(cid:3)

(p)gj:

(cid:3)
r
(cid:3)
p

(p

As the cost of this approach depends on the size of the
version-space, it can be relevant in the early stages of the
searching. The next variant delays its construction until
the best rule emerges as signi(cid:12)cantly more likely than other
candidates.
Lucky: It is a hybrid of the former two approaches, and it
works in two phases: (cid:12)rst, it accumulates enough evidence of
the correctness of a rule by using Entropy; then, it switches
to Greedy modality to con(cid:12)rm it. The switch is triggered
by a (cid:12)xed threshold (cid:21)r(cid:3) on the probability of the most likely
rule r

This approach can be seen as a generalization of Greedy:
at the beginning it waits to observe enough evidence before
allocating all its trust on the most likely rule.

(cid:3)

.

Example 3. Reconsider the running example in Figure 1,
and the t.s. L1 = fjohn.s.me+g. Suppose that P(R) = 0:96
and that the probability is equally distributed among the set
of candidate rules.

P (v+
1

jL1) and P (v

(cid:0)
1

jL1) can be computed as follow:

v1

anthony.h.me

laura.w.me

Oscar
5500
: : :

P (v+
1
0:24
0:24 (cid:1) 2
0:24 (cid:1) 2
0:24
: : :

jL1) P (v

(cid:0)
1

jL1)
0:24 (cid:1) 3
0:24 (cid:1) 2
0:24 (cid:1) 2
0:24 (cid:1) 3

: : :

From P (vl
obtained as follows:

1jL1) by using Eq. 4 the entropy H(v) can be

v1

anthony.h.me (cid:0)0:24 (cid:1) log(0:24) (cid:0) 0:72 (cid:1) log(0:72) = 0:58
(cid:0)0:48 (cid:1) log(0:48) (cid:0) 0:48 (cid:1) log(0:48) = 0:70
laura.w.me
(cid:0)0:48 (cid:1) log(0:48) (cid:0) 0:48 (cid:1) log(0:48) = 0:70
(cid:0)0:24 (cid:1) log(0:24) (cid:0) 0:72 (cid:1) log(0:72) = 0:58

H(v1)

Oscar
5500
: : :

: : :

Hence, Entropy can chooses v1 = laura.w.me or v1 =
Oscar as the next value to query to get L2 = L1 [fv1g. Note
that P (rijL1) = 0:24, i = 1; : : : ; 6 (rules extracting the same
vector are indistinguishable) and the set of admissible rules
after L1 are equally probable. In this case, Greedy would
end up with a random selection of the most likely rule.

266It makes use of the probability P (Rh

4.2 SRM: Dynamically Expanding the Rule Set
expandRuleSet() is in charge of deciding whether and
when expanding the set of candidate rules used from a hi-
erarchy of classes Rh. We refer to this technique as SRM,
since it is inspired by the Structural Risk Minimization prin-
ciple originally proposed by the statistical learning commu-
nity [15, 14] as a tool for dealing with the problem of over-
(cid:12)tting.
Lk ) that the correct
rule is not present in the current set of candidate rules Rh
after observing as input a given t.s. Lk.
We use a simple implementation expandRuleSet() based
on a prede(cid:12)ned (cid:12)xed threshold (cid:21)R over P (R
expandRuleSet(R, L) f
if (R = Rm) return 0; // max expansion reached
if (P (RL) > (cid:21)R ) return +1;
else return 0;

Lk ):

g

The set of rules is therefore enlarged lazily, i.e., only when-
ever according to P (RL) there is evidence that a correct rule
is not amongst the currently available candidates.
4.3 Termination strategies

The implementation of halt() depends on the overall goal
of the search strategy. A simple approach considers a mini-
mum threshold (cid:21)r on the probability of the best rule:

halt(R, L) f return (argmaxr2RL

P (rjL) > (cid:21)r); g

This function looks for the best rule r that suits the t.s.
and terminates as soon as it (cid:12)nds a rule with probability
higher then (cid:21)r. It is an appropriate solution in many prac-
tical settings.

5. SAMPLING STRATEGIES

So far we considered feasible the application of our al-
gorithm alf to the whole set of input page. However, in
many practical cases this assumption is unrealistic because
of the number of pages (e.g., consider www.imdb.com, which
provides more than 6 (cid:1) 106 pages about actors). Finding a
sampling set that is \cheaper" to work on, and yet it repre-
sents a larger population, is a traditional statistic problem.
In this section we contextualize this issue in our setting and
move to the related problem of sampling the input pages into
a much smaller set of sample pages. The extraction rules can
be evaluated on the sample set much more eﬃciently than
on the whole set of pages; at the same time, a represen-
tative sample set must preserve the power of diﬀerentiating
the rules by showing all their diﬀerences. However, the sam-
ple pages need to be carefully selected to be representative
while, at the same time, minimizing their number.

These aspects are often neglected in the literature. Typ-
ically, sample pages are selected randomly, or they are col-
lected following straightforward crawling strategies. While
random samples could end up not representing the whole
set of pages, crawling strategies can lead to the composition
of biased samples. As an example, www.imdb.com exposes
its content mainly in the form of top-lists, such as top-list
movies, top-list actors and so on. A crawler following the
links in these lists will inherently collect biased samples con-
centrated around \famous" instances.

We formulate the problem of (cid:12)nding a set I (cid:26) U such
that jIj ≪ jUj yet I is representative (with respect to a
given class of extraction rules R) of all the pages in U . The
representativeness of a set of pages I (cid:26) U wrt a set of rules
R can be formalized by introducing the disagreement set of
two extraction rules.

Given a set of pages P , and a set of rules R, the dis-
agreement set, denoted as DP (ri; rj), between two rules
ri; rj 2 R, is the set of pages in P making observable their
diﬀerences: DP (ri; rj) = fp 2 P : ri(p) ̸= rj(p)g, i.e., the
subset of pages in P on which ri and rj extract diﬀerent
values. Two rules ri, rj extract from P the same vector of
values, and hence are indistinguishable for our purposes, if
and only if DP (ri; rj) = ∅.
We say that a subset I (cid:18) U is representative of U wrt a

set of rules R if and only if:

8ri; rj 2 R; [DI (ri; rj) = ∅ () DU (ri; rj) = ∅]:

In other terms, I is representative of U wrt to R if all the
diﬀerences amongst the rules in R are also observable on I.

Example 4. Consider again our running example in Fig-
ure 1 and suppose that I = fps; pwg, while U = fph; ps; pwg.
I does not represent U since DU (r2; r5) = fphg whereas
DI (r2; r5) = ∅.

Given the set of input pages U , and the class of rules R,
there exist many representative subsets, including U itself.
As discussed above, our goal is to (cid:12)nd a small sample set.
Finding the smallest one is an instance of the well-known
Set Cover problem: a page diﬀerentiates the set of rules
that extract distinct values from it.4 Set covering is an NP-
complete problem but actually we do not need to compute
the optimal sample set: it suﬃces to estimate it by consid-
ering a small but not necessary minimal set of pages.

Listing 2 proposes PageSampler, a greedy sampling al-
gorithm to extract a representative set of pages I wrt a class
of rules R from a large set of input pages U in O(jUj(cid:1)jRLaj)
time and O(jRLaj) space.

Listing 2 PageSampler: A greedy sampling strategy
Input: a set of pages U ;
Input: a class of rules R;
Input: a set of initial annotations La;
Output: a set I (cid:18) U that is representative of U ; wrt R
1: let I = ∅;
2: let n = 0;
3: for p 2 U do
4:
5:
6:
7:
end if
8: end for
9: return I;

if (jRLa (I [ fpg)j > n) then

I   I [ fpg;
n   jRLa (I)j;

PageSampler processes the whole set of pages U (lines 3-
It maintains a set of pages I, initially empty, that is

8).

4The problem reduces to (cid:12)nding the smallest set of pages
such that the union of the sets of rules diﬀerentiated from
them equals the set of rules diﬀerentiated directly by U .

267Entity
Site
www.imdb.com
Actor
www.imdb.com
Movie
www.allmusic.com Band
www.allmusic.com Album
www.nasdaq.com

Stock quote

jUj
5 (cid:1) 105
5 (cid:1) 105
5 (cid:1) 105
5 (cid:1) 105
7 (cid:1) 103

jICj

30
42
36
29
15

Strategy
Random
Greedy
Lucky

Entropy

#MQ

#MQ

SRM oﬀ

SRM on

379
398
196
205

190
169
132
116

%MQ Precision
Saved
SRM on
50%
58%
33%
44%

0.998
0.998
0.996
0.998

Recall
SRM on

0.977
0.983
0.995
0.99

Table 2: Dataset 1

Table 3: Total number of MQ for Dataset 1

representative wrt the subset of pages already processed. It
selects as representative only those pages that increase the
number of diﬀerent vectors extracted by the set of admissible
rules RLa (line 4). The pages selected according to this
criterion make observable new diﬀerences between at least
two rules that were otherwise indistinguishable in the subset
of pages processed until the previous iteration.

Example 5. Consider the running example and suppose
that PageSampler has already processed ps and pw, pro-
ducing I = fps; pwg. Let RL1 = fr2; r5; r6g be the set of
0 = fjohn.s.meg. The pages
admissible rules wrt to L1 = v+
in I do not diﬀerentiate r5 from r2 and r6: r2(I) = r6(I) =
r5(I). However, when processing the next page ph, Page-
Sampler detects the diﬀerent behavior of r5 wrt other rules:
r2(ph) = r6(ph) ̸= r5(ph), and then adds it to I.

To clarify how PageSampler is related to the disagree-
ment sets, consider that if jRLa (I [ fpg)j > jRLa (I)j it fol-
lows that there exist at least two rules ri; rj 2 RLa such
that ri(p) ̸= rj(p) and DI[fpg
(ri; rj) n DI (ri; rj) = fpg.
Conversely, if jRLa (I [ fpg)j = jRLa (I)j then it follows that
(ri; rj) n DI (ri; rj) = ∅;8ri; rj 2 RLa . Therefore,
DI[fpg
PageSampler maintains the representativeness of I for the
subset of U already processed by adding a page p to I if and
only if p changes the disagreement sets of the rules.

6. EXPERIMENTS

In this section we describe the experiments conducted to
evaluate our approach. Section 6.1 presents the results of
the learning algorithm alf. Our experiments mainly con-
centrate on the impact of the SRM technique, which dynam-
ically expands the class of the extraction rules: the results
show that, thanks to SRM, alf always reduces the number
of membership queries, without penalizing precision and re-
call of the generated rules. Section 6.2 illustrates the results
of experiments to evaluate the eﬀectiveness of the sampling
strategy implemented by the PageSampler algorithm. Our
experimental results show that a few dozens of pages selected
by PageSampler are suﬃcient to represent large collections
of 105 pages from real-life websites.
6.1 Learning with ALF

We considered two distinct datasets to evaluate the learn-

ing algorithm alf.

The (cid:12)rst dataset has been obtained by downloading pages
from large websites related to speci(cid:12)c domain entities, as
shown in Table 2. We wrote ad-hoc crawling programs, and
let them collect around 5 (cid:1) 105 pages for each entity from
www.imdb.com and www.allmusic.com, and all the available
pages about stock quotes from www.nasdaq.com (around 7 (cid:1)
103). For each entity we selected about 10 attributes, for a
total of 40 attributes. We manually crafted a golden XPath

jrg (U )\r(U )j

; R =

jrg (U )\r(U )j

jrg (U )j

rule for every attribute to extract its values. The (non-null)
values extracted by the golden rules over the whole sets of
pages were then used to compute and evaluate the precision
and recall of the best rule inferred by our learning algorithm
alf. For each rule r generated by our algorithm wrt a golden
rule rg, we used the standard metrics of precision (P ), and

jr(U )j

recall (R), as follows: P =

.
We run the PageSampler algorithm over these sample
sets of pages to derive a representative sample for every do-
main (the sizes of the input sets, jUj, and of representative
samples, jICj, are shown in Table 2). Over these sets we run
the alf algorithm to infer the extraction rules or the target
attributes. In this experiment we set the probability thresh-
old that governs the halt condition to 0:9, and the maximum
expressiveness to R5.

We were mainly interested to evaluate the impact of the
SRM technique used by alf and the diﬀerent strategies to
choose the next membership query. Table 3 summarizes the
results of the experiment. We report the number of mem-
bership queries (#MQ) with and without the SRM tech-
nique for all the chooseQuestion() strategies. Observe
that SRM almost halves #MQ. The most eﬃcient strategy
is Entropy, which signi(cid:12)cantly outperforms the baseline,
represented by Random, and Greedy. However, there is
a small price to pay: without SRM, a perfect rule is found
(precision and recall equal 1.0 not shown in the Table 3),
whereas SRM introduces a loss lower than 1%. This is due
to early decisions made by the SRM technique: sometimes
it decides to bet on the current, and imprecise, set of can-
didate rules rather than expanding the class and searching
inside larger classes.

Another experiment aimed at considering the behavior of
alf by using diﬀerent chooseQuestion() strategies, wrt
the size of the hypothesis space, which in our context corre-
sponds to the number of admissible vectors after the initial
annotations, i.e., jRLa (I)j.
Intuitively, the size of the hy-
pothesis space is a measure of the cost that any learning
algorithm needs to pay to infer a rule.5

The plots in Figure 2 show the average number of mem-
bership queries vs size of the hypothesis space. Observe that
the SRM technique (plot on the bottom) always reduces the
number of queries needed by Entropy wrt the case in which
it uses a (cid:12)xed expressiveness. Also, note that when jRLa (I)j
is low, the diﬀerences in terms of #MQ are not apparent.
On the contrary, when jRLa (I)j ≫ 5, Random performs
worse than other strategies. Entropy and Lucky outper-
form the other approaches and, as expected from an active
learning algorithm [13], #MQ follows a logarithmic trend
with respect to the size of the hypothesis space.

5This is strictly related to the sample complexity commonly
used by the machine learning community, as the amount of
training data to learn a concept [3].

268Domain

Movies

Actors

Stocks

Albums

Bands

Sampling
Crawler IB
Random IR

Representative IC

Crawler IB
Random IR

Representative IC

Crawler IB
Random IR

Representative IC

Crawler IB
Random IR

Representative IC

Crawler IB
Random IR

Representative IC

jIj
250
250
42
250
250
30
86
86
15
258
258
29
289
289
36

P

0.98
0.99
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

R

0.71
0.99
1.00
1.00
0.96
1.00
0.98
0.99
1.00
0.99
1.00
1.00
0.68
1.00
1.00

Table 5: Precision and recall with diﬀerent sampling
strategies

6.2 Sampling with PAGESAMPLER

We now discuss the experiments to evaluate the sampling
algorithm PageSampler. For this evaluation we used the
pages of the (cid:12)rst dataset (Table 2). We collected three sam-
ple sets I according to diﬀerent strategies, as follows:
(cid:15) IB represents a \biased sample": many large websites
propose navigation paths to facilitate the browsing to-
wards lists of relevant objects (e.g. famous actors, top-
stocks, etc.). In our experiments, for each entity we
downloaded the pages from the (cid:12)rst list proposed by
the sites. Therefore the size jIBj corresponds to the
dimension of the proposed list.

(cid:15) IR is a set of pages randomly selected from the whole

set U of pages with jIRj equals jIBj.

(cid:15) IC is the representative sample set as computed by our
sampling algorithm starting from the pages collected
in our data set. jICj is determined by the algorithm.
The (cid:12)rst strategy does not pick up pages from the whole
set of input pages U , while the second one chooses the sam-
ple pages in an uninformed way. These sampling strategies
are used by many wrapper inference approaches more fo-
cused on the inference phase rather than on the sampling.

To evaluate the role of the three sampling strategies, Ta-
ble 5 reports the average precision and recall computed over
the attributes of all the entities of each domain. We inferred
the extraction rules by running alf (without SRM) on the
three samples obtained. We obtained perfect rules when
the inference was performed on the representative sample
IC ; conversely, both the random set IR and the biased set
IB loose precision and recall for a majority of cases, with a
signi(cid:12)cant lost of recall with IB for bands and movies.

Table 5 also reports the size of the samples:

it is worth
observing that the representative sample set IC is always
much smaller than the random sample set IR. This is an
important point as it aﬀects the running times of the learn-
ing algorithm, which performs better when working on small
samples. Figure 3 illustrates this issue: the graphic plots the
average wrapper learning times (in logarithmic scale) vs the
size of the random sample jIRj (number of pages). The curve
associated to IR describes the learning times to compute the
wrapper using a random sample of increasing size. As an ex-
ample, for a random sample of 50 pages, it runs in about 15

Figure 2: #MQ vs size of the hypothesis space with
SRM disabled (top) and enabled (bottom)

Strategy
Random
Greedy
Lucky

Entropy

#MQ

#MQ

SRM oﬀ

SRM on

1092
977
903
880

728
764
684
683

%MQ
Saved
34%
22%
25%
25%

Table 4: Total number of MQ for Dataset 2

It is interesting to observe that Greedy with SRM ex-
hibits very good performances with attributes with a large
hypothesis space, while it performs like Random with SRM
disabled. The explanation is that Greedy concentrates the
queries on the most likely hypothesis by building the whole
version-space in order to (cid:12)nd the shortest con(cid:12)rming t.s.
However, our probabilistic model equally redistributes the
uniform prior p.d.f. among all the admissible rules, and
Greedy ends up choosing, randomly, among a set of rules
of the same probability. SRM solves this issue, as it does
with Random, by concentrating the random queries around
the most likely class of rules.

To evaluate SRM in a diﬀerent setting we used another
dataset composed by a large number of attributes (250) from
100 websites, including popular ones, such as amazon.com,
youtube.com, and ebay.com. Also for the attributes of this
dataset, we manually wrote the golden rules. However, for
each website we downloaded a small number of pages (a few
dozens). As a consequence, we obtained a dataset with a
large number of attributes, but with a limited hypothesis
space. Even in this dataset, the application of SRM pro-
duces signi(cid:12)cant improvements, as reported in Table 4 (we
do not report precision and recall, since we did not register
any loss in this experiment).

 0 2 4 6 8 10 12 14 0 5 10 15 20 25 30 35 40MQsize of hypothesis space (|RLa(I)|)GreedyRandomLuckyEntropy 0 2 4 6 8 10 12 14 0 5 10 15 20 25 30 35 40MQsize of hypothesis space (|RLa(I)|)SRM GreedySRM RandomSRM LuckySRM Entropy269hypotheses statically, i.e., before performing the inference.
Once set, the set of candidate rules cannot be changed with-
out seriously revisiting the inference algorithm. Therefore
they usually oversize the expressiveness of the formal lan-
guage used to specify the extraction rules and additional
samples are required only to compensate with the excess of
expressiveness.

In this paper we concentrate on training data provided by
means of a human intervention. A diﬀerent solution to ex-
ploit supervised approaches without any human intervention
consists of relying on existing repositories to automatically
annotate web pages [8]. Unfortunately, in many domains
suitable data does not exist at all (consider pages that pub-
lish subjective values, such as customer ratings, or real time
data, such as stock quote prices). Also, the existing reposi-
tories might be biased over speci(cid:12)c instances (typically, the
most popular): such a biased information will annotate just
a subset of the target pages, possibly preventing the gener-
ation of a valid wrapper.

Active learning approaches for wrapper induction have
been proposed in [9, 12]. However, also in these works the
expressiveness is statically de(cid:12)ned. The latter approach re-
quires complex user interaction, since the user has to choose
the correct wrapper within a set of ranked proposals.

A few recent proposals try to scale the wrapper inference
to the web scale [8, 10]. In [8] the authors leverage an avail-
able dataset, but they ignore the presence of biased samples
(as suggested by its running example based on popular ob-
jects itself), while in [10] it is needed domain knowledge that
only an human expert can provide.

8. CONCLUSIONS AND FUTURE WORK
Our work is mainly motivated by the success of crowd
sourcing platforms, which can be used to scale wrapper gen-
eration. We propose a framework that allows supervised
inference with simple mermbership queries suitable for the
non-expert workers of a crowd platform. An original al-
gorithm, alf, applies active learning techniques to infer a
wrapper, while minimizing the number of queries. alf dy-
namically sets the expressiveness of the wrapper formalism,
leading to a signi(cid:12)cant reduction of the number of queries
needed to infer a wrapper. It does not depend on the speci(cid:12)c
classes of rules presented in the paper, and can be instanti-
ated with other formalisms. We developed a complimentary
sampling algorithm, PageSampler, to select for the learn-
ing phase a small yet representative set of sample pages from
a much larger set of pages to wrap.

Experimental results prove the eﬀectiveness of the ap-
proach. The dynamic expansion of the expressiveness of the
wrapper formalism reduces the number of queries to learn
a wrapper, with tangible cost savings. The sampling strat-
egy leads to the selection of a small number of samples that
eﬀectively represents a much larger set of pages.

The quality model and the cost model proposed in our
framework are the basis for future developments. For in-
stance, the cost in term of total dollars spent for inferring a
wrapper of the desidered quality can take into account both
the cost of a PageSampler execution over large collection
of pages on a cloud platform (such as EC2) and the number
of MQ required by alf on a crowd platform.

We are studying strategies to further optimize the interac-
tions with the crowd. Namely, we are studying extensions of
our bayesian model in order to manage worker’s mistakes.

Figure 3: Wrapper learning times vs sample size

secs; for a random sample of 450 pages, it runs in 100 secs.
The curve associated to the representative sample IC reports
the learning times to infer the wrapper over a representative
sample IC whose pages have been selected from a random
sample IR with that number of pages. For example, from a
random sample of jIRj = 450 pages, PageSampler selected
a representative sample composed of jICj = 25 pages, and on
this sample alf inferred the wrapper in about 7 secs. Com-
puting the representative sample has its own costs. How-
ever, as we can observe from the curves on Figure 3, even
counting these costs the overall computation cost (sampling
IR to compute IC + learning on IC ) is lower than the time
required by learning without sampling (learning on IR).

7. RELATED WORK

In machine learning, the number of labeled samples needed
by a supervised learning algorithm to infer a good hypoth-
esis is called sample complexity [3], and has been studied
from several perspectives. For instance, similarly to our set-
ting, [1] discusses the problem of exactly inferring a concept,
i.e., a set of elements, by means of membership queries, i.e.,
question of the type \is this an element of the target con-
cept?". However, the main idea underlying our approach has
been proposed by the statistical learning community [15], in
which a loss function is given in order to characterize the
quality of the produced hypothesis.

The structural risk minimization (SRM) [14], i.e., the de-
composition of the set of hypotheses into a hierarchy of sub-
classes, aims at avoiding the over(cid:12)tting problem: since the
class of hypotheses studied by this community might be so
expressive to be able to arbitrarily reduce the loss, a trade-
oﬀ with other quality criteria is needed to avoid that the
learning algorithm selects the hypothesis perfectly describ-
ing the training data, rather than their underlying patterns.
Many researchers have proposed several variations of the
learning paradigm to make it practically feasible in diﬀerent
applicative contexts: the learning approaches in which the
inference algorithm is free to choose which sample to label
next are usually de(cid:12)ned active [13]. These have recently
gained interest, since, as clari(cid:12)ed in [3], they might produce
exponential improvements over the number of samples wrt
traditional supervised approaches.

To the best of our knowledge and diﬀerently from our pro-
posal, all the approaches for inferring wrappers over struc-
tured websites developed by the researchers in the wrap-
per inference community [2, 4, 7, 11, 16], de(cid:12)ne the set of

 1 10 100 50 100 150 200 250 300 350 400 450time (secs)|IR| (# of pages)Learning on IRFinding ICLearning on IC2709. REFERENCES
[1] D. Angluin. Queries revisited. Theor. Comput. Sci.,

313(2):175{194, 2004.

[2] A. Arasu and H. Garcia-Molina. Extracting structured

data from web pages. In SIGMOD Conference, pages
337{348. ACM, 2003.

[3] M.-F. Balcan, S. Hanneke, and J. W. Vaughan. The
true sample complexity of active learning. Machine
Learning, 80(2-3):111{139, 2010.

[4] C.-H. Chang and S.-C. Lui. IEPAD: information
extraction based on pattern discovery. In WWW,
pages 681{688, 2001.

[5] R. Creo, V. Crescenzi, D. Qiu, and P. Merialdo.

Minimizing the costs of the training data for learning
web wrappers. In VLDS, pages 35{40, 2012.

[6] V. Crescenzi and G. Mecca. Automatic information

extraction from large websites. J. ACM,
51(5):731{779, 2004.

[7] V. Crescenzi and P. Merialdo. Wrapper inference for
ambiguous web pages. Applied Arti(cid:12)cial Intelligence,
22(1&2):21{52, 2008.

[8] N. N. Dalvi, R. Kumar, and M. A. Soliman.

Automatic wrappers for large scale web extraction.
PVLDB, 4(4):219{230, 2011.

[9] U. Irmak and T. Suel. Interactive wrapper generation

with minimal user eﬀort. In WWW, pages 553{563.
ACM, 2006.

[10] T. Furche, G. Gottlob, G. Grasso, O. Gunes, X. Guo,

A. Kravchenko, G. Orsi, C. Schallhart, A. J. Sellers,
and C. Wang. DIADEM: domain-centric, intelligent,
automated data extraction methodology. In WWW
(Companion Volume), pages 267{270. ACM, 2012.
[11] G. Gottlob, C. Koch, R. Baumgartner, M. Herzog,

and S. Flesca. The lixto data extraction project - back
and forth between theory and practice. In PODS,
pages 1{12. ACM, 2004.

[12] I. Muslea, S. Minton, and C. A. Knoblock. Active
learning with multiple views. J. Artif. Intell. Res.
(JAIR), 27:203{233, 2006.

[13] B. Settles. Active learning literature survey. Computer

Sciences Technical Report 1648, University of
Wisconsin{Madison, 2009.

[14] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and

M. Anthony. Structural risk minimization over
data-dependent hierarchies. IEEE Transactions on
Information Theory, 44(5):1926{1940, 1998.

[15] V. Vapnik. An overview of statistical learning theory.

IEEE Transactions on Neural Networks,
10(5):988{999, 1999.

[16] Y. Zhai and B. Liu. Structured data extraction from

the web based on partial tree alignment. IEEE Trans.
Knowl. Data Eng., 18(12):1614{1628, 2006.

271