Crowdsourced Judgement Elicitation with Endogenous

Proﬁciency

Anirban Dasgupta

Yahoo! Labs

Sunnyvale, CA, USA

anirban@yahoo-inc.com

ABSTRACT
Crowdsourcing is now widely used to replace judgement or eval-
uation by an expert authority with an aggregate evaluation from a
number of non-experts, in applications ranging from rating and cat-
egorizing online content all the way to evaluation of student assign-
ments in massively open online courses (MOOCs) via peer grading.
A key issue in these settings, where direct monitoring of both effort
and accuracy is infeasible, is incentivizing agents in the ‘crowd’ to
put in effort to make good evaluations, as well as to truthfully report
their evaluations. We study the design of mechanisms for crowd-
sourced judgement elicitation when workers strategically choose
both their reports and the effort they put into their evaluations. This
leads to a new family of information elicitation problems with un-
observable ground truth, where an agent’s proﬁciency— the prob-
ability with which she correctly evaluates the underlying ground
truth— is endogenously determined by her strategic choice of how
much effort to put into the task.

Our main contribution is a simple, new, mechanism for binary
information elicitation for multiple tasks when agents have endoge-
nous proﬁciencies, with the following properties: (i) Exerting max-
imum effort followed by truthful reporting of observations is a Nash
equilibrium. (ii) This is the equilibrium with maximum payoff to all
agents, even when agents have different maximum proﬁciencies,
can use mixed strategies, and can choose a different strategy for
each of their tasks. Our information elicitation mechanism requires
only minimal bounds on the priors, asks agents to only report their
own evaluations, and does not require any conditions on a diverging
number of agent reports per task to achieve its incentive properties.
The main idea behind our mechanism is to use the presence of mul-
tiple tasks and ratings to estimate a reporting statistic to identify and
penalize low-effort agreement— the mechanism rewards agents for
agreeing with another ‘reference’ report on the same task, but also
penalizes for blind agreement by subtracting out this statistic term,
designed so that agents obtain rewards only when they put in effort
into their observations.

Categories and Subject Descriptors
J.4 [Computer Applications]: Social & Behavioral Sciences

Keywords
Crowdsourcing, Information elicitation, Peer prediction, Mecha-
nism design, Game theory

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

Arpita Ghosh
Cornell University
Ithaca, NY, USA

arpitaghosh@cornell.edu

1.

INTRODUCTION

Crowdsourcing, where a problem or task is broadcast to a crowd
of potential participants for solution, is used for an increasingly
wide variety of tasks on the Web. One particularly common appli-
cation of crowdsourcing is in the context of making evaluations, or
judgements— when the number of evaluations required is too large
for a single expert, a solution is to replace the expert by an evalua-
tion aggregated from a ‘crowd’ recruited on an online crowdsourc-
ing platform such as Amazon Mechanical Turk. Crowdsourced
judgement elicitation is now used for a number of applications such
as image classiﬁcation and labeling, judging the quality of online
content, identifying abusive or adult content, and most recently for
peer grading in online education, where Massively Open Online
Courses (MOOCs) with enrollment in the hundreds of thousands
crowdsource the problem of evaluating assignments submitted by
students back to the class itself. While one issue in the context of
crowdsourcing evaluations is how best to aggregate the evaluations
obtained from the crowd, there is also a key question of eliciting
the best possible evaluations from the crowd in the ﬁrst place.

The problem of designing incentive mechanisms for such crowd-
sourced judgement elicitation scenarios has two aspects. First, sup-
pose each worker has already evaluated, or formed a judgement on,
the tasks allocated to her. Since the ‘ground truth’ for each task
is unknown to the system, a natural solution is to reward work-
ers based on other workers’ reports for the same task (this being
the only available source of information about this ground truth)1.
The problem of designing rewards to incentivize agents to truth-
fully report their observation, rather than, for example, a report that
is more likely to agree with other agents’ reports, is an informa-
tion elicitation problem with unobservable ground truth. Informa-
tion elicitation has been recently been addressed in the literature
in the context of eliciting opinions online (such as user opinions
about products, or experiences with service providers); see §1.1.
However, in those settings, agents (users) have already formed an
opinion after receiving their signal (for example a user who buys
a product forms an opinion about it after buying it)— so agents
only need to be incentivized to incur the cost to report this opinion
and ﬁnd it more proﬁtable to report their opinions truthfully than to
report a different opinion.

In the crowdsourcing settings we consider, however, the user
does not have such a pre-formed, or experiential, opinion anyway,

1It is of course infeasible for a requester to monitor every worker’s
performance on her task, since this would be a problem of the same
scale as simply performing all the tasks herself. We also note that a
naive approach of randomly checking some subset of evaluations,
either via inserting tasks with known responses, or via random
checking by the requester, turns out to be very wasteful of effort
at the scale neccessary to achieve the right incentives.

319but rather forms a judgement as part of her task— further, the ac-
curacy of this judgement depends on whether or not the agent puts
in effort into it (for instance, a worker evaluating whether images
contain objectionable content could put in no effort and declare all
images to be clean, or put in effort into identifying which images
are actually appropriate; a similar choice applies in other contexts
like peer-grading). A key issue in these crowdsourced judgement
elicitation scenarios is therefore incentivizing effort2— that is, en-
suring that agents make the best judgements that they possibly can
(in addition, of course, to ensuring that they then truthfully report
this observation). This leads to a new kind of information elici-
tation problem where an agent’s proﬁciency now depends on her
effort choice, and so is endogenous and unknown to the system—
even if an agent’s maximum proﬁciency is known, the actual proﬁ-
ciency with which she performs a task is an endogenous, strategic
choice and therefore cannot be assumed as ﬁxed or given.

A mechanism for information elicitation in this setting should
make it ‘most beneﬁcial’, if not the only beneﬁcial strategy, for
agents to not just report their observations truthfully, but to also
make the best observations they can in the ﬁrst place. Also, it is
even more important now to ensure that the payoffs from all agents
always blindly reporting the same observation (for instance, declar-
ing all content to be good) are strictly smaller than the payoffs from
truthfully reporting what was actually observed, since declaring all
tasks to be of some predecided type requires no effort and there-
fore incurs no cost, whereas actually putting in effort into making
observations will incur a nonzero cost. Finally, unlike mechanisms
designed for settings where a large audience is being polled for its
opinion about a single event, a mechanism here must retain its in-
centive properties even when there are only a few reports per task—
this is because it can be infeasible, due to either monetary or ef-
fort constraints, to solicit reports from a large number of agents
for each task. (For example, the number of tasks in peer grading
scales linearly with the number of agents, limiting the number of
reports available for each task since each student can only grade a
few assignments; similarly, the total cost to the requester in crowd-
sourcing platforms such as Amazon Mechanical Turk scales lin-
early with the number of workers reporting on each task). How
can we elicit the best possible evaluations from agents whose pro-
ﬁciency of evaluation depends on their strategically chosen effort,
when the ground truth as well as the effort levels of agents are un-
observable to the mechanism?

Our Contributions. We introduce a model for information elicita-
tion with endogenous proﬁciency, where an agent’s strategic choice
of whether or not to put in effort into a task endogenously deter-
mines her proﬁciency (the probability of correctly evaluating the
ground truth) for that task. We focus on the design of mecha-
nisms for binary information elicitation, i.e., when the underlying
ground truth is binary (corresponding to eliciting ‘good’ or ‘bad’
ratings). While generalizing to an arbitrary underlying type space
is an immediate direction for further work, we note that a number
of interesting judgement and evaluation tasks, for example iden-
tifying adult content or correctness evaluation, are indeed binary;
also, even very recent literature providing improved mechanisms
for information elicitation (e.g. [17, 19]), as well as experimental

2We thank David Evans (VP Education, Udacity) for pointing out
this issue in the context of peer-grading applications— while stu-
dents might put in their best efforts on grading screening assign-
ments to ensure they demonstrate the minimum proﬁciency re-
quired to be allowed to grade, how can we be sure that they will
continue to work with the same proﬁciency when grading home-
works outside of this screening set?

work on the performance of elicitation mechanisms [16, 5], focuses
on models with binary ground truth.

Our main contribution is a simple, new, mechanism for binary
information elicitation for multiple tasks when agents have endoge-
nous proﬁciencies. Our mechanism has the following incentive
properties.

(i) Exerting maximum effort followed by truthful reporting of

observations is a Nash equilibrium.

(ii) This is the equilibrium with maximum payoff to all agents,
even when agents have different maximum proﬁciencies, can
use mixed strategies, and can choose a different strategy for
each of their tasks.
Showing that full-effort truthtelling leads to the maximum re-
ward amongst all equilibria (including those involving mixed
strategies) requires arguing about the rewards to agents in all
possible equilibria that may arise. To do this, we use a ma-
trix representation of strategies where every strategy can be
written as a convex combination of ‘basis’ strategies, so that
maximizing a function over the set of all possible strategies
is equivalent to a maximization over the space of coefﬁcients
in this convex combination. This representation lets us show
that the reward to an agent over all possible strategy choices
(by herself and other agents), and therefore over all equilib-
ria, is maximized when all agents use the strategy of full-
effort truthful reporting.

(iii) Suppose there is some positive probability, however small,
that there is some ‘trusted’ agent for each task who will re-
port on that task truthfully with proﬁciency greater than half.
Then the equilibrium where all agents put in full effort and
report truthfully on all their tasks is essentially the only equi-
librium of our mechanism, even if the mechanism does not
know the identity of the trusted agents.

We note that our mechanism requires only minimal bounds on
the priors and imposes no conditions on a diverging number of
agent reports per task to achieve its incentive properties— to the
best of our knowledge, previous mechanisms for information elic-
itation do not provide all these guarantees simultaneously, even
when proﬁciency is not an endogenously determined choice (see
§1.1 for a discussion).
The main idea behind our mechanism M is the following. With
just one task, it is difﬁcult to distinguish between agreement rising
from high-effort observations of the same ground truth, and ‘blind’
agreement achieved by the low-effort strategy of always making
the same report. We use the presence of multiple tasks and rat-
ings to distinguish between these two scenarios and appropriately
reward or penalize agents to incentivize high effort— M rewards
an agent i for her report on task j for agreeing with another ‘refer-
ence’ agent rj(i)’s report on the same task, but also penalizes for
blind agreement by subtracting out a statistic term corresponding to
the part of i and rj(i)’s agreement on task j that is to be ‘expected
anyway’ given their reporting statistics estimated from other tasks.
This statistic term is chosen so that there is no beneﬁt to making
reports that are independent of the ground truth; the incentive prop-
erties of the mechanism follow from this property that agents obtain
positive rewards only when they put effort into their evaluations.
1.1 Related Work

The problem of designing incentives for crowdsourced judge-
ment elicitation is closely related to the growing literature on in-
formation elicitation mechanisms. The key difference between this

320literature (discussed in greater detail below) and our work is that
agents in the settings motivating past work have opinions that are
experientially formed anyway— independent, and outside of, any
mechanisms to elicit opinions— so that agents only need be in-
centivized to participate and truthfully report these opinions.
In
contrast, agents in the crowdsourcing settings we study do not have
such experientially formed opinions to report— an agent makes a
judgement only because it is part of her task, expending effort to
form her judgement, and therefore must be incentivized to both ex-
pend this effort and then to truthfully report her evaluation. There
are also other differences in terms of the models and guarantees in
previous mechanisms for information elicitation; we discuss this
literature below.

The peer-prediction method, introduced by Miller, Resnick and
Zeckhauser [12], is a mechanism for the information elicitation
problem for general outcome spaces where truthful reporting is a
Nash equilibrium, uses proper scoring rules to reward agents for
reports that are predictive of other agents’ reports. The main dif-
ference between our mechanism and [12], as well as other mech-
anisms based on the peer prediction method [8, 9, 18, 10, 19],
is in the model of agent proﬁciency.
In peer-prediction models,
while agents can decide whether to incur the cost to participate
(i.e., submit their opinion), an agent’s proﬁciency— the distribu-
tion of opinions or evaluations conditional on ground truth— is
exogenously determined (and common to all agents, and in most
models, known to the center). That is, an agent might not partici-
pate at all, but if she does participate she is assumed to have some
known proﬁciency. In contrast, in our setting, agents can choose not
just whether or not to participate, but also endogenously determine
their proﬁciency conditional on participating through their effort
choice. Thus, while peer prediction mechanisms do need to incen-
tivize agents to participate (by submitting a report), they then know
the proﬁciency of agents who do submit a report, and therefore can,
and do, dispense rewards that crucially use knowledge of this proﬁ-
ciency. In contrast, even agents who do submit reports in our setting
cannot be assumed to be using their maximum proﬁciency to make
their evaluations, and therefore cannot be rewarded based on any
assumed level of proﬁciency. Additionally, truthtelling, while an
equilibrium, is not necessarily the maximum-reward equilibrium in
these existing peer-prediction mechanisms— [6] shows that for the
mechanisms in [12, 8], the strategies of always reporting ‘good’
or always reporting ‘bad’ both constitute Nash equilibria, at least
one of which generates higher payoff than truthtelling. Such blind
strategy equilibria can be eliminated and honest reporting made the
unique Nash equilibrium by designing the payments as in [10], but
this needs agents to be restricted to pure reporting strategies, and
requires full knowledge of the prior and conditional probability dis-
tributions to compute the rewards.

The Bayesian Truth Serum (BTS) [15] is another mechanism for
information elicitation with unobservable ground truth. BTS does
not use the knowledge of a common prior to compute rewards, but
rather collects two reports from each agent— an ‘information’ re-
port which is the agent’s own observation, as well as a ‘prediction’
report which is the agent’s prediction about the distribution of infor-
mation reports from the population— and uses these to compute re-
wards such that truthful reporting is the highest-reward Nash equi-
librium of the BTS mechanism. In addition to the key difference
of exogenous versus endogenous proﬁciencies discussed above, an
important limitation of BTS in the crowdsourcing setting is that it
requires the number of agents n reporting on a task to diverge to
ensure its incentive properties. This n → ∞ requirement is infea-
sible in our setting due to the scaling of cost with number of re-
ports as discussed in the introduction. [17] provides a robust BTS

mechanism (RBTS) that works even for small populations (again
in the same non-endogenous proﬁciency model as BTS and peer
prediction mechanisms), and also ensures payments are positive,
making the mechanism ex-post individually rational in contrast to
BTS. However, the RBTS mechanism does not retain the property
of truthtelling being the highest reward Nash equilibrium— indeed,
the ‘blind agreement’ equilibrium via constant reports achieves the
maximum possible reward in RBTS, whereas truthtelling might in
fact lead to lower rewards.

There is also work on information elicitation in conducting sur-
veys and online polling [11, 7], both of which are not quite ap-
propriate for our crowdsourcing setting. The mechanism in [11] is
weakly incentive compatible (agents are indifferent between lying
and truthtelling), while [7] presents a online mechanism that is not
incentive compatible in the sense that we use and potentially re-
quires a large (constant) number of agents to converge to the true
result. For other work on information elicitation, albeit in settings
very different from ours, see [3, 2, 14].

We note also that we model settings where there is indeed a
notion of a ground truth, albeit unobservable, so that proﬁcient
agents who put in effort are more likely than not to correctly ob-
serve this ground truth. Peer-prediction methods, as well as the
Bayesian truth serum, are designed for settings where there may be
no underlying ground truth at all, and the mechanism only seeks
to elicit agents’ true observations (whatever they are) which means
that some agents might well be in the minority even when they
truthfully report their observation— this makes the peer prediction
setting ‘harder’ along the dimension of inducing truthful reports,
but easier along the dimension of not needing to incentivize agents
to choose to exert effort to make high-proﬁciency observations.

We note that our problem can also be cast as a version of a
principal-agent problem with a very large number of agents, al-
though the principal cannot directly observe an agent’s ‘output’ as
in standard models. While there is a vast literature in economics
on the principal-agent problem too large to describe here (see, eg,
[1] and references therein), none of this literature, to the best of our
knowledge, addresses our problem. Finally, there is also a large
orthogonal body of work on the problem of learning unknown (but
exogenous) agent proﬁciencies, as well as on the problem of opti-
mally combining reports from agents with differing proﬁciencies to
come up with the best aggregate evaluation in various models and
settings. These problems of learning exogenous agent proﬁcien-
cies and optimally aggregating agent reports are orthogonal to our
problem of providing incentives to agents with endogenous, effort-
dependent proﬁciencies to elicit the best possible evaluations from
them.

2. MODEL

We now present a simple abstraction of the problem of designing
mechanisms for crowdsourced judgement elicitation settings where
agents’ proﬁciencies are determined by strategic effort choice.

Tasks. There are m tasks, or objects, j = 1, . . . , m, where each
task has some underlying ‘true quality’, or type, ¯Xj. This true type
¯Xj is unknown to the system. We assume that the types are binary-
valued: ¯Xj is either H (or 1, corresponding to high-quality) or L
(or 0, for low quality) for all j; we use 1 and H (resp. 0 and L)
interchangeably throughout for convenience. The prior probabili-
ties of H and L for all tasks are denoted by P[H] and P[L]. We
assume throughout that max(P[H],P[L]) < 1, i.e., that there is at
least some uncertainty in the underlying qualities of the objects.

Agents. There are n workers or agents i = 1, . . . , n who noisily
evaluate, or form judgements on, the qualities of objects. We say
agent i performs task j if i evaluates object j. Agent i’s judgement

321on task j is denoted by ˆXij ∈ {0, 1}, where ˆXij is 0 if i evaluates
j to be of type L and ˆXij is 1 if i evaluates it to be H. Having made
an evaluation ˆXij, an agent can choose to report any value Xij ∈
{0, 1} either based on, or independent of, her actual evaluation ˆXij.
We denote the set of tasks performed by an agent i by J(i), and
let I(j) denote the set of agents who perform task j. We will as-
sume for notational simplicity that |J(i)| = D and |I(j)| = T for
all agents i and tasks j.

Proﬁciency. An agent’s proﬁciency at a task is the probability
with which she correctly evaluates its true type or quality. We as-
sume that an agent’s proﬁciency is an increasing function of the
effort she puts into making her evaluation. Let eij denote agent
i’s effort level for task j: we assume for simplicity that effort is
binary-valued, eij ∈ {0, 1}. Putting in 0 effort has cost cij(0) = 0,
whereas putting in full effort has cost cij(1) ≥ 0 (we note that our
results also extend to a linear model with continuous effort where
eij ∈ [0, 1] and the probability pi(eij) of correctly observing ¯Xj
as well as the cost ci(eij) increase linearly with eij).

An agent who puts in zero effort makes evaluations with proﬁ-
ciency pij(0) = 1/2 and does no better than random guessing, i.e.,
Pr( ˆXij = ¯Xj|eij = 0) = 1/2. An agent who puts in full effort
eij = 1 attains her maximum proﬁciency, Pr( ˆXij = ¯Xj|eij =
1) = pij(1) = pi. Note that this maximum proﬁciency pi can
be different for individual agents modeling their different abilities,
and need not be known to the center. We assume that the max-
imum proﬁciency pi ≥ 1
2 for all i— this minimum requirement
on agent ability can be ensured in online crowdsourcing settings
by prescreening workers on a representative set of tasks (Amazon
Mechanical Turk, for instance, offers the ability to prescreen work-
ers [13, 4], whereas in peer-grading applications such as on Cours-
era, students are given a set of pre-graded assignments to measure
their grading abilities prior to grading their peers, the results of
which can be used as a prescreen.)

We note that our results also extend easily to the case where the
maximum proﬁciency of an agent depends on whether the object
is of type H or L, i.e., the probabilities of correctly observing the
ground truth when putting in full effort are different for different
ground truths, Pr( ˆXij = ¯Xj| ¯Xj = H) (cid:54)= Pr( ˆXij = ¯Xj| ¯Xj =
L) (of course, different agents can continue to have different max-
imum proﬁciences).

Strategies. Agents strategically choose both their effort levels
and reports on each task to maximize their total utility, which is
the difference between the reward received for their reports and the
cost incurred in making evaluations. Formally, an agent i’s strategy
is a vector of D tuples [(eij, fij)], specifying her effort level eij as
well as the function fij she uses to map her actual evaluation ˆXij
into her report Xij for each of her tasks. Note that since an agent’s
proﬁciency on a task pij is a function of her strategically chosen
effort eij, the proﬁciency of agent i for task j is endogenous in our
model.

For a single task, we use the notation (1, X) to denote the choice
of full effort eij = 1 and truthfully reporting one’s evaluation (i.e.,
fij is the identity function Xij = ˆXij), (1, X c) to denote full
effort followed by inverting one’s evaluation, and (0, r) to denote
the choice of exerting no effort (eij = 0) and simply reporting
the outcome of a random coin toss with probability r of returning
H. We use [(1, X)] to denote the strategy of using full effort and
truthtelling on all of an agent’s tasks, and similarly [(1, X c)] and
[(0, r)] for the other strategies.

Mechanisms. A mechanism in this setting takes as input the set
of all received reports Xij and computes a reward for each agent
based on her reports, as well as possibly the reports of other agents.

Note that the mechanism has no access3 to the underlying true qual-
ities ¯Xj for any task, and so cannot use the ¯Xj to determine agents’
rewards. A set of effort levels and reporting functions [(eij, fij)] is
a full-information Nash equilibrium of a mechanism if no agent i
can strictly improve her expected utility by choosing either a differ-
ent effort level ˆeij, or a different function ˆfij to map her evaluation
ˆXij into her report Xij. Here, the expectation is over the random-
ness in agents’ noisy evaluation of the underlying ground truth, as
well as any randomness in the mechanism.

We will be interested in designing mechanisms for which it is
(i) an equilibrium for all agents to put in full effort and report their
evaluations truthfully on all tasks, i.e., use strategies [(1, X)], and
(ii) for which [(1, X)] is the maximum utility (if not unique) equi-
librium. We emphasize here that we do not address the problem
of how to optimally aggregate the T reports Xij for task j into
a ﬁnal estimate of ¯Xj:
this is an orthogonal problem requiring
application-speciﬁc modeling; our only goal is to elicit the best
possible judgements to aggregate, by ensuring that agents ﬁnd it
most proﬁtable to put in maximum effort into their evaluations and
then report these evaluations truthfully.

3. MECHANISM

The main idea behind our mechanism M is following. Recall
that a mechanism does not have access to the true qualities ¯Xj, and
therefore must compute rewards for agents that do not rely on di-
rectly observing ¯Xj. Since the only source of information about ¯Xj
comes from the reports Xij, a natural solution is to reward based
on some form of agreement between different agents reporting on
j, similar to the peer-prediction setting [12]. However, an easy way
for agents to achieve perfect agreement with no effort is to always
report H (or L). With just one task, it is difﬁcult for a mechanism to
distinguish between the scenario where agents achieve agreement
by making accurate, high-effort, evaluations of the same ground
truth, and the low-effort scenario where agents achieve agreement
by always reporting H, especially if P[H] is high. However, in
our setting, we have the beneﬁt of multiple tasks and ratings, which
could potentially be used to distinguish between these two strate-
gies and appropriately reward agents to incentivize high effort.
M uses the presence of multiple ratings to subtract out a statis-
tic term Bij from the agreement score, chosen so that there is no
beneﬁt to making reports that are independent of ¯Xj— roughly
speaking, M rewards an agent i for her report on task j for agree-
ing with another ‘reference’ agent rj(i)’s report on the same task,
but only beyond what would be expected if i and rj(i) were ran-
domly tossing coins with their respective empirical frequencies of
heads.

Let d denote the number of other reports made by i and rj(i)
that are used in the computation of this statistic term Bij based on
the observed frequency of heads for each pair (i, j). We use Md
to denote the version of M which uses d other reports from each
of i and rj(i) to compute Bij. To completely specify Md, we
also need to specify a reference rater rj(i) as well as this set of
d (non-overlapping) tasks performed by i and rj(i), for which we
use the following notation. (We require these d other tasks to be
non-overlapping so that the reports for these tasks Xik and Xrj (i)l

3Crowdsourcing is used typically precisely in scenarios where the
number of tasks is too large for the principal (or a set of trusted
agents chosen by the principal) to carry out herself, so it is at best
feasible to verify the ground truth for a tiny fraction of all tasks,
which fraction turns out to be inadequate (a formal statement is
omitted here) to incentivize effort using knowledge of the ¯Xj.

322are independent4, which is necessary to achieve the incentive prop-
erties of Md.)

DEFINITION 1

(Sij, Srj (i)j). Consider agent i and task j ∈
J(i), and a reference rater rj(i). Given a value of d (1 ≤ d ≤
D− 1), let Sij and Srj (i)j be sets of d non-overlapping tasks other
than task j performed by i and rj(i) respectively, i.e.,
Sij ⊆ J(i) \ j, Srj (i)j ⊆ J(rj(i)) \ j, Sij ∩ Srj (i)j = ∅,
|Sij| = |Srj (i)j| = d.
A mechanism Md is completely speciﬁed by reference raters
rj(i) and the sets Sij and Srj (i)j, and rewards agents as deﬁned
below. Note that Md only uses agents’ reports Xij to compute
rewards and not their maximum proﬁciencies pi, which therefore
need not be known to the system.

DEFINITION 2

(MECHANISM Md). Md computes an agent
i’s reward for her report Xij ∈ {0, 1} on task j, Rij, by comparing
against a ‘reference rater’ rj(i)’s report Xrj (i)j for j, as follows:
(1)

Rij = Aij − Bij, where
Aij = XijXrj (i)j + (1 − Xij)(1 − Xrj (i)j),

and

(cid:80)

(cid:80)

Bij = (

+ (1 −

Xik

(cid:80)

k∈Sij
d
k∈Sij
d

)(

Xik

l∈Srj (i)j
d

(cid:80)

)(1 −

Xrj (i)l

)

(2)

l∈Srj (i)j
d

Xrj (i)l

),

ﬁnal reward to an agent i is βRi, where Ri =(cid:80)

where the sets Sij and Srj (i)j in Bij are as in Deﬁnition 1. The
j∈J(i) Rij and β
is simply a non-negative scaling parameter that is chosen based on
agents’ costs of effort.

The ﬁrst term, Aij, in Rij is an ‘agreement’ reward, and is 1
when i and rj(i) both agree on their report, i.e., when Xij =
Xrj (i)j = 1 or when Xij = Xrj (i)j = 0. The second term
Bij is the ‘statistic’ term which, roughly speaking, deducts from
the agreement reward whatever part of i and rj(i)’s agreement on
task j is to be ‘expected anyway’ given their reporting statistics,
i.e., the relative frequencies with which they report H and L. This
deduction is what gives M its nice incentive properties— while M
rewards agents for agreement via Aij, M also penalizes for blind
agreement that agents achieve without effort, by subtracting out the
Bij term corresponding to the expected frequency of agreement if
i and rj(i) were randomly choosing reports corresponding to their
estimated means.

For example, suppose all agents were to always report H. Then
Aij is always 1, but Bij = 1 as well so that the net reward is
0; similarly if agents chose their reports according to a random
cointoss, even one with the ‘correct’ bias P[H], the value of Aij
is exactly equal to Bij since there is no correlation between the
reports for a particular task, again leading to a reward of 0. The re-
ward function Rij is designed so that it only rewards agents when
they put in effort into their evaluations, which leads to the desirable
incentive properties of Md.
(We note that there are other natu-
ral statistics which might incentivize agents away from low-effort
reports— e.g., rewarding reports which collectively have an empir-
ical mean close to P[H], or for variance. However, it turns out that
appropriately balancing the agreement term (which is necessary to
ensure agents cannot simply report according to a cointoss with

4We assume that co-raters’ identities are kept unknown to agents,
so there is no collusion between i and rj(i).

bias P[H]) with a term penalizing blind agreement to simultane-
ously ensure that [(1, X)] is an equilibrium and the most desirable
equilibrium is hard to accomplish.)

There are two natural choices for the parameter d, i.e., how many
reports of i and rj(i) to include for estimating the statistic term that
5. (i) In MD−1, we
we subtract from the agreement score in Rij
set d = D − 1 and include all reports of agents i and rj(i), except
those on their common task j. Here, the non-overlap requirement
for sets Sij and Srj (i)j says that an agent i and her reference rater
rj(i) for task j have only that task j in common. (ii) In M1, we
set d = 1, i.e., subtract away the correlation between the report of
i and rj(i) on exactly one other non-overlapping task. In M1, the
non-overlap condition only requires that for each agent-task pair,
there is a reference agent rj(i) available who has rated one other
task that is different from the remaining tasks rated by i, a condition
that is much easier to satisfy than that in MD−1. In § 4, we will see
that M1 will require that the choices of (j, j(cid:48)), where {j(cid:48)} = Sij
is the task used in the statistic term of i’s reward for task j, are such
that each task j(cid:48) performed by i is used exactly once to determine
Rij for j (cid:54)= j(cid:48). Note that this is always feasible, for instance by
using task j + 1 in the statistic term for task j for j = 1, . . . , D− 1
and task 1 for task D.

4. ANALYZING M

In this section, we analyze equilibrium behavior in Md.

4.1 Preliminaries

Recall that proﬁciency is the probability of correctly evaluat-
ing the true quality. We use p[H] (respectively p[L]) to denote
the probability that an agent observes H (respectively L) when
making evaluations with proﬁciency p, i.e., the probability that
ˆXij = H is p[H] = pP[H] + (1 − p)P[L]. Similarly, q[H], q[L]
and pi[H], pi[L] correspond to the probabilities of seeing H and L
when making evaluations with proﬁciencies q and pi respectively.

Matrix representation of strategies. We will frequently need to
consider the space of all possible strategies an agent may use in
the equilibrium analysis of Md. While the choice of effort level
eij in an agent’s strategy [(eij, fij)] is easily described— there are
only two possible effort levels 1 and 0— the space of functions fij
through which an agent can map her evaluation ˆXij into her report
Xij is much larger. For instance, an agent could choose fij corre-
sponding to making an evaluation, performing a Bayesian update
of her prior on ¯Xj, and choosing the report with the higher poste-
rior probability. We now discuss a way to represent strategies that
will allow us to easily describe the set of all reporting functions fij.

vector oij ∈ R2, where oij = (cid:2)1 0(cid:3)T if i observes a H, and
oij =(cid:2)0 1(cid:3)T if i observes a L, where aT denotes the transpose

of a. For the purpose of analyzing Md, any choice of reporting
function fij can then be described via a 2 × 2 matrix

An agent i’s evaluation ˆXij can also be written as a two-dimensional

(cid:20) x

1 − x

(cid:21)

,

1 − y

y

M ij =

where x is the probability with which i chooses to report H after
observing H, i.e., x = Pr(Xij = H| ˆXij = H), and similarly
y = Pr(Xij = L| ˆXij = L). Observe that the choice of effort eij
affects only oij and its ‘correctness’, or correlation with the (vec-

5Understanding the effect of the parameter d in our mechanisms,
which appears irrelevant to the mechanism’s behavior when agents
are risk-neutral, is an interesting open question.

323tor representing the) actual quality ¯Xj, and the choice of reporting
function fij only affects M ij.

Any reporting matrix M ij of the form above can be written as
a convex combination of four matrices— one for each of the fij
corresponding to (i) truthful reporting (Xij = ˆXij) (ii) inverting
(Xij = ˆX c
ij), and (iii, iv) always reporting H or L independent of
one’s evaluation (Xij = H and Xij = L respectively):

MX =

, MXc =

, MH =

, ML =

(cid:20)1 0

(cid:21)

0 1

(cid:20)0 1
(cid:21)

1 0

(cid:20)1 1

(cid:21)

0 0

(cid:20)0 0
(cid:21)

1 1

.

That is, M ij = α1MX + α2MXc + α3MH + α4ML, where α1 =
x−α3, α2 = 1−y−α3, and α3 = x−y and α4 = 0 if x ≥ y, and
α3 = 0 and α4 = y − x if y > x. It is easily veriﬁed that αi ≥ 0,

and (cid:80) αi = 1, so that this is a convex combination. Since all

The agent’s ﬁnal report Xij is then described by the vector M ijoij ∈

possible reporting strategies fij can be described by appropriately
choosing the values of x ∈ [0, 1] and y ∈ [0, 1] in M ij, every
reporting function fij can be written as a convex combination of
these four matrices.
R2, where the ﬁrst entry is the probability that i reports H, i.e.,
Xij = H, and the second entry is the probability that she reports
Xij = L. The expected reward of agent i for task j can therefore
be written using the matrix-vector representation (where T denotes
transpose and 1 is the vector of all ones) as

E[Rij] = E[(M rj (i)jorj (i)j)T M ijoij

+ (1 − M rj (i)jorj (i)j)T (1 − M ijoij)]
− [(M rj (i)jE[orj (i)j])T M ijE[oij]
+ (1 − M rj (i)jE[orj (i)j])T (1 − M ijE[oij])],

which is linear in M ij. So the payoff from an arbitrary reporting
function fij can be written as the corresponding linear combination
of the payoffs from each of the ‘basis’ functions (corresponding to
MX , MXc , MH and ML) constituting fij = M ij. We will use
this to argue that it is adequate to consider deviations to each of
the remaining basis reporting functions and show that they yield
strictly lower reward to establish that [(1, X)] is an equilibrium of
Md.

Equivalent strategies. For the equilibrium analysis, we will use the
following simple facts. (i) The strategy (0, X) (i.e., using zero ef-
fort but truthfully reporting one’s evaluation) is equivalent to the
strategy (0, r) with r = 1/2, i.e., to the strategy of putting in no ef-
fort, and randomly reporting H or L independent of the evaluation
ˆXij with probability 1/2 each. (ii) The strategy (1, r) is equivalent
to the strategy (0, r), since the report Xij in both cases is com-
pletely independent of the evaluation ˆXij and therefore of eij.

Cost of effort. While agents do incur a higher cost when using
eij = 1 as compared to eij = 0, we will not need to explicitly deal
with the cost in the equilibrium analysis— if the expected reward
from using a strategy where eij = 1 is strictly greater than the
reward from any strategy with eij = 0, the rewards Rij can always
be scaled appropriately using the factor β (in Deﬁnition 2) to ensure
that the net utility (reward minus cost) is strictly greater as well.

We note that bounds on the factor β could be estimated empiri-
cally without knowledge of the priors by estimating the cost of ef-
fort cij from maximum proﬁciencies obtained from a pre-screening
(§2), by conducting a series of trials with increasing rewards and
then using individual rationality to estimate the cost of effort from
observed proﬁciencies in these trials.

Individual rationality and non-negativity of payments. The ex-
pected payments made by our mechanism to each agent are always
nonnegative in the full-effort truthful reporting equilibrium, i.e.,
when all agents use strategies [(1, X)]. To ensure that the pay-
ments are also non-negative for every instance (of the tasks and
reports) and not only in expectation, note that it sufﬁces to add 1
to the payments currently speciﬁed, since the penalty term Bij in
the reward Rij is bounded above by 1. We also note that individ-
ual rationality can be achieved by using a value of β large enough
to ensure that the net utility βRij − c(1) remains non-negative for
all values of P[H]— while the expected payment Rij does go to
zero as P[H] tends to 1 (i.e., in the limit of vanishing uncertainty
as the underlying ground truth is more and more likely to always
be H (or always be L)), as long as there is some bound  > 0 such
that max{P[H],P[L]} ≤ 1 − , a simple calculation can be used
to determine a value β∗() such that the resulting mechanism with
β = β∗ leads to nonnegative utilities for all agents in the full-effort
truth-telling Nash equilibrium of M.
4.2 Equilibrium analysis

We now analyze the equilibria of Md. We begin with a technical

lemma and a deﬁnition.

LEMMA 3. Let fα(p, q) = pq+(1−p)(1−q)−α(p[H]q[H]+
(1−p[H])(1−q[H])). If α ≤ 1, (i) fα(p, q) is strictly increasing in
p if q > 1/2, and strictly increasing in q if p > 1/2. (ii) fα(p, q) is
nonnegative if p, q ≥ 1/2, and positive if p, q > 1/2. (iii) Denote
f (p, q) (cid:44) f1(p, q). Then, f (p, q) = f (q, p) = f (1 − p, 1 − q).
Also f (p, 1 − q) = f (1 − p, q) = −f (p, q).

PROOF. Recall that p[H] = pP[H]+(1−p)P[L], and similarly

for q[H].

fα(p, q) = p(2q − 1) + (1 − q)

− α(pP[H] + (1 − p)P[L])(2q[H] − 1) − (1 − q[H])
= p [(2q − 1) − α(P[H] − P[L])(2q[H] − 1)] + K−p
= p(2q − 1)(1 − α(P[H] − P[L])2) + K−p,

where K−p is a term that does not depend on p, and we use
2q[H] − 1 = (2q − 1)(P[H] − P[L]) in the last step.
Note that P[H]−P[L] < P[H] < 1 if max(P[H],P[L]) < 1,
so that 1 − α(P[H] − P[L])2 > 0 if α ≤ 1. Therefore, fα(p, q)
is linear in p with strictly positive coefﬁcient when q > 1/2 and
α ≤ 1. An identical argument can be used for q since fα(p, q) can
be written as a linear function of q exactly as for p:

fα(p, q) = q(2p − 1)(1 − α(P[H] − P[L])2) + K−q.

This proves the ﬁrst claim.
For nonnegativity of fα(p, q) on p ∈ [1/2, 1], we simply argue
that f1(p, q) is increasing in q when p ∈ [1/2, 1], and 0 at q =
1/2. So for any q > 1/2, f1(p, q) ≥ 0 for any p ∈ [1/2, 1].
But fα(p, q) is decreasing in α, so fα(p, q) is nonnegative for any
α ≤ 1 as well.
The ﬁnal claims about f (p, q) and f (1 − p, q) can be veriﬁed
just by substituting the deﬁnitions of p[H] and q[H] and from sym-
metry in p and q.

DEFINITION 4

(Tij, dij). Let Tij be the set of all tasks j(cid:48) (cid:54)= j
such that j ∈ Sij(cid:48), i.e., Tij is the set of tasks j(cid:48) for which i’s report
on task j is used to compute the statistic term of i’s reward Rij(cid:48) for
task j(cid:48). We use dij = |Tij| to denote the number of such tasks j(cid:48).
Our main equilibrium result states that under a mild set of con-
ditions on the choice of reference raters rj(i) and sets Tij, exerting
full effort and reporting truthfully on all tasks is an equilibrium of

324Md— even when agents have different maximum proﬁciencies and
can choose a different strategy for each task (for instance, an agent
could choose to shirk effort on some tasks and put in effort on the
others).

The main idea behind this result is best explained for the special
case where all agents have the same maximum proﬁciency pi = p
and are restricted to using the same strategy for each task. Here, the
expected payoff from using [(1, X)] turns out to be f (p, p) where f
is as deﬁned in Lemma 3, while the payoff from playing [(0, r)] is
exactly 0 (independent of other agents’ strategies); the payoff from
deviating to [(1, X c)] when other agents play [(1, X)] is −f (p, p).
Since f (p, p) > 0 for p > 1/2 and increases with p, it is a best
response for every agent to attain maximum proﬁciency and truth-
fully report her evaluation. Extending the argument when agents
can have both different maximum proﬁciencies pi and use differ-
ent strategy choices for each task requires more care, and are what
necessitate the conditions on the task assignment in Theorem 5.

THEOREM 5. Suppose pi > 1/2 for all i, and for each agent
i, for each task j ∈ J(i), (i) dij = d, and (ii) E[prj (i)] =
(i)] (cid:44) ¯pi, where the expectation is over the random-
Ejl∈Tij [prjl
ness in the assignment of reference raters to tasks and the sets Tij.
Then, [(1, X)] is an equilibrium of Md.

The ﬁrst condition in Theorem 5, dij = d, says that each task
j performed by an agent i must contribute to computing the re-
ward via the statistic term for exactly d other tasks in J(i), where
d is the number of reports used to compute the ‘empirical fre-
quency’ of H reports by i in the statistic term. The second condi-
tion E[prj (i)] = Ejl∈Tij [prjl
(i)] says that an agent i should expect
the average proﬁciency of her reference rater rj(i) to be equal for
all the tasks that she performs, i.e., agent i should not be able to
identify any particular task where her reference raters are, on aver-
age, worse than the reference raters for her other tasks (intuitively,
this can lead to agent i shirking effort on this task being a proﬁtable
deviation)6. The ﬁrst condition holds for each of the two speciﬁc
mechanisms M1 and MD−1, and the second condition can be sat-
isﬁed by a randomization of the agents before assignment7. We
now prove the result.

PROOF. Consider agent i, and suppose all other agents use strat-
egy [(1, X)], i.e., put in full effort with truthtelling on all their
tasks. It will be enough to consider pure strategy deviations, and
show that there is no beneﬁcial deviation to (1, X c), or (0, r) for
any r ∈ [0, 1] on any single task or subset of tasks.
Consider a particular assignment of reference raters rj(i) and
the sets Sij (and Tij). If the other agents all report according to
[(1, X)], then the total expected reward to agent i from all her D
tasks in this assignment is

6We note that the conditions on Md (which can be easily satisﬁed
for d = 1 and D−1) arise because we allow both differing abilities
pi and choosing a different strategy for each task— if either gener-
alization is waived, [(1, X)] can be shown to be an equilibrium of
Md even without imposing these conditions.
7The details for this randomized assignment of tasks to agents so
that Sij and Srj (i)j are non-overlapping for all i, j and satisfy the
conditions in Theorem 5 is included in the full version of the paper.

E[Ri] =

E[Xij Xrj (i)j + (1 − Xij )(1 − Xrj (i)j )]−

(cid:80)

k∈Sij
d

E[Xik]

prj (i)[H] + (1 −

E[Xik]

k∈Sij
d

)(1 − prj (i)[H])

(cid:35)

D(cid:88)

j=1

=

(cid:34)(cid:80)
D(cid:88)
(cid:34)(cid:80)
D(cid:88)
(cid:88)

j=1

j=1

=

E[Xik]

k∈Sij
d

(cid:104)

(cid:104)
(cid:18) E[Xij ]

E

jl∈Tij

d

E[Xij Xrj (i)j + (1 − Xij )(1 − Xrj (i)j )]−

(cid:35)

(cid:105) −

(2prj (i)[H] − 1) + (1 − prj (i)[H])

Xij Xrj (i)j + (1 − Xij )(1 − Xrj (i)j )

(cid:19)
(i)[H] − 1)

(cid:105)
− (1 − prj (i)[H])

,

(2prjl

where the expectation is over any randomness in the strategy of i
as well as randomness in i and rj(i)’s evaluations for each task j,
and we rearrange to collect Xij terms in the last step.

Now, agent i can receive different reference raters and task sets
Sij in different assignments. So to compute her expected reward,
agent i will also take an expectation over the randomness in the as-
signment of reference raters to tasks and the sets Sij, which appear
in the summation above via Tij.
(i)] (cid:44) ¯pi. Us-
ing this condition and taking the expectation over the randomness
in the assignments of rj(i) and Sij, the expected reward of i is

Recall the condition that E[prj (i)] = Ejl∈Tij [prjl

E

j=1

(cid:104)
(cid:104)
D(cid:88)
(cid:18) E[Xij ]
(cid:104)

d

E[Ri] =

(cid:88)
D(cid:88)

(cid:104)

jl∈Tij

=

Xij Xrj (i)j + (1 − Xij )(1 − Xrj (i)j )

(2¯pi[H] − 1)

− (1 − ¯pi[H])

(cid:19)

(cid:105) −

(cid:105)

(cid:105) −

E

Xij Xrj (i)j + (1 − Xij )(1 − Xrj (i)j )

j=1
dij
d

E[Xij ](2¯pi[H] − 1) − (1 − ¯pi[H])

where ¯pi[H] = E[prj (i)[H]] = Ejl∈Tij [prjl
The expected reward to agent i, when she makes evaluations with
proﬁciency qj for task j and truthfully reports these evaluations
(Xij = ˆXij), is then

(i)[H]].

qj ¯pi + (1 − qj )(1 − ¯pi) − dij
d

qj [H](2¯pi[H] − 1) − (1 − ¯pi[H])

(cid:105)

qj ¯pi + (1 − qj )(1 − ¯pi)−

(cid:105)

)(1 − ¯pi[H])

(qj [H]¯pi[H] + (1 − q[H])(1 − ¯pi[H])) − (1 − dij
d

(qj , ¯pi) + (

f dij
d

dij
d

− 1)(1 − ¯pi[H])

.

(3)

where the expectation is taken over randomness in all agents’ eval-
uations, as well as over randomness in the choices of rj(i) and Sij.
We can now show that choosing full effort and truthtelling on
all tasks is a best response when all other agents use [(1, X)] if
(qj, ¯pi) is increasing in qj pro-
dij = d. First, by Lemma 3, f dij
d ≤ 1, so agent i should choose full effort to achieve her
vided dij

d

(cid:105)

,

(cid:21)

E[Ri]

D(cid:88)
D(cid:88)

j=1

(cid:104)
(cid:104)

=

=

j=1
dij
d

D(cid:88)

(cid:20)

j=1

=

325maximum proﬁciency pi on all tasks. Next, note that in terms of the
expected reward, using proﬁciency qj and reporting X c is equiv-
alent to using proﬁciency 1 − qj and reporting X. So again by
Lemma 3, deviating to X c, i.e., (1 − qj), on any task is strictly
dominated by X for qj > 1/2 and ¯pi > 1/2.

Finally, if agent i chooses fij as the function which reports the
outcome of a random cointoss with probability r of H for any task
j, the component of E[Ri] contributed by the term corresponding
to Xij becomes

(cid:104)

(cid:105) −

E

(cid:17)
E[Xij ]prj (i)[H] + (1 − E[Xij ])(1 − prj (i)[H])

(cid:16)
Xij Xrj (i)j + (1 − Xij )(1 − Xrj (i)j )
dij
d
= rprj (i)[H] + (1 − r)(1 − prj (i)[H])−
(rprj (i)[H] + (1 − r)(1 − prj (i)[H])))

= 0,

Since we need dij

j∈J(i) Rij can always be scaled

Therefore, the rewards Ri =(cid:80)

which is strictly smaller than the reward from fij = X in (3) if
d ≥ 1, since f (qj, ¯pi) is strictly positive when
qj > 1/2 and dij
qj, ¯pi > 1/2 by Lemma 3.
d ≤ 1 to ensure that (1, X c) is not a proﬁtable
d ≥ 1 to ensure that (0, r) is not a proﬁtable de-
deviation, and dij
viation, requiring dij = d simultaneously satisﬁes both conditions.
Therefore, if dij = d, deviating from (1, X) on any task j leads to
a strict decrease in reward to agent i. Since the total reward to agent
i can be decomposed into the sum of D terms which each depend
only on the report Xij and therefore the strategy for the single task
j, any deviation from [(1, X)] for any single task or subset of tasks
strictly decreases i’s expected reward.
appropriately to ensure that [(1, X)] is an equilibrium of Md.
Other equilibria. While [(1, X)] is an equilibrium, Md can have
other equilibria as well— for instance, the strategy [(0, r)], where
all agents report the outcome of a random cointoss with bias r on
each task, is also an equilibrium of Md for all r ∈ [0, 1], albeit with
0 reward to each agent. In fact, as we show in the next theorem, no
equilibrium, symmetric or asymmetric, in pure or mixed strategies,
can yield higher reward8 to agents than [(1, X)], as long as agents
‘treat tasks equally’ (for example, while an agent may choose to
shirk effort on one task and work on all others, each of her tasks is
equally likely to be the one she shirks on). We will refer to this as
tasks being ‘apriori equivalent’, so that agents cannot distinguish
between tasks prior to putting in effort on them (or equivalently,
the assignment of reference raters is such that agents will not ﬁnd it
beneﬁcial (in terms of expected reward) to use a different strategy
for a speciﬁc task). Note that this assumption is particularly reason-
able in the context of applications where agents are recruited for a
collection of similar tasks as in crowdsourced abuse/adult content
identiﬁcation, or in peer grading where each task is an anonymous
student’s solution to the same problem.

THEOREM 6. Suppose pi > 1/2, and tasks are apriori equiva-
lent. Then, the equilibrium where all agents choose [(1, X)] yields
maximum reward to each agent.

8We note that another equilibrium which achieves the same max-
imum expected reward is [(1, X c)], where all agents put in full
effort to make their evaluations, but then all invert their evaluations
for their reports. However, [(1, X c)] is a rather unnatural, and risky,
strategy, and one that is unlikely to arise in practice. Also, as we
will see later, [(1, X c)] can also lead to lower rewards when there
are some agents who always report truthfully.

PROOF. Consider a particular agent i and task j, and a single
potential reference rater rj(i) for (i, j). Recall from the prelim-
inaries that agent i’s choice of fij can be described via a matrix
M = α1MX + α2MXc + α3MH + α4ML, and that we denote
i’s evaluation via a vector o, where o = [1 0]T if i observes H
and o = [0 1]T if i observes L. Similarly, let us describe rj(i)’s
choice of reporting function via the matrix M(cid:48) with corresponding
coefﬁcients α(cid:48)
Since tasks are apriori equivalent, each player i (hence rj(i) too)
uses strategies such that E[Xij] = E[Xik] for all j, k ∈ J(i).
Then, we can rewrite the expected reward for agent i on task j,
when paired with reference rater rj(i), as

i, and denote rj(i)’s evaluation by o(cid:48).

E[Rij ] = 2(E[Xij Xrj (i)j ] − E[Xij ]E[Xrj (i)j ]).

1 + α2α(cid:48)

2)I + α2α(cid:48)

Using the matrix-vector representation, substituting M, M(cid:48) with
their representations in terms of the basis matrices and expanding,
and evaluating the matrix-matrix products, we have
Xij Xrj (i)j = o(cid:48)T M(cid:48)T M o = o(cid:48)T RM o, where
RM = (α1α(cid:48)
1MXc + α1α(cid:48)
+ (α3α(cid:48)
+ (α4α(cid:48)
and I, 1 denote the identity matrix and the matrix of all ones
in R2×2 respectively, and we use M T
Xc MXc = I,
Xc ML = MH,
M T
and M T

X MX = M T
Xc MH = ML, M T

H ML = 0. Similarly,
E[Xij ]E[Xrj (i)j ] = E[o(cid:48)T M(cid:48)T ]E[M o] = E[o(cid:48)T ]RM E[o],

4)1 + (α3α(cid:48)
2)ML + (α1α(cid:48)

2M T
Xc
2)MH + (α1α(cid:48)

3 + α4α(cid:48)
1 + α3α(cid:48)

L ML = 1, M T

H MH = M T

1 + α4α(cid:48)

3 + α2α(cid:48)

4 + α2α(cid:48)

3)M T
L ,

4)M T
H

Now, note that MH o = [o1 + o2

where RM is as deﬁned above.
0]T = [1 0]T since o1 +
o2 = 1 for any evaluation vector o by deﬁnition, so that E[o(cid:48)T MH o] =
E[o(cid:48)T ]E[MH o], since MH o is a constant. The same is the case for
L o], E[o(cid:48)T MLo].
each of the terms E[o(cid:48)T 1o], E[o(cid:48)T M T
Therefore, these terms cancel out when taking the difference
E[XijXrj (i)j] − E[Xij]E[Xrj (i)j] (corresponding to the reward
from either agent choosing to report Xij independent of her evalu-

ation being 0). Also note that E[oij] =(cid:2)p[H] p[L](cid:3)T if agent i

H o], E[o(cid:48)T M T

+ (α2α(cid:48)

2] − E[o(cid:48)

1o2 + o1o(cid:48)

Xc o] − E[o(cid:48)T ]M T

2)(E[o(cid:48)T o] − E[o(cid:48)]T E[o])

1]E[o1] − E[o2]E[o(cid:48)
2])
1]E[o2] − E[o1]E[o(cid:48)
2] − E[o(cid:48)
2])
(cid:16)

makes evaluations with proﬁciency p. Suppose the agents use effort
leading to proﬁciencies p and p(cid:48) respectively. Then, we have
E[Xij Xrj (i)j ] − E[Xij ]E[Xrj (i)j ]
= (α1α(cid:48)
+ α2α(cid:48)
+ α1α(cid:48)
= (α1α(cid:48)

1 + α2α(cid:48)
1(E[o(cid:48)T MXc o] − E[o(cid:48)T ]MXc E[o])
2(E[o(cid:48)T M T
Xc E[o])
1o1 + o2o(cid:48)
2)(E[o(cid:48)
1 + α2α(cid:48)
(cid:16)
2)(E[o(cid:48)
1 + α1α(cid:48)
(cid:17)
1 + α2α(cid:48)
pp(cid:48) + (1 − p)(1 − p(cid:48)) − p[H]p(cid:48)[H]−
2)
p(1 − p(cid:48)) +

= (α1α(cid:48)
(1 − p[H])(1 − p(cid:48)[H])
(1 − p)p(cid:48) − p[H](1 − p(cid:48)[H]) − (1 − p[H])p(cid:48)[H]
Now, note that multiplier of (α1α(cid:48)
2) is precisely f (p, p(cid:48)),
which by Lemma 3 is nonnegative if p, p(cid:48) ≥ 1/2, and strictly pos-
itive if p, p(cid:48) > 1/2. Also, for p, p(cid:48) ≥ 1/2, note that p[H] ≤ p and
p(cid:48)[H] ≤ p(cid:48). Now, the function g(x, y) = x(1 − y) + y(1 − x)
is decreasing in both x and y for x, y ∈ [ 1
2 , 1] (taking derivatives),
so the multiplier of (α2α(cid:48)
2) is non-positive, and negative if
p, p(cid:48) > 1/2.
So the maximum value that E[XijXrj (i)j] − E[Xij]E[Xrj (i)j]
i = 1, is

can take for nonnegative coefﬁcients with(cid:80) αi = (cid:80) α(cid:48)

1 + α1α(cid:48)
2)

1 + α2α(cid:48)

1 + α1α(cid:48)

+ (α2α(cid:48)

(cid:17)

.

3263 = α(cid:48)

1 + α2α(cid:48)

2) when α2 = 1− α1 and α(cid:48)

f (p, p(cid:48)), which is obtained by setting α3 = α4 = 0, α(cid:48)
4 = 0
(i.e., with no weight on random independent reporting), and a1 =
α(cid:48)
1 = 1, α2 = α(cid:48)
2 = 0 (or viceversa): this is because the maximum
2 = 1− α(cid:48)
value of term (α1α(cid:48)
1
is 1 and is achieved with these values, which also minimize the
value of the term (α2α(cid:48)
2) with the non-positive multiplier,
2) ≥ 0 and is equal to 0 for these values of
since (α2α(cid:48)
αi, α(cid:48)
i. Also, since f (p, p(cid:48)) increases with increasing p and p(cid:48),
it is maximized when agents put in full effort and achieve their
maximum proﬁciencies pi, prj (i).

1 + α1α(cid:48)

1 + α1α(cid:48)

Therefore the expected reward for the single component of E[Rij]
coming from a speciﬁc reference rater achieves its upper bound
when both agents use [(1, X)]. The same argument applies for
each reference rater, and therefore to the expected reward E[Rij],
and establishes the claim.

We next investigate what kinds of Nash equilibria might exist where
agents use low effort with any positive probability. Apriori, it is
reasonable to expect that there would be mixed-strategy equilibria
where agents randomize between working and shirking, i.e., put
in effort (choose eij = 1) sometimes and not (choose eij = 0)
some other times. However, we next show that as long as tasks
are apriori equivalent and agents only randomize between report-
ing truthfully and reporting the outcome of an independent random
cointoss (i.e., they do not invert evaluations), the only equilibrium
in which any agent uses any support on (0, r) is the one in which
all agents always use (0, r) on all their tasks. To show this, we
start with the following useful lemma saying that an agent who
uses a low-effort strategy any fraction of the time will always have
a beneﬁcial deviation as long as some reference agent plays (1, X)
with some positive probability. Roughly speaking, this is because
as long as there is some probability that an agent’s reference rater
plays (1, X) rather than (0, r), the agent strictly beneﬁts by always
playing (1, X) to maximize the probability of both agents playing
(1, X), which is the only time the agent obtains a positive reward.
LEMMA 7. Suppose the probability of agent i using strategy
(1, X) is δ and strategy (0, ri) is 1 − δ for each task j ∈ J(i).
Suppose i’s potential reference raters rj(i) use strategies (1, X)
and (0, rrj (i)) with probabilities rj (i) and 1 − rj (i) respectively,
for each task j ∈ J(i). If rj (i) > 0 for any reference rater with
proﬁciency prj (i) > 1/2, then agent i has a (strict) proﬁtable devi-
ation to δ(cid:48) = 1, i.e., to always using strategy (1, X), for all values
of ri ∈ [0, 1].

PROOF. Consider a particular task j, and let k = 1, . . . , K be
the potential reference rater for (i, j). Let ak denote the probability
that k is the reference rater for agent i for task j. By linearity of
expectation, i’s expected reward for j can be written as

(cid:104)

K(cid:88)

k=1

E[Rij ] =

δk(pipk + (1 − pi)(1 − pk)

ak

− (pi[H]pk[H] + (1 − pi[H])(1 − pk)))
+ (1 − δ)k(ripk[H] + (1 − ri)(1 − pk[H])

− (ripk[H] + (1 − ri)(1 − pk)))

+ δ(1 − k)(pi[H]rk + (1 − pi[H])(1 − rk)

− (pi[H]rk + (1 − pi[H])(1 − rk)))

+ (1 − δ)(1 − k)(rirk + (1 − ri)(1 − rk) − (rirk + (1 − ri)(1 − rk)))

(cid:88)
(cid:88)

k

k

= δ

akk(pipk + (1 − pi)(1 − pk)

− (pi[H]pk[H] + (1 − pi[H])(1 − pk[H])))

= δ

akkf1(pi, pk).

Now, E[Rij] is linear in δ, and by Lemma 3, the coefﬁcient of δ is
nonnegative for all k and pk ≥ 1/2, and strictly greater than 0 if
k > 0 for some k with pk > 1/2. Therefore, i can strictly increase
her expected reward E[Rij] by increasing δ for any δ < 1, as long
as there is some reference agent k with k > 0 and pk > 1/2.
The same argument holds for each task j ∈ J(i), and therefore
to strictly improve i’s total reward E[Ri], we only need one refer-
ence rater across all tasks to satisfy k > 0 and pk > 1/2 to obtain
a strictly beneﬁcial deviation (recall that we assumed pi ≥ 1/2 for
all i).

This lemma immediately allows us to show that the only low-
effort equilibria of M that we reasonably9 need to be concerned
about is the pure-strategy equilibrium in which eij = 0 for all i, j.
Note that different agents could use different ri (or even rij) in such
equilibria, but all agents will receive reward 0 in all such equilibria.

THEOREM 8. Suppose every agent can be a reference rater with
some non-zero probability for every other agent, and tasks are apri-
ori equivalent. Then, the only equilibria (symmetric or asymmet-
ric) in which agents mix between (1, X) and any low-effort strategy
[(0, rij)] with non-trivial support on [(0, rij)] are those where all
agents always use low effort on all tasks.

Eliminating low-effort equilibria. Our ﬁnal result uses Lemma 7
to obtain a result about eliminating low-effort equilibria. Suppose
there are some trusted agents (for example, an instructor or TA in
the peer-grading context or workers with long histories of accurate
evaluations or good performance in crowdsourcing platforms) who
always report truthfully with proﬁciency t > 1/2. Let t denote
the minimum probability, over all agents i, that the reference rater
for agent i is such a trusted agent (note that we can ensure t > 0
by having the trusted agent randomly choose each task with pos-
itive probability). Lemma 7 immediately gives us the following
result for Md, arising from the fact that the reward from playing a
random strategy (0, r) is exactly 0—the presence of trusted agents
with a non-zero probability, however small, is enough to eliminate
low-effort equilibria altogether.

THEOREM 9. Suppose t > 0. Then [(0, rij)] is not an equi-

librium of M for any rij ∈ [0, 1].

PROOF. Suppose all agents except the trusted agent use the strat-
egy (0, rij), and t is the probability that the trusted agent is the
reference rater for any agent-task pair. Then, since agent i reports
Xij according to a random coin toss independent of the actual real-
ization of j, the payoff from any reference rater, whether the trusted
agent or another agent playing (0, r) is 0. For notational simplicity,
let r = rij, r(cid:48) = rrj (i)j.

E[Rij ]
= t(rt[H] + (1 − r)(1 − t[H]) − (rt[H] + (1 − r)(1 − t[H]))
+ (1 − t)(rr(cid:48) + (1 − r)(1 − r(cid:48)) − (rr(cid:48) + (1 − r)(1 − r(cid:48))))
= 0.

By deviating to (1, X), agent i can strictly improve her payoff as
long as t > 0 and t, p > 1/2, since her expected reward from this
deviation is

E[Rij ]
= t(pt + (1 − p)(1 − t) − (p[H]t[H] + (1 − p[H])(1 − t[H]))
+ (1 − t)(rr(cid:48) + (1 − r)(1 − r(cid:48)) − (rr(cid:48) + (1 − r)(1 − r(cid:48)))
> 0,

9(We say reasonably because of the technical possibility of equi-
libria where some agents mix over (1, X c) as well.)

(cid:105)

327since the coefﬁcient of t is positive for t, p > 1/2 by Lemma 3.
Therefore, there is a strictly beneﬁcial deviation to (1, X), so there
is a choice of multiplier for the reward such that the payoff to agent
i, which is the difference between the reward and the cost of effort
c, is strictly positive as well. So (0, rij) is not an equilibrium of M
when t > 0.

This result, while simple, is fairly strong: as long as some posi-
tive fraction of the population can be trusted to always report truth-
fully with proﬁciency greater than 1/2, the only reasonable10 equi-
librium of M is the high-effort equilibrium [(1, X)], no matter how
small this fraction. In particular, note that M does not need to as-
sign a higher reward for agreement with a trusted agent to achieve
this result, and therefore does not need to know the identity of the
trusted agents. In contrast, the mechanism which rewards agents
for agreement with a reference rater without subtracting out our
statistic term must use a higher reward w(t) for agreement with
to eliminate low-effort
the trusted agents which increases as 1
t
equilibria11— this, in addition to being undesirably large, also re-
quires identiﬁcation of trusted agents.

5. DISCUSSION

In this paper, we introduced the problem of information elicita-
tion when agents’ proﬁciencies are endogenously determined as a
function of their effort, and presented a simple mechanism which
uses the presence of multiple tasks to identify and penalize low-
effort agreement to incentivize effort when tasks have binary types.
Our mechanism has the property that maximum effort followed by
truthful reporting is the Nash equilibrium with maximum payoff to
all agents, including mixed strategy equilibria. In addition to han-
dling endogenous agent proﬁciencies, to the best of our knowledge
this is the ﬁrst mechanism for information elicitation with this ’best
Nash equilibrium’ property over all pure and over mixed strategy
equilibria that requires agents to only report their own evaluations
(i.e., without requiring ‘prediction’ reports of their beliefs about
other agents’ reports), and does not impose any requirement on a
diverging number of agent reports per task to achieve its incentive
properties. Our mechanism provides a starting point for designing
information elicitation mechanisms for several crowdsourcing set-
tings where proﬁciency is an endogenous, effort-dependent choice,
such as image labeling, tagging, and peer grading in online educa-
tion.

We use the simplest possible model that captures the complexi-
ties arising from strategically determined agent proﬁciencies, lead-
ing to a number of immediate directions for further work. First,
our underlying outcome space is binary (H or L)— modeling and
extending the mechanism to allow a richer space of outcomes and
feedback is one of the most immediate and challenging directions
for further work. Also, our model of effort is binary, where agents
either exert full effort and achieve maximum proﬁciency, or exert

10Again, we say reasonable rather than unique because (1, X c) does
remain an equilibrium of M for all t less than a threshold value—
however, in addition to being an unnatural and risky strategy, this
equilibrium yields strictly smaller payoffs than [(1, X)] when t >
0. Note also that the introduction of such trusted agents does not
introduce new equilibria, and that [(1, X)] remains an equilibrium
of M.)
11The same is the case for a mechanism based on rewarding for the
‘right’ variance, which does retain [(1, X)] as a maximum reward
equilibrium, but still requires identifying the trusted agents and re-
warding extra for agreement with them.

no effort to achieve the baseline proﬁciency. While our results ex-
tend to a model where proﬁciency increases linearly with cost, a
natural question is how they extend to more general models, for
example, with convex costs. Finally, a very interesting direction is
that of heterogenous tasks with task-speciﬁc priors and abilities. In
our model, tasks are homogenous with the same prior P[H], and
agents have the same cost and maximum proﬁciency for each task.
If tasks differ in difﬁculty, and agents can observe the difﬁculty of
a task prior to putting in effort, there are clear incentives to shirk
on harder tasks while putting in effort for the easier ones. While
tasks are indeed apriori homogenous (or can be partitioned to be
so) in some crowdsourcing settings, there are other applications
where some tasks are clearly harder than others; also, agents may
have task-speciﬁc abilities. Designing mechanisms with strong in-
centive properties for this setting is a very promising and important
direction for further work.

Acknowledgements. We thank David Evans, Patrick Hummel
and David Stavens for helpful discussions and pointers to related
literature, and the anonymous referees for comments and sugges-
tions that helped improve the presentation of the paper.

6. REFERENCES
[1] P. Bolton and M. Dewatripont. Contract theory. The MIT

Press, 2005.

[2] Y. Chen, D. Bacon, I. Kash, D. Parkes, M. Rao, and

M. Sridharan. Predicting your own effort. In Proceedings of
the 11th International Conference on Autonomous and
Multiagent Systems (AAMAS 2012), 2012.

[3] S. Goel, D. M. Reeves, and D. M. Pennock. Collective

revelation: A mechanism for self-veriﬁed, weighted, and
truthful predictions. In Proceedings of the 10th ACM
conference on Electronic commerce (EC 2009), pages
265–274. ACM.

[4] C. Harris. You’re Hired! An Examination of Crowdsourcing

Incentive Models in Human Resource Tasks. In M. Lease,
V. Carvalho, and E. Yilmaz, editors, Proceedings of the
Workshop on Crowdsourcing for Search and Data Mining
(CSDM) at the Fourth ACM International Conference on
Web Search and Data Mining (WSDM), pages 15–18, Hong
Kong, China, February 2011.

[5] L. John, G. Loewenstein, and D. Prelec. Measuring the

prevalence of questionable research practices with incentives
for truth telling. Psychological science, 23(5):524–532,
2012.

[6] R. Jurca and B. Faltings. Enforcing truthful strategies in

incentive compatible reputation mechanisms. In Proceedings
of the 1st International Workshop on Internet and Network
Economics (WINE 2005), volume 3828, pages 268–277.
ACM, Springer Verlag.

[7] R. Jurca and B. Faltings. Incentives for expressing opinions

in online polls. In Proceedings of the 9th ACM conference on
Electronic commerce (EC 2008), pages 119–128, New York,
NY, USA. ACM.

[8] R. Jurca and B. Faltings. Minimum payments that reward

honest reputation feedback. In Proceedings of the 7th ACM
conference on Electronic commerce (EC 2006), pages
190–199. ACM.

[9] R. Jurca and B. Faltings. Robust incentive-compatible
feedback payments. In Trust, Reputation and Security:
Theories and Practice, volume 4452, pages 204 ˝U–218.
Springer-Verlag, 2007.

328[10] R. Jurca and B. Faltings. Mechanisms for making crowds

truthful. J. Artif. Int. Res., 34(1):209–253, Mar. 2009.

[11] N. Lambert and Y. Shoham. Truthful surveys. Proceedings of

the 3rd International Workshop on Internet and Network
Economics (WINE 2008), pages 154–165.

[12] N. Miller, P. Resnick, and R. Zeckhauser. Eliciting
informative feedback: The peer-prediction method.
Management Science, pages 1359–1373, 2005.

[13] G. Paolacci, J. Chandler, and P. Ipeirotis. Running

experiments on amazon mechanical turk. Judgment and
Decision Making, 5(5):411–419, 2010.

[14] A. Papakonstantinou, A. Rogers, E. Gerding, and

N. Jennings. Mechanism design for the truthful elicitation of
costly probabilistic estimates in distributed information
systems. Artiﬁcial Intelligence, 175(2):648–672, 2011.

[15] D. Prelec. A Bayesian Truth Serum for subjective data.

Science, 306(5695):462–466, 2004.

[16] D. Prelec and H. Seung. An algorithm that ﬁnds truth even if

most people are wrong. Working Paper, 2007.

[17] J. Witkowski and D. Parkes. A robust Bayesian Truth Serum

for small populations. In Proceedings of the 26th AAAI
Conference on Artiﬁcial Intelligence (AAAI 2012).

[18] J. Witkowski and D. C. Parkes. Peer prediction with private

beliefs. In Proceedings of the 1st Workshop on Social
Computing and User Generated Content (SC 2011).

[19] J. Witkowski and D. C. Parkes. Peer Prediction without a

Common Prior. In Proceedings of the 13th ACM Conference
on Electronic Commerce (EC 2012). ACM.

329