Estimating Clustering Coefﬁcients and Size of Social

Networks via Random Walk

Stephen J. Hardiman∗
Capital Fund Management,

France

hardimas@tcd.ie

ABSTRACT
Online social networks have become a major force in today’s
society and economy. The largest of today’s social networks
may have hundreds of millions to more than a billion users.
Such networks are too large to be downloaded or stored lo-
cally, even if terms of use and privacy policies were to permit
doing so. This limitation complicates even simple computa-
tional tasks. One such task is computing the clustering coef-
ﬁcient of a network. Another task is to compute the network
size (number of registered users) or a subpopulation size.
The clustering coeﬃcient, a classic measure of network con-
nectivity, comes in two ﬂavors, global and network average.
In this work, we provide eﬃcient algorithms for estimating
these measures which (1) assume no prior knowledge about
the network; and (2) access the network using only the pub-
licly available interface. More precisely, this work provides
three new estimation algorithms (a) the ﬁrst external access
algorithm for estimating the global clustering coeﬃcient; (b)
an external access algorithm that improves on the accuracy
of previous network average clustering coeﬃcient estimation
algorithms; and (c) an improved external access network size
estimation algorithm.

The main insight oﬀered by this work is that only a rela-
tively small number of public interface calls are required to
allow our algorithms to achieve a high accuracy estimation.
Our approach is to view a social network as an undirected
graph and use the public interface to retrieve a random walk.
To estimate the clustering coeﬃcient, the connectivity of
each node in the random walk sequence is tested in turn.
We show that the error of this estimation drops exponen-
tially in the number of random walk steps. Another insight
of this work is the fact that, although the proposed algo-
rithms can be used to estimate the clustering coeﬃcient of
any undirected graph, they are particularly eﬃcient on social
network-like graphs. To improve the network size prior-art
estimation algorithms, we count node collision one step be-
fore they actually occur.
In our experiments we validate
our algorithms on several publicly available social network
datasets. Our results validate the theoretical claims and
demonstrate the eﬀectiveness of our algorithms.

∗

Research was conducted while the author was unaﬃliated.

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW2013, May. 13-17, Rio de Janeiro, Brazil
ACM 978-1-4503-2035-1/13/05.

Liran Katzir

Advanced Technology Labs,
Microsoft Research, Israel
lirank@microsoft.com

Categories and Subject Descriptors
F.2.2 [Theory of Computing]: Analysis of Algorithms
and Problem Complexity—Nonnumerical Algorithms and
Problems
General Terms
Algorithms
Keywords
Estimation, Sampling, Clustering Coeﬃcient, Social Net-
work

1.

INTRODUCTION

The popularity of online social networks has grown enor-
mously in recent years. Users of the most popular social
network, FacebookTM, now number greater than a billion1.
This popularity has increased interest in analyzing the prop-
erties of these networks. In [2, 13, 21] the authors investigate
structural measures of online social networks, including de-
gree distribution and clustering coeﬃcient.

Large social networks, as well as search engines, provide
a public interface as part of their service. Estimating struc-
tural measures of the network using only these public inter-
faces is a research question that has received much attention
in recent studies. Search engine public interfaces have been
used in [6, 8] to estimate corpus size, index freshness, and
density of duplicates, and in [7] estimate the impressionrank
of a webpage. Online social network public interfaces have
been used in [13, 14, 25] to estimate the assortativity co-
eﬃcient, degree distribution, and clustering coeﬃcients of
online social networks, as well as in [14, 15] to estimate the
number of registered users.

In practical scenarios, the underlying social network may
be available only through a public interface. The public
interface of most social networks provides the ability to re-
trieve a list of a user’s connections (“friends”). By applying
this function iteratively to a random member of the connec-
tion list one can eﬀectively perform a random walk on the
network. Although the public interface allows us to store the
social network locally, this practice is considered impracti-
cal due to high time/space/communication cost and often
violates the terms of use agreement. In light of this, in this
paper we proceed under the assumption that (1) only ex-
ternal access to the social network is available; and (2) only
a small number of users/nodes can be sampled. The main

1http://newsroom.fb.com/News/457/One-Billion-People-
on-Facebook

539insight oﬀered by this work is that, even under these limita-
tions, our algorithms achieve a good estimation accuracy of
the network’s structural measures.

This work focuses on two particular structural measures.
The ﬁrst measure is called the clustering coeﬃcient. The sec-
ond measure is the size of the network. Namely, the number
of registered users in the network2.

The clustering coeﬃcient comes in two main ﬂavors, (1)
the network average clustering coeﬃcient [12]; and (2) the
global clustering coeﬃcient [12]. Both measures are impor-
tant for the understanding of the network structure. First,
we introduce the local clustering coeﬃcient of a node in a
graph as the ratio of the number of edges between its neigh-
bors to the maximal possible number of such edges. The
network average clustering coeﬃcient of a graph is the local
clustering coeﬃcient averaged over the set of nodes in the
graph. The global clustering coeﬃcient of a graph is the
ratio of the number of triangles (ordered triples of diﬀer-
ent nodes in which are all nodes connected) to the number
of connected triplets (ordered triples of diﬀerent nodes in
which consecutive nodes are connected).

The size of the network is one of the basic structural mea-
sures. The network size can determine the worth of a net-
work (for business development). For certain applications
in business development and advertisement, the size of a
social network subpopulation is extremely important. For
example, the number of users of an online product or the
number of potential users for a product. The subpopulation
fraction (which can also be estimated eﬃciently [15]) and
the network size can determine the size of the subpopula-
tion. Although some networks report their size periodically,
the diﬀerence between consecutive reports can be more than
ten percent. Moreover, even if this number is reported every
day, an unbiased independent estimate would be beneﬁcial.
This work contains three main contributions. The ﬁrst
and principal contribution is the ﬁrst external access esti-
mator for the global clustering coeﬃcient. The second con-
tribution is an improved external access estimator for the
network average clustering coeﬃcient. The third contribu-
tion is an improved external access estimator for the network
size.

The rest of this paper is organized as follows. Section 2
surveys related work. Section 3 provided preliminaries and
notations. Section 4 details our clustering coeﬃcient estima-
tors. Section 5 details our network size estimator. Section 6
reports our experimental results. We conclude the paper in
Section 7.

2. BACKGROUND AND PRIOR WORK

We consider the social network as an undirected graph
where nodes and edges are represented by users and friend-
ship connections. Although the algorithms presented in this
paper are correct for general graphs, the structure of social
networks renders them even more eﬀective.

Both the network average and the global clustering coeﬃ-
cient (also known as transitivity) are a long studied classical
computer science problem. The running time of the naive
algorithm for computing them is O(n3) for dense graphs
(where n is the number of nodes in the graph), and it is con-
sidered impractical for large graphs. For the global cluster-

2Technically, the algorithm estimates the size of the largest
connected component and isolated users are neglected.

ing coeﬃcient, the most challenging part of the computation
is counting the total number of triangles, since computing
the number of connected triplets is done in linear time. To
this end, the computation of global clustering coeﬃcient and
the computation of the number of triangles is equivalent.

We provide references for a partial list (most recent) for
several directions for estimating the number of triangles.
Alon et al. [3] provided an exact algorithm for the count-
ing the number of triangles. The running time of this al-
2ω
ω+1 ) = O(E1.41), where ω < 2.376 is the
gorithm is O(E
exponent of matrix multiplication. Avron [4] provided an
estimator based on numerical matrix-vector multiplication
using O(log2 n) samples, each of which requires O(|E|) time
(where E is the set of nodes in the graph). Both these algo-
rithm access the entire graph.

Buriol et al. [10] provided an approximate solution to the
global clustering coeﬃcient in the streaming model. The
streaming models allows the algorithm to have a single pass
on the input while (1) reading the edges in arbitrary/vertex
ordered appearance (diﬀerent algorithms) and (2) use con-
stant amount of space. Becchetti et al. [9] provided an
algorithm for the network average clustering coeﬃcient in
the streaming model. In contrast to [3, 4] these works as-
sume there is no random access to the graph. However, the
streaming algorithms access each edge at least once.

Schank et al. [27], provided estimators for both the global
and network average clustering coeﬃcient which only uses
a sample of the nodes. However, unlike our work, the al-
gorithms assume there is an eﬃcient way to sample nodes
with distribution that is tailored to the clustering coeﬃcient.
Speciﬁcally, for the network average clustering coeﬃcient the
sampling distribution is the uniform distribution and for the
global clustering coeﬃcient each node vi with degree di is
sampled proportionally to di(di − 1). In contrast, the algo-
rithms provided in this work do not even assume the number
of nodes is known and does not require a tailored sampling
distribution.

Another research direction [13, 25] addresses the problem
of estimating the local clustering coeﬃcient with external
access3.
In these papers, the graph can only be accessed
via the exploration of nodes that lie on the frontier of previ-
ously explored nodes. Ribeiro et al [25] explored the graph
using a random walk. Gjoka et al. [13] explored the graph
using Metropolis-Hastings random walk that generates uni-
form samples from the nodes set. In both these papers, the
computation requires augmenting the set of explored nodes,
S, with further exploration of S’s ego network. An ego net-
work of a set of users S, is the set of users S(cid:3)
that contains
all the users in S and all their (immediate) friends [13, 28].
In this work, we perform a random walk but remove the
requirement of exploring the ego network. This diﬀerence
is illustrated in Figure 1. The random walk contains three
nodes v1, v2, v3. Our approach requires the exploration of
the nodes v1, v2, v3 (marked by a thick circle), the ego net-
work approach requires additional exploration of the nodes
v4, v5, v6. In total the ego network requires exploration of
all the nodes v1, v2, . . . , v6 (marked by solid ﬁll).
In sec-
tion 6, we show that the algorithm provided in this paper

3Ref [25] mistakenly refers to the global clustering coeﬃ-
cient, but provides an accurate deﬁnition of the network
average clustering coeﬃcient.

540va

vb

vc

in random walk

in ego network

visible by random walk
and ego network

visible by ego network
only

beyond reach

visible by neither

v3

v5

v7

v1

v2

v9

v4

v6

v8

number of samples needed to guarantee convergence for a
ﬁxed accuracy is O(n1/4 log n) [15].

In some applications the size of a subpopulation needs
to be estimated. This subpopulation is deﬁned by a prop-
erty of the user’s proﬁle. For example, the number of regis-
tered users who use a speciﬁed online product. Estimating
the size of a subpopulation requires multiplying the total
size of the network by the ratio the target nodes to the
total nodes which could also be estimated by the random
walk [15]. In this work, we improve the network size estima-
tion algorithms by using not only the visited node ids, but
also the adjacency list of each of the visited nodes. This is
done by counting node collision one step before they actu-
ally occur. Namely, two nodes on the random walk (enough
nodes apart) that share a connection. We call this collision
a neighbor collision.

Figure 1: An example of a random walk with its
corresponding ego network augmentation.

outperform competing approaches [25, 13] on all the social
networks we study.

Another method for estimating the clustering coeﬃcient
from a random walk was presented in [14]. This algorithm
uses only the ids of nodes visited by a random walk and
does not assume any prior information. In contrast, the al-
gorithms in this paper assume not only the node ids are
visible, but also their list of friends (adjacency list). Practi-
cally, if this assumption holds, it renders [14] uncompetitive.
In this work, two estimators are provided for the cluster-
ing coeﬃcient. The ﬁrst for the network average clustering
coeﬃcient and the second for the global clustering coeﬃ-
cient. Both estimators use samples taken from a random
walk on the graph. Namely, not only that the algorithms do
not access the entire graph, they do not even have random
access to the graph’s nodes and edges. The only assumption
is that a random walk can be performed via the public inter-
face, and the visited node ids along with their list of friends
(adjacency list). This is the case for many social networks.
Indeed, the act of performing a random walk at all in an
online social network typically necessitates having access to
this information.

Both [14, 15] provide estimators for the total number of
registered users in the network. These algorithms use only
the node ids visited on the random walk and do not assume
any prior information on the graph. The underlying idea
in both papers is to count node collision, a pair of indices
(k, l) such that the same node appears in the kth and lth
location of the random walk. Nodes on the random walk
are highly correlated when their index distances (|k − l|) are
short, which increases the probability of a node collision. To
ensure a uniﬁed probability of collision across all node pairs,
a collision is counted only if the nodes appear a signiﬁcant
number of steps apart. These works diﬀer in the way they
select these pairs.
In [15] the estimator chooses all pairs
in which both k and l are a multiple of a parameter m,
while [14] chooses all pairs in which m ≤ |k − l|. Choosing
all pairs [14] is practically better, but harder to analyze. The
convergence of social network like graphs is very fast and
depends on the degree distribution. For example, if the node
degrees are distributed according to a Zipﬁan distribution
n and parameter α = 2, then the
with maximum degree of

√

3. PRELIMINARIES AND NOTATIONS
We denote by G(V, E) the social network’s underlying
undirected graph, where V = {v1, v2, . . . , vn} is the set of
nodes (users) and E is the set of edges (friendship connec-
tions). Additionally, we denote by di the degree of node
i=1 di = 2|E|. The
vi and the sum of degrees by D =
maximum degree of a node in the graph is noted by dmax =
i=1 di.
maxn
We denote by an n× n matrix A the adjacency matrix for
graph G. Namely, Ai,k = Ak,i = 1 if node vi is connected
by an edge to node vk and 0 otherwise. We assume no self
loops, thus Ai,i = 0 for all i.

(cid:2)

n

Definition 1. A triplet of nodes (vj , vi, vk) is called con-
nected if vj is connected to vi, vi is connected to vk, and
j < k. Formally, if Aj,i = 1, Ai,k = 1, and j < k.

Definition 2. A triangle is a connected triplet (vj, vi, vk)

in which vj and vk are connected. Formally, if Aj,k = 1.

(cid:2)

(cid:2)

Following these deﬁnitions, a triplet of nodes is connected
if j < k and Aj,iAi,k = 1 and it is a triangle if j < k
and Ai,jAi,kAj,k = 1. For a speciﬁc node vi, the number
of connected triplets (vj , vi, vk) is thus
Aj,iAi,k. Note
Aj,iAi,k = di(di−1)/2 since there are di(di−1)/2
that
choices for j < k in which both Aj,i = 1 and Ai,k = 1.
For a speciﬁc node vi, the number of (vj , vi, vk) triangles is
denoted by li =
Ai,j Ai,kAj,k (it is also the number of
edges between neighbors of vi) .

(cid:2)

j<k

j<k

j<k

Definition 3. The local clustering coeﬃcient [12] for node
vi, denoted by ci, is deﬁned as the ratio of the number of
(vj , vi, vk) triangles to the number of (vj , vi, vk) connected
triplets. Formally,

ci =

2li

di(di − 1)

Note that ci ∈ [0, 1]. In the case where di = 1 or di = 0, we
have ci = 0.

Definition 4. The network average clustering coeﬃcient

[12], denoted by cl, is deﬁned by
n(cid:3)

cl =

1
n

ci

i=1

541Definition 5. The global clustering coeﬃcient [12], de-
noted by cg, is deﬁned as the ratio of the total number of
triangles to the total number of connected triplets. Formally,

(cid:2)

(cid:2)
i=1 li
i=1 di(di − 1)

2
n

n

cg =

Note that a set of three nodes {vj , vi, vk} forms three dif-
ferent triangles4 one is counted in lj , a second in li, and a
third in lk.

The ﬁrst step of the estimation algorithms is to generate
a random walk. A random walk with r steps on G, denote
by R = (x1, x2, . . . , xr), is deﬁned as follows: start from an
arbitrary starting node vx1 , then move to one of the neigh-
boring nodes uniformly at random (with probability 1
)
and repeat r − 1 times. We use Pr [A] to denote the prob-
dxi
ability that event A occurred. We denote the distribution
induced by R, as

πR = (Pr [xr = 1] , Pr [xr = 2] , . . . ,Pr [ xr = n]) .

The probability Pr [xr = i] after many random walk steps
converges to pi (cid:2) di/D and the vector π = (p1, p2, . . . , pn)
is called the stationary distribution of G.

In our estimators, we assume that x1 is drawn from the
stationary distribution5. This assumption is valid because
we can always perform an initial random walk from an ar-
bitrary node to draw a starting node from the stationary
distribution.

The actual number of steps needed to converge to the
stationary distribution depends on the mixing time of G.
There are several deﬁnitions of mixing time, many of which
are known to be equivalent up to constant factors. All def-
initions take an  parameter to measure the distance be-
tween the stationary and the induced distribution. Both
the book [17] and the survey [19] provide excellent overview
on random walks and mixing times. We denote the mixing
time of graph G by τ () or τ ( is assumed to be a small
constant). We use the following deﬁnition:

Definition 6. Let R = (x1, x2, . . . , xr) be a random walk.
Then, let the distance between π and πR be the maximum dif-
ference between the probability of drawing a speciﬁc node xr
over all possible choices of nodes x1 and xr. Namely,

d(r) =

n

n

max
x1=1

max
i=1

|pi − Pr [xr = i]|.

We have τ () = min {r | d(r) ≤ }.

be r = log2 n for the Facebook network, r = 3 log2 n for
the DBLP and youtube networks, and r = 10 log2 n for the
Live Journal network. Both the low mixing time and the
relatively high value of the clustering coeﬃcients enable the
clustering coeﬃcient estimation algorithms in this paper to
provide accurate result with relatively low number of sam-
ples. Notations are summarized in Table 1.

G
n
A
vi
di
D
r
xk
pi
π
li
cl
cg
ˆcl
ˆcg
ˆn
τ ()
dmax

underlying undirected graph
number of nodes in the graph

adjacency matrix for G
(cid:2)

degree of node vi
the sum all nodes degrees

node in G

n

i=1 di

total number of steps in the random walk
the index of kth node in the random walk

p(xk = i) = di
D

the stationary distribution (p1, p2, . . . , pn)
number of edges between neighbors of vi

network average (local) clustering coeﬃcient

global clustering coeﬃcient

cl estimation
cg estimation
n estimation
mixing time
i=1 di
maxn

Table 1: Summary of notations

4. CLUSTERING COEFFICIENT ESTIMA-

TION

We now present the main observation used in both net-
work average and global clustering coeﬃcient estimators.
Given a random walk (x1, x2, . . . , xr), we deﬁne a new vari-
able φk = Axk−1,xk+1 for every 2 ≤ k ≤ r − 1. For any
function f (xk) the following holds6:

E [φkf (xk)] =

=

=

i=1

n(cid:3)
n(cid:3)
n(cid:3)

i=1

i=1

piE [φkf (xk)|xk = i]

di
D

1
D

2li
d2
i

2li
di

f (vi)

f (vi).

(1)

Social network graphs are known to have low mixing times
and constant clustering coeﬃcients (which are not extremely
small). Recently, Addario-Berry et al [1] proved rigorously
that the mixing time of Newman-Watts [23, 24] small world
networks is Θ(log2 n). Mohaisen et al. [22] provide numeri-
cal evaluation of the mixing time of several networks. The
authors claim that “the mixing time is much larger than an-
ticipated”. However, Table 1 and Figure 2 in their paper
show that to have d(r) ≈ 0, the number of steps should
4In some references a triangle is deﬁned by an unordered set
of three nodes, in which case cg is deﬁned by three times the
ratio of the total number of triangles to the total number of
connected triplets.
5This is not necessary in practice. However, the running
time bound is tighter with this assumption.

The ﬁrst equality holds due to the law of total expectation.
The second equality holds because there are d2
i equal proba-
bility combinations of (xk−1, vi, xk+1) out of which only 2li
form a triangle (vj , vi, vk) or a reverse triangle (vk, vi, vj).
Notice that in a triangle or a reverse triangle vj is connected
to vk (Aj,k = 1). The third equality holds due to algebraic
manipulation.
4.1 Network average clustering coefﬁcient

To estimate cl, we introduce two variables. First, we de-
1
ﬁne Φl as a weighted sum of φjs, Φl = 1
−1 .
r−2
dxk
Second, we deﬁne Ψl as the sum of the sampled nodes re-
ciprocal degrees, Ψl = 1
r
6We choose f (vi) = 1/ (di − 1) for the network average clus-
tering and f (vi) = di for the global clustering estimator.

r−1
k=2 φk

(cid:2)

1
dxk

r
k=1

.

(cid:2)

542Using linearity of expectation and Eq (1) it is easy to

(cid:4)
(cid:4)

compute Φl and Ψl expectation.

(cid:5)

n(cid:3)

E [Φl] = E

E [Ψl] = E

φk

1
− 1
(cid:5)
dxk
n(cid:3)

1
dxk

=

i=1

1
D

2li

di(di − 1)
n
D

=

i=1

di
D

1
di

=

n(cid:3)

i=1

ci

=

1
D

From the above equations we can isolate cl and get that:

n(cid:3)

i=1

cl =

1
n

ci =

E [Φl]
E [Ψl]

Intuitively, both Φl and Ψl converge to their expected values
and the estimator Φl/Ψl converges to cl as well.

Definition 7. Let ˆcl be the estimator for cl, deﬁned as

follows:

ˆcl (cid:2) Φl
Ψl

.

Lemma 1. For any  ≤ 1/8 and δ ≤ 1 we have:

Pr[cl(1 − ) ≤ ˆcl ≤ cl(1 + )] ≥ 1 − δ

(cid:7)
when the number of samples, r, satisﬁes:

(cid:6)

r ≥ rl ∈ O

D
ncl

τ ()

.

Proof. The proof ﬁrst ﬁnds the number of step, rl, which
guarantees both Φl and Ψl be within /3 approximations to
their expected values with probability at least 1 − δ/2. See
Appendix A for more details. Since the probability of Φl or
Ψl deviating from their expected value is at most δ/2, the
probability of either Φl or Ψl deviating is at most δ (using
the union bound). Then, we use the fact that

(1− )cl ≤ (1 − 

3 )
(1 + 
3 )

E [Φl]
E [Ψl]

≤ Φl
Ψl

≤ (1 + 
3 )
(1 − 
3 )

E [Φl]
E [Ψl]

≤ (1 + )cl

to complete the proof.

Note that for social network like graph the mixing time is
assumed to be relatively low (for Newman-Watts networks
τ () = O(log2 n) [1]), D = O(n) and cl is a small constant.
Thus, the number of steps needed is linear in the mixing
time, τ ().
4.2 Global Clustering Coefﬁcient

To estimate cg, we introduce two variables. First, we de-
r−1
ﬁne Φg as a weighted sum of φjs, Φg = 1
k=2 φkdxk .
r−2
Second, we deﬁne Ψg as the sum of the sampled nodes de-
grees minus one, Ψg = 1
r

k=1 dxk

− 1.

(cid:2)

Using linearity of expectation and Eq (1) it is easy to

r

(cid:2)

compute Φg and Ψg expectation.

E [Φg] = E [φkdxk ] =

E [Ψg] = E [dxk

− 1] =

n(cid:3)
n(cid:3)

i=1

1
D

i=1

n(cid:3)

i=1

1
D

di =

2li
di
D (di − 1) =
di

2li
n(cid:3)

i=1

di(di − 1)

1
D

From the above equations we can isolate cg and get that:

cg =

1(cid:2)
i=1 di(di − 1)

n

n(cid:3)

i=1

2li =

E [Φg]
E [Ψg]

.

Intuitively, both Φg and Ψg converge to their expected

values and the estimator Φg/Ψg converges to cl as well.

Definition 8. Let ˆcg be the estimator for cg, deﬁned as

follows:

ˆcg (cid:2) Φg
Ψg

.

Lemma 2. For any  ≤ 1/8 and δ ≤ 1 we have:

Pr[cg(1 − ) ≤ ˆcg ≤ cg(1 + )] ≥ 1 − δ
(cid:7)

when the number of samples, r, satisﬁes:
Ddmax
i=1 di(di − 1)
n

r ≥ rg ∈ O

(cid:2)

(cid:6)

cg

τ ()

.

The proof is similar to the proof of Lemma 1, except the
number of steps rg that guarantees convergences for Φg and
Ψg is diﬀerent. See Appendix B for more details.

Both estimators presented in this section are consistent.
Formally, as the number of samples, r, grows the estimators
converge to the true value. This also implies the estimators
are asymptotically unbiased.

5. NETWORK SIZE ESTIMATION

In this section we present an estimator for the graph size
(number of nodes). The estimator uses observations of node
pairs which are “far away” from each other in the random
walk (as in Ref [14]). This assumption is needed to ensure
both nodes in a pair are (approximately) uncorrelated: each
drawn from the stationary distribution7. Speciﬁcally, the es-
timator examines node pairs whose index distance is greater
than a threshold m. Formally,

I = {(k, l) | m ≤ |k − l| ∧ 1 ≤ k, l ≤ r} .

The estimator counts weighted neighbor collisions. A neigh-

bor collision is a pair of indices (k, l) such that vxk and vxl
share a common neighbor. Formally, let Ai be the set of ver-
tices adjacent to vi. Thus, Ai ∩ Aj is the set of nodes neigh-
boring both vi and vj. Given a random walk (x1, x2, . . . , xr),
|. Note that if
we deﬁne a new variable φk,l = |Axk
(k, l) ∈ I, then
(cid:7)2
(cid:5)
(cid:4)

∩ Axl

(cid:6)

di
D

dj
D

|Ai ∩ Aj| 1
didj

=

dj
D

.

n(cid:3)

j=1

n(cid:3)

j=1

n(cid:3)
(cid:2)

i=1

E

φk,l

1
dxl
dxk
(cid:2)

=

n
i=1

n

j=1 |Ai ∩ Aj| =

n

To see why
j consider the
following combinatorial proof. For a node vk, the number of
connected triplets (vi, vk, vj ) with no restrictions on i and
(cid:2)
j is d2
k. Thus, the total number of connected triplets is
k=1 d2
k. Alternatively, for nodes vi and vj the number
of connected triplets (vi, vk, vj ) is |Ai ∩ Aj|. Thus, the to-
(cid:2)
(cid:2)
tal number of connected triplets can also be expressed by
n
i=1
Next, we deﬁne Φn to be the averaged value of φk,l

j=1 |Ai ∩ Aj|.

over all possible choices of (k, l) ∈ I. Namely,

1
dxk
dxl

n

(cid:2)
j=1 d2

n

(cid:3)
(k,l)∈I

Φn =

1|I|

φk,l

1
dxl
dxk

.

7The larger the value of m, the smaller the bias in the esti-
mate introduced by this correlation, but increasing m means
fewer observations of node pairs and a larger estimator vari-
ance. However, note that we again beneﬁt from the fast-
mixing nature of social graphs, and m need only be of the
order O(log2 n).

543Let Ψn be the averaged sum of
of (k, l) ∈ I. Formally,

over all possible choices

Ψn =

1|I|

.

|I| Ψn =

(cid:2)
be eﬃciently computed for every k in O(1), using a cumu-
k=1 dxk ,
lative sum precomputation. Speciﬁcally, if Bq =
then

q

r(cid:3)

l=1

1
dxl

(cid:10)
Br − B(l+m)+ + B(l−m)−

(cid:11)

.

To compute Φn one must ﬁrst construct an inverted index
of neighboring nodes. In document-term view, each node is
a document containing adjacent nodes as terms. Speciﬁ-
cally if vj is a neighbor of xk then k is a term in vj . The
running time of creating an inverted index is linear in the
2
d
number of terms (O(rdmax) worst case and O(r
(cid:2)
D ) ex-
i
pected). Then, the entry for vj holds a list Lj of all indices
in which vj is a neighbor. Thus, |I| Φn =
j=1 Cj, where
(cid:2)
. To eﬃciently compute Cj
Cj =
in O(|Lj|), a precomputation Bq(j) =
should
be used (similarly to the computation of Ψn).

(cid:2)
(k,l)∈I|k∈Lj∧l∈Lj

1
dxk
dxl

q≥k∈Lj

(cid:2)

1
dxk

n
i=1

n

6. EXPERIMENTAL EVALUATION

6.1 Networks with public dataset

We demonstrate the eﬀectiveness of the estimators by
experimenting with social networks with known structure.
Datasets statistics are enclosed in Table 2.

Network
DBLP
Orkut
Flickr

Live Journal

n

977,987
3,072,448
2,173,370
4,843,953

D/n
8.457
76.28
20.92
17.69

cl

0.7231
0.1704
0.3616
0.3508

cg

0.1868
0.0413
0.1076
0.1179

Table 2: Networks statistics

In all our datasets we perform the following: (1) if the orig-
inal network is directed, the direction is removed (the edge is
made undirected); (2) only the network’s largest connected
component is retained and the rest of the nodes/users are
dropped. All the datasets we use are publicly available9.

DBLP In the “Digital Bibliography and Library Project”
(DBLP[18]) dataset each entry is a reference to a paper
which contains a title and a list of authors.
In the
corresponding network each node is an author and an
edge between two authors represent co-authorship of
one or more papers. We used a snapshot taken Oct 01,
2012.

Orkut Orkut is a general purpose social network. The
dataset contains a partial snapshot (11.3% of the nodes)
taken during 2006 by [21]. In this social network the
friendship connections (edges) are undirected.

Flickr Flickr is an online social network with focus on photo
sharing. The dataset contains a partial snapshot taken
during 2006–2007 by [20]. In this social network the
friendship connections (edges) are directed.

9The DBLP, Orkut, Flickr, and LiveJournal are pub-
licly
and
http://konect.uni-koblenz.de/networks/{orkut-links,ﬂickr-
growth,soc-LiveJournal1} [16], respectivly.

http://dblp.uni-trier.de/xml/

available

at

dxk
dxl

(cid:3)
(k,l)∈I
n(cid:3)

(cid:5)

dxk
dxl
(cid:6)

dj
D

(cid:7)2

(cid:4)
(cid:4)

Due to linearity of expectation, we have

E [Φn] = E

E [Ψn] = E

φk,l

dxk
dxl

1
dxl
dxk
(cid:5)
n(cid:3)

=

n(cid:3)

=

i=1

j=1

j=1
di
D

(cid:7)2

(cid:6)

n(cid:3)

j=1

dj
D

dj
D

dj
di

= n

Notice that n = E [Ψn]/E [Φn]. Intuitively, both Ψn and

Φn converge to their expected values and the estimator Ψn/Φn
converges to n as well.

Definition 9. Let ˆn be the estimator for n, deﬁned as

follows:

ˆn (cid:2) Ψn
Φn

.

Prior art algorithm [14, 15] count the number of node
collisions, C, and estimates n by Ψn/C. A node collision
is a pair of indices (k, l) such that such that xk = xl.
In
contrast Φn counts neighbor collision and estimates n by
Ψn/Φn.

Lemma 3. The neighbor collision estimator, ˆn (deﬁni-
tion 9), has conﬁdence intervals tighter than the node colli-
sion estimator.

Proof. Formally, C = 1|I|

(k,l)∈I 1xk=xl where 1xk=xl
is 1 if xk = xl and 0 otherwise. The key observation is that
(cid:9)

(cid:8)

1xk+1=xl+1 | xk, xl

E

= φk,l

1
dxl
dxk

.

(cid:2)

This stems from the combinatorial argument that (a) there
are dxk
dxl equally likely joint node transitions from xk and
| of
xl to xk+1 and xl+1; and (b) in only φk,l = |Axk
them xk+1 = xl+1 holds. Note that, xk is uncorrelated with
xl when (k, l) ∈ I. Using this observation we have,
(cid:9)

∩ Axl

(cid:8)

1xk+1=xl+1 | xk, xl

.

E

Φn =

1|I|

(cid:3)
(k,l)∈I

This is the Conditional Monte Carlo estimator8 of C, which
guarantees Var [C] ≥ Var [Φn] [26](Section 5.4).
5.1 Implementation notes

n
i=1

(cid:2)

2
d
D ).
i

The straight forward computation of Ψn and Φn running
time is O(r2) and O(r2d2
max) respectively. However, a care-
ful implementation can reduce this complexity to O(r) and
O(rdmax) respectively. For Φn the expected running can be
reduced to O(r
First, we deﬁne (l+m)+ to be min {r, l + m} and (l−m)
−
to be max {l − m, 1}. For the computation of Ψn instead of
(cid:2)(l+m)+
multiplying the value of 1
by each dxk separately, it is mul-
dxl
k=(l−m)− dxk . The sum in turn, can
tiplied by the sum of
8Note that if (k, l) ∈ I, then (k − 1, l − 1) ∈ I except for
k = 1 which holds only for a negligible fraction of the pairs.

544LiveJournal LiveJournal is an on-line social network with
focus on journals and blogs. The dataset contains a
partial snapshot of the nodes taken by [5].
In this
social network the friendship connections (edges) are
directed.

The x-axis in our ﬁgures is the percentage of mined nodes
(number of mined nodes over the total number of network
nodes). The y-axis is the relative estimated value (esti-
mate value over the true value). We display [5%, 95%]-
conﬁdence intervals for all ﬁgures. A [5%, 95%]-conﬁdence
interval of random variable z, is deﬁned as the interval [L, U ]
such that Pr [z ≤ L] = 0.05 and Pr [z ≤ U ] = 0.95. Thus,
Pr [z ∈ [L, U ]] = 0.9. To estimate the conﬁdence interval,
each simulation was run independently 100,000 times. The
values L and U are estimated by the 5th and 95th percentile
values respectively.

In subsections 6.2 and 6.3 we compare the prior art al-
gorithms method with the random walk approach described
in this work. For comparison we consider the following ap-
proaches: (1) the estimator based on random walk combined
with ego network exploration described in [25] (labeled RW
Ego network); and (2) the estimator based on Metropolis-
Hastings sampling with ego network exploration described
in [13] (labeled MH Ego Network). The estimator described
in subsection 4.1 is labeled random walk.
In the random
walk estimator (our approach) the number of mined nodes is
exactly the random walk’s length, while in the Ego network
algorithms (prior art) the mined nodes include the (sampled)
walk nodes as well as their neighbors.

In subsection 6.4 we compare prior art node collision esti-
mator [14, 15] (labeled node collision) with the new proposed
neighbor collision estimator (labeled neighbor collision).
6.2 Network average clustering coefﬁcient

Figure 2 displays conﬁdence intervals for all algorithms
and datasets. The proposed random walk estimator sig-
niﬁcantly outperforms ego network estimators. Speciﬁcally,
using only 1% of the network size, the conﬁdence intervals
of the random walk estimator are about ﬁfty percent tighter
for the DBLP network and four times as tight for the Orkut,
Flickr, and LiveJournal networks. The exact numbers are
enclosed in Table 3.

Network
DBLP
Orkut
Flickr
LiveJ

random walk
[0.967, 1.033]
[0.916, 1.085]
[0.891, 1.111]
[0.951, 1.054]

MH Ego

RW Ego

[0.942, 1.051]
[0.583, 1.468]
[0.557, 1.415]
[0.816, 1.200]

[0.910, 1.073]
[0.426, 1.658]
[0.064, 2.023]
[0.645, 1.329]

Table 3: Network average clustering [5%,95%]-
conﬁdence interval for 1% mined nodes.

6.3 Global clustering coefﬁcient

In this subsection there is no prior art algorithm for com-
parison. To have a baseline, we retroﬁt the ego network es-
timator for computing the global clustering coeﬃcient. The
global clustering coeﬃcient can be viewed as a weighted sum
of local clustering coeﬃcients. The ego network sampling es-
− 1)
timators multiplies each observed cxk by wk = dxk (dxk
and divide the total by the sum W =

(cid:2)

Figure 3 displays conﬁdence intervals for all algorithms
and datasets. The proposed random walk estimator sig-

wk.

k

niﬁcantly outperforms ego network estimators by an even
greater margin when compared with the network average
clustering coeﬃcient estimators. The curve for metropolis
hasting ego network in missing in the Flickr graph because
all the values are greater than 8, which demonstrate the
estimator’s ineﬃciency. In the LiveJournal graph, one can
see the upper 95% curves are even increasing. These curves
converge only after 5% of the network is sampled. Using
only 1% of the network size, the conﬁdence intervals of the
random walk estimator are about three times tighter for the
DBLP network and ten times tighter for the Orkut network.
The ego network estimators for the Flickr and LiveJournal
networks are extremely inaccurate in the [0.1%, 2%] range.
The exact numbers are enclosed in Table 4.

Network
DBLP
Orkut
Flickr
LiveJ

random walk
[0.869, 1.180]
[0.892, 1.130]
[0.922, 1.078]
[0.620, 1.523]

MH Ego

RW Ego

[0.659, 1.919]
[0.424, 2.711]
[0.212, 10.07]
[0.235, 4.275]

[0.609, 1.485]
[0.317, 3.068]
[0.176, 1.588]
[0.246, 3.051]

Table 4: Global clustering [5%,95%]-conﬁdence in-
terval for 1% mined nodes.

6.4 Network size

In this subsection we compare the node collision and neigh-
bor collision estimators.
In all estimators the number of
mined nodes is exactly the random walk’s length. We used
m = 2.5%r as the separation parameter for all estimators.
Namely, we used about 95% of the maximum number of
(k, l) pairs (|I| ≈ 0.95r2). In Figure 4 we see that the neigh-
bor collision estimator outperforms the node collision esti-
mator. The node collision estimator and neighbor collision
estimator are Ψn/C and Ψn/Φn respectively. The perfor-
mance of the estimators depend on the variance of Ψn, C,
and Φn. The performance of the neighbor collision reduces
the variance of one factor, but retains the variance of Ψn.
Therefore, we see a diﬀerent performance impact on these
≈ 1∓x+x2∓·· ·+x2k
datasets. Moreover, the fact that
explains why the neighbor collision estimator has a greater
impact on performance in the early stages of convergence
when r is small.

1
1±x

Using only 1% of the network size, there was a signiﬁcant
accuracy improvement in the DBLP network, a noticeable
improvement for the Orkut network, and negligible improve-
ment for the Flickr and LiveJ networks. The exact number
are enclosed in Table 5. The second column is prior art
node-collision estimator; the third column is the proposed
new neighbor collision estimator; and the fourth column is
the conﬁdence bound improvement10.

7. CONCLUSIONS

We presented algorithms for estimating the (1) network
average clustering coeﬃcient; (2) global clustering coeﬃ-
cient; and (3) the number of registered users. These algo-
rithms use the information collected by random walk, namely,
the ids of the visited nodes along with their adjacency list.

10In the DBLP network the change from 1.384 to 1.221 in the
95% conﬁdence implies a (0.384−0.221)/0.384 improvement
and the change in the 5% conﬁdence from 0.752 to 0.815
implies a (0.815 − 0.752)/(1 − 0.752) improvement.

545e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

 1.3

 1.2

 1.1

 1

 0.9

 0.8

 0.7

 3

 2.5

 2

 1.5

 1

 0.5

 0

DBLP network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

Flickr network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

 2.5

 2

 1.5

 1

 0.5

 0

 2

 1.5

 1

 0.5

 0

Orkut network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

LiveJournal network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

Figure 2: Estimation of the network average clustering coeﬃcient conﬁdence interval vs. the percentage of
mined nodes.

Network
DBLP
Orkut
Flickr
LiveJ

Node

[0.752, 1.384]
[0.849, 1.187]
[0.846, 1.203]
[0.780, 1.232]

Neighbor

[0.815, 1.221]
[0.860, 1.161]
[0.843, 1.208]
[0.785, 1.218]

improvement
[25.4%, 42.5%]
[7.30%, 13.9%]
[1.91%, 2.40%]
[2.27%, 6.03%]

Table 5: Network size [5%,95%]-conﬁdence interval
for 1% mined nodes.

For the clustering coeﬃcients algorithms we showed that (1)
for social-network like graphs these algorithms considerably
outperform prior art (sampling the ego network of each sam-
pled node); and (2) an analytic bound on the number of steps
required for convergence. For the number of registered users
algorithm we showed, both analytically and experimentally,
that the new suggested algorithm is strictly more accurate
than prior art node collision algorithms.

Ego network algorithms sample all the adjacency lists of
nodes in the random walk, while the random walk estimator
samples only two nodes from this list (previous and next
node of the random walk). Investigating between these two
extremes might give rise to further improvement.

8. REFERENCES
[1] L. Addario-Berry and T. Lei. The mixing time of the

newman–watts small world. In SODA, pages
1661–1668, 2012.

[2] Y.-Y. Ahn, S. Han, H. Kwak, S. B. Moon, and

H. Jeong. Analysis of topological characteristics of
huge online social networking services. In WWW,
pages 835–844, 2007.

[3] N. Alon, R. Yuster, and U. Zwick. Finding and

counting given length cycles. Algorithmica,
17(3):209–223, 1997.

[4] H. Avron. Counting triangles in large graphs using
randomized matrix trace estimation. In Large-Scale
Data Mining: Theory and Applications (KDD
Workshop), 2010.

[5] L. Backstrom, D. P. Huttenlocher, J. M. Kleinberg,

and X. Lan. Group formation in large social networks:
membership, growth, and evolution. In KDD, pages
44–54, 2006.

[6] Z. Bar-Yossef and M. Gurevich. Random sampling
from a search engine’s index. J. ACM, 55(5), 2008.

[7] Z. Bar-Yossef and M. Gurevich. Estimating the

impressionrank of web pages. In WWW, pages 41–50,
2009.

[8] Z. Bar-Yossef and M. Gurevich. Eﬃcient search engine

measurements. TWEB, 5(4):18, 2011.

546e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

 3

 2.5

 2

 1.5

 1

 0.5

 2.5

 2

 1.5

 1

 0.5

 0

DBLP network

MH Ego network
RW Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

Flickr network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

 5

 4

 3

 2

 1

 0

 6

 5

 4

 3

 2

 1

 0

Orkut network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

LiveJournal network

RW Ego network
MH Ego network
Random walk

 0

 0.2  0.4  0.6  0.8

 1

 1.2  1.4  1.6  1.8

 2

Percentage of mined nodes

Figure 3: Estimation of the global clustering coeﬃcient conﬁdence interval vs. the percentage of mined nodes.

[9] L. Becchetti, P. Boldi, C. Castillo, and A. Gionis.

[16] J. Kunegis. KONECT – the Koblenz Network

Eﬃcient algorithms for large-scale local triangle
counting. TKDD, 4(3), 2010.

[10] L. S. Buriol, G. Frahling, S. Leonardi,

A. Marchetti-Spaccamela, and C. Sohler. Counting
triangles in data streams. In PODS, pages 253–262,
2006.

[11] K.-M. Chung, H. Lam, Z. Liu, and M. Mitzenmacher.

Chernoﬀ-hoeﬀding bounds for markov chains:
Generalized and simpliﬁed. In STACS, pages 124–135,
2012.

[12] L. F. Costa, F. A. Rodriguez, G. Travieso, and

P. R. V. Boas. Characterization of complex networks:
A survey of measurements. Advances in Physics,
56(1):167–242, Aug. 2006.

[13] M. Gjoka, M. Kurant, C. T. Butts, and

A. Markopoulou. Walking in facebook: A case study
of unbiased sampling of OSNs. Proceedings of IEEE
INFOCOM 2010, pages 1–9, 2010.

[14] S. J. Hardiman, P. Richmond, and S. Hutzler.

Calculating statistics of complex networks through
random walks with an application to the on-line social
network bebo. European Physics Journal B,
71(4):611–622, 2009.

[15] L. Katzir, E. Liberty, and O. Somekh. Estimating

sizes of social networks via biased sampling. In
WWW, pages 597–606, 2011.

Collection. http://konect.uni-koblenz.de/, 2012.

[17] D. A. Levin, Y. Peres, and E. L. Wilmer. Markov

Chains and Mixing Times. American Mathematical
Society, 2008.

[18] M. Ley. The DBLP computer science bibliography:

Evolution, research issues, perspectives. In Proc. Int.
Symp. on String Processing and Information Retrieval,
pages 1–10, 2002.

[19] L. Lov´asz and P. Winkler. Mixing times. microsurveys

in discrete. In DimacsWorkshop, 1998.

[20] A. Mislove, H. S. Koppula, K. P. Gummadi,

P. Druschel, and B. Bhattacharjee. Growth of the
ﬂickr social network. In Proceedings of the 1st ACM
SIGCOMM Workshop on Social Networks
(WOSN’08), August 2008.

[21] A. Mislove, M. Marcon, P. K. Gummadi, P. Druschel,

and B. Bhattacharjee. Measurement and analysis of
online social networks. In Internet Measurement
Comference, pages 29–42, 2007.

[22] A. Mohaisen, A. Yun, and Y. Kim. Measuring the

mixing time of social graphs. In Internet Measurement
Conference, pages 383–389, 2010.

[23] M. Newman and D. Watts. Renormalization group
analysis of the small-world network model. Physics
Letters A, 263:341–346, 1999.

547Orkut network

Node Collision
Neighbor Collision

 2.6
 2.4
 2.2
 2
 1.8
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

 2.2

 2

 1.8

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 2.6
 2.4
 2.2
 2
 1.8
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4

DBLP network

Node Collision
Neighbor Collision

 0.5

 1

 1.5

 2

 2.5

Percentage of mined nodes

Flickr network

Node Collision
Neighbor Collision

 0

 0.2

 0.4

 0.6

 0.8

 1

 1.2

 1.4

 1.6

Percentage of mined nodes

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

e
u
l
a
v

 

n
o
i
t
a
m

i
t
s
e
 
e
v
i
t
a
l
e
R

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9

 1

Percentage of mined nodes

LiveJournal network

Node Collision
Neighbor Collision

 0.4

 0.6

 0.8

 1

 1.2

 1.4

 1.6

Percentage of mined nodes

 2.2

 2

 1.8

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.2

Figure 4: Estimation of the network size conﬁdence interval vs. the percentage of mined nodes.

[24] M. Newman and D. Watts. Scaling and percolation in

the small-world network model. Physical Review E,
60:7332–7342, 1999.

[25] B. F. Ribeiro and D. F. Towsley. Estimating and

sampling graphs with multidimensional random walks.
In Internet Measurement Conference, pages 390–403,
2010.

[26] R. Y. Rubinstein and D. P. Kroese. Simulation and

the Monte Carlo Method. Wiley Series in Probability
and Statistics, 2 edition, 2007.

[27] T. Schank and D. Wagner. Approximating clustering

coeﬃcient and transitivity. J. Graph Algorithms Appl.,
9(2):265–275, 2005.

[28] S. Wasserman and K. Faust. Social Network Analysis:

Methods and Applications. Cambridge University
Press, 1994.

APPENDIX
A. CONCENTRATION OF ΨL AND ΦL

In the proof of Lemma 1 we required that the variables Ψl
and Φl give an /3 approximation to their expected values
with probability at least 1 − δ/2.

To prove both Ψl or Φl are concentrated we ﬁrst restate

a theorem from Chung et al. [11]:

Theorem 4

Let M be an er-
godic Markov chain with state space [n] and stationary dis-

(Theorem 3.1 [11]).

π =

(cid:2)

tribution π. Let τ = τ () be its -mixing time for  ≤ 1
8 . Let
(x1, x2, . . . , xr) denote an r-step random walk on M start-
ing from an initial distribution ϕ on [n], i.e., x1 ← ϕ. Let
(cid:11)ϕ(cid:11)
. For every k ∈ [r], let fk : [n] → [0, 1] be
a weight function at step k such that the expected weight
(cid:2)
Ev←π[fk(xk)] = μ for all k. Deﬁne the total weight of
the walk (x1, x2, . . . , xr) by Z (cid:2)
k=1 fk(xk). There ex-
ists some constant c (which is independent of μ, δ and )
such that for 0 < δ <1

n
i=1

2
ϕ
i
πi

r

Pr [|Z − μr| > μr] ≤ c(cid:11)ϕ(cid:11)

e−

2

μr/72τ ,

π

or equivalently

(cid:4)(cid:12)(cid:12)(cid:12)(cid:12) Z

r

Pr

(cid:12)(cid:12)(cid:12)(cid:12) > μ
(cid:5)

− μ

≤ c(cid:11)ϕ(cid:11)

π

e−

2

μr/72τ .

Lemma 5. There is a constant value, ξ, such that if r ≥

rΨl = ξ D

n

τ (), we have

(cid:4)
|Ψl − E [Ψl]| ≤ E [Ψl]

(cid:5)

Pr

≥ 1 − δ
2

. We assume that

(cid:13)

(cid:14)

1
dxk

= n
D .

3

(cid:14)

Proof. Let fk(xk) = f (xk) = 1
dxk

π = 1. We have, E [Ψl] = E

ϕ ≈ π, and thus (cid:11)ϕ(cid:11)
From Theorem 4,

(cid:13)
|Ψl − E [Ψl]| >

Pr



3

E [Ψl]

≤ ce−

2

nr/9·72·τ D

548≤
τ (). Since  and δ are constants, this ends the

nr/9·72·τ D, we have rΨl

2 = ce−

2

Extracting rΨl for which δ
˜ξ log(δ) 1
2
proof.

D
n

Lemma 6. There is a constant value, ξ, such that if r ≥

rΦl = ξ D
ncl

τ (), we have

(cid:4)

(cid:5)

Pr

|Φl − E [Φl]| ≤ E [Φl]

3

≥ 1 − δ
2

dxk

Axk ,xk+2
−1

Proof. For this bounds, we cannot apply Therorem 4
directly since fj depends on previously visited node. How-
ever, since
only depends on a 3-nodes history, we
observe a related Markov chain that remembers the last
three visited nodes. To this end, ˜M has ˜n = n × n × n
nodes, and (x1, x2, x3) ← (x2, x3, x4) with the same transi-
Axk−1,xk+1
tion probability of x3 to x4 in M . Let fk(˜xk) =
.
We assume that ϕ ≈ π, and thus (cid:11)ϕ(cid:11)
(cid:2)
π = 1. We have,
i=1 ci = n
cl. From Theo-
E [Φl] = E
(cid:14)
rem 4,

(cid:13)
φk

1
−1
dxk

= 1
D

dxk

−1

(cid:14)

D

n

(cid:13)
|Φl − E [Φl]| >

Pr

≤ ce−

2

ncl(r−2)/9·72·˜τ D

2

2

D
ncl

Extracting rΦl for which δ
rΦl
the proof.

ncl (r−2)/9·72·˜τ D, we have
≤ ˜ξ log(δ) 1
˜τ . Since  and δ are constants, this ends
Note that ˜τ () ≤ τ (). To see this, in the true station-
ary distribution the probability of drawing xk−1, xk, xk+1 is
dxk−1
. After τ () steps, the probability of drawing
xk−1 is at most  distance away. Therefore, probability of
drawing xk−1, xk, xk+1 is
, and thus
D
the diﬀerence is bounded by  1
dxk

(cid:16)
± 

≤ .

dxk−1

1
dxk

1
dxk

dxk+1

dxk+1

dxk+1

(cid:15)

D

1

1

1



3

E [Φl]
2 = ce−

To conclude we combine Lemma 5 and 6, and choose

rl = max {rΨl
B. CONCENTRATION OF ΨG AND ΦG

, rΦl

}.

In the proof of Lemma 2 we require that the variables Ψg
and Φg give an /3 approximation to their expected values
with probability at least 1 − δ/2.

Lemma 7. There is a constant value, ξ, such that if r ≥

n

(cid:2)

Pr

(cid:5)

Ddmax

rΨg = ξ

τ (), we have

i=1 di(di−1)

(cid:4)
|Ψg − E [Ψg]| ≤ E [Ψg]
−1
dxk
dmax (all values in [0, 1]).
We assume that ϕ ≈ π, and thus (cid:11)ϕ(cid:11)
(cid:2)
π = 1. We have,
i=1 di(di − 1). From
dmax E [Ψg] = E
Theorem 4,

Proof. Let fk(xk) = f (xk) =

≥ 1 − δ
2

−1
dxk
dmax

Ddmax

(cid:13)

=

3

n

1

1

(cid:14)
(cid:12)(cid:12)(cid:12)(cid:12) >

(cid:4)(cid:12)(cid:12)(cid:12)(cid:12) Ψg

dmax

Pr

− E [Ψg]
dmax

(cid:5)



E [Ψl]
dmax
3
2 = ce−
i=1 di(di−1)

Ddmax

2(cid:2)

n

≤ ce

− 

2(cid:2)

n
i=1 di
9·72·τ Ddmax

(di

−1)r

n

i=1 di(di−1)r/9·72·τ Ddmax,
τ (). Since  and δ are

Extracting rΨg for which δ
≤ ˜ξ log(δ) 1
we have rΨg
(cid:2)
constants, this ends the proof.

2

Lemma 8. There is a constant value, ξ, such that if r ≥

rΦg = ξ

(cid:2)

cg

Ddmax
n

τ (), we have
(cid:4)
i=1 di(di−1)
|Φg − E [Φg]| ≤ E [Φg]

Pr

(cid:5)

≥ 1 − δ
2

3

Proof. The proof combines the division by dmax of lemma 7

and the the 3-node history markov chain ˜M of lemma 6.

549