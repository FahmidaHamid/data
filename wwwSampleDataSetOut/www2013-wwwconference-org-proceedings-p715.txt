Diversiﬁed Recommendation on Graphs:

Pitfalls, Measures, and Algorithms

Onur Küçüktunç1,2, Erik Saule1, Kamer Kaya1, and Ümit V. Çatalyürek1,3

1Dept. Biomedical Informatics, The Ohio State University

2Dept. of Computer Science and Engineering, The Ohio State University
3Dept. of Electrical and Computer Engineering, The Ohio State University

kucuktunc.1@osu.edu, {esaule,kamer,umit}@bmi.osu.edu

ABSTRACT
Result diversiﬁcation has gained a lot of attention as a way
to answer ambiguous queries and to tackle the redundancy
problem in the results. In the last decade, diversiﬁcation has
been applied on or integrated into the process of PageRank-
or eigenvector-based methods that run on various graphs, in-
cluding social networks, collaboration networks in academia,
web and product co-purchasing graphs. For these applica-
tions, the diversiﬁcation problem is usually addressed as a
bicriteria objective optimization problem of relevance and
diversity. However, such an approach is questionable since
a query-oblivious diversiﬁcation algorithm that recommends
most of its results without even considering the query may
perform the best on these commonly used measures.
In
this paper, we show the deﬁciencies of popular evaluation
techniques of diversiﬁcation methods, and investigate multi-
ple relevance and diversity measures to understand whether
they have any correlations. Next, we propose a novel mea-
sure called expanded relevance which combines both rele-
vance and diversity into a single function in order to mea-
sure the coverage of the relevant part of the graph. We also
present a new greedy diversiﬁcation algorithm called Best-
Coverage, which optimizes the expanded relevance of the
result set with (1− 1/e)-approximation. With a rigorous ex-
perimentation on graphs from various applications, we show
that the proposed method is eﬃcient and eﬀective for many
use cases.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search pro-
cess; H.3.4 [Systems and Software]: Performance evalua-
tion (eﬃciency and eﬀectiveness)

Keywords
diversity; relevance; graph mining; result diversiﬁcation

1.

INTRODUCTION

Algorithms developed for graph-based recommendation
are very popular among web services; for instance, Ama-
zon uses co-purchasing information to recommend products
to its customers, IMDB recommends movies to its visitors
based on the information such as director, cast, and ratings,
and Google uses the web-graph and the user histories for

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

personalized web search. The recommendations are usually
made based on user preferences, either explicitly expressed
or based on what she has been looking at recently. These
preferences are used as the objects of known interest to seed
the algorithms.

One of the common problems of popular recommenda-
tion algorithms is the pollution of top recommendations with
many similar items, i.e., redundancy. It is typically not in-
teresting to be recommended slight variations of the same
product if you have a wide interest. The redundancy prob-
lem is solved via result diversiﬁcation, which has gained a lot
of attention in many ﬁelds recently [1, 8, 12, 14, 16, 19, 20,
21]. Diversiﬁcation usually refers to the process of return-
ing a set of items which are related to the query, but also
dissimilar among each other. The problem of recommend-
ing a diversiﬁed set is inherently qualitative and is evaluated
diﬀerently in various contexts [3, 4, 7, 23].

Most diversiﬁcation studies in the literature rely on var-
ious assumptions, e.g., objects and queries are categorized
beforehand [22], or there is a known distribution that spec-
iﬁes the probability of a given query belonging to some cat-
egories [1]. In the context of information retrieval or web
search, since the search queries are often ambiguous or mul-
tifaceted, a query should represent the intent of an average
user with a probability distribution [22]. Intent-aware meth-
ods in the literature aim to cover various relevant categories
with one or more objects, or as TREC deﬁnes its diversity
task, “documents are judged on the basis of the subtopics,
based on whether or not a document satisﬁes the informa-
tion need associated with that subtopic” [6].

In this work, we assume that the graph itself is the only in-
formation we have, and no categories or intents are available.
We are interested in providing recommendations based on a
set of objects of interest. The recommended items should be
related to the user’s interests while being dissimilar to each
other. This particular problem has attracted a lot of at-
tention recently, and many algorithms and evaluations have
been proposed [5, 8, 13, 14, 16, 20, 24, 25].

Evaluation of algorithms’ quality is one interest of the pa-
per. Usually, algorithms are evaluated by expressing the
problem as a bicriteria optimization problem. The ﬁrst cri-
teria is related to relevancy, e.g., the sum of the personalized
PageRank scores, and the second is related to diversity, e.g.,
the density or the expansion ratio of the subgraph formed
by the recommended set. These two criteria are either ag-
gregated (often with a simple linear aggregation) or they are
considered simultaneously with Pareto dominance (where
the solutions are in the relevancy-diversity objective space).

715As the ﬁrst contribution, we show that such an evaluation is
inappropriate. Indeed, we design query-oblivious algorithms
for the two popular combinations of objectives that return
most of the recommendations without considering the user’s
interests, yet, perform the best on these commonly used
measures.

We argue that a result diversiﬁcation algorithm should
be evaluated under a measure which tightly integrates the
query in its value. The goodness measure proposed in [20]
has such a property; however, it is shown to be dominated
by the relevance. We propose a new measure called expanded
relevance (exprel(cid:2)) which computes the coverage of the rel-
evant part of the graph. We show that the query-oblivious
algorithms cannot optimize exprel(cid:2).

We also investigate various quality indices by computing
their pairwise correlations. This highlights that the good-
ness measure is highly correlated with the sum of ranking
scores. That is the algorithms that perform well on good-
ness produce results sets which are not much diﬀerent from
top-k relevant set. The exprel(cid:2) measure we propose appears
to have no high correlation with other measures.

To optimize exprel(cid:2) of the result set, we propose a greedy
algorithm BestCoverage. Because of the submodular prop-
erties of exprel(cid:2), BestCoverage is a (1− 1/e)-approximation
algorithm with complexity O(knΔ
), where k is the number
of recommended items, n is the number of vertices in the
graph, and Δ is the maximum degree. We propose a relax-
ation of BestCoverage with complexity O(k¯δ
), where
¯δ is the average degree of the graph. We experimentally
show that the relaxation carries no signiﬁcant harm to the
expanded relevance of the results.

Δ

(cid:2)

(cid:2)

(cid:2)

2. BACKGROUND
2.1 Problem Deﬁnition

We target the problem of diverse recommendation on
graphs assuming that the user has a history or speciﬁed
interests in some of the items. Therefore, the objective is to
return a set of items which extend those interests.

Let G = (V, E) be an undirected graph where V =

{v1, . . . , vn} is the vertex set and E is the edge set. Given a
set of m seed nodes Q = {q1, . . . , qm} s.t. Q ⊆ V , and a pa-
rameter k, return top-k items which are relevant to the ones
in Q. With diversity in mind, we want to recommend items
not only relevant to Q, but also covering diﬀerent aspects of
the query set.
2.2 PageRank and personalized PageRank

We deﬁne a random walk on G arising from following the
edges (links) with equal probability and a random restart
at an arbitrary vertex with (1 − d) teleportation probabil-
ity. The probability distribution over the states follows the
discrete time evolution equation:

pt+1 = P pt,

(1)

where pt is the vector of probabilities of being on a certain
state at iteration t, and P is the transition matrix deﬁned as:

(cid:2)

(1 − d) 1
n + d 1
(1 − d) 1
n ,

δ(v) ,

if (u, v) ∈ E
otherwise,

P(u, v) =

(2)
where δ(v) is the degree of the vertex v ∈ V . If the network
is ergodic (i.e., irreducible and non-periodic), (1) converges

(cid:2)

to a stationary distribution π = Pπ after a number of
iterations. And the ﬁnal distribution π gives the PageRank
scores [2] of the nodes based on centrality.
In our problem, a set of nodes Q was given as a query, and
we want the random walks to teleport to only those given
∗
nodes. Let us deﬁne a prior distribution p
if v ∈ Q
otherwise.
∗

1/m,
0,

If we substitute the (1/n)s in (2) with p

, we get a vari-
ant of PageRank, which is known as personalized PageRank
(PPR) ortopic-sensitive PageRank
[10]. PPR scores can be
used as the relevance scores of the items in the graph. Note
that the rank of each seed node is reset after the system
reaches to a steady state, i.e., ∀q ∈ Q, πq ← 0, since the
objective is to extend Q with the results.

such that

(v) =

(3)

∗

p

PPR is preferred as the scoring function in our discussions
because (i) some of the methods in the experiments are vari-
ants of PPR which compute relevant but diverse set of results,
(ii) some measures and objective functions are deﬁned on
the stationary distribution of PPR, and (iii) alternative scor-
ing functions and probability distributions on graph produce
similar results to PPR. On the other hand, the discussions on
evaluations and some diversiﬁcation techniques are indepen-
dent of the preferred scoring function, hence we believe that
the discussions will still interest the majority of the readers.
2.3 Result Diversiﬁcation on Graphs

We classify the diversiﬁcation methods for the recom-
mendation problem based on whether the algorithm needs
to rank the items only once or multiple times.

Diversiﬁcation by query reﬁnement. This set of algo-
rithms rank the items k times to select the results one by
one, and reﬁne the search at each step.

GrassHopper [24] is a well-known diversiﬁcation algorithm
which ranks the graph k times by turning the highest-ranked
vertex into a sink node at each iteration. Since the proba-
bilities will be collected by the sink nodes when the random
walk converges, the algorithm estimates the ranks with the
number of visits to each node before convergence. GrassHop-
per uses matrix inversion to ﬁnd the expected number of
visits; however, inverting a sparse matrix makes it dense,
which is not practical for the large and sparse graphs we are
interested in. Therefore, we estimate the number of visits
by iteratively computing the cumulative ranks of the nodes
with PPR.

GSparse [13] employs an incremental ranking approach
similar to GrassHopper, but the algorithm disconnects the
selected node from the graph instead of converting it into a
sink node. After executing the ranking function, the graph is
sparsiﬁed for the next iteration by removing all the edges of
the highest ranked node. This way, the graph becomes less
dense around the selected nodes, hence, the remaining nodes
at these regions will attract less visits during the random
walk. The process is repeated until k nodes are selected.

Recently, manifold ranking has become an alternative to
personalized PageRank and several diversiﬁcation methods
were proposed based on the idea of turning highly ranked
nodes into sinks [5, 8, 25]. Aside from the ranking strategy,
manifold ranking with sink points is quite similar to
GrassHopper when the probabilities are estimated with
cumulative scores. Since the manifold ranking is a diﬀerent

716(cid:3)

ranking of the graph, we carry out our experiments based
only on PPR by leaving the discussion of using manifold
ranking instead of PPR open for the time being.

Diversiﬁcation by vertex selection. The following algo-
rithms run the ranking function once, then carefully select
a number of vertices to ﬁnd a diverse result set.

DivRank [16] adjusts the transition matrix based on
the number of visits to the vertices so far using a
variant of random walks, called vertex-reinforced random
walks (VRRW) [18]. It assumes that there is always an or-
ganic link for all the nodes returning back to the node itself
which is followed with probability (1 − α):
if u (cid:6)= v
otherwise,

w(u,v)
α
δ(u) ,
1 − α,

p0(u, v) =

(cid:2)

(4)

where w(u, v) is equal to 1 for (u, v) ∈ E
The transition matrix Pt at iteration t is computed with

, and 0 otherwise.

(cid:3)

Pt(u, v) = (1 − d) p

∗

(v) +d

p0(u, v) ηt(v)
z∈V p0(u, z) ηt(z)

,

(5)

∗

(v) is given in (3), andη t(v) is the number of vis-
where p
its of vertex v up to iteration t. It ensures that the highly
ranked nodes collect more value over the iterations, resulting
in the so called rich-gets-richer mechanism. In each itera-
tion of VRRW, the transition probabilities from a vertex u
to its neighbors are adjusted by the number of times they
are visited up to that iteration t. Therefore, u gives a high
portion of its rank to the frequently visited neighbors. Since
the tracking of ηt(.) is nontrivial, the authors propose to
estimate it with cumulative ranks (CDivRank), i.e., the sum
of the scores upto iteration t, or, since the ranks will con-
verge after suﬃcient number of iterations, with pointwise
ranks (PDivRank), i.e., the last score at iteration t − 1.

A recently proposed algorithm, Dragon [20], employs a
greedy heuristic to ﬁnd a near-optimal result set that opti-
mizes the goodness measure, which punishes the score when
two neighbors are included in the results (see (15)). We will
investigate this measure more in the upcoming section.

Frequently visited nodes tend to increase the ranks of
their neighbors because of the smoothing process of ran-
dom walks [16]. Based on this observation, algorithms using
local maxima have been proposed. The Relaxed Local Max-
ima algorithm (k-RLM) [13] incrementally includes each local
maxima within top-k2 results to S until |S| = k by removing
it from the subgraph for the next iteration.

3. MEASURES AND EVALUATION
3.1 Classical relevance and diversity measures
Let us ﬁrst review some classical measures for computing
the relevance and diversity of the results with respect to the
query. The measures are important since either they are
typically used as –or a part of– the objective function of the
diversiﬁcation method, or the results are evaluated based on
those measures.
Normalized relevance: The relevancy score of a set can
be computed by comparing the original ranking scores of the
resulting set with the top-k ranking list [20], deﬁned as

(cid:3)
(cid:3)k

v∈S πv
i=1 ˆπi

rel(S) =

,

(6)

1 ˆπ does not denote estimated or predicted relevance scores.

(cid:3)k

(cid:3)

i=1 ˆπi is preferred over

where ˆπ is the sorted ranks in non-increasing order.1 Nor-
v∈S πv since
malization with
the distribution of scores in a random walk depends on the
graph size, query, connectivity, etc., and normalized scores
are comparable among diﬀerent settings.
Diﬀerence ratio: A diversiﬁed result set is expected to be
somewhat diﬀerent than the top-k relevant set. Because the
highly ranked nodes increase the ranks of their neighbors
[16], the top-k results, recommended by the original PPR,
is not diverse enough as shown in [19] and in our experi-
ments. Nevertheless, the original result set has the utmost
relevancy. This fact can mislead the evaluation of the ex-
perimental results. Therefore, we decided to measure the
diﬀerence of each result set from the set of original top-k
nodes. Given ˆS to be the top-k relevant set, the diﬀerence
ratio is computed with

diﬀ(S, ˆS) = 1 − |S ∩ ˆS|
|S|

.

(7)

nDCG: We use normalized discounted cumulative gain
(nDCG), for measuring the relevancy as well as the ordering
of the results. It is deﬁned as

nDCGk =

πs1 +

ˆπ1 +

πsi
log2 i
ˆπi

log2 i

,

(8)

i=2

i=2

(cid:3)k
(cid:3)k

where π is the relevancy vector (e.g., stationary distribution
of a random walk), ˆπ is the sorted π in non-increasing order,
and si ∈ S is the ith point in result setS .
(cid:6)-step graph density: A variant of graph density measure
is the (cid:6)-step graph density [20], which takes the eﬀect of
in-direct neighbors into account. It is computed with

(cid:3)

dens(cid:2)(S) =

u,v∈S,u(cid:5)=v d(cid:2)(u, v)
|S| × (|S| − 1)

,

(9)

where d(cid:2)(u, v) = 1 when v is reachable from u within (cid:6) steps,
i.e., d(u, v) ≤ (cid:6), and 0 otherwise. The inverse of dens(cid:2)(S) is
used for the evaluation of diversity in [16].
(cid:6)-expansion ratio: As an alternative to density, expan-
sion ratio and its variant (cid:6)-expansion ratio [14] measure the
coverage of the graph by the solution set, computed with:

σ(cid:2)(S) =

|N(cid:2)(S)|

n

,

(10)

where the expansion set with 1-distance neighbors is deﬁned
as N (S) =S ∪ {v ∈ (V − S) :∃ u ∈ S, (u, v) ∈ E}, and the
(cid:6)-step expansion set is deﬁned in [14] as:

N(cid:2)(S) = S ∪ {v ∈ (V − S) : ∃u ∈ S, d(u, v) ≤ (cid:6)}.

(11)

Note that the intent-aware measures, such as intent-
aware expected reciprocal rank (ERR-IA) [4], α-normalized
discounted cumulative gain (α-nDCG@k) [7], intent-aware
mean average precision (MAP-IA) [1], are not included to
the discussions, but they are important measures for eval-
uating the diversity of the results when data and queries
have some already known categorical labels. Our problem
has no assumptions of a known distribution that speciﬁes
the probability of an item belonging to a category.

717 1
 1

 0.8
 0.8

 0.6
 0.6

 0.4
 0.4

 0.2
 0.2

2
2
s
s
n
n
e
e
d
d

 0
 0

 0
 0

top-90%+random
top-75%+random
top-50%+random
top-25%+random
All random

better

 0.2
 0.2

 0.4
 0.4

rel
rel

 0.6
 0.6

 0.8
 0.8

 1
 1

(a) rel vs. dens2

 0.3
 0.3

 0.25
 0.25

 0.2
 0.2

2
2

σ
σ

 0.15
 0.15

 0.1
 0.1

 0.05
 0.05

 0
 0

 0
 0

top-90%+greedy-σ2
top-75%+greedy-σ2
top-50%+greedy-σ2
top-25%+greedy-σ2
All greedy-σ2

better

 1

 1

 0.8

 0.8

top-%+random
top-%+greedy-σ2
other algorithms

l
l

2
2
e
e
r
r
p
p
x
x
e
e

 0.6

 0.6

 0.4

 0.4

 0.2
 0.2

 0.4
 0.4

rel
rel

 0.6
 0.6

 0.8
 0.8

 1
 1

(b) rel vs. σ2

 0.2

 0.2

 0

 0

 5 10  20
 5 10  20

 50
 50
k
k

(c) exprel2

 100
 100

Figure 1: Evaluation of top-%+random (red) and top-%+greedy-σ2 (blue) methods versus other algorithms
(gray) based on selected relevance/diversity measure pairs and combined exprel2 measure. The other algo-
rithms include GrassHopper, DivRank, Dragon, k-RLM, GSparse, and BestCoverage, but they are not highlighted here
since we do not want to prematurely compare those against each other.

3.2 Bicriteria optimization measures

Maximum Marginal Relevance (MMR) [3] is the most pop-
ular diversiﬁcation method that optimizes a bicriteria ob-
jective, marginal relevance, which is a linear combination of
independently measured relevance and novelty. The method
greedily and implicitly optimizes the following objective as-
suming that the similarity of all items to the query items
are already computed in π:

(cid:4)

fMM R(S) = (1 − λ)

πv − λ

(cid:4)

v∈S

u∈S

max
v∈S
u(cid:5)=v

sim(u, v),

(12)

where λ is the importance of relevance over novelty and sim
is a similarity metric. The problem with (12) is that two
diﬀerent measures are aggregated without taking their com-
patibility into account.

The same premise is also valid for any type of linear aggre-
gation of a relevance and a diversity measure. For example,
[14] tries to optimize the following diversiﬁed ranking mea-
sure:

(cid:4)

|N (S)|

fL(S) =

πv + λ

v∈S

n

,

(13)

where λ is the tradeoﬀ between relevance and diversity, and
the diversity of the result set is measured with the expansion
ratio. Similarly in [15], relevance part is scaled with (1− λ).
Other bicriteria objectives include max-sum diversiﬁca-
tion, which reduces to MaxSumDispersion problem, max-
min diversiﬁcation, which reduces to MaxMinDispersion
problem, etc. For example, k-similar diversiﬁcation set prob-
lem [21] is deﬁned based on MaxSumDispersion as:
fM SD(S) = (k−1)(1−λ)

div(u, v), (14)

(cid:4)

(cid:4)

(cid:4)

πv +2λ

v∈S

u∈S

v∈S
u(cid:5)=v

where div(u, v) can be selected as a weighted similarity,
tf/idf cosine similarity, or the Euclidean distance depend-
ing on the problem. We refer the reader to [9] for more
information on objectives and distance functions.
3.3 Bicriteria optimization is not the answer
We argue that bicriteria optimization is inappropriate,
and hence, the diversiﬁcation methods that seem to opti-

mize both criteria are problematic. Let us return back to
our original problem: the items in a graph structure are
ranked based on a given query and a ranking method (e.g.,
PPR), and our aim is to rerank those items so that we can
include more results from diﬀerent aspects of the query and
reduce redundancy of top-k relevant set.

Suppose that we work on the web graph and we want to
diversify the results of a search engine which displays k = 10
results to the user. Do you think the quality of the top-k
list would improve if we replace some results from the end
of the list with random web pages?

We design two query-oblivious algorithms for the two pop-
ular combinations of objectives, which are monotonous (e.g.,
linear or quadratic) aggregations of max-sum relevance and
max-sum diversity (graph density dens or expansion ratio σ)
objectives. The algorithms will return some of the results
without considering the user’s interests, yet, will perform
the best on the following commonly used measures:

• top-%+random: returns a given percentage of the
results (e.g., 50%, 75%, etc.) from top-k, and the rest
randomly from the search space.

• top-%+greedy-σ2: returns a given percentage of the

results (e.g., 50%, 75%, etc.) from top-k, and try to
maximize σ2 with the rest of the results without taking
the query into account.

To prove our point, we compute the normalized relevance
(rel) and selected diversity measure (dens2 and σ2) of the
results for the diversiﬁcation methods in the literature and
for the query-oblivious algorithms.2 We ﬁt a multi-variate
Gaussian on top of the results to show the mean and mo-
ments of the distribution when two objectives are considered
simultaneously. A result which further minimizes dens2 and
maximizes rel and σ2 is favorable and better. This is shown
with an arrow in the Figs. 1(a) and 1(b).

2Figure 1 gives only the results for amazon0601 dataset using sce-
nario 3 queries and k = 20. Comparisons of query-oblivious meth-
ods on given bicriteria measures and exprel2 for other datasets
and query types are provided in the supplementary material:
http://bmi.osu.edu/hpc/data/Kucuktunc13WWW/randoms.pdf

718Figure 1(a) shows the results of top-%+random as well
as other algorithms with respect to rel vs. dens2 evaluation.
Figure 1(b) similarly shows the results of top-%+greedy-
σ2 as well as other algorithms with respect to rel vs. σ2 eval-
uation. Here, query-oblivious methods seem to recommend
the best result sets when a bicriteria evaluation is used. Yet,
we know that those algorithms are designed to trick the eval-
uation, as well as produce useless results in user’s point of
view.

Using only the ﬁrst half of top-k results gives a normalized
relevance score greater than or equal to 0.5 since the ranks
are sorted in non-increasing order. Furthermore, the ranks
has a power-law distribution that makes rel much higher
than 0.5. Therefore, the relevance objective is mostly satis-
ﬁed.

We further argue that no matter which relevance and di-
versity measures are selected, there always exists a query-
oblivious algorithm which optimizes both measures in a
meaningless way, useless in practice but looks great on the
paper. This is the problem of evaluating result diversiﬁca-
tion as a bicriteria optimization problem with a relevance
measure that ignores diversity, and a diversity mea-
sure that ignores relevancy.
3.4 Combined measures

As a result of our experiments on bicriteria optimization,
we argue that we need a combined measure that tightly inte-
grates both relevance and diversity aspects of the result set.
It is reasonable to design the combined measure based on
the query, the rankings, and the graph structure we already
have.

The goodness measure [20] is a step towards a meaningful
combined measure. It penalizes the score when two results
share an edge, meaning that they are neighbors and they
possibly increase their ranks by feeding each other during
the random walk. The measure is computed with

of the search space. Therefore, the items having separate
expansion sets will increase the coverage. However, coverage
is not the only aspect of exprel(cid:2). The proposed measure also
takes the ranking scores into account, and hence the quality
of the covered part.

The eﬀect of each result is limited with the given (cid:6) pa-
rameter, i.e., a result covers only its neighbors in the graph
if (cid:6) = 1, or neighbors of neighbors if (cid:6) = 2. Higher values of
(cid:6) are generally not preferred since the expansion set tends
to cover most of the graph in those cases.

An important property of exprel(cid:2) measure is that query-
oblivious algorithms cannot optimize it. Because, the high-
est ranked items are mostly not diverse enough, and the
rest of the results (randomly selected independent of the
query) will not contribute much to the measure. Figure 1(c)
shows that neither top-%+random (red) nor top-%+greedy-
σ2 (blue) can optimize the measure while the diversiﬁcation
algorithms (gray) can score higher. This proves the validity
of the measure for diversiﬁcation.

Table 1: Correlations of the diﬀerent relevance, di-
versity, and combined measures. Pearson correla-
tion scores are given on the lower triangle of the
matrix. High correlations are highlighted.

nDCG

diﬀ

dens1

dens2

σ1

σ2

goodness

exprel
1

exprel
2

rel

–

l
e
r

G 0.87

C
D
n

–

ﬀ -0.95 -0.80

i
d

–

1

s
n
e
d

2

s
n
e
d

0.74

0.72

-0.76

–

0.76

0.67

-0.76

0.78

–

(cid:4)

fG(S) = 2

i∈S

(cid:4)

πi − d

i,j∈S
− (1−d)

A(j, i)πj

(cid:4)

(cid:4)

πj

j∈S

i∈S

∗

p

(i),

(15)

1
σ

-0.25

-0.19

0.29

-0.30

-0.01

–

2
σ

-0.25

-0.19

0.29

-0.31

-0.03

0.99

–

where A is the row-normalized adjacency matrix of the
graph. However, we will show in Section 3.5 that goodness
is highly dominated by relevance, which reﬂects negatively
on the results of Dragon in the experiments.

We present a combined measure of the (cid:6)-step expansion
ratio (σ2) and relevancy scores (rel), which are two popular
diversity and relevance measures in the literature, in order
to quantify the relevant-part coverage of the graph:

s
s
e
n
d
o
o
g

1

l
e
r
p
x
e

2

l
e
r
p
x
e

0.96

0.86

-0.90

0.70

0.67

-0.21

-0.21

–

0.34

0.59

-0.28

0.37

0.44

-0.01

-0.03

0.32

–

0.20

0.37

-0.13

0.17

0.33

0.26

0.23

0.21

0.86

–

(cid:6)-step expanded relevance:

(cid:4)

exprel(cid:2)(S) =

πv

v∈N(cid:2)(S)

(16)

where N(cid:2)(S) is the (cid:6)-step expansion set of the result set
S, and π is the PPR scores of the items in the graph.

This new measure explicitly evaluates the diversity of the
results in terms of coverage with the given set.
In other
words, when two results are close to each other in the graph,
their expansion sets intersect narrowing the covered part

3.5 Correlations of the measures

We investigate the mentioned relevance, diversity, and
combined measures by computing their pairwise correlations
based on the results of the algorithms given in Section 2.3 as
well as the query-oblivious top-%+random methods given in
the previous section. Table 1 shows the correlations of 10
measures as scatter plots as well as their correlation scores.3

3Table 1 shows only the measure correlations on amazon0601
dataset and with k = 20. The results are consistent across various
datasets, scenarios, and k values. A complete comparison set is
provided in the supplementary material:
http://bmi.osu.edu/hpc/data/Kucuktunc13WWW/corr.pdf

719For the relevance measures, rel is highly correlated with
nDCG although the latter considers the order of the results.
rel is also anti-correlated with diﬀ , meaning that as the ratio
of results other than top-k start to increase, the normalized
relevance decreases accordingly.

For the graph diversity measures, (cid:6)-step expansion ratios
(σ1 and σ2) are highly correlated among each other. On
the other hand, graph density-based measures (dens1 and
dens2) do not seem to have any high correlation with other
measures.

Among the combined measures, goodness is highly corre-
lated with rel. This highlights that the goodness measure is
dominated by the sum of ranking scores, meaning that algo-
rithms that perform better on goodness do not return results
that are much diﬀerent from the top-k results of PPR.

The proposed exprel(cid:2) measure, on the other hand, appears
to have no high correlation with any of the other relevance
or diversity measures, proving that it is something diﬀerent
than the already known measures. Although the expanded
relevance is based on both rel and expansion ratio (σ), very
low correlation is observed in the results.

4. BEST COVERAGE METHOD

Our strategy so far was to review the attempts to ﬁnd a
good objective function for the result diversiﬁcation problem
on graphs. We have shown that a bicriteria optimization of
relevance and diversity can be tricked, and a combined mea-
sure should be constructed carefully. The proposed exprel(cid:2)
measure seems to cover both aspects of the intended objec-
tive, yet cannot be optimized by the query-oblivious algo-
rithms. We argue that this novel measure can be naturally
used as an objective function of a diversiﬁcation algorithm.
4.1 Problem formulation and complexity

Given a graph G = (V, E), a vector of ranking scores π
(stationary distribution of PPR scores in our case) computed
based on the query setQ, and the number of required re-
sults k, our objective is to maximize the expanded relevance
(exprel(cid:2)) of the result set S:
(cid:3)

(cid:4)

) = argmax
S(cid:2)⊆V
|S(cid:2)|=k

v∈N(cid:2)(S(cid:2)

)

πv,

(17)

exprel(cid:2)(S

S = argmax
S(cid:2)⊆V
|S(cid:2)|=k
(cid:3)

where N(cid:2)(S
problem as exprel(cid:2)-diversiﬁed top-k ranking (DTR(cid:6)).

) is the (cid:6)-step expansion set. We refer to this

However, it is not hard to see that the objective of ﬁnd-
ing a subset of k elements that maximizes the expanded
relevance is NP-hard. Assuming the graph G and the rank-
ing scores π are arbitrary, DTR(cid:6) is a generalization of the
weighted maximum coverage problem (WMCP) which is NP-
Complete [11]. WMCP is expressed as a set O of ob-
jects oi with a value ωi and z sets of objects rj ⊆ O,
R = {r1, r2, . . . , rz}. The problem is to select a subset of R,
P ⊆ R such that |P| = x which maximizes
oi∈{rj :rj∈P} ωi.
The key of the reduction for (cid:6) = 1 is to construct an instance
of DTR(cid:6) with a bipartite graph G = (V = R ∪ O, E) where
(rj, oi) ∈ E iﬀ oi ∈ rj. We set πrj = 0, πoi = ωi and k = x.
The solutions of DTR(cid:6) are dominated by sets S where all
the vertices are in R. Indeed, since πrj = 0,∀rj there is no
advantage in selecting a vertex in O. The rest of the reduc-
tion is obvious for (cid:6) = 1. For other values of (cid:6), the reduction
is similar, except each edge of the bipartite graph is replaced
in a path of (cid:6) edges.

(cid:3)

Note that the proposed objective in (17) is independent of
ordering since the function is deﬁned over an unordered set.
This is usually reasonable because there is an assumption
that users will consider all k results [1, 14, 20]. In practice,
diﬀerent users may stop at diﬀerent number of results, hence,
several DCG-based metrics are commonly used to compute
the importance of returning results in an ideal ordering. The
near-optimal solutions that we will present in the following
section can still output an ordered set of results based on
the marginal utility of each selected item at the moment of
its inclusion.
4.2 Greedy solution: BestCoverage

Although the optimal solution of the proposed objective
function (see (17)) is NP-hard, we will show that a greedy so-
lution that selects the item with the highest marginal utility
at each step is the best possible polynomial time approxi-
mation for the problem.
Let us deﬁne the marginal utility for a given vertexv and
result set S as g(v, S), such that g(v,∅) = exprel(cid:2)({v}) be-
v(cid:2)∈V (cid:2) πv(cid:2) where
fore any results are selected, and g(v, S) =
= N(cid:2)({v})− N(cid:2)(S) represents the (cid:6)-step expansion set of
(cid:3)
V
vertex v without the items that have already been covered
by another result. In other words, g(v, S) is the increase on
the exprel(cid:2) measure if v is included to the result set, i.e.,
exprel(cid:2)(S ∪ {v}) =exprel (cid:2)(S) +g (v, S).

(cid:3)

ALGORITHM 1: BestCoverage
Input: k, G, π, (cid:3)
Output: a list of recommendations S
S = ∅
while |S| < k do
∗}

∗ ← argmaxv g(v, S)
v
S ← S ∪ {v

return S

Algorithm 1 incrementally selects the item with the high-
est marginal utility in each step, then includes it to the result
set S. This way, the items that contribute the most to the
expanded relevance of the ﬁnal results are greedily selected
as a solution to the given optimization problem. In order
to show that the greedy algorithm solves the problem quite
well, we ﬁrst prove that the exprel(cid:2) is a submodular function:
Definition 4.1. (Submodularity) Given a ﬁnite set
V → R is submodular if and only if
V , a set function f : 2
for all subsets S and T such that S ⊆ T ⊆ V , and j ∈ V \ T ,
f (S ∪ {j}) − f (S) ≥ f (T ∪ {j}) − f (T ).

Lemma 4.2. exprel(cid:2) is a submodular function.

The proof of the lemma follows directly from the deﬁni-
tions of submodularity and exprel(cid:2). Greedy algorithms are
known to generate good solutions when maximizing submod-
ular functions with a cardinality constraint and were used
in [1, 14].

∗
(cid:3)

Theorem 4.3. [17] For a submodular set function f , let
be the optimal set of k elements that maximizes f (S), and
S
be the k-element set constructed greedily by selecting an
S
element one at a time that gives the largest marginal increase
to f . Then f (S
Corollary 4.4. BestCoverage is an (1 − 1/e)-approx-
imation algorithm for the exprel(cid:2)-diversiﬁed top-k ranking
problem.

) ≥ (1 − 1/e)f (S

(cid:3)

∗

).

720(cid:2)

∗

4.3 Analysis and relaxation of the algorithm
BestCoverage (BC) is a (1 − 1/e)-approximation for max-
imizing exprel(cid:2) with complexity O(knΔ
) where n is the
number of vertices in the graph, k is the number of recom-
mended objects, and Δ is the maximum degree of the graph.
Obviously, the implementation in Algorithm 1 can be im-
proved by storing the marginal utility for every vertex at the
expense of O(n) space, and updating only the vertices that
to S would aﬀect. However, for (cid:6) = 2,
the inclusion of v
the number of vertices to be updated is |N4({v
∗})|, which
is O(Δ4) in the worst case. Initializing the marginal utility
incurs a cost of O(nΔ
). Once a vertex is added to setS ,
the impact of its distance (cid:6) neighbors must be adjusted. For
a given vertex, adjusting its impact costs O(Δ
). For each
neigh-
iteration of the algorithm the impact of at most Δ
bors need to be adjusted. Though, each vertex adjusts its
(cid:2)}) adjustments.
impact only once, so there are O(min{n, kΔ
Finally, selecting the vertex with maximal marginal utility
requires O(n) operations4 per iteration. The overall com-
plexity of the algorithm is O(nΔ
+ kn).

+ min{n, kΔ

(cid:2)}Δ

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:3)

(cid:3)

(cid:3)

vertices where k

ALGORITHM 2: BestCoverage (relaxed)
Input: k, G, π, (cid:3)
Output: a list of recommendations S
S = ∅
Sort(V ) w.r.t πi non-increasing
S1 ← V [1..k
], i.e., top-k
∀v ∈ S1, g(v) ← g(v, ∅)
∀v ∈ S1, c(v) ← Uncovered
while |S| < k do
∗ ← argmaxv∈S1 g(v)
v
∗}
S ← S ∪ {v
S2 ← N(cid:2)({v
∗})
(cid:3) ∈ S2 do
for each v
(cid:3)
) =Uncovered then
S3 ← N(cid:2)({v
(cid:3)})
∀u ∈ S3, g(u) ← g(u) − πv(cid:2)
c(v

) ← Covered

if c(v

(cid:3)

= k¯δ(cid:2)

return S

(cid:3)

(cid:3)

With this optimization, most of the time is spent on ini-
tializing the marginal utility. We experimentally found that
results of PPR
the returned results are chosen from top-k
is proportional to k and the average de-
ranks, where k
gree of the graph. We propose a relaxation of BestCov-
erage which only considers including in the result set the
(cid:2)
top-k¯δ
highest ranked vertices solely based on the rele-
vance scores where ¯δ is the average degree of the graph.
All the vertices of the graph still contributes to marginal
utility. The complexity of the relaxed version drops to
(cid:2)}) since the cost of the com-
O(min{n, kΔ
putation of the initial marginal utility is now asymptotically
dominated by the cost of adjusting them. Algorithm 2 gives
the relaxed BestCoverage algorithm with all mentioned im-
provements. The impact of the relaxation on the quality of
the solution will be discussed in Section 5.3.

+k min{n, k¯δ

(cid:2)}Δ

(cid:2)

4It might appear that using a ﬁbonnacci heap should allow to
reach a better complexity, but we require the extract-max and
decrease-key operations which are incompatible.

5. EXPERIMENTS
5.1 Datasets

In the experiments we use one graph instance for each
targeted application area, i.e., product recommendation on
shopping websites, collaborator and patent recommendation
in academia, friend recommendation on social networks, and
personalized web search. The graphs are publicly available
at Stanford Large Network Dataset Collection5.
In sum-
mary, amazon0601 is the Amazon product co-purchasing
network collected on June 2003. ca-AstroPh is the collab-
oration network between authors of the papers submitted
to arXiv astrophysics category. cit-Patents is the citation
network between U.S. patents granted between 1975 and
1999. soc-LiveJournal1 is the graph of LiveJournal so-
cial network, and web-Google is the web graph released in
2002 by Google.

The mentioned graphs are re-labeled, converted into undi-
rected graphs. The properties of the graphs are given in Ta-
ble 2. Note that ¯δ is the average degree of the graph, D is
the diameter of the graph, i.e., maximum undirected short-
est path length, D90% is the 90-percentile eﬀective diameter,
and CC is the average clustering coeﬃcient.

|V |
403.3K

Table 2: Properties of graphs used in experiments.
¯δ D D90% CC
7.6 0.42
5.1 0.63
9.4 0.09
6.5 0.31
8.1 0.60

|E|
3.3M 16.8 21
18.7K 396.1K 42.2 14
3.7M 16.5M 8.7 22
4.8M 68.9M 28.4 18
5.1M 11.6 22

Dataset
amazon0601
ca-AstroPh
cit-Patents
soc-LiveJournal1
web-Google

875.7K

5.2 Scenarios and query generation

We generate the queries for the experiments based on

three diﬀerent real-world scenarios:
Scenario 1: A random vertex in the graph is selected as
the query. This scenario represents the case where the sys-
tem does not have any information on the user. For product
recommendation, the user can be visiting a product page
without signing in to the system. For academic recommen-
dation tasks, a professor can be looking for collaborators.
Scenario 2: A random vertex v along with 10–100 vertices
within two distance to v are selected as a query.
In this
scenario, v and the selected vertices represent an area of in-
terest. For example, the user can be searching for a product
within a category, or interested in an academic ﬁeld. In a
social network, the friend list of a person can be used as the
query for friend suggestion.
Scenario 3: 2 to 10 random vertices are selected as diﬀerent
interests of the user, and a total of 10 to 100 vertices around
those interests are added to the query set. Multiple areas of
interest is the most common use case for these applications
where users are registered to the system and already have a
search or purchase history.

For each dataset, 750 queries were generated, where the
average number of the seed nodes varies between 1 and 50
for the scenarios 1 and 3, respectively. In total 3,750 query
sets representing diﬀerent real-world cases were used in the
experiments.

5Available at: http://snap.stanford.edu/data/index.html

721l

e
r

f
f
i

d

 1

 0.8

 0.6

 0.4

 0.2

 0

 1

 0.8

 0.6

 0.4

 0.2

 0

amazon0601, combined

 1

0.8

0.6

0.4

0.2

 0

amazon0601, combined

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM

GSparse
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 1

0.8

0.6

0.4

0.2

 0

soc-LiveJournal1, combined

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank

k-RLM
GSparse
BC1
BC1 (relaxed)

 0.45

 0.4

 0.35

 0.3

 0.25

2

σ

 0.2

 0.15

 0.1

 0.05

 0

amazon0601, combined

BC1 (relaxed)
BC2 (relaxed)

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM
GSparse
BC1
BC2

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 0

ca-AstroPh, combined

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank

k-RLM
GSparse
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 5  10  20

 50
k

 100

 5  10  20

 50
k

 100

 5  10  20

 50
k

 100

 5  10  20

 50
k

 100

soc-LiveJournal1, combined

Figure 3: Coverage (σ2) of the algorithms with vary-
ing k. BestCoverage and DivRank variants have the
highest coverage on the graphs while Dragon, GSparse,
and k-RLM have similar coverages to top-k results.

 5  10  20

 50
k

 100

 5  10  20

 50
k

 100

Figure 2: Normalized relevance (rel) and diﬀerent
ratio (diﬀ ) scores with varyingk . Dragon and GSparse
return results around 70% similar to the top-k rele-
vant set, this is generally not enough to improve the
diversity of the results.

5.3 Results

We experiment with the algorithms given in Section 2.3,
the datasets described in Section 5.1, and the queries deﬁned
in Section 5.2. For the methods that use the ranking scores
of PPR, we ﬁx d = 0.9 and the number of PPR iterations
to 20 in order to be consistent between diﬀerent queries.
For the VRRW computation of DivRank methods, we set
α = 0.25 and the number of iterations to 50 since VRRW
usually takes more iterations to converge. All ranking func-
tions are implemented eﬃciently with sparse matrix-dense
vector multiplication (SpMxV) operations.

On amazon0601, ca-AstroPh, and soc-LiveJournal1
datasets, we observed that the results of diﬀerent scenarios
are similar. Hence, we combine the scenarios and display
the results on all queries6. Also note that the results of
BC2 and its relaxation are omitted from the plots of soc-
LiveJournal1 dataset because of the impractical runtimes.
Normalized relevance (rel) and diﬀerence ratio (diﬀ ) plots
in Figure 2 show that Dragon and GSparse methods almost
always return the results having 70% similar items to top-k
relevant set, and more than 80% rel score. A low rel score
is not an indication of being dissimilar to the query (unless
rel → 0); on the other hand, since the scores have a power-
law distribution, a high rel score usually implies that the
algorithm ignored the diversity of the results and did not
change many results in order to keep the relevancy high.
The actual diﬀ measures are also given in Figure 2.

6Due to space limitation we only display one plot per observation
highlighted in the text. The complete set of plots for each dataset,
scenario, and measure is provided in the supplementary material:
http://bmi.osu.edu/hpc/data/Kucuktunc13WWW/results.pdf

amazon0601, combined

 0.8

 0.75

 0.7

 0.65

l

2
e
r
p
x
e

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank

k-RLM
GSparse
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 0.8

0.75

 0.7

0.65

 0.6

0.55

 0.5

0.45

 0.4

0.35

 0.3

soc-LiveJournal1, combined

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM
GSparse
BC1
BC1 (relaxed)

 100

 100

 50
k

 5  10  20

 5  10  20

 50
k
Figure 4: Expanded relevance (exprel
2) with vary-
ing k. BC1 and BC2 variants mostly score the best,
GrassHopper performs high in soc-LiveJournal1. Al-
though PDivRank gives the highest coverage on ama-
zon0601 (Fig. 3), it fails to cover the relevant parts.

Based on the expansion ratios (σ2) in Figure 3, BestCov-
erage and DivRank variants, especially PDivRank and BC2,
have the highest scores, hence the highest coverage on the
graphs with their diversiﬁed result set. Dragon, GSparse,
as well as k-RLM have expansion ratios similar to the top-k
results, meaning that these algorithms do not improve the
coverage of the given graphs enough. GSparse reduces the
expansion ratio even more than the top-k set, proving that it
is inappropriate for the diversiﬁcation task. It is important
to note that σ2 scores are meaningless by itself since query-
oblivious greedy-σ2 algorithm would maximize the coverage.
Figure 4 shows the proposed expanded relevance scores
(exprel
2) of the result sets. BC1 and BC2 variants are signif-
icantly better than the other algorithms, where GrassHop-
per is able to score closer to BestCoverage only in soc-
LiveJournal1 dataset. Although DivRank variants per-
form the highest based on expansion ratio (see Figure 3),
their results are shown to be unable to cover the relevant
parts of the graph as they score lower than BestCoverage
variants.

For cit-Patents and web-Google datasets, we report
the results on queries of scenarios 1 and 3 separately. Here
we omit the results of scenario-2 queries since they are in
between scenarios 1 and 3. These plots share the conclusions
we have made so far based on the results on previous three
datasets; however, they present diﬀerent behavior based on
the chosen scenario, so we provide a deeper analysis on those.

722cit-Patents, scenario 1

 0.7

 0.6

 0.5

ca-AstroPh, combined

 10000

soc-LiveJournal1, combined

GSparse
BC1
BC1 (relaxed)

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM

 1000

 100

 10

p
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 100

 5  10  20

 50
k

 100

web-Google, scenario 1

PPR (top-k)
GrassHopper
Dragon

 5  10  20

g

PDivRank
CDivRank
k-RLM
GSparse

 50
k

cit-Patents, scenario 1

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM

GSparse
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 1000

 100

 10

 1

 0.5

0.45

 0.4

0.35

 0.3

0.25

 0.2

0.15

 0.1

 10

 1

 0.1

)
c
e
s
(
 

e
m

i
t

 0.01

 10000

)
c
e
s
(
 

e
m

i
t

 1000

 100

cit-Patents, scenario 3

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank

 50
k

k-RLM
GSparse
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 100

web-Google, scenario 3

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank

 50
k

k-RLM
GSparse
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)

 100

l

2
e
r
p
x
e

 0.4

 0.3

 0.2

 0.1

 0.88

 0.87

 0.86

 0.85

 0.84

l

2
e
r
p
x
e

 0.83

 0.82

 0.81

 0.8

 5  10  20

 50
k

 100

 5  10  20

web-Google, scenario 1

0.86

0.84

0.82

 0.8

0.78

0.76

0.74

0.72

 0.7

0.68

0.66

 5  10  20

 50
k

 100

 5  10  20

 10

 5  10  20

 50
k

 100

 5  10  20

 50
k

 100

Figure 5: Expanded relevance (exprel
2) with varying
k. BestCoverage variants perform higher than usual
on cit-Patents dataset with scenario-1 queries be-
cause of the low average degree (¯δ = 8.7) and low
clustering coeﬃcient (CC = 0.09) of the graph. The
relaxed algorithms perform closer to their originals,
meaning that they were both eﬃcient and eﬀective
on this type of sparsely connected graphs.

 0.06

 0.05

 0.04

2

 0.03

σ

 0.02

 0.01

 0

web-Google, scenario 1

BC1 (relaxed)
BC2 (relaxed)

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM
GSparse
BC1
BC2

cit-Patents, scenario 1

BC1 (relaxed)
BC2 (relaxed)

PPR (top-k)
GrassHopper
Dragon
PDivRank
CDivRank
k-RLM
GSparse
BC1
BC2

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 0

 5  10  20

 50
k

 100

 5  10  20

 50
k

 100

Figure 6: Coverage (σ2) of the algorithms with vary-
ing k. DivRank variants appear to be implicitly opti-
mizing the size of the expansion set, without consid-
ering whether those results are still relevant to the
query (cf. corresponding exprel

2 in Figure 5).

Figure 5 shows that the exprel

2 results on cit-Patents
dataset vary based on the scenario chosen to generate the
queries.
In fact, the results are higher than normal for
scenario-1 queries. This is because of the low average de-
gree (¯δ = 8.7) and low clustering coeﬃcient (CC = 0.09)
of the graph. Also note that the relaxations of BC1 and BC2
perform closer to BC1 and BC2, meaning that the relaxed al-
gorithms are both eﬃcient and also eﬀective on this type of
sparsely connected graphs.

It is also more clear on plots in Figure 6 that DivRank
variants implicitly optimize the expansion ratio (σ2) of the

Figure 7: Running times of the algorithms with
varying k. BC1 method always perform better with a
running time less than GrassHopper and DivRank vari-
ants, while the relaxed versions score similarly with
a slight overhead on top of the PPR computation.

results, but without considering whether those results are
still relevant to the query. As a striking example of scenario-
1 queries on web-Google dataset, it is quite interesting to
see an algorithm to perform the best with respect to the size
of the expansion set, but almost the worst with respect to
the relevancy of the same set (see Figure 5).

With the runtime experiments shown in Figure 7, we
also conﬁrm that the relaxed variants of BestCoverage per-
form closer to their originals (see Figure 4) with an order
of magnitude or more gain in eﬃciency. In all cases, even
in soc-LiveJournal1, which is the largest dataset in our
experiments, the BC1 method always performs better with
a running time less than GrassHopper and DivRank vari-
ants, while the relaxed version scores closer enough with a
running time slightly higher than the original PPR computa-
tion. Therefore, in terms of the running times, the eﬃcient
algorithms are generally ordered according to PPR ≤k-RLM
≤ BC1(relaxed) ≤ Dragon ≤ BC1. Conﬁrming the ob-

servation in [16], DivRank variants are more eﬃcient than
GrassHopper for k > 10. Runtime of BC2 depends on the
dataset properties while its relaxed variant has comparable
running times to DivRank variants. Both BC2 and its variant
has a very high runtime on ca-AstroPh since this dataset
has the highest average degree (¯δ = 42.2) and the clustering
coeﬃcient (CC = 0.63), hence, eachexprel
2 computation is
more costly than the ones on other datasets.
5.4

Intent-aware results

Among the ﬁve datasets we selected for the experiments,
cit-Patents has the categorical information. One of the
426 class labels was assigned to each patent, where those
classes hierarchically belong to 36 subtopics and 6 high-

723e
g
a
r
e
v
o
c
 
s
s
a
C

l

 70

 60

 50

 40

 30

 20

 10

 0

e
g
a
r
e
v
o
c
 
c
p
o
b
u
S

t

i

 30

 25

 20

 15

 10

 5

 0

e
g
a
r
e
v
o
c
 
c
p
o
T

i

 6

 5

 4

 3

 2

 1

 0

 5  10

 20

 50
k

 100

 5  10

 20

 50
k

 100

 5  10

 20

 50
k

 100

(a) Class coverage

(b) Subtopic coverage

(c) Topic coverage

)
s
s
a
c
(
 
l
l

l

a
c
e
r
-

S

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

i

t

)
c
p
o
b
u
s
(
 
l
l

a
c
e
r
-

S

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

i

)
c
p
o
t
(
 
l
l

a
c
e
r
-

S

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

5

10
k

20

5

10
k

20

5

10
k

20

(d) S-recall on classes

(e) S-recall on subtopics

(f) S-recall on topics

Figure 8: Intent-aware results on cit-Patents dataset with scenario-3 queries.

PPR (top-k)
Dragon
PDivRank
CDivRank
k-RLM
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)
AllRandom

PPR (top-k)
Dragon
PDivRank
CDivRank
k-RLM
BC1
BC2
BC1 (relaxed)
BC2 (relaxed)
AllRandom

level topics7. Here we present an evaluation of the intent-
oblivious algorithms against intent-aware measures. This
evaluation provides a validation of the diversiﬁcation tech-
niques with an external measure such as group coverage [14]
and S-recall [23].
Intents of a query setQ is extracted by collecting the
classes, subtopics, and topics of each seed node. Since our
aim is to evaluate the results based on the coverage of dif-
ferent groups, we only use scenario-3 queries that represent
multiple interests.

One measure we are interested in is the group coverage as
a diversity measure [14]. It computes the number of groups
covered by the result set and deﬁned on classes, subtopics,
and topics based on the intended level of granularity. How-
ever, this measure omits the actual intent of a query, assum-
ing that the intent is given with the classes of the seed nodes.
Subtopic recall (S-recall ) has been deﬁned as the percent-
age of relevant subtopics covered by the result set [23]. It
has also been redeﬁned as Intent-Coverage [25], and used in
the experiments of [22]. S-recall of a result set S based on
the set of intents of the query I is computed with

(cid:4)

S-recall(S, I) =

1|I|

Bi(S),

i∈I

(18)

where Bi(S) is a binary variable indicating whether intent i
is found in the results.

We give the results of group coverage and S-recall on
classes, subtopics, and topics in Figure 8. The algorithms
GrassHopper and GSparse are not included to the results
since they perform worse than PPR. The results of AllRan-
dom are included to give a comparison between the results
of top-k relevant set (PPR) and ones chosen randomly.

As the group coverage plots show, top-k ranked items of
PPR do not have the necessary diversity in the result set,
hence, the number of groups that are covered by these items
are the lowest of all. On the other hand, a randomized
method brings irrelevant items from the search space with-
out considering their relevance to the user query. The re-

7Available at: http://data.nber.org/patents/

sults of all of the diversiﬁcation algorithms reside between
those two extremes, where the PDivRank covers the most,
and Dragon covers the least number of groups.

However, S-recall index measures whether a covered group
was actually useful or not. Obviously, AllRandom scores the
lowest as it dismisses the actual query (you may omit the S-
recall on topics since there are only 6 groups in this granular-
ity level). Among the algorithms, BC2 variants and BC1 score
the best while BC1 (relaxed) and DivRank variants have
similar S-recall scores, even though BC1 (relaxed) is a much
faster algorithm than any DivRank variant (see Figure 7).

6. CONCLUSIONS AND FUTURE WORK
In this paper, we address the problem of evaluating re-
sult diversiﬁcation as a bicriteria optimization problem with
a relevance measure that ignores diversity, and a diversity
measure that ignores relevance to the query. We prove it by
running query-oblivious algorithms on two commonly used
combination of objectives. Next, we argue that a result di-
versiﬁcation algorithm should be evaluated under a measure
which tightly integrates the query in its value, and presented
a new measure called expanded relevance. Investigating var-
ious quality indices by computing their pairwise correlation,
we also show that this new measure has no direct correlation
with any other measure. In the second part of the paper,
we analyze the complexity of the solution that maximizes
the expanded relevance of the results, and based on the sub-
modularity property of the objective, we present a greedy
algorithm called BestCoverage, and its eﬃcient relaxation.
We experimentally show that the relaxation carries no sig-
niﬁcant harm to the expanded relevance of the solution.

As a future work, we plan to investigate the behavior of
the exprel(cid:2) measure on social networks with ground-truth
communities.
Acknowledgments
This work was supported in parts by the DOE grant DE-FC02-
06ER2775 and by the NSF grants CNS-0643969, OCI-0904809,
and OCI-0904802.

7247. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.
Diversifying search results. In Proc. ACM Int’l Conf.
Web Search and Data Mining, pages 5–14, 2009.

[2] S. Brin and L. Page. The anatomy of a large-scale

hypertextual web search engine. In Proc. Int’l Conf.
World Wide Web, pages 107–117, 1998.

[3] J. Carbonell and J. Goldstein. The use of MMR,

diversity-based reranking for reordering documents
and producing summaries. In Proc. Int’l ACM SIGIR
Conf. Research and Development in Information
Retrieval, pages 335–336, 1998.

[4] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.

Expected reciprocal rank for graded relevance. In
Proc. ACM Conf. Information and Knowledge
Management, pages 621–630, 2009.

[5] X. Cheng, P. Du, J. Guo, X. Zhu, and Y. Chen.

Ranking on data manifold with sink points. IEEE
Transactions on Knowledge and Data Engineering,
25(1):177–191, Jan 2013.

[6] C. L. Clarke, N. Craswell, I. Soboroﬀ, and E. M.

Voorhees. Overview of the TREC 2011 Web Track. In
Proc. Text Retrieval Conference (TREC), 2011.

[7] C. L. Clarke, M. Kolla, G. V. Cormack,

O. Vechtomova, A. Ashkan, S. B¨uttcher, and
I. MacKinnon. Novelty and diversity in information
retrieval evaluation. In Proc. Int’l ACM SIGIR Conf.
Research and Development in Information Retrieval,
pages 659–666, 2008.

[8] P. Du, J. Guo, J. Zhang, and X. Cheng. Manifold

ranking with sink points for update summarization. In
Proc. ACM Conf. Information and Knowledge
Management, pages 1757–1760, 2010.

[9] S. Gollapudi and A. Sharma. An axiomatic approach

for result diversiﬁcation. In Proc. Int’l Conf. World
Wide Web, pages 381–390, 2009.

[10] T. H. Haveliwala. Topic-sensitive PageRank. In Proc.

Int’l Conf. World Wide Web, pages 517–526, 2002.

[11] D. S. Hochbaum, editor. Approximation Algorithms for

NP-hard problems. PWS publishing company, 1997.

[12] O. Kucuktunc and H. Ferhatosmanoglu. λ-diverse

nearest neighbors browsing for multidimensional data.
IEEE Transactions on Knowledge and Data
Engineering, 25(3):481 –493, Mar 2013.

[13] O. Kucuktunc, E. Saule, K. Kaya, and U. V.

Catalyurek. Diversifying citation recommendation.
Technical Report arXiv:1209.5809, ArXiv, Sep 2012.

[14] R.-H. Li and J. X. Yu. Scalable diversiﬁed ranking on
large graphs. In Proc. IEEE Int’l Conf. Data Mining,
pages 1152–1157, 2011.

[15] R.-H. Li and J. X. Yu. Scalable diversiﬁed ranking on

large graphs. IEEE Transactions on Knowledge and
Data Engineering, PP(99):1, 2012. preprint.

[16] Q. Mei, J. Guo, and D. Radev. DivRank: the interplay

of prestige and diversity in information networks. In
Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery
and Data Mining, pages 1009–1018, 2010.

[17] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An
analysis of approximations for maximizing submodular
set functions-I. Mathematical Programming,
14(1):265–294, 1978.

[18] R. Pemantle. Vertex-reinforced random walk. Probab.

Theory Related Fields, 92:117–136, 1992.

[19] F. Radlinski and S. Dumais. Improving personalized
web search using result diversiﬁcation. In Proc. Int’l
ACM SIGIR Conf. Research and Development in
Information Retrieval, pages 691–692, 2006.

[20] H. Tong, J. He, Z. Wen, R. Konuru, and C.-Y. Lin.
Diversiﬁed ranking on large graphs: an optimization
viewpoint. In Proc. ACM SIGKDD Int’l Conf.
Knowledge Discovery and Data Mining, pages
1028–1036, 2011.

[21] M. R. Vieira, H. L. Razente, M. C. Barioni,

M. Hadjieleftheriou, D. Srivastava, C. Traina, and
V. J. Tsotras. On query result diversiﬁcation. In Proc.
IEEE Int’l Conf. Data Engineering, pages 1163 –1174,
2011.

[22] M. J. Welch, J. Cho, and C. Olston. Search result

diversity for informational queries. In Proc. Int’l Conf.
World Wide Web, pages 237–246, 2011.

[23] C. X. Zhai, W. W. Cohen, and J. Laﬀerty. Beyond

independent relevance: methods and evaluation
metrics for subtopic retrieval. In Proc. Int’l ACM
SIGIR Conf. Research and Development in
Information Retrieval, pages 10–17, 2003.
[24] X. Zhu, A. B. Goldberg, J. V. Gael, and

D. Andrzejewski. Improving diversity in ranking using
absorbing random walks. In Proc. HLT-NAACL, pages
97–104, 2007.

[25] X. Zhu, J. Guo, X. Cheng, P. Du, and H.-W. Shen. A

uniﬁed framework for recommending diverse and
relevant queries. In Proc. Int’l Conf. World Wide
Web, pages 37–46, 2011.

725