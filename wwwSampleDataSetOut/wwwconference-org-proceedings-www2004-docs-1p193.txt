Using URLs and Table Layout for Web Classiﬁcation Tasks(cid:3)

Lawrence Kai Shih and David R. Karger

Massachusetts Institute of Technology

Computer Science and Artiﬁcial Intelligence Laboratory

200 Technology Square
Cambridge, MA 02139

kai@mit.edu, karger@mit.edu

ABSTRACT
We propose new features and algorithms for automating Web-page
classiﬁcation tasks such as content recommendation and ad block-
ing. We show that the automated classiﬁcation of Web pages can
be much improved if, instead of looking at their textual content, we
consider each links’s URL and the visual placement of those links
on a referring page. These features are unusual: rather than being
scalar measurements like word counts they are tree structured—
describing the position of the item in a tree. We develop a model
and algorithm for machine learning using such tree-structured fea-
tures. We apply our methods in automated tools for recognizing and
blocking Web advertisements and for recommending “interesting”
news stories to a reader. Experiments show that our algorithms are
both faster and more accurate than those based on the text content
of Web documents.

Categories and Subject Descriptors
H.1 [Information Systems]: Models and Principles

General Terms
Algorithms, Experimentation

Keywords
Classiﬁcation, Tree Structures, News Recommendation, Web Ap-
plications

1.

INTRODUCTION

We propose new features and algorithms for use in automated
Web classiﬁcation tasks, such as content recommendation and ad
blocking, that help users cope with the mass of information on the
Web. An obvious approach to such classiﬁcation tasks is to use
the extensively available library of text and image classiﬁcation
tools. But in this paper, we argue that two features particular to web
documents—their URLs, and the placement of links to them on a
referring page—can be used even more effectively for such classi-
ﬁcation tasks. We pursue the intuition that content providers tend
to choose URLs and page layouts that coherently structure their

(cid:3)(Produces the WWW2004-speciﬁc release, location and copyright
information). For use with www2004-submission.cls V1.0. Sup-
ported by ACM.

Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, New York, USA.
ACM 1-58113-844-X/04/0005.

PICK

PICK

Figure 1: Screen-shots from an original CNN page (left) and
the same page viewed through the Daily You (right). Notice the
Daily You’s version removes the advertisements, some of the
navigation boxes, and also writes the word “pick” (emphasized
in picture) near recommended news articles.

content according to topic, and that such topical structuring can be
exploited in classiﬁcation tasks. For example, even with no under-
standing of the text of a newspaper, one might guess relationships
between articles based on visual groupings alone (for instance, that
articles under the same heading are all about the same topic or were
of similar importance). The goal of this paper is to formalize such
intuitions into a general way of algorithmically predicting the prop-
erties of the targets of unvisited links.

Two key steps in classiﬁcation are to select the set of features that
will be examined and the decision rule that will be applied to clas-
sify based on those features. In many ad-blocking applications1, the
features include, for example, the dimensions of the image being
considered and the decision rules (“an ad is an image 250 by 100
pixels”) are laboriously hand-coded. A big disadvantage of such
an approach is the need for human effort to create the rules and to
write new ones as advertisements evolve. To ﬁx this, systems such
as AdEater attempt to apply machine learning, automatically gen-
erating classiﬁcation rules by examining a set of labeled training
examples [13]. In recommendation systems, since different deci-
sions rules work for each user, machine learning is almost always
used. Typically, the Web is treated as a large text corpus: the nu-
merous features used are the words in the documents, and standard
machine learning algorithms such as Naive Bayes or support vector
machines are applied [2].

The Web is more than just text, however: it contains rich, human-
oriented structure suitable for learning. In this paper, we argue that

1for instance, http://webwasher.com

193two features particular to Web documents, URLs and the visual
placement of links on a page, can be of great value in document
classiﬁcation. We show that machine-learning classiﬁers based on
these features can be simultaneously more efﬁcient and more accu-
rate than those based on the document text.

Our motivating example for these classiﬁcation problems is The
Daily You, a tool providing personalized news recommendations
from the Web2. The Daily You uses URLs and table layout to solve
two important classiﬁcation problems: the blocking of Web adver-
tisements and the page regions and outbound hyper-links predicted
to be “interesting” to its user (see Figure 1).

The Daily You’s framework is typical of machine learning appli-
cations: given a number of training examples (documents labeled
as advertisements, or as interesting to the user), The Daily You at-
tempts to make predictions about new, unseen items. Intuitively, a
good prediction strategy is to ﬁnd training examples that are “simi-
lar” to the new item, and predict that the new example has the same
class as those similar training examples.

In typical text classiﬁcation applications, similarity is measured
by word overlap—documents are the same to the extent that they
incorporate similar words (or phrases).
In this paper, we take a
different approach to similarity that stresses the relative position of
items in a tree:

(cid:15) On many Web sites, page URLs are organized in a hierarchy
according to subject. For example, this year’s articles about
space on the CNN Web site have a URL preﬁxed by
cnn.com/2003/tech/space, which can be interpreted as place-
ment in the “space” subtree of the “tech” subtree of the cnn
tree. On the natural assumption that a user is typically in-
terested in certain subjects but not others, the location of an
article in the URL tree is suggestive of the user’s interest in
it. Similarly, on many Web sites advertisements often bear
links pointing back to a single “ad” subdirectory of the site.
Indeed, commercial tools such as WebWasher let users man-
ually specify certain URL “preﬁxes” as indicators of ads that
should be blocked.

(cid:15) We ﬁnd that Web-sites often base the visual layout of their
index pages on a subject taxonomy. This layout is often hier-
archical and reﬂected in a recursive table layout that can be
detected in the (hierarchical) parse tree of the HTML docu-
ment. For example, the CNN Web-site front page offers a
“table of contents” partitioning its stories under a number of
labels such as “U.S.,” “World,” “Travel”, and “Education.”
These placements represent subject classiﬁcations that may
well be strong indicators of “interestingness” for a reader.
Similarly, advertisements often have a speciﬁc placement in
the page layout.

These two examples suggest the possibility of classifying a docu-
ment based on its position in some preexisting taxonomy (the class
label itself is not part of the taxonomy).

Most classiﬁcation algorithms deal primarily with features that
have been reduced to numbers, such as the number of occurrences
of the word “apple” in a document or the length and width of an
image. To instead implement the idea of classiﬁcation using such
tree-based features, we need to solve several problems. First, we
need to develop a classiﬁcation model that allows us to train and
make predictions using the tree-based feature. Second, we need
to show how classiﬁcation using that model can be implemented
efﬁciently.

2publicly accessible at http://daily-you.csail.mit.edu

To design a model, we use the tree as a Bayes net upon which
we impose a “mutation” model—nodes in the tree are usually of the
same class as their parents, but have some probability of ﬂipping to
a different class. To design algorithms, we adapt Bayes-net learning
methods to our application domain. Conveniently, these Bayes-net
algorithms can be made incremental, such that adding new exam-
ples and querying for the likely class of new items (based on their
position in the tree) is very fast.

We then describe our algorithm’s application to news recom-
mendation and ad-blocking problems, and summarize experimen-
tal data showing that our approach works well. On the ad-blocking
front, we show that our ad-blocking algorithm, trained without hu-
man input (using a simple heuristic for identifying ads) is able to
learn to block ads with efﬁcacy matching that of a commercial,
hand-coded classiﬁer with numerous human-derived classiﬁcation
rules. On the news recommendation front, we report the results of
a study involving 176 users; our tree-based classiﬁer required few
training examples and signiﬁcantly outperformed a state-of-the-art
classiﬁer (the support vector machine) in terms of both speed and
accuracy applied to either traditional or tree-structured features.

Besides its advantages in learning efﬁciency and accuracy, our
approach has one other important beneﬁt speciﬁc to the Web. Un-
like text-based classiﬁers, which must fetch the content of the page
being classiﬁed, our classiﬁers do their work by looking at the
pointer to the page, and need not fetch the page itself at all. Thus,
our tree-based classiﬁers require orders of magnitude less band-
width than traditional text based classiﬁers for the same problem.
Under current network conditions, this translates into a signiﬁcantly
faster system.

1.1 Related Work

Some related work has been done on prioritizing the spidering
for topic-relevant search. Rennie and McCallum [18] used rein-
forcement learning to map the text surrounding a link to rank pages
for spidering. Search engines such as Google [5] are also said to use
anchor text in links pointing to a page in order to decide whether
that page is relevant to a query. Chakrabarti, et al. [7] built upon
that work, creating a more robust set of features by parsing out the
structure of the HTML, as we do. However, they choose to treat an
HTML document as a linear sequence of (textual) tokens, and em-
phasize the idea of learning an appropriately-sized and -weighted
“window” around the link that should be used to classify the link.
Chakrabarti, et al. also propose to use the textual content of the re-
ferring page to predict classes of the referring page, and to use those
predicted classes to make predictions about links on that page. All
this work shares our idea of using extrinsic features to evaluate the
class of an unseen document, but does not fully take advantage of
tree structures as we do.

Trees in the form of taxonomies play a large role in machine
learning; however, such taxonomies are generally the target out-
put rather than a useful input, as in our work. Many classiﬁca-
tion algorithms attempt to use traditional features (such as word
counts) to create taxonomies over the documents, or to insert new
documents into a preexisting taxonomy of old documents [15, 12].
Work which aims to use tree position as a feature for learning other
classiﬁcations seems much less common. Haussler [9] is, to our
knowledge, the ﬁrst to propose using position in a preexisting tree
as a feature for classiﬁcation. The model he proposed is more
speciﬁc than ours, requiring that the concept to be learned form
a conjunction of subtrees. Among other differences with our work,
Haussler’s model does not capture classes that are complements of
subtrees. In our terminology, Haussler’s work only allows for for-
ward mutations, but does not allow for backward mutations (more

194on this discussed below). Agrawal and Srikant [1] have a model for
combining catalogs of documents that live in different taxonomies.
Their model is simple in that it assumes that the taxonomy has no
internal nodes (for more complex taxonomies, they discard all the
internal nodes, connecting the root directly to the leaves). This al-
lows for the use of a simpler algorithm (naive Bayes) but discards
rich information about the ﬁner-grained relationships between the
leaves.

Another type of related work is wrapper induction [14]. In wrap-
per induction, the goal is to identify the part of a Web page contain-
ing a speciﬁc piece of information. An example goal would be to
ﬁnd a speciﬁc pattern, like a stock’s quote on some ﬁnancial Web
page, day after day. Wrapper induction often uses formatting infor-
mation to identify the right parts of a page, but we are not aware of
any that uses URL structure. One could view our work as trying to
learn the wrappers that contains things that interest a given user.

Our application resembles that of other prominent Web news ser-
vices. Most existing work only operates on pre-set sites, while ours
allows the users to specify arbitrary target sites. The large commer-
cial services aggregate pages together (http://my.yahoo.com),
but require pre-set sites (which presumably have been manually
conﬁgured to feed speciﬁc stories to Yahoo!) and do not make
user-speciﬁc recommendations. Some research applications like
NewsDude [4] do make such recommendations, but only from pre-
set news sources. NewsDude speciﬁcally uses text-based classi-
ﬁers both to select good articles and to remove articles that seem
overly redundant with already seen articles. An earlier attempt at
user-proﬁling [17] also used a Bayesian text model to predict inter-
esting Web pages. Newsblaster [3] and a similar service at Google
(http://news.google.com) scan multiple pre-set sites and sum-
marize similar articles. Newsblaster uses complex natural language
parsing routines to combine articles from multiple sites into one
summary article.

Other Web applications allow the user to select their own set
of news sources, like the Montage system [2]. Rather than fo-
cusing on new information, Montage is more like an “automated
book-marks builder.” It watches the user to identify pages they
visit frequently, and creates collections of links that let the user get
to those pages more quickly. It uses traditional text-classiﬁcation
algorithms (SVMs) to break these bookmarks into coherent topic
categories. In contrast, our system aims to recommend Web pages
that are new but that we believe will be interesting to the user.

2. URL AND TABLE FEATURES

In this section we discuss in greater detail two tree-structured
features that are particularly relevant to certain Web classiﬁcation
tasks.
2.1 URL trees

The World Wide Web Consortium argues that document URLs

should be opaque (http://www.w3.org/Axioms.html#opaque).
On this Web page, Tim Berners-Lee writes his axiom of opaque
URIs: “... you should not look at the contents of the URI string to
gain other information...”.

In contrast to those style guidelines, most URLs nowadays have
human-oriented meanings that are useful for recommendation prob-
lems.
Indeed, the guideline’s URL contain semantics including
authorship (w3.org), that the page is written in HTML, and that
the topic relates to an “Axiom about Opaqueness.” As the docu-
ment’s URL demonstrates (somewhat ironically), URLs are more
than simply pointers: authors and editors assign important mean-
ings to URLs. They do this to make internal organization simpler
(authorship rights, ﬁle permissions, self-categorization), and some-

times to make that organization scheme clear to readers. Read-
ers often make inferences from URLs, which is why browsers and
search engines usually display URLs along with the text description
of a link. We can infer from a URL that a document serves a partic-
ular function (a speciﬁc Web directory might always serve ads); or
relates to a topic (‘business’ stories might be under one directory);
or has a certain authorship. Or, we might delete a sufﬁx of an URL
in an attempt to move to a more “general” page still related to our
starting point. In short, similar documents (as deﬁned by the site’s
authors) often reside under similar URLs. A good URL structure
provides helpful contextual clues for the reader.

URLs are extremely good features for learning. First, they are
easy to extract and relatively stable. Each URL maps uniquely to a
document, and any fetchable document must have a URL. In con-
trast, other Web features like anchor text, alt tags, and image sizes,
are optional and not unique to a document. Of course, URLs can
be obfuscated, hidden or changed in automated fashion; but such
changes simultaneously make it difﬁcult for users and search en-
gines to ﬁnd and return to information. Second, URLs can be read
without downloading the target document, which lets us perform
classiﬁcation more quickly. This is a necessary condition for real-
time classiﬁcation tasks like ad-blocking. Third, as we argue below,
URLs have an intuitive and simple mapping to certain classiﬁca-
tion problems. For example, we give empirical evidence in Section
4 that the URL is highly correlated with whether a link is an ad-
vertisement or not. Most advertisement clicks are tracked through
a small number of programs; these programs are usually contained
in subtrees of the URL tree, like http://doubleclick.net
or http://nytimes.com/adx/... .

To convert a URL into a tree-shape, we tokenized the URL by
the characters /, ? and &. The / is a standard delimiter for directo-
ries that was continued into Web directories; ? and & are standard
delimiters for passing variables into a script. The left-most item
(http:) becomes the root node of the tree. Successive tokens in
the URL (i.e. nytimes.com) become the children of the previ-
ous token. Note that our construction guarantees we end up with a
tree, even if the web site itself is not tree shaped (two pages may
point to the same URL, but it is the URL itself that deﬁnes the tree
location).

2.2 HTML Table Trees

Similarly, the visual layout of a page is typically organized to
help a user understand how to use a site. This layout tends to be
templated—most pages will retain a ‘look and feel’ even though the
underlying content might be dynamic. For example, different arti-
cles on one particular topic might appear in the same place on the
page day after day. The page layout is usually controlled by HTML
table tags, corresponding to rectangular groupings of text, images
and links. Often, one table along the side or top of a page will
contain much of the site’s navigation. The content of a site might
use tables to group together articles by importance (the headline
news section of a news-magazine), by subject, or chronologically
(newest items typically at the top). Like the URL, this page layout
can be used to eliminate certain content (such as the banners at the
top of the page); or to focus on other content (the headlines, or the
sports section). Like the URL feature, tables make good features
for machine learning. For a page to display properly in browsers,
the tags have to obey a standardized HTML grammar; this also
makes the table feature easy to extract. In the next section, we give
the example of a Chinese Web site that might be understood even
barring understanding the speciﬁcs of the content on the site.

To convert the HTML table structure into a tree-shape, we used
a hand-written Perl program that extracted the HTML table tags

195A

B-1

<HTML> …

<Table…></Table>
<Table…>

<Td><Table>…

A-1

<a href=…>
<a href=…>
<a href=…>
<a href=…>

A

</Table></Td>
….
<Td><Table>…

<a href=…>
<a href=…>

B-1
B-2

</table></td>

…
</HTML>

…

B-1

B-2

…

Without knowing any speciﬁcs of the Web page, most people can
make good guesses about some of its semantic attributes.

Figure 2 also shows how a computer algorithm might reﬂect the
earlier-stated intuition that items visually clustered might have sim-
ilar properties. For example, if one person mentioned that the top
link in box A was an advertisement, it might be reasonable to guess
that everything in box A was an advertisement too. Similarly, if a
person expressed interest in B-1, a natural recommendation might
be B-2.

The upper right-hand side of Figure 2 displays snippets of the
HTML that describes the site; various visually distinct areas from
the Web page are encoded into continuous blocks of HTML. The
HTML forms a tree, shown by indentations. That tree is abstracted
on the bottom left. Suppose someone labels that node A-1 is an ad-
vertisement (white), while node B-1 is content (black). The tree on
the bottom right shows a potential generalization of that informa-
tion in which everything in box A is considered an advertisement.
The work described here tries to formalize and build algorithms
that automatically make generalizations like these.

A-1

B-1

B-2

A-1

B-1

B-2

…

…

3.2 The Generative Model

A

A

Figure 2: Shown is an abstraction of Web problem to the do-
main of “tree learning.” The top-left shows the original Web-
site. The top-right shows that visual portions of the page are
collected in chunks of HTML, which are indented to show the
HTML’s tree structure. The bottom left shows the abstracted
tree, receiving a partial labeling of the page: white (“advertise-
ment”), black (“content”) and gray (“unknown”) nodes. The
bottom right shows one potential generalization of the tree
which suggests everything in box A is an advertisement.

(<table> and <td>). The root of the tree is the entire page’s
HTML. The children of a node are the next lower level of table
elements.

Note that while the same story might be headlined in more than
one place on the page (e.g., an article might appear in both “world”
and “education”), for the purpose of this paper, we treat those two
appearances as two separate leaves on the taxonomy.

3. LEARNING MODEL

In this section, we outline our learning model and algorithm. We
classify using a generative model. We deﬁne a process that en-
forces our ideas about correlation: items nearby in the (URL or
layout) tree usually have the same class. Our training data tells us
the classes of some of the items, and our model suggests other ar-
eas of the tree that might be similar. More precisely, we model our
learning problem using a Bayes net, and apply fast algorithms to
perform classiﬁcation on that Bayes net. In the next section, we
give a short intuitive description of the types of relationships our
algorithm and model might ﬁnd.

3.1 Intuition Behind Model

Suppose someone was given a Web page, like the one featured at
the top left of Figure 2, and asked to recommend a link to a friend.
Suppose the recommender could not read any Chinese, had never
visited the particular site3, and generally could not decipher any of
the contents of the page. Given that the friend liked the article in
B-1, one might reasonably recommend the adjacent article in B-2.

Our goal is to create a generative model in which nearby items
tend to have the same class. More formally, we deﬁne a probability
distribution over the possible classes of items in the tree. We aim
for the probability distribution to reﬂect our intuition that nearby
items usually have the same class. To accomplish this, we use a
Bayes net over the tree holding the items to be classiﬁed. Our as-
sumption that the class to be learned is correlated to the tree posi-
tion is captured in a model based on mutations. We consider the
tree holding the items we wish to classify. The actual items are
usually leaves in this tree, but we extend our model to assume that
the internal nodes, which may or may not represent actual items,
also have “hidden” classes. Some children of internal nodes may
“mutate” into different classes. We begin by assigning some class
at the tree’s root. We then work our way down the tree, declaring
that each node is probably of the same class as its parent but has a
small chance of being of the other class.

Formally, we consider two classes 0 and 1, and specify a for-
ward mutation probability (cid:18) and a backward mutation probability
(cid:30) (conversely, with probabilities 1 (cid:0) (cid:18) and 1 (cid:0) (cid:30), respectively, no
change occurs). We declare that the class of a node follows proba-
bilistically from the class of its parent. Suppose node x has parent
y in the tree. Let Nx and Ny denote the classes of nodes x and y
respectively. Then

Pr[Nx = 1 j Ny = 0] = (cid:18) and
Pr[Nx = 0 j Ny = 1] = (cid:30):

Applying this rule downward from the root provides a generative
model of class assignments to nodes in the tree. Given the root’s
class, we can ﬂip coins according to the formulas above to deter-
mine the children’s classes; given these we can generate the grand-
children’s classes, and so on.

To initiate the process we must choose a value for the root class.
In the case of ad-blocking it might be natural to argue that the root
of a web site is surely not an ad. As this is a general model, we are
certainly able to choose a prior biased toward one class or another.
However, if we wish to minimize the number of free parameters to
only (cid:18) and (cid:30), we can declare the root class to be a function of those
already existing parameters. For the root node r, we declare that

3http://dailynews.sina.com

Pr[Nr = 0] = (cid:30)=((cid:18) + (cid:30))

196This formula is useful: if x is a child of the root, then

we ﬁnd that

Pr[Nx = 0] = Pr[Nx = 0 j Nr = 0] Pr[Nr = 0]+

Pr[Nx = 0 j Nr = 1] Pr[Nr = 1]

(cid:30)

(cid:18) + (cid:30)

+ (cid:30)

(cid:18)

(cid:18) + (cid:30)

=(1 (cid:0) (cid:18))

=

(cid:30)

(cid:18) + (cid:30)

:

In other words, with this root probability, all nodes in the tree have
the same probability of being class 0, prior to labeling any of the
leaves. Later, as we label the leaves, these probabilities will change
(see the next sub-section).

If (cid:18) and (cid:30) are small, our model asserts that a node is likely to
have the same class as its parent, and thus likely to have the same
class as its siblings and other nearby nodes in the tree. As the mu-
tation probabilities increase, the correlation between nearby nodes
in the tree decreases. We usually constrain (cid:18) and (cid:30) to be less than
:5, which indicates the probability of a mutation is less than the
probability of a non-mutation.

To summarize the steps in creating a Bayes net out of the given

hierarchy:

(cid:15) set conditional probabilities for all edges in the tree: for node

x with parent y,
pr[Nx = 1 j Ny = 0] = (cid:18)
pr[Nx = 0 j Ny = 1] = (cid:30)

(cid:15) set the root prior: p[N0 = 1] = (cid:18)=((cid:30) + (cid:18))

Next, we discuss how the probabilities in the tree change when

leaves are labeled.

3.3 The Learning Algorithm

With this model, in the absence of any evidence, we make the
same prediction about every node x: that it has probability Pr[Nx =
0] = (cid:30)=((cid:18) + (cid:30)) of being in class 0. Things become more interest-
ing when we are given some evidence. Suppose that there is a set
of (leaf) nodes E whose classes are known. This will inﬂuence our
predictions about other nodes: in particular, we want to compute
the Pr[Nx = 0 j E] that node x has class 0 conditioned on the
evidence E. We can expand this conditional probability as

Pr[Nx = 0 and E]

Pr[E]

:

Note that both the numerator and denominator are simply the prob-
ability of some set of nodes (E in the denominator, Nx [ E in
the numerator) getting certain labels. So if we can compute such a
quantity for general evidence, we can compute the desired condi-
tional probability. We focus on the computation of Pr[E].

We use a standard Bayes-net inference algorithm. Suppose that
the root r has two children x and y, and let Ex and Ey denote the
labeled leaf nodes in the x and y subtrees, respectively. Because
of our generative model, given the class of the root r, the classes
of nodes in the two subtrees are independent of one another, and
their probabilities can be multiplied. That is, Pr[E j Nr = 0] =
Pr[Ex j r = 0] (cid:1) Pr[Ey j r = 0], and similarly for the case
Nr = 1. But recall that to generate the classes in x’s subtree given
that Nr = 0, our ﬁrst step is to pick a random class for x, and then
assign classes to items below x based on the class of x. Formally,

Pr[Ex j Nr = 0] = Pr[Ex j Nx = 0] Pr[Nx = 0 j Nr = 0]

+ Pr[Ex j Nx = 1] Pr[Nx = 1 j Nr = 0]

=(1 (cid:0) (cid:18)) Pr[Ex j Nx = 0] + (cid:18) Pr[Ex j Nx = 1]

More generally, let us write px0 = Pr[Ex j Nx = 0] and px1 =
Pr[Ex j Nx = 1] (note that these two quantities need not sum
to 1). Our analysis above, generalized to an arbitrary number of
children, says that for any node x,

px0 = Y

((1 (cid:0) (cid:18))py0 + (cid:18)py1)

y2children(x)

px1 = Y

((cid:30)py0 + (1 (cid:0) (cid:30))py1)

y2children(x)

Technically, the product over children is actually limited to children
whose subtrees contain nodes from E.

The above algorithm can compute, working up from the leaves,
two quantities px0 and px1 at each node x in the tree. At the end
of the recursion we have pr0 and pr1 and the root. At this point we
can compute

Pr[E] = pr0 Pr[Nr = 0] + pr1 Pr[Nr = 1]

= pr0

(cid:30)

(cid:18) + (cid:30)

+ pr1

(cid:18)

(cid:18) + (cid:30)

During the computation, each node was “inspected” a constant
number of times (to produce values for its unique parent), so the
entire computation takes time linear in the size of the tree. As dis-
cussed above, we can run the computation once for the training data
E, and once for the extended data E [ fNxg, in order to compute
Pr[Nx j E].

It would be unfortunate if we had to run a linear time (in the total
number of nodes) computation to classify each new candidate item,
but fortunately we can do much better. Notice that when a labeled
item x is added to E, it only inﬂuences the probabilities py0 and
py1 for nodes y that are ancestors of the new node x (this follows
by induction since a node’s values depend only on its children’s
values). Thus, if we compute and store the quantities py0 and py1
for all nodes y in the initial classiﬁed training set, then the com-
putation of Pr[E [ fNxg] (and from it Pr[Nx j E]) can be done
by walking up the tree from x. This takes time proportional to the
depth of x, which is effectively constant in our applications (few
URLs or table structures have depth exceeding 10). Some small
care needs to be applied to maintain this constant time bound when
nodes have large numbers of children, but it can be done [20].

In summary, with linear storage space, we can hold a data struc-

ture for the training data E that lets us

(cid:15) compute Pr[Nx j E] for any query node x, or

(cid:15) add a new labeled node x to the training set E

in time proportional to the depth of the tree, which is in effect con-
stant for most trees. Note that in fact, we need only store parameters
for nodes that are ancestors of an evidence node (the rest of the tree
can be created on the ﬂy as needed).

4. AD-BLOCKING

We now consider an application of our technique to ad blocking.
We show how a learner based on tree features can match the per-
formance of a commercial, hand-coded ad-blocker.
Intriguingly,

197our learner requires no human training—instead, it generates its
own training data using a slow but simple heuristic based on link
redirection.

Ad-blocking is a difﬁcult problem because advertisements change
over time, in response to new advertising campaigns or new ad-
blocking technology. This makes it quite burdensome to maintain
an ad-blocker based on hand-coded rules: when new advertise-
ments arrive, a static ad-blocker’s performance suffers; typically
engineers must code a new set of rules, and users must periodically
download those rules.

As we have mentioned, our approach is to have the computer
learn sets of rules; such an approach means less work for the en-
gineers and fewer rule updates for the users. A typical approach
would be for engineers to label some small set of training examples,
and have the classiﬁer learn a classiﬁcation rule based on them. An
example of this training-based approach is AdEater [13]. AdEater
got a 97% accuracy (the only metric reported) on their ad block-
ing experiments, though it requires 3,300 hand labeled images to
achieve that accuracy. AdEater uses humans to perform the rela-
tively simple task of labeling images as ads or not; the machine
learning takes on the more difﬁcult task of discovering rules. How-
ever, AdEater suffers in the same way that most ad-blocking pro-
grams do: it takes considerable human effort and time to produce
an update; hence its accuracy decreases as new ad forms surface.

Of course, hand labeling large numbers of images is also time
consuming. So we actually go one step further, using no human
training at all. In order to avoid human training, we use a slow,
but reasonably accurate, heuristic to label links on a page as ei-
ther advertisements or not (of course, AdEater could have used the
same training methodology). Instead of using high-quality human
data, we use a heuristic called Redirect. Redirect labels a link as
an ad if fetching the link yields a redirection to another site. The
redirect heuristic makes sense because it captures the normal pro-
cess that advertisers use: tracking the click, then sending the user
to the advertised site. Notice that this is much more of a “content”
based heuristic than image sizes which are “form” heuristics; ads
can take any shape and size, but most current advertisements incor-
porate some type of tracking mechanism.

As we shall see, the Redirect rule is about as accurate as Web-
Washer at identifying ads. But there is a big barrier to using Redi-
rect itself as an ad blocker: to decide whether to block an ad, Redi-
rect must fetch it ﬁrst. This generates a signiﬁcant overhead in
additional connections and downloads—one which in today’s net-
work environment makes the Redirect heuristic too slow to use in
real time ad blocking. However, we will show that our tree-based
learner can predict a redirect without actually trying it—this gives
us an approximation to the redirect heuristic that can be used for
real-time ad blocking.

In practice, click-through tracking requires back-end infrastruc-
ture like databases and CGI scripts. Therefore, ads tend to be lo-
cated together under a small number of URL directories per site
(i.e., under xyz.com/adserver). It is rare for a site to have advertise-
ments and content in the same leaves of the URL tree. Therefore,
we use our tree learning algorithms to associate existing URLs with
an ‘ad’ or ‘not ad’ label provided by the Redirect heuristic. As a
new page is loaded, the learning model predicts whether new URLs
are ‘ads’ or ‘not.’ A big advantage of the redirect heuristic is that
training examples can be provided automatically by an off-line al-
gorithm that, when the user is doing other things, visits sites and
takes the time to check and follow redirects. In other words, we
can train without any human input.

4.1 Ad-blocking Framework

In order to label the advertisements on a given page, we down-
loaded the page and saved the HTML into a cached ﬁle. The cached
ﬁle was necessary so that every algorithm would be viewing exactly
the same Web page. We went through the cached ﬁle locating links
on the page. We used a Perl expression to ﬁnd all links on a page,
with a regular expression that matched <a href= ... /a>.
This corresponds to a standard HTML deﬁnition of links on pages.
From the list of links we found on the cached page, we only used
links that contained images. This is because, at the time of our ex-
periments, the commercial ad-blocker (WebWasher) only blocked
image-based advertisements. To detect images embedded within
links, we only took the subset of links whose anchors contained
the text <img, which is a standard HTML deﬁnition for displaying
images. For a given page, we call this subset of the links on a page
image-links.

Given this list of image-links, we used four systems to label
whether a link contained an advertisement or not. The four sys-
tems are listed below:

(cid:15) WebWasher is a commercial product with handwritten rules
that uses many features like the dimensions of the ad, the URL
and the text within an image. It is in use by four million users.
WebWasher can be downloaded free for educational and personal
use4. We used the WebWasher Linux version, which was installed
as a proxy. The WebWasher proxy reads in the requested Web page,
erases the advertisements, then forward the modiﬁed page to the
Web client.

In order to have WebWasher label a page, we ran our cached
ﬁle through the WebWasher application. Webwasher removed all
the image-based links it deemed advertisements. Any link that
was in our original list of image-links but was not present in the
WebWasher-processed page was deemed an advertisement by the
WebWasher method.

(cid:15) Redirect is the simple heuristic mentioned above that moni-
tors third-party redirects. As we mentioned, it is a somewhat noisy
heuristic, meaning its accuracy is less than humans’; but it can run
in the background without human input. Redirect used a small
amount of Perl code, along with the lynx program 5 to check for
redirects that went to top-level domains different from the originat-
ing page.

(cid:15) Learn-WW is our tree-based URL algorithm, trained on Web-
Washer’s output. The URL provided the tree-structure as discussed
in Section 2.1, and WebWasher provided the labeling for the leaf
nodes.

(cid:15) Learn-RD is our tree-based URL algorithm, trained on Redi-
rect. The method for training Learn-RD is identical to that of
Learn-WW, except that the RD (third-party redirect) heuristic was
used in place of WebWasher to label the leaf nodes.

As we have mentioned before, RD alone is a good heuristic, but
cannot operate in real-time. Our hope is to train a classiﬁer like
Learn-RD off-line periodically with RD, allowing for a fast ad-
blocker that has been trained by the RD heuristic. We might also
hope that the learner could ﬁnd a general rule that ignored certain
RD errors so became more accurate than the heuristic.

4.2 Testing and Training Data

In the previous sub-section we described a variety of algorithms
designed to label whether the links on a page contained advertise-
ments or not. In this section, we describe how we acquired training
and testing data.

4http://www.Webwasher.com/client/download/private use/index.html
5http://lynx.org

198Top 25 Sites weather

.935 (.136)
Web-Washer
Learn-WW .931 (.118)
Redirect
.933 (.08)
.934 (.08)
Learn-RD

.907
.860
.842
.837

look
smart
.857

1

.857

1

euniverse

.517
.517

1
1

Table 1: Shown are the Web blocking accuracies for several
Web sites, along with standard deviation information for the
top 25 Web sites.

We generated a dataset from Media Metrix’s largest 25 Web
properties as of January 20026. Empirical evidence shows the aver-
age user spends all their time on a small number of the largest sites.
We felt that blocking ads of the largest 25 Web properties would be
both representative and beneﬁcial to many, if not most, users.

We crawled through a given Web site, randomly picking eleven
pages linked from the front page that shared their top-level domain
with the front page.

The links on those eleven pages, along with the links on the front
page, were divided randomly into a six-page training and a six-page
test set. Each Web site went through the random training and test-
ing twice. WebWasher and Redirect classiﬁed each linked image
in the training group, and those classiﬁcations, along with the link
URL, were used to train Learn-WW and Learn-RD respectively.

Next, all four classiﬁers were applied to the linked images on the
test group. If all four classiﬁers agreed that an image was either an
ad or all agreed it was not an ad, they were all deemed to have clas-
siﬁed the image correctly. Spot-checks suggested that agreement
between all methods almost always lead to the correct prediction.
If one technique disagreed with the others, we manually judged
whether the image was really an ad or not.
4.3 Experimental Results

In total, 2696 images were classiﬁed, and all four classiﬁers
ended up with an average classiﬁcation accuracy across all sites
of within a quarter percent of 93.25% (see Table 4.3). The ﬁrst
column contains average error rates for the 25 sites, with standard
deviations in parentheses. Standard deviations are based on site-to-
site comparisons. The overall false negative rates (labeling an ad
as content) and false positive rates (labeling content as ads) were
approximately 26% and 1%, respectively, and that was fairly con-
sistent between all the classiﬁers. Note that the mistakes were bi-
ased toward false negatives, which means the classiﬁers let through
some ads, but rarely blocked content. This is probably the appro-
priate behavior for an ad-blocker.

Table 4.3 also shows data for speciﬁc sites. On various sites,
trainers beat learners (weather.com); and learners beat their trainers
(looksmart.com). Thus learners are not exact imitations of their
trainers, but on average end up with the same accuracy rates.

The standard deviations of the four classiﬁers are relatively large
because errors tend to cluster around a few Web sites. WebWasher,
for example, does poorly marking ads on euniverse.com, which
uses different dimensions for its ad images than many sites. Redi-
rect does poorly on portals and internal ads. For example, Redirect
would not label a New York Times advertisement for its own news-
paper as an advertisement.

It is possible for the learners to generalize to better performance
than the trainers. For example, all of the New York Times adver-
tisements are in a few URL directories. Thus, a few incorrect ex-
amples from Redirect are ignored in favor of the larger number of

6http://www.jmm.com/xp/jmm/press/MediaMetrixTop50.xml

correct examples; the learner correctly “overrides” Redirect. Con-
versely, the learners can also generalize incorrectly; if the trainers
are a little more wrong than right, the learners end up generalizing
in the wrong direction and consistently mis-labeling the links. Such
problems happened on the weather.com Web site.

The Redirect heuristic worked well; its accuracy was not statis-
tically different from the commercial ad-blocking program. Redi-
rect’s simplicity suggests that, for now, that it is correctly under-
standing the mechanisms by which most sites serve up ads (i.e.
through third party redirects). That mechanism can certainly change,
but such changes would be harder for advertisers to make than sim-
ply changing the advertisement sizes, which can foil WebWasher at
times.

The URL tree seemed to be a good feature for ad-blocking. Both
WebWasher’s and Redirect’s rules seemed to be captured in many
cases by the presence or absence of a single speciﬁc URL pre-
ﬁx. Had the URL been a poor feature, one would expect the tree-
learning based ad-blockers to form more complex hypotheses and
have much lower accuracies.

A spot-check of random sites (using a random link generator like
www.mangle.ca) suggests that the URL feature works even bet-
ter on smaller sites than larger sites, because smaller sites tend to
use third party advertisers whose URLs are almost entirely used to
serve advertisements, like doubleclick.com.

We wish to make a few points about our results. First, our Learn-
RD algorithm achieved performance comparable to a commercial
ad-blocker, without needing complex, hand-written rules. In fact
our system performs better in some respects. Most ad-blocking sys-
tems do not remove text-based ads (ads, for example, placed within
search engine results), while both Redirect and our learner trained
on Redirect acts no differently for image-based ads and text-based
ads. Second, we argue that our ad-blocking classiﬁer will adapt
in the long term better than a static version of WebWasher, since it
can update rules nightly without any human input. Of course, in the
adversarial world of advertisements, it is probable that this system,
too, would be defeated if it became widespread: an advertiser could
deliberately obfuscate their URLs. Third, we point out that unlike
the black and white “redirect” heuristic, our learner gives pages a
range of “blackness” scores. It can thus be tuned to trade off false
positives and false negatives depending on the user’s preference.

5. RECOMMENDATION EXPERIMENTS

In the previous section on Ad blocking, we showed that machine
learning on tree-based features could match or outperform hand-
coded rules (WebWasher) and an inefﬁcient heuristic (Redirect). In
this section, we demonstrate that tree-based learning can also out-
perform classiﬁcation algorithms based on traditional textual (and
other) features. As we mentioned in the introduction, our goal is
to build real-world Web systems that save time and effort on behalf
of users. One of those systems was a recommendation system de-
signed to ﬁnd interesting links customized to individual users. In
this section, we describe the system and discuss experiments we
performed to judge the effectiveness of such a system.

We ran a user study on 176 Web participants to get real news
story data from users. We asked participants to click on any stories
they normally would read, and conducted experiments into the best
algorithms for predicting test clicks on new pages given training
clicks on previous pages.

Overall our two tree algorithms performed well, even against the
support vector machine which is commonly thought of as one of
the best general-purpose learning algorithms. We showed that our
features, alone or coupled with our algorithms, can increase the
accuracy of classiﬁcation. In the next sub-section (5.1) we describe

199the set-up of the user study, followed by a sub-section (5.2) on the
algorithms we tried and their advantages and disadvantages. That
is followed with empirical results and analysis of our algorithms
(5.3).
5.1 User-Study Set-Up

We performed a study involving 176 users. Users were presented
with 5 pages containing links to stories, and asked to click on the
links they would normally click on. The pages were visually unal-
tered replicas of pages downloaded from the Web, consisting of 5
consecutive days worth of The New York Times front page (Septem-
ber 15th through 19th, 2003). The pages and user study are avail-
able from our server7. Based on the submitted email addresses of
the participants, the users encompassed a broad range of people
from throughout the world.

In order to encourage a large number of participants, we made
the experiments easy to complete and gave ﬁnancial incentives for
doing the experiments. The experiment was entirely conducted in
one session on the Web, by using cached copies of the ﬁve pages
mentioned above.

Some users did not complete the study, and a small number of
others did not click on any links; these were discarded from the
study and are not considered part of the 176-user sample set.

Each click on a link within the user study would place a check-
mark in a check-box corresponding to that link, and when the user
submitted the page, a list of all the clicked (and un-clicked) links
was noted on the server. The clicked (and un-clicked) links were
used to generate positive and negative examples with a variety of
features (next subsection) which were then made available to vari-
ous learning algorithms for prediction of unseen clicks.

We chose The New York Times as one of the most popular news
sites in the world, meaning that the content of those pages was
broadly focused. We believe it is a representative of news sites
that many users read.

5.1.1 Features

For each link on each page, we collected a variety of features for
use in our algorithms, to the extent practicable. For each link, we
have a copy of the link’s URL, the anchor text of the link, the posi-
tion of that link in the table structure of the page, and the full text
of the article. In total there were 1105 links across the ﬁve pages8.
There are some difﬁculties with collecting the data for every link,
particularly the textual data.

For example, not every link has anchor text inside it, since some
links are only images. Therefore, when available, we took the “alt”
tags to stand for the text. Even so, there are several image-only
links that have no available anchor text.
In total, approximately
1% of the links did not have anchor text we could parse within
them. We will refer to data based on the anchor text in the link as
“Anchor.”

It is even more difﬁcult to fetch all of the pages behind the links.
For example, many sites (including the ones we chose) only al-
low registered users, which generally means that the fetching agent
needs to understand and respond to cookies. Harder still is follow-
ing the many types of redirects and translating properly between
absolute and relative links. Also many pages (again, including the
pages we chose), have Javascript links that require a Javascript in-
terpreter to understand. We used Lynx as a browser, and wrote spe-
cial routines to follow redirects, to automatically log in to the site
via cookies, and to translate between absolute and relative links.
7http://daily-you.csail.mit.edu/data3
8The raw recommendation data and features are available at
http://www.ai.mit.edu/ kai/recommendation dataset.txt

While not perfect, we believe we gathered as many of the target
pages as was reasonably possible. We did not follow Javascript-
based links because we were not aware of any reasonable way to
place a Javascript parser within Lynx.

There is also a problem of rights for visiting Web sites. Many
Web sites (including those we chose) do not allow spiders on the
pages underneath the home page. Also, standard etiquette says you
may not download more than one page every ﬁve seconds. We
followed the second rule but not the ﬁrst since the ﬁrst makes it
impossible to grab the full text of articles. One occasional conse-
quence of having a slight delay between downloading Web pages is
that sometimes the pages disappear before downloading. Though
we tried to get as much of the pages as possible, around 5% of them
were not download-able for the reasons given above. We will refer
to the data based on the full text of the fetched document as “Doc.”
The URL of the link is comparatively easy to read, as it is a pre-
requisite to having a usable link (and a pre-requisite to being able to
download any pages, for both our robot and for a user). The table
structure the links sits in is slightly more difﬁcult, because it re-
quires parsing the page into table elements, but every link has a po-
sition in the table structure. Both of these datasets were complete:
i.e., for every link a person could click on, there is a corresponding
URL “URL” and table element “Table” that link sat within.

To summarize, there were four basic features used:
Anchor: the anchor text within the link.
Doc: the full text of the words in the linked document
Url: the URL in a form that retains its path through the tree
Table: the location of the link in the table, in a form that retains

its path through a tree.

5.2 Recommendation Algorithms

We tried several algorithms with various datasets labeled with
each user’s click data. That is, for each of the 1105 data points,
176 different combinations of positive and negative labellings were
found based on the user’s empirical news selection.

We used two “general purpose” classiﬁers, Naive Bayes “NB”,
[8] and the support vector machine “SVM” [6], across all four of
the feature sets, plus our tree learner “TL” on the two tree-structure
features. That is, some of our feature/classiﬁer pairs gave the SVM
and NB classiﬁers the same URL and Table features the tree algo-
rithm used.

We choose the SVM because it is considered to be one of the best
general-purpose discriminant classiﬁers [11]. We used a standard,
fast, publicly available implementation called SVMFu [19].

In order to give the various non-tree algorithms (SVM, Naive
Bayes) a chance to learn based on the URL and Table tree-features,
we took each node from the tree and translated it into a ‘word’ that
contained information about the path to that node. For example, the
URL http://nytimes.com/business was tokenized into three
tokens: http://, http://nytimes.com, and
http://nytimes.com/business. We chose this “nested” tok-
enization instead of the obvious splitting up of the URL into “words”
so that the representation would still convey the exact position of a
node in the tree, giving the non-tree algorithms the opportunity to
generalize in the same way that our tree-algorithms might. In par-
ticular, two nodes that are distant in the tree but happen to have the
same words will, under our scheme, still have completely differ-
ent tokens. We believe that this hierarchical tokenization gives the
non-tree algorithms a fair chance to exploit the same features as our
tree. For example, if all “relevant” links are in a speciﬁc subtree,
the SVM can learn that class by identifying and classifying based
on the token representing the root of that subtree.

200Classiﬁer
1. Random
2. Perfect
3. NB-Doc
4. NB-Anchor
5. NB-Table
6. NB-Url
7. SVM-Doc
8. SVM-Anchor
9. SVM-Table
10. SVM-URL
11. TL-Table
12. TL-URL

Top Rec Top 3 Recs Top 5 Recs Top 10
29
857
81
302
72
80
59
220
177
308
401
385

330
6093
594
1615
576
1036
520
1438
934
1953
1512
2149

104
2488
232
713
180
319
156
614
472
839
900
979

162
3899
342
1021
278
507
257
913
662
1268
1176
1388

Table 2: Summary of all the classiﬁers and features on The
New York Times datasets. The numbers represent the number
of clicked recommendations each classiﬁer would get, if they
(columns) recommended the top, top three, top ﬁve, and top
ten highest scoring links. Bolded items have higher accuracies
than non-bolded items for a given column (statistical signiﬁ-
cance methodology is reported in the Appendix).

5.2.1 Test Environment Parameters

Overall we tried a wide variety of algorithms on a cross-validated
set of The New York Times data. From our ﬁve documents, we used
the links on 4 of them for training and the links on the remaining
one for testing, across all of the 176 users. In total, this corresponds
to 182,325 classiﬁed links per experiment (1105 Times links x 176
users).

We tried a total of 10 different algorithms corresponding to mix-

tures of different algorithms and features:

Support Vector Machine (C=1): SVM-Anchor SVM-Doc SVM-

Url SVM-Table

Naive Bayes ((cid:11) = 1): NB-Anchor NB-Doc NB-Url NB-Table
Tree Learning ((cid:18) = :2 (cid:30) = :05):TL-Url TL-Table
The algorithms we used required speciﬁc parameter settings that
we describe here so the experiments can be replicated. We chose
Naive Bayes because it is a fast, common model-based classiﬁer
whose model does not work well with tree structures. Speciﬁcally,
NB assumes independence between features whereas a tree implies
certain strict dependencies. For NB we used an (cid:11) smoothing pa-
rameter of 1 [8].

The SVM required the setting of a “C” parameter that measures
the outlier penalty. We used a smaller portion of the test set (2
pages) to determine an optimal C parameter for the four differ-
ent feature sets. We tried C values of 1, 3, 5 and 10. Across the
four feature types (Link, Doc, Url, Table), C did not substantially
change the results, so we used a C parameter of 1. Other parameters
included setting the number of cache rows to 3000 (this changed
speed but not accuracy) and setting the kernel to linear. Both the
kernel and the data were represented as ﬂoating point numbers.

We used our Tree Learning (TL) algorithm on the two tree-structured

features, Url and Table. We set the backward mutation rate to .2,
and cross validated the forward mutation rate on the same 2-page
training set we used for the SVM. We tried forward mutation rates
of .05, .1, .15, .2, .25, and .3. In general, lower forward mutation
rates worked better, so we used .05 in successive experiments.

5.3 Recommendation Results

On average, users selected nine links a day (out of an average
of 221 possibilities each day). Each of the classiﬁers produced a

ranked list of recommendations for each user and each page (one
page represents one day of stories). Results are written in terms of
the number of correct recommendations across all users and links
within the top recommendation, top three recommendations, top
ﬁve recommendations, and top ten recommendations. Complete
results are shown in Table 2. A perfect classiﬁer (second row)
would have achieved 857 correct recommendations when produc-
ing one recommendation per day per user (176 users times 5 days;
a small number of users did not click on any recommendations for
some days). A random classiﬁer on the other hand, would have
done a (statistically) signiﬁcantly worse job than any of the trained
classiﬁers. Statistical signiﬁcance is reported in terms of a non-
parametric statistical test found in Hollander and Wolfe [10] and
detailed in Shih [20].

Overall the best performing algorithm was the tree learning al-
gorithm applied to the URL feature, which had the best scores for
three out of the four categories. The second best algorithm was a
tie between the SVM applied to the URL feature and the tree learn-
ing algorithm applied to the table feature. Each of these performed
well in different categories.

Rows 3 and 4 (NB) and 7 and 8 (SVM) shows a comparison
between two of the text features (Anchor and Doc) against two al-
gorithms (NB and SVM). In both cases, the anchor text provides
a much better feature than the fetched document’s text. This may
be because the anchor text is supposed to contain a summary of the
document, and it is easier for the classiﬁer to understand these sum-
maries than the documents themselves (perhaps the summaries use
a smaller feature set than the full documents so require less training
data). This is a positive result for text classiﬁers because the anchor
text is much easier to fetch than the target document’s text (which,
as mentioned before, can be slow and can be difﬁcult technically to
download). The differences are all statistically signiﬁcant.

Rows 5, 9 and 11 (Table feature) and 6, 10 and 11 (URL fea-
ture) shows the difference between the various algorithms on the
tree structured features. As mentioned previously, we ‘tree-iﬁed’
the features for the beneﬁt of NB and the SVM so each algorithm
would be aware of the position in the tree. The Bayes-net algo-
rithm does the best job of taking advantage of the tree structured
features. This may be partially due to its domain knowledge of the
problem, coupled with a relatively small amount of training data. In
those situations, model-based classiﬁers often perform better than
discriminative classiﬁers [16]. This is particularly true on the table
features, which tend to be much ﬂatter than the URL: the depth of
the table tree is much more constant than the depth of the URL tree.
The next best performing algorithm is the SVM, which generalizes
well considering it has no domain knowledge. The worst classiﬁer
on tree-structured features is NB. We believe this is because NB
assumes independence between features while a tree actually gen-
erates highly dependent features (a given child always has the same
parent). The differences for 3, 5 and 10 recommendations are all
statistically signiﬁcant between the classiﬁers.

Rows 4, 10 and 12 shows the best feature given the various al-
gorithms on our data sets. Naive Bayes needs to use the text-based
features (since it is unsuitable for the tree-based features), and as
discussed before, the best text-based feature was the anchor text.
Both the SVM and the tree-learner did best on the URL feature,
suggesting that the URL feature is a good feature for use on recom-
mendation problems like the one we posed. The differences for 3,
5 and 10 recommendations are all statistically signiﬁcant between
the classiﬁers.

An important caveat in our results is that our system was aiming
to learn what links a user would click on, not what stories a user
would enjoy reading. It is perhaps unsurprising that the anchor text

201is a better predictor of user clicks than the body, since the anchor
text is what users actually see when making their click decision.
It is an interesting open question to evaluate the ability of our tree
learner to identify stories the user would enjoy reading; this eval-
uation however would involve signiﬁcantly more overhead since it
would require test subjects to actually read all the articles being
consider, rather than just reading their anchors.

6. CONCLUSIONS

We began with the observation that the proper choice of features
can have a signiﬁcant impact on the performance of classiﬁcation
algorithms. Since Web site authors have an incentive to organize
their materials, for the sake of both the author and their audience,
we hypothesized that the URL of documents and the physical place-
ment of elements on a page could provide clues into the Web site’s
fundamental organization.

To take advantage of this observation, we noted that URLs and
table structure can both be viewed as trees, and this facilitated cer-
tain machine learning algorithms. These learning techniques try to
ﬁnd correlations between certain properties (i.e., this document is
an ad) with the document’s location in either the URL or table tree.
We argued that our new features and learning would produce
fast, good classiﬁcation schemes. We gave empirical results on two
Web applications: an ad-blocker and a recommendation system.
We showed that the ad-blocker could achieve commercial-grade
accuracy without requiring any human inputs. The recommenda-
tion results showed that our tree-learning approach outperformed
conventional techniques and features on real world news recom-
mendation data.

Our tree-based algorithms exhibit a well-recognized tradeoff be-
tween speciﬁcity and accuracy. The text-based classiﬁers we com-
pared our work to are very general: the set of words that character-
izes documents “interesting” to a given user is not (very) speciﬁc
to any particular site. Our URL classiﬁer is site speciﬁc—what it
learns about the URLs of ads on one site will not generalize to other
sites. Thus, it needs to train separately on each site, and will make
no useful recommendations until such training is complete. Our
table-based classiﬁer is even more speciﬁc, as it focuses on the lay-
out of a speciﬁc page, and requires training data speciﬁcally from
that page in order to make recommendations.

7. ACKNOWLEDGMENTS

This work was supported by the MIT Oxygen Partnership. We
thank the early users of the system for their feedback on the system,
Mike Masnick for his remarks on user interface, Mark Ackerman
for his help editing earlier versions of this document and Leslie
Kaelbling and Jason Rennie for their help on the machine learning
aspects of the system.

8. REFERENCES
[1] R. Agrawal and R. Srikant. On integrating catalogs. In

Proceedings of 10th Intl. Conference on the World Wide Web,
pages 603–612, Hong Kong, CN, 2001. ACM Press, New
York, US.

[2] C. R. Anderson and E. Horvitz. Web montage: a dynamic

personalized start page. In Proceedings of the Eleventh Intl.
Conference on World Wide Web, pages 704–712. ACM Press,
2002.

[3] R. Barzilay, N. Elhadad, and K. R. McKeown. Inferring
strategies for sentence ordering in multidocument news
summarization. Journal of Artiﬁcial Intelligence Research,
17:35–55, 2002.

[4] D. Billsus and M. J. Pazzani. A hybrid user model for news

story classiﬁcation. In Proceedings of the Seventh Intl.
Conference on User Modeling, pages 99–108.
Springer-Verlag New York, Inc., 1999.

[5] S. Brin and L. Page. The anatomy of a large-scale

hypertextual Web search engine. Computer Networks and
ISDN Systems, 30(1–7):107–117, 1998.

[6] C. J. C. Burges. A tutorial on support vector machines for

pattern recognition. Data Mining and Knowledge Discovery,
2(2):121–167, 1998.

[7] S. Chakrabarti, K. Punera, and M. Subramanyam.

Accelerated focused crawling through online relevance
feedback. In Proceedings of the Eleventh Intl. Conference on
World Wide Web, pages 148–159. ACM Press, 2002.

[8] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene

Analysis. Wiley and Sons, Inc., 1973.

[9] D. Haussler. Quantifying inductive bias: AI learning

algorithms and Valiant’s learning framework. Artiﬁcial
Intelligence, 36(2):177–221, Sept. 1988.

[10] M. Hollander and D. A. Wolfe. Nonparametric Statistical

Methods. John Wiley and Sons, 1973.

[11] T. Joachims. A probabilistic analysis of the rocchio

algorithm with tﬁdf for text categorization. In Proceedings of
the Fourteenth Intl. Conference on Machine Learning, 1997.

[12] D. Koller and M. Sahami. Hierarchically classifying

documents using very few words. In Proceedings of the 14th
Intl. Conference on Machine Learning, pages 170–178, 1997.

[13] N. Kushmerick. Learning to remove internet advertisement.

In O. Etzioni, J. P. M¨uller, and J. M. Bradshaw, editors,
Proceedings of the Third International Conference on
Autonomous Agents (Agents’99), pages 175–181, Seattle,
WA, USA, 1999. ACM Press.

[14] N. Kushmerick, D. S. Weld, and R. B. Doorenbos. Wrapper

induction for information extraction. In Intl. Joint
Conference on Artiﬁcial Intelligence (IJCAI), pages
729–737, 1997.

[15] A. K. McCallum, R. Rosenfeld, T. M. Mitchell, and A. Y.

Ng. Improving text classiﬁcation by shrinkage in a hierarchy
of classes. In J. W. Shavlik, editor, Proceedings of the 15th
Intl. Conference on Machine Learning, pages 359–367,
Madison, US, 1998. Morgan Kaufmann Publishers, San
Francisco, US.

[16] A. Y. Ng and M. I. Jordan. On discriminative vs. generative

classiﬁers: A comparison of logistic regression and naive
bayes. In Advances in Neural Information Processing
Systems 14 (NIPS*01), 2002.

[17] M. J. Pazzani and D. Billsus. Learning and revising user

proﬁles: The identiﬁcation of interesting web sites. Machine
Learning, 27(3):313–331, 1997.

[18] J. Rennie and A. K. McCallum. Using reinforcement learning

to spider the Web efﬁciently. In I. Bratko and S. Dzeroski,
editors, Proceedings of ICML-99, 16th International
Conference on Machine Learning, pages 335–343, Bled, SL,
1999. Morgan Kaufmann Publishers, San Francisco, US.

[19] R. Rifkin. Svmfu. http://ﬁve-percent-nation.mit.edu/SvmFu/,

2000.

[20] L. K. Shih. Machine Learning of Web Documents. PhD

thesis, Massachusetts Institute of Technology, Feb. 2004.

202