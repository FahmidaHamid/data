Newsjunkie: Providing Personalized Newsfeeds via

Analysis of Information Novelty

∗

Evgeniy Gabrilovich

CS Department

Technion

32000 Haifa, Israel

gabr@cs.technion.ac.il

Susan Dumais
Microsoft Research
One Microsoft Way

Redmond, WA 98052, USA
sdumais@microsoft.com

Eric Horvitz

Microsoft Research
One Microsoft Way

Redmond, WA 98052, USA
horvitz@microsoft.com

ABSTRACT
We present a principled methodology for ﬁltering news stories by
formal measures of information novelty, and show how the tech-
niques can be used to custom-tailor newsfeeds based on informa-
tion that a user has already reviewed. We review methods for an-
alyzing novelty and then describe Newsjunkie, a system that per-
sonalizes news for users by identifying the novelty of stories in the
context of stories they have already reviewed. Newsjunkie em-
ploys novelty-analysis algorithms that represent articles as words
and named entities. The algorithms analyze inter- and intra- docu-
ment dynamics by considering how information evolves over time
from article to article, as well as within individual articles. We re-
view the results of a user study undertaken to gauge the value of
the approach over legacy time-based review of newsfeeds, and also
to compare the performance of alternate distance metrics that are
used to estimate the dissimilarity between candidate new articles
and sets of previously reviewed articles.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval—information ﬁltering; H.3.4 [Information Storage
and Retrieval]: Systems and software—user proﬁles and alert ser-
vices; H.3.1 [Information Storage and Retrieval]: Content Anal-
ysis and Indexing—linguistic processing; G.3 [Probability and
Statistics]: distribution functions, experimental design

General Terms
Algorithms, Experimentation, Human Factors

Keywords
News, novelty detection, personalization

1.

INTRODUCTION

Just a decade ago, large-scale ﬂows of information such as news-
feeds were owned, monitored, and ﬁltered by organizations special-
izing in the provision of news. The Web has brought the challenges
and opportunities of managing and absorbing newsfeeds to all inter-
ested users. We have pursued mathematical tools and user-interface
∗The research described herein was conducted while the ﬁrst author
was on a summer internship with Microsoft Research, during the
summer of 2003.
Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, New York, USA.
ACM 1-58113-844-X/04/0005.

designs that can assist people with extracting the most relevant in-
formation from news sources. One approach to navigating a large
corpus of news information is to identify differences and similari-
ties between various decompositions of the collections. Such dif-
ferencing machinery could be useful for distinguishing what is be-
ing said for the ﬁrst time from what has already been mentioned, as
well as in revealing differences of opinion on the issues covered.

Identifying “important” information has been an essential aspect
of studies on Web search and text summarization. Search methods
focus on identifying a set of documents that maximally satisﬁes
a user’s acute information needs. Summarization strives at com-
pressing large quantities of text into a more concise formulation. In
the absence of automated methods for identifying the deep seman-
tics associated with text, prior work in summarization has typically
operated at the level of complete sentences, weaving together the
most representative sentences to create a document summary. Re-
search on search and summarization has generally overlooked the
dynamics of informational content arriving continuously over time.
We shall present methods for identifying information novelty
and show how these methods can be applied to manage content
that evolves over time. We start by describing a general framework
for comparing collections of documents. We assume documents
are organized into groups by their content or source, and analyze
inter-group and intra-group differences and commonalities. Jux-
taposing two groups of documents devoted to the same topic but
derived from two distinct sources (e.g., news coverage of an event
in different parts of the world) can reveal interesting differences
of opinions and overall interpretations of situations. Moving from
static collections to sets of articles generated over time, we can
examine the evolution of content. For example, we can seek to ex-
amine a stream of news articles evolving over time on a common
story, with the goal of highlighting truly informative updates and
ﬁltering out a large mass of articles that largely relay “more of the
same.”

In contrast to prior work in text summarization, we work at the
level of individual words rather than entire sentences. Working at
this ﬁner resolution, we gather detailed statistics on word occur-
rence across sets of documents in order to characterize differences
and similarities among these sets. We further enhance the simple
bag of words model by extracting named entities that denote names
of people, organizations, and geographical locations. In contrast to
phrases and collocations—whose discriminative semantic proper-
ties are usually outweighed by lack of sufﬁcient statistics—named
entities identify relatively stable tokens that are used in a common
manner by many writers on a given topic, and so their use con-
tributes a considerable amount of information. In fact, one type of
analysis we describe below represents articles using only the named

482entities found in them. We found that, for some topics, this anal-
ysis was comparable or even superior in performance to methods
that manipulate the full bag of words.

We will focus on the analysis of live streams of news. Live news
streams pose tantalizing challenges and opportunities for research.
Newsfeeds span enormous amounts of data, present a cornucopia
of opinions and views, and include a wide spectrum of formats and
content from short updates on breaking news, to major recaps of
story developments, to mere reiterations of “the same old facts”
reported over and over again. We describe algorithms that identify
signiﬁcant updates on the stories being tracked, relieving the users
from having to sift through long lists of similar articles arriving
from different news sources. The methods provide the basis for
personalized news portal and news alerting services that promise
to minimize the time and disruptions to users who wish to follow
evolving news stories.

The contributions of this paper are threefold. First, we present a
framework for identifying differences in sets of documents by an-
alyzing the distributions of words and recognized named entities.
This framework can be applied to compare individual documents,
sets of documents, or a document and a set (for example, a new
article vs.
the union of previously reviewed news articles on the
topic). Second, we present a collection of algorithms that operate
on live news streams and provide users with a personalized news
experience. These algorithms are implemented in a system named
Newsjunkie that presents users with maximally informative news
updates. Users can request updates per every user-deﬁned period
or per each burst of reports about a story. Users can also tune the
desired degree of relevance of these updates to the core story, al-
lowing delivery of offshoot articles that report on related or similar
stories. Finally, we describe an evaluation method which presents
users with a single seed story and sets of articles ranked by differ-
ent novelty-assessing metrics, and seeks to understand how partic-
ipants perceive the novelty of these sets in the context of the seed
story. As we shall discuss, the results of this study highlight sev-
eral interesting issues such as what is considered a story “on a given
topic,” how people judge novelty, and how their judgments are in-
ﬂuenced by the relevance of the information being read to the topic
deﬁned by the seed story.

The remainder of this paper is organized as follows.

In Sec-
tion 2, we review related research in language modeling, text sum-
marization and topic detection and tracking (TDT). In Section 3,
we present a framework for comparing text collections, and dis-
cuss its applications to ﬁnding differences both between and within
groups of documents. We report the results of experiments with
users in Section 4.
In Section 5, we describe two new types of
document analyses we developed that allow users to personalize
the frequency and content of the news updates that they receive.
Finally, we discuss future research directions in Section 6.

2. BACKGROUND AND RELATED WORK
The AT&T Internet Difference Engine (AIDE) [9] was one of
the earliest attempts to develop a tool for comparing the content of
material drawn from the Internet. The HtmlDiff system built in the
course of this project looked for simple syntactic differences be-
tween Web pages similarly to what the Unix diff utility does; how-
ever, no attempt was made to capture semantic differences between
the pages.

In an effort to characterize content differences beyond simple
syntactic variations, several past studies focused on identifying
words that are particularly characteristic of a given text. Kilgar-
riff [15] presents a good summary of these studies as well as their

potential applications to characterizing text genres and differences
in male and female speech.

Comparing distributional properties of individual words natu-
rally evolved into comparing language models for entire text col-
lections, which was found useful in two ways. In corpus linguistics,
researchers studied to what degree reasoning about one text collec-
tion can be based on the model computed for another collection
[15]. In the realm of information retrieval, comparing the language
models for documents returned by a query and the entire collection
was found to be valuable for predicting query quality (i.e., how well
the query is formulated and how focused the results are expected to
be) [6, 7].

In this work we reason about the similarity of document sets by
comparing their language models using several distance metrics.
Another way to compare document sets is to represent the docu-
ments as bags of words, and use established word similarity metrics
that sum over pairwise distances between individual words. Prior
work on semantic word similarity developed in two major direc-
tions, either using purely statistical analysis such as Latent Seman-
tic Indexing (LSI) [8] and Hyperspace Analog to Language (HAL)
[2], or capitalizing on background knowledge resources such as
WordNet [20]. The two approaches have also been combined in a
single framework [11]. Lee [17] investigated measures of distribu-
tional word similarity by using language models similar to ours to
improve probability estimations for unseen word cooccurrences.

We seek to order news by novelty in an iterative manner, attempt-
ing at each step to identify the article that carries the maximum
amount of new information, in the context of a background model
composed from content that has been already reviewed. This ap-
proach is conceptually related to the notion of Maximum Marginal
Relevance (MMR) [3], developed in the context of information re-
trieval. When selecting the next document to be returned in re-
sponse to a query, the MMR criterion prefers relevant documents
that are maximally different from documents that have been se-
lected before.

Research on Topic Detection and Tracking (TDT) has led to in-
vestigations of a variety of problems related to novelty detection.
Yang et al. [27] studied a speciﬁc form of novelty detection, namely
First Story Detection (FSD). The authors used a Rocchio-style text
classiﬁer [26] to classify documents into predeﬁned broad topics,
and then measured the novelty of new documents given the top-
ics predicted for them. Documents were represented as vectors of
words and named entities weighted with a TF.IDF scheme [22].

Working in the domain of news, Swan and Jensen [24] automat-
ically generated timelines from historic date-tagged news corpora
(TDT). They used a χ2 test to identify days on which the number
of occurrences of a given word or phrase exceed some (empirically
determined) threshold, and then generated timelines by grouping
together contiguous sequences of such days.

Kleinberg [16] used randomized inﬁnite-state automata to model
burstiness and hierarchical structure in text streams. This work was
not speciﬁcally focused on news, but experimented with other kinds
of time-stamped text corpora such as personal email archives and
conference proceedings spanning a number of years. Sample ap-
plications of this formal model include identiﬁcation of increased
activity bursts in email trails, as well as computing the most impor-
tant words in conference paper titles over different time periods. In
contrast to [24], this approach zooms into the bursts to determine
their hierarchical structure, identifying short, intense bursts within
longer but weaker ones.

Assessing the novelty of a quantity of text is related to analyzing
its most informative sentences. The latter task has been the fo-
cus of research in extractive text summarization, which combines

483most important sentences of a collection of documents to produce
a summary. A study performed in the course of the Columbia
Newsblaster project [23] identiﬁed key sentences by looking for
importance-signaling words and high-content verbs obtained by an-
alyzing large news corpora, as well as ﬁnding dominant concepts
by consulting WordNet [10]. Allan et al. [1] computed usefulness
and novelty measures at the sentence level by developing proba-
bilistic models for news topics and events. Collins-Thompson et al.
[4] selected sentences by using a C4.5 [19] classiﬁer with a set of
surface and semantic sentence features; they also compared pairs
of sentences to check if one of them is a statistical translation of
the other.

In contrast to prior research, our work focuses on developing a
system that manipulates live newsfeeds and offers users personal-
ized updates that are maximally novel in the context of information
the user has reviewed before.

3. A FRAMEWORK FOR COMPARING

TEXT COLLECTIONS

Given two sets of textual content, how can we characterize the
differences between them? Answering this question is useful in a
variety of applications, including automatic proﬁling and compari-
son of text collections, automatic identiﬁcation of different views,
scopes and interests reﬂected in the texts, and automatic identiﬁca-
tion of novel information.

In general, several aspects of “difference” may be investigated:
• Differences in content may reﬂect the different ways a par-
ticular person or event is described in two sets of documents.
For example, consider analyzing differences in predeﬁned
partitions, e.g., comparing US vs. European reports on var-
ious political issues, or comparing the coverage of a recent
blackout of the East Coast of the United States in the news
coming from sources based in the East Coast and West Coast.
• Differences in structural organization may go well beyond
text and also consider link structure of Web sites, e.g., com-
paring IBM Web site vs. Intel Web site.

• Differences in time (i.e., temporal aspects of content differ-
ences) can reveal interesting topical changes in series of doc-
uments. This kind of analysis can be used to compare today’s
news vs. the news published a month or a year ago, to track
changes in search engine query logs over time, or to identify
temporal changes in topics in users’ personal email.

Temporal differences are a particularly interesting case, and in
this paper we focus on automatically assessing the novelty over
time of news articles coming from live newsfeeds. Speciﬁcally,
we formulated the following two research challenges:

1. Characterization of novelty in news stories, which allows us
to order news articles so that each article adds maximum in-
formation to the (union of) previously read ones.

2. Studying topic evolution over time, which enables us to quan-
tify the importance and relevance of news updates, granting
end users control over these parameters and offering them a
personalized news experience.

In the remainder of this section, we outline a methodology for
analyzing newsfeeds and describe an algorithm for ranking news
articles by predicting the amount of novel information they carry.
The results of an empirical evaluation of this algorithm are reported
in the next section. Section 5 studies topic evolution over time and
proposes a new approach to analyzing different types of articles.

3.1 Methodology

We developed a software toolset named Newsjunkie that imple-
ments a collection of algorithms and a number of visualization op-
tions for comparing text collections. Newsjunkie represents doc-
uments as bags of words augmented with named entities extracted
from the text. In-house extraction tools were used for this purpose,
which identiﬁed names of people, organizations and geographical
locations.

Document groups contain documents with some common prop-
erty, and constitute the basic unit of comparison. Examples of such
common properties can be a particular topic or source of news (e.g.,
blackout stories coming from the East Coast news agencies). We
draw inferences about the differences between document groups by
ﬁrst building a language model for each group, and then comparing
the models using some similarity metric (see Section 3.2). To fa-
cilitate exploring a variety of language models, Newsjunkie repre-
sents documents either as smoothed probability distributions1 over
all the features (words + named entities), or as vectors of TF.IDF
weights [22] (in the same feature space).
3.2 Ranking news by novelty

Let us imagine the common situation where something interest-
ing happens in the world, and the event is picked by the news me-
dia. If the event is of sufﬁcient public interest, the ensuing devel-
opments are tracked in the news as well. Suppose you have read
an initial report and, at some later time, are interested in catching
up with the story. In the presence of Internet sites that aggregate
thousands of news sources such as Google News or Yahoo! News2,
your acute information-seeking goal can be satisﬁed in many ways
and with many more updates than even the most avid news junkie
has the time to review. Automated tools for sifting through a large
quantity of documents on a topic that work to identify nuggets of
genuinely new information can provide great value.

Avoiding redundancy and overlap can help minimize the over-
head associated with tracking news stories. There is a great deal of
redundancy in news stories. For example, when new developments
or investigation results are expected but no new information is yet
available, news agencies often ﬁll in the void with recaps of earlier
developments until new information is available. The situation is
further aggravated by the fact that many news agencies acquire part
of their content from major multi-national content providers such
as Reuters or Associated Press. Users of news sites do not want
to read every piece of information over and over again. Users are
primarily interested in learning what’s new. Thus, ordering news
articles by novelty promises to be useful.

We use a number of document similarity metrics to identify arti-
cles that are most different from the union of those read previously.3

respectively.

Then,

We implemented the following distance metrics:
• Kullback-Leibler (KL) divergence [5], a classical asymmet-
ric information-theoretic measure. Assume we need to com-
pute the distance between a document d and a set of doc-
uments R. Let us denote the probabilistic distributions of
words (and named entities if available) in d and R by pd and
pR,
=
1Two smoothing options were implemented, using either Laplace’s
law of succession [21] or linear smoothing with word probabilities
in the entire text collection [18]; the latter option was used through-
out the experiments reported in this paper.
2http://news.google.com and
http://dailynews.yahoo.com, respectively.
3In what follows, we use the term distance metrics to emphasize
the fact that we are actually looking for documents that are most
dissimilar from documents reviewed earlier.

distKL(pd, pR)

484(cid:80)

w∈words({d}∪R) pd(w) log pd(w)

pR(w) . Note that the compu-
tation of log pd(w)
pR(w) requires both distributions to be smoothed
to eliminate zero values (corresponding to words that appear
in d but not in R, or vice versa).

• Jensen-Shannon (JS) divergence [5], a symmetric variant of
the KL divergence. Using the deﬁnitions of the previous
item, distJS(pd, pR) = distKL(pd,q)+distKL(pR,q)
, where
q = pd+pR

2

.

2

• Cosine of vectors of raw probabilities (computation does not

require smoothed probabilities).

• Cosine of vectors of TF.IDF feature weights.
• A metric we formulated to measure the density of previ-
ously unseen named entities in an article (referred as NE).
The intuition for this metric is based on our conjecture that
novel information is often conveyed through the introduc-
tion of new named entities, such as the names of people,
organizations, and places. Using the notation of Figure 1,
the NE metric can be deﬁned as follows: Let N E(R) be a
set of named entities found in a set of documents R. Let
N Eu(R1, R2) be a set of unique named entities found in the
set of documents R1 and not found in the set R2. That is,
N Eu(R1, R2) = {e|e ∈ N E(R1) ∧ e /∈ N E(R2)}. Then,
distN E(d, R) = N Eu({d}, R)/length(d). Normalization
by document length is essential, as, without normalization
the NE score will tend to rise with length, because of the
probabilistic inﬂuence of length on seeing additional named
entities; the longer the document is, the higher the chance it
contains more named entities.

These distance metrics can be harnessed to identify novel content
for presentation to users. In the Newsjunkie application, we apply
the novelty ranking algorithm iteratively to produce a small set of
articles that a reader will be interested in. We employ a greedy, in-
cremental analysis. The algorithm initially compares all the avail-
able updates to the seed story that the user has read, and selects the
article least similar to it. This article is then added to the seed story
(forming a group of two documents), and the algorithm looks for
the next update most dissimilar to these articles combined, and so
on. The pseudocode for the ranking algorithm is outlined in Fig-
ure 1.

Algorithm RANKNEWSBYNOVELTY(dist, seed, D, n)

R ← seed // initialization
for i = 1 to min(n,|D|) do

d ← argmaxdi∈D{dist(di, R)}
R ← R ∪ {d}; D ← D \ {d}

where dist is the distance metric, seed – seed story, D – a set of
relevant updates, n – the desired number of updates to select,
R - list of articles ordered by novelty.

Figure 1: Ranking news by novelty.

4. EMPIRICAL EVALUATION

To validate the algorithm presented in the previous section, we
conducted an experiment that asked subjects to evaluate sets of
news articles ordered by a variety of distance metrics.

4.1 Data

For the experiments described herein we used a live newsfeed
supplied by Moreover Technologies4, who aggregates news articles
from over 4000 Internet sources. A simple clustering algorithm was
used to group stories discussing the same events (called topics in
the sequel). We used 12 clusters that correspond to topics reported
in the news in mid-September 2003. The 12 topics covered news
reports over a time span of 2 to 9 days, and represented between
36 and 328 articles. Topics included coverage of a new outbreak of
SARS in Singapore, the California governor recall, the Pope’s visit
to Slovenia, etc. (Table 1 shows the full list of topics).
4.2 Description of the evaluation procedure

Judging novelty is a subjective task. One way to obtain statis-
tically meaningful results is to average the judgments of a set of
users. In order to compare different novelty-ranking metrics, we
asked participants to read several sets of articles ordered by alter-
nate metrics, and to decide which sets carried the most novel in-
formation. Note that this scenario requires the evaluators to keep
in mind all the article sets they read until they rate them. Because
it is difﬁcult to keep several sets of articles on an unfamiliar topic
in memory, we limited our experiment to evaluating the following
three metrics:

1. The KL divergence was selected due to its appealing informa-

tion-theoretic basis (KL).

2. The metric counting named entities was selected as a linguis-

tically motivated alternative (NE).

3. The chronological ordering of articles was used as a baseline

(ORG).

For each of the 12 topics, we selected the ﬁrst story as the seed
story, and used the three metrics described above to order the rest
of the stories by novelty using the algorithm RANKNEWSBYNOV-
ELTY (Figure 1). The algorithm ﬁrst selects the most novel article
relative to the seed story. This article is then added to the seed
story to form a new model of what the user is familiar with, and
the next most novel article selected. Three articles were selected in
this manner for each of the three metrics and each of the 12 topics.
For each topic, the subjects were ﬁrst asked to read the seed story
to get background about the topic. They were then shown the three
sets of articles (each set chosen by one of the metrics), and asked
to rate the sets from most novel to least novel set. They were in-
structed to think of the task as identifying the set of articles that
they would choose for a friend who had reviewed the seed story,
and now desired to learn what was new. The presentation order
of the sets generated by the three metrics was randomized across
participants.

Originally, we had quite a few reservations about whether such
an evaluation procedure would be feasible at all. Not only had the
users to bear in mind quite a lot of information before pronounc-
ing their decision, but the situation was further complicated by the
varying relevance of articles to the seed story (we discuss this issue
in detail in Section 5.3). The procedure described above was re-
ﬁned through a series of calibration experiments, in which we tried
several techniques for eliciting judgments, and then thoroughly de-
briefed the subjects after each experiment. One notable alternative
4http://www.moreover.com.
Another possible option
would have been to use standard Topic Detection and Tracking
(TDT) datasets used in TREC experiments. However, as TDT data
dates back to 1998–99, we thought that current news stories from
Moreover Technologies’ feeds would be more engaging for the vol-
unteers evaluating our system in action.

485Topic id

Topic description

topic1
topic2
topic3
topic4
topic5
topic6
topic7
topic8
topic9
topic10
topic11
topic12

Pizza robbery
RIAA sues MP3 users
Sharon visits India
Pope visits Slovakia
Swedish FM killed
Al-Qaeda
CA governor recall
MS bugs
SARS in Singapore
Iran develops A-bomb
NASA investigation
Hurricane Isabel

#times most novel
KL
ORG
5
2
2
9
5
8
4
3
7
3
2
4

NE
4
7
3
0
4
1
2
5
1
5
5
5

1
0
4
0
0
0
3
1
1
2
3
0

Mean rank

KL
1.7
1.8
2.6
1.0
1.4
1.1
1.7
1.9
1.3
2.2
2.1
1.9

NE
1.6
1.2
1.7
2.2
1.6
2.1
2.2
1.6
2.0
1.7
1.6
1.6

ORG
2.7
3.0
1.8
2.8
3.0
2.8
2.1
2.6
2.7
2.1
2.3
2.6

Figure 2: Cumulative results.

would be to present the users with pairs of next articles, and ask
them to give relative judgements as to which of the two articles
carries more novel information given the seed story. We chose the
set-based approach because it was more similar to the scenario we
wanted to support, namely, given one article, to ﬁnd a set of new ar-
ticles to read. Furthermore, pairwise tests would also have involved
more reading load for the participants.
4.3 Experimental results

Overall we obtained 111 user judgments on 12 topics, averaging
9–10 judgments per topic. Figure 2 shows the number of times
each metric was rated the most, medium and least novel. As can be
readily seen from the graph, the sets generated by the KL and NE
metrics were rated more novel than those produced by the baseline
metric (ORG).

Table 1 presents per-topic results. The three penultimate co-
lumns show the number of times each metric was rated the most
novel for each topic. The last three columns show mean ranks of
the metrics, assuming the most novel is assigned the rank of 1,
medium novel – 2, and least novel – 3.

We used Wilcoxon Signed Ranks Test5 [25] to assess the statisti-
cal signiﬁcance of experimental results. Comparing the mean ranks
of metrics across all the topics (as summarized in Figure 2), both
KL and NE were found superior to ORG at p < 0.001. Consider-
ing individual per-topic results, the ORG metric never achieved the
lowest (= best) rank of all three metrics. In 6 cases (topics 2, 4, 5,
6, 9, 12), the difference in mean rank between ORG and the lowest-
scoring metric was statistically signiﬁcant at p < 0.05, and in one
additional case the signiﬁcance was borderline at p = 0.068 (topic
8). Comparing the two best metrics (KL vs. NE), the difference
in favor of KL was statistically signiﬁcant at p < 0.05 for topics
4 and 6, and borderline signiﬁcant (p = 0.083) for topic 9. The
difference in mean ranks in favor of NE was borderline signiﬁcant
for topics 2 and 3 (p = 0.096 and p = 0.057, respectively).

The experiment we conducted should be considered a prelimi-
nary one as it only involved 12 different topics. Nevertheless, it
allowed us to verify our evaluation procedure, as well as to observe
the superiority of the two metrics tested (KL and NE) over the base-
line (ORG). Based on these results, we do not observe signiﬁcant
difference in performance between KL and NE. However, we be-

5This nonparametric test is used as an alternative to the familiar
paired t-test when the underlying distribution cannot be assumed
to be normal.

Table 1: Results by topic.

lieve that these preliminary results warrant further investigation of
the potential content-sensitivity of the value of using KL versus
NE metrics. In our future research we will attempt to characterize
the properties of collections of articles on topics that could indicate
where each metric performs the best for users.

5. PERSONALIZED NEWS UPDATES

Algorithm RANKNEWSBYNOVELTY presented and evaluated in
the previous section works under the assumption that a user wants
to catch up with latest story developments some time after initially
reading about it. In this case the algorithm orders the recent articles
by their novelty compared to the seed story, and then the user can
read a number of highest-scoring articles depending on how much
spare time he or she can allocate for the reading.

But what if the user wants to be updated continuously as the new
developments actually happen? Some logistic support is needed
to constantly keep track of the articles the user reads in order to
estimate the novelty of the new articles streaming in the news-
feed. Based on user’s personal preferences, that is, how often the
user is interested in getting updates on the story, the server decides
which articles to display. Therefore, an online decision mechanism
is needed that determines whether any article contains sufﬁciently
new information to warrant its delivery to the user. In a more gen-
eral analysis of the beneﬁts versus the costs of alerting, there are
opportunities to balance the informational value of particular arti-
cles or groups of articles with the cost of interrupting users, based
on a consideration of their context [14].

In what follows, we examine two scenarios of updating users
with current news. The ﬁrst scenario (discussed in Section 5.1)
assumes the user is interested in getting updates once a day, while
the second scenario (Section 5.2) updates the user continuously by
monitoring incoming news for bursts of novel information. Finally,
Section 5.3 introduces a mechanism that allows users to control the
degree of relevance of the articles they are about to read.
5.1 Picking a single daily update

Let us ﬁrst consider the simpler case when the user wants to see

no more than a single daily update on the story.

One way to achieve this aim would be to use an algorithm sim-
ilar to RANKNEWSBYNOVELTY, that is, accumulate the stories
received on all the preceding days, and assess the novelty of each
new story that arrived today by computing its distance from the ac-
cumulated set. The main problem with this approach is that the
more stories are pooled, the less signiﬁcant becomes the distance
from any new story to the pool. After several days worth of articles
have been accumulated, even a major update will be seen as barely
new.

01020304050607080KLNEORG# times mostnovel# times mediumnovel# times leastnovel 486Algorithm PICKDAILYUPDATE(dist, Bg, D, thresh)

d ← argmaxdi∈D{dist(di, Bg)}
if dist(d, Bg) > thresh then display(d)
Bg ← D

where dist is the distance metric, Bg – the background reference
set (union of all the relevant articles received on the preceding
day), D – a set of new articles received today, thresh – user-
deﬁned sensitivity threshold.

Figure 3: Picking daily updates.

To avoid this pitfall, we modify our original algorithm as shown
in Figure 3. Given the user and her choice of the topic to track,
algorithm PICKDAILYUPDATE compares the articles received to-
day with the union of all the articles received the day before. The
algorithm picks the most informative update compared to what was
known yesterday, and shows it to the user, provided that the update
carries enough new information (i.e., its estimated novelty is above
the user’s personalized threshold). Such conditioning endows the
system with the ability to relay to the user truly informative updates
and to ﬁlter out articles that only recap previously known details.
The algorithm can be trivially generalized to identify n most infor-
mative updates per day.

It could be argued that by ignoring all the days before the imme-
diately preceding one, algorithm PICKDAILYUPDATE might also
consider novel those articles that recap what was said several days
ago. In practice this rarely happens, as most of the articles are writ-
ten in the way that interleaves new information with some back-
ground on previous developments. In our future work we plan to
consider more elaborate distance metrics, that consider all previous
articles relevant to the topic but decay their weight with age.

5.2 Reporting breaking news

The algorithm presented in Section 5.1 is still largely an “ofﬂine”
procedure, as it updates users at predeﬁned time intervals. Hard-
core news junkies might ﬁnd it frustrating to wait for daily sched-
uled news updates! For some, a more responsive form of analysis
may be desired.

Taking the previous idea to the extreme and comparing every ar-
ticle to the preceding one will not work well, as the system will pre-
dict nearly every article as novel. Instead, we use a sliding window
covering a number of preceding articles to estimate the novelty of
the current one. Observe that estimating distances between articles
and a preceding window of ﬁxed-length facilitates the comparison
of scores. We evaluated different window lengths of 20–60 articles.
We found that lengths of approximately 40 typically worked well
in practice.

In contrast to the algorithm PICKDAILYUPDATE, the background
reference set now becomes much shorter, namely, 40 articles in-
stead of a full day’s content. This increases the likelihood that the
window is not long enough to cover delayed reports and recaps that
follow long after the story was initially reported. In order to ﬁlter
out such repetitions, we ﬁrst need to better understand the nature of
news reports.

When an event or information update about an event of impor-
tance occurs, many news sources pick up the new development and
report it within a fairly short time frame. If we successively plot
the distance between each article and the preceding window, such
arrival of new information will result in peaks in the graph. We
call such peaks a burst of novelty. At the beginning of each burst,
additional articles tend to add new details causing the graph to rise.
As time passes, the sliding window covers more and more articles

Algorithm IDENTIFYBREAKINGNEWS(dist, D, l, f w, thresh)

i=1 di ∈ D

W indow ←(cid:83)l

for i = l + 1 to |D| do

Scoresi ← dist(di, W indow)
W indow ← (W indow \ di−l) ∪ di

Scoresf ilt ← M edianF ilter(Scores, f w)
for j = 1 to |Scoresf ilt| do

if Scoresf ilt

j > thresh then

display(dj+l)
skip to the beginning of the next burst

where dist is the distance metric, D – a sequence of relevant
articles, l – sliding window length, f w – median ﬁlter width,
thresh – user-deﬁned sensitivity threshold.

Figure 4: Identifying breaking news.

conveying this recent development and the following articles do
not have the same novelty; as a result, the computed novelty heads
downward signifying the end of the burst.

Delayed reports of events as well as recaps on a story are less
likely to be correlated in time between different sources. Such re-
ports may appear novel compared to the preceding window, but
since they are usually isolated they only cause narrow spikes in the
graph. In order to discard such standalone spikes and not to ad-
mit them as genuine updates, we need to ﬁlter the novelty signal
appropriately.

The median ﬁlter [12] provides exactly this functionality by re-
ducing the amount of noise in the signal. The ﬁlter successively
considers each data point in the signal and adapts it to better re-
semble its surroundings, effectively smoothing the original signal
and removing outliers. Speciﬁcally, a median ﬁlter of width w ﬁrst
sorts the w data points within the window centered on the current
point, then replaces the latter with the median value of these points.
After computing the distance between every article and a sliding
window covering the preceding ones, we pass the resultant signal
through a median ﬁlter. We considered ﬁlters of width 3–7; the
ﬁlter of width 5 appears to work well in the majority of cases.

We note that the use of a median ﬁlter may delay the routing of
novel articles to users, as we need to consider several following ar-
ticles to reliably detect the beginning of a new burst. However, we
found that such delays are rather small (half the width of the me-
dian ﬁlter used), and the utility of the ﬁlter more than compensates
for this inconvenience. If users are willing to tolerate some addi-
tional delay, the algorithm can scan forward several dozens of arti-
cles from the moment a burst is detected, in order to select the most
informative update instead of simply picking the one that starts the
burst. Combination approaches are also feasible such as the ren-
dering of an early update on breaking news, and then waiting for a
more informed burst analysis to send the best article on the devel-
opment. Figure 4 shows the pseudocode the algorithm IDENTIFY-
BREAKINGNEWS that implements burst analysis for news alerting.

Figure 5 shows the application of the algorithm IDENTIFYBREAK-

INGNEWS to a sample topic. The topic in question is devoted to a
bank robbery case in Erie, Pennsylvania, USA, where a group of
criminals apparently seized a pizza delivery man, locked a bomb
device to his neck and, according to statements made by the deliv-
ery man, forced him to rob a local bank. The man was promptly
apprehended by police, but soon afterwards the device detonated
and killed him. The bizarre initial story and ensuing investiga-
tion were tracked by many news sources for some two weeks in
September 2003. The x-axis of the ﬁgure corresponds to the se-
quence of articles as they arrived in time, and the y-axis plots (raw
and median-ﬁltered) distance values for each article given the pre-

487sion, but the new information they add is sufﬁciently differ-
ent from that reported in the seed story to warrant a new trail.

4. Irrelevant articles are those due to clustering and parsing

mistakes.

Of these classes, relationship types 2 and 3 are probably what
most users want to see. But how can we identify them automati-
cally, and how can we empower the users themselves to exercise
control over this spectrum?

To achieve this aim we suggest a new type of document analysis
that scrutinizes intra-document dynamics. As opposed to previ-
ous kinds of analysis that compared entire documents to one an-
other, the new technique “zooms into” documents estimating the
relevance of their parts.

We start with building a language model for every document, and
ﬁx a distance metric to use, e.g., KL divergence. Then, for each
document, we slide a window over its words and plot the distance
scores of each such window versus the seed story. We construe the
score of a window of words as a sum of pointwise scores of each
word vs. the seed story, as stipulated by comparing the language
model of the current document with that of the seed story using the
selected metric. Several different window lengths were considered,
and the value of 20 was found to work well in practice.

An important property of this technique is that it goes beyond the
proverbial bag of words, and considers the document words in their
original context. We opted for using sliding contextual windows
rather than apparently more appealing paragraph units, since using
a ﬁxed-length window makes distance scores directly comparable.
Another obvious choice of the comparison unit would be individual
sentences. However, we believe that performing this analysis at the
sentence level would consider too little information, and the range
of possible scores would be too large to be useful. A recent study in
novelty detection [13] corroborates this reasoning — when the im-
portance of individual sentences (deemed relevant) is considered,
93% of them are classiﬁed as carrying novel information.

Figure 6 shows sample results of intra-document analysis. The
seed story for this analysis was a report on a new case of SARS
in Singapore. Articles that mostly recap what has already been
said typically have a very limited dynamic range and low absolute
scores. Elaboration articles usually have higher absolute scores
that reﬂect the new information they carry. One elaboration for this
story reported that the patient’s wife was being held under quar-
antine. Further along this spectrum, articles that may qualify as
offshoots but are still anchored to the events described in the seed
story have a much wider dynamic range. One offshoot was a story
that focused on the impact of SARS on the Asian stock market, and
another was on progress on a SARS vaccine. Both offshoot arti-
cles used the recent case as a starting point, but were really about a
related topic. We believe that analyzing intra-document dynamics
such as the dynamic range and patterns of novelty scores are useful
in identifying different types of information that readers would like
to follow.

6. CONCLUSIONS AND FUTURE WORK

The Web has been providing users with a rich set of news sources.
It is deceptively easy for Internet surfers to browse multitudes of
sources in pursuit of news updates, yet sifting through large quanti-
ties of news can involve the reading of large quantities of redundant
material.

We presented a collection of algorithms that analyze live news-
feeds and identify articles that carry most novel information given
a model of what the user has read before. To this end, we extend
the conventional bag of words representation with named entities

Dotted line represents raw distance scores, solid line —
median-ﬁltered scores.
Figure 5: Identifying breaking news — sample plot of raw and
ﬁltered novelty signals.

ceding sliding window. Raw distance scores are represented by a
dotted line, and ﬁltered scores are plotted with a solid line. The
text boxes accompanying the ﬁgure comment on the actual events
that correspond to the identiﬁed novelty bursts, and show which
potentially spurious peaks have been discarded by the ﬁlter. The
smoothed novelty score, which incorporates the median ﬁlter, does
a good job of capturing the main developments in the story (inter-
views with friends, details about the weapon, FBI bulletin for two
suspects, and a copycat case), while at the same time ﬁltering out
spurious peaks of novelty.
5.3 Characterization of article types

When we debriefed several users who completed the experiments
described in Section 4, several of them reported that it was difﬁcult
to judge the novelty of articles because of their varying relevance
to the seed story. In some cases this had to do with errors in the
tagging of the news stories by the newsfeed we relied upon, while
in one or two extreme cases a failure of the Moreover parser caused
grossly unrelated articles to be glued to the relevant ones. In other
cases the variance in relevance was due to the differences in writing
styles and policies among different publications.

These comments led us to believe that the novelty scores we
compute should not be relied upon as a sole selection criterion;
some articles are identiﬁed as novel by virtue of changing the topic.
To further reﬁne the analysis of informational novelty, we have for-
mulated a classiﬁcation of types of novelty, based on different re-
lationships between an article and a seed story. These classes of
relationships include:

1. Recap articles are those that are clearly relevant, but only
offer reviews of what has already been reported and carry
little new information.

2. Elaboration articles add new, relevant information on the

topic set forth by the seed article.

3. Offshoot articles are also relevant to the mainstream discus-

     Friends and neighbors believe delivery man is innocent Found gun disguised as a cane FBI looking for 2 people Repetitions [filtered] Completely unrelated story (tagging failure by news provider) [filtered] Copycat case in Missouri Repetitions [filtered] Articles in time Distance scores 488topic-focused discussions. Although technically possible, we be-
lieve that using these techniques for estimating the novelty of email
messages would be less useful. In personal email, information is
usually transmitted in an extremely compact way, so that even the
most novel piece of information may be conveyed in a single word
or only a few words. Therefore, much deeper language understand-
ing is required to adequately reﬂect the importance of information,
while more statistically-oriented approaches relying on a bag of
words might overlook such distinctions.

7. ACKNOWLEDGMENTS

We thank Uri Nodelman for ongoing discussions and construc-
tive comments. We are also grateful to Lucy Vanderwende, Mike
Calcagno and Kevin Humphreys for the assistance with the use of
tools for extracting named entities from text.

8. REFERENCES
[1] J. Allan, V. Khandelwal, and R. Gupta. Temporal summaries

of news topics. In Proceedings of the 24th International
Conference on Research and Development in Information
Retrieval, pages 10–18, 2001.

[2] C. Burgess, K. Livesay, and L. Kevin. Explorations in
context space: Words, sentences, discourse. Discourse
Processes, 25(2&3):211–258, 1998.

[3] J. Carbonell and J. Goldstein. The use of MMR,

diversity-based reranking for reordering documents and
producing summaries. In Proceedings of the 21st
International Conference on Research and Development in
Information Retrieval, pages 335–336, 1998.

[4] K. Collins-Thompson, P. Ogilvie, Y. Zhang, and J. Callan.

Information ﬁltering, novelty detection, and named-page
ﬁnding. In Proceedings of the 11th Text REtrieval
Conference. National Institute of Standards and Technology,
2002.

[5] T. Cover and J. Thomas. Elements of Information Theory.

John Wiley & Sons, 1991.

[6] S. Cronen-Townsend and W. B. Croft. Quantifying query

ambiguity. In Proceedings of the Human Language
Technology Conference (HLT-2002), pages 94–98, March
2002.

[7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting

query performance. In Proceedings of the 25th International
Conference on Research and Development in Information
Retrieval, pages 299–306, August 2002.

[8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,

and R. Harshman. Indexing by latent semantic analysis.
Journal of the American Society For Information Science,
41:391–407, 1990.

[9] F. Douglis, T. Ball, Y.-F. Chen, and E. Koutsoﬁos. The AT&T
internet difference engine: Tracking and viewing changes on
the web. World Wide Web, pages 27–44, January 1998.

[10] C. Fellbaum, editor. WordNet: An Electronic Lexical

Database. MIT Press, 1998.

[11] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,

G. Wolfman, and E. Ruppin. Placing search in context: The
concept revisited. ACM Transactions on Information
Systems, 20(1):116–131, January 2002.

[12] R. Gonzalez and P. Wintz. Digital Image Processing.

Addison-Wesley, second edition, 1987.

[13] D. Harman. Overview of the TREC 2002 novelty track. In
Proceedings of the 11th Text REtrieval Conference, pages

The x-axis shows contextual word windows ending at word i; the
y-axis plots the distance from each window to the seed story.
Figure 6: Examples of relationships of articles with a seed story.

extracted from the text, and use a variety of distance metrics to
estimate the dissimilarity between each news article and a collec-
tion of the previously read ones. The techniques underlying the
algorithms analyze inter- and intra-document dynamics by study-
ing how the delivery of information evolves over time from article
to article, as well as within each individual article at the level of
contextual word windows.

News browsers incorporating these algorithms can offer users a
personalized news experience, giving users the ability to tune both
the desired frequency of news updates and the degree to which
these updates should be similar to the seed story, via exercising
control over the novelty constraint.

To evaluate the algorithm for ranking news articles by novelty,
we propose a new evaluation scheme that asks users to read several
sets of articles ordered by different metrics, and rate them from
the most to least novel. Although at ﬁrst glance this task appears
very hard as it requires the users to keep in mind all the articles they
read, in practice the scheme was feasible. Debrieﬁng the users after
the experiments offered interesting insights into how people judge
novelty and relevance issues.

This research can be extended in several directions. We plan
to investigate more sophisticated distance metrics that incorporate
some of the basic metrics we described herein, as well as use com-
plex weighting windows that vary document weights over time.
We also plan to combine information-theoretic measures such as
KL and JS with a representation based solely on named entities,
to estimate the amount of novelty carried by the latter in a more
principled way. Of particular interest and practical use is the char-
acterization of which metrics are best for predicting the novelty of
articles for different types of topics. The notion of intra-story pat-
terns of novelty is a rich area for exploration. In this work we only
scratched the surface by modeling a few article types of particular
interest in the context of news. However, we believe a much richer
ontology would be necessary to capture the entire possible variety
of article types. We hope that further research in these directions
will provide additional insights into principles and applications for
personalizing news.

Finally, we observe that techniques similar to those we described
can also be applied to other types of content, such as blogs and
newsgroups, assessing the novelty of postings within threads or

 48946–55. ACM Press, 2002. NIST Special Publication
500-251.

[14] E. Horvitz, C. Kadie, T. Paek, and D. Hovel. Models of

attention in computing and communication: From principles
to applications. Communications of the ACM, 46(3):52–59,
March 2003. http://research.microsoft.com/
˜horvitz/cacm-attention.htm.

[15] A. Kilgarriff. Comparing corpora. International Journal of

Corpus Linguistics, 6(1):97–133, 2001.

[16] J. Kleinberg. Bursty and hierarchical structure in streams. In

Proceedings of the 8th International Conference on
Knowledge Discovery and Data Mining, 2002.

[22] G. Salton and C. Buckley. Term weighting approaches in

automatic text retrieval. Information Processing and
Management, 24(5):513–523, 1988.

[23] B. Schiffman, A. Nenkova, and K. McKeown. Experiments

in multidocument summarization. In Proceedings of the
Human Language Technology Conference (HLT-2002),
March 2002.

[24] R. Swan and D. Jensen. Timemines: Constructing timelines
with statistical models of word usage. In Proceedings of the
ACM SIGKDD 2000 Workshop on Text Mining, pages 73–80,
2000.

[25] F. Wilcoxon. Individual comparisons by ranking methods.

[17] L. Lee. Measures of distributional similarity. In Proceedings

Biometrics, 1:80–83, 1945.

of the 37th Annual Meeting of the ACL, 1999.

[26] Y. Yang, T. Ault, and T. Pierce. Combining multiple learning

[18] C. D. Manning and H. Schuetze. Foundations of Statistical

Natural Language Processing. The MIT Press, 2000.
[19] J. R. Quinlan. C4.5: Programs for Machine Learning.

Morgan Kaufmann, 1993.

[20] P. Resnik. Semantic similarity in a taxonomy: An

information-based measure and its application to problems
of ambiguity in natural language. Journal of Artiﬁcial
Intelligence Research, 11:95–130, 1999.

[21] E. Ristad. A natural law of succession. Technical Report

Technical Report CS-TR-495-95, Princeton University, 1995.

strategies for effective cross-validation. In Proceedings of
ICML-00, 17th International Conference on Machine
Learning, pages 1167–1182, 2000.

[27] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.

Topic-conditioned novelty detection. In Proceedings of the
Internaltional Conference on Knowledge Discovery and
Data Mining, pages 688–693, 2002.

490