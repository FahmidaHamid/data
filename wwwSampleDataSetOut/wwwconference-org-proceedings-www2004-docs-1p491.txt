Information Diffusion Through Blogspace

D. Gruhl

IBM Research, Almaden
dgruhl@us.ibm.com

R. Guha

IBM Research, Almaden
rguha@us.ibm.com

David Liben-Nowell

Laboratory for Computer Science, MIT

dln@theory.lcs.mit.edu

A. Tomkins

IBM Research, Almaden

tomkins@almaden.ibm.com

ABSTRACT
We study the dynamics of information propagation in environments
of low-overhead personal publishing, using a large collection of
weblogs over time as our example domain. We characterize and
model this collection at two levels. First, we present a macroscopic
characterization of topic propagation through our corpus, formal-
izing the notion of long-running “chatter” topics consisting recur-
sively of “spike” topics generated by outside world events, or more
rarely, by resonances within the community. Second, we present
a microscopic characterization of propagation from individual to
individual, drawing on the theory of infectious diseases to model
the ﬂow. We propose, validate, and employ an algorithm to induce
the underlying propagation network from a sequence of posts, and
report on the results.

Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Statistics; G.2.2
[Discrete Mathematics]: Graph Theory; I.2.6 [Artiﬁcial Intelli-
gence]: Learning; I.6 [Computing Methodologies]: Simulation
and Modeling; J.4 [Social and Behavioral Sciences]: Sociology

General Terms
Algorithms, Experimentation, Measurement, Theory

Keywords
Viruses, memes, information propagation, viral propagation, topic
structure, topic characterization, blogs.

1.

INTRODUCTION

Over the course of history, the structure of societies and the re-
lations between different societies have been shaped to a great ex-
tent by the ﬂow of information in them [9]. More recently, over
the last 15–20 years, there has been interest not just in observing
these ﬂows, but also in inﬂuencing and creating them. Doing this
requires a deep understanding of the macro- and micro-level struc-
tures involved, and this in turn has focused attention on modeling
and predicting these ﬂows. This paper studies the propagation of
discussion topics from person to person through the social network
represented by the space of all weblogs.

Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, New York, USA.
ACM 1-58113-844-X/04/0005.

The mainstream adoption of the Internet and Web has changed
the physics of information diffusion. Until a few years ago, the
major barrier for someone who wanted a piece of information to
spread through a community was the cost of the technical infras-
tructure required to reach a large number of people. Today, with
widespread access to the Internet, this bottleneck has largely been
removed. In this context, personal publishing modalities such as
weblogs have become prevalent. Weblogs, or “blogs,” are personal
online diaries managed by easy-to-use software packages that allow
single-click publishing of daily entries. The contents are observa-
tions and discussions ranging from the mainstream to the startlingly
personal. There are several million weblogs in existence today. Un-
like earlier mechanisms for spreading information at the grassroots
level, weblogs are open to frequent widespread observation, and
thus offer an inexpensive opportunity to capture large volumes of
information ﬂows at the individual level. Furthermore, recent elec-
tronic publication standards allow us to gather dated news articles
from sources such as Reuters and the AP Newswire in order to an-
alyze weblogs in the context of current affairs; these sources have
enormous inﬂuence on the content of weblogs.

Weblogs typically manifest signiﬁcant interlinking, both within
entries, and in boilerplate matter used to situate the weblog in a
neighborhood of other weblogs that participate in the same dis-
tributed conversation. Kumar et al. [19] analyze the “burstiness” of
blogs, capturing bursts of activity within blog communities based
on an analysis of the evolving link structure. Here, we focus in-
stead on the propagation of topics from one blog to the next, based
on the text of the weblog rather than its hyperlinks. Using this in-
formation, we seek to characterize information diffusion along two
dimensions:

Topics: We are interested in ﬁrst identifying the set of postings that
are about some topic, and then characterizing the different
patterns into which the collection of postings about the topic
may fall. We propose that topics are mostly composed of a
union of chatter (ongoing discussion whose subtopic ﬂow is
largely determined by decisions of the authors) and spikes
(short-term, high-intensity discussion of real-world events
that are relevant to the topic). We develop a model to cap-
ture this observed structure.

Individuals: Though the advent of personal publication gives ev-
eryone the same reach, individual behavior differs dramati-
cally. We begin by characterizing four categories of individ-
uals based on their typical posting behavior within the life
cycle of a topic. We then develop a model for information
diffusion based on the theory of the spread of infectious dis-

491eases; the parameters of the model capture how a new topic
spreads from blog to blog. We give an algorithm to learn
the parameters of the model based on real data, and apply
the algorithm to real (and synthetic) blog data. As a result,
we are able to identify particular individuals who are highly
effective at contributing to the spread of “infectious” topics.

2. RELATED WORK

There is a rich literature around propagation through networks
that is relevant to our work, from a variety of ﬁelds ranging from
thermodynamics to epidemiology to marketing. We provide here
a broad survey of the area, with pointers to more detailed survey
works where possible, and give some details around recent work in
disease propagation that is closest in spirit to the models we present.
2.1 Information Propagation and Epidemics
Much previous research investigating the ﬂow of information
through networks has been based upon the analogy between the
spread of disease and the spread of information in networks. This
analogy brings centuries of study of epidemiology to bear on ques-
tions of information diffusion. (See, for example, the book of Bai-
ley [4] for some of the extensive work in this ﬁeld.)

Classical disease-propagation models in epidemiology are based
upon the cycle of disease in a host: a person is ﬁrst susceptible
(S) to the disease. If then exposed to the disease by an infectious
contact, the person becomes infected (I) (and infectious) with some
probability. The disease then runs its course in that host, who is
subsequently recovered (R) (or removed, depending on the viru-
lence of the disease). A recovered individual is immune to the
disease for some period of time, but the immunity may eventu-
ally wear off. Thus SIR models diseases in which recovered hosts
are never again susceptible to the disease—as with a disease con-
ferring lifetime immunity, like chicken pox, or a highly virulent
disease from which the host does not recover—while SIRS models
the situation in which a recovered host eventually becomes suscep-
tible again, as with inﬂuenza. In blogspace, one might interpret the
SIRS model as follows: a blogger who has not yet written about a
topic is exposed to the topic by reading the blog of a friend. She
decides to write about the topic, becoming infected. The topic may
then spread to readers of her blog. Later, she may revisit the topic
from a different perspective, and write about it again.

Girvan et al. [11] study a SIR model with mutation, in which a
node u is immune to any strain of the disease which is sufﬁciently
close to a strain with which u was previously infected. They ob-
serve that for certain parameters it is possible to generate periodic
outbreaks, in which the disease oscillates between periods of epi-
demic outbreak and periods of calm while it mutates into a new
form. In blogspace, one could imagine the mutation of Arnold qua
movie star into Arnold qua governor. (We observe this kind of ebb
and ﬂow in the popularity of various “spiky chatter”-type memes.
See Section 4.2.1.)

Early studies of propagation took place on “fully mixed” or “ho-
mogeneous” networks in which a node’s contacts are chosen ran-
domly from the entire network. Recent work, however, focuses
on more realistic models based on social networks. In a model of
small-world networks deﬁned by Watts and Strogatz [28], Moore
and Newman [21] are able to calculate the minimum transmission
probability for which a disease will spread from one seed node to
infect a constant fraction of the entire network (known as the epi-
demic threshold).

We now review some previous research on epidemic spreading
on networks that follow a power law, in which the probability that
the degree of a node is k is proportional to k−α, for a constant α

typically between 2 and 3. Many real-world networks have this
property [20], including the social network deﬁned by blog-to-blog
links [19]. Pastor-Satorras and Vespignani [25] analyze an SIS
model of computer virus propagation in power-law networks, show-
ing that—in stark contrast to random or regular networks—the epi-
demic threshold is zero, so an epidemic will always occur. These
results can be interpreted in terms of the robustness of the network
to random edge failure, as follows. Suppose that each edge in the
network is deleted independently with probability (1− ε); we con-
sider the network “robust” if most of the nodes are still connected.
It is easy to see that nodes that remain in the same component as
some initiator v0 after the edge deletion process are exactly the
same nodes that v0 infects according to the disease transmission
model above. This question has been considered from the perspec-
tive of error tolerance of networks like the Internet: what happens
to the network if a random (1− ε)-fraction of the links in the Inter-
net fail? Many researchers have observed that power-law networks
exhibit extremely high error tolerance [2, 6].

In blogspace, however, many topics propagate without becoming
epidemics, so such a model would be inappropriate. One reﬁne-
ment is to consider a more accurate model of power-law networks.
Egu´ıluz and Klemm [10] have demonstrated a non-zero epidemic
threshold under the SIS model in power-law networks produced by
a certain generative model that takes into account the high clus-
tering coefﬁcient—the probability that two neighbors of a node are
themselves neighbors—found in real social networks [28]. Another
reﬁnement is to modify the transmission model. Wu et al. [30] con-
sider the ﬂow of information through real and synthetic email net-
works under a model in which the probability of infection decays as
the distance to the initiator v0 increases. They observe that meme
outbreaks under their model are typically limited in scope—unlike
in the corresponding model without decay, where the epidemic
threshold is zero—exactly as one observes in real data. Newman
et al. [24] have also empirically examined the simulated spread of
email viruses by examining the network deﬁned by the email ad-
dress books of a user community. Finally, Newman [23] is able to
calculate properties of disease outbreaks, including the distribution
of outbreak sizes and the epidemic threshold, for an SIR model of
disease propagation.

2.2 The Diffusion of Innovation

The spread of a piece of information through a social network
can also be viewed as the propagation of an innovation through
the network. (For example, the URL of a website that provides a
new, valuable service is such a piece of information.) In the ﬁeld
of sociology, there has been extensive study of the diffusion of in-
novation in social networks, examining the role of word of mouth
in spreading innovations. At a particular point in time, some nodes
in the network have adopted the innovation, and others have not.
Two fundamental models for the process by which nodes adopt new
ideas have been considered in the literature:

• Threshold models [14]. Each node u in the network chooses
a threshold tu ∈ [0, 1], typically drawn from some proba-
bility distribution. Every neighbor v of u has a nonnegative
v∈Γ(u) wu,v ≤ 1, and u

connection weight wu,v so thatP
adopts if and only if tu ≤P

adopters v∈Γ(u) wu,v.

• Cascade models [13]. Whenever a social contact v ∈ Γ(u)
of a node u adopts, then u adopts with some probability
pv,u. (In other words, every time a person close to a per-
son u adopts, there is a chance that u will decide to “follow”
v and adopt as well.)

492In the Independent Cascade model of Goldenberg, Eitan, and Muller
[13], we are given a set of N nodes, some of which have already
adopted. At the initial state, some non-empty set of nodes are “acti-
vated.” At each successive step, some (possibly empty) set of nodes
become activated. The episode is considered to be over when no
new activations occur. The set of nodes are connected in a directed
graph with each edge (u, v) labeled with a probability pu,v. When
node u is activated in step t, each node v that has an arc (u, v)
is activated with probability pu,v. This inﬂuence is independent
of the history of all other node activations. (If v is not activated
in that time step, then u will never activate v.) The General Cas-
cade model of Kempe, Kleinberg, and Tardos [17] generalizes the
Independent Cascade model—and also simultaneously generalizes
the threshold models described above—by discharging the inde-
pendence assumption.

Kempe et al. are interested in a related problem on social net-
works with a marketing motivation: assuming that innovations prop-
agate according to such a model, and given a number k, ﬁnd the k
“seed” nodes S∗
k that maximize the expected number of adopters of
the innovation if S∗
k adopt initially. (One can then give free samples
of a product to S∗
k, for example.)
2.3 Game-Theoretic Approaches

The propagation of information through a social network has
also been studied from a game-theoretic perspective, in which one
postulates an increase in utility for players who adopt the new inno-
vation or learn the new information if enough of their friends have
also adopted. (For example, each player chooses whether to switch
from video tape to DVDs; a person with friends who have made the
same choice can beneﬁt by borrowing movies.) In blogspace, shar-
ing discussion of a new and interesting topic with others in one’s
immediate social circle may bring pleasure or even increased status.
Morris [22] and Young [31] consider a setting like the following
coordination game: in every time step, each node in a social net-
work chooses a type {0, 1}. Here we interpret players of type one
to have adopted the meme. Each player i receives a positive payoff
for each of its neighbors that has the same type as i, in addition to
an intrinsic beneﬁt that i derives from its type. (Each player may
have a distinct utility for adopting, depending on his inherent in-
terest in the topic.) Suppose that all but a small number of players
initially have type 0. Morris and Young explore the question of
whether type 1’s can “take over” the graph if every node chooses
to switch to type 0 with probability increasing as the number of i’s
neighbors that are of type 0 increases.

There has also been work in the economics community on mod-
els of the growth of social networks when an agent u can selﬁshly
decide to form a link with another agent v, who may have infor-
mation that u desires to learn. There is a cost borne by u to estab-
lishing such a link, and a proﬁt for the information which u learns
through this link. This research explores properties of the social
network which forms under this scenario [5, 16].

3. CORPUS DETAILS

One of the challenges in any study involving tens of thousands
of publishers is the tracking of individual publications. Fortunately
for us, most of the publishers, including the major media sources,
now provide descriptions of their publications using RSS (rich site
summary, or, occasionally, really simple syndication) [18]. RSS,
which was originally developed to support the personalization of
the Netcenter portal, has now been adopted by the weblog commu-
nity as a simple mechanism for syndication. In the present work,
we focus on RSS because of its consistent presentation of dates—a
key feature for this type of temporal tracking.

Figure 1: Number of blog postings (a) by time of day and (b)
by day of week, normalized to the local time of the poster.

Our corpus was collected by daily crawls of 11,804 RSS blog
feeds. We collected 2K–10K blog postings per day—Sundays were
low, Wednesdays high—across these blogs, for a total of 401,021
postings in our data set. (Each posting corresponds to an “item” en-
try in RSS.) Complementing this, we also crawled 14 RSS channels
from rss.news.yahoo.com hourly, to identify when topics
were being driven by major media or real-world events, as opposed
to arising within blogspace itself. The blog entries were stored as
parent/child entities in WebFountain [29] and analyzed with a half-
dozen special-purpose blog annotators to extract the various date
formats popular in RSS, convert to UTF8, detag, etc.

See Figure 1 for the proﬁle of blog postings within a day and
from day-to-day, normalized by the poster’s time zone. The most
frequent posting is at 10AM. There is a pronounced dip at 6 and
7PM (the commute home? dinner? Must-See-TV?), an odd plateau
between 2 and 3AM and a global minimum at 5AM. Posting seems
to peak midweek, and dips considerably on weekends.

4. CHARACTERIZATION AND MODELING

OF TOPICS

In this section, we explore the topics discussed in our data. We
differentiate between two families of models: (i) horizon models,
which aim to capture the long-term changes (over the course of
months, years, or even decades) in the primary focus of discussion
even as large chatter topics (like Iraq and Microsoft, as of this writ-
ing) wax and wane; and (ii) snapshot models, which focus on short-

493term behavior (weeks or months) while the background “chatter”
topics are assumed to remain ﬁxed. This paper explores snapshot
models; we do not address horizon models, but instead raise the
issue as an interesting open problem.

4.1 Topic Identiﬁcation and Tracking

To support our goal of characterizing topic activity, we must ﬁrst
ﬁnd and track topics through our corpus. The ﬁeld of topic detec-
tion and tracking has studied this problem in depth for a number
of years—NIST has run a series of workshops and open evalua-
tion challenges [27]; see also, for example, [3]. Our requirements
are somewhat different from theirs; we require schemes that pro-
vide views into a number of important topics at different levels
(very focused to very broad), but rather than either high precision
or high recall, we instead require that our detected set contain good
representatives of all classes of topics. We have thus evaluated a
range of simple techniques, chosen the ones that were most effec-
tive given our goals, and then manually validated different subsets
of this broader set for use in particular experiments.

Our evaluations of these different techniques revealed some un-
expected gaps in our intuition regarding blogspace; we give a brief
walk-through here. First, we treated references to particular web-
sites as topics, in the sense that bloggers would read about these
“interesting” sites in another blog and then choose to write about
them. However, while there are over 100K distinct links in our
corpus, under 700 of them appear 10 times or more—not enough
to chart statistically signiﬁcant information ﬂows. Next, we con-
sidered recurring sequences of words using sequential pattern min-
ing [1]. We discovered under 500 such recurrent sequences, many
of which represented automatically generated server text, or com-
mon phrases such as “I don’t think I will” and “I don’t understand
why.” We then turned to references to entities deﬁned in the TAP
ontology [15]. This provided around 50K instances of references
to 3700 distinct entities, but fewer than 700 of these entities oc-
curred more than 10 times. The next two broader sets provided
us with most of the fodder for our experiments. We began with
a naive formulation of proper nouns: all repeated sequences of up-
percase words surrounded by lowercase text. This provided us with
11K such features, of which more than half occurred at least 10
times. Finally, we considered individual terms under a ranking de-
signed to discover “interesting” terms. We rank a term t by the
ratio of the number of times that t is mentioned on a particular day
i (the term frequency tf (i)) to the average number of times t was
mentioned on previous days (the cumulative inverse document fre-
j=0 tf (j ).
Using a threshold of tf (i) > 10 and tfcidf (i) > 3 we generate
roughly 20,000 relevant terms.

quency). More formally, tfcidf (i) = (i − 1)tf (i)/Pi−1

All features extracted using any of these methods are then spot-
ted wherever they occur in the corpus, and extracted with metadata
indicating the date and blog of occurrence.

4.2 Characterization of Topic Structure

To understand the structure and composition of topics, we man-
ually studied the daily frequency pattern of postings containing a
large number of particular phrases. We analyzed the 12K individ-
ual words most highly ranked under the tfcidf ranking described
above. Most of these graphs do not represent topics in a classi-
cal sense, but many do. We hand-identiﬁed 340 classical topics, a
sample of which is shown in Table 1.

Next, based on our observations, we attempt to understand the
structure and dynamics of topics by decomposing them along two
orthogonal axes:
internally driven, sustained discussion we call
chatter; and externally induced sharp rises in postings we call spikes.

apple
blair
china
guantanamo
schwarzenegger
zidane

ashcroft
arianna
bustamante
boykin
diana
davis
kazaa
harvard
udell
siegfried
gizmodo microsoft

astronaut
chibi
farfarello
longhorn
wildﬁres
saddam

Table 1: Example topics identiﬁed during manual scan.

We then reﬁne our model by exploring the decomposition of these
spikes into subtopics, so that a topic can be seen as the union of
chatter and spikes about a variety of subtopics.

4.2.1 Topic = Chatter + Spikes

There is a community of bloggers interested in any topic that ap-
pears in postings. On any given day, some of the bloggers express
new thoughts on the topic, or react to topical postings by other blog-
gers. This constitutes the chatter on that topic.

Occasionally, an event occurring in the real world induces a re-
action from bloggers, and we see a spike in the number of postings
on a topic. Spikes do not typically propagate through blogspace, in
the sense that bloggers typically learn about spikes not from other
blogs, but instead from a broad range of channels including main-
stream media. Thus, we can assume all informed authors are aware
of the topical event and have an opportunity to write about it.

On rare occasions, the chatter reaches resonance, i.e., someone
makes a posting to which everyone reacts sharply, thereby caus-
ing a spike. The main characteristic of resonance is that a spike
arises from either no external input or a very small external input.
The formation of order (a spike) out of chaos (chatter) has been
observed in a variety of situations [26], though observation of our
data reveals that this happens very rarely in blogspace. In fact, the
only sustained block re-posting meme that we observed in our data
consisted of the “aoccdrnig to rscheearch at an elingsh uinervtisy
it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny
iprmoetnt tihng is taht the frist and lsat ltteer is at the rghit pclae”
story which came out of nowhere, spiked and died in about 2 weeks
(with most postings over a four-day period).

Depending on the average chatter level and pertinence of the
topic to the real world, topics can be roughly placed into one of
the following three categories, with examples shown in Figure 2:

Just Spike: Topics which at some point during our collection win-
dow went from inactive to very active, then back to inactive.
These topics have a very low chatter level. E.g., Chibi.

Spiky Chatter: Topics which have a signiﬁcant chatter level and
which are very sensitive to external world events. They react
quickly and strongly to external events, and therefore have
many spikes. E.g., Microsoft.

Mostly Chatter: Topics which were continuously discussed at rel-
atively moderate levels through the entire period of our dis-
cussion window, with small variation from the mean. E.g.,
Alzheimer’s.

Spiky Chatter topics typically have a fairly high level of chatter,
with the community responding to external world events with a
spike; their persistent existence is what differentiates Spiky Chat-
ter from spikes. They consist of a superposition of multiple spikes,
plus a set of background discussion unrelated to any particular cur-
rent event. For example, the Microsoft topic contains numerous
spikes (for example, a spike towards the end of our window around
a major announcement about Longhorn, a forthcoming version of

494Figure 2: Three types of topic patterns:
the topic “Chibi”
(green line with single spike in center of graph) is Just Spike;
“Microsoft” (blue line with peaks and valleys throughout
graph) is Spiky Chatter; and “Alzheimer’s” (red line with rel-
atively ﬂat content) is Mostly Chatter.

windows
exchange
outlook
eolas
scoble
ms

server
ie
msn
xp
pdc
vb

services
ofﬁce
gates
netscape
motorola
acrobat

longhorn
msdn
redmond
powerpoint
avalon
xaml

Figure 3: The topic density for posts on Microsoft, both before
and after spike removal.

series
pc
apple
ram
unix
ms

server
ie
jobs
xp
pcs
macs

os
mac
dell
explorer
linux
quicktime macintosh

longhorn
gui
ui
drm
apples

Table 2: Top coverage terms for Microsoft spikes.

Table 3: Top coverage spike terms for Windows. Terms on a
grey background are also spike terms for Microsoft (Table 2).

Windows) plus ongoing chatter of people expressing opinions or
offering diatribes regarding the company and its products.

4.2.2 Topic = Chatter + Spiky Subtopics

In this section, we reﬁne our model of Topic = Chatter + Spikes
by examining whether the spikes themselves are decomposable.
Intuitively, the community associated with a topic can be seen as
randomly choosing a subtopic and posting about it. When an ex-
ternal world event occurs, it is often particular to something very
speciﬁc—that is, a subtopic—especially for complex topics.
In
this section, we consider a subtopic-based analysis using the spikes
in the complex, highly posted topic “Microsoft” as a case study.
Microsoft was especially appropriate for this analysis, as several
Microsoft-related events occurred during the collection of our data
set, including the announcement of blog support in Longhorn.

We used a multi-step process to identify some key terms for
this experiment. First, we looked at every proper noun x that co-
occurred with the target term “Microsoft” in the data. For each we
compute the support s (the number of times that x co-occurred with
the target) and the reverse conﬁdence cr := P (target|x).

Thresholds for s and cr were manipulated to generate rational
term sets. As is common with these cases, we do not have a hard-
and-fast support and conﬁdence algorithm, but found that s in the
range of 10 to 20 and cr in the range of 0.10 to 0.25 worked well.
For the target “Microsoft,” this generates the terms found in Ta-
ble 2. Of course, this is not a complete list of relevant subtopics,
but serves rather as a test set. For these terms, we looked at their
occurrences, and deﬁned a spike as an area where the posts in a
given day exceeded µ + 2σ. We then extended the area to either
side until a local minimum less than the mean was reached. We
refer to posts during these intervals as spike posts.

Now, having identiﬁed the top coverage terms, we deleted spike
posts related to one of the identiﬁed terms from the Microsoft topic.
The results are plotted in Figure 3. The de-spiked posts line shows
a considerable reduction in the spikes of the Microsoft graph, with
minor reduction elsewhere. Note that even in the spiky area we
are not getting a complete reduction, suggesting we may not have
found all the synonymous terms for those spike events, or that
subtopic spikes may be correlated with a latent general topic spike
as well.

This analysis in no way implies that the topics in Table 2 are
atomic. We also explored the subtopic “Windows”—one of the
subtopics with better coverage—and looked at its decomposition.
The proper noun selection was performed as before, generating
the term set in Table 3. There is some duplication of terms from
Table 2, as the topics “Microsoft” and “Windows” overlap sig-
niﬁcantly. However, some terms unique to Windows appear, es-
pecially the comparison to Apple (Apple, Steve Jobs, Quicktime,
Mac, Macs, Macintosh).

Applying these terms to the Windows posting frequency, we see
the results in Figure 4. Again, we see a similar reduction in spikes,
indicating that we have found much of the spiky behavior of this
topic. As might be expected with a more focused topic, the top
24 spike terms have better coverage for “Windows” than for “Mi-
crosoft,” leaving a fairly uniform chatter.

This case study strongly supports our notion of a spike and chat-
ter model of blog posting. While not presented here, similar behav-
ior was observed in a number of other topics (terrorism, Linux, the
California recall election, etc.).

495Figure 4: The topic density for posts on Windows, both before
and after spike removal.

Figure 5: Distribution of spike duration and period (spacing
between two consecutive spike centers) within chatter topics.

4.2.3 Characterization of Spikes

Having presented a qualitative decomposition of topics into chat-
ter and spikes, we now present measurements to quantify the nature
of these spikes. Each chatter topic can be characterized by two
parameters corresponding to the chatter level (distribution of the
number of posts per day) and the spike pattern (distribution of the
frequency, volume, and shape of spikes).

To perform these evaluations, we hand-tagged a large number of
topics into the categories given in Section 4.2.1. Of those hand-
tagged topics, 118 fell into the chatter category; we performed this
characterization study on those topics. We used the simple spike
deﬁnition of Section 4.2.2 to determine where the spikes occurred
in each chatter topic; an examination of the spikes found by this
algorithm led us to believe that, while simple, it indeed captures
our intuition for the spikes in the graph.

To begin, the average number of posts per day for non-spike re-
gions of our collection of chatter topics ranges between 1.6 to 106.
The distribution of non-spike daily average is well-approximated
by Pr[average number of posts per day > x] ∼ ce−x.

Next, we focus on characteristics of spike activity. Figure 5
shows the distribution of the duration of spikes, as well as their
period, the interval from the center of one spike to the next. Most
spikes in our hand-labeled chatter topics last about 5–10 days. The
median period between spike centers is about two weeks.

Figure 6: Average daily volume of spikes within chatter topics.

Figure 6 shows the distribution of average daily volume for spike
periods. In addition to the distribution shown in the ﬁgure, we ob-
served that the median spike among our chatter topics peaks at 2.7
times the mean, and rises and falls with an average change of 2.14
times the mean in daily volume.
5. CHARACTERIZATION AND MODELING

OF INDIVIDUALS

We have covered the high-level statistical “thermodynamic” view
of the data in terms of aggregates of posts at the topic level; now
we turn to a view more akin to particle dynamics, in which we at-
tempt to uncover the path of particular topics through the various
individuals who make up blogspace. We begin in Section 5.1 by
categorizing individuals into a small number of classes, just as we
did for topics in the previous section. Next, in Section 5.2 we for-
mulate a model for propagation of topics from person to person
through blogspace, and we present and validate an algorithm for
inducing the model. Finally, we apply the model to real data, and
give some preliminary applications.

Our model is akin to traditional models of disease propagation,
in which individuals become “infected” by a topic, and may then
pass that topic along to others with whom they have close contact.
In our arena, close contact is a directed concept, since a may read
the blog of b, but not vice versa. Such a model gives a thorough
understanding of how topics may travel from person to person. Un-
fortunately, we do not have access to direct information about the
source that inspired an author to post a message. Instead, we have
access only to the surface form of the information: the sequence in
which hundreds, thousands, or tens of thousands of topics spread
across blogspace. Our algorithm processes these sequences and ex-
tracts the most likely communication channels to explain the prop-
agation, based on the underlying model.
5.1 Characterizing Individuals

We begin with a quick sense of the textual output of our users.
Figure 7 shows the distribution of the number of posts per user for
the duration of our data-collection window. The distribution closely
approximates the expected power law [20].

We now wish to classify these users. We adopt a simple set of
predicates on topics that will allow us to associate particular posts
with parts of the life cycle of the topic. Given this information,
we will ask whether particular individuals are correlated with each
section of the life cycle. The predicates are deﬁned in the context
of a particular time window, so a topic observed during a different
time window might trigger different predicates. See Table 4 for the

496Region
Users with > 4 posts
and > µ + 3σ
Total posts this region

Up
20

Down Mid
55
157

Spike
310

1733

3300

12453

55624

Table 5: Number of users associated with each region.

the category, and therefore determine the extent to which we should
be surprised by the behavior of the given user. We set our thresh-
old for surprise when the number of occurrences is more than three
standard deviations beyond the mean of the random user.

Using this technique, we give the number of users who are un-
usually strong contributors to each region in Table 5. In some cases,
as for the Up region, the numbers are relatively low, but the total
number of posts in the region is also quite small. The correlation
is quite strong, leading us to suggest that evaluating broader deﬁni-
tions of a “ramp up” phase in the discussion of a topic may identify
a larger set of users correlated with this region. For regions such
as Mid or Spike, the number of associated users is quite substan-
tial, indicating that there are signiﬁcant differing roles played by
individuals in the life cycle of a topic.
5.2 Model of Individual Propagation

We derive our formal model from the Independent Cascade model
of Goldenberg et al. [13], which has been generalized by the Gen-
eral Cascade Model of Kempe et al. [17]. We are given a set of
N nodes, corresponding to the authors. At the initial state of each
episode, some possibly empty set of nodes have written about the
topic. At each successive state, some possibly empty set of authors
write about the topic. We present the model in the SIR framework,
in which authors do not write multiple postings on the topic; then in
Section 5.4 we consider an extension into the more accurate SIRS
framework, allowing authors to write repeatedly on the same topic.
We consider the episode to be over when no new articles appear for
some number of time steps, the timeout interval.

Under the Independent Cascade Model, the authors are connected
by a directed graph, where each edge (v, w) is labeled with a copy
probability κv,w. When author v writes an article at time t, each
node w that has an arc from v to w writes an article about the topic
at time t + 1 with probability κv,w. This inﬂuence is independent
of the history of whether any other neighbors of w have written on
the topic. The General Cascade Model can be seen as generalizing
this by eliminating the assumption of independence.

We introduce the notion that a user may visit certain blogs fre-
quently, and other blogs infrequently. We capture this with an ad-
ditional edge parameter ru,v, denoting the probability that u reads
v’s blog on any given day.

Formally, propagation in our model occurs as follows. If a topic
exists at vertex u on a given day—i.e., u has previously written
about the topic—then we compute the probability that the topic
will propagate from u to a neighboring vertex v as follows. Node
v reads the topic from node u on any given day with reading prob-
ability ru,v, so we choose a delay from an exponential distribution
with parameter ru,v. Then, with probability κu,v, the author of v
will choose to write about it. If v reads the topic and chooses not
to copy it, then v will never copy that topic from u; there is only a
single opportunity for a topic to propagate along any given edge.

Alternatively, one may imagine that once u is infected, node v
will become infected with probability κu,vru,v on any given day,
but once the ru,v coin comes up heads, no further trials are made.
See Section 5.4 for some extensions to the model.

Thus, given the transmission graph (and, in particular, each edge’s
reading frequency r and copy probability κ), the distribution of

Figure 7: Distribution of number of posts by user.

Predicate Algorithm

Ramp-
Down

RampUp All days in ﬁrst 20% of
post mass below mean,
and average day during
this period below µ− σ/2.
All days in last 20% of
post mass below mean,
and average day during
this period below µ− σ/2.
All days during middle
25% of post mass above
mean, and average day
during this period above
µ + σ/2.
For some day, number of
posts exceeds µ + 2σ.

Mid-
High

Spike

% of
topics
3.7%

Region

20%
post

First
of
mass.

Last 20% of
post mass.

5.1%

9.4%

Middle
25% of post
mass.

18.2%

From spike
to inﬂection
point below
µ, both di-
rections.

Table 4: Life-cycle predicates on topics, and the fraction of top-
ics containing each region type.

deﬁnitions of these predicates, and the fraction of topics that evince
each of these regions.

We can then attempt to locate users whose posts tend to appear in
RampUp, RampDown, MidHigh, or Spike regions of topics. How-
ever, we must exercise caution in tracking this correspondence: for
example, we wish to avoid capturing users who simply happened
to post more frequently during the early part of our data-collection
window, and thus are more likely to post during regions identiﬁed
as RampUp by our predicates. We therefore consider the probabil-
ity pi that a post on day i falls into a given category (e.g., RampUp).
For any given user, we then consider the pair (ti, ci) of total posts
on day i and posts in the category on day i, respectively. The total
i ci. We can then deﬁne
a “random” user who contributes the same number of posts each
day, but does so without bias for or against the category. The ex-
pected number of posts in the category for the random user is then
i piti. Because the random user produces a sum of independent
random variables, each of which is simply a series of Bernoulli
trials with some bias depending on the day, we can determine the
probability that the random user would produce C or more posts in

number of posts in the category is C =P
P

497propagation patterns is now fully established. Given a community
and a timeout interval, our goal is therefore to learn the arcs and
associated probabilities from a set of episodes. Using these proba-
bilities, given the initial fragment of a new episode, we would like
to be able to predict the propagation pattern of the episode.
5.3 Induction of the Transmission Graph

In the following, we make a closed world assumption that all oc-
currences of a topic except the ﬁrst are the result of communication
via edges in the network. In Section 5.4, we discuss weakening this
assumption by introducing an “outside world” node into the model.
A topic in the following is a URL, phrase, name, or any other
representation of a meme that can be tracked from page to page.
We gather all blog entries that contain a particular topic into a list
[(u1, t1), (u2, t2), . . . , (uk, tk)] sorted by publication date of the
blog, where ui is the universal identiﬁer for blog i, and ti is the
ﬁrst time at which blog ui contained a reference to the topic. We
refer to this list as the traversal sequence for the topic.

We wish to induce the relevant edges among a candidate set of
Θ(n2) edges, but we have only limited data. We shall make critical
use of the following observation: the fact that blog a appears in
a traversal sequence, and blog b does not appear later in the same
sequence gives us evidence about the (a, b) edge—that is, if b were
a regular reader of a’s blog with a reasonable copy probability, then
sometimes memes discussed by a should appear in b’s blog. Thus,
we gain information from both the presence and absence of entries
in the traversal sequence.

We present an EM-like algorithm to induce the parameters of the
transmission graph [8], in which we ﬁrst compute a “soft assign-
ment” of each new infection to the edges that may have caused it,
and then update the edge parameters to increase the likelihood of
the assigned infections. Assume that we have an initial guess at
the value of r and κ for each edge, and we wish to improve our
estimate of these values. We adopt a two-stage process:

Soft-Assignment Step: Using the current version of the transmis-
sion graph, compute for each topic and each pair (u, v) the
probability that the topic traversed the (u, v) edge.

Parameter-Update Step: For ﬁxed u and v, recompute ru,v and

κu,v based on the posterior probabilities computed above.

5.3.1 Soft-Assignment Step

We are given as input the traversal sequence for a particular topic
j. For each v in the sequence, we consider all previous vertices u in
the sequence, and compute the probability pu,v that topic j would
have been copied from u to v, given the delay between u and v in
the sequence. We then normalize by the sum of these probabilities
to compute posteriors of the probability that each node u was v’s
source of inspiration. That is, setting r = ru,v, κ = κu,v, and δ to
be the delay in days between u and v in topic j:

P
w<v rw,v(1 − rw,v)δw,v κw,v

r(1 − r)δκ

.

pu,v :=

In practice, for efﬁciency reasons, we consider only the 20 values
of w closest to v, and require propagation to occur within 30 days.

5.3.2 Parameter-Update Step

We perform the following operation for each ﬁxed u and v.
Let S1 denote the set of topics j such that topic j appeared ﬁrst
at node u and subsequently at node v, and let S2 denote the set
of topics j such that u was infected with topic j but v was never
infected with the topic.

For each topic j ∈ S1, we require as input the pair (pj, δj),
where pj is the posterior probability computed above that u in-
fected v with topic j, and δj is the delay in days between the ap-
pearance of the topic in u and in v. For every topic j ∈ S2, we
require as input the value δj, where δj days elapsed between the
appearance of topic j at node u and the end of our snapshot.

We can then estimate an updated version of r and κ as follows:

P
P

r :=

j∈S1
j∈S1

pj
pjδj

P

P

κ :=

j∈S1

pj

Pr[r ≤ δj]

j∈S1∪S2

where Pr[a ≤ b] = (1 − a)(1 − (1 − a)b) is the probability that a
geometric distribution with parameter a has value ≤ b. (Given the
pj’s, the updated 1/r is the expected delay in topics copied from u
to v, and the updated κ is the ratio of the expected number of topics
at u copied by v to the expected number of such topics read by v.)

5.3.3

Iteration and Convergence

We now have an improved guess at the transmission graph, so
we can return to the soft-assignment step and recompute posteriors,
iterating until convergence. In the ﬁrst step, we use our model of
the graph to guess how data traveled; in the second, we use our
guess about how data traveled to improve our model of the graph.
For our data sets, the values of r and κ converge within 2–5
iterations, depending on the data, to a vector of values within 1%
of the limiting value under the L2 norm.
5.4 Extensions to the Model
The real world. Most blog topics do not travel exclusively through
blogspace; rather, they are real-world events that are covered to
some extent in traditional media. During online coverage of the
topic, certain bloggers may read about the topic in other blogs and
respond, while others may read about the topic in the newspaper
and write without reference to other blogs. Our model can be ex-
tended by introducing a “real world” node, which we view as writ-
ing about a topic whenever that the topic is covered sufﬁciently in
the media. Transmission probabilities and delays are handled just
as before, though we assume that essentially all bloggers will re-
ceive input from this “real world” node.

Span of attention. Blogging communities can become quite large,
and most people do not have the time to read more than a few blogs
on any regular basis. This phenomenon can be modeled either by
limiting the in-degree of nodes, or by allowing only some small
number of in-edges to inﬂuence a particular node at any time step.
We can extend the model to support this phenomenon by adding an
attention threshold parameter. More sophisticated models can cap-
ture the fact that the attention threshold is a function of the other
episodes that are occurring at the same time—the more concurrent
episodes, the lower the attention threshold for each episode. This
can explain the phenomenon that during high-chatter events like the
Iraq war or the California elections, many other topics that would
otherwise have received a lot of attention in fact received little. The
algorithm to learn the graph would require signiﬁcant modiﬁcation
to incorporate these changes.

Stickiness. As described above, the probability that a node v will
be infected with topic j by a node u in our model depends only
on the parameters ru,v and κu,v, and is independent of the topic
itself. Realistically, certain topics are inherently more interesting
than others, and thus are more likely to be copied. To extend the
model, we introduce the stickiness Sj of each topic j that controls
the probability that the topic will “stick” with v. The probability
of infection when v reads u’s blog now becomes κu,vSj instead
of just κu,v. (Stickiness of a topic is analogous to virulence of a

498Topics per node µr
2
4
6

0.718
0.703
0.694

σr
0.175
0.157
0.134

µκ
0.141
0.107
0.103

σκ
0.455
0.039
0.034

Table 6: Mean and standard deviation for r and κ in low-trafﬁc
synthetic benchmark. Correct values: r = 0.66, κ = 0.1.

disease.) Our algorithm for inducing parameters of the induction
graph requires only minor modiﬁcation for the updating of p, r,
and κ if we knew the Sj’s, but we still must compute or induce the
stickiness values. Often we can employ outside information, such
as empirical data on the popularity of a particular topic. Stickiness
can also be learned from the model using a maximum likelihood
estimation. However, the likelihood equations appear quite com-
plicated, and the estimation would be computationally expensive.
We have not pursued this direction.

Multiple Posts.
In our domain, authors routinely write multiple
posts on the same topic. The framework presented above extends
naturally to this case, except that traversal sequences of the form
[(u1, t1), (u1, t2), (u3, t3), . . .] are possible. Thus, in estimating
copy probabilities κ and delays r, we must consider the disjoint
events that u3 received the information from the ﬁrst instance of
u1, or the second instance. The relevant expectations must now be
taken over multiple instances of u, but the equations are otherwise
unchanged. The experiments described below, however, simply as-
sume that the reader of a blog will respond to the most recent post
on a particular topic, rather than to a prior post.
5.5 Validation of the Algorithm

5.5.1 Validation for Synthetic Data

In order to validate the algorithm, we created a synthetic se-
ries of propagation networks, ran each synthetic network to gen-
erate observable sequences of infection by particular topics, and
then ran our mining algorithm to extract the underlying propaga-
tion network. The synthetic graphs are modiﬁed Erd¨os-Renyi ran-
dom graphs: a number of vertices n is ﬁxed, as is a target degree
d.1 Each vertex selects d out-neighbors uniformly with replace-
ment from the vertex set; all parallel edges and self-loops are then
removed. Each edge is then given an (r, κ) value; we used r = 2/3
and κ = 1/10 for our tests.

We began with a synthetic graph with n = 1000 and d = 3. For
this graph, we performed multiple trials of a synthetic benchmark
in which a topic begins at a single vertex, and then propagates ac-
cording to a model. The number of trials per vertex ranged from 20
to 60. We refer to this benchmark below as the “impulse response
topics.” Due to the small value of κ, between 2 and 6 topics orig-
inating from each vertex propagate to at least one other vertex, on
average. We considered only edges that were traversed by at least
three topics with probability at least 0.1. We then compared the re-
sulting edge set against the edge set from the original propagation
network. An edge was counted as erroneous if it appeared in only
one of those two graphs—i.e., we penalize for both missing edges
and unnecessary edges. The algorithm requires little data to infer
the correct edges: once it saw 6 topics per node on average, it cor-
rectly inferred 2663 of the 3000 edges, plus 4 erroneous additional
edges. For this benchmark, the algorithm converges in two itera-
tions. The mean and standard deviation of the inferred values of r
1We validate the model on these simple graphs; future work in-
volves validation on other graph types, such as synthetic power-law
graphs.

Figure 8: Distribution of Inverse Mean Propagation Delay (r)
and Copy Probability (κ) for RSS data.

and κ for this experiment are shown in Table 6.

Next, we turn to a propagation model with higher degrees in
which topics tend to take off and propagate throughout the graph,
making it more difﬁcult to learn exactly how the information had
traveled. The parameters are n = 500, d = 9, and we take 20
topics per node. Topic sizes range from 1 to slightly over 200. The
estimated r values have mean 0.73 and standard deviation 0.12; the
κ values have mean 0.08 and standard deviation 0.03. The system
identiﬁes almost all relevant edges (to within 1%), and identiﬁes a
further almost 9% spurious edges due to the more complex struc-
ture of this task. Thus, both the edges and the estimated parameters
of the edges are very close to the underlying model.

5.5.2 Validation and Analysis for Real Data

Now that we have validated the algorithm on synthetic data, we
validate the model itself against our data. We run the graph in-
duction algorithm as described above on all the ProperName se-
quences in our dataset. As we have seen, roughly 20% of these se-
quences contain spikes, and fewer than 10% contain RampUp and
RampDown areas. So the dataset consists of both signal and noise.
Rather than introducing a “real world” node to modeling communi-
cation through the general media, we restrict our attention to topics
for which at least 90% of the occurrences are in blogspace, rather
than in our RSS media content. This focuses on about 7K topics.

To validate that the model has in fact discovered the correct
edges, we performed two experiments. First, we downloaded the
top 100 blogs as reported by http://blogstreet.com. Of
the 100 blogs, 70 of them were in our RSS-generated dataset. We
then used the model to rank individual nodes of the network based
on the amount of trafﬁc ﬂowing through those nodes. Of the 70
nodes in our dataset, 49 were in the top 10% of blogs in our analy-
sis; 40 were in the top 5%, and 24 were in the top 1.2%.

As a second validation, we ranked all edges in the ﬁnal model
by the expected number of topics that ﬂowed down the edge, and
produced the top 200. We hand-examined a random sample of this
set, and in 90% of the cases were able to ﬁnd a link between the
two blogs. Note that we were able to make use of the structure of
blogspace in the discovery of these links (i.e., blogrolls, and userids
appearing inline), while the algorithm did not have access to these
mechanisms, and made its determinations based on topics alone.

Figure 8 shows the distributions of r and κ as learned by the
algorithm on the approximately 7K topics described above. Most
edges have an expected propagation delay (1/r) of fewer than 5

499Figure 9: Expected Trafﬁc over 7K episodes for RSS data.

Figure 10: Fanout per individual.

days; the mean r is 0.28 and the standard deviation is 0.22. Copy
probabilities are quite low, with mean 0.04 and standard deviation
0.07, indicating that even bloggers who commonly read from an-
other source are selective in the topics they choose to write about.
Figure 9 shows the distribution of expected trafﬁc along each
edge; i.e., over the set of 7K given topics, for a particular edge
(a, b), how many times does b read about something on a and con-
sequently write about it? The iteration converges to about 4000
edges with trafﬁc. Popular edges might have 50 expected copies;
the median edge has 1–2 total expected messages that traverse it.

5.6 Nature of the Induced Transmission Graph
Now that we have learned the transmission graph from real data,

we consider two quick analyses of its nature.

5.6.1 Fanout by individual

Certain individuals are likely to pass topics on to many friends,
while others never see a follow-on response. We can measure the
expected number of follow-on infections generated by each person
in the graph; we refer to this number as the fanout. Most users
leave the topic with less energy than it arrived, transmitting to an
expected less than one additional person, as we would expect; thus,
very few topics reach resonance and cover blogspace through grass-
roots channels. Some users, however, provide a boost to every topic
they post about—over time, these are the users who can have sig-
niﬁcant impact on a community, or even on blogspace overall. The
fanout results are shown in Figure 10. The point to the very left of
the graph with fanout 5.3 is a standout; she is a classic “connec-
tor” in the sense of The Tipping Point [12] with a huge collection
of friends, a broad set of interests, and an intelligent and up-to-date
blog.

5.6.2 Critical Linkages

We may ask further whether any individuals tend to be more
strongly associated with topics that begin to take off. If we con-
sider the random set of “impulse response” topics from the previous
section, there are 357 of them that reach more four or more users.
Although the topics have been started at all possible start points
in the graph, and on average hit only 5 users, there is nonetheless
a single user who is present in 42 of the 357 episodes. Similarly,
there are 18 users present in at least 20 of the 357 episodes—as
has often been pointed out, marketers would do well to understand
these 18 users.

6. FUTURE WORK AND APPLICATIONS
In this section, we adopt a broader perspective and sketch some
possible domains in which a better understanding of the ﬂow of
information through networks might be a powerful tool.

News Services

Over the past few years, we have seen the launch of a number
of alert-based news services, which attempt to ﬁlter the large vol-
ume of online news items and identify a small number of impor-
tant, high-impact stories relevant to a given topic. The explosion
in the volume of news items poses a signiﬁcant challenge for mak-
ing these services useful. Weblogs compound this problem: while
some blog postings may be sufﬁciently important to merit notiﬁca-
tion, it can be difﬁcult to identify the crucial posts in high-chatter
topics.
(Corporate press releases pose a similar problem: while
some press releases are important and newsworthy, the vast major-
ity are comparatively irrelevant marketing propaganda.) Sites like
DayPop [7] attempt to track spikes, but the lack of a topic structure
reduces their value. Our topic model contributes to a solution for
this problem by enabling us to identify subtopics that are experienc-
ing spikes. Such an approach leverages the blogging community’s
reaction to external world events, as manifested by spikes in blog
postings, to identify news events that are worthy of attention. We
believe that this view of the blogging community—as a giant col-
laborative ﬁltering mechanism built around an implicit web of trust,
as manifested in propagation patterns between individuals—offers
great potential.

Marketing

Weblogs offer an excellent, inexpensive, and nearly real-time
tool for evaluating the effectiveness and health of a company’s im-
age and image-affecting activities. The ability to perform such
evaluations in the real world (and not in experimental focus groups)
can be a powerful and—given the substantial marketing expendi-
tures of many organizations—important tool.

For example, a company launching a new advertising campaign
can gain signiﬁcant value from being able to judge (and thus, hope-
fully, increase through tuning) the effectiveness of the campaign.
To the extent that the blogging community is representative of the
target audience for such a campaign, marketers can measure uptake
of key messages by deﬁning and tracking the appropriate topics.

The topic model might be used in the development of public rela-
tions campaigns, as well. Typically a company has a wide variety of
distinct possible emphases for an advertisement or a press release,

500and must select one of these directions. As discussed previously,
high-chatter topics tend to exhibit larger spikes; thus choosing to
emphasize a high-chatter (sub)topic can increase the likelihood of
the message eliciting a large reaction.

The chatter level on a topic can potentially also be used for keep-
ing tabs on the “mindshare” that a company has. As illustrated in
our examples and case studies, high visibility companies such as
Microsoft and Apple exhibit a high chatter level; tracking this chat-
ter could provide an early view of trends in share and perception.

Resonance

Resonance is the fascinating phenomenon in which a massive re-
sponse in the community is triggered by a minute event in the real
world. It is an extremely rare phenomenon; we were surprised to
ﬁnd even a few instances of resonance in our data set. Understand-
ing what causes resonance in networks like blogspace is an interest-
ing future direction for research, from both the computational and
the sociological perspective. The observation of the spontaneous
generation of order from chaos is not new [26], but perhaps the ac-
cess to blog data can shed new insight on this type of phenomenon.
Resonance is the marketeers’ Holy Grail [12]; a better understand-
ing of the cause of resonance would have massive implications for
marketing.

7. REFERENCES
[1] Rakesh Agrawal and Ramakrishnan Srikant. Mining

sequential patterns. In Proc. ICDE, pages 3–14, 1995.

[2] R´eka Albert, Hawoong Jeong, and Albert-L´aszl´o Barab´asi.

Error and attack tolerance of complex networks. Nature, 406,
July 2000.

[3] J. Allan, editor. Topic Detection and Tracking: Event-based

Information Organization. Kluwer, 2002.

[4] Norman Bailey. The Mathematical Theory of Infectious

Diseases and its Applications. Grifﬁn, London, 2nd edition,
1975.

[5] Venkatesh Bala and Sanjeev Goyal. A strategic analysis of

network reliability. Review of Economic Design, 5:205–228,
2000.

[6] B´ela Bollabas and Oliver Riordan. Robustness and
vulnerability of scale-free random graphs. Internet
Mathematics, 1(1), 2003.

[7] Daypop. http://www.daypop.com.
[8] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood

from incomplete data via the EM algorithm. J. Royal Stat.
Soc., 1977.

[9] Jared Diamond. Guns, Germs, and Steel. Random House,

1997.

[10] V´ıctor M. Egu´ıluz and Konstantin Klemm. Epidemic

threshold in structured scale-free networks. Physical Review
Letters, 89, 2002. cond-mat/0205439.

[11] Michelle Girvan, Duncan S. Callaway, M. E. J. Newman, and

Steven H. Strogatz. A simple model of epidemics with
pathogen mutation. Phys. Rev. E, 65(031915), 2002.
nlin.CD/0105044.

[12] Malcolm Gladwell. The Tipping Point: How little things can

make a big difference. Little Brown & Co., 2000.

[13] Jacob Goldenberg, Barak Libai, and Eitan Muller. Talk of the
network: A complex systems look at the underlying process
of word-of-mouth. Marketing Letters, 12(3):211–223, 2001.
[14] Mark Granovetter. Threshold models of collective behavior.

American Journal of Sociology, 83(6):1420–1443, 1987.

[15] R. V. Guha and Rob McCool. TAP: A system for integrating

web services into a global knowledge base.

[16] Hans Haller and Sudipta Sarangi. Nash networks with
heterogeneous agents. Working Paper Series E-2001-1,
Virginia Tech, 2003.

[17] David Kempe, Jon Kleinberg, and ´Eva Tardos. Maximizing

the spread of inﬂuence through a social network. In
Proc. KDD, 2003.

[18] Andrew King. The evolution of RSS.

http://www.webreference.com/authoring/
languages/xml/rss/1/.

[19] Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and

Andrew Tomkins. On the bursty evolution of blogspace. In
Proc. WWW, 2003.

[20] M. Mitzenmacher. A brief history of lognormal and power

law distributions. In Allerton Comm. Control Comput., 2001.

[21] Cristopher Moore and M. E. J. Newman. Epidemics and

percolation in small-world networks. Phys. Rev. E,
61:5678–5682, 2000. cond-mat/9911492.

[22] Stephen Morris. Contagion. Review of Economic Studies, 67,

2000.

[23] M. E. J. Newman. The spread of epidemic disease on

networks. Phys. Rev. E, 66(016128), 2002.
cond-mat/0205009.

[24] M. E. J. Newman, Stephanie Forrest, and Justin Balthrop.
Email networks and the spread of computer viruses. Phys.
Rev. E, 66(035101), 2002.

[25] Romauldo Pasto-Satorras and Alessandro Vespignani.

Epidemic spreading in scale-free networks.
Phys. Rev. Letters, 86(14):3200–3203, April 2001.

[26] Steven Strogatz. Sync: The emerging science of spontaneous

order. Hyperion, 2003.

[27] Topic Detection and Tracking (TDT-2003).

http://www.nist.gov/TDT.

[28] D. Watts and S. Strogatz. Collective dynamics of

‘small-world’ networks. Nature, 393:440–442, 1998.

[29] WebFountain.

http://www.almaden.ibm.com/WebFountain/.

[30] Fang Wu, Bernardo A. Huberman, Lada A. Adamic, and

Joshua R. Tyler. Information ﬂow in social groups.
Manuscript, 2003.

[31] H. Peyton Young. The diffusion of innovation in social

networks. Sante Fe Institute Working Paper 02-04-018, 2002.

501