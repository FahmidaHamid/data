Mining Models of Human Activities from the Web 

Mike Perkowitz, Matthai Philipose, 

Kenneth Fishkin 
Intel Research Seattle 

1100 NE 45th St 
Seattle, WA 98105 

(206) 633-6555 

mike.perkowitz@intel.com 

 

Donald J. Patterson 

Computer Science & Engineering 

University of Washington 

Box 352350 

Seattle, WA  98195-2350 

(206) 543-1695 

djp3@cs.washington.edu 

ABSTRACT 
The  ability  to  determine  what  day-to-day  activity  (such  as 
cooking  pasta,  taking  a  pill,  or  watching  a  video)  a  person  is 
performing is of interest in many application domains. A system 
that  can  do  this  requires  models of the activities of interest, but 
model construction does not scale well: humans must specify low-
level details, such as segmentation and feature selection of sensor 
data,  and  high-level  structure,  such  as  spatio-temporal  relations 
between  states  of  the  model,  for  each  and  every  activity.    As  a 
result,  previous  practical  activity  recognition  systems  have  been 
content  to  model  a  tiny  fraction  of  the  thousands  of  human 
activities  that  are  potentially  useful  to  detect.  In  this  paper,  we 
present an approach to sensing and modeling activities that scales 
to a much larger class of activities than before. We show how a 
new  class  of  sensors,  based  on  Radio  Frequency  Identification 
(RFID)  tags,  can  directly  yield  semantic  terms  that  describe  the 
state of the physical world. These sensors allow us to formulate 
activity models by translating labeled activities, such as “cooking 
pasta”,  into  probabilistic  collections  of  object  terms,  such  as 
“pot”. Given this view of activity models as text translations, we 
show  how  to  mine  definitions  of  activities  in  an  unsupervised 
manner  from  the  web.  We  have  used  our  technique  to  mine 
definitions for over 20,000 activities. We experimentally validate 
our  approach  using  data  gathered  from  actual human activity as 
well as simulated data.  

Categories and Subject Descriptors 
I.2.6 [Artificial Intelligence]: Learning – knowledge acquisition.  
I.2.7 [Artificial Intelligence]: Natural Language Processing – text 
analysis.  

General Terms 
Algorithms, Performance, Design. 

Keywords 
Activity inference, activity models, RFID, web mining. 

1.  INTRODUCTION 
Systems that recognize human activities have long been regarded 
as enabling a variety of useful applications. Proposed applications 
include  activity-based  actuation  (e.g.  dimming  lights  when  a 
video  is  being watched), to prompting (e.g. providing directions 
for  someone  using  unfamiliar  facilities  and  appliances),  and 

 

Copyright is held by the author/owner(s). 
WWW 2004, May 17-22, 2004, New York, NY USA. 
ACM 1-58113-844-X/04/0005. 
 

notification  (e.g.  informing  caregivers  when  an  elderly  person 
fails to perform key activities of daily living). Unfortunately, such 
applications  have  been  slow  in  materializing,  even  as  research 
prototypes. A key reason is that the process of developing these 
applications is not scalable. In particular, developing recognizers 
for even small classes of activities is a highly specialized activity 
involving  months  to  years  of  work  by  specialists  in  pattern 
recognition.  The resulting systems are typically only applicable 
to  certain  applications  and  deployment  contexts.  The  cost  of 
developing recognition infrastructure is thus both too high for the 
ordinary  developer  and  not  amortizable  across  applications,  or 
even  over  multiple  deployments  of  the  same  application.  Many 
researchers  have  therefore  recognized  the  value  of  developing  a 
general  system  that  recognizes  a  large  and  useful  class  of 
activities  with  minimal  incremental  effort  from  programmers  or 
end users.  
A  broadly  applicable  and  easy-to-use  system  must  overcome  at 
least two major challenges. First, the system should be general-
purpose:  it  should  not,  by  its  very  design,  be  incapable  of 
recognizing new activities of interest. Design decisions that limit 
a  system’s  generality  include  the  sensors  used,  the  features 
derived from the sensors, the relationships between these features 
that comprise activities modeled in the system, and the algorithms 
used  for  matching  observations  to  models.  Second,  the  system 
must  facilitate  model  extraction:  even  if  the  system  can  in 
principle  recognize  complex  activities  once  models  for  these 
activities  are  specified,  specifying  robust,  widely-applicable 
models can itself be a considerable burden even for experts. At its 
heart, the model of an activity is a human belief. A good model 
extraction  scheme needs to tap into these beliefs with minimum 
interaction with humans, a seemingly paradoxical task.  
The  common  approach  to  facilitating  model  extraction  is  to 
support  learning  of  the  models  from  sensor  data.  In  some  cases 
[1][11][13]  the  developers  define  the  structure  of  the  possible 
models, but the system tunes the parameters of the model based 
on  examples  from  the user. In others [2][4][14] the system uses 
pattern  recognition 
to  recognize  spatio-temporal 
patterns 
in  an 
unsupervised manner. However, the user is then expected to label 
these patterns. Unfortunately the burden of supervising (whether 
providing examples or perusing patterns to determine appropriate 
labels)  still  makes  model  extraction  onerous  in  both  cases. 
Further, the variety of activities whose models can be extracted is 
quite restricted: by the scenarios anticipated (and formalized) by 
the  developers  in  the  former  case,  and  by  the  built-in  pattern 
recognizers in the latter. 
In  this  paper,  we  present  a  technique  for  extracting  models  of 
human  activity  that  finesses  the  problem  of  directly  involving 
humans  in  order  to  learn  about  their  beliefs.  In  the  tradition  of 

that  may  constitute  “interesting”  activities 

techniques 

573much recent work on exploiting the web as a repository of human 
beliefs,  we  show  how  to  mine  very  large  libraries  of  human 
activities  from  the  web,  instead  of  analyzing  sensor  data.  In 
particular,  we  use  step-by-step  human  descriptions  of  activities 
(such as recipes and how-tos) from appropriate websites to obtain 
the structure of our models, and supplement these with web-wide 
measures of similarity between concepts and terms to obtain the 
parameter  values  for  our  models. The technique is feasible with 
very  lightweight  natural  language  tools  and  has  enabled  our 
system to mine a library of models for roughly twenty thousand 
mundane activities, covering many aspects of human life.  
At  the  heart  of  our  technique  is  a  breakthrough  in  sensing 
technology. Advances in miniaturization and manufacturing have 
yielded  postage-stamp  sized,  forty-cent  radio  transceivers  called 
Radio Frequency Identification (RFID) tags that can be attached 
unobtrusively  to  objects  as  small  as  a  toothbrush.  The  tags  are 
wireless and battery free. When queried by radio, the tag responds 
with a globally unique identifier using power scavenged from the 
querying  signal.  When  combined  with  special  wearable  and 
ambient tag readers and a database mapping identifiers to names, 
RFID  technology  can  reliably  name  the  objects  with  which  a 
person  is  interacting.  Because  of  the  sensors’  accuracy  and 
specificity  we  model  activities  in  a  novel  way:  we  define  an 
activity  in  terms  of  the  probability  and  sequence  of  the  objects 
that are physically involved in the activity.  We are thus able to 
view models as probabilistic translations from terms that represent 
an activity name (such as “drinking tea”) to terms that represent 
the objects involved (such as “teacup” and “teabag”).  
To show that the resulting models (and the recognition system as 
a whole) can be used for detecting actual activities, we present the 
results  of  analyzing  activities  of  daily  living  (ADLs)  data  from 
eight subjects in a real home. ADLs are regarded as indicators of 
patient wellness, and professional caregivers are often required by 
law  to  record  them.  Our  users  performed  subsets  of  fourteen 
classes of activities comprising 66 activities in all.  To explore the 
usefulness of the remaining thousands of models, and to evaluate 
the components of our model extractor more thoroughly, we also 
present  results  based  on  simulated  traces  and  cross-corpus 
comparisons. 
In this paper, we make three main contributions: 
1.  We  formulate  the  problem  of  extracting  activity  models  as 
that  of  generating  translations  between  natural  language 
terms. 

2.  We  describe  a  simple  set  of  techniques  to  mine  these 

translations automatically from the web. 

3.  We validate these techniques via a combination of real-world 

experiments and simulations. 

To the best of our knowledge, this paper is the first to show how 
to mine useful models of human physical activity from the web.  
The  system  for  mining  models  is  part  of  a  larger  activity 
recognition  system  called 
the  Proactive  Activity  Toolkit 
(PROACT)[15]. To place the model extractor in context, section 2 
sketches  the  structure  and  usage  model  for PROACT. Section 3 
describes in detail how we mine models. Section 4 evaluates the 
model  extractor.  Section  5  presents  related  work.  Section  6 
summarizes the paper and presents future work.   
 

tagging 

tens 

involves 

2.  ACTIVITY INFERENCING SYSTEM 
Below,  we  describe  how  the  PROACT  activity  recognition 
system, which uses the mined models, is intended to be used.  We 
then  describe  the  high-level  structure  of  PROACT,  place  the 
model miner in context.  
2.1  Usage model 
PROACT  assumes  that  “interesting”  objects  in  the  environment 
contain  RFID  tags.  These  can  be  purchased  off  the  shelf,  cost 
roughly  $0.40  each,  have  the  form  factor  of  postage  stamps 
(including  adhesive  backing),  and  can  withstand  day-to-day  use 
for  years.  PROACT  deployment 
to 
hundreds  of  objects  in  their  environment.  This  can  be  done 
incrementally.  As the number of tags increase more accurate and 
detailed  recognition  becomes  possible.  Tagging  an  object 
involves sticking an RFID tag on it, and making a database entry 
mapping the tag ID to a name.  Current trends indicate that within 
a few years, many household objects may be RFID-tagged before 
purchase, thus eliminating the overhead of tagging.  
Users employ RFID tag readers to track tag objects they interact 
with.  They  may  wear  tag-detecting  bracelets  or  gloves,  place 
medium-range readers in corners of rooms, or run robots, vacuum 
cleaners, or janitorial carts, with mounted long-range readers. As 
users go about their daily activities, the readers detect tags that (a) 
users touch, (b) are close to them, or (c) are moved by them, and 
thereby  deduce  which  objects  are  currently  involved  in  an 
activity.  PROACT  uses  the  sequence  and  timing  of  object 
involvement to deduce what activity is happening.  
An application can query PROACT at any time for the likelihood 
of  various  activities  being  tracked  or  details  of  those  activities 
(e.g.  objects  involved  or  durations),  or  subscribe  for  event 
notification  when  activities  occur  with  a  specified  degree  of 
certainty.  Programmers  name  activities  using  plain  English 
phrases (e.g. “paying bills”). The phrase used can either be chosen 
from a pre-mined list provided by PROACT, or can be a new one 
provided by the programmer. For a new phrase, the programmer 
can define the activity by providing a text document containing an 
English  description  of  the  steps  involved  in  the  activity  (much 
like  a  recipe).  The  model  extractor  converts  text  into  activity 
definitions. 
2.2  System Overview 
Figure  1  presents  the  main  components  of  PROACT.  It  is 
centered  on  an  inference  engine  which,  given  models  for 
activities, and sequences of sensor readings, returns the likelihood 
of  current  activities.  The  models  are  produced  by  the  model 
extractor, which extracts them automatically from text documents, 
including  but  not  limited  to  websites.    The  sensor  readings  are 
produced  while  the  end-user  performs  activities.  For debugging, 
PROACT  provides  an  activity  viewer,  which  provides 
programmers  with  a  real-time  view  of activities in progress, the 
sensor data seen, and an indication of how belief in each activity 
changes with the data.  A more detailed description of the system 
is  in  [15].  We  elaborate  on  some  of  the  relevant  components 
below. 

574whereas sugar is involved in the flavoring phase 40% of 
the time. 

 

 

Figure 3: PROACT Model for Making Tea 

 
Our  model  is  based  on  a  particle  filter  implementation  of  
Bayesian  reasoning.  Equation  1  shows  the  Bayesian  update 
equation which provides the mathematical interpretation to Figure 
3.    In  this  equation  the  probability  of  being  in  a  given  state, xt, 
given  the  sequence  of  observations,  z1...zt,  is  related  to  three 
quantities:  a  sensor  model,  a  state  transition  model  and  a  prior 
distribution.  The sensor model accounts for sensor error, and the 
expectation of seeing a given RFID when engaged in an activity.  
The state transition model accommodates the graphical structure 
that links the nodes in the activities, the timing constraints on the 
nodes and the probabilistic requirement of seeing certain RFID's 
before an activity can be considered complete. Finally, the prior 
distribution accounts for the state of the world before any sensors 
are seen as well as a recursive description of the state of the world 
at the previous time step. 
 

Equation 1: The Bayesian update equation 

 
Our model of activities is quite simple. Possible variations on the 
basic  theme  include  modeling  activities  using  trees  or  graphs 
instead of linear lists; requiring timing information for each step; 
modeling  sources of error, such as sensor error and model error 
separately;  and  modeling  more  complex  belief  structure  at  each 
step. However, we have deliberately chosen to keep our models 
simple, because they are easier to mine and faster to reason with. 
A  key  result  of  this  paper  is  that  even  these  simple  models  are 
quite  effective,  while  supporting  automated  extraction  and  real-
time tracking. 
2.2.3  Inference Engine 
The  inference  engine  converts  the  activity  models  produced  by 
the  mining  engine  into  Dynamic  Bayesian  Networks.  We  use  a 
Sequential Monte Carlo (SMC) approximation to probabilistically 
solve  for  the  most  likely  activities.    The  inference  engine  is 
adapted from that used in related work on transportation behavior 
inference [14].  
3.  THE MODEL EXTRACTOR 
The  model  extractor  builds  formal  models  of  activities  (of  the 
kind  specified  in  section  2.2.2),  given  definitions  for  activities 
written  in  natural  language  by  humans.  Such  definitions  (which 

Figure 1: A High-Level View of PROACT 

 
2.2.1  Sensors 
PROACT  depends  on  being  able  to  observe  objects  that  are 
“involved”  in  activities.  As  mentioned  previously,  we  detect 
objects by detecting RFID tags stuck to the objects. The left-hand 
image in Figure 2 shows three types of off-the-shelf tags that may 
be used for this purpose. The first two have a 3-5 cm range, the 
third 2-6 meters. 

5cm

Figure 2: RFID Tags (L) and Glove-Based Reader (R) 

  
Currently, we use two different kinds of RFID readers to detect 
two  types  of  involvement.  First,  we  use  long-range  readers 
mounted  on  a  mobile  robot  platform  to  map  the  location  of 
objects  in  the  activity  space.  Coupled  with  the  location  of  the 
user, this information gives the set of objects close to a person at a 
given  time.  Second,  we  use  a  short-range  reader  built  into  the 
palm of a glove that can determine the objects that are touched. 
The glove-based reader is on the right of Figure 2. 
2.2.2  Models 
We now describe our model of activities. As an example, Figure 3 
shows  how  the  activity  of  making  tea  is  modeled.  We  model 
activities  as  a  Dynamic  Bayes  Net  representation  of  a  Hidden 
Semi-markov  Model.    Each  model  is  composed  of  a  sequence 
s1,…,sn  of  steps  in  the  activity.  If  a  step  follows  another  in  the 
sequence, then the latter must temporally follow the former in any 
valid instance of the activity. The example shows three steps for 
making tea, each drawn as a circle, corresponding to (A) boiling 
water, (B) steeping, and (C) flavoring the tea. Each step si has:  

•  An  optional  duration 

  In 

ti,  modeled  as  a  Gaussian 
probability  distribution  (µi,σi). 
this  example, 
steeping  the  tea  is  expected  to  take  2  minutes.    The 
amount of time required to boil the water and flavor the 
tea is unknown, and does not influence the reasoning. 
The set {(oi1, pi1),…,(oiNi, piNi)} of Ni objects oij involved, 
along  with  the  probability  pij  of  involvement  of  those 
objects.  In  Figure  3,  for  instance,  we  expect  to  see  a 
teapot  70%  of  the  time  that  we  are  boiling  water, 

• 

575we will generically call “directions” below) include how-tos (e.g. 
those  at  ehow.com),  recipes  (e.g.  from epicurious.com), training 
manuals, experimental protocols, and facility/device use manuals. 
For example, Figure 4 shows directions for making tea. 

Making Tea: 
1. 

Fill a teapot from the faucet. Place kettle on the stove and 
boil. 
Pour hot water into a cup, filling ¾ of the cup. Immerse 
teabag in cup for two minutes and dispose of teabag.  

2. 

3.  Add milk and sugar to taste. 

 

Figure 4: Directions for Making Tea 
3.1  Syntactic structure of directions 
A  key  observation  in  enabling  this  translation  is  that,  in  many 
cases, the structure of natural language directions closely parallels 
the  structure  of  our  formal  model.  Typically,  each  direction 
consists of: 
1.  A title t for the activity, and 
2.  A textual list r1,…,rm of steps. Each step ri has: 

Possibly a special keyword delimiting duration di, and 

• 
•  A  natural-language  description  nldi, 

typically  a 
paragraph,  of  what  to  do  during  the  step,  typically 
mentioning  some  subset  of  the  objects  involved  in  the 
step,  and  often  also  constraints  on  the  duration  of  the 
step. 

The example of Figure 4 has title t = “Making Tea”, number of 
steps  m  =  3,  no  formally  stated  duration  for  each  step,  a  two-
minute minimum constraint on the duration of step 2. The objects 
mentioned in each step are highlighted in bold font.  
In practice, each corpus of directions we wish to mine has its own 
concrete syntax (keywords, numbering scheme, indentation, etc), 
which needs to be parsed into the above abstract syntax. For each 
corpus,  therefore,  we  require  a  user  to  provide  a  front-end  that 
performs this parsing. 
3.2  Converting directions to activity models 
The  similarity  of  structure  between  the  formal  models  and  the 
directions suggests the following scheme for converting the latter 
into the former. Intuitively, we produce one step in the model for 
each step in the directions; the objects mentioned in the paragraph 
for  the  step  are  those  involved  with  the  step  in  the  model.  We 
name key steps in bold, and provide acronyms for them when we 
need to use them in the future. 
1.  Labeling.  Set  label  l  of  the  mined  model  to  title  t  of  the 

directions. 

2.  Parsing Steps. For each step ri  in list r1,…,rm generate step 

si  as follows: 
a. 

If  ri  has  keyword-delimited  duration  di,  set  the  mean 
duration µi for the step to di, and standard deviation σi 
to S(di, i, l). 

b.  Let  Oi=O(nldi)  be  the  set  of  terms  that  represents 
objects in the descriptive paragraph nldi. For each term 
oij  in  Oi,  calculate  the  probability  P(oij,  i,  l)  that  an 
object named oij  is touched in step i of activity l.  

• 

• 

3.  Tagged  Object  Filtering.  Given  the  set  Odeployed  of  tagged 
named objects in the space where the activity is happening, 
we remove from our model all observations related to objects 
not in Odeployed. This step exploits the fact that in most actual 
deployments of the system, the set of objects the system can 
possibly  see  is  often  much  smaller  than  the  set  of  objects 
named in the models. 

The scheme depends on three helper functions:  
• 

Function  S(d,  i,  l)  computes  the  standard  deviation  for  the 
duration of step i of activity l (which has mean duration d).  
A very simple definition is S(d, i, l) = N seconds, where N is 
an integer fixed for each corpus being mined. 
Function  O(nld)  computes  the  set  of  terms  representing 
objects in paragraph nld. This happens as follows: 
a.  Object Extraction. Compute the set T of terms in nld 
that represent objects, as per an ontology over the terms. 
We  currently  use  the  WordNet  [18]  ontology,  and 
include  all 
that  have  either  “object”  or 
“substance” as hypernyms.  

terms 

b.  Noun-Phrase Extraction. Compute the subset of terms 
in  T  that  are  used  as  nouns  in  the  original  paragraph, 
nld.  We  currently  use  the  QTag  tagger  [10]  on  the 
incoming paragraph for part-of-speech tagging. We run 
the  tagged  paragraph  through  a  customized  regular-
expression based noun-phrase extractor, which extracts 
the set of maximal phrases that contains only nouns. 

Function P(o, i, l) computes the probability that object oi  is 
involved in step i of the activity labeled l. We consider two 
alternative approximations for P, neither of which makes an 
effort to distinguish between two different steps of a model 
i.e. they ignore parameter i of function P. 
a.  Fixed Probabilities. P(o, i, l) = Pobj. where 0<Pobj<1 is 
fixed for each corpus being mined. In our experiments 
below we use Pobj  = 0.5. 

b.  Google Conditional Probabilities (GCP). P(o, i, l) = 
GoogleCount(“l”+o)/GoogleCount(l).  GoogleCount(s) 
is the number of pages on the web matching string s as 
reported by Google [6], and s+s’ is the concatenation of 
string s and s’. For example, if the phrase “making tea” 
has  24,200 matches, and the phrase “making tea” cup 
has  7,340  matches,  we  conclude  that  the  conditional 
probability  of  a  cup  being  involved  in  (any  step  of) 
making tea is 7340/24,200 = 0.3.  
Cup 
Stapler 

keyboard 

wrench 

Diaper 

remote 
control 

object→ 
↓activity 
making 
tea 
changing 
baby 
watching 
television  
copying 
paper 
sending  e-
mail 
auto repair 

0.30 

0.01 

0.01 

0.09 

0.14 

0.08 

0.07 

0.01 

0.06 

0.03 

0.01 

0.01 

0.01 

0.00 

0.01 

0.09 

0.06 

0.02 

0.00 

0.00 

0.00 

0.01 

0.00 

0.00 

0.03 

0.07 

0.02 

0.05 

0.04 

0.04 

0.01 

0.00 

0.00 

0.00 

0.00 

0.05 

Table 1: GCP for some common activities and objects 

576The GCP is predicated on what we call the mirror assumption: the 
probability of an object being involved in an activity in the real 
world is reflected by the probability of the phrases describing the 
two co-occurring in human discourse (and therefore on the web). 
It practice, since we interpret the results of our inference engines 
as  likelihoods  rather  than  probabilities,  it  is  sufficient  for  our 
purposes  that  the  probabilities  are  consistent;  if  the  actual 
probability  of  involvement  of  one  object-activity  pair  is  lower 
than that of the other, the GCP of the one should be lower than 
that of the other.  
Table 1 presents the GCPs for six activities and six objects. We 
pick  the  objects  such  that  the  i’th  object  is  intuitively  one  that 
would be involved in activity i, but not in any other. The i’th row 
and i’th column of the table contains the GCP of the i’th object 
being involved in the j’th activity. 
 In most cases, as expected, the table has its highest values along 
the  diagonal.  Pairings  that  intuitively  seem  unlikely  (such  as 
“diaper”  and  “making  tea”)  have  substantially  lower  value  than 
more plausible ones (e.g. “cup” and “making tea”). Although the 
relative values of the probabilities are on the whole sensible, the 
absolute  values  are  lower  than  one  might  expect  in  most  cases. 
For instance one expects to use a keyboard more than 4% of the 
time  when  sending  e-mail!  It  is  therefore  important  that  any 
scheme  that  uses  GCP  require  at  most  consistency,  but  not 
absolute accuracy, of probabilities. 
3.3  Example 
To  show  how  the  model  extractor  works,  we  apply  the  model-
mining  steps  of  the  previous  subsection  to  the  directions  for 
making tea:  
1. 

[Labeling]  As  per  step  1,  the  label  for  the  new  activity  is 
“Making Tea”. 
[Parsing Steps] As per step 2, the new activity has m = 3 
steps.  
a.  Since  none  of  the  steps  in  the  figure  have  a keyword-
delineated duration, we do not ascribe durations to any 
of the steps. 

2. 

b.  Figure  5  shows  the  result  of  applying  the  helper 
functions  to  each  of  the  paragraphs  1,  2  and  3  of  the 
incoming directions.  
i.  Sets T1 through T3 show terms describing objects, as 
extracted  by  the  OT  pass  for  each  of  the  three 
paragraphs.  Note  that  the  terms  include  “filling”, 
which 
in 
WordNet, is as a verb here. 

though  a  hyponym  of  “substance” 

ii.  Sets O1 through O3 are the subsets of T1 through T3, 
as  produced  by  the  POST  pass  that  are  used  as 
nouns in the directions. Since “filling” is used as a 
verb, it is eliminated here.  

iii.  Sets  s1  through  s3  are  the  result  of  adding  to  each 
object  the  probability  that  it  is  involved  in  the 
activity as a whole, using GCP to get probabilities.  
Note that this resulting model has structure very similar to 
that of the model in Figure 3, except that the probabilities 
are  different  and  that  the  new  model  has  no  timing 
constraints.  

3. 

[Tagged Object Filtering] Finally, suppose that we use the 
model to track activities in a space where the set Odeployed of 

names used for tagged objects is {kettle, stove, cup, teabag, 
milk}.  Sets  t1  through  t3  result  from  removing  from  sets  s1 
through s3 objects that are not in this set.  

After Object Extraction: 

T1: {kettle, faucet, stove} 
T2: {water, cup, filling, teabag}  
T3: {milk, sugar} 

After Noun Phrase Extraction: 

O1: {kettle, faucet, stove} 
O2: {water, cup, teabag}  
O3: {milk, sugar} 

After Google Conditional Probabilities: 

s1: {(kettle, 0.11), (faucet, 0.01), (stove, 0.08)} 
s2: {(water, 0.50), (cup, 0.30), (teabag, 0.01)}  
s3: {(milk, 0.16), (sugar, 0.16)} 
After Tagged Object Filtering:  
Odeployed = {kettle, stove, cup, teabag, milk} 

t1: {(kettle, 0.11) , (stove, 0.08)} 
t2: {(cup, 0.30), (teabag, 0.01)}  
t3: {(milk, 0.16)}

Figure 5: Steps in Mining the Directions for Making Tea 

 

4.  EVALUATION 
We have used the techniques described in the previous section to 
mine  roughly  21,300  models  from  two  websites  that  provide 
directions  for  activities.  We  mined  ehow.com  for  roughly  2300 
directions  on  performing domestic tasks (from “boiling water in 
the  microwave”  to  “change  your  air  filter”),  and  ffts.com  and 
epicurious.com for a further 400 and 18,600 recipes respectively. 
In the rest of this section, we evaluate these models. None of the 
models  we  mined  for  this  set  of  experiments  have  keyword-
delimited  timing  constraints;  we  have  used  a  default  of  five 
seconds per activity step. 
We  are  interested  in  answering  two  main  questions  through  our 
evaluation: 
1.  Are the models good enough? We wish to evaluate whether 
the models we mined contain sufficient information to enable 
correct  recognition  of  object  traces  gathered  from  actual 
humans  performing  activities.  The  subjects  may,  of  course, 
perform  the  activities  in  a  wide  variety  of  ways,  most  of 
which will hopefully be captured by the model. 

2.  How  effective  are  the  various  steps  that  comprise  the 
model  extractor?  We  wish  to  evaluate  the  impact  of  the 
various steps (and of alternate design options) of the mining 
scheme.  

We assume for the evaluation that the inference engine is fixed to 
be the Monte-Carlo based solver sketched in section 2.2.3. 
A  comprehensive  but 
to  determine 
sufficiency  and  necessity  would  be  to  collect,  from  real-world 
activities,  traces  that  exercise  all  models  we  have  mined.  These 
traces would represent the large variety of ways in which people 
perform  activities.  By  comparing  to  ground  truth  the  results 
reported  by  the  inference  engine  on  these  traces,  we  could 

impractical  strategy 

577evaluate sufficiency of all the models. By comparing accuracy of 
results  with  various  (combinations  of)  mining  techniques  turned 
off, we could evaluate the efficacy of those techniques. 
Unfortunately,  it  is  impractical  to  collect  actual  usage  data  for 
thousands  of  activities.  The  activities  are time consuming, often 
complex  and  sometimes  unpleasant;  getting  subjects  to  perform 
all of them in various combinations is clearly an impractical task. 
Instead,  we  use  the  following  three  strategies  to  approximate 
comprehensive  evaluation.  The 
to 
increasingly  larger  classes  of activities, but provide increasingly 
indirect evidence about the quality of our system: 
1.  Human 

three  options  apply 

recognition 

activity-trace 

(HTR).  We 
instrumented  the  home  of  one  of  the  researchers  with  108 
RFID tags. Over a period of six weeks, we collected traces 
(consisting  of  time-stamped  RFID  tag  numbers)  from  14 
subjects, while each performed a randomly chosen 12 of 14 
Activities of Daily Living (ADLs). Our subjects all wore the 
glove-based  RFID  reader  (Figure  2)  to  allow  tracking  of 
touched objects. We picked our ADLs from state-mandated 
ADL lists. Each ADL (e.g. housework) corresponds to many 
ordinary  activities  (e.g.  washing  dishes,  making  beds, 
dusting, vacuuming). In all, our subjects picked from roughly 
50 activities of the latter kind. Subjects recorded activities as 
they performed them, to provide us with ground truth.  
To  measure  the  quality  of  our  models,  we  select  66  of  the 
models mined from ehow.com.  These correspond to the fifty 
activities being performed by the user (some user activities 
are  described  by  more  than  one  model).  We  measure  the 
accuracy  with  which  our  inference  engine  infers  activities 
given these models. We define accuracy below. 
Inter-corpus  consistency  (ICC).  In  the  absence  of  actual 
activity traces, we cannot measure the ability of our models 
to  represent  real  activities  directly.  However,  we  can  still 
indirectly measure the ability of our models to represent the 
diverse  ways 
in  which  activities  may  be  performed. 
Specifically,  if  an  activity  model  mined  from  a  particular 
corpus  is  to  represent  most  instances  of  the  corresponding 
activity,  it  must,  in  particular  be  compatible  with  another 
representation  of 
latter 
description  presumably  represents  some  person’s  way  of 
performing the activity).  
Based  on  the  above  insight,  we  use  two  different  corpora 
(epicurious.com and ffts.com) containing the same activities 
to  test  the  quality  of  our  models.  Each  corpus  contains 
roughly  120  recipes  for  making  cookies.  We  first  generate 
models  for  all  activities  in  the  two  corpora.  Next,  for  each 
model  derived  from  one  corpus,  we  generate 
traces 
compatible with model: for each state in the model, in order, 
we include in the trace a subset of the objects corresponding 
to that state. Each object is picked with probability equal to 
its conditional probability pij. We measure the accuracy with 
which  we  infer  activities  on  these  traces  given  the  models 
generated from the second corpus. 
Intra-corpus distinguishability (ICD). Unfortunately, even 
inter-corpus comparisons are difficult to perform extensively 
due  to  lack  of  data.  Most  activities  in  ehow.com,  for 
instance,  are  not  easily  available  from  other  sources.  We 
therefore adopt a third strategy. Once again, we resort to an 
indirect  technique  for  measuring  if  the  mined  models  are 
sufficient  for  recognizing  traces  from  real  activities.  We 

the  same  activity  (since 

the 

2. 

3. 

restrict 

observe that if the models are good enough to recognize their 
corresponding  activities,  then  in  particular  each  model 
should have a stronger match than any of the other models 
on activities that conform perfectly to it. In other words, the 
models  must  contain  enough  information  to  distinguish 
themselves from each other. 
To  keep  the  number  of  activities  inferred  simultaneously 
manageable,  we 
testing 
distinguishability  between  models  from  the  same  activity 
domain.  To  this  end,  we  select  seven  domains  (ADLs, 
automobiles,  food,  grooming,  parties  and  the  two  cookie 
datasets 
of 
distinguishability,  we  measure  the  accuracy  with  which 
traces generated from models in a given domain are detected, 
given  the  set  of  all  models  in  the  domain.  Since  the  traces 
generated  are  “perfect”  with  respect  to  the  model  being 
matched against, the distinguishability metric is in a sense a 
“limit” on how well the models can do given our inference 
engine. 

above).  As 

a  measure 

described 

ourselves 

to 

The  three  strategies  above  rely  on  measuring  the  accuracy  with 
which activity traces can be detected. To determine accuracy over 
a  trace,  we  split  the  trace  into  five-second  windows.  Accuracy 
then refers to the fraction of windows where our inference engine 
gives the same result as ground truth. 
In the following subsections, we present the limit measurements 
from our ICD study, followed by the more direct quality measures 
from  the  HTR  and  ICC  measurements.  Next,  we  analyze  the 
contribution of the component techniques of the model extractor 
towards accuracy. Finally, we measure the effect of some of these 
techniques in producing more compact models. 
4.1  Intra-Corpus Distinguishability 
Figure  6  shows  the  distinguishability  for  each  of  the  seven 
domains mentioned above. The distinguishability of a domain is 
the accuracy across the result of concatenating all traces for that 
domain  i.e.  the  fraction  of  all  5-second  windows  across  all 
activities  in  the  domain  that  we  labeled  correctly.  The  numbers 
range from 67% for autos to 95% for the second cookie domain. 
The  implication  is  that  if  the  sensor  trace  from  an  activity 
conforms closely to that expected by the model for that activity, 
the models we mined for the domains are distinct enough, and our 
activity inference engine sensitive enough, that we can correctly 
identify the activity much of the time.  

y
c
a
r
u
c
c
A

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

ADLs

Auto

Food

Grooming
Domains

Parties

Cookies-1

Cookies-2

Figure 6: Distinguishability Within Activity Domains 

 

5784.2  Human and inter-corpus trace recognition 
Figure  7  presents 
the  HTR 
measurements  (testing  against  measured  human  traces,  labeled 
ADLs),  and 
inter-corpus 
consistency, labeled Cookie-1 and Cookie-2).  

the  accuracy  achieved  on 

ICC  measurements 

the 

(testing 

0.6

0.5

0.4

0.3

0.2

0.1

0

y
c
a
r
u
c
c
A

ADLs

Cookie-1
Domains

Cookie-2

 

Figure 7: Accuracy of Recognition for Three Datasets 

 
The  bar  labeled  “ADLs”  represents  the  accuracy  across  the 
unified trace that results from concatenating the traces from all 14 
subjects across all 12 activities that they performed. It therefore 
says  that  for  50%  of  all  5-second  timeslots,  we  are  able  to 
correctly  infer  the  ongoing  activity.  Although  the  number  may 
seem  modest,  we  point  out  for  comparison  that  a  random 
assignment of activities to slots, given that there are 66 possible 
activities being modeled, will yield only 1.5% accuracy. We also 
note  that  we  are  unaware  of  any  previous  work  that  recognizes 
such a large variety of activities being performed by such a large 
number  of  subjects 
in  an  unmodified  home  (even  with 
handcrafted activity models) with at least this level of accuracy. 
The  fact  that  recognition  rates  for  ADLs  on  real  traces  is 
significantly  lower  than  the  distinguishability  implies  that  the 
traces seen in practice deviate from models. There are two main 
reasons  for  this  deviation.  First,  the  traces  violate  some  of  the 
assumptions  underlying  our  models.  For  instance,  many  objects 
touched  and  were  not  tagged,  tagged  objects  were  sometimes 
missed,  and  objects  from  different  activities  were  interleaved. 
Second, the models were not fully general representations of the 
corresponding  activities.  For  instance,  they  imposed  extraneous 
ordering constraints, omitted possible objects or included optional 
objects. 
The  bar  labeled  Cookie-1  represents  the  accuracy  of  Cookie-1 
models across the unified trace that results from concatenating the 
individual traces generated from the models of Cookie-2, and vice 
versa for the bar labeled Cookie-2. As explained previously, these 
bars  address  the  question  of  how  generally  valid  the  models 
mined  from  each  of  these  corpora  are.  In  comparison  with  the 
95% distinguishability rates, the accuracy is roughly 40% in both 
cases. There are two reasons for this discrepancy. One, of course, 
is that the identical recipe can have quite different structure in the 
two corpora. The other is that for some of the recipes, there is no 
(unique)  counterpart  in  the  other  corpus.  Our  results  may  have 
improved had we removed these. 
4.3  Impact of techniques on accuracy 
We wish to measure the relative impact of our various techniques 
on  recognition  accuracy.  Accordingly,  we  use  the  full  system, 

with all techniques in use, as a baseline and remove parts of the 
system one at a time. Figure 8(a) and (b) present the efficacy of 
the various techniques that comprise our model extractor on the 
ADLs and the two cookie datasets respectively. In each chart, the 
bar  labeled  “All”  uses  all  available  techniques.  The  other  bars 
represent  our  models  generated  without  Google  probabilities 
(using  a  fixed  probability  of  0.5  for  all  objects),  without 
performing  tagged object filtering, and without performing part-
of-speech  tagging  and  noun  phrase  extraction,  respectively.  The 
error bars show a 95% confidence interval around the results. 
 

y
c
a
r
u
c
c
A

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

All

No Google

No Filt

No POS  

Model Variations

Figure 8(a): Impact of Selectively Disabling Mining 

Techniques with ADL Dataset

 
 
 

y
c
a
r
u
c
c
A

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

Cookies-1
Cookies-2

All

No Google

No Filt

No POS  

Model Variations

Figure 8(b): Impact of Selectively Disabling Mining 

Techniques with Cookies Dataset 

 
The markedly different shapes of the two charts make clear that 
the domains of ADLs and Cookies are quite different. In the ADL 
domain,  using  Google  probabilities  actually  seems  to  hurt 
accuracy.  We  therefore  made  an  additional  comparison  between 
using none of our techniques and using only Google probabilities. 
This comparison is shown in Figure 9, with the “All techniques” 
accuracy for comparison. Clearly, Google probabilities alone do 
have  a  positive  effect.  We  believe  that  these  various  techniques 
interact, so that doing them all together does not necessarily help. 
The  Google  probabilities  provide  some  information,  but,  as  we 
saw in Error! Reference source not found., the probabilities are 
noisy;  once  we  have  done  part-of-speech  tagging  and  object 
filtering, Google probabilities provide no additional information, 
but the inherent noise hurts overall accuracy. This provides strong 
motivation for future work in improving these probabilities. 
In comparison to the ADL domain, the Cookie domain involves 
many  more  activities  and  many  more  objects,  and  our  best 

 

 

579is  significantly  worse.  In 

performance 
this  domain,  each 
technique  contributes  some  additional  information,  without  as 
much  apparent  interaction.  As  a  result,  we  see  the  highest 
accuracy  when  using  all  techniques.  These  differences  between 
the  ADL  and  Cookie  domains  suggest  that  choosing  the  best 
technique for a particular problem domain may depend highly on  
the structure of that domain.  

y
c
a
r
u
c
c
A

0.6

0.55

0.5

0.45

0.4

0.35

0.3

Nothing

Google Only  

Model Variations

All

 

Figure 9: Impact of Google probabilities with ADL Dataset 
4.4  Impact of techniques on compactness 
The various techniques we apply in the extractor have significant 
impact on the resulting models. Figure 10 shows how the average 
number of objects per activity varies as we apply part-of-speech 
tagging  and  tagged  object  filtering  to  models  in  the  ADL  and 
Cookie domains. As discussed in Section 4.3, these two domains 
differ  significantly  in  their  structure.  ADL  activity  models  are 
only  somewhat  reduced  when  we  filter  for  noun  phrases,  but 
drastically  reduced  by  tagged  object  filtering.  The  resulting 
models  are  significantly  smaller.  In  fact,  approximately  30%  of 
activity  steps  become  empty;  when  we  remove  these  empty 
nodes, we change the structure of the resulting activities. On the 
other  hand,  the  Cookie  activities  are  barely  affected  by  tagged 
object  filtering,  but  are  significantly  reduced  by  part-of-speech 
tagging.  These  models  contain  many  more  objects,  and  these 
techniques do not create any significant change in structure. 

60

50

40

30

20

10

0

l

e
d
o
M
y
t
i

 

v

i
t
c
A
 
r
e
p

 

s
t
c
e

j

 

b
O
n
a
e
M

Original
POS
Filtering
Both

 

ADLs

Cookie-1
Domain

Cookie-2

Figure 10: Impact of Mining Techniques on Model Size 

5.  RELATED WORK 
To the best of our knowledge our work is the first to mine models 
of  human  physical  activities  directly  from  textual  or  web-based 

descriptions. Related work falls into two main categories, and we 
view our work as being complementary to both. 
In the past few years, much work has gone into extracting models 
of  human  beliefs  (other  than  activities)  such  as  shared  interests 
[17], preferences [3], reputations [5], concept definitions [9] and 
spam [16] by mining virtual communities such as the web, chat 
rooms  and  newsgroups.    In  many  cases,  these  efforts  have 
revolved around a mixture of techniques similar to those we use: 
lightweight  natural  language  processing  (NLP),  document  co-
occurrence  analysis  for  detecting  semantic  proximity,  mining 
topic and community-specific corpora. Even the notion of Google 
Conditional  Probability  (GCP)  has  in  essence  been  used  for 
extracting  conditional  probability  of  beliefs  [7],  although  our 
application  of  Google  to  real-world  phenomena  (via  the  mirror 
assumption)  is  novel.  These  projects  also  employ  a  variety  of 
techniques  (such  as  link-structure  analysis  and  more  advanced 
NLP techniques) that could profitably be used in our system. In 
some cases, the mined models are even used by virtual sensors to 
recognize  new  data;  for  instance,  spam  classification  software 
uses mined models to classify incoming data. None of this work, 
however, makes the connection between mining models from data 
and recognizing activities in the physical world. 
An  extensive  literature  exists  on  activity  recognition,  most  of  it 
using computer vision. In most cases [1][11][12], the focus is on 
identifying  features,  model  structure  and  mining  algorithms  that 
enable robust detection of small subsets of activities. The models 
used  in  these  systems  are  constructed  and  labeled  by  hand.  In 
some of these cases, the parameters of the model are learned on 
the fly, whereas the structure and labels are provided by clients of 
the  system.  Similarly,  some  systems  [12]  learn  patterns  of  user 
behavior, essentially modeling their likely future actions, but do 
not recognize or model activities per se. 
A  smaller  set  [2][8]  of  systems  learn  quantitative  models  of 
classes  of  activities;  they  observe  video  footage  depicting  the 
movement  of  pre-defined  robust  features  and  extract  probability 
density 
feature 
configurations  as  “interesting”  models.  The  models  so  extracted 
are sufficient for their intended purpose (e.g. enabling computers 
to  mimic  facial  movements  and  gestures,  and  detecting  periodic 
phenomena in footage of people’s lives). However, in order to use 
them  as  part  of  a  library  of  activities,  the  extracted  models  still 
have to be labeled. Viewing the quantitative models and labeling 
them is a challenging task for humans.  
A final small class of systems [4] extracts models automatically 
from  video,  but  provides  qualitative  descriptions  of  the  footage. 
For  instance,  Fernyhough  et  al.  are  able  to  detect  one  car 
overtaking another in video footage and report the phenomena as 
a sequence of three operators: traveling behind-right in the same 
direction,  traveling  right  in  the  same  direction,  and  traveling 
infront-right  in  the same direction, rather than as a probabilistic 
graphical model over low-level features. The resulting description 
models are, of course, much easier for humans to parse and then 
label.  However,  it  is  conceivable  that  more  models  with  more 
steps will get more difficult to comprehend or specify even in this 
qualitative  fashion.  Further,  the  system  designer  has  to  build  in 
the primitives (such as infront-right) and the grammar to combine 
them. 
We  view  our  techniques  as  being  complementary  to  the  ones 
discussed above. In particular, our models are suited to modeling 
activities  that  are  characterized  by  the  identity  of  objects 
involved. 
In  many  cases,  however,  activities  are  better 

functions  over  possible 

sequences  of 

580characterized  by  geometry,  color  and  texture  information;  the 
above techniques are more suited for such activities.  
6.  SUMMARY AND FUTURE WORK 
We  show  how  the  use of novel sensing technology (RFID tags) 
allows  us  to  view  human  activity  models  as  probabilistic 
translations  from  natural  language  phrases  to  words  describing 
objects.  Given  this  view  of  models,  we  provide  a  suite  of 
techniques that allow conventional natural language descriptions 
of  activities  (such  as  how-tos  and  recipes)  to  be  completely 
automatically  converted  into  formal  models  of  activities.  We 
show,  through  a  combination  of  measured  data  of  human 
activities and simulations, that the models we generate are useful. 
We  also  provide  an  analysis  of  the  contribution  of  our  mining 
techniques  to  improving  the  accuracy  and  compactness  of  our 
models. Our techniques have allowed us to construct many orders 
of  magnitude  more  models  for  human  activities  than  described 
previously. 
This  paper  is  essentially  an  introduction  to  the  idea  of  mining 
activity detection from the web. We have identified an extensive 
research agenda that follows up on these ideas. 
Our  first  order  of  business  for  the  future  is  to  perform  a  more 
comprehensive  evaluation  of  our  models.  We  intend  to  collect 
human  traces  on  many  more  activities,  with  many  more  tagged 
objects  in  the  activity  space  (a  few  thousand  as  opposed  to  the 
current  hundred  or  so),  and  on  more  challenging,  but  common, 
patterns of activity such as multi-person and interleaved activities. 
We would like to find corpora that provide alternate descriptions 
for  many  of  our  activities,  both  for  performing  inter-corpus 
consistency  comparisons,  and  more  importantly  to  examine 
techniques  for  combining  multiple  descriptions  for  a  given 
activity.  Finally,  we  have  not  yet  attempted  to  mine  timing 
constraints on activity steps on any large scale; we hope to locate 
corpora  where  timing  constraints  can  be  extracted  easily  and  to 
test the usefulness of the timed models that result.  
We  also  plan 
the 
effectiveness  of  our  mined  models.  We  are  examining  how  to 
mine  observables  relevant  to  sensors  other  than  RFID  tags.  In 
particular, we intend to include location in our models, extracting 
location  information  from  textual  descriptions.  For example, the 
activity  representing  “baking  cookies”  might  be  annotated  with 
the location “kitchen”. The location observables in the model are 
then triggered by location sensors as opposed to RFID sensors.  
Close examination of how we fail in recognizing certain activities 
has led us to believe that it will be profitable to model activities as 
partial  orders,  as  opposed  to  lists.  We  are  considering  simple 
natural-language 
determine 
dependencies  between  two  steps  of  a  list  of  directions.  In 
addition,  human  activities  are  often  described  in  terms  of  each 
other,  as  when  the  recipe  “basic  pie  crust”  is  given  as  a 
component of the recipe “apple pie”. We intend to augment our 
model of activities to allow references to other activities, and to 
then  mine  these  references  based  on  syntactic  cues.  We  expect 
that,  in  many  cases,  clients  of  our  system  may  be  interested  in 
activities  for  which  no  step-by-step  description  is  known  (e.g. 
“paying bills”). In these cases, we suspect that we can still create 
simple  naïve-Bayesian  models  by  simply  identifying  object 
names  that  are  “semantically  close”  to  the  phrase  describing 
activities.  Our  early  forays  in  this  direction  have  been  quite 
promising.  

techniques  for 

to  explore 

improving 

processing 

techniques 

to 

A difficulty in connecting mined activities with tagged objects is 
that the activity models may refer to objects synonymously. For 
example, we might have an object in our kitchen tagged “skillet” 
while  our  pancake  recipe  calls  for  a  “frying  pan”.  One  possible 
solution  to  the  problem  is  to  use  synsets  to  represent  objects. 
Synsets  –  collections  of  synonymous  words  –  can  be  extracted 
from WordNet. Another solution would be to classify objects with 
an object ontology – again, an ontology can be constructed from 
WordNet  –  and  recognize  objects  that  are  close  in  the  ontology 
can sometimes be used interchangeably in activities. 
In this paper, we assumed that the spaces in which activities are 
performed  have  been  extensively  tagged  with  RFID  tags.  In 
practice, however, the person tagging the space may have only a 
limited  number  of  tags  available,  and  may  value  feedback  on 
which objects are most profitable to tag, given the set of activities 
to be detected. We call this the “eigenobject” problem, and have 
preliminary ideas on solving it.  
7.  ACKNOWLEDGMENTS 
We 
thank  Suzi  Soroczak  and  Meliha  Yetisgen-Yildiz  for 
performing  the  noun  phrase  extraction.  We  thank  Henry  Kautz 
and Tanzeem Choudhury for feedback and discussions.  
8.  REFERENCES  
[1]  Bobick, A. F. and Ivanov, Y.A. Action Recognition Using 

Probabilistic Parsing. CVPR 1998: 196-202 .  

[2]  Clarkson, B. Life Patterns: Structure from Wearable Sensors. 

PhD Thesis, MIT, 2002. 

[3]  Dave, K., Lawrence, S., and Pennock, D. M. Mining the 

Peanut Gallery: Opinion Extraction and Semantic 
Classification of Product Reviews. WWW 2003: 519-528 
[4]  Fernyhough, J., Cohn, A. G. and Hogg, D. Constructing 

Qualitative Event Models Automatically from Video Input. 
Image and Vision Computing, 18, 2000, pages 81-103. 

[5]  Gibson, D. Communities and Reputation on the Web. PhD 

Thesis, UC Berkeley, 2002.  

[6]  Google. http://www.google.com 
[7]  Heylighen, F. Mining Associative Meanings from the Web: 

From Word Disambiguation to the Global Brain. 
Proceedings of Trends in Special Language and Language 
Technology, R. Temmerman (ed.), Standaard Publishers, 
Brussels, 2001. 

[8]  Jebara, T. and Pentland, A. Action Reaction Learning: 
Automatic Visual Analysis and Synthesis of Interactive 
Behaviour. ICVS 1999: 273-292 

[9]  Liu, B., Chin, C. W., and Ng, H.T. Mining Topic-specific 
Concepts and Definitions on the Web. WWW 2003: 251-
260. 

[10] Mason, O. QTag home page. http://web.bham.ac.uk/ 

o.mason/software/tagger/index.html 

[11] Moore, D.J., Essa, I. A., and Hayes, M. H. III. Exploiting 
Human Actions and Object Context for Recognition Tasks. 
In IEEE International Conference on Computer Vision, 
volume 1, pages 80-86, Corfu, Greece, September 1999. 

[12] Mozer, M. C. (1998). The Neural Network House: An 

Environment that Adapts to its Inhabitants. In M. Coen 

581(Ed.), Proceedings of the American Association for 
Artificial Intelligence Spring Symposium on Intelligent 
Environments (pp. 110-114). Menlo, Park, CA: AAAI 
Press.  

[13] Oliver, N., Horvitz, E., and Garg, A. Layered 

Representations for Human Activity Recognition. In Fourth 
IEEE Int. Conf. on Multimodal Interfaces, pages 3--8, 2002. 
[14] Patterson,  D.,  Liao,  L.,  Fox,  D.  and  Kautz,  H.  Inferring 
High-Level  Behavior  from  Low-Level  Sensors.  Ubicomp 
2003. 

Research  Seattle  Technical  Memo  IRS-TR-03-013, 
December 2003. 

[16] Sahami,  M., Dumais, S., Heckerman, D. and Horvitz, E. A 
Bayesian  Approach  to  Filtering  Junk  E-mail.  In  AAAI-98 
Workshop on Learning for Text Categorization, 1998. 

[17] Schwartz, M. F. and Wood, D. C. M. Discovering Shared 

Interests using Graph Analysis. Communications of the 
ACM, 36(8):78--89, 1993. 

[18] WordNet. www.cogsci.princeton.edu/~wn 

[15] Philipose, M., Fishkin, K., Perkowitz, M., Patterson, D., and 
Haehnel,  D.  The  Probabilistic  Activity  Toolkit:  Towards 
Interfaces.  Intel 
Enabling  Activity  Aware  Computer 

 

582