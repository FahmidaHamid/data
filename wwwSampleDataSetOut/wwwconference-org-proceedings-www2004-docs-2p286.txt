Matching Web Site Structure and Content

Vassil Gedov1

Carsten Stolz2

Ralph Neuneier3

Michal Skubacz3

Dietmar Seipel1

fgedov,seipelg@informatik.uni-wuerzburg.de, University of W¨urzburg, Germany1

carsten.stolz@ku-eichstaett.de, University of Eichst¨att-Ingolstadt, Germany2

fralph.neuneier,michal.skubaczg@siemens.com, Siemens AG, Corporate Technology, Germany3

ABSTRACT
To keep an overview of a complex corporate web sites, it is
crucial to understand the relationship of contents, structure
and the user’s behavior. In this paper, we describe an ap-
proach which is allowing us to compare web page content
with the information implicitly de(cid:12)ned by the structure of
the web site. We start by describing each web page with
a set of key words. We combine this information with the
link structure in an algorithm generating a context based
description. By comparing both descriptions, we draw con-
clusions about the semantic relationship of a web page and
its neighborhood. In this way, we indicate whether a page
(cid:12)ts in the content of its neighborhood. Doing this, we implic-
itly identify topics which span over several connected web
pages. With our approach we support redesign processes by
assessing the actual structure and content of a web site with
designer’s concepts.

General Terms: Algorithms

Categories and Subject Descriptors: H.3.3 Informa-
tion Systems Information search and retrieval; I.5.4 Com-
puting methodologies Applications

Keywords: Web Structure, Web Content Mining, Seman-
tic Description

1.

INTRODUCTION

Facing the complexity of dynamically generated corpo-
rate web sites, it becomes increasingly di(cid:14)cult to under-
stand user navigation patterns without deep knowledge of
the content and the semantical linkage of the pages. Another
challenge for web site owners as well as web strategists is to
keep track of the information structures on the web site in
the content management system. Following recent research
concerning integration of web structure, content and usage
mining [1] [2], we intend to provide an insight into the re-
lationship between the structure and the content. We (cid:12)rst
try to extract the key content from each page taking its hy-
pertext markup into account. Then, we reason about the
role of the web structure[3] since we believe that the linkage
created by the web designer conceals a semantic connection
between web pages.

Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, NY USA.
ACM 1-58113-912-8/04/0005.

2. COMBINING STRUCTURE AND CON-

TENT INFORMATION

Before associating structure and content with each other,
we will try to de(cid:12)ne them. The structure of a web site is
determined by the link structure between its web pages. A
link in this context is considered to be a link encoded as a
HTML tag. We do not consider the anchor text of a link
as structural information since we believe it belongs to the
content information. Excluding self-references, we focus on
hard coded inter-page links represented by a directed cyclic
graph.

The content of a web site may consist of text, hyper-
text, meta information or multimedia content like pictures,
(cid:12)gures, video or sound. Here, we concentrate on text and
html since they contain most of the information. As more
and more web sites use di(cid:11)erent media types it could be
reasonable to include those in future work .

In order to combine structure and content, we have to
map structural and content information to a common
concept. We achieve it by putting them in a common data
structure, in order to make them comparable. Next, we com-
bine the textual content of a web page with linked contents,
and thus we implicitly create a content-structure graph.

3. ALGORITHM

In this section we describe how to create a content - struc-
ture graph and compare it with the structure graph of a web
site. Thereafter, we present a way to measure the distance
between both.

3.1 Structure and Content Description

From each page we extract the links within a web site
generating an implicit directed cyclic structure graph. We
consider site-internal links for each web site, respectively.
In order to describe the content, we extract all words and
stem them. Further more, stop words are (cid:12)ltered as we are
only interested in content, and not in style or grammatical
information. Next, we calculate word frequencies per web
page. We assume that the most frequent words de(cid:12)ne the
content of the web page. The highest ranked words are said
to be key words, where their number is proportional to the
text length. Not all key words contribute to the semantic:
very frequent key words like the company’s name, occurring
virtually on all pages, are pruned. Whereby the pruning
threshold must be determined empirically. Additionally to

286I

I

I

I

I

x
i

O

LCp

i

OI

O

O

O

Figure 1: The Local Context LC

the key words, phrases from HTML tags are taken into ac-
count since we believe that they re(cid:13)ects author’s focus. This
could be for example, <h1> which typically describes the
major topic.
3.2 Mapping

For a single page, we combine its set of key words with
the key words of its direct neighbourhood. The latter is
de(cid:12)ned as the pages, having a direct link to (inedges) or
from (outedges) the page in focus (see (cid:12)gure 1). We call it
the local context (LC) of a page xi where:

LC(xi) = fxj jxj 2 (inedges(xi) [ outedges(xi))g

(1)

We perform the combination of structure and content by
generating a new set of key words. The new set is initiated
with the own key words of the processed web page. Now,
involving the site structure, we add the key words from the
LC and accumulate the frequencies over the entire key word
set. Additionally, the anchor tags from the LC are included
as key words. To avoid overweighting of the LC, we assigned
low constant frequencies to the key words originating from
it. Then, we arrange the set according to the accumulated
frequency. The number of words, which we keep, is again
proportional to the text length. We perform this process
bottom-up with respect to the minimal click path from the
entry point of the web site. For already processed pages we
use the new combined descriptions.

We interpret the results of our algorithm as an intersec-
tion between the contents of the LC and the particular page.
Moreover, we assume that salient words ascend towards the
root page. In the empirical study, we will discuss the rela-
tionship between the key word set which describes the page
alone content and the new combined set.

4. EMPIRICAL STUDY

We selected corporate web sites of Bayer, Motorola and
Siemens in order to evaluate the performance of the above
introduced algorithm. For a random set of test pages, we
compared the automatically generated key word sets with
manually compiled abstracts.

Studying the results, we observed that the page key words
(like those in the left column of table 1) cover nearly all
topics mentioned in the abstracts which was desired.

Next, we analyzed the results of the mapping from section
3.2. An example is shown in the right column of table 1.
Comparing this combined set with the manual abstract as
well as with page key words, we observed that some key
words - topics fell out. These key words carried page speci(cid:12)c
information.
In contrast, the newly inserted words re(cid:13)ect
topics present in the LC of the page in focus. A manual
review of the LC con(cid:12)rmed the relevance of the new words.

Page key words

Network
revenue
Wireless
Quality
Operators
subscribers
Infrastructure
Operability
Capacity and Coverage

LC key words

13 Network
7 Operators
6 GPRS
6
Support
6 CDMA
6
5 GSM
1 UMTS
1 Contact Motorola

Brochure

32.03
14.11
11.9
10.3
8.14
5.92
5.53
4.22

4

Table 1: www.motorola.com/networkoperators

Regarding the two di(cid:11)erent sets of key words, the di(cid:11)er-
ences between them can be interpreted as follows.
In the
case of a homogeneous LC with topics matching the content
of the page in focus, we observed no or only slight changes
in its key word set. On the other hand, pages dealing with
di(cid:11)erent topics than its homogeneous LC, showed signi(cid:12)-
cant di(cid:11)erences. Whereas a heterogeneous LC, regardless of
the topic of the page it is unlikely to have any signi(cid:12)cant
in(cid:13)uence on the resulting set.

While evaluating the mapping, we observed several e(cid:11)ects
which led us to improvements: In contrast to the page-wise
extracts, the HTML tag based key words were removed from
mapping in order to rule out local information, speci(cid:12)c only
to one page in the LC. On the other hand, we kept the
anchor tags linking to this page as they enhance the relation
between structure and content.

Our experiments showed that by excluding inedges from
the LC, the key words re(cid:13)ected the relationship of the page
and the subsequent contents more suitably. By ’subsequent’
we imply that, except for navigation links, the outgoing links
lead to more detailed information on the parent topic.

5. CONCLUSIONS

We showed that our approach is able to make structure
and content comparable.
In this way, our approach can
indicate whether content of a page (cid:12)ts to content of the
web pages in its neighborhood whereas the neighborhood
is de(cid:12)ned by the link structure. Additionally, we implic-
itly identify topics which span over several connected web
pages. Which in a way leads us to discovering of semanti-
cal relationships. With our algorithm we can support web
designers and strategists by comparing their intentions with
the actual structure and content of a web site. A subsequent
redesign of a web site incorporating our results can improve
user perception and customer retention.

Our future research includes developing more di(cid:11)erenti-
ating measurements for the structure and content analysis.
Furthermore, we (cid:12)nd it interesting to combine our approach
with usage patterns to improve the topic identi(cid:12)cation ca-
pabilities of our algorithm.

6. REFERENCES
[1] K. Bharat and M. Henzinger. Improved algorithms for topic

distillation in a hyperlinked environment Proceedings of
SIGIR-98, p. 104{111, ACM Press, 1998

[2] A. Sun and E.-P. Lim. Web unit mining: (cid:12)nding and

classifying subgraphs of web pages. In Proceedings 12th Int.
Conf. on Information and knowledge management, p. 108{115.
ACM Press, 2003.

[3] Soumen Chakrabarti. Mining the Web - Discovering

Knowledge from Hypertext Data. Morgan Kaufmann, 2002.

287