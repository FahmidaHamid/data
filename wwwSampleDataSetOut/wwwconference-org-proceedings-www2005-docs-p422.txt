TrustGuard: Countering Vulnerabilities in Reputation

Management for Decentralized Overlay Networks

Mudhakar Srivatsa, Li Xiong, Ling Liu

College of Computing

Georgia Institute of Technology

fmudhakar, lxiong, lingliug@cc.gatech.edu

ABSTRACT
Reputation systems have been popular in estimating the trustwor-
thiness and predicting the future behavior of nodes in a large-scale
distributed system where nodes may transact with one another
without prior knowledge or experience. One of the fundamental
challenges in distributed reputation management is to understand
vulnerabilities and develop mechanisms that can minimize the po-
tential damages to a system by malicious nodes. In this paper,
we identify three vulnerabilities that are detrimental to decen-
tralized reputation management and propose TrustGuard (cid:0) a
safeguard framework for providing a highly dependable and yet
e(cid:14)cient reputation system. First, we provide a dependable trust
model and a set of formal methods to handle strategic malicious
nodes that continuously change their behavior to gain unfair ad-
vantages in the system. Second, a transaction based reputation
system must cope with the vulnerability that malicious nodes may
misuse the system by (cid:13)ooding feedbacks with fake transactions.
Third, but not least, we identify the importance of (cid:12)ltering out
dishonest feedbacks when computing reputation-based trust of a
node, including the feedbacks (cid:12)led by malicious nodes through
collusion. Our experiments show that, comparing with existing
reputation systems, our framework is highly dependable and e(cid:11)ec-
tive in countering malicious nodes regarding strategic oscillating
behavior, (cid:13)ooding malevolent feedbacks with fake transactions,
and dishonest feedbacks.

Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications;
C.4 [Performance of Systems]: Security, Reliability|
Reputation Management, Overlay Networks

General Terms
Security, Performance, Reliability

INTRODUCTION

1.
A variety of electronic markets and online communities have
reputation system built in, such as eBay, Amazon, Yahoo!
Auction, Edeal, Slashdot, Entrepreneur. Recent works [4,
1, 3, 11, 19] suggested reputation based trust systems as
an e(cid:11)ective way for nodes to identify and avoid malicious
nodes in order to minimize the threat and protect the sys-
tem from possible misuses and abuses by malicious nodes
Copyright is held by the International World Wide Web Conference Com›
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2005, May 10›14, 2005, Chiba, Japan.
ACM 1›59593›046›9/05/0005.

in a decentralized overlay networks. Such systems typically
assign each node a trust value based on the transactions it
has performed with others and the feedbacks it has received.
For example, XRep [4] provides a protocol complementing
current Gnutella protocol by allowing peers to keep track of
and share information about the reputation of other peers
and resources. EigenTrust [11] presents an algorithm similar
to PageRank [15] that computes a trust value by assuming
trust is transitive and demonstrated its bene(cid:12)ts in address-
ing fake (cid:12)le downloads in a peer-to-peer (cid:12)le sharing network.
However, few of the reputation management work so far
have focused on the vulnerabilities of a reputation system
itself. One of the detrimental vulnerabilities is that a mali-
cious node may strategically alter its behavior in a way that
bene(cid:12)ts itself such as starting to behave maliciously after it
attains a high reputation. Another widely recognized vul-
nerability is the shilling attack [12] where malicious nodes
submit dishonest feedback and collude with each other to
boost their own ratings or bad-mouth non-malicious nodes.
Last, but not the least, malicious nodes can (cid:13)ood numerous
fake feedbacks through fake transactions in a transaction-
based feedback system.

With these issues in mind, we present TrustGuard (cid:0) a
highly dependable reputation-based trust building frame-
work. The paper has a number of unique contributions.
First, we introduce a highly dependable trust model to e(cid:11)ec-
tively handle strategic oscillations by malicious nodes (Sec-
tion 3). Second, we propose a feedback admission con-
trol mechanism to ensure that only transactions with secure
proofs can be used to (cid:12)le feedbacks (Section 4). Third, we
propose feedback credibility based algorithms for e(cid:11)ectively
(cid:12)ltering out dishonest feedbacks (Section 5). We also present
a set of simulation based experiments, showing the e(cid:11)ective-
ness of the TrustGuard approach in guarding against each of
the above vulnerabilities with minimal overhead. We con-
clude the paper with a brief overview of the related work
(Section 7), and a conclusion (Section 8).

2. TRUSTGUARD: AN OVERVIEW
2.1 System Architecture

We (cid:12)rst present a high level overview of the TrustGuard
framework 1. Figure 1 shows a sketch of the decentralized
architecture of the dependable reputation management sys-
tem. The callout shows that each node has a transaction
1This research is partially supported by NSF CNS CCR,
NSF ITR, DoE SciDAC, CERCS Research Grant, IBM Fac-
ulty Award, IBM SUR grant, and HP Equipment Grant.

be identi(cid:12)ed through some digital certi(cid:12)cation based mecha-
nism. Readers may refer to [2, 18, 7] for a detailed discussion
on security issues in overlay networks.
2.2 Problem Statement and Solution Approach
The TrustGuard framework is equipped with several im-
portant safeguard components. In the rest of the paper, we
focus on the following three types of vulnerabilities, analyze
the potential threats and describe countermeasures against
such vulnerabilities using TrustGuard.
Strategic Oscillation Guard. Most existing reputation
systems such as eBay use a combination of average feedbacks
and the number of transactions performed by a node as indi-
cators of its trust value. Our experiments show that using a
simple average does not guard the reputation system against
oscillating behavior or dishonest feedbacks. For example, a
bad node may behave non-maliciously until it attains a good
reputation (re(cid:13)ected in its trust value) and then behave ma-
liciously. Or it could oscillate between building and milking
reputation. A dependable reputation system should be able
to penalize malicious nodes for such dynamic and strategic
behavioral changes. In TrustGuard, we promote the incor-
poration of the reputation history and behavior (cid:13)uctuations
of nodes into the estimation of their trustworthiness. We use
adaptive parameters to allow di(cid:11)erent weighting functions
to be applied to current reputation, reputation history, and
reputation (cid:13)uctuations.
Fake Transaction Detection. In a typical transaction-
based feedback system, after each transaction, the two par-
ticipating nodes have an opportunity to submit feedbacks
about each other. This brings two vulnerabilities. First, a
malicious node may (cid:13)ood numerous ratings on another node
with fake transactions. Second, a malicious node may sub-
mit dishonest feedback about a transaction. A dependable
trust model should be equipped with mechanisms to handle
malicious manipulation of feedbacks to guard the system
against such fake transactions, and to di(cid:11)erentiate dishon-
est feedback from honest ones. In TrustGuard approach, we
propose to bind a feedback to a transaction through trans-
action proofs. In other words, a feedback between nodes n
and m on a given transaction is stored if and only if n and
m indeed transacted with each other.
Dishonest Feedback Filter. While the fake transaction
detection guarantees that a feedback is associated with a
real transaction, a malicious node may submit dishonest
feedbacks in order to boost the ratings of other malicious
nodes or bad-mouth non-malicious nodes. The situation is
made much worse when a group of malicious nodes make
collusive attempts to manipulate the ratings. In this paper,
we build a dishonest feedback (cid:12)lter to di(cid:11)erentiate dishon-
est feedbacks from honest ones. The (cid:12)lter essentially assigns
a credibility value to a feedback source and weights a feed-
back in proportion with its credibility. We study two such
credibility measures and their e(cid:11)ectiveness in (cid:12)ltering out
dishonest feedbacks in both non-collusive and collusive set-
tings.

3. STRATEGIC MALICIOUS NODES
We de(cid:12)ne a strategic malicious node as a node that adapts
its behavioral pattern (with time) so as to maximize its ma-
licious goals. Consider a scenario wherein a bad node does
not misbehave until it earns a high trust value. The scenario
becomes more complicated when bad nodes decide to alter-

Figure 1: TrustGuard’s Architecture

manager, a trust evaluation engine and a feedback data stor-
age service. Whenever a node n wants to transact with an-
other node m, it calls the Trust Evaluation Engine to per-
form a trust evaluation of node m. It collects feedback about
node m from the network through the overlay protocol and
aggregates them into a trust value. Such computation is
guarded by strategic oscillation guard and dishonest feed-
back (cid:12)lters. The Transaction Manager consists of four com-
ponents. The trust-based node selection component uses
the trust value output from the trust evaluation engine to
make trust decisions before calling the transaction execution
component. Before performing a transaction, the transac-
tion proof exchange component is responsible for generating
and exchanging transaction proofs. Once the transaction
is completed, the feedbacks are manually entered by the
transacting users. The transacting nodes then route these
feedbacks to designated nodes on the overlay network for
storage through a decentralized overlay protocol (e.g. DHT
based protocol). The designated nodes then invoke their
data storage service and admit a feedback only if it passes
the feedback admission control where fake transactions are
detected. The feedback storage service is also responsible
for storing reputation and trust data on the overlay network
securely, including maintaining replicas for feedbacks and
trust values. We build the TrustGuard storage service on
top of PeerTrust [19].

Although we implement the TrustGuard framework using
a decentralized implementation that distributes the storage
and computation of the trust values of the nodes, it is im-
portant to note that one could implement TrustGuard using
di(cid:11)erent degrees of centralization. At one extremity, third-
party trusted servers could be used for both trust evalua-
tion and feedback storage. One can also utilize the trusted
servers to support only selected functionality, for example,
the transaction proof exchange (Section 4).

Finally, we assume that TrustGuard architecture is built
on top of a secure overlay network. Thus, the overlay net-
work should be capable of routing messages despite the pres-
ence of some malicious nodes and ensure that all nodes can

nate between good and bad behavior at regular or arbitrary
frequencies. In this paper, we primarily focus on strategic
oscillations by malicious nodes and describe concrete and
systematic techniques taken by TrustGuard to address both
steady and sudden changes in the behavioral pattern of a
node without adding heavy overheads to the system. Other
possible behavioral strategies that could be employed by ma-
licious nodes are not considered in this paper.

A dependable trust model should be capable of handling
the following four important issues: (P 1) sudden (cid:13)uctua-
tions in node behavior, (P 2) distinguish an increase and
decrease in node behavior, (P 3) tolerate unintentional er-
rors, and (P 4) re(cid:13)ect consistent node behavior. We propose
a dependable trust model that computes reputation-based
trust of a node by taking into consideration: current feed-
back reports about the node, its historical reputation, and
the (cid:13)uctuations in the node’s current behavior. First, we
present an optimization theory based cost metric (Section
3.1) to formalize our design goals and then present Trust-
Guard’s dependable trust model (Section 3.2).
3.1 Cost Model
The primary goal of our safeguard techniques is to maximize
the cost that the malicious nodes have to pay in order to gain
advantage of the trust system. We (cid:12)rst formally de(cid:12)ne the
behavior of a non-malicious and a malicious node in the sys-
tem using the game theory approach [5]. A non-malicious
node is the commitment type and a long-run player who
would consistently behave well, because cooperation is the
action that maximizes the player’s lifetime payo(cid:11)s. In con-
trast a strategic malicious node corresponds to an oppor-
tunistic player who cheats whenever it is advantageous for
him to do so. Now we formally describe a cost model for
building reputation-based trust and use this cost model to
illustrate the basic ideas of maximizing the cost (penalty)
to be paid by anyone behaving maliciously. Let T Vn(t) de-
note the trust value as evaluated by the system for node n
at time t (0 (cid:20) T Vn(t) (cid:20) 1). Let BHn(t) denote the actual
behavior of node n at time t (0 (cid:20) BHn(t) (cid:20) 1), modeled as
the fraction of transactions that would be honestly executed
by node n between an in(cid:12)nitesimally small time interval t
and t + dt. Then, we de(cid:12)ne the cost function for a node b
as shown in Equation 1.

cost(b) = lim
t!1

1
t

(cid:3) Z t

0

(BHb(x) (cid:0) T Vb(x)) dx

(1)

Let G be the set of good nodes and B be the set of bad nodes.
The objective is 8g 2 G : T Vg(t) (cid:25) 1 and 8b 2 B : cost(b)
is maximized. Figure 2 provides an intuitive illustration
of the above cost function for a strategic malicious node
oscillating between acting good and bad. Referring to Figure
2, observe that the problem of maximizing the cost paid by
the malicious nodes can be reduced to maximizing the area
under Yn(t)(cid:0)Xn(t), that is, minimizing the extent of misuse
(Xn(t) = max(T Vn()(cid:0)BHn(t); 0)) and maximizing the cost
of building reputation (Yn(t) = max(BHn(t) (cid:0) T Vn(t); 0)).
In addition to maximizing the cost metric, we require
TrustGuard to ensure that any node behaving well for an
extended period of time attains a good reputation. How-
ever, we should ensure that the cost of increasing a node’s
reputation depends on the extent to which the node misbe-
haved in the past.

1

0

Y
n

Xn

Behavior
Trust Value

nX  : Extent of Misuse
nY  : Work done to 

   build reputation

time

Figure 2: Cost of Building Reputation

3.2 Dependable Trust Model
Bearing the above analysis in mind, we present TrustGuard’s
dependable trust model in this section. Let R(t) denote the
raw trust value of node n at time t. Any of the existing
trust evaluation mechanisms such as [19, 11] can be used
to calculate R(t). The simplest form can be an average
of the ratings over the recent period of time. Let T V (t)
denote the dependable trust value of node n at time t and we
compute T V (t) using Equation 2. Note that R0(t) denotes
the derivative of R(x) at x = t.

T V (t) = (cid:11) (cid:3) R(t) + (cid:12) (cid:3)

1
t

(cid:3) Z t

0

R(x)dx + (cid:13) (cid:3) R0(t)

(2)

Equation 2 resembles a Proportional-Integral-Derivative con-
troller used in control systems [14]. The (cid:12)rst component
(proportional) refers to the contribution of the current re-
ports received at time t. The second component (integral)
represents the past performance of the node (history infor-
mation). The third component (derivative) re(cid:13)ects the sud-
den changes in the trust value of a node in the very recent
past. Choosing a larger value for (cid:11) biases the trust value
of a node n to the reports currently received about n. A
larger value of (cid:12) gives heavier weight to the performance
of the node n in the past. The averaging nature of the
proportional and integral components enables our model to
tolerate errors in raw trust values Rn(t) (P 3) and re(cid:13)ect
consistent node behavior (P 4). A larger value of (cid:13) ampli-
(cid:12)es sudden changes in behavior of the node in the recent
past (as indicated by the derivative of the trust value) and
handles sudden (cid:13)uctuations in node behavior (P 1). We dis-
cuss techniques to distinguish increase and decrease in node
behavior (P 2) later in this Section.

We now describe a simple discretized implementation of
the abstract dependable trust model described above. For
simplicity, we assume that the trust values of nodes are up-
dated periodically within each time period T . Let successive
time periods (intervals) be numbered with consecutive inte-
gers starting from zero. We call T V [i] the dependable trust
value of node n in the interval i. T V [i] can be viewed as a
function of three parameters: (1) the feedback reports re-
ceived in the interval i, (2) the integral over the set of the
past trust values of node n, and (3) the current derivative
of the trust value of node n.
Incorporating feedbacks by computing R[i]. Let R[i]
denote the raw reputation value of node n computed as an
aggregation of the feedbacks received by node n in interval
i. Let us for now assume that all the feedbacks in the sys-
tem are honest and transactions are not faked. In such a
scenario, R[i] can be computed by using a simple average
over all the feedback ratings received by node n in time in-
terval i. We defer the extension of our safeguard to handle
dishonest feedbacks and fake transactions later to sections 4
and 5 respectively.

Incorporating History by Computing Integral. We
now compute the integral (history) component of the trust
value of node n at interval i, denoted as H[i]. Suppose
the system stores the trust value of node n over the last
maxH (maximum history) intervals, H[i] could be derived
as a weighted sum over the last maxH reputation values of
node n using Equation 3.

H[i] =

maxH

Xk=1

R[i (cid:0) k] (cid:3)

wk

k=1 wk

PmaxH

(3)

The weights wk could be chosen either optimistically or pes-
simistically. An example of an optimistic summarization is
the exponentially weighted sum, that is, wk = (cid:26)k(cid:0)1 (typ-
ically, (cid:26) < 1). Note that choosing (cid:26) = 1 is equivalent to
H being the average of the past maxH reputation values of
node n. Also, with (cid:26) < 1, H gives more importance to the
more recent reputation values of node n. We consider these
evaluations of H optimistic since they allow nodes to attain
higher trust values rather quickly. On the contrary, a pes-
simistic estimate of H could be obtained with wk = 1
R[i(cid:0)k] .
Such an evaluation assigns more importance to those inter-
vals where the node behaved particularly badly.
Strengthening the dependability of T V [i]. Once we
have calculated the feedback-based reputation (R[i]) for the
node n in the interval i and its past reputation history (H[i]),
we can use Equation 4 to compute the derivative component
(D[i]). Note that Equation 4 uses H[i] instead of R[i (cid:0) 1]
for stability reasons.

D[i] = R[i] (cid:0) H[i]

(4)

We now compute the dependable trust value T V [i] for

node n in the interval i using Equation 5:

T V [i] = (cid:11) (cid:3) R[i] + (cid:12) (cid:3) H[i] + (cid:13)(D[i]) (cid:3) D[i]
where (cid:13)(x) = (cid:13)1 if x (cid:21) 0 and (cid:13)(x) = (cid:13)2 if x < 0

(5)

In this equation, T V [i] is derived by associating di(cid:11)erent
weights (cid:13)1 and (cid:13)2 for a positive gradient and a negative gra-
dient of the trust value respectively, enhancing the depend-
ability of T V [i] with respect to sudden behavioral changes
of node n. One of the main motivations in doing so is to set
(cid:13)1 < (cid:12) < (cid:13)2, thereby increasing the strength of the deriva-
tive component (with respect to the integral component)
when a node shows a fast degradation of its behavior, and
lowering the strength of the derivative component when a
node is building up its reputation (recall P 2 in our design
goal). Our experiments (see Section 6) show that one can
use the rich set of tunable parameters provided by Equation
5 to handle both steady and sudden changes in the behavior
of a strategic malicious node.
3.3 Fading Memories

In TrustGuard, we compute the dependable trust value
of a node n in interval i based on its current reputation,
its reputation history prior to interval i and its reputation
(cid:13)uctuation.
In computing reputation history, we assume
that the system stores the reputation-based trust values of
node n for the past maxH number of intervals. By using a
smaller value for maxH, we potentially let the wrong-doings
by a malicious node to be forgotten in approximately maxH
time intervals. However, using a very large value for maxH
may not be a feasible solution for at least two reasons: (i)
The number of trust values held on behalf of a long standing

t+1

t

t−1

t−2

t−3

t−4

t−5

t−6

t−7

t−8

FTV[0]

FTV[1]

FTV[2]

future

FTV’[0] FTV’[1]

FTV’[2]

past

FTV’[1] = FTV[0] + FTV[1] * (2  − 1)   FTV’[2] = FTV[1] + FTV[2] * (2  − 1)

1

2

1
2

2
2

Figure 3: Updating Fading Memories: F T V [i] de-
notes the faded values at time t and F T V 0[i] denotes
the faded values at time t + 1

member of the system could become extremely large. (ii)
The computation time for our trust model (Equations 3 and
5) increases with the amount of data to be processed. In the
(cid:12)rst prototype of TrustGuard, we introduce fading memories
as a performance optimization technique to reduce the space
and time complexity of computing T V [i] by allowing a trade-
o(cid:11) between the history size and the precision of the historical
reputation estimate.

We propose to aggregate data over intervals of exponen-
tially increasing length in the past fk0; k1; (cid:1) (cid:1) (cid:1) ; km(cid:0)1g into
m values (for some integer k > 0). Observe that the aggre-
gates in the recent past are taken over a smaller number of
intervals and are hence more precise. This permits the sys-
tem to maintain more detailed information about the recent
trust values of node n and retain fading memories (less de-
tailed) about the older trust values of node n. Given a (cid:12)xed
value to the system-de(cid:12)ned parameter m, one can trade-o(cid:11)
the precision and the history size by adjusting the value of
k.

Now we describe how we implement fading memories in
TrustGuard. To simplify the discussion, let us assume that
k = 2. With fading memory optimization, our goal is to
summarize the last 2m (cid:0) 1 (Pm(cid:0)1
i=0 2i = 2m (cid:0) 1) trust values
of a node by maintaining just m (=log2(2m)) values. This
can be done in two steps. (i) we need a mechanism to ag-
gregate 2m (cid:0) 1 trust values into m values, and (ii) we need
a mechanism to update these m values after each interval.
In the interval
t, the system maintains trust values in intervals t (cid:0) 1; t (cid:0)
2; (cid:1) (cid:1) (cid:1) ; t (cid:0) 2m in the form of m trust values by summarizing
intervals t (cid:0) 2j ; t (cid:0) 2j (cid:0) 1; (cid:1) (cid:1) (cid:1) ; t (cid:0) 2j+1 + 1 for every j (j =
0; 1; (cid:1) (cid:1) (cid:1) ; m (cid:0) 1), instead of maintaining one trust value for
each of the 2m (cid:0) 1 time intervals. Figure 3 provides an
example where k = 2 and m = 3.

TrustGuard performs Step 1 as follows.

Now we discuss how TrustGuard performs Step 2. Let
F T V t[j] (0 (cid:20) j (cid:20) m (cid:0) 1) denote the faded trust val-
ues of node n at interval t.
Ideally, re-computing F T V
for interval t requires all of the past 2m (cid:0) 1 trust values.
With fading memories we only store m summarization val-
ues instead of all the 2m (cid:0) 1 trust values. Thus, at inter-
val t we approximate the trust value for an interval t (cid:0) i
(1 (cid:20) i (cid:20) 2m) by F T V t[blog2 ic]. We use Equation 6 to ap-
proximate the updates to the faded trust values for interval j
(j = 0; 1; 2; (cid:1) (cid:1) (cid:1) ; m(cid:0)1) with the base case F T V t+1[0] = R[t].

Figure 3 gives a graphical illustration of Equation 6 for
m = 3.

(F T V t[j] (cid:3) (2j (cid:0) 1) + F T V t[j (cid:0) 1])

(6)

F T V t+1[j] =

2j
4. FAKE TRANSACTIONS
We have presented a dependable trust metric, focusing on
incorporating reputation history and reputation (cid:13)uctuation
to guard a reputation system from strategic oscillation of
malicious nodes. We dedicate this and the next section to
vulnerabilities due to fake transactions and dishonest feed-
backs and their TrustGuard countermeasures.

In TrustGuard, we tackle the problem of fake transac-
tions by having a feedback bound to a transaction through
a transaction proof such that a feedback can be successfully
(cid:12)led only if the node (cid:12)ling the feedback can show the proof of
the transaction. Our transaction proofs satisfy the following
properties: (i) Transaction proofs are unforgeable, and are
hence generated only if the transacting nodes indeed wished
to transact with each other, and (ii) Transaction proofs are
always exchanged atomically; that is, a malicious node m
cannot obtain a proof from a non-malicious node n without
sending its own proof to node n. The atomicity property
of the exchange of proofs guarantees fairness; that is, each
of the transacting parties would be able to (cid:12)le feedbacks
at the end of the transaction. In the absence of exchange
atomicity a malicious node m could obtain a proof from
node n but not provide its proof to the non-malicious node
n; hence, a non-malicious node n may never be able to (cid:12)le
complaints against the malicious node m. Note that if the
entire system is managed by a centralized trusted authority
(like eBay) then one can completely eliminate the problem
of fake transactions. Our focus is on building a distributed
and decentralized solution to handle fake transactions.

We (cid:12)rst present a technique to generate unforgeable proofs
that curb a malicious node from (cid:13)ooding feedbacks on other
non-malicious nodes. Then we employ techniques based on
electronic fair-exchange protocol to ensure that transaction
proofs are exchanged fairly (atomically). It is important to
note that the proofs act as signed contracts and are thus
exchanged before the actual transaction takes place. If the
exchange fails, a good node would not perform the transac-
tion. Nonetheless, if the exchange were unfair, a bad node
could (cid:12)le a feedback for a transaction that never actually
happened.

Note that the fake transaction detection does not prevent
two collusive malicious nodes from faking a large number
of transactions between each other, and further give good
ratings with exchanged transaction proofs. This type of col-
lusion will be handled by our next safeguard - dishonest
feedback (cid:12)lter.
4.1 Unforgeable Transaction Proofs
A simple and secure way to construct proofs of transactions
is to use a public key cryptography based scheme. Assume
that every node n has an associated pair of public key and a
private key pair, namely, hP Kn; RKni. We assume that the
public keys are tied to nodes using digital certi(cid:12)cates that
are notarized by trusted certi(cid:12)cation authorities. A trans-
action T is de(cid:12)ned as T = hT xn Descri k htime stampi,
where hT xn Descri is a description of the transaction and
the symbol k denotes string concatenation. Node n signs the
transaction with its private key to generate a transaction

proof P T = RKn(T ) and send it to node m. If the proofs
are fairly exchanged then node n would obtain RKm(T ) as
a proof of transaction T with node m and vice versa. The
key challenge now is how to exchange proofs atomically to
guarantee fairness.
4.2 Fair Exchange of Transaction Proofs

Signi(cid:12)cant work has been done in the (cid:12)eld of fair elec-
tronic exchange [16, 13], aiming at guaranteeing exchange
atomicity. There are several trade-o(cid:11)s involved in employ-
ing a fair-exchange protocol in the context of reputation
management.
In this section we analyze the feasibility of
trust value based fair-exchange protocol and optimistic fair-
exchange protocol for TrustGuard.
Trust Value based Fair-Exchange Protocol. Intuitively,
one could achieve fair exchange of proofs between nodes n
and m ( T Vn > T Vm) by enforcing that the lower trust value
node m sends its proof (cid:12)rst to the higher trust value node n;
following which the higher trust value node n would send its
proof to the lower trust value node m. However, this solu-
tion is (cid:13)awed. For example, a malicious node m with a high
trust value may always obtain a proof from non-malicious
node n with a lower trust value, but not deliver its proof
to node n. Hence, a malicious node may pursue its ma-
licious activities inde(cid:12)nitely without being detected by the
trust system.
Optimistic Fair-Exchange Protocol. In the (cid:12)rst proto-
type of TrustGuard, we adopt an optimistic fair-exchange
protocol for exchanging transaction proofs. Optimistic fair-
exchange protocols guarantee fair-exchange of two electronic
items between two mutually distrusting parties by utilizing
trusted third parties (ttp). However, they reduce the in-
volvement of a ttp to only those exchanges that result in a
con(cid:13)ict. Assuming that most of the parties in an open elec-
tronic commerce environment are good, the ttp is hopefully
involved infrequently.

In particular, TrustGuard adopts the optimistic protocol
for fair contract signing proposed by Micali [13]. The pro-
tocol assumes that the transacting parties n and m have
already negotiated a would-be contract C. The nodes n and
m now need to exchange the signed contracts, (RKn(C) and
RKm(C)) fairly. The protocol guarantees that if both the
nodes commit to the contract then node n has a proof that
node m has committed to the contract C and vice-versa;
even if one of the parties does not commit to the contract
C then neither party gets any proof of commitment from
the other party. We map this protocol for fairly exchang-
ing transaction proofs by using contract C as C = T =
hT xn Descri k htime stampi.

One of the major advantages of using such an optimistic
fair-exchange protocol is that the ttp need not be always
online. The ttp can infrequently come up online and resolve
all outstanding con(cid:13)icts before going o(cid:15)ine. A strategic
malicious node could exploit the delay in con(cid:13)ict resolution
as shown in Figure 4. Let us assume that the ttp stays online
for a time period Ton and then stays o(cid:15)ine for a time period
Tof f . When the malicious node is building reputation, it
behaves honestly and exchanges transaction proofs fairly (Y
in Figure 4). However, after the malicious node has attained
high reputation, it unfairly exchanges several proofs with
other nodes in the system. By synchronizing the schedule
of the ttp, the malicious node can ensure that none of the
con(cid:13)icts caused by its malicious behavior is resolved within

1

0

Y

X

Ton

Toff

Behavior
Trust Value

X: Extent of Misuse
Y: Work done to 

build reputation

time

Figure 4: Cost of Building Reputation with Delayed
Con(cid:13)ict Resolution

Tof f time units (X in Figure 4). Hence, despite of the fact
that the malicious node behaved badly over a period of Tof f
time units its reputation does not fall. However, the moment
all outstanding con(cid:13)icts are resolved, the malicious node’s
reputation falls very steeply. Observe that cost paid by a
malicious node (see Equation 1) is much lower in Figure 4
when compared to Figure 2 (wherein Tof f = 0).

In conclusion, one needs to choose Tof f very carefully so
that: (i) the attackers do not have enough time to compro-
mise the trusted third party, and (ii) maximize the cost paid
by those malicious nodes that strategically exploit delayed
con(cid:13)ict resolution.

5. DISHONEST FEEDBACKS
In the previous section we have discussed techniques to en-
sure that both transacting nodes have a fair chance to sub-
mit feedbacks. In this section we extend our safeguard model
to handle dishonest feedbacks. The goal of guarding from
dishonest feedbacks is to develop algorithms that can e(cid:11)ec-
tively (cid:12)lter out dishonest feedbacks (cid:12)led by malicious nodes
in the system.

We propose to use a credibility factor as a (cid:12)lter in es-
timating the reputation-based trust value of a node in the
presence of dishonest feedbacks. Recall that we use T Vn
to denote the dependable trust value of node n and Rn to
denote the reputation-based trust value of node n without
incorporating past history (Integral component) and (cid:13)uctu-
ations (Derivative component). The main idea of using a
credibility-based feedback (cid:12)lter in computing Rn is to as-
sign higher weight to the credible feedbacks about node n
and lower weight to the dishonest ones.

Concretely, we (cid:12)rst extend the naive average based com-
putation of trust value (Section 3.2) into a weighted aver-
age. Let I(n) denote the set of interactions (transactions)
performed by node n. Let Fn(u) denote the normalized
feedback rating (between 0 and 1) that a node n receives
after performing an interaction u with another node. Let
CRn(u) denote the feedback credibility of the node u:x who
submitted the feedback about node n after interaction u.
The reputation-based trust of node n can be computed as

Rn = Pu2I(n) Fn(u) (cid:3) CRn(u). The information about

the set of transactions performed (I(n)) and the feedbacks
received (Fn(u) for u 2 I(n)) can be collected automati-
cally [19]. Our goal is to design a credibility (cid:12)lter function
that is most e(cid:11)ective in ensuring that more credible feed-
backs are weighted higher and vice-versa.

A simple and intuitive solution is to measure feedback
credibility of a node n using its trust value T Vn. We call it
the Trust-Value based credibility Measure (TVM for short).
Let T Vu:x denote the trust value of node u:x who had in-

teraction u with node n. We can compute the trust value
based credibility measure of node u:x in the interaction u,
denoted by CRT V M

(u), using Equation 7.

n

CRT V M

n

(u) =

T Vu:x

Pu2I(n) T Vu:x

(7)

Several existing reputation-based trust systems use TVM
or its variant to measure feedback credibility [19, 11]. The
TVM solution is based on two fundamental assumptions.
First, untrustworthy nodes are more likely to submit false
or misleading feedbacks in order to hide their own malicious
behavior. Second, trustworthy nodes are more likely to be
honest on the feedback they provide. It is widely recognized
that the (cid:12)rst assumption is generally true but the second as-
sumption may not be true. For example, it is possible that
a node may maintain a good reputation by providing high
quality services but send malicious feedbacks to its competi-
tors. This motivates us to design a more e(cid:11)ective credibility
measure.

We propose to use a personalized similarity measure (PSM
for short) to rate the feedback credibility of another node
x through node n’s personalized experience. Concretely, a
node n will use a personalized similarity between itself and
node x to weigh all the feedbacks (cid:12)led by node x on any
other node (say y) in the system. Let IJ S(n; x) denote the
set of common nodes with whom both node n and x have
interacted, and I(n; r) denotes the collection of interactions
between node n and node r. We compute similarity between
node n and x based on the root mean square of the di(cid:11)er-
ences in their feedback over the nodes in IJ S(n; x). More
speci(cid:12)cally, given a node m and an interaction u 2 I(m)
performed by node m with node u:x, node n computes a per-
sonalized similarity-based credibility factor for u, denoted as
CRP SM

(u), using Equation 8.

n

CRP SM

n

(u) =

where

(8)

Sim(n; u:x)

Pu2I(n) Sim(n; u:x)

Sim(n; x) = 1 (cid:0) vuut
A(n; r) = Pv2I(n;r) Fn(v)

jI(n; r)j

Pr2IJ S(n;x) (A(n; r) (cid:0) A(x; r))2

jIJ S(n; x)j

This notion of personalized (local) credibility measure pro-
vides a great deal of (cid:13)exibility and stronger predictive value
as the feedback from similar raters are given more weight.
It also acts as an e(cid:11)ective defense against potential mali-
cious cliques of nodes that only give good ratings within the
clique and give bad rating outside the clique. Using per-
sonalized credibility to weight the feedbacks will result in a
low credibility for dishonest feedbacks by malicious cliques.
This is particularly true when measuring the feedback sim-
ilarity between a node m in a clique and a node n outside
the clique. Our experiments show that PSM outperforms
TVM when the percentage of malicious nodes become large
and when the malicious nodes collude with each other.

6. EVALUATION
In this section, we report results from our simulation based
experiments to evaluate TrustGuard’s approach to build de-
pendable reputation management. We implemented our

’model’

 1.8

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

’mean’
’TrustGuard-exp’
’TrustGuard-invtv’
’model’

 1.8

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

i

r
o
v
a
h
e
B
e
d
o
N

 

i

r
o
v
a
h
e
B
e
d
o
N

 

’TrustGuard-alpha’
’TrustGuard-beta-invtv’
’TrustGuard-gamma’
’non-adaptive’
’model’

 1.8

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

i

r
o
v
a
h
e
B
e
d
o
N

 

 0

 0

 5

 10

 20

 25

 30

 15
Time

 0

 0

 5

 10

 20

 25

 30

 15
Time

 0

 0

 5

 10

 20

 25

 30

 15
Time

Figure 5: Model I

Figure 6: Optimistic versus Pes-
simistic Summarization

Figure 7: E(cid:11)ect of Varying Param-
eters in the Trust Model

simulator using a discrete event simulation [8] model. Our
system comprises of about N = 1024 nodes; a random p%
of them is chosen to behave maliciously.
In the following
portions of this section, we demonstrate the e(cid:11)ectiveness of
the three guards that we have proposed in this paper.
6.1 Guarding from Strategic Node Behaviors
In this section we evaluate the ability of our trust model to
handle dynamic node behaviors. We (cid:12)rst study the behavior
of our guard against strategic oscillations by comparing the
optimistic and pessimistic summarization techniques. We
demonstrate the signi(cid:12)cance of various parameters in our
dependable trust metrics by varying the weights assigned to
reports received in the recent time window ((cid:11)), the history
((cid:12)), and the derivative component ((cid:13)). Then, we show the
impact of history size (maxH) on the e(cid:11)ectiveness of our
trust model and the advantages of storing past experiences
using fading memories.

For all experiments reported in this section, we studied
four di(cid:11)erent models of strategic malicious behaviors (refer
Section 3.1 for the de(cid:12)nition of node behavior). In Model I
shown in Figure 5, the malicious nodes oscillate from good to
bad behavior at intervals of regular time periods. In model
II, the malicious nodes oscillate between good and bad be-
haviors at exponentially distributed intervals. In model III,
the malicious nodes choose a random level of goodness and
stay that level for an exponentially distributed duration of
time.
In model IV the malicious node shows a sinusoidal
change in its behavior that is the node steadily and continu-
ously changes its behavior unlike models I, II and III which
show sudden (cid:13)uctuations.
6.1.1 Comparing Optimistic and Pessimistic Summa›

rizations

We (cid:12)rst compare the two types of weighted summarization
techniques discussed in Section 3.2. Figure 6 shows the
values obtained on summarization given the node behavior
model I shown in Figure 5 (using (cid:26) = 0:7, maxH = 10
and time period of malicious behavior oscillation = 10).
The result shows that mean value (mean) and exponentially
weighted sum (exp) have similar e(cid:11)ect and they both are
more optimistic than the inverse trust value weighted sum
(invtv). Observe that the more pessimistic a summarization
is, the harder it is for a node to attain a high trust value in a
short span of time and the easier it is to drop its trust value
very quickly. Also observe that the exponentially weighted
sum in comparison to the mean rises quite steeply making
it unsuitable for summarization.

6.1.2 Trust Model Parameters
Figure 7 shows the results obtained from our trust model
with various parameter settings under the malicious behav-
ior shown in model I (m1). alpha shows the results obtained
when (cid:11) is the dominant parameter ((cid:11) (cid:29) (cid:12); (cid:13)). With a dom-
inant (cid:11) the trust model almost follows the actual behavior of
the node since it amounts to disregarding the history or the
current (cid:13)uctuations in the behavior of the node (see Equa-
tion 2).
beta-invtv shows the results obtained with (cid:12) as
the dominant parameter using inverse trust value weighted
sum. With more importance given to the behavior history
of a node, the trust value of a node does not change very
quickly. Instead it slowly and steadily adapts to its actual
behavior. gamma shows the results obtained with (cid:13) being
the dominant factor. With a large (cid:13) the trust value responds
very swiftly to sudden changes in the behavior of the node.
Observe the steep jumps in the trust value that correspond
to the time instants when the node changes its behavior.
These results match our intuition, namely, (cid:11), (cid:12) and (cid:13) are
indeed the weights attached to the current behavior, his-
torical behavior and the (cid:13)uctuations in a node’s behavior.
Finally, non-adaptive shows the trust value of a node in the
absence of dependable schemes to handle dynamic node be-
haviors. From Figure 7 it is evident that the cost paid by
a malicious node in a non-adaptive model is almost zero,
while that in a dependable model is quite signi(cid:12)cant. A
more concrete evaluation that considers the combined e(cid:11)ect
of various trust model parameters is a part of our ongoing
work.
6.1.3 Varying History Size
In this section we show the e(cid:11)ect of history size maxH on
the cost (see Equation 1) paid by malicious nodes. Figure
8 shows a scenario wherein the malicious nodes oscillate in
their behavior every 10 time units. Note that in this exper-
iment we used (cid:11) = 0:2, (cid:12) = 0:8, (cid:13)1 = 0:05 and (cid:13)2 = 0:2.
Based on our experiences with the dependable trust model
one needs to choose (cid:11) and (cid:12) such that (cid:12)
(cid:11) is comparable to
maxH (intuitively, this weights the history component in
proportion to its size (maxH)). Note that this experiment
uses maxH = 5 which is less than the time period of os-
cillations by the malicious nodes. From Figure 8 it is clear
that the dependable trust models (TrustGuard-adaptive in
(cid:12)gure) performs better in terms of cost to be paid by the
malicious nodes than the non-adaptive trust model (recall
the cost model in Section 3.1). However, this does not en-
tirely maximize the cost paid by malicious nodes. Figure

i

r
o
v
a
h
e
B
e
d
o
N

 

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

’TrustGuard-adaptive’
’non-adaptive’
’m1’

 0

 5

 10  15  20  25  30  35  40  45  50

Time

i

r
o
v
a
h
e
B
e
d
o
N

 

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

’TrustGuard-adaptive’
’non-adaptive’
’m1’

’TrustGuard-adaptive’
’TrustGuard-ftv’
’m1’

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

i

r
o
v
a
h
e
B
e
d
o
N

 

 0

 5

 10  15  20  25  30  35  40  45  50

Time

 0

 0

 50

 100

 200

 250

 300

 150
Time

Figure 8: Trust Model with a Small
History

Figure 9: Trust Model with a Large
History

Figure 10: Trust Model with Fad-
ing Memories

9 shows the trust values obtained when (cid:11) = 0:1, (cid:12) = 0:9,
(cid:13)1 = 0:05, (cid:13)2 = 0:2 and maxH = 15 (larger than the time
period of oscillation by the malicious node). Clearly, having
a larger history ensures that one can maximize the cost paid
by the malicious nodes. In fact, one observes that the cost
paid by malicious nodes for maxH equal to 5, 10 and 15
are in the ratio of 0:63 : 1 : 3:02 respectively. This obser-
vation tells us that if a strategic malicious node knew that
maxH = 5, then it would oscillate at a period equal to 5
time intervals since anyway the system does not remember
its past performance beyond 5 time intervals. In short, by
knowing the exact value of maxH, a strategic malicious node
would start to oscillate with time period equal to maxH so
as to minimize its cost. It is interesting to note that, when
the non-adaptive model is used, the cost paid by malicious
nodes is close to zero for all values of time period of behavior
oscillation and history size maxH.
6.1.4 Fading Memories
We now evaluate the e(cid:11)ectiveness of the fading memories
technique in e(cid:14)ciently storing the performance of a node
over the last 2maxH intervals using a logarithmically small
number of values. Figure 10 shows the e(cid:11)ect of using fad-
ing memories when a malicious node oscillates with time
period equal to 100 time units. It compares a dependable
trust model (TrustGuard-adaptive in (cid:12)gure) with maxH =
10 and a dependable trust model using fading memories
(TrustGuard-ftv in (cid:12)gure) based technique with m = 8.
From Figure 10 it is apparent that using a simple adaptive
technique with maxH = 10 enables a bad node to recover
from its bad behavior that stretched over 100 time units in
just 10 additional time units, since the past performance
of the node is simply forgotten after 10 time units. As we
discussed in Section 3, one of the design principles for de-
pendable trust management is to prevent a bad node that
has performed poorly over an extended period of time to at-
tain a high trust value quickly. Clearly, the adaptive fading
memories based technique can perform really well in this re-
gard, since using just 8 values, it can record the performance
of the node over its last 256 (28) time intervals. It is impor-
tant to note that the solution based on fading memories has
bounded e(cid:11)ectiveness in the sense that by setting m = 8,
any node could erase its malicious past over 256 time inter-
vals. However, the key bene(cid:12)t of our fading memories based
approach is its ability to increase the cost paid by malicious
nodes, with minimal overhead on the system performance.

6.1.5 Other Strategic Oscillation Models
We also studied the cost of building reputation under dif-
ferent bad node behavior models discussed in the beginning
of Section 6.1. From our experiments, we observed that
the response of our trust model towards models II, III and
IV are functionally identical to that obtained from model
I (Figure 5). However, from an adversarial point of view,
we observed that these strategies do not aid in minimizing
the cost to be paid by malicious nodes to gain a good rep-
utation when compared to model I. In fact, the cost paid
by malicious nodes using models I, II, III and IV are in the
ratio of 1 : 2:28 : 2:08 : 1:36. In models II and III, the mali-
cious nodes do not pursue their malicious activities the very
moment they attain a high reputation.
In model IV, the
malicious nodes slowly degrade their behavior, which does
not given them good bene(cid:12)ts (see the extent of misuse Xn(t)
in Figure 2) when compared to a steep/sudden fall. Hence,
a strategic malicious node that is aware of maxH would
oscillate with time period maxH in order to minimize its
cost (refer Equation 1). Nonetheless this emphasizes the
goodness of our dependable trust model since it is capable
of e(cid:11)ectively handling even its worst vulnerability (model I
with oscillation time period maxH).
6.2 Guarding from Fake Transactions
In this section we study the feasibility of using optimistic
fair-exchange protocol for exchanging transaction proofs.
6.2.1 Trust Value Based Protocol Vs Optimistic Pro›

tocol

Figure 11 shows the percentage of fair exchange of transac-
tion proofs with progress in time for the two exchange proto-
cols discussed in Section 4, namely the trust value based fair
exchange protocol and the optimistic fair exchange protocol.
The experiment measures the percentage of fair transactions
when 20% of the nodes are malicious. The trust value based
exchange scheme su(cid:11)ers because a strategic malicious node
may gain high trust value initially and then fake arbitrarily
large number of transactions by unfairly exchanging trans-
action proofs without being detected by the system.
6.2.2 Trusted Server Of(cid:3)ine Duration in Optimistic

Protocol

As we have discussed in Section 4, it is important to decrease
the amount of time the trusted third party server is online so
as to make it less susceptible to attackers. However, doing
so increases the amount of time it takes to resolve a con(cid:13)ict.

s
e
g
n
a
h
c
x
E

 
r
i
a
F

 
f

o

 

e
g
a

t

n
e
c
r
e
P

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

’no-fair-exchange’
’trust-value-based-exchange’
’optimistic-fair-exchange’

 1000

 3000

 2000
 4000
Number of Transactions

 5000

 6000

r
o
r
r

 

E
n
o

i
t

t

a
u
p
m
o
C

 
t
s
u
r
T

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

’trust-naive’
’trust-tvm’
’trust-psm’

0

0.1

0.3

0.2
0.6
Percentage of Malicious Nodes

0.4

0.5

r
o
r
r

 

E
n
o

i
t

t

a
u
p
m
o
C

 
t
s
u
r
T

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.7

0.8

’trust-naive’
’trust-tvm’
’trust-psm’

0

0.1

0.3

0.2
0.6
Percentage of Malicious Nodes

0.4

0.5

0.7

0.8

Figure 11:
Fair Exchange of
Transaction Proofs: Optimistic Vs
Trust-Value Based Exchange Pro-
tocol

Figure 12: Robustness in Non-
Collusive Setting

Figure 13: Robustness in Collusive
Setting

Tof f
Cost

0
1

0.05
0.9

0.1
0.85

0.2
0.78

0.3
0.66

Table 1: Relative Cost paid by Malicious Nodes Vs
Tof f (normalized by maxH)

We have shown in Section 4.2 that a malicious node can ex-
ploit the delay in con(cid:13)ict resolution to may ensure that none
of its malicious activities be made visible to the trust man-
agement system for Tof f units of time. In this experiment,
we show how the transaction success rate varies with Tof f ,
the time period for which the trusted third party server is
o(cid:15)ine.

Table 1 shows the normalized cost (see Equation 1) paid
by malicious nodes when we introduce a delay in con(cid:13)ict
resolution. We have normalized Tof f with the history size
(maxH) maintained by TrustGuard’s adaptive trust model
(see Section 3). Figure 1 shows that in order to keep the
cost from dropping more that 10%, Tof f should be no more
than 5% of maxH. Note that this is another scenario where
fading memories (see Section 3) helps the system. Fading
memories essentially allow the history size (maxH) to be
very large and hence the duration of the time for which a
trusted third party server is o(cid:15)ine could be su(cid:14)ciently large
without signi(cid:12)cantly decreasing the cost paid by malicious
nodes.
6.3 Guarding from Dishonest Feedbacks
In this section we present an evaluation of our algorithm
to (cid:12)lter dishonest feedbacks (Section 5). Recall that the
fake transaction guard does not prevent fake transactions
between two malicious nodes. So, we simulated two settings,
namely, non-collusive and collusive setting. In the collusive
setting, a group of collusive malicious nodes may attempt
to deterministically boost their ratings by providing highly
positive feedbacks on each other through innumerable fake
transactions.

Figures 12 and 13 show the error in trust computation as
a function of the fraction of malicious nodes in the system in
a non-collusive and a collusive setting respectively. Observe
that the naive technique (an average of the feedback without
credibility factor) for computing trust drops almost linearly
with fraction of malicious nodes. Also, the naive technique
and the TVM approach are extremely sensitive to collusive
attempts even when the number of malicious nodes is very

small. On the other hand, the PSM approach remains ef-
fective even with both large fraction of malicious nodes and
collusion. Recall that PSM computes a personalized trust
value and hence, the trust value of a node may be di(cid:11)erent
from the perspective of various other nodes in the system.
For example, the trust value of a node m from the perspec-
tive of other nodes within its clique may be very high, and
yet, the trust value of node m as seen by other nodes in the
system might be very low. Therefore, the similarity met-
ric used by PSM is very e(cid:11)ective even in an overwhelming
presence of collusive malicious nodes.

7. RELATED WORK
Dellarocas [5] provides a working survey for research in game
theory and economics on reputation. The game theory based
research lays the foundation for online reputation systems
research and provides interesting insights into the complex
behavioral dynamics.

Related to reputation systems that help establishing trust
among entities based on their past behaviors and feedbacks,
there is research on propagating trust among entities based
on their trust relationship. E.g. Yu and Singh [20] proposed
a framework based on a gossip protocol. Richardson et al.
[17] developed a path-algebra model for trust propagation.
Very recently, Guha et al. [9] developed a formal framework
for propagating both trust and distrust. The TrustGuard
framework is capable of accomodating these algorithms by
replacing its dishonest feedback guard.

In the P2P domain reputation management systems like
P2Prep [3], Xrep [4] and EigenTrust [11] have emerged.
P2PRep provides a protocol on top of Gnutella to estimate
trustworthiness of a node.
It does not discuss trust met-
rics in detail and does not have evaluations. XRep extends
P2PRep by assigning a reputation value for both peers and
resources. EigenTrust assumes that trust is transitive and
addresses the weakness of the assumption and the collusion
problem by assuming there are pre-trusted nodes in the sys-
tem. We argue that pre-trusted nodes may not be available
in all cases. More importantly, neither of these reputation
management systems addresses the temporal dimension of
this problem (strategic behavior by malicious nodes) and
the problem of fake transactions.

Dellarocas [6] has shown that storing feedback informa-
tion on the most recent time interval is enough; and that
summarizing feedback information for more than one win-

dow of time interval does not improve the reputation system.
However, this result subsumes that there are no errors in the
feedbacks and that all nodes behave rationally. In the pres-
ence of dishonest feedbacks there are bound to be errors in
identifying a honest feedback from a dishonest one. Further,
our experiments show that the history component helps in
stabilizing the system by avoiding transient (cid:13)uctuations due
to transient errors or dishonest feedbacks.

B. Yu and M. P. Singh [20] suggest re(cid:12)ning personal opin-
ions di(cid:11)erently for cooperation and defection and achieves
a certain level of adaptivity. Our dependable trust model
is based upon the PID controller popularly used in control
theory [14], as against ad hoc techniques suggested in their
paper.

S. K. Lam and J. Riedl [12] experimentally studied sev-
eral types of shilling attacks on recommender systems. Our
experiments show that TrustGuard is resistant to random
shilling attacks. As a part of our future work, we hope
to model and analyze di(cid:11)erent types of shilling attacks on
reputation systems and enhance our algorithms to further
counter them.

Fair exchange protocols [16, 13, 10] have been the prime
focus of researchers working in the (cid:12)eld of electronic com-
merce. Ray and Ray [16] provides a survey on fair exchange
of digital products between transacting parties. They com-
pare various algorithms including trusted third parties, true
& weak fair exchanges, gradual exchanges and optimistic ex-
changes. In this paper, we used an optimistic fair-exchange
protocol proposed by Micali [13] for fair-contract signing.

8. CONCLUSION
We have presented TrustGuard (cid:0) a framework for build-
ing distributed dependable reputation management systems
with the countermeasures against three detrimental vulner-
abilities, namely, (i) strategic oscillation guard, (ii) fake
transaction guard, and (iii) dishonest feedback guard.
In
TrustGuard we promote a modular design such that one
could add more safeguard components, or replace the tech-
niques for one module without having to worry about the
rest of the system. The main contribution of this paper
is threefold. First, we proposed to measure the trustworthi-
ness of peers based on current reputation, reputation history
and reputation (cid:13)uctuation and develop formal techniques to
counter strategic oscillation of malicious nodes. Second, we
presented electronic fair-exchange protocol based techniques
to rule out the possibility of faking transactions in the sys-
tem. Third, we developed algorithms to (cid:12)lter out dishon-
est feedbacks in the presence of collusive malicious nodes.
We have demonstrated the e(cid:11)ectiveness of these techniques
through an extensive set of simulation based experiments.
We believe that the TrustGuard approach can e(cid:14)ciently and
e(cid:11)ectively guard a large-scale distributed reputation system,
making it more dependable than other existing reputation-
based trust systems.

9. REFERENCES
[1] K. Aberer and Z. Despotovic. Managing trust in a

peer-2-peer information system. In Proceedings of the
10th International Conference of Information and
Knowledge Management, 2001.

[2] M. Castro, P. Druschel, A. Ganesh, A. Rowstron, and

D. S. Wallach. Secure routing for structured

peer-to-peer overlay networks. In Operating Systems
Design and Implementation (OSDI), 2002.

[3] F. Cornelli, E. Damiani, S. D. C. di Vimercati,

S. Paraboschi, and P. Samarati. Choosing reputable
servents in a p2p network. In Proceedings of the 11th
World Wide Web Conference, 2002.

[4] E. Damiani, S. Vimercati, S. Paraboschi, P. Samarati,

and F. Violante. A reputation-based approach for
choosing reliable resources in peer-to-peer networks.
In CCS, 2002.

[5] C. Dellarocas. The digitization of word-of-mouth:

Promises and challenges of online reputation
mechanism. In Management Science, 2003.

[6] C. Dellarocas. Sanctioning reputation mechanisms in

online trading environments with moral hazard. In
MIT Sloan Working Paper No. 4297-03, 2004.

[7] J. Douceur. The sybil attack. In 2nd Annual IPTPS

Workshop, 2002.

[8] G. S. Fishman. Discrete-event simulation. Springer

Series in Operations Research.

[9] R. Guha, R. Kumar, P. Raghavan, and A. Tomkins.
Propagation of trust and distrust. In Proceedings of
the 13th World Wide Web Conference, 2004.

[10] F. C. G. Holger Vogt, Henning Pagnia. Modular fair

exchange protocols for electronic commerce. In Annual
Computer Security Applications Conference, 1999.

[11] S. Kamvar, M. Schlosser, and H. Garcia-Molina.

Eigentrust: Reputation management in p2p networks.
In Proceedings of the 12th WWW Conference, 2003.

[12] S. K. Lam and J. Riedl. Shilling recommender systems

for fun and pro(cid:12)t. In Proceedings of the 13th World
Wide Web Conference, 2004.

[13] S. Micali. Simple and fast optimistic protocols for fair

electronic exchange. In The Proceedings of ACM
PODC, 2003.

[14] H. Ozbay. Introduction to feedback control theory.

CRC Press Inc.

[15] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
Technical report, 1998.

[16] I. Ray and I. Ray. Fair exchange in e-commerce. In

ACM SIGEcomm Exchange, 2001.

[17] M. Richardson, R. Agarwal, and P. Domingos. Trust
management for the semantic web. In Proceedings of
International Semantic Web Conference, 2003.

[18] M. Srivatsa and L. Liu. Vulnerabilities and security

issues in structured overlay networks: A quantitative
analysis. In Proceedings of the Annual Computer
Security Applications Conference (ACSAC), 2004.

[19] L. Xiong and L. Liu. A reputation-based trust model

for peer-to-peer ecommerce communities. In IEEE
Conference on E-Commerce (CEC’03), 2003.

[20] B. Yu and M. P. Singh. A social mechanism of

reputation management in electronic communities. In
Proceedings of the 4th International Workshop on
Cooperative Information Agents, 2000.

