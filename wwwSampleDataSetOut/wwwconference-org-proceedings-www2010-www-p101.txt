The Anatomy of an Ad: Structured Indexing and Retrieval

for Sponsored Search

Michael Bendersky†, Evgeniy Gabrilovich‡, Vanja Josifovski‡, and Donald Metzler‡

† Department of Computer Science, University of Massachusetts, 140 Governors Drive, Amherst, MA 01003

‡ Yahoo! Research, 4301 Great America Parkway, Santa Clara, CA 95054

bemike@cs.umass.edu | {gabr | vanjaj | metzler}@yahoo-inc.com

ABSTRACT
The core task of sponsored search is to retrieve relevant ads
for the user’s query. Ads can be retrieved either by exact
match, when their bid term is identical to the query, or
by advanced match, which indexes ads as documents and
is similar to standard information retrieval (IR). Recently,
there has been a great deal of research into developing ad-
vanced match ranking algorithms. However, no previous
research has addressed the ad indexing problem. Unlike
most traditional search problems, the ad corpus is deﬁned
hierarchically in terms of advertiser accounts, campaigns,
and ad groups, which further consist of creatives and bid
terms. This hierarchical structure makes indexing highly
non-trivial, as na¨ıvely indexing all possible “displayable” ads
leads to a prohibitively large and ineﬀective index. We show
that ad retrieval using such an index is not only slow, but
its precision is suboptimal as well. We investigate various
strategies for compact, hierarchy-aware indexing of spon-
sored search ads through adaptation of standard IR indexing
techniques. We also propose a new ad retrieval method that
yields more relevant ads by exploiting the structured nature
of the ad corpus. Experiments carried out over a large ad
test collection from a commercial search engine show that
our proposed methods are highly eﬀective and eﬃcient com-
pared to more standard indexing and retrieval approaches.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Experimentation

Keywords
Structured retrieval, sponsored search

1.

INTRODUCTION

Textual ads are ubiquitous on the Web today, as they ap-
pear on a wide variety of Web pages ranging from obscure
forums to major search engine result pages. Online textual
ads connect advertisers to prospective customers, and clicks
on these ads by Web users account for a signiﬁcant fraction
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

of the revenue for many online businesses: from news pub-
lishers to blogs to Web search engines. As of 2008, textual
advertising comprised approximately 40% of the $22 billion
online advertising market, which is predicted to double in
size over the next 5 years.1.

Textual ads are distributed via two primary channels,
namely, sponsored search and content match. In sponsored
search, textual ads are shown alongside of the Web search
results, while in content match, contextually relevant ads are
displayed on third-party Web pages. Historically, sponsored
search evolved by allowing advertisers to explicitly bid on
queries (bid terms2) that they wished to display their ads
for. In this paradigm, the burden of ad selection was placed
primarily on the advertisers, since they needed to skillfully
choose a comprehensive list of queries relevant for their ads.
This scenario is called exact match, since the query and the
bid term must match exactly for an ad to be shown.

However, it quickly became apparent that it is virtually
impossible to explicitly enumerate billions of less popular
“tail” queries, hence it is impractical to cover them via exact
match, even though such queries provide valuable advertis-
ing opportunities. To unlock the revenue potential of these
numerous yet individually infrequent queries, the advanced
match method was introduced. Here, the query and the bid
term no longer need to match exactly, and ads are selected
algorithmically by the search engine.

Recently, information retrieval techniques have been pro-
posed for advanced match by indexing the ads as documents
using the ad text visible to the user as well as the ad’s bid
terms [25, 4, 5]. Ads are then selected using an ad query
that is generated from the user’s query (sponsored search)
or the Web page on which ads are to be displayed (content
match). The ad query is executed against the index of ads
using standard IR matching and ranking techniques. Most
implementations of advanced match make the simplifying
assumption that ads are atomic units that are independent
of each other, even though ads from the same advertiser
could be quite similar or nearly identical.

In practice, however, textual ads are deﬁned and orga-
nized as a structured database with several types of entities,
as shown in Figure 1. Each advertiser has one or more ac-
counts. Each account in turn contains several ad campaigns,
which have diﬀerent temporal or thematic goals (e.g., sale
of home appliances during the holiday season). Campaigns

1Source: eMarketer.com
2Bid terms are also sometimes referred to as bid phrases or simply
terms in a sponsored search setting. Therefore, we use these
concepts interchangeably throughout the paper.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA101Figure 1: Schema of an ad database

consist of ad groups, which include multiple creatives (the
visible text of the ad) and bid terms. Bid terms correspond
to diﬀerent products or services oﬀered by the advertiser,
while creatives represent diﬀerent ways to advertise those
products. Any creative can be paired with any bid term
within the same ad group to create an actual ad displayed
to the user. Search engines usually allow ad groups to con-
tain a few dozen creatives and up to a thousand bid terms.
This type of ad schema has been designed with the adver-
tisers’ needs in mind, as it allows the advertisers to easily
deﬁne a large number of ads for a variety of products and
marketing messages.
In view of this hierarchical deﬁnition, the ad retrieval
problem can be formulated as the retrieval of (cid:104)creative, term(cid:105)
pairs from a structured schema. In this paper we study the
key issues in postulating ad retrieval as a structured retrieval
problem, where the unit of retrieval is deﬁned hierarchically.
We also discuss several crucial tradeoﬀs that must be ana-
lyzed in this unique retrieval scenario.
Na¨ıvely indexing all possible retrieval units (i.e., all pos-
sible (cid:104)creative, term(cid:105) combinations) using standard IR in-
dexing approaches would result in a signiﬁcant amount of
wasted storage, since each creative and bid term will be in-
dexed multiple times due to the Cartesian product seman-
tics. Most of the inverted indexing algorithms used in mod-
ern search engines incur increased cost with larger index
sizes, making the na¨ıve approach infeasible in practice. To
avoid this problem, we explore several hierarchical index-
ing schemes that signiﬁcantly reduce the amount of duplica-
tion. In our study, we also quantify the impact that diﬀerent

indexing strategies have on ad retrieval eﬀectiveness. We
further develop a novel ranking algorithm that exploits the
hierarchical structure and is both more eﬃcient and more
eﬀective than the na¨ıve indexing approach. This is done by
employing a multi-phase retrieval approach, where we ﬁrst
retrieve an ad group, then select an optimal creative, and
ﬁnally choose a bid term that makes the resulting ad the
most relevant for the given query.

The main contributions of this paper are threefold. First,
we propose novel, eﬃcient ways of indexing sponsored search
ads. We ﬁrst transform the ad corpus to a collection of hi-
erarchically structured textual documents, and then adapt
standard IR indexing techniques to construct a compact yet
eﬀective ad index. The approaches we propose can also be
applied to other retrieval tasks where the retrieval units are
structured hierarchically. Second, we propose several rank-
ing strategies that leverage the hierarchical structure of the
ad corpus to achieve more accurate ad ranking. Finally, we
conduct a large-scale evaluation using sponsored search data
from a large commercial search engine that helps quantify,
in a real world setting, the usefulness of our proposed struc-
tured indexing and retrieval approaches.

The remainder of the paper proceeds as following. In Sec-
tion 2 we survey the related work.
In Sections 3 and 4
we explain our novel structured indexing and retrieval ap-
proaches, respectively. Section 5 discusses our experimental
evaluation carried out over real-life sponsored search data
from a large commercial search engine. Finally, Section 6
concludes the paper.

AdvertiserAccount 1Account 2Account 3Campaign 1Campaign 2Campaign 3Ad group 1Ad group 2Ad group 3CreativesBid phrasesAd.........Brand name appliancesFree delivery before Cristmaswww.appliances-r-us.com miele stovemiele microwavekitchen Aidcusinartwolf...Kitchen appliances...Buy appliances on Black FridayNew Year deals on lawn & garden toolsBrand name appliancesBuy one get second freewww.appliances-r-us.com Best deal on appliances!Buy now and get 20% offwww.appliances-r-us.com ...WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA1022. BACKGROUND AND RELATED WORK
Textual advertising is a signiﬁcant source of revenue for
modern Web search engines. It is therefore not surprising
that there is a growing research interest in designing bet-
ter algorithms for sponsored search. Advanced match has
been a focus of a few recent studies, most of which focus on
the query rewriting paradigm [13, 23, 15]. However, recent
work shows that employing a more ﬂexible advanced match
paradigm, which uses IR techniques to index ads as docu-
ments, leads to signiﬁcant improvements in sponsored search
performance [4, 25, 5]. Standard IR techniques such as tf-idf
weighting [4], pseudo-relevance feedback [5], query reformu-
lation [3, 25], clustering [29], document categorization [4,
1] and supervised learning of optimal ranking functions [16]
have been shown to be highly beneﬁcial for improving the
quality of the retrieved ads.

Structured indexing and retrieval is a research ﬁeld that

has received increasing attention, especially due to the spread-
ing adoption of the XML format for representing documents
[10, 17]. Techniques that take into account document struc-
ture were shown to be beneﬁcial in a variety of domains
ranging from Web search [26] to book retrieval [14] to desk-
top search [9]. To the best of our knowledge, our work is
the ﬁrst to apply the principles of structured retrieval in
sponsored search.

In many domains, a simple linear combination of scores
from diﬀerent document ﬁelds is suﬃcient for improving re-
trieval performance in the structured retrieval setting [2, 7,
20]. In the case of ad retrieval, however, such a straightfor-
ward approach is not suﬃcient. As we show in Section 4.1,
due to the heterogeneity inherent in the ad databases, more
elaborate feature design and parameter estimation techniques
are necessary to achieve performance gains over the baseline
that does not employ any structural information.

Structure was also recently found to be useful for the text
classiﬁcation task [22]. Speciﬁcally for sponsored search,
Raghavan and Hillard [24] reported that a weighted combi-
nation of features based on creative and term ﬁelds helps
reduce the number of irrelevant ads shown to the user (a
task called ad ﬁltration). In this work, we show that adding
features that exploit the hierarchical structure of ad groups
helps to further improve the accuracy of ad ﬁltration.
3. AD INDEXING AND RETRIEVAL
As explained in Section 1, our target retrieval unit is a
(cid:104)creative, term(cid:105) pair, which together comprise a displayable
ad. Thus, our ultimate goal is to produce a ranked list of
relevant (cid:104)creative, term(cid:105) pairs.

Our index contains two types of ﬁelds: creative ﬁelds (c)
and term ﬁelds (t). Each creative ﬁeld consists of three
sub-ﬁelds: title, description and URL, as shown in Fig. 1.
Each ad group g consists of several creative and term ﬁelds,
grouped by an advertiser (see Fig. 1). The set of all ad
groups is denoted G, and the sets of all creatives and terms
(for all the ads groups) are denoted C and T, respectively.
The set of creatives or terms associated with a speciﬁc ad
group g is denoted Cg and Tg, respectively. Hence, the total
number of unique ﬁelds associated with a given ad corpus
is:

|C| + |T| = |G|(|Cg| + |Tg|),

(1)
where |Cg| and |Tg| denote the average number of creatives
and terms per ad group, respectively.

3.1 Scoring Function and Ranking

Although our structured retrieval approach is general enough

to incorporate any type of scoring functions, in this work we
take the well known language modeling approach to infor-
mation retrieval [8, 33]. In this approach, indexing units are
scored by their probability of generating the query terms
[21]. Formally, given a query q and an indexing unit u, we
score each unit using a unigram language model

(cid:89)

wi∈q

p(q|u) =

p(wi|u),

where p(wi|u) is estimated using Dirichlet smoothing [34],
so that the ﬁnal scoring function is as follows:

scq(u) (cid:44)

log

tfwi,u + µ

(cid:80)
|u| + µ

tfwi ,C

wj∈C tfwj ,C

,

(2)

(cid:88)

wi∈q

where tfw,k is the number of occurences of a term w, in
either a particular indexing unit (k = u) or the entire col-
lection (k = C), and µ is a free parameter, which controls
the amount of smoothing.

Indexing units are then ranked in descending order, based
on their score. As the user typically only sees a limited num-
ber of sponsored search results in response to her query, we
assume that only a list of top K indexing units [u(1), . . . , u(K)]
is retrieved in practice. Each unit u in the list is associated
with an ad group identiﬁer gIdu. To promote diversity and
advertiser coverage in the ranked list, we can require that
[gIdu(1), . . . , gIdu(K)] are unique, thereby limiting the num-
ber of ads retrieved from a single ad group to one.
3.2 Structured Ad Indexes

We now describe three proposed indexing strategies for
sponsored search. These strategies take into account the ad
structure outlined in Section 1. The strategies presented in
this paper investigate the indexing of the two lower levels
of the hierarchal ad structure (Fig. 1), namely the ad group
and the creative-bid term levels. The underlying principles
of these strategies, however, are general enough to easily
extend them to the higher levels of ad hierarchy, which we
intend to do as part of future work.

The strategies presented diﬀer in their choice of the atomic
indexing unit and their ranking algorithms. Table 1 pro-
vides a formal deﬁnition of indexing units and estimated
index sizes for each strategy. In what follows, we provide
a detailed description of these indexing strategies and their
applications to sponsored search.

3.2.1 Term Coupling Index (CTInd)
In the ﬁrst indexing scheme, we index units that are com-
posed of (cid:104)creative, term(cid:105) pairs (cid:104)c, t(cid:105). This is the most ﬁne-
grained indexing unit, and this approach eﬀectively indexes
the Cartesian product of the creatives and terms in each ad
group. Since we index all possible (cid:104)creative, term(cid:105) pairs,
the retrieval with this index is a single-level process — no
postprocessing is required, since indexing units correspond
to displayable ads. As we shall see below, other (more com-
pact) indexes require some additional ranking after the ini-
tial retrieval is performed. The CTInd index, on the other
hand, only requires ﬁltering by gIdt, as we retrieve a single
displayable ad per ad group (deduping). Algorithm 1 shows
the retrieval algorithm pseudocode.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA103Index
CTInd
CrtvInd
AdGrpInd {(cid:104)Cg, Tg(cid:105) : g ∈ G}

Indexing unit
{(cid:104)c, t(cid:105) : c ∈ Cg, t ∈ Tg, g ∈ G}
{(cid:104)c, Tg(cid:105) : c ∈ Cg, g ∈ G}

# indexing units # indexed ﬁelds
|G||Tg||Cg|
|G||Cg|
|G|

2|G||Tg||Cg|
|G||Cg|(1 + |Tg|)
|C| + |T|

Table 1: Summary of the three index versions.

Algorithm 1 CTRank (CTInd index)
1: (cid:104)c, t(cid:105) ← [u(1), . . . , u(K)] ⊆ C × T
2: FILTER (c, t) BY gIdt
3: return (cid:104)c, t(cid:105)

In the CTInd index, the average number of indexing units
per ad group is a product of cardinalities |Cg||Tg|, and each
such indexing unit contains two ﬁelds (i.e., a creative and
term ﬁeld). The expected number of ﬁelds indexed by the
CTInd index is, therefore, 2|G||Cg||Tg|.
3.2.2 Creative Coupling Index (CrtvInd)
The indexing unit in this index is a single creative c cou-
pled with all the bid terms associated with its ad group
gIdc. This is a much smaller index that CTInd, since it
no longer computes a Cartesian product of every creative
and every term. Using this index, in order to retrieve a
(cid:104)creative, term(cid:105) pair (cid:104)c, t(cid:105), we ﬁrst retrieve a ranked list of
creatives, ﬁlter them by gIdc and then retrieve the term with
the highest score associated with each creative in the index;
the score is computed according to Eq. 2. Algorithm 2 shows
the retrieval algorithm pseudocode.

Algorithm 2 CrtvRank (CrtvInd index)
1: c ← [u(1), . . . , u(K)] ⊆ C
2: FILTER c BY gIdc
3: t ← [argmaxt∈gIdc
4: return (cid:104)c, t(cid:105)

scq(t) : c ∈ c]

In the CrtvInd index, for each creative we index, on av-
erage, 1 + |Tg| term ﬁelds, and there are total of |G||Cg|
creatives in the collection. The expected number of indexed
ﬁelds in this index is, therefore, |G||Cg|(1 + |Tg|).
3.2.3 Ad Group Coupling Index (AdGrpInd)
In this approach, the indexing unit is the ad group itself.
In order to retrieve a (cid:104)creative, term(cid:105) pair, ﬁrst a ranked list
of ad groups is retrieved. Then, for each ad group, a creative
and a term with the highest scores are retrieved. Since the
creative and the term scores are independent, and assuming
that the scoring function is monotonic3, the retrieved (cid:104)c, t(cid:105)
pair is the pair with the highest score in the ad group. The
algorithm pseudocode is presented in Algorithm 3.

Note that the AdGrpInd index is the only index type
with no duplicated ﬁelds. The number of indexing units is
the same as the number of ad groups, |G|, and for each ad
group there are, on average, |Tg| + |Cg| ﬁelds. Therefore,
the number of indexed ﬁelds is |G|(|Tg| + |Cg|), which is
also the number of unique ﬁelds in the ad corpus, |C| + |T|,
as shown in Eq. 1.

This is the most compact index of the three alternatives
we explore. Note also that this indexing scheme eliminates
3A scoring function is monotonic if each query term match has a
positive contribution to the overall score.

the need for ﬁltering the results by the ad group identiﬁer
gIdu, since by deﬁnition each retrieved ad will be constructed
from a diﬀerent ad group.

Algorithm 3 AdGrpRank (AdGrpInd index)
1: g ← [u(1), . . . , u(K)] ⊆ G
2: c ← [argmaxc∈gIdg
3: t ← [argmaxt∈gIdg
4: return (cid:104)g, c, t(cid:105)

scq(c) : g ∈ g]
scq(t) : g ∈ g]

4. STRUCTURED RERANKING

In the previous section, we discussed the basic ranking al-
gorithms for each of the proposed indexing structures. In
this section, we show that the basic ranking is sometimes
not suﬃcient to produce the best retrieval results. As the
size of each indexing unit grows, which is the case with the
CrtvInd and AdGrpInd indexes, we face challenges that
hinder the performance of the retrieval models. To address
these challenges, which are described in Section 4.1, we pro-
pose a structured reranking strategy in Section 4.2.
4.1 Motivation for Structured Reranking

In the indexing strategies described in Section 3.2, the in-
dexed and retrieved units are hierarchically structured from
atomic and composite ﬁelds. Previous work on structured
document retrieval shows that a combination of ﬁeld scores
often yields better retrieval performance than matching each
ﬁeld independently [7, 20, 2].

To test this hypothesis, we design a simple experiment
that combines the scores obtained for the ad group and
the (cid:104)creative, term(cid:105) pair retrieved by AdGrpRank (Algo-
rithm 3). Formally, ads are reranked based on a mixture of
scores:

scq(g, c, t) = λscq(uc,t) + (1 − λ)scq(g),

(3)
where uc,t is a (cid:104)creative, term(cid:105) pair (cid:104)c, t(cid:105) treated as a single
indexing unit, and scq(·) is as deﬁned in Eq. 2.

Fig. 2 shows the retrieval performance of this rerank-
ing (as measured by NDCG@10 [11]). Setting λ = 0 in
Eq. 3 produces ranking that is equivalent to that of AdGr-
pRank, while setting λ = 1 produces ranking that ignores
the ad group structure, and is, therefore, equivalent to that
of CTRank.

It should be noted that in contrast to previous work, where
mixture models usually yield improvements [7, 20, 2], com-
bining the (cid:104)creative, term(cid:105) pair score with the ad group
score is detrimental — setting λ = 1, and ignoring the ad
group structure, yields the best performance. We postulate
that this is a result of several key traits that diﬀerentiate
sponsored search from other information retrieval tasks.

First of all, sponsored search is characterized by the large
variance in ad group lengths. While some ad groups contain
only a few bid terms, others can have up to 1,000 bid terms

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA104iting the choice of features to ﬁeld scores alone yields the
mixture model in Eq. 3. Since this model does not succeed
in outperforming the baseline method that uses no struc-
tural information (CTRank), we expand the set of features
used by StructRank to address the issues speciﬁc to spon-
sored search that were described in Section 4.1. As we show
in Section 5, using this expanded set of features results in
substantial performance improvements on a number of tasks.

Algorithm 4 StructRank (AdGrpInd index)
1: (cid:104)g, c, t(cid:105) ← AdGrpRank
2: SORT DESC (cid:104)g, c, t(cid:105) BY rScq(g, c, t)
3: return (cid:104)g, c, t(cid:105)

4.2.1 Reranking Features
We now deﬁne the features used by StructRank, as well
as its parameter optimization. Recall that the features have
the form f (g, c, t), and are therefore deﬁned over a (cid:104)c, t(cid:105) pair
and an ad group associated with it. The features used are
as follows.
• crtvTermPairScore is the score of the given (cid:104)creative, term(cid:105)
pair. It is equivalent to the scq(uc,t) component in the mix-
ture model in Eq. 3.
• adGrpScore is the score of the entire ad group, from which
the (cid:104)creative, term(cid:105) pair was selected. It is equivalent to the
scq(g) component in the mixture model in Eq. 3.
• adGrpTermCount is the number of term ﬁelds in a given
ad group. As previously mentioned, a number of bid terms
associated with an ad group can vary signiﬁcantly. Since
document length can aﬀect the document’s prior probability
of retrieval [27], we explicitly add the number of bid terms
as a feature in our model.
• adGrpEntropy is the entropy of an ad group. The entropy
is computed over the individual words of the ad group as
w∈g pg(w) log pg(w), where the probability of word wi
is computed using a maximum likelihood estimate pg(wi) =
. Following previous work, where entropy was

−(cid:80)
(cid:80)

tfwi,g

wj∈g tfwj ,g

found related to document heterogeneity [2], we use entropy
as an estimate of ad group cohesiveness—ad groups with
smaller entropy will tend to be more cohesive.
• adGrpQueryCover produces a real number r ∈ [0, 1], such
that r is the ratio of query words “covered” by any ﬁeld in the
ad group. It is common, especially for longer queries, that
not all query terms will appear in the selected creative and
term pair. The adGrpQueryCover feature help diﬀerentiate
between the ad groups that achieve high relevance scores
due to a disproportional repetition of a single query word
(bid term spamming) and those that achieve high relevance
scores due to a more comprehensive coverage of query words.
• adGrp[Field]Ratio is the fraction of ﬁelds of type [Field]
in an ad group that match at least one query word. Intu-
itively, we expect that ad groups that have high ﬁeld match
ratios w.r.t. query, will yield more relevant (cid:104)creative, term(cid:105)
pairs, since these ad groups will tend to be more focused on
the query topic. [Field] denotes either one of the sub-ﬁelds
of the creative ﬁeld, or the entire term ﬁeld. This produces
three features based on the location of the match: adGrpURL-
Ratio, adGrpTitleRatio and adGrpTermRatio.

Therefore, we have a total of 8 features. Of course, adding
other features is possible, but we limit ourselves to this small
well-deﬁned set of features for the purpose of this study.

Figure 2: Mixture method retrieval performance
with 0 ≤ λ ≤ 1. Dotted line indicates the perfor-
mance attained by CTRank with no mixture.

associated with them. This causes a strong length bias: the
probability of shorter documents (ad groups with smaller
number of terms) to be retrieved is higher than their prob-
ability of being relevant. While length bias is a well known
phenomenon in information retrieval [27], its eﬀects are more
pronounced in sponsored search, since the latter has a much
higher variance of document lengths.

Another issue that is unique for sponsored search is the
cohesiveness of the ad groups. In traditional retrieval, it is
usually assumed that documents, even structured ones, are
about a single topic; however, this assumption does not nec-
essarily hold for ad groups. A set of bid terms associated
with an ad group can range from being focused and cohesive
(associated with a single service or product) to being frag-
mented or even scattered (associated with several products
or even with a broad set of generic bid terms), depending on
the strategy of the advertiser. There is also a possibility that
some advertisers might misuse the bidding mechanisms by
purposefully associating unrelated terms to their ad groups
in an attempt to attract more customers (spamming).

Since existing structured retrieval techniques do not ad-
dress these issues, we propose a novel structured rerank-
ing approach that is speciﬁcally tailored towards sponsored
search. Our approach relies on the ad structure and employs
features that go beyond the simple mixture model in Eq. 3.
4.2 Structured Reranking Method

Our structured reranking method is a generalization of the
mixture model presented in the previous section. It takes
into account a given (cid:104)creative, term(cid:105) pair and the associated
ad group. The method assumes we do an initial retrieval
round using AdGrpRank. Then, the method reranks the
initially retrieved ads using a linear model

n(cid:88)

rScq(g, c, t) (cid:44)

λifi(g, c, t),

(4)

i=1

where fi(·) is a feature function, λi are weights assigned to
each function, and n is the number of such functions used
for the reranking.

We refer to this reranking procedure as StructRank, and
the entire retrieval procedure is described in Algorithm 4.
Clearly, the choice of features used in StructRank plays
a crucial role in the resulting algorithm performance. Lim-

Mixture MethodlnDCG@1000.10.30.50.70.910.650.700.750.80WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA1054.2.2 Reranking Optimization
In this section, we present our method for optimizing the
free parameters λi in the ranking function in Eq. 4. When
the number of free parameters is small (as, for instance, is
the case in Eq. 3), it is possible to optimize the parameters
using an exhaustive search over the parameter space. How-
ever, when more features are introduced, such an exhaustive
search quickly becomes infeasible.

To address this problem, we rely on a large and grow-
ing body of literature on the learning to rank methods for
information retrieval (see Liu [18] for a survey). Learning
to rank methods allow eﬀective parameter optimization for
ranking functions with respect to various retrieval metrics,
even when a number of free parameters is high.

In this work, we employ a simple yet eﬀective learning to
rank approach that directly optimizes the retrieval metric
of choice (e.g., NDCG@k). It is easy to see that our rank-
ing function is linear w.r.t. λi. Therefore, we make use of
the coordinate ascent algorithm proposed by Metzler and
Croft [19]. This algorithm iteratively optimizes a multivari-
ate objective function (in our case, rScq(g, c, t)) by perform-
ing a series of one-dimensional line searches. It repeatedly
cycles through each parameter λi, holding all other param-
eters ﬁxed while optimizing λi. This process is performed
iteratively over all parameters until the gain in the target
metric is below a certain threshold.

Although we use the coordinate ascent algorithm primar-
ily for its simplicity and eﬃciency, any other learning to
rank approach that estimates the parameters for linear mod-
els can be used. Other possible learning to rank algorithms
include ranking SVMs [12], SV M M AP [32] or RankNet [6].
Due to the linearity of our ranking function, query de-
pendent features (e.g., query length) cannot be readily in-
corporated into StructRank, since they will have the same
contribution across all documents associated with a query.
While this can be addressed by using non-linear rankers,
such rankers typically require more training data and are
more prone to overﬁtting than linear models [31]. As a mid-
dle ground between linear and non-linear approaches, we
bin our queries, and train a speciﬁc model for each bin. Any
query-dependent feature (or combination of thereof) can be
used for query binning. In our experiments we found that
binning by query length is both conceptually simple and
empirically eﬀective for retrieval optimization.

5. EXPERIMENTS

In this section we describe the experimental results of our
work.
In Section 5.1 we describe the experimental setup.
In Section 5.2 we empirically demonstrate the diﬀerences in
index sizes and query run-times between the three ad index
versions described in Section 3. In Section 5.3 we compare
the retrieval eﬀectiveness of these three index versions, as
well as demonstrate the beneﬁts of the structured reranking
approach proposed in Section 4. Finally, in Section 5.4,
we show the beneﬁts of employing the ad structure for ad
ﬁltering.
5.1 Experimental Setup

All indexing and retrieval experiments are implemented
using an open-source search engine Indri4.
Indri natively
supports indexing and retrieval of structured documents [28],

4http://www.lemurproject.org/indri/

and provides a best-in-class ad hoc retrieval performance us-
ing a popular language modeling approach for information
retrieval [21]. Indri allows us to compare the performance
of all our retrieval methods using the same search engine.

We constructed our dataset as follows. We sampled a set
of queries from a Web search log using stratiﬁed sampling.
We then randomly divided the queries into three disjoint
sets: a development set of 1,570 queries used for debugging,
feature selection and parameter tuning; a training set of
3,134 queries, and a held-out testing set of 773 queries with
at least one relevant (non-Bad, see below) judgment associ-
ated with each query. We then retrieved ads for these queries
using a commercial ad retrieval system, and had them eval-
uated by human judges using graded relevance judgments.
This resulted in a set of judged query-ad pairs, and our
experiments then consisted of reranking these pairs using
each of the proposed methods. Each judged ad consisted
of a single (cid:104)creative, term(cid:105) pair, and one such pair per ad
group was judged. Overall, our collection consisted of 5,477
queries and 93,632 unique ad groups that contained a total
of 239,011 creative ﬁelds and 5,143,010 term ﬁelds.

In all our retrieval experiments, all documents and queries
were stemmed using Krovetz stemmer, and no stopword re-
moval was performed. The smoothing parameter µ in Eq. 2
is set to 90, which was observed to be the optimal setting
on the development set.

In the (re)ranking experiments, we measured the per-
formance using the normalized discounted cumulative gain
measure (NDCG), a standard retrieval metric for Web re-
trieval [11]. The relevance grades assigned by human judges
were [Perfect, Excellent, Good, Fair, Bad], and we used the
following DCG gains for these grades [10, 7, 3, 0.5, 0], re-
spectively. In the parameter optimization of our reranking
model, we use normalized DCG at position 10 as the target
metric for the coordinate ascent search (Section 4.2.2).

As described in Section 4.2.2, we found that performance
of the StructRank algorithm can be improved by separat-
ing the training set into several query bins, and training a
separate linear model for each bin. Ideally, each bin should
contain only queries of a certain type (for instance, only nav-
igational or informational queries).
In the lack of explicit
query type information, we simply bin queries by length.
We use three similar-sized bins, one for one word queries,
one for two and three word queries and one for queries of
four words or longer. We found that this binning procedure
signiﬁcantly improves performance on the development set,
and used it in the rest of our ranking experiments.
5.2 Retrieval Efﬁciency

5.2.1 Index sizes
To test the eﬃciency of our proposed indexing strategies,
we create 4 sub-samples of the increasing size from the en-
tire corpus of 93,632 ad groups. Sub-sample sizes ranged be-
tween 25% and 100% of the entire corpus. Each of these sub-
samples was separately indexed using one of the three index-
ing methods: CTInd, CrtvInd and AdGrpInd. Overall,
the combination of 4 sub-sample sizes and 3 index types
yielded 12 experimental indexes. Table 2 details the num-
ber of documents in each sub-sample, and the size of all the
indexes constructed. As can be seen from Table 2, index
AdGrpInd is, as expected, the smallest of the three ver-
sions, both in terms of the number of documents, and the

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA106Sample Size = 25%
# Docs
4,394,022

CTInd
CrtvInd
AdGrpInd

59,636
23,367

Sample Size = 50%

Index Size # Docs
8,662,762
120,538
46,712

3.3G
223M
91M

Index Size

6.4G
429M
167M

Sample Size = 75%
# Docs

Index Size

Sample Size = 100%
# Docs

Index Size

12,525,841

178,025
69,656

9.2G
616M
240M

16,814,306

239,002
93,632

13G
820M
317M

Table 2: Details of index samples.

(a)

(b)

(c)

Figure 3: Retrieval runtime for the three index types. (a) No reranking; (b) with reranking of the top 10
results for CrtvInd and AdGrpInd using the crtvTermPairScore feature; (c) the diﬀerence between reranking
and no-reranking runtimes.

overall index size. This is due to the fact that it is the only
version of the index where no ﬁeld duplications occur.

Due to a large number of terms that can be potentially
associated with a single ad group (average number of terms
per ad group is approximately 50, with maximum number of
terms per ad group bounded by 1,000), the CTInd index is
the most costly, both in terms of number of documents and
the index size. On the smallest 25% sub-sample, the size
of index CTInd is 36 times larger than that of the index
AdGrpInd. Moreover, this diﬀerence grows with the num-
ber of ad groups, as the ad group ﬁelds in index CTInd
are not uniquely indexed. On the entire corpus, the size
of index CTInd is already 41 times larger than that of in-
dex AdGrpInd. Note that in our experiments, only a small
number of ad groups (∼ 100, 000) is used. As the ad corpora
used by commercial search engines typically contain millions
of ad groups and are frequently updated, constructing the
(cid:104)creative, term(cid:105) pairs index CTInd becomes practically in-
feasible.

The same trend, though on a smaller scale, occurs when
comparing the CrtvInd index and the AdGrpInd index.
Each ad group contains, on average, 2.5 creatives, and the
CrtvInd strategy indexes each of the terms once for each
of them. As can be seen in Table 2, the same ratio of 2.5
approximately explains the diﬀerence in index size between
CrtvInd and AdGrpInd.

5.2.2 Query Runtime
Based on the relative sizes of the indexes, we expected to
see a signiﬁcant reduction in query runtime when AdGr-
pInd is used, especially compared to CTInd. To test this,
we sample 900 queries from our development set, and run
them on all 12 indexes, with or without reranking, as shown
in Fig. 3. Each curve corresponds to an index type; each
point on the curve corresponds to one of the sub-samples.

It is evident that without the reranking step the execution
of the queries in CTInd is signiﬁcantly slower than that of

the other two methods (Fig. 3 (a)). Moreover, the diﬀerence
in runtime increases with the index size. Reranking the top
ten results (Fig. 3 (b)) increases the runtime for the two
indexes where reranking is performed5, however the runtime
still remains well below of that of CTInd. Finally, Fig. 3 (c),
shows the diﬀerence between the runtime with or without
reranking for CrtvInd and AdGrpInd. Note, that this
diﬀerence increases slowly with the size of the index. This
growth is much slower than that demonstrated by CTInd,
indicating that even for larger index sizes, reranking with
the CrtvInd or AdGrpInd indexes will remain a more
eﬃcient strategy than retrieval with the CTInd index.
5.3 Ad Ranking

In the previous section, we have shown that the AdGr-
pInd index provides the smallest index size and the best
retrieval eﬃciency, overall. In this section, we demonstrate
that when the proposed structured reranking algorithm Struc-
tRank is applied, AdGrpInd can provide the best retrieval
eﬀectiveness (in terms of nDCG), as well.

To this end, we consider the ad ranking task. This task is
similar to the standard ad hoc information retrieval setting.
Given a user query, ads are ranked by their relevance score
and the top ones are presented to the user. Note, that in
contrast to standard document retrieval, only the selected
(cid:104)creative, term(cid:105) pair is shown to the user, and not the entire
ad group.

Our retrieval algorithms diﬀer in the way the underlying
index is structured and in the way the relevance score is com-
puted (see Section 3). We evaluate all the algorithms using a
set of judged query-document pairs originally retrieved by a
commercial ad retrieval system6. All the methods are evalu-
ated using a held-out set of 773 queries that have at least one

5Recall that reranking is not needed for the CTInd index.
6We refrain from comparing our experimental system to the pro-
duction system, since it takes into account many other features
not accounted for in this study.

0100200300400Total Runtime (no reranking)Sample Size (% of ad groups)Runtime (sec)25%50%75%100%CTCplCrtvCplAdGrpCpl0100200300400Total Runtime (with reranking)Sample Size (% of ad groups)Runtime (sec)25%50%75%100%CTCplCrtvCplAdGrpCpl0100200300400Total Reranking RuntimeSample Size (% of ad groups)Runtime (sec)25%50%75%100%CrtvCplAdGrpCplWWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA107AdGrpRank
CrtvRank
CTRank
StructRank

nDCG@1

0.570
0.577
0.655∗
0.679∗
†
(+3.66%)

nDCG@5

0.631
0.654
0.725∗
0.742∗
†
(+2.34%)

nDCG@10

0.668
0.698
0.767∗
0.780∗
†
(+1.69%)

Table 3: Retrieval eﬀectiveness of all the retrieval
methods. The ‘∗’ symbol denotes statistically sig-
niﬁcant diﬀerence between the results of CTRank
and those of AdGrpRank and CrtvRank methods,
while the ‘†’ symbol denotes statistically signiﬁcant
diﬀerence between StructRank and CTRank (using
Wilcoxon sign test, α < 0.05). The numbers in the
parentheses indicate % improvement of StructRank
over the CTRank baseline.

non-Bad judgment associated with them (that is, the rank-
ing in response to this queries can be potentially improved
by reranking). We employ each of the retrieval strategies
described in Section 3 as baselines.
In addition, we use
the StructRank algorithm (Algorithm 4) to test whether
structural information captured by this method beneﬁts the
ranking.

Table 3 summarizes the retrieval results for all four re-
trieval algorithms. First, we note that CTRank has the
best performance among the baseline methods. CTRank
outperforms the other baselines to a statistically signiﬁcant
degree on all nDCG measures. While using CTRank for
ranking using a CTInd index is infeasible for large cor-
pora (as was explained in Section 5.2), Table 3 shows that
it can signiﬁcantly improve the performance when applied
as a reranking method on a smaller index.

The key result in Table 3 is the performance of the Struc-
tRank algorithm. As described in Section 4.2, StructRank
ranking function encodes the hierarchical structure of an ad
group as a vector of features, and linearly combines these fea-
tures. StructRank consistently and signiﬁcantly improves
the performance over the CTRank baseline at all NDCG
positions. The improvements at the top ranks are particu-
larly high (3.7% improvement for nDCG@1); this is crucial
for sponsored search since the user only sees a few top results
(ads).

Fig. 4 plots the nDCG curves corresponding to the four re-
trieval algorithms from Table 3. The performance improve-
ments attained by StructRank over the baselines are con-
sistent over ranks 1 through 10.

As mentioned in Section 5.1, StructRank learns a sep-
arate linear model for each query length bin. Table 4 pro-
vides a breakdown of retrieval performance improvements
attained for each of these bins. Table 4 shows consistent
improvements over the best baseline, CTRank, in all the
bins. These performance gains are the most striking for
long (four words or more) queries, reaching more than 7%
improvement for NDCG@1.

This indicates the importance of using the structural in-
formation of ad groups for longer queries, as the probability
of an exact or even partial match of these queries to any
single (cid:104)creative, term(cid:105) pair in the index is low. Therefore,
the ad group structure provides a much needed context to
improve the coverage and the relevance of the ads displayed
in response to these long queries.

Figure 4: NDCG Curves.

Finally, Table 5 shows the highest weighted features, as
learned for each of the query length bins. Note the domi-
nantly high weight of a crtvTermPairScore feature, and its
increase with the query length. This indicates the high im-
portance of a close match with a (cid:104)creative, term(cid:105) pair, if
such exists, especially for long queries. This high weight also
explains the success of the CTRank algorithm, which sig-
niﬁcantly outperforms the other two baseline methods (see
Table 3).

In addition, we note the diﬀerences in feature weights
learned for each bin. While for short, single-word queries,
features traditionally associated with navigational queries
(like title and URL match) receive high weights, for longer
queries, query word coverage features (for instance, the fea-
ture adGrpTermQueryCover) are more important. These diﬀer-
ences showcase both the beneﬁt of our strategy of learning
a separate model for each query bin, and an adequacy of
query length as the binning criterion.
5.4 Ad Filtration

In this section we apply our algorithms to the ad ﬁltra-
tion task [24].
In this task, the goal is to learn a binary
classiﬁer that is trained to distinguish between relevant and
non-relevant ads. This type of classiﬁer is often applied to
an initial ranked list of ads that has been returned by an in-
formation retrieval system such as the one described in the
previous section.

The classiﬁer is used to pass only the most relevant ads
to the next stage of ranking, where they can be reranked
using non-textual features such as click-through rates and
bid amounts. Filtration of non-relevant ads is important in
sponsored search, where the display of an inappropriate ad
can lead to a decrease of click-through rate on the entire list.
On the other hand, a failure to show a highly relevant ad
can lead to a loss of potential revenue for the search engine.
Accordingly, we convert the original graded relevance judg-
ments into a binary class label. All the (cid:104)creative, term(cid:105)
pairs rated as Bad w.r.t. a query are assigned a negative la-
bel, while all the other pairs are assigned a positive label.
As is often the case with relevance judgments in information
retrieval, the majority of our data is associated with the neg-
ative class. Out of 27,763 rated pairs in our development set,
only 6,219 pairs have a positive label.

nDCG CurvespositionsnDCG@123456789100.550.600.650.700.750.80AdGrpRetCrtvRetCTRetStructRankWWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA108Len1 (143 queries)

Len2+3 (443 queries)

Len4+ (187 queries)

nDCG@1

nDCG@5

nDCG@10

nDCG@1

CTRank
StructRank

0.765
0.803

0.841
0.849

0.865
0.871

0.655
0.669

(+4.96%)

(+0.95%)

(+0.69%)

(+2.14%)

nDCG@5
0.716
0.731∗
(+2.09%)

nDCG@10
0.757
0.769∗
(+1.59%)

nDCG@1

0.569
0.610

(+7.2%)

nDCG@5
0.656
0.686∗
(+4.57%)

nDCG@10
0.715
0.737∗
(+3.08%)

Table 4: Retrieval eﬀectiveness of retrieval methods CTRank and StructRank, for queries of diﬀerent length.
The ∗ denotes statistically signiﬁcant diﬀerence with CTRank method (Wilcoxon sign test, α < 0.05). The
numbers in the parentheses indicate % improvement of StructRank over CTRank baseline.

Len1

crtvTermPairScore
adGrpUrlMatchRatio
adGrpEntropy
adGrpTitleMatchRatio
adGrpTermQueryCover

0.83
0.05
0.04
0.03
0.02

Len2+3
crtvTermPairScore
adGrpEntropy
adGrpScore
adGrpTermQueryCover
adGrpUrlMatchRatio

0.93
0.03
0.02
0.02

< 0.01

Len4+
crtvTermPairScore
adGrpTermQueryCover
adGrpTermCount
adGrpEntropy
adGrpUrlMatchRatio

0.95
0.03

< 0.01
< 0.01
< 0.01

Table 5: Learned weights for diﬀerent query groups.

AdGrpRank
CrtvRank
CTRank
StructRank

Precision

0.510
0.596
0.612
0.680

Recall
0.399
0.458
0.560
0.660

F1

0.448
0.518
0.585
0.670

(+11.1%)

(+17.9%)

(+14.5%)

Table 6: Summary of ad ﬁltration results. Precision
and recall are reported at the decision threshold that
achieves maximum F1.

We use C4.5 decision tree implemented in Weka for all our
classiﬁcation experiments [30]. In the development phase of
our experiments, we tested other linear and non-linear clas-
siﬁers implemented in Weka, and found that C4.5 decision
tree performed better than other stand-alone classiﬁers, and
its performance was comparable to that obtained by com-
plex ensemble classiﬁers like AdaBoost, while being much
faster at train time.

The class imbalance present in our data often leads to
a poor recall of positive examples. Since Weka allows to
output a prediction conﬁdence level for each instance, we
tune the prediction threshold of our classiﬁer (a conﬁdence
level below which the instance is classiﬁed as negative) to
optimize the F1 measure on the development set.
Previous work [24] concentrated on identifying the fea-
tures within the (cid:104)creative, term(cid:105) pair that improve the ad
In this work, we are interested in
ﬁltration performance.
the importance of features outside the (cid:104)creative, term(cid:105) pair,
namely the structural features from the entire ad group, as
discussed in Section 4.2.1. To this end, we construct three
baseline classiﬁers, each using as a single feature an out-
put of the retrieval algorithms CTRank, CrtvRank and
AdGrpRank ((cid:104)creative, term(cid:105) pair, creative and ad group
scores, respectively). We compare the performance of these
baseline classiﬁers, to the performance of a classiﬁer which
uses all the structural features used by StructRank.

Table 6 shows the precision, recall and F1 measure at-
tained by the three baseline classiﬁers and the structural
classiﬁer, denoted StructRank, for an ad ﬁltration task.
As Table 6 demonstrates, StructRank is superior to all
the baselines. It achieves more than 10% improvement over
the best performing baseline, CTRank, on all three mea-

sures, and all the improvements are statistically signiﬁcant
according to the McNemar’s test (α < 0.05).

Table 7 provides a more detailed analysis of the ad ﬁl-
tration experiments. Table 7 (a) breaks down the perfor-
mance of each classiﬁer (in terms of F1 measure) by query
length bins, similarly to the analysis shown in Table 4 for
the NDCG measure. The key observation in this table is
the importance of structural features for long queries. While
the contribution of these features is signiﬁcant for all query
lengths, for long queries (having with four or more words)
they provide an almost 90% improvement (sic!) over the
baselines that use no structural information.

Finally, Table 7 (b) details the ﬁltration rates for all ﬁve
relevance grades. It is clear that StructRank consistently
outperforms the other baselines, ﬁltering out fewer relevant
and more non-relevant ads, across the entire range of rele-
vance grades.

6. CONCLUSIONS

In this paper we investigated the important yet often over-
looked issue of ad indexing. Existing advanced match algo-
rithms index the ads using standard IR indexing techniques
with ﬂat document representation. However, in practice ads
are inherently hierarchically structured and organized into
accounts, campaigns and ad groups. We investigated three
diﬀerent strategies for indexing this structured data corpus,
including creative-bid term coupling, creative coupling, and
ad group coupling. We showed that the creative-bid term
coupling, which is the most straightforward implementation,
results in an infeasibly large index, while the ad group cou-
pling index was both signiﬁcantly smaller and much more
practically useful.

In addition to investigating diﬀerent indexing strategies,
we also explored how to eﬀectively retrieve ads using struc-
tured ad indexes. We proposed a novel reranking strat-
egy that exploits the ad corpus structure. Our structured
reranking approach makes use of a linear machine-learned
ranking function that uses a variety of structural ad features.
As we show, the approach is not only useful for ranking ads,
but also for ﬁltering (classifying) bad ads.

We conducted a comprehensive set of experiments on a
sponsored search test collection from a large commercial
search engine. Our experimental results show that using the

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA109AdGrpRank

CrtvRank
CTRank

StructRank

Len1
(2419)
0.636
0.667
0.750
0.834

(+11.2%)
(a)

Len2+3
(16182)

0.427
0.518
0.571
0.664

Len4+
(9162)
0.286
0.343
0.272
0.516

(+16.3%)

(+89.7%)

Grade
Perfect (54)
Excellent (71)
Good (980)
Fair (5114)
Overall Rel. (6219)
Bad (21544)

AdGrpRank CrtvRank CTRank StructRank

0.574
0.437
0.542
0.614
0.601
0.889

0.204
0.380
0.419
0.572
0.542
0.910

(b)

0.278
0.268
0.339
0.463
0.440
0.897

0.204
0.225
0.199
0.370
0.340
0.910

Table 7: (a) F1 measures for the ad ﬁltration classiﬁers, binned by query length. (b) Detailed ﬁltration
rates (ratio of ﬁltered ads) for each relevance grade. The numbers in parentheses indicate the number of
(cid:104)creative, term(cid:105) pairs in each group.

proposed structured reranking strategy with the (small) ad
group index yields statistically signiﬁcant improvements in
retrieval and ﬁltering eﬀectiveness over the simple approach
that produces a combinatorially huge creative-bid term in-
dex. To summarize, we show that a small, structured index
can be used to retrieve highly relevant sponsored search ads.
All of the experiments reported in this paper were con-
ducted on a sponsored search test collection. However, we
believe that our methodology can be directly applied to the
content match scenario as well. Although the construction of
the ad query would be diﬀerent in that case (the input would
be a Web page rather than a search query), the server-side
indexing and retrieval of ads should mostly stay the same as
in sponsored search. We intend to apply our methodology
to content match advertising in future work.

7. REFERENCES

[1] A. Anagnostopoulos, A. Z. Broder, E. Gabrilovich,

V. Josifovski, and L. Riedel. Just-in-time contextual
advertising. In Proceedings of CIKM, pages 331–340, 2007.

[2] M. Bendersky and O. Kurland. Utilizing Passage-Based

Language Models for Document Retrieval. In Proceedings of
ECIR, pages 162–174, 2008.

[16] A. Lacerda, M. Cristo, M. A. Gon¸calves, W. Fan, N. Ziviani,
and B. Ribeiro-Neto. Learning to advertise. In Proceedings of
SIGIR, pages 549–556, 2006.

[17] M. Lalmas. XML information retrieval. In Encyclopedia of

Library and Information Sciences. Taylor and Francis Group,
2009.

[18] T.-Y. Liu. Learning to rank for information retrieval.

Foundations and Trends in Information Retrieval, 3(3), 2009.

[19] D. Metzler and W. B. Croft. Linear feature-based models for
information retrieval. Information Retrieval, 10(3):257–274,
2007.

[20] P. Ogilvie and J. Callan. Hierarchical language models for XML

component retrieval. In Advances in XML Information
Retrieval, pages 224–237. Springer, 2005.

[21] J. M. Ponte and W. B. Croft. A language modeling approach to
information retrieval. In Proceedings of SIGIR, pages 275–281,
1998.

[22] X. Qi and B. D. Davison. Classiﬁers without borders:

incorporating ﬁelded text from neighboring web pages. In
Proceedings of SIGIR, pages 643–650, 2008.

[23] F. Radlinski, A. Broder, P. Ciccolo, E. Gabrilovich,

V. Josifovski, and L. Riedel. Optimizing relevance and revenue
in ad search: A query substitution approach. In Proceedings of
SIGIR, 2008.

[24] H. Raghavan and D. Hillard. A relevance model based ﬁlter for

improving ad quality. In Proceedings of SIGIR, 2009.

[25] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S. de Moura.

Impedance coupling in content-targeted advertising. In
Proceedings of SIGIR, 2005.

[3] A. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski, D. Metzler,

[26] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25

L. Riedel, and J. Yuan. Online expansion of rare queries for
sponsored search. In Proceedings of WWW, pages 511–520,
2009.

[4] A. Broder, M. Fontoura, V. Josifovski, and L. Riedel. A

semantic approach to contextual advertising. In Proceedings of
SIGIR, pages 559–566, 2007.

[5] A. Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich,

V. Josifovski, and L. Riedel. Search advertising using web
relevance feedback. In Proceedings of CIKM, 2008.

[6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using
gradient descent. In Proceedings of ICML, 2005.

[7] J. P. Callan. Passage-level evidence in document retrieval. In

Proceedings of SIGIR, pages 302–310, 1994.

[8] W. B. Croft and J. Laﬀerty. Language Modeling for

Information Retrieval (The Information Retrieval Series).
Springer, December 2003.

[9] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, and D. C.

Robbins. Stuﬀ I’ve seen: a system for personal information
retrieval and re-use. In Proceedings of SIGIR, pages 72–79,
2003.

[10] N. Fuhr, M. Lalmas, S. Malik, and Z. Szlavik. Advances in

XML Information Retrieval. Springer, 2005.

[11] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of IR techniques. ACM TOIS, 20(4):422–446, 2002.

[12] T. Joachims. Optimizing search engines using clickthrough

data. In Proceedings of KDD, pages 133–142, 2002.

[13] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating

query substitutions. In Proc. of WWW, pages 387–396, 2006.

[14] G. Kazai and A. Doucet. Overview of the INEX 2007 book

search track: Booksearch ’07. SIGIR Forum, 42(1):2–15, 2008.
[15] A. C. K¨onig, K. Church, and M. Markov. A data structure for

sponsored search. In Proceedings of ICDE, 2009.

extension to multiple weighted ﬁelds. In Proceedings of CIKM,
pages 42–49, 2004.

[27] A. Singhal, G. Salton, M. Mitra, and C. Buckley. Document

length normalization. Information Processing and
Management, 32(5):619–633, 1996.

[28] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A

language model-based search engine for complex queries. In
Proceedings of the International Conference on Intelligence
Analysis, 2004.

[29] H. Wang, Y. Liang, L. Fu, G.-R. Xue, and Y. Yu. Eﬃcient

query expansion for advertisement search. In Proceedings of
SIGIR, pages 51–58, 2009.

[30] I. H. Witten and E. Frank. Data Mining: Practical machine

learning tools and techniques. Morgan Kaufmann, 2nd edition,
2005.

[31] Q. Wu, C. Burges, K. Svore, and J. Gao. Adapting boosting for

information retrieval measures. Information Retrieval, pages
1–17, 2009.

[32] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support

vector method for optimizing average precision. In Proceedings
of SIGIR, pages 271 – 278, 2007.

[33] C. Zhai. Statistical language models for information retrieval a

critical review. Foundations and Trends in Information
Retrieval, 2(3):137–213, 2008.

[34] C. Zhai and J. Laﬀerty. A study of smoothing methods for

language models applied to information retrieval. ACM
Transactions on Inf. Sys. (TOIS), 22(2):179–214, 2004.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA110