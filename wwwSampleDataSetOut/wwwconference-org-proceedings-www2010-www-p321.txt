Do You Want to Take Notes?

Identifying Research Missions in Yahoo! Search Pad

Debora Donato

Yahoo! Labs

Sunnyvale, CA

debora@yahoo-inc.com

Tom Chi
Yahoo! Inc.

Sunnyvale, CA

tomchi@yahoo-inc.com

Francesco Bonchi

Yahoo! Research
Barcelona, Spain

bonchi@yahoo-inc.com

Yoelle Maarek
Yahoo! Research

Haifa, Israel

yoelle.maarek@yahoo.com

ABSTRACT
Addressing user’s information needs has been one of the
main goals of Web search engines since their early days.
In some cases, users cannot see their needs immediately an-
swered by search results, simply because these needs are too
complex and involve multiple aspects that are not covered
by a single Web or search results page. This typically hap-
pens when users investigate a certain topic in domains such
as education, travel or health, which often require collect-
ing facts and information from many pages. We refer to this
type of activities as “research missions”. These research mis-
sions account for 10% of users’ sessions and more than 25%
of all query volume, as veriﬁed by a manual analysis that
was conducted by Yahoo! editors.

We demonstrate in this paper that such missions can be
automatically identiﬁed on-the-ﬂy, as the user interacts with
the search engine, through careful runtime analysis of query
ﬂows and query sessions.

The on-the-ﬂy automatic identiﬁcation of research mis-
sions has been implemented in Search Pad, a novel Yahoo!
application that was launched in 2009, and that we present
in this paper. Search Pad helps users keeping trace of re-
sults they have consulted. Its novelty however is that unlike
previous notes taking products, it is automatically triggered
only when the system decides, with a fair level of conﬁdence,
that the user is undertaking a research mission and thus is
in the right context for gathering notes. Beyond the Search
Pad speciﬁc application, we believe that changing the level
of granularity of query modeling, from an isolated query to
a list of queries pertaining to the same research missions, so
as to better reﬂect a certain type of information needs, can
be beneﬁcial in a number of other Web search applications.
Session-awareness is growing and it is likely to play, in the
near future, a fundamental role in many on-line tasks: this
paper presents a ﬁrst step on this path.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search pro-
cess; H.3.5 [Online Information Services]: Web-based
services

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

General Terms
Human Factors

Keywords
Task-oriented search, Persistent search, Query logs, Sessions

1.

INTRODUCTION

Users turn to search engines in order to satisfy their in-
formation needs [8]. The classical scenario is for them to ex-
press their needs as a free-text query and then scan through
results in order to identify full or partial answers. Their
needs can then be:

• Satisﬁed: this happens when users obtain the answer
they were seeking immediately on the ﬁrst page of re-
sults. They can either directly read the answer on
the results page itself, when generated for instance by
services such as a Google’s oneboxes [16] or a Yahoo
shortcut [32], (e.g., calculator, weather and sports re-
sults), or reach it after clicking on one of the top ranked
pages.

• Not satisﬁed: this happens when (1) users did not
succeed in formulating their query, when (2) relevant
content does not exist, or ﬁnally when (3) relevant con-
tent does exist but the search engine can not identify
it. By default, most engines will focus at this stage
on the ﬁrst case (since there is not much they can do
in real time with the two other cases) and will oﬀer
users query assistance tools, pointing them to related
queries that might bring better results.

• Partially satisﬁed: this usually happens when users
have complex information needs, for which there does
not exist one single Web page that holds all the needed
information. In this scenario, the user will like some of
results, disregard others and continue exploring, click-
ing on a few results, trying out related queries, while
still gathering information. Such sessions can last from
a few minutes to several days. Examples include travel
needs (when a traveler veriﬁes hotels, restaurants or
transportation means around a certain location), edu-
cation needs (when students work on an assignment),
or medical needs (when a patient studies a speciﬁc ill-
ness, its symptoms and treatments).

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA321We are focusing in this paper on the third case, in which
the users’ needs are too complex or too heterogeneous to be
answered in one shot. We refer to these non-ephemeral in-
formation seeking tasks as “research missions”. This notion
relates to the notion of information gathering on the Web as
introduced by Kellar et al. in [20], and deﬁned as the col-
lection of information from multiple sources. In the user’s
study they conducted over a period of one week in 2005
with 21 university students, the authors reported that infor-
mation gathering tasks accounted for 13.4% of overall Web
usage and was the fourth most important activity on the
Web after “transactions” (46.7%), “just browsing” (19.9%)
and “fact ﬁnding” (18.3%).

Research missions, as we deﬁne them, restrict and reﬁne
the notion of information gathering as they can occur only
during search sessions as opposed to overall Web usage. By
manually analyzing actual query sessions over a period of
3 days, we veriﬁed that on average 10% of search sessions
are “research missions” but more interestingly, that about
25% of query volume occurs in these sessions. Based on
this data, we argue that research missions deserve special
attention and treatment.

Our longer-term vision is that automatically identifying
these research missions could potentially break the classical
search paradigm, which maps one query to a list of results,
into one where entire sessions are analyzed for intent, and
results match session intent. We believe that by consider-
ing queries in the context of a session of related expressions
of a common need, rather than in isolation, search engines
might be able to achieve a better understanding of the real
users’ intent. Our belief is supported by some recent works
showing that not only the previous query, but also the long-
term interests of users are important for understanding their
information needs [23, 27].

As a ﬁrst step towards this long-term goal, we propose to
demonstrate that research missions can indeed be discovered
on the ﬂy, while users are interacting with the search engine,
with a good enough precision rate to make them valuable.
In order to substantiate our claim, we will use the recently
deployed Yahoo! Search Pad application, which integrates
a research mission identiﬁcation component that we have
developed.

Search Pad is an application that has been designed pre-
cisely to help users undertake research missions. Search Pad
allows users to easily keep trace of results they have con-
sulted. They can arrange and annotate them for later per-
sonal usage or for sharing with others. Search Pad’s novelty
comes from its being triggered only when the search en-
gine believes that the user is undertaking a research mission
rather than looking for quick, disposable results. Visited
pages are automatically added to the appropriate search pad
(either the current one when the query belongs to the same
research mission or a new one if necessary) without requir-
ing the user to speciﬁcally “mark” them. There have been
multiple attempts in the past to support similar functional-
ity, such as Bharat’s original Searchpad1 [4] research tool,
or Google’s notebook [14], whose development has now been
discontinued [22]. A major diﬀerence between these previous
tools and Yahoo! Search Pad was that they required users
to proactively invoke the notes taking tool and to manu-
ally mark relevant pages. In contrast, Search Pad’s novelty

1Note that Bharat’s tool was called “SearchPad” in one
word, while ours is called “Search Pad” in two words.

comes from its automatic identiﬁcation of research missions
and its being triggered only when the system decides, with
a fair level of conﬁdence, that the user is in the right context
for gathering notes.

As such, Search Pad represents the ideal application for us
to verify our claim that identifying and using search missions
is valuable to users. Search Pad is automatically triggered
at query time when a search mission is identiﬁed.

The goal of this paper is thus to demonstrate, using ac-
tual data gathered by Search Pad since it has already been
deployed in the US for a few months, that research missions
can be automatically detected, at the scale of Web (meeting
performance and scalability requirements) and with enough
accuracy that they bring value to users.

The rest of this paper is organized as follows. Section 2
describes the Search Pad application that we will use as ba-
sis for demonstrating the validity of our approach. Section 3
provides the needed preliminaries and deﬁnitions of the tech-
nical problem tackled in this paper: automatically identify-
ing research missions. Section 4 describes our solution, i.e.,
the internals of the triggering component of Search Pad, a
machine-learning based module aimed at detecting research
missions to trigger Search Pad. Section 5 reports our eval-
uation using Search Pad and provides results inferred from
actual usage data. We conclude with directions for future
work, ﬁrst for enhancing Search Pad, and then for leverag-
ing the beneﬁts of research missions in other domains such
as core ranking.

2. SEARCH PAD: THE APPLICATION

In spite of recent eﬀorts at personalizing search, and stor-
ing past interactions for logged-in users, Web search remains
basically “stateless”.

Search Pad is a feature of Yahoo! Search that was launched
in July 2009 and precisely addresses this issue. It help users
keep track of related searches and visited pages that relate
to a same “research mission”2.

Figure 1: Take notes on Barcelona cheap ﬂights

2Note that we have deﬁned this concept only intuitively so
far, which is suﬃcient for understanding the application and
usage scenarios. We will formalize the deﬁnition of search
missions in the following section.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA322To illustrate, let us consider the following interaction sce-
nario with Search Pad. A user is planning a trip to Barcelona
and starts issuing a few related queries, such as Barcelona
cheap flights, Barcelona airport and Barcelona air-
port transfer, clicking each time on a few results. When
issuing the query Barcelona airport transfer, Search Pad
detects that the user has been researching a topic rather
than looking for quick answers and asks whether she wants
to take notes, as shown in Figure 1.

If she accepts, she will then be shown a “Search Pad docu-
ment” , or “pad” for short, already populated with the links
of the pages she has visited during the current research mis-
sion, together with their thumbnails, as shown in Figure 2.
At this stage, the user can delete some of the links, move
them around, add some personal notes, and eventually save
the pad.

Figure 2: An opened pad object

The pad object is fully editable, supporting drag and drop
and paste operations and can be reopened and reused at a
later time, for sessions that span over a number of days or
weeks.
It can also be shared on Facebook, Twitter, and
Delicious or simply embedded via a persistent HTML link
into any Web page, as illustrated in Figure 3.

Figure 3: Sharing a pad object

Previous eﬀorts at taking notes while searching or brows-
ing the Web can be roughly classiﬁed into three categories:

Link centric eﬀorts were the ﬁrst to appear as it was clear
that users would not easily remember URLs of interesting
pages while surﬁng the Web or visiting search results. Book-
marks were invented for this purpose and were an integral
part of Web browsers since the early days of the Web (under
the name “Hotlist” in the original Mosaic, or “Favorites” in
Internet Explorer). In spite of a few research attempts at au-
tomatically structuring them [21, 24], bookmarks have been
shown to be complex to manage as soon as their number goes
over a few dozens [1]. Some applications such as Zotero [33]
or hosted bookmark services, as oﬀered by major Web search
engines, do a slightly better job at managing and searching
bookmarks. Google launched in 2006 its Notebook under
the form of a hosted service, and browser plug-in, that al-
lowed users to store links in a collection of “notebooks”, and
annotate them with comments and labels. This tool saw
its development discontinued in 2009 and stopped accepting
new users. There exist numerous alternatives for note tak-
ing, such as Evernote [11], yet no clear winner has emerged
to make the unanimity in this market. Finally bookmarks
took an interesting turn with the social bookmarking3 phe-
nomenon, in the Web2.0 model of sharing comments and
opinions with the community. Nevertheless, at the personal
level, native browser bookmarks are still the most handy
means of keeping track of interesting search results.

Page centric eﬀorts work at a ﬁner level of granularity
by allowing users not only to save interesting results but to
directly annotate paragraphs or passages in the page. An
example of application providing this functionality is Di-
igo [10], which allows users to attach sticky notes to high-
lighted part of the Web page in a persistent manner. Hunter
Gatherer [28] is a similar tool that allows to collect compo-
nents from within Web Pages in just one single page. The
tool supports both the within-page collection making and
the information management. Another recent example is
Google Sidewiki [15], a new feature of Google’s toolbar that
allows users to publish and share annotations on a given
Web page.

Search centric eﬀorts are slightly diﬀerent as they are spe-
ciﬁc to search results, while the previous ones can also relate
to arbitrary pages reached by other means. The ﬁrst piece
of work in this space was SearchPad [4], which used a proxy
to decorate results from a list of search engines (namely Al-
taVista, Excite, Google and HotBot) with a “Mark” but-
ton. Clicking on this button resulted in storing the se-
lected results in SearchPad together with their associated
queries. Users could at any time visit SearchPad to retrieve
all “bookmarked queries”, and their associated “leads”, i.e
results manually marked by users. A few years later, Ask
oﬀered the related MyStuﬀ service [2]. MyStuﬀ also dec-
orates each search result with a “save” link, which allows
registered users to save interesting results for later usage.

Yahoo! Search Pad diﬀers from these solutions on several

aspects

3We do not discuss social bookmarks here as they are out of
the scope this paper, which focuses on the personal beneﬁts
of identifying research missions.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA3231. The most critical diﬀerentiator is the triggering mech-
anism, which prompts the user for taking notes only
when chances of its being useful are high.
In other
words, Search Pad is made clearly visible to users dur-
ing these research missions for which it should be most
needed, and stays out of the way otherwise. This fea-
ture is to our knowledge not available in any other
existing tool and is supported by the live research mis-
sion detection mechanism, which is our key technical
contribution in this paper.

2. Another important feature is the automatic distinc-
tion between diﬀerent sessions based not only on time
considerations but also on topical coherences between
related queries as more formally deﬁned later. Thus,
when a user undertakes a diﬀerent research mission,
Search Pad should segment between those and auto-
matically start a new pad, if given suﬃcient evidence.

3. Additionally, links are automatically added to a search
pad in such a manner that when a pad is prompted to
a user, it is prepopulated with content. While not all
links might be relevant to the same extent, this moti-
vates the user to continue taking notes. It is also easier
to delete less relevant links, as opposed to breaking the
search ﬂow by selectively adding them.

4. The last diﬀerence is that Search Pad does not require
users to use a dedicated plug-in, a speciﬁc toolbar or
separate application, but is built-in within the main
search service of Yahoo!. Users will be required to
sign-in (but not to register to a speciﬁc service) only
when saving a pad. This feature signiﬁcantly reduces
the adoption barrier.

We will focus in the rest of this paper on the ﬁrst two
aspects, which are the ones that required technical novelty,
namely the automatic identiﬁcation and segmentation of re-
search missions.

3. PROBLEM STATEMENT

Numerous studies [5, 6, 7, 17, 19, 25, 26, 29] have inves-
tigated various aspects of users’ behavior on the Web. As
a result, it is now commonly agreed upon that the informa-
tion inferred by mining users’ search activities is extremely
valuable to search engines. Given that the extracted infor-
mation is adequately anonymized and aggregated, it is a
critical source of information for delivering quality results in
any type of search engines service, such as ranking, spelling
and query assistance, to name just a few.

We deﬁne below the basic elements and objects that are

used in mining users’ interactions.

3.1 General Deﬁnitions

Query log. Search engines store information about user’s
interactions in “query logs”. Query logs diﬀer in format be-
tween search engines, but at a bare minimum typically as-
sociate with each submitted query, a list of results, as well
as whether or not users clicked on them. More formally a
query log L is a set of records hqi, ui, ti, Vi, Cii, where: qi is
the submitted query, ui is an anonymized identiﬁer for the
user who submitted the query, ti is a timestamp, Vi is the
set of documents returned as results to the query, and Ci

is the set of documents clicked by the user (which can be
empty).

Sessions. We reuse here the deﬁnition of query session (or
session for short) that was given in [5]. A session is the
sequence of queries issued by a single user within a speciﬁc
time limit. If u ∈ U is the user identiﬁer and tθ is a timeout
threshold, a user query session S is deﬁned as a maximal
ordered sequence

S = (cid:10) hqi1 , ui1 , ti1 i, . . . , hqik , uik , tik i (cid:11),

where ui1 = · · · = uik = u ∈ U , ti1 ≤ · · · ≤ tik , and
tij+1 − tij ≤ tθ, for all j = 1, 2, . . . , k − 1.

The activity of a same user is split in two (or more)
sessions whenever the time interval between two sequential
queries exceeds the timeout threshold. The typical timeout
is tθ = 30 minutes [9, 25, 30].

Chains and Missions, Radlinski and Joachims [26] de-
ﬁne a query chain (or chain for short) as a sequence of re-
formulated queries that express a same information need.
Baeza-Yates et al. deﬁne in [3] the related concept of log-
ical session and Jones et al.
in [19] deﬁne the concept of
mission, as “a related set of information needs, resulting in
one or more goals”, where a goal is “an atomic information
need, resulting in one or more queries”. To illustrate, Jones
et al. give, as example of a sequence of queries that can
be mapped into a same mission, the following set of con-
secutive queries: (brake pads, auto repair, auto body
shop, batteries, car batteries, buy car battery on-
line).

3.2 Research Sessions and Missions

An interesting distinction between chains and missions is
that chains deal with the actual sequence of queries, while
missions refer to the underlying information needs. Follow-
ing this distinction, we deﬁne here the notions of research
session and associated research mission.

A research session belongs to the same space as a chain
and represents a set of queries (and associated information
as provided by query logs) that fulﬁlls certain constraints.

A research mission, in the same space as Jones’ missions, is
a set of related and complex information needs. Such com-
plexity is reﬂected by the user’s engagement such as the
time spent or the number of atomic tasks accomplished by
the user in the attempt of fulﬁlling these needs. Thus, a
research session is deﬁned as the set of all the user activities
(queries and clicks) needed to fulﬁll a research mission.

We introduce these two deﬁnitions to insist on the fact
that research sessions do not follow some arbitrary timeout
rules, as sessions typically do via the previously mentioned
tθ threshold.
Indeed, as research sessions reﬂect research
missions, they can span over a number of hours if not days.
To illustrate, consider our previous example of a user who
is planning a trip to Barcelona and searches for cheap plane
tickets and accommodations. She will typically pursue this
task over a period of several days. Consequently the queries
composing a research session do not need to be consecutive.
Following the previous example, our user might search for
plane tickets then search for reviews and show times of a few
movies before going to the theater the same evening. She
will then return to her trip planning the next day, searching
for a hotel in Barcelona. Thus, a general session bounded
by a given tθ may contain queries reﬂecting several distinct

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA324missions, e.g., trip to Barcelona, best movie to see tonight,
and a research session may contain queries originating from
several distinct sessions, e.g., one session on a day, and an-
other on the following day.

As they are not limited in time, research sessions use other
signals to identify a shared research mission, namely topical
coherence and user’s engagement.

To reﬂect topical coherence, we use the two functions f
and s as deﬁned below. First, given a query q ∈ Q and a
set of topics T , we use a function f : Q → T that maps
each query q in the query log L to a topic p. In the ex-
treme case, f can simply be the identity function, no topical
association is provided, and we will not be able to identify
that the queries auto repair and car batteries, as per
our previous example, belong to the same research session.
We will verify in the evaluation section that an adequate
choice of f does increase recall and thus recommend pick-
ing a function f that can be computed eﬃciently at the
scale of Web traﬃc. A number of options exists to deﬁne f ,
depending on the desired level of precision, one possible so-
lution being to use Wikipedia as source of world knowledge
as done in [13]. Most popular Web search engines do have at
their disposal such functions that they use to improve recall
for sponsored search [12] and this what we recommend using
in our case.

In addition, we use a similarity measure among topics in
T , that is a function s : T × T → [0, 1]. Again in the
extreme case of f being the identity function, a simple simi-
larity function can be the normalized ratio of common words
or stems between two queries. In that case, in our previous
example of “batteries” and “car batteries”, the measure
of similarity will be 0.5. More sophisticated techniques as
proposed by [13] can be used.

To reﬂect user’s engagement, we augment the basic hq, u, ti
query/user/time session tuple with the click information
stored in query logs, namely the set of clicked results C.
We use C or rather the size of C, as will be seen below, as
a strong signal of the complexity of a research mission. The
larger |C|, the more probable it is that the user did not see
her needs satisﬁed and is in the unsatisﬁed/partially satis-
ﬁed category and therefore continues exploring. Intuitively,
long sessions with a suﬃcient number of related queries, for
which the user exhibits a stronger engagement in terms of
actions performed, that is, with a high |C|, are good can-
didates for research sessions. A more sophisticated model
that would allow to better distinguish between unsatisﬁed
and partially satisﬁed needs would be to work at a ﬁner level
of granularity and verify that some of the clicks are “good
clicks” in the sense that the user spent enough time reading
the selected results and came back to the session to continue
exploring. We reserve this ﬁner analysis for future work and
limit ourselves ﬁrst to the simple observation of the number
of clicks.

Thus, we formally deﬁne a research session R as a maximal

order sequence

R = (cid:10) hqi1 , ui1 , ti1 , Ci1 i, . . . , hqik , uik , tik , Cik i (cid:11),

where, for given thresholds sθ, kθ and cθ, we have:
1. ui1 = · · · = uik = u ∈ U , and ti1 ≤ · · · ≤ tik ≤ τ ;
2. ∀l, j ∈ {i1, . . . , ik} s(f (ql), f (qj)) ≥ sθ;

3. |R| = k ≥ kθ;
4. Pk

j=1 |Cij | ≥ cθ.

The second condition reﬂects topical coherence, while the
third and fourth ones reﬂect user’s engagement, via the total
number of issued queries and the total number of clicked
results.

In the following section, we present our solution for au-
tomatically detecting research sessions and thus identifying
research missions.

4. AUTOMATIC IDENTIFICATION OF

RESEARCH MISSIONS

The Search Pad application previously introduced makes
research missions persistent by storing in a pad relevant in-
formation pertaining to a given research session. As such, it
is the ideal application for us to verify that research missions
can be eﬃciently and eﬀectively identiﬁed, either for direct
usage as described in Section 2 or future research.

Our contribution to Search Pad was its Triggering Sys-
tem, whose goal is precisely to trigger Search Pad whenever
a research mission is identiﬁed. The rest of this section de-
scribes the architecture and internals of this system.

4.1 Architecture of the Triggering System

As discussed in the previous session, Search Pad must be
triggered when the user is involved in a research mission,
which is a long, complex sequence of searches that are topi-
cally coherent. Therefore the Triggering System must focus
on signals such as session length, user’s engagement and top-
ical coherence. Moreover, some query topics are more likely
to be involved in research missions, such as, for instance,
travel, health, or job search.

Keeping in mind these consideration, we built the Trig-
gering System of Search Pad as a two-level classiﬁer, whose
architecture in depicted in Figure 4.

Figure 4: The core of the Triggering System.

The system is built around three base models, which are
each responsible for diﬀerent tasks, and a meta-classiﬁer,
called hereafter the “mixer”, that receives the signals arriving
from the base classiﬁers and combines them in order to make
the ﬁnal decision of whether to trigger Search Pad or not.

Given the scale of deployment of a system like Search Pad,
it was necessary to allow administrators to react fast and
tune, boost or disable certain features in case of poor un-
expected behavior. Consequently we opted for a two-tier
architecture that gives control to the Search Pad adminis-
trators without requiring deep understanding of the internal
operations.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA325More speciﬁcally, at the higher level, the details of the base
classiﬁers remain hidden in a black-box mode, and only the
semantics of the signal they produce need to be understood.
In addition, the output of the mixer is a probability value
that expresses a conﬁdence level. Search Pad is triggered if
this probability is higher than a given threshold.

Consequently, depending on on-going marketing or assess-
ment studies, the administrators can decide to favor preci-
sion over recall or conversely, without requiring any retrain-
ing or deep understanding of the base, simply by changing
this triggering threshold.

Finally, the mixer combines the signals of the various clas-
siﬁers via a simple interpretable formula. The inﬂuence of
each classiﬁer can thus be changed by the system adminis-
trator, and additional rules can be manually added so as to
change the triggering behavior. An example of the rules that
can be embedded in the decision mechanism are rules that
promote or demote certain topics based on the “Black Lists”
and the “Boost Lists”. These lists consist respectively of
topic categories for which Search Pad must not be triggered,
(because the topics might be oﬀensive to users for instance)
or for which Search Pad can trigger even with weaker signals
(because they are more likely to relate to research missions
such as health topics for instance).

While it is not recommended to abuse the system and
arbitrarily change the diﬀerent parameters and thresholds,
this ﬂexibility is necessary as mentioned before in order to
adapt quickly to market needs and changes.

4.2 The Components of the Triggering System

The Mixer. As discussed above, the mixer receives the
signals from the three base classiﬁers. In particular, for a
sequence of two consecutive queries q1, q2 the mixer receives
the following signals:

• research(q1, q2). This signal indicates whether the two
queries are part of a complex research mission or not.
The mixer also receives as auxiliary signal the conﬁ-
dence of such prediction.

• same mission(q1, q2). This signal indicates whether
the two queries are topically coherent and thus sus-
ceptible to be part of the same mission. Also in this
case, the mixer receives as auxiliary signal the conﬁ-
dence of the prediction.

• f (q1) and f (q2) provide the topics of q1 and q2, and
s(f (q1), f (q2)) estimates the similarity between these
two topics, as per the notation introduced in Section 3.

The users’ engagement in research missions is estimated
by the total number of queries issued in the session (among
other variables). For this system, we ﬁxed the number of
queries (k@) to 3, as a rough heuristic for having a long
enough session to do our analysis. Hence, the signals pro-
duced by the base classiﬁers for the last 3 queries are kept in
a “Session State” and used by the mixer to carry its decision.
Based on all these signals, a formula is learned by means
of logistic regression [31]. The formula returns a probability
p such that the higher p the more likely it is the current
session be a research session. The mixer produces its ﬁnal
recommendation, namely trigger/do not trigger, based on
the value of T , the triggering threshold, where T ∈ [0, 1], B
a boosting factor, whose value belongs to the interval [1, 5].

The decision is made according to the following set of

rules:

• if the topics of the research session belong to the pre-

viously mentioned topics black list, do not trigger.

• else if the topics of the research session belong to the
previously mentioned topics boost list and p ≥ T /B
(the larger the value of B, the easier the triggering for
those topics becomes), then trigger

• else if p ≥ T , then trigger.

We next describe the three base classiﬁers and their sig-

nals.
Research Detector and Mission Detector. The Re-
search Detector component is in charge of the classiﬁcation
task behind the signal research(q1, q2) described above. The
Mission Detector component is in charge of the classiﬁcation
task behind the signal same mission(q1, q2) also described
above. Both the Research Detector and the Mission De-
tector components are boosted decision trees classiﬁers [31],
trained on the same large set of data (approximately 40K
pairs of consecutive queries) that we gathered as follows.

We sampled several day-sessions from a subset of Yahoo!
Search users during a week toward the end of 2008. The
sampling was stratiﬁed over the days of the week, so as not
to over-represent any particular day of the week. The time
period was long enough to capture extended search patterns
for some users, exceeding typical 30-minute timeouts, and
allowing for missions to extend over multiple days.

A group of annotators were instructed to exhaustively ex-
amine each session and to annotate each pair of consecu-
tive queries q1, q2 and with the labels research(q1, q2) and
same mission(q1, q2). The annotators inspected the entire
search results page for each query, including URLs, page
titles, relevant snippets, and features such as spelling and
other query suggestions. They were also shown clicks to aid
them in their judgments.

Both classiﬁers used a set of 30 features that we computed
for each pair of consecutive queries q1, q2. Many of these fea-
tures were shown to be eﬀective for query segmentation [17,
18, 19] and can be summarized as follows:

• Textual features. We compute the textual similarity
of queries q1 and q2 using various similarity measures,
including cosine similarity, Jaccard coeﬃcient, and size
of intersection. Those measures are computed on sets
of stemmed words and on character-level 3-grams.

• Session features. We compute the number of clicks
and queries in the current session, the number of queries
since last click, number of clicks since last queries, etc.

• Time-related features. We compute the time dif-
ference between q1 and q2, the sum of reciprocals of
time diﬀerence for the pair q1, q2, total session time,
etc.

The prediction task of the Mission Detector is quite easy,
while the one of the Research Detector is hard.
Indeed,
the Mission Detector achieves a very high accuracy, approx-
imately close to the 95%, while the Research Detector ex-
hibits an accuracy around 75%. The most predictive fea-
tures for the mission boundaries detection are textual fea-
tures, among which size of the intersection on character-level

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA3263-grams and cosine similarity computed on sets of stemmed
words. Also temporal features play an important role: the
closer q1 and q2 are in time, the more likely is that they are
part of the same mission.

For the research detection, the most relevant features are
the session-based ones. In particular, the number of clicks
and number of queries since the beginning of the session.
Also the length of the queries is a predictive feature: intu-
itively, a longer query is likely to be a complex query.
Topic Classiﬁer. The last component, the Topic Classi-
ﬁer, is responsible for the signals f (q1) and f (q2). For this
purpose, we re-used an existing in-house Yahoo! tool as a
black box. As mentioned before, most Web search engines
have built a similar component for various usages. For each
query q it returns a topic category taken from taxonomy
of 1026 categories hierarchically organized in a tree with
maximum depth 7. Therefore, the similarity among topics,
s(f (q1), f (q2)), is deﬁned as a distance on the tree.
The boosting list described above was created observing the
most likely topics returned by this classiﬁer for queries be-
longing to research missions. We limited the topics in the
boosting list to the ﬁrst two levels of the category tree.

5. EVALUATION

For the training and the evaluation, we took advantage
of the editorial services internally available at Yahoo!, and
15 Yahoo! editors manually examined 7,303 users’ sessions
from a pool of 10,000, each session gathering all the queries
issued by a same user over a period of 3 days.

The editors were instructed to decorate each query with
one of the following labels (1) research, (2) maybe research
(3) not research (4) adult and (5) can’t tell. In the case of a
“research” label, since research sequences may not necessar-
ily be contiguous, the editors were asked to qualify the label
with a counter, namely “research 1”, “research 2”, etc.
in
order to make clear which queries belong to which research
session.

Finally the editors were given qualitative guidelines to
help them decide what a research mission is.
It was crit-
ical for us to verify that underlying research missions can
be qualitatively identiﬁed so as for us to devise techniques
to quantitatively identify the associated research sessions,
which suﬃciently good approximation. The guidelines were
mostly given “by example” to insist on the qualitative as-
pects. They included actual examples of research missions
and some of their representative queries, some less intuitive
than the usual academic, travel or health examples we pre-
viously gave, such as:

• shopping research, e.g., “I need to ﬁnd the best deal

on HDTVs”.

• political research e.g., “who should I vote for?” or “how
do I ﬁnally win that argument with my dad about
global warming?”.

• local research e.g., “I’m trying to choose a karate dojo
for my kids” or “what are the best sushi bars in town?”.

• how-to research e.g., “I’m learning how to play the
guitar” or “I’m collecting recipes for the big pie bake-
oﬀ next month”.

Additional research/not research instances of variations of
similar queries were also provided in order to help the editors

distinguish between them at a ﬁner grain. Such examples
included:

• Not research: a user looking for the correct spelling of
“iambic pentameter”, since a single fact is suﬃcient to
completely satisfy the need

• Research: a user collecting spelling information on a

variety of verse forms, e.g., “trochaic hexameter”.

• Not research: a user looking for the score of a particu-
lar football game, since this score is ﬁnal and will not
change

• Research: a user collecting the latest stats on the play-
ers in their fantasy football team, since these stats will
change, and the user will have to conduct similar re-
search again

In order to distinguish between these ﬁne cases, the editors
had to go beyond the isolated query expressions and do their
best at “guessing” the user’s actual needs.

This huge editorial eﬀort allowed us to verify that research
sessions were signiﬁcant enough in Web search traﬃc to jus-
tify special attention. The result of this study was that on
average 10% of sessions qualify as research sessions as an
underlying research mission could be identiﬁed for them by
our editors. We then computed the numbers of queries that
occurred during these research sessions as compared to the
overall number of queries in all sessions and found out that
were responsible for more than 25% of query volume. This
result alone was in our view suﬃcient to motivate further
study of research missions.

The ﬁrst type of experiments we had to conduct was to
verify that Search Pad would trigger without signiﬁcantly
aﬀecting latency at the Web scale traﬃc. We conducted in
March and April 2009, two “bucket experiments”, over 1%
and 3% of Yahoo! search traﬃc to this eﬀect. We veriﬁed in
both cases that the incurred average latency did not go over
12ms and was thus acceptable from a user’s perspective.

We also measured during these experiments that the av-
erage number of queries in a research session amounted to 5
queries.

We did not rely on the triggering prompt as validation
of search sessions as there is a discoverability issue with
such artifacts.
Indeed we noticed that the Click-Through
Rate (commonly referred to as CTR) remained constant in-
dependently of the coverage and this signaled to us that
more eﬀorts need to be invested to make the prompt more
discoverable. However, an encouraging ﬁgure is that we see
the number of users steadily increasing.

We also conducted a few live experiments in order to set
the a priori value of some of the thresholds we previously
introduced and that were added to make the system easily
tunable to market needs. We run the system oﬀ-line vary-
ing the value of the threshold T in [0.1, 0.2, ..., 0.8, 0.9]. For
each value of T we count the number of triggering events.
Following the standard deﬁnitions in Information Retrieval,
we then measured the precision and recall of the system (in
Figures 5 and 6).
In particular the precision is the num-
ber of triggering events correspondent to research sessions
divided by the total number of triggering events. The recall
is the number of triggering events correspondent to research
sessions divided by the total number of research sessions.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA327Traﬃc

US

1%
3%

April, 23th
Mar, 26th

T
0.6
0.5

Prompt Coverage

6%

11.5%

Table 1: Coverage for diﬀerent values of T

quire complex and lengthy interaction with search engines.
Research session are excerpts of query log sessions that ex-
hibit certain signals indicating that users are undertaking a
research mission.

It has been empirically veriﬁed through the manual anal-
ysis of more than 7000 users’ sessions, covering 3-day long
activities each, that about 10% of all users’ sessions are asso-
ciated with an underlying research mission, and even more
interestingly, that they were responsible for more than 25%
of the query volume.

A distinctive characteristic of such research missions is
that they are complex and that in many cases, users will
return to them over a period of time collecting information
that will help them research the topic at hand. The current
ephemeral nature of search sessions in Web search engines
does not provide any direct support for this type of activ-
ities. As this under-addressed need was identiﬁed, a novel
application was developed, Search Pad, that automatically
gathers notes about results visited during such sessions. One
unique feature of Search Pad is that it is visibly triggered
at run time when the search engine detects that the user is
undertaking a research mission. We explained in this paper
how we devised an approach for automatically detecting re-
search sessions, and how we embodied it in the triggering
component of the Search Pad application.

A few months after the initial launch of Search Pad, we
have now suﬃcient data to verify that detecting research
sessions at run time at the scale of Web traﬃc is feasible
and that the quality of our triggering is suﬃcient for users
to continue using and saving Search Pad documents.

We believe that identifying research sessions for the sim-
ple purpose of making them persistent is only a very ﬁrst,
yet critical step, in the exploitation of research missions.
We plan in the future to continue the studies on research
missions in several directions.

First, we would like to continue improving Search Pad in
various manners. We would like to reﬁne our understanding
of research missions, and increase our recall and precision
measures, by ﬁner analysis of users’ activities. An imme-
diate improvement, which was mentioned earlier, would be
not to measure user’s engagement by the simple number of
results visited for each query, but by a compound score that
would also take into account the time spent by users on these
results (too short a time would indicate irrelevance, too long
a time that would indicate abandonment of the task) in or-
der to signal that the result contain relevant information (in
an implicit relevant feedback spirit).

A better analysis of Search Pad session data would also
help us understand the more common usage patterns and
thus allow us not only to improve its user experience, but
automate the tuning of certain parameters such as the boost-
ing and triggering thresholds, as well as the content of the
Boost and Black Lists.

In addition, we would like to study the exploitation of re-
search missions in other contexts and other search activities.

Figure 5: Precision in function of threshold T

Figure 6: Recall in function of threshold T

These plots allowed us change the threshold in function
of the desired recall and precision ratios in given markets.
More responsive and early-adopter markets, such as Taiwan
for instance, will tolerate higher recall at the cost of lower
precision. In Table 1, we report the coverage, deﬁned as the
total number of prompts over the total number of searches,
for two diﬀerent values of the threshold T .

We measured the coverage over two diﬀerent datasets col-
lected by sampling 3% and 1% of the Yahoo! search traﬃc
during the same March, and April 2009 bucket experiments.
In the ﬁrst case, we set T = 0.5 and observed a coverage
of 11.5%, while in the second case, we set T = 0.6 with a
corresponding coverage of 6%. A threshold set at 0.6% thus
represents the most conservative choice: it guarantees a rea-
sonable precision but a very low recall, given our ground
truth estimation that 10% of sessions are research sessions.
Coverage is obviously increased when using a boosting list
that favors speciﬁc topics.
In-house user behavior studies
identiﬁed 511 categories, i.e., at level 2 in the hierarchical
tree classiﬁcation generated by the topical classiﬁer mod-
ule. These sub-categories belong to the following 11 main
classes: Automotive, Consumer Packaged Goods, Finance,
Health Pharma, Issues and Causes, Life Stages, Miscella-
neous, Retail, Small Business and B2B, Sport, Technology
and Travel. To evaluate the increase in coverage due to the
boost list, we simulated oﬀ-line the behavior of the trigger-
ing component over 6, 418 consecutive query triples and we
counted how many of the triggered events are indeed deter-
mined by the boosting parameter B. Our simulation showed
that 393 prompt events over a total of 2, 764, that is, a rel-
ative increase of 14%, can be credited to the boosting list.

6. CONCLUSIONS

We have deﬁned in this paper the concepts of research
missions and associated research sessions. Research mis-
sions represent a certain type of information needs that re-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA328An extremely challenging yet promising one, would be the
improvement of search results for queries that are identiﬁed
as part of a search mission. These queries are typically “hard
queries” in the sense that the search engine can verify that
the user’s information needs were not immediately satisﬁed.
We could learn from common patterns in research sessions
over a large population of users. By verifying that a query
q4 has a very high probability to follow queries, q1, q2 and
q3, as part of a same research mission, we could for instance,
if given suﬃcient evidence, start bringing good results from
q4 already at the q3 stage even before the user had a chance
to issue q4. We believe that, in general, considering research
sessions as an entity, as opposed to an atomic query, could
provide tremendous value in many aspects of ranking.

7. ACKNOWLEDGMENTS

We would like to thank for their valuable suggestions and
help many colleagues at Yahoo!
(in alphabetical order):
Rob Aseron, Ricado Baeza-Yates, Carlos Castillo, Marcus
Chan, Vivian Li Dufour, Sarah Ellinger, Ashley Hall, Is-
abelle Peyrichoux, Flavian Vasile and Shen Hong Zhu.

In addition, we would like to acknowledge the tremendous
eﬀort done by the entire Search Pad team at Yahoo! – the
application would not have launched without their collective
eﬀort – and ﬁnally, our users, who by their clicks and queries
make our research possible.

8. REFERENCES
[1] D. Abrams, R. Baecker, and M. Chignell. Information

archiving with bookmarks: personal web space
construction and organization. In CHI ’98:
Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 41–48, New York,
NY, USA, 1998. ACM Press/Addison-Wesley
Publishing Co.

[2] Ask mystuﬀ.

http://about.ask.com/en/docs/mystuﬀ/tour.shtml.

[3] R. Baeza-Yates. Graphs from search engine queries. In
Theory and Practice of Computer Science (SOFSEM),
volume 4362 of LNCS, pages 1–8, Harrachov, Czech
Republic, January 2007. Springer.

[4] K. Bharat. Searchpad: Explicit capture of search

context to support web search. In WWW’00:
Proceedings of the 9th World Wide Web Conference.
ACM Press, 2000.

[5] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis,

and S. Vigna. The query-ﬂow graph: model and
applications. In CIKM’08: Proceeding of the
Information and Knowledge Management Conference,
pages 10 pp.+, October 2008.

[6] P. Boldi, F. Bonchi, C. Castillo, D. Donato, and

S. Vigna. Query suggestions using query-ﬂow graphs.
In WSCD ’09: Proceedings of the 2009 workshop on
Web Search Click Data, pages 56–63, New York, NY,
USA, 2009. ACM.

[7] P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. From

”dango” to ”japanese cakes”: Query reformulation
models and patterns. In Proceedings of the 2009
IEEE/WIC/ACM International Conference on Web
Intelligence, (WI 2009), pages 183–190.

[8] A. Z. Broder. A taxonomy of web search. SIGIR

Forum, 36(2):3–10, 2002.

[9] L. Catledge and J. Pitkow. Characterizing browsing

behaviors on the world wide web. Computer Networks
and ISDN Systems, 6(27), 1995.

[10] Diigo. http://www.diigo.com.
[11] Evernote. http://www.evernote.com.
[12] E. Gabrilovich, M. Fontoura, A. Joshi, V. Josifovski,

L. Riedel, and T. Zhang. Classifying search queries
using the web as a source of knowledge. ACM
Transactions on the Web, 3(2):1–28, 2009.

[13] E. Gabrilovich and S. Markovitch. Computing

semantic relatedness using wikipedia-based explicit
semantic analysis. In IJCAI’07: Proceedings of the
20th international joint conference on Artiﬁcal
intelligence, pages 1606–1611, San Francisco, CA,
USA, 2007. Morgan Kaufmann Publishers Inc.

[14] Google notebook. http://www.google.com/notebook.
[15] Google sidewiki. http://www.google.com/sidewiki.
[16] Google. Search features.

http://www.google.com/help/features.html.

[17] D. He and A. G¨oker. Detecting session boundaries

from web user logs. In Proceedings of the BCS-IRSG
22nd annual colloquium on information retrieval
research, pages 57–66, Cambridge, UK, 2000.

[18] D. He, A. G¨oker, and D. J. Harper. Combining

evidence for automatic web session identiﬁcation. Inf.
Process. Manage., 38(5):727–742, September 2002.

[19] R. Jones and K. L. Klinkner. Beyond the session
timeout: automatic hierarchical segmentation of
search topics in query logs. In CIKM ’08: Proceeding
of the 17th ACM conference on Information and
knowledge mining, pages 699–708, New York, NY,
USA, 2008. ACM.

[20] M. Kellar, C. Watters, and M. Shepherd. A ﬁeld study
characterizing web-based information-seeking tasks. J.
Am. Soc. Inf. Sci. Technol., 58(7):999–1018, May
2007.

[21] R. M. Keller, S. R. Wolfe, J. R. Chen, J. L.

Rabinowitz, and N. Mathe. A bookmarking service for
organizing and sharing urls. Comput. Netw. ISDN
Syst., 29(8-13):1103–1114, 1997. Also appeared in the
Proceedings of WWW’6, Santa-Clara, CA, USA.

[22] R. Krishnan. Google notebook blog.

http://googlenotebookblog.blogspot.com/2009/01/stopping-
development-on-google-notebook.html, Jan
2009.

[23] J. Luxenburger, S. Elbassuoni, and G. Weikum.

Matching task proﬁles and user needs in personalized
web search. In CIKM, 2008.

[24] Y. S. Maarek and I. Z. B. Shaul. Automatically

organizing bookmarks per contents. Comput. Netw.
ISDN Syst., 28(7-11):1321–1333, 1996.

[25] B. Piwowarski and H. Zaragoza. Predictive user click
models based on click-through history. In CIKM ’07:
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge
management, pages 175–182, New York, NY, USA,
2007. ACM.

[26] F. Radlinski and T. Joachims. Query chains: learning

to rank from implicit feedback. In KDD ’05:
Proceeding of the eleventh ACM SIGKDD
international conference on Knowledge discovery in

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA329data mining, pages 239–248, New York, NY, USA,
2005. ACM Press.

[27] M. Richardson. Learning about the world through

long-term query logs. ACM Trans. Web, 2(4), 2008.

[28] M. C. Schraefel, Y. Zhu, D. Modjeska, D. Wigdor, and

S. Zhao. Hunter gatherer: interaction support for the
creation and management of within-web-page
collections. In WWW ’02: Proceedings of the 11th
international conference on World Wide Web, pages
172–181, New York, NY, USA, 2002. ACM.
[29] C. Silverstein, H. Marais, M. Henzinger, and

M. Moricz. Analysis of a very large web search engine
query log. SIGIR Forum, 33(1):6–12, 1999.

[30] J. Teevan, E. Adar, R. Jones, and M. A. S. Potts.
Information re-retrieval: repeat queries in yahoo’s
logs. In SIGIR ’07: Proceedings of the 30th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 151–158,
New York, NY, USA, 2007. ACM.

[31] I. H. Witten and E. Frank. Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementations (The Morgan Kaufmann Series in
Data Management Systems). Morgan Kaufmann, 1st
edition, October 1999.

[32] Yahoo. Search features.

http://tools.search.yahoo.com/newsearch/resources.

[33] Zotero. http://www.zotero.org.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA330