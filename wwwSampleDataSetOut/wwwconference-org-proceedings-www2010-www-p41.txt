Matrix "Bit"loaded: A Scalable Lightweight Join Query

Processor for RDF Data

Medha Atre†, Vineet Chaoji‡, Mohammed J. Zaki†, and James A. Hendler†

†Dept. of Computer Science, Rensselaer Polytechnic Institute, Troy NY, USA

{atrem, zaki, hendler}@cs.rpi.edu
‡Yahoo! Labs, Bangalore, India

chaojv@yahoo-inc.com

ABSTRACT
The Semantic Web community, until now, has used tradi-
tional database systems for the storage and querying of RDF
data. The SPARQL query language also closely follows SQL
syntax. As a natural consequence, most of the SPARQL
query processing techniques are based on database query
processing and optimization techniques. For SPARQL join
query optimization, previous works like RDF-3X and Hexa-
store have proposed to use 6-way indexes on the RDF data.
Although these indexes speed up merge-joins by orders of
magnitude, for complex join queries generating large inter-
mediate join results, the scalability of the query processor
still remains a challenge.

In this paper, we introduce (i) BitMat – a compressed
bit-matrix structure for storing huge RDF graphs, and (ii) a
novel, light-weight SPARQL join query processing method
that employs an initial pruning technique, followed by a
variable-binding-matching algorithm on BitMats to produce
the ﬁnal results. Our query processing method does not
build intermediate join tables and works directly on the com-
pressed data. We have demonstrated our method against
RDF graphs of upto 1.33 billion triples – the largest among
results published until now (single-node, non-parallel sys-
tems), and have compared our method with the state-of-
the-art RDF stores – RDF-3X and MonetDB. Our results
show that the competing methods are most eﬀective with
highly selective queries. On the other hand, BitMat can
deliver 2-3 orders of magnitude better performance on com-
plex, low-selectivity queries over massive data.

Categories and Subject Descriptors
H.2.4 [Systems]: Query Processing

General Terms
Algorithms, Performance, Experimentation

1.

INTRODUCTION

Resource Description Framework (RDF)1, a W3C stan-
dard for representing any information, and SPARQL2, a
query language for RDF, are gaining importance as semantic

1

2

http://www.w3.org/TR/rdf-syntax-grammar/
http://www.w3.org/TR/rdf-sparql-query/

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

data is increasingly becoming available in the RDF format.
RDF data consists of triples where each triple presents a re-
lationship between its subject (S) and object (O), the name of
the relationship is given by the predicate (P), and the triple
is represented as (S P O). Such an RDF data can be repre-
sented as a labeled directed graph and it can be serialized
and stored in a relational database simply as a 3-column ta-
ble where each tuple in that table represents a triple in the
original RDF graph.

RDF is extensively being used for representing data from
the ﬁelds of bioinformatics, life sciences, social networks, and
Wikipedia as well. Since disk-space is getting cheaper, stor-
ing this huge RDF data does not pose as big a problem as ex-
ecuting queries on them. Querying these huge graphs needs
scanning the stored data and indexes created over it, reading
that data inside memory, executing query algorithms on it,
and building the ﬁnal results of the query. Hence, a desired
query processing algorithm is one which (i) keeps the under-
lying size of the data small (using compression techniques),
(ii) can work on the compressed data without uncompressing
it, and (iii) doesn’t build large intermediate results. A lot
of work has already gone in using compression techniques in
storing the data in a column-store database as well as trying
to work on the compressed data without uncompressing it
by lazy materialization [2, 4]. In this paper, we go one step
further and propose a compressed bitcube of RDF data and
a novel SPARQL join query processing approach which al-
ways works on the compressed data by producing the ﬁnal
results in a streaming fashion without building intermediate
join tables.

A SPARQL join query, which can also be viewed as a Ba-
sic Graph Pattern Matching (BGP) query, or a conjunctive
triple pattern query, resembles closely to an SQL join query
(in fact any SPARQL join query can be systematically trans-
lated into an SQL join query [9]). A typical SPARQL join
query looks like the one shown in Figure 1. This query shows
a join between three triple patterns.

Such join queries can be broadly classiﬁed into three cat-
egories. The ﬁrst type is – queries having highly selec-
tive triple patterns3. E.g., consider the query (?s :residesIn
USA)(?s :hasSSN “123-45-6789”). Since SSN is a unique
attribute of a person, the second triple pattern has only
one triple associated with it thereby being highly selective.
The second type is – queries having triple patterns with
low-selectivity but which generate few results, i.e., highly
selective join results. E.g., consider a multi-national orga-

3

Selectivity of a triple pattern is low if there are more number of

triples associated with it and vice versa.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA41SPARQL join query

SELECT *
WHERE {

?m rdf:type :movie .
?n rdf_type :movie .
?m :similar_to ?n

}

Equivalent SQL join query

Note: RDF graph stored as tripletable

SELECT * FROM
tripletable AS A, tripletable AS B, tripletable AS C
WHERE A.subject = B.subject
AND A.predicate = ":similar_to"
AND A.object = ":movie"

AND B.object = ":movie"

AND A.object = C.subject
AND B.predicate = "rdf:type"

AND C.predicate = "rdf:type";

Figure 1: An example of SPARQL join query

nization BigOrg, having employees on the order of few mil-
lions all over the world. Now consider a densely populated
country like India having population close to 1.2 billion and
consider a query like (?s :residesIn India)(?s :worksFor Big-
Org). Although the number of triples associated with the
two triple patterns is quite high, their join will produce much
fewer number of results as there are only a few employees
of BigOrg in India. The third type of queries are the ones
having low-selectivity triple patterns and low-selectivity join
results, i.e., the ones generating a lot of results. For instance,
a modiﬁcation of the ﬁrst query given above, to ﬁnd SSNs
of all the people (?s :residesIn USA)(?s :hasSSN ?y).

Most of the systems which generate various indexes on
the data do well on the ﬁrst type of queries – highly selec-
tive triple patterns. Especially having 6-way indexes helps
in picking the right set of triples at the beginning, avoid-
ing scanning a large amount of data. For the second type
of queries – low-selectivity triple patterns, but highly selec-
tive join results – systems using join selectivity estimation
or pre-computed join tables/indexes get beneﬁted to a cer-
tain extent. Although as shown in our experiments, join
selectivity estimation does not always help in improving the
query performance in case of complex joins involving low-
selectivity intermediate join results. For the third type of
queries – low-selectivity triple patterns generating a large
number of results – even the state-of-the-art systems run
into problems (as shown by our evaluation).

Although disk-space is growing at a much faster speed, the
available main-memory still remains very small compared to
it. This creates the main bottleneck while executing queries
of the second and third type mentioned above. Hence our
goal is to build a scalable query algorithm which operates
on the compressed data. Second our goal is to not generate
intermediate join tables, thereby keeping the memory foot-
print of the system small (hence light-weight), and cope well
with the second and third type of queries mentioned above.
Our key contributions in this work are:

1. A compressed data structure for RDF data – called
BitMat – to increase the size of the data that can ﬁt
in memory.

2. A novel algorithm that performs eﬃcient pruning of
the triples during the ﬁrst phase of SPARQL join query
execution and in the second phase, performs variable
binding matching across the triple patterns to obtain
the ﬁnal results (both phases use compressed BitMats
without any join table construction).

3. Procedures and algorithms implemented to work on

the compressed data directly.

4. Experiments on very large RDF graphs (∼845 million
and ∼1.33 billion) using a set of queries published on

the web by the owners of the datasets, showing a com-
parison with the state-of-the-art RDF storage systems
– RDF-3X and MonetDB. Our results indicate that
competing methods are much better on high-selectivity
queries, whereas our method outperforms them by 2-
3 orders of magnitude on complex queries with low-
selectivity intermediate join results.

Work presented in this paper is a considerable extension of
our preliminary work outlined previously in [5].

2. RELATED WORK

RDF data can be serialized and stored in a database and a
SPARQL join query can be executed as an SQL join, hence
recently a lot of database join query optimization techniques
have been applied to improve the performance of SPARQL
join queries. Notably, in the past couple of years, C-Store
[3], RDF-3X [16], MonetDB [21], and Hexastore [24] systems
have proposed ways of optimizing SPARQL join queries.

Out of these systems, C-Store and MonetDB exploit the
fact that typically RDF data has much less number of prop-
erties (predicates), thereby vertically partitioning the data
for each unique predicate and sorting each partition (predi-
cate table) on subject, object order (creating a subject-object
index on each property table). Hexastore and RDF-3X make
use of the fact that an RDF triple is a ﬁxed 3-dimensional
entity and hence they create all 6-way indexes (SPO, SOP,
PSO, POS, OPS, OSP). Although Hexastore does share
common indexes within these 6 indexes, e.g., SPO and PSO
share the “O” index, without any compression, it suﬀers from
5-fold increase in the space required to store these indexes.
RDF-3X goes one step further and compresses these indexes
as described in their paper [15]. RDF-3X also implements
several other join optimization techniques like RDF spe-
ciﬁc Sideways-Information-Passing, selectivity estimation,
merge-joins, and using bloom-ﬁlters for hash joins.

Along with these systems, there are other systems being
developed for RDF data storage and querying, such as, Jena-
TDB [1] and Virtuoso [10]. Jena-TDB faces scalability issues
while executing queries on very large datasets. Along with
these, BRAHMS [11] and GRIN [22] focus more on path-like
queries on RDF data, typically which cannot be expressed
using existing SPARQL syntax.

Most systems built to store and query RDF data typically
use a left-deep join tree which requires materialization of the
intermediate join results in case of a complex join query in-
volving several join variables. Merge-joins cannot always be
used while performing later joins, especially when the join
column of an intermediate result is not sorted. In contrast
to these, in our system instead of using sophisticated join
optimization techniques, we have followed a simple rule of
keeping the data compressed without materializing the in-
termediate join results. This helps to keep a large amount of
required data in memory. We execute the join by following
a novel algorithm, which propagates the constraints on the
join-variable bindings among diﬀerent join variables in the
query. Our technique is reminiscent of the concept of semi-
joins [7, 6] as discussed further in Section 4.1. We consider
our query processing engine light-weight – light-weight on
runtime memory consumption as well as optimization tech-
niques. We have shown results by analyzing where our sys-
tem outperforms the state-of-the-art systems like RDF-3X
and MonetDB.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA42Subject

:the_matrix
:the_thirteenth_floor
:the_matrix
:the_thirteenth_floor
:the_matrix
:the_thirteenth_floor

Predicate
:releasedIn
:releasedIn
:similar_to
:similar_to
rdf:type
rdf:type

Object

"1999"
"1999"
:the_matrix_reloaded
:the_matrix
:movie
:movie

S−dimension

0
1

0

0
c

0

0

0

0
d

0
0
a

1
1
b

P−dimension

1
1

0
0

3

2

0
0

0
0

0

0

1

0

1

O−dimension

S−O and O−S BitMats for Ps

Note: a = :the_matrix, b = "1999", c = :movie, d = :the_matrix_reloaded

:releasedIn
c     d

b

a

:the_matrix

0     1     0    0

:the_thirteenth_floor

0     1     0    0

1
S−O

O−S

Transpose

0     0

1     1

0     0

0     0

:similar_to
c     d

b

a

0     0     0    1

1     0     0    0

2
S−O

0     1

0     0

0     0

1     0

O−S

rdf:type

b

c     d

a

0     0     1    0

0     0     1    0

3
S−O

0     0

0     0

1     1

0     0

O−S

Figure 2: Example of S-O, O-S BitMat construction procedure for each P

3. BITMAT CONSTRUCTION

Figure 2 shows sample RDF data and gives a pictorial
representation of constructing a BitMat. Let Vs, Vp and
Vo denote the sets of distinct subjects, predicates, and ob-
jects, respectively, in the RDF data. This RDF data can be
represented by a 3D bit-cube, where each dimension of the
bitcube represents subjects (S), predicates (P), and objects
(O). The volume of this bitcube is Vs × Vp × Vo. Each cell
in the bitcube represents a unique RDF triple that can be
formed by the combination of S, P, O positions which are
the coordinates of that bit. A bit set to 1 denotes presence
of that triple in the given RDF data.

This 3D bit-cube is sliced along a dimension to get 2D ma-
trices. Figure 2 shows slicing along the P-dimension, which
gives S-O bit-matrices (BitMats). Inverting an S-O BitMat
gives an O-S BitMat. We store these S-O and O-S BitMats
for each P value. In all we get |Vp| such S-O and O-S matri-
ces. Additionally, we slice the bitcube along S and O dimen-
sions which gives P-O and P-S BitMats respectively. Note
that we do not store inverted O-P and S-P BitMats, since
based on our experience, usage of those BitMats is rare; and
even if needed their construction from the corresponding P-
O or P-S BitMat is easier due to the relatively few number
of predicates in typical RDF data. In all we have |Vs| P-O
BitMats and |Vo| P-S BitMats. To summarize, for each P
value we have a S-O and an O-S BitMat, for each S value –
a P-O BitMat, and for each O value – a P-S BitMat (in all
2|Vp| + |Vs| + |Vo| such BitMats).
There are total |Vs| × |Vp| × |Vo| possible triples with the
given Vs, Vp, and Vo sets. But it is observed that typically
RDF data contains much fewer number of triples, hence the
S-O, O-S, P-S, P-O BitMats are very sparse. We make use
of this fact by applying gap compression on each bit-row of
these four types of BitMats. In gap compression scheme, a
bit-row of “0011000” will be represented as “[0] 2 2 3”. That
is, starting with the ﬁrst bit value, we record alternating run
lengths of 0s and 1s.

We also store the number of triples in each compressed
BitMat (this statistics is useful while executing our query
algorithm as described later). Along with this, we store two
bitarrays – row and column bitarray – which give a con-
densed representation of all the non-empty row and column
values in the given BitMat. For example, in Figure 2, for the
S-O BitMat of “:similar to” predicate (marked by BitMat “2”
in Figure 2), we store row bitarray “1 1” and a column bitar-
ray “1 0 0 1”, and for the O-S BitMat we store row bitarray
“1 0 0 1”, and column bitarray “1 1” respectively. These
bitarrays are useful while performing “star join” queries (as
elaborated in the Evaluation section). We store the com-
pressed S-O, O-S, P-S, P-O BitMats in one ﬁle on the disk
and maintain a meta-ﬁle which gives the oﬀset of each Bit-
Mat inside the BitMat ﬁle. Due to this, addition or deletion

of the triples might require moving a large amount of data,
but if bulk updates are expected on the RDF data, all the
BitMats can be rebuilt at once since the BitMat construc-
tion time even for very large data is very small (as shown at
the end of the Evaluation section).

The above construction reveals that each unique S, P, and
O is mapped to a unique position along each dimension and
this position can be represented as an integer ID. We decide
this mapping with the following procedure: Let Vso repre-
sent the Vs ∩ Vo set. Each element in Vso, along with the
elements in Vs, Vp and Vo, is assigned an integer ID as fol-
lows:

• Common subjects and objects: Set Vso is mapped to a

sequence of integers: 1 to |Vso|.

• Subjects: Set Vs − Vso is mapped to a sequence of in-

tegers: |Vso| + 1 to |Vs|.

• Predicates: Set Vp is mapped to a sequence of integers:

1 to |Vp|.

• Objects: Set Vo − Vso is mapped to a sequence of inte-

gers: |Vso| + 1 to |Vo|.

The common subject-object identiﬁer assignment facilitates
the bitwise operations in join queries wherein an S position
in one triple pattern is joined over an O position in another
triple pattern (e.g. ?n in the query in Figure 1). For the
present considerations, we do not handle joins across S-P
and P-O dimensions. Such queries are rare in the context
of assertional RDF data. None of the benchmark queries
published for the large RDF datasets have queries having
joins over S-P or P-O dimensions. Hence overlapping S,
P, O IDs except for the common S and Os do not pose a
problem while processing a query.

With respect to the construction described above, the RD-
FCube [14] system is conceptually closest to BitMat. RD-
FCube also builds a 3D cube of S, P, and O dimensions.
However, RDFCube’s design approximates the mapping of
a triple to a cell by treating each cell as a hash bucket con-
taining multiple triples. They primarily used this as a dis-
tributed structure in a peer-to-peer setup (RDFPeers [8])
to reduce the network traﬃc for processing join queries in
a conventional manner.
In contrast, BitMat’s compressed
structure maintains unique mapping of a triple to a sin-
gle bit, and also employs a diﬀerent query processing algo-
rithm. Further, RDFCube has demonstrated their results
on a bitcube of only up to 100,000 triples, whereas we have
used more than 1.33 billion triples in this paper.
3.1 BitMat Operations

In this section we deﬁne two basic operations fold and
unfold which are used by our join query algorithm. Fold

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA43and unfold operate on the S-O, O-S, P-O, P-S compressed
BitMats constructed while storing the original RDF data.

(1) Fold: fold operation represented as ‘fold(BitMat, Re-
tainDimension) returns bitArray’ folds the input BitMat by
retaining the RetainDimension. For example, if in an S-O
BitMat of a given predicate P, RetainDimension is set to
‘columns’, then BitMat is folded along the subject “rows”
resulting into a single bitarray, i.e., all the subject bit-rows
are ORed together to give an “object” bit-array. Intuitively,
a bit set to 1 in this array indicates the presence of at least
one triple with the “object” corresponding to that position
in the given S-O BitMat. Without loss of generalization,
this procedure can be applied to any of the S-O, O-S, P-O,
P-S BitMats with “rows” or “columns” as RetainDimension.
(2) Unfold: Speciﬁed as ‘unfold(BitMat, MaskBitArray,
RetainDimension)’, unfolds the MaskBitArray on the Bit-
Mat. Intuitively, in the unfold operation, for every bit set to
0 in the MaskBitArray all the bits corresponding to that po-
sition of the RetainDimension in the BitMat are cleared. For
example, unfold(BitMat, ‘011000’, ‘columns’) would result
in a bitwise AND of “[0] 1 2 3” (gap compressed representa-
tion of ‘011000’) and each row of the BitMat.

Note that fold and unfold operations are implemented to
operate directly on a compressed BitMat. For example, a
bitwise AND of compressed arrays – arr1 as [0] 2 3 4 and
arr2 as [1] 3 4 2 – can be performed by sequentially looking
at their “gap values”. E.g., AND the ﬁrst gap of arr1 – 2 0s
and arr2 – 3 1s, which gives the ﬁrst gap of 2 0s in the result.
Since the two gaps were of uneven length, there is a leftover
1 from the ﬁrst gap of arr2. Now AND the second gap of
arr1 – 3 1s, and leftover ﬁrst gap of 1 1s from arr2, which
gives second gap in the result – 1 1s, so on and so forth.
Bitwise OR on the compressed bitarrays can be done with
AND using simple Boolean logic (a OR b) = NOT(NOT(a)
AND NOT(b)). A bitwise NOT operation on a compressed
bitarray is simply – NOT([0] 2 3 4) = [1] 2 3 4.

4.

JOIN PROCESSING ALGORITHM

Before describing our join processing algorithm, we would

like to note some facts about the join process.

Property 1. Each triple pattern in a given join query
has a set of RDF triples associated with it which satisfy that
triple pattern. These triples generate bindings for the vari-
ables in that triple pattern.
If the triples associated with
another triple pattern containing the same variable cannot
generate a particular binding, then that binding should be
dropped.
In that case, all the triples having that binding
value should be dropped from the triple patterns which con-
tain that variable.

Property 2. If two join variables in a given query ap-
pear in the same triple pattern, then any change in the bind-
ings of one join variable can change the bindings of the other
join variable as well.

Property 3. A join between two or more triple patterns
over a join variable indicates an intersection between bind-
ings of that join variable generated by the triples associated
with the respective triple patterns.

To elaborate the use of these properties, consider the query
given in Figure 1. ?m and ?n appear in the same triple pat-
tern (?m :similar to ?n). A position marked with “?” in the

?m

G

jvar

?n

?m rdf:type :movie

?m :similar_to ?n

?n rdf:type :movie

SS

BitMat

1

BitMat

2

SO

G

tp

BitMat

3

Figure 3: Graph G for the query in Figure 1

triple pattern is variable. If we perform a join of (?m :simi-
lar to ?n) (?m rdf:type :movie) ﬁrst, we get two bindings for
the variable ?m viz.
:the matrix and :the thirteenth- ﬂoor
and two for ?n :the matrix reloaded and :the matrix corre-
sponding to ?m’s bindings. When we do the join between
(?n rdf:type :movie)(?m :similar to ?n), we consider bind-
ings generated for ?m and ?n after the ﬁrst join. After the
join on ?n, binding :the matrix reloaded for ?n gets dropped,
hence the triple (:the matrix :similar to :the matrix reloaded)
gets dropped from the triples associated with (?m :similar to
?n) which in turn drops the the binding :the matrix for ?m.
Properties 1, 2 and 3 together establish the basis of our
pruning algorithm. We propagate the constraints on the
bindings of each join variable in a given triple pattern to all
other triple patterns and do aggressive pruning of the RDF
triples associated with them.
4.1 Step 1 – Pruning the RDF Triples

First we construct a constraint graph4 G out of a given

join query. The constraint graph is built as follows:

1. Each triple pattern in the join query is denoted by a
tp-node in G. Hence forth we use the terms “tp-node”
and “triple pattern” interchangeably. A jvar-node in
G corresponds to a join variable in the query. Hence
forth we use terms “jvar-node” and “join variable” in-
terchangeably.

2. An undirected, unlabeled edge between a jvar-node
and a tp-node exists in G if that join variable appears
in the triple pattern represented by the tp-node. This
edge represents the dependency between triples asso-
ciated with the tp-node and the join variable bindings
(ref. Property 1).

3. An edge exists between two jvar-nodes if the two join
variables appear in the same triple pattern. This undi-
rected, unlabeled edge represents the dependency be-
tween their bindings (ref. Property 2).

4. An edge between two tp-nodes exists if they share a
join variable between them. This is an undirected,
labeled edge with potentially multiple labels. Multiple
labels can appear if the two triple patterns share more
than one join variables. The labels denote the type
of join between the two triple patterns – SS denotes
subject-subject join, SO denotes subject-object join
etc.

For a query having no Cartesian joins 5, constraint graph

4

This graph is reminiscent of similar terminology used in the

constraint satisfaction literature.
5

A Cartesian join is where there is no shared variable in a set of
triple patterns, and hence the result of the query is a full Cartesian
product of all triples associated with each triple pattern.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA44G is always connected. Figure 3 shows the constraint graph
for the join query given in Figure 1.

Before starting the pruning algorithm, we initialize each
tp-node by loading the triples which match that triple pat-
tern.
In Section 3 we elaborated the construction of four
types of BitMats viz. S-O and O-S for each P value, P-S for
each O value and P-O for each S value. Assuming that a
given query does not have any triple pattern with all variable
positions (e.g. ?x ?y ?z) we initialize the BitMats associated
with each triple pattern using the four types of stored Bit-
Mats (case of all-variable triple pattern is discussed at the
end of this section). E.g., if the triple pattern in the query is
of type (?s :p2 :o321) then we load only one row correspond-
ing to “:p2” from the P-S BitMat created for “:o321”. If the
triple pattern is of type (?s :p6 ?o) then we load either the
S-O or O-S BitMat created for “:p6”. If ?s is a join variable
and ?o is not, we load S-O BitMat and vice versa. If both,
?s and ?o, are join variables, then the decision depends on
whether ?s will be processed before ?o. If a join over ?s is
processed before ?o, we load S-O BitMat and vice versa.

If we have a triple pattern of type (:s2 ?p :o6), then ﬁrst
we decide whether P-S BitMat for “:o6” has less number of
triples or P-O BitMat for “:s2” has less number of triples.
If P-S BitMat has less number of triples, then we load the
P-S BitMat by keeping only the bit corresponding to “:s2”
in each row and mask out all other bits. Note that all these
operations are done directly on the compressed BitMats.
Thus at the end of initialization, each tp-node has a BitMat
associated with it which contains only the triples matching
that triple pattern. For example, BitM at1 associated with
(?m rdf:type :movie) has just a single row corresponding to
“rdf:type” loaded from the P-S BitMat created for the object
value “:movie”.
Now we start the pruning algorithm. First, we consider an
induced subgraph Gjvar of G containing only jvar-nodes. By
the construction of graph G, Gjvar is also always connected
(see Figure 3). Gjvar can be cyclic or acyclic. Next, we
embed a tree on Gjvar discarding any cyclic edges. To prop-
agate the constraints on join variable bindings (Property 2),
we walk over this tree from root to the leaves and backwards
in breadth-ﬁrst-search manner. At every jvar-node, we take
intersection of bindings generated by its adjacent tp-nodes
and after the intersection, drop the triples from tp-node Bit-
Mats as a result of the dropped bindings.
It can be seen that by the construction of graph G and fol-
lowing the tree over Gjvar, constraints on the join variable
bindings get propagated to other jvar-nodes through the tp-
node BitMats (when the triples get dropped), and this prop-
agation follows an alternating path between jvar-nodes and
tp-nodes. This procedure is elaborated in Algorithm(1). A
topological sort of an undirected tree is nothing but enumer-
ating all the nodes from root to leaves in a breadth-ﬁrst-
search fashion.

For each node in the topological sorted list of join vari-
ables, we call prune for jvar (Lines 2 – 4 in Algorithm(1)).
A topological sort ensures that a child jvar node always gets
processed after all of its ancestors. The bitwise AND be-
tween folded bitarrays in prune for jvar(J) computes the
intersection of all the bindings generated by the tp-nodes
which contain J (Lines 2 – 5 in Algorithm(2)). According
to Property 1, for any binding dropped in the intersection,
the respective triples are removed from the BitMats asso-
ciated with the tp-nodes which contain J using the unfold

operation (Lines 6 – 9 in Algorithm(2)). getDimension re-
turns the position of J in the BitMat of the triple pattern.
For instance, getDimension(?n, (?m :similar to ?n)) can re-
turn column or row depending on whether it is an S-O or
O-S BitMat.

Algorithm 1 Pruning Step
1: queue q = topological sort(V (Gjvar ))
2: for each J in q do
3:
prune for jvar(J)
4: end for
5: queue q rev = q.reverse() - leaves(Gjvar )
6: for each K in q rev do
7:
8: end for

prune for jvar(K)

dim = getDimension(J, T )

Algorithm 2 prune for jvar(jvar-node J)
1: MaskBitArrJ = a bit-array containing all 1 bits.
2: for each tp-node T adjacent to J do
3:
4: MaskBitArrJ = MaskBitArrJ AND fold(BitMatT , dim)
5: end for
6: for each tp-node T adjacent to J do
7:
8:
9: end for

dim = getDimension(J, T )
unfold(BitMatT , MaskBitArrJ , dim)

One such pass over all the jvar-nodes ensures that the
constraints are propagated to the adjacent jvar-nodes from
root to leaves of the tree. For a complete propagation of con-
straints, we traverse jvar-nodes second time by following the
reverse order of the ﬁrst pass (Lines 5 – 8 in Algorithm(1)).
The leaves of the tree embedded on Gjvar appear last in
queue q. Since they are processed last in the ﬁrst traversal
over the tree, in the second traversal, we directly start with
the parent nodes of these leaves (Line 5 in Algorithm(1)).
Notably, since we take intersection of the bindings in each
pass, the number of triples in the tp-node’s BitMat decrease
monotonically as the constraints are propagated.

At the end of Algorithm(1), each tp-node contains a much
reduced set of triples adhering to the constraints on join
variable bindings. Typically, when Gjvar is acyclic, this set
of triples is also minimal, i.e., each triple in the BitMat of
a triple pattern is necessary to generate one or more ﬁnal
results. If Gjvar is cyclic, then the set of triples is not guar-
antied to be minimal. But in any case, the unwanted set of
triples get dropped in the following phase of ﬁnal result set
generation.

Our pruning method closely resembles the idea of semi-
joins [7, 6]. Semi-joins also build a query graph (QG) where
the nodes of the graph are relations (tables) and an edge
between the two nodes indicate a join between the two re-
lations. A semi-join QG for a SPARQL join query can be
reduced to Gjvar in BitMat’s constraint graph by following
simple transformation: each edge in the QG is a node with
the join-variable name in Gjvar, two nodes in Gjvar have an
edge between them if the corresponding edges in the QG
are incident on the same node in the QG. If a QG is proper
cyclic [6], Gjvar is cyclic and if QG is a tree query then Gjvar
is acyclic. Bernstein et al. have proved in [7, 6] that for the
tree queries, semi-joins can fully reduce the database for a
given query, i.e., at the end of a semi-join the database has
minimal tuples, whereas cyclic queries cannot be guarantied
to have full reducers. A formal proof of minimal triple set
generation in case of an acyclic Gjvar in our method is rem-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA45iniscent of this proof. It is not included in this paper due to
space constraints.

Pn

In our current implementation, we load the BitMat as-
sociated with each triple pattern at the beginning of query
processing and then never seek a disk access in the entire
lifespan of the query. This necessitates that for a query hav-
ing n triple patterns it needs
i=0 size(BitM ati) amount of
memory at the beginning. This poses limitations for queries
having triple patterns with all variable positions (?x ?y ?z),
as it is not feasible to load a BitMat for the all-variable tp-
node containing the entire dataset in memory. Also, due to
this condition, it can happen that for certain queries with
highly selective triple patterns, the memory requirements
of the conventional query processors are lesser than BitMat
as they do not need to load entire indexes in memory to
perform joins (e.g. RDF-3X). Notably, BitMat’s memory
requirement remains linear in terms of the triples associ-
ated with the triple patterns in the query, whereas for con-
ventional query processors it can degrade polynomially for
low-selectivity multi-join queries.
Simple optimizations
While performing the pruning step as elaborated before, we
use some simple statistical optimization techniques.

Tree root selection: After initialization, in a join query
with n triple patterns, we sort all the triple patterns ﬁrst
in the order of increasing number of triples associated with
them.
If the ﬁrst triple pattern in this list has only one
join variable, we pick this join variable as the root of the
tree embedded on the graph Gjvar as described before. If it
has more than one join variables, then we scan through the
sorted list of triple patterns and ﬁnd another triple pattern
such that it shares a join variable with the ﬁrst triple pattern
(since constraint graph G is always connected for the queries
without Cartesian joins, we are sure to ﬁnd such a triple
pattern). We then assign this shared join variable as the
root of the tree embedded on Gjvar. This method is similar
to choosing tables with least number of triples to be joined
ﬁrst in the SQL joins.

Early stopping condition: While performing the pruning
at each jvar-node, at any point if the M askBitArrJ contains
all 0 bits, that is a direct evidence of the query generating
empty set of results. If such a condition occurs we exit the
query processing at that point telling that the query has 0
results. This avoids unnecessary further processing of other
join variables and fold/unfold operations over BitMats.

4.2 Step 2 – Generating Final Results

After the pruning phase, we are left with a much reduced
set of triples associated with each triple pattern. Intuitively,
each BitMat of a triple pattern can be viewed as a com-
pressed table in a relational database. Hence, one way of
producing the results could have been to simply materialize
these BitMats into tables and perform standard joins over
them. But our goal is to avoid building intermediate join
results by building a left-deep join tree; which precludes a
2-way sequence of joins as done in a typical SQL query pro-
cessor. In our method, we build and output an entire result-
ing row of variable bindings, which is similar to multi-way
joins.

Notably for this process, we use at most k size additional
memory buﬀer, where k is the number of variables in the
query (and hence the additional buﬀer size is negligible). We

keep a map of bindings for all k variables at a time, output
one result when all k variables are mapped, and proceed
to generate the next result (hence we call these “streaming
results”).

Let us assume that a query has n triple patterns and N
is the maximum number of triples in any of the n BitMats
associated with the triple patterns. For simplicity, we de-
note BitM ati as the BitMat associated with the ith tp-node
(tpnodei).

A simple brute force approach can be as follows: Choose
say BitM at1, pick a triple from it. This triple will generate
bindings for the variables in tpnode1. Store these bindings in
the map. Next pick the ﬁrst triple from BitM at2 and gen-
erate bindings for the variables in tpnode2. If tpnode2 and
tpnode1 share one or more join variables, check the map
if the variable bindings generated by both of them are the
same, if not, pick a second triple from BitM at2. Repeat
this procedure until you get the variable bindings consistent
with the ones stored in the map. Then consider BitM at3
and repeat the same procedure as described above. Repeat
this procedure until the last BitM atn in the query.
If a
triple in BitM atn generates valid variable bindings, such
that all k variables are mapped to the bindings, output one
result. Now start with BitM at1 again and choose the second
triple, store the bindings for the variables in tpnode1, and re-
peat the same process. In general, while generating variable
bindings from any BitM ati, check all the variable bindings
stored in the map. We can quickly see that the worst case
complexity of such a brute force approach is O(N n).

Since BitMat is a fully inverted index structure, in prac-
tice we devise following method which speeds up the above
procedure by several orders of magnitude: In general a Bit-
Mat having lesser number of triples generates lesser number
of unique bindings for the variables in its tp-node. This
means that in the ﬁnal results of the query, these bindings
will get repeated more often in diﬀerent result rows than
other bindings (just like the product of two columns where
ﬁrst column has lesser number of rows than the other – val-
ues from the ﬁrst column get repeated more often in the
product). Making use of this fact, we choose a BitMat as
BitM at1, which has the least number of triples, to be pro-
cessed ﬁrst (similar to the way of choosing the table having
least number of triples to join ﬁrst), generate bindings for
the variables in tpnode1, and store them in the map. Next
instead of picking BitM at2 randomly, we pick a tpnode2
which shares a join variable with tpnode1. Depending on
the variable bindings stored in the map, we directly locate
the triples which can satisfy these bindings inside BitM at2.
Recall that BitMat being a completely inverted index struc-
ture, it is easy to locate speciﬁc triples.
If no such triple
exists in BitM at2, we discard the variable bindings in the
map, go back to BitM at1, and pick the second triple from it
to generate new bindings (this can happen only in case of a
cyclic Gjvar). If BitM at2 generates variable bindings consis-
tent with BitM at1, pick tpnode3 which shares join variables
either with tpnode1 or tpnode2. Considering the constraint
graph given in Figure 3, let Gtp be an induced subgraph of
G having only tp-nodes and edges between them. We make
use of Gtp to make the choice of the next tp-node at every
step. Hence it can be seen that after one walk over all the
tp-nodes of Gtp, if the map has all k variables mapped to
bindings, we output one result. The procedure is repeated
again until all the triples in BitM at1 are exhausted.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA46Presently the ﬁnal phase (Step 2) always projects out
bindings of all variables in the query unless it is a “star join”.
However, in the future, depending on the nature of the con-
straint graph G for a given query and the variable bindings
asked by the SELECT clause, it might not be required to
traverse Gjvar twice (ref. Section 4.1) thereby further im-
proving the overall query processing time.

Note that all the procedures described previously work
on a compressed BitMat. Since the pruning phase only re-
duces the number of triples in the BitMat monotonically, the
memory requirement of the query processor goes on reduc-
ing as the pruning progresses and the ﬁnal phase of result
generation doesn’t build join tables.

To conclude the description of our procedure, we would
like to point out certain key diﬀerences of our query process-
ing algorithm from the typical bitmap index joins. BitMat’s
structure is similar to the idea of compressed bitmap indexes
which are widely used to improve joins in OLAP data ware-
housing techniques [17, 12, 20]. But an SQL join between
multiple tables over diﬀerent columns cannot always make
use of the bitmap indexes for the later joins. This is due to
the fact that after the ﬁrst level of join, a relational query
processor has to materialize the results of the previous join
to carry out the next join and this materialized table does
not always have indexes formed on it (unless join-indexes are
precomputed based on heuristics). As opposed to that, Bit-
Mat’s pruning and ﬁnal result generation steps always use
compressed BitMats, without materializing the intermediate
join results.

5. EVALUATION

BitMat structure and query algorithm is developed in C
and is compiled using g++ with -O3 optimization ﬂag. For
the experiments we used a Dell Optiplex 755 PC having 3.0
GHz Intel E6850 Core 2 Duo Processor, 4 GB of memory,
running 64 bit 2.6.28-15 Linux Kernel (Ubuntu 9.04 distri-
bution), with 7 GB of swap space on a 7200 rpm disk with
1 TB capacity.
5.1 Choice of competitive RDF stores

We had a wide choice to select the systems for compet-
itive evaluation due to the availability of numerous RDF
triplestores. We experimented with Hexastore6, Jena-TDB,
RDF-3X, and MonetDB. Out of these we chose RDF-3X
(v0.3.3) and MonetDB (v5.14.2) – latest versions – for our
evaluation, as they could load a large amount of RDF data,
gave better performance than others, and are open-source
systems used by the research community for performance
intensive RDF query execution.

Like BitMat, RDF-3X maps strings/URIs in RDF data
to integer IDs and mainly operates on these IDs, building
the entire result of a query in the integer ID format.
It
converts the IDs to strings using their dictionary mapping
just before outputting the results in a user readable format.
We observed that RDF-3X was taking signiﬁcant amount
of time to convert IDs into strings; in certain cases it took
even more time for this conversion than the time taken for
the core query execution. Current BitMat system doesn’t
support a formal SPARQL query parser interface and the
interface to output the results in the string format is still
under preliminary development. Hence for a fair compari-

6

We obtained compiled binaries of Hexastore from the authors.

son, we disabled the ID to string mapping in RDF-3X, which
improved their query times a lot. All the RDF-3X query
times reported in this paper are without their ID to string
mapping.

For a fair comparison, we loaded MonetDB7 by inserting
the integer IDs generated out of BitMat dictionary mapping
(ref. Section 3). Hence essentially all the MonetDB queries
were performed on S, P, Os as integer IDs. We created sepa-
rate predicate tables in MonetDB by inserting the respective
triples by ordering on S-O values [21] and used these predi-
cate tables in the query whenever there is a bound predicate
in the triple pattern instead of the giant triple-table contain-
ing all the triples.
5.2 Choice of datasets and queries

We chose UniProt dataset with 845,074,885 triples,

147,524,984 subjects, 95 predicates, and 128,321,926 objects
[23], which is a protein dataset. We also generated a dataset
using LUBM [13] – a synthetic data generator provided by
Lehigh University – with over 10,000 universities which gave
1,335,081,176 unique triples with 217,206,845 subjects, 18
predicates, and 161,413,042 objects. LUBM is widely used
by the Semantic Web community for benchmarking triple-
stores.

For UniProt dataset we used 6 out of 8 queries published
by RDF-3X in [16] (Q7-Q10, Q12, Q13 in our list, leaving
out 2 queries which have ‘all-variable’ triple patterns). To
increase the diversity, we also included 5 more queries (out
of 9) published by the UniProt dataset owners [19] (Q1, Q4-
Q6, Q11 in our list). We removed the FILTER condition in
the original Q1 as currently it is not supported by our query
processor and had to modify a ‘bound position’ in Q5, Q11
as that value did not exist in the dataset. We modiﬁed two
of the RDF-3X queries by removing some bound positions
to reduce the selectivity of triple patterns (Q2, Q3 in our
list). For the LUBM dataset, OpenRDF has published a
list of queries [18]. But many of these queries are simple 2-
triple pattern queries or they are quite similar to each other.
Hence we chose 7 representative queries out of this list. All
the queries are listed in Appendix A.
5.3 Discussion

For the evaluation, we measured the following parame-
ters: (i) query execution times (cold and warm cache). This
is an end-to-end time counted from the time of query sub-
mission to the time including outputting the ﬁnal results.
For cold cache we dropped the ﬁle systems caches using
/bin/sync and echo 3 > /proc/sys/vm/drop_caches, (ii)
initial number of triples – the sum of triples matching each
triple pattern in the query, and (iii) the number of results.
The evaluation is given in Tables 1, 2, and 3. Query times
were averaged over 10 consecutive runs. Geometric mean*
is the geometric mean of the query times excluding the ones
on which RDF-3X failed to complete processing.

Note that our current BitMat query processing system
does not use any sophisticated cache management (like Mon-
etDB) and also does not mmap dataﬁles into the memory (like
RDF-3X). Due to this, as opposed to RDF-3X and Mon-
etDB, in most of the queries the diﬀerence between our cold
and warm cache times was not very high.

After the evaluation, we could classify the queries into
3 categories – queries where BitMat clearly excelled over
7

MonetDB was compiled using “--enable-optimization” ﬂag.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA47Table 1: Evaluation – UniProt 845 million triples (time in seconds, best times are boldfaced)

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

BitMat
MonetDB
RDF-3X

451.365
548.21
Aborted

BitMat
MonetDB
RDF-3X
#Results
#Initial triples

440.868
495.64
Aborted
160,198,689
92,965,468

269.526
303.2134
525.105

263.071
267.532
487.1815
90,981,843
73,618,481

Cold cache
9.396
9.63
1.38

173.324
124.3563
244.58

Warm cache
8.305
0.584
0.077

168.6735
113.818
226.050
50,192,929
78,840,372

78.35
97.28
4.636

77.442
96.02
1.008

1.34
11.28
0.902

0.448
0.822
0.0064

9.33
9.91
0.892

8.36
0.861
0.003

13.06
15.93
1.353

10.87
0.362
0.0299

0
16,626,073

179,316
60,260,006

0
15,408,126

0
16,625,901

19
53,677,336

Table 2: Evaluation – UniProt 845 million triples (time in seconds, best times are boldfaced)

Q9

Q10

Q11

Q12

Q13

11.43
21.37
1.718

9.78
0.611
0.047

10.49
21.39
1.549

8.69
0.563
0.0469

Cold cache

15.56
12.33
3.268

26.98
2.468
2.804

Warm cache

14.13
0.71
0.547

25.19
0.744
0.295

17.37
12.884
1.765

15.77
1.02
0.0486

2
19,312,584

28
20,594,986

8893
20,951,969

2495
38,141,013

9
38,064,279

Geom.
Mean

25.775
27.891
N/A

21.754
3.845
N/A

Geom. Mean*
(without Q1)

20.304
21.761
4.268

16.929
2.565
0.255

BitMat
MonetDB
RDF-3X

BitMat
MonetDB
RDF-3X
#Results
#Initial triples

Table 3: Evaluation – LUBM 1.33 billion triples (time in seconds, best times are boldfaced)

Q1

Q2

Q3

Q4

Q5

Q6

BitMat
MonetDB
RDF-3X

51.21
109.35
Aborted

2.71
27.17
34.868

BitMat
MonetDB
RDF-3X
#Results
#Initial triples

48.57
96.65
Aborted
2528
165,397,764

2.11
6.56
29.033
10,799,863
224,805,759

6.56
455.23
2328.753

Cold cache

2.45
34.12
0.588

Warm cache

0.686
3.209
0.0024

1.94
398.46
2028.6855
0
219,416,877

0.503
18.89
0.425

0.27
0.566
0.0029

3.81
14.6
1.129

2.85
0.542
0.1814

Geom.
Mean

4.0285
48.3195
N/A

2.1719
7.9301
N/A

Geom. Mean*
(without Q1)

2.4227
41.0377
7.4474

1.1666
4.8094
0.5947

10
438,912,513

10
3,000,966

125
9,100,649

both RDF-3X and MonetDB (in both cold and warm cache
times), queries where BitMat did better than one of the
systems or the reported query times were comparable to the
other systems, and queries where BitMat’s performance was
worse than both RDF-3X and MonetDB.

Queries of the ﬁrst type are – Q1, Q2 of UniProt and
Q1, Q2, Q3 of LUBM. Notably, UniProt Q1, Q2 had a high
number of initial triples and the join results were quite un-
selective. As was our initial conjecture, BitMat did much
better on such queries than RDF-3X and MonetDB. On the
other hand, LUBM Q1 and Q3 were more complex queries
having a high number of initial triples associated with the
triple patterns, but the ﬁnal number of results were quite
small (2528 and 0 respectively). For these queries, the initial
selectivity of the triple patterns and selectivity of the inter-
mediate join results were quite low, but together they gave
highly selective results (these queries have cyclic dependency
among join variables – ref. Section 4.1). For these queries
BitMat was upto 3 orders of magnitude faster than RDF-
3X and MonetDB due to its way of producing join results
without materializing the intermediate join tables. RDF-

3X aborted while executing UniProt Q1 and LUBM Q1 as
the system ran out of its physical memory and swap space.
We executed the same queries on RDF-3X on a higher con-
ﬁguration server having 16 GB physical memory. RDF-3X
processed UniProt Q1 in 858.464 sec; was observed to con-
sume ∼11 GB resident memory. For LUBM Q1, RDF-3X
took 1613.178 sec and the peak memory consumption was
∼11 GB. For both queries BitMat took 448.169 and 50.70
sec, and consumed ∼2.6 GB and ∼3 GB respectively on the
same server.

A “star join” query was the one where many triple pat-
terns joined on one variable, the query had only one join
variable, and that variable got projected in the ﬁnal results.
LUBM Q2, Q4, Q5 were star-join queries. For star-joins
BitMat worked much better, because our query processor
doesn’t need to load the BitMats of all the triple patterns
in memory. It just loads the pre-computed row or column
bitarrays of each BitMat associated with the triple pattern
(ref. Section 3). The ﬁnal result generation phase consists
of just listing out the 1-bit positions from the bitwise AND
of the loaded bitarrays (similar to the bitmap index joins).

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA48Notably, although Q2 has only two triple patterns in it, each
has very low selectivity and the query generates a lot of re-
sults compared to query Q4 and Q5.

RDF-3X did very well on the UniProt queries Q7-Q10,
Q12, Q13. These queries have a lot of triple patterns, with
many having bound predicate and object positions which
make them highly selective. Also many of these triple pat-
terns join on one variable where RDF-3X’s method of Side-
ways Information Passing worked much better. The number
of results produced by these queries were highly selective too
(less than 30 results for 5/6 queries).

In the case of UniProt Q4, Q5, Q7-Q10 BitMat did bet-
ter than MonetDB (cold cache), but RDF-3X still outper-
formed BitMat (Q7-Q10 are the queries published by RDF-
3X). For Q6 the margin of diﬀerence between cold cache
times of RDF-3X and BitMat was quite small. But in case
of Q5, the diﬀerence was quite high. Further dissection of
BitMat query processing times revealed that initialization
and pruning phases were very fast, but more than 90% of
the time was spent in the last phase of the result construc-
tion. The reason behind this is – our current data structures
and result enumeration algorithm are not tuned to exploit
the “locality” in memory while generating the ﬁnal results.
This query has only one join variable but all the variables in
the query get projected in the results. On the other hand,
for UniProt Q11-Q13, more than 90% of the query process-
ing time was spent in the initialization to load the BitMats
associated with each triple pattern (ref. Section 4.1).
In
the future, this eﬀect can be alleviated by implementing a
“lazy loading” of the BitMats associated with the tp-nodes –
instead of loading all the BitMats at the beginning, one can
wait until the very ﬁrst join and then load only the required
portion of the BitMat in the unfold operation.

To summarize the results – it was evident that for com-
plex join queries with low-selectivity intermediate results,
BitMat outperformed both RDF-3X and MonetDB by a sig-
niﬁcant margin. Although for queries with highly selective
triple patterns generating fewer results, RDF-3X and Mon-
etDB performed better. This re-emphasizes our initial goal
of targeting low-selectivity queries with our novel query pro-
cessing algorithm.

In view of these results, we would like to mention one spe-
ciﬁc LUBM query which turned out be an outlier (LUBM
Q7). RDF-3X aborted due to the system running out of
memory on the Dell PC. BitMat took several hours to pro-
cess this query, although the processor clock showed that
the query spent only ∼200 seconds actually executing on
the processor. MonetDB processed it in 449.048 sec. For
further investigations, we evaluated this query on the server
having 16 GB of memory and we found the following:

• BitMat ﬁnished processing this query in 139.94 sec.
The peak resident memory consumption was reported
to be 6.3 GB, and on an average the process consumed
5 GB of the resident memory. The BitMat associated
with tp-node ‘?x ub:takesCourse ?z’ was very large,
∼3.4 GB, having 288,017,530 triples (22% of the total
triples) in it – largest among all the predicates. Thus,
on the 4 GB Dell PC, BitMat process spent a lot of
time in the kernel waiting for the pages to be allocated.
MonetDB handled this situation well due to its better
cache-memory management.

• MonetDB processed this query in 136.082 sec on the

16 GB memory server, but the peak resident memory
consumption was 9 GB, and on an average the process
consumed 8.6 GB of resident memory.

• RDF-3X processed the same query in 66.139 sec on
the same server, but its peak resident memory con-
sumption was 14 GB and on an average it consumed
13 GB of resident memory for most of the lifespan of
the query.

This query has 442,351,492 initial triples associated with
it – largest among the listed LUBM queries – and generates
439,994 results. We believe that with a “lazy loading” strat-
egy along with “proactive cache management” BitMat would
be able to handle these type of queries in a better manner
in future.
The on disk size of all 2|Vp| + |Vs| + |Vo| BitMats (ref.
Section 3) was 48 GB and 67 GB for UniProt and LUBM
respectively and corresponding LZ77 compressed dictionary
mappings were 3.2 GB and 1.8 GB. These BitMat sizes in-
clude the size of the meta-ﬁle too. But note that for any
given query with n triple patterns, the runtime memory re-
i=1 size(BitM ati); which is typically a
quirement is just
much smaller fraction of the total dataﬁle size. For RDF-3X
and MonetDB the on-disk size of dataﬁles were 42 GB and
16 GB for UniProt, and 70 GB and 25 GB for LUBM. The
sizes of raw RDF Ntriple ﬁles of UniProt and LUBM were
205 GB and 451 GB respectively.

Pn

We used an external Perl script to parse the raw triples
It took ∼12
and build string to ID dictionary mapping.
hours to parse and build dictionary mappings of LUBM data
and ∼9 hours for UniProt data. After parsing the data,
the S-O, O-S, P-S, P-O BitMats for UniProt and LUBM
were built in 41 and 56 minutes respectively. This process is
much faster than parsing due to our method of building the
compressed bit-row of a BitMat directly without building an
uncompressed array ﬁrst using the ID based triples sorted
on their S, P, O positions (details of this process are omitted
due to space constraints).

6. CONCLUSION AND FUTURE WORK

In this paper we demonstrated a novel method of process-
ing RDF join queries, following a simple principle of keeping
the data compressed as much as possible, without building
intermediate join tables, and producing the ﬁnal results in a
streaming fashion. Our evaluation using the state-of-the-art
RDF stores like RDF-3X and MonetDB showed that while
RDF-3X and MonetDB gave better performance on highly
selective queries, BitMat gave much superior performance
on low-selectivity queries, where sophisticated query opti-
mization techniques did not fetch a lot of beneﬁts. BitMat
could deliver over 3 orders of magnitude better performance
for some of the queries (e.g., LUBM Q3, warm cache).

Notably, working on the compressed data fetches beneﬁts
when the size of the underlying data is higher and when the
selectivity of triple patterns in the query and intermediate
join results is lower. This is due to our query processing
algorithm which keeps the runtime memory footprint small.
On the other hand, for queries with highly selective triple
patterns, processing the joins in a conventional manner can
be more beneﬁcial.

BitMat system is a prototype implementation of our query
processing algorithm. Since BitMat’s basic data structure
resembles compressed bitmap indexes, in the future it is

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA49possible to develop a hybrid system having BitMat’s query
processing algorithm and the conventional query processor.
The system can choose the method of processing the query
based on heuristics and selectivity of the triple patterns in
the query. Along with these avenues, in the future, we plan
to improve the system by further optimizing our algorithm.

Acknowledgments
We would like to thank Dr. Jagannathan Srinivasan for
his invaluable inputs in the initial phase of the work and
Thomas Neumann for his timely help in working with RDF-
3X sources.

7. REFERENCES
[1] Jena TDB. http://jena.sourceforge.net/TDB/.
[2] D. J. Abadi, S. R. Madden, and M. C. Ferreira. Integrating
Compression and Execution in Column Oriented Database
Systems. In SIGMOD, 2006.

[3] D. J. Abadi, A. Marcus, S. R. Madden, and K. Hollenbach.

Scalable Semantic Web Data Management using Vertical
Partitioning. In PVLDB, 2007.

[4] D. J. Abadi, D. S. Myers, D. J. DeWitt, and S. R. Madden.
Materialization Strategies in a Column-Oriented DBMS. In
ICDE, 2007.

[5] M. Atre and J. A. Hendler. BitMat: A Main-memory

Bit-Matrix of RDF Triples. In SSWS workshop at ISWC,
2009.

[6] P. A. Bernstein and D.-M. W. Chiu. Using semi-joins to

solve relational queries. Journal of the ACM, 28(1), 1981.

[7] P. A. Bernstein and N. Goodman. Power of natural

semijoins. SIAM Journal of Computing, 10(4), 1981.

[8] M. Cai and M. Frank. RDFPeers: A Scalable Distributed

RDF Repository based on a Structured Peer-to-Peer
Network. In WWW, 2004.

[9] R. Cyganiak. A Relational Algebra for SPARQL. Technical

Report, HP Laboratories Bristol, (HPL-2005-170), 2005.

[10] O. Erling. Virtuoso, 2006.

http://virtuoso.openlinksw.com/-
wiki/main/Main/VOSBitmapIndexing.

[11] M. Janik and K. Kochut. BRAHMS: A WorkBench RDF

Store and High Performance Memory System for Semantic
Association Discovery. In ISWC, 2005.

[12] T. Johnson. Performance Measurements of Compressed

Bitmap Indices. In PVLDB, 1999.

[13] LUBM. http://swat.cse.lehigh.edu/projects/lubm/.
[14] A. Matono, S. M. Pahlevi, and I. Kojima. RDFCube: A

P2P-based Three-dimensional Index for Structural Joins on
Distributed Triple Stores. In DBISP2P at VLDB, 2006.

[15] T. Neumann and G. Weikum. RDF3X: a RISC style Engine

for RDF. In PVLDB, 2008.

[16] T. Neumann and G. Weikum. Scalable join processing on

very large RDF graphs. In SIGMOD, 2009.

[17] P. O’Neil and G. Graefe. Multi-Table Joins Through

Bitmapped Join Indices. In SIGMOD Record, volume 24,
September 1995.

[18] OpenRDF LUBM Queries. http://repo.aduna-

software.org/viewvc/org.openrdf/?pathrev=6875.

[19] UniProt RDF Queries. http://dev.isb-

sib.ch/projects/expasy4j-webng/query.html#examples.

[20] S. Sarawagi. Indexing OLAP data. Data Engineering

Bulletin, 20(1), 1997.

[21] L. Sidirourgos, R. Goncalves, M. Kersten, et al.

Column-store Support for RDF Data Management: not all
swans are white. In PVLDB, 2008.

[22] O. Udrea, A. Pugliese, and V. Subrahmanian. GRIN: A

Graph Based RDF Index. In AAAI, 2007.

[23] UniProt RDF. http://dev.isb-sib.ch/projects/uniprot-rdf/.

[24] C. Weiss, P. Karras, and A. Bernstein. Hexastore: Sextuple
Indexing for Semantic Web Data Management. In PVLDB,
2008.

APPENDIX
A. QUERIES

?p1 rdf:type uni:Protein .

A.1 UniProt queries
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX uni: <http://purl.uniprot.org/core/> PREFIX uni2:
<http://purl.uniprot.org/>
Q1: SELECT ?modiﬁed ?author ?citation ?title ?protein WHERE
{ ?protein rdf:type uni:Protein . ?protein uni:/modiﬁed ?modiﬁed
. ?protein uni:citation ?citation . ?citation uni:author ?author .
?citation uni:title ?title .}
Q2: SELECT ?a ?vo ?b ?ab ?x ?z ?p WHERE { ?a uni:encodedBy
?vo . ?a schema:seeAlso ?x . ?b uni:sequence ?z . ?b uni:replaces
?p . ?b rdf:type uni:Protein . ?a uni:replaces ?ab . ?ab uni:repla-
cedBy ?b . }
Q3: SELECT ?a ?x ?vo ?b ?ab ?z ?p WHERE { ?a schema:seeAlso
?x . ?a uni:encodedBy ?vo . ?b uni:sequence ?z . ?b uni:replaces
?p . ?b uni:modiﬁed “2008-07-22” . ?b rdf:type uni:Protein . ?a
uni:replaces ?ab . ?ab uni:replacedBy ?b . }
Q4: SELECT ?name ?gene ?protein WHERE { ?protein rdf:type
uni:Protein . ?protein uni:encodedBy ?gene . ?gene uni:name
“hup” . ?protein uni:name ?name }
Q5: SELECT ?related ?p ?protein WHERE { ?protein rdf:type
uni:Protein . ?protein ?p uni2:keywords/482 . ?protein rdfs:see-
Also ?related . }
Q6: SELECT ?p2 ?interaction ?p1 WHERE { ?p1 uni:enzyme
uni2:enzyme/2.7.7.- .
?interaction
uni:participant ?p1 . ?interaction rdf:type uni:Interaction . ?in-
teraction uni:participant ?p2 . ?p2 rdf:type uni:Protein . ?p2
uni:enzyme uni2:enzyme/3.1.3.16 .}
Q7-Q10: Same as Q1, Q3, Q6, Q8 in [16] respectively.
Q11: SELECT ?aa ?s ?protein WHERE { ?protein uni:organism
?protein
uni2:taxonomy/287.
uni:sequence ?s . ?s rdf:value ?aa . }
Q12-Q13: Same as Q5 and Q7 respectively in [16],
Note: We project all the variables in the queries Q7-Q10, Q12,
Q13 above.
A.2 LUBM queries
PREFIX ub: <http://www.lehigh.edu/˜zhp2/2004/0401/univ-
bench.owl#>
Q1: SELECT ?x ?y ?z WHERE { ?z ub:subOrganizationOf
?y .
?z rdf:type ub:Department
?y rdf:type ub:University .
. ?x ub:memberOf ?z . ?x rdf:type ub:GraduateStudent . ?x
ub:undergraduateDegreeFrom ?y . }
Q2: SELECT ?x WHERE { ?x rdf:type ub:Course . ?x ub:name
?y . }
Q3: SELECT ?x ?y ?z WHERE { ?x rdf:type ub:Undergraduate-
Student. ?y rdf:type ub:University . ?z rdf:type ub:Department .
?x ub:memberOf ?z . ?z ub:subOrganizationOf ?y . ?x ub:under-
graduateDegreeFrom ?y . }
Q4: SELECT ?x WHERE { ?x ub:worksFor <http://www.-
Department0.University0.edu> . ?x rdf:type ub:FullProfessor .
?x ub:name ?y1 . ?x ub:emailAddress ?y2 . ?x ub:telephone ?y3
. }
Q5: SELECT ?x WHERE { ?x ub:subOrganizationOf <http-
://www.Department0.University0.edu> . ?x rdf:type ub:Research-
Group}
Q6: SELECT ?x ?y WHERE { ?y ub:subOrganizationOf <http-
://www.University0.edu> .
?x
?y rdf:type ub:Department .
ub:worksFor ?y . ?x rdf:type ub:FullProfessor . }
Q7: SELECT ?x ?y ?z WHERE { ?y ub:teacherOf ?z .
?y
rdf:type ub:FullProfessor . ?z rdf:type ub:Course . ?x ub:advisor
?y . ?x rdf:type ub:UndergraduateStudent . ?x ub:takesCourse
?z}

?protein rdf:type uni:Protein .

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA50