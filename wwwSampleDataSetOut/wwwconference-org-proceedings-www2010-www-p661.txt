A Contextual-Bandit Approach to

Personalized News Article Recommendation

Lihong Li†, Wei Chu†,
lihong,chuwei@yahoo-

†Yahoo! Labs

inc.com

John Langford‡
jl@yahoo-inc.com

‡Yahoo! Labs

∗

schapire@cs.princeton.edu

Robert E. Schapire+
+Dept of Computer Science

Princeton University

ABSTRACT
Personalized web services strive to adapt their services (advertise-
ments, news articles, etc.)
to individual users by making use of
both content and user information. Despite a few recent advances,
this problem remains challenging for at least two reasons. First,
web service is featured with dynamically changing pools of con-
tent, rendering traditional collaborative ﬁltering methods inappli-
cable. Second, the scale of most web services of practical interest
calls for solutions that are both fast in learning and computation.

In this work, we model personalized recommendation of news
articles as a contextual bandit problem, a principled approach in
which a learning algorithm sequentially selects articles to serve
users based on contextual information about the users and articles,
while simultaneously adapting its article-selection strategy based
on user-click feedback to maximize total user clicks.

The contributions of this work are three-fold. First, we propose
a new, general contextual bandit algorithm that is computationally
efﬁcient and well motivated from learning theory. Second, we ar-
gue that any bandit algorithm can be reliably evaluated ofﬂine us-
ing previously recorded random trafﬁc. Finally, using this ofﬂine
evaluation method, we successfully applied our new algorithm to
a Yahoo! Front Page Today Module dataset containing over 33
million events. Results showed a 12.5% click lift compared to a
standard context-free bandit algorithm, and the advantage becomes
even greater when data gets more scarce.

Categories and Subject Descriptors
H.3.5 [Information Systems]: On-line Information Services; I.2.6
[Computing Methodologies]: Learning

General Terms
Algorithms, Experimentation

Keywords
Contextual bandit, web service, personalization, recommender sys-
tems, exploration/exploitation dilemma

1.

INTRODUCTION

This paper addresses the challenge of identifying the most appro-
priate web-based content at the best time for individual users. Most
∗

This work was done while R. Schapire visited Yahoo! Labs.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

service vendors acquire and maintain a large amount of content in
their repository, for instance, for ﬁltering news articles [14] or for
the display of advertisements [5]. Moreover, the content of such a
web-service repository changes dynamically, undergoing frequent
insertions and deletions. In such a setting, it is crucial to quickly
identify interesting content for users. For instance, a news ﬁlter
must promptly identify the popularity of breaking news, while also
adapting to the fading value of existing, aging news stories.

It is generally difﬁcult to model popularity and temporal changes
based solely on content information. In practice, we usually ex-
plore the unknown by collecting consumers’ feedback in real time
to evaluate the popularity of new content while monitoring changes
in its value [3]. For instance, a small amount of trafﬁc can be des-
ignated for such exploration. Based on the users’ response (such
as clicks) to randomly selected content on this small slice of traf-
ﬁc, the most popular content can be identiﬁed and exploited on the
remaining trafﬁc. This strategy, with random exploration on an 
fraction of the trafﬁc and greedy exploitation on the rest, is known
as -greedy. Advanced exploration approaches such as EXP3 [8]
or UCB1 [7] could be applied as well. Intuitively, we need to dis-
tribute more trafﬁc to new content to learn its value more quickly,
and fewer users to track temporal changes of existing content.

Recently, personalized recommendation has become a desirable
feature for websites to improve user satisfaction by tailoring con-
tent presentation to suit individual users’ needs [10]. Personal-
ization involves a process of gathering and storing user attributes,
managing content assets, and, based on an analysis of current and
past users’ behavior, delivering the individually best content to the
present user being served.

Often, both users and content are represented by sets of fea-
tures. User features may include historical activities at an aggre-
gated level as well as declared demographic information. Content
features may contain descriptive information and categories. In this
scenario, exploration and exploitation have to be deployed at an in-
dividual level since the views of different users on the same con-
tent can vary signiﬁcantly. Since there may be a very large number
of possible choices or actions available, it becomes critical to rec-
ognize commonalities between content items and to transfer that
knowledge across the content pool.

Traditional recommender systems, including collaborative ﬁl-
tering, content-based ﬁltering and hybrid approaches, can provide
meaningful recommendations at an individual level by leveraging
users’ interests as demonstrated by their past activity. Collaborative
ﬁltering [25], by recognizing similarities across users based on their
consumption history, provides a good recommendation solution to
the scenarios where overlap in historical consumption across users
is relatively high and the content universe is almost static. Content-
based ﬁltering helps to identify new items which well match an

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA661existing user’s consumption proﬁle, but the recommended items
are always similar to the items previously taken by the user [20].
Hybrid approaches [11] have been developed by combining two
or more recommendation techniques; for example, the inability of
collaborative ﬁltering to recommend new items is commonly alle-
viated by combining it with content-based ﬁltering.

However, as noted above, in many web-based scenarios, the con-
tent universe undergoes frequent changes, with content popular-
ity changing over time as well. Furthermore, a signiﬁcant num-
ber of visitors are likely to be entirely new with no historical con-
sumption record whatsoever; this is known as a cold-start situa-
tion [21]. These issues make traditional recommender-system ap-
proaches difﬁcult to apply, as shown by prior empirical studies [12].
It thus becomes indispensable to learn the goodness of match be-
tween user interests and content when one or both of them are new.
However, acquiring such information can be expensive and may
reduce user satisfaction in the short term, raising the question of
optimally balancing the two competing goals: maximizing user sat-
isfaction in the long run, and gathering information about goodness
of match between user interests and content.

The above problem is indeed known as a feature-based explo-
ration/exploitation problem. In this paper, we formulate it as a con-
textual bandit problem, a principled approach in which a learning
algorithm sequentially selects articles to serve users based on con-
textual information of the user and articles, while simultaneously
adapting its article-selection strategy based on user-click feedback
to maximize total user clicks in the long run. We deﬁne a bandit
problem and then review some existing approaches in Section 2.
Then, we propose a new algorithm, LinUCB, in Section 3 which
has a similar regret analysis to the best known algorithms for com-
peting with the best linear predictor, with a lower computational
overhead. We also address the problem of ofﬂine evaluation in
Section 4, showing this is possible for any explore/exploit strat-
egy when interactions are independent and identically distributed
(i.i.d.), as might be a reasonable assumption for different users. We
then test our new algorithm and several existing algorithms using
this ofﬂine evaluation strategy in Section 5.

2. FORMULATION & RELATED WORK

In this section, we deﬁne the K-armed contextual bandit prob-
lem formally, and as an example, show how it can model the per-
sonalized news article recommendation problem. We then discuss
existing methods and their limitations.
2.1 A Multi-armed Bandit Formulation

The problem of personalized news article recommendation can
be naturally modeled as a multi-armed bandit problem with context
information. Following previous work [18], we call it a contextual
bandit.1 Formally, a contextual-bandit algorithm A proceeds in dis-
crete trials t = 1, 2, 3, . . . In trial t:
1. The algorithm observes the current user ut and a set At of
arms or actions together with their feature vectors xt,a for
a ∈ At. The vector xt,a summarizes information of both the
user ut and arm a, and will be referred to as the context.
2. Based on observed payoffs in previous trials, A chooses an
arm at ∈ At, and receives payoff rt,at whose expectation
depends on both the user ut and the arm at.

3. The algorithm then improves its arm-selection strategy with
the new observation, (xt,at , at, rt,at). It is important to em-

1In the literature, contextual bandits are sometimes called bandits
with covariate, bandits with side information, associative bandits,
and associative reinforcement learning.

i

PT

phasize here that no feedback (namely, the payoff rt,a) is
observed for unchosen arms a (cid:4)= at. The consequence of
this fact is discussed in more details in the next subsection.
hPT

In the process above, the total T -trial payoff of A is deﬁned as
t=1 rt,at . Similarly, we deﬁne the optimal expected T -trial pay-
off as E
t is the arm with maximum ex-
pected payoff at trial t. Our goal is to design A so that the expected
total payoff above is maximized. Equivalently, we may ﬁnd an al-
gorithm so that its regret with respect to the optimal arm-selection
strategy is minimized. Here, the T -trial regret RA(T ) of algorithm
A is deﬁned formally by

, where a∗

t=1 rt,a∗

t

"

TX

#

"

TX

#

RA(T )

def
= E

− E

rt,a∗

t

rt,at

.

(1)

t=1

t=1

An important special case of the general contextual bandit prob-
lem is the well-known K-armed bandit in which (i) the arm set At
remains unchanged and contains K arms for all t, and (ii) the user
ut (or equivalently, the context (xt,1,··· , xt,K)) is the same for
all t. Since both the arm set and contexts are constant at every trial,
they make no difference to a bandit algorithm, and so we will also
refer to this type of bandit as a context-free bandit.

In the context of article recommendation, we may view articles
in the pool as arms. When a presented article is clicked, a payoff
of 1 is incurred; otherwise, the payoff is 0. With this deﬁnition
of payoff, the expected payoff of an article is precisely its click-
through rate (CTR), and choosing an article with maximum CTR
is equivalent to maximizing the expected number of clicks from
users, which in turn is the same as maximizing the total expected
payoff in our bandit formulation.

Furthermore, in web services we often have access to user infor-
mation which can be used to infer a user’s interest and to choose
news articles that are probably most interesting to her. For example,
it is much more likely for a male teenager to be interested in an arti-
cle about iPod products rather than retirement plans. Therefore, we
may “summarize” users and articles by a set of informative features
that describe them compactly. By doing so, a bandit algorithm can
generalize CTR information from one article/user to another, and
learn to choose good articles more quickly, especially for new users
and articles.
2.2 Existing Bandit Algorithms

The fundamental challenge in bandit problems is the need for
balancing exploration and exploitation. To minimize the regret in
Eq. (1), an algorithm A exploits its past experience to select the arm
that appears best. On the other hand, this seemingly optimal arm
may in fact be suboptimal, due to imprecision in A’s knowledge. In
order to avoid this undesired situation, A has to explore by actually
choosing seemingly suboptimal arms so as to gather more informa-
tion about them (c.f., step 3 in the bandit process deﬁned in the pre-
vious subsection). Exploration can increase short-term regret since
some suboptimal arms may be chosen. However, obtaining infor-
mation about the arms’ average payoffs (i.e., exploration) can re-
ﬁne A’s estimate of the arms’ payoffs and in turn reduce long-term
regret. Clearly, neither a purely exploring nor a purely exploiting
algorithm works best in general, and a good tradeoff is needed.

The context-free K-armed bandit problem has been studied by
statisticians for a long time [9, 24, 26]. One of the simplest and
most straightforward algorithms is -greedy. In each trial t, this
algorithm ﬁrst estimates the average payoff ˆμt,a of each arm a.
Then, with probability 1 − , it chooses the greedy arm (i.e., the
arm with highest payoff estimate); with probability , it chooses a
random arm. In the limit, each arm will be tried inﬁnitely often,

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA662and so the payoff estimate ˆμt,a converges to the true value μa with
probability 1. Furthermore, by decaying  appropriately (e.g., [24]),
the per-step regret, RA(T )/T , converges to 0 with probability 1.

In contrast to the unguided exploration strategy adopted by -
greedy, another class of algorithms generally known as upper con-
ﬁdence bound algorithms [4, 7, 17] use a smarter way to balance
exploration and exploitation. Speciﬁcally, in trial t, these algo-
rithms estimate both the mean payoff ˆμt,a of each arm a as well
as a corresponding conﬁdence interval ct,a, so that |ˆμt,a − μa| <
ct,a holds with high probability. They then select the arm that
achieves a highest upper conﬁdence bound (UCB for short): at =
arg maxa (ˆμt,a + ct,a). With appropriately deﬁned conﬁdence in-
tervals, it can be shown that such algorithms have a small total T -
trial regret that is only logarithmic in the total number of trials T ,
which turns out to be optimal [17].

While context-free K-armed bandits are extensively studied and
well understood, the more general contextual bandit problem has
√
remained challenging. The EXP4 algorithm [8] uses the exponen-
tial weighting technique to achieve an ˜O(
T ) regret,2 but the com-
putational complexity may be exponential in the number of fea-
tures. Another general contextual bandit algorithm is the epoch-
greedy algorithm [18] that is similar to -greedy with shrinking
. This algorithm is computationally efﬁcient given an oracle opti-
mizer but has the weaker regret guarantee of ˜O(T 2/3).

Algorithms with stronger regret guarantees may be designed un-
der various modeling assumptions about the bandit. Assuming the
expected payoff of an arm is linear in its features, Auer [6] de-
√
scribes the LinRel algorithm that is essentially a UCB-type ap-
proach and shows that one of its variants has a regret of ˜O(
T ), a
signiﬁcant improvement over earlier algorithms [1].

Finally, we note that there exist another class of bandit al-
gorithms based on Bayes rule, such as Gittins index meth-
ods [15]. With appropriately deﬁned prior distributions, Bayesian
approaches may have good performance. These methods require
extensive ofﬂine engineering to obtain good prior models, and are
often computationally prohibitive without coupling with approxi-
mation techniques [2].

3. ALGORITHM

Given asymptotic optimality and the strong regret bound of UCB
methods for context-free bandit algorithms, it is tempting to de-
vise similar algorithms for contextual bandit problems. Given some
parametric form of payoff function, a number of methods exist to
estimate from data the conﬁdence interval of the parameters with
which we can compute a UCB of the estimated arm payoff. Such
an approach, however, is expensive in general.

In this work, we show that a conﬁdence interval can be com-
puted efﬁciently in closed form when the payoff model is linear,
and call this algorithm LinUCB. For convenience of exposition, we
ﬁrst describe the simpler form for disjoint linear models, and then
consider the general case of hybrid models in Section 3.2. We note
LinUCB is a generic contextual bandit algorithms which applies to
applications other than personalized news article recommendation.
3.1 LinUCB with Disjoint Linear Models

Using the notation of Section 2.1, we assume the expected payoff
of an arm a is linear in its d-dimensional feature xt,a with some
unknown coefﬁcient vector θθθ∗

a; namely, for all t,

E[rt,a|xt,a] = x(cid:3)

t,aθθθ∗
a.

(2)

This model is called disjoint since the parameters are not shared
2Note ˜O(·) is the same as O(·) but suppresses logarithmic factors.

among different arms. Let Da be a design matrix of dimension
m × d at trial t, whose rows correspond to m training inputs (e.g.,
m contexts that are observed previously for article a), and ba ∈
m be the corresponding response vector (e.g., the corresponding
R
m click/no-click user feedback). Applying ridge regression to the
training data (Da, ca) gives an estimate of the coefﬁcients:

ˆθθθa = (D(cid:3)

−1D(cid:3)

a Da + Id)

(3)
where Id is the d × d identity matrix. When components in ca are
independent conditioned on corresponding rows in Da, it can be
shown [27] that, with probability at least 1 − δ,

a ca,

t,aˆθθθa − E[rt,a|xt,a]

x(cid:3)
t,a(D(cid:3)

a Da + Id)−1xt,a

(4)

q
˛˛˛ ≤ α

˛˛˛x(cid:3)

for any δ > 0 and xt,a ∈ R
ln(2/δ)/2 is a
constant. In other words, the inequality above gives a reasonably
tight UCB for the expected payoff of arm a, from which a UCB-
type arm-selection strategy can be derived: at each trial t, choose

d, where α = 1 +

„

q

p

«

at

def
= arg max
a∈At

x(cid:3)
t,aˆθθθa + α

t,aA−1
x(cid:3)

a xt,a

,

(5)

where Aa

def

= D(cid:3)

a Da + Id.

q

t,aθθθ∗

a xt,a, and then

The conﬁdence interval in Eq. (4) may be motivated and derived
from other principles. For instance, ridge regression can also be
interpreted as a Bayesian point estimate, where the posterior dis-
tribution of the coefﬁcient vector, denoted as p(θθθa), is Gaussian
with mean ˆθθθa and covariance A−1
a . Given the current model, the
predictive variance of the expected payoff x(cid:3)
a is evaluated as
t,aA−1
x(cid:3)
t,aA−1
x(cid:3)
a xt,a becomes the standard de-
viation. Furthermore, in information theory [19], the differential
entropy of p(θθθa) is deﬁned as − 1
2 ln((2π)d det Aa). The entropy
of p(θθθa) when updated by the inclusion of the new point xt,a then
becomes − 1
t,a)). The entropy reduc-
t,aA−1
tion in the model posterior is 1
a xt,a). This quan-
tity is often used to evaluate model improvement contributed from
xt,a. Therefore, the criterion for arm selection in Eq. (5) can also
be regarded as an additive trade-off between the payoff estimate
and model uncertainty reduction.

2 ln((2π)d det (Aa + xt,ax(cid:3)
2 ln(1 + x(cid:3)

Algorithm 1 gives a detailed description of the entire LinUCB
algorithm, whose only input parameter is α. Note the value of α
given in Eq. (4) may be conservatively large in some applications,
and so optimizing this parameter may result in higher total payoffs
in practice. Like all UCB methods, LinUCB always chooses the
arm with highest UCB (as in Eq. (5)).

a

def

= A−1

This algorithm has a few nice properties. First, its computational
complexity is linear in the number of arms and at most cubic in
the number of features. To decrease computation further, we may
update Aat in every step (which takes O(d2) time), but compute
and cache Qa
(for all a) periodically instead of in real-
time. Second, the algorithm works well for a dynamic arm set,
and remains efﬁcient as long as the size of At is not too large. This
case is true in many applications. In news article recommendation,
for instance, editors add/remove articles to/from a pool and the pool
size remains essentially constant. Third, although it is not the focus
of the present paper, we can adapt the analysis from [6] to show the
following: if the arm set At is ﬁxed and contains K arms, then the
conﬁdence interval (i.e., the right-hand side of Eq. (4)) decreases
fast enough with more and more data, and then prove the strong
regret bound of ˜O(
KdT ), matching the state-of-the-art result [6]
for bandits satisfying Eq. (2). These theoretical results indicate
fundamental soundness and efﬁciency of the algorithm.

√

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA663d

Aa ← Id (d-dimensional identity matrix)
ba ← 0d×1 (d-dimensional zero vector)

Observe features of all arms a ∈ At: xt,a ∈ R
for all a ∈ At do
if a is new then

Algorithm 1 LinUCB with disjoint linear models.
0: Inputs: α ∈ R+
1: for t = 1, 2, 3, . . . , T do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: Aat ← Aat + xt,at x(cid:3)
13:
14: end for

end for
Choose arm at = arg maxa∈At pt,a with ties broken arbi-
trarily, and observe a real-valued payoff rt
bat ← bat + rtxt,at

end if
ˆθθθa ← A−1
a ba
(cid:3)
pt,a ← ˆθθθ
a xt,a + α

t,aA−1
x(cid:3)

q

a xt,a

t,at

Finally, we note that, under the assumption that input features
xt,a were drawn i.i.d. from a normal distribution (in addition to the
modeling assumption in Eq. (2)), Pavlidis et al. [22] came up with
a similar algorithm that uses a least-squares solution ˜θθθa instead of
our ridge-regression solution (ˆθθθa in Eq. (3)) to compute the UCB.
However, our approach (and theoretical analysis) is more general
and remains valid even when input features are nonstationary. More
importantly, we will discuss in the next section how to extend the
basic Algorithm 1 to a much more interesting case not covered by
Pavlidis et al.
3.2 LinUCB with Hybrid Linear Models
Algorithm 1 (or the similar algorithm in [22]) computes the in-
verse of the matrix, D(cid:3)
a Da), where Da is again
the design matrix with rows corresponding to features in the train-
ing data. These matrices of all arms have ﬁxed dimension d × d,
and can be updated efﬁciently and incrementally. Moreover, their
inverses can be computed easily as the parameters in Algorithm 1
are disjoint: the solution ˆθθθa in Eq. (3) is not affected by training
data of other arms, and so can be computed separately. We now
consider the more interesting case with hybrid models.

a Da + Id (or D(cid:3)

In many applications including ours, it is helpful to use features
that are shared by all arms, in addition to the arm-speciﬁc ones. For
example, in news article recommendation, a user may prefer only
articles about politics for which this provides a mechanism. Hence,
it is helpful to have features that have both shared and non-shared
components. Formally, we adopt the following hybrid model by
adding another linear term to the right-hand side of Eq. (2):

t,aβββ∗

E[rt,a|xt,a] = z(cid:3)
where zt,a ∈ R
k is the feature of the current user/article combina-
tion, and βββ∗
is an unknown coefﬁcient vector common to all arms.
This model is hybrid in the sense that some of the coefﬁcients βββ∗
are shared by all arms, while others θθθ∗

t,aθθθ∗
a,

+ x(cid:3)

a are not.

(6)

For hybrid models, we can no longer use Algorithm 1 as the
conﬁdence intervals of various arms are not independent due to the
shared features. Fortunately, there is an efﬁcient way to compute
an UCB along the same line of reasoning as in the previous sec-
tion. The derivation relies heavily on block matrix inversion tech-
niques. Due to space limitation, we only give the pseudocode in
Algorithm 2 (where lines 5 and 12 compute the ridge-regression
solution of the coefﬁcients, and line 13 computes the conﬁdence
interval), and leave detailed derivations to a full paper. Here, we

k+d

0 b0

Observe features of all arms a ∈ At: (zt,a, xt,a) ∈ R
ˆβββ ← A−1
for all a ∈ At do
if a is new then

Algorithm 2 LinUCB with hybrid linear models.
0: Inputs: α ∈ R+
1: A0 ← Ik (k-dimensional identity matrix)
2: b0 ← 0k (k-dimensional zero vector)
3: for t = 1, 2, 3, . . . , T do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Aa ← Id (d-dimensional identity matrix)
Ba ← 0d×k (d-by-k zero matrix)
ba ← 0d×1 (d-dimensional zero vector)

“
end if
ˆθθθa ← A−1
st,a ← z(cid:3)
x(cid:3)
t,aA−1
pt,a ← z(cid:3)

ba − Ba ˆβββ
t,aA−1
a xt,a + x(cid:3)
t,a ˆβββ + x(cid:3)

0 zt,a − 2z(cid:3)
t,aA−1
a BaA−1
0 B(cid:3)
t,aA−1
√
t,aˆθθθa + α
st,a

0 B(cid:3)
a A−1

a A−1

a xt,a

”

a

a xt,a +

at

14:
end for
15:
Choose arm at = arg maxa∈At pt,a with ties broken arbi-
16:
trarily, and observe a real-valued payoff rt
17: A0 ← A0 + B(cid:3)
A−1
Bat
b0 ← b0 + B(cid:3)
A−1
bat
18:
19: Aat ← Aat + xt,at x(cid:3)
t,at
20: Bat ← Bat + xt,at z(cid:3)
bat ← bat + rtxt,at
21:
22: A0 ← A0 + zt,at z(cid:3)
b0 ← b0 + rtzt,at − B(cid:3)
23:
24: end for

− B(cid:3)
A−1

A−1
at
bat

Bat

t,at

t,at

at

at

at

at

at

at

only point out the important fact that the algorithm is computation-
ally efﬁcient since the building blocks in the algorithm (A0, b0,
Aa, Ba, and ba) all have ﬁxed dimensions and can be updated
incrementally. Furthermore, quantities associated with arms not
existing in At no longer get involved in the computation. Finally,
we can also compute and cache the inverses (A−1
a ) pe-
riodically instead of at the end of each trial to reduce the per-trial
computational complexity to O(d2 + k2).

and A−1

0

4. EVALUATION METHODOLOGY

Compared to machine learning in the more standard supervised
setting, evaluation of methods in a contextual bandit setting is frus-
tratingly difﬁcult. Our goal here is to measure the performance of a
bandit algorithm π, that is, a rule for selecting an arm at each time
step based on the preceding interactions (such as the algorithms de-
scribed above). Because of the interactive nature of the problem, it
would seem that the only way to do this is to actually run the algo-
rithm on “live” data. However, in practice, this approach is likely to
be infeasible due to the serious logistical challenges that it presents.
Rather, we may only have ofﬂine data available that was collected
at a previous time using an entirely different logging policy. Be-
cause payoffs are only observed for the arms chosen by the logging
policy, which are likely to often differ from those chosen by the
algorithm π being evaluated, it is not at all clear how to evaluate
π based only on such logged data. This evaluation problem may
be viewed as a special case of the so-called “off-policy evaluation
problem” in reinforcement learning (see, c.f., [23]).

One solution is to build a simulator to model the bandit process
from the logged data, and then evaluate π with the simulator. How-
ever, the modeling step will introduce bias in the simulator and so
make it hard to justify the reliability of this simulator-based evalu-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA664ation approach. In contrast, we propose an approach that is simple
to implement, grounded on logged data, and unbiased.

In this section, we describe a provably reliable technique for car-
rying out such an evaluation, assuming that the individual events
are i.i.d., and that the logging policy that was used to gather the
logged data chose each arm at each time step uniformly at random.
Although we omit the details, this latter assumption can be weak-
ened considerably so that any randomized logging policy is allowed
and our solution can be modiﬁed accordingly using rejection sam-
pling, but at the cost of decreased efﬁciency in using data.

More precisely, we suppose that there is some unknown dis-
tribution D from which tuples are drawn i.i.d. of the form
(x1, ..., xK, r1, . . . , rK), each consisting of observed feature vec-
tors and hidden payoffs for all arms. We also posit access to a large
sequence of logged events resulting from the interaction of the log-
ging policy with the world. Each such event consists of the context
vectors x1, ..., xK, a selected arm a and the resulting observed pay-
off ra. Crucially, only the payoff ra is observed for the single arm
a that was chosen uniformly at random. For simplicity of presenta-
tion, we take this sequence of logged events to be an inﬁnitely long
stream; however, we also give explicit bounds on the actual ﬁnite
number of events required by our evaluation method.

Our goal is to use this data to evaluate a bandit algorithm π.
Formally, π is a (possibly randomized) mapping for selecting the
arm at at time t based on the history ht−1 of t−1 preceding events,
together with the current context vectors xt1, ..., xtK.

Our proposed policy evaluator is shown in Algorithm 3. The
method takes as input a policy π and a desired number of “good”
events T on which to base the evaluation. We then step through
the stream of logged events one by one. If, given the current his-
tory ht−1, it happens that the policy π chooses the same arm a as
the one that was selected by the logging policy, then the event is
retained, that is, added to the history, and the total payoff Rt up-
dated. Otherwise, if the policy π selects a different arm from the
one that was taken by the logging policy, then the event is entirely
ignored, and the algorithm proceeds to the next event without any
other change in its state.

Note that, because the logging policy chooses each arm uni-
formly at random, each event is retained by this algorithm with
probability exactly 1/K, independent of everything else. This
means that the events which are retained have the same distribution
as if they were selected by D. As a result, we can prove that two
processes are equivalent: the ﬁrst is evaluating the policy against T
real-world events from D, and the second is evaluating the policy
using the policy evaluator on a stream of logged events.

THEOREM 1. For all distributions D of contexts, all policies π,

all T , and all sequences of events hT ,

Pr

Policy_Evaluator(π,S)

(hT ) = Pr
π,D

(hT )

where S is a stream of events drawn i.i.d. from a uniform random
logging policy and D. Furthermore, the expected number of events
obtained from the stream to gather a history hT of length T is KT .

This theorem says that every history hT has the identical prob-
ability in the real world as in the policy evaluator. Many statistics
of these histories, such as the average payoff RT /T returned by
Algorithm 3, are therefore unbiased estimates of the value of the
algorithm π. Further, the theorem states that KT logged events are
required, in expectation, to retain a sample of size T .

PROOF. The proof is by induction on t = 1, . . . , T starting with
a base case of the empty history which has probability 1 when t = 0

repeat

Algorithm 3 Policy_Evaluator.
0: Inputs: T > 0; policy π; stream of events
1: h0 ← ∅ {An initially empty history}
2: R0 ← 0 {An initially zero total payoff}
3: for t = 1, 2, 3, . . . , T do
4:
5:
6:
7:
8: Rt ← Rt−1 + ra
9: end for
10: Output: RT /T

Get next event (x1, ..., xK , a, ra)

until π(ht−1, (x1, ..., xK)) = a
ht ← CONCATENATE(ht−1, (x1, ..., xK , a, ra))

under both methods of evaluation. In the inductive case, assume
that we have for all t − 1:
Pr

(ht−1)

(ht−1) = Pr
π,D

Policy_Evaluator(π,S)

and want to prove the same statement for any history ht. Since the
data is i.i.d. and any randomization in the policy is independent of
randomization in the world, we need only prove that conditioned
on the history ht−1 the distribution over the t-th event is the same
for each process. In other words, we must show:

Pr

Policy_Evaluator(π,S)

= Pr
D

(xt,1, ..., xt,K, rt,a) Pr

π(ht−1)

((xt,1, ..., xt,K, a, rt,a) | ht−1)

(a | xt,1, ..., xt,K).

Since the arm a is chosen uniformly at random in the logging pol-
icy, the probability that the policy evaluator exits the inner loop is
identical for any policy, any history, any features, and any arm, im-
plying this happens for the last event with the probability of the
last event, PrD(xt,1, ..., xt,K, rt,a). Similarly, since the policy π’s
distribution over arms is independent conditioned on the history
ht−1 and features (xt,1, ..., xt,K ), the probability of arm a is just
Prπ(ht−1)(a|xt,1, ..., xt,K ).
Finally, since each event from the stream is retained with proba-
bility exactly 1/K, the expected number required to retain T events
is exactly KT .

5. EXPERIMENTS

In this section, we verify the capacity of the proposed LinUCB
algorithm on a real-world application using the ofﬂine evaluation
method of Section 4. We start with an introduction of the problem
setting in Yahoo! Today-Module, and then describe the user/item
attributes we used in experiments. Finally, we deﬁne performance
metrics and report experimental results with comparison to a few
standard (contextual) bandit algorithms.
5.1 Yahoo! Today Module

The Today Module is the most prominent panel on the Yahoo!
Front Page, which is also one of the most visited pages on the In-
ternet; see a snapshot in Figure 1. The default “Featured” tab in the
Today Module highlights one of four high-quality articles, mainly
news, while the four articles are selected from an hourly-refreshed
article pool curated by human editors. As illustrated in Figure 1,
there are four articles at footer positions, indexed by F1–F4. Each
article is represented by a small picture and a title. One of the four
articles is highlighted at the story position, which is featured by a
large picture, a title and a short summary along with related links.
By default, the article at F1 is highlighted at the story position. A

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA665about 1000 binary categories that summarize the user’s consump-
tion history within Yahoo! properties. Other than these features, no
other information was used to identify a user.

Similarly, each article was represented by a raw feature vector of
about 100 categorical features constructed in the same way. These
features include: (i) URL categories: tens of classes inferred from
the URL of the article resource; and (ii) editor categories: tens of
topics tagged by human editors to summarize the article content.

We followed a previous procedure [12] to encode categorical
user/article features as binary vectors and then normalize each fea-
ture vector to unit length. We also augmented each feature vector
with a constant feature of value 1. Now each article and user was
represented by a feature vector of 83 and 1193 entries, respectively.
To further reduce dimensionality and capture nonlinearity in
these raw features, we carried out conjoint analysis based on ran-
dom exploration data collected in September 2008. Following a
previous approach to dimensionality reduction [13], we projected
user features onto article categories and then clustered users with
similar preferences into groups. More speciﬁcally:
• We ﬁrst used logistic regression (LR) to ﬁt a bilinear model
for click probability given raw user/article features so that
φφφ(cid:3)
u Wφφφa approximated the probability that the user u clicks
on article a, where φφφu and φφφa were the corresponding feature
vectors, and W was a weight matrix optimized by LR.
• Raw user features were then projected onto an induced space
u W. Here, the ith component in ψψψu
by computing ψψψu
for user u may be interpreted as the degree to which the user
likes the ith category of articles. K-means was applied to
group users in the induced ψψψu space into 5 clusters.
• The ﬁnal user feature was a six-vector: ﬁve entries corre-
sponded to membership of that user in these 5 clusters (com-
puted with a Gaussian kernel and then normalized so that
they sum up to unity), and the sixth was a constant feature 1.
At trial t, each article a has a separate six-dimensional feature xt,a
that is exactly the six-dimensional feature constructed as above for
user ut. Since these article features do not overlap, they are for
disjoint linear models deﬁned in Section 3.

= φφφ(cid:3)

def

For each article a, we performed the same dimensionality reduc-
tion to obtain a six-dimensional article feature (including a constant
1 feature). Its outer product with a user feature gave 6 × 6 = 36
features, denoted zt,a ∈ R
36, that corresponded to the shared fea-
tures in Eq. (6), and thus (zt,a, xt,a) could be used in the hybrid
linear model. Note the features zt,a contains user-article interac-
tion information, while xt,a contains user information only.

Here, we intentionally used ﬁve users (and articles) groups,
which has been shown to be representative in segmentation anal-
ysis [13]. Another reason for using a relatively small feature space
is that, in online services, storing and retrieving large amounts of
user/article information will be too expensive to be practical.

5.3 Compared Algorithms

The algorithms empirically evaluated in our experiments can be

categorized into three groups:
I. Algorithms that make no use of features. These correspond to
the context-free K-armed bandit algorithms that ignore all contexts
(i.e., user/article information).
• random: A random policy always chooses one of the candi-
date articles from the pool with equal probability. This algo-
rithm requires no parameters and does not “learn” over time.
• -greedy: As described in Section 2.2, it estimates each arti-
cle’s CTR; then it chooses a random article with probability
, and chooses the article of the highest CTR estimate with
probability 1 − . The only parameter of this policy is .

Figure 1: A snapshot of the “Featured” tab in the Today Mod-
ule on Yahoo! Front Page. By default, the article at F1 position
is highlighted at the story position.

user can click on the highlighted article at the story position to read
more details if she is interested in the article. The event is recorded
as a story click. To draw visitors’ attention, we would like to rank
available articles according to individual interests, and highlight the
most attractive article for each visitor at the story position.
5.2 Experiment Setup

This subsection gives a detailed description of our experimental
setup, including data collection, feature construction, performance
evaluation, and competing algorithms.

5.2.1 Data Collection

We collected events from a random bucket in May 2009. Users
were randomly selected to the bucket with a certain probability per
visiting view.3 In this bucket, articles were randomly selected from
the article pool to serve users. To avoid exposure bias at footer
positions, we only focused on users’ interactions with F1 articles
at the story position. Each user interaction event consists of three
components: (i) the random article chosen to serve the user, (ii)
user/article information, and (iii) whether the user clicks on the ar-
ticle at the story position. Section 4 shows these random events can
be used to reliably evaluate a bandit algorithm’s expected payoff.

There were about 4.7 million events in the random bucket on
May 01. We used this day’s events (called “tuning data”) for model
validation to decide the optimal parameter for each competing ban-
dit algorithm. Then we ran these algorithms with tuned parameters
on a one-week event set (called “evaluation data”) in the random
bucket from May 03–09, which contained about 36 million events.

5.2.2 Feature Construction

We now describe the user/article features constructed for our ex-
periments. Two sets of features for the disjoint and hybrid models,
respectively, were used to test the two forms of LinUCB in Sec-
tion 3 and to verify our conjecture that hybrid models can improve
learning speed.

We start with raw user features that were selected by “support”.
The support of a feature is the fraction of users having that feature.
To reduce noise in the data, we only selected features with high
support. Speciﬁcally, we used a feature when its support is at least
0.1. Then, each user was originally represented by a raw feature
vector of over 1000 categorical components, which include: (i) de-
mographic information: gender (2 classes) and age discretized into
10 segments; (ii) geographic features: about 200 metropolitan lo-
cations worldwide and U.S. states; and (iii) behavioral categories:
3We call
browser, the user may not fall into the random bucket again.

it view-based randomization. After refreshing her

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA666 2

 1.8

 1.6

 1.4

 1.2

r
t
c

 1

 0

 2

 1.8

 1.6

 1.4

 1.2

r
t
c

 1

 0

 2

 1.8

 1.6

 1.4

 1.2

r
t
c

 1

 0

 2

 1.8

 1.6

 1.4

 1.2

r
t
c

 1

 0

ucb
ucb (warm)
ucb (seg)
linucb (disjoint)
linucb (hybrid)
omniscient

 0.2

 0.4

 0.6

 0.8
α

 1

 1.2

 1.4

(b) Deployment bucket.

ucb
ucb (warm)
ucb (seg)
linucb (simple)
linucb (hybrid)
omniscient

 0.2

 0.4

 0.6

 0.8
α

 1

 1.2

 1.4

(d) Learning bucket.

ε-greedy
ε-greedy (warm)
ε-greedy (seg)
ε-greedy (disjoint)
ε-greedy (hybrid)
omniscient

 0.2

 0.4

ε

 0.6

 0.8

 1

(a) Deployment bucket.

ε-greedy
ε-greedy (warm)
ε-greedy (seg)
ε-greedy (disjoint)
ε-greedy (hybrid)
omniscient

 0.2

 0.4

ε

 0.6

 0.8

 1

(c) Learning bucket.

Figure 2: Parameter tuning: CTRs of various algorithms on the one-day tuning dataset.

• ucb: As described in Section 2.2, this policy estimates each
article’s CTR as well as a conﬁdence interval of the estimate,
and always chooses the article with the highest UCB. Speciﬁ-
cally, following UCB1 [7], we computed an article a’s conﬁ-
dence interval by ct,a = α√
nt,a , where nt,a is the number of
times a was chosen prior to trial t, and α > 0 is a parameter.
• omniscient: Such a policy achieves the best empirical
context-free CTR from hindsight. It ﬁrst computes each ar-
ticle’s empirical CTR from logged events, and then always
chooses the article with highest empircal CTR when it is
evaluated using the same logged events. This algorithm re-
quires no parameters and does not “learn” over time.

III. Algorithms that learn user-speciﬁc CTRs online.

• -greedy (seg): Each user is assigned to the closest user
cluster among the ﬁve constructed in Section 5.2.2, and so all
users are partitioned into ﬁve groups (a.k.a. user segments),
in each of which a separate copy of -greedy was run.

II. Algorithms with “warm start”—an intermediate step towards
personalized services. The idea is to provide an ofﬂine-estimated
user-speciﬁc adjustment on articles’ context-free CTRs over the
whole trafﬁc. The offset serves as an initialization on CTR estimate
for new content, a.k.a.“warm start”. We re-trained the bilinear lo-
gistic regression model studied in [12] on Sept 2008 random trafﬁc
data, using features zt,a constructed above. The selection criterion
then becomes the sum of the context-free CTR estimate and a bi-
linear term for a user-speciﬁc CTR adjustment. In training, CTR
was estimated using the context-free -greedy with  = 1.
• -greedy (warm): This algorithm is the same as -greedy
except it adds the user-speciﬁc CTR correction to the article’s
context-free CTR estimate.
• ucb (warm): This algorithm is the same as the previous one

but replaces -greedy with ucb.

• ucb (seg): This algorithm is similar to -greedy (seg) ex-
cept it ran a copy of ucb in each of the ﬁve user segments.
• -greedy (disjoint): This is -greedy with disjoint models,
and may be viewed as a close variant of epoch-greedy [18].
• linucb (disjoint): This is Algorithm 1 with disjoint models.
• -greedy (hybrid): This is -greedy with hybrid models,
and may be viewed as a close variant of epoch-greedy.
• linucb (hybrid): This is Algorithm 2 with hybrid models.

5.4 Performance Metric

An algorithm’s CTR is deﬁned as the ratio of the number of
clicks it receives and the number of steps it is run. We used all
algorithms’ CTRs on the random logged events for performance
comparison. To protect business-sensitive information, we report
an algorithm’s relative CTR, which is the algorithm’s CTR divided
by the random policy’s. Therefore, we will not report a random pol-
icy’s relative CTR as it is always 1 by deﬁnition. For convenience,
we will use the term “CTR” from now on instead of “relative CTR”.
For each algorithm, we are interested in two CTRs motivated
by our application, which may be useful for other similar applica-
tions. When deploying the methods to Yahoo!’s front page, one
reasonable way is to randomly split all trafﬁc to this page into two
buckets [3]. The ﬁrst, called “learning bucket”, usually consists of
a small fraction of trafﬁc on which various bandit algorithms are
run to learn/estimate article CTRs. The other, called “deployment
bucket”, is where Yahoo! Front Page greedily serves users using
CTR estimates obained from the learning bucket. Note that “learn-
ing” and “deployment” are interleaved in this problem, and so in
every view falling into the deployment bucket, the article with the
highest current (user-speciﬁc) CTR estimate is chosen; this esti-
mate may change later if the learning bucket gets more data. CTRs
in both buckets were estimated with Algorithm 3.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA667algorithm

-greedy

ucb

-greedy (seg)

ucb (seg)

-greedy (disjoint)

linucb (disjoint)

-greedy (hybrid)

linucb (hybrid)

size = 30%

size = 20%

size = 10%

size = 5%

size = 1%

learn
1.326
0%
1.535

learn
1.273
0%
1.488

1.46

1.585

1.119

deploy
1.549
0%
1.569

size = 100%
deploy
learn
deploy
deploy
learn
1.596
1.326
1.465
1.541
1.326
0%
0%
0%
0%
0%
1.594
1.569
1.582
1.541
1.446
18.3%
2.7% 15.8% 1.3% 16.9% 5.2%
0%
9%
1.742
1.446
1.652
1.474
1.284
7.2% 10.1% 2.3% −12% 0.6% −3.1%
9.1%
9%
1.781
1.677
1.446
1.529
1.742
11.6% 26.5%
13.6% 11.7% 15.3%
13%
1.451
1.769
1.309
1.686
1.529
10.8% −1.2% 9.4%
9.4%
1.387
1.795
1.384
1.719
12.5% 24.2% 11.6% 13.7% 10.7% 8.7%
4.6%
1.348
1.739
1.449
1.7%
9%
1.73
1.535
8.4%

1.398
1.689
13.3%
9%
1.262
1.624
1.529
2.3%
4.8% 20.1% 4.4%
1.382
1.655
1.714
12%
13%
1.342
1.636
1.58
8.8%
5.6% 13.8% 7.8%
1.708
1.482
1.675
10.3% 27.2% 14.3% 15.8% 12.7% 16.6% 20.1%

1.555
17.3%
1.337
0.8%
1.507

deploy
1.234
0%
1.354
9.7%
1.245

deploy
learn
learn
1.409
1.292
1.139
0%
0%
0%
1.541
1.465
1.22
9.4%
13.4%
7.1%
1.407
1.281
1.072
−0.8% 0.9% −5.8%
0%
1.532
1.25
1.32
8.7%
9.7%
2.2%
1.432
1.183
1.345
1.6%
4.1%
3.9%
1.197
1.574
1.245
11.7% −3.5%
5.1%
1.2
1.415
1.465
5.4%
9.5%
4%
1.446
1.588
1.507
27%

1.647

1.521
14.7%
1.663
25.4%

1.345
1.4%
1.591
20%

1.636

1.68
9%
1.691
9.7%

1.619

Table 1: Performance evaluation: CTRs of all algorithms on the one-week evaluation dataset in the deployment and learning buckets
(denoted by “deploy” and “learn” in the table, respectively). The numbers with a percentage is the CTR lift compared to -greedy.

Since the deployment bucket is often larger than the learning
bucket, CTR in the deployment bucket is more important. How-
ever, a higher CTR in the learning bucket suggests a faster learning
rate (or equivalently, smaller regret) for a bandit algorithm. There-
fore, we chose to report algorithm CTRs in both buckets.
5.5 Experimental Results
5.5.1 Results for Tuning Data

Each of the competing algorithms (except random and omni-
scient) in Section 5.3 requires a single parameter:  for -greedy
algorithms and α for UCB ones. We used tuning data to optimize
these parameters. Figure 2 shows how the CTR of each algorithm
changes with respective parameters. All results were obtained by
a single run, but given the size of our dataset and the unbiasedness
result in Theorem 1, the reported numbers are statistically reliable.
First, as seen from Figure 2, the CTR curves in the learning buck-
ets often possess the inverted U-shape. When the parameter ( or
α) is too small, there was insufﬁcient exploration, the algorithms
failed to identify good articles, and had a smaller number of clicks.
On the other hand, when the parameter is too large, the algorithms
appeared to over-explore and thus wasted some of the opportunities
to increase the number of clicks. Based on these plots on tuning
data, we chose appropriate parameters for each algorithm and ran
it once on the evaluation data in the next subsection.

Second, it can be concluded from the plots that warm-start in-
formation is indeed helpful for ﬁnding a better match between user
interest and article content, compared to the no-feature versions of
-greedy and UCB. Speciﬁcally, both -greedy (warm) and ucb
(warm) were able to beat omniscient, the highest CTRs achiev-
able by context-free policies in hindsight. However, performance
of the two algorithms using warm-start information is not as stable
as algorithms that learn the weights online. Since the ofﬂine model
for “warm start” was trained with article CTRs estimated on all ran-
dom trafﬁc [12], -greedy (warm) gets more stable performance
in the deployment bucket when  is close to 1. The warm start part
also helps ucb (warm) in the learning bucket by selecting more at-
tractive articles to users from scratch, but did not help ucb (warm)
in determining the best online for deployment. Since ucb relies
on the a conﬁdence interval for exploration, it is hard to correct
the initialization bias introduced by “warm start”. In contrast, all
online-learning algorithms were able to consistently beat the omni-
scient policy. Therefore, we did not try the warm-start algorithms
on the evaluation data.

Third, -greedy algorithms (on the left of Figure 2) achieved sim-
ilar CTR as upper conﬁdence bound ones (on the right of Figure 2)
in the deployment bucket when appropriate parameters were used.
Thus, both types of algorithms appeared to learn comparable poli-
cies. However, they seemed to have lower CTR in the learning
bucket, which is consistent with the empirical ﬁndings of context-
free algorithms [2] in real bucket tests.

Finally, to compare algorithms when data are sparse, we repeated
the same parameter tuning process for each algorithm with fewer
data, at the level of 30%, 20%, 10%, 5%, and 1%. Note that we
still used all data to evaluate an algorithm’s CTR as done in Algo-
rithm 3, but then only a fraction of available data were randomly
chosen to be used by the algorithm to improve its policy.

5.5.2 Results for Evaluation Data

With parameters optimized on the tuning data (c.f., Figure 2), we
ran the algorithms on the evaluation data and summarized the CTRs
in Table 1. The table also reports the CTR lift compared to the
baseline of -greedy. The CTR of omniscient was 1.615, and so
a signiﬁcantly larger CTR of an algorithm indicates its effective use
of user/article features for personalization. Recall that the reported
CTRs were normalized by the random policy’s CTR. We examine
the results more closely in the following subsections.

On the Use of Features.

We ﬁrst investigate whether it helps to use features in article rec-
ommendation.
It is clear from Table 1 that, by considering user
features, both -greedy (seg/disjoint/hybrid) and UCB methods
(ucb (seg) and linucb (disjoint/hybrid)) were able to achieve a
CTR lift of around 10%, compared to the baseline -greedy.

To better visualize the effect of features, Figure 3 shows how an
article’s CTR (when chosen by an algorithm) was lifted compared
to its base CTR (namely, the context-free CTR).4 Here, an article’s
base CTR measures how interesting it is to a random user, and was
estimated from logged events. Therefore, a high ratio of the lifted
and base CTRs of an article is a strong indicator that an algorithm
does recommend this article to potentially interested users. Fig-
ure 3(a) shows neither -greedy nor ucb was able to lift article
CTRs, since they made no use of user information. In contrast, all

4To avoid inaccurate CTR estimates, only 50 articles that were
chosen most often by an algorithm were included in its own plots.
Hence, the plots for different algorithms are not comparable.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA668 3

 3

 3

 3

 2

r
t
c
 

d
e

t
f
i
l

 1

 0

 0

 2

r
t
c
 

d
e

t
f
i
l

 1

 0

 0

 3

 1

 2

base ctr

 2

r
t
c
 

d
e

t
f
i
l

 1

 0

 0

 3

 1

 2

base ctr

 2

r
t
c
 

d
e

t
f
i
l

 1

 0

 0

 3

 1

 2

base ctr

 1

 2

base ctr

 3

(a) -greedy and ucb

(b) seg:-greedy and ucb

(c) disjoint:-greedy and linucb

(d) hybrid:-greedy and linucb

Figure 3: Scatterplots of the base CTR vs. lifted CTR (in the learning bucket) of the 50 most frequently selected articles when 100%
evaluation data were used. Red crosses are for -greedy algorithms, and blue circles are for UCB algorithms. Note that the sets of
most frequently chosen articles varied with algorithms; see the text for details.

the other three plots show clear beneﬁts by considering personal-
ized recommendation. In an extreme case (Figure 3(c)), one of the
article’s CTR was lifted from 1.31 to 3.03—a 132% improvement.
Furthermore, it is consistent with our previous results on tuning
data that, compared to -greedy algorithms, UCB methods achieved
higher CTRs in the deployment bucket, and the advantage was even
greater in the learning bucket. As mentioned in Section 2.2, -
greedy approaches are unguided because they choose articles uni-
formly at random for exploration. In contrast, exploration in upper
conﬁdence bound methods are effectively guided by conﬁdence
intervals—a measure of uncertainty in an algorithm’s CTR esti-
mate. Our experimental results imply the effectiveness of upper
conﬁdence bound methods and we believe they have similar bene-
ﬁts in many other applications as well.

On the Size of Data.

One of the challenges in personalized web services is the scale
of the applications. In our problem, for example, a small pool of
news articles were hand-picked by human editors. But if we wish
to allow more choices or use automated article selection methods
to determine the article pool, the number of articles can be too large
even for the high volume of Yahoo! trafﬁc. Therefore, it becomes
critical for an algorithm to quickly identify a good match between
user interests and article contents when data are sparse. In our ex-
periments, we artiﬁcially reduced data size (to the levels of 30%,
20%, 10%, 5%, and 1%, respectively) to mimic the situation where
we have a large article pool but a ﬁxed volume of trafﬁc.

To better visualize the comparison results, we use bar graphs in
Figure 4 to plot all algorithms’ CTRs with various data sparsity
levels. A few observations are in order. First, at all data sparsity
levels, features were still useful. At the level of 1%, for instance,
we observed a 10.3% improvement of linucb (hybrid)’s CTR in the
deployment bucket (1.493) over ucb’s (1.354).

Second, UCB methods consistently outperformed -greedy ones
in the deployment bucket.5 The advantage over -greedy was even
more apparent when data size was smaller.

Third, compared to ucb (seg) and linucb (disjoint), linucb (hy-
brid) showed signiﬁcant beneﬁts when data size was small. Re-
call that in hybrid models, some features are shared by all articles,
making it possible for CTR information of one article to be “trans-
ferred” to others. This advantage is particularly useful when the
article pool is large. In contrast, in disjoint models, feedback of

5In the less important learning bucket, there were two exceptions
for linucb (disjoint).

one article may not be utilized by other articles; the same is true for
ucb (seg). Figure 4(a) shows transfer learning is indeed helpful
when data are sparse.

Comparing ucb (seg) and linucb (disjoint).

From Figure 4(a), it can be seen that ucb (seg) and linucb (dis-
joint) had similar performance. We believe it was no coincidence.
Recall that features in our disjoint model are actually normalized
membership measures of a user in the ﬁve clusters described in
Section 5.2.2. Hence, these features may be viewed as a “soft”
version of the user assignment process adopted by ucb (seg).

Figure 5 plots the histogram of a user’s relative membership
measure to the closest cluster, namely, the largest component of the
user’s ﬁve, non-constant features. It is clear that most users were
quite close to one of the ﬁve cluster centers: the maximum mem-
bership of about 85% users were higher than 0.5, and about 40% of
them were higher than 0.8. Therefore, many of these features have
a highly dominating component, making the feature vector similar
to the “hard” version of user group assignment.

We believe that adding more features with diverse components,
such as those found by principal component analysis, would be nec-
essary to further distinguish linucb (disjoint) from ucb (seg).

6. CONCLUSIONS

This paper takes a contextual-bandit approach to personalized
web-based services such as news article recommendation. We pro-
posed a simple and reliable method for evaluating bandit algo-
rithms directly from logged events, so that the often problematic
simulator-building step could be avoided. Based on real Yahoo!
Front Page trafﬁc, we found that upper conﬁdence bound methods
generally outperform the simpler yet unguided -greedy methods.
Furthermore, our new algorithm LinUCB shows advantages when
data are sparse, suggesting its effectiveness to personalized web
services when the number of contents in the pool is large.

In the future, we plan to investigate bandit approaches to other
similar web-based serviced such as online advertising, and com-
pare our algorithms to related methods such as Banditron [16]. A
second direction is to extend the bandit formulation and algorithms
in which an “arm” may refer to a complex object rather than an
item (like an article). An example is ranking, where an arm corre-
sponds to a permutation of retrieved webpages. Finally, user inter-
ests change over time, and so it is interesting to consider temporal
information in bandit algorithms.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA669ε-greedy
ucb
ε-greedy (seg)
ucb (seg)
ε-greedy (disjoint)

linucb (disjoint)
ε-greedy (hybrid)
linucb (hybrid)
omniscient

100%

30%

20%

10%

data size

5%

1%

(a) CTRs in the deployment bucket.

100%

30%

20%

10%

data size

5%

1%

 1.8

 1.6

r
t
c

 1.4

 1.2

 1

 1.8

 1.6

r
t
c

 1.4

 1.2

 1

(b) CTRs in the learning bucket.

Figure 4: CTRs in evaluation data with varying data sizes.

 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

maximum user membership feature

Figure 5: User maximum membership histogram.

7. ACKNOWLEDGMENTS

We thank Deepak Agarwal, Bee-Chung Chen, Daniel Hsu, and
Kishore Papineni for many helpful discussions, István Szita and
Tom Walsh for clarifying their algorithm, and Taylor Xi and the
anonymous reviewers for suggestions that improved the presenta-
tion of the paper.

8. REFERENCES
[1] N. Abe, A. W. Biermann, and P. M. Long. Reinforcement learning

with immediate rewards and linear hypotheses. Algorithmica,
37(4):263–293, 2003.

[2] D. Agarwal, B.-C. Chen, and P. Elango. Explore/exploit schemes for
web content optimization. In Proc. of the 9th International Conf. on
Data Mining, 2009.

[3] D. Agarwal, B.-C. Chen, P. Elango, N. Motgi, S.-T. Park,

R. Ramakrishnan, S. Roy, and J. Zachariah. Online models for
content optimization. In Advances in Neural Information Processing
Systems 21, pages 17–24, 2009.

[4] R. Agrawal. Sample mean based index policies with o(log n) regret

for the multi-armed bandit problem. Advances in Applied
Probability, 27(4):1054–1078, 1995.

[5] A. Anagnostopoulos, A. Z. Broder, E. Gabrilovich, V. Josifovski, and

L. Riedel. Just-in-time contextual advertising. In Proc. of the 16th

ACM Conf. on Information and Knowledge Management, pages
331–340, 2007.

[6] P. Auer. Using conﬁdence bounds for exploitation-exploration

trade-offs. Journal of Machine Learning Research, 3:397–422, 2002.

[7] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the
multiarmed bandit problem. Machine Learning, 47(2–3):235–256,
2002.

[8] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The
nonstochastic multiarmed bandit problem. SIAM Journal on
Computing, 32(1):48–77, 2002.

[9] D. A. Berry and B. Fristedt. Bandit Problems: Sequential Allocation

of Experiments. Monographs on Statistics and Applied Probability.
Chapman and Hall, 1985.

[10] P. Brusilovsky, A. Kobsa, and W. Nejdl, editors. The Adaptive Web —

Methods and Strategies of Web Personalization, volume 4321 of
Lecture Notes in Computer Science. Springer Berlin / Heidelberg,
2007.

[11] R. Burke. Hybrid systems for personalized recommendations. In

B. Mobasher and S. S. Anand, editors, Intelligent Techniques for Web
Personalization. Springer-Verlag, 2005.

[12] W. Chu and S.-T. Park. Personalized recommendation on dynamic

content using predictive bilinear models. In Proc. of the 18th
International Conf. on World Wide Web, pages 691–700, 2009.

[13] W. Chu, S.-T. Park, T. Beaupre, N. Motgi, A. Phadke,

S. Chakraborty, and J. Zachariah. A case study of behavior-driven
conjoint analysis on Yahoo!: Front Page Today Module. In Proc. of
the 15th ACM SIGKDD International Conf. on Knowledge Discovery
and Data Mining, pages 1097–1104, 2009.

[14] A. Das, M. Datar, A. Garg, and S. Rajaram. Google news

personalization: scalable online collaborative ﬁltering. In Proc. of the
16th International World Wide Web Conf., 2007.

[15] J. Gittins. Bandit processes and dynamic allocation indices. Journal

of the Royal Statistical Society. Series B (Methodological),
41:148–177, 1979.

[16] S. M. Kakade, S. Shalev-Shwartz, and A. Tewari. Efﬁcient bandit

algorithms for online multiclass prediction. In Proc. of the 25th
International Conf. on Machine Learning, pages 440–447, 2008.

[17] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive

allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985.
[18] J. Langford and T. Zhang. The epoch-greedy algorithm for contextual
multi-armed bandits. In Advances in Neural Information Processing
Systems 20, 2008.

[19] D. J. C. MacKay. Information Theory, Inference, and Learning

Algorithms. Cambridge University Press, 2003.

[20] D. Mladenic. Text-learning and related intelligent agents: A survey.

IEEE Intelligent Agents, pages 44–54, 1999.

[21] S.-T. Park, D. Pennock, O. Madani, N. Good, and D. DeCoste. Naïve
ﬁlterbots for robust cold-start recommendations. In Proc. of the 12th
ACM SIGKDD International Conf. on Knowledge Discovery and
Data Mining, pages 699–705, 2006.

[22] N. G. Pavlidis, D. K. Tasoulis, and D. J. Hand. Simulation studies of

multi-armed bandits with covariates. In Proceedings on the 10th
International Conf. on Computer Modeling and Simulation, pages
493–498, 2008.

[23] D. Precup, R. S. Sutton, and S. P. Singh. Eligibility traces for

off-policy policy evaluation. In Proc. of the 17th Interational Conf.
on Machine Learning, pages 759–766, 2000.

[24] H. Robbins. Some aspects of the sequential design of experiments.

Bulletin of the American Mathematical Society, 58(5):527–535,
1952.

[25] J. B. Schafer, J. Konstan, and J. Riedi. Recommender systems in

e-commerce. In Proc. of the 1st ACM Conf. on Electronic Commerce,
1999.

[26] W. R. Thompson. On the likelihood that one unknown probability

exceeds another in view of the evidence of two samples. Biometrika,
25(3–4):285–294, 1933.

[27] T. J. Walsh, I. Szita, C. Diuk, and M. L. Littman. Exploring compact

reinforcement-learning representations with linear regression. In
Proc. of the 25th Conf. on Uncertainty in Artiﬁcial Intelligence, 2009.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA670