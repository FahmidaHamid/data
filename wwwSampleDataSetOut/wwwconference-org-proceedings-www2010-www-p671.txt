b-Bit Minwise Hashing

Ping Li∗

Department of Statistical Science

Faculty of Computing and Information Science

Cornell University,

Ithaca, NY 14853

pingli@cornell.edu

ABSTRACT
This paper establishes the theoretical framework of b-bit minwise hashing.
The original minwise hashing method has become a standard technique for
estimating set similarity (e.g., resemblance) with applications in informa-
tion retrieval, data management, computational advertising, etc.

By only storing b bits of each hashed value (e.g., b = 1 or 2), we gain
substantial advantages in terms of storage space. We prove the basic theo-
retical results and provide an unbiased estimator of the resemblance for any
b. We demonstrate that, even in the least favorable scenario, using b = 1
may reduce the storage space at least by a factor of 21.3 (or 10.7) compared
to b = 64 (or b = 32), if one is interested in resemblance ≥ 0.5.
Categories and Subject Descriptors
H.2.8 [Database Applications]: Data Mining
General Terms
Algorithms, Performance, Theory
INTRODUCTION
1.

Computing the size of set intersections is a fundamental problem
in information retrieval, databases, and machine learning. Given
two sets, S1 and S2, where

S1, S2 ⊆ Ω = {0, 1, 2, ..., D − 1},

a basic task is to compute the joint size a = |S1 ∩ S2|, which
measures the (un-normalized) similarity between S1 and S2. The
resemblance, denoted by R, is a normalized similarity measure:
, where f1 = |S1|, f2 = |S2|.

R =

|S1 ∩ S2|
|S1 ∪ S2| =

f1 + f2 − a

a

In large datasets encountered in information retrieval and databases,

efﬁciently computing the joint sizes is often highly challenging
[3, 18]. Detecting duplicate web pages is a classical example [4, 6].
Typically, each Web document can be processed as “a bag of
shingles,” where a shingle consists of w contiguous words in a doc-
ument. Here w is a tuning parameter and was set to be w = 5
in several studies [4, 6, 12]. Clearly, the total number of possible
shingles is huge. Considering merely 105 unique English words,
the total number of possible 5-shingles should be D = (105)5 =
O(1025). Prior studies used D = 264 [12] and D = 240 [4, 6].
1.1 Minwise Hashing

In their seminal work, Broder and his colleagues developed min-
wise hashing and successfully applied the technique to duplicate
∗

Supported by Microsoft, NSF-DMS and ONR-YIP.
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

Arnd Christian König

Microsoft Research
Microsoft Corporation
Redmond, WA 98052

chrisko@microsoft.com

Web page removal [4, 6]. Since then, there have been considerable
theoretical and methodological developments [5, 8, 19, 21–23, 26].
As a general technique for estimating set similarity, minwise
hashing has been applied to a wide range of applications, for ex-
ample, content matching for online advertising [30], detection of
large-scale redundancy in enterprise ﬁle systems [14], syntactic
similarity algorithms for enterprise information management [27],
compressing social networks [9], advertising diversiﬁcation [17],
community extraction and classiﬁcation in the Web graph [11],
graph sampling [29], wireless sensor networks [25], Web spam
[24,33], Web graph compression [7], and text reuse in the Web [2].
Here, we give a brief introduction to this algorithm. Suppose a

random permutation π is performed on Ω, i.e.,

π : Ω −→ Ω,

where Ω = {0, 1, ..., D − 1}.

An elementary probability argument shows that

Pr (min(π(S1)) = min(π(S2))) =

|S1 ∩ S2|
|S1 ∪ S2| = R.

(1)

After k minwise independent permutations, π1, π2, ..., πk, one

can estimate R without bias, as a binomial:

1
k

k(cid:2)
(cid:4)

j=1

=

ˆRM =

(cid:3)

Var

ˆRM

1{min(πj(S1)) = min(πj(S2))},

R(1 − R).

1
k

(2)

(3)

Throughout the paper, we frequently use the terms “sample” and
“sample size” (i.e., k). In minwise hashing, a sample is a hashed
value, min(πj (Si)), which may require e.g., 64 bits to store [12].

1.2 Our Main Contributions

In this paper, we establish a uniﬁed theoretical framework for
b-bit minwise hashing. Instead of using b = 64 bits [12] or 40
bits [4, 6], our theoretical results suggest using as few as b = 1 or
b = 2 bits can yield signiﬁcant improvements.

In b-bit minwise hashing, a sample consists of b bits only, as op-
posed to e.g., 64 bits in the original minwise hashing. Intuitively,
using fewer bits per sample will increase the estimation variance,
compared to (3), at the same sample size k. Thus, we will have to
increase k to maintain the same accuracy. Interestingly, our theo-
retical results will demonstrate that, when resemblance is not too
small (e.g., R ≥ 0.5, the threshold used in [4, 6]), we do not have
to increase k much. This means our proposed b-bit minwise hash-
ing can be used to improve estimation accuracy and signiﬁcantly
reduce storage requirements at the same time.

For example, when b = 1 and R = 0.5, the estimation variance
will increase at most by a factor of 3. In this case, in order not to
lose accuracy, we have to increase the sample size by a factor of

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA6713. If we originally stored each hashed value using 64 bits [12], the
improvement by using b = 1 will be 64/3 = 21.3.

Algorithm 1 illustrates the procedure of b-bit minwise hashing,

based on the theoretical results in Sec. 2.

Algorithm 1 The b-bit minwise hashing algorithm, applied to esti-
mating pairwise resemblances in a collection of N sets.
Input: Sets Sn ∈ Ω = {0, 1, ..., D − 1}, n = 1 to N.
Pre-processing:
1): Generate k random permutations πj : Ω → Ω, j = 1 to k.
2): For each set Sn and each permutation πj, store the lowest b bits of
(cid:6)(cid:7)b
min (πj (Sn)), denoted by en,i,j, i = 1 to b.
Estimation: (Use two sets S1 and S2 as an example.)
i=1 1{e1,i,πj
1): Compute ˆEb = 1
k
ˆEb−C1,b
2): Estimate the resemblance by ˆRb =
1−C2,b
are from Theorem 1 in Sec. 2.

(cid:8)
} = 1

, where C1,b and C2,b

(cid:5)k

= e2,i,πj

j=1

.

1.3 Comparisons with LSH Algorithms

Locality Sensitive Hashing (LSH) [8,20] is a set of techniques for
performing approximate search in high dimensions. In the context
of estimating set intersections, there exist LSH families for estimat-
ing the resemblance, the arccosine and the Hamming distance [1].
In [8, 16], the authors describe LSH hashing schemes that map
objects to {0, 1} (i.e., 1-bit schemes). The algorithms for the con-
struction, however, are problem speciﬁc. Two discovered 1-bit
schemes are the sign random projections (also known as simhash)
[8] and the Hamming distance LSH algorithm proposed by [20].
Our b-bit minwise hashing proposes a new construction, which
maps objects to {0, 1, ..., 2b − 1} instead of just {0, 1}. While our
major focus is to compare with the original minwise hashing, we
also conduct comparisons with the other two known 1-bit schemes.
1.3.1 Sign Random Projections

−1

a√

The method of sign (1-bit) random projections estimates the ar-
, using our notation for sets S1
ccosine, which is cos
and S2. A separate technical report is devoted to comparing b-bit
minwise hashing with sign (1-bit) random projections. See
www.stat.cornell.edu/~li/hashing/RP_minwise.pdf.
That report demonstrates that, unless the similarity level is very
low, b-bit minwise hashing outperforms sign random projections.

f1f2

The method of sign random projections has received signiﬁcant
attention in the context of duplicate detection. According to [28],
a great advantage of simhash over minwise hashing is the smaller
size of the ﬁngerprints required for duplicate detection. The space-
reduction of b-bit minwise hashing overcomes this issue.
1.3.2 The Hamming Distance LSH Algorithm

distance LSH algorithm developed in [20] (and surveyed in [1]):

Sec. 4 will compare b-bit minwise hashing with the Hamming
• When the Hamming distance LSH algorithm is implemented
naively, to achieve the same level of accuracy, its required
storage space will be many magnitudes larger than that of
b-bit minwise hashing in sparse data (i.e., |Si|/D is small).
• If we only store the non-zero locations in the Hamming dis-
tance LSH algorithm, then its required storage space will be
about one magnitude larger (e.g., 10 to 30 times).

2. THE FUNDAMENTAL RESULTS

Consider two sets, S1 and S2,

S1, S2 ⊆ Ω = {0, 1, 2, ..., D − 1},
f1 = |S1|, f2 = |S2|, a = |S1 ∩ S2|

(cid:3)

(cid:4)

Apply a random permutation π on S1 and S2: π : Ω −→ Ω. Deﬁne
the minimum values under π to be z1 and z2:

z1 = min (π (S1)) ,

z2 = min (π (S2)) .

Deﬁne e1,i = ith lowest bit of z1, and e2,i = ith lowest bit of z2.
Theorem 1 derives the main probability formula.

(cid:9)

(cid:11)
THEOREM 1. Assume D is large.
1{e1,i = e2,i} = 1

b(cid:10)

Eb = Pr

i=1

where

r1 =

f1
D

,

C1,b = A1,b

,

f2
D
+ A2,b

r2 =
r2

r1 + r2

r1

r1

r1 + r2

r2

r1 + r2

,

,

= C1,b + (1 − C2,b) R

(4)

(5)

(6)

C2,b = A1,b

+ A2,b

r1 + r2
r1 [1 − r1]
1 − [1 − r1]

2b−1

2b−1

r2 [1 − r2]
1 − [1 − r2]

,

2b

A2,b =

A1,b =
(7)
For a ﬁxed rj (where j ∈ {1, 2}), Aj,b is a monotonically de-
For a ﬁxed b, Aj,b is a monotonically decreasing function of rj ∈

creasing function of b = 1, 2, 3, ....

2b

.

[0, 1], reaching a limit:

Aj,b =

lim
rj→0

.

1
2b

(8)

Proof: See Appendix A.2

D and r2 = f2

Theorem 1 says that, for a given b, the desired probability (4) is
determined by R and the ratios, r1 = f1
D . The only
assumption needed in the proof of Theorem 1 is that D should be
large, which is always satisﬁed in practice.
Aj,b (j ∈ {1, 2}) is a decreasing function of rj and Aj,b ≤ 1
2b .
As b increases, Aj,b converges to zero very quickly. In fact, when
b ≥ 32, one can essentially view Aj,b = 0.
2.1 An Intuitive (Heuristic) Explanation

nation of Theorem 1. Consider b = 1. One might expect that

A simple heuristic argument may provide a more intuitive expla-
Pr (e1,1 = e2,1) =Pr (e1,1 = e2,1|z1 = z2) Pr (z1 = z2)
+Pr (e1,1 = e2,1|z1 (cid:9)= z2) Pr (z1 (cid:9)= z2)
??≈R +

(1 − R) =

1 + R

,

1
2

2

because when z1 and z2 are not equal, the chance that their last bits
are equal “may be” approximately 1
2 . This heuristic argument is
actually consistent with Theorem 1 when r1, r2 → 0. According to
(8), as r1, r2 → 0, we have A1,1, A2,1 → 1
2 , and C1,1, C2,1 → 1
also; and hence the probability (4) approaches 1+R
2 .

In practice, when a very accurate estimate is not necessary, one
might actually use this approximate formula to simplify the estima-
tor. The errors, however, could be quite noticeable when r1, r2 are
not negligible; see Sec. 5.2.
2.2 The Unbiased Estimator

2

Theorem 1 suggests an unbiased estimator ˆRb for R:

ˆRb =

ˆEb =

,

ˆEb − C1,b
(cid:12)
1 − C2,b
b(cid:10)
k(cid:2)
1
k

j=1

i=1

(cid:13)
1{e1,i,πj = e2,i,πj} = 1

,

(9)

(10)

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA672(cid:3)

(cid:4)
ˆEb
Var
[1 − C2,b]
(cid:4)
(cid:3)

2.3 The Variance-Space Trade-off

As we decrease b, the space needed for storing each “sample”
will be smaller; the estimation variance (11) at the same sample
size k, however, will increase. This variance-space trade-off can be
precisely quantiﬁed by the storage factor B(b; R, r1, r2):

B(b; R, r1, r2) = b × Var
b [C1,b + (1 − C2,b)R] [1 − C1,b − (1 − C2,b)R]

× k

ˆRb

=

[1 − C2,b]

2

.

(12)

(cid:3)

(cid:4)

Lower B(b) is better. The ratio, B(b1;R,r1,r2)
B(b2;R,r1,r2) , measures the im-
provement of using b = b2 (e.g., b2 = 1) over using b = b1 (e.g.,
b1 = 64). Some algebra yields the following Theorem.

THEOREM 2. If r1 = r2 and b1 > b2, then
A1,b1 (1 − R) + R
A1,b2 (1 − R) + R

B(b1; R, r1, r2)
B(b2; R, r1, r2)

b1
b2

=

1 − A1,b2
1 − A1,b1

,

(13)

is a monotonically increasing function of R ∈ [0, 1].

If R → 1 (which implies r1 → r2), then

B(b1; R, r1, r2)
B(b2; R, r1, r2)

→ b1
b2

1 − A1,b2
1 − A1,b1

.

(14)

If r1 = r2, b2 = 1, b1 ≥ 32 (hence we treat A1,b = 0), then

B(b1; R, r1, r2)
B(1; R, r1, r2)

R + 1 − r1
Proof: We omit the proof due to its simplicity.2

= b1

R

(15)

Suppose the original minwise hashing used 64 bits to store each
sample, then the maximum improvement of b-bit minwise hashing
would be 64-fold, attained when r1 = r2 = 1 and R = 1, accord-
ing to (15). In the least favorable situation, i.e., r1, r2 → 0, the
improvement will still be 64

3 = 21.3-fold when R = 0.5.

Fig. 1 plots B(64)

B(b) , to directly visualize the relative improve-
ments, which are consistent with what Theorem 2 predicts. The
plots show that, when R is very large (which is the case in many
practical applications), it is always good to use b = 1. However,
when R is small, using larger b may be better. The cut-off point
depends on r1, r2, R. For example, when r1 = r2 and both are
small, it would be better to use b = 2 than b = 1 if R < 0.4, as
shown in Fig. 1.

where e1,i,πj (e2,i,πj ) denotes the ith lowest bit of z1 (z2), under
the permutation πj. Following property of binomial distribution,

(cid:3)

(cid:4)

ˆRb

=

Var
[C1,b + (1 − C2,b)R] [1 − C1,b − (1 − C2,b)R]

2 =

Eb(1 − Eb)
[1 − C2,b]

2

1
k

t

n
e
m
e
v
o
r
p
m

I

(11)

=

1
k

(cid:3)

[1 − C2,b]

2

ˆRb

(cid:4)

For large b, Var
mator for the original minwise hashing:
R(1 − R)
(cid:4)

=
In fact, when b ≥ 32, Var
indistinguishable for practical purposes.

b→∞ Var
lim

(cid:3)

ˆRb

ˆRb

k

converges to the variance of ˆRM , the esti-

(cid:4)

(cid:3)
(cid:4)

ˆRM

= Var

(cid:3)

.

t

n
e
m
e
v
o
r
p
m

I

and Var

ˆRM

are numerically

35

30

25

20

15

10

5

0
0
60

50

40

30

20

10

r1 = r2 = 10−10

0.2

0.4

0.6

Resemblance (R)

r1 = r2 = 0.5

b = 1

b = 2

b = 3

b = 4

0.8

1

b = 1

b = 2

b = 3

b = 4

35

30

25

20

15

10

5

0
0
60

50

40

30

20

10

t

n
e
m
e
v
o
r
p
m

I

t

n
e
m
e
v
o
r
p
m

I

r1 = r2 = 0.1

0.2

0.4

0.6

Resemblance (R)

b = 1

b = 2
b = 3
b = 4

0.8

1

b = 1

b = 2

b = 3

b = 4

0
0

0.2

0.4

0.6

0.8

1

r1 = r2 = 0.9
0.2
0.4

0
0

0.6

0.8

1

Resemblance (R)
Figure 1: B(64)
B(b) , the relative storage improvement of using b =
1, 2, 3, 4 bits, compared to using 64 bits. B(b) is deﬁned in (12).

Resemblance (R)

3. EXPERIMENTS

Experiment 1 is a sanity check, to verify: (A) our proposed
estimator ˆRb in (9), is indeed unbiased; and (B) its variance follows
the prediction by our formula in (11).

Experiment 2 is a duplicate detection task using a Microsoft

proprietary collection of 1,000,000 news articles.

Experiment 3 is another duplicate detection task using 300,000

UCI NYTimes news articles.
3.1 Experiment 1

The data, extracted from Microsoft Web crawls, consists of 10
pairs of sets (i.e., total 20 words). Each set consists of the document
IDs which contain the word at least once. Thus, this experiment is
for estimating word associations.

Table 1: Ten pairs of words used in Experiment 1. For example,
“KONG” and “HONG” correspond to the two sets of document IDs
which contained word “KONG” and word “HONG” respectively.

Word 1

Word 2

KONG
RIGHTS
OF
GAMBIA
UNITED
SAN
CREDIT
TIME
LOW
A

HONG
RESERVED
AND
KIRIBATI
STATES
FRANCISCO
CARD
JOB
PAY
TEST

r1
0.0145
0.187
0.570
0.0031
0.062
0.049
0.046
0.189
0.045
0.596

r2
0.0143
0.172
0.554
0.0028
0.061
0.025
0.041
0.05
0.043
0.035

R

0.925
0.877
0.771
0.712
0.591
0.476
0.285
0.128
0.112
0.052

B(32)
B(1)
15.5
16.6
20.4
13.3
12.4
10.7
7.3
4.3
3.4
3.1

B(64)
B(1)
31.0
32.2
40.8
26.6
24.8
21.4
14.6
8.6
6.8
6.2

B(1) and B(64)

Table 1 summarizes the data and also provides the theoretical im-
provements, B(32)
B(1) . The words were selected to include
highly frequent word pairs (e.g., “OF-AND”), highly rare word
pairs (e.g., “GAMBIA-KIRIBATI”), highly unbalanced pairs (e.g.,
”A-Test”), highly similar pairs (e.g, “KONG-HONG”), as well as
word pairs that are not quite similar (e.g., “LOW-PAY”).

We estimate the resemblance using the original minwise hashing

estimator ˆRM and the b-bit estimator ˆRb (b = 1, 2, 3).

3.1.1 Validating the Unbiasedness

Figure 2 presents the estimation biases for the selected 2 word
pairs. Theoretically, both estimators, ˆRM and ˆRb, are unbiased
(i.e., the y-axis in Figure 2 should be zero, after an inﬁnite number
of repetitions). Figure 2 veriﬁes this fact because the empirical
biases are all very small and no systematic biases can be observed.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA673x 10−4
b=3

M

b=1

4

2

0

−2

−4

−6

s
a
B

i

 

−8
101

KONG − HONG

b=2

b=1

x 10−4

b=3

M

 

s
a
B

i

5

0

−5

−10

b=1

M
b = 1
b = 2
b = 3
M

b=2

M

b=2

b=1

A − TEST

 

b = 1
b = 2
b = 3
M

102

Sample size k

103

 

101

102

Sample size k

103

Figure 2: Empirical biases from 25000 simulations at each sam-
ple size k. “M” denotes the original minwise hashing.

3.1.2 Validating the Variance Formula

Figure 3 plots the empirical mean square errors (MSE = variance
+ bias2) in solid lines, and the theoretical variances (11) in dashed
lines, for 6 word pairs (instead of 10 pairs, due to the space limit).
All dashed lines are invisible because they overlap with the cor-
responding solid curves. Thus, this experiment validates that the
variance formula (11) is accurate and ˆRb is indeed unbiased (oth-
erwise, MSE will differ from the variance).

)

E
S
M

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

)

E
S
M

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

)

E
S
M

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

10−2

10−3

b=1
2

3

M

10−4

 

101

KONG − HONG

102

Sample size k

10−2

10−3

b=1
2
M

OF − AND

10−4

 

101

10−1

10−2

10−3

10−4

 

101

102

Sample size k

b=1

b=2

3
M

LOW − PAY

102

Sample size k

 

b = 1
b = 2
b = 3
M
Theor.

)

E
S
M

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

103

 

b = 1
b = 2
b = 3
M
Theor.

)

E
S
M

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

103

 

b = 1
b = 2
b = 3
M
Theor.

)

E
S
M

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

10−2

10−3

b=1
2

M

 

b = 1
b = 2
b = 3
M
Theor.

RIGHTS − RESERVED

10−4

 

101

10−1

10−2

10−3

10−4

 

101

10−1

10−2

10−3

102

Sample size k

103

 

b = 1
b = 2
b = 3
M
Theor.

103

 

b = 1
b = 2
b = 3
M
Theor.

b=1
2

M

3

GAMBIA − KIRIBATI

102

Sample size k

b=1
b=2

b=3
M

10−4

A − TEST

103

 

101

102

Sample size k

103

Figure 3: Mean square errors (MSEs). “M” denotes the orig-
inal minwise hashing. “Theor.” denotes the theoretical vari-
ances of Var( ˆRb)(11) and Var( ˆRM )(3). The dashed curves,
however, are invisible because the empirical MSEs overlapped
the theoretical variances. At the same k, Var( ˆR1) > Var( ˆR2) >
Var( ˆR3) > Var( ˆRM ). However, ˆR1 ( ˆR2) only requires 1 bit (2
bits) per sample, while ˆRM requires 32 or 64 bits.

3.2 Experiment 2: Microsoft News Data

To illustrate the improvements by the use of b-bit minwise hash-
ing on a real-life application, we conducted a duplicate detection
experiment using a corpus of 106 news documents. The dataset
was crawled as part of the BLEWS project at Microsoft [15]. We
computed pairwise resemblances for all documents and retrieved
documents pairs with resemblance R larger than a threshold R0.

We estimate the resemblances using ˆRb with b = 1, 2, 4 bits, and

the original minwise hashing (using 32 bits). Figure 4 presents the
precision & recall curves. The recall values (bottom two panels in
Figure 4) are all very high and do not differentiate the estimators.

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

l
l

a
c
e
R

 

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0

2

b=2

b=1

R0 = 0.3

 

b=1
b=2
b=4
M

i

i

n
o
s
c
e
r
P

200

300

Sample size (k)

400

500

 

i

i

n
o
s
c
e
r
P

R0 = 0.5

b=1
b=2
b=4
M

200

300

Sample size (k)

400

500

 

100

2

b=1

100

b=1

i

i

n
o
s
c
e
r
P

R0 = 0.7

b=1
b=2
b=4
M

100

100

200

300

Sample size (k)

Recall

R0 = 0.3

200

300

Sample size (k)

400

500

 

l
l

a
c
e
R

b=1
b=2
b=4
M

400

500

 

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0

2

2

b=1

R0 = 0.4

100

b=1

2

100

b=1

100

200

300

Sample size (k)

R0 = 0.6

200

300

Sample size (k)

R0 = 0.8

200

300

Sample size (k)

Recall

R0 = 0.8

100

200

300

Sample size (k)

 

b=1
b=2
b=4
M

400

500

 

b=1
b=2
b=4
M

400

500

 

b=1
b=2
b=4
M

400

500

 

b=1
b=2
b=4
M

400

500

Figure 4: Microsoft collection of news data. The task is to re-
trieve news article pairs with resemblance R ≥ R0. The recall
curves (bottom two panels) indicate all estimators are equally
good (in recalls). The precision curves are more interesting for
differentiating estimators. For example, when R0 = 0.4 (top
right panel), in order to achieve a precision = 0.80, the estima-
tors ˆRM , ˆR4, ˆR2, and ˆR1 require k = 50, 50, 75, 145 samples,
respectively, indicating ˆR4, ˆR2, and ˆR1 respectively improve
ˆRM by 8-fold, 10.7-fold, and 11-fold.

The precision curves for ˆR4 (using 4 bits per sample) and ˆRM
(using 32 bits per sample) are almost indistinguishable, suggesting
a 8-fold improvement in space using b = 4.

When using b = 1 or 2, the space improvements are normally
around 10-fold to 20-fold, compared to ˆRM , especially for achiev-
ing high precisions (e.g., ≥ 0.9). This experiment again conﬁrms
the signiﬁcant improvement of the b-bit minwise hashing using
b = 1 (or 2). Table 2 summarizes the relative improvements.

In this experiment, ˆRM only used 32 bits per sample. For even
larger applications, however, 64 bits per sample may be necessary
[12]; and the improvements of ˆRb will be even more signiﬁcant.

Note that in the context of (Web) document duplicate detection,
in addition to shingling, a number of specialized hash-signatures
have been proposed, which leverage properties of natural-language

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA674k coordinates are randomly sampled from Ω = {0, 1, ..., D − 1}.
We denote the samples of yi by hi, where hi = {hij , j = 1 to k}
is a k-dimensional vector. These samples will be used to estimate
the Hamming distance H (using S1, S2 as an example):

H =

[y1i (cid:9)= y2i] = |S1 ∪ S2| − |S1 ∩ S2| = f1 + f2 − 2a.

D−1(cid:2)

i=0

Using the samples h1 and h2, an unbiased estimator of H is simply

k(cid:2)

j=1

ˆH =

D
k

[h1j (cid:9)= h2j ] ,
2(cid:16) − E2

(16)

(cid:17)

([h1j (cid:9)= h2j ])
(cid:9)(cid:5)D−1

i=0 [y1i (cid:9)= y2i]

(cid:11)2(cid:19)

2

−

D

whose variance would be

(cid:3)

(cid:4)

Var

ˆH

=

=

k

E

(cid:14)
(cid:15)
[h1j (cid:9)= h2j ]
(cid:18)(cid:5)D−1
i=0 [y1i (cid:9)= y2i]
(cid:20)

(cid:21)

D
− H 2
D2

.

H
D

D2
k2

D2
k

D2
k

=

(cid:4)

(cid:3)

(17)
The above analysis assumes k (cid:11) D (which is satisﬁed in prac-
in (17) by D−k
tice); otherwise one should multiply the Var
D−1 ,
the “ﬁnite sample correction factor.”
It would be interesting to
compare ˆH with b-bit minwise hashing. In order to estimate H, we
need to convert the resemblance estimator ˆRb (9) to ˆHb:
ˆHb = f1 + f2 − 2

(f1 + f2). (18)

(f1 + f2) =

ˆRb

ˆH

1 + ˆRb

The variance of ˆHb can be computed from Var

ˆRb

(11) using the

“delta method” in statistics (note that

(cid:3)

(cid:4)

Var

ˆHb

=Var

ˆRb

(f1 + f2)

2

(cid:3)
(cid:3)

(cid:4)
(cid:4)

=Var

ˆRb

4(r1 + r2)2
(1 + R)4

D2

+ O

−2

(1+x)2 ):

(cid:25)2
(cid:24)

1
k2

(cid:25)

(cid:24)

1
k2

(19)

+ O

(cid:25)

.

(cid:4)

1 − ˆRb
(cid:3)
1 + ˆRb
(cid:23)(cid:6)
(cid:22)
(cid:24) −2

1−x
1+x

=

(1 + R)2

Recall ri = fi/D. To verify the variances in (17) and (19), we
conduct experiments using the same data as in Experiment 1. This
time, we estimate H instead of R, using both ˆH (16) and ˆHb (18).
Figure 6 reports the mean square errors, together with the the-
oretical variances (17) and (19). We can see that the theoretical
variance formulas are accurate. When the data is not dense, the
estimator ˆHb (18) given by b-bit minwise hashing is much more
accurate than the estimator ˆH (16). However, when the data is
dense (e.g., “OF-AND”), ˆH could still outperform ˆHb.

We now compare the actual storage needed by ˆHb and ˆH. We

deﬁne the following two ratios to make fair comparisons:

(cid:3)

(cid:4)
(cid:3)

× k
× bk

Var

, Gb =

ˆH

Var

(cid:4)
× r1+r2
× bk

2

ˆHb

64k

.

(20)

(cid:3)
(cid:3)

(cid:4)
(cid:4)

Var

ˆH

Var

ˆHb

text (such as the placement of stopwords [31]). However, our ap-
proach is not aimed at any speciﬁc type of data, but is a general,
domain-independent technique. Also, to the extent that other ap-
proaches rely on minwise hashing for signature computation, these
may be combined with our techniques.

Table 2: Relative improvement (in space) of ˆRb (using b bits per sam-
ple) over ˆRM (32 bits per sample). For precision = 0.9, 0.95, we ﬁnd the
required sample sizes (from Figure 4) for ˆRM and ˆRb and use them to
estimate the required storage in bits. The values in the table are the
ratios of the storage costs. The improvements are consistent with the
theoretical predictions in Figure 1.

R0

Precision = 0.9
b = 1 2
4
8.8
0.3 — 5.7
8.3
10.0
0.4
0.5
12.7
8.4
8.6
11.7
0.6
9.6
14.8
0.7
8.0
10.3
0.8
0.9
14.0
10.7

9.2
10.8
12.9
16.0
17.4
16.6

4

Precision = 0.95
b = 1 2
— — 7.1
8.2
— 10.0
8.2
10.1
7.7
8.5
10.5 12.4
7.6
15.4 12.7
7.7
18.7 14.2
23.0 17.6
9.7

3.3 Experiment 3: UCI NYTimes Data

We conducted another duplicate detection experiment on a pub-
lic (UCI) collection of 300,000 NYTimes articles. The purpose is
to ensure that our experiment will be repeatable by those who can
not access the proprietary data in Experiment 2.

Figure 5 presents the precision curves for representative thresh-
old R0’s. The recall curves are not shown because they could not
differentiate estimators, just like in Experiment 1. The curves con-
ﬁrm again that using b = 1 or b = 2 bits, ˆRb could improve the
original minwise hashing (using 32 bits per sample) by a factor of
10 or more. The curves for ˆRb with b = 4 almost always overlap
with the curves for ˆRM , verifying an expected 8-fold improvement.

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

 

2

 

b=1
b=2
b=4
M

R0 = 0.5

i

i

n
o
s
c
e
r
P

200

300

Sample size (k)

400

500

 

2

b=1

100

b=1

i

i

n
o
s
c
e
r
P

R0 = 0.7

b=1
b=2
b=4
M

100

200

300

400

500

Sample size (k)

2

2

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

 

b=1

100

b=1

100

R0 = 0.6

200

300

Sample size (k)

R0 = 0.8

200

300

Sample size (k)

 

b=1
b=2
b=4
M

400

500

 

b=1
b=2
b=4
M

400

500

Figure 5: UCI collection of NYTimes data. The task is to re-
trieve news article pairs with resemblance R ≥ R0.

Wb =

4. COMPARISONS WITH THE HAMMING

DISTANCE LSH ALGORITHM

The Hamming distance LSH algorithm proposed in [20] is an
inﬂuential 1-bit LSH scheme. In this algorithm, a set Si, is mapped
into a D-dimensional binary vector, yi:

yit = 1, if t ∈ Si; yit = 0, otherwise.

Wb and Gb are deﬁned in the same spirit as the ratio of the storage
factors introduced in Sec. 2.3. Recall each sample of b-bit minwise
hashing requires b bits (i.e., bk bits per set). If we assume each
sample in the Hamming distance LSH requires 1 bit, then Wb in
(20) is a fair indicator and Wb > 1 means ˆHb outperforms ˆH.

However, as can be veriﬁed in Fig. 6 and Fig 7, when r1 and r2
are small (which is usually the case in practice), Wb tends to be very

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA675)

E
S
M

 

(
 
r
o
r
r
e
e
r
a
u
q
s
 
n
a
e
M

)

E
S
M

 

(
 
r
o
r
r
e
e
r
a
u
q
s
 
n
a
e
M

102

101

100

10−1

10−2

 

101

101

100

10−1

10−2

10−3

 

101

KONG − HONG

 

H

b=1

b=2

102
103
Sample size k

104

 

101

 

)

E
S
M

 

(
 
r
o
r
r
e
e
r
a
u
q
s
 
n
a
e
M

101

100

10−1

10−2

10−3

 

101

UNITED−STATES

H

b=1

b=2

102
103
Sample size k

104

100

10−1

10−2

10−3

b=1

OF − AND

 

b=2

H

large, when r1, r2 are small. However, when r1 is very large (e.g.,
0.9), it is possible that W1 < 1, meaning that the Hamming dis-
tance LSH could still outperform b-bit minwise in dense data.

By only storing the non-zero locations, Figure 7 illustrates that
b-bit minwise hashing will outperform the Hamming distance LSH
algorithm, usually by a factor of 10 (for small R) to 30 (for large R
and r1 ≈ r2).

)

E
S
M

 

(
 
r
o
r
r
e
e
r
a
u
q
s
 
n
a
e
M

102
103
Sample size k

104

 

LOW − PAY

5. DISCUSSIONS
5.1 Computational Overhead

The previous results establish the signiﬁcant reduction in storage
requirements possible using b-bit minwise hashing. This section
demonstrates that these also translate into signiﬁcant improvements
in computational overhead in the estimation phrase. The compu-
tational cost in the preprocessing phrase, however, will increase.

H

b=1

b=2

102
103
Sample size k

104

5.1.1 Preprocessing Phrase

Figure 6: MSEs (normalized by H 2), for comparing ˆH (16)
with ˆHb (18). In each panel, three solid curves stand for ˆH (la-
beled by “H”), ˆH1 (by ”b=1”), and ˆH2 (by ”b=2”), respectively.
The dashed lines are the corresponding theoretical variances
(17) and (19), which are largely overlapped by the solid lines.
When the sample size k is not large, the empirical MSEs of ˆHb
deviate from the theoretical variances, due to the bias caused
by the nonlinear transformation of ˆHb from ˆRb in (18).

large, indicating a highly signiﬁcant improvement of b-bit minwise
hashing over the Hamming distance LSH algorithm in [20].

We consider in practice one will most likely implement the al-
gorithm by only storing non-zero locations. In other words, for set
Si, only ri × k locations need to be stored (each is assumed to use
64 bits). Thus, the total bits on average will be r1+r2
64k (per set).
In fact, we have the following Theorem for Gb when r1, r2 → 0.
THEOREM 3. Consider r1, r2 → 0, and Gb as deﬁned in (20).

2

(cid:4)
(cid:3)
b − 1
2
2b − 1
2b
Proof: We omit the proof due to its simplicity. 2
Figure 7 plots W1 and G1, for r1 = r2 = 10

then Gb → 8
b
then Gb → 64
b

If R → 0,
If R → 1,

.

−4, 0.001,
0.01, 0,1 (which are probably reasonable in practice), as well as
r1 = r2 = 0.9 (as a sanity check). Note that, not all combinations
of r1, r2, R are possible. For example, when r1 = r2 = 1, then R
has to be 1.

−6, 10

.

(21)

(22)

1

W

106
105
104
103
102
101
100
10−1
0

W1,  r2 = r1

r1 = 1e−6
1e−4
0.001

0.01
0.1

0.2

0.4

0.6

Resemblance (R)

r1 = 0.9
0.8

1

60

50

40

1

G

30

20

10

0
0

G1,  r2 = r1

r1 = 0.9

r1 = 1e−6 to 0.1

0.2

0.4

0.6

0.8

1

Resemblance (R)

−6, 10

Figure 7: W1 and G1 as deﬁned in (20). We consider r1 =
−4, 0.001, 0.01, 0.1, 0.9. Note that not all combinations
10
of (r1, r2, R) are possible. The plot for G1 also veriﬁes the the-
oretical limits proved in Theorem 3.

Figure 7 conﬁrms our theoretical results. W1 will be extremely

In the preprocessing phrase, we need to generate minwise hash-
ing functions and apply them to all the sets for creating ﬁngerprints.
This phrase is actually fairly fast [4] and is usually done off-line,
incurring a one-time cost. Also, sets can be individually processed,
meaning that this step is easy to parallelize.

The computation required for b-bit minwise hashes differs from
the computation of traditional minwise hashes in two respects: (A)
we require a larger number of (smaller-sized) samples, in turn re-
quiring more hashing and (B) the packing of b-bit samples into 64-
bit (or 32-bit) words requires additional bit-manipulation.

It turns out the overhead for (B) is small and the overall compu-
tation time scales nearly linearly with k; see Fig. 8. As we have
analyzed, b-bit minwise hashing only requires increasing k by a
small factor such as 3. Therefore, we consider the overhead in the
preprocessing stage not to be a major issue. Also, it is important to
note that b-bit minwise hashing provides the ﬂexibility of trading
storage with preprocessing time by using b > 1.

)
c
e
s
(
 
e
m
T

i

x 104

4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
 
50

32 bits
1 bit

FP

 

4−U

100
250
Sample size k (# hashing)

150

200

2−U

300

Figure 8: Running time in the preprocessing phrase on 100K
news articles. 3 hashing functions were used: 2-universal hash-
ing (labeled by “2-U”), 4-universal hashing (labeled by “4-U”),
and full permutations (labeled by “FP”). Experiments with
1-bit hashing are reported in 3 dashed lines, which are only
slightly higher (due to additional bit-packing) than their corre-
sponding solid lines (the original minwise hashing using 32-bit).

The experiment in Fig. 8 was conducted on 100K articles from
the BLEWS project [15]. We considered 3 hashing functions: ﬁrst,
2-universal hash functions (computed using the fast universal hash-
ing scheme described [10]); second, 4-universal hash-functions (com-
puted using the CWtrick algorithm of [32]); and ﬁnally full ran-
dom permutations (computed using the Fisher-Yates shufﬂe [13]).

5.1.2 Estimation Phrase
We have shown earlier that, when R ≥ 0.5 and b = 1, we expect
a storage reduction of at least a factor of 21.3, compared to using

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA67664 bits.
computational overhead of the estimation.

In the following, we will analyze how this impacts the

Here, the key operation is the computation of the number of iden-
tical b-bit samples. While standard hash signatures that are multi-
ples of 16-bit can easily be compared using a single machine in-
struction, efﬁciently computing the overlap between b-bit samples
for small b is less straightforward. In the following, we will de-
scribe techniques for computing the number of identical b-bit sam-
ples when these are stored in a compact manner, meaning that in-
dividual b-bit samples e1,i,j and e2,i,j, i = 1, . . . , b, j = 1, . . . k
are packed into arrays Al[1, . . . , k·b
w ], l = 1, 2 of w-bit words. To
compute the number of identical b-bit samples, we iterate through
the arrays; for an each offset h, we ﬁrst compute v = A1[h] ⊕
A2[h], where ⊕ denotes the bitwise-XOR. Subsequently, the h-th
bit of v will be set if and only if the h-th bits in A1[h] and A2[h] are
different. Hence, to compute the number of overlapping b-bit sam-
ples encoded in A1[h] and A2[h], we need to compute the number
of b-bit blocks ending at offsets divisible by b that only contain 0s.
The case of b = 1 corresponds to the problem of counting the
number of 0-bits in a word. We tested different methods suggested
in [34] and found the fastest approach to be pre-computing an array
bits[1, . . . , 216], such that bits[t] corresponds to the number of 0-
bits in the binary representation of t. Then we can compute the
number of 0-bits in v (in case of w = 32) as

c = bits[v & 0xffffu] + bits[(v (cid:13) 16) & 0xffffu].

Interestingly, we can use the same method for the cases where
b > 1, as we only need to modify the values stored in bits, set-
ting bits[i] to the number of b-bit blocks that only contain 0-bits in
the binary representation of i.

We evaluated this approach using a loop computing the number
of identical samples in two signatures covering a total of 1.8 billion
32-bit words (using a 64-bit Intel 6600 Processor). Here, the 1-
bit hashing requires 1.67x the time that the 32-bit minwise hashing
requires.The results were essentially identical for b = 2.

Combined with the reduction in overall storage (for a given ac-
curacy level), this means a signiﬁcant speed improvement in the
estimation phase: suppose in the original minwise hashing, each
sample is stored using 64 bits. If we use 1-bit minwise hashing and
consider R > 0.5, our previous analysis has shown that we could
gain a storage reduction at least by a factor of 64/3 = 21.3 fold.
The improvement in computational efﬁciency would be 21.3/1.67
= 12.8 fold, which is still signiﬁcant.
5.2 Reducing Storage Overhead for r1 and r2
D and
D . The storage cost could be a concern if r1 (r2) must be

The unbiased estimator ˆRb (9) requires knowing r1 = f1

r2 = f1
represented with a high accuracy (e.g., 64 bits).

This section illustrates that we only need to quantize r1 and r2
into Q levels, where Q = 24 is probably good enough and Q = 28
is more than sufﬁcient. In other words, for each set, we only need
to increase the total storage by 4 bits or 8 bits, which are negligible.
For simplicity, we carry out the analysis for b = 1 and r1 =
r2 = r. In this case, A1,1 = A2,1 = C1,1 = C2,1 = 1−r
2−r , and the
correct estimator, denoted by ˆR1,r would be
(cid:4)
ˆR1,r = (2 − r) ˆE1 − (1 − r),
Bias

(cid:3)

(cid:4)

(cid:3)

ˆR1,r

(cid:3)

(cid:4)

ˆR1,r

− R = 0,
= E
(1 − r + R)(1 − R)

.

k

Var

ˆR1,r

=

See the deﬁnition of ˆE1 in (10). Now, suppose we only store an

approximate value of r, denoted by ˜r. The corresponding (approx-
imate) estimator is denoted by ˆR1,˜r:
(cid:4)
ˆR1,˜r = (2 − ˜r) ˆE1 − (1 − ˜r),

(cid:4)

(cid:3)

(cid:3)

Bias

ˆR1,˜r

(cid:3)

(cid:4)

ˆR1,˜r

− R =
= E
(1 − r + R)(1 − R)

Var

ˆR1,˜r

=

k

,

(˜r − r)(1 − R)
2 − r
(2 − ˜r)2
(2 − r)2
.

Thus, the (absolute) bias is upper bounded by |˜r−r| (in the worst
case, i.e., R = 0 and r = 1). Using Q = 24 levels of quantization,
the bias is bounded by 1/16 = 0.0625. In a reasonable situation,
e.g., R ≥ 0.5, the bias will be much smaller than 0.0625. Of
course, if we increase the quantization levels to Q = 28, the bias
(< 1/256 = 0.0039) will be negligible, even in the worst case.

Similarly, by examining the difference of the variances,

(cid:4)

(cid:3)

− Var

(cid:4)(cid:26)(cid:26)(cid:26)

(cid:26)(cid:26)(cid:26)Var

(cid:3)
|˜r − r|

=

ˆR1,˜r

ˆR1,r
(1 − r + R)(1 − R)

(4 − ˜r − r)
(2 − r)2
we can see that Q = 28 would be more than sufﬁcient.
5.3 Combining Bits for Enhancing Performance

k

,

Our theoretical and empirical results have conﬁrmed that, when
the resemblance R is reasonably high, each bit per sample may con-
tain strong information for estimating the similarity. This naturally
leads to the conjecture that, when R is close to 1, one might further
improve the performance by looking at a combination of multiple
bits (i.e., “b < 1”). One simple approach is to combine two bits
from two permutations using XOR (⊕) operations.

Recall e1,1,π denotes the lowest bit of the hashed value under π.

Theorem 1 has proved that

E1 = Pr (e1,1,π = e2,1,π) = C1,1 + (1 − C2,1) R

Consider two permutations π1 and π2. We store

x1 = e1,1,π1 ⊕ e1,1,π2 ,

x2 = e2,1,π1 ⊕ e2,1,π2

Then x1 = x2 either when e1,1,π1 = e2,1,π1 and e1,1,π2 = e2,1,π2 ,
or, when e1,1,π1 (cid:9)= e2,1,π1 and e1,1,π2 (cid:9)= e2,1,π2 . Thus
2,

T = Pr (x1 = x2) = E2

1 + (1 − E1)

(23)

which is a quadratic equation with a solution
2T − 1 + 1 − 2C1,1

√

R =

2 − 2C2,1

.

(24)

We can estimate T without bias as a binomial. The resultant es-
timator for R will be biased, at small sample size k, due to the
nonlinearity. We will recommend the following estimator

(cid:27)

max{2 ˆT − 1, 0} + 1 − 2C1,1

ˆR1/2 =

2 − 2C2,1

(25)
The truncation max{ . , 0} will introduce further bias; but it is nec-
essary and is usually a good bias-variance trade-off. We use ˆR1/2
to indicate that two bits are combined into one. The asymptotic
variance of ˆR1/2 can be derived using the “delta method”

.

(cid:3)

(cid:4)

Var

ˆR1/2

=

T (1 − T )

1
k

4(1 − C2,1)2(2T − 1)

+ O

(cid:24)

(cid:25)

1
k2

.

(26)

Note that each sample is still stored using 1 bit, despite that we use
“b = 1/2” to denote this estimator.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA677Interestingly, as R → 1, ˆR1/2 does twice as well as ˆR1:

(cid:3)
(cid:3)

(cid:4)
(cid:4) = lim

R→1

Var

ˆR1

lim
R→1

Var

ˆR1/2

2(1 − 2E1)2
(1 − E1)2 + E2

1

= 2.

(27)

(Recall, if R = 1, then r1 = r2, C1,1 = C2,1, and E1 = C1,1 +
1 − C2,1 = 1.) On the other hand, ˆR1/2 may not be good when R
is not too large. For example, one can numerically show that

(cid:3)

(cid:4)

(cid:3)

(cid:4)

Var

ˆR1

< Var

ˆR1/2

,

if R < 0.5774, r1, r2 → 0

Figure 9 plots the empirical MSEs for four word pairs in Ex-
periment 1, for ˆR1/2, ˆR1, and ˆRM . For the highly similar pair,
“KONG-HONG,” ˆR1/2 exhibits superior performance compared to
ˆR1. For the fairly similar pair, “OF-AND,” ˆR1/2 is still consid-
erably better. For “UNITED-STATES,” whose R = 0.591, ˆR1/2
performs similarly to ˆR1.
For “LOW-PAY,” whose R = 0.112
only, the theoretical variance of ˆR1/2 is very large. However, ow-
ing to the truncation in (25) (i.e., the variance-bias trade-off), the
empirical performance of ˆR1/2 is not too bad.

)

E
S
M

(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

10−2

10−3

10−4

 

101

10−1

)

E
S
M

(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

M 1/2

b=1

KONG − HONG

102

Sample size k

b = 1/2

10−2

10−3

b=1

M

UNITED − STATES

 

b = 1
b = 1/2
M
Theor.

103

 

b = 1
b = 1/2
M
Theor.

)

E
S
M

(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

)

E
S
M

(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

10−2

10−3

b = 1/2

M

1/2

OF − AND

 

b = 1
b = 1/2
M
Theor.

b = 1

103

 

b = 1
b = 1/2
M
Theor.

10−4

 

101

100

10−2

10−4

 

101

102

Sample size k
b = 1/2

b = 1/2

M

b=1

LOW − PAY

102

Sample size k

(cid:3)

(cid:4)

7. REFERENCES
[1] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms
for approximate nearest neighbor in high dimensions. In Commun.
ACM, volume 51, pages 117–122, 2009.

[2] Michael Bendersky and W. Bruce Croft. Finding text reuse on the

web. In WSDM, pages 262–271, 2009.

[3] Sergey Brin, James Davis, and Hector Garcia-Molina. Copy

detection mechanisms for digital documents. In SIGMOD, pages
398–409, 1995.

[4] Andrei Z. Broder. On the resemblance and containment of

documents. In Sequences, pages 21–29, 1997.

[5] Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael
Mitzenmacher. Min-wise independent permutations. Journal of
Computer Systems and Sciences, 60(3):630–659, 2000.

[6] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and

Geoffrey Zweig. Syntactic clustering of the web. In WWW, pages
1157 – 1166, 1997.

[7] Gregory Buehrer and Kumar Chellapilla. A scalable pattern mining
approach to web graph compression with communities. In WSDM,
pages 95–106, 2008.

[8] Moses S. Charikar. Similarity estimation techniques from rounding

algorithms. In STOC, pages 380–388, 2002.

[9] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Michael

Mitzenmacher, Alessandro Panconesi, and Prabhakar Raghavan. On
compressing social networks. In KDD, pages 219–228, 2009.

[10] Dietzfelbinger, Martin and Hagerup, Torben and Katajainen, Jyrki

and Penttonen, Martti A reliable randomized algorithm for the
closest-pair problem. Journal of Algorithms, 25(1):19–51, 1997.

[11] Yon Dourisboure, Filippo Geraci, and Marco Pellegrini. Extraction
and classiﬁcation of dense implicit communities in the web graph.
ACM Trans. Web, 3(2):1–36, 2009.

[12] D. Fetterly, M. Manasse, M. Najork, and J. Wiener. A large-scale

study of the evolution of web pages. In WWW, pages 669–678, 2003.

[13] R.A. Fisher and F. Yates. Statistical Tables for Biological,
Agricultural and Medical Research. Oliver & Boyd, 1948.

[14] George Forman, Kave Eshghi, and Jaap Suermondt. Efﬁcient
detection of large-scale redundancy in enterprise ﬁle systems.
SIGOPS Oper. Syst. Rev., 43(1):84–91, 2009.

[15] Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel Fisher,

Matthew Hurst, and Arnd Christian König. Blews: Using blogs to
provide context for news articles. In AAAI, 2008.

10−4

 

101

102

Sample size k

103

Figure 9: MSEs for comparing ˆR1/2 (25) with ˆR1 and ˆRM . Due
to the bias of ˆR1/2, the theoretical variances Var
, i.e.,
(26), deviate from the empirical MSEs when k is small.

ˆR1/2

In a summary, for applications which care about very high simi-

larities, combining bits can reduce storage even further.

6. CONCLUSION

The minwise hashing technique has been widely used as a stan-
dard duplicate detection approach in the context of information re-
trieval, for efﬁciently computing set similarity in massive data sets.
Prior studies commonly used 64 bits to store each hashed value.

This study proposes b-bit minwise hashing, by only storing the
lowest b bits of each hashed value. We theoretically prove that,
when the similarity is reasonably high (e.g., resemblance ≥ 0.5),
using b = 1 bit per hashed value can, even in the worst case, gain a
21.3-fold improvement in storage space, compared to storing each
hashed value using 64 bits. We also discussed the idea of com-
bining 2 bits from different hashed values, to further enhance the
improvement, when the target similarity is very high.

Our proposed method is simple and requires only minimal mod-
iﬁcation to the original minwise hashing algorithm. We expect our
method will be adopted in practice.

103

[16] Aristides Gionis and Dimitrios Gunopulos and Nick Koudas.

Efﬁcient and Tunable Similar Set Retrieval. In SIGMOD, pages
247-258, 2001.

[17] Sreenivas Gollapudi and Aneesh Sharma. An axiomatic approach for

result diversiﬁcation. In WWW, pages 381–390, 2009.

[18] Monika .R. Henzinge. Algorithmic challenges in web search engines.

Internet Mathematics, 1(1):115–123, 2004.

[19] Piotr Indyk. A small approximately min-wise independent family of

hash functions. Journal of Algorithm, 38(1):84–90, 2001.

[20] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors:

Towards removing the curse of dimensionality. In STOC, 1998.

[21] Toshiya Itoh, Yoshinori Takei, and Jun Tarui. On the sample size of

k-restricted min-wise independent permutations and other k-wise
distributions. In STOC, pages 710–718, 2003.

[22] P. Li and K. Church. A Sketch Algorithm for Estimating Two-way

and Multi-way Associations Computational Linguistics, pages
305–354, 2007. (Preliminary results appeared in HLT/EMNLP 2005.)

[23] P. Li, K. Church and T. Hastie. One Sketch For All: Theory and
Applications of Conditional Random Sampling. In NIPS, 2008.

[24] Nitin Jindal and Bing Liu. Opinion spam and analysis. In WSDM,

pages 219–230, 2008.

[25] Konstantinos Kalpakis and Shilang Tang. Collaborative data

gathering in wireless sensor networks using measurement
co-occurrence. Computer Commu., 31(10):1979–1992, 2008.
[26] Eyal Kaplan, Moni Naor, and Omer Reingold. Derandomized

constructions of k-wise (almost) independent permutations.
Algorithmica, 55(1):113–133, 2009.

[27] Ludmila, Kave Eshghi, Charles B. Morrey III, Joseph Tucek, and
Alistair Veitch. Probabilistic frequent itemset mining in uncertain
databases. In KDD, pages 1087–1096, 2009.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA678[28] Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma.

Detecting Near-Duplicates for Web-Crawling. In WWW, 2007.
[29] Marc Najork, Sreenivas Gollapudi, and Rina Panigrahy. Less is
more: sampling the neighborhood graph makes salsa better and
faster. In WSDM, pages 242–251, 2009.

[30] Sandeep Pandey, Andrei Broder, Flavio Chierichetti, Vanja

Josifovski, Ravi Kumar, and Sergei Vassilvitskii. Nearest-neighbor
caching for content-match applications. In WWW, 441–450, 2009.

[31] Martin Theobald, Jonathan Siddharth, and Andreas Paepcke.

Spotsigs: robust and efﬁcient near duplicate detection in large web
collections. In SIGIR, pages 563–570, 2008.

[32] Mikkel Thorup and Yin Zhang. Tabulation based 4-universal hashing

with applications to second moment estimation. In SODA, 2004.

[33] Tanguy Urvoy, Emmanuel Chauveau, Pascal Filoche, and Thomas

Lavergne. Tracking web spam with html style similarities. ACM
Trans. Web, 2(1):1–28, 2008.

[34] Henry S. Warren. Hacker’s Delight. Addison-Wesley, 2002.
APPENDIX
A. PROOF OF THEOREM 1
Consider two sets, S1, S2 ⊆ Ω = {0, 1, 2, ..., D − 1}. Denote
f1 = |S1|, f2 = |S2|, and a = |S1 ∩ S2|. Apply a random
permutation π on S1 and S2: π : Ω −→ Ω. Deﬁne the minimum
values under π to be z1 and z2:
z1 = min (π (S1)) ,

z2 = min (π (S2)) .

Deﬁne e1,i = ith lowest bit of z1, and e2,i = ith lowest bit of z2.
The task is to derive Pr

(cid:3)(cid:7)b
(cid:4)
i=1 1{e1,i = e2,i} = 1

,

which can be decomposed to be

(cid:9)
(cid:9)

b(cid:10)
b(cid:10)

i=1

i=1

Pr

+Pr

(cid:11)
(cid:11)

1{e1,i = e2,i} = 1, z1 = z2

1{e1,i = e2,i} = 1, z1 (cid:7)= z2

(cid:9)

b(cid:10)

The expressions for P1, P2, and P3 can be understood by the
experiment of randomly throwing f1+f2−a balls into D locations,
labeled 0, 1, 2, ..., D − 1. Those f1 + f2 − a balls belong to three
disjoint sets: S1 − S1 ∩ S2, S2 − S1 ∩ S2, and S1 ∩ S2. Without
any restriction, the total number of combinations should be P3.
To understand P1 and P2, we need to consider two cases:
1. The jth element is not in S1 ∩ S2: =⇒ P1.
D−j−1

We ﬁrst allocate the a = |S1 ∩ S2| overlapping elements ran-
domly in [j + 1, D − 1], resulting in
combinations.
Then we allocate the remaining f2−a−1 elements in S2 also
randomly in the unoccupied locations in [j + 1, D − 1], re-
combinations. Finally, we allocate the
sulting in
remaining elements in S1 randomly in the unoccupied loca-
tions in [i + 1, D − 1], which has
combinations.

D−j−1−a
f2−a−1

D−i−1−f2
f1−a−1

(cid:16)

(cid:15)

(cid:16)

(cid:15)

(cid:15)

(cid:16)

a

2. The jth element is in S1 ∩ S2: =⇒ P2.

After conducing expansions and cancelations, we obtain

(cid:3)

=

=

=

Pr (z1 = i, z2 = j, i < j) =

P1 + P2

P3

(cid:4)

1
a

+ 1

f2−a

(a−1)!(f1−a−1)!(f2−a−1)!(D−j−f2)!(D−i−f1−f2+a)!

(D−j−1)!(D−i−1−f2)!

f2(f1 − a)(D − j − 1)!(D − f2 − i − 1)!(D − f1 − f2 + a)!

a!(f1−a)!(f2−a)!(D−f1−f2+a)!

D!

f2(f1 − a)

=

f2
D

f1 − a
D − 1

(cid:7)j−i−2
(cid:7)i−1
D!(D − f2 − j)!(D − f1 − f2 + a − i)!
t=0(D − f1 − f2 + a − t)
j−i−2(cid:10)

(cid:7)j
(D − f2 − i − 1 − t)
t=0(D − t)
i−1(cid:10)

D − f2 − i − 1 − t

t=0

D − f1 − f2 + a − t
D + i − j − 1 − t

D − 2 − t

t=0

t=0

(cid:11)

For convenience, we introduce the following notation:

r1 =

f1
D

,

r2 =

f2
D

,

s =

a
D

.

.

Also, we assume D is large (which is always satisﬁed in practice).
Thus, we can obtain a reasonable approximation:

=Pr (z1 = z2) + Pr

1{e1,i = e2,i} = 1, z1 (cid:7)= z2

(cid:9)

b(cid:10)

=R + Pr

i=1

1{e1,i = e2,i} = 1, z1 (cid:7)= z2

(cid:11)

where R =

i=1
|S1∩S2|
|S1∪S2| = Pr (z1 = z2) is the resemblance.

When b = 1, the task boils down to estimating

Pr (e1,1 = e2,1, z1 (cid:7)= z2)
(cid:2)
(cid:2)

(cid:2)
(cid:2)

j(cid:9)=i,j=0,2,4,...

i=0,2,4,...

i=1,3,5,...

j(cid:9)=i,j=1,3,5,...

⎧⎨
⎩
⎧⎨
⎩

=

+

⎫⎬
⎭
⎫⎬
⎭ .

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

Therefore, we need the following basic probability formula:

Pr (z1 = i, z2 = j, i (cid:9)= j) .

We start with

P1 + P2

P3

a

(cid:4)

(cid:3)D
(cid:4)(cid:3) D − a
(cid:3)D − j − 1
f1 − a
(cid:3)D − j − 1
a − 1

(cid:4)(cid:3)D − f1
(cid:4)(cid:3)D − j − 1 − a
f2 − a
(cid:4)(cid:3)D − j − a
f2 − a − 1
f2 − a

a

,

(cid:4)

,

(cid:4)(cid:3)D − i − 1 − f2
(cid:4)(cid:3)D − i − 1 − f2
(cid:4)
f1 − a − 1
f1 − a − 1

.

P3 =

P1 =

P2 =

Pr (z1 = i, z2 = j, i < j)
=r2(r1 − s) [1 − r2]

j−i−1

[1 − (r1 + r2 − s)]

i

Similarly, we obtain, for large D,

Pr (z1 = i, z2 = j, i > j)
=r1(r2 − s) [1 − r1]

i−j−1

[1 − (r1 + r2 − s)]

j

Now we have the tool to calculate the probability

Pr (e1,1 = e2,1, z1 (cid:9)= z2)
(cid:2)
(cid:2)

(cid:2)
(cid:2)

j(cid:9)=i,j=0,2,4,...

i=0,2,4,...

=

+

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

⎫⎬
⎭
⎫⎬
⎭

For example, (again, assuming D is large)

Pr (z1 = 0, z2 = 2, 4, 6, ...)
=r2(r1 − s)
=r2(r1 − s)

[1 − r2] + [1 − r2]
1 − r2
1 − [1 − r2]2

(cid:16)

+ ...

3

+ [1 − r2]

5

⎧⎨
⎩
⎧⎨
⎩

(cid:15)

Pr (z1 = i, z2 = j, i < j) =

, where

i=1,3,5,...

j(cid:9)=i,j=1,3,5,...

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA679(cid:16)
Pr (z1 = 1, z2 = 3, 5, 7, ...) = r2(r1 − s)[1 − (r1 + r2 − s)]
+ ...

×(cid:15)

3

5

[1 − r2] + [1 − r2]
1 − [1 − r2]2

1 − r2

+ [1 − r2]
.

=r2(r1 − s)[1 − (r1 + r2 − s)]
Therefore,(cid:2)
(cid:2)

i<j,j=0,2,4,...

i=0,2,4,...

(cid:12) (cid:2)
(cid:12) (cid:2)
1 − r2

+

i=1,3,5,...

i<j,j=1,3,5,...

×

1 − [1 − r2]2

=r2(r1 − s)
(cid:15)
=r2(r1 − s)

By symmetry, we know(cid:2)

1 − r2
1 − [1 − r2]2
(cid:12) (cid:2)
(cid:12) (cid:2)
1 − r1

j=0,2,4,...

i>j,i=0,2,4,...

(cid:2)

+

j=1,3,5,...

i>j,i=1,3,5,...

=r1(r2 − s)

1 − [1 − r1]2

(cid:13)
(cid:13)

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

(cid:16)

+ ...

(cid:13)
(cid:13)

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

1

r1 + r2 − s

.

1 + [1 − (r1 + r2 − s)] + [1 − (r1 + r2 − s)]

2

1

r1 + r2 − s

.

2b−1

r2 [1 − r2]
1 − [1 − r2]

.

2b

Therefore, we can obtain the desired probability, for b = 1,

Combining the probabilities, we obtain

=

Pr (e1,1 = e2,1, z1 (cid:9)= z2)
r2(1 − r2)
1 − [1 − r2]2
r2 − s
=A1,1

r1 + r2 − s + A2,1

r1 − s

r1 + r2 − s +

r1 + r2 − s

,

where

,

2b

A2,b =

A1,b =

2b−1

r1 [1 − r1]
1 − [1 − r1]
b=1(cid:10)

(cid:11)
1{e1,i = e2,i} = 1

(cid:9)

Pr

i=1

=R + A1,1

=R + A1,1

r2 − s
f2 − a

r1 + r2 − s + A2,1
f1 + f2 − a + A2,1

r1 − s
r1 + r2 − s
f1 − a
f1 + f2 − a

f2 − R

1+R (f1 + f2)

=R + A1,1

f1 + f2 − R
f2 − Rf1
=R + A1,1
f1 + f2
=C1,1 + (1 − C2,1)R

+ A2,1

1+R (f1 + f2)

f1 − Rf2
f1 + f2

+ A2,1

f1 − a

f1 + f2 − a

where

C1,b = A1,b

C2,b = A1,b

r2

r1 + r2

r1

r1 + r2

+ A2,b

+ A2,b

r1

r1 + r2

r2

r1 + r2

.

To this end, we have proved the main result for b = 1.

r1(1 − r1)
1 − [1 − r1]2
r1 − s

r2 − s

r1 + r2 − s

i=1

r2 − s
=R + A1,b
=C1,b + (1 − C2,b)R,

r1 + r2 − s + A2,b

r1 − s

r1 + r2 − s

Next, we consider b > 1. Due to the space limit, we only provide

a sketch of the proof. When b = 2, we need

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

i=0,4,8,...

j(cid:9)=i,j=0,4,8,...

Pr (e1,1 = e2,1, e1,2 = e2,2, z1 (cid:9)= z2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)

(cid:2)
(cid:2)
(cid:2)
(cid:2)

j(cid:9)=i,j=2,6,10,...

j(cid:9)=i,j=1,5,9,...

i=2,6,10,...

i=1,5,9,...

⎧⎨
⎩
⎧⎨
⎩
⎧⎨
⎩
⎧⎨
⎩

i=3,7,11,...

j(cid:9)=i,j=3,7,11,...

=

+

+

+

⎫⎬
⎭
⎫⎬
⎭
⎫⎬
⎭
⎫⎬
⎭

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

We again use the basic probability formula Pr (z1 = i, z2 = j, i < j)
and the sum of (different) geometric series, for example,

[1 − r2]

3

+ [1 − r2]

7

+ [1 − r2]

11

+ ... =

[1 − r2]22−1
1 − [1 − r2]22 .

Similarly, for general b, we will need
[1 − r2]

2×2b−1

2b−1

+ [1 − r2]
(cid:9)
b(cid:10)

+ [1 − r2]
(cid:11)
1{e1,i = e2,i} = 1

Pr

After more algebra, we prove the general case:

3×2b−1

+ ... =

[1 − r2]2b−1
1 − [1 − r2]2b

.

∂A1,b
∂b =

It remains to show some useful properties of A1,b (same for

A2,b). The ﬁrst derivative of A1,b with respect to b is

Thus, A1,b is a monotonically decreasing function of b. Also,
[1 − r1]2b−2

2b−1 − r1

−
≤0

(cid:4)

−[1 − r1]2b

(cid:3)
(cid:15)
(cid:16)2
r1[1 − r1]2b−1 log(1 − r1) log 2
1 − [1 − r1]2b
(cid:15)
log(1 − r1) log 2 r1
1 − [1 − r1]2b

1 − [1 − r1]2b
(cid:3)
(cid:16)2
1 − [1 − r1]2b−1
(Note that log(1 − r1) ≤ 0)
(cid:16)
2b − 1
(cid:16)
(cid:15)
2b[1 − r1]2b−1
(cid:16)
[1 − r1]2b−2
2b − 1
1 − [1 − r1]2b
(cid:16)2
− 2b[1 − r1]2b−1r1 [1 − r1]
1 − [1 − r1]2b
(cid:15)
[1 − r1]2b−2
br1 − [1 − r1]
1 − 2
1 − [1 − r1]2b

2b−1 − r1
(cid:15)

[1 − r1]
(cid:15)

≤ 0.

(cid:16)2

2b−1

(cid:4)

(cid:3)

(cid:15)

=

=

=

2b

(cid:4)

,

1
2b

lim
r1→0

A1,b = lim
r1→0
[1 − r1]

∂A1,b
∂r1

Note that (1 − x)c ≥ 1− cx, for c ≥ 1 and x ≤ 1. Therefore A1,b
is a monotonically decreasing function of r1.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA680