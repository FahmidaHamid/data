Exploiting Social Context for Review Quality Prediction

Yue Lu

UIUC Computer Science

Urbana, IL, USA
yuelu2@uiuc.edu

Panayiotis Tsaparas

Microsoft Research

Mountain View, CA, USA
panats@microsoft.com

Alexandros Ntoulas

Microsoft Research

Mountain View, CA, USA
antoulas@microsoft.com

Livia Polanyi

Microsoft Corporation

San Francisco, CA, USA
lipolany@microsoft.com

ABSTRACT
Online reviews in which users publish detailed commentary about
their experiences and opinions with products, services, or events
are extremely valuable to users who rely on them to make informed
decisions. However, reviews vary greatly in quality and are con-
stantly increasing in number, therefore, automatic assessment of
review helpfulness is of growing importance. Previous work has
addressed the problem by treating a review as a stand-alone docu-
ment, extracting features from the review text, and learning a func-
tion based on these features for predicting the review quality. In
this work, we exploit contextual information about authors’ iden-
tities and social networks for improving review quality prediction.
We propose a generic framework for incorporating social context
information by adding regularization constraints to the text-based
predictor. Our approach can effectively use the social context infor-
mation available for large quantities of unlabeled reviews. It also
has the advantage that the resulting predictor is usable even when
social context is unavailable. We validate our framework within
a real commerce portal and experimentally demonstrate that using
social context information can help improve the accuracy of re-
view quality prediction especially when the available training data
is sparse.

Categories and Subject Descriptors
H.3.m [Information Storage and Retrieval]: Miscellaneous; I.2.6
[Artiﬁcial Intelligence]: Learning

General Terms
Algorithms, Experimentation

Keywords
review quality, review helpfulness, social network, graph regular-
ization

1.

INTRODUCTION

Web 2.0 has empowered users to actively interact with each other,
forming social networks around mutually interesting information
and publishing large amounts of useful user-generated content on-
line. One popular and important type of such user-generated con-
tent is the review, where users post detailed commentary on online
portals about their experiences and opinions on products, events,
or services. Reviews play a central role in the decision-making
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

process of online users for a variety of tasks including purchas-
ing products, booking ﬂights and hotels, selecting restaurants, and
picking movies to watch. Sites like Yelp.com and Epinions.
com have created a viable business as review portals, while part of
the popularity and success of Amazon.com is attributed to their
comprehensive user reviews. As online commerce activity contin-
ues to grow [9], the role of online reviews is expected to become
increasingly important.

Unfortunately, the abundance of user-generated content comes
at a price. For every interesting opinion, or helpful review, there
are also large amounts of spam content, unhelpful opinions, as well
as highly subjective and misleading information. Sifting through
large quantities of reviews to identify high quality and useful infor-
mation is a tedious, error-prone process. It is thus highly desirable
to develop reliable methods to assess the quality of reviews auto-
matically. Robust and reliable review quality prediction will enable
sites to surface high-quality reviews to users while beneﬁting other
important popular applications such as sentiment extraction and re-
view summarization [8, 7], by providing high-quality content on
which to operate.

Automatic review quality prediction is useful even for sites pro-
viding a mechanism where users can evaluate or rate the helpful-
ness of a review (e.g. Amazon.com and Epinions.com). Not
all reviews receive the same helpfulness evaluation [10]. There is a
rich-get-richer effect [11] where the top reviews accumulate more
and more ratings, while more recent reviews are rarely read and
thus not rated. Furthermore, such helpfulness evaluation is avail-
able only within a speciﬁc Web site, and is not comparable across
different sources. However, it would be more useful for users if
reviews from different sources for the same item could be aggre-
gated and rated automatically on the same scale. This need is ad-
dressed by a number of increasingly popular aggregation sites such
as Wise.com. For these sites, automatic review rating is essential
in order to meaningfully present the collected reviews.

Most previous work [17, 10, 11, 6, 12, 15] attempts to solve the
problem of review evaluation by treating each review as a stand-
alone text document, extracting features from the text and learn-
ing a function based on these features for predicting review qual-
ity. However, in addition to textual content, there is much more
information available that is useful for this task. Online reviews
are produced by identiﬁable authors (reviewers) who interact with
one another to form social networks. The history of reviewers and
their social network interactions provide a social context for the
reviews. In our approach, we mine combined textual, and social
context information to evaluate the quality of individual reviewers
and to assess the quality of the reviews.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA691In this paper, we investigate how the social context of reviews
can help enhance the accuracy of a text-based quality predictor. To
the best of our knowledge, this is the ﬁrst time that textual, author
and social network information are combined for assessing review
quality. Expressed very generally, our idea is that social context
reveals a lot about the quality of reviewers, which in turn affects
the quality of the reviews. We formulate hypotheses that capture
this intuition and then mathematically model these hypotheses by
developing regularization constraints which augment text-based re-
view quality prediction. The resulting quality predictor is formu-
lated into a well-formed convex optimization problem with efﬁ-
cient solution. The proposed regularization framework falls under
the category of semi-supervised learning, making use of a small
amount of labeled data as well as a large amount of unlabeled data.
It also has the advantage that the learned predictor is applicable
to any review, even reviews from different sources or reviews for
which the reviewer’s social context is not available. Finally, we ex-
periment with real review data from an online commerce portal. We
test our hypotheses and show that they hold for all three categories
of data we consider. We then experimentally demonstrate that our
novel regularization methods that combine social context with text
information can lead to improved accuracy of review quality pre-
diction, especially when the available training data is sparse.

The remainder of our paper is structured as follows. We ﬁrst
formally deﬁne the problem in Section 2. In Section 3 we present
a text-based quality predictor which we use as our baseline.
In
Section 4, we outline our proposed methods for exploiting social
context, formulate our hypotheses, and provide the mathematical
modeling. In Section 5 we experimentally validate our hypotheses,
evaluate the prediction performance of our methods and compare
against baselines. Finally, we go over the related work in Section 6
and conclude in Section 7.

2. PROBLEM DEFINITION
A review system consists of three sets of three different types of
entities: a set I = {i1, ..., iN} of N items (products, events, or ser-
vices); a set R = {r1, ..., rn} of n reviews over these items; and a
set U = {u1, ..., um} of m reviewers (or users) that have authored
these reviews. Each entity has a set of attributes T associated with
it. For an item i or a user u, Ti and Tu are sets of attribute-value
pairs describing the item and the user respectively while for a re-
view r, Tr is the text of the review. We are also given relationships
between these sets of entities. There is a function M : R → I
that maps each review r to a unique item ir = M (r); an author-
ship function A : R → U, that maps each review r to a unique
reviewer ur = A(r); and a relation S ⊂ U × U that deﬁnes the
social network relationships between users.

Since each review is associated with a unique item, we omit
the set I, unless necessary, and assume all information about the
item ir (item identiﬁer and attributes) is included as part of the at-
tributes Tr of review r. We also model the social network relation
as a directed graph GS = (U, S) with adjacency matrix S, where
Suv = 1 if there is a link or edge from u to v and zero otherwise.
We assume that the links between users in the social network cap-
ture semantics of trust and friendship: the meaning of user u linking
to user v is that u values the opinions of user v as a reviewer.

The information about the authors of the reviews along with the
social network of the reviewers places the reviews within a social
context. More formally we have the following deﬁnition.

DEFINITION 1

(SOCIAL CONTEXT). Given a set of reviews
R, we deﬁne the social context of the set R as the triple C(R) =
(cid:4)U, A, S(cid:5), of the set of reviewers U, the authorship function A, and
the social network relation S.

The set of reviews R contains both labeled (RL) and unlabeled
(RU ) reviews. For each review ri ∈ RL in the labeled subset of
reviews we observe a numeric value qi that captures the true quality
and helpfulness of the review. We use L = {(ri, qi)}, to denote
the set of review-quality pairs. Such quality values can be obtained
through manual labeling or through feedback mechanisms in place
for some online portals.
Given the input data {RL ∪ RU , C(R), L}, we want to learn a
quality predictor Q that, for a review r, predicts the quality of the
review. A review r is represented as an f-dimensional real vector r
over a feature space F constructed from the information in R and
f → R that
C(R). So the quality predictor is a function Q : R
maps a review feature vector to a numerical quality value.
Previous work has used the information in {RL, L} for learn-
ing a quality predictor, based mostly on different kinds of textual
features. In this paper, we investigate how to enhance the quality
predictor function Q using the social context C(R) of the reviews
in addition to the information in {RL, L}. Our exploration for the
prediction function Q takes the following steps. First we construct
a text-based baseline predictor that makes use of only the informa-
tion in {RL, L}. Then we enhance this predictor by adding social
context features that we extract from C(RL). In the last step, which
is the focus of this paper, we propose a novel semi-supervised tech-
nique that makes use of the labeled data {RL, L}, the unlabeled
data RU , and the social context information C(R) for both labeled
and unlabeled data.

3. TEXT-BASED QUALITY PREDICTION
The text of a review provides rich information about its quality.
In this section, we build a baseline supervised predictor that makes
use of a variety of textual features as detailed in the top part of
Table 1. We group the features into four different types.
Text-statistics features: This category includes features that are
based on aggregate statistics over the text, such as the length of
the review, the average length of a sentence, or the richness of the
vocabulary.
Syntactic Features: This category includes features that take into
account the Part-Of-Speech (POS) tagging of the words in the text.
We collect statistics based on the POS tags to create features such
as percentage of nouns, adjectives, punctuations, etc.
Conformity features: This category compares a review r with
other reviews by looking at the KL-divergence between the uni-
gram language model Tr of the review r for item i, and the uni-
gram model T i of an “average” review that contains the text of all
(cid:2)
reviews for item i. This feature is used to measure how much the
review conforms to the average and is deﬁned as DKL(Tr||T i) =
w Tr(w) log(Tr(w)/T i(w)) where w takes values over the to-

kens of the unigram models.
Sentiment features:This category considers features that take into
account the positive or negative sentiment of words in the review.
The occurrence of such words is a good indication about the strength
of the opinion of the reviewer.
With this feature set F , we can now represent each review r as
an f-dimensional vector r. Given the labeled data in {RL, L}, we
f → R that for a review ri it pre-
want to learn a function Q : R
dicts a numerical value ˆqi as its quality. We formulate the problem
as a linear regression problem, where the function Q is deﬁned as
a linear combination of the features in F . More formally, the func-
tion Q is fully deﬁned by an f-dimensional column weight vector
w, such that Q(r) = wT r, where wT denotes the transpose of
the vector. In the following, since Q is uniquely determined the by

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA692Feature Description

Total number of tokens.
Total number of sentences.
Ratio of unique words
Average sentence length.
Ratio of capitalized sentences.
Ratio of nouns.
Ratio of adjectives.
Ratio of comparatives.
Ratio of verbs.
Ratio of adverbs.
Ratio of foreign words.
Ratio of symbols.
Ratio of numbers.
Ratio of punctuation symbols.
KL div DKL(Tr||T i)
Ratio of positive sentiment words.
Ratio of negative sentiment words.

Type

Text-Stat
Text-Stat
Text-Stat
Text-Stat
Text-Stat
Syntactic
Syntactic
Syntactic
Syntactic
Syntactic
Syntactic
Syntactic
Syntactic
Syntactic
Conformity
Sentiment
Sentiment

Feature Name
TEXT FEATURES
NumToken
NumSent
UniqWordRatio
SentLen
CapRatio
POS:NN
POS:ADJ
POS:COMP
POS:V:
POS:RB
POS:FW
POS:SYM
POS:CD
POS:PP
KLall
PosSEN
NegSEN
SOCIAL NETWORK FEATURES
ReviewNum
AvgRating
In-Degree
Out-Degree
PageRank

Author
Author
SocialNetwork
SocialNetwork Out-degree of the author.
SocialNetwork

Num. of past reviews by the author.
Past average rating for the author.
In-degree of the author.

PageRank score of the author.

Table 1: Textual Features and Social Context Features

weight vector w and vice versa, we will use Q and w interchange-
ably. Our goal is to ﬁnd the f-dimensional weight vector ˆw that
minimizes the objective function:

1
n(cid:2)

L(wT ri, qi) + αwT w

i=1

Ω(w) =

(1)
where L is the loss function that measures distance of the predicted
quality Q(ri) = wT ri of review ri ∈ RL with the true quality
value qi, n(cid:2) is the number of training examples, and α ≥ 0 is
regularization parameter for w. In our work, we use squared error
loss (or quadratic loss), and we minimize the function

n(cid:2)(cid:3)

n(cid:2)(cid:3)

i=1

Ω1(w) =

1
n(cid:2)

2

+ αwT w

(2)

(wT ri − qi)
n(cid:2)(cid:3)

i + αn(cid:2)I)

n(cid:2)(cid:3)

i=1

−1

qiri

The closed form solution for ˆw is given by

Ω1(w) = (

ˆw = arg max

rirT
where I is the identity matrix of size f.

i=1

w

Once we have learned the weight vector w, we can apply it to any
review feature vector and predict the quality of unlabeled reviews.

4.

INCORPORATING SOCIAL CONTEXT
The solution we describe in Section 3 considers each review as
a stand-alone text document. As we have discussed, in many cases
we also have available the social context of the reviews, that is,
additional information about the authors of the reviews, and their
social network. In this section we discuss different ways of incor-
porating social context into the quality predictor we described in
Section 3. Our work is based on the following two premises:

1. The quality of a review depends on the quality of the re-
viewer. Estimating the quality of the reviewer can help in
estimating the quality of the review.

2. The quality of a reviewer depends on the quality of their
peers in the social network. We can obtain information about
the quality of the reviewers using information from the qual-
ity of their friends in their social network.

We investigate two different ways of incorporating the social
context information into the linear quality predictor. The ﬁrst is
a straightforward expansion of the feature space to include features
extracted from the social context. The second approach is novel in
that it deﬁnes constraints between reviews, and between reviewers,
and adds regularizers to the linear regression formulation to enforce
these constraints. We describe these two approaches in detail in the
following sections.
4.1 Extracting features from social context

A straightforward use of the social context information is by ex-
tracting additional features for the quality predictor function. The
social context features we consider are shown in the bottom part
of Table 1. The features capture the engagement of the author
(ReviewNum), the historical quality of the reviewer (AvgRating),
and the status of the author in the social network (In/Out-Degree,
PageRank).

This approach of using social context is simple and it ﬁts directly
into our existing linear regression formulation. We can still use
Equation 2 for optimizing the function Q, which is now deﬁned
over the expanded feature set F . The disadvantage is that such
information is not always available for all reviews. Consider for
example, a review written anonymously, or a review by a new user
with no history or social network information. Predicting using
social network features is no longer applicable. Furthermore, as the
dimension of features increases, the necessary amount of labeled
training data to learn a good prediction function also increases.
4.2 Extracting constraints from social context
We now present a novel alternative use of the social context that
does not rely on explicit features, but instead deﬁnes a set of con-
straints for the text-based predictor. These constraints deﬁne hy-
potheses about how reviewers behave individually or within the so-
cial network. We require that the quality predictor respects these
constraints, forcing our objective function to take into account re-
lationships between reviews, and between different reviewers.

4.2.1

Social Context Hypotheses

We now describe our hypotheses, and how these hypotheses can
be used in enhancing the prediction of the review quality. In Sec-
tion 5 we validate them experimentally on real-world data, and we
demonstrate that they hold for all the three data sets we consider.
Author Consistency Hypothesis: The hypothesis is that reviews
from the same author will be of similar quality. A reviewer that
writes high quality reviews is likely to continue writing good re-
views, while a reviewer with poor reviews is likely to continue
writing poor reviews.
Trust Consistency Hypothesis: We make the assumption that a
link from a user u1 to a user u2 is an explicit or implicit statement
of trust. The hypothesis is that the reviewers trust other reviewers
in a rational way. In this case, reviewer u1 trusts reviewer u2 only
if the quality of reviewer u2 is at least as high as that of reviewer
u1. Intuitively, we claim that it does not make sense for users in the
social network to trust someone with quality lower than themselves.
Co-Citation Consistency Hypothesis: The hypothesis is that peo-
ple are consistent in how they trust other people. So if two review-
ers u1, and u2 are trusted by the same third reviewer u3, then their
quality should be similar.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA693more than that of u2, and thus enforcing the trust consistency hy-
pothesis.
Formally, for a reviewer u, let hu be the n-dimensional normalized
indicator vector where hu(i) = 1/|Ru| if user u has written review
ri, and zero otherwise. Then we have that ¯Q(u) = wT Rhu. We
can thus write the objective function as
(wT ri − qi)
(cid:10)

n(cid:2)(cid:3)
(cid:3)

+ αwT w

Ω3(w) =

(cid:11)(cid:5)2

1
n(cid:2)

(cid:4)

(6)

i=1

2

0, wT Rhu − wT Rhv

+ β

Suv

max

u,v∈U

where S is the social network matrix. The optimization problem
is still convex, but due to the max function, no nice closed form
solution exists. We can still solve it and ﬁnd the global optimum by
gradient descent, where the gradient of the objective function is

n(cid:2)(cid:3)

i=1

rirT

i w − 1
(cid:3)
n(cid:2)

n(cid:2)(cid:3)

i=1

riqi + αw

SuvR(hu − hv)(hu − hv)

T RT w

∂Ω3(w)

2∂w

=

1
n(cid:2)

+ β

u,v,

wT R(hu−hv )>0

Let H = [h1, ..., hm] be an n×m matrix deﬁned over all reviewers
and Z be a new matrix such that

diag(wT RH)S − S diag(wT RH)

if

> 0

uv

(cid:15)

(cid:16)

⎧⎨
⎩ Suv

0

Zuv =

otherwise

(4)

Now we can rewrite the gradient as

∂Ω3(w)
2∂w =

1
n(cid:2)

rirT

i w − 1
n(cid:2)

n(cid:2)(cid:3)

i=1

n(cid:2)(cid:3)

i=1

riqi + αw

Link Consistency Hypothesis: The hypothesis is that if two peo-
ple are connected in the social network (u1 trusts u2, or u2 trusts
u1, or both), then their quality should be similar. The intuition is
that two users that are linked to each other in some way, are more
likely to share similar characteristics than two random users. This
is the weakest of the four hypotheses but we observed that it is still
useful in practice.

4.2.2 Exploiting hypotheses for regularization

We now describe how we enforce the hypotheses deﬁned above
by designing regularizing constraints to add into the text-based lin-
ear regression deﬁned in Section 3.
Author Consistency: We enforce this hypothesis by adding a regu-
larization term into the regression model where we require that the
quality of reviews from the same author is similar. Let Ru denote
the set of reviews authored by reviewer u, including both labeled
and unlabeled reviews. Then the objective function becomes:

Ω2(Q) = Ω1(Q) + β

(Q(ri) − Q(rj))

2

(3)

(cid:3)

(cid:3)

u∈U

ri,rj∈Ru

Minimizing the regularization constraint will force reviews of the
same author u to receive similar quality values. We can formulate
this as a graph regularization. The graph adjacency matrix A is
deﬁned as Aij = 1 if review ri and review rj are authored by the
same reviewer, and zero otherwise. Then, Equation 3 becomes:

Ω2(w) =

(cid:4)

n(cid:2)(cid:3)
(cid:3)

i=1

1
n(cid:2)

+ β

Aij

i<j

(cid:5)2

wT ri − qi
(cid:4)

wT ri − wT rj

+ αwT w

(cid:5)2

Let R = [r1, ..., rn] be an f × n feature-review matrix deﬁned
over all reviews (both labeled and unlabeled). Then the last regu-
larization constraint of Equation 4 can be written as

(cid:4)

Aij

(cid:3)

i<j

(cid:5)2

wT ri − wT rj
(cid:2)

= wT RΔART w

ΔA = DA − A is the graph Laplacian, and DA is a diagonal
matrix with DAii =
j Aij. The new optimization problem is
still convex with the closed form solution [21]:
i + αn(cid:2)I + βn(cid:2)RΔART

n(cid:2)(cid:3)

n(cid:2)(cid:3)

ˆw = (

rirT

qiri

−1

)

i=1

i=1

Trust Consistency: Let u be a reviewer. Given a review quality
predictor function Q, we deﬁne the reviewer quality ¯Q(u) as the
average quality of all the reviews authored by this reviewer as it is
estimated by our quality predictor. That is,

(cid:2)

(cid:2)

¯Q(u) =

Q(r)

r∈Ru
|Ru|

=

wT ri

r∈Ru
|Ru|

(5)

We enforce the trust consistency hypothesis by adding a regulariza-
tion constraint to Equation 2. Let Nu denote the set of reviewers
that are linked to by reviewer u. We have

(cid:3)

(cid:3)

(cid:6)

(cid:7)

(cid:8)(cid:9)2

0, ¯Q(u1) − ¯Q(u2)

Ω3(Q) = Ω1(Q) + β

max

u1

u2∈Nu1

The regularization term is greater than zero for each pair of review-
ers u1 and u2 where u1 trusts u2, but the estimated quality of u1
is greater than that of u2. Minimizing function Ω3 will push such
cases closer to zero, forcing the quality of a reviewer u1 to be no

+ βRHΔZHT RT w

where ΔZ = DZ + DZT − Z − ZT can be thought of the graph
Laplacian generalized for directed graphs with DZ and DZT the
diagonal matrices of the row, and column sums of Z respectively.
Co-Citation Consistency: We enforce this hypothesis by adding
a regularization term into the regression model, where we require
that the quality of reviews authored by two co-cited reviewers is
similar. Then, the objective function (Equation 2) becomes:
¯Q(x) − ¯Q(y)

Ω4(Q) = Ω1(Q) + β

(cid:3)

(cid:3)

(cid:9)2

(cid:6)

u∈U

x,y∈Nu

Minimizing function Ω4 will cause the quality difference of review-
ers x and y to be pushed closer to zero, making them more similar.
We can again formulate these constraints as a graph regularizaton.
Let C be the co-citation graph adjacency matrix, where Cij = 1
if two reviewers ui and uj are both trusted by at least one other
reviewer u. Using the same deﬁnition of matrix R and vector hu
as for trust consistency, the objective function now becomes

(cid:5)2

(cid:4)

n(cid:2)(cid:3)
(cid:3)

i=1

1
n(cid:2)

wT ri − qi
(cid:4)

Ω4(w) =

+ β

Cij

i<j

wT Rhi − wT Rhj

(7)

(cid:17)

n(cid:2)(cid:3)

Let ΔC be the Laplacian of graph C. The closed form solution is

ˆw =

i + αn(cid:2)I + βn(cid:2)RHΔCHT RT

rirT

riqi

i=1

i=1

+ αwT w

(cid:5)2
(cid:18)−1 n(cid:2)(cid:3)

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA694Link Consistency: The regularization for this hypothesis is very
similar to the one for the co-citation consistency. We treat the trust
network as an undirected graph. Let B be the corresponding ma-
trix, where Bij = 1 if Sij = 1 or Sji = 1. Our objective function
now becomes

(cid:5)2

(cid:4)

n(cid:2)(cid:3)
(cid:3)

i=1

1
n(cid:2)

wT ri − qi
(cid:4)

+ β

Bij

i<j

wT Rhi − wT Rhj

+ αwT w

(cid:5)2
(cid:18)−1 n(cid:2)(cid:3)

(8)

riqi

Ω5(w) =

(cid:17)

n(cid:2)(cid:3)

with a similar closed form solution

ˆw =

i + αn(cid:2)I + βn(cid:2)RHΔBHT RT

rirT

i=1

i=1

In all these cases, β is a weight on the added regularization term
which deﬁnes a trade-off between the mean squared error loss and
the regularization constraint in the ﬁnal objective function.

Adding the regularization makes our problem a semi-supervised
learning problem. That is, our algorithms operate on both the la-
beled and the unlabeled data. Although, only the labels of the la-
beled data are known to the algorithm, the unlabeled data are also
used for optimizing the regularized regression functions. This gives
considerable more ﬂexibility to the algorithm, since it is able to op-
erate even with little labeled data by making use of the unlabeled
data and the constraints deﬁned by the social context. Furthermore,
through regularization the signal from the social context is incor-
porated into the textual features. The resulting predictor function
operates only on textual features, so it can be applied even in the
case where there is no social context.

5. EXPERIMENTS

In this section, we present the experimental evaluation of our
techniques. For our experiments we use product reviews obtained
from a real online commerce portal. We begin by describing the
characteristics and preprocessing of our data sets. Then, we test
the hypotheses we proposed in Section 4.2.2 on these real-world
datasets. Finally, we evaluate the prediction performance of differ-
ent methods and conduct some analysis.
5.1 Data Sets

Our experiments employ the data from Ciao UK1, a community
review web site. In Ciao, people not only write critical reviews for
all kinds of products and services, but also rate the reviews written
by others. Furthermore, people can add members to their network
of trusted members or “Circle of Trust”, if they ﬁnd their reviews
consistently interesting and helpful.

We collected reviews, reviewers, and ratings up to May, 2009
for all products in three categories: Cellphones, Beauty, and Digi-
tal Cameras (DC). We use the average rating of the reviews (a real
value between 0 and 5) as our gold standard of review quality. In or-
der for the gold standard to be robust and resistant to outlier raters,
we use only reviews with at least ﬁve ratings from different raters.
We then apply some further pruning by imposing the conditions
shown in the top part of Table 2. The purpose of the pruning is
to obtain a dataset that is both large enough and has sufﬁcient so-
cial context information. Because we need some information about
reviewers’ history in order to test our Reviewer Consistency hy-
pothesis, we require reviewers for Cellphone and Beauty to have at
least two reviews each. We also require reviewers to be part of the

1http://www.ciao.co.uk/

Cellphone

Beauty Digital Camera

PRUNING SETTINGS
min # of ratings/ review
min # of reviews/reviewer
min # of trust links/reviewer
min # of reviews/ product
STATISTICS
# of reviews
# of reviewers
# of products
# of links in Trust
# of links in Link
# of links in Cocitation
Trust graph density
Link graph density
Cociation graph density
Avg # of reviews/reviewer
Ratio of Reciprocal links
Clustering coefﬁcient
CHARACTERISTICS
Social Context
Quality Distribution

5
2
1
5

1943
881
158
2905
4644
13678
0.0075
0.0120
0.0353
2.2054
0.4014
0.2286

5
2
1
10

4849
1709
308
20374
32104
188610
0.0140
0.0220
0.1292
2.8373
0.4243
0.3072

5
1
0
5

3697
3465
380
3894
6022
22136
0.0006
0.0010
0.0037
1.0670
0.4535
0.2523

rich

balanced

rich

skewed

sparse
balanced

Table 2: Data Pruning Settings, Statistics, and Characteristics

y
t
i
s
n
e
D

Cellphone
Beauty
Digital Camera

0

1

2

3

4

5

Review Quality

Figure 1: Density Estimate of Gold Standard Review Quality.

trust social network (with at least one link in the social network), in
order to test our hypotheses and methods based on social networks.
Finally, we require for each product to have some representation
in the dataset, that is, a sufﬁciently large number of reviews. The
pruning thresholds are selected per category, so as to obtain sufﬁ-
cient volume of data. For the Digital Cameras category, this results
in a minimum amount of pruning. Although DC reviews do not
contain much social context information, we still include them here
for comparison and generality purposes.

2|E|

From the statistics in Table 2, we can see that Cellphone and
Beauty reviews contain more rich social context information than
DC reviews in the sense that the average number of reviews per
reviewer is more than twice that for Digital Cameras, and the link
density (deﬁned as D =
|V |(|V |−1) for a graph with vertices V and
edges E) is more than 10 times that of Digital Cameras. We also
plot the Kernel-smoothing density estimate (pdf) of the samples qi
(the gold standard review quality) in Figure 1. The distributions of
qi for the three categories are quite different. Beauty reviews are
highly concentrated at rating 4, while Cellphone and DC reviews
have a more balanced distribution of quality. We summarize the
characteristics of the three data sets in the bottom of Table 2.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA695STD
Rel:DifferentReviewer
Rel:SameReviewer
p-value

Cellphone

0.9187
0.5937

Beauty
0.7017
0.4518

Digital Camera

0.9571
0.6176

1.37E-48*

1.57E-287*

3.12E-11*

Table 3: Statistics of Review Quality Difference to Support Re-
viewer Consistency Hypothesis

5.2 Consistency Hypotheses Testing

Before evaluating the prediction performance of different algo-
rithms, we ﬁrst validate our four consistency hypotheses over our
data sets.

5.2.1 Author Consistency Hypothesis

For each dataset, we consider all n2 pairs of reviews (ri, rj), and
we divide them into two disjoint groups: Rel:DifferentReviewer
(cid:9)= uj,
if ri and rj are authored by different reviewers, i.e., ui
and Rel:SameReviewer if ui = uj. In each group, for each pair
(ri, rj) we compute the difference in quality, dqij = qi − qj, of
the two reviews. Since for each value dqij we also include value
dqji = −dqij the mean value of dqij for both groups is zero. We
are interested in the standard deviation, std(dqij), that captures
how much variability there is in the difference of quality between
reviews for the two groups. Table 3 shows the results for the differ-
ent datasets. For a visual comparison, in Figure 2 we also plot the
Kernel-smoothing density estimates of the two groups.

We observe that the standard deviation of the quality difference
of two reviews by the same author is much lower than that of
two reviews from different authors. This indicates that review-
ers are, to some extent, consistent in the quality of reviews they
write. The ﬁgures also clearly indicate that the density curve for
Rel:SameReviewer
is more concentrated around zero than
Rel:DifferentReviewer for all three categories. Moreover, two-sam-
ple Kolmogorov-Smirnov (KS) test of the samples in the two groups
indicates that the difference of the two groups is statistically signif-
icant. The p-values are shown in the last row of Table 3. The star
next to the p-value means there is strong evidence (p < 0.01) that
the two samples come from different distributions.

5.2.2

Social Network Consistency Hypotheses

In order to test the three social network consistency hypotheses,
namely Trust Consistency, Co-Citation Consistency and Link Con-
(ui)−
sistency, we look at the empirical distribution of d ¯Q∗
¯Q∗
(uj), i.e., the difference in quality of two reviewers, where, sim-
ilar to Equation 5

ij = ¯Q∗

(cid:2)

¯Q∗

(u) =

ri∈Ru
|Ru|

qi

(9)

is deﬁned as the average quality of the reviews written by u in our
dataset, but using gold standard quality. Again, we group the pairs
of reviewers (ui, uj) into the the following sets depending on the
relationship between the two reviewers.
Rel:None: User ui is not linked to user uj, i.e., Bij = 0.
Rel:Trust: User ui trusts user uj, i.e., Sij = 1.
Rel:Cocitation: Users ui and uj are trusted by at least one other
reviewer u3, i.e., Cij = 1.
Rel:Link: User ui trusts user uj, or uj trusts ui, i.e., Bij = 1.
In Figure 3, we plot the Kernel-smoothing density estimate of
the d ¯Q∗
ij values for the four different sets of pairs, for the three
categories. We further show in Table 4 the moments (mean and

Cellphone
p-value
Rel:None
Rel:Trust
Rel:Link
Moments
Mean
Variance
Beauty
p-value
Rel:None
Rel:Trust
Rel:Link
Moments
Mean
Variance
Digital Camera
p-value
Rel:None
Rel:Trust
Rel:Link
Moments
Mean
Variance

Rel:None
-
-
-
Rel:None
0.0000
0.6727

Rel:None
-
-
-
Rel:None
0.0000
0.4331

Rel:None
-
-
-
Rel:None
0.0000
0.8763

Rel:Trust
3.20E-82*
-
-
Rel:Trust
-0.1376
0.3255

Rel:Trust
0.00E+00*
-
-
Rel:Trust
-0.0824
0.1806

Rel:Trust
1.76E-135*
-
-
Rel:Trust
-0.1481
0.4068

Rel:Link
4.53E-44*
3.44E-16*
-
Rel:Link
0.0000
0.3485

Rel:Link
0.00E+00*
3.83E-59*
-
Rel:Link
0.0000
0.1907

Rel:Link
2.14E-87*
1.46E-21*
-
Rel:Link
0.0000
0.4471

Rel:Cocitation
6.12E-177*
6.89E-22*
0.0657
Rel:Cocitation
0.0000
0.2914

Rel:Cocitation
0.00E+00*
3.75E-101*
0.3003
Rel:Cocitation
0.0000
0.1903

Rel:Cocitation
0.00E+00*
2.10E-34*
0.3052
Rel:Cocitation
0.0000
0.4059

Table 4: Statistics of Reviewer Quality Difference to Support
Social Network Consistency Hypotheses.

variance) of the four density estimates and p-values of the KS-test
between pairs of density estimates.

(ui) − ¯Q∗

The ﬁrst observation is that the distribution of Rel:Trust is skewed
towards the negative with a negative mean. This supports the Trust
Consistency Hypothesis that when ui trusts uj, the quality of ui is
usually lower than that of uj, i.e., ¯Q∗
(uj) < 0. The
remaining three distributions are all symmetric with mean zero.
However, Rel:Cocitation and Rel:Link have a much more concen-
trated peak around zero, i.e., smaller variance, compared with
Rel:None. This supports the Co-Citation and Link Consistency Hy-
potheses that reviewers are more similar in quality (quality differ-
ence closer to zero) if they are co-trusted by others, or linked in a
trust graph regardless of direction.

In the results of the KS-test, we have only one high p-value,
for Rel:Link and Rel:Cocitation, while all the other pairs have p-
values close to zero. This implies that Rel:Trust, Rel:Cocitation, or
Rel:Link do not come from the same distribution as Rel:None. This
observation directly connects the quality of reviewers with their re-
lations in the social network. The correlation between Rel:Link
and Rel:Cocitation could potentially be explained by the relatively
high reciprocity ratio (the percentage of links in the Trust social
network that are reciprocal), and the relatively high clustering co-
efﬁcient [14] which measures the tendency of triples to form trian-
gles.

In summary, our experiments indicate that there exists correla-
tion between review quality, reviewer quality, and social context.
For all the three data sets considered, the statistics support our hy-
potheses for designing the regularizers.
5.3 Prediction Performance

For all three datasets (Cellphones, Beauty, and Digital Cameras),
we randomly split the data into training and testing sets: 50% of the
products for training (Rtrain), and 50% for testing (Rtest). We keep
the test data ﬁxed, while sub-sampling from the training data to
generate training sets of different sizes (10%, 25%, 50% or 100%
of the training data). Our goal is to study the effect of different
amount of training data on the prediction performance. We draw
10 independent random splits, and we report test set mean and stan-
dard deviation for our evaluation metrics. A polynomial kernel is

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA696y
t
i
s
n
e
D

−2

y
t
i
s
n
e
D

−2

Rel:DifferentReviewer
Rel:SameReviewer

Rel:DifferentReviewer
Rel:SameReviewer

Rel:DifferentReviewer
Rel:SameReviewer

y
t
i
s
n
e
D

y
t
i
s
n
e
D

0

−1
1
Review Quality Difference
(a) Cellphone

2

−2

−1
1
Review Quality Difference

0

(b) Beauty

2

−2

0

−1
1
Review Quality Difference
(c) Digital Cameras

Figure 2: Density Estimates of Review Quality Difference.

Rel:None
Rel:Trust
Rel:Link
Rel:Cocitation

Rel:None
Rel:Trust
Rel:Link
Rel:Cocitation

Rel:None
Rel:Trust
Rel:Link
Rel:Cocitation

y
t
i
s
n
e
D

2

−1

y
t
i
s
n
e
D

1

−2

−0.5
Difference in Reviewer Quality

0.5

0

−1

0

1

Difference in Reviewer Quality

(a) Cellphone

(b) Beauty

Figure 3: Density Estimates of Reviewer Quality Difference.

−1

0

1

Difference in Reviewer Quality
(c) Digital Cameras

2

2

used to enrich the feature representation for the linear model. We
ﬁx the parameter α of Linear Regression to the value that gives the
best performance for the text-based baseline. Then, we report the
best prediction performance by tuning the regularization weight β.
We will discuss the parameter sensitivity in Section 5.3.3, while
leaving the automatic optimization of parameters as future work.

We evaluate the effectiveness of different prediction methods us-

ing Mean Squared Error (MSE) over the test set Rtest of size nt,

nt(cid:3)

i=1

M SE(Rtest) =

1
nt

(Q(ri) − qi)

2

MSE measures how much our predicted quality deviates from the
true quality. A smaller value indicates a more accurate prediction.

5.3.1 Simple Text-free Baselines

Since the graph statistics in Section 5.2 support our design of
regularizers, we will examine a few text-free baselines (TBL) that
are based solely on social context. These baselines also serve as a
sanity check for the experiments we report in the following section.
For the following, r denotes a test review written by reviewer ur,
and ¯Q∗
(u) is the quality of reviewer u as deﬁned in Equation 9,
when computed over the training data. If reviewer u has no reviews
in the training data, ¯Q∗
(u) is undeﬁned. We consider the following
baselines for predicting the quality of r.
TBL:Mean: Simply predict as the mean review quality in the train-
ing data Rtrain, i.e., Q(r) = 1
i=1 qi.
nt
TBL:Reviewer: Predict as the quality ¯Q∗
the training data. If it is not deﬁned, predict as TBL:Mean.
TBL:Link: Predict as the mean quality of all the reviewers con-
nected to ur in the link graph; if no such reviewer exists in the
training set, or the value is undeﬁned simply predict as TBL:Mean.

(ur) of the author ur in

(cid:2)nt

TBL:CoCitation: Similar to TBL:Link, predict as the mean qual-
ity of all reviewers connected to ur in the Co-Citation graph. If this
is not deﬁned predict as TBL:Mean.

We compare the four simple text-free baselines against BL:Text:
the Linear Regression baseline that uses only text information. Fig-
ure 4 shows the MSE with standard deviation where the x-axis
corresponds to the different percentages of the training data we
used. We observe that none of the text-free baselines works as
well as Linear Regression with textual features, suggesting that
social context by itself cannot accurately predict the quality of a
review. The MSE of the text-free baselines is lower for the Beauty
category, where quality distribution is highly skewed at 4, but the
text-based predictor is still signiﬁcantly better. Out of the three
social-context based baselines, TBL:Reviewer appears to provide
more accurate prediction than the other two when there is rich so-
cial context (Cellphones and Beauty), but it offers marginal im-
provements over TBL:Mean in the case where the social context
is sparse (Digital Cameras). TBL:CoCitation consistently outper-
forms TBL:Link, which is in line with our observation in Table 4
that the variance of Rel:Cocitation is smaller than that of Rel:Link.

5.3.2

Incorporating Social Context

We now compare the different techniques for review quality pre-
diction that make use of text and social context of reviews. We
consider the following methods.
BL:Text: Linear Regression described in Section 3 (Equation 2)
using only textual features.
BL:Text+Rvr: Linear Regression described in Section 4.1 using
both textual, and social context features.
REG:Reviewer: Linear Regression with a regularizer under Re-
viewer Consistency Hypothesis (Equation 4).

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA697 

E
d
e
r
a
u
q
S
n
a
e
M

 

0.4

0.3

0.2

10%

 

E
d
e
r
a
u
q
S
n
a
e
M

 

0.3

0.25

0.2

0.15

0.1

 

E
d
e
r
a
u
q
S
n
a
e
M

 

0.4

0.3

0.2

100%

25%

50%

Percentage of Training Data

0.5

r
o
r
r

TBL:Mean
TBL:Reviewer
TBL:Link
TBL:Cocitation
BL:Text

r
o
r
r

TBL:Mean
TBL:Reviewer
TBL:Link
TBL:Cocitation
BL:Text

0.5

r
o
r
r

TBL:Mean
TBL:Reviewer
TBL:Link
TBL:Cocitation
BL:Text

25%

50%

100%

10%

Percentage of Training Data

(a) Cellphone

(b) Beauty

10%

25%

50%

100%

Percentage of Training Data
(c) Digital Cameras

Figure 4: MSE of Simple Text-free Baselines V.S. Text-only Baseline.

TRAINING SUBSET
Cellphone
BL:Text
BL:Text+Rvr
REG:Link
REG:CoCitation
REG:Trust
REG:Reviewer
Beauty
BL:Text
BL:Text+Rvr
REG:Link
REG:CoCitation
REG:Trust
REG:Reviewer
Digital Camera
BL:Text
BL:Text+Rvr
REG:Link
REG:CoCitation
REG:Trust
REG:Reviewer

10%
0.2852±0.0558
0.3137±0.1079(9.99%)
0.2642±0.0292(-7.36%)
0.2635±0.0359(-7.61%)
0.2563±0.0317(-10.13%)
0.2468±0.0223(-13.46%)
0.125±0.0132
0.122±0.0123(-2.40%)
0.1174±0.0073(-6.08%)
0.1166±0.007(-6.72%)
0.1157±0.0058(-7.44%)
0.112±0.0063(-10.40%)
0.2392±0.0192
0.2541±0.0239(6.23%)
0.2355±0.0211(-1.55%)
0.2346±0.0204(-1.92%)
0.2302±0.0183(-3.76%)
0.2373±0.0189(-0.79%)

25%
0.2183±0.0402
0.2249±0.0518(3.02%)
0.2113±0.0294(-3.21%)
0.2064±0.0226(-5.45%)
0.2035±0.0205(-6.78%)
0.1958±0.0116(-10.31%)
0.1051±0.0064
0.0973±0.0062(-7.42%)
0.1036±0.0054(-1.43%)
0.1036±0.0054(-1.43%)
0.1022±0.0044(-2.76%)
0.1021±0.0049(-2.85%)
0.2007±0.0136
0.2011±0.0106(0.20%)
0.2002±0.0125(-0.25%)
0.1994±0.0132(-0.65%)
0.1984±0.0127(-1.15%)
0.2005±0.0135(-0.10%)

50%
0.1787±0.0143
0.1728±0.0116(-3.30%)
0.1781±0.014(-0.34%)
0.1771±0.0133(-0.90%)
0.1768±0.0134(-1.06%)
0.1728±0.01(-3.30%)
0.0994±0.0014
0.089±0.002(-10.46%)
0.0991±0.0016(-0.30%)
0.099±0.0016(-0.40%)
0.0986±0.0021(-0.80%)
0.0984±0.0018(-1.01%)
0.1897±0.0125
0.1869±0.0096(-1.48%)
0.1894±0.0124(-0.16%)
0.1893±0.0126(-0.21%)
0.189±0.0124(-0.37%)
0.1896±0.0124(-0.05%)

100%
0.1654±0.0112
0.1552±0.0095(-6.17%)
0.1652±0.0111(-0.12%)
0.1647±0.0107(-0.42%)
0.1647±0.0108(-0.42%)
0.1635±0.0089(-1.15%)
0.0969±0.0028
0.0857±0.0027(-11.56%)
0.0968±0.0028(-0.10%)
0.0968±0.003(-0.10%)
0.0966±0.0029(-0.31%)
0.0964±0.0028(-0.52%)
0.1848±0.0127
0.1801±0.0115(-2.54%)
0.1848±0.0127(0.00%)
0.1848±0.0126(0.00%)
0.1846±0.0127(-0.11%)
0.1848±0.0127(0.00%)

Table 5: MSE of Using Social Context as Features and as Regularization vs. Text-based Baseline

REG:Link: Linear Regression with a regularizer under Link Con-
sistency Hypothesis (Equation 8).
REG:Cocitation: Linear Regression with a regularizer under Co-
ciation Consistency Hypothesis (Equation 7).
REG:Trust: Linear Regression with a regularizer under Trust Con-
sistency Hypothesis (Equation 6)

It is possible to consider combinations of the different regular-
izers. This would introduce multiple β parameters (one for each
regularizer), and careful tuning is required to make the technique
work. We defer the exploration of this idea to future work.

The results of our experiments are summarized in Table 5 where
we show the mean MSE and the standard deviation for all tech-
niques, over all categories, for different training data sizes. In the
parentheses we have the percentage of reduction over MSE of the
text-based baseline BL:Text. The best result (largest decrease of
MSE) for each data set and each training size is emphasized in
bold.

The ﬁrst observation is that adding social context as additional
features BL:Text+Rvr can improve signiﬁcantly over the text-only
baseline when there is sufﬁcient amount of training data. The more
training data available, the better the performance. BL:Text+Rvr
gives the best improvement for training percentage of 50% and
100% for all three categories. We expect a similar trend for larger
amounts of training data. On the other hand, when there is little

training data, the social context features are too sparse to be help-
ful, and it may be the case that the MSE actually increases, e.g.,
when training with 10% and 25% of the training data for Cellphone,
and training with 10% for Digital Cameras. There are techniques
for dealing with sparse data, however, exploring such techniques is
beyond the scope of this paper.

Using social context as regularization (method names starting
with REG) consistently improves over the text-only baseline. The
advantage of the regularization methods is most signiﬁcant when
the training size is small, e.g. using training percentage of 10% and
25% in all three data sets. This is often the case in practice, where
we have limited resources for obtaining labeled training data, while
there are large amounts of unlabeled data available.

Among the different regularization techniques, for both Cell-
phone and Beauty reviews, where there is relatively rich social
context information, REG:Reviewer appears to be the most ef-
fective. For the Cellphone dataset, REG:Reviewer outperforms
BL:Text+Rvr even with 50% of training data, indicating that so-
cial context regularization can be helpful when we have rich social
context and balanced data. Among the regularization methods us-
ing the social network, REG:Trust, which is based on the most
reasonable hypothesis, performs best in practice. This means that
the direction of the trust social network carries more useful infor-
mation than the simpliﬁed undirected link graphs and co-citation
graphs.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA698Finally, for the Digital Camera reviews where the social context
is very sparse there is still some improvement observed using reg-
ularization when the training data is small, but the improvement is
not as signiﬁcant as on the other two categories where the social
context is richer; that is exactly what we expected.

In addition to the experiments on our test data, we are interested
in testing our algorithms on data for which we have no social con-
text information. Our premise is that using regularization can help
to incorporate signals from the social network to the text-based pre-
dictor, thus improving accuracy prediction even if social context is
not available. We now validate this premise. We use the Cell-
phone dataset, and we consider the case where we train on 10%
of the training data. Within the test data of Cellphone, there is a
subset of data (144 reviews on average across splits) that has no
social context information, i.e., the author has only one review, and
is not in the social network.2 Regularization methods only adjust
weights on textual features and are thus applicable to those anony-
mous reviews too, even though these reviews do not contribute to
the added regularization terms. In Table 6, we report the percent-
age of improvement of four regularization methods over BL:Text.
We still observe some improvement on anonymous reviews with no
social context, although as expected less than on reviews with so-
cial context. This indicates the the generalizablity of the proposed
regularization methods.

To further support the generalizablity claim, we try an extra set
of experiments testing our regularization methods on a held-out set
of reviews which are not used in the optmization process and for
which we use only the textual features and hide their social context.
More speciﬁcally, after learning a quality prediction function Q
using 10% of the training data, we apply it to the remaining 90%
of the training data, by multiplying the learned weight vector w
with the text feature vectors of the held-out reviews. From the last
row in Table 6, we can clearly see that compared with the text-only
baseline, all regularization methods can learn a better weight vector
w that captures more accurately the importance of textual features
for predicting the true quality on the held-out set.

In summary, we make the following observations.
• Adding social context as features is effective only when there
is enough training data to learn the importance of those addi-
tional features.

• On the other hand, regularization methods work best when
there is little training data by exploiting the constraints de-
ﬁned by the social context and the large amount of unlabeled
data.

• Since regularization techniques incorporate the social con-
text information into the text-based predictor, they provide
improvements even when applied to data without any social
context.

5.3.3 Parameter Sensitivity

Regularization methods have one parameter β to set: the trade-
off weight for the regularization term. The value of the regulariza-
tion weight deﬁnes our conﬁdence in the regularizer: a higher value
results in a higher penalty when violating the corresponding regu-
larization hypothesis. In the objective functions (Equations 4, 6, 7,
and 8), the contribution from the regularization term depends on β
as well as the number of non-zero edges in the regularization graph.

2Although we prune the data by requiring that each reviewer has at
least two reviews and a link in the social network, due to multiple
consecutive pruning conditions some reviewers end up with only
one review and no links in the ﬁnal pruned subset.

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

0.32

0.3

0.28

0.26

0.24

10−4

BL:Text
REG:Link
REG:Cocitation
REG:Trust
REG:Reviewer

BL:Text
REG:Link
REG:Cocitation
REG:Trust
REG:Reviewer

0.14

r
o
r
r

0.135

 

E
d
e
r
a
u
q
S
n
a
e
M

 

0.13

0.125

0.12

0.115

10−3

10−2

10−1

Sum of Regularization Weight
(a) Cellphone

100

0.11

10−4

10−3

10−2

10−1

100

Sum of Regularization Weight

(b) Beauty

Figure 5: Parameter Sensitivity.

(cid:2)

We deﬁne the sum of regularization weight as σ = β
ij Mij,
where M can be the co-author matrix A, the directed trust matrix
S, the co-citation matrix C, or the undirected link matrix B.

Figure 5 shows how the prediction performance of regularization
methods varies as we use different values of σ. We only show the
parameter sensitivity for Cellphone and Beauty reviews where the
social context is relatively rich. The training data size is ﬁxed to
be 10%. As we can see, even though Cellphone and Beauty re-
views carry different characteristics, the curves follow a very sim-
ilar trend: as long as we set σ ≤ 0.1, all regularization methods
achieve consistently better performance than the baseline. As σ
goes to zero, the performance converges to the text-based base-
line. In addition, the shape of the performance curve depends on
the corresponding hypothesis. For example, the optimum σ for
REG:Trust is larger than that of REG:Link and REG:Cociation.
Also, even with a value of σ higher than the optimum, the er-
ror of the REG:Reviewer does not increase as quickly as for the
other methods. These observations are in line with the previous
observations that the history of the reviewer (REG:Reviewer) and
the Trust graph (REG:Trust) provide a better signal than the Co-
Citation graph, or the Link graph.

6. RELATED WORK

The problem of assessing the quality of user-generated content
has recently attracted increasing attention. Most previous work [17,
10, 11, 6, 12, 15] has typically focused on automatically deter-
mining the quality (or helpfulness, or utility) of reviews by using
textual features. The problem of determining review quality is for-
mulated as a classiﬁcation or regression problem with users’ votes
serving as the ground-truth. In this context, Zhang and Varadarajan
[17] found that shallow syntactic features from the text of reviews
are most useful, while review length seems weakly correlated with
review quality. In addition to textual features, Kim et al. [10] in-
cluded metadata features including ratings given to an item under
review and concluded that review length and the number of stars in
product rating are most helpful within their SVM regression model.
Ghose and Ipeirotis [6] combined econometric models with textual
subjectivity analysis and demonstrated evidence that extreme re-
views are considered to be most helpful. In [12], the authors in-
corporated reviewers’ expertise and review timeliness in addition
to the writing style of the review in a non-linear regression model.
In our work, we extend previous work by using author and social
network information in order to assess review quality.

Although user votes can be helpful as ground-truth data, Liu et al
[11] identiﬁed a discrepancy between votes coming from Amazon.
com and votes coming from an independent study. More speciﬁ-
cally, they identiﬁed a “rich-get-richer” effect, where reviews accu-
mulate votes more quickly depending on the number of votes they
already have. This observation further enhances our motivation to

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA699Test on
All
Reviews with no social context
Reviews with social context
Held-out reviews with hidden social context

# of Reviews

REG:Link

REG:CoCitation

REG:Trust

REG:Reviewer

1066
144
922
893

7.36%
3.33%
8.11%
10.38%

7.61%
1.08%
8.84%
9.64%

10.13%
3.15%
11.47%
11.73%

13.46%
6.63%
14.75%
11.34%

Table 6: Improvement of Regularization Methods over BL:Text (Cellphone)

automatically determine the quality of reviews in order to avoid
such biases. Danescu-Niculescu-Mizil et al. [5] showed that the
perceived helpfulness of a review depends not only on its content
but also on the other reviews of the same product. We include one
of their hypotheses, i.e. conformity hypothesis, as a feature into
our model. A recent paper [15] took an un-supervised approach to
ﬁnding the most helpful book reviews. Although their method is
shown to outperform users’ votes, it is evaluated on only 12 books
and thus is not clear whether it is robust and generalizable.

The problem of assessing the quality of user-generated data is
also critical in domains other than reviews. For example, previous
works [2, 4] focused on assessing the quality of postings within
the community question/answering domain. The work in [2] com-
bines textual features with user and community meta-data features
for assessing the quality of questions and answers. In [4], the au-
thors propose a co-training idea that jointly models the quality of
the author and the review. However, their work does not model
user relationships, bur rather uses all community information for
exacting features.

Regularization using graphs has appeared as a type of effective
method in the semi-supervised learning literature [19]. The inter-
ested reader may examine [18, 20, 3]. The resulting formulation
is usually a well-formed convex optimization problem which has a
unique and efﬁciently computable solution. These types of graph
regularization methods have been successfully applied in Web-page
categorization [16] and Web spam detection [1]. In both cases, the
link structure among Web pages is nicely exploited by the regular-
ization which, in most cases, has improved the predictive accuracy
within the problem at hand. Recently, Mei et al. [13] propose to
enhance topic models by regularizing on a contextual graph struc-
ture. In our scenario, the social network of the reviewers deﬁnes
the context, and we exploit it to enhance review quality prediction.

7. CONCLUSION AND FUTURE WORK

In this paper we studied the problem of automatically determin-
ing review quality using social context information. We studied
two methods for incorporating social context in the quality pre-
diction: either as features, or as regularization constraints, based
on a set of hypotheses that we validated experimentally. We have
demonstrated that prediction accuracy of a text-based classiﬁer can
greatly improve, when working with little training data, by using
regularization on social context.
Importantly, our regularization
techniques make the general approach applicable even when social
context information is unavailable. The method we propose is quite
generalizable and applicable for quality (or attribute) estimation of
other types of user-generated content. This is a direction that we
intend to explore further.

As further future work, social context can be enhanced with ad-
ditional information about items and authors. Information about
product attributes, for example, enables estimates of similarity be-
tween products, or categories of products which can be exploited
as additional constraints. Furthermore, although a portal may lack
an explicit trust network, we plan to construct an implicit network
using the ratings reviewers attach to each others’ reviews and then

apply our techniques to this case. Finally, rather than predicting the
quality of each review, it would be interesting to adapt our tech-
niques for computing a ranking of a set of reviews.

8. REFERENCES
[1] J. Abernethy, O. Chapelle, and C. Castillo. Web spam identiﬁcation
through content and hyperlinks. In AIRWeb ’08, pages 41–44, 2008.

[2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne.
Finding high-quality content in social media. In WSDM, pages
183–194. ACM, 2008.

[3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A

geometric framework for learning from labeled and unlabeled
examples. Journal of Machine Learning Research, 7:2399–2434,
2006.

[4] J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. Learning to

recognize reliable users and content in social media with coupled
mutual reinforcement. In WWW, pages 51–60. ACM, 2009.

[5] C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg, and L. Lee.

How opinions are received by online communities: a case study on
amazon.com helpfulness votes. In WWW ’09, pages 141–150, 2009.

[6] A. Ghose and P. G. Ipeirotis. Estimating the Helpfulness and

Economic Impact of Product Reviews: Mining Text and Reviewer
Characteristics. (January 24, 2010), Available at SSRN:
http://ssrn.com/abstract=1261751.

[7] M. Hu and B. Liu. Mining and summarizing customer reviews. In

KDD, pages 168–177, 2004.

[8] M. Hu and B. Liu. Mining opinion features in customer reviews. In

AAAI, pages 755–760, 2004.

[9] C. Johnson. Us ecommerce forcast: 2008 to 2012.

http://www.forrester.com/Research/Document/
Excerpt/0,7211,41592,00.html.

[10] S.-M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.

Automatically assessing review helpfulness. In EMNLP, pages
423–430, Sydney, Australia, July 2006.

[11] J. Liu, Y. Cao, C.-Y. Lin, Y. Huang, and M. Zhou. Low-quality

product review detection in opinion summarization. In
EMNLP-CoNLL, pages 334–342, 2007. Poster paper.

[12] Y. Liu, X. Huang, A. An, and X. Yu. Modeling and predicting the

helpfulness of online reviews. In ICDM, pages 443–452, 2008.

[13] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic modeling with
network regularization. In WWW ’08, pages 101–110, 2008.

[14] M. E. J. Newman. The structure and function of complex networks.

SIAM Review, 45:167–256, 2003.

[15] O. Tsur and A. Rappoport. Revrank: a fully unsupervised algorithm

for selecting the most helpful book reviews. In ICWSM, 2009.

[16] T. Zhang, A. Popescul, and B. Dom. Linear prediction models with

graph regularization for web-page categorization. In KDD, pages
821–826. ACM, 2006.

[17] Z. Zhang and B. Varadarajan. Utility scoring of product reviews. In

CIKM ’06, pages 51–57, New York, NY, USA, 2006. ACM.

[18] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf.

Learning with local and global consistency. In NIPS, 2003.

[19] X. Zhu. Semi-supervised learning literature survey. Technical Report
1530, Computer Sciences, University of Wisconsin-Madison, 2005.
[20] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning

using gaussian ﬁelds and harmonic functions. In ICML, pages
912–919. AAAI Press, 2003.

[21] X. Zhu and A. B. Goldberg. Introduction to Semi-Supervised

Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine
Learning. Morgan & Claypool Publishers, 2009.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA700