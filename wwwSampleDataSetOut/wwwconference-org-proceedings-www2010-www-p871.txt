Monitoring Algorithms for Negative Feedback Systems

Mark Sandler

Google Inc

76 9th Avenue
New York, NY,

U.S.A.

sandler@google.com

S. Muthukrishnan

Google, Inc

76 9th Avenue
New York, NY,

U.S.A.

muthu@google.com

ABSTRACT
There are many online systems where millions of users post
original content such as videos, reviews of items such as
products, services and businesses, etc. While there are gen-
eral rules for good behavior or even formal Terms of Service,
there are still users who post content that is not suitable.
Increasingly, online systems rely on other users who view
the posted content to provide feedback.

We study online systems where users report negative feed-
back, i.e., report abuse; these systems are quite distinct from
much studied, traditional reputation systems that focus on
eliciting popularity of content by various voting methods.
The central problem that we study here is how to moni-
tor the quality of negative feedback, that is, detect negative
feedback which is incorrect, or perhaps even malicious. Sys-
tems address this problem by testing ﬂags manually, which
is an expensive operation. As a result, there is a tradeoﬀ
between the number of manual tests and the number of er-
rors deﬁned as the number of incorrect ﬂags the monitoring
system misses.

Our contributions are as follows:

• We initiate a systematic study of negative feedbacks
systems. Our framework is general enough to be ap-
plicable for a variety of systems. In this framework,
the number of errors the system admits is bounded
over the worst case of adversarial users while simul-
taneously the system performs only small amount of
manual testing for multitude of standard users who
might still err while reporting.

• Our main contribution is a randomized monitoring al-
gorithm that we call Adaptive Probabilistic Testing
(APT), that is simple to implement and has guaran-
tees on expected number of errors. Even for adversar-
ial users, the total expected error is bounded by εN
over N ﬂags for a given ε > 0. Simultaneously, the
number of tests performed by the algorithm is within
a constant factor of the best possible algorithm for
standard users.

• Finally, we present empirical study of our algorithm
that shows its performance on both synthetic data and
real

Copyright is held by the International World Wide Web Conference Com(cid:173)
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
.
ACM 978(cid:173)1(cid:173)60558(cid:173)799(cid:173)8/10/04.

data accumulated from a variety of negative feedback
systems at Google. Our study indicates that the algo-
rithm performs better than the analysis above shows.

Categories and Subject Descriptors
H.1.2 [Information Systems]: Models and Principles—
User/Machine Systems; I.2.0 [Computing Methodolo-
gies]: Artiﬁcial Intelligence—General

General Terms
Human Factors,Algorithms,Experimentation

Keywords
Negative Feedback Systems, User Reputation, Probabilistic
Analysis

1.

INTRODUCTION

The World Wide Web made information dissemination
much easier and more eﬃcient, from communication and
commerce to content. In particular, not only did more oﬄine
content come online and access to it made easier, but gener-
ation and publication of content has become easier. Millions
of users publish blogs, self-made videos, and post comments
or reviews of products and businesses such as hotels, restau-
rants, and others. Users also post and answer questions, tag
pictures and maps, form and nurture social networks, etc.
This giant publication system works by implicit understand-
ing that content should be appropriate (e.g., avoid porn),
legal (e.g., no illegally copied content), believed to be cor-
rect (e.g., when tagging maps and answering questions fac-
tually), or respectful of others privacy (e.g., with blogging
and social networking), etc.; explicitly, these are enforced
by Terms of Service that gives the platform provider the
right to remove content, cancel access, report to police, etc.
However, for a variety of reasons — non-professional users,
monetary incentives, lack of cost to action, etc. — such
user-generated content is always suspect. The challenge is
how to identify inappropriate content at the Internet scale
where millions of pieces of content being generated every
day. While sophisticated algorithms that explicitly identify
inappropriate content are used in cases (e.g., with ﬁnding
copyright violations), a common solution has been to rely
on the community, i.e., other users, to identify and “ﬂag”
inappropriate content.

We refer to such systems where users ﬂag or report in-
appropriate content as negative feedback systems. Exam-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA871ples of negative feedback systems include Amazon reviews1,
or YouTube comments2. There are many other examples.
These systems should be distinguished from others that rely
on user participation, in particular those that identify pop-
ular items based on user feedback. These use various voting
schemes to tap into the wisdom of crowds and aggregate
positive feedback, for say ranking content.3 Such ratings
systems rely on the fact that single popular item can score
thousands of ratings. In contrast, negative feedback systems
are expected to take action quickly, in some cases even after
a single ﬂag on certain content.

The community-based policing in negative feedback sys-
tems faces a diﬀerent challenge. How do the systems know
when a user’s ﬂag is genuine and something to be acted
upon? Flags may be simply incorrect due to an error on
the user’s part (mistaken about an answer to a query), or
maliciousness (ﬂag and remove a good review for a competi-
tor), or even cluelessness; in some cases, there may be re-
venge ﬂags. Known systems solve this problem by manually
evaluating some or all the ﬂags. There are two underlying
assumptions here. First, humans can identify a correct ﬂag.
Second, human testing of ﬂags is more scalable than human
testing of the original contents. These are reasonable as-
sumptions in many applications. As an example, consider
YouTube comments: humans can readily spot incorrect ﬂags
of spam, and the number of ﬂags is several orders of magni-
tude smaller than the number of either user-generated videos
or comments.4 However, as the systems grow in size, even
testing of all ﬂags becomes prohibitively expensive and be-
comes prone to denial of service style attacks. The challenge
is then reduced to a tradeoﬀ: number of ﬂags tested by hu-
mans versus the number of incorrect ﬂags the system misses.
Testing all the ﬂags by humans will be prohibitive but de-
tect all incorrect ﬂags, and testing none will allow far too
many incorrect ﬂags. In this paper, we study this tradeoﬀ.

1.1 Our Formulation

Consider a set of items I, and a user u who generates
a sequence of ﬂags i1, i2, . . . , iN . These ﬂags correspond
to the items ij that the user u deems abusive. The ﬂag
could either be true or false. A ﬂag ij is True means that
the item ij violates Terms of Service; a false ﬂag indicates
that the item is not abusive and user committed an error
(whether honestly or maliciously) when reporting it. For
each ﬂag, the monitoring algorithm performs one of the
three actions, A = {accept, reject, test}, and the outcome is
S = {accepted, rejected, positive, negative}. The ﬁrst two
states correspond to the case when algorithm accepted the
report and took appropriate action against the item (say, re-
moved or demoted it) or rejected the ﬂag (did not perform
any action against content) without any further testing. The
last two correspond to the case where the algorithm chooses
to perform external, human-based testing and discovered
1See
views.
Introduction-Algorithms-Third-Thomas-Cormen/dp/
0262033844/
2See
where reported spam comments are hidden by default.
3For example, see http://digg.com/about/.
4Of course, since humans do not test content that are not
ﬂagged by any user, there could potentially be false nega-
tives. This, however, is a reasonable outcome, since it indi-
cates that the content is not seen by too many users.

re-
http://www.amazon.com/

http://www.youtube.com/watch?v=4TpRAp0WWLs

This”
e.g.,

“Report

link

in

product

the true state of the ﬂag. If the algorithm chooses to test,
depending on the outcome of the test the system will either
accept or reject the item. We further assume that an item is
not ﬂagged multiple times; in fact, multiple ﬂags are seldom
seen in our applications and can be handled as a sequence
of independent, single ﬂags. In fact as we discuss later, very
few ﬂags per item, is a crucial diﬀerence between our system
and traditional reputation systems. Formally,

• User strategy is a function U : A∗ × r → {true, false}
that takes as an input vector A∗ of past actions of the
monitoring algorithm, a random vector r, and gener-
ates the next ﬂag.

• The monitoring algorithm R : S∗×r → A is a function
that takes as an input the vector S∗ of past outcomes, a
random vector r and returns the action for the current
ﬂag.

Interestingly, we do not assume any structure between
items (such as, two videos were posted by the same user)
or model the correlation between items with ﬂags (such as,
reviews in poor language tend to get ﬂags, or videos with
DJ mixes get fewer ﬂags). Our approach here is to focus on
users alone, and ignore the signals from the content. This
may be seen as analogous to web search where link analy-
sis, without content analysis, gives lot of information [5, 9].
The additional motivation for us is that focusing on user
analysis makes our model applicable across content types
(be they video, review text or others) and general. Finally,
notice that the monitoring algorithm has no access to user
strategy and it only learns something about user when it
decides to test the ﬂag, and not in other actions. This is
in contrast with standard learning with experts framework
where algorithm learns something on each action or step.

Say user u places N ﬂags. Given a randomized monitoring

algorithm A we measure:

• tA

u (N ), the expected number of tests performed by al-
gorithm A on the sequence of N ﬂags by user u, and

• eA

u (N ), the expected number of errors — an incorrect
ﬂag that is accepted or a correct ﬂag that is rejected
— by the algorithm for user u.

1.2 Our Contributions

To the best of our knowledge this is the ﬁrst eﬀort to for-
malize the problem of monitoring negative feedback. More
formally, we study the tradeoﬀ between tA
u (N ).
• Our main contribution is the design of a randomized
algorithm called Adaptive Probabilistic Testing (AP T )
to process ﬂags in real time as they are being submitted
by users, and a detailed theoretical analysis of it. We
prove that AP T satisﬁes the following:

u (N ) and eA

u

– [Adversarial Users] eAP T

(N ) ≤ εN in expecta-
tion against any user u and any ﬁxed ε, 0 ≤ ε ≤ 1.
The user strategy could be adversarial and user
can observe actions of the monitoring algorithms
once they are performed and adjust his strategy
arbitrarily.

– [Standard Users] We denote by STD(p) a stan-
dard user who errs with some probability p in-
dependently on each ﬂag. Let OPT be the op-
timal algorithm for monitoring such a user with

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA872eOPT
STD(p)(N ) ≤ εN . We show that for APT algo-
rithm we have:

tAP T
STD(p)(N ) ≤ 4 tOPT

STD(p)(N ) + o(N ).

in other words, the adaptive testing algorithm
performs within constant factor of the best pos-
sible algorithm for that particular type of user.

• We present an experimental study of our algorithm
with synthetic and real data collected from various
Google systems. Our algorithm satisﬁes eu(N ) ≤ εN
almost always (not only in expectation), and at the
same time, behaves signiﬁcantly better than our theo-
retical analysis, more like tAP T
STD(p)(N ) +
o(N ).

STD(p)(N ) ≤ tOPT

Thus the framework we use for evaluating any monitor-
ing algorithm involves two properties: (1) the number of
errors should be bounded in all cases including adversarial
users, and simultaneously, (2) in a system where overwhelm-
ing majority of the users are nonmalicious, the monitoring
algorithm should perform almost as well as the best possible
algorithm that satisﬁes (1).5 It is our experience that prac-
titioners do need both of the properties above. Systems need
to be robust when it comes to spammers, but also graceful
for majority of users who tend to be honest. Notice that it
is not trivial for a monitoring algorithm to satisfy these two
properties simultaneously. For example, a naive approach
would be to test a user at the beginning to see if the user
is standard, determine p and then thereafter run OPT for
user STD(p). This however will not satisfy the ﬁrst prop-
erty because a strategy available to a user is to pretend to be
STD(0) at the beginning (e.g. never lie) and then switch to
STD(1). In fact, our algorithm is far more graceful: if an ad-
versarial user becomes standard only for certain consecutive
number of ﬂags and is arbitrary elsewhere, our algorithm
will automatically behave like OPT for most of that portion
when user is standard. We do not formally abstract this
property for the various portions, and only focus on the two
properties above over the entire sequence of ﬂags.

1.3 Related Work and Technical Overview

Our model is very general and can be applied to many
real world negative feedback systems. Because of its gener-
ality however it is reminiscent of many other problems. In
particular it is reminiscent of “online label prediction prob-
lems”, where the task is to predict the label on each item.
But typically in such problems, there is an underlying well-
structured class of hypotheses and errors are measured with
respect to best in this class.
In contrast, users may have
arbitrary strategy in our problems, and no structured class
of hypotheses may ﬁt their behavior. In multiarmed ban-
dit [2], expert learning setting with limited feedback [6],
apple-tasting [7], and other learning problems, for each on-
line item, one is given the correct label for all experts or
arms after the action is performed, but in our problem, we
obtain the correct label only when we test an item. Hence
our monitoring algorithms have to work more agnostically.
Our approach is also reminiscent of large class of “reputa-
tion problems” where a user’s reputation is measured based

on his agreement with other users or item quality and is
used as weight to measure how much the system values user
feedback. Reputation problems arise in user driven systems
such as Wikipedia [1, 4] and others [3, 8]. Such reputation
schemes do not apply directly to negative feedback systems,
since our systems do not have opportunity to adjust a users’
errors based on other users.6

Despite the plethora of work in related areas, and practical
motivations, to the best of our knowledge there is very little
work done in the area of negative feedback systems. In fact,
the only work we are aware of is empirical paper by Zheleva
et al [10] that considers a problem of computing trust score
of e-mail users reporting spam/non-spam. Their results con-
siders a community-based scoring and a ﬁxed scoring func-
tion and present empirical results, however, their algorithm
provides no guarantees against malicious users.

Approaching our problem from the ﬁrst principles, it is
immediately clear that the monitoring algorithm has to use
randomization to determine what ﬂags to test. The algo-
rithm we design is simple and natural, keeping a testing
probability pi that is adjusted based on feedback from tests,
up or down based on whether tests reveal correct or incor-
rect ﬂags. The main diﬃculty is its analysis because of the
dependence of the state of the algorithm to the entire tra-
jectory of testing probabilities and user strategies. In par-
ticular, the analysis of expected behavior relies on careful
upper bounding the hitting times of sequences of Bernoulli
trials with stopping probabilities pi1 ≥ pi2 ≥ pil . . . . The
stopping probability itself changes non-deterministically, in
fact inﬂuenced by user strategy, over time in such a way that
there is no explicit upper bound on expectation. Instead, we
rely on expectations of hitting times conditioned on the fact
that stopping probability stays above some ﬁxed value, and
then generalize the results.

2. ALGORITHM AND INTUITION

In this section we describe our algorithm and provide in-

tuition why it works; we defer full analysis to Section 3.

We begin by presenting a monitoring algorithm which only
performs two actions: test and accept. This algorithm has
applications of its own such as when leaving abusive content
after it was reported is unacceptable for legal reasons. Sim-
ilarly, a monitoring algorithm which either rejects or tests
has applications of its own in systems where accidentally re-
moving content bears high cost. Later we generalize these
two algorithms into an universal monitoring algorithm with
at most ε1N false positive (accepts erroneous ﬂags) and at
most ε2N false negative errors (ignores correct ﬂags).

The high level idea behind the algorithm is as follows.
When ﬂag i arrives, the algorithm ﬂips a pi-biased coin and
tests the ﬂag with probability pi and accepts it otherwise.
After each ﬂag, the new probability pi+1 is computed. The
crux is to determine the update rule for the consequent of
pi’s. A naive approach would be to set p1 = ··· = pk = 1
for some k, and use the fraction of incorrect ﬂags out of k
as pj for j > k. However, if user changes strategy after k
ﬂags, this method will fail. A diﬀerent approach is to ﬁx
some window size w and test a fraction in each window i
to estimate the number ni of incorrect ﬂags, and use ni/w

5For calibration, tOPT
STD(p)(N ) = c(ε, p)N tests if p > ε and
O(1) otherwise. This is far fewer than what an optimal
algorithm needs for an adversarial user.

6If some item was demoted or deleted on request of a user,
it will no longer be equally visible to other users, and thus
likely to become uncorrectable.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA873as testing probability for subsequent window. Again, if user
changes strategy between windows, this method will fail and
make far too many errors. Thinking about this further, one
will realize that we need a more ﬂexible way to combine
testing probability, and knowledge from each testing. Our
approach is based on the following simple observation – if
we test ﬂags with probability p, then upon discovery of a
false ﬂag, the expected number of missed false ﬂags is 1−p
p ,
independently of user strategy. Indeed, since each false ﬂag
is tested with probability p, the expected stopping time on
a sequence of false ﬂags is 1−p
p . Thus, intuitively if between
two consecutive discoveries of false ﬂags the testing proba-
bility was in the range [p′, p′′] then we have both lower and
upper bound on the expected number of missed ﬂags in be-
tween as 1−p′
. The formal proof of this statement
is not obvious since the actual probability p′ is changing non-
deterministically and is in fact not bounded away from zero
in advance. A technical achievement in our paper is to in-
deed develop this idea formally as in Theorem 3.1, but this
intuition suﬃces. Using this, we can keep track of approxi-
mately how many false items we have accepted (e.g. number
of false positive errors), and thus can chose new pi in such
a way so that we satisfy the constraint on the number of
false positives in expectation. See Algorithm 1 for the full
details.

p′ and 1−p′′

p

Algorithm 1 Test-Accept Algorithm
Input: A stream of ﬂags.
Output: For each ﬂag we either accept or test it. If the ﬂag
is tested the algorithm learns its true state
Description:

1. Set testing probability pi = 1, estimated number of

false skipped ﬂags at L = 0;

2. For each ﬂag i,

(a) Test ﬂag with probability pi and accept otherwise.
(b) If ﬂag is tested and the ﬂag is false, set L ←

L + 1−pi
pi

(c) Set the new testing probability pi+1 ← 1

εi+1−L .

Test-accept and test-reject cases are symmetric (with de-
fault action accept action being replaced by reject, and in
step 2b, L increases if the ﬂag is true). For completeness
Algorithm 2, provides full details on test-reject algorithm.

Combining Two Cases.
The idea behind test-accept-reject algorithm is that we just
run test-accept and test-reject algorithms in parallel, with
only one of them being active and producing the next ac-
tion. After every step, both algorithms advance one step,
and we re-set active algorithm to the one which has lower
probability of testing. The complete algorithm is given on
Figure 3.

3. ANALYSIS

We ﬁrst analyze errors by the monitoring algorithm
against an adversarial user; later, we analyze the number
of tests it performs against a standard user.

Algorithm 2 Test-Reject Algorithm
Input: A stream of ﬂags.
Output: For each ﬂag we either reject or test it. If the ﬂag
is tested the algorithm learns its true state
Description:

1. Set testing probability pi = 1, estimated number of

true skipped ﬂags at L = 0;

2. For each ﬂag i,

(a) Test ﬂag with probability pi and reject otherwise.
(b) If ﬂag is tested and it is true, set L ← L + 1−pi
(c) Set the new testing probability pi+1 ← 1
εi+1−L .

pi

Algorithm 3 Adaptive Probabilistic Testing algorithm
Input: Stream of ﬂags, constants ε1, ε2
Output: For each ﬂag, output test, accept or reject
Description:

Let A and B be the test-accept and test-reject algo-
rithms respectively.
For each ﬂag, the algorithms A and B are run in parallel
to produce probabilities pAi and pBi .
If pAi < pBi , set algorithm A as active, B as passive.
Else, set B as active and A as passive.
Active algorithm ﬂips a coin performs its action and
updates its state. Passive algorithm is executed to up-
date its pi, but the suggested action is not performed.

3.1 Adversarial Users

We begin by analyzing the test-accept algorithm. For
each ﬂag this algorithm tests each ﬂag with certain prob-
ability and accepts it otherwise. Thus the only type of error
admitted is false positives, where algorithm accepts a false
ﬂag. Intuitively, how many undetected false ﬂags there are
between two detected ones? We begin by estimating the run
length until the ﬁrst detected ﬂag, if the testing probabilities
is some non-increasing sequence {pi}.

Lemma 3.1. Let {ri} be a sequence of Bernoulli tri-
als with parameter pi, where {pi} is monotonically non-
for
increasing sequence, and pi itself can depend on rj,
j < i. Let Q ∈ [0,∞] be the hitting time for the sequence
{r0, r1, . . .}. In other words random variable Q is equal to
the ﬁrst index i, such that ri = 1. Then for any γ, we have
the expectation bound:
E [Q|pQ ≥ γ] ≤ (1− γ)/γ and E [Q|pQ] ≤ (1− pQ)/pQ (1)
and further the realizations are concentrated around the ex-
pectation:
Pr [Q > c/γ |pQ ≥ γ] ≤ e−c and Pr [Q > c/pQ] ≤ e−c (2)
Proof. Consider sequence r(γ)
i = ri if pi ≥ γ
and is Bernoulli trial with probability γ otherwise. And
suppose Q(γ) is a hitting time for r(γ)

, such that r(γ)

i

. Then

i

EhQ(γ)i = EhQ(γ)|pQ ≥ γi Pr [pQ ≥ γ]

+ EhQ(γ)|pQ < γi Pr [pQ < γ] ≥ EhQ(γ)|pQ ≥ γi

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA874f1 = 2

f2 = 3

f3 = 4

f4 = 6

f5 = 7

True

False

False

False

True

False

False

Tested

Accepted

Accepted

R1 = 2 (2 missed false ﬂags)

Tested
g1 = 4

Accepted

Accepted

R2 = 1 (1 missed ﬂag)

Tested

g2 = 7

f6 = 9

False

Tested

g3 = 9

True

Tested

R3 = 0

Figure 1: A sequence of ﬂags. Gray ﬂag indicates that the ﬂag was not tested and its true state is unavailable
to the algorithm. fi indicates indices of all false ﬂags (both discovered and not), gi indicates realization of
indices of discovered false ﬂags. Ri is a realization of random variable “the number of undiscovered ﬂags
between two sequential gi’s”.

where in the ﬁrst transition we used the linearity of expecta-
tion and in the second we used the fact that for any ﬁxed se-

quence P = {pi}, EhQ(γ)|pQ ≥ γ, Pi < EhQ(γ)|pQ ≤ γ, Pi.
On the other hand Q(γ) and Q are equal to each other if
pQ ≥ γ. Thus, we have

similarly for probabilities we have:

E [Q|pQ ≥ γ] = EhQ(γ)|pQ ≥ γi ≤ EhQ(γ)i
Pr»Q ≥
γ |pQ ≥ γ–
γ–

γ |pQ ≥ γ– = Pr»Q(γ) ≥
≤ Pr»Q(γ) ≥

c

c

c

Now we just need to show EhQ(γ)i ≤ (1 − γ)/γ and
PrhQ(γ) ≥ cγi ≤ e−c. Observe that Q(γ) can be upper
bounded by geometric random variable G ≥ 0 with pa-
rameter γ. Indeed, let us suppose gi is 1 with probability
min{1, γ/pi} if r(γ)
i = 1, and is 0 otherwise. Uncondition-
ally each gi is 1 with probability γ. Thus, hitting time G
for {gi} is a geometric random variable, and by deﬁnition
G ≥ Q. Since expectation of G is (1− γ)/γ we have the ﬁrst
part of our lemma. The second part of equation (1) follows
from the deﬁnition of conditional expectation. To prove the
equation (2), we note that

Pr [G > c/γ] = (1 − γ)

c

γ +1 ≤ e−c

since it is exactly the probability that a sequence of Bernoulli
trials with identical probability γ does not hit 1 after c
γ
steps. Since Q(γ) ≥ G in the entire space, we have the
desired bound.

Theorem 3.2. For the test-accept algorithm, the expected

number of errors eu(N ) ≤ εN for an adversarial user u.
Proof. We count the expected number of undetected false
positives so far after we test the ith ﬂag. The crux is to con-
sider the underlying sequence of false ﬂags and correspond-
ing testing probability, and hide all the true ﬂags inside the
probability changes pi and apply lemma 3.1.

Suppose the false ﬂags have occurred at positions f1, f2
. . . fl. We do not know what those fi are, but our goal
is to show that for any sequence the desired lower bounds
holds. Denote ri a random variable that indicates whether
i-th false ﬂag has been accepted without testing. In other
words ri is a sequence of bernoulli trials each occurring with
probability 1 − pfi .

Consider g0, g1, ...gl′ where g0 = 0, and gi is an index
of the ith detected false ﬂag. In other words {gi} is a ran-
dom subsequence of {fi} where algorithm detected false ﬂag.
Note that while fi are unknown, gi are the steps of the al-
gorithm where we test and discover false ﬂags and thus are
known. Let Ri denote a random variable that is equal to
the number of false ﬂags between ﬂags gi−1 and gi. We il-
lustrate all the notation we used with an example on Figure
rfj . And thus
i=1 rfi , therefore it is suﬃcient for us to esti-

3.1. It is easy to see that Ri =Pj:gi−1≤fj <gi
Pl′
i=1 Ri = Pl
mate EhPl′

i=1 Rii. Note that Ri is a hitting time for the

sequence of pgi−1 , . . . pgi , where the sequence if over hidden
false ﬂags pgi is not bounded a-priori. Since our algorithm
does not increase testing probability if it does not detect
false ﬂags by Lemma 3.1

E [Ri|pgi ] ≤

1 − pgi

pgi

.

Further, note that for ﬁxed pgi the expectation is bounded
independently of all earlier probabilities and therefore:

l′

Xi=1

E [Ri|pg0 . . . pgi ] =

≤

l′

l′

Xi=1
Xi=1

EˆE [Ri|pgi ]˛˛pg1 , . . . pgi−1˜
1 − pgi
pgi ≤ εN.

where the last transition follows from the step 2c of Al-
gorithm 1, where we have pgi =
ε(gi−1)+1−L and thus
1−pgi
. Hence

1

1−pgj
pgj

j=1

pgi ≤ εgi − Lgi , where Lgi = Pi
Pl′

i=1
To ﬁnish the proof:

1−pgi
pgi ≤ εN.

l′

Xi=1

l′

E [Ri] = E2
Xi=1
4

E [Ri|pg1 ]3
5

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA875Expanding the right hand part, we have:

l′

Xi=2

E [Ri|pg1 ]3
5

E2
E [R1|pg1 ] +
4
= E2
4E [R1|pg1 ] + E [R2|pg1 , pg2 ] +
= ·· · = E2
E [Ri|g1, . . . gi]3
Xi=1
4
5
pgi 3
≤ E2
1 − pgi
4

5 ≤ εN

Xi

l′

l′

l′

Xi=3

E [Ri|pg1 , pg2 ]3
5

Similarly to above, the same results apply to the test-
reject algorithm. Combining these two results together we
have:

Theorem 3.3. For AP T monitoring algorithm the ex-
pected number of false positives is at most ε2N and the ex-
pected number of false negatives is at most ε2N .

Indeed, lets A denote the set of items where test-
Proof.
accept algorithm was active, and let B denote the set of
items where test-reject algorithm was active. During the B
phase, the test-accept algorithm did not produce any mis-
take (since no item was accepted), thus the expected number
of errors still test-accept is ε1N . More formally, we deﬁne
Ri as aa hitting times, of detecting false ﬂags, with an extra
constraint that the algorithm must have been active, and
the analysis carries through.

3.2 Standard Users

In this section, we consider standard users. Recall that for
a standard user, each ﬂag is incorrect with some unknown
probability pu. This models two dimensions about users
in negative feedback systems. First, even genuine users err
sometimes, but it is not correlated across items, and hence
we assume it is with some ﬁxed, independent, but unknown
probability pu. Second, some of non-malicious users might
be clueless and err; in such cases, the error probability pu is
again considered independently random, not correlated with
the items. We abstract these as STD(p) users. Note that p
may be small as in the ﬁrst case or large as in the second
case. What is the minimum number of tests we need to
perform to guarantee at most ε1N of false positive? Since
the user is random the only parameters we can tune are the
number of tests T , the number of accepts A and the number
of rejects R with the goal of minimizing T , since it does not
matter which ﬂags got tested:

T + A + R = N, Ap ≤ ε1N, R(1 − p) ≤ ε2N, min T

Thus if p ≤ ε1 then we can accept all the ﬂags and not do
any testing. On the other hand if p ≥ 1 − ε2, then we can
reject all ﬂags and again not perform any testing.
In the
general case it can be shown that the total fraction of tested
ﬂags will be at least 1 − ε1
1−p . In the case when ε2 = 0
we get the total fraction of ﬂags that needs to be tested is
at least p−ε1

and if ε1 = 0 it becomes 1−p−ε2

p − ε2

.

p

We now analyze the behavior of our algorithm and show
that for a standard user the algorithm is competitive with

1−p

respect to the optimal algorithm described above. Equiva-
lently, we prove that if p ≤ ε then the expected number of
tests is o(N ) and if p ≥ ε then it is bounded by 4 OPT.
As empirical evaluation in Section 4 shows, the analysis be-
low is very likely to be not tight, and providing a tighter
constant is an interesting open problem.

p

Theorem 3.4. For a STD(p) user with N ﬂags, each
false with probability p, the test-accept algorithm performs
ε tests if p ≤ ε, and γN + c tests oth-
in expectation βN
erwise, where γ = 4 p−ε
and c and β are O(1). Similarly
p
if p ≥ 1 − ε

test-reject algorithm performs at most βN
and 4 1−p−ε

1−p

ε

1−p N + c otherwise.

Proof. Suppose our target is ε fraction of errors. It is easy
to see that the algorithm can be reformulated as follows.
At step i test with probability
1+εi , and every time the
item is tested, the probability of testing resets back to 1
with probability p. The question then becomes what is the
expected number of tests we will perform? The full proof is
given in the appendix.

1

Finally, we analyze the performance of the AP T algorithm.

Theorem 3.5. Consider STD(p) a user u with N ﬂags.
The number of tests performed by the AP T algorithm is at
most 4 OP T + 2 max(ε1, ε2)N + o(N ).

Proof.

The total number of tests is the lesser of the number of
tests performed by either of the test-accept and test-reject
algorithms in isolation. Thus it is suﬃcient to only consider
ε1 ≤ p ≤ 1 − ε2, otherwise, by Theorem 3.4 the expected
number of tests is o(N). For the latter case we have, the
expected number of tests is:

tAP T
ST D(p)(N ) ≤ 4n min(1 −

ε1
p

, 1 −

ε2
1 − p

).

(3)

ST D(p)(N ) ≥ N (1− ε1

If p ≥ 1/2 , the number of tests performed by the optimal
1−p ) ≥ N (1− ε1
algorithm is tOP T
p −2ε2).
Similarly, for p ≤ 1/2 the number of tests is bounded by:
ST D(p(N ) ≥ N (1− ε1
tOP T
1−p −2ε1), combining
these two inequalities with equation (3) we have the desired
result.

p − ε2
1−p ) ≥ n(1− ε1

p − ε2

4. EXPERIMENTS

In this section we perform two kinds of experiments. First,
on synthetic data, we compare our algorithm with the opti-
mal algorithm which knows user strategy in advance. The
primary goal here is to show that not only the algorithm per-
forms only constant times as optimal, but to further demon-
strate that the constant is very close to 1.

Second, we present results from running our algorithm on
real data consisting of abuse reports submitted to several
Google properties. An important observation here is that
since our algorithms are not using the actual content in any
way, the only important quantity is the number of reports
per user. In all our experiments we assume that acceptable
error level for either false positive or false negative type of
errors is 0.1.

Synthetic data. We ﬁrst demonstrate the performance
(number of tests and number of errors) of the algorithm
against standard users. To achieve this we plot the optimal

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA876Figure 2: Experiments on synthetic data

s
t
s
e

t
/
s
r
o
r
r
e

 
f

o

 
r
e
b
m
u
N

600

500

400

300

200

100

0

Number of tests performed by the algortihm
Minimum number of tests required
False negatives
False positives

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

y
t
i
l
i

b
a
b
o
r
P

1

0.8

0.6

0.4

0.2

0

0

Testing probability
User probability of error
OPT
False-positive-error-rate
False-negative-error-rate

1000

2000

3000

4000

5000

The probability of error by the user

number of flags reported

(a) The number of tests and the number of errors admitted by
AP T algorithm when user happens to be STD(p) vs that by the
optimal algorithm that knows p.

(b) Performance of the AP T algorithm for STD(p) user that
changes p over time.

number of tests for ﬁxed acceptable error ε = 0.1 and p
changing from 0.01 to 1 for STD(p) against average number
of tests performed by AP T . For each p we assume 1000 ﬂags
and run the experiments 30 times, to get accurate estimate
of the expected number of tests performed. The results are
presented in Figure 3(a). It is clear that our algorithm is in
fact much closer to the optimal than the theoretical analysis
above suggests, and tAP T
STD(p)(N ) +
o(N ).

STD(p)(N ) is more like tOPT

On the Figure 3(b) we consider a user who changes his
error probability over time. The step-function with extreme
values between 0 and 1 is user real error rate. The best
optimal test rate (if the algorithm knew the underlying con-
stant), is step function bounded by 0.5. The line, closely
following the latter, shows the testing probability for the
AP T algorithm when STD(p) user keeps changing p. It is
clear that AP T algorithm automatically adjusts its testing
rate nicely to be nearly close to the best testing for STD(p)
for whatever p the user uses for a period of time.

Experiments with real data. In this section we present
experimental results that we perform using real data, which
contains a subset of abuse reports accumulated by various
Google services over the period of time of about 2 years.
The dataset contained roughly about 650 randomly selected
anonymized users who submitted at least 50 reports to the
system (some submitted considerably more). Their total
contribution was about 230,000 ﬂags. The algorithm com-
puted testing probability independently for every user, and
thus our guarantees apply for every user independently. Our
goal was to measure to average testing rate as a function of
the total number of ﬂags, since it translates to immediate re-
duction of amount of manual labor required. On Figure 3(c)
we plot the number of total ﬂags arrived into system (blue
curve), vs the total number of tested ﬂags (green curve).
The bottom three curves show the actual fraction of admit-
ted errors vs the acceptable error levels.

To illustrate the actual fraction of tested ﬂags we refer
to Figure 3(d). As one can see the testing ratio in general
hovers around 0.35, which means that only roughly 1 in

every 3 user ﬂags needs to get tested, and the remainder
can be acted on automatically.

5. FUTURE WORK AND CONCLUSIONS

We described a simple model for monitoring negative feed-
back systems, and presented the AP T algorithm with ex-
pected number of errors ≤ εN for even adversarial users;
for a standard user STD(p), the expected number of tests is
close to that of the optimal algorithm for STD(p). Practi-
tioners look for such algorithms that are resistant to adver-
sary users, while still being gracefully eﬃcient to standard
users. We have found these algorithms useful for some of
the Google systems.

u

From a technical point of view, the immediate open ques-
tion is if our analysis of AP T algorithm can be improved
since our experiments indicate tAP T
STD(p)(N ) behaves more like
tOPT
STD(p)(N ) + o(N ). Further, could we extend the expected
case analysis in this paper to high concentration bounds?
We are able to analyze the AP T algorithm and show that
the eAP T
(N ) is within twice the expectation with over-
whelming probability for any user, under certain mild con-
ditions. This result appears in Appendix B. We need to use
martingale inequalities to prove these bounds, but they re-
quire bounded diﬀerence, whereas hitting times of our algo-
rithm are not bounded. We overcome this problem by suit-
ably modifying Azuma’s inequality. This analysis may be
of independent interest. Similar high concentration bounds
for tAP T

STD(p)(N ) would be of interest.

From a conceptual point of view, negative feedback sys-
tems are abundant on the Internet, and we need more re-
search on useful monitoring algorithms. For example, an
extended model is to

consider items having attributes and typical user error as
some function of those attributes that we need to learn. Sim-
ilarly considering standard (e.g. non malicious) user whose
behavior evolves over time is also important. A potential
ﬁrst step in this direction is to compare the performance
of a monitoring algorithm with the optimal (but not nec-
essarily spam resistant) algorithm that is required to have

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA877s
g
a

l
f

#

180000

160000

140000

120000

100000

80000

60000

40000

20000

0

0

Total flags
Flags tested
Missed Positives
Missed Negatives
Acceptable Error Level

1

0.9

0.8

0.7

0.6

0.5

0.4

l

a

t

o

t
/

d
e

t
s
e

t

100

200

300

400

500

600

700

0

100

200

300

400

500

600

700

days

days

(c) The number of ﬂags that the system tested vs.
number of ﬂags vs. the number of errors

the total

(d) The ratio between number of tested ﬂags and the total num-
ber of ﬂags as a function of time

Figure 3: Performance of the algorithm on ﬂags received from real users.

bounded number of error on every preﬁx sequence of user
ﬂags for users that change their failure probability grace-
fully over time. Another potentially interesting direction
which has practical justiﬁcation is to allow monitoring algo-
rithms to take retroactive actions such as deciding to test an
item which was accepted or rejected earlier. Finally, a rich
direction is to not consider each user individually, but group
them according to their past history of reports. This allows
a reduction in the amount of testing for users who provide
only a few ﬂags since such users can contribute a signiﬁcant
fraction of ﬂags in many real-world systems. Such a group-
ing can be dynamic and depend on users’ strategies as well
as other properties, and guarantees will be relative to the
group. We hope our work here spurs principled research on
monitoring algorithms for negative feedback systems that
are much needed.

5.1 Acknowledgements

Authors would like to thank Nir Ailon, Raoul-Sam
Daruwala, Yishay Mansour and Ruoming Pang and anony-
mous reviewers for interesting discussion and good feedback
on the paper.

6. REFERENCES
[1] B. Adler and L. de Alfaro. A content-driven

reputation system for the Wikipedia. In Proc. of the
16th Intl. World Wide Web Conf.(WWW 2007).

[2] D. Berry and B. Fristedt. Bandit Problems. Chapman

and Hall, 1985.

[3] R. Bhattacharjee and A. Goel. Avoiding ballot

stuﬃng in eBay-like reputation systems. In
Proceedings of the 2005 ACM SIGCOMM workshop on
Economics of peer-to-peer systems, pages 133–137.
ACM New York, NY, USA, 2005.

[4] R. Bhattacharjee and A. Goel. Algorithms and
incentives for robust ranking. In SODA, pages
425–433. Society for Industrial and Applied
Mathematics Philadelphia, PA, USA, 2007.

[5] S. Brin and L. Page. The anatomy of a large-scale

hypertextual Web search engine. Computer networks
and ISDN systems, 30(1-7):107–117, 1998.

[6] N. Cesa-Bianchi and G. Lugosi. Prediction, learning,

and games. Cambridge University Press, 2006.

[7] D. Helmbold, N. Littlestone, and P. Long. Apple

tasting. Information and Computation, 161(2):85–139,
2000.

[8] S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina.

The eigentrust algorithm for reputation management
in p2p networks. In WWW ’03: Proceedings of the
12th international conference on World Wide Web,
pages 640–651, New York, NY, USA, 2003. ACM.

[9] J. Kleinberg. Authoritative sources in a hyperlinked

environment. Journal of the ACM, 46(5):604–632,
1999.

[10] E. Zheleva, A. Kolcz, and L. Getoor. Trusting spam

reporters: A reporter-based reputation system for
email ﬁltering. ACM Trans. Inf. Syst., 27(1):1–27,
2008.

APPENDIX

A. PROOF OF 3.4

1

Proof of Theorem 3.4. We reformulate the algorithm
equivalently: at step i test with probability
1+εi , and ev-
ery time we reset the probability p. The question then be-
comes what is the expected number of tests we will per-
form? Let Xi be the random variable that is 1 if there
were no reset until step i, and ith ﬂag was tested (whether
or not the probability was reset on ﬂag i.) Further let
Pi be the probability that there was no reset until step
i (whether or not reset happened on step i). Obviously
1+εi . The number of
tests that happened before and including the ﬁrst reset is
Pi
1+εi . Let Fn de-
notes the random variable indicating how many tests were
performed during the ﬁrst n steps. Let Fn,i denote random
variables indicating how many tests were performed by the

Pi = Qi−1
the random variable: EˆPn

1+εj ) and E [Xi] = Pi

j=1(1 − p

i=1 Xi˜ =Pn

i=1

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA878algorithm after i-th step respectively, provided that the reset
occurred at position i, and is 0 if the reset hasn’t occurred.
Also let Gn denote the number of tests performed by the
algorithm before and including the ﬁrst reset. Note that the
1+εi Fn−i. On
algorithm has no memory and thus Fn,i = Pi
the other hand since each test causes a reset with probabil-
ity p the expected number of tests before the ﬁrst reset can
be expressed as

p

Substituting that into (4) we have:

E [Fn] ≤

=

1
p

1
p

n−1

+

1 + εi

pPiγ(n + c − i)

Xi=1
+ (1 − Pn)(γn + c) −

=

1
p

(1 − Pn+1)

≤ γn + c +

1
p − Pnc − γp

Thus we need to prove:

n−1

pPiγi
1 + εi

Xi=1
Xi=1
(1 + ε)(1 − p + εi)α+1

i(1 − p + ε)α+1

n−1

.

1
p − Pncγ − γp

n−1

Xi=1

i(1 − p + ε)α+1

(1 + ε)(1 − p + εi)α+1 ≤ 0

(6)

x

(q + εx)α+1 =

n−1

Let q = 1 − p:
i
Xi=1
(q + ε)α+1 +

(q + εi)α+1 ≥

=

1

=

=

1

(q + ε)α+1 +

1

(q + ε)α+1 +

n

1

x=2

(q + ε)α+1 +Z n
(−1 + α)αε2(q + εx)α˛˛˛˛
−(q + αεx)
(−ε + p)p(q + εx)α˛˛˛˛

(p − ε)p(q + 2ε)α −

−(q + px)

1 + p

n

x=2

x=2

q + pn

(p − ε)p(q + εn)α

E [Gn] =

Pi

1 + εi

n

Xi=1
≤ min(

1
p

) ≤
Where in the ﬁrst upper bound, we used

ε

,

ε

ln(1 + εn)

ln(1 + εn)

1

1 + εi ≤Z n

1

1

εx + 1

dx ≤

ln(1 + εn)

ε

.

Thus:

X i = 1n
E [Fn] = E" n
Xi=1
Xi=1
Now we estimate Pn
ln[Pi] = ln(1 −

have:

i=1

=

n

Fn,i + Gn#

(4)

(5)

Pi

1 + εi

pE [Fn−i] +

ln(1 + εn)

ε

Pi
1+εi [1 + pE [Fn−i]]. First of all we

p

1 + ε

) +

i−1

Xj=2

ln(1 −

p

1 + εj

)

After some algebra, where we approximate the sum and
integrating, we have

e and β = 2 ln(1 + εn) 1+ε

Case p ≤ ε. We need to show that E [Fn] ≤ βnα where
α = p
ε . The proof is by induction.
The base is obvious, to prove the induction hypothesis we
have:

n

ε

ε

ε

+

≤

≤

≤

ln(1 + εn)

Pi × p × E [Fn−i]

1 + εi

ln(1 + εn)

ln(1 + εn)

ln(1 + εn)

E [Fn] ≤

β(1 − p + ε)α+1nα
(1 + ε)(1 − p + εn)α

Xi=1
+ (1 − Pn+1)Fn−1
+ βnα −
+ βnα − β
+ βnα −
where ﬁrst we used Pn−1
1+εi = 1 − Pn, then that (1− p +
ε) ≥ 1 and (1 − p + εn)α ≤ (1 + εn)α ≤ 1 + (εn)α α < 1,
1+εαn ≥ 1/2.
and ﬁnally,
Case p ≥ ε. We have to prove E [Fn] ≤ γn + c. If p ≥ 4/3ε
the result is obvious since 4 p−ε
p ≥ 1. We consider p ≤ 4/3ε
only. After some algebra, we get the following bound:

(1 + ε)(1 + εαnα)
2(1 + ε) ≤ βnα

ln(1 + εn)

≤

nα

pPi

i=1

nα

β

ε

ε

Pi
1 + εi ≥

(1 − p + ε)

p
ε +1
(1 + ε)(1 − p + iε)

p
ε +1

Pi ≥

(1 − p + ε)

p
ε +1

(1 + ε)(1 − p + (i − 1)ε)

p
ε

≥

1

(q + ε)α+1 +

1 + p

(p − ε)p(q + 2ε)α −

substituting we have:

p

1 + ε

n

Xi=1

i(q + ε)α+1
(q + εi)α+1 ≥

p(q + ε)α+1

1 + ε

`

−

q + pn

(p − ε)p(q + εn)α´

p

+

1 + ε

(1 + p)(q + ε)α+1

(1 + ε)(p − ε)(q + 2ε)α −

(q + pN )(q + ε)α+1
≥
(1 + ε)(p − ε)(q + nε)α
Note
term asymptotically behaves as
O(n1−α) = o(1), and on the other hand we have Pn ≥
(1+ε)(q+(n−1)ε)α , thus by adjusting c independently of n we
can guarantee:

(q+ε)α+1

that

last

the

cPn −

(q + pN )(q + ε)α+1

(1 + ε)(p − ε)(q + N ε)α ≥ 1

thus we have:

cPn +

p

1 + ε

p

1 + ε

+

n

i(q + ε)α+1
(q + εi)α+1 ≥
(1 + p)(q + ε)α+1

Xi=1
(1 + ε)(p − ε)(q + 2ε)α − 1

p

(1 + p)(q + ε)
(1 + ε)(p − ε)

1 + ε − 1 +
(p − 1 − ε)(p − ε) + (1 − p2)(1 − p + ε)

(1 −

1 − p + 2ε

p

)

(1 + ε)(p − ε)

(q + ε)2(q − p2 + ε)

(1 + ε)(p − ε)

≥

1

4(p − ε)

≥

≥

≥

≥

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA879Using this bound and substituting γ = 4(p−ε)
ately have (6) satisﬁed, as needed.

p

we immedi-

B. CONCENTRATION BOUNDS ON NUM(cid:173)

BER OF ERRORS

In this section we show that under certain conditions the
number of missed false ﬂags is also tightly concentrated
around its expectation. In particular we introduce an extra
constraint to algorithm 1, that requires that each pi ≥q 1
i .
Curbing at this probability adds O(√N ) extra tests, the
adversarial case is not aﬀected, and the standard user ex-
pectation only adds o(N ), thus the analysis carries through.
The proof is based on application of Azuma inequality to the
sequence of Ri with a twist that since Ri are unbounded we
need to bound each Ri with high probability.

Lemma B.1. Suppose X1 . . . Xn be a submartingale such

that Pr [Xi+1 − Xi ≥ ci] ≤ δ

n , then

Pr [Xn ≥ λ] ≤ exp[

] + δ

λ

i

2P c2

Proof.
In order to prove the inequality we introduce
bounded random variables that are almost always equal to
Xi, and then use Azuma inequality combined with union
bound. Let Yi = Xi − Xi−1, and let Y ′i = min(Yi, ci),
and X′i = Pn
i=1 Y ′i , then using Azuma inequality we have
Pr [X′m ≥ λ] ≤ exp[− λ
], Consider the underlying prob-
ability space and let Ω(.) denotes the subspace where con-
dition (.) is satisﬁed.

2 P c2
i

We have Ω(X′n 6= Xn) ⊆ Ω(Y ≥ ci, for some i), thus
Pr [X′n 6= Xn] ≤ nδ
n = δ. On the other hand we have
Ω(Xn ≥ γ) ⊆ Ω(X′n 6= Xn) ∪ (Ω(X′n ≥ γ) ∩ Ω(X′n = Xn)).
Thus:

Pr [Xn ≥ λ] ≤ PrˆX′n 6= Xn˜ + PrˆX′n 6= Xn˜

≤ δ + exp[−

λ
i=1 c2
i

]

2Pn

as needed.

have

Theorem B.2

(Concentration). If all pi ≥ q 1
Pr [(e(N ) − εN ) ≥ εN ] ≤ 2 exp[−ε2/3N 1/6 + log N ]

i we

.

Note that the constraint on pi is on the algorithm execution
(e.g. we control all pi), and not on the stopping times out-
comes, thus we are not introducing any hidden conditioning
and do not change distributions. Proof. The idea of the
proof is to apply Azuma inequality and use lemma B.1 to
get the desired bound.

c

1−pgi
pgi

Fix δ = exp[−ε2/3N 1/6 + log N ] and denote c = log N/δ,
By using lemma 3.1 we
and consider Ti =
have E [Ri − Ti|Ti, R0 . . . Ri−1] ≤ 0 for any Ti, and thus
E [Ri − Ti|R0 . . . Ri−1] < 0 thereforePi
i=0(Ri−Ti) is a sub-
martingale. On the other hand, we have:
PrhRi − Ti ≥ c√Ni ≤ Pr»Ri − Ti ≥
pgi– ≤ exp[−c] ≤
δ
N
Where we have used that gi ≤ N , constraint on pgi ≥ i
√gi
and the lemma 3.1 to bound PrhRi − Ti ≥ c
pgii.
Thus we can use lemma B.1 with bound ci ≤
log(N/δ)√N . For any l′ and λ = εN we have:
Pr2
4

5 ≤ δ + exp[−

Ri − Ti ≥ εN3

2l′√N log2(N/δ)
ε2√N

Xi

ε2N 2

l′

]

.

,

≤ exp[−

(log N − log δ)2 ] + δ

Now we substitute for log δ into the exponent and we get as
needed:

Ri − Ti ≥ εN3

l′

Pr2
Xi
4

ε2√N

5 ≤ δ + exp[−

(log N − log δ)2 ]
ε2√N
(ε2/3N 1/6)2 ] + δ ≤ 2δ.

≤ exp[−

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA880