Visualizing Differences in Web Search Algorithms Using

the Expected Weighted Hoeffding Distance

Mingxuan Sun

Guy Lebanon

Kevyn Collins-Thompson

Georgia Inst. of Technology

Georgia Inst. of Technology

Atlanta, GA 30332

Atlanta, GA 30332

Microsoft Research
Redmond, WA 98052

cynthia@cc.gatech.edu

lebanon@cc.gatech.edu

kevynct@microsoft.com

ABSTRACT
We introduce a new dissimilarity function for ranked lists,
the expected weighted Hoeﬀding distance, that has several
advantages over current dissimilarity measures for ranked
search results. First, it is easily customized for users who
pay varying degrees of attention to websites at diﬀerent
ranks. Second, unlike existing measures such as general-
ized Kendall’s tau, it is based on a true metric, preserving
meaningful embeddings when visualization techniques like
multi-dimensional scaling are applied. Third, our measure
can eﬀectively handle partial or missing rank information
while retaining a probabilistic interpretation. Finally, the
measure can be made computationally tractable and we give
a highly eﬃcient algorithm for computing it. We then apply
our new metric with multi-dimensional scaling to visualize
and explore relationships between the result sets from dif-
ferent search engines, showing how the weighted Hoeﬀding
distance can distinguish important diﬀerences in search en-
gine behavior that are not apparent with other rank-distance
metrics. Such visualizations are highly eﬀective at summa-
rizing and analyzing insights on which search engines to use,
what search strategies users can employ, and how search re-
sults evolve over time. We demonstrate our techniques using
a collection of popular search engines, a representative set of
queries, and frequently used query manipulation methods.

Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Re-
trieval; G.2.1 [Discrete Mathematics]: Combinatorics—
Permutations and combinations

General Terms
Algorithms, Measurement, Experimentation

Keywords
Expected Weighted Hoeﬀding Distance, Search Algorithm
dissimilarity, Ranking

1.

INTRODUCTION

Search engines return ranked lists of documents or web-
sites in response to a query, with the precise forms of the
ranked lists depending on the internal mechanisms of the
engines. We consider the problems of comparing and visu-
alizing the similarity relationships between diﬀerent search

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

algorithms. The term search algorithm is an intentionally
vague term corresponding to a mechanism for producing
ranked lists of websites in response to queries. We focus on
the following three interpretations of search algorithms: (a)
diﬀerent search engines, (b) a single search engine subjected
to diﬀerent query manipulation techniques by the user, and
(c) a single engine queried across diﬀerent internal states.

A visualization of relationships among type-(a) search al-
gorithms may be useful as it reveals which search engines
users should or should not use. For example, two search
engines that output very similar ranked lists may provide
redundant information to the user. On the other hand, two
search engines that output very dissimilar lists are worth ex-
amining more closely. We emphasize that we do not consider
here the issue of search quality or relevance. The techniques
developed in this paper allow users to understand the sim-
ilarity relationships among search algorithms. Evaluating
retrieval quality is a separate, well-studied problem that is
beyond the scope of this paper.

Visualizing relationships among type-(b) search algorithms
is useful as it indicates which query manipulation techniques
are worth using. For example, a query times square may
be manipulated by the user (prior to entering it in the search
box) to times AND square. Such manipulations may result
in diﬀerent ranked lists output by the search engine. Un-
derstanding the similarities between these ranked lists may
provide the user with insights into which manipulations are
redundant and which are worth exploring further.

Understanding the relationships among type-(c) search
algorithms is useful from the point of view of design and
modiﬁcation of search engines. Search engines are complex
programs containing many tunable parameters, each one in-
ﬂuencing the formation of the ranked list in a diﬀerent way.
For example, commercial search engines frequently update
the internal index which may result in diﬀerent ranked lists
output in diﬀerent dates (in response to the same query). It
is important for both the users and the search engineers to
understand how much the ranked lists diﬀer across consec-
utive days and how much this diﬀerence changes as internal
parameters are modiﬁed.

There are several techniques for visualizing complex data
such as search algorithms. One of the most popular tech-
niques is multidimensional scaling (MDS) [3], which trans-
forms complex high dimensional data s1, . . . , sm into 2-D
vectors z1, . . . , zm that are easily visualized by displaying
them on a 2-D scatter plot. Assuming that a suitable dissim-
ilarity measure between the high dimensional data ρ(si, sj )
has been identiﬁed, MDS computes the 2-D embedding of
the high dimensional data si 7→ zi ∈ R2, i = 1, . . . , m that

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA931minimizes the spatial distortion caused by the embedding

R(z1, . . . , zm) = X

i<j

(ρ(si, sj ) − kzi − zjk)2 .

(1)

In other words, the coordinates z1, . . . , zm in R2 correspond-
ing to s1, . . . , sm are selected to minimize the distortion

(z1, . . . , zm) = arg min

R(z′

1, . . . , z′

m).

(2)

z′

1,...,z′

m

Variations of MDS with slightly diﬀerent distortions (1) and
objective functions (2) may be found in [3].

Our contributions in this paper are (1) to develop a suit-
able dissimilarity function for search algorithms ρ(si, sj) and
study its properties, (2) to examine the use of MDS in the
context of visualizing search algorithms of types (a), (b),
and (c), and (3) to validate it using synthetic and web data.

2. RELATED WORK

The analysis of search engine outputs is a large area of
research within the IR community. Most approaches evalu-
ate rankings output by search engines against human judg-
ment or some other ground truth [8, 10, 21, 20].
In this
area, Carterette [4] recently enumerated the limitations of
Kendall’s tau for comparing system performance. He pro-
posed an alternative method for computing rank distance
that accounts for dependency between items, which is com-
puted as the solution to a minimization problem. The result-
ing measure, however, has problems with interpretation and
is highly dependent on the number of systems and queries
being analyzed, and is used instead as input to a p-value
diﬀerence estimator for comparing system performance. In
general, increased industry and research interest in measur-
ing dissimilarity between diﬀerent search engines has lead to
a variety of comparison tools1.

The key for comparing and visualizing ranked lists output
by search engines is to deﬁne an appropriate distance met-
ric. A straightforward way to measure such distance is to
compute the overlap of the two lists [13]. Such an approach
is problematic, however, as it is invariant with respect to re-
ordering or the ranked lists. Popular distance measures be-
tween permutations are Kendall’s tau, Spearman’s rho, the
footrule, Ulam’s distance, and Cayley’s distance [5]. There
are several ways to extend these permutation distances to
partially ranked lists output by search engines, including
Hausdorﬀ distance [5] and expected distances [1, 15].

Substantial research on the interaction between users and

search engines [9, 11] shows that users’ attention drops quickly
from top to bottom ranks2. One problem with many pro-
posed dissimilarities, however, is that they do not distin-
guish between disagreement at top rankings and at the bot-
tom rankings. One exception is [7] who considers stage-wise
ranking processes that generalize Kendall’s tau. This gener-
alization, however, is not unique as it depends on the order
in which the diﬀerent ranking stages are selected. Rank cor-
relation coeﬃcient such as NDCG [10] adopts inverse loga-
rithm function as the discount rank factor. However it is not
symmetric and is intended as an evaluation measure against
ground truth, not a comparison measure between ranked
lists. Another example is the inverse measure [2] that em-

1e.g.www.bing-vs-google.com, www.searchrater.com
2A popular discount function is the logarithm function but
other discount functions have been proposed as well.

phasizes disagreement at top ranks, where the weight func-
tion decays linearly with the rank.

While much previous work on visualization of search al-
gorithms is based on document similarity, e.g. [17], there is
renewed interest in visualizing and analyzing set-level diﬀer-
ences between results from diﬀerent search systems. Fagin
et al. [6] and Bar-Ilan et al. [2] investigate the relationship
among engines by examining the pairwise distance matrix
but do not make the connection with visualization. Liggett
and Buckley [14] used multi-dimensional scaling over rank-
ing dissimilarity to examine search system variations due
to the eﬀect of query expansion, where the dissimilarity
was based on Spearman’s coeﬃcient. In addition, tools like
MetaCrystal [19] regard search results as a set of items and
visualize the common items among diﬀerent engines. Tem-
poral studies of search engines have been examined by [2]
and [20] who compare search engine results across multiple
time periods.

3. DISTANCES AND DISSIMILARITIES

As mentioned in Section 1, eﬀective visualization of search
algorithms using MDS depends on the quality of the dissim-
ilarity measure ρ. We describe in this section a new measure
based on the expectation of the weighted Hoeﬀding distance
on permutations and examine its properties.

We start by considering several desired properties for ρ(si, sj).

It should be (i) symmetric, (ii) interpretable with respect to
search algorithms retrieving ranked lists of diﬀerent lengths,
(iii) ﬂexible enough to model the increased attention users
pay to top ranks over bottom ranks, (iv) computationally ef-
ﬁcient, and (v) aggregate information over multiple queries
in a meaningful way.

The symmetry property (i) is relatively straightforward.
Property (ii) addresses an important and often overlooked
issue. How should ranks in a short ranked list be compared
with ranks in a long one? How should we count websites that
appear only in the longer (or shorter) ranked list? Many
previously proposed dissimilarity measures provide ad-hoc
answers to these questions which may lower the quality of
the MDS embedding. Property (iii) refers to the diﬀerences
in attention users pay to websites listed in top vs. bottom
ranks. The dissimilarity measure should take this into ac-
count and provide a dissimilarity similar to that experienced
by users, as opposed to a rank-symmetric formula. The ef-
ﬁciency property (iv) can be critical for online use and we
address that in Sec. 3.2. Property (v) refers to the fact that
ρ(si, sj) should aggregate information from multiple queries
with each query contributing the “correct” amount to the
ﬁnal dissimilarity ρ(si, sj).

Dissimilarity functions examined in previous studies sat-
isfy some but not all of these properties (see Figure 2). In
this paper we propose to deﬁne ρ using an expectation over
the weighted Hoeﬀding distance. The expectation and the
properties of the weighted Hoeﬀding distance provide a clear
probabilistic interpretation and ensure that properties (i)-
(v) are satisﬁed.

We start by deﬁning the weighted Hoeﬀding distance which
is a novel distance on permutations dw(π, σ). The weight
vector w provides the ﬂexibility necessary for satisfying (iii)
while the metric property satisﬁes (i). We then extend it to
a dissimilarity ρ(si(q), sj(q)) over ranked lists si(q), si(j) by
taking expectations with respect to the sets of permutations
S(si(q)), S(sj (q)) consistent with the ranked lists. Above,
we consider search algorithms si, sj as functions from queries

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA932to ranked lists and si(q) represents the ranked list retrieved
by si in response to the query q.

The function ρ is extended to search algorithms by tak-
ing another expectation, this time with respect to queries
q sampled from a representative set of queries Q. The ex-
pectations ensure that properties (ii) and (v) are satisﬁed.
We derive an eﬃcient closed form for the double expectation
that veriﬁes property (iv) in Sec. 3.2 and give a pseudo-code
implementation.

Formally, we deﬁne

ρ(si, sj) = E q∼Q{ρ(si(q), sj (q))}

= E q∼Q E π∼S(si(q))E σ∼S(sj (q)){dw(π, σ)}

(3)

where dw(π, σ) is a distance between permutations π, σ de-
ﬁned in Section 3.1, E π∼S(si(q))E σ∼S(sj (q)) is the expecta-
tion with respect to permutations π, σ that are sampled from
the sets of all permutations consistent with the ranked lists
output by the two algorithms si(q), sj(q) (respectively), and
E q∼Q is an expectation with respect to all queries sampled
from a representative set of queries Q. In the absence of any
evidence to the contrary, we assume a uniform distribution
over the set of queries Q and over the sets of permutations
consistent with si(q), sj(q).

We proceed with a description of dw(π, σ) in Section 3.1
and then follow up in Section 3.2 with additional details
regarding the expectations in (3) and how to compute them.

3.1 Weighted Hoeffding Distance

The weighted Hoeﬀding distance is a distance between
permutations, here considered as permutations over the n
indexed websites in the internet3. The fact that n is ex-
tremely large should not bother us at this point as we will
derive closed form expressions eliminating any online com-
plexity terms depending on n. For simplicity we refer to the
websites using the integers {1, . . . , n}.

A permutation over n websites π is a bijection from {1, . . . , n}

to itself mapping websites to ranks. That is π(6) is the rank
given to website 6 and π−1(2) is the website that is assigned
second rank. A permutation is thus a full ordering over the
entire web and we denote the set of all such permutations
by Sn. We will represent a permutation by a sorted list of
websites from most preferred to least, separated by verti-
cal bars i.e. π−1(1)| · · · |π−1(n); for example, for n = 5 one
permutation ranking item 3 as ﬁrst and 2 as last is 3|5|1|4|2.
Our proposed distance dw(π, σ) is a variation of the earth
movers distance4 [18] on permutations. It may also be re-
garded as a weighted version of the Hoeﬀding distance [15].
It is best described as the minimum amount of work needed
to transform the permutation π to σ. Work, in this case, is
the total amount of work needed to bring each item from its
rank in π to its rank in σ i.e., the r-item is transported from
rank k = π(r) to l = σ(r) (for all r = 1, . . . , n) requiring
wk + · · · + wl−1 work (assuming k < l) where wk is the work
required to transport an item from rank k to k + 1. For
example, the distance d(1|2|3, 2|1|3) is w1 + w1 due to the
sequence of moves 1|2|3 → |1, 2|3 → 2|1|3. Another example

3There are several ways to deﬁne the number of indexed
websites in the internet. In any case, this number is very
large and is growing continuously. We avoid its dynamic
nature and consider it as a ﬁxed number.
4The earth mover distance between two non-negative valued
function is the minimum amount of work needed to trans-
form one to the other, when the functions are viewed as
representing spatial distributions of earth or rubble.

dw(π, σ) =

d′
w(u, v) =

r=1

X
8><
Pv−1
t=u wt
d′
w(v, u)
>:
0

(4)

(5)

if u < v
if u > v
otherwise

.

is d(1|2|3, 3|1|2) = w1 + w2 + w2 + w1 due to the sequence
of moves 1|2|3 → |1, 2|3 → |1|2, 3 → |1, 3|2 → 3|1|2.

Formally, the distance may be written as

n

d′
w(π(r), σ(r)) where

The weight vector w = (w1, . . . , wn−1) allows diﬀerentiat-
ing the work associated with moving items across top and
bottom ranks. A monotonic decreasing weight vector, e.g.,
wt = t−q, t = 1, . . . , n − 1, q ≥ 0 correctly captures the fact
that disagreements in top ranks should matter more than
disagreements in bottom ranks [9, 11, 16]. The exponent q
is the corresponding rate of decay. A linear or slower rate
0 ≤ q ≤ 1 may be appropriate for persistent search engine
users who are not very deterred by low-ranking websites.
Choosing q → 0 retrieves a weighting mathematically simi-
lar to the log function weighting that is used in NDCG [10]
to emphasize top ranks. A quadratic or cubic decay q = 2, 3
may be appropriate for users who do not pay substantial
attention to bottom ranks. The weight may be modiﬁed to
wt = max(t−q − ǫ, 0), ǫ > 0 to capture the fact that many
users simply do not look at results beyond a certain rank.
While it is possible to select an intuitive value of q, it is
more desirable to select one that agrees with user studies.
An MDS embedding of permutations using dw appears in
Figure 1 (see Section 4 for more details).

Proposition 1. Assuming w is a positive vector, the weighted

Hoeﬀding distance (4) is a metric.

Proof. Non-negativity dw(π, σ) ≥ 0 and symmetry dw(π, σ)

= dw(σ, π) are trivial to show. Similarly it is easy to see that
dw(π, σ) = 0 iﬀ π = σ. The triangle inequality holds as

dw(π, σ) + dw(σ, ϕ) =

≥

n

X

r=1

n

X

r=1

d′
w(π(r), σ(r)) + d′

w(σ(r), ϕ(r))

d′
w(π(r), ϕ(r)) = dw(π, ϕ)

(6)

where the inequality (6) holds due to the positivity of w.

The weighted Hoeﬀding distance has several nice proper-
ties that make it more appropriate for our purposes than
other permutation measures. First, it allows customization
to diﬀerent users who pay varying degrees of attention to
websites in diﬀerent ranks (typically higher attention is paid
to higher ranks). Standard permutation distances such as
Kendall’s tau, Spearman’s rho, the footrule, Ulam’s distance
and Cayley’s distance treat all ranks uniformly [5]. Second,
it is a true metric in contrast to the generalized Kendall’s tau
[7]. Third, its clear interpretation allows explicit speciﬁca-
tion of the weight vector based on user studies. Finally, it is
computationally tractable to compute the weighted Hoeﬀd-
ing distance as well as its expectation over partially ranked
lists corresponding to ρ in (8). Figure 2 summarizes the
advantages of our distance over other dissimilarities.

3.2 Ranked Lists and Expectations

A ranked list output by a search algorithm forms an or-
dered list hi1, . . . , iki of a subset of the websites {i1, . . . , ik} ⊂

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA93312543

21543

21453

12453

12534

21534

12435

21435

21354

12354

12345

21345

12345

12354

12435

12453

12534

12543

21345

21354

21435

21453

12345
12354

12435
12453

12534
12543

21534

21543

21345
21354

21435
21453

21534
21543

Figure 1: MDS embedding of permutations over n = 5 websites. The embeddings were computed using the weighted Hoeﬀding distance
with uniform weight function wt = 1 (left), linear weight function wt = 1/t (middle) and quadratic weight function wt = 1/t2 (right).
The permutations starting with 1 and 2 (colored in red) and the permutations starting with 2 and 1 (colored in blue) become more
spatially disparate as the rate of weight decay increases. This represents the increased importance assigned to agreement in top ranks
as we move from uniform to linear and quadratic decay.

Kendall/Spear [5]
Fligner Kendall [7]
E Kendall top k [6]

E Spearman [14]
InverseMeasure [2]

NDCG [10]

(i)
X

(ii)

(iii)

(v)

(iv)
X

X X

X X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

E Weighted Hoeﬀding X X

Figure 2: Summary of how diﬀerent dissimilarities satisfy prop-
erties (i)-(v) in Section 3.

{1, . . . , n}. Diﬀerent search strategies may result in lists of
diﬀerent sizes but in general k is much smaller than n. In
addition to the notation hi1, . . . , iki we also denote it using
the bar notation as

i1|i2| · · · |ik|ik+1, . . . , in where

{ik+1, . . . , in} = {1, . . . , n} \ {i1, i2, · · · , ik}

(7)

indicating that the unranked items {1, . . . , n} \ {i1, . . . , ik}
are ranked after the k items. Partial rankings (7) are not
identical to permutations as there is no known preference
among the unranked items {1, . . . , n} \ {i1, . . . , ik}. We
therefore omit vertical lines between these items and list
them separated by commas i.e., 3|2|1, 4 is equivalent to the
ranked list h3, 2i which prefers 3 over 2 and ranks 1 and 4
last without clear preference between them.

It is natural to identify a ranked list hi1, . . . , iki as a full
permutation of the web that is unknown except for the fact
that it agrees with the website ranking in hi1, . . . , iki. De-
noting the set of permutations whose website ordering does
not contradict hi1, . . . , iki as S(hi1, . . . , iki), we have that
hi1, . . . , iki corresponds to a random draw from S(hi1, . . . , iki).
Assuming lack of additional knowledge, we consider all per-
mutations in S(hi1, . . . , iki) as equally likely resulting in

ρ(hi1, . . . , iki, hj1, . . . , jli)

def

= E π∼S(hi1,...,iki),σ∼S(hj1 ,...,jli)d(π, σ)

1

=

(n − k)!(n − l)! X

π∈S(hi1,...,iki)

X

σ∈S(hj1,...,jl i)

d(π, σ).

(8)

For example, consider the case of n = 5 with two search
strategies returning the following ranked lists h3, 1, 4i =
3|1|4|2, 5 and h1, 5i = 1|5|2, 3, 4. The expected distance is

ρ(3|1|4|2, 5, 1|5|2, 3, 4) =

1

2 · 6

(d(3|1|4|2|5, 1|5|2|3|4)

+ d(3|1|4|5|2, 1|5|2|3|4) + · · · + d(3|1|4|2|5, 1|5|4|3|2)

Expression (8) provides a natural mechanism to incorpo-
rate information from partially ranked lists. It is diﬃcult to
compare directly two ranked lists hi1, . . . , iki, hj1, . . . , jli of
diﬀerent sizes. However, the permutations in S(hi1, . . . , iki)
and S(hj1, . . . , jki) are directly comparable to each other as
they are permutations over the same set of websites. The
expectation (8) aggregates information over such directly
comparable events to provide a single interpretable and co-
herent dissimilarity measure. Figure 3 displays the MDS
embedding for partial rankings using the expected ρ (8) (see
Section 4 for more details).

The expectation deﬁning ρ in (8) appears to require in-
surmountable computation as it includes summations over
(n − k)!(n − l)! elements with n being the size of the web.
However, using techniques similar to the ones developed in
[15] we are able to derive the following closed form.

Proposition 2. The following closed form applies to the
expected distance over the weighted Hoeﬀding distance (4).

ρ(hi1, . . . , iki, hj1, . . . , jli) =

n

X

r=1

¯d(r) where

(10)

¯d(r)=

8>>><
>>>:

d′
w(u, v)
n−l Pn
1
n−k Pn

1

1

n−k

t=l+1 d′
t=k+1 d′

w(t, u)
w(t, v)
t=k+1 Pn

n−l Pn

1

r ∈ A ∩ B
r ∈ A ∩ Bc
r ∈ Ac ∩ B

.

s=l+1 d′

w(t, s) otherwise

Above, A = {i1, . . . , ik}, B = {j1, . . . , jl}, and u ∈ {1, . . . , k},
v ∈ {1, . . . , l} are the respective ranks of r in {i1, . . . , ik} and
{j1, . . . , jl} (it they exist).

Proof. A careful examination of (4) reveals that it may

be written in matrix notation:

d(π, σ) = tr(Aπ△AT
σ )

(11)

where tr is the trace operator, △ is the n×n distance matrix
with elements △uv = d′
w(u, v), and Aπ, Aσ are permutation
matrices corresponding to the permutations π and σ i.e.,
[Aπ]uv = 1 iﬀ π(u) = v. Using equation (11), we have

ρ(hi1, . . . , iki, hj1, . . . , jli)

= Pπ∈S(hi1,...,iki) Pσ∈S(hj1,...,jk i) tr(Aπ△AT
σ )

(n − k)!(n − l)!

+ d(3|1|4|5|2, 1|5|4|3|2)).

(9)

= tr( ˆMhii△( ˆMhji)T )

(12)

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA93415

51

4

14

41

45

54

4

42

41

45

43

42

5

14

12345

24

25

52

15

1

24

25

1

12

21

13

31

2

23

32

43

34

3

53

12

12345

35

13

21

2

31

23

32

1

14

15

12

13

12345

4

41

42

43 45

54

51

5

52

53

21

35

2

24

25

23

34

3

54

51

52

5

53

31

34

32

35

3

Figure 3: MDS embedding of ranked lists of varying lengths (k varies) over a total of n = 5 websites. The embeddings were computed
using the expected weighted Hoeﬀding’s distance (8) with uniform weight function wt = 1 (left), linear weight function wt = 1/t (middle)
and quadratic weight function wt = 1/t2 (right). We observe the same phenomenon that we saw in Figure 1 for permutations. The
expected distance (8) separates ranked lists agreeing in their top rankings (denoted by diﬀerent colors) better as the weights decay faster.

where
ˆMhii = Pπ∈S(hi1,...,ik i) Aπ

(n − k)!

,

ˆMhji = Pσ∈S(hj1,...,jli) Aσ
(13)

(n − l)!

.

Note that the marginal matrices ˆMhii, ˆMhji have a proba-
bilistic interpretation as their u, v entries represent the prob-
ability that item u is ranked at v. Combining (12) with
Lemma 1 below completes the proof.

Lemma 1. Let ˆM be the marginal matrix for a top-k ranked
list hi1, . . . , iki with a total of n items as in (13). If r ∈
{i1, . . . , ik} and r = is for some s = 1, . . . , k, then ˆMrj = δjs
where δab = 1 if a = b and 0 otherwise. If r 6∈ {i1, . . . , ik}
then ˆMrj = 0 for j = 1, . . . , k and 1/(n − k) otherwise.

Proof. For a top-k ranking hi1, . . . , iki out of n items,
the size of the set S(hi1, . . . , iki) is (n − k)!. Each of the
permutations compatible with it has exactly the same top-
k ranks.
If r ∈ {i1, . . . , ik} and r = is for some s =
1, . . . , k then the number of permutations compatible with
hi1, . . . , iki that assign rank s to the item is (n − k)!. Simi-
larly, the number of consistent permutations assigning rank
other than s to the item is 0. As a result we have ˆMrs =
(n−k)! = 1 and ˆMrj = 0 for j 6= s. If r 6∈ {i1, . . . , ik}, the
(n−k)!
number of permutations consistent with the ranked list that
assign rank j ∈ {k + 1, . . . , n} to the item is (n − k − 1)!.
Similarly, the number of permutations that assign rank j ∈
{1, . . . , k} to the item is 0. As a result ˆMrj = 0 for j =
1, . . . , k, and ˆMrj = (n−k−1)!

n−k for j = k+1, . . . , n.

(n−k)! = 1

The expected distance (8) may be computed very eﬃ-
ciently, assuming that some combinatorial numbers are pre-
computed oﬄine. Bounding k, l by a certain number m such
that k, l ≤ m ≪ n, the online complexity is O(k + l) and the
oﬄine complexity is O(n + m2). Proposition 3 makes this
precise. A pseudo-code description of the distance compu-
tation algorithm is given as Algorithm 3.1.

Proposition 3. Let hi1, . . . , iki and hj1, . . . , jli be top-k
and top-l ranks on a total n items with k, l ≤ m ≪ n. As-
suming that d′
w(u, v) in (4) is computable in constant time
and space complexity (as is the case for many polynomial
decaying weight vectors w), the online space and time com-
plexity is O(k + l). The oﬄine space complexity is O(m2)
and the oﬄine time complexity is O(n + m2).

kl = 1
n−k

1

m×m and DE2

w(t, v), and DE2

kv = 1
s=l+1 d′

Proof. From equation 10, the oﬄine pre-computation
m×m, where Duv =

requires computing Dm×m, DE1
n−k Pn
w(u, v), DE1
d′
t=k+1 d′
n−l
Pn
t=k+1 Pn
w(t, s). The space complexity for comput-
ing these matrices is O(m2). The time complexity to com-
pute Dm×m is O(m2). Exploiting features of cumulative
sums and the matrix Dm×m it can be shown that comput-
m×m requires O(n + m2) time. Similarly, comput-
ing DE1
m×m requires O(n + m2) time. As a result, the to-
ing DE2
tal oﬄine complexity is O(m2) space and O(n + m2) time.
Given the three precomputed matrices, computing the ex-
pected distance for two partially ranked lists hi1, . . . , iki and
hj1, . . . , jli requires O(k+l) time and space. The reasons are
that given two lists, the time to identify overlapping items
from two lists of size k and l is O(k + l) and that for items
ranked by at least one engine, we need to use the look-up
table no more than k + l times and another extra look-up
for items never ranked in both.

4. SIMULATION STUDY

To evaluate the proposed framework we conducted two
sets of analyses. The ﬁrst analysis (in this section) uses
synthetic data to examine properties of the embedding and
compare it to alternative methods under controlled settings.
The second set of experiments (next section) includes real
world search engine data.
Its goal is to demonstrate the
beneﬁt and applicability of the framework in the context of
a real world visualization problem.

We start by examining the embedding of permutations
over n = 5 websites. A small number is chosen intentionally
for illustrative purposes. We consider two sets of permuta-
tions. The ﬁrst set contains all permutations ranking item
1 ﬁrst and item 2 second. The second set contains all per-
mutations ranking item 2 ﬁrst and item 1 second. Figure 1
displays the MDS embedding of these two sets of permuta-
tions based on the weighted Hoeﬀding’s distance with con-
stant weights wt = 1 (left), linear weight wt = t−1 (middle)
and quadratic weight wt = t−2 (right). The ﬁrst set of per-
mutations are colored red and the second set blue.

The uniform weight MDS embedding does not pay par-
ticular attention to diﬀering websites in the top ranks and
so the red and blue permutations are interspersed together.
This is also the embedding obtained by using the Kendall’s
tau distance as in [5, 6, 12]. Moving to linearly decaying
and quadratic decaying weights increases the separation be-
tween these two groups dramatically. The diﬀerences in web-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA935Oﬀ-line:
1. Specify n, the number of total items and m the list length bound.
2. Precompute matrices Dm×m, DE1
On-line:
3. Call Expected-Weighted-Hoeﬀding(π, σ) for lists π and σ

m×m (Section 3).

m×m and DE2

Expected-Weighted-Hoeffding(π, σ)

if πmark[i] > 0

then sum ← sum + D[i, πmark[i]]
else

1 k1 ← size(π)
2 k2 ← size(σ)
3 [πmark, σmark ] = Mark-Rank(π, σ);
4 sum ← 0;
5 for i ← 1 to k1
6 do
7
8
9
10
11
12 count ← 0
13 for i ← 1 to k2
14 do
15
16
17
18
19 sum ← sum + (n − k1 − count) · DE2[k1, k2];
20 return sum;

then sum ← sum + DE1[k1, i]

sum ← sum + DE1[k2, i]

if σmark[i] == 0

count ← count + 1

Mark-Rank(a, b)
1 k1 = size(a)
2 k2 = size(b)
3 amark = zeros(1 . . . k1), bmark = zeros(1 . . . k2)
4 for i ← 1 to k1
5 do for j ← 1 to k2
6
do if a[i] = b[j]
7
8
9

then amark[i] = j
bmark[j] = i

10 return [amark , bmark]
Algorithm 3.1: Algorithm to compute expected weighted Ho-
eﬀding distance between two ranked lists π and σ. The online
complexity of the above algorithm is O(k1k2). A slightly more
complex algorithm can achieve online complexity O(k1 + k2) as
described in Proposition 3.

sites occupying top ranks are emphasized while diﬀerences in
websites occupying bottom ranks are de-emphasized. This
demonstrates the ineﬀectiveness of using Kendall’s tau dis-
tance or uniform weight Hoeﬀding distance in the context
of search engines. The precise form of the weight - linear,
quadratic, or higher decay rate depends on the degree to
which a user pays more attention to higher ranked websites.
The second simulated experiment is similar to the ﬁrst,
but it contains partially ranked lists as opposed to permu-
tations. We form ﬁve groups - each one containing par-
tially ranked lists ranking a particular website at the top.
Figure 3 displays the MDS embedding of these ﬁve sets of
permutations based on the expected weighted Hoeﬀding dis-
tance ρ(hi1, . . . , iki, hj1, . . . , jli) in (8) using constant weights
wt = 1 (left), linear weights wt = t−1 (middle) and quadratic
weights wt = t−2 (right). Ranked lists in each of the diﬀer-
ent groups are displayed in diﬀerent colors.

We observe a similar conclusion with the expected dis-
tance over partially ranked lists as we did with the distances
over permutations. The ﬁve groups are relatively inter-
spersed for uniform weights and get increasingly separated
as the rate of weight decay increases. This represents the
fact that as the decay rate increases, disagreements in top
ranks are emphasized over disagreement at bottom ranks.

We also conducted some comparisons between the weighted
Hoeﬀding distance and alternative distance measures. Ta-
ble 1 shows how one recently proposed measure, the inverse
measure [2], lacks discriminative power, assigning the same

d to 1|2|3|4|5

2
3
4
5
1|3
1|4
1|5

InverseMeasure wt = t−1 wt = t−2
0.7539
0.8589
0.8901
0.8988
0.2049
0.2464
0.2581

0.6500
0.7786
0.8357
0.8571
0.3048
0.3810
0.4095

0.8374
0.8374
0.8374
0.8374
0.2481
0.2481
0.2481

Table 1: A comparison of the inverse measure [2] with the
weighted Hoeﬀding distance indicates that the inverse measure
lacks discriminative power as it assigns the same dissimilarity to
very diﬀerent ranked lists (n = 5).

d to 1|2|3|4|5

1|2|3|5|4|
2|1|3|4|5|

1|4|2|

1|
2|1
5|

5|4|3|2|1|

n = 5
0.0117
0.7464
0.1268
0.1064
0.7726
0.9395
1.0000

n = 10
0.0176
0.6755
0.1362
0.1592
0.7283
0.9280
0.9025

n = 103
0.0670
0.6660
0.1950
0.2656
0.7515
0.9820
0.8727

n = 105
0.0698
0.6683
0.1980
0.2692
0.7543
0.9851
0.8748

n = 107
0.0699
0.6683
0.1981
0.2692
0.7543
0.9852
0.8748

Table 2: A comparison of weighted Hoeﬀding distance with cubic
weight decay wt = t−3 reveals that increasing n beyond a certain
size does not alter the distances between partially ranked lists.
This indicates lack of sensitivity to the precise value of n as well
as computational speedup resulting from replacing n by n′ ≪ n.

dissimilarity to very diﬀerent ranked lists. Kendall’s tau
and the other distances proposed in [5, 6] lack the ability
to distinguish disagreement in top ranks and bottom ranks.
In particular, Kendall’s tau is identical to our weighted Ho-
eﬀding distance with uniform weights (see Figures 1-3 for
a demonstration of its inadequacy). NDCG [10] and other
precision recall measures rely on comparing a ranked list to a
ground truth of relevant and not-relevant websites. As such
they are not symmetric and are inappropriate for computing
MDS embedding based on pairwise distances.

Table 2 shows a comparison of weighted Hoeﬀding dis-
tances with wt = t−3 for diﬀerent sizes of the web n.
It
reveals that increasing n beyond a certain size does not alter
the distances between partially ranked lists. This indicates
a lack of sensitivity to the precise value of n as well as com-
putational speedup resulting from replacing n by n′ ≪ n.

5. SEARCH ENGINE EXPERIMENTS

We discuss in this section three experiments conducted on
real world search engine data. In the ﬁrst experiment we vi-
sualize the similarities and diﬀerences between nine diﬀerent
search engines: altavista.com, alltheweb.com, ask.com,
google.com, lycos.com, live.com, yahoo.com, aol.com, and
dogpile.com. We collected 50 popular queries online in
each of six diﬀerent categories: company names, questions5,
sports, tourism6, university names, and celebrity names.
These queries form a representative sample of queries Q
within each category over which we average the expected
distance ρ according to (3). Figure 4 shows several queries
for each one of the topic categories. We visualize search
result sets within each of the query categories in order to
examine whether the discovered similarity patterns are spe-
ciﬁc to a query category, or are generalizable across many
diﬀerent query types.

5queries from http://answers.yahoo.com
6queries from http://en.wikipedia.org/wiki/Tourism

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA936Queries
Times Square, Sydney Opera House, Eiﬀel Tower, Niagara Falls, Disneyland, British Museum, Giza Pyramids

Categories
Tourism
Celebrity Names Michael Bolton, Michael Jackson, Jackie Chan, Harrison Ford, Halle Berry, Whoopi Goldberg, Robert Zemeckis
Sports
University Names Georgia Institute of Technology, University of Florida, Virginia Tech, University of California Berkeley
Company
Questions
Temporal Queries AIG Bonuses, G20 major economies, Timothy Geitner, Immigration Policy, NCAA Tournament Schedule

Football, Acrobatics, Karate, Pole Vault, Butterﬂy Stroke, Scuba Diving,Table Tennis, Beach Volleyball, Marathon

Goldman Sachs, Facebook, Honda, Cisco Systems, Nordstrom, CarMax, Wallmart, American Express, Microsoft
How are ﬂying buttresses constructed, Does toothpaste expire, How are winners selected for the Nobel Prize

Figure 4: Selected queries from each of the 6 query categories, and from the set used for examining temporal variations.

university

9

 

3

celebrity

3

 

9

6

8

4

4

8

 

6

1. altavista
2. alltheweb
3. ask
4. google
5. lycos
6. live
7. yahoo
8. aol
9. dogpile

1

7

5
2

 

1. altavista
2. alltheweb
3. ask
4. google
5. lycos
6. live
7. yahoo
8. aol
9. dogpile

7
1
2
5

company

9

3

 

6

12

5

7

6

5

1

7

2

4

8

 

 

sports

8
4

4

8

questions

9

6

3

7

5
12

 

tourism

 

6

7
5

1
2

 

4
8

 

 

3

9

9

3

Figure 5: MDS embedding of search engine results over 6 sets of representative queries: company names, university names, celebrity
names, questions, sports, and tourism. The MDS was based on the expected weighted Hoeﬀding distance with linear weighting wt = t−1
over the top 100 sites. Circle sizes indicate position variance with respect to within category queries.

1. altavista
2. alltheweb
3. ask
4. google
5. lycos
6. live
7. yahoo
8. aol
9. dogpile

celebrity

celebrity

 

 

1. altavista
2. alltheweb
3. ask
4. google
5. lycos
6. live
7. yahoo
8. aol
9. dogpile

 

 

Figure 6: MDS embedding of search engine results over the query category celebrity with diﬀerent query manipulations. The MDS was
computed based on the expected distance (3) with (8) corresponding to the weighted Hoeﬀding distance with quadratic decaying weights
(left panel) and Kendall top k distance [6] (right panel). Each marker represents a combination of one of the 9 search engines and one
of the 5 query manipulation techniques. By comparison, the embedding of the Kendall top-k distance (right) lacks discriminative power
and results in a loss of information.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA9375.1 Search Engines Similarities

Figure 5 displays the MDS embedding of each of the nine
engines for the six query categories, based on the expected
weighted Hoeﬀding distance (3) with linear weight decay.
The ρ quantity was averaged over the 50 representative queries
from that category. Each search engine is represented as a
circle whose center is the 2-D coordinates obtained from the
MDS embedding.

The radii of the circles in Figure 5 were scaled proportion-
ally to the positional stability of the search engine. More
precisely, we scaled the radius of the circle corresponding to
the i-th search engine proportionally to its distance variance
over the 50 queries

 

 

altav alweb lycos yahoo ask google aol

dog msn

stability(i)

def

= X

j:j6=i

Var q∼Q{ρ(si(q), sj(q))}.

(14)

Figure 7: Hierarchical clustering dendogram for the nine search
engines over the query category Companies.

Scaling the circles according to (14) provides a visual indi-
cation of how much will the position change if one or more
queries are deleted or added to Q. This can also be in-
terpreted as the degree of uncertainty regarding the precise
location of the search engines due to the averaging over Q.
Examining Figure 5 reveals several interesting facts. To
begin with there are ﬁve distinct clusters. The ﬁrst and
largest one contains the engines altavista, alltheweb, lycos,
and yahoo (indicated by the numeric codes 1,2,5,7). These
four search engines are clustered together very tightly in all
6 query categories. The second cluster is composed of google
and AOL (numeric codes 4, 8) who also appear in very close
proximity across all 6 query categories. The remaining three
clusters contain individual engines: live, dogpile, and ask.

The clusters in the embedding do in fact mirror the tech-
nology relationships that have evolved in the search engine
industry. FAST, the company behind alltheweb, bought
Lycos and was subsequently bought by Overture who also
bought Altavista7. Overture was subsequently bought by
the fourth member of the cluster, Yahoo. All four search
engines in the ﬁrst cluster have close proximity in the em-
bedding and yet are dissimilar from the remaining competi-
tors. The second cluster, for Google and AOL, reﬂects the
fact that AOL now relies heavily on Google’s web search
technology, leading to extremely similar ranked lists.

The remaining engines are quite distinct. Dogpile is a
meta-search engine which incorporates the input of the other
major search engines. We see that dogpile’s results are
roughly equidistant from both Yahoo and Google clusters
for all query categories. Figure 5 also shows that dogpile is
more similar to the two remaining engines - Live and Ask.
Apparently, dogpile emphasizes pages highly-ranked by Live
and Ask in its meta search more than Google and AOL and
more than Yahoo, Lycos, Altavista, and alltheweb.

We present the similarity structure between the search en-
gines in Figure 7. The ﬁgure displays a dendogram output
by standard hierarchical bottom up clustering. The clus-
tering was computed based on the expected Hoeﬀding simi-
larity measure for queries sampled from the query category
Companies. The dendogram in the ﬁgure conﬁrms the anal-
ysis above visually. Altavista, alltheweb, lycos, and yahoo
all form a tightly knit cluster. A similar tight cluster con-
tains google and aol. More loosely it can also be said that
google and aol are closer to the remaining search engines
(ask, dogpile, and msn) than the yahoo cluster.

7http://google.blogspace.com/archives/000845.html

Engine/Distance

Ask
Live

Google
Yahoo

(b) +
0.5308
0.5625
0.3553
0.4281

(c) “ ”
0.6903

0.5639
0.4117
0.4647

(d) and
0.6424
0.5006
0.5584
0.5777

(e) or
0.6447
0.5374
0.5500
0.5918

Figure 8: The expected distance of diﬀerent query manipulations
from the original query for diﬀerent search engines.

5.2 Query Manipulations

In the second experiment we used search engine data to
examine the sensitivity of the search engines to four com-
monly used query manipulation techniques. Assuming that
the queries contained several words w1w2 · · · wl with l > 1,
the query manipulation techniques that we considered were

(a) w1w2 · · · wl ⇒ w1w2 · · · wl
(b) w1w2 · · · wl ⇒ w1 + w2 + · · · + wl
(c) w1w2 · · · wl ⇒ “w1w2 · · · wl”
(d) w1w2 · · · wl ⇒ w1 and w2 and · · · and wl
(e) w1w2 · · · wl ⇒ w1 or w2 or · · · or wl

with the ﬁrst technique (a) being the identity i.e. no query
manipulation. The embeddings of queries in the query cat-
egory celebrity are displayed in Figure 6. The left panel dis-
plays the MDS embedding based on our expected weighted
Hoeﬀding distance with quadratic decaying weight. As a
comparison, the right panel shows the MDS based on Kendall’s
top k distance as described by Fagin et al. [6]. Each marker
in the ﬁgure represents the MDS embedding of a particu-
lar engine using a particular query manipulation technique
which brings the total number of markers to 9 · 5 = 45.

Comparing the left and right panels shows that visualiz-
ing using Kendall’s top k distance [6] lacks discriminative
power. The points in the right panel fall almost on top of
each other limiting their use for visualization purposes. In
contrast, the points in the left panel (weighted Hoeﬀding
distance) diﬀerentiate among not only diﬀerent engines but
also diﬀerent types of query manipulations.

In particular, it shows that most search engines produce
two diﬀerent clusters of results corresponding to two sets
of query manipulation techniques: transformations {(a),(b),
(c)} in one cluster and transformations {(d),(e)} in the other
cluster. Live and ask form an exception to that rule forming
clusters {(a),(d)}, {(b),(c)}, {(e)} (live) and {(a),(b),(d),(e)},
{(c)} (ask). Figure 8 shows the query manipulations that
produce ranked lists most distinct from the original query:
(c) for ask and live, (d) for google, and (e) for yahoo.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA9381. altavista
2. alltheweb
3. google
4. lycos
5. live
6. yahoo
7. aol

 

 

1. to Day2
2. to Day6

2

3

4

5

6

7

 

0.6

0.4

0.2

0

 
1

Figure 9: Top: MDS embedding of search engine results over
seven days for a set of queries based on temporal events. The
MDS embedding was based on the expected Hoeﬀding distance
with linear weighting wt = 1/t over the top 50 sites. Bottom: The
dissimilarity of Yahoo results over seven days with respective to
a reference day for a set of queries based on temporal events.

5.3 Temporal Variation

In the third experiment, we visualize search result sets
created by replicating 7 out of the 9 search engines over 7
consecutive days resulting in 7 · 7 = 49 search result sets.
The search engines were queried on a daily basis during
3/25/2009 - 3/31/2009 and the returned results were em-
bedded in 2-D for visualization. In contrast to the previous
two experiments, we used a separate query category which
was speciﬁcally aimed at capturing time sensitive matters.
For example, we ignored tourism queries such as Eiﬀel Tower
due to their time insensitive nature and instead used queries
such as Timothy Geitner or AIG bonuses which dominated
the news in March 2009. See Fig. 4 for more examples.

The embedded rankings are displayed in Figure 9 (top).
The embedding reveals that the yahoo cluster (yahoo, al-
tavista, alltheweb, and lycos) shows a high degree of tem-
poral variability, and in particular a sharp spatial shift on
the third day from the bottom region to the top left region.
This could be interpreted either as a change in the index,
reﬂecting the dynamic nature of the Web, or an internal
change in the retrieval algorithms underlying the engines.
The other engines were more stable as their ranked lists
changed very little with the temporal variation. Note that
as the queries were time sensitive this should not be inter-
preted as a measure of robustness, but rather as a stability
measure for the internal index and ranking mechanisms. In-
terestingly, Vaughan [20] also reports that Altavista shows
temporal jumps with Google being more stable over a set of
queries in 2004.

Figure 9 (bottom) shows the expected distance between
the yahoo search results across the seven consecutive days
and a reference point (2nd and 6th day). As expected, for
each one of the two plots, the deviation to the reference date
increases monotonically with the temporal diﬀerence. The

slope of the curve represents the degree of temporal change
∆(st, st+τ ) between yahoo at time t and at time t + τ as a
function of τ with respect to the reference point t.

5.4 Validation of the MDS Embedding

A central assumption of this study is that MDS embed-
dings can give a faithful representation of the distances in
the original higher-dimensional space. Thus, we now pro-
vide a validation of the MDS embedding as a visualization
tool, diagnosing whether MDS is providing a reasonable em-
bedding and which MDS variant should be preferable.

The most common tool for validating the MDS embedding
is Shepard’s plot (Figure 10, top) which displays a scatter
plot contrasting the original dissimilarities (on the x axis)
and the corresponding distances after the embedding (on
the y axis). Points on the diagonal represent zero distortion
and a curve that deviates substantially from the diagonal
represents substantial distortion. The Shepard’s plot in Fig-
ure 10 (top) corresponds to the metric MDS using the stan-
dard stress criterion as described in (2). The plot displays
low distortion with a tendency to undervalue dissimilarities
in the range [0, 0.5] and to overvalue dissimilarities in the
range [0.5, 0.9]. Such a systematic discrepancy between the
way small and large distances are captured is undesirable.

An alternative is non-metric MDS which achieves an em-
bedding by transforming the original dissimilarities into al-
ternative quantities called disparities using a monotonic in-
creasing mapping which are then approximated by the em-
bedding distances [3]. Doing so preserves the relative or-
dering of the original dissimilarities and thus (assuming the
embedding distances approximate well the disparities) accu-
rately represent the spatial relationship between the points.
Figure 10 (bottom) displays the Shepard’s plot for the same
data embedded using non-metric stress MDS with the dis-
parities displayed as a red line. Despite the fact that its
numeric distortion is higher, the non-metric MDS is a vi-
able alternative to the metric MDS, and is what was used
to generate the ﬁgures in this paper.

6. DISCUSSION

In this paper we present a framework for visualizing rela-
tionships between search algorithms. The framework starts
by deriving an expected distance based on the earth mover’s
distance on permutations and then extends it to partially
ranked lists by taking expectations over all permutations
consistent with the ranked lists. The expected distance ρ is
then averaged over representative queries q ∈ Q to obtain
a dissimilarity measure for use in multidimensional scaling
embedding. The expected distance has several nice proper-
ties including being computationally eﬃcient, customizable
through a selection of the weight vector w, and interpretable.
We explore the validity of the framework using a simula-
tion study which indicates that the weighted Hoeﬀding dis-
tance is more appropriate than Kendall’s tau and more dis-
criminative than the inverse measure. It is also more appro-
priate for MDS embedding than non-symmetric precision-
recall measures such as NDCG. We also demonstrate the ro-
bustness of the proposed distance with respect to the choice
of n and its eﬃcient computation with complexity that is lin-
ear in the sizes of the ranked lists (assuming some quantities
are precomputed oﬄine).

Experiments on search engine data reveal several inter-
esting clusters which are corroborated by examining recent
news stories about the web search industry. We demon-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA9390.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Figure 10: Shepard plots (embedding distances as a function of
the original dissimilarities) for 2D MDS embeddings correspond-
ing to the weighted Hoeﬀding distance of search engine results
over query category celebrity with diﬀerent query manipulations.
The metric stress MDS [3] (top panel) produces an embedding
that has a lower overall distortion than the non-metric stress
MDS [3] (bottom panel). The non-metric stress MDS, however,
achieves an embedding by transforming the original dissimilari-
ties into alternative quantities called disparities using a monotonic
increasing mapping (red line in bottom panel).

strate how to visualize the positional stability by scaling the
MDS markers proportionally to the total distance variance
and how to visualize sensitivity of search engines to popular
query manipulation techniques. We also use the visualiza-
tion framework to examine how the search results vary over
consecutive days.

Search engines use complex proprietary algorithms con-
taining many parameters that are automatically tuned based
on human provided ground truth information. As a result,
it is diﬃcult for search engineers to have a detailed under-
standing of how precisely their search engine works. For ex-
ternal users the problem is even worse as they are not privy
to the internal algorithmic details. Our framework provides
visual assistance in understanding the relationship between
a search algorithm’s ranked results and its dependence on
internal and external parameters. Such visualization may
lead to better designed search engines as the engineers im-
prove their understanding of how search engines depend on
the internal parameters. It may also improve the search ex-
perience as users understand better the relationship between
diﬀerent engines and their dependency on query manipula-
tion techniques and external parameters such as time.

7. ACKNOWLEDGMENTS

We thank H. Zha and H. Li for helpful comments. This

research was funded in part by NSF grant DMS-0907466.

8. REFERENCES
[1] M. Alvo and P. Cabilio. Rank correlation methods for

missing data. The Canadian Journal of Statistics,
23(4):345–358, 1995.

[2] J. Bar-Ilan, K. Keenoy, E. Yaari, and M. Levene. User
rankings of search engine results. Journal of American
Society for Information Science and Technology,
58(9):1254–1266, 2007.

[3] I. Borg and P. J. F. Groenen. Modern

Multidimensional Scaling. Springer, 2nd edition, 2005.

[4] B. Carterette. On rank correlation and the distance
between rankings. In Proc. of the 32nd ACM SIGIR
Conference, 2009.

[5] D. E. Critchlow. Metric Methods for Analyzing

Partially Ranked Data. Lecture Notes in Statistics,
volume 34, Springer, 1985.

[6] R. Fagin, R. Kumar, and D. Sivakumar. Comparing

top k lists. In Proc. of ACM SODA, 2003.

[7] M. A. Fligner and J. S. Verducci. Distance based

ranking models. Journal of the Royal Statistical
Society B, 43:359–369, 1986.

[8] M. Gordon and P. Pathak. Finding information on the

world wide web: the retrieval eﬀectiveness of search
engines. Information processing and management,
35(2):141–180, 1999.

[9] L. Granka, T. Joachims, and G. Gay. Eye-tracking

analysis of user behavior in www search. In Proc. of
the ACM-SIGIR conference, pages 478–479, 2004.

[10] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Transactions on
Information Systems, 20(4):422–446, 2002.

[11] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In Proc. of the ACM-SIGIR
conference, pages 154–161, 2005.

[12] P. Kidwell, G. Lebanon, and W. S. Cleveland.

Visualizing incomplete and partially ranked data.
IEEE Transactions on Visualization and Computer
Graphics, 14(6):1356–1363, 2008.

[13] J. H. Lee. Combining multiple evidence from diﬀerent

properties of weighting schemes. In Proc. of the 18th
ACM SIGIR conference, 1995.

[14] W. Liggett and C. Buckley. Query expansion seen

through return order of relevant documents. Technical
report, NIST, 2001.

[15] J. I. Marden. Analyzing and modeling rank data. CRC

Press, 1996.

[16] A. Moﬀat and J. Zobel. Rank-biased precision for

measurement of retrieval eﬀectiveness. ACM
Transactions on Information Systems, 27, 2008.

[17] M. E. Rorvig. Images of similarity: A visual

exploration of optimal similarity metrics and scaling
properties of TREC topic-document sets. JASIS,
50(8):639–651, 1999.

[18] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth

mover’s distance as a metric for image retrieval.
International Journal of Computer Vision, 40(2),
2000.

[19] A. Spoerri. Metacrystal: A visual interface for meta

searching. In Proceedings of ACM CHI, 2004.

[20] L. Vaughan. New measurements for search engine

evaluation proposed and tested. Information
processing and management, 40(4):677–691, 2004.
[21] E. Yilmaz, J. A. Aslam, and S. Robertson. A new

rank correlation coeﬃcient for information retrieval. In
Proc. of the 31st ACM SIGIR conference, 2008.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA940