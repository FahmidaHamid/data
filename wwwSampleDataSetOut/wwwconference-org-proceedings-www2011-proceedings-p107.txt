Context-Sensitive Query Auto-Completion∗

∗

Ziv Bar-Yossef

Google Inc.

MATAM, Bldg 30

POB 15096, Haifa 31905, Israel

zivby@ee.technion.ac.il

Naama Kraus

Computer Science Department

Technion

Haifa 32000, Israel

nkraus@cs.technion.ac.il

ABSTRACT
Query auto completion is known to provide poor predictions
of the user’s query when her input preﬁx is very short (e.g.,
one or two characters). In this paper we show that context,
such as the user’s recent queries, can be used to improve
the prediction quality considerably even for such short pre-
ﬁxes. We propose a context-sensitive query auto completion
algorithm, NearestCompletion, which outputs the comple-
tions of the user’s input that are most similar to the context
queries. To measure similarity, we represent queries and con-
texts as high-dimensional term-weighted vectors and resort
to cosine similarity. The mapping from queries to vectors
is done through a new query expansion technique that we
introduce, which expands a query by traversing the query
recommendation tree rooted at the query.

In order to evaluate our approach, we performed exten-
sive experimentation over the public AOL query log. We
demonstrate that when the recent user’s queries are rele-
vant to the current query she is typing, then after typing a
single character, NearestCompletion’s MRR is 48% higher
relative to the MRR of the standard MostPopularComple-
tion algorithm on average. When the context is irrelevant,
however, NearestCompletion’s MRR is essentially zero. To
mitigate this problem, we propose HybridCompletion, which
is a hybrid of NearestCompletion with MostPopularComple-
tion. HybridCompletion is shown to dominate both Near-
estCompletion and MostPopularCompletion, achieving a to-
tal improvement of 31.5% in MRR relative to MostPopular-
Completion on average.

Categories and Subject Descriptors: H.3.3: Informa-
tion Search and Retrieval.
General Terms: Algorithms.
Keywords: query auto-completion, query expansion, context-
awareness.

∗

Despite the aﬃliation of the ﬁrst author, this paper did
not rely on privileged access to Google’s resources, data, or
∗
technologies. The entire research was done at the Technion.
Also an adjunct senior lecturer at the Technion, Depart-
ment of Electrical Engineering.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

1.

INTRODUCTION

Query auto completion [4, 23, 14, 3] is one of the most
visible features in Web Search today.
It is oﬀered by all
major search engines and in almost all their search boxes.
Query auto completion helps the user formulate her query,
while she is typing it.
Its main purpose is to predict the
user’s intended query and thereby save her keystrokes. With
the advent of instant as-you-type search results (a la the
recently released Google Instant1), the importance of correct
query prediction is even more acute, because it determines
the speed at which the user sees the suitable results for her
intended search and the amount of irrelevant results that
are displayed to her along the way.

The basic principle that underlies most query auto com-
pletion systems is the wisdom of the crowds. The search
engine suggests to the user the completions that have been
most popular among users in the past (we call this algorithm
MostPopularCompletion). For example, for the preﬁx am,
Bing suggests amazon and american express as the top com-
pletions, because these have been the most popular queries
starting with am. As the user is typing more characters, the
space of possible completions narrows down, and thus the
prediction probability increases. For example, if the user is
looking for american presidents in Bing, after typing the 14
characters american presi the desired query becomes the top
completion.

Clearly, during the ﬁrst few keystrokes the user is typing,
the search engine has little information about her real intent,
and thus the suggested completions are likely to mispredict
her query.
In our experiments, conducted over the public
AOL query log [19], we found that after the ﬁrst character,
MostPopularCompletion’s average MRR is only 0.187.

The objective of this study is to tackle the most chal-
lenging query auto completion scenario: after the user has
entered only one character, try to predict the user’s query
reliably. Being able to predict the user’s query on her ﬁrst
character rather than, say, on her 10-th character would not
only save the user a few keystrokes, but would also make
the whole search experience more interactive, as the feed-
back cycle of (query → results → query reﬁnement) would
be shortened signiﬁcantly.
In addition, cutting down the
search time for all users implies lower load on the search en-
gine, which translates to savings in machine resources and
power.

But how can we overcome the inherent lack of informa-
tion when the user has entered only a few characters of her

1http://www.google.com/instant/

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India107intended query? Our main observation is that the user typ-
ically has some context, which can reveal more information
about her intent. For example, if just before entering the
characters am the user searched for richard nickson, it is
more likely that the user is looking for american presidents
than for amazon or american airlines. Similarly, if the user
was browsing a page about President Lincoln or reading an
article about american history. On the other hand, if the
user has just tweeted about a planned trip, american air-
lines might be the more probable query. Recent queries,
recently visited web pages, and recent tweets are examples
of online activities that may indicate the user’s intent and,
if available, could be used by the search engine to better
predict the query even after a few keystrokes. This is called
context-sensitive query auto completion. While the idea is
very intuitive and context has been used in other scenarios
to disambiguate user intent (e.g., in search [16, 11] and in
query recommendations [10, 13, 9, 6, 21]), there is almost
no published work on its application to query completion.

Of the many diﬀerent possible user contexts, our focus
in this study is on the user’s recent queries (within the
same session), as they are readily available to search en-
gines. Based on our empirical analysis of the AOL log, 49%
of the searches are preceded by a diﬀerent search in the same
session, and are thus amenable to context-sensitive query
completion.

One possible approach to use recent queries to improve
query auto completion is to generalize MostPopularComple-
tion to rely on the popularity of query sequences rather than
just the popularity of individual queries. Suppose the user’s
previous query in the same session is y and that the cur-
rent user input (query preﬁx) is x. Then, the search engine
will suggest the completions of x that were most frequently
searched for after y. For example, if after the query richard
nixon the most popular successive query starting with am is
american presidents, the search engine will suggest ameri-
can presidents as its top completion. This is in fact the main
principle underlying most of the work on context-sensitive
query recommendations [10, 13, 9, 6, 21]. The main caveat
of this approach is that it heavily relies on the existence of
reoccurring query sequences in search logs. Nevertheless,
due to the long-tail distribution of query frequencies, many
of the query sequences generated by users have never oc-
curred before (by our estimate, 89% of the query pairs are
new).

Some studies tried to mitigate the sparsity of query se-
quences by clustering similar query sequences together, based
on features extracted from queries, like their topical cate-
gories or their terms [10, 9]. Machine learning techniques,
like HMMs, are then used to predict the intended query,
if the sequence of previous queries can be associated with
a cluster in the model. This approach is still challenged
by long-tail contexts, i.e., when the most recent query (or
queries) have rarely occurred in the log (by our estimates,
in 37% of the query pairs the former query has not occurred
in the log before).
In this case, the sequence of previous
queries may not be easily associated with a cluster in the
model. Moreover, none of these previous studies took the
user input (preﬁx) into account in the prediction, so their
applicability to query auto completion is still unknown.

We take a diﬀerent approach to tackle this problem. Our
algorithm relies on the following similarity assumption: when
the context is relevant to the intended user query, the in-

tended query is likely to be similar to the context queries.
The similarity may be syntactic (e.g., american airlines →
american airlines ﬂight status) or only semantic (e.g., amer-
ican airlines → continental). By our estimates, 56% of the
reﬁnements are non-syntactic.

Based on the similarity assumption, we propose the Near-
estCompletion algorithm, which works as follows: given a
user input x and a context y, the algorithm suggests to the
user the completions of x that are most similar to y. C hoos-
ing the suitable similarity measure is non-trivial, though,
because we would like it to be both correlated with refor-
mulation likelihood (that is, the more similar two queries A
and B are the more likely they are to be reformulations of
each other, and vice versa) and universally applicable (that
is, the similarity is meaningful for any pair of queries A and
B). The former requirement guarantees that the comple-
tions that are similar to the context are indeed more likely to
be the user’s intended query. The latter requirement makes
sure that the algorithm can deal with any user input.

The above two requirements make many of the state-
of-the-art query similarity measures less appealing for this
problem. For example, syntactic measures, like edit dis-
tance, do not take all reformulation types into account. Sim-
ilarity measures that are based on co-occurrence in search
sessions [24, 12], on co-clicks [2, 10], or on user search be-
havioral models [6, 18, 9, 21], are not universally applicable
to all query pairs due to their low coverage of queries, as
long tail queries are rare in the query log. Similarity mea-
sures that are based on search result similarity [8] are not
necessarily correlated with reformulation likelihood.

Given a context, query recommendation algorithms [2, 24,
21] output a list of recommendations that are likely refor-
mulations of the previous query. So a possible similarity
measure would be one that associates each query with its
recommendations. The main caveat with this approach is
that query recommendation algorithms are frequently de-
signed to output only a few high quality recommendations
and thus it is plausible that none of them are compatible
with the user’s input. Hence, this technique is not univer-
sally applicable.

We propose a new method of measuring query similar-
ity, which expands on the latter recommendations based ap-
proach, but is universally applicable and is thus more suit-
able to query completion. Similarly to the result-based sim-
ilarity of Broder et al. [8], we expand each query to a richer
representation as a high-dimensional feature vector and then
measure cosine similarity between the expanded representa-
tions. The main novelty in our approach is that the rich
representation of a query is constructed not from its search
results, but rather from its recommendation tree. That is,
we expand the query by iteratively applying a black box
query recommendation algorithm on the query, on its rec-
ommendations, on their recommendations, and so on. The
nodes of the traversed tree of recommendations are tok-
enized, stemmed, and split into n-grams. These n-grams are
the features of the expanded representation vector and the
weight of each n-gram is computed based on its frequency
and depth in the tree.

The above representation has two appealing properties.
First, as the basic building block in the construction is a
black-box query recommendation algorithm, we can lever-
age any state-of-the-art algorithm and inherit its power in
predicting query reformulations. Second, the above scheme

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India108provides a continuous spectrum of exceedingly rich repre-
sentations, depending on the depth of the tree traversed.
For example, a depth-0 traversal results in the n-grams of
the root query itself, while a depth-2 traversal results in the
n-grams of the query, its recommendations, and their rec-
ommendations. The main point is that the feature space
remains the same, regardless of the traversal depth. So even
if we cannot traverse the recommendation tree of a certain
query (e.g., because it’s a long-tail query for which there are
no recommendations available), the similarity between its
representation and the richer representation of other queries
is meaningful. This property ensures that our query auto
completion algorithm is applicable even for long-tail con-
texts that have never been observed in the log before.

Our empirical analysis shows that the average MRR of

NearestCompletion (with depth-3 traversal) over queries whose
context is relevant is 48% higher relative to the average MRR
of MostPopularCompletion over the same queries. However,
when the context is irrelevant to the intended query, Near-
estCompletion becomes destructive, so its average MRR is
19% lower relative to the average MRR of MostPopular-
Completion over all queries. To mitigate this problem, our
ﬁnal algorithm, HybridCompletion, is a hybrid of MostPop-
ularCompletion and NearestCompletion. Each of the two
algorithms provides a list of top k matches. We aggregate
the two lists by standardizing the contextual score and the
popularity score of each candidate completion and then com-
puting a ﬁnal score which is a convex combination of the two
scores. The completions are ranked by these ﬁnal scores. We
show that HybridCompletion dominates both NearestCom-
pletion and MostPopularCompletion.
Its average MRR is
31.5% higher relative to the average MRR of MostPopular-
Completion.

As our new algorithm relies on the standard cosine simi-
larity measure between vectors, it can be implemented eﬃ-
ciently over standard high-performance search architectures.
This is a crucial property of the algorithm, because query
auto completions need to be provided to the user in a split-
second as she is typing her query. Note that the rich repre-
sentation of the recent queries can be cached and retrieved
quickly as the user is typing her current query. The current
query requires no enrichment.

The rest of the paper is organized as follows. After re-
viewing some related work in Section 2, we provide a brief
background about query auto completion algorithms in Sec-
tion 3. We describe the NearestCompletion algorithm in
Section 4 and the HybridCompletion algorithm in Section 5.
In Section 6 we provide a detailed empirical study of the two
algorithms and compare them to MostPopularCompletion.
We end with some concluding remarks in Section 7.

2. RELATED WORK

Query auto-completion has received relatively little at-
tention in the literature (see, e.g., [23, 4, 3, 14, 1]). All
major search engines rely on query logs to generate query
auto-completions. To the best of our knowledge, there is no
published work on context-sensitive log-based query auto-
completion. Arias et al. [1] suggest a context-sensitive query
auto-completion algorithm for mobile search. Their comple-
tions, however, are thesaurus-based concepts whose relat-
edness to the user’s context is determined by a rule-based
mechanism. This approach does not seem to ﬁt the scala-
bility requirements of web search.

Similarly to query completions, query recommendations
assist users in phrasing their intent. Numerous query rec-
ommendation algorithms have been introduced, relying on
varied techniques, including topic clustering [2, 21, 5], query
co-occurrence analysis [12], session analysis [24, 13, 9], and
user search behavioral models [18, 21, 6, 22]. Query rec-
ommendation algorithms and query auto-completion algo-
rithms diﬀer in their input (a full query vs. a preﬁx of a
query) and in their output (arbitrary reformulations of the
query vs. (mostly) completions of the user’s preﬁx). Note
that our framework can leverage any state-of-the-art query
recommendation algorithm to construct its rich query rep-
resentations.

Several context-sensitive query recommendation algorithms

have been proposed recently. Boldi et al. [6] compute query
recommendations by running Personalized PageRank on their
Query Flow Graph. As the mass of PageRank’s telepor-
tation vector is concentrated on the context queries, the
recommendations generated are context-sensitive. The run
time eﬃciency of this algorithm is questionable, as PageR-
ank computation is heavy. Cao et al. [10, 9] and He et
al. [13] train models for query sequences based on analysis
of such sequences in search logs (using, among others, some
machine learning models, like Variable Length HMM and
Mixture Variable Memory MM). At run time, these models
are used to predict the next user’s query from her previous
queries. It is not totally clear how these techniques deal with
long-tail contexts that have never occurred in the log before.
In comparison, our algorithms are adapted to query auto-
completion, can deal with long tail contexts, have scalable
runtime performance, and are robust to irrelevant contexts.
The framework we use in this paper for query auto com-
pletion is close to the one used by Broder et al. [8] to ex-
pand long-tail queries for the purpose of matching relevant
ads. Broder et al. built an index of enriched representa-
tions of popular and semi-popular queries and in run time
retrieved the queries that are most similar to the current
user query. These similar queries are then used to retrieve
relevant ads. Apart from the diﬀerent applications and the
diﬀerent evaluation methodologies, the two frameworks dif-
fer in their core component: the rich query representation.
Broder et al. mostly rely on their result-based query expan-
sion approach [7], while we focus on traversal of the query
recommendation tree.

Finally, query expansion is a well established ﬁeld (see [17]
for an overview). Classical methods expand the query us-
ing thesauri. This is limited and non-scalable. Roccio’s
method [20] expands the query from terms that occur in
its search results. This is non-trivial to do without adding
also a lot of noise that causes a “topic drift”. More modern
methods rely on query log analysis (e.g., [15]). Since query
recommendations usually rely on such query expansion tech-
niques as building blocks, our query expansion algorithm can
be viewed as bootstrapping state-of-the-art expansion tech-
niques to obtain richer vocabularies.

3. QUERY AUTO COMPLETION

A query auto completion (QAC) algorithm accepts a user
input x, which is a sequence of characters typed by the user
in the search engine’s search box. The user input is typically
a preﬁx of a complete query q that the user intends to enter.
The algorithm returns a list of k completions, which are
suggestions for queries, from which the user can select.

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India109A completion c is said to be a hit, if it equals the query q
that the user was about to enter. In this paper we will focus
on hits as the main measure of success for QAC algorithms,
as it is relatively easy to estimate hit rates when inspecting
search logs. In reality, a QAC algorithm may be successful
even if it returns a completion that is diﬀerent from the
query the user was about to type but that describes the
same information need.

Most QAC algorithms share the following framework. In
an oﬄine phase a query database is built. The database con-
sists of a large collection of queries that are of high quality
and represent the intents of the search engine’s users. Ma-
jor search engines build this database from their query logs
by extracting the most frequently searched queries. Smaller
search engines, which do not have suﬃcient user traﬃc, con-
struct the database from prominent phrases that occur in the
corpus being searched.

Each QAC algorithm deﬁnes its own criteria for determin-
ing whether a query q is an eligible completion for an input
x. Traditional QAC algorithms require that q is a proper
string completion of x (i.e., that x is a preﬁx of q). For in-
stance, barack obama is a proper completion of the input bar.
Advanced QAC algorithms support also non-proper comple-
tions, like mid-string completions (e.g., ob → barack obama)
and spell corrections (e.g., barak → barack obama). We will
denote the set of queries that are eligible completions of an
input x by completions(x).

At run-time, the QAC algorithm accepts an input x, and
selects the top k eligible completions for x. Completions
are ordered by a quality score, which represents how likely
each completion is to be a hit. Since the algorithm needs
to provide the user with the suggested completions as she is
typing the query, it has to be ultra-eﬃcient. To achieve this
high performance, the algorithm needs a data structure, like
a TRIE or a hash table, that supports fast lookups into the
query database using preﬁx keys.

MostPopularCompletion is the standard and most pop-
ular QAC algorithm. The quality score it assigns to each
query q is the frequency of this query in the log from which
the query database was built. That is, for an input x, Most-
PopularCompletion returns the k completions of x that were
searched for most frequently.

One way to look at this algorithm is as an approximate
Maximum Likelihood Estimator. Let p be the probability
distribution of incoming queries. That is, p(q) is the proba-
bility that the next query the search engines receives is q. In
an ideal world, the search engine would have known p(q) for
all q ahead of time. Given a user input x, the search engine
could have then applied Maximum Likelihood Estimation to
predict the user’s query q as follows:

M LE(x) = argmaxq∈completions(x) p(q).

MostPopularCompletion essentially implements this algo-
rithm, except that it approximates the next-query distri-
bution p by the normalized frequencies of queries in the log.

4. NEAREST COMPLETION

As we will see in Section 6, MostPopularCompletion fre-
quently fails to produce hits when the user input is still very
short (say, 1-2 characters long). The NearestCompletion al-
gorithm that we introduce next uses the user’s recent queries
as context of the user input x. When the context is relevant

to the query the user is typing, the algorithm has better
chances of producing a hit.

Search sessions. A logical search session is an interac-
tive process in which the user (re-)formulates queries while
searching for documents satisfying a particular information
need. It consists of a sequence of queries q1, . . . , qt (t ≥ 1)
issued by the user. The context of a user input x, where x is
the preﬁx of some query qi in the session, is the sequence of
queries q1, . . . , qi−1 preceding qi. Note that if x is the preﬁx
of the ﬁrst query in the session, its context is empty.

Since all the queries in a logical session pertain to the
same information need, the context of a user input is always
relevant to this input by deﬁnition. In reality, however, de-
tecting logical sessions is non-trivial as a user may switch her
information need within a short time frame. Mis-detection
of logical search sessions leads to mis-detection of contexts.
In this section we ignore this problem and assume we have
a perfect session detector, so contexts are always relevant.
We will address this problem in the next section.

The reader may wonder at this point how a QAC algo-
rithm that runs on the search engine’s server can access the
user’s recent queries at run-time. The most straightforward
solution is to keep the user’s recent queries in a cookie, if the
user agrees to it. Every time the user performs a search, the
search engine returns the results and also updates a cookie
(that the browser stores on the user’s machine) with the
latest search. When the user types characters in the search
engine’s search box, the browser sends the user’s input along
with the cookie to the search engine.

The basic algorithm. A context-sensitive extension of
the Maximum Likelihood Estimator described above works
as follows. Let p(q|C) denote the conditional probability
that the next query is q given that the current context is C.
If the search engine would have known p(q|C) for all q and
C, it could have predicted the next query as follows:
M LE(x|C) = argmaxq∈completions(x) p(q|C).

Approximating p(q|C) is trickier than approximating p(q),
because query logs are too sparse to provide meaningful esti-
mates of p(q|C) for most (q, C) pairs. We tackle this problem
by casting it as an information retrieval (IR) problem: we
treat the context C as a “query” and the queries in the query
database as “documents”. Indeed, our goal is to sift through
the many possible completions of x and ﬁnd the ones that
are most related to the context C.

Looking at this problem through the IR prism, we can
now resort to standard IR techniques. We chose to imple-
ment NearestCompletion using the traditional Vector Space
Model, which is supported by the search library we used
in our experiments. Each context C is mapped to a term-
weighted vector vC in some high dimensional space V . Sim-
ilarly, each query q in the query database is mapped to a
vector vQ ∈ V . NearestCompletion then outputs the com-
pletion q of x whose vector vq has the highest cosine simi-
larity to vC :
(cid:6)vq, vC(cid:7)
||vq|| · ||vC|| .

NearestCompletion(x, C) = argmaxq∈completions(x)

More generally, NearestCompletion outputs the k comple-
tions of x whose vectors are most similar to vC .

Context representation. Contexts and queries are ob-
jects of diﬀerent types, so it may not be clear how to rep-

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India110resent both as vectors in the same space. However, since
contexts are sequences of queries, then we can produce con-
text representations from query representations. Formally,
if C = q1, . . . , qt is a context and vq1 , . . . , vqt are the cor-
responding vectors, we produce the context vector vC as a
linear combination of the query vectors:

vC =

t(cid:2)

i=1

wivqi .

The weights w1, . . . , wt are non-negative. They specify the
relative contribution of each context query to the context
vector. As the more recent a context query is, the more
likely it is to be relevant to the current query, the weights
need to be monotonically non-decreasing. In our empirical
analysis we experimented with diﬀerent weight functions:
recent-query-only (wt = 1 and wi = 0 for all i < t), linear
decay (wi = 1/(t − i + 1)), logarithmic decay (wi = 1/(1 +
ln(t − i + 1))), and exponential decay (wi = 1/et−i)).
Vector representation via query expansion. The most
important ingredient of the NearestCompletion algorithm is
the representation of queries as term-weighted vectors. This
representation eventually determines how NearestComple-
tion ranks the completions of x. Ideally, we need this rank-
ing to be consistent with the ranking of completions by the
conditional probabilities p(q|C). Note that p(q|C) is the
probability that the user reformulates the context queries C
to the current query q. Hence, the vector representation of
queries needs to yield a similarity measure so that similar
queries are likely reformulations of each other and vice versa.
A naive approach would be to represent a query by its
terms, as a bag of words. The resulting similarity measure
can capture syntactic reformulations, such as my baby is not
eating well → baby eating disorder, but not semantic reﬁne-
ments, like baby eating disorder → infant nutrition. The
problem is that queries are short, and thus their vocabulary
is too sparse to capture semantic relationships.

In order to overcome this sparsity problem, we expand
each query into a rich representation. Query expansion [17]
is used to augment the textual query with related terms, like
synonyms. For example, the query baby eating disorder may
be expanded to baby infant eating food nutrition disorder
illness and the query infant nutrition may be expanded to
infant baby nutrition food. The two expanded forms now
have high cosine similarity.

Recommendation-based query expansion. We intro-
duce a new query expansion technique that leverages a large
body of work on query recommendation. A query recommen-
dation algorithm suggests to the user high quality reformu-
lations of her query. Query recommendation algorithms rely
on existing query expansion techniques and in addition are
tuned to provide likely reformulations of the query. Hence,
a plausible expansion of a query could be the list of rec-
ommendations provided for it by such an algorithm. The
advantage is that the added terms are high precision key-
words that appear in likely reformulations of the query. It
follows that two queries that have similar expanded forms of
this sort share their reformulation vocabulary and are thus
more likely to be reformulations of each other.

The above approach produces high precision query expan-
sions, but may lack coverage. If the query recommendation
algorithm produces a small number of recommendations for

each query (as most such algorithms do), then the result-
ing expanded forms would be too sparse. To overcome this
problem, we expand each query not just by its direct recom-
mendations but rather by the whole recommendation tree
rooted at this query.

Formally, let A be a query recommendation algorithm,
and let us denote by A(q) the top k recommendations that
A provides for a query q.

Definition 4.1

(Query recommendation tree). Let
d ≥ 0 be an integer. Let Tq,d denote the depth-d query rec-
ommendation tree of query q. The root node of Tq,d corre-
sponds to the query q. The children of each node v in the
tree correspond to the recommendations for v (A(v)).

Note that the same query may occur multiple times in the

tree and possibly at diﬀerent levels of the tree.

The query recommendation tree is the main building block
in the construction of the expanded form vq of a query
q. The coordinates of vq correspond to n-grams that oc-
cur within the queries in the tree. n-grams are extracted
as follows: each query in the tree is tokenized into terms,
stop-words are eliminated, and the terms are stemmed. The
resulting queries are split into overlapping n-grams, where
n = 1, . . . , N and N is an upper bound on the size of n-grams
we care about.

Let z be an n-gram. If z was not extracted by the above
process, then vq[z] = 0. If z was extracted, let T (z) denote
the nodes of the tree that contain z. We deﬁne the weight
of z in vq as follows:

⎞
⎠ · ln(IDF (z)).

⎛
⎝ (cid:2)

u∈T (z)

vq[z] =

weight(depth(u))

This is essentially a TF-IDF weight. The sum counts the
number of occurrences of z in the tree, but it assigns diﬀer-
ent weights to diﬀerent occurrences. The weight of an occur-
rence depends on the depth of the node in which it is found.
Note that the deeper the node is, the weaker is its connec-
tion to the root query q, and hence the function weight(·) is
monotonically non-increasing. We experimented with vari-
ous weighting functions, including linear decay, logarithmic
decay, and exponential decay. IDF (z) is the inverse fre-
quency of z in the entire query database.

System architecture.
Since NearestCompletion relies
on the Vector Space Model, it can leverage standard and
optimized information retrieval architectures. In the oﬄine
phase, the algorithm computes the rich representation of
each query in the query database. The resulting vectors
are indexed in an inverted index. In addition, each query
is indexed by its set of eligible preﬁxes, so one can retrieve
all completions of a given preﬁx quickly (note that wildcard
operator that is supported by standard search architectures
can also be used for preﬁx-based retrieval).

In run time, the algorithm accepts the user input x and
the context C. The rich representation of the queries that
constitute C should be available in a cache, because these
queries have been recently processed by the search engine.
The algorithm can therefore compute the rich representation
vC of C. It then retrieves from the index the queries that
are completions of x and whose rich representation is most
similar to vC .

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India1115. HYBRID COMPLETION

NearestCompletion is designed to work well when the user
input has a non-empty context and this context is relevant
to the query that the user is typing. In practice, however,
many queries have no context (51% by our experiments). In
addition, due to incorrect segmentation of search sessions,
recent queries that are deemed as context may not be rele-
vant to the current query at all (by our experiments, 40% of
the query pairs have diﬀerent information needs). In all of
these cases NearestCompletion relies either on no informa-
tion or on false information and thus exhibits poor quality.
On the other hand, the standard MostPopularCompletion
algorithm is not dependent on context, and thus can do well
even if the context is empty or irrelevant.
It would have
been nice if one could identify these cases and use Most-
PopularCompletion instead of NearestCompletion in them.
Recognizing that the context is empty is easy. However, how
can one detect that the context is irrelevant, at run time?
HybridCompletion circumvents this problem by using both
algorithms when the context is non-empty.

Given a user input x and a context C, HybridCompletion
produces two ranked lists of completions of x: LN C consists
of the top (cid:2) completions returned by NearestCompletion and
LM P C consists of the top (cid:2) completions returned by Most-
PopularCompletion. The ﬁnal ranked list of completions
LHC is constructed by aggregating the two lists.

The results in each of the two lists are ranked by quality
scores: LN C is ranked by a similarity score, which we de-
note by simscore(·), and LM P C is ranked by a popularity
score, which we denote by popscore(·). The aggregated list
LHC is constructed by combining the two scoring functions
into a single hybrid score, denote hybscore(·). As simscore
and popscore use diﬀerent units and scales, they need to be
standardized before they can be combined. In order to stan-
dardize simscore, we estimate the mean similarity score and
the standard deviation of similarity scores in the list LN C .
The standard similarity score is then calculated as

simscore(q) − μ

,

σ

Zsimscore(q) =

where μ and σ are the estimated mean and standard devia-
tion. The standard popularity score is calculated similarly.
The hybrid score is deﬁned as a convex combination of the
two scores:
hybscore(q) = α · Zsimscore(q) + (1 − α) · Zpopscore(q),
where 0 ≤ α ≤ 1 is a tunable parameter determining the
weight of the similarity score relative to the weight of the
popularity score. One can think of α as the prior probability
that the next query has a relevant context and thus would re-
quire a context-sensitive completion. Note that when α = 0
HybridCompletion is identical to MostPopularCompletion,
and when α = 1 HybridCompletion is identical to Nearest-
Completion (except for inputs that have empty context). In
Section 6, we experiment with diﬀerent values of α in order
to tune it appropriately.

HybridCompletion, as described above, uses one global
value for α, which is common to all inputs and contexts.
There may be scenarios though where it is desired to alter
the value of α adaptively. For example, if we have some in-
dication that the context is not relevant to the current user
input, we may want to reduce α, while if we have the oppo-
site indication, we may want to increase α. Since the focus

of this paper is not on session segmentation and context
relevancy detection, we have not addressed this direction.

It is important to note that HybridCompletion may rerank
the original lists of completions it receives. For example,
among the most popular completions, it will promote the
ones that are more similar to the context, and, conversely,
among the most similar completions, it will promote the
more popular ones. This implies that HybridCompletion
can dominate both MostPopularCompletion and Nearest-
Completion not only on average, but also on individual in-
puts.

6. EMPIRICAL STUDY

Our empirical study has two goals: (a) compare the best
conﬁguration of our algorithms to the standard MostPop-
ularCompletion algorithm; and (b) study the eﬀect of the
diﬀerent parameters of our algorithms on the quality of the
results. To this end, we came up with an automatic evalua-
tion methodology for QAC algorithms, which estimates their
MRR based on a given query log. We used the AOL query
log [19] in our experiments, as it is publicly available and
suﬃciently large to guarantee statistical signiﬁcance (other
public query logs are either access-restricted or are small).

Experimental setup. For performing the empirical study
we implemented the standard MostPopularCompletion algo-
rithm and our two algorithms (NearestCompletion and Hy-
bridCompletion). The query database used by all algorithms
was constructed from the queries that appear in the AOL
log. We segmented the log into sessions, using a simple stan-
dard segmentation heuristic (every interval of 30 minute idle
time denotes a session boundary). We eliminated from the
data all click information and merged duplicate queries that
belong to the same session. The ﬁnal data sets consisted
of 21,092,882 queries in 10,738,766 sessions. We found that
40% of the sessions were of length greater than 1 and 49% of
the queries were preceded by one or more queries in the same
session (hence being amenable to context-sensitive QAC).

We partitioned the AOL log into two parts: a training
set, consisting of 80% of the log, and a test set, consisting of
the remaining 20%. We computed a rich representation (see
Section 4) for a subset of 55,422 of the training set queries.
The query recommendation trees required for these rich rep-
resentations were built using Google Suggest’s query auto-
completion service. We scraped the auto completions using
the public external service (we did not rely on privileged ac-
cess to internal Google services or data). Since Google poses
strict rate limits on scrapers, we did not compute rich query
representation for all the queries in the training set.

The query database used for training each of the 3 al-
gorithms we considered consisted of these 55,422 queries.
The frequency counts used by MostPopularCompletion were
computed based on the entire training set, and not based
only on the queries in the query database.

NearestCompletion and HybridCompletion were implemented

by customizing Lucene2 to our needs. The experiments were
performed in October 2010 on a dual Intel Xeon 3.4GHz pro-
cessor workstation with 4GB RAM and two 320GB disks.

Evaluation framework. We evaluate a QAC algorithm
by how well it predicts the query the user is about to type.
The prediction quality depends on whether the algorithm

2http://lucene.apache.org/java/docs/index.html

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India112succeeds to generate a hit, and if it does, on the position of
this hit.

Since all the algorithms we evaluate work the same when
the current user input has no context, then our evaluation
focused only on queries that have context. We generated
sample queries with corresponding contexts as follows. We
randomly sampled 40,000 sessions from the test set. For each
selected session, we picked the ﬁrst query among the queries
in the session that have context (i.e., they are not the ﬁrst
one in the session) and that have a rich query representation
(i.e., we scraped their recommendations/auto-completions
from Google).
If the session had no such queries, it was
dropped from the sample. This resulted in 7,311 queries
and contexts.

Our experiments focused on query auto-completion after
having a single character from the current query. This set-
ting is the most challenging and thus demonstrates the dif-
ferences among the diﬀerent algorithms most crisply.

Evaluation metric. Recall that for a particular query
q and context C, an algorithm A is said to have a hit at
position (cid:2), if after receiving C and the ﬁrst character of q, the
algorithm returns q as the (cid:2)-th completion for this character.
We write in this case that hitrank(A, q, C) =(cid:2) (if A has no
hit at all, then hitrank(A, q, C) =∞ ). Mean Reciprocal
Rank (MRR) is a standard measure for evaluating retrieval
that is aimed at a speciﬁc object. The reciprocal rank of an
algorithm A on a particular (query, context) pair (q, C) is
1/ hitrank(A, q, C) (note that the reciprocal rank is 0 when
the algorithm has no hit). MRR is the expected reciprocal
rank of the algorithm on a random (query, context) pair.
To estimate MRR, we take a random sample S of (query,
context) pairs, and compute the mean reciprocal rank of
the algorithm over these pairs:
(cid:2)

MRR(A) =

1|S|

1

.

(q,C)∈S

hitrank(A, q, C)

MRR treats all (query, context) pairs equally. We ob-
serve, however, that some user inputs are easier to complete
than others. For example, if the user input is the letter ’z’,
then since there are few words that start with z, the auto-
completion task is easier and is likely to produce better pre-
dictions. On the other hand, if the user input is the letter
’s’, the numerous possible completions make the prediction
task much harder. This motivates us to work with a weighted
version of MRR (denoted wMRR). Rather than treating all
(query, context) pairs uniformly, we weight them according
to the number of completions available for the preﬁx of the
query.

Comparison experiments. Our ﬁrst set of experiments
compare NearestCompletion and HybridCompletion to the
standard MostPopularCompletion. The comparison is based
on the 7,311 random (query, context) pairs collected from
our training set. We used in these experiments the param-
eter values that we found to be the most cost-eﬀective: (a)
recommendation algorithm: Google’s auto-completion; (b)
recommendation tree depth: 3; (c) depth weighting func-
tion: exponential decay; (d) unigram model; (e) used only
the most recent query; (f) α in HybridCompletion: 0.5.

We start with an anecdotal comparison (Table 1) of the
completions provided by the three algorithms on some of the
above pairs (note: these are real examples taken from the
AOL log). The ﬁrst two examples demonstrate the eﬀect of

context on query auto completion after the user has typed
a single character of her intended query. In these examples,
the user’s intended query and the context are related (in the
ﬁrst example they are syntactically related and in the second
one they are only semantically related). The NearestCom-
pletion algorithm detects the similarity and thus provides
the correct prediction at one of the top 2 positions. As the
intended queries in these cases are not popular, MostPopu-
larCompletion fails to hit the correct completion even in its
top 10 suggestions. The utter failure of MostPopularCom-
pletion has only a minor eﬀect on HybridCompletion, which
suggests the correct completion at one of its top 5 positions.
The third example exhibits the opposite scenario: the con-
text is irrelevant to the user’s intended query and the in-
tended query is popular. Consequently, MostPopularCom-
pletion hits the correct query at the top position, while Near-
estCompletion completely fails. This time HybridComple-
tion beneﬁts from the success of MostPopularCompletion
and hits the correct completion also at its top spot.

Figure 1 provides a comparison of weighted MRR of the
three algorithms on the 7,311 (query, context) pairs. We
validated that the results are all statistically signiﬁcant. It
is very clear that HybridCompletion dominates both Near-
estCompletion and MostPopularCompletion. For example,
HybridCompletion’s wMRR is 0.246 compared to only 0.187
of MostPopularCompletion (an improvement of 31.5%). It
is also clear that the quality of NearestCompletion is infe-
rior to MostPopularCompletion (by 19.8%), so it cannot be
used as is for query auto completion. The graph also distin-
guishes between pairs in which the context has a rich rep-
resentation (i.e., the recommendations for this context have
been scraped from Google) and pairs in which the context
has only a thin representation (based on the context query
itself, without the recommendations). Note that the latter
simulates the case the context is long-tail and the search
engine has no recommendations for it. The results indicate
that even such long-tail contexts are useful, and thus Hybrid-
Completion is doing better than MostPopularCompletion.

5
5
0
Z















0.246

0.243

0.193

0.187

0.15

0.3

1HDUHVW
0RVW3RSXODU
+\EULG

0.23

0.17

0.138

$OO

5LFK

&RQWH[W7\SH

7KLQ

Figure 1: Weighted MRR of the 3 algorithms on
7,311 (query, context) pairs. Results are for all
pairs, for pairs in which the context has a rich rep-
resentation, and for pairs in which the context does
not have a rich representation.

Figure 2 demonstrates that HybridCompletion dominates
MostPopularCompletion not only on average but also with

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India113Context
french ﬂag

Query
italian ﬂag

neptune

uranus

MostPopularCompletion
internet, im help, irs, ikea,
internet explorer

ups, usps, united airlines,
usbank, used cars

improving
acer
laptop
battery

bank of amer-
ica

bank of america , banko-
famerica, best buy, bed bath
and beyond, billing

italy,

ireland,

itunes and
ire-

NearestCompletion
italian ﬂag,
french,
aland
uranus, uranas, university,
university of chicago, ultra-
sound
battery powered ride ons,
battery plus charlotte nc,
battery died while driving,
best buy, battery replace-
ment for palm tungsten c

HybridCompletion
internet, italian ﬂag, itunes
and french, im help, irs

uranus, uranas, ups, united
airlines, usps

bank of america , best
buy, battery powered ride
ons, bankofamerica, battery
died while driving

Table 1: The top 5 completions provided by the 3 algorithms on (query, context) pairs taken from the AOL
log. In the ﬁrst two examples the context and the query are related, while in the last one they are not.

high probability. In this graph we compare the (weighted)
fractions of (query,context) pairs on which the MRR values
of one algorithm is higher than that of the other algorithm.
Since small diﬀerences in MRR values are insigniﬁcant (e.g.,
if one algorithm ranks the intended query at position 9 and
the other at position 10, the algorithms perform essentially
the same on this query), we deem the two algorithms to
do equally well on some input pair if the diﬀerence in their
MRR values on this pair is at most . Clearly, the lower the
value of , the tighter is the comparison.

After discarding input pairs on which the MRR of both
algorithms was 0, we were left with 3,894 pairs. Figure 2
demonstrates that for a wide range of  values, HybridCom-
pletion is superior to MostPopularCompletion on a larger
fraction of input pairs.



V
U
L
D
S

I
R
Q
R
L
W
F
D
U
I

G
H
W
K
J
L
H

:



















(TXDO055V
0RVW3RSXODU
V055LVKLJKHU
+\EULG
V055LVKLJKHU







(SVLORQYDOXHV







Figure 2: Weighted fractions of (query,context)
pairs where MostPopularCompletion’s MRR value
is higher than that of HybridCompletion in at least
an epsilon and vice versa.

We drilled down the results in order to understand the
relative strengths and weaknesses of the three algorithms on
diﬀerent inputs. To this end, we selected a random sample
of 198 (query, context) pairs from the set of 7,311 pairs, and
manually tagged each of them as related (i.e., the query is
related to the context; 60% of the pairs) and unrelated (40%
of the pairs). Table 2 compares the quality of the three
algorithms separately on the related pairs and on the un-
related pairs. The results indicate that when the intended
query is related to the context, NearestCompletion is very

successful, achieving wMRR that is 48% higher relative to
MostPopularCompletion. HybridCompletion is even better
because it takes both the context and the popularity into
account. On the other hand, when the query and context
are unrelated, NearestCompletion is essentially useless. Hy-
bridCompletion is even better than NearestCompletion for
related pairs, while its quality for unrelated pairs is moder-
ately lower (in 20.3%) than that of MostPopularCompletion.

Related context

Unrelated context

MostPopular Nearest Hybrid
0.280
0.181

0.163
0.227

0.242

0

Table 2: Weighted MRR of the three algorithms,
broken down by whether the intended query and
the context are related or not.

Next, we broke down the 7,311 sample pairs into buck-
ets based on the frequency of the intended query in the
query log. The buckets correspond to exponentially increas-
ing frequency ranges. Figure 3 plots the wMRR of each al-
gorithm in each of the buckets. As expected, MostPopular-
Completion is very successful at predicting popular queries.
It supersedes NearestCompletion for such queries, because
its success is independent of whether the context is related
to the intended query or not. On the other hand, when the
intended query is long-tail (low popularity), NearestComple-
tion manages to use the context to achieve a relatively high
prediction quality, while MostPopularCompletion exhibits
very poor quality. Note that HybridCompletion essentially
takes the upper envelope of the two algorithms, and manages
to achieve almost as high quality in all popularity ranges.

One peculiar artifact exhibited in this experiment is that
the quality of NearestCompletion slightly deteriorates for
popular queries (whose frequency in the log is above 10,000).
We analyzed these queries and found that the fraction of
unrelated contexts such queries have (58%) is much higher
than the fraction of unrelated contexts for low popularity
queries (only 36.5%). This explains the lower quality of
NearestCompletion for such queries.

Parameter tuning experiments. We conducted a set of
experiments in which we examined the eﬀect of the diﬀer-
ent parameters of our algorithms on their quality. Figure 4
shows the inﬂuence of the depth of the query recommenda-
tion tree used in the construction of the rich query represen-
tation on the quality of NearestCompletion. We examined

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India114100

10-1

10-2

10-3

10-4

R
R
M
w

10-5

101

102

Nearest
Hybrid
MostPopular

105

106

103

Frequency

104

Figure 3: Weighted MRR of the 3 algorithms as a
function of the frequency of the intended query in
the query log (log-log scale).

depths 0 to 3. As expected, the quality of NearestComple-
tion increases, as the depth increases, since the vocabulary
of the rich representation is richer. Note that the returns
are starting to diminish at depth 3. While we could not run
the experiment with larger depths, due to Google’s scraping
limitations, we expect this trend to continue as the depth in-
creases. Thus, a recommendation tree of depth 2 or 3 seems
to be the most cost-eﬀective for this application.

5
5
0
Z















'HSWK
'HSWK
'HSWK
'HSWK

$OO

5LFK

&RQWH[W7\SH

7KLQ

Figure 4: Weighted MRR of NearestCompletion as a
function of the recommendation tree depth. Results
are for all sample (query, context) pairs, for the ones
in which the context has a rich representation, and
for the ones in which the context does not have a
rich representation.

Next, we measured the eﬀect of the context length on
the quality of NearestCompletion. Our experiments (see
Table 3) demonstrate that increasing the number of recent
queries being taken into account slightly improves the qual-
ity of the algorithm, as the vocabulary that describes the
context is enriched. The eﬀect is not as signiﬁcant as the
recommendation tree depth, though.

Context length

1

2

3

wMRR

0.139

0.154

0.164

Table 3: Weighted MRR of NearestCompletion as a
function of the context length. Results are for 2,374
(query, context) pairs in which the context was of
length at least 3.

Table 4 shows the dependence of NearestCompletion’s qual-
ity on the query recommendation algorithm used to gen-
erate the recommendation trees. We compared two algo-
rithms: Google’s query auto-completions and Google’s re-
lated search. The results demonstrate that the rich repre-
sentations generated from Google’s related searches are more
eﬀective (quality improves in 12.4%). Nevertheless, in most
of our experiments we opted to use Google’s query auto com-
pletions, because the diﬀerence is not huge and since they
are much easier to scrape (the query latency is lower and
the rate limits posed by Google are higher). We thus con-
clude that the auto completions are more cost-eﬀective for
this purpose.

Auto-completions Related searches

wMRR

0.177

0.199

Table 4: Weighted MRR of NearestCompletion us-
ing two diﬀerent query recommendation algorithms
(by Google) for generating recommendation trees of
depth 2. Results are only for (query, context) pairs
in which the context has a rich representation.

The parameter α in HybridCompletion controls the bal-
ance between NearestCompletion and MostPopularComple-
tion. Recall that for α = 1 HybridCompletion is the same
as NearestCompletion and for α = 0 it is the same as Most-
PopularCompletion. Figure 5 analyzes the eﬀect of α on the
quality of the algorithm. As noted above, MostPopularCom-
pletion is better than NearestCompletion when the intended
query is popular and NearestCompletion is better when the
context query is related to the intended query. The results
show that α = 0.5 is the best choice in aggregate.

100

10-1

10-2

10-3

10-4

R
R
M
w

10-5

101

102

0
0.25
0.5
0.75
1
105

106

103

Frequency

104

Figure 5: Weighted MRR of HybridCompletion as
a function of α (log-log scale).

We measured the inﬂuence of other parameters of the al-
gorithm, like the choice of the depth weighting function, the
choice of the context weighting function and the N-grams
maximum length. We have not found signiﬁcant diﬀerences
in quality among the diﬀerent alternatives. We suppress the
actual results from this draft, due to lack of space.

7. CONCLUSIONS

In this paper we proposed the ﬁrst context-sensitive al-
gorithm for query auto-completion. The algorithm, Near-
estCompletion, suggests to the user completions of her in-
put preﬁx that are most similar to the recent queries the

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India115user has just entered. We show that when the input pre-
ﬁx is short (1 character) and the context is relevant to the
user’s intended query, then the weighted MRR of Nearest-
Completion is 48% higher than that of the standard Most-
PopularCompletion algorithm. On the other hand, when
the context is irrelevant, NearestCompletion is useless. We
then propose HybridCompletion, which is a convex combi-
nation of NearestCompletion and MostPopularCompletion.
HybridCompletion is shown to be at least as good as Near-
estCompletion when the context is relevant and almost as
good as MostPopularCompletion when the context is irrele-
vant.

NearestCompletion computes the similarity between queries

as the cosine similarity between their rich representations.
To produce rich query representation we introduce a new
query expansion technique, based on traversal of the query
recommendation tree rooted at the query. This technique
may be of independent interest for other applications of
query expansion.

There are a number of possible interesting directions for
further development of our techniques.
(a) The choice of
the optimal α value of HybridCompletion may be done adap-
tively. An algorithm which learns an optimal α as a function
of the context features is likely to improve the quality of the
combination. (b) Predicting the ﬁrst query in a session still
remains an open problem. Here one may need to rely on
other contextual signals, like the user’s recently visited page
or the user’s long-term search history. (c) The rich query
representation may be further ﬁne tuned by promoting the
prominent keywords in the representation and demoting the
less prominent ones. (d) We introduce a novel method for
query expansion based on the query recommendation tree.
It will be of interest to compare between the quality of our
suggested technique and the quality of standard query ex-
pansion techniques. Such a comparison may be done in the
scope of context-sensitive query auto-completion, as well as
in other relevant IR tasks such as document search.

Acknowledgments. We thank Shraga Kraus for his care-
ful review of the paper and for his guidance with MATLAB.

8. REFERENCES

[1] M. Arias, J. M. Cantera, J. Vegas, P. de la Fuente,

J. C. Alonso, G. G. Bernardo, C. Llamas, and
´A. Zubizarreta. Context-based personalization for
mobile web search. In PersDB, pages 33–39, 2008.

[2] R. Baeza-Yates, C. Hurtado, and M. Mendoza.

Improving search engines by query clustering. J. Am.
Soc. Inf. Sci. Technol., 58(12):1793–1804, 2007.
[3] H. Bast, D. Majumdar, and I. Weber. Eﬃcient

interactive query expansion with complete search. In
CIKM, pages 857–860, 2007.

[4] H. Bast and I. Weber. Type less, ﬁnd more: fast
autocompletion search with a succinct index. In
SIGIR, pages 364–371, 2006.

[5] D. Beeferman and A. Berger. Agglomerative clustering
of a search engine query log. In Proceedings of the sixth
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 407–416, 2000.
[6] P. Boldi, F. Bonchi, C. Castillo, D. Donato, and

S. Vigna. Query suggestions using query-ﬂow graphs.
In WSCD, pages 56–63, 2009.

[7] A. Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich,

V. Josifovski, and L. Riedel. Search advertising using
web relevance feedback. In CIKM, pages 1013–1022,
2008.

[8] A. Z. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski,

D. Metzler, L. Riedel, and J. Yuan. Online expansion
of rare queries for sponsored search. In WWW, pages
511–520, 2009.

[9] H. Cao, D. Jiang, J. Pei, E. Chen, and H. Li. Towards
context-aware search by learning a very large variable
length hidden markov model from search logs. In
WWW, pages 191–200, 2009.

[10] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and

H. Li. Context-aware query suggestion by mining
click-through and session data. In KDD, pages
875–883, 2008.

[11] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,

Z. Solan, G. Wolfman, and E. Ruppin. Placing search
in context: the concept revisited. ACM Trans. Inf.
Syst., 20(1):116–131, 2002.

[12] B. M. Fonseca, P. B. Golgher, E. S. de Moura, and

N. Ziviani. Using association rules to discover search
engines related queries. In LA-WEB, page 66, 2003.

[13] Q. He, D. Jiang, Z. Liao, S. C. H. Hoi, K. Chang,

E. Lim, and H. Li. Web query recommendation via
sequential query prediction. In ICDE, pages
1443–1454, 2009.

[14] S. Ji, G. Li, C. Li, and J. Feng. Eﬃcient interactive

fuzzy keyword search. In WWW, pages 371–380, 2009.

[15] R. Jones, B. Rey, O. Madani, and W. Greiner.

Generating query substitutions. In WWW, pages
387–396, 2006.

[16] R. Kraft, C. C. Chang, F. Maghoul, and R. Kumar.

Searching with context. In WWW, pages 477–486,
2006.

[17] C. D. Manning, P. Raghavan, and H. Sch¨utze.

Introduction to Information Retrieval. Cambridge
University Press, 2008.

[18] Q. Mei, D. Zhou, and K. Church. Query suggestion
using hitting time. In CIKM, pages 469–478, 2008.

[19] G. Pass, A. Chowdhury, and C. Torgeson. A picture of

search. In 1st InfoScale, 2006.

[20] J. J. Rocchio. Relevance Feedback in Information

Retrieval. Prentice Hall, 1971.

[21] E. Sadikov, J. Madhavan, L. Wang, and A. Halevy.

Clustering query reﬁnements by user intent. In
WWW, pages 841–850, 2010.

[22] S. K. Tyler and J. Teevan. Large scale query log

analysis of re-ﬁnding. In Proceedings of the third ACM
international conference on Web search and data
mining, pages 191–200, 2010.

[23] R. W. White and G. Marchionini. Examining the

eﬀectiveness of real-time query expansion. Inf.
Process. Manage., 43(3):685–704, 2007.

[24] Z. Zhang and O. Nasraoui. Mining search engine
query logs for query recommendation. In WWW,
pages 1039–1040, 2006.

WWW 2011 – Session: Query AnalysisMarch 28–April 1, 2011, Hyderabad, India116