SourceRank: Relevance and Trust Assessment for Deep

Web Sources Based on Inter-Source Agreement

Raju Balakrishnan, and Subbarao Kambhampati
Computer Science and Engineering, Arizona State University

∗

Tempe AZ USA 85287

rajub@asu.edu, rao@asu.edu

ABSTRACT
One immediate challenge in searching the deep web databases
is source selection—i.e.
selecting the most relevant web
databases for answering a given query. The existing database
selection methods (both text and relational) assess the source
quality based on the query-similarity-based relevance assess-
ment. When applied to the deep web these methods have
two deﬁciencies. First is that the methods are agnostic to
the correctness (trustworthiness) of the sources. Secondly,
the query based relevance does not consider the importance
of the results. These two considerations are essential for
the open collections like the deep web. Since a number of
sources provide answers to any query, we conjuncture that
the agreements between these answers are likely to be helpful
in assessing the importance and the trustworthiness of the
sources. We compute the agreement between the sources as
the agreement of the answers returned. While computing
the agreement, we also measure and compensate for possi-
ble collusion between the sources. This adjusted agreement
is modeled as a graph with sources at the vertices. On this
agreement graph, a quality score of a source that we call
SourceRank, is calculated as the stationary visit probabil-
ity of a random walk. We evaluate SourceRank in multi-
ple domains, including sources in Google Base, with sizes
up to 675 sources. We demonstrate that the SourceRank
tracks source corruption. Further, our relevance evaluations
show that SourceRank improves precision by 22-60% over
the Google Base and other baseline methods. SourceRank
has been implemented in a system called Factal.

Categories and Subject Descriptors
H.3.5 [INFORMATION STORAGE AND RETRIEVAL]:
Online Information Services—Web-based services

General Terms
Algorithms, Experimentation

1.

INTRODUCTION

By many accounts, surface web containing HTML pages
is only a fraction of the overall information available on
∗
and two Google research awards.

This research is supported by ONR grant N000140910032

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

the web. The remaining is hidden behind a welter of web-
accessible relational databases. By some estimates, the data
contained in this collection—popularly referred to as the
deep web—is estimated to be in tens of millions [25]. Search-
ing the deep web has been identiﬁed as the next big chal-
lenge in information management [30]. The most promis-
ing approach that has emerged for searching and exploiting
the sources on the deep web is data integration. A critical
advantage of integration to surface web search is that the
integration system (mediator) can leverage the semantics
implied in the structure of deep web tuples. Realizing this
approach however poses several fundamental challenges, the
most immediate of which is that of source selection. Brieﬂy,
given a query, the source selection problem involves selecting
the best subset of sources for answering the query.

Although source selection problem received some atten-
tion in the context of text and relational databases (c.f. [26,
9, 13, 27, 21]) existing approaches are focused on assessing
the relevance of a source based on local measures of simi-
larity between the query and the answers expected from the
source. In the context of deep web, such a purely source-
local approach has two important deﬁciencies:

1. Query based relevance assessment is insensitive to the
importance of the source results. For example, the
query godfather matches the classic movie The Godfa-
ther as well as the little known movie Little Godfather.
Intuitively, most users are likely to be looking for the
classic movie.

2. The source selection is agnostic to the trustworthiness
of the answers. Trustworthiness is a measure of cor-
rectness of the answer (in contrast to relevance, which
assesses whether a tuple is answering the query, not
the correctness of the information). For example, for
the query The Godfather, many databases in Google
Base return copies of the book with unrealistically low
prices to attract the user attention. When the user
proceeds towards the checkout, these low priced items
would turn out to be either out of stock or a diﬀer-
ent item with the same title and cover (e.g. solution
manual of the text book).

A global measure of trust and importance is particularly
important for uncontrolled collections like the deep web,
since sources try to artiﬁcially boost their rankings. A global
relevance measure should consider popularity of a result, as
the popular results tend to be relevant. Moreover, it is im-
prudent to measure trustworthiness of sources based on local
measures; since the measure of trustworthiness of a source

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India227should not depend on any information the source provides
about itself. In general, the trustworthiness of a particular
source has to be evaluated in terms of the endorsement of
the source by other sources.

Result Agreement as Implicit Endorsement: Given
that the source selection challenges are similar in a way to
“page” selection challenges on the web, an initial idea is to
adapt a hyper-link based methods like PageRank [11] or au-
thorities and hubs [22] from the surface web. However, the
hyper-link based endorsement is not directly applicable to
the web databases since there are no explicit links across
records. To overcome this problem, we create an implicit
endorsement structure between the sources based on the
agreement between the results. Two sources agree with each
other if they return the same records in answer to the same
query. It is easy to see that this agreement based analysis
will solve the result importance and source trust problems
mentioned above. Result importance is handled by the fact
that the important results are likely to be returned by a
larger number of sources. For example, the classic Godfa-
ther movie is returned by hundreds of sources while the Little
Godfather is returned by less than ten sources on a Google
Products search [1]. A global relevance assessment based
on the agreement of the results would thus have ranked the
classic Godfather high. Similarly, regarding trust, the cor-
ruption of results can be captured by an agreement based
method, since other legitimate sources answering the same
query are likely to disagree with the incorrect results (e.g.
disagree with unrealistically low price of the book result).
We provide a formal explanation for why agreement implies
trust and relevance in Subsection 3.1 below.

Challenges in Computing Result Agreement: Agree-
ment computation between the web databases poses multi-
ple challenges that necessitate combination and extension of
methods from relational and text databases. The primary
challenge is that diﬀerent web databases may represent the
same entity syntactically diﬀerently, making the agreement
computation hard [14]. To solve this problem, we combine
record linkage models with entity matching techniques for
accurate and speedy agreement computation. Further, at-
tributes matchings are weighted by the computed attribute
importance. Another challenge in computing agreement is
that most web databases are non-cooperative—i.e. they do
not allow access to full data or source statistics. Instead, ac-
cess is limited to retrieving a set of top-k answers to a simple
key word query. To handle this, we adapt query based sam-
pling methods used for text databases [12].

Combating Source Collusion: Like PageRank, databases
may enhance SourceRank by colluding with each other. Dif-
ferentiating genuine agreement between the sources from the
collusion increases the robustness of the SourceRank. We
devise a method to detect the source dependence based on
answers to the “large answer” queries. A large answer query
is a very general keyword like “DVD” or “director” with a
large set of possible answers. If two sources always return
the same answers to these type of queries, they are likely
to be dependent (colluding). We expand on this intuition
to measure and compensate for the source collusion while
calculating the agreement.

Implementation and Evaluation: We implemented the
SourceRank based source selection in a system called Fac-
tal, which may be accessed at http://factal.eas.asu.edu/

(details of the system are given in [8]). To compare the
performance, we evaluated the ability of SourceRank to se-
lect trustworthy and relevant sources over two sets of web
sources—(i) sets of books and movie databases in TEL-8
repository [5] and (ii) books and movie sources from Google
Base [1]. Our evaluation shows that SourceRank improves
the relevance of source selection by 22-60% over the existing
methods. We also show that the SourceRank combined with
the default Google Base result ranking improves the top−k
precision of results by 23-50% over stand-alone Google Base.
Trustworthiness of source selection is evaluated as the abil-
ity to remove sources with corrupted attributes. Our experi-
ments show that the SourceRank diminishes almost linearly
with the source corruption.

The overall contributions of the paper are: (i) An agree-
ment based method to calculate relevance of the deep web
sources based on popularity. (ii) An agreement based method
to calculate trustworthiness of deep web sources. (iii) Do-
main independent computation of the agreement between
the deep web databases. (iv) A method for detecting collu-
sion between the web databases, and (v) Formal evaluations
on large sets of sources.

The rest of this paper is organized as follows. Next section
discusses the related work. Section 3 provides a formal justi-
ﬁcation for calculating source reputation based on the agree-
ment of sources, and presents the SourceRank calculation
method. The following section explains the computation of
agreement between the sources, and describes how sources
are sampled to get the seed results for supporting agree-
ment computation. Next, we explain our collusion detection
method.
In Section 6, SourceRank is applied to multiple
domains and types of sources to demonstrate the improved
relevance and trustworthiness. We also evaluate source col-
lusion detection and the time to calculate the SourceRank.

2. RELATED WORK

The indispensability and diﬃculty of source selection for
the deep web has been recognized previously [25]. Current
relational database selection methods minimize cost by re-
trieving maximum number of distinct records from minimum
number of sources [26]. Cost based web database selection is
formulated as selecting the least number of databases max-
imizing number of relevant tuples (coverage). The related
problem of collecting source statistics [26, 21] has also been
researched. The problem of ranking database tuples for key
word search is addressed [10].

Considering research in the text databases selection, Callan
et al. [13] formulated a method called CORI for query spe-
ciﬁc selection based on relevance. Cooperative and non-
cooperative text database sampling [12, 21] and selection
considering coverage and overlap to minimize cost [28, 27]
are addressed by a number of researchers.

Combining multiple retrieval methods for text documents
has been used for improved accuracy [16]. Lee [24] observes
that the diﬀerent methods are likely to agree on the same
relevant documents than on irrelevant documents. This ob-
servation rhymes with our argument in Section 3 in giving
a basis for agreement-based relevance assessment. For the
surface web, Gy¨ongyi et al. [20] proposed trust rank, an ex-
tension of page rank considering trustworthiness of hyper-
linked pages. Agrawal et al. [6] explored ranking database
search records by comparing to corresponding web search
results.

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India228Universal Set
of Tuples (U)

ele v a nt
T u ple s( R )

R

Relevant
&True (RT)

S3

R1

T u ples(T)
Tru e

R3

(a)

R2

0.6

0.78

0.86

0.22

(b)

S1

0.14

0.4

S2

Figure 1: (a) Model for explaining why the agreement implies trust and relevance. Universal set U is the
search space, RT is the intersection of trustworthy tuple set T and relevant tuple set R (RT is unknown).
R1, R2 and R3 are the result sets of three sources. (b) A sample agreement graph structure of three sources.
The weight of the edge from Si to Sj is computed by Equation 5.

A probabilistic framework for trust assessment based on
agreement of web pages for question answering has been
presented by Yin et al. [31]. Their framework however does
not consider the inﬂuence of relevance on agreement, mul-
tiple correct answers to a query, record linkage and non-
cooperative sources; thus limiting its usability for the deep
web. Dong et al. [18, 17] extended this model consider-
ing source dependence using the same basic model as Yin
et al. As we shall see, the collusion detection in the deep
web needs to address diﬀerent constraints like multiple true
values, non-cooperative sources, and ranked answer sets.

3. SOURCERANK: TRUST AND

RELEVANCE RANKING OF SOURCES
In this section we formalize the argument that the rele-
vance and trustworthiness of a source manifests as the agree-
ment of its results with those from other sources. We also ex-
plain the 2-step SourceRank calculation process: (i) creating
a source graph based on agreement between the sources (ii)
assessing the source reputation based on this source graph.

3.1 Agreement as Endorsement

In this section we show that the result set agreement is
an implicit form of endorsement. In Figure 1(a) let RT be
the set of relevant and trustworthy tuples for a query, and
U be the search space (the universal set of tuples searched).
Let r1 and r2 be two tuples independently picked by two
sources from RT (i.e. they are relevant and trustworthy),
and PA(r1, r2) be the probability of agreement of the tuples
(for now think of “agreement” of tuples in terms of high de-
gree of similarity; we shall look at the speciﬁc way agreement
between tuples is measured in Section 4).

PA(r1, r2) =

1|RT|

(1)

Similarly let f1 and f2 be two irrelevant (or untrustworthy)
tuples picked by two sources and PA(f1, f2) be the agree-
ment probability of these two tuples. Since f1 and f2 are

from U − RT

PA(f1, f2) =

1

|U − RT|

(2)

For any web database search, the search space is much larger
|U| (cid:3) |RT|. Applying
than the set of relevant tuples, i.e.
this in Equation 1 and 2 implies

PA(r1, r2) (cid:3) PA(f1, f2)

(3)

For example, assume that the user issues the query God-
father for the Godfather movie trilogy. Three movies in the
trilogy— The Godfather I, II and III—are thus the results
relevant to the user. Let us assume that the total number of
movies searched by all the databases (search space U ) is 104.
In this case PA(r1, r2) = 1
104 (strictly
speaking
104−3 ). Similarly the probability of three sources
agreeing are 1
108 for relevant and irrelevant results
respectively.

3 and PA(f1, f2) = 1

1

9 and 1

Let us now extend this argument for answer sets from
two sources. In Figure 1(a) R1, R2 and R3 are the result
sets returned by three independent sources. The result sets
are best eﬀort estimates of RT (assuming a good number of
genuine sources). Typically the results sets from individual
sources would contain a fraction of relevant and trustworthy
tuples from RT , and a fraction of irrelevant tuples from U −
RT . By the argument in the preceding paragraph, tuples
from RT are likely to agree with much higher probability
than tuples from U−RT . This implies that the more relevant
tuples a source returns, the more likely that other sources
agree with its results.

Though the explanation above assumes independent sources,

it holds for partially dependent sources as well. However,
the ratio of two probabilities (i.e. the ratio of probability in
Equation 1 to Equation 2) will be smaller than that for the
independent sources. For added robustness of the SourceR-
ank against source dependence, in Section 5 we assess and
compensate for the collusion between the sources.
3.2 Creating The Agreement Graph

To facilitate the computation of SourceRank, we represent
the agreement between the source result sets as an agree-

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India229ment graph. Agreement graph is a directed weighted graph
as shown in example Figure 1(b).
In this graph, the ver-
tices represent the sources, and weighted edges represent
the agreement between the sources. The edge weights cor-
respond to the normalized agreement values between the
sources. For example, let R1 and R2 be the result sets of
the source S1 and S2 respectively. Let a = A(R1, R2) be the
agreement between the results sets (calculated as described
in Section 4). In the agreement graph we create two edges:
a|R2| ; and one from
one from S1 to S2 with weight equal to
a|R1| . The semantics of the
S2 to S1 with weight equal to
weighted link from S1 to S2 is that S1 endorses S2, where
the fraction of tuples endorsed in S2 is equal to the weight.
Since the endorsement weights are equal to the fraction of
tuples, rather than the absolute number, they are asymmet-
ric.

As we shall see in Section 4, the agreement weights are
estimated based on the results to a set of sample queries. To
account for the “sampling bias” in addition to the agreement
links described above, we also add “smoothing links” with
small weights between every pair of vertices. Adding this
smoothing probability, the overall weight w(S1 → S2) of the
link from S1 to S2 is:

(cid:2)

(4)

(5)

AQ(S1, S2) =

q∈Q

A(R1q, R2q)

|R2q|

w(S1 → S2) = β + (1 − β) × AQ(S1, S2)

|Q|

where R1q and R2q are the answer sets of S1 and S2 for the
query q, and Q is the set of sampling queries over which the
agreement is computed. β is the smoothing factor. We set β
at 0.1 for our experiments. Empirical studies like Gleich et
al. [19] may help more accurate estimation. These smooth-
ing links strongly connect agreement graph (we shall see
that strong connectivity is important for the convergence of
SourceRank calculation). Finally we normalize the weights
of out links from every vertex by dividing the edge weights
by sum of the out edge weights from the vertex. This nor-
malization allows us to interpret the edge weights as the
transition probabilities for the random walk computations.
3.3 Calculating SourceRank

Let us start by considering certain desiderata that a rea-
sonable measure of reputation deﬁned with respect to the
agreement graph must satisfy:

1. Nodes with high in-degree should get higher rank—
since high in-degree sources are endorsed by a large
number of sources, they are likely to be more trust-
worthy and relevant.

2. Endorsement from a source with a high in-degree should
be more respected than endorsed from a source having
smaller in-degree. Since a highly endorsed source is
likely to be more relevant and trustworthy, the source
endorsed by a highly endorsed source is also likely to
be of high quality.

The agreement graph described above provides important
guidance in selecting relevant and trustworthy sources. Any
source that has a high degree of endorsement by other rele-
vant sources is itself a relevant and trustworthy source. This
transitive propagation of source relevance (trustworthiness)

through agreement links can be captured in terms of a ﬁxed
point computation [11]. In particular, if we view the agree-
ment graph as a markov chain, with sources as the states,
and the weights on agreement edges specifying the prob-
abilities of transition from one state to another, then the
asymptotic stationary visit probabilities of the markov ran-
dom walk will correspond to a measure of the global rele-
vance of that source. We call this measure SourceRank.

The markov random walk based ranking does satisfy the
two desiderata described above. The graph is strongly con-
nected and irreducible, hence the random walk is guaranteed
to converge to the unique stationary visit probabilities for
every node. This stationary visit probability of a a node is
used as the SourceRank of that source.

4. AGREEMENT COMPUTATION

AND SAMPLING

If the sources are fully relational and share the same schema,

then agreement between two tuples will reduce to equality
between them. On the other extreme, if the sources are text
databases then the agreement between two items will have
to be measured in terms of textual similarity. Deep web
sources present an interesting middle ground between the
free-text sources in IR, and the fully structured sources in
relational databases. Hence to address challenges in agree-
ment computation of deep web results we have to combine
and extend methods from both these disciplines. In the fol-
lowing subsection, we will describe agreement computation
and sampling sources to compute agreement.
4.1 Computing Agreement

Computing agreement between the sources involves fol-
lowing three levels of similarity computations: (a) attribute
value similarity (b) tuple similarity, and (c) result set simi-
larity.

If the diﬀerent web
(a) Attribute value similarity:
databases were using common domains1 for the names, cal-
culating agreement between the databases is trivial. But
unfortunately, assumption of common domains rarely holds
in web databases [14]. For example, the title and casting at-
tributes of tuples referring to the same movie returned from
two databases are shown in Table 1(a) and 1(b). Identifying
the semantic similarity between these tuples is not straight-
forward, since the titles and actor lists show wide syntactic
variation.

The textual similarity measures work best for scenarios in-
volving web databases with no common domains [14]. Since
this challenge of matching attribute values is essentially a
name matching task, we calculate the agreement between
attribute values using SoftTF-IDF with Jaro-Winkler as the
similarity measure [15]. SoftTF-IDF measure is similar to
the normal TF-IDF measure. But instead of considering
only exact same words in two documents to calculate similar-
ity, SoftTF-IDF also considers occurrences of similar words.
be the set of words for w ∈ vi such that there is some u ∈ vj
with sim(w, u) > θ. Let D(w, vj) = maxu∈vj sim(w, u).
The V(w, vi) are the normal TF values weighted by log(IDF )
1common domains means names referring to the same entity
are the same for all the databases, or can be easily mapped
to each other by normalization

Formally, let vi and vj be the values compared, and C(θ, vi, vj)

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India230(a)

(b)

Title

Casting

Title

Casting

1 Godfather, The: The Coppola

James Caan /

1 The Godfather - The Coppola Marlon Brando,

Restoration

Marlon Brando more

Restoration Giftset [Blu-ray]

Al Pacino

Table 1: Sample tuples returned by two movies databases to the query Godfather are shown in Table (a)
(tuples from ﬁrst source) and (b) (tuples from second source).

used in the basic TF-IDF. SoftTFIDF is calculated as,
V(w, vi)V(u, vj)D(w, vj)

SIM(vi, vj) =

(cid:2)

w∈C(θ,vi,vj )

(6)

We used Jaro-Winkler as a secondary distance function sim
above with an empirically determined θ = 0.6. Compara-
tive studies show that this combination provides best per-
formance for name matching [15]. For pure numerical values
(like price) we calculate similarity as the ratio of the diﬀer-
ence of values to the maximum of the two values.

(b) Tuple similarity: The tuples are modeled as a vector
of bags [14]. The problem of matching between two tuples
based on the vector of bags model is shown in Figure 2. If
we know which attribute in t1 maps to which attribute in t2,
then the similarity between the tuples is simply the sum of
the similarities between the matching values. The problem
of ﬁnding this mapping is the well known automated answer
schema mapping problem in web databases [29]. We do not
assume predeﬁned answer schema mapping, and hence re-
construct the schema mapping based on the attribute value
similarities as described below.

The complexity of similarity computation between the at-
tribute values (i.e. building edges and weights in Figure 2) of
two tuples t1 and t2 is O(|t1||t2|) (this is equal to the number
of attribute value comparisons required). After computing
these edges, a single attribute value in t1 may be similar to
multiple attributes in t2 and vice versa. The optimal match-
ing should pick the edges (matches) such that the sum of the
matched edge weights would be maximum.

(cid:3)

Sopt(t, t

) = arg max

M

SIM(v1, v2)

(7)

(cid:2)

(vi∈t,v2∈t(cid:2))∈M

Note that this problem is isomorphic to the well known
“maximum weighted bipartite matching problem”. The Hun-
garian algorithm gives the lowest time complexity for the
maximum matching problem, and is O(V 2log(V ) +V E ) (in
the context of our agreement calculation problem, V is the
number attribute values to be matched, and E is the num-
ber of similarity values). Since E is O(V 2) for our problem
the overall time complexity is O(V 3).

Running time is an important factor for calculating agree-
ment at the web scale. Considering this, instead of the
O(V 3) optimal matching discussed above, we use the O(V 2)
greedy matching algorithm as a reasonable balance between
time complexity and performance. To match tuples, say t1
and t2 in Figure 2, the ﬁrst attribute value of t1 is greed-
ily matched against the most similar attribute value of t2.
Two attributes values are matched only if the similarity ex-
ceeds a threshold value (we used an empirically determined
threshold of 0.6 in our experiments). Subsequently, the sec-
ond attribute value in the ﬁrst tuple is matched against the
most similar unmatched attribute value in the second tuple

t1

t2

Godfather

Paperback

Used $9.99

0

.

7

0.91.0

0.3

0

.

4

0
.
7

Paperback The Godfather $13.99

Figure 2: Example tuple similarity calculation. The
dotted line edges denote the similarities computed,
and the solid edges represent the matches picked by
the greedy matching algorithm.

and so on. The edges selected by this greedy matching step
are shown in solid lines in Figure 2. The agreement between
the tuples is calculated as the sum of the similarities of the
individual matched values. The two tuples are considered
matching if they exceed a empirically determined threshold
of similarity.

The Fellagi-Saunter record linkage model [23] suggests
that the attribute values occurring less frequently are more
indicative of the semantic similarity between the tuples. For
example, two entities with the common title The Godfather
are more likely to be denoting same book than two entities
with common format paperback ). To account for this, we
weight the similarities between the matched attributes in
the step above as

(cid:3)

) =

S(t, t

vi,vj∈M wijSIM(vi, vj)
(cid:4)(cid:3)

(8)

(cid:3)

A(R1q, R2q) = arg max

M

(t∈R1q ,t(cid:2)∈R2q )∈M

(cid:3)

where M is the optimal matched pairs of tuples between
R1q and R2q and S(t, t
) are as calculated in Equation 8.
Since this is again a bipartite matching problem similar to
Equation 7, we use a greedy matching. The ﬁrst tuple in R1q
is matched greedily against the tuple with highest match in

vi,vj∈M w2
ij
(cid:3)

k IDF ik
|vi|

where vi,vj are attribute values of t and t
respectively, and
wi,j is the weight assigned to the match between vi and vj
based on the mean inverse document frequency of the tokens
in vi and vj. Speciﬁcally, the wij’s are calculated as,

(cid:5)(cid:3)

(cid:6)

(cid:5)(cid:3)

log

wij = log

(9)
where vi is the ith attribute value and IDF ik is the inverse
document frequency of the kth token of the ith attribute
value. This is similar to the weighting of terms in TFIDF.
(c) Result Set Similarity: The agreement between two
result sets R1q and R2q from two sources for a query q is
deﬁned as,

(cid:6)

l IDF jl
|vj|

(cid:2)

(cid:3)

)

S(t, t

(10)

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India231R2q. Subsequently, the second tuple in R1q is matched with
the most similar unmatched tuple in R2q and so on. The
agreement between the two result sets is calculated as the
sum of the agreements between the matched tuples. The
agreement thus calculated is used in the Equation 4.

We calculate agreement between the top-k (with k = 5)
answer sets of the each query in the sampled set described
in the subsection below. We stick to top-k results since most
web information systems focus on providing best answers in
the top few positions (a reasonable strategy given that the
users rarely go below the top few results). The agreements
of the answers to the entire set of sampling queries is used in
Equation 4 to compute the agreement between the sources.
Note that even though we used top-k answers, the normal-
ization against the answer set size in Equation 4 is required,
since the answer set sizes vary as some sources return less
than k results to some queries.
4.2 Sampling Sources

Web databases are typically non-cooperative, i.e. they do
not share the statistics about the data they contain, or allow
access to the entire set of data. Thus, the agreement graph
must be computed over a sampled set. In this section we
describe the sampling strategy used for our experiments on
web databases (see Section 6). For sampling, we assume only
a form based query interface allowing key word queries; simi-
lar to the query based sampling used for the non-cooperative
text databases [12].

For generating sampling queries, we use the publicly avail-
able book and movie listings. We use two hundred queries
each from book and movie domain for sampling. To generate
queries for the book domain, we randomly select 200 books
from the New York Times yearly number one book listing
from the year 1940 to 2007 [3]. For the sampling query set of
movie domain, we use 200 random movies from the second
edition of New York Times movie guide [4].

As key word queries for sampling, we use partial titles of
the books/movies. We generate partial title queries by ran-
domly deleting words from titles of length more than one
word. The probability of deletion of a word is set to 0.5.
The use of partial queries is motivated by the fact that two
sources are less likely to agree with each other on partial title
queries. This is because partial titles are less constraining
and thus result in a larger number of possible answers com-
pared to full title queries. Hence agreement on answers to
partial queries is more indicative of agreement between the
sources (our initial experiments validated this assumption).
We perform a query based sampling of database by send-
ing the queries to the title keyword search ﬁelds of the
sources. The sampling is automated here, but we wrote
our own parsing rules to parse the result tuples from the re-
turned HTML pages. This parsing of tuples has been solved
previously [7], and can be automated. (parsing is not re-
quired for Google Base experiments as structured tuples are
returned.)

5. ASSESSING SOURCE COLLUSION

We measure the collusion of web databases on top-k an-
swer sets, since agreement is also computed on top-k an-
swers. Two issues that complicate collusion detection are
(i) even non-colluding databases in the same domain may
contain almost the same data. For example, many movie
sources may contain all Hollywood movies. (ii) top-k an-

swers from even non-colluding databases in the same domain
are likely to be similar. For example, two movie databases
are likely to return all three movies in Godfather trilogy for
the query Godfather. The collusion measure should not clas-
sify these genuine data and ranking correlations as collusion.
On the other hand, mirrors or near-mirrors with same data
and ranking functions need to be identiﬁed.

The basic intuition we use for collusion detection is that
if two sources return the same top-k answers to the queries
with large number of possible answers (e.g. queries con-
taining only stop words), they are possibly colluding. More
formally, for two ranked sets of answers, the expected agree-
ment between top-k answers E(Ak) is

(cid:7)

k

n (1 − e)
(1 − e)

E(Ak) =

if k < n
otherwise

(11)

where top-k answers are used to calculate agreement, size
of the answer set is n, and e is the error rate due to ap-
proximate matching. This means that for queries with large
number of answers (i.e. n (cid:3) k as k is ﬁxed) the expected
agreement between two independent sources is very low. As
a corollary, if the agreement between two sources on a large
answer query is high, they are likely to be colluding.

To generate a set of queries with large answer sets, we
fetched a set of two hundred keywords with highest docu-
ment frequencies from the crawl described in the Section 4.2.
Sources are probed with these queries. The agreement be-
tween the answer sets are computed based on this crawl
according to Equation 4. These agreements are seen as a
measure of the collusion between the sources. The agree-
ment computed between the same two sources on the sam-
ples based on genuine queries described in Section 4.2 is
multiplied by (1− collusion) to get the adjusted agreement.
These adjusted agreements are used for computing SourceR-
ank for the experiments below. We also provide a stand-
alone evaluation of collusion measure in Section 6.5.

6. PERFORMANCE EVALUATION

The methods described are implemented as a system namely

Factal (URL: http://factal.eas.asu.edu/); the system
details may be found in [8].
In this section we evaluate
the eﬀectiveness of SourceRank (computed based on collu-
sion adjusted-agreement) as the basis for domain speciﬁc
source selection that is sensitive to relevance and trustwor-
thiness of sources. The top-k precision and discounted cu-
mulative gain (DCG) of SourceRank-base source selection is
compared with three existing methods: (i) Coverage based
ranking used in relational databases, (ii) CORI ranking used
in text databases, and (iii) Google Product search on Google
Base.
6.1 Experimental Setup

Databases: We performed the evaluations in two vertical
domains—sellers of books and movies (movies include DVD,
Blu-Ray etc.). We used three sets of data bases— (i) a set of
stand-alone online data sources (ii) hundreds of data sources
collected via Google Base (iii) a million IMDB records [2].

The databases listed in TEL-8 database list in the UIUC
deep web interface repository [5] are used for online eval-
uations (we remove non-working sources). We used six-
teen movie databases and seventeen book databases from
the TEL-8 repository. In addition to these, we added ﬁve
video sharing databases to the movie domain and ﬁve library

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India232Precision
DCG

Coverage

SourceRank

CORI

SR-Coverage   

SR-CORI   

(a)

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0.25

0.2

0.15

0.1

0.05

0

Precision

DCG

Coverage

SourceRank

CORI

SR-Coverage   

SR-CORI   

(b)

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0.5

0.4

0.3

0.2

0.1

0

Gbase

Gbase-Domain

SourceRank

Coverage

Gbase

Gbase-Domain

SourceRank

Coverage

(c)

(d)

Figure 3: (a-b) Comparison of precision and DCG of top-4 online sources selected by Coverage, SourceRank,
CORI, Combination of SourceRank with Coverage (SR-Coverage) and CORI (SR-CORI) for movies (ﬁgure
a) and books (ﬁgure b). (c-d) Comparison of top-5 precision of results returned by SourceRank, Google Base
and Coverage for movies (ﬁgure c) and books (ﬁgure d).

sources to the book domain. These out-of-domain sources
are added to increase the variance in source quality. If all
sources are of similar quality, diﬀerent rankings do not make
a diﬀerence.

Google Base is a collection of data from a large number
of web databases, with an API-based access to data return-
ing ranked results [1]. The Google Products Search works
on Google Base. Each source in Google Base has a source
id. For selecting domain sources, we probed the Google Base
with a set of ten book/movie titles as queries. From the ﬁrst
400 results to each query, we collected source ids; and consid-
ered them as a source belonging to that particular domain.
This way, we collected a set of 675 book sources and 209
movie sources for our evaluations. Sampling is performed
through Google Base API’s as described in Section 4.2.

Test Query Set: Test query sets for both book and movie
domains are selected from diﬀerent lists than the sampling
query set, so that test and sampling sets are disjoint. The
movie and book titles in several categories are obtained from
a movie sharing site and a favorite books list. We generated
queries by randomly removing words from the movie/book
titles with probability of 0.5—in the same way as described

for the sampling queries above. We used partial titles as
the test queries, since typical web user queries are partial
descriptions of objects. The number of queries are used in
diﬀerent experiments varies between 50 to 80, so as to attain
95% conﬁdence levels.
6.2 Baseline Methods
Coverage: Coverage is computed as the mean relevance of
the top-5 results to the sampling queries described in Sec-
tion 4.2 above. For assessing the relevance of the results, we
used the SoftTF-IDF with Jaro-Winkler similarity between
the query and the results (recall that the same similarity
measure is used for the agreement computation).

CORI: To collect source statistics for CORI [13], we used
terms with highest document frequency from the sample
crawl data describe in Section 4.2 as crawling queries. Callan et
al. [12] observe that good performance is obtained by using
highest document frequency terms in related text databases
as queries to crawl. Similarly, we used two hundred high
tuple-frequency queries and used top-10 results for each query
to create resource descriptions for CORI. We used the same
parameters as found to be optimal by Callan et al. [13].

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India233SourceRank
Coverage
CORI

60

50

40

30

20

10

0

SourceRank
Coverage
CORI

45

40

35

30

25

20

15

10

5

0

→

 
)

%
(
k
n
a
R
n

 

i
 

e
s
a
e
r
c
e
D

→

 
)

%
(
k
n
a
R
n

 

i
 

e
s
a
e
r
c
e
D

−10
0

0.1

0.2

0.4

0.3
0.6
Corruption Level →

0.5

0.7

0.8

0.9

−5
0

0.1

0.2

0.4

0.3
0.6
Corruption Level →

0.5

0.7

0.8

0.9

(a)

(b)

Figure 4: Decrease in ranks of the sources with increasing source corruption levels for (a) movies and (b)
books domain. The SourceRank reduces almost linearly with corruption, while CORI and Coverage are
insensitive to the corruption.

CORI is used as the baseline, since the later developments
like ReDDE [28] depend on database size estimation by sam-
pling, and it is not demonstrated that this size estimation
would work on the ranked results from web sources.
6.3 Relevance Evaluation
Assessing Relevance: To assess the relevance, we used
randomly chosen queries from test queries described above
in Section 6.1. These queries are issued to the top-k sources
selected by diﬀerent methods. The results returned are man-
ually classiﬁed as relevant and non-relevant. The ﬁrst au-
thor performed the classiﬁcation of the tuples, since around
14,000 tuples were to be classiﬁed as relevant and irrelevant.
The classiﬁcation is simple and almost rule based. For exam-
ple, assume that the query is Wild West, and the original
movie name from which the partial query is generated is
Wild Wild West (as described in the test query description
in Section 6.1). If the result tuple refers to the movie Wild
Wild West (i.e. DVD, Blu-Ray etc. of the movie), then the
result is classiﬁed as relevant, otherwise it is classiﬁed to be
irrelevant. Similarly for books, if the result is the queried
book to sell, it is classiﬁed as relevant and otherwise it is
classiﬁed as irrelevant. As an insurance against biased clas-
siﬁcation by the author, we randomly mixed tuples from all
methods in a single ﬁle; so that the author did not know
which method each result came from while he does the clas-
siﬁcation. All the evaluations are performed to diﬀerentiate
SourceRank precision and DCG from competing methods by
non-overlapping conﬁdence intervals at a signiﬁcance level of
95% or more.

Online Sources: We compared mean top-5 precision and
DCG of top-4 Sources (we avoided normalization in NDCG
since ranked lists are of equal length). Five methods, namely
Coverage, SourceRank, CORI, and two linear combinations
of SourceRank with CORI and Coverage—(0.1×SourceRank+
0.9×CORI) and (0.5×Coverage+0.5×SourceRank)—are
compared. The higher weight for CORI in CORI-SourceRank
combination is to compensate for the higher dispersion of
SourceRank compared to CORI.

The results of the top-4 source selection experiments in
movie and books domain are shown in Figure 3(a) and 3(b).

0.228

For both the domains, SourceRank clearly outperforms the
Coverage and CORI. For the movie domain, SourceRank in-
×
creases precision over Coverage by 73.0% (i.e. 0.395−0.228
100) and over CORI by 29.3%. DCG of SourceRank is higher
by 90.4% and and 20.8% over Coverage and CORI respec-
tively. For the books domain, SourceRank improves both
precision and DCG over CORI as well as Coverage by ap-
proximately 30%. The SourceRank outperforms stand-alone
CORI and Coverage in both precision and DCG at a conﬁ-
dence level of 95%. Though the primary target of the eval-
uation is not diﬀerentiating SourceRank and combinations,
it may be worth mentioning that SourceRank outperforms
the combinations at a conﬁdence level more than 90% in
most cases. This is not surprising, since the sources selected
return the results based on query based relevance. Hence
the results from SourceRank-only source selection implicitly
account for the query similarity (keep in mind that CORI
and Coverage select sources based on query relevance).

As a note on the seemingly low precision values, these are
mean relevance of the top-5 results. Many of the queries
used have less than ﬁve possible relevant answers (e.g. a
book title query may have only paperback and hard cover for
the book as relevant answers). But since the web databases
always tend to return full ﬁrst page of results average top-5
precision is bound to be low.

Google Base:

In these experiments we tested if the
precision of Google Base search results can be improved
by combining SourceRank with the default Google Base
relevance ranking. Google Base tuple ranking is applied
on top of source selection by SourceRank and compared
with stand-alone Google Base Ranking. This combination
of source selection with Google Base is required for perfor-
mance comparison, since source ranking cannot be directly
compared with the tuple ranking of Google Base. For the
book domain, we calculated SourceRank for 675 book do-
main sources selected as described in Section 6.1. Out of
these 675 sources, we selected top-67 (10%) sources based
on SourceRank. Google Base is made to query only on this
top-67 Sources, and the precision of top-5 tuples is compared
with that of Google Base Ranking without this source selec-
tion step. Similarly for the movie domain, top-21 sources are

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India2341

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
1

0.9

0.8

0.7

0.6

0.5

Rank Correlation →

0.4

Collusion
Agreement
Adjusted Agreement

0.3

0.2

0.1

0

→

t

 
)
s
e
u
n
M

i

(
 

e
m
T

i

350

300

250

200

150

100

50

0
0

Books

100

200

Number of Sources →

300

400

500

600

Figure 5: Variation of Collusion, Agreement and Ad-
justed Agreement with rank correlations. Adjusted
Agreement is Agreement × (1 − collusion).

Figure 6: Time to compute agreement against number
of sources.

selected. DCG is not computed for these experiments since
all the results are ranked by Google Base ranking, hence
ranking order comparison is not required.

In Figure 3(c) and 3(d), the GBase is the stand-alone
Google Base ranking. GBase-Domain is the Google Base
ranking searching only in the domain sources selected using
our query probing. For example, in Figure 3(d), Google Base
is made to search only on the 675 book domain sources used
in our experiments. The plots SourceRank and Coverage
are Google Base tuple rank applied to the tuples from top-
10% sources selected by the SourceRank and Coverage based
source selections respectively. SourceRank outperforms all
other methods (conﬁdence levels are 95% or more). For the
movie domain, SourceRank precision exceeds Google Base
by 38% and coverage by 23%. For books the diﬀerences
are 53% and 25% with Google Base and Coverage respec-
tively. The small diﬀerence between the Google Base and
Google Base-domain has low statistical signiﬁcance (below
80%) hence not conclusive.

6.4 Trustworthiness Evaluation

In the next set of experiments, we evaluate the ability
of SourceRank to eliminate untrustworthy sources. For tu-
ples, corruption in the attribute values not speciﬁed in the
query manifests as untrustworthy results, whereas mismatch
in attributes values speciﬁed in the query manifests as the
irrelevant results. Since the title is the speciﬁed attribute for
our queries, we corrupted the attributes other than the title
values of the source crawls. Values are replaced by random
strings for corruption. SourceRank, Coverage and CORI
ranks are recomputed using these corrupted crawls, and re-
duction in ranks of the corrupted sources are calculated. The
experiment is repeated ﬁfty times for each corruption level,
reselecting sources to corrupt randomly for each repetition.
The percentage of reduction for a method is computed as
the mean reduction in these runs. Since CORI ranking is
query speciﬁc, the decrease in CORI rank is calculated as
the average decrease in rank over ten test queries.

The results of the experiments for movies and books do-
main are shown in Figure 4. The Coverage and CORI are
oblivious of the corruption, and do not lower rank of the cor-
rupted sources. This is not surprising, since any query based
relevance measure would not be able to capture the corrup-
tion in the attributes not speciﬁed in the query. On the
other hand, the SourceRank of the corrupted sources reduces
almost linearly with the corruption level. This corruption-

sensitivity of SourceRank would be helpful in solving the
trust problems we discussed in the introduction (e.g. the
solution manual with the same title and low non-existent
prices etc).
6.5 Collusion Evaluation

We also performed a stand-alone ground truth evalua-
tion of collusion and adjusted agreement. Since the ground
truth—degree of collusion—of the online databases is un-
known, these evaluations are performed using controlled rank-
ing functions on a data set of a million records from IMDB [2].
The records are replicated to create two databases of one
million each. For a query, the set of tuples are fetched based
on the key word match and ranked. To implement rank-
ing, a random score is assigned to each tuple and tuples
are sorted on this score. If these scores for a given tuple in
two databases are independent random numbers, the rank-
ings are completely independent. If the score for a tuple is
the same for both the databases, rankings are completely
correlated. To achieve mid levels of correlations, weighted
combinations of two independent random numbers are used.
Figure 5 shows the variation of collusion, agreement, and
adjusted agreement with the correlation of the two databases.
The correlation is progressively reduced from left to right.
At the left, they are complete mirrors with the same ranking
and data, and as we go right, the rank correlation decreases.
As we observe in the graph, when the databases have the
same rankings, the collusion and agreements are the same,
making the adjusted agreement zero. This clearly cancels
out agreement between mirrors and near mirrors. Even for
a small reduction in the rank correlation, the collusion falls
rapidly, whereas agreement reduces more gradually. Con-
sequently the adjusted agreement increases rapidly. This
rapid increase avoids canceling agreement between the gen-
uine sources.
In particular, the low sensitivity of the ad-
justed agreement in the correlation range 0.9 to 0 shows its
immunity to the genuine correlations of databases. At low
correlations, the adjusted agreement is almost the same as
the original agreement as desired. These experiments sat-
isfy the two desiderata of collusion detection we discussed
in Section 5. The method penalizes mirrors and near mir-
rors, whereas genuine agreement between the sources is kept
intact.
6.6 Timing Evaluation

We already know that random walk computation is fea-

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India235sible at web scale [11]. Hence for the timing experiments,
we focus on the agreement graph computation time. The
agreement computation is O(n2k2) where n is the number
of sources and top-k result set from each source is used for
calculating the agreement graph (k is a constant factor in
practice). We performed all experiments on a 3.16 GHz, 3.25
GB RAM Intel Desktop PC with Windows XP Operating
System.

Figure 6 shows the variation of agreement graph com-
putation time of the 600 of the book sources from Google
Base we used. As expected from time complexity formulae
above, the time increases in second order polynomial time.
Since the time complexity is quadratic, large scale compu-
tation of SourceRank should be feasible. Also note that the
agreement graph computation is easy to parallelize. The dif-
ferent processing nodes can be assigned to compute a sub-
set of agreement values between the sources. These agree-
ment values can be computed in isolation—without inter-
process communication to pass intermediate results between
the nodes.

7. CONCLUSION

A compelling holy grail for the information retrieval re-
search is to integrate and search the structured deep web
sources. An immediate problem posed by this quest is source
selection, i.e. selecting relevant and trustworthy sources to
answer a query. Past approaches to this problem depended
on purely query based measures to assess the relevance of
a source. The relevance assessment based solely on query
similarity is easily tampered by the content owner, as the
measure is insensitive to the popularity and trustworthiness
of the results. The sheer number and uncontrolled nature of
the sources in the deep web leads to signiﬁcant variability
among the sources, and necessitates a more robust measure
of relevance sensitive to source popularity and trustworthi-
ness. To this end, we proposed SourceRank, a global mea-
sure derived solely from the degree of agreement between the
results returned by individual sources. SourceRank plays a
role akin to PageRank but for data sources. Unlike PageR-
ank however, it is derived from implicit endorsement (mea-
sured in terms of agreement) rather than from explicit hy-
perlinks. For added robustness of the ranking, we assess
and compensate for the source collusion while computing the
agreements. Our comprehensive empirical evaluation shows
that SourceRank improves relevance sources selected com-
pared to existing methods and eﬀectively removes corrupted
sources. We also demonstrated that combining SourceRank
with Google Product search ranking signiﬁcantly improves
the quality of the results.

8. REFERENCES
[1] Goolge products. http://www.google.com/products.
[2] IMDB movie database. http://www.imdb.com.
[3] New york times best sellers.

http://www.hawes.com/number1s.htm.

[4] New york times guide to best 1000 movies.

http://www.nytimes.com/ref/movies/1000best.html.

[5] UIUC TEL-8 repository. http://metaquerier.cs.uiuc.

edu/repository/datasets/tel-8/index.html.

[6] S. Agrawal, K. Chakrabarti, S. Chaudhuri, V. Ganti,

A. Konig, and D. Xin. Exploiting web search engines to
search structured databases. In Proceedings of WWW,
pages 501–510. ACM, 2009.

[7] A. Arasu and H. Garcia-Molina. Extracting structured data

from Web pages. In Proceedings of SIGMOD.

[8] R. Balakrishnan and S. Kambhampati. Factal: Integrating

Deep Web Based on Trust and Relevance. In Proceedings of
WWW. ACM, 2011.

[9] M. Bender, S. Michel, P. Triantaﬁllou, G. Weikum, and
C. Zimmer. Improving collection selection with overlap
awareness in P2P search engines. SIGIR, pages 67–74, 2005.

[10] G. Bhalotia, A. Hulgeri, C. Nakhe, S. Chakrabarti, and

S. Sudarshan. Keyword searching and browsing in
databases using BANKS. In ICDE, page 0431, 2002.

[11] S. Brin and L. Page. The anatomy of a large-scale

hypertextual Web search engine. Computer Networks and
ISDN Systems, 30(1-7):107–117, 1998.

[12] J. Callan and M. Connell. Query-based sampling of text

databases. ACM TOIS, 19(2):97–130, 2001.

[13] J. Callan, Z. Lu, and W. Croft. Searching distributed

collections with inference networks. In Proceedings of ACM
SIGIR, pages 21–28. ACM, NY, USA, 1995.

[14] W. Cohen. Integration of heterogeneous databases without
common domains using queries based on textual similarity.
ACM SIGMOD Record, 27(2):201–212, 1998.

[15] W. Cohen, P. Ravikumar, and S. Fienberg. A comparison

of string distance metrics for name-matching tasks. In
IIWeb Workshop, 2003.

[16] W. Croft. Combining approaches to information retrieval.

Advances in information retrieval, 7:1–36, 2000.

[17] X. Dong, L. Berti-Equille, Y. Hu, and D. Srivastava. Global
detection of complex copying relationships between sources.
Proceedings of the VLDB Endowment, 3(1), 2010.

[18] X. Dong, L. Berti-Equille, and D. Srivastava. Integrating

conﬂicting data: the role of source dependence. In PVLDB,
2009.

[19] D. Gleich, P. Constantine, A. Flaxman, and

A. Gunawardana. Tracking the random surfer: empirically
measured teleportation parameters in PageRank. In
Proceedings of WWW, 2010.

[20] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. Combating

web spam with trustrank. In Proceedings of VLDB, 2004.

[21] P. Ipeirotis and L. Gravano. When one sample is not

enough: improving text database selection using shrinkage.
SIGMOD, pages 767–778, 2004.

[22] J. KLEINBERG. Authoritative Sources in a Hyperlinked
Environment. Journal of the ACM, 46(5):604–632, 1999.

[23] N. Koudas, S. Sarawagi, and D. Srivastava. Record linkage:

similarity measures and algorithms. In Proceedings of
SIGMOD, page 803. ACM, 2006.

[24] J. Lee. Analyses of multiple evidence combination. In ACM

SIGIR Forum, volume 31, page 276. ACM, 1997.

[25] J. Madhavan, A. Halevy, S. Cohen, X. Dong, S. Jeﬀery,

D. Ko, and C. Yu. Structured Data Meets the Web: A Few
Observations. Data Engineering, 31(4), 2006.

[26] Z. Nie and S. Kambhampati. A Frequency-based Approach

for Mining Coverage Statistics in Data Integration.
Proceedings of ICDE, page 387, 2004.

[27] M. Shokouhi and J. Zobel. Federated text retrieval from

uncooperative overlapped collections. In Proceedings of the
ACM SIGIR. ACM, 2007.

[28] L. Si and J. Callan. Relevant document distribution

estimation method for resource selection. In Proceedings of
ACM SIGIR, pages 298–305, 2003.

[29] J. Wang, J. Wen, F. Lochovsky, and W. Ma. Instance-based

schema matching for web databases by domain-speciﬁc
query probing. In In Proceedings of the VLDB, pages
408–419. VLDB Endowment, 2004.

[30] A. Wright. Searching the deep web. Commmunications of

ACM, 2008.

[31] X. Yin, J. Han, and P. Yu. Truth discovery with multiple

conﬂicting information providers on the web. TKDE, 2008.

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India236