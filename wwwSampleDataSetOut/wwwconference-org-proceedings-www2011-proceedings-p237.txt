Search Result Diversity for Informational Queries

Michael J. Welch∗
Sunnyvale, CA 94089

Yahoo! Inc.

mjwelch@yahoo-inc.com

Junghoo Cho

UCLA Computer Science Dept

Los Angeles, CA 90095
cho@cs.ucla.edu

Christopher Olston

Yahoo! Research

Santa Clara, CA 95054

olston@yahoo-inc.com

ABSTRACT
Ambiguous queries constitute a signiﬁcant fraction of search
instances and pose real challenges to web search engines.
With current approaches the top results for these queries
tend to be homogeneous, making it diﬃcult for users in-
terested in less popular aspects to ﬁnd relevant documents.
While existing research in search diversiﬁcation oﬀers several
solutions for introducing variety into the results, the major-
ity of such work is predicated, implicitly or otherwise, on
the assumption that a single relevant document will fulﬁll a
user’s information need, making them inadequate for many
informational queries.
In this paper we present a search-
diversiﬁcation algorithm particularly suitable for informa-
tional queries by explicitly modeling that the user may need
more than one page to satisfy their need. This modeling
enables our algorithm to make a well-informed tradeoﬀ be-
tween a user’s desire for multiple relevant documents, prob-
abilistic information about an average user’s interest in the
subtopics of a multifaceted query, and uncertainty in classi-
fying documents into those subtopics. We evaluate the eﬀec-
tiveness of our algorithm against commercial search engine
results and other modern ranking strategies, demonstrating
notable improvement in multiple document scenarios.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Retrieval models

General Terms
Algorithms, Experimentation, Performance

Keywords
search diversity, expected hits, informational queries

1.

INTRODUCTION

Web search engines typically display a linear list of re-
sults for a user query, ranked by numerous factors such as
relevance to the search terms and overall popularity. Search
queries, however, are often underspeciﬁed, ambiguous, or
∗

Work completed while author was a graduate student in the UCLA

Computer Science Department.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

multifaceted. The query “virus” could refer to, for example,
a computer virus or a biological virus, and it is nearly im-
possible to know which meaning the user intended. With an
ambiguous query, a few interpretations often dominate the
top results, leaving less popular aspects uncovered. Users in-
terested in less prevalent meanings encounter diﬃculty ﬁnd-
ing relevant documents.

Ambiguous queries constitute a signiﬁcant fraction of query
instances [19, 20], and we must ﬁnd suitable ways to cope
with them. Studies on search diversiﬁcation aim to address
this problem by introducing a diverse set of pages into search
results. Common to a majority of prior research, however,
is the “single relevant document assumption.” In fact some
proposed approaches are provably optimal for various re-
trieval metrics under the assumption a user requires only
one relevant document from their intended subtopic. We ar-
gue, however, that this assumption is an over simpliﬁcation.
Many users will not be satisﬁed with only one relevant doc-
ument, particularly for informational queries, and a search
diversiﬁcation strategy must properly account for them.

In this paper we concentrate on the problem of diversifying
search results for informational queries. Improving the re-
sults for informational queries will signiﬁcantly improve the
search experience for many users because they tend to spend
a disproportionate amount of time on informational queries.
That is, navigational queries result in short interactions be-
cause the user already has a particular website in mind and
simply uses the search engine as a pseudo-bookmark to lo-
cate the URL. For informational queries, however, the exact
documents of interest are not known in advance. Users typi-
cally inspect the results for an informational query more in-
depth, carefully exploring many pages in the result set [11].
Optimizing these queries will reduce the burden placed on
the user by helping them ﬁnd a suﬃcient number of relevant
documents more quickly.

The distinct search behavior for informational queries dic-
tates the following modeling requirements:
(1) Users of-
ten need more than one document to satisfy their informa-
tion need, so the diversiﬁcation model should properly ac-
count for users who need multiple relevant documents. (2)
Ambiguous queries often have several potential subtopics.
While a user tends to have one particular subtopic in mind,
that subtopic is not known by the search engine. (3) The
content of each document also tends to focus on only one of
the possible subtopics, but the search engine lacks explicit
topic classiﬁcation for the majority of documents.

In this paper we present a model that accounts for the
above requirements for informational queries and deﬁne a

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India237measure of user satisfaction with respect to that model. We
then present an algorithm which introduces diversity into
search results for informational queries such that we max-
imize the number of users who are able to ﬁnd a suﬃcient
number of documents related to their intended subtopic. Ex-
periments show that our algorithm increases the expected
performance of the top 10 results by 130% compared to a
commercial search engine and 51% over a state-of-the-art
diversiﬁcation algorithm [1] in certain cases, while still per-
forming well on traditional metrics designed under single
relevant document assumptions.

The remainder of this paper is organized as follows.

In
Section 2 we discuss related work in search diversiﬁcation.
We present an overview of our model and assumptions and
deﬁne our goal metric in Section 3.
In Section 4 we an-
alyze two simpliﬁcations of those assumptions and explore
the complete formulation of our algorithm. In Section 5 we
describe a related algorithm in more detail and highlight
the diﬀerences between our approaches. Section 6 describes
potential methods for approximating the probability distri-
butions necessary for our algorithm. We present our experi-
mental results in Section 7 and discuss conclusions and areas
for further research in Section 8.

2. RELATED WORK

Search diversiﬁcation has been studied in several contexts
with many diﬀerent approaches. Early techniques focused
on the content of documents already selected, traditionally
weighing between measures of query relevance and relative
novelty of new documents. These methods tend to produce
diversity as a side eﬀect of novelty and make no use of ex-
plicit knowledge of potential subtopics or user intent. Car-
bonell and Goldstein’s work on Maximal Marginal Relevance
(MMR) [4] is a classic example of such a strategy, which can
be employed to re-rank documents and promote diversity.

Zhai, Cohen, and Laﬀerty [24] propose a framework which
models dependent relevance and describe a generic greedy
approach to ranking documents for subtopic retrieval. Their
ranking strategy is based on a tradeoﬀ between selecting
documents of high value and minimizing cost, where doc-
uments which include relevant, previously uncovered infor-
mation have higher value, and those that are irrelevant or
repeat already seen information have a larger cost. With
the goal of optimizing a ranking for their Subtopic recall
(S-recall) and Subtopic precision (S-precision) metrics, they
implicitly assume that a single document relevant to a cat-
egory is suﬃcient for a user.

Chen and Karger [5] use Bayesian retrieval models and
condition selection of subsequent documents by making as-
sumptions about the relevance of the previously retrieved
documents. While their approach is capable of selecting
anywhere between 0 < k ≤ n relevant documents, they fo-
cus primarily on optimizing single document (k = 1) and
perfect precision (k = n) scenarios. Their model does not
explicitly consider user intent or document categorizations,
and it is unclear how their technique can best be applied to
interleave documents from multiple subtopics into a single
ranking when single document assumptions are removed.

Wang and Zhu introduce an approach to diversiﬁcation
based on economic portfolio theory [23]. Their model con-
siders a “risk” tradeoﬀ between the expected relevance of a
set of documents and correlation between them, modeled as
the mean and variance. They demonstrate the algorithm is

capable of a wide range of “risk preferences”, though it is
unclear how to choose the proper parameters to maximize
their algorithm’s performance under our proposed model.

Pretschner and Gauch [15] present early work in modeling
user proﬁles as weighted nodes in an explicit taxonomy, and
explore methods for employing those taxonomies in search
personalization for ambiguous queries. Their work shows
modest gains in relevance are possible with re-ranking and
ﬁltering based on those proﬁles. Liu et al. [12] study the use
of general and per-user proﬁles constructed from category
hierarchies for disambiguation of user queries.

Agrawal et al.

introduce a model similar to ours in [1],
where their objective is to maximize the probability an av-
erage user ﬁnds at least one useful result. Under assump-
tions of probabilistic query intent and document categoriza-
tion, they present a proof showing the selection of docu-
ments which optimize against that criteria is NP-hard, and
oﬀer an approximation algorithm with a bounded error from
the optimal solution under certain assumptions. They also
show their algorithm is optimal when all documents belong
to a single category. Their algorithm does, however, con-
tain potential weaknesses, which we explore in more depth
in Section 5.

Researchers have also considered meaningful ways to eval-
uate the performance of search diversiﬁcation and subtopic
retrieval algorithms. Classic ranked retrieval metrics such
as NDCG, MRR, and MAP have been augmented [6, 1] to
take user intent into account. Metrics such as search length
(SL) [7] and k-call [5], and their aggregated forms, are well
suited to evaluate diversiﬁcation of search systems under sin-
gle document assumptions. The %no metric [22] measures
the ability of a system to retrieve at least one relevant result
in the top ten. Other metrics, such as Subtopic recall and
Subtopic precision [24], explicitly measure the subtopic cov-
erage of a result set or the eﬃciency at which an algorithm
represents the relevant subtopics.

We use several of these existing metrics to evaluate the
performance of our algorithm under single document sce-
narios. We also deﬁne the expected hits metric to evaluate
diversiﬁcation algorithms under the more general assump-
tion that a user may require multiple documents. We will
detail our metric in the following sections.

3. DIVERSIFICATION MODEL OVERVIEW
Given an ambiguous query, our goal is to select the set of
documents which will satisfy the majority of users. Commer-
cial search engines frequently return homogeneous document
sets for such queries, which is sub-optimal in most cases. We
therefore study ambiguous queries as a search diversiﬁcation
problem, with the goal of introducing diversity by identify-
ing the relevant subtopics for an ambiguous query and using
the probability of user interest in each of those subtopics to
produce a document ranking which increases the likelihood
an average user ﬁnds suﬃcient relevant documents.

We concentrate on informational queries, where users of-
ten require more than one relevant document. Our model
takes probabilistic information about query intent, relevance
of documents to the possible query subtopics, as well as the
number of pertinent documents a user requires into consid-
eration, and assumes these factors to be independent. By
considering query intent likelihood, we are able to identify
which subtopics are most important to the users. Document
categorization probabilities help estimate how likely a doc-

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India238ument is to satisfy a particular subtopic. Estimating how
many relevant documents a user will require enables us to
weigh the expected beneﬁts of providing additional docu-
ments from already represented subtopics against exploring
less covered subtopics. The assumption that users often re-
quire multiple documents relevant to their intended subtopic
breaks with traditional work in search diversiﬁcation.

Each of the necessary distributions are discussed further
in the subsequent sections, followed by the deﬁnition of our
goal metric.
3.1 Relevant Document Requirements

For informational queries it is important to consider how
many relevant documents a user will visit. For example, if
most users want to see 10 relevant documents, diversifying
the results in the top 10 may actually lower the satisfaction
for many users. The number of documents j a user requires
to satisfy her need, however, is often relatively small. Show-
ing a user more than j relevant documents is generally un-
necessary. We model j as a distribution over the number
of relevant documents a user requires: user U is expected
to require j documents related to their subtopic of interest
with probability Pr(J = j|U ), for j > 0.
3.2 User Intent

User intent represents the likelihood an average user is in-
terested in a particular facet of an ambiguous query. The
user intent probability distribution is important for deter-
mining the relative importance of each subtopic.
In our
model, a user issues a search query for an ambiguous topic
T which has m subtopics T1, T2,··· , Tm. For a given user
U who queries for topic T , we consider a distribution over
subtopics of interest to U : U is interested in subtopic Ti
with probability Pr(Ti|U ).
3.3 Document Categorization

Web search engines perform quite well at retrieving doc-
uments relevant to query terms. To select a diverse set of
documents for an ambiguous query, however, ﬁrst requires
determining which subtopic(s) each document belongs to.
Automatic classiﬁcation is a diﬃcult problem, and manual
classiﬁcation of documents is infeasible on a web scale. Ac-
curate document categorization is also important, as it tells
us, probabilistically, which subtopic(s) a particular docu-
ment satisﬁes. We model document categorization as a prob-
ability distribution. For a document d which is relevant to
topic T , we assume a distribution over the subtopics: d is
relevant to Ti with probability Pr(Ti|d).
3.4 Objectives
Given the probability distributions Pr(J|U ), Pr(Ti|U ), and
Pr(Ti|D), the absence of any additional contextualizing in-
formation, and a choice of any n documents to display, our
task is to select documents such that we maximize the like-
lihood of user satisfaction.

To be clear about our objective, we must deﬁne “user sat-
isfaction”. The simplest satisfaction measurement could be
binary: a user either does or does not ﬁnd as many docu-
ments as they desired from their intended subtopic. While
it is possible to deﬁne a goal function and optimize for such
criteria, this model does not seem to adequately reﬂect the
real world.
If a user wants ﬁve relevant documents, but
only ﬁnds four to click on, they are likely still partially sat-

isﬁed. We therefore deﬁne our objective in terms of hits,
where a hit constitutes a click on a document which satisﬁes
the subtopic the user is interested in. We then achieve our
goal of optimal user satisfaction by maximizing the expected
number of hits for the average user.

Consider a simpliﬁed example where a user issues the
query virus. Assume they are interested in biological viruses,
and 3 of the returned documents R are about biological
viruses. Using the required-documents distribution Pr(J|U )
we can calculate how many documents the user is expected
to click on. If the user is interested in one, two, or three rel-
evant documents, they are expected to click on as many. If
they are interested in more than three documents, they can
only click on the three that are displayed. We thus compute
the expected number of hits as:

E(R) = 1·Pr(J = 1|U )+2·Pr(J = 2|U )+3·

|R|(cid:2)

j=3

Pr(J = j|U )

The above example shows how, given Pr(J|U ), we can
compute the expected number of hits for a set of documents
when user intent and document categorizations are known.
In reality these are not known values, but rather probability
distributions.
In the next section we will show how these
distributions factor in to the model and present our algo-
rithm for selecting a set of results R such that we maximize
the expected number of hits.

4. DIVERSIFICATION MODEL

Our general approach is to successively select documents,
at each step choosing the document which adds the maxi-
mum additional expected hits. If our goal were to return at
least one relevant result, this document would most likely
come from a subtopic not yet covered. In our model, how-
ever, this is not always the case, as we may beneﬁt more
users by returning additional documents from a popular
subtopic.

To determine how to best select documents, we must ex-
amine the eﬀects of the probability distributions discussed
in Section 3 on the expected number of hits. We begin by
analyzing two simpliﬁed cases of those distributions. First,
we will assume perfect knowledge of user intent. Second, we
will assume perfect document classiﬁcation.
4.1 Perfect Knowledge of User Intent

The ﬁrst case we examine is when we know exactly which
subtopic Ti a user is interested in but document classiﬁcation
is probabilistic. To calculate the expected number of hits for
a set of documents when we know the user intent, we must
consider how many documents j the user requires, and how
many of the documents presented are relevant, denoted as
k. A user will click on at most j documents, so returning
more than j is unnecessary. Likewise, a user will see at most
k relevant documents, and thus can click on no more than
k.

We compute the expected number of hits for a set of n

documents R as:

E(R) =

j=1

n(cid:2)

Pr(J = j|U )

n(cid:2)

k=1

Pr(Ki = k|R) min(j, k)

(1)

In Equation 1, Ki is deﬁned as the event that k documents
in R belong to Ti. To compute this probability, we begin by

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India239deﬁning the probability that no documents from R satisfy
Ti as:

Pr(Ki = 0|R) =

(1 − Pr(Ti|dr))

n(cid:3)

r=1

In the general case where a user requires k relevant docu-
ments, we can expand this equation to:

Pr(Ki = k|R) = Pr(Ti|d1) Pr(Ki = k − 1|R \ {d1})
+ (1 − Pr(Ti|d1)) Pr(Ki = k|R \ {d1})

From Equation 1, Pr(J|U ) is independent of which subtopic
the user is interested in, and thus only Pr(Ki|R) will be af-
fected by the choice of documents. Since Pr(Ki = k|R)
is the only term in the equation dependent on the selected
documents, and the user is only interested in subtopic Ti,
we can maximize the the expected number of hits E(R) by
selecting the documents with the highest Pr(Ti|D) values,
that is, by maximizing Pr(Ki = n|R). Under these con-
ditions, our strategy for selecting documents is similar to
the greedy approach for optimizing k-call presented by Chen
and Karger [5], using Pr(Ti|D) to select the documents most
likely related to Ti.
4.2 Perfect Document Classiﬁcation

We next make the assumption that each document is clas-
siﬁed into a single subtopic category, but user intent is un-
known. In terms of the probability distributions described
in Section 3, perfect classiﬁcation means D is divided into
non-overlapping subsets D1, D2,···D m such that for each
subtopic Ti, Pr(Ti|d ∈ Di) = 1 and ∀j(cid:3)=i Pr(Tj|d ∈ Di) = 0.
In this case, we study how to combine user intent and
relevant document requirement distributions to best allocate
documents from subtopics and maximize user satisfaction.
We start by again deﬁning the number of documents selected
from subtopic Ti as Ki and enforce the condition that for
i=1 Ki = n. As in the previous
the m subtopics of T,
case, a user will click on up to j documents from subtopic
Ti, and can click on at most Ki documents if Ki < j.

(cid:4)m

We calculate the expected number of hits for an average

user with the following equation:

n(cid:2)

m(cid:2)

E(R) =

j=1

i=1

Pr(Ti|U ) Pr(J = j|U ) min(j, Ki)

(2)

4.2.1 Solving For K
When we know exactly which subtopic each document
belongs to, our main task becomes deciding how many doc-
uments from each subtopic should be included in the results.
That is, we need to pick the set of {Ki} values which will
maximize the expected number of hits.
Given all the possible values for {Ki}, we can calculate the
expected hits of each and choose an optimal solution. With
n documents to choose from m subtopics, the number of
combinations of {Ki} values which satisfy the requirement
(cid:6)
(cid:4)m
, making it infeasible to consider
all possible combinations for a query. We can greatly reduce
the search space, however, as many combinations are clearly
not optimal. Allocating all documents to the least probable
subtopic, for example, will not result in the maximum num-
ber of hits. Intuitively, an optimal solution should contain at
least as many documents from the most probable subtopic
as a less popular one. We formalize this notion with the
following Proposition:

i=1 Ki = n is

(cid:5)
n+m−1

n

Proposition 4.1. Without loss of generality,

label the
subtopics of topic T as T1, T2,··· , Tm such that Pr(T1|U ) ≥
Pr(T2|U ) ≥ ··· ≥ Pr(Tm|U ). Then an optimal solution to
Equation 2 satisﬁes the following properties:

j=1

Pr(J = j|U ) = 1

• n(cid:2)
• m(cid:2)
• K1 ≥ K2 ≥ ·· · ≥K m
Proof. Assume an initial set of K values {K1, K2,··· , Km}

Ki = n

i=1

(cid:4)m

i=1 Ki = n and Kx < Ky for some x < y, with
such that
expected number of hits E(K) as deﬁned in Equation 2.
Then we can construct a set ˆK = {K1,··· , Kx +1, ··· , Ky−
1, ···K m} with expected hits:
E( ˆK) = E(K)

+ (Pr(Tx|U )

Pr(J = j|U ))

n(cid:2)

− (Pr(Ty|U )

j=Kx+1
n(cid:2)

Pr(J = j|U ))

j=Ky

≥ E(K) + (Pr(Tx|U ) − Pr(Ty|U ))
≥ E(K)

n(cid:2)

j=Ky

Pr(J = j|U )

4.2.2 Document Selection
In practice it is not necessary to enumerate and test all
possible {Ki} values, as we can optimize for Equation 2 di-
rectly. We select the documents to return using an algorithm
which factors in both Pr(Ti|U ) and Pr(J|U ) while adhering
to Proposition 4.1, and update Ki after each selection ac-
cordingly.

Algorithm KnownClassiﬁcation
(∗ Rank documents to maximize Equation 2 ∗)
1. R ←∅
2. D ←All relevant documents
3. K1 = K2... = Km = 0
4. while |R| < n
5.
6.
7.

i ←ARGMAX(Pr(Ti|U ) Pr(J > Ki|U ))
Ki ←Ki + 1
R ←R ∪ NextDocument(Di , R)

To choose each successive document, KnownClassiﬁcation
takes a greedy approach. The algorithm ﬁrst determines
which subtopic will provide the maximum marginal beneﬁt
to the average user. The marginal utility of a subtopic is
the expected increase in hits produced by adding another
document from it, and is the product of the user interest
in the subtopic Pr(Ti|U ) and the probability that users will
want another document from that subtopic Pr(J > Ki|U ).
Once the next subtopic is chosen, a search engine may select
the next document to return from Di \ R using its standard
ranking functions.
4.3 Complete Model

We now eliminate the simplifying assumptions and discuss
how to compute the expected hits when neither document
classiﬁcations nor user intents are perfectly known. With

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India240user intent uncertain, we need to calculate the expected hits
probabilistically over all of the possible subtopics instead of
only a single, known Ti from Equation 1. From Equation 2
we can no longer say the user will click on min(j, Ki) doc-
uments, as we have no guarantees on the number of docu-
ments which actually satisfy subtopic Ti. Instead, we expect
the user to click on min(j, k) documents, based on the prob-
ability that k relevant documents are available to the user.
Combining the two simpliﬁed equations and making use of
all three probability distributions, the equation for expected
number of hits becomes:

n(cid:2)

m(cid:2)

E(R) =

Pr(Ti|U ) Pr(J = j|U )

n(cid:2)

j=1

i=1

k=1

Pr(Ki = k|R) min(j, k)

(3)

which maximizes this probability is proven to be NP-Hard,
and the authors propose the IA-Select algorithm as an ap-
proximation, which is shown to produce an optimal solu-
tion to Diversify when every document belongs to a single
subtopic.
Key to their algorithm is the notion of a conditional proba-
bility of subtopics, U (Ti|R), which measures the probability
that the user is still interested in subtopic Ti given the doc-
uments already chosen in R. The conditional probability
of each subtopic is initialized to the user intent probabil-
ity Pr(Ti|U ). The algorithm successively selects documents
which have the highest marginal utility, computed for each
document as the sum, over each subtopic, of the subtopic’s
conditional probability and the document’s score for that
subtopic:

m(cid:2)

g(d|R) =

Pr(Ti|d)U (Ti|R)

Algorithm Diversity-IQ
(∗ Rank documents to maximize Equation 3 ∗)
1. R ←∅
2. D ←All relevant documents
3. while |R| < n
4.
5.
6.

d ←ARGMAX(ΔE(d|R, D))
R ←R ∪ {d}
D ←D \ {d}

Diversity-IQ outlines how to select the set of documents
R such that we maximize the expected number of hits for an
ambiguous informational query. We adopt the approach of
KnownClassiﬁcation, selecting each successive document by
determining which will maximize the increase in expected
hits given the documents already returned.

The ΔE computation for a document is dependent on sev-
eral factors, including its subtopic scores, the user interest
in those subtopics, and the conditional probabilities of how
many documents from each subtopic are already included in
R. Using a dynamic programming algorithm, we can up-
date the Pr(Ki|R) values once per iteration. Thus we have
an overall computational complexity of O(|R| · |D| · m) for
choosing each successive document, or O(n2 · |D| · m) for
re-ranking the top n documents.
5. COMPARISON WITH IA-SELECT

In this section we brieﬂy go over the work on search di-
versiﬁcation by Agrawal et al. [1] to better understand when
prior work may perform sub-optimally, and how our ap-
proach may overcome such scenarios.
5.1 Overview of IA-Select

Agrawal et al. investigate the problem of ambiguous queries

with the overall objective of maximizing the probability that
an average user ﬁnds at least one relevant document in the
top n search results. Their model assumes an explicit tax-
onomy of subtopics is available, and both documents and
queries may fall into multiple subtopics. Queries belong to
a set of subtopics with a known probability distribution,
which eﬀectively represents the user intent for a given query
Pr(Ti|U ). Likewise, documents belong to a set of subtopics,
and the relevance to each subtopic is measured probabilisti-
cally, much like Pr(Ti|d).

Given this model and set of distributions, they formulate
the Diversify function, which measures the probability that
a set of n documents satisﬁes an “average” user for an am-
biguous query. The objective to select the set of documents

i=1

After a document d is selected, the conditional probability
of each subtopic is updated to reﬂect the inclusion of d in R
using Bayes’ theorem:

∀i : U (Ti|R) = (1 − Pr(Ti|d))U (Ti|R \ {d})
5.2 Observed Limitations of IA-Select

(4)

In our experiments with IA-Select, we observed the al-
gorithm often selects one document from each subtopic, in
the order of subtopic popularity, and then degenerates into
random document selection. We believe this behavior is sub-
optimal. Even if every subtopic is represented once in the
results, an average user is more likely to want to see ad-
ditional documents from more popular subtopics if there is
room.

From our investigation, we ﬁnd that this behavior is due
to the following limitation. When deciding which document
to select next, IA-Select uses the conditional probability
U (Ti|R), which measures the likelihood that the user is still
interested in subtopic Ti given the documents already se-
lected in set R. IA-Select assumes that the user is no longer
very interested in subtopic Ti once at least one document
believed to satisfy Ti is present in R, meaning U (Ti|R) be-
comes very small.

When IA-Select is used with any document classiﬁcation
function which assigns subtopic scores approaching 1.0, the
Bayesian update step in Equation 4 is problematic. To illus-
trate the issue more clearly, consider an extreme case where
a document is classiﬁed to “perfectly” belong to any subtopic
(Pr(Ti|d) = 1). In that case, the subtopic will have its condi-
tional probability set to zero. That is, if even one document
from each subtopic has such a score, every conditional util-
ity value will be set to zero, and the algorithm is reduced to
random selection. Note that this behavior is not limited to
the extreme case when Pr(Ti|d) = 1. As long as the Pr(Ti|d)
values are suﬃciently high, all conditional subtopic proba-
bilities will quickly become very small, and the algorithm
exhibits similar behavior.

Zero-utility is particularly problematic if we consider that
selecting multiple documents from a subtopic may be beneﬁ-
cial, which may be the case even in simple situations such as
a query having fewer subtopics than document “slots” to ﬁll
(m < n). We avoid the zero-utility problem by computing
the marginal beneﬁt of a subtopic in terms of the probabil-
ity that a user wants additional documents from it, which
depends on Pr(J|U ) and Pr(Ki|R). As long as Pr(J|U ) > 0,
each subtopic will always have non-zero utility.

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India241To illustrate the issue more clearly and accentuate how
our algorithm avoids the zero-utility problem, we will walk
through a simple example. In this example we use binary
classiﬁcation scores for clarity and to underscore the poten-
tial problems with IA-Select only. As we will see later in
our experimental section, IA-Select exhibits similar behav-
ior, to a lesser degree, even under widely used probabilistic
classiﬁers which may assign any value between 0 and 1.
5.3 Descriptive Example

Assume two subtopics T1 and T2, with two documents
classiﬁed into each subtopic. Our example will use the fol-
lowing probabilities and subtopic scores:
• Pr(T1|U ) = 0.7 and Pr(T2|U ) = 0.3.
• Pr(J|U ) = (0.6, 0.3, 0.1).
• D = {d1 = d2 = (1.0, 0.0), d3 = d4 = (0.0, 1.0)}
• n = 3.

5.3.1 Diversity-IQ
To choose the ﬁrst document, Diversity-IQ computes the

marginal utility of each document:
ΔE(d1|∅) = ΔE(d2|∅) = 0.7
ΔE(d3|∅) = ΔE(d4|∅) = 0.3

(cid:4)3
(cid:4)3

j=1 Pr(J = j|U ) = 0.7
j=1 Pr(J = j|U ) = 0.3

The ﬁrst document selected is chosen arbitrarily between
{d1, d2}. To choose the second document, we compute the
marginal utility of each remaining document:
ΔE(d2|{d1}) = 0.7
ΔE(d3|{d1}) = ΔE(d4|{d1}) = 0.3

j=2 Pr(J = j|U ) = 0.28

(cid:4)3

As d3 and d4 both provide the same increase in expected
hits, we again choose arbitrarily between them. Thus we
have R = {d1, d3} after the ﬁrst two iterations. To choose
the third document, we again compute the marginal utility
of the remaining documents:
ΔE(d2|{d1, d3}) = 0.7
ΔE(d4|{d1, d3}) = 0.3

j=2 Pr(J = j|U ) = 0.28
j=2 Pr(J = j|U ) = 0.12

(cid:4)3
(cid:4)3

Since d2 has a higher marginal utility than d4, it is added
to the result set, for a ﬁnal ranking R = {d1, d3, d2} with
expected hits E(R) = 1.28.

5.3.2 IA-Select
For IA-Select, we initialize the utility of each subtopic to
the user intent probabilities and compute the marginal util-
ity of each document:
(cid:4)2
g(d1|∅) = g(d2|∅) =
(cid:4)2
g(d3|∅) = g(d4|∅) =

i=1 Pr(Ti|d)U (Ti|R) = 0.7
i=1 Pr(Ti|d)U (Ti|R) = 0.3

After choosing arbitrarily between {d1, d2}, we update the

conditional probability of the subtopics:
U (T1|{d1}) = (1 − Pr(T1|d1))U (T1|∅) = 0.0
U (T2|{d1}) = (1 − Pr(T2|d1))U (T2|∅) = 0.3

We recompute the marginal utility of each document:

(cid:4)2

g(d2|{d1}) =
g(d3|{d1}) = g(d4|{d1}) =

i=1 Pr(Ti|d)U (Ti|R) = 0.0

(cid:4)2

i=1 Pr(Ti|d)U (Ti|R) = 0.3

Again, we choose arbitrarily between {d3, d4} and update

the conditional probability for each subtopic:
U (T1|{d1, d3}) = (1 − Pr(T1|d3))U (T1|{d1}) = 0.0
U (T2|{d1, d3}) = (1 − Pr(T2|d3))U (T2|{d1}) = 0.0

At this point, we still need to select a third document
(n = 3), but the conditional utility of each subtopic is zero,
meaning the marginal utility of every document will be zero.
Intuitively, we would expect the more probable subtopic to
be a better choice for the “average” user in this situation,
but IA-Select will randomly choose between {d2, d4}. Note
that there is a substantial diﬀerence in the expected hits
depending on which we choose: d2 will increase the expected
hits by 0.28, while d4 by only 0.12.
5.4 Discussion

One may contend that the outlined issues can easily be
patched by smoothing or enforcing a limit on the maximum
score assigned to any particular subtopic. In our evaluations
we will also show that, while placing such arbitrary limits
on subtopic scores can improve IA-Select’s performance on
the expected hits metric to a certain degree, it still exhibits
similar behavior, and the improvements come at the cost of
degraded performance on other metrics.

It is also worth noting that Equation 3 is, in fact, a gener-
alization of the Diversify goal function. That is, if we make
the assumption that all users require exactly one document
(setting Pr(J = 1|U ) = 1), Diversity-IQ will yield the same
ranking as IA-Select.

6. DISTRIBUTION MEASUREMENTS

Our algorithm requires three distributions which describe
(1) the number of relevant documents a user is expected to
require, (2) the probability of user intent in each subtopic,
and (3) the probability a document satisﬁes each subtopic.
Given the broad range of possible queries and the number
of documents on the web, automatic methods for approx-
imating these distributions are necessary for a real world
deployment. In this section we suggest possible techniques
to approximate them using data sources available to web
search engines. In Section 7 we specify which data sources
are used for each experiment.
6.1 Measuring Document Requirements

Knowing the number of relevant documents a user is ex-
pected to require is necessary to determine how much di-
versity we can introduce in the results without harming the
hit-rate for popular subtopics. One method to approximate
this distribution is using click-through data from query logs.
Figure 1 shows the number of click-throughs for each query
session with at least one click from a locally collected query
log [16]. We observe that other publicly available query
logs show a similar distribution.
In our log, users clicked
on an average of 1.52 results for queries with at least one
click-through. Other studies of web search logs report an
average of 3.18 clicks-per-query [13] when empty sessions
are removed.
6.2 Measuring User Intent

The user intent distribution measures the probability that
an average user is interested in a particular subtopic for a
given query. We have shown in Section 4.2.1 that if subtopic

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India242the top 200 results from each subtopic query to form a single
document set.

(cid:4)m

Given a set of m subtopics T and collection of documents
D, a document classiﬁcation function C(d, T ) assigns a nor-
malized probability score Pr(Ti|d) for each Ti ∈ T , such
i=1 Pr(Ti|d) = 1. We consider two such classiﬁcation
that
functions in our evaluations: (1) query-based classiﬁcation,
and (2) Latent Dirichlet Allocation (LDA) [2].

Query-based classiﬁcation uses knowledge of which subtopic

(cid:2)m

queries returned each document. For each of the subtopic
queries that returned a document d in its top 200 results,
we compute a score Si(d) = cos(dc, W (Ti)), where dc and
W (Ti) are vector space model representations of the text
from the search result snippet and the Wikipedia page for
subtopic Ti, respectively, and cos is the cosine similarity
function. We normalize these scores to assign Pr(Ti|d) =
Si(d)
j=1 Sj (d) .
The second classiﬁcation method we consider is Latent
Dirichlet Allocation (LDA). LDA requires a set of docu-
ments, a number of topics m, and two hyperparameters α
and β which control smoothing of Dirichlet priors for top-
In typical applications, 0 < α <1 and
ics and words.
β is set to 0.1 or 0.01 [21]. We construct an LDA topic
model for each document set and assign Pr(Ti|d) from the
resulting θ distribution. The LDA topics are aligned with
the known subtopics in a greedy fashion using knowledge of
which subtopic queries retrieved the document.

7. EVALUATION

We conducted several experiments to assess the overall ef-
fectiveness of Diversity-IQ. The evaluations include an anal-
ysis of our objective of maximizing the expected number of
hits, as well as comparisons using established subtopic re-
trieval metrics. For each metric, we compare our Diversity-
IQ algorithm against the state-of-the-art IA-Select algo-
rithm presented by Agrawal et al. [1] as well as the original
ranking returned by a commercial web search engine (SE).
7.1 Query Set

One of the diﬃculties in evaluating a system designed to
introduce diversity is the lack of standard testing data. Eval-
uating diversiﬁcation requires a set of ambiguous queries,
and until recently, no benchmark query sets or relevance
judgements exist explicitly for the task of diversiﬁcation re-
search. TREC5 added a diversity task to the Web track be-
ginning in 2009. The data includes 50 queries, each with a
set of selected subtopic aspects. Unfortunately, this dataset
and evaluation criteria were designed assuming a single rel-
evant document is suﬃcient, and so it is diﬃcult to adapt
them to our setting.

Although techniques exist to identify ambiguous queries [20]

which would be beneﬁcial in a real world deployment, we’re
primarily concerned with evaluating the performance bene-
ﬁts of our algorithm, and thus we opted for a simpler ap-
proach to form a testing set. We generated a set of am-
biguous queries using a small search log with a few hundred
thousand entries collected from our local network. A query
is marked as ambiguous if a Wikipedia disambiguation page
exists for the terms. We randomly selected 50 queries from
this candidate set for our evaluations.

5http://trec.nist.gov/

Figure 1: Clicks per query

Ti is more likely than subtopic Tj , then showing the user at
least as many documents from Ti as Tj is a necessary condi-
tion for optimizing the expected number of hits. Therefore
an accurate estimate of user interest in each subtopic is im-
portant. Possible sources for this information include:
• The frequency of popular query reﬁnements for am-
• The click-through history for documents returned by
• Frequency of subtopic queries, which can be measured
using information about search volume and trends.12

an ambiguous query.

biguous queries [17].

6.3 Measuring Document Categorization

The document categorization distribution tells us which
of the m subtopics a particular document belongs to. Unsu-
pervised document classiﬁcation techniques often require an
estimate for m and a suﬃciently large collection of relevant
documents. We look at these issues next.

6.3.1 Subtopic Estimation
We investigated two sources for discovering the subtopics
for a given ambiguous query:
(1) WordNet [8], and (2)
Wikipedia.3 WordNet is a popular lexical database which
includes term relationships. Unfortunately, data for queries
such as movies, song titles, and proper nouns are sparse in
WordNet. The second source we examined is Wikipedia.
Among the millions of articles on Wikipedia are over 60,000
disambiguation pages for the English language. These pages
list several possible meanings of term, covering a wider range
of entity types.

Other approaches for identifying subtopics include mining
popular query reﬁnements [17] and using categories from the
Open Directory Project.4

6.3.2 Document Classiﬁcation
Radlinski and Dumais show that homogeneity in the top

results generally hinders the eﬀectiveness of personalization [17].
Without a suﬃcient number of documents from each subtopic,
unsupervised classiﬁcation techniques will be unable to gen-
erate meaningful topics.
In our experiments we therefore
opt for a metasearch strategy. We form a collection of doc-
uments for an ambiguous query by issuing each relevant
Wikipedia subtopic page title as a search query. We merge

1http://www.google.com/insights/search/
2http://www.bing.com/xrank/
3http://www.wikipedia.org/
4http://www.dmoz.org/

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India2437.2 Probability Distributions

Figure 1 shows the click-through distribution for all queries,

(cid:4)n

but studies indicate that navigational queries account for
anywhere from 10-25% of web searches [3, 18] and typically
result in a single click [11]. Removing these queries from
the query log would produce a more accurate distribution
for our algorithm, but automatically classifying queries as
informational or navigational is a diﬃcult task. To avoid
unfairly penalizing our algorithm with a click-distribution
containing navigational queries, we approximate the distri-
bution of how many relevant documents a user will require
using the geometric series Pr(J = j|U ) = 2
−j, which repre-
sents an average of 2 clicks-per-query, displays an exponen-
tial decay characteristic like Figure 1, and has the property
j=1 Pr(J = j|U ) = 1, which conforms with the con-
lim
n→∞
ditions of Proposition 4.1.
To measure the user intent distribution Pr(Ti|U ) for our
experiments, we conducted a survey using Amazon’s Me-
chanical Turk6. For each query we asked 10 survey partic-
ipants to select all of the subtopics they associate with the
query from the given choices. To keep the task manageable,
we limited our study to queries with at most 20 subtopics,
with an average of 8.5 subtopics per query. To ensure all
subtopics were considered, those which received no votes
were assigned a non-zero value of 0.01.
Unless otherwise noted, the document-subtopic probabil-
ity scores Pr(Ti|d) were assigned using the GibbsLDA++[14]
implementation of LDA (see Section 6.3.2). Parameters were
set at α = 0.2 and β = 0.1, based on values found to work
well for text collections [9].
7.3 Expected Hits

We analyzed Diversity-IQ with our overall goal metric of
maximizing the expected number of hits. For each test query
we compute the expected number of hits for each ranking
strategy over increasing values of n using Equation 3. A ma-
jority of users do not look beyond the ﬁrst result page [10],
making eﬃciency of the top documents particularly impor-
tant. We limit our evaluation to the top 10 results, as com-
mercial search engines typically show 10 results per page.

Figure 2 shows the mean expected hits computed over
the test query set for the top 10 results with the three
ranking approaches. Figure 2(a) assigns Pr(Ti|D) using the
subquery-based classiﬁcation method, while Figure 2(b) uses
LDA to assign subtopic scores. In both cases, the expected
hits from the top document is comparable, as providing at
least one document from the most probable subtopic is gen-
erally the initial strategy taken by both our algorithm and
IA-Select. After the top few results, however, our algorithm
may ﬁnd additional beneﬁts from providing additional docu-
ments from popular subtopics, and thus our algorithm tends
to increase the expected hits more rapidly.

We measured the runtime performance of each algorithm
on a 2.6GHz Intel Core 2 Duo CPU with 4 GB memory
running Mac OS X 10.6. Implementations were written in
Python. To select the top 10 results, Diversity-IQ required
an average of 28.8ms, while IA-Select averaged 28.5ms.
7.3.1 Classiﬁcation Score Range
We brieﬂy look at how the range of average classiﬁcation
scores can eﬀect the expected hits for both algorithms. For

6https://www.mturk.com/mturk/welcome

Figure 3: Eﬀect of subtopic scores

Figure 4: Varying number of required documents

each query we performed LDA classiﬁcation with varying
values of α and β. We identify the subtopic with the highest
individual score for each document and compute the average
of these scores over all documents. Figure 3 plots this aver-
age “top” subtopic score against the corresponding expected
hits for each algorithm. The ﬁgure shows that Diversity-
IQ outperforms IA-Select on expected hits regardless of the
classiﬁcation scores, and IA-Select suﬀers a signiﬁcant drop
in performance as potential subtopic scores approach 0.7.

7.3.2 Requiring Multiple Documents
We next study the eﬀects of Pr(J|U ) on expected hits,
and in particular performance as the number of relevant
documents a user is expected to require increases. Figure 4
shows the expected hits for n = 10 as we vary the number
of documents users are expected to require from 1 to 4. As
we can see, for users who require only one relevant docu-
ment (j = 1), our algorithms have equal performance. In
all cases where users want more than one document, how-
ever, Diversity-IQ outperforms IA-Select. As expected, we
can see that our algorithm’s relative performance improves
as users are expected to require additional documents.
7.4 Single Document Metrics

Having demonstrated the performance advantages of our
algorithm with respect to the more general model, we turn
our attention to metrics based on returning at least one rel-
evant document. As our algorithm may ﬁnd it beneﬁcial
to return multiple documents from popular subtopics be-
fore any documents from unpopular subtopics, we expect an
algorithm focused on returning at least one relevant docu-
ment, such as IA-Select, to outperform ours on these met-

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India244(a) Expected Hits (Query-based)

(b) Expected Hits (LDA)

Figure 2: Expected Hits

Figure 5: Subtopic recall

Figure 6: MRR-IA @ n

rics. Nonetheless, we feel it is important to compare the
algorithms on a level playing ﬁeld and quantify the diﬀer-
ences in these scenarios as well.

7.4.1 Subtopic Recall
Subtopic recall (S-recall) at rank N was deﬁned by Zhai,

Cohen, and Laﬀerty [24] as the percentage of relevant subtopics
covered by the top N documents. Assuming all users want
one relevant document and a uniform user intent probabil-
ity distribution, S-recall serves as an indication of expected
user satisfaction with the top N . S-recall requires binary
relevance judgements: a document either does or does not
satisfy a particular subtopic. To compute the S-recall for our
evaluation we consider a document as satisfying a subtopic
if its subtopic score is above a certain threshold, which we
set at Pr(Ti|d) ≥ 0.3.

Figure 5 plots the average subtopic recall for our evalua-
tion set. As expected, IA-Select outperforms our algorithm
on S-recall for the highest ranked documents. Nonetheless,
our algorithm outperforms the original search engine rank-
ing, and on average covers over one-half of the subtopics
within the top 10 results.

7.4.2 MRR-IA
S-recall assumes all subtopics are equally important. In
reality we know that certain subtopics are often considerably
more likely than others. To evaluate the eﬀectiveness of our
algorithm identifying such subtopics and presenting them

early, we consider the “intent aware” Mean Reciprocal Rank
(MRR-IA) metric deﬁned in [1]. MRR-IA measures the tra-
ditional mean reciprocal rank over each subtopic, weighted
by their probability of user intent. Again, we use the thresh-
old 0.3 to determine when a document satisﬁes a subtopic.
Figure 6 shows our algorithm outperforms the original
search engine ranking and a small decrease in performance
(approximately -6%) with respect to IA-Select at n = 10.
This indicates that our algorithm is still able to identify the
most probable subtopics and present at least one document
from each early in the ranking, thus performing well for a
majority of users even when we assume one relevant docu-
ment is suﬃcient.
7.5 Smoothing IA-Select

As noted earlier, we can partially address the weakness in
IA-Select by imposing limits on the maximum score assigned
to any particular subtopic. We now evaluate the eﬀects of
varying that limit on expected hits, MRR-IA, and S-recall.
For these experiments, we modiﬁed the Bayesian update step
of IA-Select (shown in Equation 4) to multiply the condi-
tional utility U of each subtopic by (1 − min(Pr(Ti|d), L)),
where L is the maximum allowable score. Figure 7 shows the
eﬀects of smoothing on IA-Select for various limits on the
maximum subtopic scores. The general trend shows that,
as we decrease the maximum allowable subtopic score, the
expected hits increase as the other metrics decrease. It is un-
clear how to intelligently select a proper “smoothing” value
for any particular number of relevant documents required.

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India245[4] J. Carbonell and J. Goldstein. The use of mmr,

diversity-based reranking for reordering documents
and producing summaries. In SIGIR, pages 335–336,
1998.

[5] H. Chen and D. R. Karger. Less is more: probabilistic

models for retrieving fewer relevant documents. In
SIGIR, pages 429–436, 2006.

[6] C. L. Clarke, M. Kolla, G. V. Cormack,

O. Vechtomova, A. Ashkan, S. B¨uttcher, and
I. MacKinnon. Novelty and diversity in information
retrieval evaluation. In SIGIR, pages 659–666, 2008.

[7] W. S. Cooper. Expected search length: A single

measure of retrieval eﬀectiveness based on the weak
ordering action of retrieval systems, 1968.

[8] C. Fellbaum. WordNet: An Electronic Lexical

Database. The MIT Press, Cambridge, MA, 1998.
[9] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc

topics. Proceedings of the National Academy of
Sciences of the United States of America,
101:5228–5235, April 2004.

[10] B. J. Jansen and A. Spink. How are we searching the
world wide web?: a comparison of nine search engine
transaction logs. Inf. Process. Manage., 42(1):248–263,
2006.

[11] U. Lee, Z. Liu, and J. Cho. Automatic identiﬁcation of

user goals in web search. In WWW, pages 391–400,
2005.

[12] F. Liu, C. Yu, and W. Meng. Personalized web search

by mapping user queries to categories. In CIKM,
pages 558–565, 2002.

[13] B. U. Oztekin, G. Karypis, and V. Kumar. Expert
agreement and content based reranking in a meta
search environment using mearf. In WWW, pages
333–344, 2002.

[14] X.-H. Phan and C.-T. Nguyen.

http://gibbslda.sourceforge.net/.

[15] A. Pretschner and S. Gauch. Ontology based

personalized search. In ICTAI, pages 391–398, 1999.

[16] F. Qiu, Z. Liu, and J. Cho. Analysis of user web traﬃc

with a focus on search activities. In WebDB, 2005.

[17] F. Radlinski and S. Dumais. Improving personalized

web search using result diversiﬁcation. In SIGIR,
pages 691–692, 2006.

[18] D. E. Rose and D. Levinson. Understanding user goals

in web search. In WWW, pages 13–19, 2004.

[19] M. Sanderson. Ambiguous queries: test collections

need more sense. In SIGIR, pages 499–506, 2008.

[20] R. Song, Z. Luo, J.-Y. Nie, Y. Yu, and H.-W. Hon.

Identiﬁcation of ambiguous queries in web search. Inf.
Process. Manage., 45(2):216–229, 2009.

[21] M. Steyvers and T. Griﬃths. Probabilistic topic

models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates, 2007.

[22] E. Voorhees. Overview of the trec 2004 robust

retrieval track. In TREC, 2004.

[23] J. Wang and J. Zhu. Portfolio theory of information

retrieval. In SIGIR, pages 115–122, 2009.

[24] C. X. Zhai, W. W. Cohen, and J. Laﬀerty. Beyond

independent relevance: methods and evaluation
metrics for subtopic retrieval. In SIGIR, pages 10–17,
2003.

Figure 7: Eﬀects of Smoothing on IA-Select

8. CONCLUSION AND FUTURE WORK

In this paper we focused on diversifying search results for
ambiguous informational queries, where users often require
multiple relevant documents. We presented a model for user
satisfaction with a set of search results, represented by the
expected number of hits, or user clicks on relevant docu-
ments, in the top n. We studied the problem of how, when
faced with an ambiguous query, a search engine can use
probabilistic knowledge of user intent, document classiﬁca-
tion, and how many relevant documents a user will require
to return a document set which maximizes the probability
of satisfaction for an average user. Experiments show our
Diversity-IQ algorithm outperforms a commercial search en-
gine and the state-of-the-art IA-Select diversiﬁcation algo-
rithm on the expected hits metric, while performing com-
parably on metrics designed under single relevant document
assumptions. Our algorithm also helps overcome the limita-
tions of IA-Select, performing well regardless of the subtopic
scores assigned by the document classiﬁcation function.

For future work, we would like to investigate a more de-
tailed model for a user’s page requirements by consider-
ing query-dependent or query-class dependent distributions.
That is, using a single distribution for page requirements
is a simpliﬁcation, and query-dependent Pr(J = j|U, q) or
query-class dependent Pr(J = j|U, C(q)) distributions may
help improve the model by allowing for diﬀerent estimates of
user need based on perceived goals (e.g. product purchase
vs. product research). As an extreme example, for navi-
gational queries we may want to assign this distribution as
Pr(J = 1|U, C(q) = nav) ≈ 1.0.

9. ACKNOWLEDGEMENTS

This work is partially supported by NSF grants, IIS-0534784

and IIS-0347993. Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of the fund-
ing institutions.

10. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.

Diversifying search results. In WSDM, pages 5–14,
2009.

[2] D. M. Blei, A. Y. Ng, M. I. Jordan, and J. Laﬀerty.

Latent dirichlet allocation. Journal of Machine
Learning Research, 3:2003, 2003.

[3] A. Broder. A taxonomy of web search. SIGIR Forum,

36(2):3–10, 2002.

WWW 2011 – Session: Trust and DiversityMarch 28–April 1, 2011, Hyderabad, India246