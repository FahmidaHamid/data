Consideration Set Generation in Commerce Search

Sayan Bhattacharya∗
Dept. of Computer Science

Duke University

bsayan@cs.duke.edu

Sreenivas Gollapudi
Microsoft Search Labs

Microsoft Research

sreenig@microsoft.com

Kamesh Munagala†
Dept. of Computer Science

Duke University

kamesh@cs.duke.edu

ABSTRACT
In commerce search, the set of products returned by a search
engine often forms the basis for all user interactions leading
up to a potential transaction on the web. Such a set of
products is known as the consideration set. In this study,
we consider the problem of generating consideration set of
products in commerce search so as to maximize user satis-
faction. One of the key features of commerce search that we
exploit in our study is the association of a set of important
attributes with the products and a set of speciﬁed attributes
with the user queries. Those important attributes not used
in the query are treated as unspeciﬁed. The attribute space
admits a natural deﬁnition of user satisfaction via user pref-
erences on the attributes and their values, viz. require that
the surfaced products be close to the speciﬁed attribute val-
ues in the query, and diverse with respect to the unspeciﬁed
attributes. We model this as a general Max-Sum Dispersion
problem wherein we are given a set of n nodes in a metric
space and the objective is to select a subset of nodes with to-
tal cost at most a given budget, and maximize the sum of the
pairwise distances between the selected nodes. In our set-
ting, each node denotes a product, the cost of a node being
inversely proportional to its relevance with respect to spec-
iﬁed attributes. The distance between two nodes quantiﬁes
the diversity with respect to the unspeciﬁed attributes. The
problem is NP-hard and a 2-approximation was previously
known only when all the nodes have unit cost.

In our setting, we do not make any assumptions on the
cost. We label this problem as the General Max-Sum Disper-
sion problem. We give the ﬁrst constant factor approxima-
tion algorithm for this problem, achieving an approximation
ratio of 2. Further, we perform extensive empirical analysis
on real-world data to show the eﬀectiveness of our algorithm.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information
Search and Retrieval
∗

Supported by NSF award CCF-0745761. Work done while

the author was an intern at Microsoft Search Labs.
†

Supported by an Alfred P. Sloan Research Fellowship, and
by NSF via CAREER award CCF-0745761 and grant CCF-
1008065.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

General Terms
Theory, Experimentation, Performance

Keywords
Approximation Algorithms, Facility Dispersion, Relevance,
Novelty

1.

INTRODUCTION

Commerce search is gaining signiﬁcance as more users
spend increasing amounts of time searching for products on
web. Typically, the shopping verticals of commercial search
engines or web portals maintain a catalog (or index) of prod-
ucts, and surface a consideration set of products from the
catalog that best match the user query. This is akin to the
result set the user sees in traditional web search. Unlike web
search, the main goal of a consideration set is to aid the user
in completing her transaction. Figure 11 underscores the im-
portance of a consideration set in commerce search where the
entropy of clickthrough rates in the top 10 positions is very
high. In fact, it is close to the normalized maximum value
of 1.0. This suggests that the user is more likely to browse
(or click) multiple results in diﬀerent positions for commerce
queries. Another fundamental diﬀerence between commerce
search and web search is the presence of structure in both
data and queries. Typically the catalog is composed of prod-
ucts in a well-structured or semi-structured format and/or
other rich metadata. For example, a digital camera is associ-
ated with attribute information such as brand, model, color,
etc. In fact, this key aspect of the data can be exploited in
commerce search. On the query side, commercial search
queries are good examples of (partially) structured queries
wherein a set of attribute values relevant to the product be-
ing searched can potentially be extracted from the query
string. For example, a user query such as 10mp Nikon Dig-
ital Camera denotes the user’s intent to see digital cameras
from a particular manufacturer (Nikon) having certain de-
sired features (10 megapixel resolution).

An important characteristic of the queries in commerce
search is that the user does not always associate a commer-
cial query with her intent to buy a product immediately.
Often, she issues many queries with diﬀerent keywords (at-
tributes of a product) and their combinations before she de-
cides to buy a product. Further, many of these exploratory
queries are often imprecise and incomplete in expressing the

1The data for this ﬁgure was obtained from mining three
months of query and click data from the search log of a
commercial search engine.

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India3171

0.8

0.6

0.4

0.2

s
e
t
a
r
 
h
g
u
o
r
h
t
k
c

i
l

c
 
f
o
 
y
p
o
r
t
n
E

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Probability of commercial intent

Figure 1: The correlation between the likelihood
of commercial intent in a query and the entropy of
clickthrough rates in the top 10 positions

user’s information need. This arises because of two reasons -
1) the user often does not have the domain knowledge. For
example, she may not be aware of the fact that Samsung
only makes 55” lcd tvs and not 52” lcd tvs, or for that mat-
ter there are no 50” lcd tvs made by any manufacturer; and
2) not all keywords (attributes) that describe a product are
used in a search query. The number of keywords typically
depends on the intent of the user such as how close is she
to buying a product. Let us elaborate with an example. A
user looking for digital cameras of a certain resolution might
start her search session with 10mp digital cameras and then
work her way toward issuing more speciﬁc queries to the
search engine such as Nikon Coolpix 10mp digital camera or
browsing on the result page to the product she is interested
in. Note that in both these queries, a large fraction of the
product attributes are still not speciﬁed. Continuing with
the example, there are multiple products in the Coolpix se-
ries with 10 megapixel resolution - the S3000 and S4000 -
with diﬀerent colors and price points. To substantiate our
point, we deﬁne the notions of speciﬁed and unspeciﬁed at-
tributes that we will use in our study to characterize the
space of relevant products for a user query. In our example,
the speciﬁed attributes are brand, model line, and resolu-
tion, while the unspeciﬁed attributes correspond to color,
optical zoom, and model number.

The above user query model motivates the development of
new techniques for generating eﬀective consideration set of
products in response to user queries. Before we summarize
our techniques for eﬀective consideration set generation, we
need to introduce another important aspect of the user query
model that we assume and exploit in our study.

Though the query acts as a point of reference to com-
pute the closeness of a product with respect to the speciﬁed
attributes, there is nothing similar to deal with the unspec-
iﬁed attributes. User preferences over diﬀerent attributes
will help us ﬁll this void. A user preference function is typ-
ically deﬁned over the attribute value space for any given
attribute. Even though the user does not explicitly and ex-
actly specify her information need through the user query,
she will value the consideration set more if the search en-
gine surface products with attribute values that are close
to or exceed her preferences. Again, consider the example
of the digital camera.
If the S3000 has (nearly) the same
attribute values as the S4000 but comes with a lower cost,
then surfacing the lower priced camera could be considered
a good search result by the user. Extending this to multiple
attributes each with diﬀerent preference functions makes the
problem challenging.

Contributions of this study
In this study, we propose methods for generating the consid-
eration set in commerce search that exploit both the speci-
ﬁed and unspeciﬁed attributes and user preferences to max-
imize user satisfaction. We observe that each product can
be represented as an attribute vector and each query as a
list of attribute values. It is natural to require that the sur-
faced products be close to the speciﬁed attribute values in
the query, and diverse with respect to the unspeciﬁed at-
tributes. This motivates the following Max-Sum Dispersion
problem. We are given n nodes in a metric space, each node
denoting a product and cost of a node being inversely pro-
portional to its relevance with respect to speciﬁed attributes
in the query. The distance between two nodes (with respect
to unspeciﬁed attributes) captures the notion of novelty or
coverage. The objective is to select a set of nodes with total
cost not more than a given budget, and maximize the sum
of the pairwise distances between the selected nodes. This
problem is known to be NP-hard, and no constant factor
approximation was known previously. We give the ﬁrst 2-
approximation algorithm for this problem along with some
implementation results. We note that our formulation is
general enough to admit attributes and their values with as-
sociated importance scores. These scores admit a natural
preference a user might have over an attribute (value) with
respect to the other attributes (values). In fact, we incorpo-
rate the relative importance of attributes and their values in
our experiments and show that our methods extend to these
settings as well.

2. RELATED WORK

The work on diversiﬁcation of search results has looked
into similar objectives as ours where the likelihood of the
user ﬁnding at least one result relevant in the result set forms
the basis of the objective function. One of the early inﬂuen-
tial work on diversiﬁcation is that of Maximal Marginal Rel-
evance (MMR) presented by Carbonell and Goldstein in [5].
In their work, a trade-oﬀ between novelty (a measure of di-
versity) and the relevance of search results is made explicit
through the use of two similarity functions, one measuring
the similarity among documents, and the other the similar-
ity between document and query. A parameter controls the
degree of trade-oﬀ.

Vee et al [15] study diversiﬁcation in the context of struc-
tured database with applications to online shopping. In their
work, items can be represented as a set of features or at-
tributes, and the objective is to select a set of items that are
as diverse as possible according to a lexicographical ordering
of attributes. In contrast, we propose an algorithm that ad-
mits general distance functions that satisfy the metric prop-
erty and user preferences over attribute values in addition to
attribute importance. Further, we provide strong theoretical
guarantees about the performance of our algorithm.

Zhai and Laﬀerty [18] formalize the notion that it is in
general insuﬃcient to simply return a set of relevant results
and that the correlations among the results are also impor-
tant. Speciﬁcally, they propose a risk minimization frame-
work for information retrieval that allows a user to deﬁne an
arbitrary loss function over the set of returned documents.
Bookstein [4] and Chen and Karger [7] both consider infor-
mation retrieval in the context of ambiguous queries. The
basic idea in these works is that documents should be se-

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India318lected sequentially according to the probability of the doc-
ument being relevant conditioned on the documents that
come before. Radlinski, Kleinberg, and Joachims [11] pro-
pose a learning algorithm to compute an ordering of search
results from a diverse set of orderings. By iterating through
all documents in each of the positions while holding ﬁxed
the documents in the other positions, they attempt to learn
a “best” ranking of documents using user clicks. Agrawal
et al. [1] take a diﬀerent approach that makes use of a tax-
onomy for classifying queries and documents, and creates a
diverse set of results in accordance to this taxonomy.

In approximation algorithms literature, all previous works
on Max-Sum Dispersion considered the Uniform-cost version
(when each node has unit cost) of the problem. Another
closely related problem is that of Max-Min Dispersion where
the objective is to maximize the minimum pairwise distance
of the selected nodes subject to a budget constraint. Both
the Uniform-cost Max-Sum and Uniform-cost Max-Min Dis-
persion problems are NP-hard [8, 17]. Ravi, Rosenkrantz,
and Tayi [12] give a 2-approximation for Uniform-cost Max-
min dispersion, and shows that it is NP-hard to obtain an
approximation ratio better than 2. For Uniform-cost Max-
sum dispersion, they provide an eﬃcient heuristic with ap-
proximation guarantee of 4, Hassin et. al. [9] give a diﬀerent
algorithm achieving an approximation ratio of 2. Birnbaum
et. al. [3] show, using factor-revealing LPs, that the eﬃ-
cient heuristic proposed in [12] has in fact an approximation
ratio of 2 for Uniform-cost Max-sum dispersion. Finally,
Rosenkrantz, Tayi, and Ravi [13] consider the Max-min dis-
persion problem and give a 2-approximation for the general
case, along with other extensions.

We conclude this section by mentioning that there is an
extensive literature on multiattribute utility theory and its
applications in commerce search that is closely related to
our work. The interested reader is referred to the article by
Wallenius et. al. [16] for an excellent survey of this ﬁeld.

3. PROBLEM FORMULATION

First, we observe that each product can be characterized
as an attribute vector, and given a query, attributes may
be classiﬁed as speciﬁed or unspeciﬁed (with respect to the
query). Moreover, an attribute is either monotone or single
peaked. For example, the price and resolution of a camera
are monotone attributes. Everyone wants a cheap camera
with high resolution. On the other hand, attributes like
color are single-peaked. We assume that a user wants a
consideration set of products that are close to the speciﬁed
attribute values, and at the same time diverse with respect
to the unspeciﬁed attributes.

Therefore we can model the above mentioned situation
as follows. We are given an undirected complete graph
G = (V, E), a weight function d : E → R, and a cost func-
tion c : V → R. Each node represents a product, the weight
of an edge captures the distance between the corresponding
products with respect to unspeciﬁed attributes, and the cost
of a node captures the distance of the corresponding prod-
uct from the speciﬁed attribute values. Throughout the rest
of the paper, we will use the terms weight of an edge (x, y)
and distance between nodes x, y interchangeably. Our ob-
jective is to select a set of nodes S with small total cost
c(S) =
v∈S c(v) and high dispersion Disp(S). The func-
tion Disp(S) captures how far away the nodes in S are from
each other.

P

Two most natural measures of dispersion are Max-min
Dispersion and Max-sum Dispersion. Max-min dispersion
of a set of nodes S is deﬁned as minu,v∈S{d(u, v)}, that
P
is, the minimum pairwise distance between the nodes. On
the other hand, Max-sum dispersion of a set S is deﬁned
x,y∈S d(x, y), that is, the sum of the pairwise distances
as
of the nodes. Intuitively, dispersion of a consideration set
should increase as more products are added to the set. How-
ever, we note that the Max-min dispersion function is not
monotone: Consider two sets S ⊂ S(cid:4)
. It may happen that
Max-min dispersion of S is strictly greater than that of S(cid:4)
.
Since Max-sum dispersion does not suﬀer from this potential
drawback, we deﬁne Disp(S) to be the Max-sum dispersion
of S.

Recall that we want to select a set of nodes with small
cost and large dispersion. One possible way to formulate
the problem is to maximize Disp(S) − λ × c(S), where λ is
some predeﬁned constant. Note that the problem can then
be described as a LP relaxation.

X

Maximize

X

d(u, v)xuv − λ

c(v)yv

LP

u,v∈V

v∈V
xuv ≤ min{yu, yv} ∀u, v ∈ V

0 ≤ xuv, yu, yv ≤ 1 ∀u, v ∈ V

(1)
(2)

Each yv is an indicator variable denoting whether or not
the node v has been included in the consideration set S.
Furthermore, the variable xuv is set to 1 if and only if
yu = yv = 1. Let {˜xuv, ˜yu, ˜yv} denote the optimal solu-
tion to the above LP. Pick an α uniformly at random from
the interval [0, 1]. For each node v ∈ V , include v in the
consideration set iﬀ α ≤ ˜yv.
It is easy to check that this
rounding scheme preserves the optimal objective value in
expectation. Hence the optimal solution can be found in
polynomial time. However, this formulation suﬀers from a
drawback. It may so happen that the total cost of the se-
lected nodes is too high, and this will defeat the intuition
that the surfaced products should be close to the speciﬁed
attributes. The best way to get around this diﬃculty is to
maximize Disp(S) subject to the constraint that c(S) is at
most some predeﬁned budget B. It will be termed as the
Max-Sum dispersion problem, and the main result of our pa-
per is a 2-approximation algorithm for this setting, provided
the edge weights satisfy triangle inequality. For complete-
ness, we note that without triangle inequality, the problem
may be hard to approximate: When all distances are either
0 or 1 and all nodes have unit cost, the problem is equiva-
lent to ﬁnding the B-densest subgraph, that is, ﬁnding a set
of B nodes such that the induced subgraph has maximum
number of edges, and the best known approximation ratio
is O(|V |1/4) [2].

While the max-sum dispersion formulation addresses the
issue of potentially unbounded cost of the consideration set,
it has its own drawback related to the quality of unspec-
iﬁed attribute values of the products it surfaces. Consider
the query brand = Nikon; Color = black; Category = digital
camera. Note that resolution of the camera is an unspeciﬁed
attribute, and our algorithm will ensure that the products
are as diverse as possible with respect to resolution. How-
ever, resolution is also a monotone attribute: We should be
surfacing only high resolution cameras. Consider two diﬀer-
ent cameras x and y with diﬀerent resolutions. The distance

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India319UniformGreedy(V, B)
if B = 0 then
Output ∅;

end
if B = 1 then

Output any node x ∈ V ;

end
Find an edge (x, y) ∈ argmax {d(u, v) : u, v ∈ V };
B(cid:4) ← B − 2;
V (cid:4) ← V \ {x, y};
Output {x, y} ∪ UniformGreedy(V (cid:4), B(cid:4)
Algorithm 1: Greedy algorithm with uniform costs

);

between the two, d(x, y) does not capture the monotonicity.
Still we can remedy the situation by considering a new dis-
tance function d(cid:4)
(x, y) = d(x, y) + w(x) + w(y), where w(x)
(resp. w(y)) denotes the importance of the resolution value
of x (resp. y). In this case, a camera with higher resolu-
tion has higher importance. We note that this works even
with multiple monotone attributes.
If the underlying dis-
tance d(u, v) is a metric, then d(cid:4)
(u, v) is also a metric and
therefore all our theoretical guarantees hold for d(cid:4)
(u, v) as
well.

Another potential drawback of the max-sum dispersion
objective is that the algorithm may select a product that
is far away from the natural consideration set.
In such a
situation, however, the outlier will have a high cost, and the
algorithm will quickly exhaust the budget if too many such
outliers are selected.

4. THE ALGORITHM

Hassin et. al. [9] proposed a simple 2-approximation for
Uniform-cost Max-Sum dispersion (see Algorithm 1). The
procedure UniformGreedy takes the set of nodes V and a
budget B as inputs, ﬁnds the edge (x, y) with highest weight,
outputs its two endpoints, and calls itself recursively with
V \ {x, y} and a budget of B − 2. In this section, we extend
their result to give a 2- approximation for general Max-Sum
dispersion where diﬀerent nodes can have diﬀerent costs.
4.1 Our Approach

First, consider the special case when the cost function
takes only small number of diﬀerent values. In other words,
suppose there is a partition of the set of nodes V into k
buckets V0, . . . Vk−1, such that all nodes in the same bucket
have same costs. Let (cid:4)w = (w0, . . . , wk−1) be the demand
vector whose ith component denotes the number of nodes
from bucket Vi selected by the optimal solution. We can
guess the vector (cid:4)w in time O(nk), where n = |V |, and it is
a polynomial in n as long as k is a constant. In Section 4.2,
we describe an extension to UniformGreedy that ﬁnds a
subset of nodes S subject to the constraint induced by vec-
tor (cid:4)w. Furthermore, we show (see Theorem 4.1) that it is
a 2-approximation to the optimal solution. The proof of
Theorem 4.1 closely follows the argument in [9].

Next, consider the general situation when all the cost val-
ues can potentially be diﬀerent from each other. Fix some
 > 0, and suppose we are allowed to overshoot the budget
by a factor of at most (1 + O()). Without any loss of gen-
erality, the maximum node cost is bounded from above by
B, the budget. Further, the minimum node cost is bounded

Greedy(V, (cid:4)w)
if wi = 0 for all i ∈ {0, . . . , k − 1} then
Output ∅;

end
if wl = 1 for some l and wi = 0 for all i (cid:10)= l then

Output any node xl ∈ Vl;

end
Let (xi, xj) be the feasible edge with maximum weight;
Let xi ∈ Vi, and xj ∈ Vj ;
if i = j then

i ← wi − 2;
w(cid:4)
i ← Vi \ {xi, xj};
V (cid:4)
else
i ← wi − 1;
w(cid:4)
j ← wj − 1;
w(cid:4)
i ← Vi \ {xi};
V (cid:4)
j ← Vj \ {xj};
V (cid:4)
end
for all t (cid:10)= i, j do
V (cid:4)
t = Vt ;
w(cid:4)
t = wt ;
end
0 ∪ . . . ∪ V (cid:4)
= V (cid:4)
V (cid:4)
0, . . . , w(cid:4)
= (w(cid:4)
(cid:4)w(cid:4)
Output {xi, xj} ∪ Greedy(V (cid:4), (cid:4)w(cid:4)
Algorithm 2: Greedy Algorithm for small number of
distinct costs

k−1. ;
k−1). ;

);

from below by B/n, since we can include in our solution all
the nodes below this threshold with a total additional cost
of B. Now discretize the range from B/n to B in powers
of (1 + ), getting L levels, and round every cost value down
to the nearest level below it. Clearly, (B/n)(1 + )L = B,
implying L = (log n − log )/ log(1 + ) ≤ (log n)/2, for suf-
ﬁciently small . Let ˜c(x) denote the rounded cost value
of node x, for all x ∈ V . Thus, the set of all nodes has
been eﬀectively partitioned into (log n)/2 buckets, and we
can guess how many nodes from each bucket is selected by
the optimal solution in O(n(log n)/2
) time. We can now run
P
the greedy algorithm from Section 4.2 to get a 2 approxima-
tion. Furthermore, note that for any subset of nodes S, if
v∈S c(v) ≤ (1 + )B.
˜c(S) =
In Section 4.3, we proceed to discretize the cost values even
further in such a way that brings down the running time to a
polynomial in n while preserving the approximation factor.
4.2 Small Number of Distinct Cost Values

P
v∈S ˜c(v) ≤ B, then c(S) =

We begin with a simple deﬁnition.

Definition 4.1. Suppose the set of nodes V has been par-
titioned into k buckets V0 . . . Vk−1. Furthermore, let (cid:4)w =
(w0, . . . , wk−1) denote the demand vector. An edge (xi, xj) ∈
E, where xi ∈ Vi, xj ∈ Vj is feasible if 1) i = j and wi ≥ 2,
or 2) i (cid:10)= j and wi, wj ≥ 1.

For a given partition of V into buckets V0 . . . Vk−1, and
demand vector (cid:4)w = (w0, . . . , wk−1), let OPT(V, (cid:4)w) denote
the subset of nodes with maximum dispersion that includes
exactly wi nodes from bucket Vi, for all i ∈ {0, . . . , k − 1}.
Theorem 4.1. Algorithm Greedy is a 2-approximation

to OPT(V, (cid:4)w).

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India320We prove Theorem 4.1 by induction on the demand vector
(cid:4)w. Suppose the claim holds for any demand vector strictly
dominated by (cid:4)w. To get some intuition behind the proof,
consider the ﬁrst edge (xi, xj) selected by Greedy, and sup-
pose both its endpoints are also included in OPT(V, (cid:4)w). Now
consider some other node u (resp. v) selected by OPT(V, (cid:4)w)
(resp. Greedy(V, (cid:4)w)). Since (xi, xj) has maximum weight
amongst the feasible edges, d(u, xi) + d(u, xj) ≤ 2d(xi, xj).
Since the edge weights satisfy triangle inequality, d(v, xi) +
d(v, xj) ≥ d(xi, xj). Thus, we have d(u, xi) + d(u, xj) ≤
2(d(v, xi) + d(v, xj)), and we can use induction hypothesis
= V \ {xi, xj} and the adjusted demand vec-
on the set V (cid:4)
tor (cid:4)w(cid:4)
(see Algorithm 2) to get the desired 2-approximation.
The complete proof is described in appendix.
4.3 Extending to General Cost Functions

wl˜cl ∈ˆ

The complete procedure is outlined in Algorithm 3. Let V0
denote the set of nodes with cost at most B/n. As explained
in Section 4.1, we can output all the nodes in V0. Now dis-
cretize the range from B/n to B in powers of (1+), getting
L levels, and round every cost value down to the nearest level
below it. Let ˜c(x) denote the rounded cost value of node x,
for all x ∈ V \ V0. It results in the set V \ V0 being parti-
tioned into L buckets V1 . . . VL, where L = (log n)/2. Let
wl denote the number of nodes selected from bucket Vl in the
optimal solution. Deﬁne ˜cl = ˜c(x) for any node x ∈ Vl. Let
, for some integer
tl ∈ {0, . . . , (log n)/3 − 1}. Under these circumstances, we
will round the total cost of the nodes selected from bucket
Vl down to tl(3B/ log n). In other words, we are pretending
that the contribution from each bucket is an integral mul-
tiple of 3B/ log n. We may underestimate the contribution
from each bucket by at most 3B/ log n. Since there are
(log n)/2 buckets, we can overshoot the budget by at most
((log n)/2) × (3B/ log n) = B. Thus, we get the following
Lemma (complete proof appears in appendix).

tl(3B/ log n), (tl + 1)(3B/ log n)

˜

Lemma 4.2. The set of nodes returned by the modiﬁed
greedy algorithm has a total cost of at most (1 + O())B, for
suﬃciently small  > 0.

Note that we need to guess L = (log n)/2 integers t1 . . . tL
such that each tl ∈ {0, . . . , (log n)/3 − 1}, and
l tl =
(log n)/3. It was shown in [6] that there are only polynomi-
ally many tuples satisfying the above mentioned condition.
For the sake of completeness, we prove the statement of
Lemma 4.3 in appendix.

Lemma 4.3.

[6] For any ﬁxed  > 0, the number of
diﬀerent L-tuples (t1, . . . , tL) is polynomial in n whenever
L = (log n)/2,
tl = (log n)/3, and each tl is a non-
negative integer.

l=1

PL

We can thus guess the correct demand vector in polyno-

mial time and run Greedy to get a 2-approximation.

Lemma 4.4. Algorithm ModifiedGreedy gives a 2-

approximation to the optimal solution.

Proof. Suppose the optimal solution satisfying the bud-
get constraint includes exactly kl nodes from bucket Vl.
ModifiedGreedy guesses in polytime a demand vector (cid:4)w(cid:4)
=
l ≥ kl, for each l ∈ {0, . . . , L}.
(w(cid:4)
ModifiedGreedy returns the set Greedy(V, (cid:4)w(cid:4)), and The-
orem 4.1 implies that it is a 2-approximation to OPT(V, (cid:4)w(cid:4)
).

L) such that w(cid:4)

0, . . . , w(cid:4)

P

ModifiedGreedy(V, B)
V0 ← {v ∈ V | c(v) ≤ B/n};
V (cid:4) ← V \ V0;
Partition V (cid:4)
˘
into L buckets V1, . . . , VL such that for
each l ∈ {1, . . . , L}, Vl ←
v ∈ V (cid:4) | (B/n)(1 + )l−1 < c(v) ≤ (B/n)(1 + )l
for l ∈ {1, . . . , L} do

¯

;

˜cl ← (B/n)(1 + )l−1;
for v ∈ Vl do
˜c(v) = ˜cl;

end

tl ∈˘

¯
end
For each l ∈ {1, . . . , L}, guess
0, 1, . . . , (log n)/3 − 1
for l ∈ {1, . . . , L} do

PL

l=1

s.t.

tl = (log n)/3;

Select the largest integer wl s.t.
wl˜cl ≤ (tl + 1)(3B/ log n);
l ← min(wl,|Vl|);
w(cid:4)
end
0 = |V0|;
Let w(cid:4)
Let (cid:4)w(cid:4) ← {w(cid:4)
S ← Greedy(V = V0 ∪ V1 ∪ . . . ∪ VL, (cid:4)w(cid:4)
Output S

2, . . . , w(cid:4)

0, w(cid:4)

L};

);

Algorithm 3: Modiﬁed Greedy Algorithm For General
Cost Functions

Since the function Disp(S) is monotone in S, dispersion of
the set OPT(V, (cid:4)w(cid:4)) is at least the optimal objective value
subject to the budget constraint. The lemma follows.

Combining the previous lemmas, we get the main result

of this section.

Theorem 4.5. ModifiedGreedy runs in polytime for
any ﬁxed suﬃciently small  > 0, overshoots the budget by
a factor of at most (1 + O()), and is a 2-approximation to
the optimal solution.

5. EXPERIMENTS

In this section, we do an empirical evaluation of our model
and algorithms described in the earlier sections. We primar-
ily evaluated the performance of our algorithm, Modified-
Greedy along two dimensions - the quality of results, and
the diversity of the products with respect to the unspeciﬁed
attributes in the query.
5.1 Setup

We used to two diﬀerent ranking functions to compare
our algorithms against. In the ﬁrst setup, we used a proto-
type commerce search engine serving results from an index
of about 30 million products taken from the Bing shopping
catalog2. We use the technique described in [14] to annotate
the query with attribute information. For the second rank-
ing function, we used the shopping vertical of a commercial
search engine. In both the cases, we compared the quality of
the results produced by our algorithms with those produced
by the respective ranking functions. In order to make the
comparison, we need attribute information related the prod-
ucts in the consideration set in each case. In the prototype

2http://shopping.bing.com

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India321s
e
i
r
e
u
q
 
f
o
 
r
e
b
m
u
N

1800
1600
1400
1200
1000
800
600
400
200
0

1

2

3

4

5

6

Number of attributes in query

Figure 2: The histogram of queries with number of
extracted attributes

y
c
n
e
u
q
e
r
F

40
35

30
25

20
15

10
5

0

17 18 19 20 21 22 23 24 25 26 27 28 30 31 32 34 39 44 47

Number of important attributes

Figure 3: The histogram of number of categories
with a given number of attributes

search engine, the search index was generated using the full
text descriptions and the structured attribute information of
the products. Thus, the consideration set produced by the
baseline structured ranking function is already enriched with
attribute information. In the case of the shopping vertical,
we extracted the attributes of the products in the consid-
eration set from the product pages surfaced by the search
engine and we mapped the attributes into the space of im-
portant attributes used in our prototype commerce search
engine.

As our query set we used a random sample of 3200 queries
from the shopping vertical of Bing. We extracted the at-
tribute value pairs for these queries. Naturally, our anno-
tator extracted structured information to diﬀerent degree
from these queries. Figure 2 shows the histogram of queries
with number of structured attributes extracted from them.
Further, we categorized these queries using a Nai¨ve Bayes
classiﬁer and found them to be spread across 173 categories
in our product taxonomy. Another important statistic is
that the average length of the query in our test set was 4.4
terms. In all the experiments, we considered the important
attributes of a category, i.e., attributes that are frequently
asked by the user or occur in a large fraction of the products3
in the index. In our case, we set the selectivity threshold to
0.75. This yielded us the histogram shown in Figure 3 for the
173 categories we consider in our experiments. This gives
us an idea of the number of unspeciﬁed attributes that our
algorithm has to deal with.

3also known as selectivity of the attribute

5.2 Structured Ranking and Distance Func-

tions

In the prototype search engine, we used a ranking func-
tion that exploits structure in both queries and products.
It is a simple combination of both structure and text.
It
exploits structure where it’s present (in the query) and falls
back to using textual relevance when no structure could be
extracted from the query. The relevance with respect to the
text is based on the well-known IR features like BM25F,
Proximity of query terms in the product description, and
fractional match of query terms. The key measure that is
used in computing the structural relevance is the distance
between corresponding attribute values. We denote the dis-
tance between any two attribute values u and v for a given
attribute i as di(u, v). Speciﬁcally, we use the likelihood of
an average user preferring v to u as the similarity4 between
u and v. This measure can be applied to both categorical
and numeric attributes. For numeric attributes, we deﬁne
di(u, v) = min(1.0, |u−v|
u ). For categorical attributes, we
adopt the method described in [10] which is based extract-
ing user preferences for attribute values from browse trails
on the web. We note that this distance function does not
satisfy all the properties of a metric.

Next, we describe the application of the distance func-
tion in our experiments. We begin with the baseline rank-
ing function. For the speciﬁed attributes, the relevance of
the product with respect to the query is computed using
the L1-norm of the distance between the corresponding at-
tribute values, i.e., c(p) = d(q, p) =
i di(u, v) where u and
v are respectively the values of attribute i in the query q
and product p. We use this scoring function along with the
textual relevance function as the baseline ranking function.
We compare the performance of ModifiedGreedy against
this function.

P

Handling Attribute Dependencies
Though we don’t explicitly deal with attribute dependencies
in this study, our algorithms admit more complex distance
functions that incorporate attribute dependencies. For ex-
ample, the algorithms proposed in [10] can be used to learn
user preferences over a set of attribute values. To keep the
problem tractable, this computation could be limited to sets
of popular attributes and their values.
5.3 Implementation of ModifiedGreedy

We now move on to the implementation of Modified-
Greedy. We will describe how we use the baseline ranking
function and the distance between unspeciﬁed attributes to
implement ModifiedGreedy. We begin with user prefer-
ences. Note that here we did not incorporate any user pref-
erences in the baseline structured ranking function. To do
so, we need to modify the distance function as

j

dp
i (u, v) =

0
di(u, v)

if v ≥ u
otherwise

for an attribute i that is monotone-upward, i.e., we don’t
penalize products with attribute value v greater than the
corresponding attribute value u in the query. A similar func-
tion is deﬁned for monotone-downward functions as well.
For single-peaked attributes, the distance stays unmodiﬁed.
4simi(u, v) = 1 − di(u, v)

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India322 

 
f
o
 
r
e
b
m
u
n
e
g
a
r
e
v
a
d
e
t
h
g
i
e
W

 

s
e
t
u
b
i
r
t
t
a

Baseline Structured Ranking
ModifiedGreedy

4

3.5

3

2.5

2

1.5

1

0.5

0

1

2

Number of attributes in query

3

4

5

Figure 4: The weighted average number of diﬀer-
ent attributes for queries grouped by the number of
speciﬁed attributes in the query

We use the modiﬁed distance function along with the tex-
tual relevance function used in the baseline structured rank-
ing function to generate the ﬁlter set of products. Note that
the ﬁlter set is a intermediate set of relevant products on
which we run ModifiedGreedy to generate the considera-
tion set for the user. Typically, the size of the ﬁlter set is
much larger (e.g., 300) than the size of the consideration set
(e.g., 10). Note that the above distance function does not
capture monotonicity. To circumvent this problem, we in-
corporate relative importance of attributes and their values
into the distance function as described in Section 3. The
computation of importance values is not the focus of this
study and we therefore assume that the importance values
are pre-computed and available to our algorithm.

Selection of the vector (cid:4)w
We also made a few changes to ModifiedGreedy to make
it run eﬃciently in the prototype search engine. The key
step is the guessing of the integers ti for i = 1, 2, ··· , L. For
each bucket we toss a coin with bias 1 − ci
C where C is the
total cost of all the buckets. Constructing (cid:4)w this way has
the desired property of picking elements from buckets with
smaller costs and thereby resulting in products which are
closer to the user query (in terms of speciﬁed attributes).
5.4 Comparison with the Baseline Structured

Ranking Function

We ran a set of experiments to compare the performance of
ModifiedGreedy against the baseline ranking function de-
scribed in the previous section. In all these experiments, we
used the top 10 results surfaced for each query in our query
set. Along with the basic attribute information such as name
of the product, manufacturer and price, we extracted infor-
mation associated with all the important attributes of each
product. In the ﬁrst experiment, we compared the amount
of important yet diverse information surfaced by both the
algorithms. To do so, we compute the number of diﬀerent
attributes surfaced in the result set weighted by their im-
portance. Figure 4 illustrates the weighted average number
of attributes for diﬀerent number of speciﬁed attributes in
the query. As the ﬁgure shows, the more general the query
(corresponds to less number of attributes), the better Mod-
ifiedGreedy performs with respect to the baseline. As the
queries get more speciﬁc (i.e., have more attributes), both
the algorithms perform equally well.

Next, we compare the relevance of the consideration set

Baseline

Number of
attributes Min Max
0.411
0.161
0.275
0.418
0.431

0.217
0.050
0.116
0.142
0.205

1
2
3
4
5

Avg
0.346
0.104
0.196
0.263
0.328

ModifiedGreedy

Min Max
0.407
0.266
0.064
0.189
0.231
0.122
0.332
0.143
0.205
0.270

Avg
0.341
0.123
0.170
0.209
0.136

Table 1: Relevance of the consideration set with re-
spect to the speciﬁed attributes of the best, worst,
and average document in the set, averaged over all
queries in test set

in both cases by computing the minimum, maximum, and
the average distance of the products from the query over the
speciﬁed attributes. These three measures give us a sense
of how close is the consideration set to the query in terms
of the best, worst and the average result in the set. Table 1
highlights the relative performance of both the algorithms.
Again, we compare the corresponding values by bucketing
the queries into buckets based on the number of attributes
extracted from the query.

As the results show, ModifiedGreedy indeed performs
as well as the baseline with respect to the nearness of the
surfaced products to the query in terms of the speciﬁed at-
tributes, i.e., the attributes recognized in the query. This
observation seems to hold across all range of queries, from
speciﬁc to general.

Finally, we performed a qualitative evaluation by conduct-
ing a user study using the Amazon Mechanical Turk plat-
form5. To compare the eﬀectiveness of the consideration set
in both cases, we selected two criteria to measure, viz., the
best product for each query and the usefulness of the un-
speciﬁed attribute values in the consideration set in helping
the user in her decision to buy the product. As part of the
evaluation, we sampled 240 queries and presented the hu-
man judge the top 10 results along with a budget (equal
to the price of the largest product in the consideration set)
and the unspeciﬁed attributes associated with the query and
asked the judge to select the product she is most likely to
buy and chose the unspeciﬁed attributes (if any) that aided
in her decision. We presented each query to 11 judges.

To get the best product, we simply selected the product
with the largest number of votes among the judges. Ties
were broken randomly. We then asked the judges to pick the
better product among the two best products (one for each
algorithm). This gives us a measure of the eﬀectiveness of
each consideration set in surfacing the product the user is
more likely to buy. In fact, the judges found the best prod-
uct surfaced by ModifiedGreedy bettered the best prod-
uct surfaced by the baseline algorithm 58.6% of the queries.
With respect to the second criteria, we also presented the
judges with the best unspeciﬁed attribute (again one for
each algorithm) and asked them to choose the better one.
Again, the judges concluded something similar. Modified-
Greedy was surfacing more relevant information related to
the unspeciﬁed attributes compared to the baseline. Speciﬁ-
cally, it did better in 67.8% of the queries. Coincidentally, in
the ﬁrst evaluation, the judges did not ﬁnd the unspeciﬁed
attributes helpful in around 21.7% of the queries while the
corresponding fraction for the consideration sets generated
by ModifiedGreedy was 10.6%. This suggests that the

5https://www.mturk.com/mturk/welcome

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India323Figure 5: The average number of unspeciﬁed at-
tribute values in the consideration set generated by
ModifiedGreedy and the shopping vertical of a com-
mercial search engine

Figure 6: The fraction of the top-10 attribute val-
ues covered by the consideration set. This fraction
is computed as a average over all the unspeciﬁed
attributes

consideration set generated by our algorithm not only sur-
faces relevant results to the user’s query but also surfaces
more helpful information not directly associated with the
speciﬁed attributes in the query.

5.5 Comparison with a Commercial Shopping

Vertical

We ran the same set of 3200 shopping queries against a
shopping vertical of a commercial search engine and scrapped
the results of each query. Beyond basic knowledge of the
ranking function of the search engine having access to the
attributes values of the products, we did not know anything
else about the ranking function itself. We simply treated it
as a black-box ranking function.

To measure the amount of diversity in the consideration
set with respect to the unspeciﬁed attributes, we aggregated
the attribute values for each product in the consideration set
over all queries, again grouping the queries according to the
number of speciﬁed attributes in them. Figure 5 shows that
the average number of unspeciﬁed attribute values surfaced
by ModifiedGreedy is larger than the corresponding num-
ber for the commercial shopping vertical. Interestingly, we
did not observe a similar diﬀerence when we compared Mod-
ifiedGreedy with the baseline structured ranking function.
We also performed a user study to compare the relevance
of the consideration sets produced by the shopping vertical
and ModifiedGreedy. As in the experiment in the last
section, we compared the best products surfaced in the each
of the consideration sets. Again, the best product of Modi-
fiedGreedy was chosen 60% of the time over the best result
produced by the shopping vertical. The diﬀerence is more
pronounced in the case of the best unspeciﬁed attribute (i.e.,
the unspeciﬁed attribute chosen by the user to be most help-
ful in their decision to buy the product) in both cases. In
fact, 78% of the users preferred the the unspeciﬁed attribute
surfaced by ModifiedGreedy to be more helpful than the
corresponding unspeciﬁed attribute in the consideration set
in the shopping vertical.

5.6 Coverage of User Intents

In the experiments described in the previous section, we
focused on coverage of important unspeciﬁed attributes where
we measured the importance using the product data in the
index. Further, we did not consider any diﬀerence in impor-

tance of the attribute values themselves. In this section, we
describe a set of experiments, where we measured the num-
ber of intents covered by the consideration sets. Here, we
deﬁne an intent to be the set of attribute values associated
with a query. For example, when the user issues a query 17
inch laptops, one of her intents could be to buy a $700 17
inch dell latitude laptop. Another could be to buy a $500
17 inch acer laptop. Thus, her intent could be deﬁned as a
distribution over the attribute values for a given product.

Bed type (Importance)

Bed Type (Importance)

Platform (0.23)

Bunk (0.19)
Teen (0.17)

Toddler (0.11)

Loft (0.11)

Trundle (0.07)

Kids (0.04)
Sleigh (0.03)
Canopy (0.03)

Adjustable (0.01)

Table 2: Relative importance of attribute values for
the attribute bed type in the category beds

In this experiment, we don’t explicitly compute this dis-
tribution. Instead we approximate it by computing the top
attribute values for each of the important attributes the user
is likely to consider in her search. Here we assume indepen-
dence between attributes and hence we treat the likelihood
of the user picking one attribute value independent of her
choice of values for other attributes. We adopt the deﬁnition
of attribute value importance described in [10] wherein the
importance is measured in terms of the user’s likelihood of
switching the value during the course of her browsing the
web for the product. Thus, an important attribute value
in the query is likely to exist in the product she eventually
buys. For completeness, we include an example here. Ta-
ble 2 shows the important attribute values for bed type in
the category home furnishings|furniture|beds.

Thus, for each consideration set generated for a query in
our test set, we compute the weighted sum of the attribute
values covered by the set using each value’s importance as
its weight. Again, to be able to compare both structured
ranking functions with the shopping vertical, we mapped
the attribute values extracted from the product pages to
the corresponding attribute values used in the search index
of our prototype search engine. This mapping was done
using the Jaccard similarity6 measure and picking the tar-

6Given two sets A and B, Jaccard(A, B) =

|A∩B|
|A∪B|

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India324[3] B. E. Birnbaum and K. J. Goldman. An improved
analysis for a greedy remote-clique algorithm using
factor-revealing lps. Algorithmica, 55(1):42–59, 2009.

[4] A. Bookstein. Information retrieval: A sequential

learning process. Journal of the American Society for
Information Sciences (ASIS), 34(5):331–342, 1983.

[5] J. G. Carbonell and J. Goldstein. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In SIGIR, pages 335–336,
1998.

[6] C. Chekuri and S. Khanna. A ptas for the multiple
knapsack problem. In SODA, pages 213–222, 2000.

[7] H. Chen and D. R. Karger. Less is more: probabilistic

models for retrieving fewer relevant documents. In
SIGIR, pages 429–436, 2006.

[8] E. Erkut. The discrete p-dispersion problem. Eur. J.

Opnl. Res., 46:48–60, 1990.

[9] R. Hassin, S. Rubinstein, and A. Tamir.

Approximation algorithms for maximum dispersion.
Oper. Res. Lett., 21(3):133–137, 1997.

[10] D. Panigrahi and S. Gollapudi. Result enrichment in

commerce search using browse trails. In WSDM, 2011.
[11] F. Radlinski, R. Kleinberg, and T. Joachims. Learning

diverse rankings with multi-armed bandits. In
International Conference on Machine Learning
(ICML), 2008. First presented at NIPS07 Workshop
on Machine Learning for Web Search.

[12] S. S. Ravi, D. J. Rosenkrantz, and G. K. Tayi.

Heuristic and special case algorithms for dispersion
problems. Operations Research, 42(2):299–310, 1994.

[13] D. J. Rosenkrantz, G. K. Tayi, and S. S. Ravi. Facility

dispersion problems under capacity and cost
constraints. J. Comb. Optim., 4(1):7–33, 2000.

[14] N. Sarkas, S. Paparizos, and P. Tsaparas. Structured

annotations of web queries. In SIGMOD, pages
771–782, 2010.

[15] E. Vee, U. Srivastava, J. Shanmugasundaram, P. Bhat,

and S. Amer-Yahia. Eﬃcient computation of diverse
query results. In ICDE, pages 228–236, 2008.

[16] J. Wallenius, J. S. Dyer, P. C. Fishburn, R. E. Steuer,

S. Zionts, and K. Deb. Multiple criteria decision
making, multiattribute utility theory: Recent
accomplishments and what lies ahead. Management
Science, 54(7):1336–1349, 2008.

[17] D. W. Wang and Y.-S. Kuo. A study on two geometric

location problems. Inf. Process. Lett., 28(6):281–286,
1988.

[18] C. Zhai and J. D. Laﬀerty. A risk minimzation

framework for information retrieval. Info. Processing
and Management, 42(1):31–55, 2006.

8. APPENDIX

8.1 Proof of Theorem 4.1

Proof. We use induction on the demand vector (cid:4)w. For
the base case, if exactly one component of (cid:4)w is 1 and ev-
ery other component is zero, there is nothing more to prove.
Suppose the claim is true for any demand vector that is
strictly dominated by (cid:4)w. The greedy algorithm selects the
edge (xi, xj), xi ∈ Vi, xj ∈ Vj before recursively calling it-
self on (V (cid:4), (cid:4)w(cid:4)
). Let S(cid:4), O(cid:4), S, O denote the set of nodes

Figure 7: The fraction of the top-10 attribute val-
ues covered by the consideration set. This fraction
is computed as a weighted average over all the un-
speciﬁed attributes

get attribute value with the highest similarity score above
a threshold (0.5 in our case).
In another experiment, we
further weigh the contribution of attribute values of an at-
tribute with the importance of the attribute. Figures 7 and
6 show the relative performance of all baseline structured
ranking function, ModifiedGreedy, and the shopping ver-
tical for the respective settings of with and without attribute
importance.

As both the ﬁgures show, ModifiedGreedy surfaces more
useful attribute values to the user in the consideration set
compared to both the baseline structured ranking function
and the shopping vertical in the case when the contribution
of attribute values of an attribute are weighted by the impor-
tance of the attribute. In the other case where all attributes
are treated uniformly, ModifiedGreedy outperforms the
other algorithms when the queries are more general.

6. CONCLUSIONS

In this study, we propose an algorithm for generating the
consideration set in commerce search that exploits both the
speciﬁed and unspeciﬁed attributes and user preferences to
maximize user satisfaction. Speciﬁcally, we show that this
setting admits a natural notion of user satisfaction that re-
quires that the surfaced products be close to the speciﬁed
attribute values in the query, and diverse with respect to the
unspeciﬁed attributes which we solve using a max-sum dis-
persion formulation. This problem is known to be NP-hard,
and no constant factor approximation was known previously.
We give the ﬁrst 2-approximation algorithm for this prob-
lem along with extensive empirical analysis involving both
real-world data and user studies.

The directions for future work include studies on the eﬀect
of diﬀerent demand vectors and budgets on the quality of the
ﬁnal solution.

7. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.

Diversifying search results. In WSDM, pages 5–14,
2009.

[2] A. Bhaskara, M. Charikar, E. Chlamtac, U. Feige, and

A. Vijayaraghavan. Detecting high log-densities: an
(1/4) approximation for densest -subgraph. In STOC,
pages 201–210, 2010.

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India3258.2 Proof of Lemma 4.3

PL

f =

Proof. Let f denote the number of diﬀerent L-tuples

with

l=1

tl = log n/3 = L/. Clearly,

!

 
L/ + L − 1

!

L − 1

≤

 

≤ ((1 + 1/)L)L−1

(L − 1)!

(1 + 1/)L

L − 1
“

= O

((1 + 1/)e)

”

L−1

The last equality follows by applying Stirling’s formula on
(L − 1)!. Noting that (1 + 1/) ≤ e1/ and plugging in the
value of L, we get

”

“

”

“

e(1+1/)(log n/2−1)

= O

nO(1/3

)

(3)

f = O

returned by Greedy(V (cid:4), (cid:4)w(cid:4)), OPT(V (cid:4), (cid:4)w(cid:4)), Greedy(V, (cid:4)w),
and OPT(V, (cid:4)w) respectively. We introduce the notations
yi, yj to denote two nodes depending on the following three
cases.

• Case 1 If xi, xj ∈ O, let yi = xi and yj = xj.
• Case 2 If xi ∈ O and xj /∈ O, let yi = xi and let yj be
some node in the set Vj \ {xi} that belongs to O.
• Case 3 If xi /∈ O and xj /∈ O, let yi (resp. yj ) be some
node in the set Vi \ {xi, xj} (resp. Vj \ {xi, xj}) that
belongs to O.

Note that

Disp(O) = Disp(O \ {yi, yj}) + d(yi, yj) +
(d(u, yi) + d(u, yj))

X

u∈O\{yi,yj}

Since O \ {yi, yj} is a solution to the instance (V (cid:4), (cid:4)w(cid:4)
),
). By Induction hypothesis,
). Combining these two inequalities,

Disp(O \ {yi, yj}) ≤ Disp(O(cid:4)
Disp(O(cid:4)
we get

) ≤ 2Disp(S(cid:4)

Disp(O \ {yi, yj}) ≤ 2Disp(S(cid:4)

)

(4)
Consider any node u ∈ O \ {yi, yj}. Since both the edges
(u, yi) and (u, yj ) are feasible for (V, (cid:4)w), and (xi, xj) is the
feasible edge for (V, (cid:4)w) with highest weight, we have d(u, yi)+
d(u, yj ) ≤ 2d(xi, xj). Now consider any node v ∈ S(cid:4)
. By
triangle inequality, d(v, xi) + d(v, xj) ≥ d(xi, xj). Thus,
d(u, yi) + d(u, yj) ≤ 2(d(v, xi) + d(v, xj)). Since both the
and O \ {xi, xj} contain same number of nodes, we
sets S(cid:4)
haveX

X

(d(u, yi) + d(u, yj )) ≤ 2

(d(v, xi) + d(v, xj))

u∈O\{xi,xj}
(5)
Combining Inequalities 3, 4, 5, and noting that d(yi, yj) ≤
d(xi, xj), we get

v∈S(cid:2)

X
Disp(O) ≤ 2Disp(S(cid:4)

) + d(xi, xj)

+2

(d(v, xi) + d(v, xj))

v∈S(cid:2)
≤ 2Disp(S)

Proof. Since the set S contains exactly w(cid:4)

l nodes from
l˜cl ≤ (tl + 1)(3B/ log n), whenever

8.3 Proof of Lemma 4.2

bucket Vl, ˜c(S ∩ Vl) = w(cid:4)
l ≥ 1. Therefore,
LX
˜c(S \ V0) =
 

l=1

˜c(S ∩ Vl) ≤ LX
LX

!

l=1

(tl + 1)(3B/ log n)

=

L +

tl

3B/ log n = (1 + )B

l=1

Next, consider any bucket Vl, l ∈ {1, . . . , L}, and note that
for any node v ∈ Vl, c(v) ≤ (1 + )˜c(v). Thus,
c(S \ V0) ≤ (1 + )˜c(S \ V0) ≤ (1 + )2B ≤ (1 + 3)B
Finally, each node in V0 has a cost at most B/n, and there
can be at most n nodes in V0. Thus, the total cost of the
nodes in V0 is at most B. Combining this with the fact
that c(S \ V0) ≤ (1 + 3)B, we get
c(S) = c(S \ V0) + c(V0) ≤ (1 + 3)B + B = (1 + 4)B

WWW 2011 – Session: E-commerceMarch 28–April 1, 2011, Hyderabad, India326