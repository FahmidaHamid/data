A Uniﬁed Framework for Recommending Diverse and

Relevant Queries

Xiaofei Zhu

Jiafeng Guo

Xueqi Cheng

Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China

{zhuxiaofei, guojiafeng, dupan,shenhuawei}@software.ict.ac.cn, cxq@ict.ac.cn

Pan Du

Hua-Wei Shen

ABSTRACT
Query recommendation has been considered as an eﬀective
way to help search users in their information seeking ac-
tivities. Traditional approaches mainly focused on recom-
mending alternative queries with close search intent to the
original query. However, to only take relevance into account
may generate redundant recommendations to users.
It is
better to provide diverse as well as relevant query recom-
mendations, so that we can cover multiple potential search
intents of users and minimize the risk that users will not
be satisﬁed. Besides, previous query recommendation ap-
proaches mostly relied on measuring the relevance or sim-
ilarity between queries in the Euclidean space. However,
there is no convincing evidence that the query space is Eu-
clidean. It is more natural and reasonable to assume that
the query space is a manifold. In this paper, therefore, we
aim to recommend diverse and relevant queries based on
the intrinsic query manifold. We propose a uniﬁed model,
named manifold ranking with stop points, for query recom-
mendation. By turning ranked queries into stop points on
the query manifold, our approach can generate query rec-
ommendations by simultaneously considering both diversity
and relevance in a uniﬁed way. Empirical experimental re-
sults on a large scale query log of a commercial search engine
show that our approach can eﬀectively generate highly di-
verse as well as closely related query recommendations.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Query formulation

General Terms
Algorithm, Experimentation, Performance, Theory

Keywords
Query Recommendation, Diversity, Manifold Ranking with
Stop Points

1.

INTRODUCTION

With the exponential growth of information on the Web,
search engine has become an indispensable tool for Web
users to seek their desired information. However, it is never

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

easy for users to formulate a proper query to search because
query is usually very short [7] and words are ambiguous [23].
Furthermore, users sometimes cannot express their search
intent precisely due to the lack of domain-speciﬁc knowledge.
Therefore, how to help users formulate a suitable query has
been recognized as a challenging problem. To overcome this
problem, a valuable technique, query recommendation, has
been employed by most commercial search engines, such as
Google 1, Yahoo!2, and Bing3 to improve usability. Query
recommendation aims to suggest queries that may better re-
ﬂect users’ information needs to help them ﬁnd what they
need more quickly.

Traditional query recommendation approaches mainly fo-
cused on recommending alternative queries with close search
intent to the original query. Query logs are widely used
in these approaches [2, 15, 23], where similar queries are
identiﬁed based on users’ historical behavior and used as
recommendations for each other. However, to only take rel-
evance/similarity into account may generate redundant rec-
ommendations. For example, when a user issues a query
‘abc’, the system may recommend him/her ‘abc television’
and ‘abc tv’, which are both very relevant to ‘abc’ but of
the equivalent meaning. Recommending such queries at the
same time will decrease the recommendation quality since
they provide almost the same information to users. There-
fore, it is important to provide diverse as well as relevant
query recommendations. By reducing the redundancy, we
are able to cover multiple potential search intents of users
and minimize the risk that users will not be satisﬁed.

In addition, previous query recommendation approaches
mostly relied on measuring the similarity between queries in
the Euclidean space, either based on query terms or click-
through data. However, there is no convincing evidence that
the query space is Euclidean. Inspired by the research work
on document modeling [26, 27], it is more natural and rea-
sonable to assume that the query space is a manifold, either
linear or non-linear. The local geometric structure is es-
sential to reveal the relationship between queries.
In our
study, we ﬁnd that ranking queries in terms of the intrinsic
global manifold structure [26, 27] is superior to the pairwise
distance in the Euclidean space.

In this paper, therefore, we propose to recommend diverse
and relevant queries based on the intrinsic query manifold.
We propose a novel uniﬁed model, named manifold rank-
ing with stop points, for query recommendation. Specif-

1http://www.google.com/
2http://www.yahoo.com/
3http://www.bing.com/

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India37ically, our approach leverages a manifold ranking process
over query manifold, which can naturally make full use of
the relationships among queries to ﬁnd relevant and salient
queries. Meanwhile, we introduce the stop points into query
manifold to capture the diversity during the ranking process.
The stop points are points that stop spreading their ranking
scores to their nearby neighbors during the manifold rank-
ing process. By turning ranked queries into stop points, the
ranking scores of other queries close to these stop points (i.e.,
queries which share similar search intent with the ranked
queries) will be naturally penalized during the ranking pro-
cess based on the intrinsic query manifold. Therefore, our
approach can generate query recommendations by simulta-
neously considering both diversity and relevance between
queries in a uniﬁed way. Like traditional manifold ranking
algorithm, the new proposed ranking approach also shows a
nice convergence property.

We conducted extensive experiments to evaluate the pro-
posed approach based on a large collection of query logs from
a commercial search engine. Empirical experimental results
show that our approach can eﬀectively generate highly di-
verse as well as closely related query recommendations.

The main contributions of our approach can be summa-
rized as follows:
(1) we ﬁrst exploit the intrinsic global
query manifold structure to measure the similarity between
queries; (2) we propose a novel ranking approach, i.e., man-
ifold ranking with stop points, for query recommendation
which addresses relevance and diversity simultaneously in a
uniﬁed way; (3) the proposed ranking approach has a nice
convergence property; (4) we show that our query recom-
mendation approach is superior to other baseline methods
in producing diverse and relevant query recommendations.
The rest of the paper is organized as follows. Section 2
introduces related work. Section 3 describes our proposed
approach in detail. Experimental results are discussed in
Section 4 and conclusion is made in Section 5.

2. RELATED WORK

Query recommendation. Query recommendation has
been employed as a core utility by many industrial search
engines, which focuses on improving queries raised by users.
The intention of query recommendation is closely related
to query expansion [24, 7, 21], query substitution [12] and
query reﬁnement[14, 9]. The main diﬀerence is that query
recommendation aims to recommend full queries submitted
by previous users. Most of the work on query recommenda-
tion is focused on measures of query similarity, where query
log data has been widely used in these approaches.

Click-through information conveyed in query log is often
leveraged to measure the similarity of queries. The basic
assumption is that queries sharing more clicked URLs are
considered more similar [4]. A query-URL bipartite graph
can be constructed from the click-through data with the
vertices on one side corresponding to queries and on the
other side to URLs. Beeferman et al. [2] applied agglomera-
tive clustering algorithm to the click-through bipartite graph
to identify related queries for recommendation. Wen et al.
[23] proposed to combine both user click-through data and
query content information to determine the query similar-
ity. Li et al. [15] recommended related queries by computing
the similarity between queries based on query-URL vector
model and leveraging a hierarchical agglomerative clustering
method to rank similar queries. Ma et al. [16] developed a

two-level query recommendation method based on two bi-
partite graphs (user-query and query-URL bipartite graphs)
extracted from the click-through data.

However, most previous work only focused on recommen-
dation relevance, while not explicitly addressed the prob-
lem of diversity. Mei et al.
[18] tackled this problem using
a hitting time approach based on the query-URL bipartite
graph. Their approach can recommend more diverse queries
by boosting long tail queries. However, the weakness of
their approach is that it would sacriﬁce the relevance con-
siderably when improving the diversity, and many long tail
queries recommended to users may not be familiar to them.
Diﬀerent from existing approaches to query recommen-
dation, our approach exploits the intrinsic global manifold
structure to measure the similarity between queries and em-
ploys a novel ranking approach to address relevance and di-
versity simultaneously.

Diversity rank. Beyond relevance, diversity has also
been recognized as a crucial criteria in ranking[5, 28, 17, 13,
25]. Top ranked results are expected to convey as little re-
dundant information as possible, and cover as many aspects
as possible.

Among the existing research work, Maximal Marginal Rel-
evance (MMR) [5] is the most well-known method used for
result set diversiﬁcation. It has been widely used in the text
summarization community. MMR utilizes a greedy strategy
that iteratively selects the best scoring object, and then up-
dates remaining object scores by computing a penalty based
on the similarity of each object with the selected object.
The closest work to ours is Grasshopper proposed by Zhu
et al. [28], which applies an absorbing random walk on the
graph. In order to achieve diversity, it turns the selected ob-
ject into an absorbing state and then selects the next object
based on the expected number of visits to each node before
absorption. Although our work share similar idea with this
approach in handling ranked objects, they are largely dif-
ferent in ranking strategy. Grasshopper uses two diﬀerent
measures, i.e., stationary distribution and expected number
of visits, to select the top ranked object and the remain-
ing objects. In contrast, all the objects are ranked with a
consistent strategy (i.e., using their ranking scores) in our
approach. A recent improvement on diversity ranking us-
ing random walk based approach is DivRank [17]. DivRank
employs a time-variant random walk process, which uses the
rich-gets-richer mechanism in ranking. However, the main
drawback is that the computation of the expected number of
visits is intractable and has to resort to some approximation
strategies.

Diﬀerent from these above approaches, we introduce the
notion of stop points in manifold ranking and propose a
uniﬁed model which simultaneously consider both relevance
and diversity for query recommendation.

Manifold ranking. The manifold ranking algorithm ﬁrst
constructs a weighted network on the data, and assigns a
positive ranking score to the input query and zero to the
remaining points which are to be ranked with respect to the
input query. Then all points spread their ranking scores to
their nearby neighbors via the network until a global stable
state is reached. Points without the input one are ranked
according to their ﬁnal ranking scores.

Manifold ranking was used to rank data with respect to
the intrinsic global manifold structure collectively revealed
by a huge amount of data [26, 27].
It has been applied

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India38in many research ﬁelds [10, 22, 19] recently where a rank-
ing is needed in essentials. For example, He et al.
[10]
leveraged manifold ranking to measure relevance between
the query and database images for image retrieval. Wan et
al.
[22] applied the manifold ranking process to utilize the
relationships between the topic and the sentences for text
summarization. However, so far there is no related work on
applying manifold ranking for query recommendation.

To the best of our knowledge, this paper is the ﬁrst article
attempt to utilize manifold ranking for query recommenda-
tion.

3. OUR APPROACH
3.1 Preliminaries and Notations
Given a set of data points (i.e. queries) X = {q0, q1, . . . , qn}
⊂ R
m , the ﬁrst point q0 is the input query and the rest of the
points qi (1 ≤ i ≤ n) are the candidate queries. Hereafter,
query and point will not be discriminated unless otherwise
speciﬁed. Let d : X × X → R denote a metric on X (e.g.
Euclidean distance), where d(qi, qj ) is the distance between
qi and qj . Let f : X → R denote a ranking function which
assigns to each point qi (0 ≤ i ≤ n) a ranking value fi. We
can view f as a vector f = [f0, . . . , fn]T . We also deﬁne a
vector y = [y0, . . . , yn]T , in which y0 = 1 for the input query
q0 and yi = 0 (1 ≤ i ≤ n) for all the candidate queries.
3.2 Query Manifold

In our work, queries are assumed to be sampled from a
low-dimensional manifold which is embedded in the high-
dimensional ambient space. Our recommendation approach
is then based on such a query manifold structure. Here
we build a k-nearest-neighbor query graph using the click-
through information in query logs to model the local query
manifold structure.

The click-through data can help us ﬁnd similar queries.
The basic idea is that if two queries share many clicked
URLs, they have similar search intent to each other [15].
Therefore, we model queries in terms of query-URL vectors,
instead of query-term vectors. We represent each query qi as
a L2-normalized vector, where each dimension corresponds
to one unique URL in the click-through data. Speciﬁcally,
given a query qi (0 ≤ i ≤ n), the j-th element of the feature
vector of qi is

(cid:2)

eij√(cid:3) m

k=1 e2

ik

j
q
i =

if qi clicked uj;

(1)

0

otherwise,

where m denotes the total number of unique URLs in the
click-through data and eij denotes the weight for the pair
of query qi and its clicked URL uj . Here we follow the CF-
IQF weighting scheme [8] and deﬁne the weight eij = cfij ×
log(n/qfj ), where cfij denotes the total click frequency on
uj given qi, qfj denotes the total number of unique queries
which have clicked uj , and n denotes the total number of
unique queries in the query log. The distance between two
queries qi and qj is then measured by the Euclidean distance
between their normalized feature vectors
i − qk

(cid:4)(cid:3)m

d(qi, qj) =

j )2.

k=1 (qk

(2)

any two points with an edge if they are among the k nearest
neighbors to each other (k = 50 in our case). In this way, we
are able to preserve the sparse property of the query mani-
fold. We deﬁne an aﬃnity matrix W for the query manifold,
where

−d(qi,qj )2

2σ2

,

wij = e

(3)

if there is an edge linking qi and qj , and wii = 0 as there
are no loops in the graph. Here σ is empirically set to 1.25.
3.3 Manifold Ranking with Stop Points

A traditional manifold ranking process [26] over the query

manifold can be described as follows:

1. Symmetrically normalize W by S = D

−1/2W D

−1/2

in which D is the diagonal matrix with (i, i)-element
equal to the sum of the i-th row of W .

2. Iterate f (t+1) = αSf (t) + (1 − α)y until convergence,

where α is a parameter in [0, 1).

3. Let f

i denote the limit of the sequence of {f
∗
each point qi according its ranking scores f
ranked ﬁrst).

i }. Rank
(t)
∗
i (largest

In the above ranking process, all the points spread their
ranking scores to their neighbors via the weighted graph.
The spread process is repeated until a global stable state is
achieved, and all the points are ranked according to their
ﬁnal ranking scores. With the traditional manifold rank-
ing process, we can obtain relevant and salient queries for
recommendation given the input query.

To explicitly address the diversity of query recommen-
dation, we introduce stop points into query manifold and
propose a novel ranking approach, named manifold ranking
with stop points. The stop points are a special type of points
on query manifold, which stop spreading their ranking scores
to their neighbors during the manifold ranking process. In-
tuitively, we can imagine the stop points as the “black holes”
on the manifold, where no ranking scores would be able to
“escape” from them. By turning ranked queries into stop
points, the ranking scores of other queries close to the stop
points (i.e., queries which share similar search intent with
the ranked queries) will be naturally penalized during the
ranking process based on the intrinsic query manifold.

Here we derive the new iteration algorithm for manifold
ranking with stop points. Let T denote the set of stop points,
and R denote the set of free points (all the data points ex-
cluding the stop points). The normalized matrix S in tradi-
tional manifold ranking can then be reorganized as a block

(cid:6)

matrix

, and the original iteration equation

(cid:5)
(cid:5)

in step 2 can be written as

SRR SRT
ST R ST T

(cid:6)(t+1)

fR
fT

(cid:5)

(cid:6)(cid:5)

(cid:6)(t)

fR
fT

(4)

= α
+ (1 − α)

SRR SRT
ST R ST T

(cid:5)

(cid:6)

yR
yT

,

With the deﬁnitions above, we construct the k-nearest-
neighbor query graph as follows. Firstly, each query is rep-
resented as a data point on the manifold. We then connect

where fR and fT denote the ranking scores of points in set
R and T respectively, and yR and yT denote the prior on
the points in set R and T respectively.

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India39Since stop points never spread their scores to their nearby
points, we set SRT = ST T = 0, then we get the new iteration
equation for manifold ranking with stop points:

(cid:6)(t+1)

(cid:5)

(cid:5)

fR
fT

(cid:6)(t)

(cid:6)(cid:5)
(cid:6)

fR
fT

SRR 0
ST R 0

(cid:5)

= α
+ (1 − α)

yR
yT

.

(5)

As we turn the queries already selected for recommenda-
tion into stop points, the ranking scores of stop points are
no longer useful for us since the stop points would not be se-
lected later again. All we care about is the ranking scores of
the free points in set R. Therefore, we only need to compute
fR with the iteration equation

(t+1)
f
R

= αSRRf

(t)

R + (1 − α)yR,

(6)

where the parameter α speciﬁes the relative contributions
to the ranking scores from neighbors and the initial ranking
scores. It is important to know that, with the stop points
introduced, the new iteration algorithm still has a nice con-
vergence property, which means it can achieve a global stable
state. This convergence property is shown in Theorem 1.

Theorem 1. The sequence {f

(t)

R } converges to

Proof. Without loss of generality, suppose f (0)

R = yR. By

iteration equation (6), we have

−1

yR.

R = (1 − α)(I − αSRR)
∗
f
(cid:3)t−1

tyR + (1 − α)

f (t)
R = (αSRR)

i=0 (αSRR)iyR.

Let P = D−1

as follows:

RRWRR, P is the similarity transformation of SRR

SRR = D

−1/2
RR
RRWRRD

−1/2
RR WRRD
RR D−1
RR P D

−1/2
RR ,

= D1/2
= D1/2

−1/2
RR

hence P and SRR have the same eigenvalues.

Let λ be an eigenvalue of P , according to the Gershgorin circle

theorem, we have

|λ − Pii| ≤(cid:3)|R|

j=1,j(cid:3)=i |Pij|,

(cid:3)|R|
where |R| is the size of the free point set. Note that Pii = 0 and
j=1,j(cid:3)=i |Pij| ≤ 1, so we have |λ| ≤ 1. Since 0 ≤ α < 1 and
i=0(αSRR)i =

|λ| ≤ 1, then limt→∞(αSRR)t = 0, limt→∞
(I − αSRR)

−1. Hence, we have

(cid:3)t−1

f∗
R = lim

t→∞ f (t)

R = (1 − α)(I − αSRR)

−1yR.

We can use this closed form to compute the ranking scores
∗
f
R for all the free points directly. In large scale real-world
problems, however, an iterative algorithm is preferable due
to computational eﬃciency.

3.4 Recommendation Approach

Based on the ranking algorithm above, we ﬁnally obtain
our query recommendation approach. We ﬁrst construct a
k-nearest-neighbor query graph to model the query manifold
structure based on query logs and set all the query points
as free points. Giving an input query, we apply the pro-
posed ranking algorithm, i.e., manifold ranking with stop
points, over the query manifold until a global stable state
is achieved, and rank the queries according to their rank-
ing scores. The free point with the largest ranking score
(except the input query) will be selected as a recommen-
dation, and set as a stop point in the following iteration.
The process iterates until a pre-speciﬁed number of recom-
mendations acquired. The recommendation algorithm using
manifold ranking with stop points is shown in Algorithm 1.
Note that it would be time consuming if we directly apply
the ranking algorithm on the whole query manifold. Since
most queries are irrelevant to the input query, we can use
a width ﬁrst search strategy to construct a sub-manifold to
save the computational cost.

Algorithm 1 Query Recommendation using Manifold
Ranking with Stop Points
Input:
q - the input query
χ - all the other queries
K - recommendation size
S - normalized aﬃnity matrix of the query manifold
T - stop point set
R - free point set
Output: Top K recommendation query set U
Initialization: U = φ, T = φ, R = χ
1: for k = 1 . . . K do
2:
3:

obtain SRR based on S, T and R.
= αSRRf (t)
iterate f (t+1)
started with f (0)
R = 0, where α is a parameter in [0,1).
select the query qk with the largest ranking score (except
the input query) as a recommendation, U = U ∪ {qk}.
turn query qk from free point into stop point, T = T ∪{qk}
and R = R − {qk}.

R + (1 − α)yR until convergence

4:

5:

R

6: end for

4. EXPERIMENTS

4.1 Data Set

Our experiments are based on the Microsoft 2006 RFP
dataset4 which contains about 15 million queries (from US
users) that were sampled over one month in May, 2006.
For each query, the following details are available: a query
ID, the query itself, the user session ID, a time-stamp, the
clicked URL, the rank of that URL and the number of re-
sults. We cleaned the raw data by ignoring non-English
queries, converting letters into lower case, and replacing all
non-alphanumeric characters in each query with whitespace.
To further reduce the noise in clicks, the click-through be-
tween a query and a URL with a frequency less than 3
was removed. After cleaning, we obtained the click-through
data with totally 191,585 queries, 251,427 URLs and 318,947
edges. On average, each query clicks 1.66 distinct URLs, and
each URL is clicked by 1.27 distinct queries.

4http://research.microsoft.com/users/nickcr/wscd09/

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India40We randomly sampled 150 queries with frequencies be-
tween 700 and 15,000 for evaluation. This restriction is to
avoid the navigational queries (for which the query recom-
mendations may not be so useful) or very speciﬁc queries
(for which there are no recommendations) [3].
4.2 Baselines

To evaluate the performance of our approach, called Mani stop

for short, for query recommendation, we adopt four baselines
for comparison:

• Naive: It represents each query as a URL vector shown
in Equation (1) and directly measures the Euclidean
distance between queries. For a given query q, queries
with smallest distance scores are ranked higher and se-
lected as recommendations. Diﬀerent from other base-
line, this model only considers relevance for recommen-
dation without emphasizing diversity.

• Hitting time[18]: It recommends queries by using the
hitting time from candidate queries qs to the test query
q as a measure for ranking. The hitting time from node
i to node j in a random walk is the expected number
of steps before node j is visited starting from node i,
and it decreases when the number of paths from i to
j increases and the lengths of the paths decrease. The
basic idea of Hitting time is to boost long tail queries
for recommendation.

• MMR (Maximal Marginal Relevance) [5]: MMR mea-
sures the relevance and diversity independently and
provides a linear combination, called “marginal rele-
vance”, as the metric. Formally,

M M R

def
= Arg max
qi∈R\S
− (1 − λ) max
qj∈S

[λSim1(qi, q)

Sim2(qi, qj )],

(7)

where R is a set of candidate queries, S is the subset of
queries in R which is already recommended, Sim1 and
Sim2 are both similarity metrics between queries and
λ is a parameter for linear combination. When λ=1
it computes the standard relevance-ranked list, while
when λ=0 it computes a maximal diversity ranking.
For a given query q, MMR will iteratively recommends
queries with the largest “marginal relevance”.

• Grasshopper (Graph Random-walk with Absorbing StateS
that HOPs among PEaks for Ranking)[28]: The Grasshop-
per model leverages an absorbing random walk over
the query graph. The model starts with a teleporting
random walk P :

P = λ(cid:7)P + (1 − λ)1r

T

,

where (cid:7)P is the raw transition matrix, r is the user-

(8)

supplied initial distribution, and λ is a parameter to
control the tradeoﬀ. When λ = 1 it ignores the user-
supplied prior ranking r, while when λ = 0 it returns to
the ranking speciﬁed by r. The query with the largest
weight is selected as the ﬁrst recommendation, which is
then set as an absorbing state. The model then reruns
the random walk with absorbing states, and selects the
next query based on the expected number of visits to
each node before absorption.

4.3 Parameter Setting

To make a fair comparison, we need to tune the param-
eters for baseline approaches. Since both Naive and Hit-
ting time involve no parameter tuning, we only need to tune
parameter λ for two approaches (MMR and Grasshopper).
Based on one held-out data with respect to the metrics in-
troduced in the latter part of this section, we tested the
two approaches using 11 λ values (i.e. 0, 0.1, 0.2, ··· , 1) and
selected the best λ value for MMR (λ = 0.6) and Grasshop-
per (λ = 0.9). In our experiments, we ﬁxed the parameter
α in our method (Mani stop) at 0.99, consistent with the
experiments performed in [26, 27].

4.4 Examples of Recommendations

Here we ﬁrst present the comparison of recommendations
generated by our approach and baseline methods. Table 1
shows two samples from our test queries including their top
10 recommendations generated by ﬁve methods.

From the results we clearly see that Naive approach tends

to recommend closely related but somewhat redundant queries.
For example, for the test query ‘abc’, we can ﬁnd equivalent
recommendations like ‘abc tv’ and ‘abc television’, or rec-
ommendations sharing very close meaning like ‘abc news’,
‘abc breaking news’ and ‘abc world news’. We can also ﬁnd
redundant examples in the recommendations for the test
query ‘yamaha’, e.g., ‘yamaha motor’, ‘yamaha motorcycle’,
and ‘yamaha motorcycles’. Since Naive method only con-
siders relevance, it will inevitably produce many redundant
recommendations.

Meanwhile, we can easily ﬁnd that the three other base-
line approaches (Hitting time, MMR, Grasshopper) recom-
mended queries with better diversity. However, there is
still some redundancy in these approaches. For example,
for the test query ‘abc’, ‘abc tv’ and ‘abc television’ are
both recommended by Hitting time and MMR, while for
test query ‘yamaha’, ‘yamaha motor’ and ‘yamaha motor-
cycles’ are both recommended by Grasshopper. Moreover,
the recommendation provided by Hitting time may not so
closely related to the original query, e.g. recommendation of
‘espn sports’ with respect to ‘abc’, and recommendation of
‘bluebook motorcycle’ with respect to ‘yamaha’. It may also
bring up noisy long tail queries which hurt user experience,
like recommendation ‘yahama’ for query ‘yamaha’.

Among all these approaches, we observe that our Mani stop
approach obtains best performance, where more diverse as
well as closely related queries can be found in its recommen-
dation results.

4.5 Automatic Evaluation

Evaluating the quality of query recommendation is diﬃ-
cult, since there is usually no ground truth of recommen-
dations. Therefore, we ﬁrst conduct automatic evaluation
over query recommendation for more objective comparison
between diﬀerent approaches. The Open Directory Project
(ODP)5 and a commercial search engine (i.e., Google) are
leveraged in this automatic evaluation. Besides, there is no
evaluation metric that seems to be universally accepted as
the best for measuring the performance of algorithms that
aim to obtain diverse rankings [20]. Therefore, we adopt
the following three metrics (Relevance, Diversity and Q-

5http://www.dmoz.org/

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India41Table 1: Examples of query recommendations provided by diﬀerent approaches (top 10 results)

query

Naive
abc shows
abc television
abc tv

Hitting time
abc shows
abc television
associated builders and

MMR
abc shows
abc breaking news
associated builders and

Grasshopper
abc tv
abc news
abc family

Mani sink
abc tv
abc news
abc nightline

abc

abc news
abc breaking news

contractors

abc tv
news stories

contractors
abc nightline
abc tv

abc shows
abc breaking news

abc family
associated builders and

abc family
abc sports
abc world news
world news tonight
abc soap operas
yamaha america
yamaha motor corp
yamaha motor
yamaha motor co
yamaha motorcycle
yamaha motors
yamaha motorcycles
yamaha quads
yamaha snowmobiles
yamaha scooters

abc news
abc world news tonight
abc family channel
espn sports
abc nightline
yahama
yamaha america
yamaha motor corp
yamaha motor co
yamaha motor
yamaha motorcycle
yamaha snowmobiles
yamaha quads
yamaha outboard motors
bluebook motorcycles

abc television
abc family
abc sports
abc daytime
goodmorning america
yamaha america
yamaha atv parts
yamaha boat motors
yamaha motor corp
yamaha snowmobiles
yamaha motor
yamaha drums
yamaha guitars
yamaha motorcycles
yamaha atvs

nightline
goodmorning america
abc sports
abc daytime
national news
yamaha motor
yamaha america
yamaha motor corp
yamaha motorcycles
motorcycles
yamaha marine
yamaha atv
yamaha motorcycle parts
yamaha snowmobiles
yamaha quads

yamaha

contractors

abc shows
abc daytime
goodmorning america
abc sports
abc soap operas
yamaha motor
yamaha motor corp
yamaha america
yamaha marine
yamaha atv
yamaha snowmobiles
yamaha drums
yamaha guitars
yamaha quads
yamaha boat motors

measure) to help evaluate the relevance and diversity in rec-
ommendation.

Relevance. We adopt the same method used in [1] to
evaluate the relevance of recommended queries. Speciﬁcally,
we measure the relevance of two queries based on the sim-
ilarity between their corresponding categories provided by
ODP.

, let C and C

Given two queries q and q

denote the cor-
responding set of top k (k = 10 in our case) ODP categories
from Google Directory6, respectively. We deﬁne the similar-
ity between two categories c ∈ C and c
as the length
(cid:7)
of their longest common preﬁx l(c, c
) divided by the length
(cid:7)
of the longest category of c and c
. More concisely, denoting
the length of a category c with |c|, the similarity between
(cid:7)
two categories c and c

(cid:7) ∈ C

is

(cid:7)

(cid:7)

(cid:7)

(cid:7)
Sim(c, c

) =

|l(c, c
(cid:7)

)|

max{|c|,|c(cid:7)|} .

(9)

For instance, the similarity between the two categories
‘Arts/Television/News’ and ‘Arts/Television/Stations/North
America /United States’ is 2/5, since they share the com-
mon preﬁx ‘Arts/Television’ and the length of the longest
category is ﬁve. We then evaluate the relevance between two
queries by measuring the simialrity between the most similar
categories of the two queries among C and C
. Speciﬁcally,
the relevance between query q and q

is then deﬁned as

(cid:7)

(cid:7)

r(q, q

(cid:7)

) = max

(cid:7)
c∈C,c(cid:2)∈C(cid:2) Sim(c, c

).

(10)

For an input query q, the relevance of its recommendations

is then deﬁned as

rel(q) =

1|U|

(cid:8)

q(cid:2)∈U

(cid:7)

),

r(q, q

(11)

where U denotes the recommendation set and |U| is the
number of queries in U .

Diversity. We measure the diversity of recommended
queries based on the diﬀerences between their top ranked
search results provided by Google. Speciﬁcally, given two

6http://www.google.com/dirhp

queries q and q
, we compute the proportion of diﬀerent
URLs among their top k ( k = 10 in our case) search results
by

(cid:7)

(cid:2)

(cid:7)

) =

d(q, q

(cid:7)

1 − |o(q,q(cid:2))|
0

k

(cid:7)

if(q (cid:7)= q
);
otherwise,

(12)

where o(q, q
top k search results of query q and q
query q, the diversity of its recommendations is deﬁned as

) is the number of overlapped URLs among the
. Then for an input

(cid:7)

(cid:9)(cid:3)

(cid:3)
q∈U
|U|(|U| − 1)

q(cid:2)∈U d(q, q(cid:7))

div(q) =

(13)
where |U| is the number of queries in the recommended
query set U .

,

Note that we do not adopt an opposite measure, e.g., the
overlap of the top ranked search results, to evaluate the rel-
evance between two queries since that kind of relevance is
not desired by query recommendation. Obviously, it is not
reasonable to assign high relevance scores to queries with
similar search results as the input query and recommend
them to users. Therefore, we evaluate relevance with a top-
ical similarity deﬁned above.

Q-measure. The two metrics above measure the rele-
vance and diversity of recommendations respectively. It is
better to have a measure which can combine these two as-
pects to have a comprehensive evaluation of the eﬀectiveness
of recommendation methods. Here we borrow the F-measure
scheme and introduce a new metric to assess relevance and
diversity tradeoﬀ, referred as Q-measure. Formally, it is
deﬁned as the weighted harmonic mean of relevance and di-
versity:

Q(q) =

=

(1 + β2) · rel(q) · div(q)

β2 · rel(q) + div(q)
(1 + β2)
β2
div(q) + 1

rel(q)

,

(14)

where β is a parameter which can be used to control the
tradeoﬀ between relevance and diversity.
If the value of

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India42β > 1, Q-measure then emphasizes the diversity.
In our
experiment settings, we have β = 1 which equally empha-
size importance of relevance and diversity.

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

0.78

0.76

e
c
n
a
v
e
e
R

l

 
1

2

3

 

Naive
Hitting_time
MMR
Grasshopper
Mani_stop

8

9

10

4
7
Size of recommendations

5

6

Figure 1: Average Relevance of Query Recommen-
dation over Diﬀerent Recommendation Size under
Five Approaches.

y
t
i
s
r
e
v
D

i

0.9

0.85

0.8

0.75

0.7

0.65

 
2

 

Naive
Hitting_time
MMR
Grasshopper
Mani_stop

3

4

5

6

7

8

9

10

Size of recommendations

Figure 2: Average Diversity of Query Recommen-
dation over Diﬀerent Recommendation Size under
Five Approaches.

0.9

0.85

0.8

e
r
u
s
a
e
m
−
Q

 

Naive
Hitting_time
MMR
Grasshopper
Mani_stop

0.75

 
2

3

4

5

6

7

8

9

10

Size of recommendations

Figure 3: Average Q-measure of Query Recommen-
dation over Diﬀerent Recommendation Size under
Five Approaches.

4.5.1 Results and Discussion

The automatic evaluations were conducted over a variety
of recommendation size (up to top 10) and the results are
presented in Figure (1-3).

Figure 1 shows the average relevance values of query rec-
ommendations under ﬁve diﬀerent methods. As expected,
for all recommendation methods, the average relevance value
gradually decreases when the recommendation size increases.
We notice that Mani stop can achieve better performance
in relevance as compared with MMR and Naive methods,
which directly measure the relevance between queries in a
Euclidean space.
It demonstrates the eﬀectiveness of the
intrinsic query manifold in capturing the relevance between
queries. We also note that the relevance of Grasshopper
drops very quickly as the recommendation number increases,
which makes its relevance performance not very stable. Be-
sides, the relevance value of Hitting time is lower than that
of all the other four approaches on average. The major rea-
son is that Hitting time employs the expected hitting time
from other queries to the give query to rank recommenda-
tions, so that it can boost long tail queries for recommenda-
tion. However, the recommendations with low hitting time
to the given query may not be necessarily closely related,
and thus hurt the relevance performance.

The average diversity values of query recommendations
under the ﬁve diﬀerent approaches are shown in Figure 27.
Not surprisingly, the diversity of Naive is the lowest one
in the ﬁve approaches, since Naive only focuses on recom-
mending queries according to their relevance with the input
query. Hitting time obtains better diversity by boosting the
long tail queries for recommendation, but the improvement
is limited. Both MMR, Grasshopper and Mani stop can re-
ceive higher diversity value by explicitly address the diver-
sity in ranking. Among these three approaches, Mani stop
obtains the highest diversity on average (0.872) by introduc-
ing the stop points into query manifold.

Figure 1 and Figure 2 show the relevance and diversity
of diﬀerent approaches, respectively. To better evaluate the
overall eﬀectiveness of diﬀerent approaches, we show the Q-
measure (a weighted harmonic mean of relevance and di-
versity) in Figure 3. From the results, we can see that
Hitting time works better than the Naive approach. Both
MMR and Grasshopper can further outperform Hitting time,
while Grasshopper obtains better results than MMR by lever-
aging a “soft” penalization. Among the ﬁve approaches, the
proposed Mani stop approach consistently outperforms the
other four baseline approaches in terms of Q-measure. We
conduct t-test (p − value <= 0.05) over the results and
ﬁnd that the performance improvement is signiﬁcant as com-
pared with the baselines.

These above results clearly conﬁrm that our Mani stop
approach can eﬀectively generate highly diverse as well as
closely related query recommendations.

4.5.2 Impact of Parameter k

As our approach is based on the query manifold structure
(i.e. k-nearest-neighbor query graph), we also study the im-
pact of parameter k, which deﬁnes the number of nearest
neighbors when constructing the graph and plays an impor-
tant role in terms of both eﬀectiveness and eﬃciency.

7The diversity at one recommendation is not shown here due
to that it is meaningless with the diversity metric.

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India43e
r
u
s
a
e
m
−
Q

0.9

0.895

0.89

0.885

0.88

0.875

0.87

0.865

0.86

0.855

0.85

 
10

Q−measure@3
Q−measure@5
Q−measure@8
Q−measure@10

20

30

40

50

60

70

80

90

100

k

Figure 4: Q-measure over Diﬀerent Recommenda-
tion Size (3,5,8,10) with k varying from 10 to 100.

Figure 4 shows the performance of our approach in terms
of Q-measure under diﬀerent recommendation sizes when
varying k from 10 to 100 . The X-axis is parameter k, while
the Y-axis is the Q-measure value measured by the auto-
matic evaluation. We can see that as increasing the number
of k, the Q-measure value exhibits a rise, until k = 50. It
indicates that, if value of k is too small, the relationship
between queries may be not be well revealed by the mani-
fold structure. If k > 50, there is a slight decrease of the
performance. The reason is that when k > 50, the possi-
bility that noisy queries are becoming added into queries’
neighborhood is increased, which will potentially aﬀect the
quality of query recommendation results.

4.6 Manual Evaluation

We further conduct manual evaluation for comparing dif-
ferent recommendation methods. For each query, we create
a recommendation pool by merging the topmost (e.g., 10 in
our work) recommendations from all the methods. Then we
invite 3 human judges, with or without computer science
background, to label the recommendations in the pool man-
ually. For each test query, the human judges are required
to identify the relevant recommendations and further group
them into clusters according to their search intent.

We create a label tool as shown in Figure 5 to help ease
the labeling process. For each test query, the human judges
are presented with the recommendation pool (the left panel),
and the search results of the query from a commercial search
engine (the right panel). When labeling a recommendation,
the human judge ﬁrst takes a relevance assessment on the
recommendation. If the recommendation is irrelevant to the
test query at all, he just skips it. If relevant, he then needs
to compare this recommendation with previous labeled rec-
ommendations and mark it in the same column as the one
with the same search intent, or mark it in a new column (as
a new intent), otherwise. The human judges are allowed to
use a search engine of their choice for better understanding
the meaning of the query and the recommendations. Note
here there are three major cases in which two queries are
considered as sharing the same search intent: (1) they are
equivalent expressions, e.g., ‘post code’ and ‘zip code’; (2)
one query is the subconcept of the other, e.g. ‘abc breaking
news’ and ‘abc news’; (3) they are closely related with each
other and share many similar search results, e.g. ‘travel di-

 

rections’ and ‘travel maps’. Since the labeling task is costly,
we just randomly pick 50 queries for manual evaluation.

I(cid:8)

4.6.1 Evaluation Metrics

With the human labeled data, here we evaluate the quality
of the recommendations produced by diﬀerent approaches
(1) α-normalized Dis-
using the following two measures:
counted Cumulative Gain (α-nDCG) [6] which has been widely
used in the TREC Web track 8 diversity task; and (2) Intent-
Coverage.

α-nDCG. The α-nDCG, which rewards diversity in rank-
ing,
is a new version of the nDCG [11], the normalized
Discounted Cumulative Gain measure. When α = 0, the
α-nDCG measure corresponds to the standard nDCG, and
when α is closer to 1, the diversity is rewarded more in the
metric. The key diﬀerence between α-nDCG and nDCG is
that they use diﬀerent gain value. For each recommenda-
tion, the gain value G(k) of α-nDCG is deﬁned as follows:

Ji(k)(1 − α)

Ci(k−1)

,

i=1

G(k) =

(15)
where Ci(k − 1) is the number of relevant recommendations
found within the top k − 1 recommendations for intent i,
Ji(k) is a binary variable indicating whether the recommen-
dation at rank k belongs to intent i or not, and I is the total
number of unique intents for each test query. The computa-
tion of α-nDCG exactly follows the procedure described in
[6] with α = 0.5.

Intent-Coverage. The Intent-Coverage measures the
proportion of unique intents covered by the top k recom-
mended queries for each test query. Since each intent repre-
sents a speciﬁed user information need, higher Intent-Coverage
indicates larger probability to satisfy diﬀerent users. Note
that the Intent-Coverage is diﬀerent from the diversity mea-
sure used in automatic evaluation, since only relevant rec-
ommendations to the test query will be considered in Intent-
Coverage. Therefore, Intent-Coverage can better reﬂect the
diversity quality of recommendations than the diversity mea-
sure in automatic evaluation. The Intent-Coverage is for-
mally deﬁned as:

I(cid:8)

Intent − Coverage(k) =

1
I

Bi(k),

(16)

i

where Bi(k) is a binary variable indicating whether the in-
tent i found within the top k recommendations or not, and
I is the total number unique intents for each test query.

In our experiments, we compare the performance of diﬀer-
ent methods in terms of α-nDCG@5, α-nDCG@10, Intent-
Coverage@5, and Intent-Coverage@10. Table 2 reports the
performance of diﬀerent recommendation approaches under
manual evaluation. The numbers in the parentheses are the
relative improvements compared with baseline methods.

From Table 2, we can see that Naive obtains the lowest
Intent-Coverage and also shows a poor overall performance
as measured by α-nDCG, since it only consider the relevance
in recommendation. Hitting time approach obtains better
performance than Naive approach as it implicitly addresses
the diversity in query recommendation by boosting long tail
queries in top ranked results. Both MMR and Grasshopper
approaches can further outperform Hitting time approach

8TREC Web Track: http://plg.uwaterloo.ca/ trecweb

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India44Figure 5: The label tool for query recommendation. The left panel shows the recommendation pool of a
given input query and the right panel shows the the search results of the query from a commercial search
engine. The columns A i in the left panel denotes the i-th intent of the test query. A recommended query
belonging to the i-th intent of the test query will be marked in the corresponding column.

Table 2: Performance of recommendation results over a sample of queries under ﬁve diﬀerent approaches. Per-
formance metrics α-nDCG@5, α-nDCG@10, Intent-Coverage@5 and Intent-Coverage@10 are shown. Num-
bers in parentheses indicates relative % improvement over Naive/Hitting time/MMR/Grasshopper. Paired
t-tests are performed, and results which show signiﬁcant improvements (p-value < 0.05) are marked ‡.

Naive
Hitting time

MMR

Grasshoppper

Mani stop

α-nDCG@5
0.717
0.770
‡
/*/*/*)
(7.4
0.799
‡
(11.4
0.794
‡
(10.7
0.838
‡
(16.9

‡
/8.8

/3.8/*/*)

/3.1/-0.6/*)

‡
/4.9

‡
/5.5

)

/*/*/*)

α-nDCG@10
0.689
0.738
‡
(7.1
0.742
‡
/0.5/*/*)
(7.7
0.768
‡
‡
/4.1/3.5
(11.5
0.806
‡
‡
(17
/8.6

‡
/9.2

/*)

‡
/4.9

/*/*/*)

Intent-Coverage@5
0.300
0.348
‡
(16
0.384
‡
‡
(28
/10.3
0.373
‡
(24.3
0.436
‡
(45.3

/7.2/-2.9/*)
‡
/13.5

‡
/25.3

/*/*)

‡
/16.9

/*/*/*)

Intent-Coverage@10
0.536
0.585
‡
(9.3
0.585
‡
(9.1
/0/*/*)
0.616
‡
(14.9
0.665
‡
(24.1

‡
/13.5

‡
/13.7

/5.1/5.3/*)

‡
/8

)

)

)

on α-nDCG by explicitly addressing recommendation diver-
sity. It seems that as a “soft” version of MMR, Grasshopper
can achieve better performance than MMR when recommen-
dation size is large. Compared with the four baseline meth-
ods, our Mani stop approach achieves the best performance
in terms of all measures, which is consistent with results re-
ported in the automatic evaluation. We also conduct the
t-test (p − value < 0.05) and ﬁnd that the improvements
over all baseline methods are signiﬁcant. It shows that by
exploiting the intrinsic global query manifold structure and
employing manifold ranking with stop points, we can rec-
ommend highly diverse as well as closely related queries.

5. CONCLUSIONS

In this paper, we address the problem of recommending
relevant and diverse queries. We propose a novel uniﬁed
model, named manifold ranking with stop points, to solve
this problem. Our approach leverages a manifold ranking
process over query manifold, which can naturally make full
use of the relationships among queries to ﬁnd relevant and
salient queries. Meanwhile, we introduce the stop points
into query manifold to capture the diversity during the rank-
ing process. In this way, our approach can generate query
recommendations by simultaneously considering both diver-
sity and relevance between queries in a uniﬁed way. Like

traditional manifold ranking algorithm, the new proposed
ranking approach also shows a nice convergence property.

Both automatic and manual evaluations are conducted to
demonstrate the eﬀectiveness of our approach. The exten-
sive empirical results clearly show that our approach can
outperforms all the four baseline methods (Naive, Hitting time,
MMR and Grasshopper) in recommending highly diverse as
well as closely related query recommendations.

For the future work, we would like to explore diﬀerent
ways for modeling the query manifold structure and investi-
gate how it may aﬀect the recommendation performance. It
would also be interesting to apply the proposed approach to
a variety of applications, e.g., image retrieval, expert ﬁnd-
ing, and product recommendation, where both diversity and
relevance are demanded in ranking.

6. ACKNOWLEDGMENTS

This research work was funded by the National Natural
Science Foundation of China under Grant No. 61003166 and
Grant No. 60933005.

7. REFERENCES
[1] R. Baeza-Yates and A. Tiberi. Extracting semantic
relations from query logs. In Proceedings of the 13th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 76–85, 2007.

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India45[2] D. Beeferman and A. Berger. Agglomerative clustering
of a search engine query log. In Proceedings of the sixth
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 407–416, 2000.
[3] P. Boldi, F. Bonchi, C. Castillo, D. Donato, and

S. Vigna. Query suggestions using query-ﬂow graphs.
In Proceedings of the 2009 workshop on Web Search
Click Data, pages 56–63, 2009.

[4] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and

H. Li. Context-aware query suggestion by mining
click-through and session data. In Proceeding of the
14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 875–883,
2008.

[5] J. Carbonell and J. Goldstein. The use of mmr,

diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st
annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 335–336, 1998.

[6] C. L. Clarke, M. Kolla, G. V. Cormack,

O. Vechtomova, A. Ashkan, S. B¨uttcher, and
I. MacKinnon. Novelty and diversity in information
retrieval evaluation. In Proceedings of the 31st annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 659–666,
2008.

[7] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma.

Probabilistic query expansion using query logs. In
Proceedings of the 11th international conference on
World Wide Web, pages 325–332, 2002.

[8] H. Deng, I. King, and M. R. Lyu. Entropy-biased

models for query representation on the click graph. In
Proceedings of the 32nd international ACM SIGIR
conference on Research and development in
information retrieval, pages 339–346, 2009.

[9] J. Guo, G. Xu, H. Li, and X. Cheng. A uniﬁed and

discriminative model for query reﬁnement. In
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 379–386, 2008.

[10] J. He, M. Li, H.-J. Zhang, H. Tong, and C. Zhang.

Manifold-ranking based image retrieval. In Proceedings
of the 12th annual ACM international conference on
Multimedia, pages 9–16, 2004.

[11] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, 2002.

[12] R. Jones, B. Rey, O. Madani, and W. Greiner.

Generating query substitutions. In Proceedings of the
15th international conference on World Wide Web,
pages 387–396, 2006.

conference on Artiﬁcial intelligence, pages 1189–1194,
2008.

[16] H. Ma, H. Yang, I. King, and M. R. Lyu. Learning
latent semantic relations from clickthrough data for
query suggestion. In Proceeding of the 17th ACM
conference on Information and knowledge
management, pages 709–718, 2008.

[17] Q. Mei, J. Guo, and D. Radev. Divrank: the interplay

of prestige and diversity in information networks. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 1009–1018, 2010.

[18] Q. Mei, D. Zhou, and K. Church. Query suggestion
using hitting time. In Proceeding of the 17th ACM
conference on Information and knowledge
management, pages 469–477, 2008.

[19] R. Ohbuchi and T. Shimizu. Ranking on semantic

manifold for shape-based 3d model retrieval. In
Proceeding of the 1st ACM international conference on
Multimedia information retrieval, pages 411–418, 2008.

[20] F. Radlinski, P. N. Bennett, B. Carterette, and

T. Joachims. Redundancy, diversity and
interdependent document relevance. SIGIR Forum,
43(2):46–52, 2009.

[21] M. Theobald, R. Schenkel, and G. Weikum. Eﬃcient
and self-tuning incremental query expansion for top-k
query processing. In Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 242–249,
2005.

[22] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based

topic-focused multi-document summarization. In
Proceedings of the 20th international joint conference
on Artiﬁcal intelligence, pages 2903–2908, 2007.

[23] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user

queries of a search engine. In Proceedings of the 10th
international conference on World Wide Web, pages
162–168, 2001.

[24] J. Xu and W. B. Croft. Query expansion using local
and global document analysis. In Proceedings of the
19th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 4–11, 1996.

[25] M. Zhang. Enhancing diversity in top-n

recommendation. In Proceedings of the third ACM
conference on Recommender systems, pages 397–400,
2009.

[26] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and

B. Sch¨olkopf. Learning with local and global
consistency. In Proceedings of the 17th Annual
Conference on Neural Information Processing
Systems, 2003.

[13] J. P. Kelly and D. Bridge. Enhancing the diversity of

[27] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and

conversational collaborative recommendations: a
comparison. Artif. Intell. Rev., 25:79–95, April 2006.

[14] R. Kraft and J. Zien. Mining anchor text for query
reﬁnement. In Proceedings of the 13th international
conference on World Wide Web, pages 666–674, 2004.
[15] L. Li, Z. Yang, L. Liu, and M. Kitsuregawa. Query-url

bipartite based approach to personalized query
recommendation. In Proceedings of the 23rd national

B. Sch¨olkopf. Ranking on data manifolds. In
Proceedings of the 17th Annual Conference on Neural
Information Processing Systems, 2003.

[28] X. Zhu, A. B. Goldberg, J. V. Gael, and

D. Andrzejewski. Improving diversity in ranking using
absorbing random walks. In Proceedings North
American Chapter of the Association for
Computational Linguistics - Human Language
Technologies (NAACL HLT), pages 97–104, 2007.

WWW 2011 – Session: RecommendationMarch 28–April 1, 2011, Hyderabad, India46