A Stochastic Learning-To-Rank Algorithm and its

Application to Contextual Advertising

∗

Maryam Karimzadehgan
Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana, IL 61801

mkarimz2@illinois.edu

Wei Li, Ruofei Zhang, Jianchang Mao

Advertising Sciences, Yahoo! Labs

4401 Great America Parkway

Santa Clara, CA 95054

{lwei,rzhang,jmao}@yahoo-inc.com

ABSTRACT
This paper is concerned with the problem of learning a
model to rank objects (Web pages, ads and etc.). We pro-
pose a framework where the ranking model is both opti-
mized and evaluated using the same information retrieval
measures such as Normalized Discounted Cumulative Gain
(NDCG) and Mean Average Precision (MAP). The main
diﬃculty in direct optimization of NDCG and MAP is that
these measures depend on the rank of objects and are not
diﬀerentiable. Most learning-to-rank methods that attempt
to optimize NDCG or MAP approximate such measures so
that they can be diﬀerentiable. In this paper, we propose
a simple yet eﬀective stochastic optimization algorithm to
directly minimize any loss function, which can be deﬁned
on NDCG or MAP for the learning-to-rank problem. The
algorithm employs Simulated Annealing along with Simplex
method for its parameter search and ﬁnds the global optimal
parameters. Experiment results using NDCG-Annealing al-
gorithm, an instance of the proposed algorithm, on LETOR
benchmark data sets show that the proposed algorithm is
both eﬀective and stable when compared to the baselines
provided in LETOR 3.0. In addition, we applied the algo-
rithm for ranking ads in contextual advertising. Our method
has shown to signiﬁcantly improve relevance in oﬄine evalu-
ation and business metrics in online tests in a real large-scale
advertising serving system. To scale our computations, we
parallelize the algorithm in a MapReduce framework run-
ning on Hadoop.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
models

General Terms
Algorithms, Experimentation

Keywords
Learning to Rank, Simulated Annealing, Simplex Algorithm,
NDCG-Annealing, Contextual Advertising, NDCG, IR Mea-
sures.
∗

Work conducted during an internship at Yahoo! Labs.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

1.

INTRODUCTION

Ranking is the central part of many applications including
document retrieval, recommender systems, advertising and
so on. Many models for ranking functions have been pro-
posed previously, including vector space model [43], proba-
bilistic model [41] and language model [35]. Web page rank-
ing previously has been done on a manually tuned ranking
function, e.g., BM25 [42]. Manual parameter tuning is usu-
ally very diﬃcult especially when there are many parame-
ters. In recent years, supervised learning methods (learning-
to-rank methods) have been devoted to automatically learn
an eﬀective ranking function from training data. Learning-
to-rank methods (e.g.,
[6, 8, 9, 15, 17, 47]) have been pro-
posed to optimize a ranking function that incorporates a
variety of features and avoids tuning a large number of pa-
rameters empirically. In document retrieval, the ranking re-
sults are usually measured in terms of MAP (Mean Average
precision) [2] and NDCG (Normalized Discounted Cumula-
tive Gain) [21] which are non-diﬀerentiable.

Learning to rank when applied to document retrieval is
as follows: Training data consists of a set of queries, a set
of documents for each query and a label for each document
which shows the degree of relevancy of the document to its
corresponding query. Each query-document pair is repre-
sented by a feature vector. A ranking function is then cre-
ated using the training data and the learned model can pre-
dict the ranking list in the training data well. In retrieval
ranking (i.e., the test stage), given a query, the ranking func-
tion assigns a score to each document and then documents
are ranked in descending order according to the assigned
scores.

Several methods of learning to rank have been proposed
in the machine learning community. Most existing methods
for document retrieval (e.g.,
[6, 7, 17, 19, 22, 31]) are de-
signed to optimize loss functions loosely related to the IR
performance measures, not loss functions directly based on
the measures. Recently, there have been some attempts to
directly optimize the performance measures in terms of the
IR evaluation measures. Some of these methods minimize
upper bounds of the loss function deﬁned on the IR mea-
sures [47, 50]. The others either approximate the IR mea-
sures with functions that are easy-to-handle [12, 44] or use
especially designed methods for optimizing the non-smooth
IR measures [1, 49].

In this paper, we propose a stochastic algorithm to learn
a ranking function that minimizes the loss function deﬁned
directly on the IR evaluation measures including NDCG
and MAP. The algorithm is based on Simulated Anneal-

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India377ing [23] framework which uses a modiﬁcation of downhill
Simplex method [32] for ﬁnding the next candidate to move,
to minimize the loss function. The method of Simulated
Annealing is a global optimization technique for large-scale
problems especially for the problems where the global mini-
mum is hidden among many local minima. Each step of the
algorithm replaces the current solution by a nearby solution
which is chosen with a probability that depends on the dif-
ference between their corresponding function values and on
a global parameter T (i.e., temperature) which is gradually
decreased. The loss function in our method is application
speciﬁc; we do not have certain restrictions on the properties
of the loss function, such as continuity and diﬀerentiability.
As an instance of the method, we deﬁne the loss function on
NDCG measure for ranking in this paper, but other metrics
can also be applied directly. We call the proposed method
NDCG-Annealing algorithm. The NDCG-Annealing algo-
rithm is general to be applied to both linear and non-linear
ranking functions at test time.

We compare the NDCG-Annealing algorithm (with lin-
ear ranking function described in section 3) with baselines
provided in the LETOR 3.0 datasets. The results on seven
datasets in LETOR 3.0 show that the NDCG-Annealing al-
gorithm can outperform the baselines and it is more stable.
We also applied the algorithm to rank ads (with non-linear
ranking function described in section 7) in contextual ad-
vertising in both online and oﬄine scenarios. In contextual
advertising, ads are shown to a user on a Web page; the
user clicks on an ad, visits the advertiser site and then con-
verts. Ads are ranked to optimize diﬀerent objectives, such
as relevance, Click-through-Rate (CTR) [10] or revenue. In
this paper, our ranking problem considers how to rank ads
for a given page to optimize relevance and CTR, which con-
sequently also increase revenue. Matching Web pages with
ads is done using the standard Cosine similarity metric. The
ranking results are ﬁrst evaluated by editors based on rele-
vancy between (page, ad) pairs. Our approach is shown to
signiﬁcantly improve the relevance over the baseline. Fur-
ther evaluation on a real large-scale advertising serving sys-
tem shows that the algorithm improves CTR and Revenue
Per 1000 Impressions (RPM) in online bucket tests. Also, to
scale our computations (building an inverted index, extract-
ing features and training the model), we parallelize our algo-
rithm in a MapReduce framework running on Hadoop [16].
The rest of the paper is organized as follows: We discuss
related work in Section 2. The problem formulation is given
in Section 3. We then present our NDCG-Annealing algo-
rithm in Section 4. We discuss experiment design and ex-
periment results on LETOR 3.0 dataset in Sections 5 and 6,
respectively. Section 7 presents the application of the pro-
posed algorithm in contextual advertising. Finally, we con-
clude the paper in Section 8.

2. RELATED WORK

“Ranking”is a key problem of many applications especially
to create a model that can sort documents for a given query.
Previous models in Information Retrieval such as BM25 [42]
and language model [35] only have a few parameters to tune.
As the ranking models become more sophisticated by con-
sidering more relevant features for documents and queries,
how to manually tune the parameters becomes a challenge.
Recently, learning-to-rank methods have been proposed to
automatically learn a ranking function using labeled train-

ing data. Three main approaches have been proposed:
1) Pointwise approaches learn a ranking function to min-
imize a loss function that is deﬁned on individual document
relevance judgment and ranking score which could be based
on regression [12] or classiﬁcation [25, 31]. One of the prob-
lems with these approaches is that the training model might
be biased towards queries with more document pairs [8].
2) Pairwise approaches learn a ranking function to min-
imize a loss function that is deﬁned on pair-wise prefer-
ences. The ranking problem is then transformed into the
binary classiﬁcation problem. Examples of such a model
are RankSVM [19, 22], RankBoost [17], RankNet [6] and
FRank [15]. The main problem with these methods is that
the objective function is formalized as minimizing errors in
classiﬁcation rather than minimizing errors in ranking of
documents. To overcome such a problem, the ﬁnal approach,
i.e., Listwise approaches consider document lists instead of
document pairs as instances in learning.
3) Listwise approaches can be classiﬁed into two cat-
egories: the ﬁrst one directly optimizes the IR measures.
Optimizing IR measures directly is diﬃcult [7] since they
depend on the rank and are not diﬀerentiable. To avoid the
computational diﬃculty, in [7], authors perform gradient de-
scent on a smoothed version of the objective function, how-
ever, the objective function is calculated implicitly through
its gradients. In [46], optimizing the expectation of IR mea-
sures is proposed, however the Monte Carlo sampling is used
to address the intractable task of computing the expectation
which might not be a good approximation for queries with a
large number of documents. SVM-MAP [50] also relaxes the
MAP metric by incorporating it into the constrains of SVM.
Since SVM-MAP optimizes MAP, it can only work with bi-
nary relevance and is not suitable for more than two levels
of relevance judgments. AdaRank [47] uses boosting to op-
timize NDCG, for that they deploy the NDCG value of each
query in the current iteration as the weight for that query in
constructing the weak ranker. The convergence of AdaRank
is conditional and not guaranteed. NDCG-Boost [45] is a
recent algorithm that optimizes the expectation of NDCG
over all the possible permutations of documents. Some other
approaches for directly optimizing IR measures use Genetic
Programming [1, 49] or approximate the IR measures with
the functions that are easy-to-handle [44, 12]. Our method
is similar to these methods as we directly optimize the IR
evaluation measure (i.e., NDCG) by using the Simulated
Annealing which uses a modiﬁcation of downhill Simplex
method for the next candidate move to ﬁnd the global min-
imum.

The second category of listwise algorithms deﬁnes loss
function as an indirect way to optimize the IR evaluation
metrics. For example, RankCosine [36] uses the cosine sim-
ilarity between the ranking list and the ground truth as a
query level loss function. ListNet [9] uses the KL-divergence
as a loss function by deﬁning a probability distribution.
There is a problem with these approaches that optimizing
the listwise loss function does not necessarily result in the
optimization of IR metrics.

For a complete survey about all learning-to-rank methods,

a reader is referred to [26].

Also, our work is related to other applications that use
Simulated Annealing including clustering [28, 48], classiﬁca-
tion [4] and [33] which uses the Simulated Annealing algo-
rithm to rank Web objects. The work in [33] calculates the

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India378object popularity scores of domain-dependent Web objects
based on their Web popularity and the object relationship
graph. They automatically assign a popularity propagation
factor for each type of object relationship. For the parame-
ter estimation, they adopt Simulated Annealing algorithm.
Also, authors in [34] have applied Simulated Annealing al-
gorithm to select a framework that selectively applies an ap-
propriate ranking function for each query. The work in [20]
adopts the algorithm for object detection in computer vision
by proposing a collaborative learning algorithm enhanced by
a Simulated Annealing step in combination with AdaBoost
algorithm. Our work is similar to all previous work in a
sense that we also use Simulated Annealing algorithm, how-
ever we combine it with the downhill Simplex algorithm and
systematically examine the algorithm in comparison with
other state-of-the-art algorithms, and apply it in a diﬀerent
application, i.e., ranking ads in contextual advertising.

Since we consider ranking in advertising as a direct appli-
cation of our method, we review the related work in contex-
tual advertising. Some previous work has focused on devel-
oping methods to match ads to pages or users’ search queries
(e.g.
[5, 39]). In these studies, the problem of matching ads
with pages is transformed into a similarity search in a vector
space. Each page or ad is represented as a vector of fea-
tures, which include words, along with higher-level semantic
classes [5]. Our method is similar to these methods in a sense
that our ranking function is based on vector space model,
however diﬀers in that we learn weights for the importance of
diﬀerent parts of the page and ad by using NDCG-Annealing
algorithm. Indeed, the work in [10] learns weights for each
word in a page, however their method introduces a lot of
parameters which makes it harder to scale up for real ad
serving system. Some other studies have examined the use
of semantic classes to match pages and ads for a common set
of semantic classes [5]. In addition, [37] introduces a page-ad
probability model in which semantic relationships between
page terms and ad terms are modeled with hidden classes.
Another work [30] applies translation techniques to improve
the ad-page matching. A recent study [3] close to our work
considers a ranking method for conversions. However, we
propose a learning method to maximize Click-through-Rate
(CTR) for impressions. Also, the work in [24] applies Ge-
netic Programming to learn ranking functions that select the
most appropriate ads.

Another line of work uses click data to produce a CTR
estimate for an ad, independent of the page or query in the
Sponsored Search scenario [38, 40]. These studies make a
simplifying assumption that the ads are selected by match-
ing the bid phrase to a phrase from the page or query in
Sponsored Search and therefore to select the most clickable
ads, one only needs to estimate the CTR on the ads with
the matching bid phrase.

We assume there is a collection of n queries for training,
and for each query qk, we have a
¯

q1, . . . , qn

¯

˘

3. PROBLEM FORMULATION
i.e., Q =
collection of mk documents, i.e., Dk =
where relevance of dk
¯n
Thus, the training set can be represented as
S =
k=1.

˘
(qk, Dk, rk)

i is given by a vector rk = (rk

1 , . . . , rk

mk ).

˘
dk
i , i = 1, . . . , mk

Linear feature-based ranking functions are of the form:

Fλ(d, q) =

X

j

λjfj(q, d)

(1)

where q is a query, d is a document, fj(q, d) is a feature
function and λj is the weight assigned to feature j. The
ranking function Fλ(d, q) is linear with respect to the model
parameters, λj’s. This model is simple in that it can combine
many diﬀerent features in a straightforward manner. There
are diﬀerent learning methods to ﬁnd the parameters that
optimize retrieval eﬀectiveness for linear ranking functions.
For our case, we use NDCG-Annealing algorithm to learn
the model parameters. Note that this algorithm is a gen-
eral approach that can also be applied to non-linear ranking
functions with respect to the model parameters (as we see
in section 7). Then, for test queries, our linear ranking func-
tion Fλ(d, q) takes a document-query pair (d, q) and outputs
a real number as a score. Documents are then ranked ac-
cording to this score.

In the next section, we describe our objective function.

3.1 Objective Function

The Discounted Cumulative Gain (DCG) score [21] is a
commonly used measure for multi-level relevance judgments.
It has a logarithmic position discount, i.e., the beneﬁt of
seeing a relevant document at position i is
The N DCG(Q,Fλ) for ranking function Fλ(q, d) is de-

log2(i+1) .

1

ﬁned as:

N DCG(Q,Fλ) =

nX

k=1

1
n

1
Zk

mkX

i=1

k

i − 1
2r
log(1 + jk
i )

(2)

where jk
i

is the rank of document dk

i within Dk for query
qk, Zk is the normalization factor [21] and n is the total
number of queries.

NDCG measure is usually truncated at a particular trun-
cation (rank) level J (e.g., the ﬁrst 10 retrieved documents),
denoted by NDCG@J by ignoring the documents after rank
J to emphasize the importance of the ﬁrst retrieved docu-
ments.

We deﬁne the loss function as:

L(Q,Fλ) = 1 − N DCG@10(Q,Fλ)

(3)

where 1 indicates the ideal NDCG value and N DCG@10(Q,Fλ)
is the NDCG value for all queries given the learned param-
eters λj’s. The learning process minimizes the aforemen-
tioned loss function by iteratively ﬁnding the best λj’s that
minimize the loss function. We consider NDCG@10 (with-
out loss of generality) to emphasize the importance of the
ﬁrst ten documents but any other value J can also be used.
In the next section, we present NDCG-Annealing algo-

rithm for learning the model parameters, i.e., λj’s.

4. PARAMETER ESTIMATION

4.1 NDCG-Annealing Algorithm

In this section we describe the details of integrating Sim-
ulated Annealing and downhill Simplex method in the opti-
mization framework to minimize the loss function associated
directly to NDCG measure.

The idea of Simulated Annealing comes from Metropo-
lis et al. [29] in which the authors described the cooling of

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India379material in a heat bath. The idea is that at high tempera-
tures, molecules of a liquid move freely and if the liquid is
cooled slowly, the mobility is lost and the atoms can then
line up. The essence of the whole process is slow cooling.
Later, Kirkpatrick et al. [23] applied the idea of Metropolis
algorithm to optimization problems by searching for feasible
solutions and converging to an optimal solution.

For estimating the model parameters, we use Simulated
Annealing algorithm [14], while the parameter search at each
temperature applies downhill Simplex method [32].
Sim-
ulated Annealing algorithm is applicable to optimizing N-
dimensional spaces (e.g., N feature spaces) by ﬁnding the
minimum of some function, i.e., L(Q,Fλ) where λ is a N-
dimensional vector, and L is the objective function. There
are some components associated with the algorithm:

1. There is a generator of random changes which is a
procedure that takes a random step from λ to λ + Δλ.
2. An objective function L(Q,Fλ) (analogy of energy)

whose minimization is the goal of the procedure.

3. There is a control parameter called Temperature (T),
with an annealing schedule by which tells how it is
lowered from high to low values.

The most important of these components is Δλ. There are
many diﬀerent schemes for choosing Δλ. One eﬃcient way
of doing Simulated Annealing minimization on continuous
control spaces is to use a modiﬁcation of downhill Simplex
method. This method only requires function evaluations,
not derivatives. A simplex is a geometrical ﬁgure which in
N dimensions, consists of N + 1 points. In N-dimensional
minimization, the downhill Simplex algorithm starts with a
guess, i.e., (N+1) points, which deﬁne an initial simplex.
The algorithm then makes its own way downhill through an
N-dimensional topology until it ﬁnds a minimum. For doing
that, the downhill Simplex method takes a set of steps. Most
steps just move the point of the simplex where the objec-
tive value is largest (highest point) to a lower point with the
smaller objective value. These steps are called reﬂections
which conserve the volume of the simplex. When it reaches
a valley ﬂoor, the algorithm contracts itself in the transverse
direction and ﬁnally it contracts itself in all directions (mul-
tiple contractions) which results in pulling itself in around
the lowest point (lowest objective value) [32]. Figure 1 shows
appropriate sequences of such steps.

Downhill Simplex method approximates the size of the re-
gion that can be reached at temperature T, and it samples
new points. If the temperature T is reduced slowly enough,
the downhill Simplex method shrinks into the region con-
taining the lowest minimum value. Also, the starting tem-
perature T0 must be hot enough to allow a move to any
neighborhood state. On the other hand, if the starting tem-
perature is set too high, the search can move to any neighbor
which results in a random search. So, ﬁnding the correct
starting temperature is a key issue in this algorithm. Also,
the way in which we decrement the temperature is critical
to the success of the algorithm. It is stated that we should
allow enough iterations at each temperature [14]. For the
choice of cooling down of annealing scheduling, we tried the
following:

Where K is the budget of total moves, and reduce T after
k moves to the above value and k is the cumulative number
of moves thus far, α is a constant. Larger values of α spend
more iterations at lower temperature. T0 is the initial tem-
perature value. We set K = 1000 and we tune α and T0
based on validation datasets.

The algorithm also needs a probability distribution to se-
lect a move that minimizes the objective function. The fol-
lowing probability distribution indicates that even at low
temperature, there is a chance of a system to be in a high
energy state which indicates that there is a chance for the
system to get out of a local minimum for ﬁnding a global one.
As a result, the system goes uphill and downhill. Therefore,
(cid:2)
for selecting the next move λ
, if the move is better than the

current position, i.e., L(Q,Fλ(cid:2) ) < L(Q,Fλ) (true downhill

(cid:2)
step), the Simulated Annealing will select λ
as a new move,
otherwise if the move is worse (uphill), it will be accepted
based on the following probability:

P rob ≈ exp(

− [L(Q,Fλ(cid:2) ) − L(Q,Fλ)]

kT

)

(5)

In the next section, we describe the evaluation of this

algorithm.

5. EXPERIMENT DESIGN

For our experiments, we use version 3.0 of LETOR pack-
age provided by Microsoft Asia [27]. The LETOR pack-
age includes several benchmark datasets and baseline results
for research on learning to rank. The datasets provided
in the LETOR package are: OHSUMED, Top Distillation
2003 (TD2003), Top Distillation 2004 (TD2004), Homepage
Finding 2003 (HP2003), Homepage Finding 2004 (HP2004),
Named Page Finding 2003 (NP2003) and Named Page Find-
ing 2004 (NP2004). There are 106 queries in the OSHUMED
dataset. The relevancy judgments provided in OHSUMED
are scored 0, 1 or 2 and there are 45 features for each query-
document pair. For HP2003, HP2004, NP2003, NP2004,
TD2003 and TD2004, there are 150, 75, 150, 75, 50 and 75
queries, respectively. For these datasets, there are 64 fea-
tures extracted for each query-document pair and a binary
relevance judgment for each pair is provided. Table 1 shows
the statistics of the datasets included in the LETOR 3.0
benchmark.

In LETOR 3.0 package, each dataset is partitioned into
ﬁve for ﬁve-fold cross validation and each fold includes train-
ing, testing and validation sets. The results of the state-of-
the-art algorithms are provided in the LETOR 3.0. Since
these baselines are representatives from each category of
learning-to-rank algorithms (i.e., pointwise, pairwise and list-
wise), we use them to compare with our proposed algorithm.
The followings are the baselines provided in the LETOR 3.0
package:

Regression: This is a simple linear regression which is a

basic pointwise approach.

RankSVM: RankSVM is a pairwise approach using Sup-

port Vector Machine [18].

FRank: FRank is a pairwise approach with a novel loss

function called Fidelity loss function [15].

ListNet: ListNet is a listwise learning-to-rank algorithm [9]

which uses cross-entropy loss for its loss function.

T = T0 × (1 − k
K

)α

(4)

AdaRank-NDCG: This is a listwise boosting algorithm

that incorporates NDCG in computing the samples [47].

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India380Figure 1: Sequence of Steps for Downhill Simplex Method

Table 1: Statistics of the datasets included in the
LETOR 3.0 benchmark.

Queries

Rel. Level

Features

Avg. Doc Per Query

OHSUMED

HP 2003
HP 2004
NP 2003
NP 2004
TD 2003
TD 2004

106
150
75
150
75
50
75

3
2
2
2
2
2
2

45
64
64
64
64
64
64

152
984
992
991
984
981
989

SVM-MAP: This is a listwise that uses support vector

machine with MAP measure [50].

We also compare our algorithm with a new algorithm
called NDCG-Boost [45]1 which optimizes the expectation
of NDCG over all possible permutations of documents.

We use the validation sets for each fold to ﬁnd the best
values for hyper-parameters (i.e., α and T0) and ﬁx K =
1000. Once we ﬁnd the best values of the hyper-parameters
based on the validation sets, we ﬁx them on training data
and use training data to ﬁnd the best parameters for features
(weights for features), i.e., λj’s. And ﬁnally, we use the
optimal learned weight parameters on the training data to
test the performance on the test data. We use the linear
scoring function to rank documents for each query on test
data, e.g., Equation 1. We then get the average across all
folds for each data set.

We adopted the common IR evaluation measure, i.e., Nor-
malized Discounted Cumulative Gain (NDCG) [21]. NDCG
is designed to measure ranking accuracy when there are
more than two levels of relevance judgments.

1The performance measured in our experiment for NDCG-
Boost is diﬀerent to the performance reported in the original
paper [45] for some datasets. The reason is that the evalua-
tions use diﬀerent versions of MATLAB which have diﬀerent
implementations for decision stump, the base classiﬁer uti-
lized in the algorithm.

6. EXPERIMENT RESULTS

6.1 Comparison of NDCG-Annealing Algorithm

with Baselines in LETOR 3.0

We compare our proposed NDCG-Annealing algorithm
with those baselines provided in LETOR 3.0. The results
for all the datasets are shown in Figures 2, 3, 4 in terms
of NDCG measure. The results show that our algorithm
outperforms all the baselines in most of the datasets. The
interesting observation from these results is that the perfor-
mance of the baseline algorithms varies from one dataset to
another; however, the performance of NDCG-Annealing al-
gorithm is the best in most datasets which indicates the con-
sistency of the algorithm for ranking purposes. For example,
FRank which performs well in OHSUMED yields a poor
performance on TD2003, HP2003 and HP2004. Similarly,
AdaRank-NDCG achieves a good performance on OHSUMED
data set; however it does not perform well on other data sets
such as TD2003, HP2003 and NP2003.

As also indicated in [11], since the OHSUMED dataset is
the only dataset with more than 2 levels of relevance, the
diﬀerence between various learning algorithms based on the
NDCG measure can be more distinguishable. We also see
from Figure 4 that our NDCG-Annealing algorithm outper-
forms all the other baseline algorithms on this dataset.

As a result, the NDCG-Annealing algorithm is more stable
and pronounced compared to the baselines in LETOR 3.0
dataset.
6.2 Parameter Sensitivity Study on LETOR

3.0

As discussed before, the starting temperature of the Sim-
ulated Annealing algorithm must be hot enough. However,
if the starting temperature is set too high, the search can
move to any neighbor which leads to a random search. As a
result, ﬁnding a correct starting temperature is important.
Also, the way in which we decrement the temperature is
critical to the success of the algorithm. Finally, how many
iterations we make at each temperature is less important
and can be set to a constant number [14].

As a result, we tuned the two important hyper-parameters,
α and T0 on validation datasets to achieve the optimal per-
formance by the algorithm. Learning the optimal values for

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India381G
C
D
N

0.5

0.45

0.4

0.35

0.3

0.25
1

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

3

4

5

@n

6

7

8

9

10

G
C
D
N

0.5

0.45

0.4

0.35

0.3

0.25
0

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

4

@n

6

8

10

G
C
D
N

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4
0

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

4

@n

6

8

10

Figure 2: Comparison of NDCG-Annealing algorithm with other baselines in terms of NDCG measure, Left:
TD2003, middle: TD2004 and right: HP2003.

G
C
D
N

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35
0

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

4

@n

6

8

10

G
C
D
N

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4
0

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

4

@n

6

8

10

G
C
D
N

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35
0

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

4

@n

6

8

10

Figure 3: Comparison of NDCG-Annealing algorithm with other baselines in terms of NDCG measure, Left:
HP2004, middle: NP2003 and right: NP2004.

G
C
D
N

0.6

0.58

0.56

0.54

0.52

0.5

0.48

0.46

0.44

0.42

0.4
0

Regression
rankSVM
listNet
AdaRank−NDCG
SVM−MAP
FRank
NDCG−Boost
NDCG−Annealing

2

4

@n

6

8

10

Figure 4: Comparison of NDCG-Annealing algorithm with other baselines in terms of NDCG measure,
OHSUMED dataset.

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India3820.5

0.48

0.46

0.44

0.42

0.4

0.38

0
1
@
G
C
D
N

0.5

0.48

0.46

0.44

0.42

0.4

0.38

0
1
@
G
C
D
N

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
1
@
G
C
D
N

0.36
0

50

100
α

150

200

0.36

0

50

100
T0

150

200

0
0

200

400

600

Number of Iterations

800

1000

Figure 5: Sensitivity of α (left) and T0 (middle) to NDCG measure on TD2003 dataset in LETOR 3.0.
Sensitivity of the number of iterations to NDCG (right).

Table 2: CTR and RPM gains by running the NDCG-Annealing algorithm online.

CTR gain over baseline RPM gain over baseline

Yahoo! Answers

Sports Game
Sports News

+86.19%
+10.88%
+59.88%

+38.58%
+9.39%
+49.09%

the hyper-parameters α and T0 is another way and we leave
it for future work.

We show the sensitivity of NDCG@10 measure to each
of these parameters. We only show the sensitivity analyses
on TD2003 data set; similar trends can be seen on other
datasets as well. Figure 5 (left) shows the sensitivity of
NDCG@10 to α parameter. For each fold, we set T0 to its
optimal value and vary α and get the average over all folds.
Figure 5 (middle) shows the sensitivity of NDCG@10 to T0
parameter. For each fold, we set α to its optimal value and
vary T0 and get the average over all folds. The results of
these sensitivity analyses indicate that the performance is
not very sensitive to the hyper-parameters when they are
large enough.

Figure 5 (right) shows the change in NDCG@10 as the
It is shown that in
NDCG-Annealing algorithm iterates.
the ﬁrst few iterations, the NDCG@10 varies a lot as the
algorithm tries to ﬁnd the best point, however for the larger
number of iterations, the NDCG-Annealing algorithm stabi-
lizes which shows that the algorithm needs to iterate enough
to ﬁnd the optimal parameters.

7. RESULTS ON APPLICATION TO ADVER-

TISING

In this section, we describe an application of the NDCG-
Annealing algorithm in an online ad serving system. We
present the results of evaluating this algorithm using Ya-
hoo! editorial dataset (oﬄine experiments) and online ex-
periments by selecting and ranking the relevant ads on web
pages and measuring the performance based on business
evaluation metrics.
7.1 Motivation and Background

Contextual advertising has received much attention nowa-
days as one of the major revenue generation mechanisms for
publishers. In contextual advertising, some ads are shown
to a user when he/she visits a Web page. Such an event

is called impression. Most major Internet companies pro-
vide some contextual advertising products with a pay-per-
click (PPC) business model where the advertiser is charged
a fee every time a user clicks on an ad. Click-through-Rate
(CTR), deﬁned as the ratio between the number of clicks
and the number of impressions, is thus an important metric
to evaluate an ad ranking system. Furthermore, it is often
believed that ads which are relevant to the page are more
likely to receive clicks and have less impact on user experi-
ence (users may not come back to visit the Web page if we
put too many irrelevant ads on the pages). Therefore, our
ranking function seeks to optimize page-ad relevance as an
indirect way to improve CTR as well as user experience.

In the next section, we describe the ad ranking function.

7.2 Ad Ranking Function

This problem of ranking ads (a) in a page (p) can be
viewed as a special setup of information retrieval, where
pages are queries and ads are documents. Our ranking
function adopts the Cosine similarity metric in vector space
model to calculate page-ad relevancy score:

F (a, p) =

P
i tf.idfpi ∗ tf.idfai
||tf.idfp||2 ∗ ||tf.idfa||2

,

(6)

where i corresponds to each token in the vocabulary, tf.idfpi

is the tf-idf score for token i in page p and tf.idfai is the tf-idf
score for token i in ad a.

Under this framework, the tf-idf score for a particular to-
ken does not depend on its location in the pages or ads.
However, a typical Web page consists of multiple sections
and it is generally believed that the title, headings, empha-
sized and strong text on a page, carry more important in-
formation than plain text in the body. For example, on a
Yahoo! Sports page, the body sometimes contains a brief
summary of recent sports news. Ads that match this part
are not necessarily relevant to this particular page. On the
other hand, ads that match the page title are more likely to

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India383be relevant. One way to diﬀerentiate among various sections
is to assign diﬀerent weights to them. For doing that, we
use a DOM-based parser to analyze Web pages and decom-
pose the content on the page based on the html tags (e.g.,
< title >, < h1 >, < em >, < body >). Similarly an ad can
be divided into multiple sections including title, description,
landing page URL and bid terms2. Now the tf-idf score for
token i in page p is calculated as:

tf.idf

(cid:2)
pi =

X

j

λj × tf.idfpj i

(7)

where λj is the weight for section j and tf.idfpj i is the
ordinary tf-idf score for token i in section j of page p. Simi-
larly, we deﬁne the new tf-idf score for each token in each ad.
(cid:2)
Replacing the tf.idf values in equation (5) with the tf.idf
values, we obtain a new function to calculate the relevancy
score between a (page, ad) pair.

It is shown in the experiments that applying weights to the
sections improves retrieval relevance, but how to set appro-
priate weights of diﬀerent sections is a challenge. Because
the ranking function is predeﬁned3 and nonlinear, many ex-
isting learning-to-rank algorithms are simply not applica-
ble. Therefore in the early version of our ranking system,
these weights were tuned by experts through several rounds
of trial-and-error4 procedures.
In this paper, we propose
to apply the NDCG-Annealing algorithm to automatically
learn the optimal weights that minimize the loss function.

To scale our computations, we adopt a parallelization ap-
proach based on the MapReduce [13] framework. MapRe-
duce is a programming model for processing large-scale data
which is highly scalable. The run-time system automatically
takes care of the data, scheduling job across machines and
managing inter-machine communication.
In our case, ev-
ery time the NDCG-Annealing algorithm needs to evaluate
a parameter set, we start a new MapReduce job.
In the
Map step, the similarity score for each (page, ad) pair is cal-
culated in parallel. Then in the Reduce step, we rank ads
based on these scores and calculate the NDCG. By doing
so, we are able to speed up the training process for large
amounts of data.

In the next sections, we describe evaluations in both oﬄine

and online scenarios.
7.3 Ofﬂine evaluation

In this section, we report the results based on the Ya-
hoo! editorial dataset. This dataset is created by editors
assigning judgments to the (page, ad) pairs. The Yahoo!
editorial dataset is composed of a set of Web pages, each
associated with the contextual ads displayed on the page,
and the corresponding editorial relevance judgment on each
(page, ad) pair. The editorial relevance judgment uses 3-

2In sponsored search, advertisers specify which keywords
they want to bid for. When a user searches for such key-
words, ads with these bid terms are qualiﬁed to be shown to
the user. However, in contextual advertising, there are no
such search keywords. But the bid terms somewhat describe
what the ad is about and should also be considered as an ad
feature.
3Because of system restrictions, we are unable to change
the ranking function format but only allowed to change the
section weights.
4We use several sets of values based on heuristics for the
parameters and run online tests to pick the best one.

grade scores with 0 (relevant), 1 (somewhat relevant), and 2
(irrelevant). The dataset has 7,259 unique Web pages. This
editorial dataset was collected between years 2008 and 2009.
It is a combination of 7 rounds of editorial judgments. In
each round, the editorial team randomly crawls 1000-2000
Web pages. Some of them are not judgeable (with no or lit-
tle content). After removing those pages, we obtain a total
number of 7,259 pages. Each page has 3-15 associated ads
which amounts to a total of 83,505 (page, ad) pairs.

A total of 14 sections are extracted from each (page, ad)
pair. The sections extracted are shown in table 3. EM, H1,
Strong and Meta keywords sections are the words contained
in html tags such as < em >, < h1 >, < strong > and
meta tags of Web pages, respectively. Page URL Segments
are extracted from page URLs, including domain name, host
name and etc.

We randomly partitioned this dataset into ﬁve folds and
created ﬁve training sets, each using four folds for train-
ing and leaving one fold out for evaluation. We report the
NDCG averaged over ﬁve folds. We compare the NDCG-
Annealing algorithm with the heuristic approach. Since
we only show three ads on each result page, we calculate
NDCG@3. NDCG@3 of the learned weights for sections
with NDCG-Annealing algorithm is improved 11.02% com-
pared with that of the baseline for section weights in the
oﬄine evaluation using Yahoo! editorial data.

Table 3: Sections extracted for oﬄine and online
evaluations.

Page Title

Page URL Segments

Page Description
Page Keywords

Body
Strong

EM
H1

Meta Keywords

Page Anchor Text

Ad Title

Ad Short Description

Ad Display URL

Ad Bid Terms

7.4 Online Evaluation

We applied the NDCG-Annealing algorithm on a real ad-
vertising serving system to select and rank ads to show on
Web pages for real users. To measure the performance, we
set up two buckets in our online ad serving system (each
Web page initiates an ad call to request a set of ads from
the ad server). One bucket is a control bucket, which uses
the expert-tuned weights in the ranking system of the ad
server. The second bucket uses the weights learned (in of-
ﬂine evaluation) by NDCG-Annealing algorithm. All other
settings are the same. The two buckets receive random ad
calls sent over equally so the other factors which may aﬀect
the experiment results are statistically the same. The goal of
the online evaluation is to measure the impact of the NGCG
Annealing learned weights to the business metrics to check
if the NDCG gains observed oﬄine can be transferred to the
gains online (this can be regarded as transferring the knowl-
edge, i.e., learned weights). Although NDCG-Annealing al-
gorithm is to optimize relevance, we have observed that rele-
vance is highly correlated with user response, CTR, and thus
the revenue generated from clicks. It is diﬃcult to directly

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India384measure the relevance improvement in online testing, so in-
stead we measure the business metrics to be a proxy of the
relevance, and those business metrics are also more tangible
and ﬁnancially important than NDCG. We use the follow-
ing two metrics, Click-through-Rate (CTR) and Revenue per
1000 Impressions (RPM) for the online traﬃc evaluation:

CT R =

RP M =

#clicks

#impressions
Revenue ∗ 1000
#impressions

Revenue is deﬁned as follows:
Consider we have N impressions and C clicks and each
click ci in C has a winning bid of bi. Then the Revenue for
these N impressions is:

P

i bi.

We do not directly train the model for online traﬃc, but
use the learned weights from oﬄine training. We use the
same sections as in table 3. In both two buckets, we return
three ads per page on Yahoo! Answers page, Sports game
and Sports news. An example of ads shown to the users
on Yahoo! Answers page is shown in Figure 6 (yellow part).
The ad server with the two buckets were put online to receive
real ad call traﬃc for two weeks. The page impressions,
clicks received on the ads and the revenue generated are
logged, based on which we compare the online performance
of the two buckets. The results for CTR and RPM are shown
in table 2. The percentage is the relative gain. As we can
see, the learned weights signiﬁcantly improve the evaluation
metrics over the baseline.
7.5 Findings

The followings are the importance of diﬀerent sections.

Note that the numbers in parentheses show the learned weights.
- Ad title has high positive impact (3.03).
- Ad bid terms (2.61) have lower impact than titles which
is about 86% of ad title. This maybe because that the ad
structure has high perceived impact than the invisible in-
serted bid term itself to the users.
- Ad description impact is relatively low (0.76).
- Page title has less positive impact (1.92) than that of page
description (2.91).
- HTML tags do not have signiﬁcantly higher impact than
plain text in page body. The tag strong (0.01) even has a
lower weight than body (0.93). H1 (1.47), EM (1.09), and
page keywords (1.21) have almost similar positive impacts.
- Page anchor has quite high positive impact (3.88).
- Page part of URL (1.83) and query part of URL (1.16)
both have quite high positive impacts.

8. CONCLUSIONS AND FUTURE WORK

Listwise approach is a new approach to learning to rank.
In this paper, we proposed a stochastic learning-to-rank al-
gorithm that uses Simulated Annealing algorithm along with
Simplex method for parameter search to minimize the loss
function that is directly deﬁned on any IR evaluation mea-
sures including NDCG and MAP. The loss function in our
method is application speciﬁc and restriction-free. As an in-
stance of the method, we optimize NDCG in this paper, but
other metrics in ranking can also be applied directly. Our ex-
periments on LETOR 3.0 benchmark dataset show that the

Figure 6: Ads (in yellow) shown on Yahoo! Answers
page.

NDCG-Annealing algorithm outperforms the state-of-the-
art algorithms both in terms of performance and stability.
We also showed an application of the proposed method in an
online ad serving system for matching and ranking ads for a
given Web page which indicates the applicability of the pro-
posed algorithm in real online applications as we improved
the CTR and RPM signiﬁcantly in the online bucket tests.
There is still remaining work in this direction; it would
be interesting to see the performance of the algorithm for
ranking real web documents. In addition, adding more ad-
vanced functionalities to handle complex constraints of the
parameters to learn, and standardize the library to be easily
applied to other applications is another line for future work.
Also, it would be interesting to propose some other ranking
functions other than vector space model for ad ranking. Al-
though, diﬀerent methods for matching and ranking ads are
proposed in the literature, there is no good understanding
of which method performs better for a real ad system. We
plan to develop a large-scale benchmark ad dataset, so that
comparing diﬀerent methods would be easier. Finally, the
optimization (training) of the NDCG-Annealing algorithm
takes time and one future work is to expedite the algorithm
by better extracting and exploiting the parallelism in the al-
gorithm to leverage MapReduce programming model more
eﬀectively.

9. ACKNOWLEDGMENTS

We thank the anonymous reviewers for their useful com-
ments. The authors would like to acknowledge Fernando
Diaz, Olivier Chapelle, Wei Chu and Jangwon Seo for use-
ful discussions.

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India38510. REFERENCES
[1] H. M. D. Almeida, M. A. Concalves, M. Cristo, and

P. Calado. A combined component approach for ﬁnding
collection-adapted ranking functions based on genetic
programming. ACM SIGIR, pages 399–406, 2007.

[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern information

retrieval. Addison Wesley.

[3] A. Bagherjeiran, A. Hatch, and A. Ratnaparkhi. Ranking

for the conversion funnel. ACM SIGIR, pages 146–153,
2010.

[4] S. Bandyopadhyay, S. K. Pal, and C. A. Murthy. Simulated

annealing based pattern classiﬁcation. Information
Sciences, pages (109):165–184, 1998.

[5] A. Z. Broder, M. Fontoura, V. Josifovski, and L. Riedel. A
semantic approach to contextual advertising. SIGIR, pages
559–566, 2007.

[6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using
gradient descent. ICML, pages 89–96, 2005.

[7] C. J. Burges, R. Ragno, and Q. V. Le. Learning to rank

with nonsmooth cost functions. NIPS, pages 193–200, 2006.
[8] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon.
Adapting ranking svm to document retrieval. ACM SIGIR,
pages 186–193, 2006.

[9] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, , and H. Li.

Learning to rank: From pairwise approach to listwise
approach. ICML, pages 129–136, 2007.

[10] D. Chakrabarti, D. Agrawal, and V. Josifovski. Contextual

advertising by combining relevance with click feedback.
WWW, pages 416–426, 2008.

[11] O. Chapelle and M. Wu. Gradient descent optimization of
smoothed information retrieval metrics. IR Journal, pages
13(3):201–215, 2010.

[12] D. Cossock and T. Zhang. Subset ranking using regression.

CoLT, pages 605–619, 2006.

[13] J. Dean and S. Ghemawat. Mapreduce: simpliﬁed data

processing on large clusters. Symposium on Operating
System Design and Implementation, pages 137–150, 2004.

[14] K. A. Dowsland. Simulated annealing. Modern Heuristic

Techniques for Combinatorial Problems, 1993.

[15] M. feng Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y.

Ma. Frank: A ranking method with ﬁdelity loss. ACM
SIGIR, pages 383–390, 2007.

[16] A. Foundation. Apache hadoop project.

http://hadoop.apache.org/.

[17] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An
eﬃcient boosting algorithm for combining preferences.
ICML, pages 170–178, 1998.

[18] R. Herbrich, T. Graepel, and K. Obermayer. Support
vector learning for ordinal regression. Artiﬁcial Neural
Networks, pages 97–102, 1999.

[19] R. Herbrich, T. Graepel, and K. Obermayer. Large margin
rank boundaries for ordinal regression. Advances in Large
Margin Classiﬁers, pages 115–132, 2000.

[20] C. Huang and R. Nevatia. High performance object

detection by collaborative learning of joint ranking of
granules features. CVPR, pages 41–48, 2010.

[21] K. Jarvelin and J. Kekalainen. Ir evaluation methods for
retrieving highly relevant documents. ACM SIGIR, pages
41–48, 2000.

[22] T. Joachims. Optimizing search engines using clickthrough

data. ACM KDD, pages 133–142, 2002.

[23] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi.

Optimization by simulated annealing. Science, pages
671–680, 1983.

[24] A. Lacerda, M. Cristo, M. Goncalves, W. Fan, N. Ziviani,
and B. Ribeiro-Neto. Learning to advertise. ACM SIGIR,
pages 549–556, 2006.

[25] P. Li, C. J. Burges, and Q. Wu. Mcrank: Learning to rank
using multiple classiﬁcation and gradient boosting. NIPS,
2007.

[26] T. Y. Liu. Learning to rank for information retrieval.

Foundations and Trends in Information Retrieval, pages
3(3):225–331, 2009.

[27] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor:

Benchmark dataset for research on learning to rank for
information retrieval. SIGIR-LTR, 2007.

[28] A. V. Lukashin and R. Fuchs. Analysis of temporal gene

expression proﬁles: Clustering by simulated annealing and
determining the optimal number of clusters.
Bioinformatics, pages 405–414, 2000.

[29] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H.

Teller, and E. Teller. Equation of state calculation by fast
computing machines. J. Of Chem. Phys., pages 1087–1091,
1953.

[30] V. Murdock, M. Ciaramita, and V. Plachouras. A

noisy-channel approach to contextual advertising. ADKDD,
pages 21–27, 2007.

[31] R. Nallapati. Discriminative models for information

retrieval. ACM SIGIR, pages 64–71, 2004.

[32] J. A. Nelder and R. Mead. A simplex method for function
minimization. Computer Journal, pages (7):308–313, 1965.
[33] Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. Object-level

ranking: Bringing order to web objects. WWW, pages
567–574, 2005.

[34] J. Peng, C. Macdonald, and I. Ounis. Learning to select a

ranking function. ECIR, pages 111–126, 2010.

[35] J. Ponte and W. Croft. A language modeling approach to
information retrieval. ACM SIGIR, pages 275–281, 1998.
[36] T. Qin, T.-Y. Liu, M. feng Tsai, X.-D. Zhang, and H. Li.

Learning to search web pages with query-level loss
functions. Technical Report, 2006.

[37] A. Ratnaparkhi. A hidden class page-ad probability model

for contextual advertising. WWW Workshop, 2008.

[38] M. Regelson and D. Fian. Predicting click-through rate

using keyword clusters. ACM EC, 2006.

[39] B. Ribeiro-Neto, M. Cristo, P. Golgher, and E. S. D.

Moura. Impedance coupling in content-targeted advertising.
ACM SIGIR, pages 496–503, 2005.

[40] M. Richardson, E. Dominowska, and R. Rango. Predicting

clicks: Estimating the click-through rate for new ads.
WWW, pages 521–530, 2007.

[41] S. Robertson and K. S. Jones. Relevance weighting of

search terms. Journal of the American Society for
Information Science, pages 27:129–146, 1976.

[42] S. Robertson and S. Walker. Some simple eﬀective

approximations to the 2-poisson model for probabilistic
weighted retrieval. ACM SIGIR, pages 232–241, 1994.

[43] G. Salton and C. Buckley. Term-weighting approaches in

automatic text retrieval. Information Processing and
Management, pages 24(5):513–523, 1988.

[44] M. Taylor, J. Guiver, S. Robertson, and T. Minka.

Softrank: optimizing non-smooth rank metrics. ACM
WSDM, pages 77–86, 2008.

[45] H. Valizadegan, R. Jin, R. Zhang, and J. Mao. Learning to

rank by optimizing ndcg measure. NIPS, 2009.

[46] M. N. Volkovs and R. S. Zemel. Boltzrank: learning to

maximize expected ranking gain. ICML, pages 1089–1096,
2009.

[47] J. Xu and H. Li. Adarank: a boosting algorithm for

information retrieval. ACM SIGIR, pages 391–398, 2007.
[48] W. Yang, L. Rueda, and A. Ngom. A simulated annealing

approach to ﬁnd the optimal parameters for fuzzy
clustering microarray data. IEEE International Conference
on The Chilean Computer Science Society, 2005.

[49] J.-Y. Yeh, J.-Y. Lin, H.-R. Ke, and W.-P. Yang. Learning

to rank for information retrieval using genetic
programming. SIGIR-LTR Workshop, 2007.

[50] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A

support vector method for optimizing average precision.
ACM SIGIR, pages 271–278, 2007.

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India386