SEISA: Set Expansion by Iterative Similarity Aggregation

∗
Yeye He

University of Wisconsin-Madison

Madison, WI 53706

heyeye@cs.wisc.edu

†

Dong Xin

Microsoft Research
Redmond, WA 98052

dongxin@microsoft.com

ABSTRACT
In this paper, we study the problem of expanding a set of given seed
entities into a more complete set by discovering other entities that
also belong to the same concept set. A typical example is to use
“Canon” and “Nikon” as seed entities, and derive other entities
(e.g., “Olympus”) in the same concept set of camera brands. In
order to discover such relevant entities, we exploit several web data
sources, including lists extracted from web pages and user queries
from a web search engine. While these web data are highly diverse
with rich information that usually cover a wide range of the do-
mains of interest, they tend to be very noisy. We observe that pre-
viously proposed random walk based approaches do not perform
very well on these noisy data sources. Accordingly, we propose a
new general framework based on iterative similarity aggregation,
and present detailed experimental results to show that, when using
general-purpose web data for set expansion, our approach outper-
forms previous techniques in terms of both precision and recall.
Categories and Subject Descriptors: H.2.8 Database Applica-
tion: Data Mining
General Terms: Algorithms.
Keywords: Set Expansion, Named Entity Recognition, Similarity
Measure.

1.

INTRODUCTION

Set expansion refers to the practical problem of expanding a
small set of “seed” entities, into a more complete set by discover-
ing other entities that also belong to the same “concept set”. Here
a “concept set” can be any collection of entities that conceptually
form a set that people have in mind, and “seeds” are the instances of
entities in the set. As an example, a person wanting to discover all
camera brand names may give a small number of well-known brand
names like “Canon” and “Nikon” as seeds, the set expansion
techniques would leverage the given data sources to discover other
camera brands, such as “Leica”, “Pentax” and “Olympus”
that are also camera brands.

Set expansion systems are of practical importance and can be
used in various applications. For instance, web search engines may
use the set expansion tools to create a comprehensive entity repos-
itory (for, say, brand names of each product category), in order
∗
Work done at Microsoft Research
†
Work done at Microsoft Research.
(dongxin@google.com).

Now with Google

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

to deliver better results to entity-oriented queries. As another ex-
ample, the task of named entity recognition can also leverage the
results generated by set expansion tools [13].

Considerable progresses have been made in developing high-
quality set expansion systems. The most relevant efforts include
Google Sets [1], which employs proprietary algorithms to do set
expansions. However, due to its proprietary nature, algorithms and
data sources behind Google Sets are not publicly available for fu-
ture research endeavors.

Another prominent line of work is the SEAL system [16, 17,
18, 19]. They adopted a two-phase strategy, where they ﬁrst build
customized text wrappers based on the input seeds in order to ex-
tract candidate entities from web pages in a precise manner. They
then use a graph-based random walk to rank candidates entities
based on their closeness to the seeds on the graph. While they have
demonstrated that this customized data extraction/ranking process
can produce results with high quality, the necessary online data ex-
traction can be costly and time-consuming.

Here we study the problem of conducting set expansion using
general web data sources without resorting to online data extrac-
tions speciﬁc to the given seeds. In particular, we look at two com-
mon types of web data: the HTML lists extracted from web pages
by web crawls (henceforth referred to as the Web Lists) and the
web search query logs (the Query Logs). We model both types of
data using bipartite graphs to provide a uniﬁed computation model.
Such general-purpose web data can be highly useful for set ex-
pansion tasks: they are very diverse in nature, with rich information
that covers most domains of interest. In addition, since these gen-
eral data are not domain/seed speciﬁc, they can be pre-processed
and optimized for efﬁciency purposes.

However, these general web data can be inherently noisy. Ran-
dom walk or other similarity measures along may not be sufﬁcient
to distinguish true results from the noises, especially when the num-
ber of seeds are limited. As we observe in our experimental evalua-
tions, that random walk based ranking techniques used in previous
work perform poorly on the general-purpose Web Lists or Query
Logs and produce results with low precision/recall. Partly because
of that, previous approaches [16, 17, 18, 19] use seed-speciﬁc and
page-speciﬁc wrappers to reduce the candidate set to a smaller and
much cleaner subset over which the random walk based ranking
techniques work reasonably well. However, we note that this addi-
tional data extraction process is at the cost of overall architectural
complexity and system responsiveness.

Unlike previous approaches, we in this work propose a general
framework that only uses general-purpose web data without resort-
ing to on-line data extractions speciﬁc to the given seeds. In par-
ticular, while previous random walk based approaches leverage the
intuition that candidates close to the given seeds in the graph struc-

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India427ture are more likely to belong to the same concept set as the seeds;
we take an alternative tack and propose to measure the quality of
an expanded set of entities relative to the given set of seeds in a
more straightforward and comprehensible way.
Intuitively, a set
of expanded results is “good” if it has two key properties: (1) the
set of produced entities are similar to the given seeds; (2) the set
of produced entities are coherent in the sense that they represent a
consistent concept. We abstract the intuitions and deﬁne quality of
the result set as the sum of two component scores: the Relevance of
a set of entities that measures their similarity with the given seeds,
and Coherence of the set of entities produced which is how closely
the entities in this set are related to each other. Based on this qual-
ity measure, we further develop a class of iterative set expansion
algorithms for which we call SEISA (Set Expansion by Iterative
Similarity Aggregation). We show that SEISA is robust and effec-
tive in producing expanded sets over noisy web data sources.

The rest of the paper is organized as follows. We summarize the
related work in the space of set expansion in Section 2. We then
formally deﬁne the problem of set expansion using the new quality
metric we propose in Section 3. In Section 4 we detail our itera-
tive set expansion algorithms. Finally we present our experimental
results in Section 5 and conclude this paper in Section 6.

2. RELATED WORK

There is a signiﬁcant body of related work in the broad space of
information extraction and named entity extraction. We will only
summarize work most relevant to set expansion due the limit of
space.

Wang and Cohen [16, 17, 18, 19] developed the SEAL system
for set expansion, using a two-phase extraction/ranking architec-
ture.
In the ﬁrst extraction phase, they build for each web page
customized wrappers using maximal left/right context that would
enclose all given seeds, which are in turn applied on the web page
from which they are constructed to extract candidate terms in ad-
dition to the given seeds. In the second ranking phase, web pages,
wrappers and candidate terms are modeled as nodes in the graph,
and random walk techniques are used to rank candidates based on
their structural proximity to the seeds in the graph. In compari-
son, our approach ranks a set of candidates as a whole based on
its relevance and coherence, and does not require page-speciﬁc and
seed-speciﬁc data extraction process.

Agichtein et al. [4] introduce the Snowball system that bootstraps
from a small number of input tuples, by ﬁrst obtaining typical con-
textual patterns of the seed tuples from the web pages, which are
used in turn to extract more tuples. While Snowball is well suited
for extracting certain types of structured data like binary relations,
it may not work well for set expansion due to its reliance on textual
context patterns (sets can be viewed as unary relations of tuples,
whose context can be much more dynamic and less predictable than
that of binary relations).

Etzioni et al. [6, 7] develop the KnowItAll system that automat-
ically extracts facts from the web using textual patterns like “cities
such as Paris, London and New York” to extract candidate entities.
Candidates are then ranked in a bootstrapping manner using statis-
tical information gathered from the search engine such as PMI over
hit counts.

Talukdar et al. [14] study the problem of set expansion from open
text. They propose to automatically identify trigger-words which
indicate patterns in a bootstrapping manner.

Ghahramani et al. [8] uses Bayesian inference to solve the prob-
lem of set expansion. It has been shown in [18] that this candidate
ranking mechanism is comparable to random walk in quality of the
results of the expanded set.

Canon

Nikon

Leica

VW

BMW

List01

List02

List03

List04

List05

Canon

Nikon

Leica

VW

BMW

camera

japan

laptop

dealer

mpg

(a) Web List Data

(b) Query Log Data

Figure 1: Bipartite graph data model

Set expansion is also somewhat related to the problem of class
label acquisition [15, 20] where the goal is to propage a set of class
labels to data instances using labeled traning examples. While the
set expansion problem can be modeled as propagating class labels
associated with seeds to candidate entities, we observe that a large
number of training examples is necessary in order for these tech-
niques to be effective.

Finally Google Sets [1] does set expansions using propriety al-

gorithms which are not publicly available.

3. PROBLEM DEFINITION
3.1 Data Model

In this work we target general web data sources. Speciﬁcally,
we look at lists extracted from the HTML web pages (the Web List
data), and the web search query logs (the Query Log data). In each
case, we model the data as bipartite graphs as in Figure 1, with
candidate terms being nodes on one side (henceforth referred to as
term-nodes) and their contexts on the other side. Since we use tex-
tual terms in the web data as candidate entities for the expanded set,
from this point on we will be using the word “term” interchange-
ably with “entity”.

Web List Data. For Web List as in Figure 1a, each unique web
list crawled from the web, (“List01”, “List02”, etc), is mod-
eled as a node on the right-hand-side, while each term that appears
in those web lists is modeled as a term-node on the left hand side.
In this example, the underlined nodes “Canon” and “Nikon” on
the left are the seed terms, while the remaining terms, including
“Leica”, “VW” and “BMW”, are possible candidate terms. There
is an edge connecting a term-node with a list-node if that term is a
members of the list. For example, the list-node List01 connects
to “Canon”, “Nikon” and “Leica”, indicating that all three
terms are members of List01, which is probably a web list on
some web page that enumerates a list of camera brands.

While it is possible to resort to additional information of the web
data to assign different weights to each edge (using the quality of
the page from which the list is extracted, for example), we in this
work adopt a simpler approach that only assigns a uniform weight
of 1 to each edge in the Web List graph. We in our experiments ﬁnd
this approach to work quite well.

Query Log Data. Query log data is modeled as in Figure 1b.
Here for each keyword query, we break up the query into two parts,
the term and the context. The context is a preﬁx or sufﬁx of the
query up to 2 tokens, and the term is the remainder of the query.
Each term is again modeled as a graph node on the left, the context
is modeled as a node on the right.

There are various ways in which we can model edges in the graph
for query log data. In this work we assign weight of the edge be-

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India428tween each pair of nodes using the Mutual Information between the
query term and query context, which is deﬁned in Deﬁnition 1.

DEFINITION 1. Let 𝑃 𝑟𝑜𝑏(𝑡) be the probability that term 𝑡 oc-
cur in the query log, 𝑃 𝑟𝑜𝑏(𝑐) be the probability that context 𝑐 oc-
cur in the query log, let 𝑃 𝑟𝑜𝑏(𝑡, 𝑐) be the probability that the term
𝑡 and context 𝑐 co-occur in the query log. The Mutual Information
H(t, c) is deﬁned as: 𝐻(𝑡, 𝑐) =

𝑃 𝑟𝑜𝑏(𝑡)∗𝑃 𝑟𝑜𝑏(𝑐)

𝑃 𝑟𝑜𝑏(𝑡,𝑐)

Furthermore, we only keep the edge between a pair of nodes
if the Mutual Information between the term and the context (or the
weight on the edge) is positive, and additionally, the co-occurrences
of the term and the context is frequent enough to be above certain
threshold. Figure 1b is an example of the resulting bipartite graph
after this simple processing.

In general, we feel that this bipartite graph model is straightfor-
ward and general enough to be applied and extended to other types
of data sources.
3.2 Similarity Metric

With this bipartite data model, intuitively the overall task of do-
ing set expansion given a set of seeds can to an extent be viewed
as the problem of ﬁnding term-nodes that are similar to the given
seed-nodes, using the right hand side nodes as the features. In order
to measure similarities between the term-nodes, common similarity
metrics, like Jaccard Similarity [11] and Cosine Similarity [11] as
deﬁned below, can all be used.

DEFINITION 2. [11] Let 𝑥, 𝑦 be two term-nodes on the left hand
side. Let 𝐿𝑥 and 𝐿𝑦 be the two sets of right side nodes that connect
to node 𝑥 and 𝑦, respectively. The Jaccard Similarity of 𝑥 and 𝑦,
∣𝐿𝑥∩𝐿𝑦∣
denoted as 𝑆𝑖𝑚𝐽𝑎𝑐(𝑥, 𝑦), is deﬁned as 𝑆𝑖𝑚𝐽𝑎𝑐(𝑥, 𝑦) =
∣𝐿𝑥∪𝐿𝑦∣ .
DEFINITION 3. [11] Let 𝑥, 𝑦 be two term-nodes on the left hand
side. Let 𝑉𝑥 and 𝑉𝑦 be the weight vectors that indicate the weights
of the edges that connect web lists to node 𝑥 and 𝑦, respectively.
The Cosine Similarity of 𝑥 and 𝑦, denoted as 𝑆𝑖𝑚𝐶𝑜𝑠(𝑥, 𝑦), is
deﬁned as 𝑆𝑖𝑚𝐶𝑜𝑠(𝑥, 𝑦) = 𝑉𝑥⋅𝑉𝑦

∥𝑉𝑥∥∥𝑉𝑦∥ .

We use the following two examples as simple illustrations of the

Jaccard similarity and Cosine similarity.

EXAMPLE 1. We ﬁrst illustrate the computation of Jaccard sim-
ilarity of two term-nodes in our bipartite graph model.
In Fig-
ure 1a, the term-node “Canon” connects to list nodes 𝐿“𝐶𝑎𝑛𝑜𝑛′′
= {“List01”, “List02”}; while the term-node “Leica” con-
nects to nodes 𝐿“𝐿𝑒𝑖𝑐𝑎′′ = {“List01”, “List02”, “List03”,
“List04”}. By Deﬁnition 2, the Jaccard similarity between the
∣𝐿“𝐶𝑎𝑛𝑜𝑛′′∩𝐿“𝐿𝑒𝑖𝑐𝑎′′∣
seed-nodes “Leica” and “Canon” is
∣𝐿“𝐶𝑎𝑛𝑜𝑛′′∪𝐿“𝐿𝑒𝑖𝑐𝑎′′∣ =
2
= 0.5. Similarly, the similarity between “Leica” and the other
4
seed-node “Nikon” is also 2
4

On the other hand, the Jaccard similarities between “VW” and
both of the seed-nodes “Canon” and “Nikon” are 0
= 0. There-
6
fore, using the Jaccard Similarity deﬁnition, the term “Leica” is
more similar to both seeds than the term “VM”.

= 0.5.

Next we show how Cosine similarity between term-nodes is com-
puted. In Figure 1a, the term-node “Canon” connects to 𝐿“𝐶𝑎𝑛𝑜𝑛′′
={“List01”, “List02”}, its edge weight vector 𝑉“Canon” is
thus (1, 1, 0, 0, 0). By the same token, the edge weight vector for
“Leica” is 𝑉“Leica” = (1, 1, 1, 1, 0). According to Deﬁnition 2,
the Cosine similarity between nodes “Leica” and “Canon” is
𝑉“𝐶𝑎𝑛𝑜𝑛′′⋅𝑉“𝐿𝑒𝑖𝑐𝑎′′
∥𝑉“𝐶𝑎𝑛𝑜𝑛′′∥∥𝑉“𝐿𝑒𝑖𝑐𝑎′′∥ = 2
= 0.71. Similarly, the similar-
ity between “Leica” and the other seed-node “Nikon” is also
2.83

= 0.71.

2.83

2

(a) A set with high relevance

(b) A set with high coherence

Figure 2: Quality of an expanded set

The Cosine similarities between “VW” and both of the seed-
nodes “Canon” and “Nikon” are 0 due to the lack of overlap
in the right side list-nodes. We again have the term “Leica” to
be more similar to seeds than the term “VM”.

While in this section we only discuss two most commonly used
similarity metrics, the Jaccard Similarity and the Cosine Similarity,
we emphasize that the set expansion framework to be introduce in
detail in Section 4 is general and extensible enough that any other
similarity metrics can be easily plugged in to be used. Furthermore,
in our experimental evaluations, we ﬁnd that the performances of
set expansion using both similarity metrics are reasonably good,
underlining the generality of the framework we propose.
3.3 Quality Measurement

While the previous work uses techniques like random walk to
rank individual terms based on their graph structure similarity to
the given seeds, we see the expanded set of entities as a whole and
propose a simple and intuitive metric to measure the quality of the
expanded set, as will be detailed in this section.

The ﬁrst observation we have is that, the more similar the ex-
panded entities are to the given seed entities, the better quality the
expanded set. This is intuitive because after all the task of set ex-
pansion is to ﬁnd entities that are in the same “concept set” as the
seeds, which by deﬁnition should be somewhat similar to the seeds.
We formalize this observation with the following deﬁnition of rel-
evance to capture the similarity between the expanded set and the
seed set.

DEFINITION 4. Let 𝑈 be the universe of entities, 𝑅 ⊆ 𝑈 be
the expanded set, and 𝑆 ⊆ 𝑈 be the seed set. Let 𝑆𝑖𝑚 : 𝑈 ×
𝑈 → [0, 1] be the function that measures the similarity of any two
entities. The relevance of 𝑅 with respect to 𝑆 is deﬁned as:

𝑆𝑟𝑒𝑙(𝑅, 𝑆) =

1

∣𝑅∣ ∗ ∣𝑆∣ ∗ ∑

𝑟∈𝑅

∑

𝑠∈𝑆

𝑆𝑖𝑚(𝑠, 𝑟)

To better illustrate this deﬁnition of relevance, we use Figure 2
to graphically demonstrate the quality of the expanded set. In both
Figure 2a and Figure 2b, the two solid dots in the middle represent
the given seed set 𝑆, while the circles surrounding these two dots
are the derived entities that constitute the expanded set 𝑅. The
similarity of any two entities is then represented as the distance of
these two dots in the graph.

It is clear that in both of these two ﬁgures, the expanded set of
entities as circled by the dashed oval are very similar (or graphi-
cally speaking, close) to the two given seeds. So in terms of our
relevance metric, both of the two sets in Figure 2a and Figure 2b
have high relevance to the given seeds.

However, we observe that this deﬁnition of relevance alone does
not fully capture the quality of the expanded set. The reason for

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India429this is that while the overarching goal of set expansion is to ﬁnd
a consistent “concept set” that are very similar to the given seeds,
there could be cases where a set of entities are similar to the seeds
but not consistent enough to be a coherent concept set. As an ex-
ample, in Figure 2a, while the expanded entities as denoted by the
circles are close to the given seeds, they are relative dispersed in
the space and may not form a consistent “concept set” as required
by set expansion. On the other hand, the the expanded entities in
Figure 2b are not only equally close to the given seeds as in Fig-
ure 2a, they are also much closer to each other to form a consistent
“concept set”. Thus, the expanded entities in Figure 2b may be a
better candidate for the expanded set than the entities in Figure 2a.
To capture the intuition in Figure 2b that the closer the entities in
the expanded set are to each other, the more coherent and thus better
the set as a whole is, we formally deﬁne the notion of coherence in
the following.

DEFINITION 5. Let 𝑈 be the universe of entities, 𝑅 ⊆ 𝑈 be the
expanded set, 𝑆𝑖𝑚 : 𝑈 × 𝑈 → [0, 1] be the function that measures
the similarity of any two entities. The coherence of 𝑅 is deﬁned as:

1

∣𝑅∣ ⋅ ∣𝑅∣ ∗

∣𝑅∣∑

∣𝑅∣∑

𝑖=1

𝑗>𝑖

𝑆𝑖𝑚(𝑟𝑖, 𝑟𝑗)

𝑆𝑐𝑜ℎ(𝑅) =

where 𝑟𝑖, 𝑟𝑗 ∈ 𝑅.

Based on the observation that both relevance and coherence con-
tribute to the quality of an expanded set, we deﬁne the quality of
the expanded set as the weighted sum of relevance and coherence
as follows.

DEFINITION 6. Let 𝑈 be the universe of entities, Let 𝑅 ⊆ 𝑈
be the expanded set, 𝑆 ⊆ 𝑈 be the seed set. Let 0 ≤ 𝛼 ≤ 1 be
the constant weight factor. The quality of the expanded set 𝑅 with
respect to the seed set 𝑆, 𝑄(𝑅, 𝑆), is deﬁned as:

𝑄(𝑅, 𝑆) = 𝛼 ∗ 𝑆𝑟𝑒𝑙(𝑅, 𝑆) + (1− 𝛼) ∗ 𝑆𝑐𝑜ℎ(𝑅)

Here, 𝛼 is a constant weight that balances the emphasis between
relevance and coherence. In our experiments that we will detail in
Section 5, we ﬁnd 𝛼 = 0.5 to be a good value in practice, and this
intuitive yet simple deﬁnition of quality works well in the extensive
experiments that are conducted.
3.4 Problem Statement

With the deﬁnition of quality metric, we formally state the prob-
lem as follows. Given the universe of candidate terms 𝑈 and some
seeds 𝑆 ⊆ 𝑈; given a similarity function 𝑆𝑖𝑚 : 𝑈 × 𝑈 →
[0, 1] that measures the similarity of any two terms, identify the
expanded seed set 𝑅, 𝑅 ⊆ 𝑈 and is of size 𝐾, such that the objec-
tive function 𝑄(𝑅, 𝑆) is maximized, where

𝑄(𝑅, 𝑆) = 𝛼 ∗ 𝑆𝑟𝑒𝑙(𝑅, 𝑆) + (1− 𝛼) ∗ 𝑆𝑐𝑜ℎ(𝑅)

Intuitively, the expanded seed set, or the ESS, is the core com-
ponent of the concept set that we want to expand, and consists of
entities that we know with high conﬁdence that belong to the de-
sired concept set. We say an ESS is good if its quality score is high.
Once a good ESS (denoted as 𝑅) is derived, individual terms 𝑡 can
then be ranked based on 𝑅 and the seed set 𝑆 using the ranking
function 𝑔(𝑡, 𝑅, 𝑆), which is again a straightforward combination
of relevance score and coherence score as follows.
∣𝑅∣∑

∣𝑆∣∑

𝑔(𝑡, 𝑅, 𝑆) =

𝑆𝑖𝑚(𝑡, 𝑠𝑖) +

𝑆𝑖𝑚(𝑡, 𝑟𝑖) (1)

𝛼
∣𝑆∣

𝑖=1

(1 − 𝛼)

∣𝑅∣

𝑖=1

where 𝑟𝑖 ∈ 𝑅 and 𝑠𝑖 ∈ 𝑆.

However, we show that the problem of ﬁnding the optimal 𝑅 of

size 𝐾 with maximum quality score is NP-hard.

THEOREM 1. Given the seed set 𝑆, the problem of ﬁnding an
entity set 𝑅 of size 𝐾 that maximizes the objective function 𝑄(𝑅, 𝑆)
is NP-Hard.

The hardness of this problem can be proved by reduction from
the maximum clique problem [9]. Details of the proof can be found
in Appendix 8.1.

4. ALGORITHMS FOR SET EXPANSION

Given Theorem 1 which states that it is NP-Hard to ﬁnd the
optimal expanded seed set (ESS), we in this section propose two
greedy algorithms, the static thresholding algorithm and the dy-
namic thresholding algorithm, that iteratively reﬁne a candidate
ESS 𝑅 of size 𝐾 to maximize 𝑄(𝑅, 𝑆) (Deﬁnition 6). Both al-
gorithms are built on top of an automatic score thresholding tech-
nique. In this section, we ﬁrst outline two algorithms, then describe
the automatic score thresholding method, and ﬁnally, we discuss
the connection of our proposed algorithms and the standard ran-
dom walk based approaches.
4.1

Iterative Similarity Aggregation

On the high level, the static thresholding algorithm ﬁxes the size
of ESS 𝑅 at the beginning, and then iteratively searches for terms in
𝑅 to maximize 𝑄(𝑅, 𝑆); while the dynamic thresholding algorithm
reﬁnes both the size of 𝑅 and contents of 𝑅 at the same time in each
iteration.
4.1.1
The static thresholding algorithm starts with a good guess of
ESS, then iteratively improve the quality metric as deﬁned in Deﬁ-
nition 6 by replacing one entity in the ESS of the previous iteration,
until the computation of ESS converges and a local maximum of
the quality score is reached. The pseudo-code of the algorithm is
described in Algorithm 1.

Static Thresholding Algorithm

The static thresholding algorithm takes two parameters, the set
of seed entities, 𝑠𝑒𝑒𝑑𝑠, and the 𝑔𝑟𝑎𝑝ℎ with all candidate 𝑡𝑒𝑟𝑚𝑠 as
left side nodes. We start by computing the relevance score of each
term with the seeds 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑠𝑒𝑒𝑑𝑠), as deﬁned in Deﬁnition 4,
in the ﬁrst for loop. We then rank the terms according to their rel-
evance scores, and pick top 𝐾 ranked terms as the initial estimate
of the ESS, 𝑅0, where the threshold value 𝐾 is determined by a
thresholding analysis of the score distribution that will be detailed
in Section 4.2.

In the subsequent iterations in the while loop, we iteratively com-
pute the new candidate ESS 𝑅𝑖𝑡𝑒𝑟 based on 𝑅𝑖𝑡𝑒𝑟−1 of the previous
iteration and progressively improve the overall quality score of the
ESS until a local maximum is reached. Speciﬁcally, in each itera-
tion 𝑖𝑡𝑒𝑟, we compute the relevance score of each candidate term
𝑡𝑒𝑟𝑚𝑖 with the previous ESS 𝑅𝑖𝑡𝑒𝑟−1, 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑅𝑖𝑡𝑒𝑟−1), and
the corresponding ranking function 𝑔(𝑡𝑒𝑟𝑚𝑖) which is a weighted
combination of the relevance score with the 𝑠𝑒𝑒𝑑𝑠, and the rele-
vance score with 𝑅𝑖𝑡𝑒𝑟−1. We then sort the candidate terms by
𝑖𝑡𝑒𝑟 ∕= 𝑅𝑖𝑡𝑒𝑟−1,
𝑔(𝑡𝑒𝑟𝑚𝑖). Let the top ranked 𝐾 terms be 𝑅′
we replace the lowest ranked term in 𝑅𝑖𝑡𝑒𝑟−1 with the top ranked
term 𝑟 ∈ 𝑅′
𝑖𝑡𝑒𝑟 that is not in 𝑅𝑖𝑡𝑒𝑟−1, and continue the iteration;
otherwise we have converged and will stop and return 𝑅𝑖𝑡𝑒𝑟−1 as
the result of ESS.

𝑖𝑡𝑒𝑟, if 𝑅′

We use the following running example to demonstrate how the
static thresholding algorithm works to compute ESS, which can
then be used to rank candidate terms for set expansion.

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India430Algorithm 1 Static Thresholding Algorithm

Static_Thresholding (𝑠𝑒𝑒𝑑𝑠, 𝑔𝑟𝑎𝑝ℎ)
for each 𝑡𝑒𝑟𝑚𝑖 in 𝑔𝑟𝑎𝑝ℎ.𝑡𝑒𝑟𝑚𝑠 do

𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖] ← 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑠𝑒𝑒𝑑𝑠)

end for
sort 𝑡𝑒𝑟𝑚𝑖 by 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖] desc
𝐾 ← Pick_Threshold(𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖])
𝑅0 ← the top 𝐾 ranked terms by 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖]
𝑖𝑡𝑒𝑟 ← 1
while true do

for each 𝑡𝑒𝑟𝑚𝑖 in 𝑔𝑟𝑎𝑝ℎ.𝑡𝑒𝑟𝑚𝑠 do

𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[𝑖] ← 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑅𝑖𝑡𝑒𝑟−1)
𝑔(𝑡𝑒𝑟𝑚𝑖) ← 𝛼 ∗ 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖] + (1 − 𝛼) ∗ 𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[𝑖]

end for
sort 𝑡𝑒𝑟𝑚𝑖 by 𝑔(𝑡𝑒𝑟𝑚𝑖) desc
𝑖𝑡𝑒𝑟 ← the top 𝐾 terms by 𝑔(𝑡𝑒𝑟𝑚𝑖)
𝑅′
𝑖𝑡𝑒𝑟 ∕= 𝑅𝑖𝑡𝑒𝑟−1 then
if 𝑅′
let 𝑟 ∈ 𝑅′
let 𝑞 ∈ 𝑅𝑖𝑡𝑒𝑟 be the last ranked term in 𝑅𝑖𝑡𝑒𝑟−1
𝑅𝑖𝑡𝑒𝑟 ← (𝑅𝑖𝑡𝑒𝑟−1 ∪ {𝑟}) − {𝑞}
𝑅𝑖𝑡𝑒𝑟 ← 𝑅𝑖𝑡𝑒𝑟−1
break

𝑖𝑡𝑒𝑟 be the top ranked term not in 𝑅𝑖𝑡𝑒𝑟−1

else

end if
𝑖𝑡𝑒𝑟 + +

end while
return 𝑅𝑖𝑡𝑒𝑟

𝑀 =

EXAMPLE 2. Let 𝑈 = {𝐴, 𝐵, 𝐶, 𝐷, 𝐸, 𝐹} be the set of 6
terms that we consider in this example, in which 𝑆 = {𝐴, 𝐵} is
the input seed set. Let the pair-wise similarity matrix 𝑀 be

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)

A
B
C
D
E
F

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)

𝐹
𝐴 𝐵 𝐶 𝐷 𝐸
0.5 0.8 0.7 0.6 0.7
1
0.6 0.7 0.7 0.5
0.5
1
0
0.8 0.6
1
0.9 0.3
0.8 0.5
0.7 0.7
1
0
0.4
0.6 0.7 0.9 0.8
1
0.3 0.5 0.3 0.5 0.4
1

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)

where each entry stands for the similarity score of the two corre-
sponding nodes using some similarity metric 𝑆𝑖𝑚 : 𝑈 × 𝑈 →
[0, 1].
For each term 𝑡𝑒𝑟𝑚𝑖 ∈ 𝑈, we ﬁrst compute its relevance score
with the seed set, 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑆). For example, 𝑆𝑟𝑒𝑙(𝐴, 𝑆) =
(𝑀 (𝐴, 𝐴) +𝑀 (𝐴, 𝐵)) = 0.75, 𝑆𝑟𝑒𝑙(𝐵, 𝑆) = 1
1
(𝑀 (𝐵, 𝐵) +
2
2
𝑀 (𝐵, 𝐴)) = 0.75, etc. This gives rise to the 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[] =
{0.75, 0.75, 0.7, 0, 7, 0.65, 0.6}.

Next we invoke the thresholding algorithm (to be detailed in Sec-
tion 4.2), which analyzes the score distribution to ﬁnd a natural
threshold point that separates the high scoring entities from the re-
maining background entities. We use the number of entities above
the threshold as the estimate of the size of ESS. In this particular
case, suppose the thresholding algorithm returns 𝐾 = 4, which
gives us 𝑅0 = {𝐴, 𝐵, 𝐶, 𝐷}.
In the ﬁrst iteration (𝑖𝑡𝑒𝑟 = 1), we compute for each term 𝑡𝑒𝑟𝑚𝑖 ∈
𝑈 its similarity score with the previous estimate of ESS, 𝑅0, to de-
rive 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑅0). For example, 𝑆𝑟𝑒𝑙(𝐴, 𝑅0) = 1
(𝑀 (𝐴, 𝐴)+
4
𝑀 (𝐴, 𝐵) + 𝑀 (𝐴, 𝐶) + 𝑀 (𝐴, 𝐷)) = 0.75, while 𝑆𝑟𝑒𝑙(𝐵, 𝑅0) =
1
(𝑀 (𝐵, 𝐴) + 𝑀 (𝐵, 𝐵) + 𝑀 (𝐵, 𝐶) + 𝑀 (𝐵, 𝐷)) = 0.7, so on
and so forth. This leads to 𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[] = {0.75, 0.7, 0.6, 0.6,
4
0.75, 0.5}.

Given the weight constant 𝛼 = 0.5 as used in the quality metric,
∗ 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[] =

∗ 𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[] + 1

the ranking scores 𝑔[] = 1
2

2

{0.75, 0.725, 0.65, 0.65, 0.7, 0.55}. Sorting the terms in 𝑈 again,
1 = {𝐴, 𝐵,
we get a different ordering (𝐴, 𝐵, 𝐸, 𝐶, 𝐷, 𝐹 ). Let 𝑅′
𝐸, 𝐶} be the top ranked 𝐾 = 4 terms in the new ordering. Given
1 ∕= 𝑅0, 𝐷 is the last ranked term in 𝑅0, and 𝐸 the top
that 𝑅′
1 but not in 𝑅0, we get 𝑅1 = 𝑅0−{𝐷} +{𝐸} =
ranked term in 𝑅′
{𝐴, 𝐵, 𝐶, 𝐸}.
In the second iteration (𝑖𝑡𝑒𝑟 = 2), we re-compute the simi-
larity score 𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[] = {0.725, 0.7, 0.825, 0.55, 0.8, 0.375}.
Combining that with 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[] = {0.75, 0.75, 0.7, 0.7, 0.65,
0.6} we have the new ranking scores 𝑔[] = {0.7375, 0.725, 0.7625,
0.625, 0.725, 0.4875}. The new top ranked 𝐾 = 4 terms in 𝑈 is
2 = {𝐶, 𝐴, 𝐵, 𝐸}. Observe that 𝑅′
𝑅′
2 == 𝑅1, we can now stop
the iteration and use the ranking scores 𝑔[] to generate a ranked
list of terms, (𝐶, 𝐴, 𝐵, 𝐸, 𝐷, 𝐹 ).

THEOREM 2. The computation of 𝑅𝑖𝑡𝑒𝑟 in Algorithm 1 is guar-
anteed to converge, thus the while loop in Algorithm 1 is bound to
terminate.

Theorem 2 states the nice property that after a ﬁxed number of
iterations, the computation of 𝑅𝑖𝑡𝑒𝑟 will converge and stops chang-
ing for subsequent iterations. Here to outline the intuition of the
proof of the convergence of our algorithm, we note that in our com-
putation of ESS, we are implicitly maximizing the quality score
of ESS. We show that this quality function will monotonically in-
crease in each iteration, until reaching a local maximum, at which
point it will converge and stop. Details of the proof can be found in
Appendix 8.2.

While the algorithm is bound to converge, we don’t have an up-
per bound on the number of iterations it may take before it stops.
However in our experiments, we observe that it converges quickly
and typically takes only a small number of iterations (less than 10).
The reason we term this algorithm static thresholding is due to
the way the estimated size of ESS, 𝐾, is determined. In this static
thresholding algorithm, once threshold 𝐾 is computed in the ﬁrst
iteration, it will stay the same in subsequent iterations. In the fol-
lowing section, we will present a different variant of the algorithm
in which 𝐾 changes from iteration to iteration.
4.1.2 Dynamic Thresholding Algorithm
While the static thresholding algorithm described in Section 4.1.1
is proven to converge, its use of the static threshold (the parameter
𝐾 in Algorithm 1) as computed in the ﬁrst iteration may not accu-
rately reﬂect the actual size of the ESS. It can be the case that in
subsequent iterations with iterative score computation it becomes
clear that based on the new score distribution, the new threshold
value – which is interpreted as the size of the ESS – is signiﬁcantly
different from the initial estimate derived from the score distribu-
tion for the ﬁrst iteration. To overcome this issue, we in this sec-
tion propose a dynamic thresholding algorithm that iteratively use
the new threshold value of the current score distribution to adjusts
the estimated size of ESS. The algorithm is described in detail in
Algorithm 2.

The structure of this algorithm is similar to Algorithm 1. We ﬁrst
compute the relevance score between each candidate term 𝑡𝑒𝑟𝑚𝑖
and the seeds. We then again invoke the thresholding procedure to
ﬁnd a good threshold value 𝐾0, and use the top ranked 𝐾0 terms
as the initial ESS, 𝑅0.

In each subsequent iteration, we again rank each term 𝑡𝑒𝑟𝑚𝑖,
using the ranking function 𝑔(𝑡𝑒𝑟𝑚𝑖). Based on the new score dis-
tribution computed using 𝑔(𝑡𝑒𝑟𝑚𝑖), we re-invoke the automatic
thresholding procedure to determine a new estimate of size of ESS,
𝐾𝑖𝑡𝑒𝑟. Observe that instead of using the initial threshold 𝐾0 com-
puted in the ﬁrst iteration as in the static thresholding algorithm,

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India431Algorithm 2 Dynamic Thresholding Algorithm

Dynamic_Thresholding (𝑠𝑒𝑒𝑑𝑠, 𝑔𝑟𝑎𝑝ℎ)
for each 𝑡𝑒𝑟𝑚𝑖 in 𝑔𝑟𝑎𝑝ℎ.𝑡𝑒𝑟𝑚𝑠 do
𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖] ← 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑠𝑒𝑒𝑑𝑠)
end for
𝐾0 ← Pick_Threshold(𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖])
sort 𝑡𝑒𝑟𝑚𝑖 by 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖] desc
𝑅0 ← the top ranked 𝐾0 terms by 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖]
𝑖𝑡𝑒𝑟 ← 1
while 𝑖𝑡𝑒𝑟 ≤ MAX_ITER do

for each 𝑡𝑒𝑟𝑚𝑖 in 𝑔𝑟𝑎𝑝ℎ.𝑡𝑒𝑟𝑚𝑠 do

𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[𝑖] ← 𝑆𝑟𝑒𝑙(𝑡𝑒𝑟𝑚𝑖, 𝑅𝑖𝑡𝑒𝑟−1)
𝑔(𝑡𝑒𝑟𝑚𝑖) ← 𝛼 ∗ 𝑆𝑖𝑚_𝑆𝑐𝑜𝑟𝑒[𝑖] + (1 − 𝛼) ∗ 𝑅𝑒𝑙_𝑆𝑐𝑜𝑟𝑒[𝑖]

end for
𝐾𝑖𝑡𝑒𝑟 ← Pick_Threshold(𝑔(𝑡𝑒𝑟𝑚𝑖))
sort 𝑡𝑒𝑟𝑚𝑖 by 𝑔(𝑡𝑒𝑟𝑚𝑖) desc
𝑖 ← the top ranked 𝐾𝑖𝑡𝑒𝑟 terms by 𝑔(𝑡𝑒𝑟𝑚𝑖)
𝑅′
𝑅𝑖𝑡𝑒𝑟 ← 𝑅′
𝑖𝑡𝑒𝑟 + +

𝑖𝑡𝑒𝑟

end while
return 𝑅𝑖𝑡𝑒𝑟

Prob.

Figure 4: Automatic thresholding to segment images

image on the left has a hand in the foreground and a dark back-
ground. The task of image segmentation is to separate the fore-
ground hand from the background to get the image on the right
hand side. Each pixel in the image has a gray scale value that again
is assumed to follow two distributions: those belong to the fore-
ground and those belong to the background.

In the computer graphics literature, a number of thresholding al-
gorithms have been proposed and shown to be effective, including
the Iterative Threshold Selection [12] and Otsu’s thresholding [10].
We in this work adopt the Otsu’s thresholding to ﬁnd the good
threshold point that naturally separates those high scoring terms
from those background terms. Formally, Otsu’s threshold is de-
ﬁned as follows.

DEFINITION 7. Let 𝜔1, 𝜔2 be the probabilities of the two classes
separated by a threshold 𝑡, and 𝜎2
2 be the variances of these
two classes. The weighted sum of the variances of the two classes
2(𝑡). The Otsu’s thresh-
is 𝑓 (𝑡) = 𝜎2
old 𝑇 is deﬁned as the one that minimizes 𝑓 (𝑡), or equivalently,
𝑇 = arg min

𝑤(𝑡) = 𝜔1(𝑡)𝜎2

1(𝑡) +𝜔 2(𝑡)𝜎2

1, 𝜎2

(𝑓 (𝑥)).

Score

𝑥

Figure 3: Thresholding to separate two score distributions

we recompute the threshold based on the new score distribution.
This dynamic thresholding technique adapts to the changes in the
score distribution and may be able to reﬂect the size of the ESS
more accurately. In practice we observe that this algorithm slightly
outperforms the static thresholding algorithm.

However, since we are dynamically changing the thresholding
value in the dynamic thresholding algorithm, we cannot guarantee
the convergence property as in the static thresholding algorithm.
Therefore we place a loop-termination condition which is the max-
imum number of iterations to execute for efﬁciency consideration.
In practice we use small number of iterations (e.g., 5) and observe
reasonable performance.
4.2 Automatic Score Thresholding

The subproblem we look at in this section, is to automatically
determine the natural threshold that best separates two underlying
score distributions from one score distribution. This problem arises
when we have a set of scores, each of which represents an estima-
tion of the likelihood of the term being a member of the “concept
set” we are trying to uncover. The assumption here is that those
terms that really belong to the “concept set” will have higher scores,
and follows some kind of score distribution as in the right curve in
Figure 3; while those that do not belong to the set will have lower
scores, but also follow a score distribution as the left curve in Fig-
ure 3. Under this assumption, the problem becomes the classical
score thresholding problem, and we tap into existing literature to
solve this thresholding problem.

In particular, the same problem arises in image segmentation in
the computer graphics, where the goal is to separate the foreground
image from the background images. For example, in Figure 4, the

Brieﬂy, Otsu’s technique sees the two sets of scores separated
by the threshold as two clusters.
It is based on the observation
that the threshold that best separates the two clusters is the point
with the least intra-cluster variances. Therefore, Otsu’s threshold-
ing uses the sum of the two intra-cluster variances as the objec-
tive function, and searches for the point that minimizes the sum
of the intra-cluster variances as the threshold. We observe that in
our experiments, this thresholding technique outperforms alterna-
tive thresholding techniques we experimented with. Given a sorted
list of scores, the Otsu’s threshold can be computed linearly.
4.3 Discussions

Having described the details of our algorithm, here we discuss
some major differences compared with existing random walk based
approaches.

Our algorithms in its essence take an iterative score computation
process, which is very similar to, say random walk based ranking
algorithms which also compute scores iteratively. How is our iter-
ative score computation algorithm different from the general ran-
dom walk? More importantly, as we have noted, we observe in our
experiments that our algorithm outperforms the random walk based
approaches. It is thus interesting to explore the differences between
our approach and the random walk based algorithms that lead to the
divergence in performance.

First, we would like to point out the key similarities between
our iterative score computation and random walk algorithms. If we
were to build a 𝑁 ×𝑁 similarity matrix 𝑀 where each entry (𝑥, 𝑦)
denotes the similarity of terms 𝑥 and 𝑦 on the left-side of the bipar-
tite graph, this matrix essentially represents a graph between terms
(instead of graph between terms and contexts). In addition, in each
iteration where we aggregate score computation, we are essentially
doing a matrix multiplication 𝛼𝑀 × 𝑉 + (1− 𝛼)𝑀 × 𝑉 ′
in which

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India432𝑉 is the (0, 1) vector of size 𝑁 that represents the given seeds,
and 𝑉 ′
is a (0, 1) vector that represents the ESS in the previous
iteration. The computation framework bears some similarity with
random walk based approach.

However, we note that there are at least three important aspects
that differentiate our algorithm from the random walk based ap-
proaches. First, we use a score thresholding mechanism as dis-
cussed in Section 4.2 to ﬁnd a good ESS (equivalently, the multipli-
cation vector 𝑉 ), which tends to be very small in size (comparing to
the set of universal terms), as opposed to normal random walk that
does not have such score thresholding. As the result, we only prop-
agate probability (or, scores) through conﬁdent nodes. Secondly,
we normalize scores in each iteration differently. The aggregate
score we compute is a weighted sum of the score computed against
the given seeds and the score computed against the ESS in each
iteration (catching both relevance and coherence), which is differ-
ent from typical random walks that only take care of relevance to
the seeds. Lastly, we treat each entity in the ESS equally and reset
their weights to be “1” after each iteration, which is again different
from random walks. Given the speciﬁc set-expansion application
we are targeting at, where each entity in the set should be con-
ceptually treated equally, the reset of weight is reasonable given a
robust automatic score thresholding mechanism. It turns out in our
experimental evaluation that these aspects are critical to achieving
reasonable set expansion performances.

5. EXPERIMENTS
5.1 Experimental Setup
5.1.1 Data Set Preparation
As we have alluded before, we experiment with two types of
data, the web lists and the query log. Both of them are modeled
as bipartite graph as described in Section 3.1. In particular, for the
web list data, we use crawlers to extract around 6 million lists in
HTML pages from the web, and the resulting bipartite graph has
58, 550, 245 edges. For the query log data, we obtain a sample
of user queries from Bing, and the resulting bipartite graph has
94, 859, 646 edges.

To evaluate the effectiveness of our algorithms, we select four
sets of concepts over which set expansion experiments are con-
ducted, namely, country names, colors, camera brands and mattress
brands. The “ground truth”, or the set of entities that is considered
to belong to each concept set are determined as follows. For coun-
try names and colors, we resort to Wikipedia and use the list of
countries in [2] and the list of atomic web browser colors in [3],
respectively. For camera brands and mattress brands, we obtained
the manually created lists from domain experts. We selected these
four categories because (1) they are across different domains; and
(2) they have different degree of difﬁculty for set expansion.

In order to compare the performance of the our algorithms and
the existing techniques, we randomly pick 6 entities from the ground
truth data for each concept set as the input starting seeds. Since
each algorithm returns a ranked list of results, we evaluate the per-
formance of these competing algorithms by measuring the preci-
sion/recall values of each algorithm at different rank positions.
5.1.2 Algorithms/Systems Compared
We conduct three broad groups of experiments to thoroughly un-
derstand the effectiveness of our set expansion algorithms. In the
ﬁrst set of experiments we compare the algorithms proposed in this
work with the state-of-art random-walk based ranking algorithms
used for set expansion, over the same web list/query log data. We

implemented two random-walk based ranking algorithm, the regu-
lar random-walk with ﬁxed teleport probability as used in [17, 18,
19], and the Adsorption random walk algorithm [5, 15, 20], which
essentially is a variant of the regular random-walk, which penal-
izes popular nodes by customizing the teleport probability based
on the edge-degree of each node. In both cases the random walk is
performed on the bipartite graph model as described in Section 3.1.
In addition, we conduct a second group of experiments that com-
pares our algorithms with the two existing software set expansion
systems, namely the SEAL set expansion system [16] and the Google
Sets system [1] through their respective public web portals. Since
SEAL interface only accepts up to 3 seeds, we pick 3 entities as
seeds in this set of experiments.

Lastly, we drill down to the dynamic thresholding algorithm (the
behavior of the static thresholding algorithm is similar) proposed in
this work, and present a set of experiments in which we vary vari-
ous parameters. Speciﬁcally, we vary the parameter 𝛼, the number
of seeds used, and the similarity metric used, etc. This allows us to
analyze and to better understand the performance characteristics of
our algorithm in response to the changes in parameter values.
5.2 Experimental Results
5.2.1 Example Output

Country
germany
japan
belgium
denmark
canada
spain

poland
sweden
norway
austria
peru
croatia

netherlands

hungary

italy

czech republic

Color
blue
red
white
green
yellow
brown
black
purple
pink
gray
orange
violet
silver
grey
gold

greyish green

switzerland

slovakia
china
slovenia

bronze
beige
navy
tan

Camera
olympus
nikon
kodak
pentax
canon
casio
fuji

panasonic

leica
fujiﬁlm
sony
ricoh
vivitar
sigma

Mattress

serta
sealy

simmons
beautyrest

sealy posturepedic

stearns foster

bassett

tempur pedic

sears

spring air
gold bond

jobri

universal furniture

champlain

konica minolta

samsung
minolta
polaroid
vistaquest

rollei

fbg

coapt systems

riverside furniture

hyla vacuum

broyhill

lifestyle solutions

Table 1: Top-20 results by Dynamic Thresholding algorithm

Table 1 lists the top 20 ranked results produced by dynamic
thresholding algorithm for the four domains that we experiment
with. The static thresholding algorithm reports similar results, as
we will see later in the precision/recall curves. In each domain,
those terms in boldface are the input seeds. The underlined terms
are the results that do not belong to the ground truth set and thus
counted as incorrect results; while the remaining terms are correct
results expanded from the input seeds.

From Table 1, we can see that in the top-20 ranked results, the
“Country” and “Camera” domains have perfect precision. “Color”
domain has only one incorrect result “greyish green” (which al-
though being a real composite color, is nonetheless not included in
our ground truth set which only includes atomic colors). The top-
20 results for “Mattress”, however, includes many noisy entires that
are incorrect, including product line names (“sealy posturepedic”
and “beautyrest”), and furniture retailers (“universal furniture” and
“riverside furniture”).

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India4331

0.5

l
l

a
c
e
R

0

0

1

0.5

l
l

a
c
e
R

0

0

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

0.5

Precision
(a) Country

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

0.5

Precision
(c) Camera

1

l
l

a
c
e
R

0.5

1

0

0

0.8

0.6

l
l

a
c
e
R

0.4

0.2

0

0

1

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

0.5

Precision

(b) Color

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

1

0.5

l
l

a
c
e
R

1

0

0

0.6

0.4

0.2

l
l

a
c
e
R

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

0.5

Precision
(a) Country

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

0.6

0.4

l
l

a
c
e
R

0.2

0

0

1

0.6

0.4

l
l

a
c
e
R

0.2

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

0.5

Precision

(b) Color

1

Dynamic(cid:3)Threshold
Static(cid:3)Threshold
Random(cid:3)Walk
Random(cid:3)Walk(cid:3)Adsp

1

0

0

0.5

Precision
(d) Mattress

0.5

Precision
(c) Camera

0

0

1

0.5

Precision
(d) Mattress

1

Figure 5: Comparison with Random-Walk based approach using
Web List data

Figure 6: Comparison with Random-Walk based approach using
Query Log data

Observing the patterns of the incorrect results in the ranked list,
we further adopt a token-based subset/superset-ﬁltering heuristics
to remove results that are likely to be incorrect. Speciﬁcally, we
remove a result in the ranked list if there is another result that ranks
higher in the list whose token set is a subset/superset of the cur-
rent result. The intuition here is that for a pair of results whose
token sets are subset-superset, the concepts that they represent also
tend to constitute a semantic superclass-subclass hierarchy. Given
that we are expanding the given seeds to produce a coherent con-
cept set, one and at most one of the two entities in the superclass-
subclass hierarchy can be correct. Therefore, we only pick the re-
sult that ranks higher (a more conﬁdent prediction) and ignore sub-
set/superset results that rank lower. As an example, since “sealy”
ranks higher in the list for “Mattress”, we will not consider the su-
perset result “sealy posturepedic” that ranks lower, which is only
a product line, or a subclass of the desired superclass concept, the
manufacturer/brand name “sealy”. Similarly “greyish green” will
not be considered since the subset “green” is also a result that ranks
higher. This simple heuristic turns out to be useful and boosts the
precision/recall results. We apply this token-based subset/superset-
ﬁltering for all algorithms/systems in our experiments.
5.2.2 Comparison With Random Walk
In this section we present the performance comparison between
the iterative thresholding algorithm proposed in this work, and the
random walk based ranking algorithms.

For the random walk based approaches, we report experimen-
tal results for both the regular random walk [17, 18, 19], and the
Adsorption random walk [5, 15, 20], on the bipartite graph as mod-
eled in Section 3.1. They are reported as the “Random Walk” and
“Random Walk Adsp” curves. We set teleport probability as 0.2.

Figure 5 shows the performance results on the Web List data for
each of the four domains that we experimented with. Note that
while Dynamic Thresholding is slightly better than Static Thresh-

olding, both approaches signiﬁcantly outperform random walk based
approaches. This conﬁrms that simple random walk based ap-
proach is not able to handle noisy data well, as we analyzed in
Section 4.3.

Figure 6 presents the same experiments run on the Query Log
data. The general trends observed in Figure 5 is conﬁrmed: Dy-
namic Thresholding and Static Thresholding outperforms the ran-
dom walk based approaches quite signiﬁcantly. We note, however,
that the precision/recall results observed on Query Log data are not
as good as the those on Web List data. This is not entirely surpris-
ing, as the query log tends to be much noisier than the HTML web
lists.

5.2.3 Comparison with Google Sets and SEAL
We additionally conduct a second group of experiments that com-
pares our algorithms against two existing systems, SEAL [16] and
Google Sets [1] 1. As we stated previously, since we do not have
complete details of the implementation of the algorithms or the
back-end data set used by either of the SEAL/Google Sets systems,
the performance numbers do not directly compare. However, we
still feel that reporting these results is useful in that it puts the per-
formance of our algorithm in the context of start-of-art existing sys-
tems, and helps us to understand the usefulness of the algorithms.
Figure 7 summarizes the performance comparison between Dy-
namic Thresholding that we propose, and SEAL/Google Sets. Since
web interface for Google Sets can take up to 5 seeds, while SEAL
can only allow for 3 seeds as input, in this set of experiments we
only use 3 seeds as input to all three algorithms. In addition, as
Google Sets only returns 50 results at most, its performance curve
is incomplete (especially for the “Country” domain, where we re-
port precision/recall up to the top-ranked 500 terms given that the

1performance numbers for SEAL and Google Sets were obtained
on 10/01/2010

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India434l
l

a
c
e
R
/
n
o
i
s
i
c
e
r
P

0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
0.64
0.62
0.6

Precision
Recall

l
l

a
c
e
r
/
n
o
i
s
i
c
e
r
p

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

DT(cid:3)3(cid:3)seeds
Google(cid:3)Set(cid:3)3(cid:3)seeds
SEAL(cid:3)3(cid:3)seeds

0

0.5

Precision

1

0

0.3

0.5
alpha

0.8

1

(b) Color

(a) Camera

Precision
Recall

0

0.3

0.5
alpha

0.8

1

(b) Mattress

l
l

a
c
e
R

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

l
l

a
c
e
R

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0

l
l

a
c
e
R

1

l
l

a
c
e
R

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

DT(cid:3)3(cid:3)seeds
Google(cid:3)Set(cid:3)3(cid:3)seeds
SEAL(cid:3)3(cid:3)seeds

0.5

Precision
(a) Country

DT(cid:3)3(cid:3)seeds
Google(cid:3)Set(cid:3)3(cid:3)seeds
SEAL(cid:3)3(cid:3)seeds

1

0.5

Precision
(c) Camera

DT(cid:3)3(cid:3)seeds
Google(cid:3)Set(cid:3)3(cid:3)seeds
SEAL(cid:3)3(cid:3)seeds

0

0.5

Precision
(d) Mattress

1

l
l

a
c
e
R

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Jaccard
Cosine

Figure 8: Sensitivity analysis to 𝛼

#(cid:3)seed(cid:3)=(cid:3)2
#(cid:3)seed(cid:3)=(cid:3)4

l
l

a
c
e
R

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.5

Precision

1

0

0.2

0.4

Precision

0.6

0.8

(a) Vary similarity metric

(b) Vary # of seeds

Figure 9: Sensitivity analysis

mance in most cases, and adopt 𝛼 = 0.5 in all remaining experi-
ments.

Next, in Figure 9a we vary the similarity metric used in our
Dynamic Thresholding algorithm for the “Camera” domain. Re-
call that our algorithm is a general framework that can work with
any similarity measurements. In this experiment we report perfor-
mance of our algorithm using both Jaccard similarity and the Co-
sine similarity metric. As can be seen in Figure 9a, while both ap-
proaches perform reasonably well, the Cosine similarity is slightly
better. Similar trends are also observed in experiments over other
domains, conﬁrming the performance advantage of using Cosine
similarity. Therefore, we only report performance using Cosine
similarity metric in all other experiments.

Finally, in Figure 9b, we vary the number of input seeds and
report the corresponding set expansion performance. Speciﬁcally,
given the 6 seeds we used for each of the four domains in the pre-
vious experiments, we pick all possible 2-seed combination out of
these 6 seeds, which gives us a total of 15 such combinations. Sim-
ilarly we pick all possible 4-seed combination out of the 6 seeds
which again gives us 15 possibilities. We then use all 15 combina-
tions of 2/4 seeds as input, to test the performance of our Dynamic
Thresholding algorithm. The results are reported in Figure 9b. The
overall trend that stands out in this ﬁgure is that the performance of
our algorithm with 4 seeds is in general much better and more sta-
ble than the case where only 2 seeds are used as input. Observe that
to the lower left corner, there are two instances of 2-seed combina-
tions that lead to extremely low precision/recall. This suggests that
our algorithm is more robust when a reasonable number of seeds
are given, and the performance may ﬂuctuate with very few num-
ber of seeds, largely depending on the quality of the seeds given.
However, we believe that it is not really hard to ﬁnd 4 input seeds
in virtually any domain, thus not a stringent requirement in general,
ensuring the usefulness of our algorithm.

Figure 7: Comparison with Google Sets and SEAL

ground-truth set is much larger for “Country”). The general ob-
servation is that while Dynamic Thresholding and Google Sets per-
form roughly the same in “Country” and “Color”, Dynamic Thresh-
olding has a slightly better precision/recall curve for “Camera” and
“Mattress”. Furthermore, in each of the four domains Dynamic
Thresholding seems to outperforms the SEAL system. Neverthe-
less, we once again emphasize that this is by no means an implica-
tion of the relative performance of these algorithms. The difference
in performance may simply because of the different data sets each
system uses. It does suggest, however, that the algorithm we de-
velop is competitive against existing set-expansion systems.

Sensitivity Analysis to Parameters

5.2.4
To better understand the performance characteristics of our pro-
posed approaches, we in this section conduct sensitivity analysis
to understand the impact of various parameters to our algorithm.
Again, we use the dynamic thresholding algorithm as example.

Figure 8 depicts the performance of the Dynamic Thresholding
algorithm with varied parameter 𝛼 in domain “Camera” and “Mat-
tress”. Recall that in Deﬁnition 4, 𝛼 is the essentially the weight
parameter used to balance the quality metric of the expanded seed
set (ESS) between relevance and coherence. 𝛼 = 0 means that we
only consider the coherence of ESS, while 𝛼 = 1 indicates that
only relevance is taken into account. Any value of 𝛼 in between
suggests a combination of both of these two metrics. In both Fig-
ure 8a and Figure 8b we can see that when 𝛼 gets extreme values (0
or 1, meaning only one of the relevance and coherence metrics is
considered), precision/recall performance numbers suffer. On the
other hand, 𝛼 values in between boosts the performance of our al-
gorithm. While we only report performance for domain “Camera”
and “Mattress” here, similar trends are observed in other domains.
In general, we observe that 𝛼 = 0.5 usually gives the best perfor-

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India4356. CONCLUSION

In this paper we studied the problem of using general-purpose
web data (web lists and query logs) to expand a set of seed entities.
We proposed a simple yet effective quality metric to measure the
expanded set, and designed two iterative thresholding algorithms
to rank candidate entities. We validated our approach using exper-
iments conducted on multiple domains, and concluded that our al-
gorithm outperforms existing techniques for set expansion on noisy
web data.

7. REFERENCES
[1] Google Sets: http://labs.google.com/sets.
[2] List of United Nations member states.

http://en.wikipedia.org/wiki/united_nations_member_states.

[3] Web colors. http://en.wikipedia.org/wiki/web_colors.
[4] E. Agichtein and L. Gravano. Snowball: extracting relations from

large plain-text collection. In JCDL, 2000.

[5] S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar,

D. Ravichandran, and M. Aly. Video suggestion and discovery for
youtube: Taking random walks through the view graph. In WWW,
2008.

[6] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked,

S. Soderland, D. S. Weld, and A. Yates. Web-scale information
extraction in KnowItAll. In WWW, 2004.

[7] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, T. Shaked,

S. Soderland, D. S. Weld, and A. Yates. Unsupervised named-entity
extraction from the web: An experimental study. In Artiﬁcal
Intelligence, 2005.

[8] Z. Ghahramani and K. A. Heller. Bayesian sets. In NIPS, 2005.
[9] R. M. Karp. Reducibility among combinatorial problems. Complexity

of Computer Computations, 1972.

[10] N. Otsu. A threshold selection method from gray-level histograms.

IEEE Transactions on Systems, Man and Cybernetics, 1979.

[11] M. S. Pang-Ning Tan and V. Kumar. Introduction to Data Mining.

2005.

[12] T. Ridler and S. Calvard. Picture thresholding using an iterative

selection method. IEEE Transactions on Systems, Man and
Cybernetics, 1978.

[13] B. Settles. Biomedical named entity recognition using conditional

random ﬁelds and rich feature sets. In CoLING, 2004.

[14] P. P. Talukdar, T. Brants, M. Liberman, and F. Pereira. A context
pattern induction method for named entity extraction. In CoNLP,
2006.

[15] P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, R. Bhagat,

and F. Pereira. Weakly-supervised acquisition of labeled class
instances using graph random walks. In EMNLP, 2008.
[16] R. Wang and W. Cohen. SEAL: http://rcwang.com/seal.
[17] R. Wang and W. Cohen. Language-independent set expansion of

named entities using the web. In ICDM, 2007.

[18] R. Wang and W. Cohen. Iterative set expansion of named entity using

the web. In ICDM, 2008.

[19] R. Wang and W. Cohen. Character-level analysis of semi-structured

documents for set expansion. In EMNLP, 2009.

[20] Y.-Y. Wang, R. Hoffmann, X. Li, and J. Szymanski. Semi-supervised
learning of semantic classes for query understanding. In CIKM, 2009.

8. APPENDIX
8.1 Proof of Theorem 1

We show that there exists a polynomial-time reduction from the
Maximum-Clique problem to the problem we stated in Section 3.4.
For any given graph 𝐺 = {𝑉, 𝐸} for which the Maximum-
Clique needs to be computed, we can build a corresponding simi-
larity matrix 𝑀 of size ∣𝑉 ∣×∣𝑉 ∣, where each row 𝑟𝑖 corresponds to
vertex 𝑣𝑖 ∈ 𝑉 , and each column 𝑐𝑗 corresponds to vertex 𝑣𝑗 ∈ 𝑉 .
The matrix entry 𝑀 (𝑟𝑖, 𝑐𝑗) is 1 if there is the edge (𝑣𝑖, 𝑣𝑗) ∈ 𝐸,
otherwise 𝑀 (𝑟𝑖, 𝑐𝑗) = 0.

Given this construction of this similarity matrix 𝑀, we prove
the claim by contradiction. Suppose there is an algorithm 𝐴 that
efﬁciently ﬁnds the optimal 𝑅 of size 𝐾 with maximum quality
𝑄(𝑅, 𝑆). If we assign 𝛼 to be 0, the decision problem of ﬁnding
Maximum-Clique of any graph 𝐺 can be solved using the corre-
sponding similarity matrix 𝑀 and algorithm 𝐴. This is because the
∣𝑅∣−1
optimal set 𝑅 computed by 𝐴 with maximum quality score (
2∣𝑅∣ )
must corresponds to a clique in the original graph. This contra-
dicts with the hardness of the Maximum-Clique problem, hence
the hardness of the problem we stated in Section 3.4.
8.2 Proof of Theorem 2

Let 𝑅 = {𝑟1, 𝑟2, ..., 𝑟𝐾−1, 𝑟𝐾} and 𝑅′ = {𝑟′

To prove that the Algorithm 1 will terminate and the computation
of 𝑅𝑖𝑡𝑒𝑟 converges, we show that the quality metric 𝑄(𝑅𝑖𝑡𝑒𝑟, 𝑆) is
monotonically increasing with the number of iteration 𝑖𝑡𝑒𝑟.
𝐾}
𝐾−1, 𝑟′
2, ..., 𝑟′
be the ESS of two subsequent iterations. Without loss of generality,
𝑖 for 1 ≤ 𝑖 ≤ 𝐾−1, and denote ˜𝑅 = {𝑟1, 𝑟2, ..., 𝑟𝐾−1} =
let 𝑟𝑖 = 𝑟′
{𝑟′
𝐾−1}, such that we have 𝑅 = ˜𝑅 ∪ {𝑟𝐾} and 𝑅′ =
2, ..., 𝑟′
1, 𝑟′
˜𝑅 ∪ {𝑟′
𝐾}. Let 𝑔(𝑟𝑗, 𝑅, 𝑆), and 𝑔(𝑟𝑗, 𝑅′, 𝑆) be the ranking func-
tion of 𝑟𝑗 against 𝑅 and 𝑅′
respectively. Observe that the quality
metric

1, 𝑟′

𝑄(𝑅, 𝑆) =

𝛼

∣𝑅∣ ⋅ ∣𝑆∣

∣𝑆∣∑

∣𝑅∣∑

𝑖=1

𝑗=1

𝑆𝑖𝑚(𝑠𝑖, 𝑟𝑗)

(1 − 𝛼)
∣𝑅∣ ⋅ ∣𝑅∣

+

∣𝑅∣∑

∣𝑅∣∑

𝑖=1

𝑗>𝑖

𝑆𝑖𝑚(𝑟𝑖, 𝑟𝑗)

and similarly

𝑄(𝑅′, 𝑆) =

𝛼

∣𝑅′∣ ⋅ ∣𝑆∣

∣𝑆∣∑

∣𝑅′∣∑

𝑖=1

𝑗=1

𝑆𝑖𝑚(𝑠𝑖, 𝑟′
𝑗)

(1 − 𝛼)
∣𝑅′∣ ⋅ ∣𝑅′∣

+

∣𝑅′∣∑

∣𝑅′∣∑

𝑖=1

𝑗>𝑖

𝑆𝑖𝑚(𝑟′

𝑖, 𝑟′
𝑗)

The difference of the quality function in two subsequent itera-

.

tions are thus

𝑄(𝑅′, 𝑆) − 𝑄(𝑅, 𝑆)

=

∣𝑆∣∑

𝛼

𝑖=1

∣𝑅∣ ⋅ ∣𝑆∣ (
(1 − 𝛼)
∣𝑅∣ ⋅ ∣𝑅∣ (
∣𝑆∣∑

+

𝑖=1

∣𝑅∣ ⋅ ∣𝑆∣ (
(1 − 𝛼)
∣𝑅∣ ⋅ ∣𝑅∣ (

+

≥ 𝛼

𝑆𝑖𝑚(𝑠𝑖, 𝑟′

𝐾−1∑

𝑆𝑖𝑚(𝑟′

∣𝑆∣∑

𝐾 ) −
𝐾 , 𝑟𝑖) − 𝐾−1∑

𝑖=1

𝑖=1

𝑖=1

𝑆𝑖𝑚(𝑠𝑖, 𝑟′

𝐾∑

𝑆𝑖𝑚(𝑟′

∣𝑆∣∑

𝐾 ) −
𝐾 , 𝑟𝑖) − 𝐾∑

𝑖=1

𝑖=1

𝑖=1

𝑆𝑖𝑚(𝑠𝑖, 𝑟𝐾 ))

𝑆𝑖𝑚(𝑟𝐾 , 𝑟𝑖))

𝑆𝑖𝑚(𝑠𝑖, 𝑟𝐾 ))

𝑆𝑖𝑚(𝑟𝐾 , 𝑟𝑖))

≥ 1
∣𝑅∣ (𝑔(𝑟′

𝐾 , 𝑅, 𝑆) − 𝑔(𝑟𝐾 , 𝑅, 𝑆))

>0.

In other words, after each iteration the quality of the new ESS
𝑄(𝑅′, 𝑆) is strictly greater than that of the previous iteration 𝑄(𝑅, 𝑆).
Since for each iteration 𝑖𝑡𝑒𝑟, 𝑅𝑖𝑡𝑒𝑟 ⊆ 𝑈, the number of different
𝑅𝑖𝑡𝑒𝑟 is ﬁnite. Thus we know the algorithm will terminate and the
computation of 𝑅𝑖𝑡𝑒𝑟 converges.

WWW 2011 – Session: Information ExtractionMarch 28–April 1, 2011, Hyderabad, India436