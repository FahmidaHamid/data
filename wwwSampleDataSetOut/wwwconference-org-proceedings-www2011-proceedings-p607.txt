Counting Triangles and the Curse of the Last Reducer

Siddharth Suri

Sergei Vassilvitskii

Yahoo! Research

11 W. 40th St, 17th Floor
New York, NY 10018, USA

{suri, sergei}@yahoo-inc.com

ABSTRACT
The clustering coeﬃcient of a node in a social network is
a fundamental measure that quantiﬁes how tightly-knit the
community is around the node. Its computation can be re-
duced to counting the number of triangles incident on the
particular node in the network. In case the graph is too big
to ﬁt into memory, this is a non-trivial task, and previous
researchers showed how to estimate the clustering coeﬃcient
in this scenario. A diﬀerent avenue of research is to to per-
form the computation in parallel, spreading it across many
machines. In recent years MapReduce has emerged as a de
facto programming paradigm for parallel computation on
massive data sets. The main focus of this work is to give
MapReduce algorithms for counting triangles which we use
to compute clustering coeﬃcients.

Our contributions are twofold. First, we describe a se-
quential triangle counting algorithm and show how to adapt
it to the MapReduce setting. This algorithm achieves a fac-
tor of 10-100 speed up over the naive approach. Second, we
present a new algorithm designed speciﬁcally for the MapRe-
duce framework. A key feature of this approach is that it
allows for a smooth tradeoﬀ between the memory available
on each individual machine and the total memory available
to the algorithm, while keeping the total work done constant.
Moreover, this algorithm can use any triangle counting algo-
rithm as a black box and distribute the computation across
many machines. We validate our algorithms on real world
datasets comprising of millions of nodes and over a billion
edges. Our results show both algorithms eﬀectively deal
with skew in the degree distribution and lead to dramatic
speed ups over the naive implementation.

Categories and Subject Descriptors
H.4.m [Information Systems]: Miscellaneous

General Terms
Algorithms, Experimentation, Theory

Keywords
Clustering Coeﬃcient, MapReduce

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

1.

INTRODUCTION

In recent years there has been an explosion in the amount
of data available for social network analysis. It has become
relatively common to study social networks consisting of tens
of millions of nodes and billions of edges [12, 13]. Since the
amount of data to be analyzed has grossly outpaced ad-
vances in the memory available on commodity hardware,
computer scientists have once again turned to parallel al-
gorithms for data processing. Mapreduce [10] and its open
source implementation, Hadoop [19], have emerged as the
standard platform for large scale distributed computation.
In this paper we will give algorithms for computing one of
the fundamental metrics for social networks, the clustering
coeﬃcient [18], in the MapReduce framework.

The clustering coeﬃcient measures the degree to which
a node’s neighbors are themselves neighbors. The ﬁeld of
sociology gives us two theories as to the importance of this
measure. The tighter the community, the more likely it is
for any given member to have interacted with and know the
reputation of any other member. Coleman and Portes [8,
15] argue that if a person were to do something against the
social norm, the consequences in a tightly-knit community
would be greater because more people would know about the
oﬀending action and more people would be able to sanction
the individual who committed it. This phenomenon helps
foster a higher degree of trust in the community and also
helps the formation of positive social norms. Thus if the
clustering coeﬃcient of an individual indicated that they
were in a tightly-knit community, that individual may be
able to better take advantage of the higher levels of trust
amongst his/her peers and more positive social norms in
his/her community.

Burt [5] contends that individuals may beneﬁt from acting
as a bridge between communities. The theory of structural
holes argues that having a relationship with two individ-
uals that do not otherwise have a relationship allows the
mediator several advantages. The mediator can take ideas
from both people and come up with something entirely new.
The person in the middle could also take ideas from one of
his/her contacts and use them to solve problems that the
other is facing. Burt also argues that the eﬀects of this type
of information brokerage do not extend past one-hop in ei-
ther direction from the mediator [6]. Since the clustering
coeﬃcient of a node measures the fraction of a nodes neigh-
bors that are also neighbors, it captures the extent to which
a node acts as a mediator between his/her friends. Thus, if
a person’s clustering coeﬃcient indicates that many of their
neighbors, are not neighbors themselves, that person might

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India607be in a position to take advantage of this type of information
brokerage.
1.1 Contributions

We provide two simple and scalable algorithms for count-
ing the number of triangles incident on each node in a graph
using MapReduce. As we will see in the next section, given
these counts, and the degree of each node, computing the
clustering coeﬃcient is straightforward. Previous work dealt
with the massive scale of these graphs by resorting to ap-
proximate solutions. Tsourakakis et al. [17] used sampling
to give an unbiased estimate of the number of triangles in
the whole graph. Becchetti et al. [2] adapt the notion of min
wise independent permutations [3] to approximately count
the number of triangles incident on every node. We show
how a judicious partition of the data can be used to achieve
comparable improvements in speed without resorting to ap-
proximations, and allows us to compute exact clustering co-
eﬃcient for all nodes.

One of the main technical challenges in computing clus-
tering coeﬃcients that we address is dealing with skew. A
salient feature of massive social network data is that there
are typically several nodes with extraordinarily high degree
[1]. In parallel computations this often leads to some ma-
chines taking a much longer time than others to complete
the same task on a diﬀerent chunk of the input. Practition-
ers are all to familiar with the “curse of the last reducer”,
in which 99% of the computation ﬁnishes quickly, but the
remaining 1% takes a disproportionately longer amount of
time.

For all of the algorithms we propose, we prove worst case
bounds on their performance. Speciﬁcally, we show our al-
gorithms perform very well under very skewed data distri-
butions. Whereas for the naive algorithm the running time
on diﬀerent partitions of the data may diﬀer by a factor of
20 or more, the algorithms we propose partition the data
across the available nodes almost equally, resulting in much
more uniform running times.

Since memory is often a limiting factor when performing
computation on large datasets, our second algorithm also
allows the user to tradeoﬀ the RAM required per machine
with the total disk space required to run the algorithm. For
example, by increasing the total amount of disk space nec-
essary by a factor of 2 allows us to reduce the maximum
memory necessary per machine by a factor of 4, while per-
forming the same amount of work in aggregate. Since RAM
is typically a much more expensive resource than hard disk
space, this leads to a favorable tradeoﬀ for the end user.
1.2 Related Work

The problem of computing the clustering coeﬃcient, or
almost equivalently counting triangles, in a graph has a rich
history. The folklore algorithm iterates over all of the nodes
and checks whether any two neighbors of a given node v are
themselves connected. Although simple, the algorithm does
not perform well in graphs with skewed degree distributions
as a single high degree node can lead to an O(n2) running
time, even on sparse graphs.

Chiba and Nishizeki [7] studied this problem in detail and
introduced an optimal sequential algorithm for this problem.
Their procedure, K3, runs in O(n) time on planar graphs,
and more generally in O(mα(G)) time where α(·) denotes
the arboricity of the graph. (Refer to [7] for further details

and deﬁnitions.) In his thesis, Schank [16], gives a further
description and analysis of a number of diﬀerent algorithms
for this problem.

To battle the fact that the graph may be too large to
ﬁt into memory Coppersmith and Kumar [9] and Buriol et
al. [4] propose streaming algorithms that estimate the to-
tal number of triangles with high accuracy all while using
limited space. Becchetti et al. [2] solve the harder prob-
lem of estimating the number of triangles incident on each
node. Their algorithm takes O(log n) sequential scans over
the data, using O(n) memory to give an approximate num-
ber of triangles incident on every node.

Perhaps the work most closely related to ours is that of
[17]. The authors give a randomized MapReduce procedure
for counting the total number of triangles in a graph and
prove that the algorithm gives the correct number of trian-
gles in expectation. They also bound the variance and em-
pirically demonstrate the speedups their algorithm achieves
over the naive approach. Our algorithms achieve compara-
ble speedups, but there are two key diﬀerences between our
work and theirs. First, we give an exact algorithm, whereas
they give one that gives the correct answer in expectation.
Second, and more importantly, our algorithms give the to-
tal number of triangles incident on each node whereas they
give the total number of triangles in the entire graph. Having
the number of triangles incident on each node is crucial for
computing the clustering coeﬃcient of each node, a metric
of importance in the study of social networks.
2. PRELIMINARIES
2.1 Notation and Deﬁnitions
Let G = (V, E) be an unweighted, undirected simple graph
and let n = |V | and m = |E|. Denote by Γ(v) the set of
neighbors of v, i.e. Γ(v) = {w ∈ V | (v, w) ∈ E}. Let dv de-
note the degree of v, dv = |Γ(v)|. The clustering coeﬃcient
[18] for a node v ∈ V in an undirected graph is deﬁned by,

cc(v) =

2

The numerator in the above quotient is the number of edges
between neighbors of v. The denominator is the number
of possible edges between neighbors of v. Thus, the clus-
tering coeﬃcient measures the fraction of a nodes neigh-
bors that are the themselves connected. There is also an
equivalent way to view the clustering coeﬃcient. For a pair
(u, w) to contribute to the numerator, they must both be
connected to v and they must both be connected to each
other. Thus {u, v, w} must form a triangle in G. A triangle
in G is simply a set of three nodes {u, v, w} ⊆ V such that
(u, v), (v, w), (u, w) ∈ E. Thus, the algorithms we give in
this paper to compute the clustering coeﬃcient will do so
by counting the number of triangles incident on each node
using MapReduce. Computing the degree of each node for
the denominator using MapReduce is straightforward, see
[11] for details.
2.2 MapReduce Basics

MapReduce has become a de facto standard for parallel
computation at terabyte and petabyte scales. In this sec-
tion we give the reader a brief overview of computation in
the MapReduce computing paradigm (see [14, 19] for more
information).

|{(u, w) ∈ E | u ∈ Γ(v) and w ∈ Γ(v)}|

.

`dv

´

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India608The input and intermediate data are stored in (cid:104)key; value(cid:105)
pairs and the computation proceeds in rounds. Each round
is split into three consecutive phases: map, shuﬄe and re-
duce. In the map phase the input is processed one tuple at a
time. This allows diﬀerent tuples to be processed by diﬀer-
ent machines and creates an opportunity for massive paral-
lelization. Each machine performing the map operation, also
known as a mapper, emits a sequence of (cid:104)key; value(cid:105) pairs
which are then passed on to the shuﬄe phase. This is the
synchronization step. In this phase, the MapReduce infras-
tructure collects all of the tuples emitted by the mappers,
aggregates the tuples with the same key together and sends
them to the same physical machine. Finally each key, along
with all the values associated with it, are processed together
during the reduce phase. Here too, the operations on data
with one key are independent of data with a diﬀerent key
and can be processed in parallel by diﬀerent machines.

Although MapReduce has allowed for computation on mas-
sive data sets to be performed on commodity hardware, it is
not a silver bullet, and designers of MapReduce algorithms
must still be careful not to overstep the bounds of the sys-
tem. The authors of [11] give a theoretical model of MapRe-
duce computation and describe the following limitations.

• Machine Memory The size of the input to a MapRe-
duce computation is typically too large to ﬁt into mem-
ory. Therefore, the memory used by a single mapper
or a reducer should be sublinear in the total size of the
input.

• Total Memory In addition to being limited in the
memory available to a single machine, the total space
used by a MapReduce computation must also remain
bounded. While some duplication of the data is al-
lowed, the total space used should always be o(n2) for
an input of size n, thus prohibiting exorbitant data
duplication [11].

• Number of Rounds Finally, since each MapReduce
round can involve shuﬄing terabytes of data across dif-
ferent machines in a data center, the number of rounds
taken by a MapReduce algorithm should remain con-
stant, or at worst logarithmic in the input size.

3. SEQUENTIAL ALGORITHMS

We begin by presenting the folklore algorithm for comput-

ing the number of triangles in the graph.
3.1 NodeIterator

We call the process of generating paths of length 2 among
the neighbors of node v, “pivoting” around v. The algorithm
works by pivoting around each node and then checking if
there exist an edge that will complete any of the resulting
2-paths to form a triangle. We give the pseudocode below:
We note that each triangle {u, v, w} is counted 6 times to-
tal (once as {u, v, w},{u, w, v},{v, u, w},{v, w, u},{w, u, v},
and {w, v, u}). The analysis of the running time is straight-
v). In con-
stant degree graphs, where dv = O(1) for all v, this is a
linear time algorithm. However, even a single high degree
node can lead to a quadratic running time. Since such high
degree nodes are often found in real world large graphs, this
algorithm is not practical for real-world, massive graphs.

forward, as the algorithm runs in time O(P

v∈V d2

for u ∈ Γ(v) do

Algorithm 1 NodeIterator(V,E)
1: T ← 0;
2: for v ∈ V do
3:
4:
5:
6:
7: return T / 3;

for w ∈ Γ(v) do
T ← T + 1/2;

if ((u, w) ∈ E) then

3.2 NodeIterator++

To see how to reduce the running time of the above algo-
rithm, note that each triangle is counted six times, twice by
pivoting around each node. Moreover, those pivots around
high degree nodes generate far more 2-paths and are thus
much more expensive than the pivots around the low de-
gree nodes. To improve upon the baseline algorithm Schank
[16] proposed that the lowest degree node in each triangle
be “responsible” for making sure the triangle gets counted.
Let (cid:31) be a total order on all of the vertices, with the prop-
erty that v (cid:31) u if dv > du, with ties broken arbitrarily (but
consistently).

Algorithm 2 NodeIterator++(V,E)
1: T ← 0;
2: for v ∈ V do
3:
4:
5:
6:
7: return T;

for u ∈ Γ(v) and u (cid:31) v do
if ((u, w) ∈ E) then

for w ∈ Γ(v) and w (cid:31) u do

T ← T + 1;

The algorithm restricts the set of 2-paths generated from
v’s neighbors, to be only those where both endpoints of the
2-path have degree higher than v. The exact algorithm is
presented in Algorithm 2. Although this is a subtle change,
it has a very large impact both in theory and in practice.

Lemma 1

([16]). The running time of Algorithm NodeIt-

erator++ is O(m3/2).

√
We note that this is the best bound possible. Consider the
“lollipop graph” consisting of a clique on
n nodes and a
√
n = O(n) edges in the graph. Furthermore, there are no
triangles among any of the nodes in the path, but there is
a triangle between any three nodes in the the clique. Thus

path on the remaining nodes. There are m = `√
the graph has`√

´ = Θ(n3/2) triangles.

´ + n −

n
2

n
3

4. MAPREDUCE ALGORITHMS

The algorithms presented in the previous section implic-
itly assume that the graph data structure ﬁts into memory of
a single machine. For massive graphs, this is no longer case
and researchers have turned to parallel algorithms to remedy
this problem. Although parallel algorithms allow for com-
putation on much larger scales, they are not a panacea, and
algorithm design remains crucial to success of algorithms.
In particular, space requirements of algorithms do not dis-
appear simply because there are more machines available.
For large enough n, O(n2) memory is still an unreasonable
request—for n ≈ 108, an n2 space algorithm requires ap-
proximately 10 petabytes of storage.

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India609In this section we describe the adaptation of sequential
algorithms in the previous section to the MapReduce frame-
work.
4.1 NodeIterator in MapReduce

We begin with the NodeIterator algorithm. The imple-

mentation of the algorithm is as follows:

• Round 1: Generate the possible length two paths in

the graph by pivoting on every node in parallel.

• Round 2: Check which of the length two paths gener-
ated in Round 1 can be closed by an edge in the graph
and count the triangles accordingly.

As we noted before, in the NodeIterator algorithm, a sin-
gle high degree node can lead to the generation of O(n2)
2-paths. Therefore we adapt the NodeIterator++ algorithm
to MapReduce. The general approach is the same as above,
and we present the exact algorithm in Algorithm 3. Note
that in Round 2, in addition to taking the output of Round
1, the algorithm also takes as input the original edge list.
The algorithm diﬀerentiates between the two diﬀerent types
of input by using a special character “$” that is assumed not
to appear anywhere else in the input.

for (u, w) : u, w ∈ S do

emit (cid:104)v; (u, w)(cid:105)

if v (cid:31) u then
emit (cid:104)u; v(cid:105)

Algorithm 3 MR-NodeIterator++(V,E)
1: Map 1: Input: (cid:104)(u, v);∅(cid:105)
2:
3:
4: Reduce 1: Input (cid:104)v; S ⊆ Γ(v)(cid:105)
5:
6:
7: Map 2:
8:
9:
10:
11:
12: Reduce 2: Input (cid:104)(u, w); S ⊆ V ∪ {$}(cid:105)
13:
14:
15:

if Input of type (cid:104)v; (u, w)(cid:105) then
if Input of type (cid:104)(u, v);∅(cid:105) then

emit (cid:104)(u, w); v(cid:105)
emit (cid:104)(u, v); $(cid:105)

if $ ∈ S then

for v ∈ S ∩ V do

emit (cid:104)v; 1(cid:105)

4.1.1 Analysis
We analyze the total time and space usage of the MapRe-
duce version of the NodeIterator++ algorithm. Correctness
and overall running time follow from the sequential case.
The next lemma implies that the total memory required per
machine is sublinear. Thus, no reducer gets ﬂooded with
too much data, even if there is skew in the input.

Lemma 2. The input to any reduce instance in the ﬁrst

round has O(

√
m) edges.

√

Proof. For the purposes of the proof, partition the nodes
into two subclasses: the high degree nodes, H = {v ∈ V :
dv ≥ √
m} and the low degree nodes, L = {c ∈ V : dv <
√
m}. Note that the total number of high degree nodes is
√
√
m). Therefore a high degree node
at most 2m/
can have at most |H| = O(
m) high degree neighbors. If
v ∈ H, the reducer which receives v as a key will only receive
u ∈ Γ(v) where du (cid:31) dv. Thus this reducer will receive at
√
most neighbors O(
m) edges. On the other hand, if the key
√
in the reduce instance is a low node, then its neighborhood
has size at most
m by deﬁnition.

m = O(

The sequential analysis also implies that:

Lemma 3. The total number of records output at the end

of the ﬁrst reduce instance is O(m3/2).

The bound in the Lemma above is a worst case bound for
a graph with m edges. As we show in Section 5, the ac-
tual amount of data output as a result of the Reduce 1 is
much smaller in practice. Finally, note that the number of
records to any reducer in round 2 for a key (u, v) is at most
O(du + dv) = O(n). On graphs where this is value is too
large to ﬁt into memory, one can use easily distribute the
computation by further partitioning the output of Round 2
across r diﬀerent machines, and sending a copy of the edge
to all r of the machines. We omit this technicality for the
sake of clarity and refer the interested reader to [11] for exact
details.
4.2 Partition Algorithm

In this section we present a diﬀerent algorithm for count-
ing triangles. The algorithm works by partitioning the graphs
into overlapping subsets so that each triangle is present in at
least one of the subsets. Given such a partition, we can then
use any sequential triangle counting algorithm as a black box
on each partition, and then simply combine the results. We
prove that the algorithm achieves perfect work eﬃciency for
the triangle counting problem. As the partitions get ﬁner
and ﬁner, the total amount of work spent on ﬁnding all of
the triangles remains at O(m3/2); instead there are simply
more machines with each doing less work. Overall the algo-
rithm eﬀectively takes any triangle counting algorithm that
works on a single machine and distributes the computation
without blowing up the total amount of work done.
We begin by providing a high level overview. First, parti-
tion the nodes into ρ equal sized groups, V = V1∪V2∪. . .∪Vρ
where ρ > 0 with Vi ∩ Vj = ∅ for i (cid:54)= j. Denote by Vijk =
Vi ∪ Vj ∪ Vk. Then let Eijk = {(u, w) ∈ E : u, w ∈ Vijk} be
the set of edges between vertices in Vi, Vj and Vk, and let
Gijk = (Vi ∪ Vj ∪ Vk, Eijk) be the induced graph on Vijk.
Let G = ∪i<j<kGijk be the set of graphs. An instance of a
graph Gijk contains 3/ρ fraction of the vertices of the original
graph, and, in expectation a O(1/ρ2) fraction of the edges.
Since each node of a triangle must be in one part of the par-
tition of V , and G contains all combinations of parts of V ,
every triangle in graph G appears in at least one graph in
the set G. The algorithm performs a weighted count of the
number of triangles in each subgraph in G. The weights cor-
rect for the fact that a triangle can occur in many subgraphs.
The MapReduce code for the partition step is presented in
Algorithm 4. Note that we do not restrict the algorithm used
in the reducer, we only insist that the triangles be weighted.
4.2.1 Analysis
We begin by proving the correctness of the algorithm

Lemma 4. Each triangle gets counted exactly once after

weighting.

Proof. Consider a triangle (w, x, y) whose vertices lie in
Vh(w), Vh(x) and Vh(z). If the vertices are mapped to distinct
partitions, i.e., h(w) (cid:54)= h(x), h(w) (cid:54)= h(y), and h(x) (cid:54)= h(z)
then the triangle will appear exactly once. If the vertices
are mapped to two distinct partitions, e.g., h(x) = h(w)
but h(x) (cid:54)= h(y), the triangle will appear in ρ− 2 times. For
example, suppose that ρ = 4, h(x) = h(y) = 1 and h(z) = 3,

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India610{Let h(·) be a universal hash function into [0, ρ − 1]}
i ← h(u)
j ← h(v)
for a ∈ [0, ρ − 1] do

for b ∈ [a + 1, ρ − 1] do

Algorithm 4 MR-GraphPartition(V,E, ρ)
1: Map 1: Input: (cid:104)(u, v); 1(cid:105)
2:
3:
4:
5:
6:
7:
8:
9:
10: Reduce 1:
11:
12:
13:
14:
15:
16:
17:
18:

for c ∈ [b + 1, ρ − 1] do
if {i, j} ⊆ {a, b, c} then
emit (cid:104)(a, b, c); (u, v)(cid:105).
Input (cid:104)(i, j, k); Eijk ⊆ E(cid:105)

Count triangles on Gijk
for every triangle (u, v, w) do

z ← ρ − 2

Scale triangle (u, v, w) weight by 1/z

z ←`h(u)

z ← 1
if h(u) = h(v) = h(w) then
elif h(u) = h(v) | h(v) = h(w) | h(u) = h(w) then

´ + h(u)(ρ − h(u) − 1) +`ρ−h(u)−1
´

2

2

then the triangle will appear in graphs G0,1,3 and G1,2,3. In
the case h(x) = h(y) = h(w) one can verify that the triangle

´+h(x)(ρ−h(x)−1)+`ρ−h(x)−1

appears exactly`h(x)

´ times.

The ﬁrst term represents graphs Gijk with i < j < h(x), the
second those with i < h(x) < k and the last those with
h(x) < j < k.

2

2

We now dive deeper into the role of the tunable parameter,
ρ. At a high level, ρ trades oﬀ the total space used by the
algorithm (as measured at the end of the map phase) with
the size of the input to any reduce instance. We quantify
this tradeoﬀ below:

Lemma 5. For a setting of ρ:
• The expected size of the input to any reduce instance

is O( m

ρ2 ).

• The expected total space used at the end of the map

phase is O(ρm).

Proof. To prove the ﬁrst bound, ﬁx one subset Gijk ∈ G.
The subset has |Vijk| = 3n
ρ vertices in expectation. Con-
sider a random edge e = (u, v) in the graph, there is a 3/ρ
probability that u ∈ Vijk, and a further 3/ρ probability that
v ∈ Vijk. Therefore, for a random edge, it is present in Gijk
with probability 9/ρ2. The ﬁrst bound follows by linearity
of expectation. For the second bound, observe that there
are O(ρ3) diﬀerent partitions, and thus the total number of
edges emitted by the map phase is O(ρ3 · m

ρ2 ) = O(mρ).

The previous lemma implies that increasing the total space
used by the algorithm by a factor of 2 results in a factor of
4 improvement in terms of the memory requirements of any
reducer. This is a favorable tradeoﬀ in practice since RAM
(memory requirements of the reducers) is typically much
more expensive than disk space (the space required to store
the output of the mappers). Finally, we show that our algo-
rithm is work eﬃcient.

Theorem 1. For any ρ ≤ √

m the total amount of work

Enron

web-BerkStan

as-Skitter
LiveJournal

Twitter

Directed Undirected

Nodes

3.7 x 104
6.9 x 105
1.7 x 106
4.8 x 106
4.2 x 107

Edges

3.7 x 105
7.6 x 106
11 x 106
6.9 x 106
1.5 x 109

Edges

3.7 x 105
1.3 x 107
22 x 106
8.6 x 107
2.4 x 109

Table 1: Statistics about the data sets used for al-
gorithm evaluation.

Proof. As we noted in section 3 the running time of the
best algorithm for counting triangles is O(m3/2). Lemma
5 shows that in the MapReduce setting the input is parti-
tioned across ρ3 machines, each with a 1/ρ fraction of the
input. Computing the graph partition takes O(1) time per
√
output edge. The total number of edges output is O(mρ) =
O(m3/2) for ρ <
m. The running time on each reducer

”3/2«

„“ m

ρ2

“ m3/2

”

ρ3

is O

= O

. Summing up over the O(ρ3)

graph partitions, we recover the O(m3/2) total work.

5. EXPERIMENTS

In this section we give an experimental evaluation of the
MapReduce algorithms deﬁned above. All of the algorithms
were run on a cluster of 1636 nodes running the Hadoop
implementation of MapReduce. We used NodeIterator++
as the triangle counting subroutine in MR-GraphPartition.
5.1 Data

We use several data sets to evaluate our algorithms all of
which are publicly available. The Twitter data set is avail-
able at [12], and the rest are from the SNAP library1. In our
experiments we consider each edge of the input to be undi-
rected to make the input graphs even larger, showcasing the
scalability of the approach. Thus if an edge (u, v) appears
in the input, we also add edge (v, u) if it did not already
exist. Table 1 shows how many edges each graph has before
and after undirecting the edges.
5.2 Scalability

As discussed in Section 3 both the NodeIterator and NodeIt-

erator++ algorithms work by computing length 2-paths in
the graph and then checking if there is an edge in the graph
that completes the 2-path to form a triangle. If there ex-
ists a node of linear degree in the input, the NodeIterator
algorithm would output roughly Ω(n2) 2-paths, whereas the
NodeIterator++ algorithm always checks at most O(m3/2)
length 2 paths. In order to see if these worst case claims ap-
ply in real-world data, Table 2 shows the number of 2-paths
generated by both algorithms.
In the case of LiveJournal
and Enron, NodeIterator++ outputs an order of magnitude
fewer 2-paths than NodeIterator. In the case of Twitter, the
reduction is almost three orders of magnitude.

As one might expect, the drastic diﬀerences in the num-
ber of 2-paths generated has a large impact on the run-
ning time of the algorithms.
In the case of Twitter, the
NodeIterator algorithm generates 247 trillion 2-paths. Even
if they were encoded using only 10 bytes each, this would
amount to 2.5 petabytes which is prohibitive to compute

performed by all of the machines is O(m3/2).

1http://snap.stanford.edu

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India611(a) LiveJournal

(b) Twitter

Figure 1: Figures 1(a) and 1(b) show the distribution of the clustering coeﬃcients for LiveJournal and
Twitter.

Enron

web-BerkStan

as-Skitter
LiveJournal

Twitter

NodeItr

51.13 x 106
56.0 x 109
32 x 109
14.5 x 109

246.87 x 1012

NodeItr++ Reduction
2.92 x 106
176 x 106
189 x 106
1.36 x 109
3.0 x 1011

17.5
318
169
10.7
820

Table 2: The number of 2-paths generated (and
shuﬄed) by the NodeIterator and the NodeItera-
tor++ algorithms along with the reduction factor.

web-BerkStan
as-Skitter
LiveJournal
Twitter

–

NodeIterator NodeIterator++ GP
1.70
2.08
10.9
483

> 840
> 100
59.5

1.77
1.90
5.33
423

Table 3: The running times of all algorithms in min-
utes.

with. The NodeIterator++ algorithm only generates 301
billion 2-paths, which takes about 2 hours. In the case of
LiveJournal computing the 2-paths via NodeIterator takes
about 50 minutes on average, whereas NodeIterator++ does
this in under 2 minutes. To compare the two algorithms, and
contrast them against the naive approach, we give the over-
all running times of all three on various real-world data sets
in Table 3.

In their work on scaling clustering coeﬃcient Tsourakakis
et al [17] give an algorithm that approximates the number
of triangles in the entire graph. The authors compare their
algorithm to the NodeIterator algorithm and across a wide
variety of data sets show that they get speedups ranging
from 10 to 100 at approximately 2% cost in accuracy. We
show that large speedups can also be obtained without any
loss in precision. The algorithm we propose also has the
added beneﬁt of computing the number of triangles incident
on each node, something that is impossible using the Doulion
algorithm [17].

Figure 1 shows the distributions of the clustering coeﬃ-

cients in the range (0, 1) for the undirected versions of Live-
Journal and Twitter. We removed nodes that have clus-
tering coeﬃcients of exactly 0 or 1 because they tend to
be sporadic users of both services that happen to have a
few friends that are either completely unconnected or com-
pletely connected. Note that there is a larger proportion of
nodes with higher clustering coeﬃcients in LiveJournal than
in Twitter. Thus it is more likely that two people connected
to a given node are also connected in LiveJournal than in
Twitter. One possible explanation for this is that people use
LiveJournal more for social interactions and such networks
often have high clustering [18]. Twitter on the other hand,
may be used more for information gathering and dissemi-
nation than for the type of social interaction going on in
LiveJournal.
5.3 Data Skew

One problem that is inherently familiar to practitioners
working on large data sets is the skew of the data. It is well
known, for example, that the degree distribution in many
naturally occurring large graphs follows a power law [1], and
almost every graph has a handful of nodes with linear degree.
This phenomenon manifests itself in computations on large
datasets as well. It is rather common to have 99% of the
map or reduce tasks ﬁnish very quickly, and then spend just
as long (if not longer) waiting for the last task to succeed.

Figures 2(a), 2(b) and 2(c) further explain the diﬀerences
in wall clock times by showing the distribution of reducer
completion times for a typical run of the pivot step of NodeIt-
erator, NodeIterator++ and the main step of MR-GraphPartition.
Figure 2(a) illustrates this “curse of the last reducer,” by
showing a heavy-tailed distribution of reducer completion
times for the NodeIterator algorithm. The majority of tasks
ﬁnish in under 10 minutes, but there are a handful of reduc-
ers that take 30 or more minutes and one that takes roughly
60 minutes. The reducers that have the longest comple-
tion times received high degree nodes to pivot on. Since for
a node of degree d, NodeIterator generates O(d2) 2-paths,
these reducers take a disproportionately long time to run.
Figures 2(b) and 2(c) show that the NodeIterator++ and
MR-GraphPartition algorithms handle skewed much better.

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India612(a) NodeIterator

(b) NodeIterator++

(c) GraphPartition

Figure 2: Figures 2(a), 2(b) and 2(c) show the distribution of the reducer wall clock times for typical runs
of the pivot step (Round 1) for NodeIterator and NodeIterator++ algorithms, and the main round of the
MR-GraphPartition algorithm. respectively. All runs in this plot were generated using 200 reducers.

Figure 3: The disk space vs. memory tradeoﬀ for the
MR-GraphPartition algorithm as empirically mea-
sured on the LiveJournal dataset. The ρ values
range from 4 to 31.

The distribution of reducer completion times is much more
concentrated around the mean.
5.4 Work Efﬁciency

In Section 4.2.1 we showed that the MR-GraphPartition
algorithm allowed the user to trade oﬀ between the mem-
ory required per machine and the total disk space required
to store the intermediate result in the shuﬄe round. We
present this tradeoﬀ for the LiveJournal graph in Figure 3,
which plots the two quantities for various values of ρ ranging
from 4 to 31. The graph follows almost exactly the tradeoﬀ
implied theoretically by Lemma 5. Initially as ρ increases,
the total increase in disk space is small but the reduction
in memory required per reducer is quite dramatic. As ρ
increases further, the decrease in memory is small on an ab-
solute scale, but leads to large and larger increases in disk
space required.

In addition to allowing the user to tune the algorithm for
the speciﬁc parameters of the cluster hardware, the diﬀerent
setting of the parameters changes the overall running time
of the job. In Figure 4 we present the running time of the
MR-GraphPartition algorithm for diﬀerent values of ρ. The

The

running time of

the MR-
Figure 4:
GraphPartition algorithm on the LiveJournal
dataset. The graph did not ﬁt into memory of a
reducer at ρ < 13. Error bars indicate 95% conﬁ-
dence intervals.

plot averages ﬁve diﬀerent measurements of ρ. Although
the measurements are noisy, since there are many other jobs
running on the cluster at the same time, there is a clear de-
crease in running time as ρ increases initially. We attribute
this to the fact that at high ρ the reduce step may need
to use more memory than available and page out to disk,
which ceases to be a problem as the input size decreases at
higher values of ρ. As ρ continues to increase, this beneﬁt
disappears and the total running time slowly increases with
ρ. This is a combination of two factors. First, for high ρ
several reduce tasks are scheduled sequentially on the avail-
able machines and are performed serially. Second, full work
eﬃciency is achieved on worst case instances—those where
the number of length two paths is Θ(m3/2). Since we are
not typically faced with the worst case instances, increasing
ρ leads to a slight increase in the total amount of work being
done.
5.5 Discussion

In this work we presented two algorithms for eﬃciently
computing clustering coeﬃcients in massive graphs. Our al-

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India613gorithms showcase diﬀerent aspects of the problem. The ﬁrst
approach, MR-NodeIterator++, is a specialized algorithm
designed explicitly for computing the clustering coeﬃcient
of every node. The second approach, MR-GraphPartition,
describes a more general framework that takes any sequen-
tial triangle counting algorithm as a black box and adapts
it to the MapReduce setting. In fact, MR-GraphPartition
can easily be extended to counting other small subgraphs
(for example K2,3) in a distributed setting. To count a sub-
graph H on h nodes, again partition the nodes into ρ equal
sized sets; create all possible subsets of size h from the ρ
partitions, and distribute the ρh subgraphs across diﬀerent
machines.

Both the MR-NodeIterator++ and the MR-GraphPartition

algorithms outperform the naive MapReduce implementa-
tion for computing the clustering coeﬃcient. The improve-
ment is consistently at least a factor of 10 and often is sig-
niﬁcantly larger.
(For example, it is impossible for us to
assess the running time of the naive algorithm on the Twit-
ter graph.) These speedup improvements are comparable to
those obtained by [17] and [2], but on a much harder prob-
lem. The output of our algorithms gives the exact number of
triangles incident on every node in the graph. A signiﬁcant
part of the improvement in the running time comes from
the fact that the proposed algorithms do a much better job
of distributing the computation evenly across the machines,
even in the presence of large skew in the input data. This
allows them to avoid the “curse of the last reducer,” whereby
the majority of machines in a distributed job ﬁnish quickly,
and a single long running job determines the overall running
time.

The two algorithms we propose also have diﬀerent theo-
retical behavior and exploit diﬀerent aspects of the MapRe-
duce infrastructure. The MR-NodeIterator++ algorithm
uses MapReduce to compute in parallel the intersection be-
tween all possible two paths and edges in the graph. Here,
the computation itself is trivial, and the infrastructure is
used to parallelize this simple approach. The MR-Graph-
Partition uses the infrastructure to divide the graph into
overlapping subgraphs, while ensuring that no triangle is
lost. The reducers then execute the sequential algorithm,
albeit on a much smaller input.

6. REFERENCES
[1] Albert-L´aszl´o Barab´asi and R´eka Albert. Emergence

of scaling in random networks. Science,
286(5439):509–512, October 1999.

[2] Luca Becchetti, Paolo Boldi, Carlos Castillo, and

Aristides Gionis. Eﬃcient semi-streaming algorithms
for local triangle counting in massive graphs. In
Proceeding of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ’08, pages 16–24, New York, NY, USA, 2008.
ACM.

[3] Andrei Z. Broder, Steven C. Glassman, Mark S.

Manasse, and Geoﬀrey Zweig. Syntactic clustering of
the web. Computer Networks, 29(8-13):1157–1166,
1997.

[4] Luciana S. Buriol, Gereon Frahling, Stefano Leonardi,

Alberto Marchetti-Spaccamela, and Christian Sohler.
Counting triangles in data streams. In Proceedings of
the twenty-ﬁfth ACM SIGMOD-SIGACT-SIGART
symposium on Principles of database systems, PODS
’06, pages 253–262, New York, NY, USA, 2006. ACM.

[5] Ronald S. Burt. Structural holes and good ideas.

American Journal of Sociology, 110(2):349–99,
September 2004.

[6] Ronald S. Burt. Second-hand brokerage: Evidence on

the importance of local structure for managers,
bankers, and analysts. Academy of Management
Journal, 50:119–148, 2007.

[7] Norishige Chiba and Takao Nishizeki. Arboricity and

subgraph listing algorithms. SIAM J. Comput.,
14(1):210–ˆa ˘A¸S223, 1985.

[8] James S. Coleman. Social capital in the creation of

human capital. American Journal of Sociology,
94:S95–S120, 1988.

[9] Don Coppersmith and Ravi Kumar. An improved data

stream algorithm for frequency moments. In
Proceedings of the ﬁfteenth annual ACM-SIAM
symposium on Discrete algorithms, SODA ’04, pages
151–156, Philadelphia, PA, USA, 2004. Society for
Industrial and Applied Mathematics.

[10] Jeﬀrey Dean and Sanjay Ghemawat. MapReduce:

Simpliﬁed data processing on large clusters. In
Proceedings of OSDI, pages 137–150, 2004.
[11] Howard Karloﬀ, Siddharth Suri, and Sergei

Vassilvitskii. A model of computation for MapReduce.
In Proceedings of the Twenty-First Annual
ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 938–948, 2010.

[12] Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. What is twitter, a social network or news
media. In Proc. 19th International World Wide Web
Conference, 2010.

[13] Jure Leskovec and Eric Horvitz. Planetary-scale views

on a large instant-messaging network. In Proc. 17th
International World Wide Web Conference, 2008.

[14] Jimmy Lin and Chris Dyer. Data-Intensive Text

Processing with MapReduce. Number 7 in Synthesis
Lectures on Human Language Technologies. Morgan
and Claypool Publishers, April 2010.

[15] Alejandro Portes. Social capital: Its origins and

applications in modern sociology. Annual Review of
Sociology, 24:1–24, 1998.

[16] Thomas Schank. Algorithmic Aspects of

Triangle-Based Network Analysis. PhD thesis,
Universit¨at Karlsruhe (TH), 2007.

[17] Charalampos E. Tsourakakis, U Kang, Gary L. Miller,
and Christos Faloutsos. Doulion: Counting triangles in
massive graphs with a coin. In Knowledge Discovery
and Data Mining (KDD), 2009.

[18] Duncan Watts and Steve Strogatz. Collective

dynamics of ’small-world’ networks. Nature,
383:440–442, 1998.

[19] Tom White. Hadoop: The Deﬁnitive Guide. O’Reilly

Media, 2009.

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India614