HyperANF: Approximating the Neighbourhood Function of

Very Large Graphs on a Budget

Paolo Boldi

Dipartimento di Scienze

dell’Informazione

Università degli Studi di

Milano, Italy

boldi@dsi.unimi.it

Marco Rosa

Dipartimento di Scienze

dell’Informazione

Università degli Studi di

Milano, Italy

marco.rosa@unimi.it

Sebastiano Vigna(cid:3)
Dipartimento di Scienze
dell’Informazione

Università degli Studi di

Milano, Italy

vigna@acm.org

ABSTRACT
The neighbourhood function NG .t / of a graph G gives, for each
t 2 N, the number of pairs of nodes hx; yi such that y is reach-
able from x in less that t hops. The neighbourhood function pro-
vides a wealth of information about the graph [10] (e.g., it easily
allows one to compute its diameter), but it is very expensive to
compute it exactly. Recently, the ANF algorithm [10] (approxi-
mate neighbourhood function) has been proposed with the purpose
of approximating NG .t / on large graphs. We describe a break-
through improvement over ANF in terms of speed and scalabil-
ity. Our algorithm, called HyperANF, uses the new HyperLogLog
counters [5] and combines them efﬁciently through broadword pro-
gramming [8]; our implementation uses talk decomposition to ex-
ploit multi-core parallelism. With HyperANF, for the ﬁrst time we
can compute in a few hours the neighbourhood function of graphs
with billions of nodes with a small error and good conﬁdence using
a standard workstation.

Then, we turn to the study of the distribution of the distances
between reachable nodes (that can be efﬁciently approximated by
means of HyperANF), and discover the surprising fact that its index
of dispersion provides a clear-cut characterisation of proper social
networks vs. web graphs. We thus propose the spid (Shortest-Paths
Index of Dispersion) of a graph as a new, informative statistics
that is able to discriminate between the above two types of graphs.
We believe this is the ﬁrst proposal of a signiﬁcant new non-local
structural index for complex networks whose computation is highly
scalable.

Categories and Subject Descriptors
G.2.2 [Graph theory]: Graph algorithms; G.3 [Probability and
Statistics]: Probabilistic algorithms

General Terms
Algorithms, Experiments

Keywords
Neighbourhood function, probabilistic counters, shortest paths, ef-
fective diameter
(cid:3)The authors have been partially supported by a Yahoo! faculty
grant and by MIUR PRIN “Query log e web crawling”.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

INTRODUCTION

1.
The neighbourhood function NG .t / of a graph returns for each
t 2 N the number of pairs of nodes hx; yi such that y is reachable
from x in less that t steps. It provides data about how fast the “av-
erage ball” around each node expands. From the neighbourhood
function, several interesting features of a graph can be estimated,
and in this paper we are in particular interested in the effective di-
ameter, a measure of the “typical” distance between nodes.

Palmer, Gibbons and Faloutsos [10] proposed an algorithm to
approximate the neighbourhood function (see their paper for a re-
view of previous attempts at approximate evaluation); the authors
distribute an associated tool, snap, which can approximate the neigh-
bourhood function of medium-sized graphs. The algorithm keeps
track of the number of nodes reachable from each node using Flajolet–
Martin counters, a kind of sketch that makes it possible to compute
the number of distinct elements of a stream in very little space. A
key observation was that counters associated with different streams
can be quickly combined into a single counter associated with the
concatenation of the original streams.

In this paper, we describe HyperANF—a breakthrough improve-
ment over ANF in terms of speed and scalability. HyperANF uses
the new HyperLogLog counters [5], and combines them efﬁciently
by means of broadword programming [8]. Each counter is made by
a number of registers, and the number of registers depends only on
the required precision. The size of each register is doubly logarith-
mic in the number of nodes of the graph, so HyperANF, for a ﬁxed
precision, scales almost linearly in memory (i.e., O.n log log n/).
By contrast, ANF memory requirement is O.n log n/.

Using HyperANF, for the ﬁrst time we can compute in a few
hours the neighbourhood function of graphs with more than one
billion nodes with a small error and good conﬁdence using a stan-
dard workstation with 128 GB of RAM. Our algorithms are imple-
ment in a tool distributed as free software within the WebGraph
framework.1

Armed with our tool, we study several datasets, spanning from
small social networks to very large web graphs. We isolate a statis-
tically deﬁned feature, the index of dispersion of the distance dis-
tribution, and show that it is able to tell “proper" social networks
from web graphs in a natural way.

2. RELATED WORK

HyperANF is an evolution of ANF [10], which is implemented
by the tool snap. We will give some timing comparison with snap,
but we can only do it for relatively small networks, as the large
memory footprint of snap precludes application to large graphs.

1See [2]. http://webgraph.dsi.unimi.it/

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India625Recently, a MapReduce-based distributed implementation of ANF
called HADI [7] has been presented. HADI runs on one of the ﬁfty
largest supercomputers—the Hadoop cluster M45. The only pub-
lished data about HADI’s performance is the computation of the
neighbourhood function of a Kronecker graph with 2 billion links,
which required half an hour using 90 machines. HyperANF can
compute the same function in less than ﬁfteen minutes on a laptop.
The rather complete survey of related literature in [7] shows that
essentially no data mining tool was able before ANF to approxi-
mate the neighbourhood function of very large graphs reliably. A
remarkable exception is Cohen’s work [3], which provides strong
theoretical guarantees but experimentally turns out to be not as scal-
able as the ANF approach; it is worth noting, though, that one of
the proposed applications of [3] (On-line estimation of weights of
growing sets) is structurally identical to ANF.

All other results published before ANF relied on a small number
of breadth-ﬁrst visits on uniformly sampled nodes—a process that
has no provable statistical accuracy or precision. Thus, in the rest
of the paper we will compare experimental data with snap and with
the published data about HADI.

3. HYPERANF

In this section, we present the HyperANF algorithm for comput-
ing an approximation of the neighbourhood function of a graph;
we start by recalling from [5] the notion of HyperLogLog counter
upon which our algorithm relies. We then describe the algorithm,
discuss how it can be implemented to be run quickly using broad-
word programming and task decomposition, and give results about
its memory requirements and precision.
3.1 HyperLogLog counters

HyperLogLog counters, as described in [5] (which is based on [4]),
are used to count approximately the number of distinct elements in
a stream. For the purposes of the present paper, we need to recall
brieﬂy their behaviour. Essentially, these probabilistic counters are
a sort of approximate set representation to which, however, we are
only allowed to pose questions about the (approximate) size of the
set.2
1 be a hash function
Let D be a ﬁxed domain and h W D ! 2
mapping each element of D into an inﬁnite binary sequence. The
function is ﬁxed with the only assumption that “bits of hashed val-
ues are assumed to be independent and to have each probability 1
2
of occurring” [5].
1, let ht .x/ denote the sequence made by
For a given x 2 2
the leftmost t bits of h.x/, and ht .x/ be the sequence of remaining
bits of x; ht is identiﬁed with its corresponding integer value in the
range f 0; 1; : : : ; 2t (cid:0) 1g. Moreover, given a binary sequence w,
.w/ be the number of leading zeroes in w plus one3 (e.g.,
we let (cid:26)
C
.00101/ D 3). Unless otherwise speciﬁed, all logarithms are in
(cid:26)
base 2.

C

The value E printed by Algorithm 1 is [5][Theorem 1] an asymp-
totically almost unbiased estimator for the number n of distinct el-
ements in the stream; for n ! 1, the relative standard deviation
(that is, the ratio between the standard deviation of E and n) is at
m, where ˇm is a suitable constant (given
most ˇm=

m  1:06=

p

p

2We remark that in principle O.log n/ bits are necessary to estimate
the number of unique elements in a stream [1]. HyperLogLog is a
practical counter that starts from the assumption that a hash func-
tion can be used to turn a stream into an idealised multiset (see [5]).
3We remark that in the original HyperLogLog papers (cid:26) is used to
C, but (cid:26) is a somewhat standard notation for the ruler func-
denote (cid:26)
tion [8].

Algorithm 1 The Hyperloglog counter as described in [5]: it allows
one to count (approximately) the number of distinct elements in a
stream. ˛m is a constant whose value depends on m and is provided
in [5]. Some technical details have been simpliﬁed.

h W D ! 2

1, a hash function from the domain of items

i   hb.x/;

end; // function add

C(cid:0)hb.x/(cid:1)(cid:9)

(indexed from 0) and set to (cid:0)1

function add.M : counter; x: item/
begin

M Œi    max˚M Œi ; (cid:26)
(cid:0)M Œj (cid:0)1
Z  (cid:16)Pm(cid:0)1

0
1 M Œ(cid:0) the counter, an array of m D 2b registers
2
3
4
5
6
7
8
9
10
11 begin
12
13
14
15
16
17
18
19

foreach item x seen in the stream begin

return E D ˛mm2Z

function size.M : counter/

add(M ,x)

end;
print size.M /

end; // function size

jD0 2

;

in [5])4. Moreover [4] even if the size of the registers (and of the
hash function) used by the algorithm is unbounded, one can limit
it to log log.n=m/ C !.n/ bits obtaining almost certainly the same
output (!.n/ is a function going to inﬁnity arbitrarily slowly); over-
all, the algorithm requires .1C o.1//(cid:1) m log log.n=m/ bits of space
(this is the reason why these counters are called HyperLogLog).
Here and in the rest of the paper we tacitly assume that m (cid:21) 64 and
that registers are made of dlog log ne bits.
3.2 The HyperANF algorithm

The approximate neighbourhood function algorithm described
in [10] is based on the observation that B.x; r/, the ball of radius r
around node x, satisﬁes

B.x; r/ D [

x!y

B.y; r (cid:0) 1/:

Since B.x; 0/ D f x g, we can compute each B.x; r/ incrementally
using sequential scans of the graph (i.e., scans in which we go in
turn through the successor list of each node). The obvious prob-
lem is that during the scan we need to access randomly the sets
B.x; r (cid:0) 1/ (the sets B.x; r/ can be just saved on disk on a update
ﬁle and reloaded later). Here probabilistic counters come into play;
to be able to use them, though, we need to endow counters with a
primitive for the union. Union can be implemented provided that
the counter associated with the stream of data AB can be computed
from the counters associated with A and B; in the case of Hyper-
LogLog counters, this is easily seen to correspond to maximising
the two counters, register by register.

4ANF uses Flajolet–Martin counters instead of HyperLogLog
counters. Since the analogous formula for Flajolet–Martin coun-
ters is 0:78=
m, for a ﬁxed precision HyperANF requires twice as
many registers as ANF, but their size is exponentially smaller.

p

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India626The observations above result in Algorithm 2:

the algorithm
keeps one HyperLogLog counter for each node; at the t-th itera-
tion of the main loop, the counter cŒv is in the same state as if
it would have been fed with B.v; t /, and so its expected value is
jB.v; t /j. As a result, the sum of all cŒv’s is an (almost) unbiased
estimator of NG .t / (for a precise statement, see Theorem 1).

Algorithm 2 The basic HyperANF algorithm in pseudocode. The
algorithm uses, for each node i 2 n, an initially empty Hyper-
LogLog counter ci . The function union.(cid:0);(cid:0)/ maximises two
counters register by register.

cŒ(cid:0), an array of n HyperLogLog counters

function union.M : counter; N : counter/

end

add v to cŒv

foreach i < m begin

M Œi    max.M Œi ; N Œi /

end; // function union
foreach v 2 n begin

0
1
2
3
4
5
6
7
8
9
end;
10
t   0;
11
12 do begin
13
14
15
16
17
18
19
20
21
22
23
24 until no counter changes its value.

m   cŒv;
foreach v ! w begin
m   union.cŒw; m/

end;
write hv; mi to disk

s  P

v size.cŒv/;
Print s (the neighbourhood function NG .t /)
foreach v 2 n begin

end;
Read the pairs hv; mi and update the array cŒ(cid:0)
t   t C 1

We remark that the only sound way of running HyperANF (or
ANF) is to wait for all counters to stabilise (e.g., the last iteration
must leave all counters unchanged). As we will see, any alterna-
tive termination condition may lead to arbitrarily large mistakes on
pathological graphs.5
3.3 HyperANF at hyper speed

Up to now, HyperANF has been described just as ANF with Hy-
perLogLog counters. The effect of this change is an exponential
reduction in the memory footprint and, consequently, in memory
access time. We now describe the the algorithmic and engineering
ideas that made HyperANF much faster, actually so fast that it is
possible to run it up to stabilisation.
Union via broadword programming. Given two HyperLogLog
counters that have been set by streams A and B, the counter associ-
ated with the stream AB can be build by maximising in parallel the
registers of each counter. That is, the register i of the new counter is
given by the maximum between the i-th register of the ﬁrst counter
and the i-th register of the second counter.

5We remark that snap uses a threshold over the relative increment
in the number of reachable pairs as a termination condition, but this
trick makes the tail of the function unreliable.

Each time we scan a successor list, we need to maximise a large
number of registers and store the resulting counter. The immediate
way of obtaining this result requires extracting the value of each
register, maximise it with the other corresponding registers, and
writing down the result in a temporary counter. This process is
extremely slow, as registers are packed in 64-bit memory words. In
the case of Flajolet–Martin counters, the problem is easily solved
by computing the logical OR of the words containing the registers.
In our case, we resort to broadword programming techniques. If the
machine word is w, we assume that at least w registers are allocated
to each counter, so each set of registers is word-aligned.
Let (cid:29) and (cid:28) denote right and left (zero-ﬁlled) shifting, &, j and
˚ denote bit-by-bit not, and, or, and xor; x denotes the bit-by-bit
complement of x.

We use Lk to denote the constant whose ones are in position 0,
that is, the constant with the lowest bit of each k-bit
k, 2k, . . .
subword set (e.g, L8 D 0x01010101010101010101). We use Hk
(cid:28) k (cid:0) 1, that is, the constant with the highest bit of
to denote Lk
each k-bit subword set (e.g, H8 D 0x8080808080808080).

It is known (see [8], or [12] for an elementary proof), that the
following expression

k y WD(cid:16)(cid:0) ..x j Hk / (cid:0) .y & Hk // j x ˚ y(cid:1) ˚ .x j y/

& Hk :

x <u

performs a parallel unsigned comparison k-by-k-bit-wise. At the
end of the computation, the highest bit of each block of k bits will
be set iff the corresponding comparison is true (i.e., the value of the
block in x is strictly smaller than the value of the block in y).

Once we have computed x <u

k, we generate a mask that is made
entirely of 1s, or of 0s, for each k-bit block, depending on whether
we should select the value of x or y for that block:

(cid:1) (cid:0) Lk

 j Hk

 ˚ .x <u

k y/

m D(cid:16)(cid:0).x <u

k y/ (cid:29) k (cid:0) 1 j Hk

This formula works by moving the high bit denoting the result of
the comparison to the least signiﬁcant bit (of each k-bit block).
Then, we or with Hk and subtract 1 from each block, obtaining
either a mask with just the high bit set (if we were starting from
1) or a mask with all bits sets except for the high bit (if we were
starting from 0). The last two operation ﬁx those values so that they
become 00(cid:1)(cid:1)(cid:1) 0 or 11(cid:1)(cid:1)(cid:1) 1. The result of the maximisation process
is now just x & m j y & m.
This discussion assumed that the set of registers of a counter is
stored in a single machine word. In a realistic setting, the registers
are spread among several consecutive words, and we use multiple
precision subtractions and shifts to apply the expressions above on
a sequence of words. All other (logical) operations have just to be
applied to each word in sequence.

All in all, by using the techniques above we can improve the
speed of maximisation by a factor of w=k, which in our case is
about 13 (for graphs of up to 232 nodes). This actually results in
a sixfold speed improvement of the overall application in typical
cases (e.g., web graphs and b D 8), as about 90% of the computa-
tion time is spent in maximisation.
Parallelisation via task decomposition. Although HyperANF is
written as a sequential algorithm, the outer loop lends itself to be
executed in parallel, which can be extremely fruitful on a modern
multicore architecture; in particular, we approach this idea using
task decomposition. We divide the iteration on the whole set of
nodes into a set of small tasks (in the order of the thousands), where
each task consists in iterating on a contiguous segment of nodes. A
pool of threads picks up the ﬁrst available task and solves it: as a
result, we obtain a performance improvement that is linear in the

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India627number of cores. Threads can be designed to be extremely agile,
helped by WebGraph’s facilities which allow us to provide each
thread with a lightweight copy of the graph that shares the bitstream
and associated information with all other threads.
Tracking modiﬁed counters. It is an easy observation that a counter
c that does not change its value is not useful for the next step of the
computation: all counters using c during their update would not
change their value when maximising with c (and we do not even
need to write c on disk). We thus keep track of modiﬁed counters
and skip altogether the maximisation step with unmodiﬁed ones.
Since, as we already remarked, 90% of computation time is spent
in maximisation, this approach leads to a large speedup after the
ﬁrst phases of the computation, when most counters are stabilised.
For the same reason, we keep track of the harmonic partial sums
of small blocks (e.g., 64) of counters. The amount of memory re-
quired is negligible, but if no counter in the block has been modi-
ﬁed, we can avoid a costly computation.
Systolic computation. HyperANF can be run in systolic mode. In
this case, we use also the transposed graph: whenever a counter
changes, it signals back to its predecessors that at the next round
they could change their values. Now, at each iteration nodes that
have not been signalled are entirely skipped during the computa-
tion. Systolic computations are fundamental to get high-precision
runs, as they reduce the cost of an iteration to scanning only the
arcs of the graph that are actually moving information around. We
switch to systolic computation when less than half of the counters
change their values.
3.4 Correctness, errors and memory usage

Very little has been published about the statistical behaviour of
ANF. The statistical properties of approximate counters are well
known, but the values of such counters for each node are highly
dependent, and adding them in a large amount can in principle lead
to an arbitrarily large variance. Thus, making precise statistical
statements about the outcome of a computation of ANF or Hyper-
ANF requires some care. The discussion in the following sections
is based on HyperANF, but its results can be applied mutatis mu-
tandis to ANF as well.
Consider the output ONG .t / of algorithm 2 at a ﬁxed iteration t.
We can see it as a random variable

ONG .t / DX

i2n

where6 each Xi;t is the HyperLogLog counter that counts nodes
reached by node i in t steps; what we want to prove in this section is
a bound on the relative standard deviation of ONG .t / (such a proof,
albeit not difﬁcult, is not provided in the papers about ANF). First
observe that [5], for a ﬁxed a number of registers m per counter, the
standard deviation of Xi;t satisﬁes

pVarŒXi;t 
jB.i; t /j  m;

where m is the guaranteed relative standard deviation of a Hy-
perLogLog counter. Using the subadditivity of standard deviation

(i.e., if A and B have ﬁnite variance,pVarŒA C B pVarŒA C
pVarŒB), we prove the following
ONG .t / of Algorithm 2 at the t-th it-
6Throughout this paper, we use von Neumann’s notation n D
f 0; 1; : : : ; n (cid:0) 1g, so i 2 n means that 0  i < n.

THEOREM 1. The output

Xi;t

Pr

eration is an asymptotically almost unbiased estimator7 of NG .t /,
that is

EŒ ONG .t /

D 1 C ı1.n/ C o.1/ for n ! 1;

q

VarŒ ONG .t /

NG .t /
(cid:0)5
where ı1 is the same as in [5][Theorem 1] (and jı1.x/j < 5 (cid:1) 10
as soon as m (cid:21) 16).
Moreover, ONG .t / has the same relative standard deviation of the
Xi ’s, that is

 m:

NG .t /

PROOF. We have that EŒ ONG .t / D E(cid:2)P
orem 1 of [5], EŒXi;t  D jB.i; t /j .1 C ı1.n/ C o.1//, hence the
q
P
ﬁrst statement. For the second result, we have:
VarŒ ONG .t /

(cid:3). By The-

pVarŒXi 

jB.i; t /j

i2n Xi;t

 P

 m

i2n

D m:

NG .t /

NG .t /

i2n
NG .t /

Since, as we recalled in Section 3.1, the relative standard devi-
ation m satisﬁes m  1:06=
m, to get a speciﬁc value  it is
sufﬁcient to choose m (cid:25) 1:12=2; this assumption yields an over-
all space requirement of about

p

1:12

2 n log log n

bits

(here, we used the obvious upper bound jB.i; t /j  n). For in-
stance, to obtain a relative standard deviation of 9:37% (in every
iteration) on a graph of one billion nodes one needs 74:5 GB of
main memory for the registers (for a comparison, snap would re-
quire 550 GB). Note that since we write to disk the new values of
the registers, this is actually the only signiﬁcant memory require-
ment (the graph can be kept on disk and mapped in memory, as it
is scanned almost sequentially).

Applying Chebyshev’s inequality, we obtain the following:

COROLLARY 1. For every ",

" ONG .t /

NG .t /

2 .1 (cid:0) "; 1 C "/

(cid:21) 1 (cid:0) 2
"2 :

m

In [5] it is argued that the HyperLogLog error is approximately
Gaussian; the counters, however, are not statistically independent
and in fact the overall error does not appear to be normally dis-
tributed. Nonetheless, for every ﬁxed t, the random variable ONG .t /
seems to be unimodal (for example, the average p-value of the
Dip unimodality test [6] for the cnr-2000 dataset is 0:011), so we
can apply the Vysochanski˘ı-Petunin inequality [13], obtaining the
bound

" ONG .t /

NG .t /

Pr

2 .1 (cid:0) "; 1 C "/

(cid:21) 1 (cid:0) 42
9"2 :

m

In the rest of the paper, to state clearly our theorems we will always
assume error " with conﬁdence 1 (cid:0) ı. It is useful, as a practical
7From now on, for the sake of readability we shall ignore the negli-
gible bias on ONG .t / as an estimator for NG .t /: the other estimators
that will appear later on will be qualiﬁed as “(almost) unbiased”,
where “almost” refers precisely to the above mentioned negligible
bias.

#

#

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India628reminder, to note that because of the above inequality for each point
of the neighbourhood function we can assume a relative error of
km with conﬁdence 1(cid:0) 4=.9k2/ (e.g., 2m with 90% conﬁdence,
or 3m with 95% conﬁdence).
As an empirical counterpart to the previous results, we consid-
ered a relatively small graph of about 325 000 nodes (cnr-2000,
see Section 6 for a full description) for which we can compute
the exact neighbourhood function NG .(cid:0)/; we ran HyperANF 500
times with m D 256. At least 96% of the samples (for all t) has
a relative error smaller than twice the theoretical relative standard
deviation 6:62%. The percentage jumps up to 100% for three times
the relative standard deviation, showing that the distribution of the
values behaves better than what the theory would guarantee.
4. DERIVING USEFUL DATA

As advocated in [10], being able to estimate the neighbourhood
function on real-world networks has several interesting applica-
tions. Unfortunately, all published results we are aware of lack sta-
tistical satellite data (such as conﬁdence intervals, or distribution of
the computed values) that make it possible to compare results from
different research groups. Thus, in this section we try to discuss
in detail how to derive useful data from an approximation of the
neighbourhood function.
The distance cdf. We start from the apparently easy task of com-
puting the cumulative distribution function of distances of the graph
G (in short, distance cdf ), which is the function HG .t / that gives
the fraction of reachable pairs at distance at most t, that is,

HG .t / D NG .t /

maxt NG .t /

:

In other words, given an exact computation of the neighbourhood
function, the distance cdf can be easily obtained by dividing all
values by the largest one. Being able to estimate NG .t / allows one
to produce a reliable approximation of the distance cdf:
and conﬁdence 1 (cid:0) ı, that is

THEOREM 2. Assume NG .t / is known for each t with error "

Pr

NG .t /

(cid:21) 1 (cid:0) ı:

2 .1 (cid:0) "; 1 C "/
Let OHG .t / D ONG .t /= maxt
ONG .t /. Then OHG .t / is an (almost) un-
biased estimator for HG .t /; moreover, for a ﬁxed sequence t0, t1,
OHG .tk / is
: : : , tk(cid:0)1, for every " and all 0  i < k we have that
known with error 2" and conﬁdence 1 (cid:0) .k C 1/ı, that is,

" ONG .t /

#

24^

i2k

Pr

OHG .ti /

HG .ti /

2 .1 (cid:0) 2"; 1 C 2"/

35 (cid:21) 1 (cid:0) .k C 1/ı:

PROOF. Note that if

1 (cid:0) "  ONG .t /=NG .t /  1 C "

holds for every t, then a fortiori

1 (cid:0) "  max

ONG .t /= max

NG .t /  1 C "

t

t

(because, although the maxima might be ﬁrst attained at different
values of t, the same holds for any larger values). As a conse-
quence,

1 (cid:0) 2"  1 (cid:0) "
1 C "



OHG .t /

HG .t /

 1 C "
1 (cid:0) "

 1 C 2":

The probability 1 (cid:0) .k C 1/ı is immediate from the union bound,
as we are considering k C 1 events at the same time.

Note two signiﬁcant limitations: ﬁrst of all, making precise state-
ments (i.e., with conﬁdence) about all points of HG .t / requires a
very high initial error and conﬁdence. Second, the theorem holds if
HyperANF has been run up to stabilisation, so that the probabilistic
guarantees of HyperLogLog hold for all t.

The ﬁrst limitation makes in practice impossible to get directly
sensible conﬁdence intervals, for instance, for the average distance
or higher moments of the distribution (we will elaborate further on
this point later). Thus, only statements about a small, ﬁnite number
of points can be approached directly.

The second limitation is somewhat more serious in theory, al-
beit in practice it can be circumvented making suitable assump-
tions about the graph under examination (which however should be
clearly stated along the data). Consider the graph G made by two
k-cliques joined by a unidirectional path of ` nodes (see Figure 2).
Even neglecting the effect of approximation, G can “fool” Hyper-
ANF (or ANF) so that the distance cdf is completely wrong (see
Figure 1) when using any stopping criterion that is not stabilisa-
tion.

Figure 1: The real cdf of the graph in Figure 2 (+), and the one
that would be computed using any termination condition that
is not stabilisation (*); here ` D 10 and k D 260.

Figure 2: Two k-cliques joined by a unidirectional path of `
nodes: terminating even one step earlier than stabilisation com-
pletely miscalculates the distance cdf (see Figure 1); the effec-
tive diameter is `C 1, but terminating even just one step earlier
than stabilisation yields an estimated effective diameter of 1.

Indeed, the exact neighbourhood function of G is given by:

8ˆ<ˆ:2k C `
.t C 1/(cid:0)2k C ` (cid:0) t

if t D 0

.` C 1/

2k C `

(cid:16)

(cid:1) (cid:0) 2k C 2k2
 (cid:0) 2k C 3k2

2

2

NG .t / D

if 1  t  `
if ` < t.

The key observation is that the very last value is signiﬁcantly larger
than all previous values, as at the last step the nodes of the right
clique become reachable from the nodes of the ﬁrst clique. Thus, if

 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10HG(t)tWWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India629iteration stops before stabilisation,8 the normalisation factor used
to compute the cdf will be smaller by (cid:25) k2 than the actual value,
causing a completely wrong estimation of the cdf, as shown in Fig-
ure 1.

Although this counterexample (which can be easily adapted to
be symmetric) is deﬁnitely pathological, it suggests that a particu-
lar care should be taken when handling graphs that present narrow
“tubes” connecting large connected components: in such scenarios,
the function NG .t / exhibits relatively long plateaux (preceded and
followed by sharp bumps) that may fool the computation of the cdf.
The effective diameter. The ﬁrst application of ANF was the com-
putation of the effective diameter. The effective diameter of G at
˛ is the smallest t0 such that HG .t0/ (cid:21) ˛; when ˛ is omitted, it
is assumed to be ˛ D :9.9 The interpolated effective diameter is
obtained in the same way on the linear interpolation of the points
of the neighbourhood function.10
Since that the function OHG .t / is necessarily monotone in t (inde-
pendently of the approximation error), from Theorem 2 we obtain:
COROLLARY 2. Assume ONG .t / is known for each t with error
" and conﬁdence 1 (cid:0) ı, and there are points s and t such that

OHG .s/
1 (cid:0) 2"

 ˛ 

OHG .t /
1 C 2"

:

Then, with probability 1 (cid:0) 3ı the effective diameter at ˛ lies in
Œs : : t .
Unfortunately, since the effective diameter depends sensitively on
the distance cdf, again termination conditions can produce arbitrary
errors. Getting back to the example of Figure 2, with a sufﬁciently
large k, for example k D 2`2 C 5` C 2, the effective diameter
is ` C 1, which would be correctly output after ` C 1 iterations,
whereas even stopping one step earlier (i.e., with t D `) would
produce 1 as output, yielding an arbitrarily large error. snap, in-
deed, fails to produce the correct result on this graph, because it
stops iterating whenever the ratio between two successive iterates
of NG is sufﬁciently close to 1.

Algorithm 3 Computing the effective diameter at ˛ of a graph G;
Algorithm 2 is used to compute ONG.

end;

foreach t D 0; 1; : : : begin

compute ONG .t / (error ", conﬁdence 1 (cid:0) ı)
if (some termination condition holds) break

0
1
2
3
4 M   max ONG .t /
(cid:0) such that ONG .D
(cid:0)
ﬁnd the largest D
5
C such that ONG .D
ﬁnd the smallest D
6
C
 with conﬁdence 1 (cid:0) 3ı
output ŒD
7
end;
8

: : D

(cid:0)

/=M  ˛.1 (cid:0) 2"/
C
/=M (cid:21) ˛.1 C 2"/

Algorithm 3 is used to estimate the effective diameter of a graph;
albeit this approach is reasonable (and actually it is similar to that
8We remark that stabilisation can occur, in principle, even before
the last step because of hash collisions in HyperLogLog counters,
but this will happen with a controlled probability.
9The actual diameter of G is its effective diameter at 1, albeit the
latter is deﬁned for all graphs whereas the former makes sense only
in the strongly connected case.
10For obvious linearity reasons, the sampled interpolated effective
diameter is an unbiased estimator of the interpolated effective di-
ameter, whereas the same does not hold for the effective diameter.

adopted by snap, although the latter does not provide any conﬁ-
dence interval), unless the neighbourhood function is known with
very high precision it is almost impossible to obtain good upper
bounds, because of the typical ﬂatness of the distance cdf after the
90th percentile. Moreover, results computed using a termination
condition different from stabilisation should always be taken with
a grain of salt because of the discussion above.
The distance density function. The situation, from a theoretical
viewpoint, is somehow even worse when we consider the density
function hG .(cid:0)/ associated with the cdf HG .(cid:0)/. Controlling the
error on hG .(cid:0)/ is not easy:
LEMMA 1. Assume that, for a given t, OHG .t / is an estimator of
HG .t / with error " and conﬁdence 1(cid:0)ı. Then O
hG .t / D hG .t /˙2"
with conﬁdence 1 (cid:0) 2ı.
PROOF. With conﬁdence 1 (cid:0) 2ı,
O
hG .t / D OHG .t / (cid:0) OHG .t (cid:0) 1/

 .1 C "/HG .t / (cid:0) .1 (cid:0) "/HG .t (cid:0) 1/  hG .t / C 2";

and similarly O

hG .t / (cid:21) hG .t / (cid:0) 2".

t

t

t

t

O

t hG .t / C 2"DG

hG .t / X

t hG .t / (cid:0) 2"DG X

Note that the bound is very weak: since our best generic lower
bound is hG .t / (cid:21) 1=n2, the relative error with which we known a
point hG .t / is 2"n2 (which, of course, is pretty useless).
Moments. Evaluation of the moments of hG .(cid:0)/ poses further
X
problems. Actually, by Lemma 1 we can deduce that
with conﬁdence 1(cid:0) 2DG ", where DG is the diameter of G, which
implies that the expected value of O
hG .(cid:0)/ is an (almost) unbiased
estimator of the expected value of hG .(cid:0)/. Nonetheless, the bounds
we obtain are horrible (and actually unusable).
The situation for the variance is even worse, as we have to prove
O
that we can use VarŒ
hG  as an estimator to VarŒhG . Note that for
a ﬁxed graph G, HG is a precise distribution and VarŒhG  is an
actual number. Conversely, O
O
hG ) is a random
OHG is an (almost) unbi-
variable11. By Theorem 2, we know that
ased pointwise estimator for HG, and that we can control its con-
centration by suitably choosing the number m of counters. We are
going to derive bounds on the approximation of VarŒhG  using the
OHG .t / up to ODG (i.e., the iteration at which HyperANF
values of
stabilises):
OHG .t / is an
LEMMA 2. Assume that, for every 0  t  ODG,
O
estimator of HG .t / with error " and conﬁdence 1(cid:0)ı; then, VarŒ
hG 
is an estimator of VarŒhG  with error

hG (and hence VarŒ

"  8"

D3
G

C 4"2 D4

G

VarŒhG 

VarŒhG 
and conﬁdence 1 (cid:0) .DG C 1/ı.
OH in Œ0 : : DG  im-
PROOF. Assuming error " on the values of
plies conﬁdence 1 (cid:0) .DG C 1/ı. Since ODG  DG < 1, and by
11More precisely, O
random variables O

hG is a sequence of (stochastically dependent)
hG .0/, O

hG .1/, . . .

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India630deﬁnition O
O

VarŒ

t

t

t

t

t

t 2 O

O
hG .t /

hG .t / D 0 for t > ODG we have (t ranges in Œ0 : : DG ):
hG  DX
2
X

hG .t / (cid:0)(cid:16)X
2
t 2(cid:0)hG .t / C 2"(cid:1) (cid:0)(cid:16)X
t hG .t / (cid:0) 2"
X
X
 VarŒhG  C 2"
(cid:0)DG C EŒhG (cid:1)
 VarŒhG  C 4"D2
G
 VarŒhG  C 8"D3
G ;

t 2 C 4"EŒhG 

X

t

t

t

t

t

where EŒhG  is the average path length. Similarly
(cid:0) 4"2D4
G :

O
hG  (cid:21) VarŒhG  (cid:0) 8"D3

VarŒ

G

Hence the statement.

The error and conﬁdence we obtain are again unusable, but the
lemma proves that with enough precision and conﬁdence on OHG .(cid:0)/
we can get precision and conﬁdence on VarŒhG .
The results in this section suggests that if computations involve
the moments the only realistic possibility is to resort to parametric
statistics to study the behaviour of the value of interest on a large
number of samples. That is, it is better to compute a large number
of relatively low-precision approximate neighbourhood functions
than a small number of high-precision ones, as from the former the
latter are easily computable by averaging, whereas it is impossible
to obtain a large number of samples of derived values from the
latter. As we will see, this approach works surprisingly well.

5. SPID

The main purpose of computing aggregated data such as the dis-
tance distribution is that we can try to deﬁne indices that express
some structural property of the graph we study, an obvious exam-
ple being the average distance, or the effective diameter.

One of the main goal of our recent research has been ﬁnding a
simple property that clearly distinguishes between social networks
deriving from human interaction (what is usually called a social
network in the strong or proper sense: DBLP, Facebook, etc.) and
web-based graphs, which share several properties of social net-
works, and as the latter arise from human activity, but present a
visibly different structure.

In this paper we propose for the ﬁrst time to use the index of dis-
persion (cid:27) 2=(cid:22) (a.k.a. variance-to-mean ratio) of the distance dis-
tribution as a measure of the “webbiness” of a social network. We
call such index the spid (shortest-paths index of dispersion)12 of G.
In particular, networks with a spid larger than one are to be consid-
ered “web-like”, whereas networks with a spid smaller than one are
to be considered “properly social”. We recall that a distribution is
called under- or over-dispersed depending on whether its index of
dispersion is smaller or larger than 1, so a network is properly so-
cial or not depending on whether its distance distribution is under-
or over-dispersed.

The intuition behind the spid is that “properly social” networks
strongly favour short connections, whereas in the web long con-
nection are not uncommon: this intuition will be conﬁrmed in Sec-
tion 6.

12If we were to follow strictly the terminology used in this paper,
this would be the index of dispersion of the distance distribution,
but we guessed that the acronym IDDD would not have been as as
successful.

Figure 3: Cumulative density function of 100 values of the spid
computed using HyperANF on cnr-2000. For comparison, we
also plot random samples of size 100 and 10 000 drawn from a
normal distribution.

As discussed in the previous section, in theory estimating the
spid is an impossible task, due to the inherent difﬁculty of evaluat-
ing the moments of hG .(cid:0)/. In practice, however, the estimate of
the spid computed directly on runs of HyperANF are quite precise.
From the actual neighbourhood function computed for cnr-2000
we deduce that the graph spid is 2:49. We then ran 100 iteration of
HyperANF with a relative standard deviation of 9:37%, computing
for each of them an estimation of the spid; these values approx-
imately follow a normal distribution of mean 2:489 and standard
deviation 0:9 (see Figure 3). We obtained analogous concentration
results for the average distance. In some pathological cases, the
distribution is not Gaussian, albeit it always turns out to be uni-
modal (in some cases, discarding few outliers), so we can apply
the Vysochanski˘ı-Petunin inequality. We will report some relevant
observations on the spid of a number of graphs after describing our
experiments.

6. EXPERIMENTS

We ran our experiments on the datasets described in Table 2:
(cid:15) the web graphs are almost all available at http://law.dsi.
unimi.it/, except for the altavista-2002 dataset that
was provided by Yahoo! within the Webscope program (Al-
taVista webpage connectivity dataset, version 1.0, http://
research.yahoo.com/Academic_Relations);13
(cid:15) for the social networks: hollywood-2009 (http://www.
imdb.com/) is a co-actorship graph where vertices represent
actors; dblp-2010 (http://www.informatik.uni-trier.
de/~ley/db/) is a scientiﬁc collaboration network where
each vertex represents a scientist and two vertices are con-
nected if they have worked together on an article; in ljournal-
2008 (http://www.livejournal.com/) nodes are users
and there is an arc from x to y if x registered y among
his friends (it is not necessary to ask y permission, so the
graph is directed); amazon-2008 (http://www.archive.
org/details/amazon_similarity_isbn/) describes sim-
ilarity among books as reported by the Amazon store; en-
ron is a partially anonymised corpus of e-mail messages ex-
changed by some Enron employees (nodes represent people
and there is an arc from x to y whenever y was the re-
cipient of a message sent by x); ﬁnally in flickr (http:

13It should be remarked by this graph, albeit widely used in the
literature, is not a good dataset. The dangling nodes are 53:74%—
an impossibly high value [11], and an almost sure indication that
all nodes in the frontier of the crawler (and not only visited nodes)
were added to the graph, and the giant component is less than 4%
of the whole graph.

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India631Graph
amazon-2008
indochina-2004
altavista-2002

Kronecker
(177 K nodes, 2 B arcs)

snap HyperANF
5 s
9.5 m
1.83 m
4.62 h
-
1.2 h
HADI (90 machines) HyperANF
2.25 m

30 m

Table 1: A comparison of the speed of snap/HADI vs. Hyper-
ANF. The tests on snap were performed on our hardware. Both
algorithms were stopped at a relative increment of 0:001. The
timings of HADI on the M45 cluster are the best reported in [7],
and both algorithms ran three iterations. We remark that a run
of HyperANF on the Kronecker graph takes less than ﬁfteen
minutes on a laptop.

//www.flickr.com/14) vertices correspond to Flickr users
and there is an edge connecting x and y whenever either ver-
tex is recorded as a contact of the other one.

tion conditions different from stabilisation do not alter the com-
puted values.

Figure 4: A plot showing the strong linear correlation between
the average distance and the effective diameter.

At the best of our knowledge, this is the ﬁrst paper where such a
wide and diverse set of data is studied, and where features such
as effective diameter or average path length are computed on very
large graphs with precise statistical guarantees.

All experiments are performed on a Linux server equipped with
Intel Xeon X5660 CPUs (2:80 GHz, 12 MB cache size) for overall
24 cores and 128 GB of RAM; the server cost about 8 900 EUR in
2010.

A brief comparison with snap and HADI timings is shown in
Table 1. Essentially, on our hardware HyperANF is two orders
of magnitudes faster than snap. Our run on the Kronecker graph is
one order of magnitude faster than HADI’s (or three orders of mag-
nitude faster, if you take into consideration the number of machines
involved), but this comparison is unfair, as in principle HADI can
scale to arbitrarily large graphs, whereas we are limited by the
amount of memory available. Nonetheless, the speedup is clearly a
breakthrough in the analysis of large graphs. It would be interest-
ing to compare our timings for the altavista-2002 dataset with
HADI’s, but none have been published.

It is this speed that makes it possible, for the ﬁrst time, to com-
pute data associated with the distance distribution with high pre-
cision and for a large number of graphs. We have 100 runs with
relative standard deviation of 9:37% for all graphs, except for those
on the altavista-2002 dataset (13:25%). All graphs are run to
stabilisation. Our computations are necessarily much longer (usu-
ally, an order of magnitude longer in iterations) than those used to
compute the effective diameter or similar measures. This is due to
the necessity of computing with high precision second-order statis-
tics that are used to compute the spid.

Previous publications used few graphs, mainly because of the
large computational effort that was necessary, and no data was
available about the number of runs. Moreover, we give precise con-
ﬁdence intervals based on parametric statistics for data depending
on the second moment, such as the spid—something that has never
done before. We gather here our ﬁndings.
A posteriori parameters are highly concentrated. According to
our experiments, computing the effective diameter, average dis-
tance and spid on a large number of low-precision runs generates
highly concentrated distributions (see the empirical standard devi-
ation in Table 2). Thus, we suggest this approach for computing
such values, provided that there is enough evidence that termina-

14We thank Yahoo! for the experimental results on the Flickr graph.

Effective diameter and average distance are essentially linearly
correlated. Figure 4 shows a scatter plot of the two values, and
the line 2x=3 C 1. The correlation between the two values has
always been folklore in the study of social networks, but we can
conﬁrm that on both social and web networks the connection can
be exactly expressed in linear terms (it would be of course interest-
ing to prove such a correlation formally, under suitable restrictions
on the structure of the graph). This fact suggests that the average
distance (which is more principled from a statistic viewpoint, and
parameter-free) should be used as the reference parameter to ex-
press the closeness between nodes. Moreover, experimentally the
standard deviation of the effective diameter in a posteriori com-
putations is usually signiﬁcantly larger than that of the average
distance.

Incidentally, the average distance of the altavista-2002 dataset
is 16:5—slightly more than what reported in [7] (possibly because
of termination conditions artifacts).
It is difﬁcult to give a priori conﬁdence intervals for the effective
diameter with a small number of runs. Unless a large number of
runs is available, so that the precision of the approximation of the
neighbourhood function can be signiﬁcantly lowered, it is impossi-
ble to provide interesting upper bounds for the effective diameter.
The spid can tell social networks from web graphs. As shown
in Table 2, even taking the standard deviation into account spids
are pretty much below 1 for social networks and above 1 for web
graphs; host graphs (not surprisingly) behave like social networks.
Note that this works both for directed and undirected graphs. Fig-
ure 5 shows the spid values obtained for our datasets plotted against
the graph size, and also witnesses that there is no correlation (a sim-
ilar graph, not shown here, testiﬁes that spid is also independent
from density). Figure 6 shows that there is some slight correlation
between the spid and the average distance: nonetheless, there is no
way to tell networks from our dataset apart using the latter value,
whereas the under- or over-dispersion of the distance distribution,
as deﬁned by the spid, never makes a mistake. Of course, we expect
to enrich this graph in time with more datasets: we are particularly
interested in gathering very large social networks to test the spid at
large sizes.

As a sanity check, we have also computed on several datasets
the spid of the giant component (see Table 3), which turns out to
be very similar to the spid of the whole graph. We see this as a

 2 4 6 8 10 12 14 16 18 0 5 10 15 20 25 30average distanceinterpolated effective diameterWWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India632Name
amazon-2008
dblp-2010
enron
ljournal-2008
flickr
hollywood-2009
indochina-2004-hosts
uk-2005-hosts
cnr-2000
eu-2005
in-2004
indochina-2004
uk@10E6
uk@10E7
it-2004
uk-2007-05
altavista-2002

Type
social (u)
social (u)
social (d)
social (d)
social (u)
social (u)
host (d)
host (d)
web (d)
web (d)
web (d)
web (d)
web (d)
web (d)
web (d)
web (d)
web (d)

Nodes
735 323
326 186
69 244
5 363 260
526 606
1 139 905
19 123
587 205
325 557
862 664
1 382 908
7 414 866
100 000
1 000 000
41 291 594
105 896 555
1 413 511 390

Arcs
5 158 388
1 615 400
276 143
79 023 142
47 097 454
113 891 327
233 380
12 825 465
3 216 152
19 235 140
16 917 053
194 109 311
3 050 615
41 247 159
1 150 725 436
3 738 733 648
6 636 600 779

spid .˙(cid:27) /
0:76 .˙0:060/
0:36 .˙0:034/
0:21 .˙0:020/
0:21 .˙0:023/
0:14 .˙0:009/
0:14 .˙0:012/
0:35 .˙0:021/
0:30 .˙0:018/
2:50 .˙0:086/
1:25 .˙0:209/
1:30 .˙0:173/
1:64 .˙0:134/
1:64 .˙0:111/
1:76 .˙0:043/
2:14 .˙0:149/
1:10 .˙0:234/
4:24 .˙0:764/

ad .˙(cid:27) /

12:05 .˙0:206/
7:34 .˙0:114/
4:24 .˙0:065/
5:99 .˙0:078/
3:50 .˙0:047/
3:87 .˙0:045/
4:26 .˙0:079/
5:93 .˙0:081/
17:35 .˙0:313/
10:17 .˙0:363/
15:40 .˙0:374/
15:63 .˙0:338/
5:97 .˙0:172/
8:96 .˙0:172/
15:02 .˙0:300/
15:39 .˙0:418/
16:69 .˙0:779/

ied .˙(cid:27) /

15:50 .˙0:433/
8:96 .˙0:215/
4:94 .˙0:103/
6:92 .˙0:143/
3:92 .˙0:049/
4:42 .˙0:109/
5:44 .˙0:164/
7:06 .˙0:151/
25:45 .˙0:357/
14:31 .˙0:988/
20:74 .˙0:792/
21:68 .˙0:658/
10:36 .˙0:251/
14:31 .˙0:341/
19:65 .˙0:698/
19:93 .˙1:030/
23:04 .˙2:517/

ed

Œ14 : : 18
Œ8 : : 10
Œ4 : : 6
Œ6 : : 8
Œ3 : : 5
Œ4 : : 5
Œ5 : : 7
Œ6 : : 8

Œ23 : : 29
Œ13 : : 16
Œ20 : : 24
Œ20 : : 26
Œ8 : : 12
Œ12 : : 17
Œ18 : : 22
Œ18 : : 23
Œ19 : : 31

Table 2: Our main data table. “Type” describes whether the given graph is a web-graph, a proper social network, or the host
quotient of a web graph (u=undirected, d=directed). The graphs uk@10E6 and uk@10E7 are obtained by visiting in a breadth-ﬁrst
fashion uk-2007-05 starting from a random node. They simulate smaller crawls of a larger network. We show spid, average distance
and interpolated effective diameter a posteriori with their empirical standard deviation, and intervals for the effective diameter with
85% conﬁdence for a comparison.

Type
Name
social (u)
amazon-2008
social (u)
dblp-2010
social (d)
enron
social (u)
ljournal-2008
social (u)
hollywood-2009
web (d)
cnr-2000
web (d)
eu-2005
web (d)
in-2004
indochina-2004 web (d)
web (d)
it-2004
web (d)
uk-2007-05

Nodes (%)
627 646 .85%/
226 413 .69%/
8 271 .12%/
4 185 423 .78%/
1 069 126 .94%/
112 023 .34%/
752 725 .87%/
593 687 .43%/
3 806 327 .51%/
29 855 421 .72%/
68 582 555 .65%/

Arcs
4 706 251
1 432 920
147 353
74 928 066
113 682 432
1 646 332
17 933 415
7 827 263
98 815 195
938 694 394
2 814 425 880

spid .˙(cid:27) /
0:73 .˙0:062/
0:37 .˙0:038/
0:18 .˙0:019/
0:19 .˙0:015/
0:14 .˙0:010/
2:50 .˙0:098/
1:30 .˙0:200/
1:49 .˙0:138/
1:60 .˙0:162/
2:13 .˙0:130/
1:16 .˙0:508/

ad .˙(cid:27) /

11:77 .˙0:184/
7:37 .˙0:115/
3:53 .˙0:053/
5:84 .˙0:077/
3:85 .˙0:044/
15:95 .˙0:351/
10:11 .˙0:350/
14:98 .˙0:389/
15:36 .˙0:395/
14:73 .˙0:303/
15:05 .˙0:318/

ied .˙(cid:27) /

15:03 .˙0:347/
9:03 .˙0:246/
3:99 .˙0:101/
6:75 .˙0:093/
4:38 .˙0:104/
23:82 .˙0:389/
14:43 .˙0:991/
20:92 .˙0:766/
21:26 .˙0:830/
19:27 .˙0:752/
19:46 .˙0:848/

ed

Œ14 : : 17
Œ8 : : 10
Œ3 : : 5
Œ6 : : 8
Œ4 : : 5

Œ22 : : 27
Œ13 : : 17
Œ19 : : 24
Œ19 : : 24
Œ17 : : 22
Œ18 : : 22

Table 3: A table parallel to Table 2, but containing data for giant connected components. Items not appearing here are either too
small, or have too small a giant component for the results to be signiﬁcant.

clear sign that the spid is largely independent of the artifacts of the
crawling process.
Direction should not be destroyed when analysing a graph. We
conﬁrm that symmetrising graphs destroys the combinatorial struc-
ture of the network: the average distance drops to very low values
in all cases, as well as the spid. This suggests that there is impor-
tant structural information that is being ignored. We also note that
since all web snapshot we have at hand are gathered by some kind
of breadth-ﬁrst visit, they represent balls of small diameter centred
at the seed: symmetrising the graph we cannot expect to get an av-
erage distance that is larger than twice the radius of the ball. All
in all, the only advantage of symmetrising a graph is a signiﬁcant
reduction in the number of iterations that are needed to complete a
computation of the neighbourhood function.15

To give a more direct idea of the level of precision of our diam-
eter estimation, we computed the actual diameter at ˛ for the cnr-
2000 dataset, and plotted it against the interval estimation obtained
by HyperANF

that the correct parallel framework for implementing a diffusing
computation is a synchronous parallel system where computation
happens at nodes and communication is sent from node to node
with messages. Such a framework, Pregel, has been recently devel-
oped at Google [9]. In a Pregel implementation of HyperANF, ev-
ery computational node sends its own counter as message to its pre-
decessors if it changed from the previous iteration, waits for incom-
ing messages from its successors, and computes the maximisation
procedure on the received messages. Due to the small size of Hy-
perLogLog counter (exponentially smaller than the Flajolet–Martin
counters used by ANF), the amount of communication would be
very small.

Although in this paper, we preferred to focus on the computation
of the spid, we remark that HyperANF can also be used to build the
radius distribution described in [7], or the related closeness central-
ity.

8. CONCLUSIONS

7. FUTURE WORK

HyperANF lends itself naturally to distributed implementations.
However, contrarily to the approach taken by HADI [7], we think
15We remark that the “diameter 7 (cid:24) 8” claim in [7] about the
altavista-2002 dataset refers to the effective diameter for the
symmetrised version of the graph.

HyperANF is a breakthrough improvement over the original ANF
techniques, mainly because of the usage of the more powerful Hy-
perLogLog counters combined with their fast broadword combina-
tion and systolic computation. HyperANF can run to stabilisation
very large graphs, computing data with statistical guarantees.

We consider, however, the introduction of the spid of a graph
the main conceptual contribution of this paper. HyperLogLog is

WWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India633Figure 5: A plot showing the spid values (vertical) for our
datasets compared with their size (i.e., number of nodes, hor-
izontal): red squares correspond to social networks, blue dia-
monds to web graphs and black circles to host graphs.

Figure 7: Effective diameters at ˛ for the cnr-2000 dataset;
red bullets show the real effective diameter, whereas green
crosses show the upper and lower extreme of the conﬁdence
interval obtained running 100 HyperANF with m D 128.

Lecture Notes in Computer Science, pages 605–617.
Springer, 2003.

[5] Philippe Flajolet, Éric Fusy, Olivier Gandouet, and Frédéric

Meunier. Hyperloglog: the analysis of a near-optimal
cardinality estimation algorithm. In Proceedings of the 13th
conference on analysis of algorithm (AofA 07), pages
127–146, 2007.

[6] J. A. Hartigan and P. M. Hartigan. The dip test of

unimodality. Ann. Statist., 13(1):70–84, 1985.

[7] U Kang, Charalampos E. Tsourakakis, Ana Paula Appel,

Christos Faloutsos, , and Jure Leskovec. HADI: Mining radii
of large graphs. ACM Transactions on Knowledge Discovery
from Data, 2010.

[8] Donald E. Knuth. The Art of Computer Programming.

Pre-Fascicle 1A. Draft of Section 7.1.3: Bitwise Tricks and
Techniques, 2007.

[9] Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik,
James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz
Czajkowski. Pregel: a system for large-scale graph
processing. In SIGMOD ’10: Proceedings of the 2010
international conference on Management of data, pages
135–146, New York, NY, USA, 2010. ACM.

[10] Christopher R. Palmer, Phillip B. Gibbons, and Christos
Faloutsos. Anf: a fast and scalable tool for data mining in
massive graphs. In KDD ’02: Proceedings of the eighth ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 81–90, New York, NY, USA, 2002.
ACM.

[11] Sebastiano Vigna. Stanford matrix considered harmful. In

Andreas Frommer, Michael W. Mahoney, and Daniel B.
Szyld, editors, Web Information Retrieval and Linear
Algebra Algorithms, number 07071 in Dagstuhl Seminar
Proceedings, 2007. http://arxiv.org/abs/0710.1962.

[12] Sebastiano Vigna. Broadword implementation of rank/select

3(cid:27) rule for unimodal distributions” [Teor. Veroyatnost. i
Mat. Statist. 21 (1979), 23–35]. Teor. Veroyatnost. i Mat.
Statist., 27:26–27, 157, 1982.

queries. In Catherine C. McGeoch, editor, Experimental
Algorithms. 7th International Workshop, WEA 2008, number
5038 in Lecture Notes in Computer Science, pages 154–168.
Springer–Verlag, 2008.

[13] D. F. Vysochanski˘ı and Yu. ¯I. Petun¯ın. Remark: “Proof of the

Figure 6: A plot showing the spid against the average distance
using the same conventions of Figure 5.

instrumental in making the computation of the spid possible, as the
latter requires a number of iterations that is an order of magnitude
larger than those required for an estimate of the effective diameter.

Acknowledgements.

Flavio Chierichetti participated to the earlier phases of this work.
We want to thank Dario Malchiodi for fruitful discussions and hints.

9. REFERENCES
[1] Noga Alon, Yossi Matias, and Mario Szegedy. The space
complexity of approximating the frequency moments. J.
Comput. Syst. Sci, 58(1):137–147, 1999.

[2] Paolo Boldi and Sebastiano Vigna. The WebGraph

framework I: Compression techniques. In Proc. of the
Thirteenth International World Wide Web Conference (WWW
2004), pages 595–601, Manhattan, USA, 2004. ACM Press.
[3] Edith Cohen. Size-estimation framework with applications to

transitive closure and reachability. J. Comput. Syst. Sci.,
55:441–453, 1997.

[4] Marianne Durand and Philippe Flajolet. Loglog counting of

large cardinalities (extended abstract). In Giuseppe Di
Battista and Uri Zwick, editors, Algorithms - ESA 2003, 11th
Annual European Symposium, Budapest, Hungary,
September 16-19, 2003, Proceedings, volume 2832 of

 0 0.5 1 1.5 2 2.5 3 3.5 4 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10spidsize 0 0.5 1 1.5 2 2.5 3 3.5 4 2 4 6 8 10 12 14 16 18spidaverage distance 0 5 10 15 20 25 30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9effective diameter at ααWWW 2011 – Session: Social Network AlgorithmsMarch 28–April 1, 2011, Hyderabad, India634