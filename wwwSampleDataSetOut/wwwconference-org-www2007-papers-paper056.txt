Robust Methodologies for Modeling Web Click

Distributions

Kamal Ali, Yahoo!
701 First Avenue

Sunnyvale, CA USA

kamal3@yahoo.com

ABSTRACT
Metrics such as click counts are vital to online businesses but
their measurement has been problematic due to inclusion of
high variance robot traﬃc. We posit that by applying sta-
tistical methods more rigorous than have been employed to
date that we can build a robust model of the distribution
of clicks following which we can set probabilistically sound
thresholds to address outliers and robots. Prior research
in this domain has used inappropriate statistical method-
ology to model distributions and current industrial prac-
tice eschews this research for conservative ad-hoc click-level
thresholds. Prevailing belief is that such distributions are
scale-free power law distributions but using more rigorous
statistical methods we ﬁnd the best description of the data is
instead provided by a scale-sensitive Zipf-Mandelbrot mix-
ture distribution. Our results are based on ten datasets
from various verticals in the Yahoo domain. Since mixture
models can overﬁt the data we take care to use the BIC log-
likelihood method which penalizes overly complex models.
Using a mixture model in the web activity domain makes
sense because there are likely multiple classes of users. In
particular, we have noticed that there is a signiﬁcantly large
set of “users” that visit the Yahoo portal exactly once a day.
We surmise these may be robots testing internet connectiv-
ity by pinging the Yahoo main website.

Backing up our quantitative analysis is graphical analysis
in which empirical distributions are plotted against theoret-
ical distributions in log-log space using robust cumulative
distribution plots. This methodology has two advantages:
plotting in log-log space allows one to visually diﬀerentiate
the various exponential distributions and secondly, cumula-
tive plots are much more robust to outliers. We plan to use
the results of this work for applications for robot removal
from web metrics business intelligence systems.

Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Sta-
tistics

General Terms
Measurement, Theory

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

Mark Scarr, Yahoo!

701 First Avenue

Sunnyvale, CA USA

mscarr@yahoo-inc.com

1.

INTRODUCTION

Modern businesses rely on accurate counts of web page-
views and clicks to calculate growth rates and market share.
Per-user page-views in the millions (per month) and other
suspicious statistics lead to the belief that a signiﬁcant amount
of traﬃc originates from robots. Failure to remove robots
can mislead businesses about their growth metrics. Fur-
thermore, we see higher temporal (month to month or day
to day) variance from the robot population so failure to re-
move them also degrades statistical power [20] in comparing
ﬁelded systems to their beta counterparts.

In order to remove robots on a more principled basis, we
need to have a better characterization of the distribution of
click behavior. One method of removing robots is to iden-
tify them with outliers and remove outliers. Outlier removal
using distributional methods proceeds by ﬁtting a model to
the observed distribution and then selecting a tail probabil-
ity (say 0.1%) to use as a deﬁnition of an outlier. Using the
model, we can then translate that probability into a statisti-
cally founded threshold of clicks and remove all “users” that
exceed that threshold. Currently, businesses use very con-
servative ad-hoc thresholds for robot removal. Knowing the
distribution more precisely would allow them to be more ag-
gressive in removing robots and thus produce more accurate
and stable metrics for business.

Most previous work in web traﬃc distribution modeling
has been done for network caching and relay applications [7,
3] - little to none has been done for web analytics. The work
that has been done did not appear to test a wide variety of
distribution families and some authors used continuous dis-
tributions [9, 1], which are not appropriate for discrete (click
and pageview) distributions. Finally, the work appears not
to have used rigorous statistical methodology. For instance,
many authors simply plot the observed distribution in log-
log space (log of frequency of x versus log(x)) and then pro-
ceed to ﬁt a straight line, using Pearson’s correlation coef-
ﬁcient [16] to measure goodness of ﬁt. This is completely
incorrect because typically the points corresponding to low
pageview and click counts represent millions of users and
other points may only represent a single user. Thus, at the
very least, a weighted regression is called for. Even so, out-
liers can have large impact on regression ﬁtting methods and
no indication was given that robust regression methods were
used. Instead of regression, we use the maximum likelihood
method (MLE) which is much more robust.

In our methodology, we try to ﬁt a large number of dis-
tribution families to the data using the robust maximum-
likelihood estimation [16] (MLE) method. Then we evalu-

ate the goodness of ﬁt using the (log) likelihood of the data
given the ﬁtted distribution. Since some models have more
parameters that can overﬁt the data, we use the Bayesian In-
formation Criterion (BIC, [17]) correction to log-likelihood.
Essentially this measure requires that more complex mod-
els “pay for their complexity” by providing a better ﬁt to
the data.
In the case where we have nested models the
likelihood-ratio test [16] is also used to see what other distri-
butions are statistically indistinguishable from the winning
one - thereby forming a set of winners. Although inappropri-
ate for count data we also use two continuous distributions
(Inverse Gaussian [9], log-normal [10]) that have been used
in prior work so as to compare their scores to the theoreti-
cally correct discrete distributions.

Graphical methods (plotting the data) are a great aid in
statistics because in two dimensions the human eye is great
at pattern detection. However,
in distribution modeling
where some points have drastically diﬀerent weight than oth-
ers, the plots can be quite misleading. Nevertheless, proper-
ties such as curvature can be discerned from plots and this
is important for this paper since if the observed distribution
has a curved form in log-log space, it favors scale-sensitive
models over scale-free power law distributions.

The rest of the paper is organized as follows: section 2
summarizes previous work in modeling distributions for the
web and other data sets with various discrete distributions
(Zipﬁan, Poisson, Negative Binomial) and simple mixture
models. Section 3 gives density functions for the candidate
distributions we use in this paper. Section 4 spells out our
approach for parameter estimation and model comparison,
section 5 describes the ten datasets and section 6 presents
results for the various ﬁtted distributions.

2. PREVIOUS WORK

Power-laws, Zipf distributions and Pareto distributions
have become somthing of a fad recently, being very pop-
ular in explaining all manner of data (city sizes, galaxy sizes
[10], words: [23], incomes [21]). We begin by carefully dis-
tinguishing these. The term “power law” is inappropriately
general for our application since it goes well beyond distrib-
utions to describe any functional relationship between y and
x where y = axk, hence we will not use this term. Zipf dis-
tributions are discrete distributions over a ﬁnite set (1...N )
with probability mass function f (x; s, N ) = Ω(N ) 1
xs where
Ω(N ) = 1/PN
1
is is a normalizing constant. For s > 1 this
distribution is normalizable even if N = ∞ and becomes the
Zeta distribution [19]. Some authors restrict the term “Zipf
distribution” for the case s = 1 and call other cases “Zipf-
like” or “Zipﬁan”. Zipf distributions (s = 1) such as word
distributions, have the scale-free property:

i=1

Scale-free: A distribution f is scale free if for
all values of x, the probability of 2x is half the
probability of x.
If a distribution is not scale-
free, we will term it as scale-sensitive.

Applied to word rankings this means that for all ranks,
the probability of seeing a word at that rank is twice the
probability of seeing a word with twice the rank (Zipf’s law
[23]). Pareto [21] distributions are continuous analogues of
the Zipf, with density given by f (x; k) = k 1

Previous work can be classiﬁed according to the types of
distributions that were tried and also more subtly, the mean-
ing of the x axis: in some work, the x axis refers to the value

xk+1 .

of a random variate (e.g. number of clicks) whereas in others
it refers to a rank of that variable. Table 1 summarizes pre-
vious work with respect to this classiﬁcation. Laherrere and
Sornette [10] take yet another approach, modeling rank as
a function of the random variate. The Inverse-Gaussian [9],
Log-Normal [10] and Weibull [10] distributions are continu-
ous distributions yet they are being used to model discrete
data.

The paper of Huberman et al.

[9] claims to have dis-
covered a “strong law of surﬁng”: that the distribution of
clicks is distributed as Inverse Gaussian (IG). However, we
believe this claim to be too strong. In particular, for our
datasets, we have found other distributions to oﬀer statis-
tically signiﬁcantly better ﬁt to the data than the Inverse
Gaussian. We doubt there is a single distribution that will
ﬁt all kinds of web surﬁng let alone constitute a “law of
surﬁng”. Huberman also claims the Inverse Gaussian has
theoretical motivation and argues that the utility of a web
surfer mirrors that of economic options whose prices are
known to follow an Inverse-Gaussian distribution. They also
show that the page-view distribution of URLs is ﬁtted by an
Inverse-Gaussian. Their approach seems not to have tried
a lot of distributions, let alone discrete ones - it seems only
that they have considered the Inverse Gaussian because of
its theoretical underpinning and then proceeded to see if it
gives a good enough ﬁt to the data.

Huberman’s work is the only other we are aware of that
points out the curvature of the distribution in log-log space
thus deprecating the scale-free power law distributions. Pre-
vious authors did not rule out better ﬁts by scale-sensitive
curved forms in log-log space: they only demonstrated that
they got a good-enough ﬁt by a line in log-log space and
thus concluded the distribution must be power-law.

Laherrere and Sornette justify their choice of Weibull be-
cause “tails of pdfs of products of a ﬁnite number of random
variables is generically a stretched exponential” [6]. How-
ever, it appears to us that there is a serious problem with
their methodology. They evalute goodness of ﬁt in log-log
space using Pearson’s correlation coeﬃcient which is incor-
rect since the plotted points with lower rank represent many
more points than those with higher ranks.

Finally, a note on previous work in mixture and zero-
adjusted models. User web behavior can be naturally parti-
tioned based on the presence or absence of a click, in other
words zero click vs. non-zero click behavior. The zero click
class might be thought of as primarily containing robot traf-
ﬁc, and the non-zero click class principally “human” traﬃc.
At the very least, if we see that a model ﬁts the data well
except at the zero point, we can conclude that the excess
of users with zero clicks may be due to a secondary phe-
nomenon: that of robots. This is the approach of mixture
models in which the zero class is modeled by a (single-valued
degenerate) distribution and the positive clicks are modeled
by a discrete positive-valued distribution. Such models are
called zero-altered or zero-inﬂated and have been used in
numerous applications to model data with an excess of ze-
roes or where the zero-class of the underlying process has
special meaning (number of defects [11], number of dental
cavities [14], and number of car crashes [12]). In all of these
domains, zero has a special meaning in that it is usually the
default scenario indicating lack of an accident or problem.

Best-ﬁt distribution
Zipf k = 1

Measure
Frequency
Population Weibull
Page-views Zipf-like, various k

Entity
Word
City
URL
Search Queries Frequency Weibull
URL
Session
Session
User

Page-views
Clicks
Clicks
Page-views ZAZM

X-axis
Rank
Population Rank
Page-views Frequency Breslau [3]
Rank
Page-views Frequency Huberman et al. [9]
Inverse Gaussian
Frequency Huberman et al. [9]
Inverse Gaussian
Clicks
ZAZM: Zero-altered Zipf-Mandelbrot Clicks
Frequency
Page-views Frequency

Y-axis
Frequency Zipf [23]

Frequency Abdullah [1]

Scarr, Ali (this paper)
Scarr, Ali (this paper)

Laherrere, Sornette [10]

Author

Table 1: Prior work in distribution modeling can be partitioned into those modeling the random variate x
versus those modeling the rank of x. The web papers diﬀer subtly in that some are modeling clicks per user
or clicks per session or searches per query.

3. DISTRIBUTION THEORY

Since this paper is concerned with modeling count data,
in the form of user clicks, we limit our attention to discrete
distributions; in particular the Negative Binomial [2], Zipf
[23], Zipf-Mandelbrot [13] (of which Zipf is a special case),
Logarithmic Series [22] and Yule-Simon [18]. The Poisson
distribution which best ﬁts data when its mean and variance
are equal is not used due to the fact our click data (table 2)
have variance-to-mean ratios far in excess of 1. The Inverse
Gaussian [9], Weibull [10] and Log-Normal [10] distributions,
which are continuous, are only included for comparison as
they have been used to model web click behavior in the past.
In addition a class of simple mixture models are also con-
sidered, motivated by the fact that zero-click users may be-
have diﬀerently to non-zero click users. A number of “users”
are actually robots that ping the Yahoo site every day to test
if they are connected to the internet. This is a diﬀerent gen-
erative phenomenon than that of regular human search and
hence justiﬁes using a mixture model. Zero-altered or zero-
inﬂated models [8, 11, 14] for count data are mixture models
that assume with probability p the only possible observation
is 0 and with probability 1− p a discrete random variable is
observed.

The zero-altered model mixes a degenerate distribution
with point mass of 1 at zero with a truncated count distri-
bution for example truncated Poisson, truncated Negative
Binomial or any discrete distribution bounded below by 1.
On the other hand the zero-inﬂated model accounts for some
of the zeros through the non-degenerate distribution ( e.g.
Poisson, Negative Binomial, etc...) and some through the
degenerate (zero) distribution.

We now present formulae for the probability mass and
density functions of the various distributions discussed above,
by considering a random variable X.

Zero-Altered (ZA):
The probability mass or density function for a zero-altered

mixture model is deﬁned as:

fX (x; p, θθθ) = 8<
:

0
x < 0
p
x = 0
(1 − p)fY (x; θθθ) x > 0

(1)

where 0 ≤ p ≤ 1, θθθ = (θ1, θ2, . . . , θk) is a vector of para-
meters and fY (x; θθθ) is any valid probability distribution on
[1,∞). From the above, the random variable X takes the
value zero with probability p and values greater than zero
with probabilities (1 − p)fY (x; θθθ).

Zero-Inﬂated (ZI):
The probability mass or density function for a zero-inﬂated

mixture model is deﬁned as:

fX (x; p, θθθ) = 8<
:

0
x < 0
p + (1 − p)fY (0; θθθ) x = 0
(1 − p)fY (x; θθθ)
x > 0

(2)

is any valid probability distribution on [0,∞).

where 0 ≤ p ≤ 1, θθθ is a vector of parameters and fY (x; θθθ)
Negative Binomial (NB) (discrete): X ∼ N B(k, µ),

the probability mass function is:

k

(

Γ(x + k)

Γ(k)Γ(x + 1)

)k(1− k

)x, x ≥ 0 (3)
f (x; k, µ) =
where k > 0 and µ ≥ 0. To ﬁt a ‘zero-altered” Negative
Binomial model (ZANB) the probability mass function of a
zero-truncated Negative Binomial distribution is required,
this is deﬁned as:

µ + k

µ + k

f0(x; k, µ) =

f (x; k, µ)
1 − f (0; k, µ)

, x > 0

(4)

From equations (1, 3, 4) the Zero-Altered Negative Bino-

mial ZAN B(p, k, µ) probability mass function is:

f (x; p, k, µ) = 8><
>:

0
p
(1 − p)

µ+k )k(1− k
Γ(x+k)( k
Γ(k)Γ(x+1)(1−( k

µ+k )x
µ+k )k)

x < 0
x = 0

x > 0

(5)
Zipf (Z) (discrete): X ∼ Z(s, N ), the probability

mass function is:

f (x; s, N ) = Ω(N )

1

xs , x ∈ (0, N ]

(6)

normalising constant.

where s > 0, N is ﬁnite and Ω(N ) = 1/PN
1
is is a
Zipf-Mandelbrot (ZM) (discrete): X ∼ ZM (s, q, N ),

i=1

the probability mass function is:

1

f (x; s, q, N ) = Ω(q, N )

(x + q)s , x ∈ [0, N ]
where s > 0, q ≥ 0, N is ﬁnite and Ω(q, N ) = 1/PN

is a normalising constant. Clearly, setting q = 0 yields the
Zipf distribution in equation (6) so non-zero q indicates cur-
vature in log-log space.
Logarithmic-Series (LS) (continuous): X ∼ LS(k),

(7)

1

(i+q)s

i=1

the probability mass function is:

f (x; k) =

(8)
Yule-Simon (YS) (discrete): X ∼ Y S(k), the prob-

log(1 − k)

, x > 0, k ∈ (0, 1)

−1

kx
x

ability mass function is:

f (x; ρ) = ρB(x, ρ + 1), x > 0, ρ > 0

(9)

where B(, ) is the Beta function.
Inverse-Gaussian (IG) (continuous): X ∼ IG(λ, µ),

the probability density function is:

f (x; λ, µ) = r λ

2πx3 exp(

−λ(x − µ)2

2µ2x

), x ≥ 0

(10)

with , λ, µ > 0.
Log-Normal (LN) (continuous): X ∼ LN (µ, σ), the

probability density function is:

xσ

2σ2

√
1
2π

exp− (log(x) − µ)2

 , x ∈ R (11)
f (x; µ, σ) =
with µ ∈ R and σ > 0. The Log-Normal distribution is
the probability distribution of any random variable whose
logarithm possesses a Normal or Gaussian distribution. In
other words if Z ∼ N (µ, σ2) is a Normally distributed ran-
dom variable then X = eZ has a Log-Normal distribution.
Weibull (W) (continuous): X ∼ W (α, β), the prob-

ability density function is:

f (x; α, β) =

α
β

(

x
β

)α−1 exp(−(

x
β

)α), x ≥ 0, α, β > 0 (12)

Zero-altered or zero-inﬂated models can be constructed
using equations (1, 2) and any of the probability mass or
density functions above. Since we are particularly interested
in the zero class, zero-altered models only are used for those
distributions where x > 0, such as Zipf, Logarithmic series
etc...

4. METHODOLOGY

We now describe the method used to ﬁt the distributions
proposed in section 3 to our observed data. In addition we
also discuss how to compare diﬀerent ﬁtted models. Maxi-
mum likelihood estimation e.g. [16, 5], a standard statistical
modeling technique, is used to ﬁt the models. It possesses a
number of desirable properties and is a widely used parame-
ter estimation tool. Once various models have been ﬁtted
they can be compared using the Bayesian Information Cri-
terion (BIC) [17] and graphical methods such as Cumulative
Distribution plots.
4.1 Maximum Likelihood Estimation

Suppose we have an independant and identically distrib-
uted (i.i.d.) sample XXX = (X1, X2, . . . , Xn) with joint prob-
ability density or mass function f (xxx|θθθ), where the data xxx =
(x1, x2, . . . , xn) and the parameter vector θθθ = (θ1, θ2, . . . , θk).
Given observed values Xi = xi, i = 1, . . . , n the likelihood
is:

L(θθθ|xxx) =

f (xi|θθθ)

n

Yi=1

(13)

In the case of discrete data the likelihood measures the
probability of observing the given data as a function of θθθ.
The maximum likelihood estimate (MLE) ˆθˆθˆθ, is the value of θθθ
that maximises the likelihood i.e. makes the observed data
“most likely”. In practice it is usually easier to equivalently
maximize the log-likelihood:

l(θθθ|xxx) =

n

Xi=1

log{f (xi|θθθ)}

(14)

As an example, consider an i.i.d. sample from a Poisson

distribution with parameter λ, then:

f (xi|λ) =

λxi e−λ

xi!

∀i = 1, . . . , n

From equation (13) the likelihood is:

L(λ|xxx) =

λxi e−λ

xi!

n

Yi=1

(15)

(16)

Since it is easier to work with the log-likelihood, from equa-

tion (14) we have:

l(λ|xxx) = log(λ)

n

Xi=1

xi − nλ − n
Xi=1

log(xi!)

(17)

Diﬀerentiating the log-likelihood in equation (17) with re-

spect to λ and setting to zero gives:

0

l

(λ) =

1
λ

n

Xi=1

xi − n = 0

(18)

Expressing equation (18) in terms of λ gives the familiar

MLE ˆλ = x.

In the above example, we maximized the log-likelihood
with respect to a single parameter λ. More generally, the
model may contain several parameters, in which case we
compute partial derivatives and set each in turn to zero. De-
pending on the particular distribution used in the likelihood,
a closed form solution may or may not exist. For the Pois-
son distribution the MLE of λ has an algebraic solution that
is the sample mean of the data. In cases where there is no
closed form solution an iterative method is employed using a
modiﬁcation of the Broyden, Fletcher, Goldfarb and Shanno
quasi-Newton algorithm [4] within the statistical software
package R [15].

Although not discussed in detail here it is also possible
to compute variances based on the Fisher Information [16]
and hence conﬁdence intervals for the maximum likelihood
estimates described above.
4.2 Comparing Models

The log-likelihood in equation (14) can be computed for
the various parametric models of interest and used as a ba-
sis for model comparison. This makes sense as the model
that has the largest log-likelihood is considered to be the
most “likely” given the observed data. However since we
are investigating models with diﬀering numbers of parame-
ters, rather than comparing log-likelihoods directly we use
the Bayesian Information Criterion (BIC) [17]. The BIC
penalizes models with more parameters so that to “win” in

Property
X-axis
Number records
Mean
Median
Variance
Variance to Mean Ratio
Minimum #Clicks
Maximum #Clicks

Session-level data User-level data
Clicks/session
1,411,298
2.78
1
22.94798
8.24
0
499

Clicks/user
22,913,854
2.29
0
75.84359
34.03
0
5,428

Table 2: Summary of our data sets.

a BIC-sense, the extra parameter needs to justify its addi-
tion with a commensurate increase in log-likelihood.

The Bayesian Information Criterion (BIC) [17] is deﬁned

as:

BIC = −2l(θθθ|xxx) + k log(n)

(19)
where l(θθθ|xxx) is the maximized log-likelihood, n is the num-
ber of observations and k = |θθθ| is the number of model pa-
rameters. We wish to minimize the BIC with respect to the
estimated model parameters. As can be seen from equation
(19) the BIC attaches a penalty to the addition of extra pa-
rameters, forcing it to prefer lower order models especially
for large n.
4.3 Graphical Methods

In the related work section we argued why linear ﬁtting
in log-log space where one axis denotes frequency is poor
methodology. In this section we describe the beneﬁts and
pitfalls of the Cumulative-Probability (Cum-Prob) plot.

Cum-Prob plots:

A Cumulative-Probability (cum-
prob) plot is related to a QQ plot. To compute the x coordi-
nate of the i’th point, the area under the histogram ( e.g. of
clicks) up to and including the i’th distinct value is divided
by the full area under the histogram. In other words, x val-
ues are empirical CDF (Cumulative Distribution Function)
values. The y values are obtained by ﬁrst doing a MLE best-
ﬁt but this time referring to the theoretical CDF. Let ci be
the i’th distinct click value over the set C of all possible click
values, then CDF is a function taking random click values
to the zero-one set: CDF : C → [0,1]. Thus both x and y
values are normed to the [0,1] interval.

5. DESCRIPTION OF DATASETS

We show results for our two primary datasets: their basic
parameters are in table 2. The ﬁrst dataset is for our main
web search engine: a random sample of sessions collected
from a week’s worth of data. A session is terminated by
the standard 30 minutes of inactivity. The second dataset
is a random sample of per user clicks, integrated over one
month of activity for that user on another Yahoo website.
Thus dataset 2 consists of a mixture of sessions.

Figures 1 and 2 show histograms of the data. We have
zoomed in to the top 10 click values since the rest of the
histogram has very low values. Note that for the per-session
web search dataset, one-click occurs more frequently than
any other value and in particular, more than zero clicks. A
zero-click session is possible since a session can have pageviews
but zero clicks. If one were to use an exponential discrete
model, one might expect the zero probability to be higher

Figure 1: Sessionized data: frequency of web-search
clicks per-session (top 10 values).

Figure 2: User-level data: frequency of Yahoo web-
site clicks per-user (top 10 values).

than the probability for one click. Thus the data indicate
immediately the need to try for a mixture model with a
special component at zero - so-called Zero-Adjusted mix-
ture models. For the per-user month-level dataset (ﬁgure
2), zero clicks are more common than one click. Note that
the user-level data is from another website (not web search)
so it is not as if multiple sessions from ﬁgure 1 are being
included in ﬁgure 2.

Ten other datasets from various other Yahoo verticals1
were also analyzed to see if our results are generalizable. In
order to get verticals that span the gamut of user behavior

1Verticals are websites such as mail.yahoo, travel.yahoo etc.

012345678910Histogram of observed data, top 10 groups0   e+002   e+054   e+05012345678910Histogram of observed data, top 10 groups0.0 e+004.0 e+068.0 e+061.2 e+07we sorted all the websites at Yahoo by volume of traﬃc and
then randomly picked a website in each decile of the sorted
list.

6. RESULTS

We begin by examining both datasets in log-log space (ﬁg-
ures 3 and 4). Log-log space brings out diﬀerences in the
various exponential distribution families that are just not
apparent in usual histograms because all such distributions
have long tails. The other advantage of log-log space is
that curvature can be easily visually spotted and this usu-
ally indicates the data is not from a scale-free distribution
(scale-free has linear form in log-log space).

Figure 4: Per-user clicks (integrated over 1 month)
for another large Yahoo website exhibit curvature
in log-log space.

The main three results of our paper are supported by these

results:

• Curvature:

In log-log space our data is curved as
indicated by the fact that the best ﬁtting distribution,
Zipf-Mandelbrot, by theory has a curved form in log-
log space. This is also visually conﬁrmed. Another
way to quantify the amount of curvature is to examine
the conﬁdence interval around the MLE (maximum
likelihood estimate) of the q parameter in the Zipf-
Mandelbrot distribution. For web search, we obtained
q = 2.0±0.01 and for the per-user dataset we obtained
q = 0.9 ± 0.002. Thus in both cases, q = 0 (which
would indicate no curvature) is emphatically excluded.
Curved forms in log-log space do not have the scale-
free property of pure Zipﬁan (power-law) distributions
- they have a natural, distinguished scale. ( e.g. [10]).
• ZAZM: The particular model form with best BIC ﬁt
is the ZAZM (Zero-Adjusted Zipf-Mandelbrot) model
for both datasets.

• Zero-Altered Mixture: Zero-altered mixture mod-
els do better than their non-mixture counterparts, ir-
respective of the distribution type. This is consistent
with our hypothesis that a diﬀerent generative compo-
nent (robots) are major contributors to the zero-click
observations.

6.1 Results on ten other websites

Next, we examine the ten other datasets (table 3). First
note the (Log-Likelihood) LL column: this shows that the
best ﬁt for each website was obtained by the Zero-Altered
Zipf-Mandelbrot mixture - not any other mixture. Second,

Figure 3: Per-session clicks for search engine results
exhibit curvature in log-log space. Observations are
dark asterisks; ﬁtted models are curves or lines.

The ﬁgures show the distribution of data, a non-mixture
Zipf-Mandelbrot, a mixture Zipf-Mandelbrot and a mixture
Zipf - we cannot ﬁt a non-mixture Zipf since it is undeﬁned
for x = 0. The ﬁgures also show a “linear ﬁt” regression line.
This line is obtained by doing linear regression on the plot-
ted points. Note how it provides a poor ﬁt for points near
log(clicks) = 0 which are just the points that represent mil-
lions of users (or sessions). This is the fundamental problem
with doing line-ﬁtting of the plotted points: all points are
equally treated whereas in reality some points correspond to
millions of users and others may correspond to a single user.
Although visual examination of the data seems to indi-
cate curvature, a more rigorous conﬁrmation would be to
ﬁt models assessed by BIC values (ﬁgures 5 and 6).
If
the winning model has, by theory, a curved form in log-
log space, then it would conﬁrm the visual observation. The
ﬁgures show the smallest (best ﬁtting) BIC values consis-
tently come from the zero-altered mixture models, with zero-
altered Zipf-Mandelbrot (ZAZM) being the best ﬁt for our
data.

**********************************************************0123402468Log−log plot of observed dataZAZ(red), ZM(blue), ZAZM(green)log( #clicks )log(freq)**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************02468051015Log−log plot of observed data with ZAZ, ZM, ZAZM and Linear fitted modelslog( #clicks )log(freq)(ZAZ)(ZM)(ZAZM)Linear FitDecile Best parsimonious Best ﬁt

1
2
3
4
6
7
9

ﬁt (BIC)
ZAZM
ZAZ
ZAZM
ZAZ
ZAZ
ZAZ
ZAZ

Sample
size

(LL)
ZAZM 22,913,854
ZAZM 239,520
ZAZM 160,137
ZAZM 21,033
ZAZM 6,461
ZAZM 6,724
ZAZM 294

Table 3: Best ﬁtting models for Yahoo websites cho-
sen to span all Yahoo websites in terms of user vol-
ume. Even though deciles 5, 8 and 10 had plenty of
data, they had too few distinct values for number of
clicks to allow ﬁtting of parameterized models.

is an adjusted measure of ﬁt) one would never lose (ﬁtting-
wise) by using ZAZM (this is clear when one recalls that the
ZAZ model is just a special case of the ZAZM model).
6.2 Graphical Results

Having established that, at least for our datasets, the Zipf-
Mandelbrot (and its Zero-Altered counterpart) oﬀer a very
good ﬁt2, we now examine supporting graphical methods:
the Quantile-Quantile (QQ) plot and Cumulative-Probability
(Cum-Prob) plot.

Figure 7 shows the cum-prob plot for the winning ZAZM
model on the web per-session data. The cumulative distrib-
ution plots are less vulnerable to outliers because they plot
areas rather than values of the random variates themselves.
An outlier has a large value but typically does not constitute
a large percentage of the overall mass (clicks summed across
all users). Hence its eﬀect is reduced in the cum-prob plot.
We can re-examine graphically our comparison between
the best BIC scoring distribution (ZAZM - Figure 7) and
one that was not competitive (Inverse Gaussian - Figure 8).
One can clearly see the ZAZM provides a good ﬁt between
theory and observation whereas the Inverse Gaussian touted
by [9] as providing a strong law of surﬁng does not hold for
Yahoo web-search data.

Figures 9 through 14 provide graphical conﬁrmation of the
LL/BIC results. ZAZM is the best ﬁt with ZAZ being the
runner-up and the other doing much less well. The inappro-
priate continuous distributions (Inverse Gaussian, Weibull
and Log-Normal) are included here only because they have
been used in prior literature [9, 10, 1], which did not in-
vestigate Zipf-Mandelbrot or mixtures. Negative-Binomial
(NB) is included because clicks are discrete counts and NB
is often used for over-dispersed (variance-to-mean ratio > 1)
data. Log-Normal is shown because it is the ﬁrst distribu-
tion people think of when they think of skewed data (and
it also has a curved form in log-log space). However, it is
not discrete and it did not perform nearly as well as the
Zipf-Mandelbrot.

2Note we can never say for sure that the data originate from
a particular form of distribution, only that of the distribu-
tions we have tried, such and such a distribution provides
the best ﬁt.

Figure 5: Sessionized web data BIC ﬁt values (low
values are better): Zero-alteration makes a big dif-
ference and within that class, the Zipf-Mandelbrot
is the best.

Figure 6: User-level BIC values (lower is better):
Zero-altered models (ZA..) do better than zero-
inﬂated (ZI..) or unadjusted (e.g. ZM). Within the
zero-adjusted set, Zipf-Mandelbrot does the best.

note that for many of these minor websites, BIC indicates
the addition of a parameter in going from Zipf to Zipf-
Mandelbrot is not “worth it”. In other words, some of these
websites do have a linear form in log-log space. However,
even in these cases, the best ﬁt per se (measured by LL) is
provided by ZAZM. And since LL measures pure ﬁt (BIC

Bayesian Information Criteria (BIC) for different models(BIC)Modelzawbnbzizmzmzalnzaigzazzayszanbzalszazm5800000600000062000006400000Bayesian Information Criteria (BIC) for different models(BIC)Modelzawbzalnnbzaigzanbzalszazzizmzmzayszazm7.6 e+077.8 e+078.0 e+078.2 e+07Figure 7: Web session data: Cumulative distri-
bution plot. Winning model: Zero-Altered Zipf-
Mandelbrot.

Figure 9: Second dataset: Cumulative distribu-
tion plot. Winning model: Zero-Altered Zipf-
Mandelbrot.

Figure 8: Web session data: Cumulative distribu-
tion plot. Losing model from strong law of surﬁng:
Zero-Altered Inverse Gaussian.

Figure 10: Second dataset: Drop the Mandelbrot
correction (Zero-Altered Zipf ).

************************************************************************************************************************************************************************************0.20.40.60.81.00.20.40.60.81.0Observed vs. expected cumulative probs ZAZM p, (q,s) independent(the line y=x included for comparison)ObservedExpected************************************************************************************************************************************************************************************0.20.40.60.81.00.20.40.60.81.0Observed vs. expected cumulative probs ZAIG (the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAZM p, (q,s) independent(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAZ(the line y=x included for comparison)ObservedExpectedFigure 11:
inappropriate
Gaussian.
man’s strong law of surﬁng.

Second dataset: A losing model:
continuous Zero-Altered Inverse
Inverse Gaussian was used in Huber-

Figure 13: Second dataset: A losing continuous
model: Zero-Altered Weibull. Weibull was used by
Laherrere et al..

Figure 12: Second dataset: A losing discrete model
with over-dispersion (Zero-Altered Negative Bino-
mial). Negative-Binomial is a reasonable guess since
it is discrete and has long tail.

Figure 14: Second dataset: Another losing contin-
uous model: Zero-Altered Log-normal. Log-normal
is one of the simplest models that has curved form
in log-log space but it is continuous.

***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAIG (the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZANB p, m independent(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.9Observed vs. expected cumulative probs ZAWB(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.9Observed vs. expected cumulative probs ZALN(the line y=x included for comparison)ObservedExpected7. CONCLUSIONS

Prevailing wisdom is that the distribution of web clicks
and pageviews follows a scale-free power law distribution.
However, we have found that a statistically signiﬁcantly
better description of the data is the scale-sensitive Zipf-
Mandelbrot distribution and that mixtures thereof further
enhances the ﬁt. Previous analyses have three disadvan-
tages: they have used a small set of candidate distributions,
analyzed out-of-date user web behavior (circa 1998) and
used questionable statistical methodologies. Although we
cannot preclude that a better ﬁtting distribution may not
one day be found, we can say for sure that the scale-sensitive
Zipf-Mandelbrot distribution provides a statistically signiﬁ-
cantly stronger ﬁt to the data than the scale-free power-law
or Zipf on a variety of verticals from the Yahoo domain.
The distribution has a deﬁnite curved form in log-log space
which in turn indicates it is not scale free.

Secondly, we have shown that better results are obtain-
able using a mixture model which treats the zero-class as
special. This is warranted because the generative process
of zero clicks might contain a signiﬁcant proportion of ro-
bot “users” and thus would be diﬀerent than the generative
process for non-zero clicks (containing mostly human users).
Since we have compared zero-adjusted mixture models to
non-mixture models we have taken care to use the BIC log-
likelihood scoring method since it makes some adjustments
for varying complexity of the models.

Finally, we have argued that the practice of ﬁtting plotted
points in log-log space is incorrect methodology and is sen-
sitive to outliers. We instead propose using the Cumulative-
Probability plots which plot empirical cumulative distribu-
tions against theoretical cumulative distributions. We plan
to use the thresholds resulting from these methods to set
probabilistically founded threshold levels for removing out-
liers and robots and thus to enjoy more stable and accurate
metrics.

8. REFERENCES
[1] G. Abdulla. Analysis and Modeling of World Wide

Web Traﬃc. PhD thesis, Virginia Tech, 1998.

[2] A. Agresti. Categorical Data Analysis. Wiley Series in

Probability and Statistics, 2002.

[3] L. Breslau, P. Cao, L. Fan, G. Phillips, and

S. Shenker. Web caching and zipf-like distributions:
Evidence and implications. In INFOCOM (1), pages
126–134, 1999.

[4] R. H. Byrd, J. Nocedal, and C. Zhu. A limited

memory algorithm for bound constrained
optimization. Journal of Scientiﬁc Computing
(SIAM), 16:1190–1208, 1995.

[5] G. Casella and R. L. Berger. Statistical Inference.

Duxbury Press, 1990.

[6] U. Frisch and D. Sornette. Extreme deviation and

applications. J. Phys. I France 7, 7:1155–1171, 1997.

[7] S. Glassman. A caching relay for the World Wide

Web. Computer Networks and ISDN Systems,
27(2):165–173, 1994.

[8] D. C. Heilbron. Zero-altered and other regression

models for count data with added zeroes. Biometrics,
36:531–547, 1994.

[9] B. A. Huberman, P. L. T. Pirolli, J. E. Pitkow, and
R. M. Lukose. Strong regularities in world wide web
surﬁng. Science, 280:95–97, 1998.

[10] J. Laherrere and D. Sornette. Stretched exponential

distributions in nature and economy: “fat tails” with
characteristic scales. The European Physical Journal
B, 2:525, 1998.

[11] D. Lambert. Zero-inﬂated poisson regression, with an

application to defects in manufacturing.
Technometrics, 34:1–14, 1992.

[12] D. Lord, S. P. Washington, and J. N. Ivan. Poisson,

poisson-gamma and zero-inﬂated regression models of
motor vehicle crashes: balancing statistical ﬁt and
theory. Accident Analysis and Prevention, 37:35–46,
2005.

[13] B. Mandelbrot. An informational theory of the

statistical structure of language. In W. Jackson,
editor, Communication Theory. Betterworths, 1953.

[14] S. M. Mwalili, E. Lesaﬀre, and D. Declerck. The

zero-inﬂated negative binomial regression model with
correction for misclassiﬁcation: An example in caries
research. Technical Report TR0462, IAP Statistics
Network, 2005.

[15] R Development Core Team. R: A language and

environment for statistical computing. R Foundation
for Statistical Computing, Vienna, Austria, 2004.
ISBN 3-900051-00-3.

[16] J. A. Rice. Mathematical Statistics and Data Analysis.

Wadsworth & Brooks/Cole, 1988.

[17] G. Schwarz. Estimating the dimension of a model. The

Annals of Statistics, 6:461–464, 1978.

[18] H. A. Simon. On a class of skew distribution

functions. Biometrika, 42:425–440, 1955.

[19] E. C. Titchmarsh. The Theory of the Riemann Zeta

Function, 2nd ed. Oxford Science Publications,
Clarendon Press, Oxford, 1986.

[20] D. G. Uitenbroek. SISA Pairwise tests.

http://home.clara.net/ sisa/ pairwhlp.htm, 1997.

[21] D. von Seggern. CRC Standard Curves and Surfaces.

CRC Press, 1993.

[22] J. R. Wilson. Logarithmic series distribution and its
use in analyzing discrete data. In Proceedings of the
Survey Research Methods Section, American
Statistical Association, pages 275–280, 1988.

[23] G. K. Zipf. Human Behaviour and the Principle of

Least-Eﬀort. Addison-Wesley, Cambridge, MA, 1949.

