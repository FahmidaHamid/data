Consistency-preserving Caching of Dynamic

Database Content∗

Niraj Tolia and M. Satyanarayanan

Carnegie Mellon University

{ntolia,satya}@cs.cmu.edu

ABSTRACT
With the growing use of dynamic web content generated from re-
lational databases, traditional caching solutions for throughput and
latency improvements are ineffective. We describe a middleware
layer called Ganesh that reduces the volume of data transmitted
without semantic interpretation of queries or results. It achieves
this reduction through the use of cryptographic hashing to detect
similarities with previous results. These beneﬁts do not require
any compromise of the strict consistency semantics provided by the
back-end database. Further, Ganesh does not require modiﬁcations
to applications, web servers, or database servers, and works with
closed-source applications and databases. Using two benchmarks
representative of dynamic web sites, measurements of our proto-
type show that it can increase end-to-end throughput by as much
as twofold for non-data intensive applications and by as much as
tenfold for data intensive ones.

Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed Sys-
tems; H.2.4 [Database Management]: Systems

General Terms
Design, Performance

Keywords
content addressable storage, relational database systems, database
caching, wide area networks, bandwidth optimization

1.

INTRODUCTION

An increasing fraction of web content is dynamically generated
from back-end relational databases. Even when database content
remains unchanged, temporal locality of access cannot be exploited
because dynamic content is not cacheable by web browsers or by
intermediate caching servers such as Akamai mirrors. In a multi-
tiered architecture, each web request can stress the WAN link be-
tween the web server and the database. This causes user expe-
rience to be highly variable because there is no caching to insu-
∗This research was supported by the National Science Foundation
(NSF) under grant number CCR-0205266. Any opinions, ﬁnd-
ings, conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views of
the NSF or Carnegie Mellon University.
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

late the client from bursty loads. Previous attempts in caching dy-
namic database content have generally weakened transactional se-
mantics [3, 4] or required application modiﬁcations [15, 34].

We report on a new solution that takes the form of a database-
agnostic middleware layer called Ganesh. Ganesh makes no effort
to semantically interpret the contents of queries or their results. In-
stead, it relies exclusively on cryptographic hashing to detect sim-
ilarities with previous results. Hash-based similarity detection has
seen increasing use in distributed ﬁle systems [26, 36, 37] for im-
proving performance on low-bandwidth networks. However, these
techniques have not been used for relational databases. Unlike
previous approaches that use generic methods to detect similarity,
Ganesh exploits the structure of relational database results to yield
superior performance improvement.

One faces at least three challenges in applying hash-based simi-
larity detection to back-end databases. First, previous work in this
space has traditionally viewed storage content as uninterpreted bags
of bits with no internal structure. This allows hash-based tech-
niques to operate on long, contiguous runs of data for maximum
effectiveness.
In contrast, relational databases have rich internal
structure that may not be as amenable to hash-based similarity de-
tection. Second, relational databases have very tight integrity and
consistency constraints that must not be compromised by the use
of hash-based techniques. Third, the source code of commercial
databases is typically not available. This is in contrast to previous
work which presumed availability of source code.

Our experiments show that Ganesh, while conceptually simple,
can improve performance signiﬁcantly at bandwidths representa-
tive of today’s commercial Internet. On benchmarks modeling multi-
tiered web applications, the throughput improvement was as high
as tenfold for data-intensive workloads. For workloads that were
not data-intensive, throughput improvements of up to twofold were
observed. Even when bandwidth was not a constraint, Ganesh had
low overhead and did not hurt performance. Our experiments also
conﬁrm that exploiting the structure present in database results is
crucial to this performance improvement.

2. BACKGROUND
2.1 Dynamic Content Generation

As the World Wide Web has grown, many web sites have decen-
tralized their data and functionality by pushing them to the edges
of the Internet. Today, eBusiness systems often use a three-tiered
architecture consisting of a front-end web server, an application
server, and a back-end database server. Figure 1 illustrates this
architecture. The ﬁrst two tiers can be replicated close to a con-
centration of clients at the edge of the Internet. This improves user
experience by lowering end-to-end latency and reducing exposure

WWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content3113. DESIGN AND IMPLEMENTATION

Ganesh exploits redundancy in the result stream to avoid trans-
mitting result fragments that are already present at the query site.
Redundancy can arise naturally in many different ways. For exam-
ple, a query repeated after a certain interval may return a different
result because of updates to the database; however, there may be
signiﬁcant commonality between the two results. As another ex-
ample, a user who is reﬁning a search may generate a sequence
of queries with overlapping results. When Ganesh detects redun-
dancy, it suppresses transmission of the corresponding result frag-
ments. Instead, it transmits a much smaller digest of those frag-
ments and lets the query site reconstruct the result through hash
lookup in a cache of previous results. In effect, Ganesh uses com-
putation at the edges to reduce Internet communication.

Our description of Ganesh focuses on four aspects. We ﬁrst ex-
plain our approach to detecting similarity in query results. Next,
we discuss how the Ganesh architecture is completely invisible to
all components of a multi-tier system. We then describe Ganesh’s
proxy-based approach and the dataﬂow for detecting similarity.
3.1 Detecting Similarity

One of the key design decisions in Ganesh is how similarity is
detected. There are many potential ways to decompose a result into
fragments. The optimal way is, of course, the one that results in the
smallest possible object for transmission for a given query’s results.
Finding this optimal decomposition is a difﬁcult problem because
of the large space of possibilities and because the optimal choice
depends on many factors such as the contents of the query’s result,
the history of recent results, and the cache management algorithm.
When an object is opaque, the use of Rabin ﬁngerprints [8, 30]
to detect common data between two objects has been successfully
shown in the past by systems such as LBFS [26] and CASPER [37].
Rabin ﬁngerprinting uses a sliding window over the data to com-
pute a rolling hash. Assuming that the hash function is uniformly
distributed, a chunk boundary is deﬁned whenever the lower order
bits of the hash value equal some predetermined value. The num-
ber of lower order bits used deﬁnes the average chunk size. These
sub-divided chunks of the object become the unit of comparison for
detecting similarity between different objects.

As the locations of boundaries found by using Rabin ﬁngerprints
is stochastically determined, they usually fail to align with any
structural properties of the underlying data. The algorithm there-
fore deals well with in-place updates, insertions and deletions. How-
ever, it performs poorly in the presence of any reordering of data.
Figure 2 shows an example where two results, A and B, consist-
ing of three rows, have the same data but have different sort at-
tributes. In the extreme case, Rabin ﬁngerprinting might be unable
to ﬁnd any similar data due to the way it detects chunk boundaries.
Fortunately, Ganesh can use domain speciﬁc knowledge for more
precise boundary detection. The information we exploit is that a
query’s result reﬂects the structure of a relational database where
all data is organized as tables and rows. It is therefore simple to
check for similarity with previous results at two granularities: ﬁrst
the entire result, and then individual rows. The end of a row in a re-
sult serves as a natural chunk boundary. It is important to note that
using the tabular structure in results only involves shallow interpre-
tation of the data. Ganesh does not perform any deeper semantic
interpretation such as understanding data types, result schema, or
integrity constraints.

Tuning Rabin ﬁngerprinting for a workload can also be difﬁcult.
If the average chunk size is too large, chunks can span multiple
result rows. However, selecting a smaller average chunk size in-
creases the amount of metadata required to the describe the results.

Figure 1: Multi-Tier Architecture

to backbone trafﬁc congestion. It can also increase the availability
and scalability of web services.

Content that is generated dynamically from the back-end database
cannot be cached in the ﬁrst two tiers. While databases can be eas-
ily replicated in a LAN, this is infeasible in a WAN because of
the difﬁcult task of simultaneously providing strong consistency,
availability, and tolerance to network partitions [7]. As a result,
databases tend to be centralized to meet the strong consistency re-
quirements of many eBusiness applications such as banking, ﬁ-
nance, and online retailing [38]. Thus, the back-end database is
usually located far from many sets of ﬁrst and second-tier nodes [2].
In the absence of both caching and replication, WAN bandwidth
can easily become a limiting factor in the performance and scala-
bility of data-intensive applications.

2.2 Hash-Based Systems

Ganesh’s focus is on efﬁcient transmission of results by discover-
ing similarities with the results of previous queries. As SQL queries
can generate large results, hash-based techniques lend themselves
well to the problem of efﬁciently transferring these large results
across bandwidth constrained links.

The use of hash-based techniques to reduce the volume of data
transmitted has emerged as a common theme of many recent stor-
age systems, as discussed in Section 8.2. These techniques rely
on some basic assumptions. Cryptographic hash functions are as-
sumed to be collision-resistant. In other words, it is computation-
ally intractable to ﬁnd two inputs that hash to the same output. The
functions are also assumed to be one-way; that is, ﬁnding an in-
put that results in a speciﬁc output is computationally infeasible.
Menezes et al. [23] provide more details about these assumptions.
The above assumptions allow hash-based systems to assume that
collisions do not occur. Hence, they are able to treat the hash of a
data item as its unique identiﬁer. A collection of data items effec-
tively becomes content-addressable, allowing a small hash to serve
as a codeword for a much larger data item in permanent storage or
network transmission.

The assumption that collisions are so rare as to be effectively
non-existent has recently come under ﬁre [17]. However, as ex-
plained by Black [5], we believe that these issues do not form a
concern for Ganesh. All communication is between trusted parts
of the system and an adversary has no way to force Ganesh to ac-
cept invalid data. Further, Ganesh does not depend critically on any
speciﬁc hash function. While we currently use SHA-1, replacing it
with a different hash function would be simple. There would be
no impact on performance as stronger hash functions (e.g. SHA-
256) only add a few extra bytes and the generated hashes are still
orders of magnitude smaller than the data items they represent. No
re-hashing of permanent storage is required since Ganesh only uses
hashing on volatile data.

Back-End Database ServerFront-End Web andApplication ServersWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content312Figure 2: Rabin Fingerprinting vs. Ganesh’s Chunking

This, in turn, would decrease the savings obtained via its use. Ra-
bin ﬁngerprinting also needs two computationally-expensive passes
over the data: once to determine chunk boundaries and one again to
generate cryptographic hashes for the chunks. Ganesh only needs
a single pass for hash generation as the chunk boundaries are pro-
vided by the data’s natural structure.

The performance comparison in Section 6 shows that Ganesh’s
row-based algorithm outperforms Rabin ﬁngerprinting. Given that
previous work has already shown that Rabin ﬁngerprinting per-
forms better than gzip [26], we do not compare Ganesh to com-
pression algorithms in this paper.
3.2 Transparency

The key factor inﬂuencing our design was the need for Ganesh
to be completely transparent to all components of a typical eBusi-
ness system: web servers, application servers, and database servers.
Without this, Ganesh stands little chance of having a signiﬁcant
real-world impact. Requiring modiﬁcations to any of the above
components would raise the barrier for entry of Ganesh into an ex-
isting system, and thus reduce its chances of adoption. Preserving
transparency is simpliﬁed by the fact that Ganesh is purely a perfor-
mance enhancement, not a functionality or usability enhancement.
We chose agent interposition as the architectural approach to re-
alizing our goal. This approach relies on the existence of a compact
programming interface that is already widely used by target soft-
ware. It also relies on a mechanism to easily add new code without
disrupting existing module structure.

These conditions are easily met in our context because of the
popularity of Java as the programming language for eBusiness sys-
tems. The Java Database Connectivity (JDBC) API [32] allows
Java applications to access a wide variety of databases and even
other tabular data repositories such as ﬂat ﬁles. Access to these
data sources is provided by JDBC drivers that translate between
the JDBC API and the database communication mechanism. Fig-
ure 3(a) shows how JDBC is typically used in an application.

As the JDBC interface is standardized, one can substitute one
JDBC driver for another without application modiﬁcations. The
JDBC driver thus becomes the natural module to exploit for code
interposition. As shown in Figure 3(b), the native JDBC driver is
replaced with a Ganesh JDBC driver that presents the same stan-
dardized interface. The Ganesh driver maintains an in-memory
cache of result fragments from previous queries and performs re-
assembly of results. At the database, we add a new process called
the Ganesh proxy. This proxy, which can be shared by multiple
front-end nodes, consists of two parts: code to detect similarity
in result fragments and the original native JDBC driver that com-
municates with the database. The use of a proxy at the database
makes Ganesh database-agnostic and simpliﬁes prototyping and
experimentation. Ganesh is thus able to work with a wide range
of databases and applications, requiring no modiﬁcations to either.
3.3 Proxy-Based Caching

The native JDBC driver shown in Figure 3(a) is a lightweight
code component supplied by the database vendor. Its main func-

(a) Native Architecture

(b) Ganesh’s Interposition-based Architecture
Figure 3: Native vs. Ganesh Architecture

tion is to mediate communication between the application and the
remote database. It forwards queries, buffers entire results, and re-
sponds to application requests to view parts of results.

The Ganesh JDBC driver shown in Figure 3(b) presents the ap-
plication with an interface identical to that provided by the native
driver. It provides the ability to reconstruct results from compact
hash-based descriptions sent by the proxy. To perform this re-
construction, the driver maintains an in-memory cache of recently-
received results. This cache is only used as a source of result frag-
ments in reconstructing results. No attempt is made by the Ganesh
driver or proxy to track database updates. The lack of cache con-
sistency does not hurt correctness as a description of the results is
always fetched from the proxy — at worst, there will be no perfor-
mance beneﬁt from using Ganesh. Stale data will simply be paged
out of the cache over time.

The Ganesh proxy accesses the database via the native JDBC
driver, which remains unchanged between Figures 3(a) and (b).
The database is thus completely unaware of the existence of the
proxy. The proxy does not examine any queries received from
the Ganesh driver but passes them to the native driver.
Instead,
the proxy is responsible for inspecting database output received
from the native driver, detecting similar results, and generating
hash-based encodings of these results whenever enough similarity
is found. While this architecture does not decrease the load on a
database, as mentioned earlier in Section 2.1, it is much easier to
replicate databases for scalability in a LAN than in a WAN.

To generate a hash-based encoding, the proxy must be aware of
what result fragments are available in the Ganesh driver’s cache.
One approach is to be optimistic, and to assume that all result frag-
ments are available. This will result in the smallest possible initial
transmission of a result. However, in cases where there is little
overlap with previous results, the Ganesh driver will have to make
many calls to the proxy during reconstruction to fetch missing re-
sult fragments. To avoid this situation, the proxy loosely tracks the
state of the Ganesh driver’s cache. Since both components are un-
der our control, it is relatively simple to do this without resorting
to gray-box techniques or explicit communication for maintaining
cache coherence. Instead, the proxy simulates the Ganesh driver’s
cache management algorithm and uses this to maintain a list of
hashes for which the Ganesh driver is likely to possess the result
fragments. In case of mistracking, there will be no loss of correct-
ness but there will be extra round-trip delays to fetch the missing
fragments.
If the client detects loss of synchronization with the
proxy, it can ask the proxy to reset the state shared between them.
Also note that the proxy does not need to keep the result fragments
themselves, only their hashes. This allows the proxy to remain
scalable even when it is shared by many front-end nodes.

ClientDatabaseWeb and Application ServerNative JDBC DriverWANClientDatabaseGanesh ProxyNative JDBC DriverWANWeb and Application ServerGanesh JDBC DriverWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content313Benchmark Dataset

Details

BBOARD

AUCTION

500,000 Users, 12,000 Stories

2.0 GB
1.3 GB 1,000,000 Users, 34,000 Items

3,298,000 Comments

Table 1: Benchmark Dataset Details

• Second, how important is Ganesh’s structural similarity de-
tection relative to Rabin ﬁngerprinting’s similarity detection?
• Third, is the overhead of the proxy-based design acceptable?

Our evaluation answers these question through controlled experi-
ments with the Ganesh prototype. This section describes the bench-
marks used, our evaluation procedure, and the experimental setup.
Results of the experiments are presented in Sections 5, 6, and 7.
4.1 Benchmarks

Our evaluation is based on two benchmarks [18] that have been
widely used by other researchers to evaluate various aspects of
multi-tier [27] and eBusiness architectures [9]. The ﬁrst bench-
mark, BBOARD, is modeled after Slashdot, a technology-oriented
news site. The second benchmark, AUCTION, is modeled after
eBay, an online auction site. In both benchmarks, most content is
dynamically generated from information stored in a database. De-
tails of the datasets used can be found in Table 1.

4.1.1 The BBOARD Benchmark
The BBOARD benchmark, also known as RUBBoS [18], mod-
els Slashdot, a popular technology-oriented web site. Slashdot ag-
gregates links to news stories and other topics of interest found
elsewhere on the web. The site also serves as a bulletin board by
allowing users to comment on the posted stories in a threaded con-
versation form. It is not uncommon for a story to gather hundreds
of comments in a matter of hours. The BBOARD benchmark is sim-
ilar to the site and models the activities of a user, including read-
only operations such as browsing the stories of the day, browsing
story categories, and viewing comments as well as write operations
such as new user registration, adding and moderating comments,
and story submission.

The benchmark consists of three different phases: a short warm-
up phase, a runtime phase representing the main body of the work-
load, and a short cool-down phase. In this paper we only report
results from the runtime phase. The warm-up phase is important
in establishing dynamic system state, but measurements from that
phase are not signiﬁcant for our evaluation. The cool-down phase
is solely for allowing the benchmark to shut down.

The warm-up, runtime, and cool-down phases are 2, 15, and 2
minutes respectively. The number of simulated clients were 400,
800, 1200, and 1600. The benchmark is available in a Java Servlets
and PHP version and has different datasets; we evaluated Ganesh
using the Java Servlets version and the Expanded dataset.

The BBOARD benchmark deﬁnes two different workloads. The
ﬁrst, the Authoring mix, consists of 70% read-only operations and
30% read-write operations. The second, the Browsing mix, con-
tains only read-only operations and does not update the database.

4.1.2 The AUCTION Benchmark
The AUCTION benchmark, also known as RUBiS [18], models
eBay, the online auction site. The eBay web site is used to buy
and sell items via an auction format. The main activities of a user
include browsing, selling, or bidding for items. Modeling the activ-
ities on this site, this benchmark includes read-only activities such
as browsing items by category and by region, as well as read-write

Figure 4: Dataﬂow for Result Handling
3.4 Encoding and Decoding Results

The Ganesh proxy receives database output as Java objects from
the native JDBC driver.
It examines this output to see if a Java
object of type ResultSet is present. The JDBC interface uses
this data type to store results of database queries. If a ResultSet
object is found, it is shrunk as discussed below. All other Java
objects are passed through unmodiﬁed.

As discussed in Section 3.1, the proxy uses the row boundaries
deﬁned in the ResultSet to partition it into fragments consist-
ing of single result rows. All ResultSet objects are converted
into objects of a new type called RecipeResultSet. We use
the term “recipe” for this compact description of a database re-
sult because of its similarity to a ﬁle recipe in the CASPER ﬁle
system [37]. The conversion replaces each result fragment that is
likely to be present in the Ganesh driver’s cache by a SHA-1 hash
of that fragment. Previously unseen result fragments are retained
verbatim. The proxy also retains hashes for the new result frag-
ments as they will be present in the driver’s cache in the future.
Note that the proxy only caches hashes for result fragments and
does not cache recipes.

The proxy constructs a RecipeResultSet by checking for
similarity at the entire result and then the row level. If the entire
result is predicted to be present in the Ganesh driver’s cache, the
RecipeResultSet is simply a single hash of the entire result.
Otherwise, it contains hashes for those rows predicted to be present
in that cache; all other rows are retained verbatim. If the proxy es-
timates an overall space savings, it will transmit the RecipeRe-
sultSet. Otherwise the original ResultSet is transmitted.

The RecipeResultSet objects are transformed back into Re-
sultSet objects by the Ganesh driver. Figure 4 illustrates Re-
sultSet handling at both ends. Each SHA-1 hash found in a
RecipeResultSet is looked up in the local cache of result frag-
ments. On a hit, the hash is replaced by the corresponding frag-
ment. On a miss, the driver contacts the Ganesh proxy to fetch the
fragment. All previously unseen result fragments that were retained
verbatim by the proxy are hashed and added to the result cache.

There should be very few misses if the proxy has accurately
tracked the Ganesh driver’s cache state. A future optimization would
be to batch the fetch of missing fragments. This would be valuable
when there are many small missing fragments in a high-latency
WAN. Once the transformation is complete, the fully reconstructed
ResultSet object is passed up to the application.

4. EXPERIMENTAL VALIDATION

Three questions follow from the goals and design of Ganesh:
• First, can performance can be improved signiﬁcantly by ex-

ploiting similarity across database results?

Object Output StreamConvert ResultSetObject Input StreamConvert ResultSetAll DataRecipe ResultSetAll DataResultSetNetworkGanesh ProxyGanesh JDBC DriverResultSetRecipe Result SetYesYesNoNoGanesh Input StreamGanesh Output StreamWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content314Figure 5: Experimental Setup

activities such as bidding for items, buying and selling items, and
leaving feedback.

As with BBOARD, the benchmark consists of three different phases.

The warm-up, runtime, and cool-down phases for this experiment
are 1.5, 15, and 1 minutes respectively. We tested Ganesh with
four client conﬁgurations where the number of test clients was set
to 400, 800, 1200, and 1600. The benchmark is available in a En-
terprise Java Bean (EJB), Java Servlets, and PHP version and has
different datasets; we evaluated Ganesh with the Java Servlets ver-
sion and the Expanded dataset.

The AUCTION benchmark deﬁnes two different workloads. The
ﬁrst, the Bidding mix, consists of 70% read-only operations and
30% read-write operations. The second, the Browsing mix, con-
tains only read-only operations and does not update the database.
4.2 Experimental Procedure

Both benchmarks involve a synthetic workload of clients access-
ing a web server. The number of clients emulated is an experi-
mental parameter. Each emulated client runs an instance of the
benchmark in its own thread, using a matrix to transition between
different benchmark states. The matrix deﬁnes a stochastic model
with probabilities of transitioning between the different states that
represent typical user actions. An example transition is a user log-
ging into the AUCTION system and then deciding on whether to
post an item for sale or bid on active auctions. Each client also
models user think time between requests. The think time is mod-
eled as an exponential distribution with a mean of 7 seconds.

We evaluate Ganesh along two axes: number of clients and WAN
bandwidth. Higher loads are especially useful in understanding
Ganesh’s performance when the CPU or disk of the database server
or proxy is the limiting factor. A previous study has shown that ap-
proximately 50% of the wide-area Internet bottlenecks observed
had an available bandwidth under 10 Mb/s [1]. Based on this work,
we focus our evaluation on the WAN bandwidth of 5 Mb/s with
66 ms of round-trip latency, representative of severely constrained
network paths, and 20 Mb/s with 33 ms of round-trip latency, repre-
sentative of a moderately constrained network path. We also report
Ganesh’s performance at 100 Mb/s with no added round-trip la-
tency. This bandwidth, representative of an unconstrained network,
is especially useful in revealing any potential overhead of Ganesh
in situations where WAN bandwidth is not the limiting factor. For
each combination of number of clients and WAN bandwidth, we
measured results from the two conﬁgurations listed below:

• Native: This conﬁguration corresponds to Figure 3(a). Na-
tive avoids Ganesh’s overhead in using a proxy and perform-
ing Java object serialization.
• Ganesh: This conﬁguration corresponds to Figure 3(b). For
a given number of clients and WAN bandwidth, comparing
these results to the corresponding Native results gives the
performance beneﬁt due to the Ganesh middleware system.

The metric used to quantify the improvement in throughput is
the number of client requests that can be serviced per second. The
metric used to quantify Ganesh’s overhead is the average response
time for a client request. For all of the experiments, the Ganesh

driver used by the application server used a cache size of 100,000
items1. The proxy was effective in tracking the Ganesh driver’s
cache state; for all of our experiments the miss rate on the driver
never exceeded 0.7%.
4.3 Experimental Setup

The experimental setup used for the benchmarks can be seen in
Figure 5. All machines were 3.2 GHz Pentium 4s (with Hyper-
Threading enabled.) With the exception of the database server, all
machines had 2 GB of SDRAM and ran the Fedora Core Linux
distribution. The database server had 4 GB of SDRAM.

We used Apache’s Tomcat as both the application server that
hosted the Java Servlets and the web server. Both benchmarks
used Java Servlets to generate the dynamic content. The database
server used the open source MySQL database. For the native JDBC
drivers, we used the Connector/J drivers provided by MySQL. The
application server used Sun’s Java Virtual Machine as the runtime
environment for the Java Servlets. The sysstat tool was used to
monitor the CPU, network, disk, and memory utilization on all ma-
chines.

The machines were connected by a switched gigabit Ethernet
network. As shown in Figure 5, the front-end web and applica-
tion server was separated from the proxy and database server by a
NetEm router [16]. This router allowed us to control the bandwidth
and latency settings on the network. The NetEm router is a stan-
dard PC with two network cards running the Linux Trafﬁc Control
and Network Emulation software. The bandwidth and latency con-
straints were only applied to the link between the application server
and the database for the native case and between the application
server and the proxy for the Ganesh case. There is no communica-
tion between the application server and the database with Ganesh
as all data ﬂows through the proxy. As our focus was on the WAN
link between the application server and the database, there were no
constraints on the link between the simulated test clients and the
web server.

5. THROUGHPUT AND RESPONSE TIME
In this section, we address the ﬁrst question raised in Section 4:
Can performance can be improved signiﬁcantly by exploiting sim-
ilarity across database results? To answer this question, we use
results from the BBOARD and AUCTION benchmarks. We use
two metrics to quantify the performance improvement obtainable
through the use of Ganesh: throughput, from the perspective of the
web server, and average response time, from the perspective of the
client. Throughput is measured in terms of the number of client
requests that can be serviced per second.
5.1 BBOARD Results and Analysis

5.1.1 Authoring Mix
Figures 6 (a) and (b) present the average number of requests ser-
viced per second and the average response time for these requests
as perceived by the clients for BBOARD’s Authoring Mix.

As Figure 6 (a) shows, Native easily saturates the 5 Mb/s link.
At 400 clients, the Native solution delivers 29 requests/sec with an
average response time of 8.3 seconds. Native’s throughput drops
with an increase in test clients as clients timeout due to conges-
tion at the application server. Usability studies have shown that
response times above 10 seconds cause the user to move on to

1As Java lacks a sizeof() operator, Java caches therefore limit
their size based on the number of objects. The size of cache dumps
taken at the end of the experiments never exceeded 212 MB.

NetEmRouterGaneshProxyClientsWeb andApplication ServerDatabase ServerWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content315(a) Throughput: Authoring Mix

(b) Response Time: Authoring Mix

(c) Throughput: Browsing Mix

(d) Response Time: Browsing Mix

Mean of three trials. The maximum standard deviation for throughput and response time was 9.8% and 11.9% of the corresponding mean.

Figure 6: BBOARD Benchmark - Throughput and Average Response Time

clients. Even with the 1600 test client conﬁguration Ganesh deliv-
ers an acceptable average response time of 8.2 seconds.

other tasks [24]. Based on these numbers, increasing the num-
ber of test clients makes the Native system unusable. Ganesh at
5 Mb/s, however, delivers a twofold improvement with 400 test
clients and a ﬁvefold improvement at 1200 clients. Ganesh’s per-
formance drops slightly at 1200 and 1600 clients as the network is
saturated. Compared to Native, Figure 6 (b) shows that Ganesh’s
response times are substantially lower with sub-second response
times at 400 clients.

Figure 6 (a) also shows that for 400 and 800 test clients Ganesh
at 5 Mb/s has the same throughput and average response time as
Native at 20 Mb/s. Only at 1200 and 1600 clients does Native at 20
Mb/s deliver higher throughput than Ganesh at 5 Mb/s.

Comparing both Ganesh and Native at 20 Mb/s, we see that
Ganesh is no longer bandwidth constrained and delivers up to a
twofold improvement over Native at 1600 test clients. As Ganesh
does not saturate the network with higher test client conﬁgurations,
at 1600 test clients, its average response time is 0.1 seconds rather
than Native’s 7.7 seconds.

As expected, there are no visible gains from Ganesh at the higher
bandwidth of 100 Mb/s where the network is no longer the bottle-
neck. Ganesh, however, still tracks Native in terms of throughput.
5.1.2 Browsing Mix
Figures 6 (c) and (d) present the average number of requests ser-
viced per second and the average response time for these requests
as perceived by the clients for BBOARD’s Browsing Mix.

Regardless of the test client conﬁguration, Figure 6 (c) shows
that Native’s throughput at 5 Mb/s is limited to 10 reqs/sec. Ganesh
at 5 Mb/s with 400 test clients, delivers more than a sixfold in-
crease in throughput. The improvement increases to over a eleven-
fold increase at 800 test clients before Ganesh saturates the net-
work. Further, Figure 6 (d) shows that Native’s average response
time of 35 seconds at 400 test clients make the system unusable.
These high response times further increase with the addition of test

Due to the data-intensive nature of the Browsing mix, Ganesh at
5 Mb/s surprisingly performs much better than Native at 20 Mb/s.
Further, as shown in Figure 6 (d), while the average response time
for Native at 20 Mb/s is acceptable at 400 test clients, it is unusable
with 800 test clients with an average response time of 15.8 seconds.
Like the 5 Mb/s case, this response time increases with the addition
of extra test clients.

Ganesh at 20 Mb/s and both Native and Ganesh at 100 Mb/s are
not bandwidth limited. However, performance plateaus out after
1200 test clients due to the database CPU being saturated.
5.1.3 Filter Variant
We were surprised by the Native performance from the BBOARD
benchmark. At the bandwidth of 5 Mb/s, Native performance was
lower than what we had expected.
It turned out the benchmark
code that displays stories read all the comments associated with
the particular story from the database and only then did some post-
processing to select the comments to be displayed. While this is
exactly the behavior of SlashCode, the code base behind the Slash-
dot web site, we decided to modify the benchmark to perform some
pre-ﬁltering at the database. This modiﬁed benchmark, named the
Filter Variant, models a developer who applies optimizations at the
SQL level to transfer less data. In the interests of brevity, we only
brieﬂy summarize the results from the Authoring mix.

For the Authoring mix, at 800 test clients at 5 Mb/s, Figure 7 (a)
shows that Native’s throughput increase by 85% when compared
to the original benchmark while Ganesh’s improvement is smaller
at 15%. Native’s performance drops above 800 clients as the test
clients time out due to high response times. The most signiﬁcant
gain for Native is seen at 20 Mb/s. At 1600 test clients, when com-
pared to the original benchmark, Native sees a 73% improvement
in throughput and a 77% reduction in average response time. While

0501001502002504008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsRequests / secNativeGanesh0.0010.010.11101004008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsAvg. Resp. Time (sec)NativeGaneshNote Logscale0501001502002504008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsRequests / secNativeGanesh0.0010.010.11101004008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsAvg. Resp. Time (sec)NativeGaneshNote LogscaleWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content316(a) Throughput: Authoring Mix

(b) Response Time: Authoring Mix

Mean of three trials. The maximum standard deviation for throughput and response time was 7.2% and 11.5% of the corresponding mean.

Figure 7: BBOARD Benchmark - Filter Variant - Throughput and Average Response Time

Ganesh sees no improvement when compared to the original, it still
processes 19% more requests/sec than Native. Thus, while the op-
timizations were more helpful to Native, Ganesh still delivers an
improvement in performance.
5.2 AUCTION Results and Analysis
5.2.1 Bidding Mix
Figures 8 (a) and (b) present the average number of requests ser-
viced per second and the average response time for these requests
as perceived by the clients for AUCTION’s Bidding Mix. As men-
tioned earlier, the Bidding mix consists of a mixture of read and
write operations.

The AUCTION benchmark is not as data intensive as BBOARD.
Therefore, most of the gains are observed at the lower bandwidth
of 5 Mb/s. Figure 8 (a) shows that the increase in throughput due
to Ganesh ranges from 8% at 400 test clients to 18% with 1600
test clients. As seen in Figure 8 (b), the average response times for
Ganesh are signiﬁcantly lower than Native ranging from a decrease
of 84% at 800 test clients to 88% at 1600 test clients.

Figure 8 (a) also shows that with a fourfold increase of band-
width from 5 Mb/s to 20 Mb/s, Native is no longer bandwidth con-
strained and there is no performance difference between Ganesh
and Native. With the higher test client conﬁgurations, we did ob-
serve that the bandwidth used by Ganesh was lower than Native.
Ganesh might still be useful in these non-constrained scenarios if
bandwidth is purchased on a metered basis. Similar results are seen
for the 100 Mb/s scenario.
5.2.2 Browsing Mix
For AUCTION’s Browsing Mix, Figures 8 (c) and (d) present the
average number of requests serviced per second and the average
response time for these requests as perceived by the clients.

Again, most of the gains are observed at lower bandwidths. At 5
Mb/s, Native and Ganesh deliver similar throughput and response
times with 400 test clients. While the throughput for both remains
the same at 800 test clients, Figure 8 (d) shows that Ganesh’s aver-
age response time is 62% lower than Native. Native saturates the
link at 800 clients and adding extra test clients only increases the
average response time. Ganesh, regardless of the test client conﬁg-
uration, is not bandwidth constrained and maintains the same re-
sponse time. At 1600 test clients, Figure 8 (c) shows that Ganesh’s
throughput is almost twice that of Native.

At the higher bandwidths of 20 and 100 Mb/s, neither Ganesh
nor Native is bandwidth limited and deliver equivalent throughput
and response times.

Benchmark Orig. Size Ganesh Size Rabin Size
219.3 MB
SelectSort1
SelectSort2
223.6 MB

223.6 MB
223.6 MB

5.4 MB
5.4 MB

Table 2: Similarity Microbenchmarks

6. STRUCTURAL VS. RABIN SIMILARITY
In this section, we address the second question raised in Sec-
tion 4: How important is Ganesh’s structural similarity detection
relative to Rabin ﬁngerprinting-based similarity detecting? To an-
swer this question, we used microbenchmarks and the BBOARD and
AUCTION benchmarks. As Ganesh always performed better than
Rabin ﬁngerprinting, we only present a subset of the results here in
the interests of brevity.
6.1 Microbenchmarks

Two microbenchmarks show an example of the effects of data
reordering on Rabin ﬁngerprinting algorithm. In the ﬁrst micro-
benchmark, SelectSort1, a query with a speciﬁed sort order selects
223.6 MB of data spread over approximately 280 K rows. The
query is then repeated with a different sort attribute. While the
same number of rows and the same data is returned, the order of
rows is different.
In such a scenario, one would expect a large
amount of similarity to be detected between both results. As Ta-
ble 2 shows, Ganesh’s row-based algorithm achieves a 97.6% re-
duction while the Rabin ﬁngerprinting algorithm, with the average
chunk size parameter set to 4 KB, only achieves a 1% reduction.
The reason, as shown earlier in Figure 2, is that with Rabin ﬁn-
gerprinting, the spans of data between two consecutive boundaries
usually cross row boundaries. With the order of the rows changing
in the second result and the Rabin ﬁngerprints now spanning dif-
ferent rows, the algorithm is unable to detect signiﬁcant similarity.
The small gain seen is mostly for those single rows that are large
enough to be broken into multiple chunks.

SelectSort2, another micro-benchmark executed the same queries
but increased the minimum chunk size of the Rabin ﬁngerprinting
algorithm. As can be seen in Table 2, even the small gain from the
previous microbenchmark disappears as the minimum chunk size
was greater than the average row size. While one can partially ad-
dress these problems by dynamically varying the parameters of the
Rabin ﬁngerprinting algorithm, this can be computationally expen-
sive, especially in the presence of changing workloads.
6.2 Application Benchmarks

We ran the BBOARD benchmark described in Section 4.1.1 on
two versions of Ganesh: the ﬁrst with Rabin ﬁngerprinting used as

0501001502002504008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsRequests / secNativeGanesh0.0010.010.11101004008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsAvg. Resp. Time (sec)NativeGaneshNote LogscaleWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content317(a) Throughput: Bidding Mix

(b) Response Time: Bidding Mix

(c) Throughput: Browsing Mix

(d) Response Time: Browsing Mix

Mean of three trials. The maximum standard deviation for throughput and response time was 2.2% and 11.8% of the corresponding mean.

Figure 8: AUCTION Benchmark - Throughput and Average Response Time
7. PROXY OVERHEAD

the chunking algorithm and the second with Ganesh’s row-based
algorithm. Rabin’s results for the Browsing Mix are normalized to
Ganesh’s results and presented in Figure 9.

As Figure 9 (a) shows, at 5 Mb/s, independent of the test client
conﬁguration, Rabin signiﬁcantly underperforms Ganesh. This hap-
pens because of a combination of two reasons. First, as outlined
in Section 3.1, Rabin ﬁnds less similarity as it does not exploit
the result’s structural information. Second, this benchmark con-
tained some queries that generated large results. In this case, Ra-
bin, with a small average chunk size, generated a large number of
objects that evicted other useful data from the cache. In contrast,
Ganesh was able to detect these large rows and correspondingly
increase the size of the chunks. This was conﬁrmed as cache statis-
tics showed that Ganesh’s hit ratio was roughly three time that of
Rabin. Throughput measurements at 20 Mb/s were similar with
the exception of Rabin’s performance with 400 test clients. In this
case, Ganesh was not network limited and, in fact, the throughput
was the same as 400 clients at 5 Mb/s. Rabin, however, took ad-
vantage of the bandwidth increase from 5 to 20 Mb/s to deliver a
slightly better performance. At 100 Mb/s, Rabin’s throughput was
almost similar to Ganesh as bandwidth was no longer a bottleneck.
The normalized response time, presented in Figure 9 (b), shows
similar trends. At 5 and 20 Mb/s, the addition of test clients de-
creases the normalized response time as Ganesh’s average response
time increases faster than Rabin’s. However, at no point does Rabin
outperform Ganesh. Note that at 400 and 800 clients at 100 Mb/s,
Rabin does have a higher overhead even when it is not bandwidth
constrained. As mentioned in Section 3.1, this is due to the fact that
Rabin has to hash each ResultSet twice. The overhead disap-
pears with 1200 and 1600 clients as the database CPU is saturated
and limits the performance of both Ganesh and Rabin.

In this section, we address the third question raised in Section 4:
Is the overhead of Ganesh’s proxy-based design acceptable? To an-
swer this question, we concentrate on its performance at the higher
bandwidths. Our evaluation in Section 5 showed that Ganesh, when
compared to Native, can deliver a substantial throughput improve-
ment at lower bandwidths. It is only at higher bandwidths that la-
tency, measured by the average response time for a client request,
and throughput, measured by the number of client requests that can
be serviced per second, overheads would be visible.

Looking at the Authoring mix of the original BBOARD bench-
mark, there are no visible gains from Ganesh at 100 Mb/s. Ganesh,
however, still tracks Native in terms of throughput. While the av-
erage response time is higher for Ganesh, the absolute difference is
in between 0.01 and 0.04 seconds and would be imperceptible to
the end-user. The Browsing mix shows an even smaller difference
in average response times. The results from the ﬁlter variant of the
BBOARD benchmarks are similar. Even for the AUCTION bench-
mark, the difference between Native and Ganesh’s response time at
100 Mb/s was never greater than 0.02 seconds. The only exception
to the above results was seen in the ﬁlter variant of the BBOARD
benchmark where Ganesh at 1600 test clients added 0.85 seconds
to the average response time. Thus, even for much faster networks
where the WAN link is not the bottleneck, Ganesh always delivers
throughput equivalent to Native. While some extra latency is added
by the proxy-based design, it is usually imperceptible.

8. RELATED WORK

To the best of our knowledge, Ganesh is the ﬁrst system that
combines the use of hash-based techniques with caching of database
results to improve throughput and response times for applications
with dynamic content. We also believe that it is also the ﬁrst sys-
tem to demonstrate the beneﬁts of using structural information for

0501001502002503003504008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsRequests / secNativeGanesh0.0010.010.11104008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsAvg. Resp. Time (sec)NativeGaneshNote Logscale0501001502002503003504008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsRequests / secNativeGanesh0.0010.010.11104008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsAvg. Resp. Time (sec)NativeGaneshNote LogscaleWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content318(a) Normalized Throughput: Higher is better

(b) Normalized Response Time: Higher is worse

For throughput, a normalized result greater than 1 implies that Rabin is better, For response time, a normalized result greater than 1 implies
that Ganesh is better. Mean of three trials. The maximum standard deviation for throughput and response time was 9.1% and 13.9% of
the corresponding mean.

Figure 9: Normalized Comparison of Ganesh vs. Rabin - BBOARD Browsing Mix

detecting similarity. In this section, we ﬁrst discuss alternative ap-
proaches to caching dynamic content and then examine other uses
of hash-based primitives in distributed systems.

8.1 Caching Dynamic Content

At the database layer, a number of systems have advocated middle-

tier caching where parts of the database are replicated at the edge or
server [3, 4, 20]. These systems either cache entire tables in what
is essentially a replicated database or use materialized views from
previous query replies [19]. They require tight integration with the
back-end database to ensure a time bound on the propagation of
updates. These systems are also usually targeted towards work-
loads that do not require strict consistency and can tolerate stale
data. Further, unlike Ganesh, some of these mid-tier caching solu-
tions [2, 3], suffer from the complexity of having to participate in
query planing and distributed query processing.

Gao et al. [15] propose using a distributed object replication
architecture where the data store’s consistency requirements are
adapted on a per-application basis. These solutions require sub-
stantial developer resources and detailed understanding of the ap-
plication being modiﬁed. While systems that attempt to automate
the partitioning and replication of an application’s database ex-
ist [34], they do not provide full transaction semantics. In com-
parison, Ganesh does not weaken any of the semantics provided by
the underlying database.

Recent work in the evaluation of edge caching options for dy-
namic web sites [38] has suggested that, without careful planning,
employing complex ofﬂoading strategies can hurt performance. In-
stead, the work advocates for an architecture in which all tiers ex-
cept the database should be ofﬂoaded to the edge. Our evaluation of
Ganesh has shown that it would beneﬁt these scenarios. To improve
database scalability, C-JDBC [10], SSS [22], and Ganymed [28]
also advocate the use of an interposition-based architecture to trans-
parently cluster and replicate databases at the middleware level.
The approaches of these architectures and Ganesh are complemen-
tary and they would beneﬁt each other.

Moving up to the presentation layer, there has been widespread
adoption of fragment-based caching [14], which improves cache
utilization by separately caching different parts of generated web
pages. While fragment-based caching works at the edge, a recent
proposal has proposed moving web page assembly to the clients to
optimize content delivery [31]. While Ganesh is not used at the pre-
sentation layer, the same principles have been applied in Duplicate
Transfer Detection [25] to increase web cache efﬁciency as well as
for web access across bandwidth limited links [33].

8.2 Hash-based Systems

The past few years have seen the emergence of many systems
that exploit hash-based techniques. At the heart of all these sys-
tems is the idea of detecting similarity in data without requiring in-
terpretation of that data. This simple yet elegant idea relies on cryp-
tographic hashing, as discussed earlier in Section 2. Successful ap-
plications of this idea span a wide range of storage systems. Exam-
ples include peer-to-peer backup of personal computing ﬁles [11],
storage-efﬁcient archiving of data [29], and ﬁnding similar ﬁles [21].
Spring and Wetherall [35] apply similar principles at the network
level. Using synchronized caches at both ends of a network link,
duplicated data is replaced by smaller tokens for transmission and
then restored at the remote end. This and other hash-based systems
such as the CASPER [37] and LBFS [26] ﬁlesystems, and Layer-2
bandwidth optimizers such as Riverbed and Peribit use Rabin ﬁn-
gerprinting [30] to discover spans of commonality in data. This ap-
proach is especially useful when data items are modiﬁed in-place
through insertions, deletions, and updates. However, as Section 6
shows, the performance of this technique can show a dramatic drop
in the presence of data reordering. Ganesh instead uses row bound-
aries as dividers for detecting similarity.

The most aggressive use of hash-based techniques is by systems
that use hashes as the primary identiﬁers for objects in persistent
storage. Storage systems such as CFS [12] and PAST [13] that
have been built using distributed hash tables fall into this category.
Single Instance Storage [6] and Venti [29] are other examples of
such systems. As discussed in Section 2.2, the use of cryptographic
hashes for addressing persistent data represents a deeper level of
faith in their collision-resistance than that assumed by Ganesh. If
time reveals shortcomings in the hash algorithm, the effort involved
in correcting the ﬂaw is much greater. In Ganesh, it is merely a
matter of replacing the hash algorithm.

9. CONCLUSION

The growing use of dynamic web content generated from re-
lational databases places increased demands on WAN bandwidth.
Traditional caching solutions for bandwidth and latency reduction
are often ineffective for such content. This paper shows that the
impact of WAN accesses to databases can be substantially reduced
through the Ganesh architecture without any compromise of the
database’s strict consistency semantics. The essence of the Ganesh
architecture is the use of computation at the edges to reduce com-
munication through the Internet. Ganesh is able to use crypto-
graphic hashes to detect similarity with previous results and send

0.00.20.40.60.81.04008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsNorm. Throughput31.83.82.82.323.832.85.83.61.82.11.11.0051015202530354008001200160040080012001600400800120016005 Mb/s20 Mb/s100 Mb/sTest ClientsNorm. Response TimeWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content319compact recipes of results rather than full results. Our design uses
interposition to achieve complete transparency: clients, application
servers, and database servers are all unaware of Ganesh’s presence
and require no modiﬁcation.

Our experimental evaluation conﬁrms that Ganesh, while con-
ceptually simple, can be highly effective in improving throughput
and response time. Our results also conﬁrm that exploiting the
structure present in database results to detect similarity is crucial
to this performance improvement.

10. REFERENCES
[1] AKELLA, A., SESHAN, S., AND SHAIKH, A. An empirical

evaluation of wide-area internet bottlenecks. In Proc. 3rd
ACM SIGCOMM Conference on Internet Measurement
(Miami Beach, FL, USA, Oct. 2003), pp. 101–114.

[2] ALTINEL, M., BORNH ¨OVD, C., KRISHNAMURTHY, S.,
MOHAN, C., PIRAHESH, H., AND REINWALD, B. Cache
tables: Paving the way for an adaptive database cache. In
Proc. of 29th VLDB (Berlin, Germany, 2003), pp. 718–729.
[3] ALTINEL, M., LUO, Q., KRISHNAMURTHY, S., MOHAN,

C., PIRAHESH, H., LINDSAY, B. G., WOO, H., AND
BROWN, L. Dbcache: Database caching for web application
servers. In Proc. 2002 ACM SIGMOD (2002), pp. 612–612.
[4] AMIRI, K., PARK, S., TEWARI, R., AND PADMANABHAN,
S. Dbproxy: A dynamic data cache for web applications. In
Proc. IEEE International Conference on Data Engineering
(ICDE) (Mar. 2003).

[5] BLACK, J. Compare-by-hash: A reasoned analysis. In Proc.
2006 USENIX Annual Technical Conference (Boston, MA,
May 2006), pp. 85–90.

[6] BOLOSKY, W. J., CORBIN, S., GOEBEL, D., , AND

DOUCEUR, J. R. Single instance storage in windows 2000.
In Proc. 4th USENIX Windows Systems Symposium (Seattle,
WA, Aug. 2000), pp. 13–24.

[7] BREWER, E. A. Lessons from giant-scale services. IEEE

Internet Computing 5, 4 (2001), 46–55.

[8] BRODER, A., GLASSMAN, S., MANASSE, M., AND

ZWEIG, G. Syntactic clustering of the web. In Proc. 6th
International WWW Conference (1997).

[9] CECCHET, E., CHANDA, A., ELNIKETY, S.,

MARGUERITE, J., AND ZWAENEPOEL, W. Performance
comparison of middleware architectures for generating
dynamic web content. In Proc. Fourth ACM/IFIP/USENIX
International Middleware Conference (Rio de Janeiro,
Brazil, June 2003).

[10] CECCHET, E., MARGUERITE, J., AND ZWAENEPOEL, W.
C-JDBC: Flexible database clustering middleware. In Proc.
2004 USENIX Annual Technical Conference (Boston, MA,
June 2004).

[11] COX, L. P., MURRAY, C. D., AND NOBLE, B. D. Pastiche:

Making backup cheap and easy. In OSDI: Symposium on
Operating Systems Design and Implementation (2002).

[12] DABEK, F., KAASHOEK, M. F., KARGER, D., MORRIS,
R., AND STOICA, I. Wide-area cooperative storage with
CFS. In 18th ACM Symposium on Operating Systems
Principles (Banff, Canada, Oct. 2001).

[13] DRUSCHEL, P., AND ROWSTRON, A. PAST: A large-scale,
persistent peer-to-peer storage utility. In HotOS VIII (Schloss
Elmau, Germany, May 2001), pp. 75–80.

[14] Edge side includes. http://www.esi.org.
[15] GAO, L., DAHLIN, M., NAYATE, A., ZHENG, J., AND

IYENGAR, A. Application speciﬁc data replication for edge
services. In WWW ’03: Proc. Twelfth International
Conference on World Wide Web (2003), pp. 449–460.

[16] HEMMINGER, S. Netem - emulating real networks in the lab.

In Proc. 2005 Linux Conference Australia (Canberra,
Australia, Apr. 2005).

[17] HENSON, V. An analysis of compare-by-hash. In Proc. 9th
Workshop on Hot Topics in Operating Systems (HotOS IX)
(May 2003), pp. 13–18.

[18] Jmob benchmarks. http://jmob.objectweb.org/.
[19] LABRINIDIS, A., AND ROUSSOPOULOS, N. Balancing

performance and data freshness in web database servers. In
Proc. 29th VLDB Conference (Sept. 2003).

[20] LARSON, P.-A., GOLDSTEIN, J., AND ZHOU, J.

Transparent mid-tier database caching in sql server. In Proc.
2003 ACM SIGMOD (2003), pp. 661–661.

[21] MANBER, U. Finding similar ﬁles in a large ﬁle system. In

Proc. USENIX Winter 1994 Technical Conference (San
Fransisco, CA, 17–21 1994), pp. 1–10.

[22] MANJHI, A., AILAMAKI, A., MAGGS, B. M., MOWRY,

T. C., OLSTON, C., AND TOMASIC, A. Simultaneous
scalability and security for data-intensive web applications.
In Proc. 2006 ACM SIGMOD (June 2006), pp. 241–252.

[23] MENEZES, A. J., VANSTONE, S. A., AND OORSCHOT, P.

C. V. Handbook of Applied Cryptography. CRC Press, 1996.

[24] MILLER, R. B. Response time in man-computer

conversational transactions. In Proc. AFIPS Fall Joint
Computer Conference (1968), pp. 267–277.

[25] MOGUL, J. C., CHAN, Y. M., AND KELLY, T. Design,

implementation, and evaluation of duplicate transfer
detection in http. In Proc. First Symposium on Networked
Systems Design and Implementation (San Francisco, CA,
Mar. 2004).

[26] MUTHITACHAROEN, A., CHEN, B., AND MAZIERES, D. A

low-bandwidth network ﬁle system. In Proc. 18th ACM
Symposium on Operating Systems Principles (Banff, Canada,
Oct. 2001).

[27] PFEIFER, D., AND JAKSCHITSCH, H. Method-based

caching in multi-tiered server applications. In Proc. Fifth
International Symposium on Distributed Objects and
Applications (Catania, Sicily, Italy, Nov. 2003).

[28] PLATTNER, C., AND ALONSO, G. Ganymed: Scalable

replication for transactional web applications. In Proc. 5th
ACM/IFIP/USENIX International Conference on
Middleware (2004), pp. 155–174.

[29] QUINLAN, S., AND DORWARD, S. Venti: A new approach
to archival storage. In Proc. FAST 2002 Conference on File
and Storage Technologies (2002).

[30] RABIN, M. Fingerprinting by random polynomials. In
Harvard University Center for Research in Computing
Technology Technical Report TR-15-81 (1981).

[31] RABINOVICH, M., XIAO, Z., DOUGLIS, F., AND

KALMANEK, C. Moving edge side includes to the real edge
– the clients. In Proc. 4th USENIX Symposium on Internet
Technologies and Systems (Seattle, WA, Mar. 2003).

[32] REESE, G. Database Programming with JDBC and Java,

1st ed. O’Reilly, June 1997.

[33] RHEA, S., LIANG, K., AND BREWER, E. Value-based web

caching. In Proc. Twelfth International World Wide Web
Conference (May 2003).

[34] SIVASUBRAMANIAN, S., ALONSO, G., PIERRE, G., AND
VAN STEEN, M. Globedb: Autonomic data replication for
web applications. In WWW ’05: Proc. 14th International
World-Wide Web conference (May 2005).
[35] SPRING, N. T., AND WETHERALL, D. A

protocol-independent technique for eliminating redundant
network trafﬁc. In Proc. of ACM SIGCOMM (Aug. 2000).

[36] TOLIA, N., HARKES, J., KOZUCH, M., AND

SATYANARAYANAN, M. Integrating portable and distributed
storage. In Proc. 3rd USENIX Conference on File and
Storage Technologies (San Francisco, CA, Mar. 2004).

[37] TOLIA, N., KOZUCH, M., SATYANARAYANAN, M., KARP,
B., PERRIG, A., AND BRESSOUD, T. Opportunistic use of
content addressable storage for distributed ﬁle systems. In
Proc. 2003 USENIX Annual Technical Conference (San
Antonio, TX, June 2003), pp. 127–140.

[38] YUAN, C., CHEN, Y., AND ZHANG, Z. Evaluation of edge
caching/ofﬂoading for dynamic content delivery. In WWW
’03: Proc. Twelfth International Conference on World Wide
Web (2003), pp. 461–471.

WWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content320