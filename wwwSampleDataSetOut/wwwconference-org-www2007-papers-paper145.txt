A Scalable Application Placement Controller

for Enterprise Data Centers

Chunqiang Tang, Malgorzata Steinder, Michael Spreitzer, and Giovanni Pacifici

IBM T.J. Watson Research Center

19 Skyline Drive, Hawthorne, NY 10532, USA
fctang, steinder, mspeitz, giovannig@us.ibm.com

ABSTRACT
Given a set of machines and a set of Web applications with
dynamically changing demands, an online application place-
ment controller decides how many instances to run for each
application and where to put them, while observing all kinds
of resource constraints. This NP hard problem has real us-
age in commercial middleware products. Existing approxi-
mation algorithms for this problem can scale to at most a
few hundred machines, and may produce placement solu-
tions that are far from optimal when system resources are
tight. In this paper, we propose a new algorithm that can
produce within 30 seconds high-quality solutions for hard
placement problems with thousands of machines and thou-
sands of applications. This scalability is crucial for dynamic
resource provisioning in large-scale enterprise data centers.
Our algorithm allows multiple applications to share a single
machine, and strives to maximize the total satis(cid:12)ed applica-
tion demand, to minimize the number of application starts
and stops, and to balance the load across machines. Com-
pared with existing state-of-the-art algorithms, for systems
with 100 machines or less, our algorithm is up to 134 times
faster, reduces application starts and stops by up to 97%,
and produces placement solutions that satisfy up to 25%
more application demands. Our algorithm has been imple-
mented and adopted in a leading commercial middleware
product for managing the performance of Web applications.

Categories and Subject Descriptors
K.6.4 [Computing Milieux]: Management of Computing
and Information Systems|System Management
General Terms
Algorithm, Management, Performance
Keywords
Application Placement, Performance Management

1.

INTRODUCTION

With the rapid growth of the Internet, many organiza-
tions increasingly rely on Web applications to deliver criti-
cal services to their customers and partners. Enterprise data
centers may run thousands of machines to host a large num-
ber of Web applications that are resource demanding and

Copyright is held by the International World Wide Web Conference Com›
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8(cid:150)12, 2007, Banff, Alberta, Canada.
ACM 978›1›59593›654›7/07/0005.

process client requests at a high rate. Previous studies have
shown that the Web request rate is bursty and can (cid:13)uctuate
dramatically in a short period of time [14]. Therefore, it is
not cost-e(cid:11)ective to over provision data centers to handle
the potential peak demands of all the applications.

To utilize system resources more e(cid:11)ectively, modern Web
applications typically run on top of a middleware system
and rely on it to dynamically allocate resources to meet their
performance goals. Some middleware systems use clustering
technology to improve scalability and availability, by inte-
grating multiple instances of an application, and presenting
them to the users as a single virtual application.

Figure 1 is an example of clustered Web applications. The
system consists of one front-end request router, three back-
end machines (A, B, and C), and three applications (x,
y, and z). The applications, for example, can be catalog
search, order processing, and account management for an
online shopping site. The request router receives external
requests and forwards them to the application instances.

To meet the performance goals of the applications, the
request router may implement functions such as admission
control, (cid:13)ow control, and load balancing. These functions
decide how to dynamically allocate resources to the running
application instances, which are well-studied topics in the
literature [4, 14]. This paper studies an equally important
problem that receives relatively less attention in the past:

Given a set of machines with constrained re-
sources and a set of Web applications with dy-
namically changing demands, how many instances
to run for each application and where to put them?

We call this problem dynamic application placement. We
assume that not every machine can run all the applications
at the same time due to limited resources such as memory.
Application placement is orthogonal to admission control,
(cid:13)ow control, and load balancing, and the quality of a place-

(cid:2)(cid:24)

(cid:7)(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:5)(cid:6)(cid:5)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:5)(cid:6)(cid:7)(cid:1)(cid:8)(cid:4)(cid:6)(cid:2)(cid:9)

(cid:7)(cid:12)

(cid:7)(cid:22)

(cid:17) (cid:2)(cid:7)(cid:18)

(cid:7)(cid:12)

(cid:7)(cid:22)

(cid:17) (cid:2)(cid:7)(cid:19)

(cid:7)(cid:12)

(cid:7)(cid:21)

(cid:17) (cid:2)(cid:7)(cid:20)

Figure 1: An example of clustered Web applications.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications331(cid:10)
(cid:11)
(cid:11)
(cid:13)
(cid:10)
(cid:14)
(cid:15)
(cid:16)
(cid:10)
(cid:11)
(cid:11)
(cid:13)
(cid:10)
(cid:14)
(cid:15)
(cid:16)
(cid:10)
(cid:11)
(cid:11)
(cid:13)
(cid:10)
(cid:14)
(cid:15)
(cid:16)
(cid:10)
(cid:11)
(cid:11)
(cid:10)
(cid:11)
(cid:11)
(cid:23)
(cid:10)
(cid:11)
(cid:11)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:10)(cid:5)(cid:7)(cid:10)(cid:11)(cid:12)

(cid:3)(cid:1)(cid:1)(cid:9)(cid:24) (cid:5)(cid:6)(cid:3)(cid:7)(cid:24) (cid:9)(cid:5)(cid:10)(cid:8)(cid:16) (cid:6)(cid:3)(cid:8)(cid:11)(cid:12)

(cid:4)(cid:11)(cid:7)(cid:15)

(cid:9)(cid:18)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:5)(cid:13) (cid:5)(cid:4)(cid:14) (cid:8)(cid:11)(cid:12)

(cid:14) (cid:12)(cid:12)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:6)(cid:3)(cid:8)(cid:12)(cid:16)

(cid:21) (cid:9)(cid:1)
(cid:9)(cid:25) (cid:5)(cid:4)(cid:8)(cid:11)(cid:12)(cid:9)(cid:11)(cid:15) (cid:9)(cid:3)(cid:1)(cid:1)(cid:10)(cid:9)(cid:21) (cid:9)

(cid:9)(cid:24) (cid:5)(cid:6)(cid:3)(cid:7)(cid:24)

(cid:5)(cid:6)(cid:11)(cid:12)(cid:27)

(cid:9)(cid:24) (cid:5)(cid:6)(cid:3)(cid:7)(cid:24)

(cid:9)(cid:25) (cid:5)(cid:4)(cid:8)(cid:11)(cid:12)(cid:9)(cid:11)(cid:15) (cid:9)(cid:3)(cid:1)(cid:1)(cid:10)(cid:21) (cid:9)
(cid:16) (cid:7)(cid:5)(cid:10)(cid:21) (cid:9)

(cid:9)(cid:25) (cid:5)(cid:4)(cid:8)(cid:11)(cid:12)(cid:9)(cid:11)(cid:15) (cid:9)(cid:6)(cid:3)(cid:4)(cid:28)
(cid:21) (cid:9)(cid:2)

(cid:16) (cid:7)(cid:5)(cid:10)(cid:21) (cid:9)

(cid:9)(cid:4)(cid:3)(cid:1)(cid:3)(cid:4)(cid:16) (cid:8)(cid:27) (cid:9)(cid:25) (cid:5)(cid:4)(cid:8)(cid:11)(cid:12)(cid:9)(cid:11)(cid:15) (cid:9)(cid:6)(cid:3)(cid:4)(cid:28)

(cid:5)(cid:6)(cid:11)(cid:12)(cid:27)
(cid:9)(cid:4)(cid:3)(cid:1)(cid:3)(cid:4)(cid:16) (cid:8)(cid:27)
(cid:29) (cid:5)(cid:10)(cid:8)(cid:12)(cid:16) (cid:4)(cid:8)(cid:16) (cid:11)(cid:7)(cid:9)(cid:6)(cid:3)(cid:8)(cid:12)(cid:16)

(cid:4)(cid:8)(cid:2)(cid:13) (cid:6)(cid:14)

(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:1)(cid:6)(cid:4)(cid:7)(cid:8)(cid:9)(cid:9)(cid:2)(cid:3)(cid:1)(cid:5)(cid:10)(cid:11)(cid:10)(cid:8)(cid:6)

(cid:5)(cid:7)(cid:8)(cid:6)(cid:12) (cid:7)(cid:3)(cid:3)(cid:10)(cid:12)

(cid:7)(cid:13) (cid:6)(cid:2)(cid:13) (cid:6)(cid:14)
(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:6)(cid:3)(cid:8)(cid:12)(cid:16)
(cid:21) (cid:9)(cid:1)
(cid:9)(cid:24)
(cid:16) (cid:10)(cid:8)(cid:12)(cid:16)
(cid:14) (cid:8)(cid:16) (cid:11)(cid:7)(cid:9)(cid:6)(cid:3)(cid:8)(cid:12)(cid:16)
(cid:21) (cid:9)(cid:3)

(cid:30) (cid:5)(cid:31)
  (cid:11)(cid:3)(cid:24)

Figure 2: Control loop for application placement.

ment solution can have profound impact on the performance
of the entire system. In Figure 1, suppose the request rate
for application z suddenly surges. Application z may not
meet the demands even if all the resources of machine C
are allocated to application z. A smart middleware system
then may react by stopping application x on machines A
and B, and using the freed resources (e.g., memory) to start
an instance of application z on both A and B.

The application placement problem can be formulated as
a variant of the Class Constrained Multiple-Knapsack Prob-
lem [12, 13]. Under multiple resource constraints (e.g., CPU
and memory) and application constraints (e.g., the need for
special hardware or software), a placement algorithm strives
to produce placement solutions that optimize multiple ob-
jectives: (1) maximizing the total satis(cid:12)ed application de-
mand, (2) minimizing the total number of application starts
and stops as they disturb the running system, and (3) bal-
ancing the load across machines.

The placement problem is NP hard. Existing approxi-
mation algorithms [6, 8] can scale to at most a few hundred
machines, and may produce placement solutions that are far
from optimal when system resources are tight. In this paper,
we propose a new approximation algorithm that signi(cid:12)cantly
and consistently outperforms existing state-of-the-art algo-
rithms in terms of both solution quality and scalability. Our
algorithm has been implemented and adopted in a leading
commercial middleware product [1].

The remainder of the paper is organized as follows. Sec-
tions 2, 3, and 4 formulate the application placement prob-
lem, describe our algorithm, and present its performance,
respectively. Section 5 discusses related work, and Section 6
concludes the paper.

2. PROBLEM FORMULATION

Figure 2 is a simpli(cid:12)ed diagram of the control loop for ap-
plication placement. For brevity, we simply refer to \appli-
cation placement" as \placement" in the rest of this paper.
The inputs to the placement controller include the current
placement of applications on machines, the resource capac-
ity of each machine, the projected resource demand of each
application, and the restrictions that specify whether a given
application can run on a given machine, e.g., some applica-
tion may require machines with special hardware or soft-

ware. Taking these inputs collected by the auxiliary com-
ponents, the placement controller computes a placement so-
lution that optimizes certain objective functions, and then
passes the solution to the placement executor to start and
stop application instances accordingly. Periodically every T
minutes, the placement controller produces a new placement
solution based on the current inputs (e.g., T =15 minutes).
Estimating application demands is a non-trivial task. We
use online pro(cid:12)ling and data regression to dynamically es-
timate the average CPU cycles needed to process one Web
request of a given application [11]. The product of the es-
timated CPU cycles per request and the projected request
rate gives the CPU cycles needed by the application per
second. We use a peer-to-peer infrastructure [16] to gather
performance metrics from a large number of machines in a
scalable and reliable fashion.

In the past, we have developed a middleware system [1,
10, 11, 9] that includes a superset of the control loop in Fig-
ure 2. In this paper, we focus on the design and evaluation
of the placement controller. The rest of this section presents
the formal formulation of the placement problem. We (cid:12)rst
discuss the system resources and application demands con-
sidered in the placement problem.

A running application instance’s consumption of CPU cy-
cles and IO bandwidth depends on the request rate. As
for memory, our system periodically and conservatively esti-
mates the upper limit of an application’s near-term memory
usage, and assumes that this upper limit does not change
until the next estimate update point, because of several
practical reasons. First, a signi(cid:12)cant amount of memory
is consumed by an application instance even if it receives
no requests. Second, memory consumption is often related
to prior application usage rather than its current load due
to data caching and delayed garbage collection. Third, be-
cause an accurate projection of memory usage is di(cid:14)cult
and many applications cannot run when the system is out
of memory, it is more reasonable to use the conservatively
estimated upper limit for memory consumption.

Among many resources, we choose CPU and memory as
the representative ones to be considered by the placement
controller. For brevity, the description of our algorithm only
considers CPU and memory, but it can deal with other types
of resources as well. For example, if the system is network-
bounded, we can use network bandwidth as the bottleneck
resource whose consumption depends on the request rate.
This introduces no changes to our algorithm.

Next, we present the formulation of the placement prob-
lem. Table 1 lists the symbols used in our discussion (see
Figure 2 for their roles). The inputs to the placement con-
troller are the current placement matrix I, the placement
restriction matrix R, the CPU/memory capacity of each
machine ((cid:10)n and (cid:0)n), and the CPU/memory demand of
each application (!m and (cid:13)m). The outputs of placement
controller are the updated placement matrix I and the load
distribution matrix L.

The placement controller strives to (cid:12)nd a placement solu-
tion that maximizes the total satis(cid:12)ed application demand.
In addition, it also tries to minimize the total number of ap-
plication starts and stops, because placement changes dis-
turb the running system and waste CPU cycles. In practice,
many J2EE applications take a few minutes to start or stop,
and take some additional time to warm up their data cache.
The last optimization goal is to balance the load across ma-
chines. Ideally, the utilization of individual machines should

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications332(cid:16)
(cid:17)
(cid:19)
(cid:20)
(cid:13)
(cid:20)
(cid:22)
(cid:23)
(cid:26)
(cid:20)
(cid:22)
(cid:23)
(cid:26)
(cid:13)
(cid:13)
!
(cid:13)
stay close to the utilization (cid:26) of the entire system.

(cid:26) =

Pm2M Pn2N Lm;n

Pn2N (cid:10)n

(1)

As we are dealing with multiple optimization objectives,
we prioritize them in the formal problem statement below.
Let I (cid:3) denote the old placement matrix, and I denote the
new placement matrix.

(i) maximize X
m2M
(ii) minimize X
m2M

(iii) minimize X
n2N

(cid:12)(cid:12)(cid:12)(cid:12)

such that

Lm;n

X
n2N
X
n2N
Pm2M Lm;n

jIm;n (cid:0) I (cid:3)

(cid:10)n

m;nj

(cid:0) (cid:26)(cid:12)(cid:12)(cid:12)(cid:12)

8m 2 M; 8n 2 N
8m 2 M; 8n 2 N
8m 2 M; 8n 2 N
8m 2 M; 8n 2 N

8n 2 N

8n 2 N

8m 2 M

Im;n = 0 or Im;n = 1
Rm;n = 0 ) Im;n = 0
Im;n = 0 ) Lm;n = 0
Lm;n (cid:21) 0
X
m2M
X
m2M
X
n2N

(cid:13)mIm;n (cid:20) (cid:0)n

Lm;n (cid:20) !m

Lm;n (cid:20) (cid:10)n

(2)

(3)

(4)

(5)

(6)
(7)
(8)

(9)

(10)

(11)

This problem is a variant of the Class Constrained Multiple-
Knapsack problem [12, 13]. It di(cid:11)ers from the prior formu-
lation mainly in that it also minimizes the number of place-
ment starts and stops. This problem is NP hard. We will
present an online approximation algorithm for solving it.

3. THE PLACEMENT ALGORITHM

This section describes our placement algorithm. Before
presenting its details, we (cid:12)rst give a high-level description
of the algorithm, a de(cid:12)nition of terms, and the key ideas
behind the algorithm.

Our algorithm repeatedly and incrementally optimizes the
placement solution in multiple rounds.
In each round, it
(cid:12)rst computes the maximum total application demand that
can be satis(cid:12)ed by the current placement solution. The al-
gorithm quits if all the application demands are satis(cid:12)ed.
Otherwise, it shifts load across machines (without place-
ment changes), and then considers stopping unproductive
application instances and starting more useful ones in order
to increase the total satis(cid:12)ed application demand. The load-
shifting step before the placement-changing step is critical
as it dramatically simpli(cid:12)es subsequent placement changes.
Note that, in the algorithm description, \placement change",
\application start/stop", and \load shifting" are all hypo-
thetical. The real placement changes are executed after the
placement algorithm terminates.
3.1 Definition of Terms

A machine is fully utilized if its residual (i.e., unused)
CPU capacity is zero ((cid:10)(cid:3)
n = 0); otherwise, it is underutilized.
An application instance is fully utilized if it runs on a fully
utilized machine. An instance of application m running on
an underutilized machine n is completely idle if it has no
load (Lm;n=0); otherwise, it is underutilized. The load of
an underutilized instance of application m can be increased

One machine in the set N .

N The set of machines.
n
M The set of applications.
m One application in the set M.
R The placement restriction matrix. Rm;n = 1 if
application m can run on machine n; Rm;n = 0
otherwise.
The placement matrix. Im;n = 1 if application m
is running on machine n; Im;n = 0 otherwise.
The load distribution matrix. Lm;n is the CPU
cycles per second allocated on machine n for ap-
plication m. L is an output of the placement algo-
rithm; it is not measured from the running system.

L

I

(cid:0)n The memory capacity of machine n.
(cid:10)n The CPU capacity of machine n.
(cid:13)m The memory demand of application m, i.e., the
memory needed to run one instance of applica-
tion m.

!m The CPU demand of application m, i.e., the total
CPU cycles per second needed for application m
throughout the entire system.

!(cid:3)

(cid:10)(cid:3)

(cid:0)(cid:3)

m The residual CPU demand of application m, i.e.,
the demand not satis(cid:12)ed by the load distribution
matrix L: !(cid:3)

m = !m (cid:0) Pn2N Lm;n.

n The residual CPU capacity of machine n, i.e., the
CPU capacity not consumed by the applications
running on machine n: (cid:10)(cid:3)
n = (cid:10)n (cid:0) Pm2M Lm;n.
n The residual memory capacity of machine n, i.e.,
the memory not consumed by the busy applica-
tions (Lm;n>0) running on machine n:
(cid:0)(cid:3)
n = (cid:0)n (cid:0) Pm:Lm;n>0 (cid:13)m.

Table 1: Symbols used in the placement algorithm.

if application m has a positive residual (i.e., unsatis(cid:12)ed)
CPU demand (!(cid:3)

m > 0).

The CPU-memory ratio of a machine n is de(cid:12)ned as its
CPU capacity divided by its memory capacity, i.e., (cid:10)n=(cid:0)n.
Intuitively, it is harder to fully utilize the CPU of machines
with a high CPU-memory ratio. The load-memory ratio of
an instance of application m running on machine n is de(cid:12)ned
as the CPU load of this instance divided by its memory con-
sumption, i.e., Lm;n=(cid:13)m. Intuitively, application instances
with a higher load-memory ratio are more \productive".
3.2 Key Ideas in Load Shifting

Figure 4 is the high-level pseudo code of our algorithm.
The details will be explained later. The core of the place()
function is a loop that incrementally optimizes the place-
ment solution. Inside the loop, it (cid:12)rst solves the max-(cid:13)ow
problem [2] in Figure 3 to compute the maximum total de-
mand ^! that can be satis(cid:12)ed by the current placement ma-
trix I. Among many possible load distribution matrices L
that can meet this maximum demand ^!, we employ sev-
eral load-shifting heuristics to (cid:12)nd the one that makes later
placement changes easier.

(cid:15) We classify the running instances of an application into
three categories: idle, underutilized, and fully utilized.
The idle instances are preferred candidates to be shut
down. We opt for leaving the fully utilized instances
intact as they already make good contributions.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications333(cid:15) Through proper load shifting, we can ensure that every
application has at most one underutilized instance in
the entire system. Reducing the number of underuti-
lized instances simpli(cid:12)es the placement problem, be-
cause the strategy to handle idle instances and fully
utilized instances are straightforward.

(cid:15) We strive to co-locate residual memory and residual
CPU on the same machines so that these resources
can be used to start new application instances. For
example, if one machine has only residual CPU while
another machine has only residual memory, neither of
them can accept new applications.

(cid:15) We strive to make idle application instances appear on
machines with relatively more residual memory. By
shutting down the idle instances, more memory will
become available for hosting applications that require
a large amount of memory.

3.3 Key Ideas in Placement Changing

The load-shifting subroutine in Figure 4 prepares the load
distribution in a way that makes later placement changes
easier. The placement-changing subroutine further employs
several heuristics to increase the total satis(cid:12)ed application
demand, to reduce placement changes, and to reduce com-
putation time.

(cid:15) The algorithm walks through the underutilized ma-
chines sequentially and makes placement changes to
them one by one in an isolated fashion. When work-
ing on a machine n, the algorithm is only concerned
with the sate of machine n and the residual applica-
tion demands. This isolation drastically reduces the
computation time.

(cid:15) The isolation of machines, however, may lead to in-
ferior placement solutions. We address this problem
by alternately executing the load-shifting subroutine
and the placement-changing subroutine for multiple
rounds. As a result, the residual application demands
released from the application instances stopped in the
previous round now have the chance of being allocated
to other machines in the later rounds.

(cid:15) When sequentially walking through the underutilized
machines, the algorithm (cid:12)rst considers machines with
a relatively high CPU-memory ratio (see the de(cid:12)nition
in Section 3.1). As it is harder to fully utilize the CPU
of these machines, we prefer to process them (cid:12)rst when
we still have abundant choices.

(cid:15) When choosing applications to run on a machine, the
algorithm tries to (cid:12)nd a combination of applications
that lead to the highest CPU utilization of this ma-
chine. It prefers to stop \unproductive" running ap-
plication instances with a relatively low load-memory
ratio to accommodate new application instances.

(cid:15) To reduce placement changes, the algorithm does not
allow stopping application instances that already de-
liver a \su(cid:14)ciently high" load. We refer to these in-
stances as pinned instances. The intuition is that, even
if we stop these instances on their current hosting ma-
chines, it is likely that we will start instances of the
same applications on other machines. Our algorithm
dynamically computes the pinning threshold for each
application.

(cid:1)

(cid:5)

(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)
(cid:1)(cid:2)(cid:2)(cid:3)(cid:5)
(cid:1)(cid:2)(cid:2)(cid:3)(cid:6)

(cid:1)(cid:2)(cid:2)(cid:3)(cid:19)

(cid:2)

(cid:3)

(cid:7) (cid:1)(cid:8)

(cid:7) (cid:1)(cid:8)

(cid:7) (cid:1)(cid:8)

(cid:12) (cid:3)(cid:13)

(cid:12) (cid:3)(cid:20)

(cid:12) (cid:3)(cid:21)

(cid:4) (cid:1)(cid:2)(cid:3)(cid:1)

(cid:6) (cid:1)(cid:2)(cid:3)(cid:2)

(cid:7) (cid:1)(cid:2)(cid:3)(cid:3)

Figure 3: This (cid:12)gure shows two network (cid:13)ow prob-
lems.
(1) When the link costs (i.e., rA=1, rB =0, and
rC =2) are not used, this (cid:12)gure is an example of the max-
(cid:13)ow problem whose solution gives the maximum total
demand that can be satis(cid:12)ed by the current placement
matrix I.
(2) When the link costs are used, it is an
example of the min-cost max-(cid:13)ow problem solved by the
load-shifting subroutine to compute a load distribution
that makes later placement changes easier.

Below, we present in detail the load-shifting subroutine,
the placement-changing subroutine, and the full placement
algorithm that utilizes these two subroutines.
3.4 The Load-Shifting Subroutine

Given the current application demands, the placement al-
gorithm solves a max-(cid:13)ow problem [2] to derive the max-
imum total demand that can be satis(cid:12)ed by the current
placement matrix I. Figure 3 is an example of this max-
(cid:13)ow problem, in which we consider four applications (w, x,
y, and z) and three machines (A, B, and C). Each applica-
tion is represented as a node in the graph. Each machine is
also represented as a node. In addition, there are a source
node and a sink node. The source node has an outgoing
link to each application m, and the capacity of this link is
the CPU demand of the application (!m). Each machine n
has an outgoing link to the sink node, and the capacity of
this link is the CPU capacity of the machine ((cid:10)n). The last
set of links are between the applications and the machines
that currently run those applications. The capacity of these
links is unlimited. In Figure 3, application x currently runs
on machines A and B. Therefore, x has two outgoing links:
x!A and x!B.
When load distribution is formulated as this max-(cid:13)ow
problem, the maximum volume of (cid:13)ows going from the source
node to the sink node is the maximum total demand ^! that
can be satis(cid:12)ed by the current placement matrix I. (Recall
that Im;n = 1 if application m is running on machine n. See
Table 1 for the notations.) If all application demands are
satis(cid:12)ed, no placement changes are needed. Otherwise, we
make placement changes in order to satisfy more application
demands. Before doing so, we (cid:12)rst adjust the load distribu-
tion matrix L produced by solving the max-(cid:13)ow problem in
Figure 3. (Recall that Lm;n is the CPU cycles per second
allocated on machine n for application m.) The goal of load
shifting is to achieve the e(cid:11)ects described in Section 3.2, e.g.,
co-locating residual CPU and residual memory on the same
set of machines, and ensuring that each application has at
most one underutilized instance in the entire system.

The task of load shifting is accomplished by solving the
min-cost max-(cid:13)ow problem [2] in Figure 3. We sort all the
machines in increasing order of residual memory capacity
(cid:0)(cid:3)
n, and associate each machine n with a rank rn that re(cid:13)ects
its position in this sorted list. The machine with rank 0 has
the least amount of residual memory. In Figure 3, the link

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications334(cid:9)
(cid:10)
(cid:11)
(cid:14)
(cid:15)
(cid:16)
(cid:17)
(cid:8)
(cid:12)
(cid:14)
(cid:10)
(cid:11)
(cid:18)
(cid:9)
(cid:10)
(cid:11)
(cid:9)
(cid:10)
(cid:11)
function place ()

f for (i = 0; i < K; i++) f

// K=10 by default.

calc max demand satis(cid:12)ed by current placement ();
if (all demands satis(cid:12)ed) break out of the loop;
load shifting ();
placement changing (pin app=false);
placement changing (pin app=true);
choose the better one as the solution; // Pin or not.
if (no improvement) break out of the loop;

// No placement changes here.

gbalance load across machines ();

g

function placement changing (boolean pin app)

f//------------------------------------------------outermost loop----

// Change the placement on one machine at a time.
for (all underutilized machines n) f

if (pin app==true) identify pinned app instances();
// Suppose machine n currently runs c not-pinned
// app instances (M1, M2, ..., Mc) sorted in
// increasing order of load-memory ratio.

//-----------------------------------------intermediate loop----
for (j=0; j < c; j++) f

if (j > 0) stop j apps on machine n (M1,M2,...,Mj);

//----------------------------------------innermost loop----
// Find apps to consume n’s residual resources
// that become available after stopping the j apps.
for (all apps x with a positive residual demand) f
gif (is the best solution for machine n so far)

if (app x (cid:12)ts on machine n) start x on n ();

record it();

ggg

Figure 4: High-level pseudo code of our algorithm.

between a machine n and the sink node is associated with the
cost rn. The cost of all the other links is zero, which is not
shown in the (cid:12)gure for brevity. In this example, machine C
has more residual memory than machine A, and machine A
has more residual memory that machine B. Therefore, the
links between the machines and the sink node have costs
rB = 0, rA = 1, and rC = 2, respectively.

The load distribution matrix L produced by solving the
min-cost max-(cid:13)ow problem in Figure 3 possesses the fol-
lowing good properties that make later placement changes
easier: (1) An application has at most one underutilized in-
stance in the entire system. (2) Residual memory and resid-
ual CPU are likely to co-locate on the same set of machines.
(3) The idle application instances appear on the machines
with relatively more residual memory.

Theorem 1. In the load distribution matrix L produced
by solving the min-cost max-(cid:13)ow problem in Figure 3, each
application has at most one underutilized instance in the
entire system.

using machine A is lower than that of using machine B
(rA < rB), the min-cost max-(cid:13)ow algorithm can further
reduce the total cost of the maximum (cid:13)ow by moving load
from machine B to machine A, which contradicts with the
fact that the current solution given by the min-cost max-(cid:13)ow
algorithm already has the lowest cost.
2

Theorem 2. In the load distribution matrix L produced
by solving the min-cost max-(cid:13)ow problem in Figure 3, if ap-
plication m has one underutilized instance running on ma-
chine n, then (1) application m’s idle instances must run
on machines whose residual memory is larger than or equal
to that of machine n; and (2) application m’s fully utilized
instances must run on machines whose residual memory is
smaller than or equal to that of machine n.

Proof: The proof is similar to that for Theorem 1.
3.5 The Placement-Changing Subroutine

2

The placement-changing subroutine takes as input the
current placement matrix I, the load distribution matrix L
generated by the load-shifting subroutine, and the residual
application demands not satis(cid:12)ed by L. It tries to increase
the total satis(cid:12)ed application demand by making placement
changes, for instance, stopping \unproductive" application
instances and starting useful ones.

The main structure of the placement-changing subroutine
consists of three nested loops (see Figure 4). The outermost
loop iterates over the machines and asks the intermediate
loop to generate a placement solution for one machine n
at a time. Suppose machine n currently runs c not-pinned
application instances (M1; M2;(cid:1)(cid:1)(cid:1) ; Mc) sorted in increas-
ing order of load-memory ratio (see the de(cid:12)nition in Sec-
tion 3.1). The intermediate loop iterates over a variable j
(0(cid:20) j(cid:20) c). In iteration j, it stops on machine n the j appli-
cations (M1; M2;(cid:1)(cid:1)(cid:1) ; Mj) while keeping the other running
applications intact, and then asks the innermost loop to (cid:12)nd
appropriate applications to consume machine n’s residual
resources. The innermost loop walks through the residual
applications, and identi(cid:12)es those that can (cid:12)t on machine n.
As the intermediate loop varies the number of stopped ap-
plications from 0 to c, it collects c + 1 di(cid:11)erent placement
solutions for machine n, among which it picks the best one
as the (cid:12)nal solution.

Below, we describe the three nested loops in detail.
The Outermost Loop. Before entering the outermost
loop, the algorithm (cid:12)rst computes the residual CPU demand
of each application. We refer to the applications with a posi-
tive residual CPU demand (i.e., !(cid:3)
m > 0) as residual applica-
tions. The algorithm inserts all the residual applications into
a right-threaded AVL tree called residual app tree. The
applications in the tree are sorted in decreasing order of
residual demand. As the algorithm progresses, the residual
demand of applications may change, and the tree is updated
accordingly. The algorithm also keeps track of the minimum
memory requirement (cid:13)min of applications in the tree,

(cid:13)min =

min

m 2 residual app tree

(cid:13)m;

(12)

Proof: We prove this by contradiction. Suppose there are
two underutilized instances of the same application running
on two underutilized machines A and B, respectively. With-
out loss of generality, we assume that machine A has less
residual memory than machine B, i.e., rA < rB. Because
machine A still has residual CPU capacity and the cost of

where (cid:13)m is the memory needed to run one instance of ap-
plication m. The algorithm uses (cid:13)min to speed up the com-
putation in the innermost loop. If a machine n’s residual
memory (cid:0)(cid:3)
n is smaller than (cid:13)min, the algorithm can immedi-
ately infer that this machine cannot accept any applications
in the residual app tree.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications335The Intermediate Loop.

The algorithm excludes fully utilized machines from the
consideration of placement changes, and sorts the underuti-
lized machines in decreasing order of CPU-memory ratio.
Starting from the machine with the highest CPU-memory
ratio, it enumerates each underutilized machine, and asks
the intermediate loop to compute a placement solution for
each machine. Because it is harder to fully utilize the CPU
of machines with a high CPU-memory ratio, we prefer to
process them (cid:12)rst when we still have abundant choices.
input

the
residual app tree and a machine n given by the outermost
loop, the intermediate loop computes a placement solution
for machine n. Suppose machine n currently runs c not-
pinned application instances. (Application instance pinning
will be discussed later.) We can stop a subset of the c ap-
plications, and use the residual resources to run other ap-
plications. In total, there are 2c cases to consider. We use
a heuristic to reduce this number to c + 1. Intuitively, we
prefer to stop the less \productive" application instances,
i.e., those with a low load-memory ratio (Lm;n=(cid:13)m).

Taking as

The algorithm sorts the not-pinned application instances
on machine n in increasing order of load-memory ratio. Let
(M1; M2;(cid:1)(cid:1)(cid:1) ; Mc) denote this sorted list. The intermediate
loop iterates over a variable j (0 (cid:20) j (cid:20) c).
In iteration j,
it stops on machine n the j applications (M1; M2;(cid:1)(cid:1)(cid:1) ; Mj)
while keeping the other running applications intact, and
then asks the innermost loop to (cid:12)nd appropriate applica-
tions to consume machine n’s residual resources that become
available after stopping the j applications. As the interme-
diate loop varies the number of stopped applications from 0
to c, it collects c + 1 placement solutions, among which it
picks as the (cid:12)nal solution the one that leads to the highest
CPU utilization of machine n.

The Innermost Loop. The intermediate loop changes
the number of applications to stop. The innermost loop
uses machine n’s residual resources to run some residual ap-
plications. Recall that the residual app tree is sorted in
decreasing order of residual CPU demand. The innermost
loop iterates over the residual applications, starting from the
one with the largest amount of residual demand. When an
application m is under consideration, the algorithm checks
two conditions: (1) whether the restriction matrix R allows
application m to run on machine n, and (2) whether ma-
chine n has su(cid:14)cient residual memory to host application
m (i.e., (cid:13)m (cid:20) (cid:0)(cid:3)
n). If both conditions are satis(cid:12)ed, it places
application m on machine n, and assigns as much load as
possible to this instance until either machine n’s CPU is
fully utilized or application m has no residual demand. Af-
ter this allocation, m’s residual demand changes, and the
residual app tree is updated accordingly.

The innermost loop iterates over the residual applications
until either (1) all the residual applications have been con-
sidered once; or (2) machine n’s CPU becomes fully utilized;
or (3) machine n’s residual memory is insu(cid:14)cient to host any
residual application (i.e., (cid:0)(cid:3)

n < (cid:13)min, see Equation 12).

3.6 The Full Placement Algorithm

The full placement algorithm is outlined in Figure 4. It
incrementally optimizes the placement solution in multiple
rounds. In each round, it (cid:12)rst invokes the load-shifting sub-
routine and then invokes the placement-changing subrou-
tine.
It repeats for up to K rounds, but quits earlier it
sees no improvement in the total satis(cid:12)ed application de-
mand after one round of execution. The last step of the

algorithm balances the load across machines. We reuse the
load-balancing component from an exiting algorithm [6], but
omit its detail here. Intuitively, it moves the new application
instances between machines to balance the load, while keep-
ing the total satis(cid:12)ed demand and the number of placement
changes the same.

The placement algorithm deals with multiple optimiza-
tion objectives. In addition to maximizing the total satis-
(cid:12)ed demand, it also strives to minimize placement changes,
because they disturb the running system and waste CPU
cycles. Our heuristic for reducing unnecessary placement
changes is not to stop application instances whose load (in
the load distribution matrix L) is above certain threshold.
We refer to them as pinned instances. The intuition is that,
even if we stop these \productive" instances on their current
hosting machines, it is likely that we will start instances of
the same applications on other machines.

Each application m has its own pinning threshold !pin
m .
The value of this threshold is crucial.
If it is too low,
the algorithm may introduce many unnecessary placement
changes. If it is too high, the total satis(cid:12)ed demand may
be low due to insu(cid:14)cient placement changes. The algo-
rithm dynamically computes the pinning threshold for each
application using information gathered in a dry-run invo-
cation to the placement-changing subroutine. The dry run
pins no application instances. After the dry run, the algo-
rithm makes a second invocation to the placement-changing
subroutine, and requires pinning the application instances
whose load is higher than or equal to the pinning threshold
of the corresponding application, i.e., Lm;n (cid:21) !pin
m . The dry
run and the second invocation use exactly the same inputs:
the matrices I and L produced by the load-shifting subrou-
tine. Between the two placement solutions produced by the
dry run and the second invocation, the algorithm picks as
the (cid:12)nal solution the one that has a higher total satis(cid:12)ed
demand. If the total satis(cid:12)ed demands are equal, it picks
the one that has less placement changes.

Next, we describe how to compute the pinning threshold
!pin
m using information gathered in the dry run. Intuitively,
if the dry run starts a new application instance, then we
should not stop any instance of the same application whose
load is higher than or equal to that of the new instance. This
is because the new instance’s load is considered su(cid:14)ciently
high by the dry run so that it is even worthwhile to start a
new instance. Let !new
m denote the minimum load assigned
to a new instance of application m started in the dry run.
!new
fLm;n after the dry rung
Im;n 2 fnew instances of app m started in the dry rung
(13)
Here Im;n represents a new instance of application m started
on machine n in the dry run. Lm;n is the load of this in-
stance. In addition, the pinning threshold also depends the
largest residual application demand !(cid:3)
max not satis(cid:12)ed in
the dry run.

m =

min

!(cid:3)

max =

max

!(cid:3)

m

(14)

m 2 fresidual app tree after the dry rung

Here !(cid:3)
m is the residual demand of application m after the
dry run. We should not stop the application instances whose
load is higher than or equal to !(cid:3)
max. If we stop these in-
stances, they would immediately become the applications
that we try to (cid:12)nd a place to run. The pinning threshold
for application m is computed as follows.

m = max (1; min (!(cid:3)
!pin

max; !new

m ))

(15)

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications336Because we do not want to pin completely idle application
instances, Equation 15 stipulates that the pinning threshold
!pin
m should be at least one CPU cycle per second.
3.7 Complexity and Practical Issues

The computation time of our placement algorithm is dom-
inated by the time spent on solving the max-(cid:13)ow problem
and the min-cost max-(cid:13)ow problem in Figure 3. One ef-
(cid:12)cient algorithm for solving the max-(cid:13)ow problem is the
highest-label pre(cid:13)ow-push algorithm [2], whose complexity
is O(s2pt), where s is the number of nodes in the graph,
and t is the number of edges in the graph. One e(cid:14)cient
algorithm for solving the min-cost (cid:13)ow problem is the en-
hanced capacity scaling algorithm [2], whose complexity is
O((s log t)(s + t log t)). Let N denote the number machines.
Due to various resource constraints, the number of appli-
cations that a machine can run is bounded by a constant.
Therefore, in the network (cid:13)ow graph, both the number s of
nodes and the number t of edges are bounded by O(N ). The
total number of application instances in the entire system is
also bounded by O(N ).

Under these assumptions, the complexity of our place-
ment algorithm is O(N 2:5). In contrast, under the same as-
sumptions, the complexity of the state-of-the-art placement
algorithm proposed by Kimbrel et al. [6, 8] is O(N 3:5). This
di(cid:11)erence in complexity is the reason why our algorithm
can do online computation for systems with thousands of
machines, while their algorithm can scale to at most a few
hundred machines (see the results in Section 4).

For the sake of brevity, our formulation of the placement
problem and our algorithm description omit several practi-
cal issues. For instance, an administrator may impose re-
strictions on the minimum/maximum number of instances
of a given application allowed in the entire system.
It is
also possible that multiple instances of the same application
need to run on a single machine because, for example, one
instance of the application cannot utilize all the CPU power
of the machine due to internal bottlenecks in the applica-
tion. Moreover, the optimization objective can be maxi-
mizing certain utility function instead of maximizing the
total satis(cid:12)ed application demand. Finally, the actual start
and stop of application instances should be carefully coordi-
nated to implement a fast transition and avoid stopping all
instances of an application at the same time. One version of
our algorithm adopted in a leading commercial middleware
product [1] addresses these practical issues, but we omit a
detailed discussion here due to space limitations.
4. EXPERIMENTAL RESULTS

This section studies the performance of our placement
algorithm, and compares it with the state-of-the-art algo-
rithm [6, 8] proposed by Kimbrel et al. We use this algo-
rithm as the baseline because the evaluation [8] shows that
it outperforms two variants of another state-of-the-art algo-
rithm [12, 13]. For brevity, we simply refer to our algorithm
and the algorithm proposed by Kimbrel et al. as the \new"
and \old" algorithms, respectively. Because the two algo-
rithms use the same technique for load balancing, we omit
its results here, and refer interested readers to [6].

Both algorithms have been implemented in a leading com-
mercial middleware product [1]. In this paper, we evaluate
only the placement controller component (as opposed to the
entire middleware), by feeding a wide variety of workloads
directly to the placement algorithms. Some of the workloads
are representative of real-world tra(cid:14)c (e.g., application de-

mands that follow a power-law distribution), while others
are extreme con(cid:12)gurations for stress test. In all the experi-
ments, we assume no placement restriction for applications,
i.e., 8m 2 M 8n 2 N Rm;n = 1.
The placement controller works in cycles. At the begin-
ning of a cycle, the placement algorithm is given a set of
machines, a set of applications, the current demands of the
applications, and the placement matrix I left from the pre-
vious cycle. The placement algorithm then produces a new
placement matrix I and a load distribution matrix L, which
are used for performance evaluation. The evaluation metrics
include the execution time, the number of placement changes
(i.e., application starts/stops), and the demand satisfaction
(i.e., the fraction of the total application demands satis(cid:12)ed
by the placement solution:

Pm2M Pn2N Lm;n

).

Pm2M !m

In the experiments, the con(cid:12)guration of machines is uni-
formly distributed over the set f1GB:1GHz, 2GB:1.6GHz,
3GB:2.4GHz, 4GB:3GHzg, where the (cid:12)rst number is mem-
ory capacity and the second number is CPU speed. The
memory requirement of applications is uniformly distributed
over the set f0.4GB, 0.8GB, 1.2GB, 1.6GBg. A system con-
(cid:12)guration includes a (cid:12)xed set of machines and applications.
All the reported data are averaged over the results on 100
randomly generated system con(cid:12)gurations. For each con-
(cid:12)guration, the placement algorithm executes for 11 cycles
(including an initial placement) under changing application
demands. Hence, each reported data point is averaged over
1,000 placement results (excluding the initial placement).

4.1 Problem Hardness

We compare the algorithms while varying the size of the
placement problem (i.e., the number of machines and appli-
cations) and the hardness of the problem. The hardness is
de(cid:12)ned from four dimensions: CPU load, memory load, ap-
plication CPU demand distribution, and demand variability.
CPU Load Factor Lcpu. It is de(cid:12)ned as the ratio be-
tween the total CPU demand and the total CPU capacity,
, where !m is the CPU demand for appli-
Lcpu =
cation m, and (cid:10)n is the CPU capacity of machine n.

Pm2M !m
Pn2N

(cid:10)

n

Memory Load Factor Lmem. Let (cid:13) denote the av-
erage memory requirement of applications, and (cid:0) denote
the average memory capacity of machines. The average
number of application instances that can be hosted on N
machines is (cid:0)
(cid:13) N . The memory load factor is de(cid:12)ned as
(cid:13) N = M (cid:13)
Lmem = M= (cid:0)
N (cid:0) , where M is the number of applica-
tions. Note that 0 (cid:20) Lmem (cid:20) 1 and the problem is most
di(cid:14)cult when Lmem = 1.
In the experiments, we vary the CPU load factor Lcpu, the
memory load factor Lmem, and the number N of machines.
The number M of applications is con(cid:12)gured according to N
and Lmem: M = (cid:0)

(cid:13) N Lmem = 2:5N Lmem.

Application Demand Distribution. Once the CPU
load factor Lcpu and the total CPU capacity (cid:10) = Pn2N (cid:10)n
are determined, the total application CPU demand is set to
(cid:10)(cid:1)Lcpu. We experiment with two di(cid:11)erent ways of partition-
ing this total demand among applications. With the uniform
distribution, each application’s initial demand is generated
uniformly at random from the range [0; 1]. With the power-
law distribution, application m’s initial demand is set to j (cid:0)(cid:11),
where (cid:11) = 2:16 and j is application m’s rank in a random
permutation of all the applications. For both uniform and
power-law distributions, the demand of each application is
normalized proportionally to the total application demand.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications337Application Demand Variability. Given a system
con(cid:12)guration, the placement algorithm executes for 11 cy-
cles. The application demands in the (cid:12)rst cycle follow either
a uniform distribution or a power-law distribution. Starting
from the second cycle, the application demands change from
cycle to cycle. The placement problem is harder to solve if
this change is drastic. We experiment with four di(cid:11)erent
demand changing patterns. With the vary-all-apps pattern,
each application’s demand changes randomly and indepen-
dently within a (cid:6)20% range of its initial demand. With
the vary-two-apps pattern, we keep the demands of all the
applications constant except for the two applications with
the largest demands. The sum of these two applications’
demands is kept constant, but the allocation between them
randomly changes 10% from cycle to cycle. With the reset-
all-apps pattern, the demands in two consecutive cycles are
independent of each other. This \unrealistic" pattern repre-
sents the most extreme demand change. With the add-apps
pattern, the placement algorithm executes for M placement
cycles, where M is the number of applications. Starting with
an empty, idle system, the demand for one new application
is introduced into the system in every cycle.

Below, we concisely represent the hardness of a placement
problem as (Lcpu, Lmem, \demand-distribution", \demand-
variability"), e.g., (Lcpu=0.9, Lmem=0.4, power-law-apps,
vary-all-apps).

4.2 Performance Results

To choose the best algorithm for the commercial prod-
uct [1], we compared more than a dozen di(cid:11)erent variants
of the placement algorithms (including some variants not
described in this paper), and generated thousands of per-
formance graphs. Due to space limitations, we present in
this paper only some representative results. Overall, when
the placement problem is easy to solve (i.e., the system has
abundant resources), both algorithms can satisfy almost all
the application demands. However, when the placement
problem is hard, the new algorithm signi(cid:12)cantly and con-
sistently outperforms the old algorithm.

Figure 5 shows the execution time of the two algorithms
for the setting (Lcpu=0.99, Lmem=1, uniform-apps, reset-
all-apps). For hard problems, the execution time of the new
algorithm is almost negligible compared with that of the old
algorithm. As an online controller, the old algorithm can
only scale to at most a few hundred machines and appli-
cations, while the new algorithm can scale to thousands of
machines and applications.

Figure 6 reports results on the scalability of the new algo-
rithm. We vary the number of machines from 100 to 7,000
and the number of applications from 250 to 17,500. The
new algorithm takes less than 30 seconds to solve the dif-
(cid:12)cult 7,000-machine, 17,500-application placement problem
under the tight resource constraints and the extreme de-
mand changes from cycle to cycle. This execution time is
measured on a 1.8GHz Pentium IV machine. The demand
satisfaction in Figure 6 stays around 0.946 as the system
size increases, showing that the new algorithm can produce
high-quality solutions regardless of the problem size. The
number of placement changes is high because this \reset-
all-apps" con(cid:12)guration for stress test unrealistically changes
application demands between cycles in an extreme manner.
The experiment in Figure 7 introduces the demand for one
new application into a 100-machine system in every place-
ment cycle. This (cid:12)gure reports the number of placement

changes occurred when adding the i-th application rather
than the aggregated number of placement changes occurred
before adding the (i+1)-th application. Because the re-
sources are not very tight (Lcpu=0.9 and Lmem=0.4), both
algorithms can satisfy all the demands, but the old algorithm
introduces a much larger number of placement changes. For
example, to handle the added demand for the last applica-
tion, the old algorithm makes 51.3 placement changes on
average, while the new algorithm makes only 1.6 placement
changes on average.
In a real system, this di(cid:11)erence can
have dramatic impact on the whole system performance.

The experiments in Figures 8, 9, and 10 use di(cid:11)erent com-
binations of CPU load factor (Hcpu=0.99 or 0.6), memory
load factor (Hmem=1 or 0.6), demand distribution (uniform
or power-law), and demand variability (vary-two-apps, vary-
all-apps, or reset-all-apps). Under all these settings, the
new algorithm consistently outperforms the old algorithm:
it improves demand satisfaction by up to 25%, and reduces
placement changes by up to 90%.

The experiment in Figure 11 varies the memory load fac-
tor Lmem from 0.1 to 1 for a 100-machine system, while (cid:12)x-
ing the CPU load factor Lcpu=0.9. As the hardness of the
problem increases, the demand satisfaction of the old algo-
rithm drops faster than that of the new algorithm. More
importantly, the number of placement changes in the old
algorithm increases dramatically. The experiment in Fig-
ure 12 varies the CPU load factor Lcpu from 0.1 to 1 for
a 100-machine system, while (cid:12)xing the memory load factor
Lmem=0.4. The old algorithm and the new algorithm have
similar performance when the CPU load factor is below 0.8.
However, when the CPU load factor is between 0.8 and 0.9,
the number of placement changes in the old algorithm in-
creases almost exponentially. The situation could get even
worse as the CPU load factor further approaches 1. To deal
with this pathological case, the improved version [6] of the
old algorithm (the version used in the comparison) added a
90% load reduction heuristic|whenever the CPU load fac-
tor is above 0.9, the old algorithm (cid:12)rst (arti(cid:12)cially) reduces
it to 0.9 by scaling all the application demands proportion-
ally, and then executes the core of the algorithm on the
reduced demands. This heuristic helps the old algorithm to
reduce placement changes, but it also decreases the demand
satisfaction (see the dip in Figure 12). In contrast, the new
algorithm achieves a better performance even without using
such hard-coded rules to handle the corner cases.

In summary, the new algorithm signi(cid:12)cantly and consis-
tently outperforms the old algorithm in all three aspects: ex-
ecution time, demand satisfaction, and placement changes.
The new algorithm’s ability to achieve a higher demand sat-
isfaction is mainly owing to its load-shifting heuristics and
the strategy that (cid:12)rst does placement changes to the ma-
chines with a high CPU-memory ratio. The new algorithm’s
fast speed is mainly owing to the strategy that does place-
ment changes to machines one by one in an isolated fashion.
Two heuristics in the new algorithm help reduce placement
changes: application instance pinning and machine isola-
tion. For hard placement problems, the old algorithm may
simultaneously free a large number of application instances
on di(cid:11)erent machines, and then try to place them, which
may produce solutions that simply shu(cid:15)e application in-
stances across machines (see Figure 7).
In contrast, due
to the new algorithm’s machine isolation strategy, it never
simultaneously frees application instances running on di(cid:11)er-
ent machines and hence avoids unnecessary shu(cid:15)ing.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications338new
old

)
s

m

(
 
e
m

i
t
 

n
o
i
t
u
c
e
x
E

 2500

 2000

 1500

 1000

 500

 0

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

5:

Figure
Execution
time with con(cid:12)guration
(Lcpu=0.99,
Lmem=1,
uniform-apps,
reset-all-
apps).

)
c
e
s
(
 
e
m

i
t
 
n
o
i
t
u
c
e
x
E

 30
 25
 20
 15
 10
 5
 0

n
o
i
t
a
c
i
f
s
i
t
a
s
 
d
n
a
m
e
D

 0.95

 0.948

 0.946

 0.944

 0.942

 0.94

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l

P

 12000
 10000
 8000
 6000
 4000
 2000
 0

0
 

0
0
0
1
 

0
0
0
2
 

0
0
0
3
 

0
0
0
4
 

0
0
0
5
 

0
0
0
6
 

0
0
0
7
 

Machines

0
 

0
0
0
1
 

0
0
0
2
 

0
0
0
3
 

0
0
0
4
 

0
0
0
5
 

0
0
0
6
 

0
0
0
7
 

Machines

0
 

0
0
0
1
 

0
0
0
2
 

0
0
0
3
 

0
0
0
4
 

0
0
0
5
 

0
0
0
6
 

0
0
0
7
 

Machines

Figure 6: Scalability of the new algorithm with con(cid:12)guration (Lcpu=0.99,
Lmem=1, uniform-app, reset-all-apps).

n
o
i
t
a
c
i
f
s
i
t
a
s
 

d
n
a
m
e
D

 1.01

 1.005

 1

 0.995

 0.99

 0.985

new
old

new
old

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l

P

 60
 50
 40
 30
 20
 10
 0

0
8

 

5
8

 

0
9

 

5
9

 

Added applications

0
0
1

 

0
8

 

5
8

 

0
9

 

5
9

 

Added applications

0
0
1

 

n
o
i
t
a
c
i
f
s
i
t
a
s
 

d
n
a
m
e
D

 1
 0.99
 0.98
 0.97
 0.96
 0.95
 0.94
 0.93
 0.92
 0.91
 0.9

new
old

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l

P

 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0

new
old

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

Figure 7: Demand satisfaction and placement
changes with con(cid:12)guration: (Lcpu=0.9, Lmem=0.4,
uniform-app, add-apps), 100 machines, and 100
applications.

Figure 8: Demand satisfaction and placement
changes with con(cid:12)guration (Lcpu=0.99, Lmem=1,
uniform-app, vary-two-apps).

n
o
i
t
a
c
i
f
s
i
t
a
s
 

d
n
a
m
e
D

 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65

new
old

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l
P

 70
 60
 50
 40
 30
 20
 10
 0

new
old

n
o
i
t
a
c
i
f
s
i
t
a
s
 

d
n
a
m
e
D

 1

 0.995

 0.99

 0.985

 0.98

new
old

new
old

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l
P

 50
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

0
1

 

0
2

 

0
3

 

0
4

 

0
5

 

0
6

 

0
7

 

0
8

 

0
9

 

Machines

0
0
1

 

Figure 9: Demand satisfaction and placement
changes with con(cid:12)guration (Lcpu=0.99, Lmem=1,
power-law-app, vary-all-apps).

Figure 10: Demand satisfaction and placement
changes with con(cid:12)guration (Lcpu=0.9, Lmem=0.6,
uniform-app, vary-all-apps).

n
o
i
t
a
c
i
f
s
i
t
a
s
 

d
n
a
m
e
D

 1
 0.99
 0.98
 0.97
 0.96
 0.95
 0.94

new
old

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l
P

 450
 400
 350
 300
 250
 200
 150
 100
 50
 0

new
old

1

.

0
 

2

.

0
 

3

.

0
 

4

.

0
 

5

.

0
 

6

.

0
 

7

.

0
 

8

.

0
 

9

.

0
 

1

 

Memory Load Factor (Lmem)

1

.

0
 

2

.

0
 

3

.

0
 

4

.

0
 

5

.

0
 

6

.

0
 

7

.

0
 

8

.

0
 

9

.

0
 

1

 

Memory Load Factor (Lmem)

n
o
i
t
a
c
i
f
s
i
t
a
s
 

d
n
a
m
e
D

 1

 0.99

 0.98

 0.97

 0.96

 0.95

new
old

s
e
g
n
a
h
c
 
t
n
e
m
e
c
a
l
P

 60
 50
 40
 30
 20
 10
 0

new
old

1

.

0

 

2

.

0

 

3

.

0

 

4

.

0

 

5

.

0

 

6

.

0

 

7

.

0

 

8

.

0

 

9

.

0

 

1

 

CPU Load Factor (Lcpu)

1

.

0

 

2

.

0

 

3

.

0

 

4

.

0

 

5

.

0

 

6

.

0

 

7

.

0

 

8

.

0

 

9

.

0

 

1

 

CPU Load Factor (Lcpu)

Figure 11: Vary Lmem from 0.1 to 1. Con(cid:12)gura-
tion: (Lcpu=0.9, Lmem=X, uniform-apps, reset-all-
apps), 100 machines.

Figure 12: Vary Lcpu from 0.1 to 1. Con(cid:12)guration:
(Lcpu=X, Lmem=0.4, uniform-apps, reset-all-apps),
100 machines.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications3395. RELATED WORK

The problem of dynamic application placement in response
to changes in application demands have been studied before.
The algorithm proposed by Kimbrel et al. [6, 8] is the closest
to our work. The comparison in Section 4 shows that our
algorithm signi(cid:12)cantly and consistently outperforms this al-
gorithm. The biggest di(cid:11)erence between the two algorithms
is that, in the previous algorithm, the placement decisions
for individual machines are not isolated|stopping one appli-
cation instance on one machine may lead to reconsideration
of the placement decisions for all the other machines.

A popular approach to dynamic server provisioning is to
allocate full machines to applications as needed [3], which
does not allow applications to share machines. In contrast,
our placement controller allows this sharing and is optimized
for it. The algorithm proposed by Urgaonkar et al. [17] al-
lows applications to share machines, but it does not dynam-
ically change the number of instances of an application, does
not try to minimize placement changes, and only considers
a single bottleneck resource.

Placement problems have also been studied in the opti-
mization literature, including bin packing, multiple knap-
sack, and multi-dimensional knapsack problems [7]. The
special case of our problem with uniform memory require-
ments was studied in [12, 13], and some approximation algo-
rithms were proposed. These algorithms have been shown to
be inferior to the algorithm proposed by Kimbrel et al. [8].
Our algorithm further signi(cid:12)cantly outperforms an improved
version [6] of the algorithm proposed by Kimbrel et al.

One limitation of our algorithm is that it makes no at-
tempt to co-locate on the same machine the set of applica-
tions that have high-volume internal communication. This
issue has been studied before [5, 15], but it still remains
a challenge to design for commercial product a fully auto-
mated algorithm that does not rely on manual o(cid:15)ine pro(cid:13)ing.

6. CONCLUSION

In this paper, we proposed an application placement con-
troller that dynamically starts and stops application instances
in response to changes in application demands.
It allows
multiple applications to share a single machine. Under mul-
tiple resource constraints, it strives to maximize the total
satis(cid:12)ed application demand, to minimize the number of
application starts and stops, and to balance the load across
machines. It signi(cid:12)cantly and consistently outperforms the
existing state-of-the-art algorithm [6, 8]. Compared with [6,
8], for systems with 100 machines or less, our algorithm is up
to 134 times faster, reduces unnecessary application starts
and stops by up to 97%, and produces solutions that satisfy
up to 25% more application demands.

We believe that our algorithm is the (cid:12)rst online algo-
rithm that, under multiple tight resource constraints, can
e(cid:14)ciently produce high-quality solutions for hard placement
problems with thousands of machines and thousands of ap-
plications. This scalability is crucial for dynamic resource
provisioning in large-scale enterprise data centers. The out-
standing performance of our algorithm stems from our novel
optimization techniques such as application pinning, load
shifting, and machine isolation. Our algorithm has been im-
plemented and adopted in a leading commercial product [1].

7. REFERENCES
[1] WebSphere Extended Deployment, http://www-

306.ibm.com/software/webservers/appserv/extend/.

[2] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, editors.
Network Flows: Theory, Algorithms, and Applications.
Prentice Hall, New Jersey, 1993. ISBN 1000499012.

[3] K. Appleby, S. Fakhouri, L. Fong, G. Goldszmidt,

M. Kalantar, S. Krishnakumar, D. Pazel, J. Pershing,
and B. Rochwerger. Oceano SLA based management
of a computing utility. In Proceedings of the
International Symposium on Integrated Network
Management, pages 14{18, Seattle, WA, May 2001.
[4] A. Fox, S. D. Gribble, Y. Chawathe, E. A. Brewer,

and P. Gauthier. Cluster-Based Scalable Network
Services. In Symposium on Operating Systems
Principles (SOSP), 1997.

[5] G. C. Hunt and M. L. Scott. The Coign Automatic

Distributed Partitioning System. In OSDI, 1999.
[6] A. Karve, T. Kimbrel, G. Paci(cid:12)ci, M. Spreitzer,

M. Steinder, M. Sviridenko, and A. Tantawi. Dynamic
Application Placement for Clustered Web
Applications. In the International World Wide Web
Conference (WWW), May 2006.

[7] H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack

Problems. Springer{Verlag, 2004.

[8] T. Kimbrel, M. Steinder, M. Sviridenko, and A. N.

Tantawi. Dynamic Application Placement Under
Service and Memory Constraints. In International
Workshop on E(cid:14)cient and Experimental Algorithms,
2005.

[9] R. Levy, J. Nagarajarao, G. Paci(cid:12)ci, M. Spreitzer,

A. N. Tantawi, and A. Youssef. Performance
management for cluster based web services. In
Proceedings of the International Symposium on
Integrated Network Management, 2003.

[10] G. Paci(cid:12)ci, W. Segmuller, M. Spreitzer, M. Steinder,
A. Tantawi, and A. Youssef. Managing the response
time for multi-tiered web applications. Technical
Report RC 23651, IBM, 2005.

[11] G. Paci(cid:12)ci, W. Segmuller, M. Spreitzer, and

A. Tantawi. Dynamic Estimation of CPU Demand of
Web Tra(cid:14)c. In Proceedings of the First International
Conference on Performance Evaluation Methodologies
and Tools (VALUETOOLS), 2006.

[12] H. Shachnai and T. Tamir. Noah’s bagels - some

combinatorial aspects. In Proc. 1st Int. Conf. on Fun
with Algorithms, 1998.

[13] H. Shachnai and T. Tamir. On two class-constrained

versions of the multiple knapsack problem.
Algorithmica, 29(3):442{467, 2001.

[14] K. Shen, H. Tang, T. Yang, and L. Chu. Integrated

Resource Management for Cluster-based Internet
Services. In Proc. of OSDI, 2002.

[15] C. Stewart and K. Shen. Performance Modeling and

System Management for Multi-component Online
Services. In Proc. of the Second USENIX Symposium
on Networked Systems Design and Implementation
(NSDI), 2005.

[16] C. Tang, R. N. Chang, and E. So. A Distributed

Service Management Infrastructure for Enterprise
Data Centers Based on Peer-to-Peer Technology. In
Proc. the International Conference on Services
Computing, 2006. Winner of the Best Paper Award.

[17] B. Urgaonkar, P. Shenoy, and T. Roscoe. Resource

overbooking and application pro(cid:12)ling in shared
hosting platforms. In Proc. of OSDI, 2002.

WWW 2007 / Track: Performance and ScalabilitySession: Performance Engineering of Web Applications340