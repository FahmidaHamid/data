Navigation-Aided Retrieval

Shashank Pandit

Carnegie Mellon University

shashank@cs.cmu.edu

Christopher Olston
Yahoo! Research

olston@yahoo-inc.com

ABSTRACT
Users searching for information in hypermedia environments
often perform querying followed by manual navigation. Yet,
the conventional text/hypertext retrieval paradigm does not
explicity take post-query navigation into account. This pa-
per proposes a new retrieval paradigm, called navigation-
aided retrieval (NAR), which treats both querying and nav-
igation as ﬁrst-class activities. In the NAR paradigm, query-
ing is seen as a means to identify starting points for naviga-
tion, and navigation is guided based on information supplied
in the query. NAR is a generalization of the conventional
probabilistic information retrieval paradigm, which implic-
itly assumes no navigation takes place.

This paper presents a formal model for navigation-aided
retrieval, and reports empirical results that point to the
real-world applicability of the model. The experiments were
performed over a large Web corpus provided by TREC, us-
ing human judgments on a new rating scale developed for
navigation-aided retrieval. In the case of ambiguous queries,
the new retrieval model identiﬁes good starting points for
post-query navigation. For less ambiguous queries that need
not be paired with navigation, the output closely matches
that of a conventional retrieval system.

Categories and Subject Descriptors
H.3 [Information Systems]: Information Storage and Re-
trieval

General Terms
Algorithms, Design, Experimentation

Keywords
Web search, Navigation, Browsing, Link analysis, Under-
speciﬁed search tasks

1.

INTRODUCTION

While performing search tasks1 in the hypertext environ-
ments such as the World Wide Web, users often supplement
querying with extensive manual navigation (i.e., traversal of

1A search task is the quest for information by a person with
insuﬃcient knowledge to solve a certain problem at hand [3].

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

hyperlinks) [30]. There are at least three reasons for using
navigation as a search tactic:

1. Diﬃculty in formulating appropriate queries:
Users often do not convey their search task to a re-
trieval system accurately enough to allow them to forgo
navigation. A variety of factors may be to blame, in-
cluding limited query language expressiveness, user in-
experience, lack of familiarity with terminology, and
cognitive ease of entering short queries.

2. Open-ended search tasks: In many cases the scope
of the task is broad, and the user has not (yet) formed
a concrete notion of what information would lead to
its successful completion. (Indeed there may not even
be a meaningful notion of completion.) For example,
consider a guitar enthusiast who recently moved to a
new city and wishes to ﬁnd out about the city’s guitar
culture and local guitar-related resources. This task’s
scope may include performances, lessons, shops, so-
cial networking, gigs, as well as other aspects the user
discovers in the process of searching (e.g., perhaps he
discovers that a local museum features an exhibit of
musical instruments). Open-ended search tasks often
entail a signiﬁcant amount of manual navigation, as
part of an extended process of exploration, discovery,
and task/query reﬁnement known as berrypicking [2]
or information foraging [29].

3. Preference for orienteering: At times, users prefer
to navigate rather than “teleport” to a target docu-
ment, because doing so enables them to understand
the surrounding context. This behavior is called ori-
enteering [36].

Despite the prevalence of navigation as a search tactic, the
conventional (hyper)text retrieval paradigm focuses uniquely
on querying.
In this paper we approach the combination
of querying and navigation as the unit of interest, in which
querying merely identiﬁes starting points for navigation, and
navigation is guided based on the user’s query.
1.1 Navigation-Aided Retrieval

We propose a new hypertext retrieval paradigm that in-
corporates post-query user navigation as an explicit compo-
nent, called navigation-aided retrieval (NAR). With NAR,
as with the conventional paradigm, users submit freely-chosen
keyword queries and are presented with a ranked list of doc-
uments. Unlike with the conventional paradigm, the docu-
ments in the list represent starting points from which the
user can commence exploration. Loosely speaking, good

WWW 2007 / Track: SearchSession: Search Potpourri391Figure 1: Screenshots of our prototype navigation-aided retrieval system, for query “guitar” over Craigslist
Pittsburgh.

starting points are hypertext documents that, while they
may not match the user’s query directly, permit easy navi-
gation to many documents that do match the query, via one
or more outgoing hyperlink paths (our starting points diﬀer
from Kleinberg’s “hubs”; see Section 2.1).

Figure 1 shows the output of our prototype NAR sys-
tem called Volant for the query “guitar” over a commu-
nity bulletin-board Web site called Craigslist Pittsburgh2.
The upper screenshot shows the initial response page (list
of starting points); the other three show sample content from
each of the top three starting points. Notice how the list of
starting points neatly categorizes the relevant information
available on the site. This categorization is a consequence
of the fact that the Web site is appropriately organized,
combined with the navigation-aware design of the scoring
function used by Volant to identify and rank starting points.
The model underlying the scoring function assumes the user
has a certain propensity to navigate outward from the ini-
tial query results, and that navigation is directed based on
the user’s search task. Importantly, our navigation-aided re-
trieval model strictly generalizes the conventional probabilis-
tic information retrieval model, which implicitly assumes no
propensity to navigate (formal details are provided in Sec-
tion 3).

Coming back to Figure 1, notice that certain hyperlinks
are highlighted (i.e., they have a shaded background). Volant
assists users as they navigate outward from the starting
points by highlighting hyperlinks that lead to one or more

2http://pittsburgh.craigslist.org

documents matching the query (“guitar,” in this case), fol-
lowing [20, 28]. In general, we refer to post-query hyperlink
annotation as navigation guidance, which is the second key
aspect of the NAR paradigm.

In NAR systems, the user may revise his query while nav-
igating. Doing so causes the navigation guidance annota-
tions (e.g., link highlighting) to update in place, without
changing the content on display.3 The user can therefore re-
ﬁne/reformulate his query incrementally, without losing his
bearings.

To summarize,

in light of the fact that users tend to
supplement querying with navigation, navigation-aided re-
trieval ensures that querying and navigation work together,
rather than interfering with each other: The initial query
takes the user to document neighborhoods that are both rel-
evant and navigable. Once there, the user can navigate while
retaining relevance indicators created by querying. Con-
versely, he can re-query while retaining the context arrived
at by navigation.
1.2 Organic versus Synthetic Structure

The importance of supplying context with query results is
well recognized [15]. Most prior work along these lines aims
to synthesize some sort of structure automatically when a
query arrives, to serve as a contextual backdrop for query re-
sults and provide semantically meaningful avenues for explo-
ration. Examples include query result clustering [40, 41] and

3The user can of course request a fresh list of starting points
instead, if desired.

WWW 2007 / Track: SearchSession: Search Potpourri392“faceted” query/navigation interfaces [16, 33]. These ap-
proaches can be characterized as navigation-aided retrieval
with synthetic structure (Synthetic NAR).

In this paper we are primarily interested in navigation-
aided retrieval using organic structure, i.e., structure that
is naturally present in pre-existing hypermedia documents
(Organic NAR). Synthetic and Organic NAR each oﬀer cer-
tain advantages. Clearly, Synthetic NAR is more ﬂexible,
and does not rely on the a priori existence of suitable start-
ing point documents. The principal advantages we perceive
for Organic NAR are:

• Human oversight. Working with pre-existing struc-
ture ensures that a human oversees the way informa-
tion is organized. Therefore it is more likely that cat-
egories make sense, have proper labels, and that each
category has information organized in a useful way
(e.g., Craigslist postings are sorted by date).

• Familiar user interface. In our paradigm, the user
interface departs only slightly from the one users are
already familiar with (i.e., queries return a simple, ﬂat
list of links to documents). Interface complexity can be
dangerous: a user study evaluating a kind of faceted
search interface [33] found that the interface, which
displays several types of information views and naviga-
tion paths simultaneously, required signiﬁcant learning
time, which hampered eﬀectiveness.

• A single view of the document collection. Unlike
with faceted search interfaces, with Organic NAR users
only have to contend with one “view” of the document
collection, and content is presented in the same way
each time they visit (modulo navigation guidance).
Hence users are more easily able to retrace their steps,
a property that is crucial for orienteering.

• Robust implementation by a third party. Our
paradigm based on exploiting pre-existing structure re-
quires no semantic knowledge. Consequently, it is rela-
tively easy to achieve a robust implementation. More-
over, the retrieval service can be provided by a third
party that does not own or understand the corpus.

(A preliminary empirical comparison of Organic and Syn-
thetic NAR is presented in Section 5.5.)
1.3 Contributions

The contributions of this paper are:

• Formal model of navigation-aided retrieval (Section 3).

• Implementation techniques for a NAR-based retrieval

system (Section 4).

• Empirical evaluation via a user study (Section 5).

Related work is discussed next.

2. RELATED WORK

We have already discussed work on presenting query re-
sults in the context of dynamically-synthesized structure,
in Section 1.2. There is a relatively small body of work
that shares our approach of leveraging pre-existing struc-
ture, which we discuss here. We consider two distinct sub-
bodies in turn: (1) ﬁnding starting points for exploration,
and (2) providing guidance during navigation.

2.1 Selecting Starting Points

Best Trails [39] is a retrieval system which selects starting
points in response to queries, however, it restricts them to
be documents matching the query. The scoring function
proposed is ad-hoc in nature and does not take into account
navigability factors, which are central to our work. Further,
its user interface departs substantially from the traditional
query/browse interface and is diﬃcult to use (as reported by
the user study in [39].) In contrast, our prototype Organic
NAR system closely adheres to familiar interfaces oﬀered by
popular query and browse tools.

The hypertext retrieval paradigm known as topic distilla-
tion [5, 7, 8] bears some similarity to NAR. Topic distillation
aims to identify a small number of high-quality documents
that are representative of a broad topic area (e.g., bicycling).
Much of the work to date on topic distillation uses as a prim-
itive the HITS algorithm [22], which identiﬁes bipartite sub-
graphs consisting of hubs (related to our notion of starting
points) and authorities. Under the HITS model, good hubs
are those that have many links to good authorities. Sym-
metrically, good authorities are those that are linked to by
many good hubs.

HITS-based approaches are inherently eﬀective only for
broad topic areas for which there are many hubs and au-
thorities. NAR does not share this limitation, and in prin-
ciple can accommodate arbitrarily narrow search tasks (see
Section 5.3). Furthermore, a NAR starting point is more
general than a HITS hub in the following ways:

• A starting point may be multiple links away from doc-

uments matching the query.

• It is easy to navigate outward from a starting point
(i.e., the link anchor texts are informative and indica-
tive of the documents reachable via the link).

There exist speciﬁc approaches [25, 6] to address each of
these concerns, but none of them is as general as NAR.

The work of [21] aims to identify nodes of maximum inﬂu-
ence in a social network, where the deﬁnition of inﬂuential
nodes is related to our notion of good starting points. As
with all prior work we are aware of, a key diﬀerence is that
our work takes navigability factors into account when eval-
uating potential starting points.

2.2 Guiding Navigation

WebWatcher [20] highlights hyperlinks along paths taken
by previous users who had posed similar queries. This ap-
proach may not be suitable for open-ended search tasks
where the query is not a good representation of the underly-
ing search task. Letizia [24] and Personal WebWatcher [26]
do not rely on queries but instead passively observe the
user’s browsing behavior in order to learn a model of his
search task, and highlight links that match the inferred task.
This approach can in principle be incorporated into a NAR
system, to augment guidance based on explicit queries.

Highlighting of hyperlinks based on an explicit query, as
a method of navigation guidance, was evaluated via a user
study in our previous work [28]. Guided navigation was
found to result in signiﬁcantly faster completion of certain
search tasks compared to traditional query and browsing
interfaces, assuming the user already knows of a suitable
starting point. Automatic identiﬁcation of good starting
points is the focus of the present paper.

WWW 2007 / Track: SearchSession: Search Potpourri393Symbol Meaning

D
T
ST

QT

set of documents in the corpus
user’s search task
answer set for search task T , i.e., the set of
documents any of which is suﬃcient to
complete the search task T
set of valid queries for search task T , i.e., the
(possibly inﬁnite) set of queries that might
be posed by a user performing T

Table 1: Table of symbols used in the NAR model

3. NAVIGATION-AIDED RETRIEVAL MODEL

A navigation-aided retrieval system takes as input a user
query, and returns one of: (1) a synthetic starting point
document (in the case of a Synthetic NAR system), or (2)
an ordered list of links to organic starting points (in the
case of an Organic NAR system). Subsequent navigation on
the part of the user is performed under guidance from the
retrieval system based on the user’s query.

In this paper we focus on Organic NAR. Our aim is to
develop a numeric scoring function for ranking of organic
starting point documents. We ﬁrst supply an abstract for-
mulation (Section 3.1), and then explore concrete instanti-
ations (Section 3.2).
3.1 Generic Model

We now describe the basic framework and terminology un-
derlying the NAR model. A quick reference for the symbols
used is provided in Table 3.1.

Let D be the set of documents in the searchable corpus.
Let T denote the user’s underlying search task, which is un-
known to the retrieval system (and perhaps not fully known
to the user himself at the outset). We assume the user is
able to complete a search task by viewing exactly one doc-
ument4 in D. The answer set (ST ) for a search task T is
deﬁned as the set of all documents, any one of which suﬃces
to complete the search task T . For a given search task T , a
valid query is one which might be posed by a user perform-
ing T . The (potentially inﬁnite) set of all valid queries is
denoted by QT = {q1, q2, . . .}.

The NAR scoring function is based on two distinct “sub-
models” that account for query relevance and user naviga-
tion, respectively:

Query submodel. A query submodel provides a belief
distribution for the answer set ST , given that the user reveals
a query q to be a member of QT . It is used to estimate the
likelihood that a particular document d results in completion
of the task, i.e., P r{d ∈ ST | q ∈ QT }.

Navigation submodel. A navigation submodel assesses
the likelihood with which a user starting from document d is
able to navigate to a document d′ ∈ ST , thereby successfully
completing the search task. To be precise, let d ; d′ denote
the event that a user performing the search task T navigates
successfully from document d to document d′, under guid-
ance G(q) provided for query q. A navigation submodel is
used to estimate P r{d ; d′ | d′ ∈ ST , q ∈ QT }.

4In general, a search task might require visiting multiple
pages for completion; we believe our model can be extended
to handle such situations.

Our goal is to formulate a scoring function, such that doc-
ument d is given a score equal to the probability that the
user completes his search task if he starts navigation at doc-
ument d. We denote the scoring function as σ(d, q). Thus,

σ(d, q)

= P r{ user completes T | user starts navigating at d}

P r{d ; d′, d′ ∈ ST | q ∈ QT }

= Pd′∈D
= Pd′ ∈D » P r{d′ ∈ ST | q ∈ QT } ×

P r{d ; d′ | d′ ∈ ST , q ∈ QT } –
3.2 Instantiations of Generic Model

We supply two instantiations of the generic NAR model
given in Section 3.1: the conventional probabilistic retrieval
model, which does not anticipate navigation outward from
query results (Section 3.2.1), and a new approach that in-
corporates a model of possible post-query user navigation
actions (Section 3.2.2).

3.2.1 Conventional Probabilistic IR Model

Our generic NAR model reduces to the classical proba-

bilistic IR model if we apply the following assumption:

Conventional IR assumption. The user has no propen-
sity to navigate outward from any of the retrieved docu-
ments, i.e.,

P r{d ; d′ | d′ ∈ ST , q ∈ QT } = 1
= 0

if d′ = d
otherwise

Under this assumption, the NAR scoring metric reduces to:

σc(d, q) = P r{d ∈ ST | q ∈ QT }

Recall that P r{d ∈ ST | q ∈ QT } represents the probability
that document d is relevant to the search task underlying
query q. Several methods have been proposed in the proba-
bilistic IR literature for estimating this term, e.g., [23, 35].

3.2.2 Navigation-Conscious Model

Now, we present an instantiation of the NAR model which
takes into account the possibility of the user navigating fur-
ther from the results retrieved for a query. We ﬁrst instan-
tiate the two submodels and then derive a formula for the
scoring function, under these submodels.

Query submodel. Any probabilistic IR relevance ranking
function can be used to estimate the term P r{d′ ∈ ST | q ∈
QT }. We refer to such an estimate as R(d′, q).

Navigation submodel. As mentioned before, the naviga-
tion submodel estimates the probability of a user browsing
from a given document to another document while perform-
ing a search task. To estimate this quantity, we adopt the
stochastic model of user navigation behavior called WUFIS
proposed by Chi et al. [9, 10]. WUFIS is based on the the-
ory of information scent [29], and has been validated against
real user traces [10].

At the heart of WUFIS lies a function W (N, d1, d2), which
gives the probability that a user whose search task is charac-
terized by information need N navigates successfully along
some path from document d1 to document d2 (N is encoded
as a term vector). Every hyperlink, via anchor and sur-
rounding context, provides some information scent regard-

WWW 2007 / Track: SearchSession: Search Potpourri394ing the content reachable by clicking that hyperlink. WUFIS
assumes that the probability of a user following a hyperlink
depends on how well his information need matches the in-
formation scent provided by the hyperlink. The information
scent is approximated by a weighted vector of terms occur-
ring in the anchor text and optionally in a small window
around the hyperlink. The cosine similarity between the in-
formation need vector and the information scent vector is
used to estimate the probability that the user follows the
given hyperlink, along with an assumed attrition probability
0 < α ≤ 1 (set to 0.85 in our experiements). The navigation
probability between two documents d1 and d2 is given by the
product of link navigation probabilities along the sequence
of links connecting d1 to d2.

We propose to use WUFIS as the basis for our navigation
submodel.5 Constructing an estimate of N from a docu-
ment in the answer set ST is a challenge in itself, but rather
orthogonal to the focus of our work. Hence, we take the
naive approach of approximating N by extracting the ﬁrst
k terms from the document. Better ways of extracting the
information need will likely lead to an improvement in per-
formance.

Let N (d) denote the information need extracted from some
d assumed to be a member of ST . We instantiate our navi-
gation submodel by setting P r{d ; d′ | d′ ∈ ST , q ∈ QT } =
W (N (d′), d, d′).

Scoring function. When we combine our query submodel
with our navigation submodel, we arrive at the following
starting point scoring function:

σn(d, q) = Xd′ ∈D

R(d′, q) × W (N (d′), d, d′)

(1)

The above formula makes intuitive sense — the two terms
embody the two key factors in assessing the likelihood of
successful task completion, loosely speaking:

1. the number of documents reachable from d that are

relevant to the search task, and

2. the ease and accuracy with which the user is able to

navigate to those documents.

4. VOLANT: A PROTOTYPE ORGANIC NAR

SYSTEM

We built a prototype Organic NAR system called Volant,
which implements the navigation-conscious model instanti-
ation described in Section 3.2.2. In this section, we describe
its design and implementation.
4.1 Overview

Figure 2 provides an overview of Volant’s architecture.
Volant consists of three components: content engine, con-
nectivity engine, and intermediary. The content engine sup-
ports retrieval and ranking of Web documents, via a text
index. The connectivity engine maintains a specialized in-
dex of linkage between pairs of documents. The intermedi-
ary intercepts HTTP requests from the user and communi-
cates with the connectivity and content engines as well as

5Here we ignore the inﬂuence of navigation guidance, be-
cause we have no reliable basis for assuming any particular
eﬀect. As future work we plan to extend WUFIS to account
for the additional “scent” provided by navigation guidance.

the WWW in order to produce the desired response — ei-
ther starting points or Web pages enhanced with navigation
guidance.

Implementation of a NAR system is not the focus of this
paper, and so we heavily relied on third party softwares
to build Volant. The Lucene text indexing and retrieval
systemserved as Volant’s content engine. The connectivity
server was built on top of a MySQL database [27], while the
intermediary was coded using Java servlets running on an
Apache Tomcat server [37]. Exploring more eﬃcient imple-
mentation options (e.g., the connectivity server in [4]) would
be valuable future work.

Next,

in Section 4.2, we describe the index structures
Volant creates oﬄine via preprocessing the corpus. Then,
we describe how Volant selects starting points in response to
keyword queries (Section 4.3) and adds navigation guidance
to subsequent Web pages that the user visits (Section 4.4).
We discuss performance and scalability issues brieﬂy in Sec-
tion 4.5.
4.2 Preprocessing

4.2.1 Content Engine

As mentioned before, Lucene serves as Volant’s content
engine, and is used to estimate the value of R(d, q), the
probability that document d is relevant to the query q. We
are not aware of any ranking function which can compute
the exact probability of a document being relevant to a
query. However, certain probabilistic models, which produce
a score approximately proportional to logarithm of the rel-
evance probability, have proved to be eﬀective for retrieval.
We follow state-of-the-art and use the Okapi BM25 scoring
function [32] to estimate R(d, q). This approximation can
lead to suboptimal results, however formulating a scoring
function which produces exact relevance probabilities is not
the focus of our work.

Lucene only supports scoring functions based on the Vec-
tor Space Model [1]. We incorporated the Okapi BM25 func-
tion into Lucene’s source code for document ranking. The
precise term weighting formula we used is:

wtd = tfd ×

log ( N −n+0.5
n+0.5 )
k1 × ((1 − b) + b × dl

avdl ) + tfd

wtd = weight of term t in document d
tfd = frequency of term t in document d
N = total number of documents in the corpus
n = number of documents matching term t
dl = length of document d

avdl = average length of a document in the corpus
b, k1 = tuning parameters

We set b = 0.75, k1 = 2, since these values have been eﬀec-
tively used by other retrieval systems [13, 18, 31].

4.2.2 Connectivity Engine

The connectivity engine is used to estimate the value of
W (N (d2), d1, d2), i.e., the probability of a user with infor-
mation need N (d2) successfully navigating from d1 to d2
under the WUFIS model.

Let WΠ(N (d2), d1, d2) denote the probability of a user
with information need N (d2) successfully navigating from
d1 to d2 along path Π. Let Π∗ denote the path for which
this value is maximized. In order to keep the computation
feasible, we assume that Π∗ is the only path connecting d1

WWW 2007 / Track: SearchSession: Search Potpourri395Keyword query

Starting Points

HTTP Request

Document with guidance

VOLANT

WWW

Intermediary

Get Starting Points

Add Guidance

Content
Engine

Connectivity

Engine

HTTP request

Original document

Figure 2: Volant architecture (overview appears above the horizontal line; details appear below).

to d2, ignoring other “low-scent” paths between d1 and d2.
Let dW denote the document immediately following d1 along
Π∗.

In the preprocessing stage, for each ordered pair hd1, d2i
of documents in the corpus, a variant of Dijkstra’s algo-
rithm [11] is used to generate tuples of the form
hd1, d2, dW , W (N (d2), d1, d2)i. These tuples are then in-
dexed so as to optimize lookups based on d2 (in our case,
we build database indexes on appropriate columns of our
MySQL tables).

4.3 Selecting Starting Points

At runtime, when a query q is received, Volant constructs

a ranked list of starting points as follows:

1. Retrieve from the content engine all documents d′ for

which R(d′, q) > 0.

2. For each document d′ retrieved in Step 1, retrieve
from the connectivity engine all documents d for which
W (N (d′), d, d′) > 0.

3. For each unique document d identiﬁed in Step 2, com-

pute the starting point score σn(d, q).

4. Sort the documents in decreasing order of σn(d, q);

truncate after the top k documents.

4.4 Adding Navigation Guidance

Given query q, and document d requested by the user,
Volant intercepts and modiﬁes d by highlighting all links on
d that lead to documents relevant to q. The procedure for
determining what links on d to highlight is as follows:

4.5 Efﬁciency and Scalability

The primary aim of this paper is to introduce the NAR
model and evaluate its eﬀectiveness (our next topic, in Sec-
tion 5). That said, we discuss eﬃciency and scalability issues
here brieﬂy.

We have used Volant with a corpus of over 1.5 million
documents with over eight million hyperlinks (the .GOV col-
lection; see Section 5). Once the content and connectivity
indexes have been created, the online components (starting
point selection and link highlighting) execute at interactive
speeds. Not surprisingly, however, the preprocessing stage
(Section 4.2) is problematic in terms of scalability. In par-
ticular, creation of the connectivity index requires listing all
pairs of documents. For a very large corpus it becomes in-
feasible to consider all such pairs. Devising principled tech-
niques to limit the number of document pairs considered
based on the likelihood of ﬁnding paths with strong scent is
an important topic for future work.

5. EVALUATION

Having presented our formal navigation-aided retrieval
model and an overview of our implementation techniques,
we turn now to evaluation.
5.1 Experimental Hypotheses

In addition to evaluating Volant’s eﬀectiveness in com-
bined query/navigation settings, we seek to study Volant’s
behavior in scenarios that do not entail navigation. Also
of interest is the eﬀectiveness of Organic NAR approaches
(such as Volant) relative to that of Synthetic NAR approaches.
Ideal experiments would test the following three hypotheses:

1. Retrieve from the content engine all documents d′ for
which R(d′, q) ≥ T , where T > 0 is a ﬁxed relevance
threshold (we used T = 0.1 in our experiments).

2. For each document d′ retrieved in Step 1, retrieve from

the connectivity engine all tuples where W (N (d′), d, d′) >
0.

3. For each hd, d′, dW , W (N (d′), d, d′)i tuple retrieved in

Step 2, highlight links on d that point to dW .

• Hypothesis 1: In query-only scenarios, Volant does
not perform signiﬁcantly worse than conventional re-
trieval approaches.

• Hypothesis 2: In combined query/navigation scenar-
ios, Volant selects high-quality starting points. Qual-
ity is further enhanced by query-driven navigation guid-
ance.

• Hypothesis 3: In a signiﬁcant fraction of query/navigation

scenarios, the best organic starting point is of higher

WWW 2007 / Track: SearchSession: Search Potpourri396BM25 MAP score Volant MAP score

p-value

0.23

0.20

0.22

Table 2: Performance on unambiguous queries.

quality than one that can be synthesized using existing
techniques.

The experiments we present in this paper represent pre-
liminary eﬀorts toward validating the above hypotheses. Be-
fore we describe our experiments, we introduce the search
tasks we used as test sets.
5.2 Search Task Test Sets

It is not immediately clear how one would identify navigation-

prone scenarios a priori, given a search task description or
query. As a proxy we rely on the notion of query clarity [14],
which measures query ambiguity (a high clarity score indi-
cates low ambiguity). Query clarity has been used to predict
retrieval eﬀectiveness. Queries for which retrieval is inef-
fective are likely to be followed by query reﬁnement and/or
navigation (which may be thought of as implicit reﬁnement).
Hence we posit that low query clarity serves as a reasonable
ﬁrst-order indicator of navigation-prone scenarios. We used
the Simpliﬁed Clarity Score (SCS) metric, which has been
shown to predict retrieval eﬀectiveness well in the case of
short queries [19], to guide our selection of two sets of search
tasks and corresponding queries:

• Unambiguous: The 20 search tasks of highest clarity
from the TREC 2000 Web track6 [17], using the “title”
ﬁeld as the query. (Average clarity of top-20 = 9.6.)
These tasks are over the TREC WT10G corpus.

• Ambiguous: 48 randomly-selected tasks from the TREC

2003 Web topic distillation track [38], which is geared
toward open-ended exploration of broad topics, again
using the “title” ﬁeld as the query. (Average clarity
= 4.5.) These search tasks are over the TREC .GOV
corpus.

The WT10G and .GOV test collections have been shown to
be structurally representative of the real Web [34].

Note that the search tasks in the topic distillation track
are almost certain to be broad, open-ended and ambiguous
in nature – we just use the clarity score as an added sup-
port for our belief. To prevent misclassiﬁcations in the Web
track search task set, we manually screened the search tasks,
and retained only those which were clearly unambiguous in
nature.

In our experiments, we use the unambiguous test set to
represent “query-only” scenarios, and the ambiguous test
set for “combined query/navigation” scenarios.
5.3 Performance on Unambiguous Queries

As a test of Hypothesis 1 (Section 5.1), we measured mean
average precision (MAP) scores on the unambiguous test
set (Section 5.2). Results are reported in Table 2. Perfor-
mance did not diﬀer signiﬁcantly. Indeed, for these queries
the ranked query result lists produced by Volant were nearly
identical to those produced by BM25. The reason is that rel-
evant documents tended not to be siblings or close cousins
in the linkage structure. Consequently, Volant deemed that
the best starting points were the relevant documents them-
selves or, more precisely, documents estimated to have the
highest chance of being relevant according to BM25.
6Ad-hoc retrieval tasks with varying degrees of ambiguity.

5.4 Performance on Ambiguous Queries

Our main experiment tests Hypothesis 2 (Section 5.1),
i.e., how well Volant performs in scenarios for which it was
conceived. This experiment uses the ambiguous test set
(Section 5.2) to measure the usefulness of starting points
provided by Volant in performing a search task via naviga-
tion. Unfortunately, the relevance judgments provided by
TREC are not suitable for this experiment, because they
merely identify documents that constitute the root of a rel-
evant subsite (or some other form of structural hub)—they
provide no indication of how useful each such hub would be
in performing the search task. Furthermore, we cannot test
the eﬃcacy of navigation guidance (which annotates doc-
uments in a query-speciﬁc manner) using the TREC judg-
ments. Therefore, we ourselves designed and conducted a
user study in order to validate Hypothesis 2.

5.4.1 User Study Design
Participants. The study comprised of 48 human judges,
who were students at CMU familiar with the Web and Web
searching, but with no knowledge of our project. Each par-
ticipant was assigned a search task (selected at random with-
out replacement), and asked to judge the suitability of var-
ious documents as starting points for this search task (a
within-subjects design). A given starting point was judged
by exactly one participant.

Evaluation criteria. The participants were encouraged to
explore the neighborhood of the starting point while forming
their assessment. Ratings were taken for four criteria, each
on a scale from 1 to 5:

• Breadth: Would a broad spectrum of people, who
are interested in diﬀerent aspects of the search task,
be satisﬁed with the information obtainable via the
starting point?

• Accessibility: Given a user with a particular special-
ized interest, would he be able to navigate easily from
the starting point to material that suits his interest?

• Appeal: Do you like the way the material is organized

and presented? 7

• Usefulness: Would most people be able to complete

their task successfully using this starting point?

The breadth, accessibility and appeal criteria focus on spe-
ciﬁc factors, whereas the usefulness criterion is intended to
elicit an overall assessment.

In this experiment, to maximize the information gathered
from the human subjects we conﬁgured Volant to prune re-
dundant starting points in the following way: documents
reachable (with high navigation probability) from the top-
ranked starting point are factored out when selecting the
second-ranked starting point, and so on. In a real-world de-
ployment it may be desirable to prune query results in such
a manner, and indeed to enforce diversity in a more general
sense8. An in-depth study of methods for ensuring diversity

7Based on a posteriori discussions with some of the subjects,
we determined that appeal may have been inﬂuenced by the
fact that documents lacked the embedded images that would
normally have been present (images were not included in the
corpus used for the TREC 2003 Web topic distillation track).
8Most popular commercial search engines already provide
such features.

WWW 2007 / Track: SearchSession: Search Potpourri397Breadth

Volant-G
Volant-U
TD

Top-1

Top-5

Top-10

Appeal

Usefulness

Top-1

Top-5

Top-10

Accessibility

5

4

3

2

1

5

4

3

2

1

5

4

3

2

1

5

4

3

2

1

Top-1

Top-5

Top-10

Top-1

Top-5

Top-10

Figure 3: Performance on ambiguous queries: starting point ratings averaged across all tasks/participants
(with 95% conﬁdence intervals).

in NAR query results is beyond the scope of this paper, and
is left as future work.

Starting points evaluated. For each search task, the fol-
lowing sets of starting points were included in the evaluation:
First, we included the top ten starting points produced by
Volant (these have hyperlink highlighting). We also included
a copy of this top-10 set with link highlighting removed, to
allow us to isolate the impact of navigation guidance in our
measurements. These 20 starting points constitute Organic
NAR results produced by our methods.

Since there is no prior work on Organic NAR, we added
the top ten results of the the best performing algorithm in
the TREC 2003 Web topic distillation track [12]. This algo-
rithm, developed by CSIRO, is a ﬁne-tuned implementation
of a sophisticated topic distillation technique, making use
of a variety of document features such as url length, anchor
text, oﬀ-site/on-site in-degrees and out-degrees, etc.

The total of 30 starting points thus obtained were ran-
domly ordered before presentation, and participants were
not told how the starting points were selected.

Eﬀort. Each participant was asked to judge a total of 30
starting points. Since a participant was expected to ex-
plore the neighborhood of a starting point before making
an assessment, we allocated ten minutes per starting point,
which meant that each participant devoted 300 minutes (or
ﬁve hours) for this study. To prevent fatigue, we distributed
the study over ﬁve days, wherein each participant was re-
quired to spend only an hour each day judging starting
points. Physical constraints (rooms, study administrators,
etc.) limited the number of participants operating simulta-

neously in a single session to ten. Therefore, we conducted
ﬁve sessions of ten users each per day, over a period of ﬁve
days.

This study involved a substantial investment of time, ef-
fort and money. The number of participants (and hence
search tasks), the number of results considered for each algo-
rithm and the number of algorithms compared were severely
restricted by our resources. The decision to use only the top
ten results of each algorithm allowed us to test several al-
gorithms/variants, without excessive burden. We feel this
decision is reasonable, given that users of commercial search
engines typically only view the top ten results.

5.4.2 Results

Figure 3 shows the starting point ratings, averaged across
all search tasks (and hence across all participants). Each
metric is plotted on a separate graph. In each graph, the
leftmost group shows the results for the top-ranked starting
points; the center group shows the average across the top
ﬁve starting points; the rightmost group shows the average
across the top ten. The label “Volant-G” corresponds to
Volant with navigation guidance; “Volant-U” corresponds
to Volant with no guidance; “TD” corresponds to the topic
distillation algorithm9.

We used the paired t-test to identify diﬀerences that carry
statistical signiﬁcance (using p = 0.05 as the cutoﬀ).
In
no case does TD perform better than either of Volant-U
or Volant-G with statistical signiﬁcance. There are cases

9Contrary to what one would expect, Volant-G and Volant-
U did not receive identical breadth ratings (the diﬀerence
was statistically signiﬁcant for Top-5 and Top-10). We at-
tribute this diﬀerence to a discrepancy in perceived breadth.

WWW 2007 / Track: SearchSession: Search Potpourri398in which the converse is true, but not across all three rank
groups. The only statistically signiﬁcant diﬀerence across all
three rank groups is that Volant-G consistently outperforms
each of Volant-U and TD in terms of usefulness. 10

5.4.3 Conclusions

First, this experiment conﬁrms the usefulness of naviga-
tion guidance. (Determining the particular form of naviga-
tion guidance that works best is left as future work.)

Second, in the absence of navigation guidance Volant per-
forms on par with the best known topic distillation algo-
rithm, even though we performed almost no tuning of Volant
prior to the experiment. While the two algorithms have sim-
ilar performance, Volant oﬀers three important advantages:

• Unlike the topic distillation algorithm, which uses a
combination of heuristics and has been tuned exten-
sively, Volant is based on a clean theoretical model.

• Volant invokes a conventional retrieval function as a
subroutine, so advances in conventional retrieval tech-
nology are trivial to incorporate into Volant to achieve
a corresponding improvement.

• As indicated by our ﬁrst experiment (Section 5.3),
Volant is suitable for unambiguous queries as well as
ambiguous ones. This property may obviate the need
to choose between two retrieval algorithms (e.g., con-
ventional IR and topic distillation) for each query.

5.5 Organic versus Synthetic NAR

To obtain a preliminary understanding of the relation-
ship between Organic and Synthetic NAR, we injected a
synthetic starting point document into the set of starting
points evaluated by each of our human subjects. We used
the well-known search result clustering algorithm of Zamir
and Etzioni [40] to generate synthetic starting points. Each
cluster produced by the algorithm is treated as a starting
point, with links to the documents belonging to that clus-
ter. Our results so far are inconclusive.

On the one hand, if we compare the top-ranked start-
ing point suggested by Volant against the synthetic starting
point generated by the clustering algorithm, we ﬁnd that
for breadth, accessibility and appeal, clustering outperforms
Volant by a statistically signiﬁcant margin. A signiﬁcant dif-
ference for usefulness was not found. Examining usefulness
on a task-by-task basis, we ﬁnd that the clustering start-
ing point was rated more useful in 40% of the search tasks,
while Volant’s top starting point was rated more useful in
17% of the search tasks (in 43% of the search tasks there
was a tie).11

On the other hand, if we broaden our focus to include all
ratings available, we ﬁnd that for at least 44% of the search
tasks the corpus contains a document that, when naviga-
tion guidance is added, is considered more useful than the
clustering starting point. If we also include documents that
were rated as equally useful, that ﬁgure rises to 96%.

10The p-values for Volant-G versus Volant-U are all below
0.005 (highly signiﬁcant); for Volant-G versus TD, all are be-
low 0.005 (again, highly signiﬁcant) except for Top-1, where
p = 0.04 (borderline signiﬁcant).
11It is unknown whether these cases arise due to characteris-
tics of the search task, the subject’s personal tastes, Volant’s
manner of ranking starting points or, more likely, some com-
bination of factors.

6. SUMMARY AND FUTURE WORK

This paper introduces a new document retrieval paradigm
called navigation-aided retrieval (NAR), designed to handle
poorly-speciﬁed and open-ended search tasks in hypertext
environments. These situations usually require a mix of
querying and navigation on the part of the user. As such,
NAR aims to integrate these two complementary search tac-
tics, which to date have primarily been treated separately.

Effectiveness. The salient properties of the NAR paradigm
are: (1) responding to queries by positioning users at suit-
able starting points for exploratory navigation; (2) helping
to guide navigation in a query-driven fashion. Our experi-
ments showed that the scoring function implied by our NAR
model performs as well as the best-known topic distillation
algorithm at selecting suitable starting points, and that our
navigation guidance mechanism is of signiﬁcant beneﬁt com-
pared with no guidance.

Relationship to conventional IR. Our formal model of
navigation-aided retrieval constitutes a strict generalization
of the conventional probabilistic IR model. Furthermore,
our empirical work suggests that in the case of unambiguous
queries for which conventional IR techniques are suﬃcient,
NAR reduces to standard IR automatically. This property,
if conﬁrmed through further experiments, would obviate the
need to choose from two alternative retrieval methods based
on the nature of the search task.

Relationship to synthetic approaches. Search result clus-
tering can be thought of as form of navigation-aided re-
trieval, as the user is expected to navigate within the dynam-
ically constructed topic hierarchy before converging on the
desired document(s). We characterize methods of this form
as Synthetic NAR, whereas our method can be thought of
as Organic NAR. Although Synthetic NAR has the obvious
advantage of being unconstrained by pre-existing structure,
in our (albeit preliminary) experiments we found that Or-
ganic NAR has signiﬁcant potential. Speciﬁcally, in nearly
all cases (96%) the best organic starting point was rated
to be at least as useful as the topic hierarchy created by a
well regarded clustering algorithm. Furthermore, in a signif-
icant fraction of cases (44%) the best organic starting point
received a higher rating than the clustering result.

Future work. While these initial results are encouraging,
there is more work to be done. First, to perform well over
corpora that contain substantial redundancy, it may be ap-
propriate to generate only topically diverse starting points.
Second, we would like to reﬁne our scoring function so that
it can be applied to synthetic starting points as well as or-
ganic ones, while still yielding good predictions. Ultimately,
we envision a uniﬁed method that presents the user with
directly relevant documents (in the case of unambiguous
queries), organic starting points for exploration (in the case
of ambiguous queries), or synthetic starting points (in the
case that the corpus does not contain suitable organic ones).

Acknowledgements
We gratefully acknowledge the assistance of Gary Hsieh,
Dhananjay Khaitan, Paul Ogilvie, Sandeep Pandey, Samuel
Wang and Andrew Yang with the extensive implementation
and evaluation eﬀort that went into this project. We also

WWW 2007 / Track: SearchSession: Search Potpourri399thank Jamie Callan, Rosie Jones and Malcolm Slaney for
helpful discussions and feedback.

pre-retrieval predictors. In Proc. Symposium on String
Processing and Information Retrieval, 2004.

7. REFERENCES
[1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern

Information Retrieval.

[2] M. J. Bates. The Design of Browsing and Berrypicking

Techniques for the On-line Search Interface. Online
Review, 13:407–431, 1989.

[3] N. J. Belkin. Anomalous States of Knowledge as the
Basis of Information Retrieval. Canadian Journal of
Information Science, 5:133–143, 1980.

[4] K. Bharat, A. Broder, M. Henzinger, P. Kumar, , and
S. Venkatasubramanian. The Connectivity Server: fast
access to linkage information on the Web. In Proc.
WWW, 1998.

[5] K. Bharat and M. Henzinger. Improved algorithms for

topic distillation in a hyperlinked environment. In
Proc. SIGIR, 1998.

[6] S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg,

P. Raghavan, and S. Rajagopalan. Automatic resource
list compilation by analyzing hyperlink structure and
associated text. In Proc. WWW, 1998.

[7] S. Chakrabarti, B. E. Dom, D. Gibson, R. Kumar,

P. Raghavan, S. Rajagopalan, and A. Tomkins.
Experiments in topic distillation. In Proc. SIGIR
Workshop on Hypertext Information Retrieval on the
Web, 1998.

[8] S. Chakrabarti, M. Joshi, and V. Tawde. Enhanced

topic distillation using text, markup tags, and
hyperlinks. In Proc. SIGIR, 2001.

[9] E. H. Chi, P. Pirolli, K. Chen, and J. Pitkow. Using

information scent to model user information needs and
actions and the Web. In Proc. SIGCHI, 2001.

[10] E. H. Chi, A. Rosien, G. Supattanasiri, A. Williams,

C. Royer, C. Chow, E. Robles, B. Dalal, J. Chen, and
S. Cousins. The Bloodhound project: Automating
discovery of web usability issues using the InfoScent
simulator. In Proc. SIGCHI, 2003.

[11] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and

C. Stein. Introduction to Algorithms. MIT
Press/McGraw-Hill. second edition, 2001.

[12] N. Craswell, D. Hawking, A. McLean, T. Upstill,

R. Wilkinson, and M. Wu. TREC12 web and
interactive track at CSIRO, 2003.

[13] N. Craswell, D. Hawking, and S. Robertson. Eﬀective

Site Finding Using Link Anchor Information. In
Research and Development in Information Retrieval,
pages 250–257, 2001.

[14] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.

Predicting query performance. In Proc. SIGIR, 2002.

[15] S. Dumais, E. Cutrell, and H. Chen. Optimizing search
by showing results in context. In Proc. SIGCHI, 2001.

[16] J. English, M. Hearst, R. Sinha, K. Swearingen, and
K. Yee. Flexible Search and Browsing using Faceted
Metadata. http://bailando.sims.berkeley.edu/
papers/flamenco02.pdf, Jan. 2002.

[17] D. Hawking. Overview of the TREC-9 web track.
[18] D. Hawking, T. Upstill, and N. Craswell. Toward

better weighting of anchors. In Proc. SIGIR, 2004.

[19] B. He and I. Ounis. Inferring query performance using

[20] T. Joachims, D. Freitag, and T. Mitchell.

WebWatcher: A tour guide for the World Wide Web.
In Proc. International Joint Conference on Artiﬁcial
Intelligence (IJCAI), 1997.

[21] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing
the spread of inﬂuence through a social network. In
Proc. SIGKDD, 2003.

[22] J. M. Kleinberg. Authoritative sources in a

hyperlinked environment. J. ACM, 46(5), 1999.
[23] V. Lavrenko and W. B. Croft. Relevance-based

language models. In Proc. SIGIR, 2001.

[24] H. Lieberman. Letizia: An agent That assists Web

browsing. In Proc. IJCAI, 1995.

[25] J. Miller, G. Rae, F. Schaefer, L. Ward, T. LoFaro,

and A. Farahat. Modiﬁcations of kleinberg’s hits
algorithm using matrix exponentiation and web log
records. In Proc. SIGIR, 2001.

[26] D. Mladenic. Using text learning to help Web

browsing. In Proc. SIGCHI, 2001.

[27] MySQL DBMS. http://www.mysql.com/.
[28] C. Olston and E. H. Chi. ScentTrails: Integrating

browsing and searching on the Web. ACM Trans. on
Computer-Human Interaction, 10(3), 2003.

[29] P. Pirolli and S. Card. Information Foraging.

Psychological Review, 1999.

[30] F. Qiu, Z. Liu, and J. Cho. Analysis of user web traﬃc

with a focus on search activities. In Proc.
International Workshop on the Web and Databases
(WebDB), June 2005.

[31] S. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. Okapi at TREC. In Proc.
TREC, 1992.

[32] S. E. Robertson, S. Walker, M. Hancock-Beaulieu,

A. Gull, and M. Lau. Okapi at TREC. In Text
REtrieval Conference, 1992.

[33] V. Sinha and D. R. Karger. Magnet: Supporting

navigation in semistructured data environments. In
Proc. SIGMOD, 2005.

[34] I. Soboroﬀ. Do TREC Web Collections Look Like the

Web? SIGIR Forum, pages 23–31, 2002.

[35] K. Sparck Jones, S. Walker, and S. Robertson. A

probabilistic model of information retrieval:
Development and comparative experiments.
Information Processing and Management, 36, 2000.
[36] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R.
Karger. The perfect search engine is not enough: A
study of orienteering behavior in directed search. In
Proc. SIGCHI, 2004.

[37] Apache Tomcat. http://tomcat.apache.org/.
[38] TREC-2003 Web Track: Guidelines. http:

//es.csiro.au/TRECWeb/guidelines 2003.html.

[39] R. Wheeldon and M. Levene. The Best Trail algorithm
for assisted navigation of Web sites. In Proc. LA-WEB
Conference on Latin American Web Congress, 2003.

[40] O. Zamir and O. Etzioni. Web document clustering: A

feasibility demonstration. In Proc. SIGIR, 1998.

[41] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma.

Learning to cluster web search results. In Proc.
SIGIR, 2004.

WWW 2007 / Track: SearchSession: Search Potpourri400