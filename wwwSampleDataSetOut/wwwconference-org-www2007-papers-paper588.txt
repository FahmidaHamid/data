Page-level Template Detection via Isotonic Smoothing

Deepayan Chakrabarti

Yahoo! Research

701 First Ave

Sunnyvale, CA 94089.

deepay@yahoo-inc.com

Ravi Kumar
Yahoo! Research

701 First Ave

Sunnyvale, CA 94089.
ravikumar@yahoo-

inc.com

Kunal Punera∗

Dept of ECE

Univ. Texas at Austin

Austin, TX 78712.

kunal@ece.utexas.edu

ABSTRACT
We develop a novel framework for the page-level template
detection problem. Our framework is built on two main
ideas. The ﬁrst is the automatic generation of training data
for a classiﬁer that, given a page, assigns a templateness
score to every DOM node of the page. The second is the
global smoothing of these per-node classiﬁer scores by solv-
ing a regularized isotonic regression problem; the latter fol-
lows from a simple yet powerful abstraction of templateness
on a page. Our extensive experiments on human-labeled test
data show that our approach detects templates eﬀectively.

Categories and Subject Descriptors
H.4.m [Information Systems]: Miscellaneous

General Terms
Algorithms, Experimentation, Measurements

Keywords
Webpage sectioning, webpage segmentation, template de-
tection, isotonic regression

1.

INTRODUCTION

The increased use of content-management systems to gen-
erate webpages has signiﬁcantly enriched the browsing ex-
perience of end users; the multitude of site navigation links,
sidebars, copyright notices, and timestamps provide easy-
to-access and often useful information to the users. From
an objective standpoint, however, these “template” struc-
tures pollute the content by digressing from the main topic
of discourse of the webpage. Furthermore, they can cripple
the performance of many modules of search engines, includ-
ing the index, ranking function, summarization, duplicate
detection, etc. With templated content currently constitut-
ing more than half of all HTML on the web and growing
steadily [3, 11], it is imperative that search engines develop
scalable tools and techniques to reliably detect templates on
a webpage.

Most existing methods for template detection operate on
a per website basis by analyzing several webpages from the
∗Most of the work was done while the author was visiting
Yahoo! Research.
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

site and identifying content and/or structure that repeats
across many pages. While these “site-level” template detec-
tion methods oﬀer a lot of promise, they are of limited use
because of the following two reasons. First, site-level tem-
plates constitute only a small fraction of all templates on
the web. For instance, page- and session-speciﬁc navigation
aids such as “Also bought” lists, ads, etc. are not captured
by the site-level notion of templates. Second, these methods
are error prone when the number of pages analyzed from a
site is statistically insigniﬁcant, either because the site is
small, or because a large fraction of the site is yet to be
crawled. In particular, they are totally inapplicable when
pages from a new website are encountered for the ﬁrst time.
An alternative paradigm that avoids many of these pitfalls is
to detect templates on per webpage basis, i.e., “page-level”
template detection. This is especially attractive since it can
be easily deployed as a drop-in module in existing crawler
work-ﬂows.

A tempting approach to page-level template detection is
to extract suﬃciently rich features from the DOM nodes and
train a classiﬁer to assign “templateness” scores to each node
in a DOM tree. While this approach is entirely plausible,
it has several handicaps. First, for the classiﬁer to have a
reasonable performance, both accurate and comprehensive
training data is required; this can involve prohibitive human
eﬀort. Second, by classifying each DOM node in isolation
this approach does not take a global view of the templateness
of nodes in the DOM tree.

In this paper we develop a novel framework for page-level

template detection.

Main contributions. Our ﬁrst contribution is a method
to automatically build a page-level templateness classiﬁer.
This method works as follows. First, we generate training
data by applying the site-level template detection method on
several randomly selected sites. Next, we deﬁne and extract
appropriate features for these site-level templates. Finally,
we use this automatically generated data to train a classi-
ﬁer that can assign a templateness score to every node in a
DOM tree. We show that our classiﬁer generalizes beyond
its site-level training data and can also discover templates
that manifest only at the page-level.

Our second contribution is the formulation of a global
property that relates templateness scores across nodes of
the DOM tree. We assert that templateness is a monotone
property: a node in the DOM tree is a template if and only
if all its children are templates. An appropriate relaxation
of this property leads to the following regularized isotonic
regression problem: given a tree with classiﬁer scores at each

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages61node, ﬁnd smoothed scores that are not far from the clas-
siﬁer scores, but satisfy the relaxed monotonicity property.
We provide an eﬃcient algorithm to optimally solve this
problem; this algorithm may be of independent interest.

On the whole, our approach eliminates the aforementioned
issues with pure classiﬁer-based approaches. An interesting
by-product of our framework is that we obtain a sectioning
of a page into segments; this is useful in many applications.
We perform an extensive set of experiments to validate
our framework and algorithm. In terms of detecting content
within templates, our algorithm achieves an f -measure in
excess of 0.65 for text and 0.75 for links on a human-labeled
test set. We highlight the applications of template detection
by showing that removing templates as a pre-processing step
boosts the accuracy of standard web mining tasks on our
datasets, by as much as 140% on duplicate detection, and
18% on webpage classiﬁcation. Further, we show that the
gains obtained by using our page-level template detection
approach are substantially greater than those obtained by
using the more expensive site-level approach.

2. RELATED WORK

Our work in this paper is related to two broad areas of

research: template detection and isotonic regression.

Site-level template detection. The problem of template
detection and removal was ﬁrst studied by Bar-Yossef and
Rajagopalan [3], who proposed a technique based on seg-
mentation of the DOM tree, followed by the selection of
certain segments as candidate templates depending on their
content. Yi et al. [26] and Yi and Liu [25] used a data struc-
ture called the style tree to take into account the metadata
for each node, instead of its content. Vieira et al. [24] framed
the template detection problem as a problem of mapping
identical nodes and subtrees in the DOM trees of two dif-
ferent pages. They proposed performing the expensive task
of template detection on a small number of pages, and then
removing all instances of these templates from the entire site
by a much cheaper approach. Gibson et al. [11] conducted a
detailed study of templates on the web, demonstrating the
prevalence of templated content and its steady growth.

All of these methods, however, need multiple pages from
the same website to perform template detection, and thus
suﬀer from the problems mentioned in the introduction.
Our page-level algorithm can use the site-level templates
detected by these methods as training data, generalizing
the concept of a template beyond what is found by these
algorithms.

Page-level template detection. Some page-level algo-
rithms have also been proposed recently. Kao et al. [14] seg-
ment a given webpage using a greedy algorithm operating
on features derived from the page. However, their method is
not completely page-level; they also use some site-level fea-
tures such as the number of links between pages on a web-
site. Debnath et al. [10] also propose a page-level algorithm
(“L-Extractor”) that applies a classiﬁer to DOM nodes (as
in our algorithm), but only certain nodes are chosen for clas-
siﬁcation, based on a predeﬁned set of tags. Kao et al. [13]
propose a scheme based on information entropy to focus
on the links and pages that are most information-rich, re-
ducing the weights of template material as a by-product.
Song et al. [22] use visual layout features of the webpage
to segment it into blocks which are then judged on their

salience and quality. Other local algorithms based on ma-
chine learning have been proposed to remove certain types of
template material. Davison [9] uses decision tree learning to
detect and remove “nepotistic” links, and Kushmerick [16]
develops a browsing assistant that learns to automatically
removes banner advertisements from pages. Another set of
papers focus only on segmentation of webpages for the pur-
pose of displaying them on small mobile device screens [2, 7,
27]. However, template detection is not their primary focus.
While similar in spirit to our page-level template detec-
tion system, these algorithms are signiﬁcantly diﬀerent in
the details. Only a subset of DOM nodes (“segments”) are
operated upon, this subset having been chosen prior to any
determination of the templateness of those segments. As a
result, should a segment itself be composed of several tem-
plate and non-template nodes, this would not be detected.
Our algorithm operates on each node in the DOM tree, and
ﬁnds segments based on the results of a templateness clas-
siﬁer; thus, it avoids these problems.

Isotonic regression. The problem of isotonic regression
crops up in a range of disciplines, from microarray data anal-
ysis [1] to epidemiology [19] and statistics [21]. Stout [23]
showed that the optimal solution for complete orders can
be computed in O(n log n) for L1, and O(n) time for L2
distance metrics. Pardalos and Xue [20] gave an O(n log n)
algorithm for L2 isotonic regression on rooted trees. An-
gelov et al. [1] recently presented an O(n2 log n) algorithm
for L1 on trees.

We propose a general version of the isotonic regression
problem on trees that subsumes the problems mentioned
above. For the L1 distance metric we provide an exact al-
gorithm to solve this general problem in O(n2 log n) time,
matching the best results on the special cases studied in the
results cited above.

3. FRAMEWORK

In this section we describe the proposed framework for
the page-level template detection problem. We ﬁrst ﬁx some
notation.

Recall the DOM tree representation of an HTML doc-
ument where each node in the DOM tree corresponds to
an HTML fragment; we identify the DOM node with the
fragment it represents. Let T be the rooted DOM tree cor-
responding to the document. From here onwards, we use
the tree T as a metaphor for the document. Let templ(T )
denote the set of all nodes in T that are templates. We use
i ∈ T to denote that node i belongs to tree T , parent(i)
to denote the parent of i in T , child(i) to denote the set of
children of i in T , and root(T ) to denote the root of T .
Let H denote the set of all possible DOM nodes. In the
page-level template detection problem, we seek a boolean
function τ : H → {0, 1} such that τ (i) = 1 for all i ∈
templ(T ), and τ (i) = 0 otherwise. In a relaxed version of
the problem, we seek a function ˜τ : H → [0, 1] where if
i ∈ templ(T ) and j /∈ templ(T ), then ˜τ (i) > ˜τ (j); using an
appropriate threshold, we can round ˜τ to make it boolean.

A ﬁrst-cut approach to page-level template detection would
be to extract suﬃciently rich features from the DOM nodes
(in the context of a page) and train a classiﬁer x : H → [0, 1]
to score the “templateness” of each node in a given page.
While this appears plausible, it has several issues when scru-
tinized closely. The ﬁrst set of issues revolve around the con-

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages62struction of the training data for our classiﬁer. For the clas-
siﬁer to learn the notion of “templateness” of DOM nodes
on the web in general, it must be trained comprehensively
over all forms of templates that it is likely to encounter. The
heterogeneity and scale of the web imply that a huge cor-
pus of accurate and diverse training data will be required.
These requirements present a daunting task that demands
tremendous human eﬀort. Secondly, this approach to clas-
siﬁcation ignores the global property of templateness in the
DOM tree, crisply stated as follows.

Property 1

(Templateness Monotonicity). A node
in the tree is a template if and only if all its children are tem-
plates. In other words, the function τ (·) is monotone on the
tree.

As is apparent, by working on each node of T in isolation,
a naive classiﬁer misses this intuitive relationship among
templateness of nodes in the tree.

Our three step framework is meant precisely to address

these issues, and is described below.

1. Automatic generation of training data. The ﬁrst
step is the automatic generation of training data. To this
end, we use the site-level template detection paradigm of
[11]. Note that even though site-level template detection is
less feasible as a web-scale template detection mechanism,
we show that we can still use it to generate training data for
our approach.

The basic intuition behind the site-level template detec-
tion approach is the following. One of the common proper-
ties of templates is that they occur repeatedly across many
pages on a single site. Therefore, if a DOM node occurs
many times on diﬀerent pages from a single site, then it
lends credible evidence that this DOM node perhaps corre-
sponds to a template.

We now describe a generic algorithm that we will call
SiteLevel (θ). This algorithm operates on a site by site
basis. For each site, it obtains a set T of random pages
from the site. Then, for each page T ∈ T and for every DOM
node i ∈ T , it computes h(i), where h(·) is a random hash
θ ⊆ H be the set of DOM nodes that occur
function. Let I +
on at least θ fraction of pages in T . Note that using hashes,
this set can be identiﬁed eﬃciently. SiteLevel returns I +
θ
as the set of DOM nodes deemed templates.

2. Classiﬁcation. The second step is to use the set of
identiﬁed by SiteLevel as training data
DOM nodes I +
θ
for a classiﬁer. For this, we ﬁrst identify appropriate fea-
tures of DOM nodes in I +
θ , in the context of the pages they
appear in. We then train a classiﬁer x : H → [0, 1] using
these features of the DOM nodes, treating those in I +
θ as
positive examples; the output of the classiﬁer is a template-
ness score for a given DOM node in a tree. The hope in
using a classiﬁer is that it can distill features from site-level
templates that can be generalized to all templates on the
web. This can help us identify templates that don’t manifest
themselves by repeatedly occurring across multiple pages on
a website — templates that a pure site-level template detec-
tion approach cannot discover by itself. As we empirically
observe in Section 6.2, this is indeed what happens.

3. Isotonic smoothing. At this point, one could use the
classiﬁer to assign a templateness score x(·) to each DOM
node in the given page T . However, as we argued earlier, this

does not fully capture the essence of the problem since the
templateness scores assigned by the classiﬁer to each DOM
node in isolation may not satisfy the property that a node
is a template if and only if all its children are templates
(Property 1). On the other hand, assuming the classiﬁer
has reasonable accuracy, the scores it assigns makes sense
for most, if not for all, of the nodes. The question now is
how to reconcile the score assigned by the classiﬁer with the
monotonicity property of the templates.

To handle this question, we ﬁrst consider a natural gen-
eralization of the monotonicity property for the case of real
valued templateness scores. Suppose y(i) is the templateness
score of a node i in the tree. Then, y(·) is said to satisfy
generalized templateness monotonicity if for every internal
node i, with children j1, . . . , j‘, y(i) = min{y(j1), . . . , y(j‘)},
i.e., the templateness of an internal node is the equal to the
least of its children’s templateness scores.

Note that generalized monotonicity ensures, ﬁrst, that the
templateness score of a node is at least the templateness
score of its parent, and second, that the templateness score
of the parent equals the templateness score of all its children,
when the children all have same templateness score. We
also have an additional requirement that the templateness
score y(·) be close to the x(·) scores assigned by the classi-
ﬁer. Generalized monotonicity together with this closeness
requirement leads to the problem of generalized isotonic re-
gression on trees, which we solve in this paper.

While we defer the detailed description of our solution to
the next section, we now highlight the advantages of our
framework. Besides addressing the issues with using just
the classiﬁer scores, our framework oﬀers additional beneﬁts.
First, the overall framework in simple and modular. Second,
any oﬀ-the-shelf classiﬁer can be used, instead of having to
design one that works speciﬁcally for the given DOM tree
structure. Third, as we will see later, a neat by-product of
isotonic smoothing is that we obtain a sectioning of a page
into segments; this can be useful in many applications.

4.

ISOTONIC SMOOTHING

In this section we formulate and solve the generalized iso-
tonic regression problem on trees. Recall that we are given
as input a DOM tree with each node labeled by a score as-
signed by the classiﬁer. The purpose of isotonic regression
is to ﬁx these scores so that they satisfy the monotonicity
constraints, while remaining as faithful as possible to the
original classiﬁer scores. Let x(i) be the classiﬁer score for
each node i ∈ T and let y(i) be the smoothed score we wish
to obtain.

The ﬁrst step in our formulation is to alter the generalized
monotonicity property in two ways. First, we only ensure
that the templateness score of a node is at most the least of
its children’s scores, instead of equal to it. This relaxation
is derived from the current domain in which the cost of mis-
classifying a non-template as a template is much higher than
vice versa. Hence, if according to the classiﬁer an internal
node’s template score is much lower than that of all of its
children, then we would want to respect that. Second, we
introduce a regularization that penalizes if, for a node i, the
templateness score y(i) is diﬀerent from those of its children
y(j1), . . . , y(jk). Clearly, if y(j1) = ··· = y(jk), then this
regularization will try to ensure that y(i) = y(j1).

Thus, we have

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages63(1) For every internal node i with children j1, . . . , j‘, y(i)

≤ min{y(j1), . . . , y(j‘)}.

For purposes of regularization, we develop the notion of
compressed score that embodies sectioning of the DOM tree
into subtrees. A compressed score is a function ˆy : T →
[0, 1] ∪ {⊥} with the following properties:

ˆy(root(T )) 6= ⊥, and
(2)
if i is an ancestor of j and ˆy(i) 6= ⊥ 6= ˆy(j), then
(3)
ˆy(i) < ˆy(j).
Let the size |ˆy| of the compressed score be the number of
places where ˆy is deﬁned; |ˆy| = |{i | i ∈ T, ˆy(i) 6= ⊥}|.
For all i ∈ T such that ˆy(i) = ⊥, let anc(i) be the closest
ancestor of i such that ˆy(anc(i)) 6= ⊥; note that such an
ancestor always exists by (2). We now interpolate ˆy to a
unique y as follows.

 ˆy(anc(i))

ˆy(i)

y(i) =

if ˆy(i) = ⊥
otherwise

It is clear that if ˆy satisﬁes (2) and (3), then the correspond-
ing interpolated y satisﬁes (1). Also, given a y satisfying (1),
it is easy to construct the unique ˆy. From now on, we use
the smoothed score y and its compressed counterpart ˆy in-
terchangeably.

Finally, the cost of a smoothed score y with respect to x

is deﬁned as

c(y) = γ · |ˆy| + d(x, y),

(4)
where γ is a penalty term that captures the cost of each
new smoothed score and d(·,·) is some distance function. It
is also possible to have a node-speciﬁc penalty term γi for
node i; for simplicity of exposition, we state the algorithm
in terms of a node-independent term γ.

This cost function and the tree structure lead to a regu-

larized version of the isotonic regression problem.

Problem 2

(Regularized Tree Isotonic Regression).

Given a tree T and x : T → [0, 1], ﬁnd y : T → [0, 1] that
satisﬁes (1) and minimizes c(y) as given by (4). 1
For the rest of the paper, we take d(·,·) to be the L1 norm
since it is robust against outliers.

Before presenting the algorithm we discuss a key property
of the L1 distance measure that aids us in designing an eﬃ-
cient algorithm for this problem. We show that the optimal
smoothed scores in y can only come from the classiﬁer scores
in x.

Lemma 3. There exists an optimal solution, ˆy, where, for
all i ∈ T , if ˆy(i) 6= ⊥, then there is a j ∈ T such that
ˆy(i) = x(j).

Proof. Consider the maximal connected subtree T 0 of
nodes in T such that (1) i ∈ T 0, and (2) for all j ∈ T 0,
y(j) = ˆy(i). If ˆy(i) is not the median of the set of scores
{x(j) | j ∈ T 0}, then we can push ˆy(i) closer to the median
by a small amount and decrease the cost of the solution
given by (4); this follows since the median is the minimizer
for L1 distance.
1Note that a special case of our problem has been consid-
ered before in statistics and computer science contexts; it is
usually referred to as the isotonic regression problem: given
~x = x1, . . . , xn, ﬁnd ~y = (y1, . . . , yn) such that y1 ≤ ··· ≤ yn
and d(~x, ~y) is minimized, where d(·,·) is some distance func-
tion. It is easy to extend this deﬁnition to the case when
the yi’s have to respect a given partial order, say, imposed
by a tree.

We build a dynamic program using the above result to
obtain an algorithm for the regularized tree isotonic regres-
sion problem. Algorithm BuildError builds up an index
function val(i, j) and an error function err(i, j) for each node
i ∈ T . The value err(i, j) represents the cost of the optimal
smoothed scores in the subtree rooted at i if its parent node
has the smoothed score y(parent(i)) = x(j). In this situa-
tion, the index val(i, j) is such that the optimal smoothed
score for node i is given by y(i) = x(val(i, j)).

If val(i, j) is the same as j, i.e, the optimal value for i and
parent(i) are the same x(j), then the only cost is the L1
distance between x(i) and x(val(i, j)), otherwise there is an
additional γ cost as well. The algorithm computes this error
function by ﬁrst computing errors as if the additional γ cost
must always be added; this intermediate result is stored in
the err0 array, where err0(j) is the error if the node under
consideration has the smoothed score y(i) = x(j). Then, it
chooses between (a) continuing with the parent’s value and
subtracting γ from the corresponding cost err0, or (b) creat-
ing a new section with a new value and paying in full the cor-
responding cost in err0. Once all error functions have been
computed, the optimal smoothed scores are obtained using
Algorithm IsotoneSmooth, which starts with the best in-
dex p(root(T )) at the root, and progressively ﬁnds the best
index p(·) for nodes lower down in the tree.
Algorithm BuildError (i, x, γ)

if (i is a leaf) then
1. for j ∈ T

/* all values node i can take */

if (x(i) − x(j) > γ) then
err(i, j) = γ; val(i, j) = i
else
err(i, j) = |x(i) − x(j)|; val(i, j) = j

else

2. for child u of node i

3. for j ∈ T

BuildError(u, x, γ)

err0(j) = |x(i) − x(j)| +P

/* all values node i can take */

k∈child(i) err(k, j) + γ

4. for j ∈ T

/* all values node parent(i) can take */

val∗ = argmink∈T,x(k)>x(j) err0(k)
err∗ = err0(val∗)
if (err0(j) − γ ≥ err∗ or i = root(T )) then
err(i, j) = err∗; val(i, j) = val∗
else
err(i, j) = err0(j) − γ; val(i, j) = j

Algorithm IsotoneSmooth (err, val)

val∗ = argmini∈T
p(root(T )) = val∗; y(root(T )) = x(val∗)
for i in a breadth-ﬁrst search order of T

err(root(T ), i)

p(i) = val(i, p(parent(i))); y(i) = x(p(i))

To demonstrate the correctness of this algorithm, we show
that the restriction of the optimal solution to a subtree is
also the optimal solution for the subtree under the mono-
tonicity constraint imposed by its parent.
Consider the subtree rooted at any non-root node i ∈ T .
Now suppose the smoothed score y(parent(i)) is speciﬁed.
Then, let z(·) be the smoothed scores of the optimal solu-
tion to the regularized tree isotonic regression problem for
this subtree, under the additional constraint that z(i) ≥
y(parent(i)).

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages64Lemma 4. For all nodes j in the subtree of i, y(j) = z(j).

Proof. Consider a smoothed solution w(·) where w(j) =
z(j) for all nodes j in the subtree of i, and w(j) = y(j)
otherwise. Since z(·) obeys the monotonicity property and
z(i) ≥ y(parent(i)), the solution w(·) obeys the monotonic-
ity property. Now, the cost c(w) is the sum of the cost for
the smoothed scores z(j) in the subtree of i and the cost
for the scores y(k) for all other nodes. Thus, the diﬀerence
between c(w) and c(y) is just the diﬀerence in costs for z(j)
and y(j) in the subtree of i, for which we know that z(·) is
the optimal. The lemma follows.

Theorem 5. Algorithm IsotoneSmooth solves the reg-

ularized tree isotonic regression problem.

Proof. The algorithm computes up the optimal smoothed
scores for each subtree, i.e., the err(·,·) arrays, while main-
taining (1) for every possible smoothed score of the par-
ent. By Lemma 3, the parent can take only ﬁnitely many
smoothed scores in the optimal solution, and by Lemma 4,
combining the optimal smoothed scores for subtrees yields
the optimal smoothed scores for the entire tree.

Complexity. Let |T| = n. The space required per node
is O(n), and so the total space required is O(n2). Next,
we consider the running time of the algorithm. In the dy-
namic program, step 1 takes O(n2) time, step 3 takes O(n2)
time amortized over all calls, and step 4 can be done in
O(n2 log n) time by storing err0 values in a heap and then
running over the nodes j ∈ T in ascending order of x(j).
Hence, the total running time is O(n2 log n). This matches
the time complexity of previously known algorithms for the
non-regularized forms of tree isotonic regression [1].

5. DETAILS OF THE SYSTEM

In this section we describe the details of both the classiﬁ-

cation and smoothing aspects of our system.
5.1 Constructing training data

As mentioned before, we used a site-level template de-
tection algorithm to generate training data for our classi-
ﬁer. The construction of training data involved two distinct
steps: collecting webpages and obtaining labeled templates.
We sampled 3, 700 websites from the Yahoo! search engine
index such that each website had at least 100 webpages.
We also biased the sampling process slightly towards pick-
ing good quality host domains, and avoided picking porno-
graphic or spam websites. Then, for each website we down-
loaded at most 200 randomly picked webpages.

All DOM nodes that occurred on more than 10% of the
pages of any website were tagged as site-level templates.
Since we wanted to learn a classiﬁer for all internal DOM
nodes we wanted representative labeled data from all lev-
els of the DOM trees. Hence, for each internal node we
computed how much of its HTML was part of a site-level
template. DOM nodes with more than 85% of their HTML
content within site-level templates were also labeled as tem-
plates. The rest of the DOM nodes were used as instances
of the non-templates class.

Note that the condition required for tagging a node as
template is very strong. This is done intentionally for two
reasons. First, recall that a node is a non-template if any

node in its subtree is a non-template. And second, the
cost of misclassifying a non-template as a template is much
higher than that of the reverse error.
5.2 Learning the classiﬁer

There are multiple steps involved in learning the classiﬁer.

Each of these steps is described below.

Preprocessing. Each webpage is preprocessed and parsed
so that features can be extracted for its DOM nodes. The
preprocessing step involves cleaning the HTML code using
Hypar2, annotating the DOM nodes with position and area
information using Mozilla3, and parsing the HTML to ob-
tain a DOM tree structure. The text in the HTML page is
also processed to remove stop words.

Feature extraction. The training data that we employ
for learning corresponds to site-level templates. However,
we want our classiﬁer to generalize to the global deﬁnition
of templates. This makes the process of feature extraction
very critical. From each DOM node, we extract features that
we believe are indicative of whether or not that DOM node
is a template. For example, intuitively, if the text within
a DOM node shares a lot of words with the title of the
webpage, then perhaps it is not a template node. Similarly,
the distance of a DOM node from the center for the page
indicates its importance to the main purpose of the page,
and hence its templateness.

In a similar fashion, we constructed several other features
from the position and area annotations of DOM nodes as
well as from the text, links, anchortext they contain. The
most discriminative features turned our to be: closeness to
the margins of the webpage, number of links per word, frac-
tion of text within anchors, the size of the anchors, fraction
of links that are intra-site, and the ratio of visible characters
to HTML content.

Classiﬁer training. We trained Logistic regression classi-
ﬁers [18] over the set of features described above. Apart from
performing very well, these classiﬁers have the additional
beneﬁt that their classiﬁcation output can be interpreted
as the probability of belonging to the predicted class.
In
our exploratory experiments we observed that distributions
of feature values varied heavily depending on the area of
the DOM node. This is because template and non-template
nodes have very diﬀerent characteristics at diﬀerent levels
of the DOM trees; these levels can be approximated by the
area of the node. Hence, we trained four logistic regression
models for DOM nodes of diﬀerent sizes. Now, given a web-
page, the appropriate logistic model is applied to each node
of the DOM tree, and the output probabilities are fed to our
post-classiﬁcation smoothing function.
5.3 Smoothing classiﬁer scores

The smoothing algorithm allows arbitrary choices of penalty

values for each tree node. However, in the domain of tem-
plate detection, there are several desiderata that a good
penalty function must try to achieve. We list these below,
along with the particular functions that we considered, and
the one that we ﬁnally settled upon.

Desiderata for penalties. There are three main desider-
ata for a smoothing algorithm in the context of template
2www.cse.iitb.ac.in/~soumen/download
3www.mozilla.org

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages65detection. First, nodes that are too small in area should
not form segments of their own. Such nodes have very little
content, and their classiﬁcation scores are unreliable. Also,
having such small segments impairs the applicability of web-
page segmentation to page visualization and browsing.

Second, adding nodes as segments should be easier as we
move up from leaves to the root. The smoothed values as-
signed to nodes high up in the tree impose constraints on
the possible values in their entire subtree. If creation of new
segments is hard, such nodes may merge with other nodes to
form larger segments whose smoothed scores may be drawn
too far away from the classiﬁcation score of the node itself,
thus hurting all nodes in their subtree.

Third, if a child node accounts for a large fraction of the
area of its parent node, then it should be harder to set
the child to a value diﬀerent from that of its parent. This
encourages the smoothing algorithm to form large sections
without too much nesting, which agrees with our intuitions
about how webpage segments are created.

Handling very small nodes. All nodes whose area is less
than 2000 sq pixels are neither classiﬁed nor smoothed; they
are “hidden,” and their eﬀect is rolled into their parent node.
Thus, a node with k hidden children acts as if it were (k + 1)
nodes, all with the same classiﬁcation value. This reduces to
multiplying the distance measure d(·,·) with (k +1), and the
smoothing algorithm can handle it trivially. This heuristic
goes some way in achieving the ﬁrst desideratum.

Penalty functions. We experimented with several penalty
functions, attempting to achieve the aforementioned desider-
ata. Starting with a user-deﬁned constant c, several trans-
formations for γi (penalty for node i) were tried:
(1) γi = c · N/Ni, where Ni is the number of nodes in the
subtree rooted at i, and N is the total number of nodes
in the DOM tree. This penalty is high for nodes near the
leaves and low for nodes near the root, satisfying the ﬁrst
and second desiderata.
(2) γi = c · A/Ai, where Ai is the area of node i, and A the
area of the whole HTML page. Again, this penalty satisﬁes
the ﬁrst and second desiderata.
(3) γi = c · Aparent(i)/Ai, where Aparent(i) is the area of the
parent of node i. This tries to achieve the third desideratum.
We tried all combinations of these penalties, over the a
large range of the constant c, and visually inspected the
results of smoothing on a few webpages. We ﬁnally settled
on setting γi = 0.01· A/Ai, which gave the best results. For
the rest of this paper, unless speciﬁed otherwise, the penalty
is always set to this function.

6. EXPERIMENTS

We now present an empirical evaluation of our system,
called PageLevel. Using human-labeled data, we show in
Section 6.1 that our approach is very eﬀective in detecting
the template sections of webpages. Then, in Sections 6.2
and 6.3 we show that applying template detection as a pre-
processing step signiﬁcantly improves accuracy on standard
web mining tasks such as duplicate webpage detection and
webpage classiﬁcation. We also show that template removal
using PageLevel provides more beneﬁts than using the
more expensive SiteLevel approach.

Dataset

Common

Random

Text
AT

Links
Text
AT

Links

PageLevel PageLevel

Basic
0.56
0.65
0.69
0.63
0.71
0.75

Smooth

0.60
0.71
0.73
0.66
0.73
0.77

Table 1: Accuracy of PageLevel on Common and
Random datasets in terms of f -measure.

6.1 Template detection performance

The desiderata for a template detection system are as fol-
lows: (a) it must divide the webpage into segments sepa-
rating template and non-template content; and (b) it must
accurately identify the webpage segments as template or
non-template.
In this section we show that our system,
PageLevel, achieves both these objectives.

Datasets. In order to evaluate the template detection per-
formance of PageLevel we manually created two labeled
datasets.
Common. We selected and manually labeled 44 pages from
websites that were commonly visited by the authors. The
selected pages come from a diverse set of domains, such as,
news websites like NYTimes and CNN, university websites
like UTexas-Austin, etc. For each webpage, the manual la-
beling process identiﬁed the largest possible HTML frag-
ments that were either entirely template or non-template.
These HTML fragments correspond to nodes in the webpage
DOM tree. Hence, for each webpage we labeled an antichain
of nodes through the DOM tree, forming an exhaustive and
disjoint cover of all leaf nodes.
Random. In order to evaluate the algorithms on webpages
more representative of the general Web, we manually labeled
100 pages selected uniformly at random from the DMOZ
directory4. The selected set of webpages is a mix of topi-
cally focused content or hub pages and entry points to larger
websites. As was done for the Common dataset, the la-
beling process identiﬁed for each webpage an antichain of
DOM nodes and marked each node as either template or
non-template.

Template detection accuracy. As we demonstrate later
in this section, text and links present within the template re-
gions mislead standard web-mining algorithms for tasks such
as duplicate detection and automated classiﬁcation. Here we
measure the eﬃcacy of PageLevel in identifying text and
links that occur within templates. We report accuracy in
terms of f -measure, which is the harmonic mean of preci-
sion (p) and recall (r); i.e. f = 2pr/(p + r). In the current
setting, precision is the fraction of words (links) identiﬁed
by PageLevel as occurring within templates that are also
manually placed within templates. Recall is the fraction
of all words (links) manually labeled as lying within tem-
plate regions that PageLevel also correctly identiﬁes as
templates. This evaluation setting has previously been used
by Vieira et al. [24].

In Table 1, we present the accuracy numbers (in terms of
f -measure) achieved by PageLevel for the two datasets:

4www.dmoz.org

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages66Figure 1: Segmentation performance of PageLevel
Basic, PageLevel Basic+Merge, and PageLevel
Smooth.

Figure 2: Variation in template detection accu-
racy on the Common dataset with changing values
of penalty. The x-axis represents the factor being
multiplied into the penalty.

Common and Random. We present accuracies for two vari-
ations of our approach: PageLevel Basic only applies the
classiﬁer to the DOM nodes individually, while PageLevel
Smooth, in addition also performs Isotonic smoothing on the
templateness scores. The performance is measured along
multiple dimensions: Text, Anchor-text (AT), and Links.
As is clear from the table, our approach is very eﬀective in
identifying all types of page content that lies within tem-
plate regions. Furthermore, smoothing is shown to signiﬁ-
cantly improve accuracy across all dimensions of evaluation,
in some cases by almost as much as 10%. An interesting
observation is that the accuracy over the Common dataset
is slightly lower than that over Random. This is because
the template structure in webpages in Common is more ex-
tensive than those in Random. Further, note that the gains
aﬀorded by the isotonic smoothing are larger on the more
diﬃcult of the two datasets.

Segmentation accuracy. As we mentioned in Section 4,
a by-product of the isotonic smoothing algorithm is a seg-
mentation of the page into DOM nodes that act as the roots
of the template and non-templates regions. Here, we show
that the segmentation found by PageLevel closely matches
the manually labeled segments.

Notice that the manual segmentation, an antichain of
nodes of the DOM tree, induces a grouping of the leaves
in which each node in the segmentation deﬁnes a group.
A leaf then belong to the group corresponding to the seg-
ment node that covers it. Similarly, the segmentation output
by PageLevel, even though it allows for nested segments,
also induces a grouping of leaves. Each leaf can be con-
sidered as belonging to the group corresponding to its clos-
est ancestor in the segmentation. Hence, we can evaluate
the PageLevel segmentation against the manually labeled
one by comparing the corresponding groupings using the
adjusted RAND index [12]. The adjusted RAND index is
a measure of how similar two groupings are, i.e., whether
pairs of objects (leaves) are together in both groupings, or
in diﬀerent groups in both groupings. It is used as a pre-
ferred measure of agreement between clusterings [17]. The
value of the adjusted RAND index is upper bounded by 1,
and its expected value for a random clustering is 0.

In Figure 1 we plot the accuracy of PageLevel segmenta-
tion in terms of adjusted RAND. The PageLevel Basic and
PageLevel Smooth approaches have been described above.
As we can see the PageLevel Basic algorithm achieves close
to random results, but this is expected since it is not per-

Figure 3: Variation in segmentation accuracy both
datasets with changing values of penalty. The x-
axis represents the factor being multiplied into the
penalty.

forming any smoothing of scores and hence almost every
leaf is in a group of its own. In contrast the segmentation
discovered by the isotonic smoothing function (PageLevel
Smooth) conforms very well to the manually labeled seg-
ments. In order to put the accuracy of PageLevel Smooth
in context, we also present numbers for a PageLevel Basic
+ Merge heuristic. This approach does a “naive” smooth-
ing of the classiﬁer scores by grouping adjacent leaves to-
gether when their templateness scores diﬀer by less than δ
(the best δ was found by exhaustive search). As we can
see from the plot, “merging” improves the scores of the
PageLevel Basic; however, the results are still far lower
than those achieved by isotonic smoothing. This shows that
the smoothing operation is constructing highly non-trivial
segmentations of webpages.

Eﬀect of variations in penalty. We have shown above
that PageLevel successfully obtains and labels template
segments within webpages. Here we discuss the sensitiv-
ity of our approach to penalty parameters in the isotonic
smoothing function.

Figure 2 plots the variation in template detection accuracy
on the Common dataset with changing values of penalty. In
the plot, the x-axis represents the factor multiplied into the
penalty in order to vary it. As we can see, an increase or
decrease in penalty results in an decrease in the template
detection accuracy. However, the decrease is larger with
higher values of penalty as this results in very few segments
and hence a mixing up of template and non-template struc-

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages67Total Pairs PageLevel

SiteLevel FullText

Dup

Non-Dup

1711

2058

1299
(76%)
1885

(91.6%)

730

(42.7%)

1712

(83.2%)

529

(30.9%)

1781

(86.5%)

Table 2: Number of duplicate and non-duplicate
pairs detected by the shingling approach after
removing templates detected by PageLevel and
SiteLevel . FullText indicates no template detec-
tion and removal.

tures into the same segment. Lower values of penalty do
not give us the improvements inherent in smoothing, but
they do not reduce the discriminative power of the Basic
classiﬁer. The same behavior is seen for Random as well.

Variations in segmentation accuracy on both datasets with
changing values of penalty are plotted in Figure 3. Just as
in the case of template detection accuracy, the segmenta-
tion accuracy also forms a unimodal curve, dropping with
high and low values of penalty. However, the drop in seg-
mentation accuracy is larger for changes in penalty values
as compared to drop in detection accuracy. This is because
the smoothing impacts the segmentation performance more
directly, as compared to detection performance. As we in-
crease (decrease) penalty values the number of groups of
leaves obtained are lesser (greater) than the manually la-
beled groups. Both these changes negatively impact the
segmentation performance. Another interesting diﬀerence is
that template detection accuracy achieves high values even
when segmentation performance is not at its peak. The
reason is that the manual labeling is binary (template or
non-template), while the segments we ﬁnd are labeled with
real numbered scores. Hence, we can still achieve a high
template detection accuracy when the smoothing function
places leaves into groups smaller than those in the manual la-
bellings. However, groups smaller than those in the manual
labeling causes a decrease in segmentation accuracy. This
indicates that if achieving good segmentation is our primary
objective, using a slightly higher value of penalty might be
advantageous.

To summarize, in this section we showed that PageLevel
accurately segments webpages, and also labels the segments
appropriately as template or non-template. Further, we
showed that isotonic smoothing is critical to its success, con-
tributing to increases in both segmentation and template
detection accuracies. We were unable to provide any com-
parisons with the site-level approach on the human labeled
data, since SiteLevel needs many pages from each website
in order to make template judgments for pages.

Next we show that webpage template detection is very
useful as a pre-processing step in several applications, such
as ﬁnding webpages with duplicate content, and webpage
classiﬁcation. Furthermore, since in these datasets we have
several webpages from the same website available, we also
present an evaluation comparing PageLevel with the site-
level template detection approach.
6.2 Application to duplicate detection

Duplicate webpages and mirrored websites present chal-
lenging problems to web search engines that crawl and index
them. Duplicated pages use up valuable index space and
duplicate results returned for search queries spoil the user

experience. Hence, detection of duplicates on the Web in a
scalable fashion has been the subject of much research [4, 5,
8]. Most duplicate detection methods rely on the concept of
shingles. For each webpage, shingles are extracted by mov-
ing a window of ﬁxed length over the text, and the ones with
the N smallest hash values are stored. Two documents that
share shingles are then considered to be near-duplicates.

Problems caused by templates. The templates regions
often contain text whose purpose is orthogonal to the main
content of the webpage. Hence, this templated content must
not be used while making decisions about whether pages
are duplicates. For example, text present within navigation
bars, copyright notices etc., must not be compared when
two pages are being checked for duplicate material. The
presence of templated content of webpages can foil dupli-
cate detection algorithms whenever the shingling process
retains shingles from the templated regions. Two pages
that have absolutely the same content, say the exact same
AP news story repeated across two diﬀerent news websites,
might be considered non-duplicates if the shingling process
retains shingles from the template regions of the webpages
as this portion of the webpages is diﬀerent. This can lead to
false negatives and cause us to return duplicate results for
queries. Similarly, two webpages with the same templated
content but diﬀerent main content might be considered du-
plicates if all the shingles hit the templated region. This
can result in false positives and cause us to ignore valuable
content on the web. In this section we evaluate the eﬀect of
templates on duplicate detection performance, and also also
compare the template detection performance of PageLevel
to the site-level approach.

The Lyrics dataset. We constructed the Lyrics dataset
by obtaining the webpages containing lyrics for the same
song from three diﬀerent websites. This way we knew that
the webpages from diﬀerent websites containing lyrics to the
same song should be considered duplicates5. We also knew
that webpages containing lyrics of diﬀerent songs, irrespec-
tive of what website they come from, should be considered
non-duplicates. We were able to obtain 2359 webpages from
the websites www.absolutelyrics.com, www.lyricsondemand.
com, and www.seeklyrics.com containing lyrics to songs by
artists ABBA, BeeGees, Beatles, Rolling Stones, Madonna,
and Bon Jovi. We chose to get lyrics by a few diverse artists
in order to minimize the possibility of cover songs. The
Lyrics dataset consists of 1711 duplicate pairs (webpages
with lyrics of the same song from diﬀerent websites) and
2058 non-duplicate pairs (webpages with lyrics of diﬀerent
songs from the same website).

Experimental setup. SiteLevel was run on all the pages
on each lyrics website and the threshold parameter was set
to 10%. This setting was seen to perform well in [11].

We used a standard shingling process. Before the shin-
gling was performed the text of the webpage is made lower-
case and only alphanumeric characters are retained. Shin-
gles are computed over moving windows of 6 consecutive
words each, and the 8 minimum hashes are stored for each
webpage. A pair of pages is tagged as a duplicate if there
are at least 4 matching hashes out of the 8 for each webpage.

5Actually, these might only be near-duplicates, due to tran-
scription errors on the diﬀerent pages. However, this aﬀects
all algorithms equally.

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages68Categories

FullText SiteLevel PageLevel PageLevel

camera
camera
camera
camera
mobile
mobile
mobile

notebook
notebook

printer

mobile

notebook

printer

tv

notebook

printer

tv

printer

tv
tv

Average

55.10
30.48
32.76
34.82
60.45
21.06
23.03
39.9
43.47
41.17
38.22

64.17
35.61
38.84
40.67
70.26
24.16
23.86
43.95
48.85
44.12
43.45

Basic
59.57
35.24
39.75
39.51
64.94
24.5
24.85
48.7
50.2
48.7
43.6

Smooth

60.17
40.03
41.18
37.21
66.92
28.55
24.79
53.91
50.94
47.57
45.13

Table 3: Averaged classiﬁcation accuracies on 2-class problems. The best accuracies for each class combination
are in bold.

We run this experiment under diﬀerent settings: (1) all seg-
ments (and words) are used (FullText), (2) only segments
tagged as non-template by PageLevel are used, and (3)
only segments tagged as non-template by SiteLevel are
used for shingling.

the text content of webpages [18] form the mainstay of web-
page classiﬁcation. In this section we perform experiments
on the eﬀect of template content on the classiﬁcation of tex-
tual content of webpages, and show that template removal
using PageLevel gives a boost in accuracy.

Results. The results of our duplicate detection experiments
are presented in Table 2. Not detecting and removing tem-
plate content (FullText) performs very badly, especially in
ﬂagging duplicate pairs. Using the PageLevel approach to
clean the data before shingling recovers 76% of the duplicate
pairs, and 92% of non-duplicate ones. These represent an
improvement of 140% and 6% respectively over the Full-
Text approach. Finally, PageLevel also outperforms the
SiteLevel template detection approach by a large margin
in both ﬂagging duplicate and non-duplicate pairs.

The Lyrics dataset is not representative of the density
of duplicates and non-duplicate pairs found on the web; we
created it to highlight the problems posed by templates to
duplicate detection algorithms. Hence, while the numbers
seen in these experiments will not apply exactly to the web in
general, the results are indicative of the beneﬁts of template
detection and removal, and the dataset serves as an appro-
priate test-bed for comparing the algorithms PageLevel
and SiteLevel.
Discussion. Why does PageLevel outperform SiteLevel
even though it is trained on the output of the latter? Com-
paring errors performed by the two on the LYRICS dataset
oﬀers us an opportunity to investigate this. As stated before,
errors occur when shingles come from templated regions of
the page. Many of the errors committed by SiteLevel in-
volved shingles from a segment on “Popular lyrics by this
Artist” that seemed to change based on the artist whose
song lyrics were being displayed. Since this segment changed
within webpages on the same site, SiteLevel was unable
to identify it as a template. However, thanks to the careful
selection of DOM node features in the page-level classiﬁer,
PageLevel generalizes beyond the site-level training data.
Thus, it picked out such segments as templates, boosting its
accuracy signiﬁcantly.
6.3 Application to webpage classiﬁcation

Automated classiﬁcation of webpages is a well-studied prob-

lem and numerous approaches have been proposed for it.
While many sources of information like hyperlinks [6], site
structure [15], etc. are often used, techniques for classifying

Problems caused by templates. Even though classiﬁca-
tion algorithms are very good at identifying and removing
noisy features, in certain scenarios templates can present a
challenging problem. Consider a binary classiﬁcation prob-
lem between classes Camera and Notebook. If the template
terms (noise) in both classes are the same, a classiﬁer would
be able to detect and remove it, say, using the correlation
of features to the class labels. However, the noisy features
could diﬀer across the two classes; say, the webpages in Cam-
era class come from CNET, and those in Notebook class come
from PCConnection, the classiﬁer will not be able to remove
the template content automatically, making template detec-
tion as a pre-processing step imperative.

Dataset and experimental setup. For the classiﬁcation
experiments we used a subset of the dataset used by Vieira
et al. [24]. The dataset consisted of webpages on 5 topics
(Camera, Notebook, Mobile, Printer, TV) obtained from 4
websites, CNET, J&R, PCConnection, and ZDNET. Details
of this dataset can be obtained in [24]. From this data, we
constructed binary classiﬁcation problems in which training
data for classes C1 and C2 were taken from diﬀerent web-
sites, and the rest of the data for these classes were used
as test data. For instance, in one binary problem, C1 is
Camera and C2 is Notebook. The training data for C1 and
C2 comes from CNET and PCConnection respectively. The
test data for C1 then comprises J&R, PCConnection, and
ZDNET, while that for C2 comprises CNET, J&R, and ZD-
NET. This evaluation setting has been used previously [24,
26]. We employed a Naive Bayes classiﬁer6 for the binary
classiﬁcation problems. The classiﬁcation accuracy numbers
we report are averaged over all possible binary classiﬁcation
problems of the type mentioned above.

We run this experimental setup with diﬀerent amounts of
webpage cleaning: (1) with all segments (and words) (Full-
Text), (2) with only segments tagged as non-template by
SiteLevel, (3) only segments tagged as non-template by
the PageLevel Basic algorithm (4) only segments tagged as
non-template by the PageLevel Smooth algorithm. Recall
6www.cs.cmu.edu/~mccallum/bow/

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages69that in PageLevel Basic algorithm the smoothing function
is disabled, and only raw templateness classiﬁer assigned
scores are used.

Results. The results of the classiﬁcation experiments are
presented in Table 3. The best accuracies for each class
combination are highlighted in bold. As we can see, using
template detection as a pre-processing step always improves
the classiﬁcation accuracy of the Naive Bayes classiﬁer. Fur-
thermore, webpage cleaning via the PageLevel algorithm
outperforms SiteLevel on a majority of class combinations.
Even among the PageLevel approaches, the use of isotonic
smoothing of the templateness classiﬁer’s output results in
better template detection and removal, as evidenced by an
increase in the Naive Bayes classiﬁer accuracy. In the ﬁnal
analysis, webpage template detection and removal via our
PageLevel system increases classiﬁcation accuracy on this
dataset by an average of 18%.

7. CONCLUSIONS

We presented a framework for classiﬁer based page-level
template detection that constructs the training data and
learns the notion of “templateness” automatically using the
site-level template detection approach. We formulated the
smoothing of classiﬁer assigned templateness scores as a reg-
ularized isotonic regression problem on trees, and presented
an eﬃcient algorithm to solve it exactly; this may be of
independent interest. Using human-labeled data we empir-
ically validated our system’s performance, and showed that
template detection at the page-level, when used as a pre-
processing step to webmining applications, such as duplicate
detection and webpage classiﬁcation, can boost accuracy sig-
niﬁcantly.

Acknowledgments. We thank Rajat Ahuja, Rupesh Mehta,
Arun Ramanujapuram, and Amit Sasturkar for their valu-
able help. We also thank the reviewers for their helpful
comments.

8. REFERENCES
[1] S. Angelov, B. Harb, S. Kannan, and L.-S. Wang.
Weighted isotonic regression under the l1 norm. In
Proc. 17th SODA, pages 783–791, 2006.

[2] S. Baluja. Browsing on small screens: Recasting
web-page segmentation into an eﬃcient machine
learning framework. In Proc. 15th WWW, pages
33–42, 2006.

[3] Z. Bar-Yossef and S. Rajagopalan. Template detection

via data mining and its applications. In Proc. 11th
WWW, pages 580–591, 2002.

[4] K. Bharat, A. Broder, J. Dean, and M. R. Henzinger.
A comparison of techniques to ﬁnd mirrored hosts on
the WWW. JASIS, 51(12):1114–1122, 2000.

[5] A. Z. Broder, S. C. Glassman, M. S. Manasse, and

G. Zweig. Syntactic clustering of the web. WWW6 /
Computer Networks, 29(8-13):1157–1166, 1997.

[6] S. Chakrabarti, B. E. Dom, and P. Indyk. Enhanced

hypertext categorization using hyperlinks. In Proc.
SIGMOD, pages 307–318, 1998.

[7] Y. Chen, X. Xie, W.-Y. Ma, and H.-J. Zhang.

Adapting web pages for small-screen devices. Internet
Computing, 9(1):50–56, 2005.

[8] J. Cho, N. Shivakumar, and H. Garcia-Molina.

Finding replicated web collections. In Proc. SIGMOD,
pages 355–366, 2000.

[9] B. Davison. Recognizing nepotistic links on the web.
In AAAI-2000 Workshop on Artiﬁcial Intelligence for
Web Search, pages 23–28, 2000.

[10] S. Debnath, P. Mitra, N. Pal, and C. L. Giles.

Automatic identiﬁcation of informative sections of web
pages. TKDE, 17(9):1233–1246, 2005.

[11] D. Gibson, K. Punera, and A. Tomkins. The volume

and evolution of web page templates. In Proc. 14th
WWW (Special interest tracks and posters), pages
830–839, 2005.

[12] L. Hubert and P. Arabie. Comparing partitions. J.

Classiﬁcation, 2:193–218, 1985.

[13] H.-Y. Kao, M.-S. Chen, S.-H. Lin, and J.-M. Ho.

Entropy-based link analysis for mining web
informative structures. In Proc. 11th CIKM, pages
574–581, 2002.

[14] H.-Y. Kao, J.-M. Ho, and M.-S. Chen. WISDOM:

Web intrapage informative structure mining based on
document object model. TKDE, 17(5):614–627, 2005.
[15] R. Kumar, K. Punera, and A. Tomkins. Hierarchical
topic segmentation of websites. In Proc. 12th KDD,
pages 257–266, 2006.

[16] N. Kushmerick. Learning to remove internet

advertisement. In Proc. 3rd Agents, pages 175–181,
1999.

[17] G. Milligan and M. Cooper. A study of the

comparability of external criteria for hierarchical
cluster analysis. Multivariate Behavioral Research,
21(4):441–458, 1986.

[18] T. Mitchell. Machine Learning. McGraw Hill, 1997.
[19] T. Morton-Jones, P. Diggle, L. Parker, H. O.

Dickinson, and K. Blinks. Additive isotonic regression
models in epidemiology. Statistics in Medicine,
19(6):849–859, 2000.

[20] P. M. Pardalos and G. Xue. Algorithms for a class of

isotonic regression problems. Algorithmica,
23(3):211–222, 1999.

[21] T. Robertson, F. T. Wright, and R. L. Dykstra.

Order-Restrictied Statistical Inference. Wiley, 1988.

[22] R. Song, H. Liu, J.-R. Wen, and W.-Y. Ma. Learning
block importance models for web pages. In Proc. 13th
WWW, pages 203–211, 2004.

[23] Q. Stout. Optimal algorithms for unimodal regression.

Computing Science and Statistics, 32:348–355, 2000.

[24] K. Vieira, A. Silva, N. Pinto, E. Moura, J. Cavalcanti,
and J. Freire. A fast and robust method for web page
template detection and removal. In Proc. 15th CIKM,
pages 256–267, 2006.

[25] L. Yi and B. Liu. Web page cleaning for web mining

through feature weighting. In Proc. 18th IJCAI, pages
43–50, 2003.

[26] L. Yi, B. Liu, and X. Li. Eliminating noisy

information in web pages for data mining. In Proc. 9th
KDD, pages 296–305, 2003.

[27] X. Yin and W. S. Lee. Using link analysis to improve
layout on mobile devices. In Proc. 13th WWW, pages
338–344, 2004.

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages70