Multiway SLCA-based Keyword Search in XML Data

Chong Sun

School of Computing
National University of

Singapore

Chee-Yong Chan
School of Computing
National University of

Singapore

Amit K. Goenka
School of Computing
National University of

Singapore

sunchong@soe.ucsc.edu

chancy@comp.nus.edu.sg

amitkuma@comp.nus.edu.sg

ABSTRACT
Keyword search for smallest lowest common ancestors (SLCAs)
in XML data has recently been proposed as a meaningful
way to identify interesting data nodes in XML data where
their subtrees contain an input set of keywords. In this pa-
per, we generalize this useful search paradigm to support
keyword search beyond the traditional AND semantics to
include both AND and OR boolean operators as well. We
ﬁrst analyze properties of the LCA computation and pro-
pose improved algorithms to solve the traditional keyword
search problem (with only AND semantics). We then ex-
tend our approach to handle general keyword search involv-
ing combinations of AND and OR boolean operators. The
eﬀectiveness of our new algorithms is demonstrated with a
comprehensive experimental performance study.

Categories and Subject Descriptors
H.2.4 [Database Management]: Systems—query process-
ing, texture databases

General Terms
Algorithms

Keywords
keyword search query, smallest lowest common ancestor,
XML

1.

INTRODUCTION

Keyword search is a convenient and widely-used approach
to retrieve information from both unstructured and struc-
tured data [1, 4, 7, 10, 11]. Its appeal stems from the fact
that keyword queries can be easily posed without requiring
to use a query language and knowing the schema or struc-
ture of the data being searched. For XML data, where the
data is viewed as a hierarchically-structured rooted tree, a
natural keyword search semantics is to return all the nodes
in XML tree that contain all the keywords in their subtrees.
However, this simple search semantics can result in return-
ing too many data nodes, many of which are only remotely
linked to the nodes containing the keywords.

A recent direction to improve the eﬀectiveness of keyword
search in XML data is based on the notion of smallest lowest
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

x1

x2

a2

c2

x3

x4

e1

b1

d2

x5

a1

e2

b2

c1

d1

a1

b1

a2

b2

b3
...

bn−1

bn

(a) T1

(b) T2

Figure 1: Example XML Trees T1 and T2

common ancestor (SLCA) semantics [14]. A keyword search
using the SLCA semantics returns nodes in the XML data
that satisfy the following two conditions: (1) the subtrees
rooted at the nodes contain all the keywords, and (2) the
nodes do not have any proper descendant node that satisﬁes
condition (1). The set of returned data nodes are referred
to as the SLCAs of the keyword search query. Another re-
cent work on keyword search based on the meaningful LCA
(MLCA) semantics also shares the similar principle as SLCA
[12].

The following example illustrates the diﬀerence between
the SLCA-based keyword search and the conventional LCA-
based keyword search.
Example 1.1 Consider the XML tree T1 shown in Fig-
ure 1(a), where the keyword nodes are annotated with sub-
scripts for ease of reference. Consider a keyword search using
the keywords {a, b, c, d, e} on T1. If the search is based on
the conventional LCA semantics, then the result is given by
{x1, x2, x3, x4} as x1 is the LCA of {a2, b2, c2, d2, e2}, x2 is
the LCA of {a1, b1, c2, d1, e1}, x3 is the LCA of {a1, b1, c1,
d2, e1}, and x4 is the LCA of {a1, b1, c1, d1, e1}. However, if
the search is based on the SLCA semantics, then the result
is given by {x4}. Observe that each of x1, x2, and x3 is
not a SLCA because it has a descendant node x4 that is a
2
SLCA.

The state-of-the-art algorithms for keyword search us-
ing SLCA semantics are the Scan Eager (SE) and Indexed
Lookup Eager (ILE) algorithms [14], which were shown to
be more eﬃcient than stack-based algorithms [8, 12]. The

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1043ILE algorithm is the algorithm of choice when the keyword
search involves at least one low frequency keyword, while the
SE algorithm performs better when the frequencies of the
keywords in the query do not vary signiﬁcantly. We clas-
sify both these algorithms as binary-SLCA approach (BS)
as they are both based on the same principle of comput-
ing the SLCAs for a query with k keywords in terms of a
sequence of k − 1 intermediate SLCA computations, where
each SLCA computation takes a pair of data node lists as
inputs and outputs another data node list. Speciﬁcally, con-
sider a search query with k keywords w1, ·· · , wk. Let Si
denote the list of XML data nodes that are labeled with key-
word ki, i ∈ [1, k]; and let Li denote the SLCAs for a query
with the ﬁrst i keywords, i ∈ [1, k]. The binary-SLCA algo-
rithms compute the SLCAs for w1, ·· · , wk by computing the
sequence L2, L3,··· , Lk, where each Li is computed by ﬁnd-
ing the SLCAs of Li−1 and Si (with L1 = S1). An important
observation exploited in the binary-SLCA algorithms is that
the result size is bounded by min{|S1|,· ·· ,|Sk|}; therefore,
by choosing the keyword with the lowest frequency as k1
(i.e., |S1| ≤ |Si| for i ∈ [1, k]), the algorithms can guarantee
that each |Li| ≤ |S1|, for i ∈ [2, k].

x1

x2

r1
······

x10

b11 ·· · b1001

a1 ······ a100

a101···· ·· a200

a901· ·····a1000 b10

b2

b1

Figure 2: Example XML Tree T3

However, a drawback of the binary-SLCA approach is that
by computing the SLCAs in terms of a series of intermediate
SLCA computations, it can often incur many unnecessary
SLCA intermediate computations even when the result size
is small as the following example illustrates.
Example 1.2 Consider the XML tree T3 in Figure 2. The
SLCAs for the keywords {a, b} in T3 are {x1, x2,··· , x10}.
Since |Sa| < |Sb|, the BS approach will enumerate each of
the “a” nodes in Sa to compute a potential SLCA with it.
Clearly, this approach results in many redundant computa-
tions; for example, the SLCA of ai and b1 gives the same
result x1 for i ∈ [1, 100]. In fact, the BS approach will incur
a total of 1000 SLCA computations to produce a result of
2
size 10.

Our ﬁrst contribution in this paper (Section 3) is the pro-
posal of a novel approach for processing SLCA-based key-
word search queries called multiway-SLCA approach (MS).
In contrast to the BS approach, our MS approach computes
each potential SLCA by taking one data node from each key-
word list Si in a single step instead of breaking the SLCA
computation into a series of intermediate binary SLCA com-
putations. Conceptually, each potential SLCA computed by
the BS approach can be thought of as being driven by some
node from S1 (i.e., the keyword list with the lowest fre-
quency); on the other hand, our MS approach picks an “an-
chor” node from among the k keyword data lists to drive the
multiway SLCA computation. By doing so, our approach is
able to optimize the selection of the anchor node (not nec-
essarily from S1) to maximize the skipping of redundant

computations. The following example provides an idea of
the skipping optimization of our MS approach.
Example 1.3 Consider the processing of the SLCA-based
keyword search with keywords {a, b} on T3 in Figure 2 using
our MS approach. MS will ﬁrst consider the ﬁrst data nodes
in all the keyword lists and selects the node that occurs the
latest in T3 as the anchor node. Thus, between a1 ∈ Sa
and b1 ∈ Sb, b1 will be selected as the anchor node (the
property behind this optimization will be explained later in
the paper). Next, using b1 as anchor, our approach will se-
lect the closest data nodes from the other keyword lists to
compute a potential SLCA. Thus, the ﬁrst SLCA is com-
puted for the set {b1, a100}. After the ﬁrst potential SLCA
is computed, our approach will consider the ﬁrst nodes in
the keyword lists that occur after b1 for the next computa-
tion (i.e., nodes a101 and b2). The next anchor node selected
is b2, and the next SLCA computation involves b2 and a200.
Clearly, the MS approach is able to skip many unnecessary
2
computations.

In addition to introducing the notion of anchor nodes that
we alluded to for minimizing redundant computations, we
also develop several optimizations to further maximize the
skipping of data nodes in the keyword list without compro-
mising correctnesses of query results.

Our second contribution in this paper (Section 4) is the
generalization of the SLCA-based approach to handle more
general keyword search queries beyond the implicit AND-
semantics to support any combinations of AND and OR
semantics. This enables more ﬂexible and expressive key-
word search queries such as “(a OR b) AND c AND (d or
E)” to be speciﬁed. We extend our MS approach to evaluate
general keyword search queries involving both AND and OR
operators.

Finally, our third contribution in this paper (Section 5) is
a comprehensive experimental performance evaluation which
demonstrates that our proposed multiway-SLCA approach
outperforms the previous binary-SLCA approach for both
traditional keyword search queries as well as generalized key-
word search queries.

2. PRELIMINARIES

Let K = {w1,·· · , wk} denote an input set of k keywords,
where each keyword wi is associated with a set Si of nodes
in an XML document T (sorted in document order1). A set
of nodes S = {v1,··· , vk} is deﬁned to be a match for K if
|S| = |K| and each vi ∈ Si for i ∈ [1, k]. We use Si to denote
the data node list (sorted in document order) associated
with the keyword wi. For simplicity and without loss of
generality, we assume w1 to be the lowest frequency keyword
among the keywords in K; i.e., |S1| ≤ |Si| for i ∈ [1, k].

A node v in T is a lowest common ancestor (or LCA) for
K if v is the lowest common ancestor node of some match
S. Moreover, v is also a smallest lowest common ancestor
(or SLCA) for K if each descendant of v in T is not a LCA
for K.
Given two nodes v and w in a document tree T , v ≺p w
denotes that v precedes w (or w succeeds v) in document
order in T ; and v (cid:5)p w denotes that v ≺p w or v = w.
More generally, given two matches V = {v1,··· , vk} and
1Document order corresponds to a preorder traversal of doc-
ument nodes.

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1044to v as follows:

closest(v, S) =(cid:0)(cid:2)
(cid:3)

pred(v, S)

if lca(v, next(v, S)) ≺a
lca(v, pred(v, S)),

next(v, S) otherwise.

W = {w1,··· , wk}, where vi, wi ∈ Si, ∀ i ∈ [1, k], we say
that V precedes W (or W succeeds V ), denoted by V ≺p W ,
if they satisfy both the following properties: (1) vi (cid:5)p wi
for each i ∈ [1, k]; and (2) V (cid:7)= W .
We use v ≺a w to denote that v is a proper ancestor of
w in T , and v (cid:5)a w to denote that v = w or v ≺a w.
Consider a node v and a set of nodes S. The function
f irst(S) returns the “ﬁrst” node v(cid:2) ∈ S such that v(cid:2) (cid:5)p vi
for each vi ∈ S. Similarly, the function last(S) returns the
“last” node v(cid:2) ∈ S such that vi (cid:5)p v(cid:2)
for each vi ∈ S. Both
functions return null if any of its input argument values is
null.
The function out(v, S) returns the “ﬁrst” node v(cid:2) ∈ S
such that v ≺p v(cid:2)
is not a descendant of v or equal to
v; i.e., out(v, S) = f irst({v(cid:2) ∈ S | v ≺p v(cid:2), v (cid:7)(cid:5)a v(cid:2)}). The
function returns null if no such node exists or if v is null.

and v(cid:2)

The function next(v, S) returns the ﬁrst node in S that
succeeds v if it exists; otherwise, it returns null. The func-
tion pred(v, S) returns the predecessor of v in S, that is,
the last node in S that precedes v if it exists; otherwise, it
returns null.

The function closest(v, S) computes the closest node in S

However, closest(v, S) returns null if both pred(v, S) and
next(v, S) are null; and it returns the non-null value if ex-
actly one of pred(v, S) and next(v, S) is null.

The function lca(S) computes the lowest common ances-
tor (or LCA) of the set of nodes S and returns null if any of
its arguments is null.

For notational convenience, we assume that the root node
of the data tree T has a virtual parent node, denoted by
droot, such that droot is a proper ancestor node of every
node in T .

The following example illustrates our deﬁnitions.

Example 2.1 Consider the XML document tree T1 shown
in Figure 1(a) and the keyword search query K = {a, b, c}.
Note that {a1, b1, c2} is a match for K; but neither {a1, c2}
nor {x1, b2, c1} is a match for K. We have e1 ≺p b2 but e2 (cid:7)≺p
x4. Moreover, {a1, b1, c1} ≺p {a2, b2, c2}. We have x2 (cid:5)a e1
and x3 (cid:7)(cid:5)a c2.
If S = {d1, c1, d2}, then f irst(S) = d1,
last(S) = d2, out(x4, S) = d2, next(x4, S) = c1, next(c1, S) =
d2, next(e2, S) = null, pred(a1, S) = d1, pred(x4, S) = null,
closest(b1, S) = c1, closest(a2, S) = d2, and lca(S) = x3. 2

3. OUR APPROACH

In this section, we present our new approach of process-
ing SLCA-based keyword search queries called the multiway-
SLCA approach (MS).

As alluded in the introduction, the key motivation behind
our MS approach is to avoid the unnecessary overhead of
the BS approach where SLCAs are computed in terms of
intermediate SLCA computations by enumerating each data
node in the lowest-frequency keyword list. As Example 1.2
demonstrates, by rigidly driving the SLCA computations
from the lowest-frequency keyword list can result in many
redundant computations particularly when the size of the
results is small.

We ﬁrst introduce the notion of anchor nodes in Sec-

tion 3.1 which is a central idea in our MS approach. Sec-
tion 3.2 then presents several important properties about
anchored matches. We present our ﬁrst MS-based algorithm
called basic multiway-SLCA (BMS) in Section 3.3, followed
by our second improved MS-based algorithm, called incre-
mental multiway-SLCA (IMS), in Section 3.4.
3.1 Anchor Nodes
A match S = {v1, ··· , vk} is said to be anchored by a
node va ∈ S if for each vi ∈ S − {va}, vi = closest(va, Si).
We refer to va as the anchor node of S.

The concept of anchor nodes is important to our multiway-
SLCA approach as it enables us to restrict matches to those
that are anchored by some nodes as the following result
demonstrates.

Lemma 3.1. If lca(S) is an SLCA and v ∈ S, then lca(S) =

), where S(cid:2)

is the set of nodes anchored by v.

lca(S(cid:2)
Proof. The proof is established by contradiction. Sup-
pose that lca(S) (cid:7)= lca(S(cid:2)
). Since lca(S) is an SLCA and
v ∈ S(cid:2) ∩ S, this implies that lca(S(cid:2)
) is a proper ancestor of
lca(S). It follows that there must exists some Si such that
lca(S) (cid:7)(cid:5)a closest(v, Si). However, since S ∩ Si (cid:7)= ∅, we
have a contradiction.

Thus, our MS approach only considers anchored sets for
computing potential SLCAs. The optimization potential of
the above result was illustrated earlier in Example 1.3. Re-
call that b1 is the selected anchor node for the SLCA com-
putation with a100 = closest(b1, Sa). By choosing b1 as the
anchor node (instead of using a1 as in the BS approach),
for the ﬁrst SLCA computation, it follows from Lemma 3.1
that it is unnecessary to compute SLCAs for matches that
include any ai, i ∈ [1, 99] because such matches would neces-
sarily include b1 and Lemma 3.1 states that it is not possible
to generate new SLCAs with such matches.
Note in our MS approach, an anchor node can be chosen
from any of keyword data lists (i.e., S1, ··· , Sk). For nota-
tional convenience, when we denote an anchor node as vm,
we also mean that vm is selected from Sm, m ∈ [1, k].
3.2 Properties

In this section, we present several important properties
that form the basis of the optimizations in our MS-based
algorithms.
S and S(cid:2)
must necessarily be disjoint.

The following result states that if the LCAs of two matches
are both distinct SLCAs, then the two matches

) are distinct SLCAs,

Lemma 3.2. If lca(S) and lca(S(cid:2)

then S ∩ S(cid:2)

= ∅.

Proof. Suppose lca(S) and lca(S(cid:2)

) are two distinct SLCAs.

If both S and S(cid:2)
contains some common node, then lca(S)
and lca(S(cid:2)
) must be related in one of three possibilities:
); (b) lca(S) is an ancestor of lca(S(cid:2)
(a) lca(S) = lca(S(cid:2)
);
or (c) lca(S) is a descendant of lca(S(cid:2)
). Case (a) contra-
dicts the fact that lca(S) and lca(S(cid:2)
) are distinct SLCAs.
Cases (b) and (c) imply that either lca(S) or lca(S(cid:2)
) is not
a SLCA, contradicting the fact that lca(S) and lca(S(cid:2)
) are
two distinct SLCAs. Thus, it follows by contradiction that
S ∩ S(cid:2)

= ∅.

Lemma 3.3. Let V and W be two matches such that V ≺p
W . If lca(W ) is not a descendant of lca(V ), then for any

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1045match X where W ≺p X, lca(X) is also not a descendant
of lca(V ).

Proof. Let V ≺p W and lca(W ) is not a descendant of
lca(V ), then either (a) all the nodes in the subtree rooted at
lca(V ) precede all the nodes in the subtree rooted at lca(W ),
or (b) lca(W ) is an ancestor of lca(V ).

For case (a), if lca(X) is a descendant of lca(V ), then
all the nodes in the subtree rooted at lca(X) must precede
all the nodes in the subtree rooted at lca(W ) contradicting
that X succeeds W . Thus, lca(X) cannot be a descendant
of lca(V ) for case (a). For case (b), W must contain some
node w such that w is not a descendant of lca(V ) and w
succeeds V ; if not, lca(W ) cannot be an ancestor of lca(V ).
Therefore, if lca(X) is a descendant of lca(V ), then this
implies that w ∈ W succeeds X, contradicting the fact that
X succeeds W . Thus, lca(X) cannot be a descendant of
lca(V ) for case (b) as well.

Lemma 3.3 is a useful generalization of Lemma 2 in [14]
that is exploited in our algorithms to determine whether a
computed LCA is guaranteed to be a SLCA. Speciﬁcally, if
V ≺p W and lca(W ) is not a descendant of lca(V ), then
one can conclude that lca(W ) is an SLCA.

Lemma 3.4. Consider two matches S and S(cid:2)
, and S is anchored by some node v. If S(cid:2)

S(cid:2)
node u where u (cid:5)p v, then lca(S(cid:2)
or an ancestor of lca(S).

, where S ≺p
contains some
) is either equal to lca(S)

. Since {u, v(cid:2)} ⊆ S(cid:2)

, we must have v (cid:5)p v(cid:2)

, v must be a descendant of lca(S(cid:2)

Proof. Let v ∈ Si, i ∈ [1, k]. Let Si ∩ S(cid:2)

= {v(cid:2)}. Since
S ≺p S(cid:2)
and
u (cid:5)p v (cid:5)p v(cid:2)
) which
implies that lca(S(cid:2)
) is either equal to lca(S), a descendant
of lca(S), or an ancestor of lca(S). However, if lca(S(cid:2)
) is a
descendant of lca(S), it would contradict the fact that S is
anchored by v. Therefore, lca(S(cid:2)
) is either equal to lca(S)
or an ancestor of lca(S).

Lemma 3.4 provides a useful property to optimize the se-
lection of the next match to be considered. Speciﬁcally, if
we have considered a match S that is anchored by a node
va, then we can skip matches that contain any node v (cid:5)p va
in S.

Lemma 3.5. Let S and S(cid:2)

contains
two nodes, where one is a descendant of lca(S), while the
other is not, then lca(S(cid:2)
) is either equal to lca(S) or an
ancestor of lca(S).

be two matches. If S(cid:2)

Proof. Let v, w ∈ S(cid:2)

, where v is a descendant of lca(S),
and w is not a descendant of lca(S). Since v is a descendant
of both lca(S) and lca(S(cid:2)
) are
equal or one is an ancestor of the other. However, since w is
not a descendant of lca(S), lca(S(cid:2)
) cannot be a descendant
of lca(S); and the claim follows.

), either lca(S) and lca(S(cid:2)

Lemma 3.5 provides another useful property to optimize
the selection of the next match to be considered. Speciﬁ-
cally, if we have considered a match S and lca(S) has been
conﬁrmed to be an SLCA, then we can skip matches S(cid:2)
that
contains some node that is a descendant of lca(S) as well as
another node that is a not a descendant of lca(S).

Lemma 3.6. Let S be a set of nodes. Then lca(S) =

lca(f irst(S), last(S)).

Proof. Since {f irst(S), last(S)} ⊆ S, therefore, lca(S)
is either equal to or an ancestor of lca(f irst(S), last(S)).
Clearly, for each v ∈ S where f irst(S) ≺p v ≺p last(S), v
is a descendant of lca(f irst(S), last(S)). It follows that
lca(S) = lca(f irst(S), last(S)).

Lemma 3.6 states that the LCA of a set of nodes S is
equivalent to the LCA of the two extreme nodes (i.e., the
ﬁrst and last nodes) in S. This property enables the LCA
computation for a set of nodes S to be improved signiﬁcantly
as it suﬃces to compute the LCA of S in terms of only its
ﬁrst and last nodes.
3.3 Basic Multiway-SLCA Algorithm (BMS)
In this section, we present our ﬁrst Multiway-SLCA-based
algorithm called Basic Multiway-SLCA (BMS) for comput-
ing SLCAs for a set of keywords {w1,· ·· , wk}. The details
are given in Algorithm 1 which takes k keyword data lists
S1,··· , Sk as input and returns the SLCAs as a collection
of nodes. Each Si is the list of data nodes associated with
keyword wi.

The algorithm computes the SLCAs iteratively. At each
iteration, an anchor node vm is judiciously selected to com-
pute the match anchored by vm and its LCA (denoted by
α). If α is potentially an SLCA, it is maintained in an inter-
mediate SLCA result list given by α1,··· , αn, n ≥ 1, where
all the LCAs αi in the list are deﬁnite SLCAs except for
the most recently computed candidate αn. To minimize the
computation of LCAs that are not SLCAs, it is important
to optimize the anchor node selected at each iteration.

Initially, step 2 initializes the ﬁrst candidate SLCA α1 to
be droot (the virtual root node of data tree); if α1 remains
as droot at the end of the algorithm (step 22), then it means
that the SLCA result list is empty. The ﬁrst anchor node
vm is selected in step 1. Instead of choosing the ﬁrst node
v1 ∈ S1 as the anchor (as is done in the BS approach), BMS
selects the ﬁrst node vm ∈ Sm, m ∈ [1, k] that is the “fur-
thest” node among all the ﬁrst nodes in S1,··· , Sk. In doing
so, all the nodes in S1 that precede u1 = closest(vm, S1) are
skipped from consideration as anchor nodes. The correct-
ness of this optimization stems from the fact that for each
v ∈ S1 that precedes u1, closest(v, Sm) must be vm; there-
fore, by Lemma 3.1, no SLCAs will be missed out by using
vm as the ﬁrst anchor node.

Steps 4 to 9 further optimize the selection of the anchor
node to ensure that the total number of candidate SLCAs
computed is no more than |S1| (elaborated in Section 3.5).
Speciﬁcally, if the selected anchor node vm precedes
closest(vm, S1), then by Lemma 3.1, no SLCAs will be omit-
ted by replacing the anchor node vm with closest(vm, S1).
The usefulness of this optimization is illustrated in Exam-
ple 3.2.

After an anchor node vm has been chosen, step 10 com-
putes the match anchored by vm, and step 11 computes the
LCA α of this match in terms of only its ﬁrst and last nodes
(based on Lemma 3.6). Steps 12 to 16 check whether the
newly computed LCA α can be a candidate SLCA; and if
so, whether α can be used to eliminate the previous candi-
date SLCA αn. Steps 17 to 20 optimize the selection of the
next anchor node by choosing the furthest possible node that
maximizes the number of skipped nodes: step 17 is based
on Lemma 3.4 while steps 18 to 20 are based on Lemma 3.5.

Example 3.1 Consider computing SLCAs for the set of

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1046vm = v1

if (m (cid:3)= 1) then

v1 = closest(vm, S1)
if (vm ≺p v1) then

Algorithm 1 Basic Multiway-SLCA (S1,··· , Sk)
1: let vm = last({f irst(Si) | i ∈ [1, k]}), where vm ∈ Sm
2: initialize n = 1; α1 = droot
3: while (vm (cid:3)= null) do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end while
22: if (α1 = droot) then return ∅ else return {α1,·· · , αn}

end if
end if
vi = closest(vm, Si) for each i ∈ [1, k], i (cid:3)= m
α = lca(f irst(v1,· ·· , vk), last(v1,·· · , vk))
if (αn (cid:5)a α) then
else if (α (cid:3)(cid:5)a αn) then
n = n + 1; αn = α

end if
vm = last({next(vm, Si) | i ∈ [1, k], vi (cid:5)p vm})
if (vm (cid:3)= null) and (αn (cid:3)(cid:5)a vm) then

vm = last({vm} ∪ {out(αn, Si) | i ∈ [1, k], i (cid:3)= m})

αn = α

end if

keywords {a, b, c, d, e} on the data tree T1 in Figure 1(a)
using the BMS algorithm. Since each keyword has the same
frequency, let S1 be the list of data nodes for keyword “a”.
The ﬁrst anchor node selected is c1, and the ﬁrst candidate
SLCA computed is α1 = lca(d1, e1, b1, a1, c1) = x4. The
next anchor node selected is a2, and the second candidate
SLCA computed is α = lca(d2, e2, b2, c2, a2) = x1. However,
since x1 (cid:5)a x4, x1 is not a SLCA. The next anchor node
selected has a null value (due to next(a2, S1) = null), and
the algorithm terminates with x4 as the only SLCA.
2

The next example illustrates the importance of the opti-

mization performed by steps 4 to 9.
Example 3.2 Consider computing SLCAs for the set of
keywords {a, b} on the datatree T2 in Figure 1(b). Here,
S1 refers to the list of nodes for keyword “a”. Using a
non-optimized BMS algorithm that excludes steps 4 to 9,
the ﬁrst anchor node is b1 and the candidate SLCA com-
puted is lca(b1, a2) = b1. Similarly, the subsequent se-
quence of anchor nodes selected is b2, b3, ··· , bn, and the
candidate SLCA computed for each of these anchor nodes
bi is lca(bi, a2) = b1. Clearly, the non-optimized BMS in-
curs many redundant SLCA computations that involve the
same a2 node.
In general, the non-optimized BMS per-
forms poorly when many anchor nodes share the same clos-
est node (w.r.t.
some keyword) that succeeds the anchor
nodes. The BMS algorithm avoids this problem by bound-
ing the number of computed SLCAs to be no more than
|S1| using steps 4 to 9. In this case, the ﬁrst anchor node
selected is optimized to a2 and the ﬁrst candidate SLCA
computed is lca(a2, b1) = b1. The next anchor node se-
lected has a null value (since next(a2, S1) = null) and the
algorithm terminates without any redundant SLCA compu-
tations. Thus, the number of candidate SLCA computations
is reduced from n to just one.
2

3.4 Incremental Multiway-SLCA Algorithm

(IMS)

In this section, we present our second Multiway-SLCA-
based algorithm called Incremental Multiway-SLCA (IMS),

vm = v1

if (m (cid:3)= 1) then

v1 = closest(vm, S1)
if (vm ≺p v1) then

end if
end if
P = {pred(vm, Si) | i ∈ [1, k], i (cid:3)= m} ∪ {vm}
N = {next(vm, Si) | i ∈ [1, k], next(vm, Si) (cid:3)= null}
initialize rmax = last(N );
repeat

Algorithm 2 Incremental Multiway-SLCA (S1,··· , Sk)
1: let vm = last({f irst(Si) | i ∈ [1, k]}), where vm ∈ Sm
2: initialize n = 1; α1 = droot
3: while (vm (cid:3)= null) do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end while
29: if (α1 = droot) then return ∅ else return {α1, ·· · , αn}

until (r = null) or (α (cid:3)(cid:5)a r) or (r = rmax)
if (r = null) or (α (cid:3)(cid:5)a r) then

if (αn (cid:5)a α) then
else if (α (cid:3)(cid:5)a αn) then
n = n + 1; αn = α

remove (cid:3) from P , where (cid:3) = f irst(P )
α = lca((cid:3), r)
r = last(r, v) where v ∈ N s.t. v = next(vm, Sj ), (cid:3) ∈ Sj

end if
vm = last(r, out(αn, S1), ··· , out(αn, Sk))

αn = α

r = vm

else

vm = r

end if

whose details are shown in Algorithm 2. IMS is an optimized
variant of BMS that reduces the number of LCA computa-
tions.
In BMS (Algorithm 1), each computation of α incurs at
least 2k − 1 LCA computations: each of the k − 1 calls to
closest function in step 10 requires two LCA computations,
and step 11 adds another LCA computation.

To avoid the large number of LCA computations incurred
by an explicit computation of M , the IMS algorithm deter-
mines f irst(M ) and last(M ) without actually computing
M . In the following, we analyze the properties of f irst(M )
and last(M ), and explain how this can be achieved. By def-
inition of the match M anchored by vm, M must satisfy the
following three conditions:

1. M ⊆ {vm} ∪ P ∪ N , where P = {pred(vm, Si) | i ∈

[1, k], i (cid:7)= m, pred(vm, Si) (cid:7)= null} and N = {next(vm, Si)
| i ∈ [1, k], i (cid:7)= m, next(vm, Si) (cid:7)= null};

2. M ∩ Si (cid:7)= ∅ ∀ i ∈ [1, k]; and
3. vm ∈ M .

Since M must contain vm and every node in P precedes vm,
it follows that f irst(M ) ∈ P ∪ {vm}. Furthermore, last(M )
can be determined once f irst(M ) is known. Let P (cid:2) ⊆ P
denote the subset of nodes in P that precedes f irst(M ) (i.e.,
= {v ∈ P | v ≺p f irst(M )}); and let N(cid:2) ⊆ N denote the
P (cid:2)
subset of nodes in N that corresponds to P (cid:2)
that succeeds vm
= {next(vm, Si) | i ∈ [1, k], pred(vm, Si) ∈ P (cid:2)}).
(i.e., N(cid:2)
Since P (cid:2) ∩ M = ∅, in order for M to satisfy condition (2),
it is necessary that M ⊇ N(cid:2)
. Moreover, since |P (cid:2)| = |N(cid:2)|
and |M| = k, we must have M = (P − P (cid:2)
) ∪ {vm} ∪ N(cid:2)
and last(M ) = last(N(cid:2)
Since there are |P| + 1 possible values for f irst(M ), let
M1, M2,··· , M|P|+1 denote the sequence of matches where
for each i ∈ [1, |P|+1], we have (1) vm ∈ Mi; (2) f irst(Mi) ∈

).

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1047P ∪ {vm}; and (3) f irst(M1) ≺p f irst(M2) ≺p ·· · ≺p
f irst(M|P|+1). In other words,
f irst(Mi) = (cid:0)(cid:2)
(cid:3)

f irst(P ∪ {vm})
f irst((P ∪ {vm})− otherwise.
i−1
j=1 f irst(Mj))
(cid:4)

Based on the preceding analysis of f irst(M ) and last(M ),
last(Mi) can be computed incrementally as follows:

if i = 1,

(1)

last(Mi) =  vm
where f irst(Mi−1) ∈ Sx.
It follows that

last(Mi−1 ∪
{next(vm, Sx)})

if i = 1,
otherwise.

(2)

f irst(M1) ≺p ··· ≺p f irst(M|P|+1) ≺p
last(M1) (cid:5)p ··· (cid:5)p last(M|P|+1).

(3)

Clearly, M = Mj for some j ∈ [1, |P|+1], where lca(Mi) (cid:5)a
lca(Mj ) for i ∈ [1, |P| + 1]. We can characterize Mj by the
following two properties:
(P1) for i ∈ [1, j), lca(Mi) (cid:5)a last(Mi+1); and
(P2) if j < |P| + 1, then lca(Mj ) (cid:7)(cid:5)a last(Mj+1).
Property (P1) implies that lca(Mi) (cid:5)a lca(Mi+1) for i ∈
[1, j). Speciﬁcally, since f irst(Mi) ≺p f irst(Mi+1) ≺p last(Mi)
(by Equation (3)), it follows that lca(Mi) (cid:5)a f irst(Mi+1);
combining this with property (P1), we have lca(Mi) (cid:5)a
lca(Mi+1).
Property (P2) implies that lca(Mi) (cid:5)a lca(Mj) for i ∈
(j, |P| + 1]. To see this, note that lca(Mj ) (cid:5)a f irst(Mi)
for i ∈ (j, |P| + 1] (by Equation (3)). Furthermore, since
lca(Mj ) (cid:7)(cid:5)a last(Mj+1) (property (P2)) and last(Mj+1) (cid:5)p
last(Mi) for i ∈ (j + 1,|P| + 1] (by Equation (3)), it follows
that lca(Mj ) (cid:7)(cid:5)a last(Mi) for i ∈ (j, |P| + 1]. Thus, for
i ∈ (j,|P| + 1], we have Mj ≺p Mi, lca(Mj ) (cid:5)a f irst(Mi)
and lca(Mj) (cid:7)(cid:5)a last(Mi); it follows from Lemma 3.5 that
lca(Mi) (cid:5)a lca(Mj).

The IMS algorithm (Algorithm 2) shares many similarities
with the BMS algorithm in the previous section. The key
diﬀerence lies in steps 10 to 17 which determines lca(M ) for
a match M anchored by a node vm without actually comput-
ing M . The repeat loop enumerates a sequence of matches
M1,M2,· ·· to compute f irst(M ) and last(M ) and hence
lca(M ). In the ith iteration of the repeat loop (steps 13 to
17), f irst(Mi) is computed in step 14 as (cid:3), and αi is com-
puted in step 15 (with r representing last(Mi)). Step 16
determines last(Mi+1) for the next iteration. The search
for M is terminated when any one of the three conditions
in step 17 is met. Firstly, if r = null, then it means that
next(vm, Sj) = null and there are no further matches in
the data; therefore, α = lca(M ) and the next anchor node
is correctly set to null by step 24. Secondly, if α (cid:7)(cid:5)a r,
then α = lca(M ) and Lemma 3.5 is applied to optimize
the selection of the next anchor node in step 24. Finally,
if r = last(N ), then it means that all the matches Mi
subsequently enumerated within the repeat loop must have
last(Mi) = last(N ) as well; therefore, M must correspond
to the very last match in the enumeration. To quickly skip
to this last match without continuing with the enumeration,
step 26 applies Lemma 3.1 to update the next anchor node
to be r.

Note that the number of LCA computations incurred by
IMS for each candidate SLCA computation is at least one
(one iteration of repeat loop) and at most k + 1 (k − 1 it-
erations of repeat loop and one call to closest function). In
contrast, BMS requires between 2k − 1 and 2k + 1 LCA
computations.
Example 3.3 Consider again computing SLCAs for the set
of keywords {a, b, c, d, e} on the data tree T1 in Figure 1(a),
where S1 is associated with keyword “a”. Using the IMS al-
gorithm, the ﬁrst anchor node selected is c1, P = {d1, e1, b1,
a1, c1}, and N = {d2, e2, b2, c2, a2}. In the ﬁrst iteration of
the repeat loop, α = lca(d1, c1) = x4. r is then updated to
d2 and the repeat loop terminates since x4 (cid:7)(cid:5)a d2. Therefore,
the ﬁrst candidate SLCA computed is α1 = x4. The next an-
chor node selected is a2, P = {d2, e2, b2, c2, a2}, and N = {}.
In the ﬁrst iteration of the repeat loop, α = lca(d2, a2) = x1.
r is then updated to null and the repeat loop terminates.
Therefore, since α (cid:5)a α1, α is deﬁnitely not a SLCA. The
next anchor node has a null value and the algorithm ter-
minates with x4 as the only SLCA. The number of LCA
computations incurred by the IMS algorithm is only two. In
contrast, the BMS algorithm incurs 18 LCA computations
2
(Example 3.1).

3.5 Analysis

In this section, we analyze the time complexity of our new
algorithms. We begin by establishing an upper bound on the
number of candidate SLCAs computed by each of the BMS
and IMS algorithms.

Lemma 3.7. The number of candidate SLCAs computed
by each of the BMS and IMS algorithms is no more than
|S1|.

Proof. We prove the claim for BMS; the proof for IMS
follows similarly. The upper bound is established by showing
that for any two candidate SLCAs computed by BMS, their
corresponding matches do not contain the same node from
S1. By step 1, the ﬁrst anchor node selected either succeeds
or is equal to f irst(S1). Let vm be the anchor node used to
compute a candidate SLCA lca(M ). By the optimization in
steps 4 to 9, vm (cid:5)p closest(vm, S1). Moreover, by step 17,
if M ∩ S1 = {v1}, then the next anchor node selected either
succeeds or is equal to next(v1, S1). Thus, since no two
matches computed by BMS share the same node from S1,
the claim is established for BMS.

We now consider the costs of the various functions. Let
d denote the height of the XML data tree, and let S denote
the data node list with the highest frequency; i.e. |Si| ≤ |S|
for i ∈ [1, k]. As in [14], we assume that each data node is
stored together with its Dewey label which enables the lca
function to be computed eﬃciently in O(d) time. The cost
of f irst(v1,··· , vk) and last(v1,··· , vk) is each O(k). The
cost of pred(v, Si), next(v, Si), out(v, Si), and closest(v, Si)
is each O(d log(|Si|)) based on a binary search of Si and
comparing nodes using their Dewey labels.
For the BMS algorithm, the cost to compute a candidate
SLCA is O(kd log(|S|)) (due to step 10). Since there are at
most |S1| candidate SLCAs (by Lemma 3.7), the time com-
plexity of BMS algorithm is O(kd|S1| log(|S|)). For the IMS
algorithm, the cost to compute a candidate SLCA is also
O(kd log(|S|)) (due to steps 10 and 11); thus the time com-
plexity of IMS algorithm is also O(kd|S1| log(|S|)). However,

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1048let T = result of evaluating Ci using some AND-algorithm
for each node v ∈ T do

Algorithm 3 Simple AND-OR-SLCA (Q)
1: let Q = C1 ∨ C2 ∨ ·· · ∨ Ck, where each
Ci = wi,1 ∧ wi,2 ∧ ·· · ∧ wi,ni , ni ≥ 1

initialize toDeleteV = false
for each node v(cid:2) ∈ R do

toDeleteV = true
exit inner for-loop
else if (v(cid:2) (cid:5)a v) then
remove v(cid:2)

from R

) then

if (v (cid:5)a v(cid:2)

2: initialize R to be empty
3: for i = 1 to k do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end for
21: return R

end if
end for
R = R ∪ T

end if
end for
if (toDeleteV) then

delete v from T

our experimental results show that IMS outperforms BMS
as the number of candidate SLCAs computed by IMS is less
than that by BMS (by up to a factor of 30).
respectively, O(kd|S|) and O(|S1|kd log(|S|) + |S1|2) [14].

In comparison, the time complexities of SE and ILE are,

4. AND-OR KEYWORD SEARCH

In this section, we examine how to process more gen-
eral SLCA-based keyword search queries that go beyond
the AND-semantics in conventional queries to support any
combination of AND and OR boolean operators. We con-
sider AND-OR keyword search queries of the form: Q =
(Q) | Q and Q | Q or Q | w, where w denotes some keyword.
In the following, we consider two approaches to process
SLCA-based AND-OR keyword search queries. The ﬁrst
approach is a straightforward application of the algorithms
presented in the preceding section for processing conven-
tional SLCA-based AND-keyword search queries by express-
ing the general AND-OR queries in disjunctive normal form
(DNF). The second approach is an extension of our Multiway-
SLCA approach (MS).

4.1 Simple AND-OR Algorithm (SA)

A straightforward approach to process a general AND-
OR query Q is to rewrite Q in DNF and evaluate it in two
stages: ﬁrst, evaluate each disjunct in Q using an exist-
ing AND-query evaluation algorithm; next, the results of
the individual evaluations are combined by eliminating in-
termediate SLCAs that are ancestor nodes of some other
intermediate SLCAs. The algorithm, referred to as Simple
AND-OR (SA) is shown in Algorithm 3.

4.2 AND-OR Multiway-SLCA (AOMS)

In this section, we show how our Multiway-SLCA ap-

proach for processing conventional AND-keyword search queries
can be easily generalized to process AND-OR keyword search
queries. The extended algorithm, called AND-OR Multiway-
SLCA (AOMS), is shown in Algorithm 4.
Our approach requires a general AND-OR query Q to be
expressed in conjunctive normal form (CNF), C1 ∧···∧ Cn,

r = vm

vm = v1

if (m (cid:3)= 1) then

v1 = closest(vm, C1)
if (vm ≺p v1) then

Algorithm 4 AND-OR Multiway-SLCA
1: initialize x = 1; α1 = droot
2: let vm = last({f irst(Ci ) | i ∈ [1, n]}), where vm = f irst(Cm)
3: while (vm (cid:3)= null) do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

end if
end if
P = {pred(vm, Ci) | i ∈ [1, n], i (cid:3)= m} ∪ {vm}
N = {next(vm, Ci) | i ∈ [1, n], next(vm, Ci) (cid:3)= null}
initialize rmax = last(N );
repeat

remove (cid:3) from P , where (cid:3) = f irst(P )
α = lca((cid:3), r)
r = last(r, v) where v ∈ N s.t. v = next(vm, Ci), (cid:3) ∈
Si,j
until (r = null) or (α (cid:3)(cid:5)a r) or (r = rmax)
if (r = null) or (α (cid:3)(cid:5)a r) then

if (αx (cid:5)a α) then
else if (α (cid:3)(cid:5)a αx) then
x = x + 1; αx = α

17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end while
29: if (α1 = droot) then return ∅ else return {α1, ·· · , αx}

end if
vm = last(r, out(αx, C1), ··· , out(αx, Cn))

αx = α

vm = r

end if

else

where each conjunct Ci = wi,1 ∨ ··· ∨ wi,ni . is a disjunction
of ni keywords, ni ≥ 1. We use Si,j to denote the list of
data nodes associated with each keyword wi,j, i ∈ [1, n], j ∈
[1, ni].

Note that AOMS algorithm is almost equivalent to IMS
algorithm except that the six functions f irst, last, pred,
next, closest, and out now involve a conjunct Ci instead of
a keyword list Si. These mildly generalized versions of the
deﬁnitions from Section 2 are extended as follows:

• f irst(Ci) = f irst({f irst(Si,j) | j ∈ [1, ni], f irst(Si,j) (cid:7)=

• last(Ci) = last({last(Si,j) | j ∈ [1, ni], last(Si,j) (cid:7)=

null}).

null}).

(cid:7)= null})

(cid:7)= null})

• pred(v, Ci) = last({pred(v, Si,j) | j ∈ [1, ni], pred(v, Si,j)

(cid:7)= null})

• next(v, Ci) = f irst({next(v, Si,j) | j ∈ [1, ni], next(v, Si,j)

• closest(v, Ci) = pred(v, Ci) if lca(v, next(v, Ci)) ≺a

lca(v, pred(v, Ci)); otherwise, closest(v, Ci) = next(v, Ci).

• out(v, Ci) = f irst({out(v, Si,j) | j ∈ [1, ni], out(v, Si,j)

For simplicity and without loss of generality, we assume

that |C1| ≤ |Ci| for i ∈ [1, n] where |Ci| =

5. EXPERIMENTAL STUDY

ni

j=1 |Si,j|.

To verify the eﬀectiveness of our proposed algorithms, we
conducted extensive experiments to compare their perfor-
mance against existing approaches for evaluating both AND
as well as AND-OR keyword search queries.

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1049)
s
m

i

(
 
e
m
T
n
o

 

i
t

l

a
u
a
v
E

)
s
m

i

(
 
e
m
T
 
n
o
i
t
a
u
a
v
E

l

)
s
m

i

(
 
e
m
T
 
n
o

i
t

l

a
u
a
v
E

 400

 350

 300

 250

 200

 150

 100

 50

 0

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

 250

 200

 150

 100

 50

 0

)
s
m

i

(
 
e
m
T
n
o

 

i
t

l

a
u
a
v
E

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

 400

 350

 300

 250

 200

 150

 100

 50

 0

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

SE

Query

IMS
(a) k2-1000-1000

ILE

IIMS

SE

Query

IMS
(b) k4-1000-1000

ILE

IIMS

)
s
m

i

(
 
e
m
T
 
n
o
i
t
a
u
a
v
E

l

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

Query

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

Query

SE

IMS

ILE

IIMS

SE

IMS

ILE

IIMS

(c) k2-100-1000

(d) k4-100-1000

)
s
m

i

(
 
e
m
T
 
n
o

i
t

l

a
u
a
v
E

 550

 500

 450

 400

 350

 300

 250

 200

 150

 100

 50

 0

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

SE

Query

IMS
(e) k2-10-10000

ILE

IIMS

SE

Query

IMS
(f) k4-10-10000

ILE

IIMS

Figure 3: Comparison for AND queries

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML10505.1 Experimental Setup

Similar to what was done in [14], where there is both
a non-indexed version of the algorithm (i.e., SE) as well
as an indexed version of the algorithm (i.e., ILE), we also
created two versions for each of our proposed algorithms.
We use IBMS, IIMS, and IAOMS, respectively, to refer to
the indexed versions of BMS, IMS, and AOMS. As in [14], in
the non-indexed algorithms, all the data nodes are organized
using a B-tree where the B-tree keys are the keywords of the
data nodes and the B-tree data associated with each B-tree
key is a list of Dewey labels of the data nodes having that
key as keyword.
In the indexed algorithms, all the data
nodes are organized using a B-tree where each B-tree key is
a composite key consisting of a keyword (as primary key)
and a Dewey number (as secondary key). No data values
are associated with this composite-key organization.

For the simple approach of evaluating AND-OR queries
(Section 4.1), we have six variants of the algorithm denoted
by SA-SE, SA-BMS, SA-IMS, SA-ILE, SA-IBMS, and SA-
IIMS; where SA-X denotes the variant using algorithm X
to evaluate the AND-subqueries of the AND-OR query.

The evaluation of the algorithms was carried out by using
diﬀerent classes of queries. Each class of AND queries is
denoted by kN-L-H, where N ,L, and H are three positive
integer values: N represents the number of keywords, and
L and H, with L ≤ H, represent two keyword frequencies
such that one of the N keywords has the low frequency L
while each of the remaining N − 1 keywords has the high
frequency H; thus, L = |S1|. Each class of AND-OR queries
(in CNF) is denoted by cM-kN-L-H, where M represents the
number of conjuncts in a query, N represents the number
of keywords in each conjunct, L represents the frequency
of each keyword in one conjunct, and H (with L ≤ H)
represents the frequency of each keyword in the remaining
M − 1 conjuncts. For each class of queries, a set of 10
random queries were generated and each query was executed
six times and its average execution time over the last ﬁve
runs was computed. All the experiments were conducted
using a DBLP dataset [6] with two million data nodes.

Our implementation used BerkeleyDB (Java Edition) [3]
to store the keyword data lists similar to what was done in
[14]. The BerkeleyDB database was conﬁgured using a page
size of 8KB and a cache size of 1GB. All the experiments
were conducted on a 3GHz dual-core desktop PC with 1GB
of RAM.

5.2 AND Keyword Search Queries

Figure 3 shows the comparison of the two binary-SLCA
algorithms (SE and ILE) against our multiway-SLCA algo-
rithms IMS and IIMS. To avoid cluttering the graphs, we
have omitted the BMS and IBMS algorithms as they were
outperformed by the IMS and IIMS algorithms (by up to an
order of magnitude), respectively. Compared to the BMS
variants, the IMS variants not only incur fewer number of
candidate SLCA computations but they are also more eﬃ-
cient in SLCA computations.

Each graph in Figure 3 shows the performance compar-
ison for a diﬀerent query class. For each query class, the
ten random queries shown (Q1 to Q10) are ordered in non-
descending order of the number of candidate SLCA compu-
tations incurred by the IMS algorithm. Figures 3(a) and
3(b) show the results for the case where the low and high
frequencies are the same. Comparing IMS and IIMS, we

observe that IIMS generally performs better than IMS only
when the number of candidate SLCA computations is small.
For the binary-SLCA algorithms, our results are consistent
with [14] with SE outperforming ILE. Overall, IMS generally
performs the best for both k2-1000-1000 and k4-1000-1000
with IIMS performing better than IMS for some cases.

Figures 3(c) and 3(d) show the results for the case where
the low and high frequencies diﬀer by a factor of 10.
In
this case, the non-indexed methods generally perform bet-
ter than the indexed methods. For k2-100-1000, IMS has
an edge over SE. For k4-100-1000, IIMS performs the best
when the number of its candidate SLCA computations is
small; otherwise, SE generally has the best performance.
Figures 3(e) and 3(f) show the results for the case where
the low and high frequencies diﬀer by a factor of 100.
In
this case, the indexed methods perform better than the non-
indexed methods. IIMS and ILE are comparable when the
number of candidate SLCA computations by IIMS is small;
otherwise, ILE gives better performance.
5.3 AND-OR Keyword Search Queries

Figure 4 compares the performance of algorithms SA-SE,
SA-IMS, AOMS, SA-ILE, SA-IIMS, and IAOMS for AND-
OR queries; algorithms SA-BMS and SA-IBMS have been
omitted as they are outperformed by the IMS variants. The
evaluation times shown in Figure 4 are average evaluation
times of ten queries. Figures 4(a) and 4(b) show the re-
sults for the case where the low and high frequencies are
the same, while Figures 4(c) and 4(d) show the results
for the case where the low and high frequencies diﬀer by
a factor of 10 and 100, respectively.
In all these cases,
the non-indexed methods outperform their indexed coun-
terparts, with AOMS giving the best performance.

6. RELATED WORK

Eﬃcient algorithms for computing LCAs on trees have
been carefully studied by a number of early work [2, 9].
However, the algorithms designed there are meant for main-
memory resident data. Schmidt et al.
[13] propose the
meet operator for querying XML document by computing
the LCAs of nodes in XML trees. XRANK [8] proposes a
stack-based keyword search algorithm and the results are
ranked by a Page-Rank hyperlink metric extended to XML.
Their ranking techniques are orthogonal to the retrieval and
could be easily incorporated into our work. Another work
XSEarch [5], which is an extension of information-retrieval
techniques, is mainly focused on the semantics and ranking
of query results.

The research work in [14, 12] is the most closely related to
our current work, and both work adopt the idea of smallest
LCA (SLCA) or Meaningful LCA (MLCA), which are simi-
lar ideas. Li et al. [12] propose a novel schema-free way to in-
corporate keyword search in XQuery. They also develop an
eﬃcient stack-based MLCA searching algorithm. XKSearch
[14] focuses on ﬁnding the smallest LCA of keywords in XML
documents, and it proposes several algorithms, which we
compared against in this paper.

More recently, there has also been a lot of interest on
keyword search in relational database systems [1, 4, 7, 10,
11] where the emphasis is mainly on optimizing join queries
to generate tree tuples.

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML10517. CONCLUSIONS

In this paper, we examined the problem of processing
SLCA-based keyword search queries for XML data. We have
presented a novel approach called multiway-SLCA approach
that is more eﬃcient than the state-of-the-art binary-SLCA
approach for evaluating SLCA-based keyword queries.
In
addition, we have also extended our approach to process
more general keyword search queries that go beyond the
traditional AND semantics to support any combination of
AND and OR boolean operators. Our experimental perfor-
mance evaluation using real XML datasets demonstrate the
eﬃciency of our new algorithms compared to previous algo-
rithms. As part of our future work, we intend to extend our
approach to handle complex keyword search queries with
any combination of AND, OR, and NOT operators.

8. REFERENCES
[1] S. Agrawal, S. Chaudhuri, and G. Das. Dbxplorer: A

system for keyword-based search over relational
databases. In ICDE, 2002.

[2] M. Bender and M. F.Colton. The LCA problem

revisited. In Latin American Theoretical Informatics,
2000.

[3] Berkeley DB. http://www.sleepycat.com.
[4] G. Bhalotia, A. Hulgeri, C. Nakhe, S. Chakrabarti,

and S. Sudarshan. Keyword searching and browsing in
datrabases using BANKS. In ICDE, 2002.

[5] S. Cohen, J. Mamou, Y. Kanza, and Y. Sagiv.

XSEarch: A Semantic Search Engine for XML. In
VLDB, 2003.

[6] DBLP. http://www.informatik.uni-trier.de/∼ley/db/.
[7] R. Goldman, N. Shivakumar, S. Venkatasubramanian,
and H. Garcia-Molina. Proximity Search in Databases.
In VLDB, 1998.

[8] L. Guo, F. Shao, C. Botev, and

J. Shanmugasundaram. XRANK: Ranked Keyword
Search over XML Documents. In SIGMOD, 2003.

[9] D. Harel and R. E. Tarjan. Fast algorithm for ﬁnding

nearest common ancestors. In SIAM Journal on
Computing, 1984.

[10] V. Hristidis and Y. Papakonstantinou. DISCOVER:
Keyword Search in Relational Databases. In VLDB,
2002.

[11] V. Hristidis, Y. Papakonstantinou, and A. Balmin.

Keyword Proximity Search on XML Graphs. In ICDE,
2003.

[12] Y. Li, C. Yu, and H. V. Jagadish. Schema-Free

XQuery. In VLDB, 2004.

[13] A. Schmidt, M. Kersten, and M. Windhouwer.
Querying XML Document Made Easy:Nearest
Concept Queries. In ICDE, 2001.

[14] Y. Xu and Y. Papakonstantinou. Eﬃcient Keyword

Search for Smallest LCAs in XML Database. In
SIGMOD, 2005.

)
s
m

(
 

i

e
m
T
n
o

 

i
t

l

a
u
a
v
E

)
s
m

(
 

i

e
m
T
n
o

 

i
t

l

a
u
a
v
E

)
s
m

(
 

i

e
m
T
n
o

 

i
t

l

a
u
a
v
E

)
s
m

(
 

i

e
m
T
n
o

 

i
t

l

a
u
a
v
E

 800

 600

 400

 200

 0

 5000

 4000

 3000

 2000

 1000

c2-k2

c2-k4

c4-k2

Query Class

(a) cM-kN-100-100

 0

c2-k2

c2-k4

c4-k2

Query Class

(b) cM-kN-1000-1000

 550

 500

 450

 400

 350

 300

 250

 200

 150

 100

 50

 0

 800

 700

 600

 500

 400

 300

 200

 100

 0

c2-k2

c2-k4

c4-k2

Query Class

(c) cM-kN-10-100

c2-k2

SA-SE
SA-IMS

c2-k4

Query Class

AOMS
SA-ILE

(d) cM-kN-10-1000

c4-k2

SA-IIMS
IAOMS

Figure 4: Comparison for AND-OR queries

WWW 2007 / Track: XML and Web DataSession: Querying and Transforming XML1052