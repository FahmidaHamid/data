Towards Domain-Independent Information Extraction

from Web Tables∗

Wolfgang Gatterbauer, Paul Bohunsky, Marcus Herzog,

Bernhard Kr¨upl, and Bernhard Pollak
Database and Artiﬁcial Intelligence Group
Vienna University of Technology, Austria

{gatter,bohunsky,herzog,kruepl,pollak}@dbai.tuwien.ac.at

ABSTRACT
Traditionally, information extraction from web tables has
focused on small, more or less homogeneous corpora, often
based on assumptions about the use of <table> tags. A
multitude of diﬀerent HTML implementations of web tables
make these approaches diﬃcult to scale. In this paper, we
approach the problem of domain-independent information
extraction from web tables by shifting our attention from the
tree-based representation of web pages to a variation of the
two-dimensional visual box model used by web browsers to
display the information on the screen. The thereby obtained
topological and style information allows us to ﬁll the gap
created by missing domain-speciﬁc knowledge about content
and table templates. We believe that, in a future step, this
approach can become the basis for a new way of large-scale
knowledge acquisition from the current “Visual Web.”

Categories and Subject Descriptors: H.2.8 [Database
Management]: Database Applications – Data Mining; H.3.3
[Information Storage and Retrieval]: Information Search and
Retrieval

General Terms: Algorithms, Experimentation, Theory

Keywords: Information extraction, Web mining, Web ta-
bles, Web page representation, Visual analysis

1.

INTRODUCTION

The Web is an enormous source of information contained
in billions of individual pages. Information extraction (IE)
tries to process this information and make it available to
structured queries. Most often, information extraction sys-
tems are targeted towards speciﬁc domains of interest and
involve either manual or semi-automatic learning of the tar-
get examples involved. In contrast, the goal of automatic
information extraction is to discover relations between data
items of interest and similar data items on a large scale and
independently of their domain without any training [2, 14].

∗This research was supported in part by a DOC scholarship

from the Austrian Academy of Sciences and by the FIT-IT
programme of the Austrian Federal Ministry for Transport,
Innovation and Technology.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

One principal idea is that an extraction system makes a sin-
gle data-driven pass over its entire corpus and extracts a
large set of relational tuples without requiring any human
input [4]. Traditionally, such domain-independent informa-
tion extraction systems aim to ﬁnd relations in unstructured
plain text by applying natural language processing tech-
niques [10]. In contrast, we approach the problem of large-
scale and domain-independent information extraction from
web tables. Tables are interesting because they present in-
formation in a condensed, rather simple, and well structured
way. At the same time, domain-independent large-scale at-
tempts at information extraction have been diﬃcult partly
because the prevailing approach of the IE research commu-
nity has been to analyze either the plain text content or the
tree structure of web pages for relevant tabular extraction
patterns. Tables, however, only explicitly reveal the true
nature of their data structure in a two-dimensional (2-D)
context, whereas the source code only “encodes” the visual
information in an implicit and rather diﬃcult to analyze
format (Fig. 1).

Figure 1: Information extraction (IE) from rendered
web pages emulates the process by which internet
users encode and decode information in the current
“Visual Web.”

We therefore deviate quite drastically from the prevail-
ing approach of web information extraction and cast the
problem into a formalism that moves the focus from a tree-
structured to a 2-D pattern recognition problem using a
variation of the CSS2 visual box model. Following this ap-
proach, we show that extracting information from web tables
is possible without reliance on “heavy” linguistic techniques
tuned to the domain of interest and, thereby, diﬀer from
previous table extraction systems, which were dominantly
domain-speciﬁc (e.g. extraction of catalogues with product
information).

INFORMATION !CMSHTMLHTTPTCP/IPINFORMATION ?BROWSERHTMLHTTPTCP/IPAuthorIE from HTML codeIE from ren-dered pageReaderWWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages71Overall, the main contributions of this paper can be sum-
marized as follows: (1) We generally classify visually struc-
tured data on the Web. (2) We cast the problem of informa-
tion extraction from the Web – in particular from web tables
– into a formalism that moves away from the commonly fa-
vored tree-based representation of web pages. (3) We moti-
vate an output format of information extraction from tables
that allows for multiple interpretations of the data extracted
in a later probabilistic information integration step. (4) We
outline a universal ground truthing methodology that allows
to ground truth web tables independent of their implemen-
tation. (5) We introduce a challenging test set of web tables
that was compiled by 63 students and ground truthed by us
using our methodology. (6) We present ﬁrst experimental
results which indicate that domain-independent information
extraction from tables can be performed without the need
of heavy linguistic methods, relying only on visual charac-
teristics of the tables.

After a brief review of related work (Section 2), we char-
acterize the phenomenon “web table” and related visually
structured data structures (Section 3). We then lay out the
formal setup of our method (Section 4), before we describe
our chosen approach in more detail (Section 5). In Section 6
we outline our method to ground truth web tables and ap-
ply it to an objectively chosen test collection of web tables.
We use this test set to evaluate our system (Section 7) and
conclude after a brief discussion of next steps (Section 8).

2. RELATED WORK

Web Table Analysis. Web table analysis has become a
widely researched topic on its own over the years. Penn
et al. [25] deﬁne genuine uses of HTML tables as document
entities where the 2-D grid is semantically signiﬁcant and
describe a couple of heuristics to distinguish genuine from
non-genuine leaf <table> tables on web pages. Yalin Wang
and Hu [33] train a classiﬁer on content features of individ-
ual cells and non-text layout features from the HTML source
to perform the same task of table location. Chen et al. [7]
employ heuristic rules to ﬁlter out non-genuine tables from
their test set and make assumptions about cell content sim-
ilarity for table recognition and interpretation. Like most of
the approaches we are aware of, their method relies on the
hierarchical HTML tag structure of the documents, most
notably that of <table> tags. Yang and Luk [36] describe
how they extract attribute-value pairs from 1-D or 2-D ta-
bles, a notion similar to our view of 1-D and 2-D lists and
tables. But for them, a 1-D table contains what they call
“mixed-cell” content, which means that the attribute name
(label) and value are in the same cell. We regard this data
structure as nested, substructured list. Yoshida et al. [38]
base their work on a general knowledge ontology and em-
ploy an expectation maximization algorithm to distinguish
between attribute and value cells. They assume that tables
do no contain any spanned cells. Tengli et al. [29] present
an algorithm that extracts tables and diﬀerentiates between
label and data cells.

All these approaches have in common that they assume
that relevant tables only appear inside leaf tables, which are
such <table> tags that do not contain other nested <table>
tags. In contrast, Lerman et al. [22] mention that just a frac-
tion of tables are actually created with <table> tags. In their
algorithm, they leverage the list page-detail page structure

present in some websites to ﬁnd boundaries between records
in what we would classify as a substructured 1-D list. They
also mention that layout is important for table extraction,
but go on to say that this means that records are separated
by HTML tags. In contrast, we base our table extraction on
positional information that is independent of the HTML tag
structure and do not rely on particular HTML structures
being present. Tijerino et al. [30] describe the automatic
generation of ontologies from normalized tables, which is a
structure they get after normalizing table-equivalent data.
Pivk et al. [26] focus on understanding table-like structures
only due to their structural dimension and transforming the
most relevant table types into F-logic frames. Neither pa-
per makes it clear to us how the described approaches locate
tables on web pages in the ﬁrst place.

We refer to two recent surveys on table analysis by Embley
et al. [12] and by Zanibbi et al. [39], which cover a broad
range of table processing literature and illustrate the general
challenges of information extraction from tables despite their
primary focus on non-HTML documents.

Visual Web Page Analysis. As far as we know, the idea
of analyzing the visual representation of a web page for con-
tent analysis originated in the area of web page segmen-
tation. Yang and Zhang [37] describe an approach which
derives features directly from the layout of web pages. By
using a “pseudo rendering process” they try to detect “vi-
sual similarities” of HTML content objects. We fully believe
in their remark that HTML tags are not stable features for
analyzing structures of HTML documents. Gu et al. [16]
describe a top-down approach to segment a web page and
detect its content structure by dividing and merging blocks.
Kovacevic et al. [19] use visual information to build up a
“M-tree”, a concept similar to the DOM tree enhanced with
screen coordinates. They then use further deﬁned heuris-
tics to recognize common page areas such as header, left
and right menu, footer and center of a page. Cai et al. [6]
describe a web page segmentation process that uses visual
information from Internet Explorer. Their VIPS algorithm
segments a DOM tree based on visual cues retrieved from the
browser’s rendition. Cosulschi et al. [9] describe an approach
that uses positional information of DOM tree elements to
calculate block correspondence between web pages.

Visual web page analysis can also be increasingly found
in information extraction literature, speciﬁcally on record
boundary detection. Zhao et al. [41], Zhai and Liu [40] and
Simon and Lausen [28] describe diﬀerent approaches for de-
tecting repetitive patterns on web pages, which are predom-
inantly source-code based and enhanced with visual cues.
In contrast, Aumann et al. [3] describe a system that works
only on a hierarchical structure of the visual representa-
tion (experiments are performed with PDF documents) and
learns to recognize text ﬁelds such as author or title from
manually tagged training sets of documents. Conversely, our
approach does not attempt to ﬁnd individual text ﬁelds, but
rather, larger structures, does not require training sets and
neither imposes a tree structure on web pages.

Visual Web Table Extraction. To our best knowledge,
the idea of actually rendering or “executing” HTML code
and using the results for detecting relational information
in web tables was ﬁrst mentioned by Cohen et al. [8]. The
described approach, however, does not actually render web

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages72pages, but rather infers relative positional information of ta-
ble nodes in an abstract table model with relative positional
information deduced from the source code. In contrast, in
our previous work [21] we describe a top-down web table
location mechanism working exclusively on visual informa-
tion obtained from the Mozilla web browser. The approach
works on word bounding boxes after manipulation of the
DOM tree and detects tables with the help of space density
graphs and recursive application of the X-Y cut algorithm.
This approach is later adapted in [20] to a bottom-up clus-
tering algorithm starting with word bounding boxes as well.
In [15], we describe a method that works on both word and
element node bounding boxes, and that is able to locate
concepts of a predeﬁned seed knowledge in web tables.

The system of this paper builds upon this last method and
extends it in the following ways: (1) the approach does not
need pre-deﬁned seed knowledge and is as such completely
domain-independent; (2) the approach can recognize and
transform the tabular information into a format, suitable
for a subsequent information integration step.

3. VISUALLY STRUCTURED DATA ON

THE WEB

Web information extraction, building upon structural fea-
tures of the data on web pages, is probably the most intense
studied research topic of web content mining [23]. Struc-
tured data formats are often used to represent information in
a concise and unambiguous form, e.g. lists of products and
services. Extracting such data in an automated fashion al-
lows to provide value added services, e.g., comparative shop-
ping [5], and meta-search [41]. As a result, a large amount
of literature has been published on diﬀerent approaches to
structured data extraction, which is also known as wrapper
generation. Such wrappers commonly try to learn certain
regularities from example source code and then to ﬁnd other
instances of such patterns.

Another approach to structured information extraction,
which we suggest in this paper, is to focus on the 2-D visual
representation of web pages as intended by authors for read-
ers in the current Visual Web [24], instead of the tree-based
representation used to encode such information (Fig. 1).
Explicit Semantic Web annotations are still rare and dy-
namic web technologies around Web 2.0 translate into an
increasingly more complicated code syntax, but with more
or less the same visual syntax used to express similar kind of
human-understandable semantic relations. The source code
carries the same amount of information, but in an implicit
and diﬃcult to analyze format. “Implicit” means that rela-
tions between individual items is not available without ﬁrst
fully analyzing and “executing” or rendering the code. And
information actually is the result of combining individual
data items together with relations between these data items
(Fig. 2). Using and decoding the available visual informa-
tion after rendering a web page allows us to draw additional
conclusions, thus ﬁlling the gap between available data and
domain-dependent semantic relations.

Such visual semantic relations can be expressed in two
principal syntactic ways: (1) By topology, which concerns
the spatial arrangement of the composing units of informa-
tion. (2) By typography or style, which concerns metadata
information such as font size, font weight or background
color. Here, we provide a broad characterization of the ﬁrst

Figure 2: In visually structured data, meaning of in-
formation (semantics) results from meaning of the
individual pieces of data (data semantics) and the
visual relations between them (visual semantics).

subcategory: visually structured information that predom-
inantly derives its meaning from the spatial arrangement
of its constituent data items.
In other words, the spatial
relations between individual elements add some important
meta information to the meaning of each data block, with-
out which the information could not be understood to its full
extent. Broadly, we ﬁnd that web tables – the focus of our
research – are, together with lists and some domain-speciﬁc
aligned graphics, one of the three dominant topological data
structures found on web pages. Our focus has been to clas-
sify the diﬀerent phenomenons according to their intended
purpose as visible to the human observer, not by their imple-
mentation, which vary for example for tables from <table>
over <div> and <li> tables to tables in non-HTML format.
All three can be found either as one dominant structure with
atomic data content or with nested substructures as classi-
ﬁed in Fig. 3. Below we give a set of deﬁnitions for these
structures which helped us to develop human-like heuristics
for our table extraction step, and which enable us to distin-
guish tables from similar visual structures as described in
section 5.1. These deﬁnitions are best understood in con-
nection with Fig. 3.

• Tables. We consider the description of tables as given
by Vanoirbeek [31] a very useful deﬁnition of tables on
the Web despite its original focus on printed docu-
ments: “In a global way, a table may be deﬁned as
a two-dimensional presentation of logical relationships
between groups of data. Those connections are re-
ﬂected by horizontal and vertical alignment of data in
a grid.” It does not include some unique properties of
web pages, which we add for the purpose of deﬁning
web tables: A web table is a two-dimensional presen-
tation of logical relations between groups of data items.
Those relations are reﬂected by diﬀerent visual prop-
erties and by horizontal and vertical alignment of the
data items in a visible or implied grid structure, which
become observable after a web page is rendered.

• Lists. Trying to ﬁnd an appropriate deﬁnition for
lists, we consulted a number of standard references like
Encylopaedia Britannica and Merriam-Webster. How-
ever, we could not ﬁnd one deﬁnition that describes
the meaning we try to convey. As such, the following
deﬁnition is the result of consulting a number of ref-
erences and signiﬁcant discussion in our group: A list
is a series of similar data items or data records. A
list can be either one-dimensional or two-dimensional;
in both variants, no hierarchical or other semantic re-
lationships in between individual list items is implied
except for a possible ordering of the items.

+INFORMATIONMeanining of overall information SEMANTICSINFORMATIONMeanining of overall information SEMANTICSDATAIndividual pieces of data DATA SEMANTICSDATAIndividual pieces of data DATA SEMANTICSRELATIONSVisual relations b/w pieces of dataVISUAL SEMANTICS=WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages73Figure 3: Tables are together with lists and some domain-speciﬁc aligned graphics one of the three dominant
spatially structured data formats found on web pages. The constituent logical units of the dominant structure
either have atomic content, or nested substructures such as lists of data records.

• Aligned graphics. Aligned graphics are such graphi-
cal depictions of relations between entities of a certain
domain, which do not fall in either of the two other cat-
egories. They are generally domain-speciﬁc (like the
period system for chemical elements), not necessarily
bounded by a rectangle, and relative spatial positions
do imply relationships between data items.

The structures described above often appear in nested
forms. The data records of search engine results are an ex-
ample of a list where the individual list items consist of
repetitive substructures. The characterization looks at the
“dominant” structure, which is the principal structure that
can not be seen as a substructure of another table, list or
other repetitive pattern.
It is important to add that this
“nestedness” does not necessarily imply nestedness in the
source code. We only focus on the visual appearance of
structures on rendered web pages, independent of their cod-
ing. Reality tends to be more complex than a model of
reality, and some “real-world” structures cannot be unam-
biguously classiﬁed in the above scheme. However, we ﬁnd
the level of detail to be a good balance between classify-
ing all available structures and applicability of the model.
It serves well the task of developing heuristics to discern ta-
bles from other structures. But before, we will formalize our
approach of IE from a visual representation of web pages.

4. FORMAL SETUP

In this section, we describe a topological representation
of web pages and compare it to the commonly used tree
representation. The choice of an appropriate web page de-
scription is an important decision as it determines the set of
features available to discriminate between relevant and ir-
relevant information in the extraction process. We will use
this web page representation and corresponding formalism
in the next chapter to describe our method of extracting
information from web tables.

DOM tree representation. Web information extraction
and wrapper generation approaches commonly aim at de-
tecting certain information patterns in the tag tree or the
DOM tree of web pages. The DOM tree as an attributed or-
dered rooted tree is a special case of a directed acyclic graph
T = (cid:104)N ,E, r(cid:105) where N = Nt ∪ Ne with Nt representing the
set of text nodes, Ne the set of element nodes, E ⊂ Ne×N the
directed edges between the nodes, and r∈Ne the designated
root node. Each text node nt ∈ Nt has one non-empty string
attribute s and each element node ne ∈ Ne a set of attributes
A with obligatory node name and optional further attribute
name-value pairs, e.g. nt = (cid:104)s(cid:105) = (cid:104)“Banﬀ, Canada”(cid:105) and
ne = (cid:104)A(cid:105) = (cid:104){name :“TD”, class :“left”, bgcolor :“green”}(cid:105).
All nodes except the root node n ∈ N\{r} have exactly
one parent node parent(n) with parent(n) ← p| (p, n)∈E,
which determines the hierarchy of the data structure. On a
conceptual level, DOM tree-based IE works on three basic
elements: (1) the text nodes Nt as primitive content ele-
ments containing the actual textual information of interest;
(2) the element nodes Ne as primitive structural elements
adding metadata to the content; and (3) the edges E be-
tween the nodes representing the actual hierarchical struc-
ture of the DOM tree. We work with analogous concepts in
a diﬀerent web page model (Fig. 4).

Visual box representation. When HTML documents are
rendered by a browser, CSS (Cascading Style Sheets) re-
present the element nodes of the document by rectangular
boxes and govern their layout according to the CSS2 box
model and the CSS2 visual formatting model [34]. We refer
to such rendered rectangles corresponding to element nodes
in the DOM model of a web page as visualized element nodes
or VENs and characterize them with the four coordinates
of their position on the screen and a vector of attributes.
We add the idea of treating individual words as rendered on
the screen as separate entities [21] and call them visualized
words. We can, therefore, represent a web page by a topo-
logical and typographical 2-D visual box model Vr = (cid:104)V,Xe(cid:105)

Tables2-D STRUCTURESListsSimple 1-D listsSimple tablesSubstructured  tablesData records and othersubstructured 1-D listsOne dominant structure with atomic contentNested sub-structuresSimple 2-D listsAligned graphicswww.in.atw3c.orgwww.dc.comwww.vis.netsure.chwww.go.fiwww.no.estheb.orgwww.si.eswww.in.atw3c.orgwww.dc.comwww.vis.netsure.chwww.go.fiwww.no.estheb.orgwww.si.esSubstructured 2-D listsTuTuTuTuMoWeThFrWeThFrMoWeThFrMoWe15:00 -17:00 -06:00 -13:00 -21:00 -17:00 -06:00 -13:00 -21:00 -17:00 -22:00 -17:00 -09:00 -09:00 -10:00 -Nerem pelenel49 ØCartus Simple graphicsSubstructured graphicsAajexAhaAndumAchantieAhernApolieAchumAieApremAdebaAjuntArunimAesopAlumAstaraAguaAmaraAtumpolAajexAhaAndumAchantieAhernApolieAchumAieApremAdebaAjuntArunimAesopAlumAstaraAguaAmaraAtumpolTortt 3ACartus1-D STRUCTURESPin 45CartusArfurid sinludieTormabe efutrie89 ØCartus Siphrtas beresAtunpil chrop99 ØCartus MarmarusineWWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages74Figure 4: IE from the visual box model of web pages
works on three diﬀerent conceptual elements than
IE from the DOM tree.

where V = Vw ∪Ve is the set of primitive data elements with
Vw representing the set of visualized words and Ve the set
of VENs. The parameter r describes a list of rendering con-
ditions like a chosen rendering algorithm and screen width.
Each VEN e∈Ve consists of the screen coordinates vector of
its four borders x = (cid:104)x1, y1, x2, y2(cid:105) and a property-value at-
tribute vector a = (cid:104)a1, ..., a|a|(cid:105) including its node name and
its computed style attributes: e = (cid:104)xe, ae(cid:105). Each visualized
word w ∈Vw has additionally one non-empty string s asso-
ciated that contains the textual content: w = (cid:104)xw, aw, s(cid:105).
The data structure Xe is a minimum double topological grid
[15] superimposed on all VENs and, as such, deﬁned by Xe =
{(cid:104)x1, y1, x2, y2(cid:105)| x1∈X1, y1∈Y1, x2∈X2, y2∈Y2, x1<x2, y1<y2}
with X1 = {x| (∃e∈Ve | xe
1 = x)} and analogous Y1, X2 and
Y2. Each point x ∈ Xe on this topological grid deﬁnes a hy-
perbox which is a rectangular area on the screen where each
border is aligned with a corresponding border of at least one
VEN. In an analogy to tree-based IE (Fig. 4), the visualized
words form the primitive content elements, VENs the prim-
itive structural elements, and the 2-D spatial topology the
data structure in our web page representation.

1 ≤ xw

1 ) ∧ (ye

The notion that a visualized element node e contains a
visualized word w implies that the bounding box of e com-
pletely contains the bounding box of w: (e contains w) ⇔
(xe
2 ). An ordered
list of n visualized words t = (cid:104)w1, w2, . . . , wn(cid:105) is said to
be in western reading order, if its sequence as arranged on
a screen is from left to right and between rows top down:
∀wi, wj ∈ t : i < j ⇔ (yi
. We

1) ∨(cid:161)

1 ) ∧ (xe

2 ) ∧ (ye

2 ≥ xw

2 ≥ yw

1 ≤ yw

(cid:162)

(yi

1 = yi

1 < yj

1 < xj
1)

1) ∧ (xi

deﬁne (cid:98)Ve as the set of extended visualized element nodes
with each (cid:98)e = (cid:104)x,(cid:98)a, t(cid:105) ∈(cid:98)Ve corresponding to a VEN e ∈ Ve
together with a string t and an attribute list (cid:98)a, where t is
characters, and(cid:98)a is a merge function of the attribute list of
e and all contained words wi, (cid:98)a = m(ae, aw1 , ..., awn ). By
pletely tiled with a set (cid:98)V f

the concatenation of the set of strings {si} of all n words
{wi} contained in e, in reading order and separated by space

a frame f , we mean a special hyperbox that can be com-
e of extended VENs. “Completely
tiled” refers to a situation where a hyperbox can be cov-
ered with VENs on the screen in such a way that the whole
area is covered and no VEN overlaps another one (MECE =
Mutually Exclusive, Collectively Exhaustive) except for the
adjacency condition. We deﬁne a frame in such a way that
e (cid:105).

it already contains the set of extended VENs: f = (cid:104)xf ,(cid:98)V f

Figure 5: The table location step of VENTex focuses
on the most dominant type of tables on the Web:
web tables, whose logical cells together render as a
completely tiled hyperbox or frame.

5.

INFORMATION EXTRACTION FROM
WEB TABLES

Our observation is that the majority of web tables topo-
logically form a frame in the visual box model. Figure 5
compares the topology of this kind of completely tiled and
“completely aligned tables” (1) with the other three types of
tables on the Web: web tables, whose logical cells are formed
by visualized element nodes but which are not completely
aligned (2:“weakly aligned tables”); web tables whose logical
cells are not contained in diﬀerent visualized element nodes
but rather as delimited words inside the same visualized ele-
ment node or (3:“delimiter based tables”); and tables which
have no equivalent HTML code and which, as such, are not
“constructed” by rendering in a web browser, e.g. ﬂash and
pdf tables or pictures of tables (4). Relative occurrences
and future changes are an educated and cautious estimate
based on a number of table sets we have seen in the course of
our studies. Depending on what kind of particular subset of
web tables one focuses on (e.g. tables highly listed in search
engines, product comparison pages, handpicked “mean” ta-
bles, older web pages, template-driven web pages from large
websites, etc.), these numbers will slightly vary. However,
the dominance of web tables formed as frames is universally
prevailing, which is why we focus on this kind of tables. As
the logical cells of tables we search for coincide with VENs
on the screen, we refer to our approach as VENTex for
Visualized Element Nodes Table extraction.

The individual steps of table understanding have been de-
ﬁned and named in the table literature in a number of ways.
We basically follow the naming and process description given
by Hurst [18]. Hurst breaks down the task of table under-
standing into the following subtasks: (1) table location, the
process of spotting tables in documents; (2) table recogni-
tion, the task of segmenting the original description of the
table into a relative spatial description; (3) functional and
(4) structural analysis; and ﬁnally (5) table interpretation,
the extraction of meaningful and unambiguously structured
information. However, we have to slightly accommodate the
descriptions above to ﬁt our speciﬁc approach. Also, we ﬁnd
a pragmatic division into three consecutive steps helpful:

• Table location: the task of identifying tables and
their constituent logical cells on web pages. Our ap-
proach focuses on tables whose logical cells together
form a frame (Fig. 5).

DOM treeVisual box modelPrimitive content elementPrimitive structural elementData structureTEXTNODESELEMENTNODESTREEHIERARCHYVISUALIZEDWORDSVISUALIZEDELEMENT NODES2-D TOPOLOGYOF BOXEST=〈N,E,r〉NtNeEB=〈V,Xe〉VwXVeExampleNoresSeruBoruEfnaViderDortemNeulidBilianuTemisYorsepAlternative fileformatsWeakly aligned tablesCompletely aligned tablesDelimiterbased tables•Pictures (.jpg)•Flash (.swf)•Acrobat (.pdf)•etc.WEB TABLES := RENDERED TABLES ON THE WEB OTHER TABLES> 95% (   )< 2% (   )< 3% (   )1234WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages75• Table recognition: the task of identifying the rel-
ative spatial relationships between individual logical
cells of a table, which we call the topological structure
of a table.

• Table interpretation:

the task of extracting and
saving information from tables in a structured format
that preserves the meta information contained in all
available visual relationships between the individual
categories for a subsequent successful probabilistic se-
mantic information integration step. We will argue
that domain-independent table interpretation cannot
result in unambiguously structured information be-
cause of existing inherent domain-speciﬁc ambiguities
that can sometimes not even be resolved by humans.

Next, we describe our current implementation and its time
complexity and speed in more detail. Our system analyzes
any given web page for the existence of tabular data, rec-
ognizes relations as implied by their spatial arrangement,
extracts a number of n-tuples together with hierarchical in-
formation about relations between their entries and saves
them in an XML data format which is heavily inﬂuenced by
the work of Wohlberg [35]. Because the ﬁrst two steps of
locating and recognizing a table are interdependent in our
approach (a table frame cannot be reliably discriminated
from non-table frames without ﬁrst knowing its topology),
we describe them as one single step of table extraction.
5.1 Table Extraction

The task of extracting web tables can be formulated as
the the task of (1) ﬁnding all frames for a given web page;
(2) discerning those which adhere to the deﬁnition of ta-
bles from section 3 and where a 2-D grid is semantically
signiﬁcant [25] from lists and other frames intended for non-
relational layout purposes; (3) transfering the content into
a topological grid description in which logical cells are ﬂush
with neighboring cells and their spatial relations are explicit.
As explained in the section on related work, Yalin Wang
and Hu [33] used a machine-learning based approach using
features deduced from the HTML code to discern genuine
from non-genuine tables. The diﬃculty of a HTML-centric
approach is that important features of individual cells, like
size and height, that are ideally suited to distinguish gen-
uine from non-genuine <table> tables, or in our case, frames
that represent tables from those that do not, are not explic-
itly available. Our approach can use such visual features,
and the formal setup from section 4 allows us to describe
a set of extraction rules and heuristics in an unambiguous
way. Currently, we have deﬁned over 20 such rules of which
table 1 lists the most important 12.

We focus on a set of eight attributes for VENs (#1) and
nine attributes for visualized words (#2). We only consider
VENs with certain names, which we found form the domi-
nant number of logical table cells on the Web (#3). Our ap-
proach focuses on tables which form a frame (#4) and elim-
inates duplicates of VENs which have the same coordinates
(#5). Adjacency between neighboring VENs is loosely de-
ﬁned with a margin of 3 pixel (#6). The LocateFrames(V(cid:48)
e)
Algorithm 1 (#7) is a heuristics to locate all frames given
a set of VENs.
It builds upon the expansion algorithm
Expand(direction, frame) from our previous work (Algorithm 1
from [15]) but works without any prior semantic word knowl-
edge by expanding from all VENs except for those that were

Table 1: Twelve most important extraction rules
and heuristics of VENTex.

attributes

# Rules or heuristics
1

prop = (cid:104)name, color, bgcolor,
ae
fsize, fstyle, fweight, ﬀamily,
textalign(cid:105)
aw
prop = ae
2
e ⊂ Ve with V(cid:48)
3 V(cid:48)
name ∈{“TD”, “TH”,“DIV”}} Only VENs with
certain
ae
names are assumed possible
logical table cells
Set of tables ⊆ set of frames Tables are assumed to form

Explications
Eight
of VENs
are considered discriminatory
enough
Visualized words have an addi-
tional link attribute

e = {e | e ∈ Ve,

prop + (cid:104)href(cid:105)

4
5 PurgeCongruentVENs(V(cid:48)
e)

eliminates

2≤ 3px)

(a x1-adjacent b) ⇔ (0 ≤
1 − xb
x1
LocateFrames(V(cid:48)
e)
1 ∧ xa
1 |, |Y f

e | (xa
1<xb
1 ∧ ya
2 <yb
2)
2 | ≥ 2, |Y f

(cid:64)a, b∈(cid:98)V f

frames
Algorithm that
congruent VENs
Adjacency between VENs is
deﬁned by their distance in
each of the 4 directions
Expansion Algorithm (see
text)
2<xb
A table cannot contain cells
with overlapping projections
2 | ≥ 3 A table is 2-D and has at least

1 <yb
1 |, |X f

(ya
|X f

2) ∨

6

7

8

9

10 CleanFrame(f )

11 (cid:64)e ∈(cid:98)V f
12 (cid:64)e∈(cid:98)V f

pA = 0.4

e | Areae > pA · Areaf ;
e | |{w|e contains w}| >

pw; pw = 20

3 rows
Cleaning Algorithm (see text)
No single logical cell can cover
more than 40% of a table
No single logical cell can con-
tain more than 20 words

part of a previous expansion step. Two theoretical problems
with this algorithm exist. In practical terms it works well.
The expansion algorithm also implicitly considers a certain
kind of spatial relationship which we consider as semanti-
cally ill-deﬁned and, therefore, discriminatory against tables
(#8). Another important algorithm (#9) is the cleaning al-
gorithm CleanFrame(f ). It deletes empty cells from the
structure that convey just layout and no semantic meaning
and as such just deletes cells whose disappearance does not
change the visual semantic relations in a table. The result
of this step is a structure with partial holes. We use a sim-
ple algorithm that works on the double topological grid to
close the structure again. Figure 6 gives an intuition about
the working of this grid transformation. The result of this
step is again a set of completely tiled 2-D frames, which still
do not necessarily represent tables. Rules #11 and #12 are
two examples of several subsequent heuristics which try to
discriminate tables based on layout characteristics.

Figure 6: The double topological grid allows to sep-
arate the step of locating a table and its composing
logical elements from recognizing its topology.

xŷ11231313ŷ212421122334--42,31y11,2233--31,21232134Double Topological GridTable Topologyxy23e1x1ˆx1ˆx1ˆx1ˆx2ˆx2ˆx2ˆx2ˆe2e3e4e5ŷ2ŷ1e1e2e3e4e5WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages76e do
frame ← (cid:104)xe, {e}(cid:105)
direction ← 0
fail ← 0
repeat

Algorithm 1 LocateFrames(Ve): locates all frames of a web
page that are not part of another, bigger frame
Input: Ve: set of visualized element nodes of web page
Return: F: set of frames of web page
1: V∗
e ← Ve
2: F ← {}
3: for all e ∈ V∗
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for
17: return hList

until expandSuccess = false
direction = (direction + 1) mod 4
until fail = 4
V∗
e − V frame
e ← V∗
F ← F ∪ V f rame

(cid:104)frame, expandSuccess(cid:105) ← Expand(direction, frame)
if (expandSuccess = true) fail ← 0 else fail ← +1

repeat

e

e

lows to express composed tables. This logical model of data
in tables was originally conceived to support the diﬀerent
stages of tabular composition, and we agree that it is a well
suited format to further operate on the underlying informa-
tion. We also think that it is possible for humans to transfer
a given table to a Wang model most of the time, and that
this process can be semi-automated for domain-speciﬁc ta-
bles. However, we think that the appropriate target format
for automatic large-scale domain-independent information
extraction and subsequent knowledge acquisition from web
tables has to be more general as we will demonstrate with
the help of Tables 2(a)-(b).

Table 2: Determining the number of dimensions of a
table in the Wang model cannot be unambiguously
made without an accompanying interpretation of the
underlying domain.

(a)

(b)

Algorithm 2 Clean(f ): purges empty spacer columns and
rows from a frame and candidate table f

Input: (cid:98)V f
Return: (cid:98)V f
1: construct double topological grid X for (cid:98)V f

e : set of extended VENs that form f

e

e : cleaned set
1 ∈X1 do

all VENs with x1 = x∗

1 are empty ∧ have same x∗

2

delete these VENs from frame

2: for all x∗
3:

if
then

4:
5:
end if
6: end for
7: for all y∗
8:
9:
10:
end if
11: end for

12: return (cid:98)V f

e

1 ∈Y1 do

if all VENs with y1 = y∗

1 are empty ∧ have same y∗

2 then

delete these VENs from frame

At the moment, these rules are still implemented in an
ad-hoc fashion. On a ﬁrst attempt, experimenting with a
number of such heuristics enabled us to model the way hu-
mans recognize tables rather eﬀectively, but still not per-
fectly. We think that considerable improvements can be
made in a future step by casting the problem of optimiz-
ing these parameters into a machine-learning framework [33]
which explicitly uses the visual features available in a 2-D
topological problem formulation.
5.2 Table Interpretation

Two important decisions have to be made for the table
interpretation step: (1) What is the appropriate output of a
table interpretation, i.e. what kind of table model or table
language should be used to describe information in tables?
(2) How can the transition from the table topology into the
model be automatized? Recent literature [13] suggests that
the appropriate target format for table analysis is a represen-
tation based on the abstract table proposed by Xinxin Wang
[32]. This table model is similar to the model of Vanoirbeek
[31] in the sense that it is a multi-dimensional model of a
table, separating the logical structure from the layout of
a table. The distinctive characteristic of this model is that
each entry does not have to be associated with labels of each
of the dimensions (or categories) simultaneously, which al-

A-Mart Milk

B-Mart Milk

Price
1.19
Sugar
0.59
Bananas 1.99
1.29
0.59

Sugar

A-Mart #01124
#01345
#01347

Price
1.19
0.59
1.99
B-Mart pr78wh5 1.29
pr62953 0.59

Each table shows the prices of products from two diﬀerent
stores A-Mart and B-Mart. Whereas Table 2(a) uses com-
mon names to specify the products, Table 2(b) uses product
denominations which are shop-speciﬁc. Interpreting the ta-
bles along the thoughts of the Wang model, Table 2(a) is a
three-dimensional table, having the categories shop, prod-
ucts, and prices. Table 2(b), however, is composed of two
two-dimensional tables, each having the categories products
and prices, with the name of each shop together with the
product numbers building a labeled domain. Once, at a
later stage, the shop-speciﬁc product names are mapped to
a domain-speciﬁc naming scheme, Table 2(b) could be in-
terpreted and queried like the other as three-dimensional
table. Therefore, interpreting Tables 2(a)-(b) as either two-
dimensional or three-dimensional at the moment of “pars-
ing” and extracting the table jumps too early to a conclu-
sion. As a domain-independent and automatic information
extraction system should ideally make a single pass over its
corpus guaranteeing scalability with the size of the corpus
[4], such a content-speciﬁc and irrevocable decision is, in our
opinion, better left to a later probabilistic information inte-
gration step. Therefore, we argue that the target format for
domain-independent information extraction from web tables
should not be the Wang model. Rather, it must be a format
that allows to leave this ﬁnal step to a later stage that an-
alyzes the data space in connection with other information
found on the Web. Our thinking is inspired by the work
of Dalvi and Suciu [11] who propose to extend probabilis-
tic databases by adding statistics on the global schema and
probabilities to the view, thus allowing to compute proba-
bilistic answers to queries instead of commonly used deter-
ministic answers.

We believe that the appropriate format is a generalized
n-tuple, which does not assign categories to each of the en-
tries of the tuples. Therefore, these tuples cannot be seen
as n-tuples in terms of the relational model of relational

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages77databases [1], but rather tuples whose relation between indi-
vidual tuples and entries can be speciﬁed a posteriori of the
extraction. Imagine the three-dimensional relational space
spanned by the three entries of the three columns from Ta-
bles 2(a) -(b). The three entries of Table 2(a) are connected
to both A-Mart and B-Mart, the three entries of Table 2(b),
however, can be clearly separated.
Imagine now the en-
try “Milk” replaced by the term “Super milk” everything
else staying the same. The question of the number of di-
mensions of the table remains ambiguous, translating into
only one single connections between A-Mart and B-Mart in
the relational space. As such this representation also allows
for a fuzzy, probabilistic notation of categories or dimen-
sions. The end result of this extraction is an XML represen-
tation, which is heavily inspired by the work of Wohlberg
[35]. Wohlberg’s table model is almost identical to the Wang
model. However, we do not use the same interpretation of
the relations speciﬁed, we just use the ﬁle format and trans-
fer all such nested structures in the form of nested sublabels.
We currently solve the issue of assigning a given table the
according n-tupel relations by having ﬁrst deﬁned a num-
ber of most commonly found table types on the Web (sim-
ilar to [38]) and then a number of discriminating heuristics
which make heavy use of the style information contained
in the attribute vector of the extended VENs. Currently
these heuristics are combined in an ad-hoc fashion and work
remains to be done to make this step more eﬃcient.

Figure 7: Domain-independent web table interpre-
tation cannot resolve some domain-speciﬁc semantic
ambiguities and therefore has to limit its scope of
analysis to visual semantics.

5.3 Performance Analysis

Loading and rendering a page happens in O(n) time, where
√
n is the number of element nodes as measure of a web
page’s size. It subsequently takes O(n
n) time to construct
the double topological grid. Whereas our previous expan-
sion algorithm working with seed knowledge had complexity
O(k
√
n) where k is the number of appearances of a keyword
on a page, we now have O(n
n), which translates into a
slight performance decline in practical terms.

√

Our current implementation extracts tables at an average
speed of around 5 seconds per page. This puts our algo-
rithm at a signiﬁcant disadvantage to other approaches that
work without rendering the page. We believe that, while
this objection is true, future approaches will not be able to
work on a broad scale without rendering web pages. E.g.,
identifying tabular structures from arbitrary web pages re-
quires full rendering of the web page together with all linked
style sheets and other sources in order to be able to work
on the actual representation of data. Every image has its
place holder and, as such, can contribute to the ﬁnal spatial
arrangement. Going from this assumption, major time im-
provements can be achieved by moving to faster rendering
engines. But despite the slower performance due to render-

ing, we believe that we will witness a gradual shift in the
IE community to rendering based information extraction in
the time to come (Fig. 1).

6. WEB TABLE GROUND TRUTHING

During our research, we were trying to ﬁnd available test
data sets to evaluate the performance of our system. We
did not ﬁnd any useful and broad web table ground truth
data set, which is why we started to build our own test data
set. Now, while existing literature agrees that the ground
truthing of interpreted tables in general is diﬃcult because
of several “truths” that might exist about what a table is
and how it is interpreted [17], we also came across several
problems in the steps prior to table interpretation.

One such obstacle is that it is not easy to create a per-
manent copy of available web pages as we explain in detail
in [27]. In order to solve this task we built an open source
Firefox extension named WebPageDump1 which can han-
dle the correct saving of most web pages. Another issue is
that marking visual tables that can be built on any element
node other than <table> tags is not trivial. Our method to
ground truth web tables starts with making a local copy of
the online web page and then generating diﬀerent XML ﬁles
that address each of the three steps of table understanding.
Whereas the ground truthing of the table interpretation is
limited by the model as explained in section 5.2, the other
steps can be fully addressed. After saving local copies, we
process the DOM tree of a web page and introduce unique
identiﬁers for elements and words without changing the lay-
out of the web page. Unique references to words are made
possible by tokenizing text nodes and wrapping all words
with a special <x> tag that does not change formatting prop-
erties in contrast to a <span> tag which sometimes inherits
already assigned properties. This <x> tag enfolds every sin-
gle word resulting in a diﬀerentiation on word-level in the
subsequent XML documents, which allows this method to be
applicable to all forms of web tables (Fig. 5). For delimiter-
based web tables, we can combine single words to ﬁctitious
logical cells forming a logical entity. To be general on the
grid level as well, we use the double topological grid struc-
ture, which allows us even to express such tables which are
not aligned at the element node level (Fig. 6).

In order to objectively and transparently test our extrac-
tion system, we ﬁrst wanted to create table ground truth
from a really broad range of web tables. One important cri-
terion was to eliminate our own inﬂuence on the web page
selection and not to “make our scientiﬁc lives easier”. So
we asked students taking a class in web information extrac-
tion at Vienna University of Technology to provide us with
a random selection of web tables, some of which we found
were actual lists according to our deﬁnition from section 3.
As result we received 493 web tables on 269 web pages pro-
vided by 63 students, which we ground truthed according to
the aforementioned method. The test set of web pages to-
gether with the ground truth XML ﬁles will be available for
download on the web page accompanying this publication2.

7. EXPERIMENTAL RESULTS

We evaluated our system on the test set described in the
previous section. We assume that a sample of web pages

1http://www.dbai.tuwien.ac.at/user/pollak/webpagedump
2http://www.dbai.tuwien.ac.at/staff/gatter/ventex/

•Topology & Typography•ContentVISUAL SEMANTICSWORD SEMANTICSInformation gathering stepInput•Link structureInformationIntegrationWeb TableExtraction InformationRetrievalWeb Table In-terpretation CRAWLINGInformation ExtractionMethodWWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages78with tables chosen by a number of people not involved in this
research is suﬃcient to construct an unbiased and objective
set of web tables on the Web.

Despite being a paper on tables, we think that a sim-
ple illustration can better convey the performance (recall r
and precision p) of our system in its individual steps than
a table. A table does show logical relationships between
individual categories and its sublabels. However, it cannot
convey comparisons within entries of the same category as
illustratively as a graphic making an assertion. The recall
of table extraction was 81%, precision 68%, recall of table
interpretation was 57%, and precision at 48%.

Figure 8: Experimental evaluation: True and false
positives in each of the three table understanding
steps as fraction of total HTML tables.

Due to the lack of standardized test sets, it is diﬃcult to
compare the results with existing solutions. Such a broad
compilation of really diverse and sometimes even for humans
diﬃcult to understand web tables has – to our knowledge –
not been tackled in the literature. The work of Wang and
Hu [33], which we have already referenced several times, re-
ports an approximate F-measure of 96% for table location
after training a classiﬁer on discriminating features from the
source code of web pages. Our seemingly inferior result for
table extraction can be set in context by observing: (1) their
approach only works on leaf <table> tables, which we esti-
mate to form less than 85% of current web tables; (2) The
approach just locates tables, but does not recognize the ac-
tual relative topological relations between cells; and (3) our
students wanted to, and did, challenge us. Information on
web pages which are commonly more of interest to auto-
mated information extraction like those from price lists or
comparison pages - which generally appeal to a larger au-
dience and are highly ranked in search engine indexes - is
commonly presented in a more “human interpretable” and,
accordingly, “machine interpretable” form following the vi-
sual approach. Test runs on such web pages have led to far
better results.

Still, we admit that our current results are far from being
perfect test scores. And staying with our chosen test set,
we think that any approach that intends to deal with such a
variety of tables and varying implementation in the source
code will need to use some kind of spatial reasoning on the
rendered web page. We believe that there is no alternative
than the visual path, but admit that details of implementa-
tion still leave space for future improvements.

8. FUTURE WORK

We are currently working to improve our approach in the

following ways:

• Table extraction. We are still in the process of im-
proving the quality of our table extraction step. We
are experimenting in a number of ways to bring the
deﬁned heuristics close to the decision process that a
human follows when spotting aligned information on
a web page. We expect that a rule-based approach
working on the correctly deﬁned set of discriminating
heuristics can bring the results close to human perfor-
mance.

• Table interpretation. Whereas we think that the
model we use is ideally ﬁtted for our task, we are
currently working on a limited base of table types.
We are currently working on an improved table phe-
nomenology, which incorporates a greater variety of
tables found on web pages. Again, as we are working
on distinctive visual characteristics, we also expect to
be able to considerably improve our accuracy except
for some very exotic cases, which would not play an im-
portant role for large-scale knowledge acquisition any-
way.

• Nested substructures. Currently we do not con-
sider any form of nested substructures whose dominant
logical elements do not consist of atomic units (Fig. 3).
We focus on locating dominant structures and regard
their content as atomic units with words ordered in
western reading order. Enlarging the scope of analysis
also to such nested structures poses less a problem to
the location and recognition, but rather the interpre-
tation step. Our current model of tables (and other
structured data) cannot be enlarged to this kind of
nested data structures on the Web.

• Other spatially and visually structured data. So
far, we have only focused on tables. We intend to en-
large the scale of this approach to other typologically
and typographically structured data. An example is
that of book list results on pages like Amazon. No spa-
tial features are present, but distinctive typographical
features like colors for attribute name and attribute
value can be used for relational learning.

• Information Integration. Our described approach
is only one step in a sequential series of three steps to
large-scale knowledge acquisition from the Web (Fig. 7).
One of our subtasks is developing the correct infras-
tructure that allows us to reason on the extracted in-
formation and attempt a probabilistic information in-
tegration from web tables.

9. CONCLUSIONS

In this paper, we formalized an unconventional and promis-
ing approach towards structured information extraction from
the Web; in particular, from web tables. The approach uses
a model of the visual representation of web pages as ren-
dered by a web browser and, therefore, shifts the problem
of information extraction from the lower level of code in-
terpretation (HTML tag structure, CSS, JavaScript code,
etc.)
to the higher level of visual features (2-D topology

PROCESS OF IE FROM WEB TABLESFalse positivesTrue positives039%81%57%100%63%r=57%p=48%TableextractionTable inter-pretationGroundtruthp=68%WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages79and typography). We have also presented a model for rep-
resenting web table structures along with algorithms to de-
rive instances of the model given some arbitrary web pages.
Our approach strives to perform well even without tuning
for speciﬁc application domains such as the interpretation
of product catalogues. We have shown this by providing
a diverse test set of web tables that has been gathered by
63 students. Although our results are preliminary at the
current state, we believe that applying a visual paradigm
towards automatic information extraction from web tables
is promising, especially given the rising complexity in the
encoding of web pages on the source code level. Speciﬁcally,
highly dynamic pages which tend to get more popular with
the rise of Web 2.0 can not be processed without complex
interpretation of the source code.

Acknowledgements. We like to thank Georg Gottlob for
overall motivation of this research, Robert Baumgartner and
his students for providing us with a challenging selection of
web tables, and Tamir Hassan and the anonymous reviewers
for helpful comments.

10. REFERENCES

[16] X. Gu, J. Chen, W.-Y. Ma, and G. Chen. Visual based content

understanding towards web adaptation. In Proc. 2nd AH, pp.
164–173. Springer, May 2002.

[17] J. Hu, R. S. Kashi, D. P. Lopresti, G. T. Wilfong, and

G. Nagy. Why table ground-truthing is hard. In Proc. 6th
ICDAR, pp. 129–133. IEEE, Sept. 2001.

[18] M. Hurst. Layout and language: Challenges for table

understanding on the Web. In Proc. 1st WDA at 6th ICDAR,
pp. 27–30, Sept. 2001.

[19] M. Kovacevic, M. Diligenti, M. Gori, and V. Milutinovic.
Recognition of common areas in a web page using visual
information: a possible application in a page classiﬁcation. In
Proc. 2nd ICDM, pp. 250–257. IEEE, Dec. 2002.

[20] B. Kr¨upl and M. Herzog. Visually guided bottom-up table

detection and segmentation in web documents. In Proc. 15th
WWW, pp. 933–934. ACM, May 2006.

[21] B. Kr¨upl, M. Herzog, and W. Gatterbauer. Using visual cues

for extraction of tabular data from arbitrary HTML
documents. In Poster Proc. 14th WWW, pp. 1000–1001.
ACM, May 2005.

[22] K. Lerman, L. Getoor, S. Minton, and C. A. Knoblock. Using

the structure of web sites for automatic segmentation of tables.
In Proc. SIGMOD, pp. 119–130. ACM, June 2004.

[23] B. Liu and K. C.-C. Chang. Editorial: special issue on web

content mining. SIGKDD Explorations, 6(2):1–4, 2004.

[24] B. Parsia and P. F. Patel-Schneider. Meaning and the

Semantic Web. In Proc. IRW at 15th WWW, May 2006.
[25] G. Penn, J. Hu, H. Luo, and R. McDonald. Flexible web

document analysis for delivery to narrow-bandwidth devices.
In Proc. 6th ICDAR, pp. 1074–1078. IEEE, Sept. 2001.

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of

[26] A. Pivk, P. Cimiano, and Y. Sure. From tables to frames.

databases. Addison-Wesley, 1995.

[2] E. Agichtein and L. Gravano. Snowball: Extracting relations
from large plain-text collections. In Proc. 5th ACM DL, pp.
85–94. ACM, June 2000.

[3] Y. Aumann, R. Feldman, Y. Liberzon, B. Rosenfeld, and
J. Schler. Visual information extraction. Knowledge and
Information Systems, 10(1):1–15, 2006.

[4] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and

O. Etzioni. Open information extraction from the Web. In
Proc. 20th IJCAI, pp. 2670–2676, Jan. 2007.

[5] M. Bilenko, S. Basu, and M. Sahami. Adaptive product

normalization: Using online learning for record linkage in
comparison shopping. In Proc. 5th ICDM, pp. 58–65. IEEE,
Nov. 2005.

Journal of Web Semantics, 3(2-3):132–146, 2005.

[27] B. Pollak and W. Gatterbauer. Creating permanent test sets of

web pages for information extraction research. In Proc. 33rd
SOFSEM: Theory and Practice of Computer Science, vol. II,
pp. 103–115, Jan. 2007.

[28] K. Simon and G. Lausen. ViPER: augmenting automatic

information extraction with visual perceptions. In Proc. 14th
CIKM, pp. 381–388. ACM, Nov. 2005.

[29] A. Tengli, Y. Yang, and N. L. Ma. Learning table extraction

from examples. In Proc. 20th COLING, pp. 987–993.
COLING, Aug. 2004.

[30] Y. A. Tijerino, D. W. Embley, D. W. Lonsdale, Y. Ding, and

G. Nagy. Towards ontology generation from tables. World
Wide Web, 8(3):261–285, 2005.

[6] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. Extracting content

[31] C. Vanoirbeek. Formatting structured tables. In Proc. of

structure for web pages based on visual representation. In
Proc. 5th APWeb, pp. 406–417. Springer, Apr. 2003.

Electronic Publishing’92, pp. 291–309. Cambridge University
Press, Apr. 1992.

[7] H.-H. Chen, S.-C. Tsai, and J.-H. Tsai. Mining tables from

[32] X. Wang. Tabular abstraction, editing, and formatting. Ph.D.

large scale HTML texts. In Proc. 18th COLING, pp. 166–172.
Morgan Kaufmann, Aug. 2000.

[8] W. W. Cohen, M. Hurst, and L. S. Jensen. A ﬂexible learning
system for wrapping tables and lists in HTML documents. In
Proc. 11th WWW, pp. 232–241. ACM, May 2002.

[9] M. Cosulschi, N. Constantinescu, and M. Gabroveanu.

Classiﬁcation and comparison of information structures from a
web page. The Annals of the University of Craiova,
31:109–121, 2004.

[10] A. Culotta, A. McCallum, and J. Betz. Integrating

probabilistic extraction models and data mining to discover
relations and patterns in text. In Proc. HLT-NAACL, pp.
296–303, New York, NY, June 2006.

[11] N. N. Dalvi and D. Suciu. Answering queries from statistics
and probabilistic views. In Proc. 31st VLDB, pp. 805–816,
Aug. 2005.

[12] D. W. Embley, M. Hurst, D. P. Lopresti, and G. Nagy.
Table-processing paradigms: a research survey. IJDAR,
8(2-3):66–86, June 2006.

[13] D. W. Embley, D. P. Lopresti, and G. Nagy. Notes on

contemporary table recognition. In Proc. 7th Int. Workshop
on Document Analysis Systems (DAS), pp. 164–175.
Springer, Feb. 2006.

[14] O. Etzioni, M. J. Cafarella, D. Downey, A.-M. Popescu,

T. Shaked, S. Soderland, D. S. Weld, and A. Yates. Methods
for domain-independent information extraction from the Web:
An experimental comparison. In Proc. 19th AAAI, pp.
391–398. AAAI Press / MIT Press, July 2004.

[15] W. Gatterbauer and P. Bohunsky. Table extraction using

spatial reasoning on the CSS2 visual box model. In Proc. 21st
AAAI, pp. 1313–1318. AAAI Press, July 2006.

thesis, University of Waterloo, 1996.

[33] Y. Wang and J. Hu. A machine learning based approach for

table detection on the Web. In Proc. 11th WWW, pp.
242–250. ACM, May 2002.

[34] H. Wium Lie, B. Bos, C. Lilley, and I. Jacobs. Cascading Style

Sheets, level 2. Technical report, World Wide Web
Consortium, 1998. See http://www.w3.org/TR/REC-CSS2.

[35] T. Wohlberg. Hypertables: Development of a structure
description language for tables in XML. Master thesis,
University of Hamburg, Germany, 1999. (Original title in
German: Hypertables: Entwicklung einer
Strukturbeschreibungssprache f¨ur Tabellen in XML).

[36] Y. Yang and W.-S. Luk. A framework for web table mining. In
Proc. 4th WIDM at 11th CIKM, pp. 36–42. ACM, Nov. 2002.

[37] Y. Yang and H. Zhang. HTML page analysis based on visual

cues. In Proc. 6th ICDAR, pp. 859–864. IEEE, Sept. 2001.

[38] M. Yoshida, K. Torisawa, and J. Tsujii. A method to integrate

tables of the world wide web. In Proc. 1st WDA at 6th
ICDAR, pp. 31–34, Sept. 2001.

[39] R. Zanibbi, D. Blostein, and J. R. Cordy. A survey of table

recognition. IJDAR, 7(1):1–16, 2004.

[40] Y. Zhai and B. Liu. Web data extraction based on partial tree
alignment. In Proc. 14th WWW, pp. 76–85. ACM, May 2005.

[41] H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. Fully

automatic wrapper generation for search engines. In Proc. 14th
WWW, pp. 66–75. ACM, May 2005.

WWW 2007 / Track: Data MiningSession: Identifying Structure in Web Pages80