Efﬁcient Training on Biased Minimax Probability Machine

for Imbalanced Text Classiﬁcation

Department of Computer Science & Engineering

Department of Computer Science & Engineering

Xiang Peng

Irwin King

The Chinese University of Hong Kong

Shatin, N.T., Hong Kong

xpeng@cse.cuhk.edu.hk

The Chinese University of Hong Kong

Shatin, N.T., Hong Kong

king@cse.cuhk.edu.hk

ABSTRACT
The Biased Minimax Probability Machine (BMPM) con-
structs a classiﬁer which deals with the imbalanced learning
tasks. In this paper, we propose a Second Order Cone Pro-
gramming (SOCP) based algorithm to train the model. We
outline the theoretical derivatives of the biased classiﬁcation
model, and address the text classiﬁcation tasks where nega-
tive training documents signiﬁcantly outnumber the positive
ones using the proposed strategy. We evaluated the learn-
ing scheme in comparison with traditional solutions on three
diﬀerent datasets. Empirical results have shown that our
method is more eﬀective and robust to handle imbalanced
text classiﬁcation problems.

Categories and Subject Descriptors
1.5.2 [Pattern Recognition]: Design Methodology—Clas-
siﬁer Design and Evaluation; H.2.8 [Database Manage-
ment]: Database Application—Data Mining

General Terms
Algorithm, Management, Experimentation

Keywords
Biased Classiﬁcation, Second Order Cone Programming, Bi-
ased Minimax Probability Machine, Text Classiﬁcation

1.

INTRODUCTION

With the rapid growth of text information on the World
Wide Web (WWW), text classiﬁcation has become one of
the most important topic in both the community of research
and engineering [1]. However there are two major problems
with current algorithms involving in text classiﬁcation task.
One key challenge is that almost all the algorithms treat the
problem as a balanced classiﬁcation task and they do not
consider the imbalanced dataset matter, which means the
number of negative documents is larger than the number of
positive ones. Take the task of learning which news articles
are of interest to a particular person reading Google News
for example. The articles which the person interested may
be just a small portion in the whole text database. Methods
that ﬁlter and present only the ones that user ﬁnds interest-
ing are highly desirable. Currently, most researchers treat

this problem as a strict binary classiﬁcation problem while
ignore the fact that the number of uninterested documents is
extremely larger than the interested ones. How to make the
returned document set as accuracy as possible is a crucial
problem. At the same time, in order to build a reliable clas-
siﬁer for text classiﬁcation, we need to train the model with
huge number of predeﬁned documents, which is usually a
very time consuming process. Thus, how to reduce the time
required for training a reliable text classiﬁer is a crucial ob-
stacle for large scale text classiﬁcation. This is particularly
challenging for text classiﬁcation of WWW documents given
its nature of large volume.

In this paper, we apply the model of Biased Minimax
Probability Machine (BMPM) to the problem of imbalanced
text classiﬁcation, and propose a new training algorithm to
tackle the complexity and accuracy issues in BMPM learning
task. This model is transformed into a Second Order Cone
Programming (SOCP) problem. Under this new proposed
framework, the imbalanced text classiﬁcation problem could
be modelled and solved eﬃciently.

The rest of this paper is organized as follows. Section II
introduces the concept of BMPM. Section III presents an ef-
fective learning algorithm based on SOCP for eﬀective train-
ing with BMPM. Section IV presents the results of our em-
pirical study.

2. BIASED MINIMAX PROBABILITY MA-

CHINE

We assume two random vectors x and y represent two
classes of data with mean and covariance matrices as {x, Σx}
and {y, Σy}, respectively in a two-category classiﬁcation
task, where x, y, x, y ∈ Rn, and Σx, Σy ∈ Rn×n. Biased
Minimax Probability Machine (BMPM) attempts to deter-
mine the hyperplane aT z = b with aT z > b being considered
as class x and aT z < b being judged as class y to separate
the important class of data x with a maximal probability
while keeping the accuracy of less important class of data y
acceptable.1 We formulate this objective as follows:

max

α,β,b,a(cid:54)=0

s.t.

inf

x∼(¯x,Σx)
y∼(¯y,Σy)

inf

α

Pr{aTx ≥ b} ≥ α,
Pr{aTy ≤ b} ≥ β,
β ≥ β0,

(1)

Copyright is held by the author/owner(s).
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

where α and β represent the lower bounds of the accuracy for
1The reader may refer to [2] for a more detailed and com-
plete description.

WWW 2007 / Poster PaperTopic: Search1153BM P MSOCP
81.42 ± 0.22 ↑
70.00 ± 0.00
83.10 ± 0.60 ↑
72.61 ± 0.84
77.85 ± 0.04

BM P MF P
80.35 ± 0.13 ↑
70.00 ± 0.00
81.07 ± 0.63 ↑
74.48 ± 0.69
77.70 ± 0.21

M P M

76.30 ± 0.28
76.30 ± 0.34
74.91 ± 0.61
75.20 ± 0.62
75.05 ± 0.37

SV M

kN N

-
-

73.23 ± 1.59
74.60 ± 0.47
73.90 ± 0.44

-
-

71.60 ± 0.38
69.40 ± 0.60
70.50 ± 0.55

α
β

T SAx
T SAy
T SA

Table 1: Lower Bound α and Test-Set Accuracy on the Reuter-21578 dataset (%)

future data classiﬁcation, namely, the worst-case accuracy.
Meanwhile, β0 is a pre-speciﬁed positive constant which rep-
resents an acceptable accuracy for the less important class.

3. EFFICIENT BMPM TRAINING

3.1 Motivation

Most of recent studies on BMPM are usually based on the
Fractional Programming problem (we name it BM P MF P )
which could be solved by Rosen Gradient method. How-
ever the problem reformulation has some crucial assumption
when doing the transformation which would lead to failure of
the model solution. Another issue is that when applying the
Fractional Programming based BM P MF P into large real-
world classiﬁcation problems, it would be very sensitive to
data dimension and very time consuming.
3.2 Proposed Strategy

Our main result is stated below.

Theorem 1. If x = y, then the minimax probability de-
cision problem (1) does not have a meaningful solution: the
optimal worst-case misclassiﬁcation probability that we ob-
tain is 1 - α∗ =1. Otherwise, an optimal hyperplane H(a∗, b∗)
exists, and can be determined by solving the convex optimiza-
tion problem:

min
t,a(cid:54)=0
s.t.

t − aT (x − y)
(cid:107) Σ
x a (cid:107)≤ 1,
y a (cid:107)≤
1−β0
β0

(cid:113)

1
2

(cid:107) Σ

1
2

t,

(cid:114)

(2)

(cid:112)

a∗T Σxa∗,

α∗

1 − α∗

and setting b to the value

(cid:115)

(cid:112)

∗

b

∗T y+

= a

β0
1 − β0

a∗T Σya∗ = a

∗T x−

where a∗ is an optimal solution of (1), and t ∈ R is a new
optimization variable. Furthermore, if either Σx or Σy is
positive deﬁnite, the optimal hyperplane is unique.

Lemma 1. The Second Order Cone Programming prob-
lem with linear objective function and norm constraints is
a convex optimization problem and thus can be solved eﬃ-
ciently.

We omit the details of the proofs due to space limitations.

4. EXPERIMENTAL RESULTS

We evaluated our proposed biased learning algorithm in
comparison to the state-of-the-art approaches by conducting
empirical comparisons on three standard datasets for text
document classiﬁcation: Reuters-21578 dataset, 20-Newsgroup

Figure 1: ROC curves on Reuters-21578 dataset:
Full Range (Left), Crucial Part (Right)

data collection and Enron Corpus. For all three datasets,
the same data pre-processing and feature selection proce-
dures are applied. Due to space limitations, we only present
our results on Reuters-21578 dataset.

Applying BMPM-based technique in text classiﬁcation is
a very straightforward task, where we just need to assume
the interested documents to be the more important class (x)
in the biased classiﬁcation framework while assume the un-
interested ones to be the less important class (y). For exper-
imental setting up, we employ Receiver Operating Charac-
teristic (ROC) analysis and Test Set Accuracy (TSA) as the
performance measurements. The involved traditional algo-
rithms are the Support Vector Machine (SVM), k -Nearest
Neighbor (k NN) and Minimax Probability Machine (MPM).
Table 1 shows the experimental results of TSA perfor-
mance evaluation, where we can see that our two BMPM
models achieve better performances than the other algo-
rithms in most of the cases while the BM P MSOCP generally
outperforms the BM P MF P method.

Furthermore, It is observed from the ROC curves in Fig-
ure 1 that most parts of the ROC curve of BMPMs are above
the corresponding curve of k NN along with the BM P MSOCP
curve is above the one of BM P MF P , which demonstrate the
superiority of the BMPM models and our proposed BM P MSOCP
algorithm.

5. ACKNOWLEDGMENTS

The work described in this paper is supported by a grant
from the Research Grants Council of the Hong Kong Special
Administrative Region, China (Project No. CUHK4235/04E)
and is aﬃliated with the Microsoft-CUHK Joint Laboratory
for Human-centric Computing and Interface Technologies.

6. REFERENCES
[1] S. C. Hoi, R. Jin, and M. Lyu. Large-scale text

categorization by batch mode active learning. In Proc.
of World Wide Web Conference, pages 633–642, 2006.

[2] K. Huang, H. Yang, I. King, M. Lyu, and L. Chan.

Minimum error minimax probability machines. Journal
of Machine Learning Research, 5:1253–1286, 2004.

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91False Positive RateTrue Positive Rate(a) ROC for Reuters−21578 DatasetBMPMSOCPBMPMFPk−NN00.050.10.150.20.250.30.350.40.450.50.50.550.60.650.70.750.80.850.90.951False Positive RateTrue Positive Rate(a) ROC for Reuters−21578 DatasetBMPMSOCPBMPMFPk−NNWWW 2007 / Poster PaperTopic: Search1154