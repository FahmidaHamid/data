On Ranking Techniques for Desktop Search ∗

Sara Cohen

Faculty of Industrial

Engineering and Management
Technion—Israel Institute of

Technology

Carmel Domshlak
Faculty of Industrial

Naama Zwerdling
Faculty of Industrial

sarac@ie.technion.ac.il

dcarmel@ie.technion.ac.il

anaama@tx.technion.ac.il

Engineering and Management
Technion—Israel Institute of

Engineering and Management
Technion—Israel Institute of

Technology

Technology

ABSTRACT
This paper addresses the desktop search problem by consid-
ering various techniques for ranking results of a search query
over the ﬁle system. First, basic ranking techniques, which
are based on a single ﬁle feature (e.g., ﬁle name, ﬁle content,
access date, etc.) are considered. Next, two learning-based
ranking schemes are presented, and are shown to be sig-
niﬁcantly more eﬀective than the basic ranking methods.
Finally, a novel ranking technique, based on query selective-
ness is considered, for use during the cold-start period of
the system. This method is also shown to be empirically
eﬀective, even though it does not involve any learning.

Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage and Retrieval]:
Information Search and Re-
trieval

General Terms: Experimentation, Human Factors

Keywords: desktop search, personal information manage-
ment, ranking

1.

INTRODUCTION

Due to the increasing storage capabilities of standard per-
sonal computers, users are no longer motivated to delete old
ﬁles. The practice of “never deleting ﬁles” has distinct ad-
vantages, since it ensures that important data will never
be accidentally removed. However, as an unfortunate side
eﬀect, the personal computer often becomes an unwieldy
mess. Thus, locating a speciﬁc ﬁle, within the ﬁle system,
may become a challenging (and even daunting) task. There-
fore, numerous research and industrial projects have evolved
in the area of personal information management (PIM) [5].
These projects aim to develop technologies allowing users to
store, access, and eﬀectively search for information in their
personal computer, e.g., [3, 2, 1].

The focus of this paper is on desktop search, i.e., eﬀec-
tive search within a personal computer. There are currently
only limited (published) insights into the question of how
to rank desktop-search results, mostly based on very simple
techniques. In this paper we focus on the problem ﬁnding
an eﬀective ranking scheme for desktop search. To this end,

∗Sara Cohen and Naama Zwerdling were partially supported

by the ISF (Grant 1032/05). Carmel Domshlak was partially
supported by the BSF (Grant 2004216).

Copyright is held by the author/owner(s).
WWW 2007, May 8–12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

we implemented a simple desktop-search tool, which returns
unranked results to the users and logs the user interaction.
We develop various ranking strategies and analyze their ef-
fectiveness, “post-mortem,” based on the logs.

2. RANKING TECHNIQUES

A query q is simply a sequence of words. A ﬁle f is a
candidate answer for q if there is at least one word appearing
in q that also appears in the content, ﬁlename, or path of
f . We discuss various techniques for ranking a candidate
answer f .

Ranking by Basic File Features. We considered both textual-
based and date-based features. For the textual-based fea-
tures we consider Content, Path, Name, whose values
correlate with the cosine distance between the tf.idf vec-
tors of the query and the content, path, name of f , respec-
tively. For the date-based features we allow AccessDate,
UpdateDate, and CreateDate whose values decrease as
the distance in time grows between the date of querying and
the access, update, and create date of f , respectively.

Learning to Rank. As we already mentioned, one of the
components of our desktop search system is the logger that
stores all the data on the past search sessions of the user. In
principle, this information can be used for learning a better
ranking method. We considered two such ranking methods.
We use a Support Vector Machine (SVM) to learn a linear
function of the basic ﬁle features that minimizes the classiﬁ-
cation error on our training logger data. (Similarly to, e.g.,
[4], we actually learned a binary relation over the candidate
answers.) Thus, the feature SVM-75 is deﬁned for each user,
by using as training data four randomly chosen subsets of
75% of its log data, with the remaining 25% used as the test
set. The results were averaged over these four samples and
over all users. Similarly, the feature SVM-90, which has a
90/10 data partition setup, was evaluated.

Learning a ranking function using a SVM can be done in
time linear in the number of ﬁle features, and polynomial
in the number of candidate answers of all queries seen thus
far. As the amount of the logged search sessions grows, the
latter complexity factor signiﬁcantly slows down the learner.
Although learning can be performed oﬄine, at some point
the system will unavoidably have to cut down the amount
of available training data, possibly using only some chrono-
logical suﬃx of the logged data. Due to this limitation, we
also considered the simple learning scheme, called LexOrd
(for “lexicographic order”), which corresponds to a lexico-

WWW 2007 / Poster PaperTopic: Search1183Feature

SVM-90
SVM-75
LexOrd

AccessDate
UpdateDate

Selective

CreateDate

Name
Path

Random
Content

Score(τ, Sall)

7.84
8.15
9.19
9.85
10.18
10.89
11.51
11.53
12.60
14.45
15.43

Feature

SVM-90
Selective
SVM-75
LexOrd

Name
Path

UpdateDate
AccessDate

Content

CreateDate

Random

Score(τ, S2-50)

4.72
4.86
4.93
5.15
5.66
6.32
6.43
6.54
6.75
6.80
6.99

Feature

SVM-90
SVM-75
LexOrd

Selective

UpdateDate
AccessDate

Name

CreateDate

Content

Path

Random

TopScorek(τ, Sall) TopScorek(τ, S2-50)
k = 1

k = 10

k = 1

k = 10

34.4
32.8
34.0

27.1

21.6
19.2
16.5
17.7
13.2
6.9
0.0

64.9
63.5
62.5

63.1

52.5
52.3
51.7
48.5
45.0
46.0
36.3

36.6
34.5
37.2

29.3

23.4
19.2
18.1
19.2
14.6
7.2
0.0

71.7
70.5
68.5

72.7

57.8
52.3
60.5
54.1
55.1
54.1
46.7

Figure 1: Expected placement of features (left table), and eﬀectiveness at k ∈ {1, 10} (right table).

graphic aggregation of single-feature-based rankings based
on their relative eﬃciency when used in isolation.

Selectiveness of Features. Learning-based ranking is rel-
evant only in presence of suﬃcient training data.
In the
context of desktop search, this can be a real obstacle, since
users tend to issue only a few queries a day. We suggest a
the simple ranking mechanism Selective that combines the
textual-based features, based on their relative selectiveness,
that can be useful for the cold-start period. Intuitively, the
ranking mechanism Selective, combines (i) the informa-
tion carried by the textual properties of the candidate an-
swers, and (ii) the frequency of textual connection between
each such property and query, within the set of candidate
answers. Thus, Selective follows the principle underly-
ing the standard idf (inverse document frequency) measure
used in IR—the contributions of diﬀerent textual features to
Selective on a given candidate answer are combined via a
weighted sum in which less “common” features (in the set
of candidate answers) are given larger weights.

3. EXPERIMENTAL EVALUATION

The desktop search engine was distributed to 19 volun-
teers. In total 1219 queries were issued, where 188 queries
had a single result (i.e., candidate answer), 916 queries has
2–50 results and 115 queries had over 50 results.

We analyzed the eﬀectiveness of our ranking mechanisms
on the set of all queries with more than one result, with 2–
50 results and with over 50 results, denoted Sall, S2-50 and
S>50, respectively. The results for S>50 are similar to those
of Sall and are omitted due to space limitations.
Two measures were used to evaluate the eﬀectiveness of
our ranking mechanisms: (1) Score(τ,S) (S ∈ {Sall,S2-50})
is the expected placement of the user’s target ﬁle,1 for queries
in S, according to the ranking mechanism τ . (2) TopScorek(τ,S),
called the eﬀectiveness of τ at k, is the percentage of queries
in S in which τ ranks the target ﬁle within the top k ﬁles.
(We only consider queries that have at least k results in this
measure.) Thus, a good ranking mechanism will have a low
value for Score(τ,S) and a high value for TopScorek(τ,S).
Our analysis of the ranking mechanisms considered is sum-
marized in Figure 1. When considering only the basic ﬁle
features, the date-based features have signiﬁcantly better
expected placement than the textual-based features for Sall,
1The target ﬁle is the ﬁle chosen by the user. We assume
that this ﬁle is unique and is recognized by the user.

while the opposite is true for S2-50.2 We believe that this
can be explained since textual-based features are most useful
when they are selective (i.e., when there are few results).

Our learning-based ranking features (SVM-75, SVM-90
and LexOrd) signiﬁcantly outperformed the basic ranking
features. For Score(τ,·), we observe that: (1) SVM-75 and
SVM-90 ended up clear winners and (2) the diﬀerence be-
tween LexOrd and the next most eﬀective method was sta-
tistically signiﬁcant. For all TopScorek(τ,·), k ∈ {1, 10}, it
can be seen that SVM-90, SVM-75, and LexOrd are more
eﬀective than all basic ranking methods.
Finally, we consider eﬀectiveness of Selective. For the
measure Score(Selective,·), we observe that Selective is
better than its most eﬀective component (i.e., the textual-
based features), and this diﬀerence is statistically signiﬁcant.
Somewhat surprisingly, on S2-50 Selective even success-
fully competes with our learning-based ranking methods—
it appears to be more eﬀective than LexOrd (with statis-
tical signiﬁcance) and is, in fact, statistically indistinguish-
able with SVM-75 and SVM-90. Thus, Selective appears
highly eﬀective for S2-50, even though it does not involve
any learning. Observe also that Selective is always among
the four best features for TopScorek(τ,·).

To summarize, we have shown that our learning-based
ranking methods are clearly much more eﬀective than the
basic ﬁle features, and that Selective is a good choice for
S2-50, during the cold-start period.

4. REFERENCES
[1] S. Dumais, E. Cutrell, JJ Cadiz, G. Jancke, R. Sarin,

and D. C. Robbins. Stuﬀ I’ve seen: a system for
personal information retrieval and re-use. In SIGIR’03.

[2] R. Fagin, R. Kumar, K. S. McCurley, J. Novak,

D. Sivakumar, J. A. Tomlin, and D. P. Williamson.
Searching the workplace web. In WWW’03.

[3] J. Gemmell, G. Bell, R. Lueder, S. Drucker, and

C. Wong. MyLifeBits: Fulﬁlling the Memex vision. In
ACM Multimedia’02.

[4] T. Joachims. Optimizing search engines using

clickthrough data. In KDD’02.

[5] J. Teevan, W. Jones, and B. B. Bederson, editors.

Special Issue on Personal Information Management,
volume 49 of Communication of the ACM, 2006.

2Statistically signiﬁcant diﬀerences in performance are de-
termined using the two-sided Wilcoxon test at the 95% con-
ﬁdence level.

WWW 2007 / Poster PaperTopic: Search1184