Size Matters: Word Count as a Measure

of Quality on Wikipedia

Joshua E. Blumenstock

School of Information

University of California at Berkeley

jblumenstock@berkeley.edu

ABSTRACT
Wikipedia, “the free encyclopedia”, now contains over two
million English articles, and is widely regarded as a high-
quality, authoritative encyclopedia. Some Wikipedia arti-
cles, however, are of questionable quality, and it is not al-
ways apparent to the visitor which articles are good and
which are bad. We propose a simple metric – word count –
for measuring article quality. In spite of its striking simplic-
ity, we show that this metric signiﬁcantly outperforms the
more complex methods described in related work.

Categories and Subject Descriptors: H.5.3 [Informa-
tion Interfaces]: Group and Organization Interfaces– Col-
laborative computing; I.2.7 [Artiﬁcial Intelligence]: Natural
Language Processing– Text analysis.

General Terms: Measurement, algorithms.

Keywords: Wikipedia, information quality, word count.

1.

INTRODUCTION

As user-generated content grows in prominence, many web
sites have employed complex mechanisms to help visitors
identify high quality content. Wikipedia maintains a list
of “featured” articles that serve as exemplars of good user-
generated content. For an article to be featured, it must
survive a rigorous nomination and peer-review process; only
one article in every thousand makes the cut. Unfortunately,
this is a laborious process, and many worthy articles never
get the oﬃcial stamp of approval. Thus, it might be useful to
have an automatic means for detecting articles of unusually
high quality.

A substantial amount of work has been done to automat-
ically evaluate the quality of Wikipedia articles. At a qual-
itative level, Lih [3] proposed using the total number of ed-
its and unique editors to measure article quality, and Cross
[2] suggested coloring text according to age so that visitors
could immediately discern its quality. Both studies, how-
ever, were primarily qualitative, and did not measure the
discriminative value of such heuristics.

The quantitative approaches found in related work tend
[6], for instance, used a
to be quite complex. Zeng et al.
dynamic bayesian network to develop a measure of trust and
quality based on the edit history of an article. Adler and de
Alfaro [1] devised similar metrics to quantify the reputation
of authors and editors.
In the work closest to our own,

Copyright is held by the author/owner(s).
WWW 2008, April 21–25, 2008, Beijing, China.
ACM 978-1-60558-085-2/08/04.

Stvilia et al.
[5], computed 19 quantitative metrics, then
used factor analysis and k-means clustering to diﬀerentiate
featured from random articles.

2. METHODS

In contrast to the complex quantitative methods found in
related work, we propose a much simpler measure of quality
for Wikipedia articles: the length of the article, measured
in words. While there are many limitations to such a met-
ric, there is good reason to believe that this metric will be
correlated to quality (see Figure 1). The simplicity of this
metric presents several advantages:

(cid:129) article length is easy to measure;
(cid:129) many of the approaches mentioned in section 1 require
information that is not easily obtained (such as the re-
vision and history used in [6], [4], and [1]);
(cid:129) other approaches typically operate in a black box fash-
ion, with arcane parameters and results that are not
easily interpreted by the average visitor to Wikipedia;
(cid:129) article length performs signiﬁcantly better than other,

more complex methods.

Random Articles

Featured Articles

y
t
i
s
n
e
d
 
y
t
i
l
i

b
a
b
o
r
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

y
t
i
s
n
e
d
 
y
t
i
l
i

b
a
b
o
r
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0

2

4

6

8

10

0

2

4

6

8

10

log(word_count)

log(word_count)

Figure 1: Word counts for featured/random articles

To test the performance of article length as a discriminant
between high and low quality articles, we followed the ap-
proach taken by Zeng et al. [6] and Stvilia et al. [5] That is,
instead of comparing our metric against a scalar measure of
article quality, we assume that featured articles are of much
higher quality than random articles, and recast the prob-
lem as a classiﬁcation task. The goal is thus to maximize
precision and recall of featured and non-featured articles.

To build a corpus, we extracted the full 5,654,236 arti-
cles from the 7/28/2007 dump of the English Wikipedia.

1095WWW 2008 / Poster PaperApril 21-25, 2008 · Beijing, Chinaclass

n

TP rate FP rate Precision Recall F-measure

Featured
1554
Random 9513

0.936
0.977

0.023
0.064

0.871
0.989

0.936
0.977

0.902
0.983

Table 1: Performance of word count in classifying featured vs. random articles.

After stripping all Wikipedia-related markup, we removed
specialized ﬁles (such as images and templates) and articles
containing fewer than ﬁfty words. This cleaned dataset con-
tained 1,554 articles classiﬁed as “featured”; we randomly
selected an additional 9,513 cleaned articles to serve as a
non-featured “random” corpus. Our corpus thus contained
a total of 11,067 articles. In the experiments described be-
low, we used 2/3 of the articles for training (7,378 articles)
and 1/3 for testing (3,689 articles), with a similar ratio of
featured/random articles in each set.

3. RESULTS

By classifying articles with greater than 2,000 words as
“featured” and those with fewer than 2,000 words as “ran-
dom,” we achieved 96.31% accuracy in the binary classiﬁ-
cation task.1 The threshold was found by minimizing the
error rate on the training set (see Figure 2). The reported
accuracy results from testing on the held-out test set.

Modest improvements could be produced by more sophis-
ticated classiﬁcation techniques. A multi-layer perceptron,
for instance, achieved an overall accuracy of 97.15%, with
an f-measure of .902 for featured articles and .983 for ran-
dom articles (see Table 1). Similar results were replicated
with a k -nearest neighbor classiﬁer (96.94% accuracy), a
logit model (96.74% accuracy), and a random-forest classi-
ﬁer (95.80% accuracy). All techniques represent a signiﬁcant
improvement over the more complex methods in Stvilia et
[4] and Zeng et al.
al.
[6], which produced 84% and 86%
accuracy, respectively.

Featured articles
Random articles
Combined

t

e
a
r
 
r
o
r
r

E

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0

1000

2000

3000

4000

5000

6000

Word count threshold

Figure 2: Accuracy of diﬀerent thresholds

Given the high accuracy of the word count metric, we nat-
urally wondered whether other simple metrics might increase
classiﬁcation accuracy. In other contexts, features such as
part of speech tags, readability metrics, and n-gram bag-of-
words have been moderately successful.
In the context of
Wikipedia quality, however, we found that word count was
hard to beat. N -gram bag-of-words classiﬁcation, for in-
stance, produced a maximum of 81% accuracy (tested with
n=1,2,3 on both svm and bayesian classiﬁers). Even using a
1A slightly higher accuracy of 96.46% was achieved with a
threshold of 1,830 words.

“kitchen sink” of thirty features such as those listed in Table
2, no classiﬁer achieved greater than 97.99% accuracy – a
modest improvement given the considerable eﬀort required
to produce these metrics and build the classiﬁers.

Frequency counts

character count

token count

complex word count
one-syll. word count

sentence count
total syllables

Readability indices

Gunning fog index

FORCAST formula

Coleman-Liau

Automatic Readability

Flesch-Kincaid
SMOG index

Structural features

internal links
category count
citation count

external links
image count
table count

reference links
reference count
section count

Table 2: Features from “kitchen sink” classiﬁcation

4. DISCUSSION AND CONCLUSIONS

We have shown that article length is a very good predictor
of whether an article will be featured on Wikipedia. Word
count is a simple metric that is considerably more accurate
than the complex methods proposed in related work, and
performs well independent of classiﬁcation algorithm and
parameters.

We do not, however, mean to exaggerate the importance
of this metric. By assuming that “featured” status is an
accurate proxy for quality, we have implied that quality can
be measured via article length. However, if our assumption
does not hold, then we can only conclude that long articles
are featured, and featured articles are long. Future work
will explore alternative standards for quality on Wikipedia.

Acknowledgments
We thank Marti Hearst for guidance and feedback.

5. REFERENCES
[1] B. T. Adler and L. de Alfaro. A content-driven reputation

system for the wikipedia. Proc. 16th Intl. Conf. on the World
Wide Web, pages 261–270, 2007.

[2] T. Cross. Puppy smoothies: Improving the reliability of open,

collaborative wikis. First Monday, 11, 2006.

[3] A. Lih. Wikipedia as participatory journalism: Reliable

sources? metrics for evaluating collaborative media as a news
resource. 13th Asian Media Information and
Communications Centre Annual Conference, 2004.

[4] B. Stvilia, M. Twidale, L. Gasser, and L. Smith. Information

quality discussions in wikipedia. Proc. 2005 ICKM, pages
101–113, 2005.

[5] B. Stvilia, M. B. Twidale, L. C. Smith, and L. Gasser.

Assessing information quality of a community-based
encyclopedia. Proc. ICIQ, pages 442–454, 2005.

[6] H. Zeng, M. Alhossaini, L. Ding, R. Fikes, and D. L.

McGuinness. Computing trust from revision history. Intl.
Conf. on Privacy, Security and Trust, 2006.

1096WWW 2008 / Poster PaperApril 21-25, 2008 · Beijing, China