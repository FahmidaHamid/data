Efﬁcient Similarity Joins for Near Duplicate Detection

Chuan Xiao Wei Wang

Xuemin Lin

School of Computer Science and Engineering

University of New South Wales

Australia

{chuanx, weiw, lxue}@cse.unsw.edu.au

Jeffrey Xu Yu

Department of Systems Engineering and

Engineering Management

Chinese University of Hong Kong

Hong Kong, China

yu@se.cuhk.edu.hk

ABSTRACT
With the increasing amount of data and the need to inte-
grate data from multiple data sources, a challenging issue
is to ﬁnd near duplicate records eﬃciently. In this paper,
we focus on eﬃcient algorithms to ﬁnd pairs of records such
that their similarities are above a given threshold. Several
existing algorithms rely on the preﬁx ﬁltering principle to
avoid computing similarity values for all possible pairs of
records. We propose new ﬁltering techniques by exploiting
the ordering information; they are integrated into the exist-
ing methods and drastically reduce the candidate sizes and
hence improve the eﬃciency. Experimental results show that
our proposed algorithms can achieve up to 2.6x–5x speed-up
over previous algorithms on several real datasets and pro-
vide alternative solutions to the near duplicate Web page
detection problem.
Categories and Subject Descriptors: H.3.3 [Informa-
tion Search and Retrieval]: Search Process, Clustering
General Terms: Algorithms, Performance
Keywords: similarity join, near duplicate detection

1.

INTRODUCTION

One of the issues accompanying the rapid growth of data
on the Internet and the growing need to integrating data
from heterogeneous sources is the existence of near dupli-
cate data. Near duplicate data bear high similarity to each
other, yet they are not bitwise identical. There are many
causes for the existence of near duplicate data: typograph-
ical errors, versioned, mirrored, or plagiarized documents,
multiple representations of the same physical object, spam
emails generated from the same template, etc. As a concrete
example, a sizeable percentage of the Web pages are found
to be near-duplicates by several studies [6, 14, 18]. These
studies suggest that around 1.7% to 7% of the Web pages
visited by crawlers are near duplicate pages.

Identifying all the near duplicate objects beneﬁts many

applications. For example,
• For Web search engines, identifying near duplicate Web
pages helps to perform focused crawling, increase the qual-
ity and diversity of query results, and identify spams [14,
11, 18].

• Many Web mining applications rely on the ability to ac-

curately and eﬃciently identify near-duplicate objects. They

Copyright
is held by the International World Wide Web Conference
Committee (IW3C2). Distribution of these papers is limited to classroom
use, and personal use by others.
WWW 2008, April 21–25, 2008, Beijing, China.
ACM 978-1-60558-085-2/08/04.

include document clustering [6], ﬁnding replicated Web
collections [9], detecting plagiarism [20], community min-
ing in a social network site [25], collaborative ﬁltering [3]
and discovering large dense graphs [15].

A quantitative way to deﬁning that two objects are near
duplicates is to use a similarity function. The similarity
function measures degree of similarity between two objects
and will return a value in [0, 1]. A higher similarity value
indicates that the objects are more similar. Thus we can
treat pairs of objects with high similarity value as near du-
plicates. A similarity join will ﬁnd all pairs of objects whose
similarities are above a given threshold.

An algorithmic challenge is how to perform the similarity
join in an eﬃcient and scalable way. A na¨ıve algorithm is to
compare every pair of objects, thus bearing a prohibitively
O(n2) time complexity. In view of such high cost, the preva-
lent approach in the past is to solve an approximate version
of the problem, i.e., ﬁnding most of, if not all, similar ob-
jects. Several synopsis-based schemes have been proposed
and widely adopted [5, 7, 10].

A recent trend is to investigate algorithms that compute
the similarity join exactly. Recent advances include inverted
index-based methods [24], preﬁx ﬁltering-based methods [8,
3] and signature-based methods [1]. Among them, the re-
cently proposed All-Pairs algorithm [3] was demonstrated
to be highly eﬃcient and be scalable to tens of millions of
records. Nevertheless, we show that the All-Pairs algorithm,
as well as other preﬁx ﬁltering-based methods, usually gen-
erates a huge amount of candidate pairs, all of which need
to be veriﬁed by the similarity function. Empirical evidence
on several real datasets shows that its candidate size grows
at a fast quadratic rate with the size of the data. Another
inherent problem is that it hinges on the hypothesis that
similar objects are likely to share rare “features” (e.g., rare
words in a collection of documents). This hypothesis might
be weakened for problems with a low similarity threshold or
with a restricted feature domain.

In this paper, we propose new exact similarity join al-
gorithms with application to near duplicate detection. We
propose a positional ﬁltering principle, which exploits the
ordering of tokens in a record and leads to upper bound
estimates of similarity scores. We show that it is comple-
mentary to the existing preﬁx ﬁltering method and can work
on tokens both in the preﬁxes and the suﬃxes. We conduct
an extensive experimental study using several real datasets,
and demonstrate that the proposed algorithms outperform
previous ones. We also show that the new algorithms can
be adapted or combined with existing approaches to pro-

131WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, Chinaduce results with better qualities or improve the runtime
eﬃciency in detecting near duplicate Web pages.

The rest of the paper is organized as follows: Section 2
presents the problem deﬁnition and preliminaries. Section 3
summarizes the existing preﬁx ﬁltering-based approaches.
Sections 4 and 5 give our proposed algorithms that inte-
grate positional ﬁltering method on the preﬁxes and suﬃxes
of the records, respectively. Generalization to other com-
monly used similarity measures is presented in Section 6. We
present our experimental results in Section 7. Related work
is covered in Section 8 and Section 9 concludes the paper.

2. PROBLEM DEFINITION AND PRELIM-

INARIES

2.1 Problem Deﬁnition

We deﬁne a record as a set of tokens taken from a ﬁ-
nite universe U = { w1, w2, . . . , w|U| }. A similarity func-
tion, sim, returns a similarity value in [0, 1] for two records.
Given a collection of records, a similarity function sim(),
and a similarity threshold t, the similarity join problem is
to ﬁnd all pairs of records, hx, yi, such that their similarities
are no smaller than the given threshold t, i.e, sim(x, y) ≥ t.
Consider the task of identifying near duplicate Web pages
for example. Each Web page is parsed, cleaned, and trans-
formed into a multiset of tokens: tokens could be stemmed
words, q-grams, or shingles [5]. Since tokens may occur mul-
tiple times in a record, we will convert a multiset of tokens
into a set of tokens by treating each subsequent occurrence
of the same token as a new token [8]. We can evaluate the
similarity of two Web pages as the Jaccard similarity be-
tween their corresponding sets of tokens.

We denote the size of a record x as |x|, which is the num-
ber of tokens in x. The document frequency of a token is the
number of records that contain the token. We can canon-
icalize a record by sorting its tokens according to a global
ordering O deﬁned on U . A document frequency ordering
Odf arranges tokens in U according to the increasing order
of tokens’ document frequencies. A record x can also be
represented as a |U|-dimensional vector, ~x, where xi = 1 if
wi ∈ x and xi = 0 otherwise.
The choice of the similarity function is highly dependent
on the application domain and thus is out of the scope of
this paper. We do consider several widely used similarity
functions. Consider two records x and y,
• Jaccard similarity is deﬁned as J(x, y) = |x∩y|
|x∪y|
• Cosine similarity is deﬁned as C(x, y) = ~x·~y
k~xk·k~yk
• Overlap similarity is deﬁned as O(x, y) = |x ∩ y|.1

.

A closely related concept is the notion of distance, which
can be evaluated by a distance function. Intuitively, a pair
of records with high similarity score should have a small dis-
tance between them. The following distance functions are
considered in this work.
• Hamming distance between x and y is deﬁned as the size
of their symmetric diﬀerence: H(x, y) = |(x− y)∩(y− x)|.
• Edit distance, also known as Levenshtein distance, mea-
sures the minimum number of edit operations needed to

= Pi xiyi
√|x|·√|y|

1For the ease of illustration, we do no normalize the overlap
similarity to [0, 1].

transform one string into the other, where an edit opera-
tion is an insertion, deletion, or substitution of a single
character.
It can be calculated via dynamic program-
ming [26].

Note that the above similarity and distance functions are
inter-related. We discuss some important relationships in
Section 2.2, and others in Section 6.

In this paper, we will focus on the Jaccard similarity, a
commonly used function for deﬁning similarity between sets.
Extension of our algorithms to handle other similarity or dis-
tance functions appears in Section 6. Therefore, in the rest
of the paper, sim(x, y) by default denotes J(x, y), unless
otherwise stated.

Example 1. Consider two text document, Dx and Dy as:

Dx =“yes as soon as possible”
Dy =“as soon as possible please”

They can be transformed into the following two records

x ={ A, B, C, D, E }
y ={ B, C, D, E, F }

with the following word-to-token mapping table:

Word
Token
Doc. Freq.

yes
A
1

as
B
2

soon

C
2

as1
D
2

possible

please

E
2

F
1

Note that the second “as” has been transformed into a token
“as1” in both records. Records can be canonicalized according
to the document frequency ordering Odf into the following
ordered sequences (denoted as [. . .])

x =[ A, B, C, D, E ]
y =[ F, B, C, D, E ]

The Jaccard similarity of x and y is 4
cosine similarity is

= 0.80.

6 = 0.67, and the

4√5·√5

2.2 Properties of Jaccard Similarity Constraints

Similarity joins essentially evaluate every pair of records
against a similarity constraint of J(x, y) ≥ t. This con-
straint can be transformed into several equivalent forms on
the overlap similarity or the Hamming distance as follows:

J(x, y) ≥ t ⇐⇒ O(x, y) ≥ α =
O(x, y) ≥ α ⇐⇒ H(x, y) ≤ |x| + |y| − 2α

1 + t · (|x| + |y|)

t

(1)

(2)

.

We can also infer the following constraint on the relative
sizes of a pair of records that meets a Jaccard constraint.

J(x, y) ≥ t =⇒ t · |x| ≤ |y|

(3)

3. PREFIX FILTERING BASED METHODS
A na¨ıve algorithm to compute t-similarity join result is to
enumerate and compare every pair of records. This method
is obviously prohibitively expensive for large datasets, as the
total number of comparisons is O(n2).

Eﬃcient algorithms exist by converting the Jaccard sim-
ilarity constraint into an equivalent overlap constraint due
to Equation (1). An eﬃcient way to ﬁnd records that over-
lap with a given record is to use inverted indices [2]. An

132WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, Chinainverted index maps a token w to a list of identiﬁers of
records that contain w. After inverted indices for all to-
kens in the record set are built, we can scan each record x,
probe the indices using every token in x, and obtain a set of
candidates; merging these candidates together gives us their
actual overlap with the current record x; ﬁnal results can be
extracted by removing records whose overlap with x is less
than ⌈ t
1+t · (|x| + |y|)⌉ (Equation (1)). The main problem
of this approach is that the inverted lists of some tokens,
often known as “stop words”, can be very long. These long
inverted lists incur signiﬁcant overhead for building and ac-
cessing them. In addition, computing the actual overlap by
probing indices essentially requires memorizing all pairs of
records that share at least one token, a number that is of-
ten prohibitively large. Several existing work takes this ap-
proach with optimization by pushing the overlap constraint
into the similarity value calculation phase. For example, [24]
employs sequential access on short inverted lists but switches
to binary search on the α − 1 longest inverted lists.
Another approach is based on the intuition that if two
canonicalized records are similar, some fragments of them
should overlap with each other, as otherwise the two records
won’t have enough overlap. This intuition can be formally
captured by the preﬁx-ﬁltering principle [8, Lemma 1] re-
phrased below.

Lemma 1

(Prefix Filtering Principle). Consider an
ordering O of the token universe U and a set of records, each
with tokens sorted in the order of O. Let the p-preﬁx of a
record x be the ﬁrst p tokens of x. If O(x, y) ≥ α, then the
(|x|− α + 1)-preﬁx of x and the (|y|− α + 1)-preﬁx of y must
share at least one token.

Since preﬁx ﬁltering is a necessary but not suﬃcient condi-
tion for the corresponding overlap constraint, we can design
an algorithm accordingly as: we ﬁrst build inverted indices
on tokens that appear in the preﬁx of each record in an in-
dexing phase. We then generate a set of candidate pairs by
merging record identiﬁers returned by probing the inverted
indices for tokens in the preﬁx of each record in a candi-
date generation phase. The candidate pairs are those
that have the potential of meeting the similarity threshold
and are guaranteed to be a superset of the ﬁnal answer due
to the preﬁx ﬁltering principle. Finally, in a veriﬁcation
phase, we evaluate the similarity of each candidate pair and
add it to the ﬁnal result if it meets the similarity threshold.
A subtle technical issue is that the preﬁx of a record de-
pends on the sizes of the other record to be compared and
thus cannot be determined before hand. The solution is
to index the longest possible preﬁxes for a record x.
It
can be shown that we only need to index a preﬁx of length
|x|−⌈t·|x|⌉+1 for every record x to ensure the preﬁx ﬁltering-
based method does not miss any similarity join result.
The major beneﬁt of this approach is that only smaller
inverted indices need to be built and accessed (by a approx-
imately (1 − t) reduction). Of course, if the ﬁltering is not
eﬀective and a large number of candidates are generated, the
eﬃciency of this approach might be diluted. We later show
that this is indeed the case and propose additional ﬁltering
methods to alleviate this problem.

There are several enhancements on the basic preﬁx-ﬁltering
scheme. [8] considers implementing the preﬁx ﬁltering method
on top of a commercial database system, while [3] further

improves the method by utilizing several other ﬁltering tech-
niques in candidate generation phase and veriﬁcation phase.

Example 2. Consider a collection of four canonicalized re-
cords based on the document frequency ordering, and the
Jaccard similarity threshold of t = 0.8:

w = [ C, D, F ]
z = [ G, A, B, E, F ]

y = [ A, B, C, D, E ]

x = [ B, C, D, E, F ]

Preﬁx length of each record u is calculated as |u|−⌈t·|u|⌉+1.
Tokens in the preﬁxes are underlined and are indexed. For
example, the inverted list for token C is [ w, x ].

Consider the record x. To generate its candidates, we need
to pair x with all records returned by inverted lists of to-
kens B and C. Hence, candidate pairs formed for x are
{hx, yi,hx, wi}.
The All-Pairs algorithm [3] also includes several other ﬁl-
tering techniques to further reduce the candidate size. For
example, it won’t consider hx, wi as a candidate pair, as
|w| < 4 and can be pruned due to Equation (3). This ﬁl-
tering method is known as size ﬁltering [1].

4. POSITIONAL FILTERING

We now describe our solution to solve the exact similar-
ity join problem. We ﬁrst introduce the positional ﬁltering,
and then propose a new algorithm, ppjoin, that combines
positional ﬁltering with the preﬁx ﬁltering-based algorithm.

4.1 Positional Filtering

Algorithm 1: ppjoin (R, t)

Input

: R is a multiset

of records sorted by the increasing order of their
sizes; each record has been canonicalized by a
global ordering O; a Jaccard similarity threshold t
Output : All pairs of records hx, yi, such that sim(x, y) ≥ t

S ← ∅;
Ii ← ∅ (1 ≤ i ≤ |U |);
for each x ∈ R do

A ← empty map from record id to int;
p ← |x| − ⌈t · |x|⌉ + 1;
for i = 1 to p do

w ← x[i];
for each (y, j) ∈ Iw such
that |y| ≥ t · |x| do

/* size filtering on |y| */

1+t (|x| + |y|)⌉;

α ← ⌈ t
ubound ← 1 + min(|x| − i, |y| − j);
if A[y] + ubound ≥ α then

A[y] ← A[y] + 1;

else

A[y] ← 0;

/* prune y */;

Iw ← Iw ∪ {(x, i)};
/* index the current prefix */;

Verify(x, A, α);

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

return S

Although a global ordering is a prerequisite of preﬁx ﬁlter-
ing, no existing algorithm fully exploits it when generating
the candidate pairs. We observe that positional informa-
tion can be utilized in several ways to further reduce the

133WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, Chinacandidate size. By positional information, we mean the po-
sition a token in a canonicalized record (starting from 1).
We illustrate the observation in the following example.

Example 3. Consider x and y from the previous example

and the same similarity threshold t = 0.8

y = [ A, B, C, D, E ]

x = [ B, C, D, E, F ]

The pair, hx, yi, does not meet the equivalent overlap con-
straint of O(x, y) ≥ 5, hence is not in the ﬁnal result. How-
ever, since they share a common token, B, in their preﬁxes,
preﬁx ﬁltering-based methods will select y as a candidate for
x.

However, if we look at the positions of the common token
B in the preﬁxes of x and y, we can obtain an estimate of
the maximum possible overlap as the sum of current over-
lap amount and the minimum number of unseen tokens in
x and y, i.e., 1 + min(3, 4) = 4. Since this upper bound of
the overlap is already smaller than the threshold of 5, we can
safely prune hx, yi.

We now formally state the positional ﬁltering principle in

Lemma 2.

Lemma 2

(Positional Filtering Principle). Consi-
der an ordering O of the token universe U and a set of
records, each with tokens sorted in the order of O. Let to-
ken w = x[i], w partitions the record into the left parti-
tion xl(w) = x[1 . . (i − 1)] and the right partition xr(w) =
x[i . .|x|]. If O(x, y) ≥ α, then for every token w ∈ x ∩ y,
O(xl(w), yl(w)) + min(|xr(w)|,|yr(w)|) ≥ α.
4.2 Positional Filtering-Based Algorithm

A natural idea to utilize the positional ﬁltering principle
is to combine it with the existing preﬁx ﬁltering method,
which already keeps tracks of the current overlap of candi-
date pairs and thus gives us O(xl(w), yl(w)).

Algorithm 1 describes our ppjoin algorithm, an extension
to the All-Pairs algorithm [3], to combine positional ﬁltering
and preﬁx-ﬁltering. Like the All-Pairs algorithm, ppjoin al-
gorithm takes as input a collection of canonicalized record
already sorted in the ascending ordered of their sizes.
It
then sequentially scans each record x, ﬁnds candidates that
intersect x’s preﬁx (x[1 . . p], Line 5) and accumulates the
overlap in a hash map A (Line 12). The generated can-
didates are further veriﬁed against the similarity threshold
(Line 16) to return the correct join result. Note that the
internal threshold used in the algorithm is an equivalent
overlap threshold α computed from the given Jaccard sim-
ilarity threshold t. The document frequency ordering Odf
is often used to canonicalize the records. It favors rare to-
kens in the preﬁxes and hence results in a small candidate
size and fast execution speed. Readers are referred to [3] for
further details on the All-Pairs algorithm.

Now we will elaborate on several novel aspects of our ex-
tension: (i) the inverted indices used (Algorithm 1, Line
15), and (ii) the use of positional ﬁltering (Algorithm 1,
Lines 9–14), and (iii) the optimized veriﬁcation algorithm
(Algorithm 2).

In Line 15, we index both tokens and their positions for
tokens in the preﬁxes so that our positional ﬁltering can uti-
lize the positional information. In Lines 9–14, we compute

an upper bound of the overlap between x and y, and only ad-
mit this pair as a candidate pair if its upper bound is no less
than the threshold α. Speciﬁcally, α is computed according
to Equation (1); ubound is an upper bound of the overlap
between right partitions of x and y with respect to the cur-
rent token w, which is derived from the number of unseen
tokens in x and y with the help of the positional information
in the index Iw; A[y] is the current overlap for left partitions
of x and y. It is then obvious that if A[y]+ubound is smaller
than α, we can prune the current candidate y (Line 14).

Algorithm 2: Verify(x, A, α)

Input

: px is the

preﬁx length of x and py is the preﬁx length of y

for each y such that A[y] > 0 do

wx ← the last token in the preﬁx of x;
wy ← the last token in the preﬁx of y;
O ← A[y];
if wx < wy then

ubound ← A[y] + |x| − px;
if ubound ≥ α then

O ← O + ˛

˛x[(px + 1) . . |x|] T y[(A[y] + 1) . . |y|]˛
˛;

else

ubound ← A[y] + |y| − py;
if ubound ≥ α then

O ← O + ˛

˛x[(A[y] + 1) . . |x|] T y[(py + 1) . . |y|]˛
˛;

if O ≥ α then

S ← S ∪ (x, y);

1

2

3

4

5

6

7

8

9

10

11

12

13

14

Algorithm 2 is designed to verify whether the actual over-
lap between x and candidates y in the current candidate set,
{ y | A[y] > 0}, meets the threshold α. Notice that we’ve
already accumulated in A[y] the amount of overlaps that
occur in the preﬁxes of x and y. An optimization is to ﬁrst
compare the last token in both preﬁxes, and only the suﬃx
of the record with the smaller token (denoted the record
as u) needs to be intersected with the entire other record
(denoted as v). This is because the preﬁx of u consists of
tokens that are smaller than wu (the last token in u’s preﬁx)
in the global ordering and v’s suﬃx consists of tokens that
are larger than wv. Since wu ≺ wv, u’s preﬁx won’t intersect
with v’s suﬃx. In fact, the workload can still be reduced:
we can skip the ﬁrst A[y] number of tokens in v since at least
A[y] tokens have overlapped with u’s preﬁx and hence won’t
contribute to any overlap with u’s suﬃx. The above method
is implemented through Lines 4, 5, 8, and 12 in Algorithm 2.
This optimization in calculating the actual overlap immedi-
ately gives rise to a pruning method. We can estimate the
upper bound of the overlap as the length of the suﬃx of u
(which is either |x| − px or |y| − py). Lines 6 and 10 in the
algorithm perform the estimation and the subsequent lines
test whether the upper bound will meet the threshold α and
prune away unpromising candidate pairs directly.

Experimental results show that utilizing positional infor-
mation can achieve substantial pruning eﬀects on real data-
sets. For example, we show the sizes of the candidates gen-
erated by ppjoin algorithm and All-Pairs algorithm for the
DBLP dataset in Table 1.

4.3 Minimizing Tokens to be Indexed

The following Lemma allows us to further reduce the num-

ber of tokens to be indexed and hence accessed.

134WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, ChinaTable 1: Candidate Size (DBLP, Jaccard)

t

All-Pairs

ppjoin

ppjoin+

0.95
0.90
0.80

199,268
1,857,987
16,98,3319

176,971
657,200
3,303,232

32,397
36,318
63,265

e
z
S
 
e
t
a
d
d
n
a
C

i

i

 
f
o
 
t
o
o
R
 
e
r
a
u
q
S

DBLP, Jaccard Similarity, t = 0.90

Result
PPJoin+
PPJoin
All-Pairs

 1400

 1200

 1000

 800

 600

 400

 200

 0

 0.2

 0.4

 0.6

 0.8

 1

Scale Factor

Figure 1: Quadratic Growth of Candidate Size

Lemma 3. Given a record x, we only need to index its
1+t · |x|⌉ + 1)-preﬁx for Algorithm 1 to produce cor-

(|x| − ⌈ 2t
rect join result.

This optimization requires us to change Line 15 in Algo-
rithm 1 such that it only index the current token w if the
current token position i is no larger than |x|−⌈ 2t
1+t ·|x|⌉ + 1.
Note that the length of the preﬁx used for probing into the
indices remains the same.

5. SUFFIX FILTERING

In this section, we ﬁrst motivate the need to looking for
further ﬁltering method, and then introduce a divide-and-
conquer based suﬃx ﬁltering method, which is a generaliza-
tion of the positional ﬁltering to the suﬃxes of the records.

5.1 Quadratic Growth of the Candidate Size

Let’s consider the asymptotic behavior of the size of the
candidate size generated by the preﬁx ﬁltering-base meth-
ods. The candidate size is O(n2) in the worst case. Our
empirical evidence on several real datasets suggests that the
growth is indeed quadratic. For example, we show the square
root of query result size and candidate sizes of the All-Pairs
algorithm and our ppjoin algorithm in Figure 1. It can be ob-
served that while positional ﬁltering helps to further reduce
the size of the candidates, it is still growing quadratically
(albeit with a much slower rate than All-Pairs).

5.2 Generaling Positional Filtering to Sufﬁxes

Given the empirical observation about the quadratic growth

rate of the candidate size, it is desirable to ﬁnd additional
pruning method in order to tackle really large datasets.

Our goal is to develop additional ﬁltering method that
prunes candidates that survive the preﬁx and positional ﬁl-
tering. Our basic idea is to generalize the positional ﬁl-
tering principle to work on the suﬃxes of candidate pairs.
However, the challenge is that the suﬃxes of records are
not indexed nor their partial overlap has been calculated.
Therefore, we face the following two technical issues: (i) how
to establish an upper bound in the absence of indices or par-
tial overlap results? (ii) how to ﬁnd the position of a token
without tokens being indexed?

We solve the ﬁrst issue by converting an overlap constraint
to an equivalent Hamming distance constraint, according to
Equation (2). We then lower bound the Hamming distance
by partitioning the suﬃxes in an coordinated way. We de-
note the suﬃx of a record x as xs. Consider a pair of records,
hx, yi, that meets the Jaccard similarity threshold t, and
without loss of generality, |y| ≤ |x|. Since their overlap in
their preﬁxes is at most the minimum length of the preﬁxes,
we can derive the following upper bound in terms of the
Hamming distance of their suﬃxes.

H(xs, ys) ≤ Hmax = 2|x| − 2⌈

t

1 + t · (|x| + |y|)⌉

− (⌈t · |x|⌉ − ⌈t · |y|⌉)

(4)

In order to check whether H(xs, ys) exceeds the maximum
allowable value, we provide an estimate of the lower bound
of H(xs, ys) below. First we choose an arbitrary token w
from ys, and divide ys into two partitions: the left parti-
tion yl and the right partition yr. The criterion for the
partitioning is that the left partition contains all the to-
kens in ys that precede w in the global ordering and the
right partition contains w (if any) and tokens in ys that
succeed w in the global ordering. Similarly, we divide xs
into xl and xr using w too (even though w might not oc-
cur in x). Since xl (xr) shares no common token with yr
(yl), H(xs, ys) = H(xl, yl) + H(xr, yr). The lower bound
of H(xl, yl) can be estimated as the diﬀerence between |xl|
and |yl|, and similarly for the right partitions. Therefore,

H(xs, ys) ≥ abs(|xl| − |yl|) + abs(|xr| − |yr|)

Finally, we can safely prune away candidates whose lower
bound Hamming distance is already larger than the allow-
able threshold Hmax.

We can generalize the above method to more than one
probing token and repeat the test several times indepen-
dently to improve the ﬁltering rate. However, we will show
that if the probings are arranged in a more coordinated way,
results from former probings can be taken into account and
make subsequent probings more eﬀective. We illustrate this
idea in the example below.

Example 4. Consider the following two suﬃxes of length
6. Cells marked with “?” indicate that we have not accessed
those cells and do not know their contents yet.

pos

xs

1

2

3

4

5

6

? D ?

? F ?

xll

xlr

xr

ys

?

? D F ?

?

yll

ylr

yr

Assume the allowable Hamming distance is 2. If we probe
the 4th token in ys (“F ”), we have the following two par-
titions of ys: yl = ys[1 . . 3] and yr = ys[4 . . 6]. Assum-
ing a magical “partition” function, we can partition xs into
xs[1 . . 4] and xs[5 . . 6] using F . The lower bound of Ham-
ming distance is abs(3 − 4) + abs(3 − 2) = 2.
If we perform the same test independently, say, using the
3rd token of ys (“D”), the lower bound of Hamming distance
is still 2. Therefore, hx, yi is not pruned away.
However, we can actually utilize the previous test result.
The result of the second probing can be viewed as a recursive

135WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, Chinapartitioning of xl and yl into xll, xlr, yll, and ylr. Obviously
the total absolute diﬀerences of the sizes of the three parti-
tions from two suﬃxes is an lower bound of their Hamming
distance, which is

abs(|xll| − |yll|) + abs(|xlr| − |ylr|) + abs(|xr| − |yr|)

= abs(1 − 2) + abs(3 − 1) + abs(2 − 3) = 4

Therefore, hx, yi can be safely pruned.

Algorithm 3: SuﬃxFilter(x, y, Hmax, d)

Input

: Two set of tokens x and y,

the maximum allowable hamming distance Hmax
between x and y, and current recursive depth d

Output : The lower

bound of hamming distance between x and y

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

2

/* always divisible */;

if d > MAXDEPTH then return abs(|x| − |y|) ;
mid ← ⌈ |y|2 ⌉; w ← y[mid];
o ← Hmax−abs(|x|−|y|)
if |x| < |y| then ol ← 1, or ← 0 else ol ← 0, or ← 1;
(yl, yr, f, diﬀ) ← Partition(y, w, mid, mid);
(xl, xr, f, diﬀ) ← Partition(x, w, mid −
o − abs(|x| − |y|) · ol, mid + o + abs(|x| − |y|) · or);
if f = 0 then

;

return Hmax + 1

H ← abs(|xl| − |yl|) + abs(|xr| − |yr|) + diﬀ;
if H > Hmax then

return H

else

Hl ← SuﬃxFilter(xl, yl, Hmax−abs(|xr|−|yr|)−diﬀ, d+1) ;
H ← Hl + abs(|xr| − |yr|) + diﬀ;
if H ≤ Hmax then

Hr ← SuﬃxFilter(xr, yr, Hmax − Hl − diﬀ, d + 1) ;
return Hl + Hr + diﬀ

else

return H

The algorithm we designed to utilize above observations
is a divide-and-conquer one (Algorithm 3). First, the to-
ken in the middle of y is chosen, and x and y are parti-
tioned into two parts respectively. The lower bounds of
Hamming distance on both left and right partitions are com-
puted and summed up to judge if the overall hamming dis-
tance is within the allowable threshold (Lines 9–10). Then
we call the SuﬃxFilter function recursively ﬁrst on the left
and then on the right partition (Lines 13–19). Probing re-
sults in the previous tests are used to help reduce the max-
imum allowable Hamming distance (Line 16) and to break
the recursion if the Hamming distance lower bound has ex-
ceeded the threshold Hmax (Lines 14–15 and 19). Finally,
only those pairs such that their lower bounding Hamming
distance meets the threshold will be considered as candidate
pairs. We also use a parameter MAXDEPTH to limit the max-
imum level of recursion (Line 1); this is aimed to strike a
balance between ﬁltering power and ﬁltering overhead.

The second technical issue is how to perform the partition
eﬃciently, especially for xs. A straight-forward approach is
to perform binary search on the whole suﬃx, an idea which
was also adopted by the ProbeCount algorithm [24]. The
partitioning cost will be O(log |xs|). Instead, we found that
the search only needs to be performed in a much smaller area
approximately centered around the position of the partition-
ing token w in y, due to the Hamming distance constraint.
We illustrate this using the following example.

Example 5. Continuing the previous example, consider par-

titioning xs according to the probing token F . The only pos-
sible area where F (for simplicity, assume F exists in xs)
can occur is within xs[3 . . 5], as otherwise, the Hamming
distance between xs and ys will exceed 2. We only need to
perform binary search within xs[3 . . 5] to ﬁnd the ﬁrst token
that is no smaller than F .

The above method can be generalized to the general case
where xs and ys have diﬀerent lengths. This is described
in Lines 4–6 in Algorithm 3. The size of the search range
is bounded by Hmax, and is likely to be smaller within the
subsequent recursive calls.

Algorithm 4 implements the partitioning process using a
partitioning token w. One thing that deviates from Exam-
ple 4 is that the right partition now does not include the
partitioning token, if any (Line 7). This is mainly to sim-
plify the pseudocode while still ensuring a tight bound on the
Hamming distance when the token w cannot be found in xs.

Algorithm 4: Partition(s, w, l, r)
: A set of tokens s, a token

Input

w, left and right bounds of searching range l, r

Output : Two subsets

of s: sl and sr, a ﬂag f indicating whether w
is in the searching range, and a ﬂag diﬀ indicating
whether the probing token w is not found in y

sl ← ∅; sr ← ∅;
if s[l] > w or s[r] < w then

return (∅, ∅, 0, 1)

p ← binary search for the position of the ﬁrst token in s that
is no smaller than w in the global ordering within s[l . . r];
sl ← s[1 . . p − 1];
if s[p] = w then

/* skip the token w */;

sr ← s[(p + 1) . . |s|];
diﬀ ← 0;

else

sr ← s[p . . |s|];
diﬀ ← 1;

return (sl, sr, 1, diﬀ)

Algorithm 5: Replacement of Line 12 in Algorithm 1

if A[y] = 0 then

Hmax ← |x| + |y| − 2 · ⌈ t
H ← SuﬃxFilter(x[(i + 1) . . |x|], y[(j + 1) . . |y|], Hmax, 1);
if H ≤ Hmax then

1+t · (|x| + |y|)⌉ − (i + j − 2);

A[y] ← A[y] + 1;

else

A[y] ← −∞;

/* avoid considering y again */;

1

2

3

4

5

6

7

8

9

10

11

12

1

2

3

4

5

6

7

Finally, we can integrate the suﬃx ﬁltering into the ppjoin
algorithm and we name the new algorithm ppjoin+. To that
end, we only need to replace the original Line 12 in Algo-
rithm 1 with the lines shown in Algorithm 5. We choose to
perform suﬃx ﬁltering only once for each candidate pair on
the ﬁrst occasion that it is formed. This is because suﬃx
ﬁltering probes the unindexed part of the records, and is
relative expensive to carry out. An additional optimization
opportunity enabled by this design is that we can further
reduce the initial allowable Hamming distance threshold to
|x| +|y|− 2⌈ t
1+t · (|x| +|y|)⌉− (i + j − 2), where i and j stand
for the positions of the ﬁrst common token w in x and y,

136WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, Chinarespectively (Line 2). Intuitively, this improvement is due
to the fact that x[1 . . (i − 1)] ∩ y[1 . . (j − 1)] = ∅ since the
current token is the ﬁrst common token between them.
The suﬃx ﬁltering employed by the ppjoin+ algorithm is
orthogonal and complementary to the preﬁx and positional
ﬁltering, and thus helps further reduce the candidate size.
Its eﬀect on the DBLP dataset can be seen in Table 1 and
Figure 1.

6. EXTENSION TO OTHER SIMILARITY

MEASURES

In this section, we brieﬂy comment on necessary modiﬁca-
tions to adapt both ppjoin and ppjoin+ algorithms to other
commonly used similarity measures. The major changes are
related to the length of the preﬁxes used for indexing (Line
15, Algorithm 1) and used for probing (Line 5, Algorithm 1),
the threshold used by size ﬁltering (Line 8, Algorithm 1) and
positional ﬁltering (Line 9, Algorithm 1), and the Hamming
distance threshold calculation (Line 2, Algorithm 5).
Overlap Similarity O(x, y) ≥ α is inherently supported
in our algorithms. The preﬁx length for a record x will be
x − α + 1. The size ﬁltering threshold is α. It can be shown
that positional ﬁltering will not help pruning candidates, but
suﬃx ﬁltering is still useful. The Hamming distance thresh-
old, Hmax, for suﬃx ﬁltering will be |x|+|y|−2α−(i+j−2).
Edit Distance Edit distance is a common distance mea-
sure for strings. An edit distance constraint can be con-
verted into weaker constraints on the overlap between the q-
gram sets of the two strings. Speciﬁcally, let |u| be the length
of the string u, a necessary condition for two strings to have
less than δ edit distance is that their corresponding q-gram
sets must have overlap no less than α = (max(|u|,|v|) + q −
1) − qδ [17].
The preﬁx length of a record x (which is now a set of
q-grams) is qδ + 1. The size ﬁltering threshold is |x|− δ. Po-
sitional ﬁltering will use an overlap threshold α = |x| − qδ.
The Hamming distance threshold, Hmax, for suﬃx ﬁltering
will be |y| − |x| + 2qδ − (i + j − 2).
Cosine Similarity We can convert a constraint on cosine
similarity to an equivalent overlap constraint as:

C(x, y) ≥ t ⇐⇒ O(x, y) ≥ lt · p|x| · |y|m

The length of the preﬁx for a record x is |x|−⌈t2 ·|x|⌉ + 1,
yet the length of the tokens to be indexed can be optimized
to |x| − ⌈t · |x|⌉ + 1. The size ﬁltering threshold is ⌈t2 ·
|x|⌉.2 Positional ﬁltering will use an overlap threshold α =
lt · p|x| · |y|m. The Hamming distance threshold, Hmax, for
suﬃx ﬁltering will be |x| +|y|− 2lt · p|x| · |y|m− (i + j − 2).
7. EXPERIMENTAL EVALUATION

In this section, we present our experimental results.

7.1 Experiment Setup

We implemented and used the following algorithms in the

experiment.

All-Pairs is an eﬃcient preﬁx ﬁltering-based algorithm ca-

pable of scaling up to tens of millions of records [3].

2These are the same bounds obtained in [3].

Dataset

n

avg len

|U |

DBLP
873,524
DBLP-3GRAM 873,524
ENRON
517,386
TREC-4GRAM 348,566
TREC-Shingle
348,566

14.0
102.5
142.4
866.9
32.0

566,518
113,169
1,180,186
1,701,746
9,788,436

Figure 2: Statistics of Datasets

ppjoin, ppjoin+ are our proposed algorithms. ppjoin inte-
grates positional ﬁltering into the All-Pairs algorithm, while
ppjoin+ further employes suﬃx ﬁltering.

All algorithms were implemented in C++. To make fair
comparisons, all algorithms use Google’s dense_hash_map
class for accumulating overlap values for candidates, as sug-
gested in [3]. All-Pairs has been shown to consistently out-
perform alternative algorithms such as ProbeCount-Sort [24],
PartEnum [1] and LSH [16], and therefore we didn’t consider
them [3].

All experiments were performed on a PC with Pentium D
3.00GHz CPU and 2GB RAM. The operating system is De-
bian 4.1. The algorithms were compiled using GCC 4.1.2
with -O3 ﬂag.

We measured both the size of the candidate pairs and the

running time for all the experiments.

Our experiments covered the following similarity mea-

sures: Jaccard similarity, and Cosine similarity.

We used several publicly available real datasets in the ex-
periment. They were selected to cover a wide spectrum of
diﬀerent characteristics (See Figure 2).

DBLP This dataset is a snapshot of the bibliography records
from the DBLP Web site. It contains almost 0.9M records;
each record is a concatenation of author name(s) and the
title of a publication. We tokenized each record using
white spaces and punctuations. The same DBLP dataset
(with smaller size) was also used in previous studies [1, 3].
DBLP-3GRAM This is the same DBLP dataset, but fur-
ther tokenized into 3-grams. Speciﬁcally, tokens in a record
are concatenated with a single whitespace, and then every
3 consecutive letters is extract as a 3-gram.

ENRON This dataset is from the Enron email collection3.
It contains about 0.5M emails from about 150 users, mostly
senior management of Enron. We tokenize the email title
and body into words using the same tokenization proce-
dure as DBLP.

TREC-4GRAM This dataset is from TREC-9 Filtering
Track Collections.4 It contains 0.35M references from the
MEDLINE database. We extracted author, title, and ab-
stract ﬁelds to from records. Records are subsequently
tokenized as in DBLP.

TREC-Shingle We applied Broder’s shingling method [5]
on TREC-4GRAM to generate 32 shingles of 4 bytes per
record, using min-wise independent permutations. TREC-
4GRAM and TREC-Shingle are dedicated to experiment
on near duplicate Web page detection (Section 7.5).

Some important statistics about the datasets are listed in

Figure 2.

3Available at http://www.cs.cmu.edu/~enron/
4Available at http://trec.nist.gov/data/t9_filtering.html.

137WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, ChinaDBLP

Enron

DBLP-3GRAM

i

e
z
S
 
e
t
a
d
d
n
a
C

i

108

107

106

105

104

 0.8

Result
PPJoin+
PPJoin
All-Pairs

 0.85

 0.9

 0.95

Jaccard Similarity

i

e
z
S
 
e
t
a
d
d
n
a
C

i

 1e+09

 1e+08

 1e+07

 1e+06

 0.8

Result
PPJoin+
PPJoin
All-Pairs

 0.85

 0.9

 0.95

Jaccard Similarity

i

e
z
S
 
e
t
a
d
d
n
a
C

i

1012
1011
1010
109
108
107
106
105
104

 0.8

Result
PPJoin+
PPJoin
All-Pairs

 0.85

 0.9

 0.95

Jaccard Similarity

(a) Jaccard, DBLP, Candidate Size

(b) Jaccard, Enron, Candidate Size

(c) Jaccard, DBLP-3GRAM, Candidate Size

DBLP

Enron

DBLP-3GRAM

)
s
d
n
o
c
e
s
(
 
e
m
T

i

 40

 35

 30

 25

 20

 15

 10

 5

 0

PPJoin+
PPJoin
All-Pairs

)
s
d
n
o
c
e
s
(
 
e
m
T

i

 350

 300

 250

 200

 150

 100

 50

 0

PPJoin+
PPJoin
All-Pairs

 0.8

 0.85

 0.9

 0.95

 0.8

 0.85

 0.9

 0.95

Jaccard Similarity

Jaccard Similarity

 10000

 1000

 100

)
s
d
n
o
c
e
S

(
 
e
m
T

i

 10

 0.8

PPJoin+
PPJoin
All-Pairs

 0.85

 0.9

 0.95

Jaccard Similarity

(d) Jaccard, DBLP, Time

(e) Jaccard, Enron, Time

(f) Jaccard, DBLP-3GRAM, Time

DBLP

Enron

DBLP

i

e
z
S
e

 

t

i

a
d
d
n
a
C

)
s
d
n
o
c
e
s
(
 

e
m
T

i

109

108

107

106

105

104

 0.8

Result
PPJoin+
PPJoin
All-Pairs

i

e
z
S
e

 

t

i

a
d
d
n
a
C

 0.85

 0.9

 0.95

Cosine Similarity

 1e+09

 1e+08

 1e+07

 1e+06

 0.8

Result
PPJoin+
PPJoin
All-Pairs

 0.85

 0.9

 0.95

Cosine Similarity

 100

 80

 60

 40

 20

)
s
d
n
o
c
e
s
(
 

e
m
T

i

 0

 0.8

PPJoin+
PPJoin
All-Pairs

 0.85

 0.9

 0.95

Cosine Similarity

(g) Cosine, DBLP, Candidate Size

(h) Cosine, Enron, Candidate Size

(i) Cosine, DBLP, Time

Enron

PPJoin+
PPJoin
All-Pairs

 800

 700

 600

 500

 400

 300

 200

 100

 0

 0.8

 0.85

 0.9

 0.95

)
s
d
n
o
c
e
S

(
 

e
m
T

i

 
f

o

 
t

 

o
o
R
e
r
a
u
q
S

 4

 3.5

 3

 2.5

 2

 1.5

 1

 0.5

DBLP, Jaccard Similarity, t = 0.90

PPJoin+
PPJoin
All-Pairs

 0.2

 0.4

 0.6

 0.8

 1

)
s
d
n
o
c
e
S

(
 

e
m
T

i

 
f

o

 
t

 

o
o
R
e
r
a
u
q
S

 11
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1

Enron, Cosine Similarity, t = 0.90

PPJoin+
PPJoin
All-Pairs

 0.2

 0.4

 0.6

 0.8

 1

Cosine Similarity

Scale Factor

Scale Factor

(j) Cosine, Enron, Time

(k) Jaccard, DBLP, Time

(l) Cosine, Enron, Time

Figure 3: Experimental Results

7.2 Jaccard Similarity

Candidate Size
Figures 3(a) to 3(c) show the sizes of
candidate pairs generated by the algorithms and the size of
the join result on the DBLP, Enron, and DBLP-3GRAM
datasets, with varying similarity thresholds from 0.80 to
0.95. Note that y-axis is in logarithm scale.

Several observations can be made:

ilarity threshold decreases.

• The size of the join result grows modestly when the sim-
• All algorithms generate more candidate pairs with the de-
crease of the similarity threshold. Obviously, the candi-
date size of All-Pairs grows the fastest. ppjoin has a decent
reduction on the candidate size of All-Pairs, as the posi-
tional ﬁltering prunes many candidates. ppjoin+ produces
the fewest candidates thanks to the additional suﬃx ﬁl-
tering.

• The candidate sizes of ppjoin+ are usually in the same
order of magnitude as the sizes of the join result for a
wide range of similarity thresholds. The only outlier is

the Enron dataset, where ppjoin+ only produces modestly
smaller candidate set than ppjoin. There are at least two
reasons: (a) the average record size of the enron dataset
is large; this allows for a larger initial Hamming distance
threshold Hmax for the suﬃx ﬁltering. Yet we only use
MAXDEPTH = 2 (for eﬃciency reasons; also see the Enron’s
true positive rate below). (b) Unlike other datasets used,
an extraordinary high percentage of candidates of ppjoin
are join results. The ratio of sizes of query result over can-
didate size by ppjoin algorithm is 38.1%, 4.9%, and 0.03%
for Enron, DBLP, and DBLP-3GRAM, respectively.
In
other words, ppjoin has already removed the majority of
false positive candidate pairs on Enron and hence it is
hard for suﬃx ﬁltering to further reduce the candidate set.

Running Time
Figures 3(d) to 3(f) show the running
time of all algorithms on the three datasets with varying
Jaccard similarity thresholds.

In all the settings, ppjoin+ is the most eﬃcent algorithm,
followed by ppjoin. Both algorithms outperform the All-Pairs
algorithm. The general trend is that the speed-up increases

138WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, Chinawith the decrease of the similarity threshold. This is be-
cause (i) index construction, probing, and other overheads
are more noticeable with a high similarity threshold, as the
result is small and easy to compute. (ii) inverted lists in
the indices are longer for a lower similarity threshold; this
increases the candidate size which in turn slows down the All-
Pairs algorithm as it does not have any other additional ﬁl-
tering mechanism. In contrast, many candidates are quickly
discarded by failing the positional or suﬃx ﬁltering used in
ppjoin and ppjoin+ algorithms.

The speed-up that our algorithms can achieve against the
All-Pairs algorithm is also dependent on the dataset. At
the 0.8 threshold, ppjoin can achieve up to 3x speed-up
against All-Pairs on both Enron and DBLP-3GRAM, and
up to 2x speed-up on DBLP. At the same threshold, ppjoin+
can achieve 5x speed-up on DBLP-3GRAM, 4x speed-up on
Enron, and 2.6x speed-up on DBLP. This trend can be ex-
plained as All-Pairs algorithm is not good at dealing with
long records and/or a small token domain.

The performance between ppjoin and ppjoin+ is most sub-
stantial on DBLP-3GRAM, where ﬁltering on the suﬃxes
helps to improve the performance drastically. The reason
why ppjoin+ has only modest performance gain over ppjoin
on Enron is because 38% of the candidates are ﬁnal results,
hence the additional ﬁltering employed in ppjoin+ won’t con-
tribute to much runtime reduction. The diﬀerence of the
two is also moderate on DBLP. This is mainly because the
average size of DBLP records is only 14 and even a brute-
force veriﬁcation using the entire suﬃx is likely to be fast,
especially in modern computer architectures.

7.3 Cosine Similarity

We ran all three algorithms on the DBLP and ENRON
datasets using the cosine similarity function, and plot the
candidate sizes in Figures 3(g) to 3(h) and running times in
Figures 3(i) to 3(j). For both metrics, the general trends are
similar to those using Jaccard similarity. A major diﬀerence
is that all algorithms now run slower for the same similarity
threshold, mainly because a cosine similarity constraint is
inherently looser than the corresponding Jaccard similarity
constraint. At the 0.8 threshold, the speed-ups of the ppjoin
and ppjoin+ algorithm is 2x and 3x on DBLP, respectively;
on Enron, the speed-ups are 1.6 and 1.7, respectively.

7.4 Varying Data Sizes

We performed the similarity join using Jaccard similar-
ity on subsets of the DBLP dataset and measured running
times.5 We randomly sampled about 20% to 100% of the
records. We scaled down the data so that the data and re-
sult distribution could remain approximately the same. We
show the square root of the running time with Jaccard sim-
ilarity for the DLBP dataset and cosine similarity for the
Enron dataset in Figures 3(k) and 3(l) (both thresholds are
ﬁxed at 0.9).

It is clear that the running time of all three algorithms
grow quadratically. This is not surprising given the fact
that the actual result size already grows quadratically (e.g.,
See Figure 1). Our proposed algorithms have demonstrated
a slower growth rate than the All-Pairs algorithm for both
similarity functions and datasets.

7.5 Near Duplicate Web Page Detection

5We also measured the candidate sizes (e.g., see Figure 1).

We also investigate a speciﬁc application of the similar-
ity join: near duplicate Web page detection. A traditional
method is based on performing approximate similarity join
on shingles computed from each record [6]. Later work pro-
posed further approximations mainly to gain more eﬃciency
at the cost of result quality.

Instead, we designed and tested three algorithms that per-
form exact similarity join on q-grams or shingles. (i) qp al-
gorithm where we use the ppjoin+ algorithm to join directly
on the set of 4-grams of each record. (ii) qa algorithm is
similar to qp except that All-Pairs algorithm is used as the
exact similarity join algorithm. (iii) sp algorithm where we
use the ppjoin+ algorithm to join on the set of shingles.

The metrics we measured are: running times, precision
and recall of the join result. Since algorithm qp returns ex-
act answer based on the q-grams of the records, its result is
a good candidate for the correct set of near duplicate docu-
ments. Hence, we deﬁne precision and recall as follows:
Recall = |Rsp| ∩ |Rqp|

Precision = |Rsp| ∩ |Rqp|

where Rx is the set of result returned by algorithm x.

We show the results in Table 2 with varying similarity

threshold values.

|Rsp|

|Rqp|

Table 2: Quality vs. Time Trade-oﬀ of Approximate
and Exact Similarity Join
Precision Recall

timeqp

timeap

timesp

t

0.95
0.90
0.85
0.80

0.38
0.48
0.58
0.57

0.11
0.06
0.04
0.03

41.98
245.03
926.54
2467.31

11.76
43.37
202.65
775.00

1.00
1.03
1.03
1.05

Several observations can be made

• Shingling-based methods will mainly suﬀer from low re-
calls in the result, meaning that only a small fraction of
truly similar Web pages will be returned. We manually
examined some similar pairs missing from Rsp (t = 0.95),
and most of the sampled pairs are likely to be near du-
plicates (e.g., they diﬀer only by typos, punctuations, or
additional annotations). Note that other variants of the
basic shingling method, e.g., systematic sampling of shin-
gles or super-shingling [6] were designed to trade result
quality for eﬃciency, and are most likely to have even
worse precision and recall values.
In contrast, exact similarity join algorithms (qp or qa)
have the appealing advantage of ﬁnding all the near du-
plicates given a similarity function.

• qp, while enjoying good result quality, requires longer
running time. However, with reasonably high similarity
threshold (0.90+), qp can ﬁnish the join in less than 45
seconds. On the other hand, qa takes substantially longer
time to perform the same join.

• sp combines the shingling and ppjoin+ together and is ex-
tremely fast even for modest similarity threshold of 0.80.
This method is likely to oﬀer better result quality than,
e.g., super-shingling, while still oﬀering high eﬃciency.

In summary, ppjoin+ algorithm can be combined with q-
grams or shingles to provide appealing alternative solutions
to tackle the near duplicate Web page detection tasks.

139WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, China8. RELATED WORK

Near Duplicate Object Detection Near duplicate ob-
ject detection has been studied under diﬀerent names in sev-
eral areas, including record linkage [27], merge-purge [19],
data deduplication [23], name matching [4], just to name a
few. [12] is a recent survey on this topic.

Similarity functions are the key to the near duplicate de-
tection task. For text documents, edit distance [26] and
Jaccard similarity on q-grams [17] are commonly used. Due
to the huge size of Web documents, similarity among docu-
ments is evaluated by Jaccard or overlap similarity on small
or ﬁx sized sketches [5, 10]. Soundex is a commonly used
phonetic similarity measures for names [22].

Exact Near Duplicate Detection Algorithm Exist-
ing methods for exact near duplicate detection usually con-
vert constraints deﬁned using one similarity function into
equivalent or weaker constraints deﬁned on another similar-
ity measure. [17] converts edit distance constraints to over-
lap constraints on q-grams. Jaccard similarity constraints
and 1/2-sided normalized overlap constraints can be con-
verted to overlap constraints [24, 8]. Constraints on overlap,
dice and Jaccard similarity measures can be coverted to con-
straints on cosine similarity [3]. [1] transforms Jaccard and
edit distance constraints to Hamming distance constraints.
The techniques proposed in previous work fall into two
categories.
In the ﬁrst category, exact near duplicate de-
tection problems are addressed by inverted list based ap-
proaches [3, 8, 24], as discussed above. The second cate-
gory of work [1] is based on the pigeon hole principle. The
records are carefully divided into partitions and then hashed
into signatures, with which candidate pairs are generated,
followed by a post-ﬁltering step to eliminate false positives.

Approximate Near Duplicate Object Detection Sev-
eral previous work [6, 7, 10, 16] has concentrated on the
problem of retrieving approximate answers to similarity func-
tions. LSH (Locality Sensitive Hashing) [16] is a well-known
approximate algorithm for the problem.
It regards each
record as a vector and generates signatures for each record
with random projections on the set of dimensions. Broder
et al.
[6] addressed the problem of identifying near dupli-
cate Web pages approximately by compressing document
records with a sketching function based on min-wise inde-
pendent permutations. The near duplicate object detection
problem is also a generalization of the well-known nearest
neighbor problem, which is studied by a wide body of work,
with many approximation techniques considered by recent
work [7, 13, 16, 21].

9. CONCLUSIONS

In this paper, we propose eﬃcient similarity join algo-
rithms by exploiting the ordering of tokens in the records.
The algorithms provide eﬃcient solutions for an array of
applications, such as duplicate Web page detection on the
Web. We show that positional ﬁltering and suﬃx ﬁltering
are complementary to the existing preﬁx ﬁltering technique.
They successfully alleviate the problem of quadratic growth
of candidate pairs when the data grows in size. We demon-
strate the superior performance of our proposed algorithms
to the existing preﬁx ﬁltering-based algorithms on several
real datasets under a wide range of parameter settings. The
proposed methods can also be adapted or integrated with

existing near duplicate Web page detection methods to im-
prove the result quality or accelerate the execution speed.

REFERENCES
[1] A. Arasu, V. Ganti, and R. Kaushik. Eﬃcient exact

set-similarity joins. In VLDB, 2006.

[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison Wesley, 1st edition edition, May 1999.
[3] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all pairs

similarity search. In WWW, 2007.

[4] M. Bilenko, R. J. Mooney, W. W. Cohen, P. Ravikumar,
and S. E. Fienberg. Adaptive name matching in informa-
tion integration. IEEE Intelligent Sys., 18(5):16–23, 2003.

[5] A. Z. Broder. On the resemblance and containment of

documents. In SEQS, 1997.

[6] A. Z. Broder, S. C. Glassman, M. S. Manasse, and

G. Zweig. Syntactic clustering of the web. Computer
Networks, 29(8-13):1157–1166, 1997.

[7] M. Charikar. Similarity estimation techniques from

rounding algorithms. In STOC, 2002.

[8] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive oper-

ator for similarity joins in data cleaning. In ICDE, 2006.

[9] J. Cho, N. Shivakumar, and H. Garcia-Molina. Finding

replicated web collections. In SIGMOD, 2000.

[10] A. Chowdhury, O. Frieder, D. A. Grossman, and M. C.

McCabe. Collection statistics for fast duplicate document
detection. ACM Trans. Inf. Syst., 20(2):171–191, 2002.

[11] J. G. Conrad, X. S. Guo, and C. P. Schriber. Online

duplicate document detection: signature reliability in a
dynamic retrieval environment. In CIKM, 2003.

[12] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios.

Duplicate record detection: A survey. TKDE, 19(1):1–16,
2007.

[13] R. Fagin, R. Kumar, and D. Sivakumar. Eﬃcient similarity
search and classiﬁcation via rank aggregation. In SIGMOD,
2003.

[14] D. Fetterly, M. Manasse, and M. Najork. On the evolution
of clusters of near-duplicate web pages. In LA-WEB, 2003.

[15] D. Gibson, R. Kumar, and A. Tomkins. Discovering large

dense subgraphs in massive graphs. In VLDB, 2005.

[16] A. Gionis, P. Indyk, and R. Motwani. Similarity search in

high dimensions via hashing. In VLDB, 1999.

[17] L. Gravano, P. G. Ipeirotis, H. V. Jagadish, N. Koudas,

S. Muthukrishnan, and D. Srivastava. Approximate string
joins in a database (almost) for free. In VLDB, 2001.

[18] M. R. Henzinger. Finding near-duplicate web pages: a

large-scale evaluation of algorithms. In SIGIR, 2006.

[19] M. A. Hern´andez and S. J. Stolfo. Real-world data is dirty:
Data cleansing and the merge/purge problem. Data Mining
and Knowledge Discovery, 2(1):9–37, 1998.

[20] T. C. Hoad and J. Zobel. Methods for identifying versioned

and plagiarized documents. JASIST, 54(3):203–215, 2003.
[21] P. Indyk and R. Motwani. Approximate nearest neighbors:

Towards removing the curse of dimensionality. In STOC,
1998.

[22] R. C. Russell. Index, U.S. patent 1,261,167, April 1918.
[23] S. Sarawagi and A. Bhamidipaty. Interactive deduplication

using active learning. In KDD, 2002.

[24] S. Sarawagi and A. Kirpal. Eﬃcient set joins on similarity

predicates. In SIGMOD, 2004.

[25] E. Spertus, M. Sahami, and O. Buyukkokten. Evaluating

similarity measures: a large-scale study in the orkut social
network. In KDD, 2005.

[26] E. Ukkonen. On approximate string matching. In FCT,

1983.

[27] W. E. Winkler. The state of record linkage and current
research problems. Technical report, U.S. Bureau of the
Census, 1999.

140WWW 2008 / Refereed Track: Data Mining - AlgorithmsApril 21-25, 2008 · Beijing, China