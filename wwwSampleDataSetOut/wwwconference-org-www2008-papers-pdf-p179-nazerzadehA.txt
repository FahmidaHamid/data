Dynamic Cost-Per-Action Mechanisms
and Applications to Online Advertising

Hamid Nazerzadeh
Stanford University
Stanford, CA 94304

hamidnz@stanford.edu

Amin Saberi

Stanford University
Stanford, CA 94304

saberi@stanford.edu

Northwestern University

Rakesh Vohra
Evanston, IL 60208

r-vohra@kellogg.nwu.edu

ABSTRACT
We study the Cost-Per-Action or Cost-Per-Acquisition (CPA)
charging scheme in online advertising. In this scheme, in-
stead of paying per click, the advertisers pay only when a
user takes a speciﬁc action (e.g. ﬁlls out a form) or completes
a transaction on their websites.

We focus on designing eﬃcient and incentive compatible
mechanisms that use this charging scheme. We describe a
mechanism based on a sampling-based learning algorithm
that under suitable assumptions is asymptotically individu-
ally rational, asymptotically Bayesian incentive compatible
and asymptotically ex-ante eﬃcient.

In particular, we demonstrate our mechanism for the case
where the utility functions of the advertisers are independent
and identically-distributed random variables as well as the
case where they evolve like independent reﬂected Brownian
motions.

Categories and Subject Descriptors
J.4 [Social and Behavioral Sciences]: Economics; F.2.0
[Analysis of Algorithms and Problem Complexity]:
General; I.2.6 [Artiﬁcial Intelligence]: Learning

General Terms
Economics, Algorithm, Theory

Keywords
Mechanism Design, Cost-Per-Action, Internet Advertising

1.

INTRODUCTION

Currently, the main two charging models in the online ad-
vertising industry are cost-per-impression (CPM) and cost-
per-click (CPC). In the CPM model, the advertisers pay the
publisher for the impression of their ads. CPM is commonly
used in traditional media (e.g. magazines and television) or
banner advertising and is more suitable when the goal of the
advertiser is to increase brand awareness.

A more attractive and more popular charging model in
online advertising is the CPC model in which the advertisers
pay the publisher only when a user clicks on their ads. In
the last few years, there has been a tremendous shift towards
the CPC charging model. CPC is adopted by search engines
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2008, April 21–25, 2008, Beijing, China.
ACM 978-1-60558-085-2/08/04.

such as Google or Yahoo! for the placement of ads next to
search results (also known as sponsored search) and on the
website of third-party publishers.

In this paper we will focus on another natural and widely
advocated charging scheme known as the Cost-Per-Action or
Cost-Per-Acquisition (CPA) model. In this model, instead
of paying per click, the advertiser pays only when a user
takes a speciﬁc action (e.g. ﬁlls out a form) or completes a
transaction. Recently, several companies like Google, eBay,
Amazon, Advertising.com, and Snap.com have started to
sell advertising in this way.

CPA models can be the ideal charging scheme, especially
for small and risk averse advertisers. We will brieﬂy describe
a few advantages of this charging scheme over CPC and refer
the reader to [18] for a more detailed discussion.

One of the drawbacks of the CPC scheme is that it re-
quires the advertisers to submit their bids before observ-
ing the proﬁts generated by the users clicking on their ads.
Learning the expected value of each click, and therefore the
right bid for the ad, is a prohibitively diﬃcult task especially
in the context of sponsored search in which the advertisers
typically bid for thousands of keywords. CPA eliminates
this problem because it allows the advertisers to report their
payoﬀ after observing the user’s action.

Another drawback of the CPC scheme is its vulnerabil-
ity to click fraud. Click fraud refers to clicks generated by
someone or something with no genuine interest in the adver-
tisement. Such clicks can be generated by the publisher of
the content who has an interest in receiving a share of the
revenue of the ad or by a rival who wishes to increase the
cost of advertising for the advertiser. Click fraud is consid-
ered by many experts to be the biggest challenge facing the
online advertising industry [13, 10, 23, 20]. CPA schemes
are less vulnerable because generating a fraudulent action
is typically more costly than generating a fraudulent click.
For example, an advertiser can deﬁne the action as a sale
and pay the publisher only when the ad yields proﬁt1.

On the other hand, there is a fundamental diﬀerence be-
tween CPA and CPC charging models. A click on the ad
can be observed by both advertiser and publisher. However,
the action of the user is hidden from the publisher and is ob-
servable only by the advertiser. Although the publisher can
require the advertisers to install a software that will monitor
actions that take place on their web site, even moderately
sophisticated advertisers can ﬁnd a way to manipulate the
software if they ﬁnd it suﬃciently proﬁtable.

1CPA makes generating a fraudulent action a more costly
enterprize, but not impossible (e.g., using a stolen credit).

179WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, ChinaAre the publishers exposed to the manipulation or misre-
porting of the advertisers in the CPA scheme? Does CPA
create an incentive for the advertisers to misreport the num-
ber of actions or their payoﬀs for the actions? The main
result of this paper is to give a negative answer to these
questions. We design a mechanism that, asymptotically and
under reasonable assumptions, removes the incentives of the
advertisers to misreport their payoﬀs. At the same time, our
mechanism has the same asymptotic eﬃciency and hence
revenue as the currently used CPC mechanisms. We will
use techniques in learning and mechanism design to obtain
this result.

In the next section, we will formally describe our model
in mechanism design terminology (see [21].) We will refer
to advertisers as agents and to the impression of an ad as
an item. For simplicity of exposition only, we assume only
one advertisement slot per page.
In section 6 we outline
how to extend our results to the case where more than one
advertisement can be displayed in each page. Although our
work is essentially motivated by online advertising, we be-
lieve that the application of our mechanism is not limited
this domain.

1.1 Model

We study the following problem: there are a number of
self-interested agents competing for identical items sold re-
peatedly at times t = 1, 2,· ·· . At each time t, a mechanism
allocates the item to one of the agents. Agents discover
their utility for the good only if it is allocated to them. If
agent i receives the good at time t, she discovers utility uit
(denominated in money) for it and reports (not necessarily
truthfully) the realized utility to the mechanism. Then, the
mechanism determines how much the agent has to pay for
receiving the item. We allow the utility of an agent to change
over time. For this environment we are interested in auction
mechanisms which have the following four properties.

1. The mechanism is individually rational in each period.

2. Agents have an incentive to truthfully report their re-

alized utilities.

3. The eﬃciency (and revenue) is, in an appropriate sense,

not too small compared to a second price auction.

4. The correctness of the mechanism does not depend on
an a-priori knowledge of the distribution of uit’s. This
feature is motivated by the Wilson doctrine [24] 2.

The precise manner in which these properties are formal-

ized is described in section 2.

We will build our mechanisms on a sampling-based learn-
ing algorithm. The learning algorithm is used to estimate
the expected utility of the agents, and consists of two alter-
nating phases: exploration and exploitation. During an ex-
ploration phase, the item is allocated for free to a randomly
chosen agent. During an exploitation phase, the mechanism
allocates the item to the agent with the highest estimated
expected utility. After each allocation, the agent who has
received the item, discovers her utility and reports it to the
mechanism. Subsequently, the mechanism updates the esti-
mate of utilities and determines the payment.
2Wilson criticizes relying too much on common-knowledge
assumptions.

We characterize a class of learning algorithms that en-
sure that the corresponding mechanism has the four desired
properties. The main diﬃculty in obtaining this result is
the following: since there is uncertainty about the utilities,
it is possible that in some periods the item is allocated to
an agent who does not have the highest utility in that pe-
riod. Hence, the natural second-highest price payment rule
would violate individual rationality. On the other hand, if
the mechanism does not charge an agent because her re-
ported utility after the allocation is low, it gives her an in-
centive to shade her reported utility down. Our mechanism
solves these problems by using an adaptive, cumulative pric-
ing scheme.

We illustrate our results by identifying simple mechanisms
that have the desired properties. We demonstrate these
mechanisms for the case in which the uit’s are independent
and identically-distributed random variables as well as the
case where their expected values evolve like independent re-
ﬂected Brownian motions. In these cases the mechanism is
actually ex-post individually rational.

In our proposed mechanism, the agents do not have to
bid for the items. This is advantageous when the bidders
themselves are unaware of their utility values. However, in
some cases, an agent might have a better estimate of her
utility for the item than our mechanism. For this reason,
we describe how we can slightly modify our mechanism to
allow those agents to bid directly.

1.2 Related Work

There is a large number of interesting results on using
machine learning techniques in mechanism design. We only
brieﬂy survey the main techniques and ideas and compare
them with the approach of this paper.

Most of these works, like [5, 8, ?, 17], consider one-shot
games or repeated auctions in which the agents leave the
environment after they received an item. In our setting we
may allocates items to an agent several times and hence, we
need to consider the strategic behavior of the agents over
time. There is also a big literature on regret minimization
or expert algorithms. In our context, these algorithms are
applicable even if the utilities of the agents are changing
arbitrarily. However, the eﬃciency (and therefore the rev-
enue) of these algorithms is comparable to the mechanisms
that allocates the item to the single best agent (expert) (e.g.
see [16]). Our goal is more ambitious: our eﬃciency is close
the most eﬃcient allocation which might allocate the item
to diﬀerent agents at diﬀerent times. On the other hand,
we focus on utility values that change smoothly (e.g. like a
Brownian motion).

In a ﬁnitely repeated version of the environment consid-
ered here, Athey and Segal [2] construct an eﬃcient, budget
balanced, mechanism where truthful revelation in each pe-
riod is Bayesian incentive compatible. Bapna and Weber [4]
consider the inﬁnite horizon version of [2] and propose a class
of incentive compatible mechanisms based on the Gittins in-
dex (see [11]). Taking a diﬀerent approach, Bergemann and
V¨alim¨aki [6] and Cavallo et al. [9] propose an incentive com-
patible generalization of the Vickrey-Clark-Groves mecha-
nism based on the marginal contribution of each agent for
this environment. All these mechanisms need the exact solu-
tion of the underlying optimization problems, and therefore
require complete information about the prior of the utilities

180WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, Chinaof the agents; also, they do not apply when the evolution of
the utilities of the agents is not stationary over time. This
violates the last of our desiderata. For a comprehensive sur-
vey in dynamic mechanism design literature see [22].

In the context of sponsored search, attention has focused
on ways of estimating click through rates. Gonen and Pavlov
[12] give a mechanism which learns the click-through rates
via sampling and show that truthful bidding is, with high
probability, a (weakly) dominant strategy in this mecha-
nism. Along this line, Wortman et al. [25] introduced an ex-
ploration scheme for learning advertisers’ click-through rates
in sponsored search which maintains the equilibrium of the
system. In these works, unlike ours, the distribution of the
utilities of agents are assumed to be ﬁxed over time.

Immorlica et al. [14], and later Mahdian and Tomak [18],
examine the vulnerability of various procedures for estimat-
ing click through, and identify a class of click through learn-
ing algorithms in which fraudulent clicks cannot increase
the expected payment per impression by more than o(1).
This is under the assumption that the slot of an agent is
ﬁxed and the bids of other agents remain constant overtime.
In contrast, we study conditions which guarantee incentive
compatibility and eﬃciency, while the utility of (all) agents
may evolve over time.

2. DEFINITIONS AND NOTATION

Suppose n agents competing in each period for a single
item. The item is sold repeatedly at time t = 1, 2,··· . De-
note by uit the nonnegative utility of agent i for the item
at time t. Utilities are denominated in a common monetary
scale.

The utilities of agents may evolve over time according to a
stochastic process. We assume that for i 6= j, the evolution
of uit and ujt are independent stochastic processes. We also
deﬁne µit = E[uit|ui1,··· , ui,t−1]. Throughout this paper,
expectations are taken conditioned on the complete history.
For simplicity of notation, we now omit those terms that
denote such a conditioning. With notational convention, it
follows, for example, that E[uit] = E[µit]. Here the second
expectation is taken over all possible histories.

Let M be a mechanism used to sell the items. At each
time, M allocates the item to one of the agents. Let i be
the agent who has received the item at time t. Deﬁne xit
to be the variable indicating the allocation of the item to i
at time t. After the allocation, agent i observes her utility,
uit, and then reports rit, as her utility for the item, to the
mechanism. Note that we do not require an agent to know
her utility for possessing the item in advance of acquiring it.
The mechanism then determines the payment, denoted by
pit.

Definition 1. An agent i is truthful if rit = uit, for all

time xit = 1, t > 0.

Our goal is to design a mechanism which has the following
properties. We assume n, the number of agents, is constant.

Individual Rationality: A mechanism is ex-post individ-
ually rational if for any time T > 0 and any agent
1 ≤ i ≤ n, the total payment of agent i does not ex-
ceed the sum of her reports:

TXt=1

xitrit − pit > 0.

M is asymptotically ex-ante individually rational if:

lim inf
T→∞

E[

TXt=1

xitµit − pit] ≥ 0.

Incentive Compatibility: This property implies that tru-
thfulness deﬁnes an asymptotic Bayesian Nash equilib-
rium. Consider agent i and suppose all agents except
i are truthful. Let Ui(T ) be the expected total proﬁt
of agent i, if agent i is truthful between time 1 and T .

Also, let eUi(T ) be the maximum of expected proﬁt of

agent i under any other strategy. Asymptotic incentive
compatibility requires that

eUi(T ) − Ui(T ) = o(Ui(T )).

Eﬃciency: An ex-ante eﬃcient mechanism allocates the
item to an agent in argmaxi{µit} at each time t (and
for each history). The total social welfare obtained
by an ex-ante eﬃcient mechanism up to time T is
t=1 maxi{µit}]. Let W (T ) be the expected wel-
fare of mechanism M between time 1 and T , when all
agents are truthful, i.e.,

E[PT

W (T ) = E[

TXt=1

xitµit]

nXi=1

Then, M is asymptotically ex-ante eﬃcient if:

E[

TXt=1

i {µit}] − W (T ) = o(W (T )).
max

3. PROPOSED MECHANISM

We build our mechanism on top of a learning algorithm
that estimates the expected utility of the agents. We re-
frain from an explicit description of the learning algorithm.
Rather, we describe suﬃcient conditions for a learning algo-
rithm that can be extended to a mechanism with all the
properties we seek (see section 3.1).
In section 4 and 5
we give two examples of environments where learning al-
gorithms satisfying these suﬃcient conditions exist.

The mechanism consists of two phases: explore and ex-
ploit. During the explore phase, with probability η(t), η :
N → [0, 1], the item is allocated for free to a randomly chosen
agent. During the exploit phase, the mechanism allocates
the item to the agent with the highest estimated expected
utility. Afterwards, the agent reports her utility to the mech-
anism and the mechanism determines the payment. We ﬁrst
formalize our assumptions about the learning algorithm and
then we discuss the payment scheme. The mechanism is
given in Figure 1.

The learning algorithm, samples uit’s at rate η(t), and
based on the history of the reports of agent i, returns an

estimate of µit. Letbµit(T ) be the estimate of the algorithm

for µit conditional on the history of the reports up to time
T . The history of the reports of agent i up to time T is
the sequence of the reported values and times of observation
of uit up to but not including time T . Note that we allow
T > t. Thus, information at time T > t can be used to revise
an estimate of µit made at some earlier time. We assume
that increasing the number of samples only increases the

181WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, ChinaFor t = 1, 2, . . .

With probability η(t), explore:

Uniformly at random, allocate the item to an agent i, 1 ≤ i ≤ n.
pit ← 0

With probability 1 − η(t), exploit:

Randomly allocate the item to an agent i ∈ argmaxi{bµit(t)}.
pit ←Pt−1

k=1 yik min{bγk(t),bµik(k)} −Pt−1

k=1 pik

rit ← the report of agent i.
pjt ← 0, j 6= i

Figure 1: Mechanism M

accuracy of the estimations, i.e.
and times T1 ≤ T2:

for any truthful agent i,

(1)

In the inequality above, and in the rest of the paper, the

E[|bµit(T1) − µit|] ≥ E[|bµit(T2) − µit|].

the random choices of the mechanism. For simplicity of no-
tation, we omit those terms that denote such a conditioning.
To describe the payments recall that γt is the second high-

expectations of bµit are taken over the evolution of uit’s and
est µit and letbγt(T ) = maxj6=i{bµjt(T )}, where i is the agent

who received the item at time t. We deﬁne yit to be the indi-
cator variable of the allocation of the item to agent i during
an exploit phase. The payment of agent i at time t, denoted
pit, is determined so that:

pik =

tXk=1

t−1Xk=1

yik min{bγk(t),bµik(k)}.

An agent only pays for items that are allocated to her
during the exploit phase, up to but not including time t. At
time t, the payment of agent i for the item she received at

time k < t is min{bγk(t),bµik(k)}. The ﬁrst term is the rem-

iniscence of the second highest pricing scheme. The second
term, under some reasonable conditions, leads to individu-
ally rationality. Since the estimations of learning algorithm
for the utilities of agents become more precise over time, our
adaptive cumulative payment scheme allows it to correct for
errors in the past.
3.1 Sufﬁcient Conditions

We start with a condition that guarantees asymptotic ex-
ante individual rationality and asymptotic incentive com-
patibility. Let lit be the last time up to time t that the item
is allocated to agent i within an exploit phase. If i has not
been allocated any item yet, lit is deﬁned to be zero. Also,

truthful up to time t.

deﬁne ∆t = maxi{|bµit(t) − µit|}, assuming all agents were

Theorem 1. If for the learning algorithm, for all 1 ≤ i ≤

n, and T > 0:

(C1) E[max1≤t≤T{µit} +PT

t=1 η(t)µit])
then mechanism M is asymptotically ex-ante individually
rational and incentive compatible.

t=1 ∆t] = o(E[PT

We outline the proof ﬁrst. As we prove in Lemma 2, by
condition (C1), the expected proﬁt of a truthful agent up
to time T is at least ( 1
t=1 η(t)µit]. Also, the
expected total error in the estimates of the payments up
t=1 ∆t]). We prove that
the total utility an agent could obtain by deviating from
the truthful strategy, between time 1 and T , is bounded by
t=1 ∆t]). Hence, the claim follows

n − o(1))E[PT
to time T is bounded by O(E[PT
O(max1≤t≤T{µit} + E[PT

Similar to other applications of learning algorithms, we
can observe a natural trade-oﬀ between exploitation and ex-
ploration rates in our context: higher exploration rates lead
to more accurate estimates of the utilities of the agents, at
the cost of eﬃciency. Condition (C1) provides us with a
lower bound on the exploration rate.

by condition (C1).

Lemma 2. If condition (C1) holds, then the expected proﬁt

of a truthful agent i up to time T , Ui(T ), is at least:

(

1
n − o(1))E[

TXt=1

η(t)µit].

Proof. The items that agent i receives during the ex-
plore phase are free. The expected total utility of agent i
from these items up to time T is 1
t=1 η(t)µit]. Let
CT = {t < liT|yit = 1, if i is truthful} be the subset of peri-
ods that agent i is charged for the item she received within
the period.

n E[PT

Ui(T ) = E[

xituit − pit]

TXt=1
xituit] + E[Xt∈CT
= E[Xt /∈CT
TXt=1
+E[Xt∈CT

η(t)µit]

≥

1
n

E[

uit − pit]

(µit − min{bγt(T ),bµit(t)})]

(2)

182WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, ChinaFor t ∈ CT :

E[(µit − min{bγt(T ),bµit(t)})I(t ∈ CT )]
≥ E[(µit −bµit(t))I(t ∈ CT )]
≥ −E[|µit −bµit(t)|]

≥ −E[∆t]

Substituting into inequality (2), by condition (C1):

Ui(T ) ≥

=

1
n

1
n

E[

E[

TXt=1
TXt=1

η(t)µit] − E[

∆t]

TXt=1
TXt=1

η(t)µit] − o(E[

η(t)µit])

(3)

Lemma 2 yields asymptotic ex-
Proof of Theorem 1:
ante individual rationality. We show that truthfulness is
asymptotically a best response when all other agents are
truthful. Fix an agent i intending to deviate and let S be
the strategy she deviates to. Fixing the evolution of all
ujt’s, 1 ≤ j ≤ n, and all random choices of the mechanism,
i.e. the steps in the explore phase and the randomly chosen
agents, let DT be the times that i receives the item un-
der strategy S during the exploit phase before time liT , i.e.
DT = {t < liT|yit = 1, if the strategy of i is S}. Similarly,

let CT = {t < liT|yit = 1, if i is truthful}. Also, letbµ0it, and
bγ0t correspond to the estimates of the mechanism when the

strategy of i is S. We ﬁrst bound the expected proﬁt of i,
under strategy S, during the exploit phase:

E[

TXt=1

yituit − pit]

E[max

≤ E[Xt∈DT
t≤T {µit}]
= E[ Xt∈DT \CT
E[ Xt∈DT ∩CT
t≤T {µit}]

µit − min{bγ0t(T ),bµ0it(t)}] +
µit − min{bγ0t(T ),bµ0it(t)}] +
µit − min{bγ0t(T ),bµ0it(t)}] +

E[max

(4)

(5)

The term E[maxt≤T{µit}] bounds the outstanding payment
of agent i; recall that the agent has not paid for the last
allocated item.

than O(∆t):

For time t ≥ 1, we examine two cases:
1. If t ∈ DT ∩CT , then agent i, in expectation, cannot de-

crease the “current price”, min{bγ0t(T ),bµ0it(t)}, by more
min{bγ0t(T ),bµ0it(t)} ≥min{bγ0t(T ),bγ0t(t)}
≥ γt − max{γt −bγ0t(T ), γt −bγ0t(t)}
≥ γt − (γt −bγ0t(T ))+ − (γt −bγ0t(t))+
Recall thatbγ0t(T ) = maxj6=i{bµ0it(T )} and all other agent

are truthful. Hence, taking expectation from both

where (z)+ = max{z, 0}.

sides, by (1):

(6)

≥ E[γtI(t ∈ DT ∩ CT )] − E[2∆t]

2. If t ∈ DT \ CT , agent i cannot increase her “expected

E[min{bγ0t(T ),bµ0it(t)}I(t ∈ DT ∩ CT )]
≥ E[(γt − (γt −bγ0t(T ))+ − (γt −bγ0t(t))+)I(t ∈ DT ∩ CT )]
proﬁt”, µit − min{bγ0t(T ),bµ0it(t)}, by more than O(∆t):
µit − min{bγ0t(T ),bµ0it(t)} ≤ µit − min{bγ0t(T ),bγ0t(t)}
≤ (µit −bµit(t)) + (bµit(t) − γt)
+ max{γt −bγ0t(T ), γt −bγ0t(t)}
≤ 2∆t + (γt −bγ0t(T ))+
+(γt −bγ0t(t))+
E[(µit − min{bγ0t(T ),bµ0it(t)})I(t ∈ DT − CT )]

≤ E[2∆tI(t ∈ DT − CT )] +

Taking expectation from both sides, by (1):

E[((γt −bγ0t(T ))+ + (γt −bγ0t(t))+)I(t ∈ DT − CT )]

≤ E[4∆t]

(7)

Substituting inequalities (6) and (7) into (5):

E[

TXt=1

yituit − pit] ≤ E[

6∆t] + E[max

t≤T {µit}]

µit − γt]

TXt=1
+E[ Xt∈DT ∩CT
TXt=1
E[Xt∈CT

≤ E[

6∆t] + E[max

t≤T {µit}] +
µit − γt] − E[ Xt∈CT \DT

(8)

µit − γt]

Substituting into (8):

E[γt − µit] ≤ E[2∆t]

For t ∈ CT , since bµit(t) ≥bγt(t), we have:
TXt=1

yituit − pit] ≤ 8E[

E[

TXt=1
+E[Xt∈CT

∆t] + E[max

t≤T {µit}]

µit − γt]

With algebraic manipulation, using (1), we get:

E[

TXt=1

yituit − pit] ≤ O(E[

TXt=1
+E[Xt∈CT

∆t] + E[max

t≤T {µit}])

µit − min{bγt(T ),bµit(t)}]

By condition (C1), we get the inequality below which com-

pletes the proof:

E[

TXt=1

yituit − pit] ≤ o(E[

η(t)µit])

TXt=1
+E[Xt∈CT

and the last inequality follows by (C1). The expected util-
ity of the truthful strategy and S during the explore phase

µit − min{bγt(T ),bµit(t)}]

183WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, Chinais equal. Therefore, by Lemma 2, the mechanism is asymp-
totically incentive compatible.
2

In the next theorem we show if the loss in eﬃciency dur-
ing exploration asymptotically goes to zero, then by Condi-
tion (C1) the mechanism is asymptotically ex-ante eﬃcient.

Theorem 3. If for the learning algorithm, in addition to

(C1), the following condition holds

(C2) E[PT

t=1 η(t) maxi{µit}] = o(E[PT

then, M is asymptotically ex-ante eﬃcient.

t=1 maxi{µit}])

expected loss in this case is equal to E[PT

Proof. M may fail to be ex-ante eﬃcient for two reasons.
First one is the loss in welfare during the exploration when
the item is allocated randomly to one of advertisers. The
t=1 η(t) maxi{µit}].
Another reason is the mistakes during exploitation. The
error in estimation can lead to allocation to an agent who
does not value the item the most. At time t, in the worst
case, the item might be allocated to an agent whose expected
utility is at most 2∆t less than the highest expected utility.
Therefore, the expected eﬃciency loss during exploration is
t=1 ∆t]). Since, for the expected welfare

of M between time 1 and T , denoted by W (T ), we have:

bounded by O(E[PT

E[

TXt=1

max
i {µit}] − W (T )

TXt=1

But, condition (C1) implies:

= O(E[

(∆t + η(t) max

i {µit})])

(9)

E[

TXt=1

∆t] = o(E[

= θ(E[

nXi=1

η(t)µit}])

η(t) max

i {µit}])

TXt=1
TXt=1

Plugging into (9):

E[

TXt=1

i {µit}] − W (T ) = O(E[
max

η(t) max

i {µit}])

TXt=1

= o(W (T ))

The last equality is followed by (C2) and implies asymptotic
ex-ante eﬃciency.

While Condition (C1) gives a lower bound on the explo-
ration rate, Condition (C2) gives an upper bound.
In the
next section, we will show with two examples how condi-
tions (C1) and (C2) can be used to adjust the exploration
rate of a learning algorithm in order to obtain eﬃciency and
incentive compatibility.

Remark 1. In Theorem 3 we showed that under some as-
sumptions, the welfare obtained by the mechanism is asymp-
totically equivalent to eﬃcient mechanism that every time
allocates the item to the agent with the highest expected util-
ity. We can give similar conditions to (C2) to guarantee
that the revenue of the mechanism is also asymptotically
equal to the revenue of the eﬃcient mechanism that every
time charges the winning agent the second highest expected
utility. To avoid repetition, we refrain from explaining this
condition in details.

3.2 Allowing agents to bid

In mechanism M no agent explicitly bids for an item.
Whether an agent receives an item or not depends on the
history of their reported utilities and the estimates that M
forms from them. This may be advantageous when the bid-
ders themselves are unaware of what their utilities will be.
However, when agents may posses a better estimate of their
utilities we would like to make use of that. For this reason
we describe how to modify M so as to allow agents to bid
for an item.
If time t occurs during an exploit phase let Bt be the set
of the agents who bid at this time. The mechanism bids
on the behalf of all agent i /∈ Bt. Denote by bit the bid of
agent i ∈ Bt for the item at time t. The modiﬁcation of M
random to one of the agents in arg maxi bit.

sets bit = bµit(t), for i /∈ B. Then, the item is allocated at
ond highest value in A. Letbγt(T ) to be equal to maxj6=i bjk.

let
A = {bjt|j ∈ Bt} ∪ {µjt|, j /∈ Bt}. Deﬁne γt as the sec-
The payment of agent i will be

If i is the agent who received the item at time t,

pit ←

t−1Xk=1

yik min{bγk(t), bik} −

pik.

t−1Xk=1

To incorporate the fact that bidders can bid for an item,

we must modify the deﬁnition of truthfulness.

Definition 2. Agent i is truthful if:

1. rit = uit, for all time xit = 1, t ≥ 1.

2. If i bids at time t, then E[|bit − µit|] ≤ E[|bµit − µit|].

Note that item 2 does not require that agent i bid their
actual utility only that their bid be closer to the mark than
the estimate. With this modiﬁcation in deﬁnition, Theo-
rems 1 and 3 continue to hold.

4.

INDEPENDENT AND IDENTICALLY
DISTRIBUTED UTILITIES

In this section, we assume that for each i, uit’s are inde-
pendent and identically-distributed random variables. For
simplicity, we deﬁne µi = E[uit], t > 0. Without loss of
generality, we also assume 0 < µi ≤ 1.
In this environment, the learning algorithm we use is an ε-
greedy algorithm for the multi-armed bandit problem3. Let

Call the mechanism based on this learning algorithm M(iid).
Lemma 4. If all agents are truthful, then, under M(iid)

E[∆t] = O(

1

√t1−

).

3See [3] for a similar algorithm.

nit =Pt−1

k=1 xit. For  ∈ (0, 1), we deﬁne:

xit

nit =

t−1Xk=1
bµit(T ) = ((PT

0,

η(t) = min{1, nt− ln1+ t}
k=1 xikrik)/niT ,

niT > 0
niT = 0

184WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, ChinaThe proof of this lemma is given in appendix A.
We show that M(iid), for ε ≤ 1

3 , satisﬁes all the desired
properties we discussed in the previous section. Moreover, it
satisﬁes a stronger notion of individual rationality. M(iid)
satisﬁes ex-post individual rationality if for any agent i, and
for all T ≥ 1:

TXt=1

pit ≤

xitrit

TXt=1

Theorem 5. M(iid) is ex-post individually rational. Also,

3 , M(iid) is asymptotically incentive compati-

for 0 ≤  ≤ 1
ble and ex-ante eﬃcient.

Proof. We ﬁrst prove ex-post individual rationality. It
is suﬃcient to prove it only for the periods that agent i has
received the item within an exploit phase. For T , such that
yiT = 1, we have:

pit =

TXt=1

≤

T−1Xt=1
yit min{bγt(T ),bµit(t)}
T−1Xt=1
T−1Xt=1
yitbγt(T ) ≤
yitbµiT (T )
T−1Xt=1
≤ nitbµiT (T ) =

xitrit

The third inequality follows because the item is allocated

to i at time T which implies bµiT (T ) ≥bγt(T ). We complete

the proof by showing that conditions (C1) and (C2) hold.
Note that µi ≤ 1. By lemma 4, for  ≤ 1
3 :

1

expected value of the error of these estimates at time t is
o(t

6 ).
We begin analyzing the mechanism by stating some well-

known properties of reﬂected Brownian motions (see [7]).

Proposition 6. Let [Wt, t ≥ 0] be a reﬂected Brownian
motion with mean zero and variance σ2; the reﬂection bar-
rier is 0. Assume the value of Wt at time t is equal to y:

E[y] = θ(√tσ2)

(12)

For T > 0, let z = Wt+T . For the probability density func-
tion of z − y we have:

Pr[(z − y) ∈ dx] ≤ r 2
Pr[|z − y| ≥ x] ≤ r 8T σ2
E[|z − y|I(|z − y| ≥ x)] ≤ r 8T σ2

πT σ2 e
1
x

π

π

−x2
2T σ2

−x2
2T σ2

e

−x2
2T σ2

e

(13)

(14)

(15)

Corollary 7. The expected value of the maximum of

µiT , 1 ≤ i ≤ n, is θ(√T ).

Note that in the corollary above n and σ are constant.
Now, similar to Lemma 4, we bound E[∆T ]. The proof is
given in appendix B.

Lemma 8. Suppose under M(B) all agents are truthful

until time T , then, E[∆T ] = O(T


2 ).

Now we are ready to prove the main theorem of this sec-

tion:

E[1+

T−1Xt=1

∆t] = O(T

1+

2 ) = o(T 1− ln1+ T ) = O(

η(t)µi).

TXt=1

Theorem 9. M(B) is ex-post individually rational. Also,
3 , M(B) is asymptotically incentive compatible

for 0 ≤  ≤ 1
and ex-ante eﬃcient.

Therefore, (C1) holds.

The welfare of any mechanism between time 1 and T is
t=1 ∆t + ηt] = o(T )

bounded by T . For any  > 0, E[1 +PT−1

which implies (C2).

5. BROWNIAN MOTION

In this section, we assume for each i, 1 ≤ i ≤ n, the
evolution of µit is a reﬂected Brownian motion with mean
zero and variance σ2
i ; the reﬂection barrier is 0. In addition,
i ≤ σ2, for some constant σ.
we assume µi0 = 0, and σ2
The mechanism observes the values of µit at discrete times
t = 1, 2, ··· .
In this environment our learning algorithm estimates the
reﬂected Brownian motion using a mean zero martingale.
We deﬁne lit is deﬁned as the last time up to time t that the
item is allocated to agent i. This includes both explore and
exploit phases. If i has not been allocated any item yet, lit
is zero.

η(t) = min{1, nt− ln2+2 t}

bµit(T ) = 8><>:

rilit
rili,t−1
rili,T

t < T
t = T

t > T

(10)

(11)

Call this mechanism M(B). For simplicity, we assume that
the advertiser reports the exact value of µit. It is not diﬃcult
to verify that the results in this section hold as long as the

Proof. We ﬁrst prove ex-post individual rationality. It
is suﬃcient to prove it only for the periods that agent i has
received the item within an exploit phase. For T , such that
yiT = 1, we have:

pit =

TXt=1

≤

≤

T−1Xt=1
T−1Xt=1
TXt=1

yit min{bγt(T ),bµit(t)}
yitbµit(t) =

T−1Xt=1

yitrili,t−1

xitrit.

We complete the proof by showing the conditions (C1)
and (C2) hold. By (12), the expected utility of each agent
at time t from random exploration is
θ(√tσ2t− ln1+ t) = θ(t

2 − ln1+ t).

1

Therefore, the expected utility up to time T from explo-
2 − ln1+ T ). By Lemma (8) and Corollary 7:
ration is θ(T

3

E[max

t≤T {µiT } +

∆t] = O(T 1+ 

2 ).

T−1Xt=1

For  ≤ 1

3 , 3

2 −  ≥ 1 + 

2 this yields Condition(C1).

185WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, ChinaBy Corollary 7, the expected value of maxi{µiT} and γT
are θ(√T ). Therefore, the expected welfare of an eﬃcient
2 ). For any 0 <  <
mechanism between time 1 and T is θ(T
1, we have:

3

θ(T

3

2 ) = ω(T

3

2 − ln1+ t + T 1+ 
2 )

By condition (C2), M(B) is asymptotically ex-ante eﬃ-
cient.

To apply this model to sponsored search we treat each
item as a bundle of search queries. Each time step is deﬁned
by the arrival of m queries. The mechanism allocates all
m queries to an advertiser and after that, the advertiser
reports the average utility for these queries. The payment
pit is now the price per item, i.e. the advertiser pays mpit
for the bundle of queries. The value of m is chosen such that
µit can be estimated with high accuracy.
6. DISCUSSION AND OPEN PROBLEMS

In this section we discuss some extensions of the mecha-

nisms.

Multiple Slots. To modify M so that it can accommodate

multiple slots we borrow from Gonen and Pavlov [12], who
assume there exist a set of conditional distributions which
determine the conditional probability that the ad in slot j1 is
clicked conditional on the ad in slot j2 being clicked. During
the exploit phase, M allocates the slots to the advertisers
with the highest expected utility, and the prices are deter-
mined according to Holmstrom’s lemma ([19], see also [1])
The estimates of the utilities are updated based on the re-
ports, using the conditional distribution.

Delayed Reports. In some applications, the value of re-
ceiving the item is realized at some later date. For example,
a user clicks on an ad and visits the website of the adver-
tiser. A couple of days later, she returns to the website
and completes a transaction.
It is not diﬃcult to adjust
the mechanism to accommodate this setting by allowing the
advertiser to report with a delay or change her report later.

Creating Multiple Identities. When a new advertiser joins
the system, in order to learn her utility value our mechanism
gives it a few items for free in the explore phase. Therefore
our mechanism is vulnerable to advertisers who can create
several identities and join the system.

It is not clear whether creating a new identity is cheap
in our context because the traﬃc generated by advertising
should eventually be routed to a legitimate business. Still,
one way to avoid this problem is to charge users without a
reliable history using CPC.

Acknowledgment. We would like to thank Arash Asad-
pour, Peter Glynn, Ashish Goel, Ramesh Johari, and Thomas
Weber for fruitful discussions. The second author acknowl-
edges the support from NSF and a gift from Google.
7. REFERENCES
[1] G. Aggarwal, A. Goel, and R. Motwani. Truthful

auctions for pricing search keywords. Proceedings of
ACM conference on Electronic Commerce, 2006.

[2] S. Athey, and I. Segal. An Eﬃcient Dynamic

Mechanism. manuscript, 2007.

[3] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time
Analysis of the Multiarmed Bandit Problem. Machine
Learning archive, Volume 47 , Issue 2-3, 235-256, 2002.
[4] A. Bapna, and T. Weber. Eﬃcient Dynamic Allocation

with Uncertain Valuations. Working Paper, 2006.

[5] M. Balcan, A. Blum, J. Hartline, and Y. Mansour.

Mechanism Design via Machine Learning. Proceedings
of 46th Annual IEEE Symposium on Foundations of
Computer Science, 2005.

[6] D. Bergemann, and J. V¨alim¨aki. Eﬃcient Dynamic

Auctions. Proceedings of Third Workshop on
Sponsored Search Auctions, 2007.

[7] A. Borodin, and P. Salminen. Handbook of Brownian

Motion: Facts and Formulae. Springer, 2002.

[8] A. Blum, V. Kumar, A. Rudra, and F. Wu. Online

Learning in Online Auctions. Proceedings of the
fourteenth annual ACM-SIAM symposium on Discrete
Algorithms, 2003.

[9] R. Cavallo, D. Parkes, and S. Singh, Eﬃcient Online

Mechanism for Persistent, Periodically Inaccessible
Self-Interested Agents. Working Paper, 2007.

[10] K. Crawford. Google CFO: Fraud A Big Threat.

CNN/Money, December 2, 2004.

[11] J. Gittins. Multi-Armed Bandit Allocation Indices.

Wiley, New York, NY, 1989.

[12] R. Gonen, and E. Pavlov. An Incentive-Compatible
Multi-Armed Bandit Mechanism. Proceedings of the
Twenty-Sixth Annual ACM Symposium on Principles
of Distributed Computing, 2007.

[13] B. Grow, B. Elgin, and M. Herbst. Click Fraud: The
dark side of online advertising. BusinessWeek. Cover
Story, October 2, 2006.

[14] N. Immorlica, K. Jain, M. Mahdian, and K. Talwar.

Click Fraud Resistant Methods for Learning
Click-Through Rates. Proceedings of the 1st Workshop
on Internet and Network Economics, 2005.

[15] B. Kitts, P. Laxminarayan, B. LeBlanc, and R. Meech.

A Formal Analysis of Search Auctions Including
Predictions on Click Fraud and Bidding Tactics.
Workshop on Sponsored Search Auctions, 2005.

[16] R. Kleinberg. Online Decision Problems With Large

Strategy Sets. Ph.D. Thesis, MIT, 2005.

[17] S. Lahaie, and D. Parkes. Applying Learning

Algorithms to Preference Elicitation. Proceedings of
the 5th ACM conference on Electronic Commerce,
2004.

[18] M. Mahdian, and K. Tomak. Pay-per-action model for
online advertising. Proceedings of the 3rd International
Workshop on Internet and Network Economics,
549-557, 2007.

[19] P. Milgrom, Putting Auction Theory to Work.

Cambridge University Press, 2004.

[20] D. Mitchell. Click Fraud and Halli-bloggers. New York

Times, July 16, 2005.

[21] N. Nisan, T. Roughgarden, E. Tardos, and

V. Vazirani, editors. Algorithmic Game Theory,
Cambridge University Press, 2007.

[22] D. Parkes. Online Mechanisms Algorithmic Game

Theory (Nisan et al. eds.), 2007.

186WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, China[23] B. Stone. When Mice Attack: Internet Scammers

Steal Money with “Click Fraud”. Newsweek, January
24, 2005.

[24] R. Wilson. Game-Theoretic Approaches to Trading
Processes. Economic Theory: Fifth World Congress,
ed. by T. Bewley, chap. 2, pp. 33-77, Cambridge
University Press, Cambridge, 1987.

[25] J. Wortman, Y. Vorobeychik, L. Li, and J. Langford.

Maintaining Equilibria During Exploration in
Sponsored Search Auctions. Proceedings of the 3rd
International Workshop on Internet and Network
Economics, 2007.

APPENDIX

A. PROOF OF LEMMA 4

Proof. We prove the lemma by showing that for any

agent i,

Pr[|µi −bµit(t)| ≥

that:

1

√t1−

µi] = o(

1
tc ),∀c > 0.

First, we estimate E[nit]. There exists a constant d such

E[nit] ≥

t−1Xk=1

η(k)

n

=

t−1Xk=1

min{

1
n

, k− ln1+ k} >

1
d

t1− ln1+ t

By the Chernoﬀ-Hoeﬀding bound:

Pr[nit ≤

E[nit]

2

] ≤ e

−t1− ln1+ t

8d

.

Inequality (1) and the Chernoﬀ-Hoeﬀding bound imply:

Pr[|µi −bµit(t)| ≥

1

√t1−

µi] =

= Pr[|µi −bµit(t)| ≥
+ Pr[|µi −bµit(t)| ≥

t1− ln1+ t µi

t1−

1

−

2d

√t1−
1
√t1−

≤ 2e
= o(

1
tc ),∀c > 0

1

µi ∧ nit ≥

E[nit]

]

2
E[nit]

µi ∧ nit <

]

2

−t1− ln1+ t

8d

+ e

Therefore, with probability 1 − o( 1
t ), for all agents, ∆t ≤
1√t1− . Since the maximum value of uit is 1, E[∆t] =
O(

1√t1− ).

B. PROOF OF LEMMA 8



2 ] = o( 1

Proof. Deﬁne Xit = |µi,T − µi,T−t|. We ﬁrst prove
T c ),∀c > 0. There exists a constant
Pr[Xit > T
Td such that for any time T ≥ Td, the probability that i has
not been randomly allocated the item in the last t < Td step
is at most:
Pr[T − li,T−1 > t] < (1 − T − ln2+2 T )t ≤ e
ln1+ T T . By equation (14) and (16),
Let t = 1

−t ln2+2 T

. (16)

T 

Pr[Xit > T



2 ] = Pr[Xit > T



2 ∧ T − li,T−1 ≤ t]



2 ∧ T − li,T−1 > t]

+ Pr[Xit > T

= o(

1
T c ),∀c > 0.



Hence, with high probability, for all the n agents, Xit ≤

T
2 , then, by Corol-
2 . If for some of the agents Xit ≥ T
lary 7, the expected value of the maximum of µit over these
agents is θ(√T ). Therefore, E[maxi{Xit}] = O(T

2 ). The
lemma follows because E[∆T ] ≤ E[maxi{Xit}].

187WWW 2008 / Refereed Track: Internet Monetization - Online AdvertisingApril 21-25, 2008 · Beijing, China