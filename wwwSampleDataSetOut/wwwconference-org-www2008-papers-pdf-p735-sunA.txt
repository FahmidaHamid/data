Mining, Indexing, and Searching for Textual Chemical

Molecule Information on the Web

Bingjun Sun

Department of Computer
Science and Engineering

Pennsylvania State University

University Park, PA 16802,

USA

bsun@cse.psu.edu

Prasenjit Mitra

College of Information

Sciences and Technology

Pennsylvania State University

University Park, PA 16802,
pmitra@ist.psu.edu

USA

C. Lee Giles

College of Information

Sciences and Technology

Pennsylvania State University

University Park, PA 16802,

USA

giles@ist.psu.edu

ABSTRACT
Current search engines do not support user searches for
chemical entities (chemical names and formulae) beyond sim-
ple keyword searches. Usually a chemical molecule can be
represented in multiple textual ways. A simple keyword
search would retrieve only the exact match and not the oth-
ers. We show how to build a search engine that enables
searches for chemical entities and demonstrate empirically
that it improves the relevance of returned documents. Our
search engine (cid:28)rst extracts chemical entities from text, per-
forms novel indexing suitable for chemical names and for-
mulae, and supports di(cid:27)erent query models that a scientist
may require. We propose a model of hierarchical condi-
tional random (cid:28)elds for chemical formula tagging that con-
siders long-term dependencies at the sentence level. To sub-
string searches of chemical names, a search engine must
index substrings of chemical names.
Indexing all possible
sub-sequences is not feasible in practice. We propose an
algorithm for independent frequent subsequence mining to
discover sub-terms of chemical names with their probabil-
ities. We then propose an unsupervised hierarchical text
segmentation (HTS) method to represent a sequence with
a tree structure based on discovered independent frequent
subsequences, so that sub-terms on the HTS tree should be
indexed. Query models with corresponding ranking func-
tions are introduced for chemical name searches. Experi-
ments show that our approaches to chemical entity tagging
perform well. Furthermore, we show that index pruning can
reduce the index size and query time without changing the
returned ranked results signi(cid:28)cantly. Finally, experiments
show that our approaches out-perform traditional methods
for document search with ambiguous chemical terms.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval(cid:22)Information (cid:28)ltering,Query formula-
tion,Retrieval models,Search process; H.3.1 [Information
Storage and Retrieval]: Content Analysis and Index-
ing(cid:22)Linguistic processing; I.2.7 [Arti(cid:28)cial Intelligence]:
Natural Language Processing(cid:22)Text analysis; J.2 [Physical
Sciences and Engineering]: Chemistry
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2008, April 21(cid:150)25, 2008, Beijing, China.
ACM 978-1-60558-085-2/08/04.

General Terms
Algorithms, Design, Experimentation

Keywords
Entity extraction, conditional random (cid:28)elds, independent
frequent subsequence, hierarchical text segmentation, index
pruning, substring search, similarity search, ranking
1.

INTRODUCTION

Chemists search for chemical entities (names and formu-
lae) on the Web [19]. We outline how a chemical entity
search engine can be built to enable such searches. We con-
jecture that a similar approach can be utilized to construct
other vertical, domain-speci(cid:28)c search engines that need to
recognize special entities.

Users search for chemical formulae using di(cid:27)erent forms
or semantics of the same molecule, e.g., they may search
for (cid:16)CH4(cid:17) or for (cid:16)H4C(cid:17). Furthermore, users use substrings
of chemical names in their search, especially, for well-known
functional groups, e.g., they may search for (cid:16)ethyl(cid:17) to search
for the name in Figure 4. General-purpose search engines
usually support exact keyword searches and do not return
documents where only parts of query keywords occur.
In
this work, we introduce multiple query semantics to allow
partial and fuzzy searches for chemical entities. To enable
fast searches, we show how to build indexes on chemical
entities.

Our investigation shows that end-users performing chem-
ical entity search using a generic search engine, like Google,
do not get very accurate results. One of the reasons is that
general-purpose search engines do not discriminate between
chemical formulae and ambiguous terms. Our search en-
gine identi(cid:28)es chemical formulae and disambiguates its oc-
currences from those of other strings that could potentially
be valid chemical formulae, e.g., (cid:16)He(cid:17), but are actually not.
Disambiguating terms is a hard problem because it requires
context-sensitive analysis.

A generic search engine does not support document re-
trieval using substring match query keywords. If we want
to enable this kind of searches, a naive approach is to con-
struct an index of all possible sub-terms in documents. Such
an index will be prohibitively large and expensive. To sup-
port partial chemical name searches, our search engine seg-
ments a chemical name into meaningful sub-terms automat-
ically by utilizing the occurrences of sub-terms in chemical

735WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, Chinanames. Such segmentation and indexing allow end-users to
perform fuzzy searches for chemical names, including sub-
string search and similarity search. Because the space of
possible substrings of all chemical names appearing in text is
extremely large, an intelligent selection needs to be made as
to which substrings should be indexed. We propose to mine
independent frequent patterns, use information about those
patterns for chemical name segmentation, and then index
sub-terms. Our empirical evaluation shows that this index-
ing scheme results in substantial memory savings while pro-
ducing comparable query results in reasonable time. Sim-
ilarly, index pruning is also applied to chemical formulae.
Finally, user interaction is provided so that scientists can
choose the relevant query expansions. Our experiments also
show that the chemical entity search engine outperforms
general purpose search engines (as expected).

Constructing an accurate domain-speci(cid:28)c search engine
is a hard problem. For example, tagging chemical enti-
ties is di(cid:30)cult due to noise from text recognition errors by
OCR, PDF transformation, etc. Chemical lexicons are of-
ten incomplete especially with respect to newly discovered
molecules. Often they do not contain all synonyms or do
not contain all the variations of a chemical formula. Un-
derstanding the natural language context of the text to de-
termine that an ambiguous string is chemical entity or not
(e.g., (cid:16)He(cid:17) may actually refer to ‘Helium(cid:146) as opposed to
the pronoun) is also a hard problem to solve with very high
precision even if dictionaries of chemical elements and for-
mulae are used. Moreover, many chemical names are long
phrases composed of several terms so that general tokeniz-
ers for natural languages may tokenize a chemical entity into
several tokens. Finally, di(cid:27)erent representations of a chem-
ical molecule (e.g., (cid:16)acetic acid(cid:17) is written as (cid:16)CH3COOH(cid:17)
or (cid:16)C2H4O2(cid:17)) have to be taken into account while index-
ing (or use other strategies like query expansion) to provide
accurate and relevant results with a good coverage.

Our search engine has two stages: 1) mining textual chem-
ical molecule information, and 2) indexing and searching for
textual chemical molecule information.
In the (cid:28)rst stage,
chemical entities are tagged from text and analyzed for in-
dexing. Machine-learning-based approaches utilizing domain
knowledge perform well, because they can mine implicit
rules as well as utilize prior knowledge. In the second stage,
each chemical entity is indexed.
In order to answer user
queries with di(cid:27)erent search semantics, we have to design
ranking functions that enable these searches. Chemical en-
tities and documents are ranked using our ranking schemes
to enable fast searches in response to queries from users.
Synonyms or similar chemical entities are suggested to the
user, and if the user clicks on any of these alternatives, the
search engine returns the corresponding documents.

Our prior work on chemical formulae in the text [25], while
this paper focuses approaches to handle chemical names. We
show how a domain-speci(cid:28)c chemical entity search engine
can be built that includes tasks of entity tagging, mining fre-
quent subsequences, text segmentation, index pruning, and
supporting di(cid:27)erent query models. Some previous work is in
[25]. We propose several novel methods for chemical name
tagging and indexing in this paper. The major contributions
of this paper are as follows:

† We propose a model of hierarchical conditional random
(cid:28)elds for entity tagging that can consider the long-term
dependencies at di(cid:27)erent levels of documents.

† We introduce a new concept, independent frequent sub-
sequences, and propose an algorithm to (cid:28)nd those sub-
sequences, with the goal to discover sub-terms with
corresponding probabilities in chemical names.

† We propose an unsupervised method of hierarchical
text segmentation and use it for chemical name index
pruning. This approach can be extended to index any
kinds of sequences, such as DNA.

† We present various query models for chemical name

searches with corresponding ranking functions.

The rest of this paper is organized as follows: Section 2 re-
views related works. Section 3 presents approaches of mining
chemical names and formulas based on CRFs and HCRFs.
We also propose algorithms for independent frequent subse-
quence mining and hierarchical text segmentation. Section
4 describes the index schemes, query models, and ranking
functions for name and formula searches. Section 5 presents
experiments and results. Conclusions and future directions
are discussed in Section 6. Web service of our system is
provided online 1.

2. RELATED WORK

Banville has provided a high-level overview on mining
chemical structure information from the literature [12]. Hid-
den Markov Models (HMMs) [9] are one of the common
methods for entity tagging. HMMs have strong indepen-
dence assumptions and su(cid:27)er from the label-bias problem.
Other methods such as Maximum Entropy Markov Models
(MEMMs) [16] and Conditional Random Fields (CRFs) [11]
have been proposed. CRFs are undirected graph models
that can avoid the label-bias problem and relax the inde-
pendence assumption. CRFs have been used in many ap-
plications, such as detecting names [17], proteins [22], genes
[18], and chemical formulae [25]. Other methods based on
lexicons and Bayesian classi(cid:28)cation have been used to recog-
nize chemical nomenclature [26]. The most common method
used to search for a chemical molecule is substructure search
[27], which retrieves all molecules with the query substruc-
ture. However, users require su(cid:30)cient knowledge to select
substructures to characterize the desired molecules for sub-
string search, so similarity search[27, 29, 23, 21] is desired
by users to bypass the substructure selection. Other re-
lated work involves mining for frequent substructures [8, 10]
or for frequent sub-patterns [28, 30]. Some previous work
has addressed how to handle entity-oriented search [6, 15].
Chemical markup languages have been proposed [20] but are
still not in wide use. None of the previous work focuses on
the same problem as ours. They are mainly about biological
entity tagging or chemical structure search. We have shown
how to extract and search for chemical formulae in text [25],
but we propose several new methods to extract and search
for chemical names in text documents.

3. MOLECULE INFORMATION MINING
In this section, we discuss three mining issues before index
construction: how to tag chemical entities from text, how to
mine frequent sub-terms from chemical names, and how to
hierarchically segment chemical names into sub-terms that
can be used for indexing.

1http://chemxseer.ist.psu.edu/

736WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, China3.1 Conditional Random Fields

Because we are building a chemical entity search engine,
accurately tagging entities in text is important. CRFs [11]
is a powerful method for tagging sequential data. We also
demonstrate how the accuracy can be improved by consid-
ering long-term dependency in the surrounding context at
di(cid:27)erent levels using hierarchical models of CRFs.

Conditional Random Fields

Suppose we have a training set S of labelled graphs. Each
graph in S is an i.i.d. sample [11]. CRFs model each graph
as an undirected graph G = (V; E). Each vertex v 2 V has
a label yv and an observation xv. Each edge e = fv; v0g 2 E
represents the mutual dependence of two labels yv; yv0. For
each sample, the conditional probability p(yjx; ‚), where x
is the observation vector of all vertices in G, y is the label
vector, and ‚ is the parameter vector of the model, repre-
sents the probability of y given x. An exponential proba-
bilistic model based on feature functions is used to model
the conditional probability,
1

exp(X

‚jFj(y; x));

(1)

p(yjx; ‚) =

Z(x)

j

where Fj(y; x) is a feature function that extracts a feature
from y and x. Z(x) is a normalization factor.

For sequential data, usually chain-structured CRF models
are applied, where only the labels (yi¡1 and yi) of neighbors
in a sequence are dependent. Moreover, usually only binary
features are considered. There are two types of features,
state features Fj = Sj(y; x) = Pjyj
i=1 sj(yi; x; i) to consider
only the label (yi) of a single vertex and transition features
Fj = Tj(y; x) = Pjyj
i=1 tj(yi¡1; yi; x; i) to consider mutual
dependence of vertex labels (yi¡1 and yi) for each edge e in
G. State features include two types: single-vertex features
obtained from the observation of a single vertex and over-
lapping features obtained from the observations of adjacent
vertices. Transition features are combinations of vertex la-
bels and state features. Each feature has a weight ‚j to
specify if the corresponding feature is favored. The weight
‚j should be highly positive if feature j tends to be (cid:16)on(cid:17) for
the training data, and highly negative if it tends to be (cid:16)o(cid:27)(cid:17).
The log-likelihood for the whole training set S is given by

L(‚) = X

log(p(yjx; ‚));

x2S

(2)

where the conditional probability of each sample can be esti-
mated from the data set. Maximize log-likelihood is applied
to estimate parameters ‚ during training. To predict labels
of x, the probabilities of all possible y are computed using
Equation 1, and y with the largest probability is the best
estimation. We introduce a feature boosting parameter (cid:181)
as weights for the true classes during the testing process to
tune the trade-o(cid:27) between precision and recall as in [25].

Hierarchical CRFs

Although CRFs can model multiple and long-term depen-
dencies on the graph and may have a better performance [14]
than without considering those dependencies,
in practice
only short-term dependencies of each vertex (i.e. a word-
occurrence) are considered due to the following reasons: 1)
usually we do not know what kind of long-term dependen-
cies exist, 2) too many features will be extracted, if all kinds
of long-term features are considered, and 3) most long-term
features are too sparse and speci(cid:28)c to be useful.

Figure 1: Illustration of Hierarchical CRFs

However, long-term dependencies at high levels may be
useful to improve the accuracy of tagging tasks. For ex-
ample, at the document level, biological articles have much
smaller probabilities of containing chemical names and for-
mulae. At the sentence level, sentences in di(cid:27)erent sections
have di(cid:27)erent probabilities and feature frequencies of the
occurrence of chemical names and formulae, e.g., references
seldom contain chemical formulae. Based on these observa-
tions, we extend CRFs to introduce Hierarchical Conditional
Random Fields (HCRFs) as illustrated in Figure 1. HCRFs
start from the highest level to the lowest level of granularity,
tag each vertex (e.g. document, sentence, or term) with la-
bels, and use labels as features and generates new features as
the interaction with low-level features. At each level, either
unsupervised or supervised learning can be used.

The probability models of HCRFs from the highest level

to the level m for a sequence are de(cid:28)ned as

p(y1jx; ‚1) =

1

Z(x)

e(Pj ‚

(1)

j Fj (y1;x)); ::::::;

(3)

p(ymjy1; :::; ym¡1; x; ‚m) =

ePj ‚

(m)
j Fj (y1;:::;ym;x)

Z(x)

;

(4)

where y1; :::; ym¡1 are the labels at the level of 1; :::; m ¡ 1,
and Fj(y1; :::; ym; x) is the feature function that extracts a
feature from the label sequences of each level y1; :::; ym and
the observation sequence x. During training, the parameter
‚ is estimated, while during testing, y1; :::; ym¡1 are esti-
mated before ym is estimated at the level m. For the level
m, besides the normal features

Sj(ym; x) = Pjyj
Tj(ym; x) = Pjyj

i=1 sj(y(m)
i=1 tj(y(m)

i

; x; i), and

i¡1 ; y(m)

i

; x; i),

there are two types of features regarding the higher levels of
label sequences y1; :::; ym¡1, non-interactive features:
; y1; :::; ym¡1) and

j(y1; :::; ym; x) = Pjymj
S0
j (y1; :::; ym; x) = Pjymj
T 0

i=1 s0
i=1 t0

i

j(y(m)
j(y(m)

i¡1 ; y(m)

i

; y1; :::; ym¡1)

which have no interaction with the observation sequence x,
and interactive features:

i

; y1; :::; ym¡1; x; i),

j (y1; :::; ym; x) = Pjymj
S00
i=1 s00
i=1 t00
T 00
j (y1; :::; ym; x) = Pjymj

j (y(m)
j (y(m)

i¡1; y(m)

; y1; :::; ym¡1; x; i)
i
that interact with the observation x.
Interactive features
are generated by the combination of the non-interactive fea-
tures and the normal features at the level m. For example
for the vertex i at the level m,s
; y1; :::; ym¡1; x; i) =
0(y(m)
0 and s are state
s
feature vectors for each vertex with sizes of js
0j and jsj, and
00 is a js
s

0j by jsj matrix of features.

; x; i), where s

; y1; :::; ym¡1)s

T (y(m)

00(y(m)

i

i

i

737WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, ChinaFeature Set

For entity tagging, our algorithm extracts two categories
of state features from sequences of terms: single-term fea-
tures, and overlapping features from adjacent terms. Single-
term features are of two types: sur(cid:28)cial features and ad-
vanced features. Sur(cid:28)cial features can be observed directly
from the term, e.g.
InitialCapital, AllCapitals, HasDigit,
HasDash, HasDot, HasBrackets, HasSuperscripts, IsTerm-
Long. Advanced features are generated by complex domain
knowledge or other data mining methods, e.g. POS tags
learned by natural language parsers, if a term follows the
syntax of chemical formulae, etc. For chemical name tag-
ging, we use a lexicon of chemical names collected online
by ourselves and WordNet [4]. We support fuzzy matching
on the Levenshtein Distance [13]. Furthermore, we check
if a term has sub-terms learned from the chemical lexicon
based the method in Section 3.5. For sentence tagging, fea-
ture examples are: ContainTermInList, ContainTermPat-
tern, ContainSequentialTermPattern. For example, a list of
journals, a list of names, and string patterns in reference,
are used for sentence tagging. Overlapping features of ad-
jacent terms are extracted. We used -1, 0, 1 as the window
of tokens, so that for each token, all overlapping features of
last token and next token are included in the feature set.
For example, for He in (cid:16)... . He is ...(cid:17), feature(termn¡1 =
\.00 ^ termn = initialCapital)=true, and feature(termn =
initialCapital ^ termn+1 = isP OST agV BZ)=true, where
isP OST agV BZ means that (cid:16)is(cid:17) is a (cid:16)present tense verb, 3rd
person singular(cid:17). The (cid:16)He(cid:17) is likely to be an English word
instead of Helium.

3.2

Independent Frequent Subsequence
Mining

In the last subsection, we use sub-terms as features for
chemical name tagging. Moreover, our system supports
searches for keywords that occur as parts of terms in docu-
ments, e.g., documents with the term (cid:16)methylethyl(cid:17) should
be returned in response to an user search for (cid:16)methyl(cid:17). In-
stead of manually discovering those chemical sub-terms by
domain experts, we propose unsupervised algorithms to (cid:28)nd
them automatically from a chemical lexicon and documents.
To support such searches, our system maintains an index
of sub-terms. Before it can index sub-terms, the system
must segment terms into meaningful sub-terms.
Indexing
all possible subsequences of all chemical names would make
the index too large resulting in slower query processing. Fur-
thermore, users will not query using all possible sub-terms of
queries, e.g., a query using keyword (cid:16)hyl(cid:17) is highly unlikely
if not completely improbable, while searches for (cid:16)methyl(cid:17)
can be expected. So, indexing (cid:16)hyl(cid:17) is a waste. There-
fore, (cid:16)methyl(cid:17) should be in the index, while (cid:16)hyl(cid:17) should
not. However, our problem has additional subtlety. While
(cid:16)hyl(cid:17) should not be indexed, another sub-term (cid:16)ethyl(cid:17) that
occurs independently of (cid:16)methyl(cid:17) should be indexed. Also,
when the user searches for (cid:16)ethyl(cid:17), we should not return the
documents that contain (cid:16)methyl(cid:17). Due to this subtlety, sim-
ply using the concepts of closed frequent subsequences and
maximal frequent subsequences as de(cid:28)ned in [28, 30] is not
enough. We introduce the important concept of indepen-
dent frequent subsequence. Our system attempts to identify
independent frequent subsequences and index them.

In the rest of this paper, we use(cid:16)sub-term(cid:17) to refer to
a substring in a term that appears frequently. Before our

system can index the sub-terms, it must segment chemical
names like (cid:16)methylethyl(cid:17) automatically into (cid:16)methyl(cid:17) and
(cid:16)ethyl(cid:17). The independent frequent sub-terms and their fre-
quencies can be used in hierarchical segmentation of chemi-
cal names (explained in details in the next subsection).

First, we introduce some notations as follows:
De(cid:28)nition 1. Sequence, Subsequence, Occurrence:
A sequence s =< t1; t2; :::; tm >, is an ordered list, where
each token ti is an item, a pair, or another sequence. Ls is
the length of s. A subsequence s0 „ s is an adjacent part
of s, < ti; ti+1; :::; tj >; 1 • i • j • m. An occurrence of
s0 in s, i.e., Occurs0„s;i;j =< ti; :::; tj >, is an instance of
s0 in s. We say that in s, Occurs0„s;i;j and Occurs00„s;i0;j0
overlap, i.e. Occurs0„s;i;j \ Occurs00„s;i0;j0
6= ;, i(cid:27) 9n; i •
n • j ^ i0 • n • j0. Unique occurrences are those without
overlapping.

De(cid:28)nition 2. Support: Given a data set D of sequences
s, Ds0 is the support of subsequence s0, which is the set of
all sequences s containing s0, i.e. s0 „ s. jDsj is the number
of sequences in Ds.

De(cid:28)nition 3. Frequent Subsequence: F reqs0„s is the
frequency of s0 in s, i.e. the count of all unique Occurs0„s. A
subsequence s0 is in the set of frequent subsequences F S, i.e.
s0 2 F S, if Ps2Ds0 F reqs0„s ‚ F reqmin, where F reqmin is
a threshold of the minimal frequency.

However, usually a sub-term is a frequent subsequence,
but not for inverse, since all subsequences of a frequent sub-
sequence are frequent, e.g. (cid:16)methyl(cid:17) (-CH3) is a meaningful
sub-term, but (cid:16)methy(cid:17) is not, although it is frequent too.
Thus, mining frequent subsequences results in much redun-
dant information. We extend two concepts from previous
work [28, 30] to remove redundant information:

De(cid:28)nition 4. Closed Frequent Subsequence: A fre-
quent subsequence s is in the set of closed frequent sub-
sequences CS,
i(cid:27) there does not exist a frequent super-
sequence s0 of s such that all the frequencies of s in s00 2 D
is the same as that of s0 in s00 for all s00 2 D. That is,
s 2 CS = fsjs 2 F S and @s0 2 F S such that s ` s0 ^ 8s00 2
D; F reqs0„s00 = F reqs„s00 g.

De(cid:28)nition 5. Maximal Frequent Subsequence: A fre-
quent subsequence s is in the set of maximal frequent sub-
sequences MS, i(cid:27) it has no frequent super-sequences, i.e.
s 2 M S = fsjs 2 F S and @s0 2 F S such that s ` s0g.

The example in Figure 3 demonstrates the set of Closed
Frequent Subsequences, CS, and Maximal Frequent Subse-
quences, M S. Given the collection D={abcde, abcdf, aba,
abd, bca} and F reqmin=2, support Dabcd={abcde, abcdf},
Dab={abcde, abcdf, aba, abd}, etc. The set of frequent sub-
sequence FS={abcd, abc, bcd, ab, bc, cd} has much redun-
dant information. CS={abcd, ab, bc} removes some redun-
dant information, e.g. abc/bcd/cd only appears in abcd.
MS={abcd} removes all redundant information as well as
useful information, e.g., ab and bc have occurrences exclud-
ing those in abcd. Thus, we need to determine if ab and bc
are still frequent excluding occurrences in abcd. For a fre-
quent sequence s0, its subsequence s ` s0 is also frequent
independently, only if the number of all the occurrences of
s not in any occurrences of s0 is larger than F reqmin. If a
subsequence s has more than one frequent super-sequences,
then all the occurrences of those super-sequences are ex-
cluded to count the independent frequency of s, IF reqs.

738WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, ChinaAlgorithm: IFSM(D,D8s,O8s,F reqmin,Lmin):
Input:
Candidate set of sequences D,
set D8s including the support Ds for each subsequence s,
set O8s including all Occurs2D,
minimal threshold value of frequency F reqmin, and
minimal length of subsequence Lmin.
Output:
Set of Independent Frequent Subsequences IS, and
Independent Frequency IF reqs of each s 2 IS.
1. Initialization: IS = f;g, length l = maxs(Ls).
2. while l ‚ Lmin, do
3.

F reqs„s0 ‚ F reqmin into Set S;

put all s 2 D, Ls = l,
Ps02Ds
while 9s 2 S; Ps02Ds

F reqs„s0 ‚ F reqmin, do

for each Occurs00„s0 \ ([s„s0 Occurs„s0 )*6= ;,

move s with the largest F reqs„s0 from S to IS;
for each s0 2 Ds

4.
5.
6.
7.
8.
9.
10.return IS and IF reqs2IS = F reqs;
*[s„s0 Occurs„s0 is the range of all Occurs„s0 in s0, except
Occurs„s0 \ Occurt„s0 6= ; ^ t 2 IS.

F reqs00„s0 ¡ ¡;

l ¡ ¡;

Figure 2: Algorithm 1: Independent Frequent Sub-
sequences Mining

Input Sequences: D = fabcde; abcdf; aba; abd; bcag,
Parameters: F reqmin = 2, Lmin = 2
l = 5, F reqabcde = F reqabcdf = 1, IS = ;;
l = 4, F reqabcd = 2, IS = fabcdg; now for s = abc=bcd=
ab=bc=cd, F reqs„abcde ¡ ¡ = 0, F reqs„abcdf ¡ ¡ = 0;
for s = cde=de, F reqs„abcde ¡ ¡ = 0;
for s = cdf =df , F reqs„abcdf ¡ ¡ = 0;
l = 3, all F req < 2, IS = fabcdg;
l = 2, F reqab = F reqab„aba + F reqab„abd = 2, but F reqbc =
F reqbc„bca = 1, so IS = fabcd; abg;
Return: IS = fabcd; abg, IF reqabcd = 2 & IF reqab = 2.

Figure 3: Illustration of Independent Frequent Sub-
sequence Mining

super-sequence (cid:16)methyl(cid:17). In CS, (cid:16)ethyl(cid:17) occurs 180 times,
since for each occurrence of (cid:16)methyl(cid:17), a (cid:16)ethyl(cid:17) occurs. Thus,
CS over-estimates the probability of (cid:16)ethyl(cid:17). If a bias exists
while estimating the probability of sub-terms, we cannot ex-
pect a good text segmentation result using the probability
(next section).

Based on these observations, we propose an algorithm for
Independent Frequent Subsequence Mining (IFSM) from a
collection of sequences in Figure 2 with an example in Fig-
ure 3. This algorithm considers sequences from the longest
sequence s to the shortest sequence, checking if s is frequent.
If F reqs ‚ F reqmin, put s in IS, remove all occurrences of
its subsequences that are in any occurrences of s, and re-
move all occurrences overlapping with any occurrences of
s. If the left occurrences of a subsequence s0 still make s0
frequent, then put s0 into IS. After mining independent
frequent sub-terms, the discovered sub-terms can used as
features in CRFs, and the independent frequencies IF req
can be used to estimate their probabilities for hierarchical
text segmentation in next section.
3.3 Hierarchical Text Segmentation

We propose an unsupervised hierarchical text segmenta-
tion method that (cid:28)rst uses segmentation symbols to seg-
ment chemical names into terms (HTS in Figure 5), and
then utilizes the independent frequent substrings discovered
for further segmentation into sub-terms (DynSeg in Figure
6). DynSeg (cid:28)nds the best segmentation with the maximal
probability that is the product of probabilities of each sub-
string under the assumption that all substrings are indepen-
dent. After mining the independent frequent substrings to
estimate the independent frequency of each substring s, the
algorithm estimates its probability by

P (s) =

IF reqs

Ps02IS IF reqs0

/ IF reqs:

For each sub-term t with Lt = m, a segmentation

seg(t) =< t1; t2:::; tm >!< s1; s2:::; sn >

(5)

(6)

Thus, in D, abcd is frequent and ab is frequent indepen-
dently, but bc is not, since ab occurs twice independently,
but bc only once independently. Thus, we get a new set
of independent frequent subsequences fabcd; abg. We de(cid:28)ne
independent frequent subsequences below:

is to cluster adjacent tokens into n subsequences, where n =
2 for hierarchical segmentation. This is the same as the case
that the number of two is applied for hierarchical clustering
without prior knowledge of cluster numbers at each level
[31]. The probability of segmentation is

De(cid:28)nition 6. Independent Frequent Subsequence: A
frequent subsequence s is in the set of independent frequent
subsequences IS, i(cid:27) the independent frequency of s, IF reqs,
i.e. the total frequency of s, excluding all the occurrences
of its independent frequent super-sequences s0 2 IS, is at
least F reqmin. That is s 2 IS = fsjs 2 F S and IF reqs ‚
F reqming, where IF reqs = Ps002D #Occurs„s00 ; 8s0 2 IS;
8s00 2 D; @Occurs0„s00 \ Occurs„s00 6= ; and #Occurs„s00 is
the number of unique occurrences of s in s00.

Why is computing CS or MS not su(cid:30)cient for our ap-
plication? Consider the case of the sub-terms (cid:16)methyl(cid:17) and
(cid:16)ethyl(cid:17). Both are independent frequent subsequences in chem-
ical texts, but not a closed or maximally frequent subse-
quence. For example, for the chemical name in Figure 4,
(cid:16)methyl(cid:17) occurs twice and (cid:16)ethyl(cid:17) occurs once independently.
Assume in the collection of names D, (cid:16)methyl(cid:17) occurs 100
times, while (cid:16)ethyl(cid:17) occurs 80 times independently. In this
case, (cid:16)ethyl(cid:17) is not discovered in MS since it has a frequent

P (seg(t)) = Y

P (si);

i2[1;n]

and the corresponding log-likelihood is

L(seg(t)) = X

log(P (si))):

i2[1;n]

(7)

(8)

Maximum likelihood is used to (cid:28)nd the best segmentation,
(9)

seg(t) = argmaxseg(t) X

log(P (si)):

i2[1;n]

DynSeg uses dynamic programming in text segmentation
[24] (Figure 6) for optimization to maximize the log-likelihood.

In practice, instead of segmenting text into n parts di-
rectly, usually hierarchical segmentation of text is utilized
and at each level a text string is segmented into two parts.
This is due to two reasons: 1) It is di(cid:30)cult to (cid:28)nd a proper n,
and 2) A text string usually has hierarchical semantic mean-
ings. For example, (cid:16)methylethyl(cid:17) is segmented into (cid:16)methyl(cid:17)

739WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, ChinaAlgorithm: DynSeg(t,IF ,r,n):
Input:
A sequence t =< t1; t2:::; tm >,
a set of independent frequent strings IF with corresponding
independent frequency IF reqs02IF ,
the tree root r, and
the number of segments n.
Output:
The tree root r with a tree representation of s.
1. if Ls = 1 return;

2. Compute all log(IF reqsi ) = log(IF req<tj ;tj+1;:::tk >);

1 • j < k • m, where si =< tj ; tj+1; :::tk >
is a subsequence of t.

3. Let M (l; 1) = log(IF req<t1;t2;:::;tl>), where 0 • l • m.

Then M (l; L) = maxd(M (d; L ¡ 1)+
log(IF req<td+1;td+2:::;tl>)), where L < n and 0 • d • l.
Note log(IF req<tj ;tj+1:::;tk >) = 0, if j > k.

4. M (m; n) = maxd(M (d; n¡1)+log(IF req<td+1;td+2:::;tl>));

where 1 • d • l.

5. segment s into subsequences < s1; s2:::; sn >
using the corresponding seg(t) for M (m; n).

6. if only one s0 2 fs1; s2:::; sng 6= ; return;
7. put each s0 2 fs1; s2:::; sng ^ s0 6= ; in a child node

r0 2 fr1; r2:::; rng of r;

8. for each subsequence s0 in r0 do
9.

DynSeg(s0,IF ,r0,n);

Figure 6: Algorithm 3: Text Segmentation by Dy-
namic Programming

e.g. (cid:16)methyl(cid:17) (-CH3) should not be returned when search-
ing for chemical names containing (cid:16)ethyl(cid:17) (-C2H5). These
indexed tokens can support similarity searches and most
meaningful substring searches as shown in the experiment
results in Section 5.4. As mentioned in Subsection 3.3, typ-
ically, users use meaningful substring searches. Thus, for a
chemical name string, (cid:16)methylethyl(cid:17), indexing (cid:16)methyl(cid:17) and
(cid:16)ethyl(cid:17) is enough, while (cid:16)hyleth(cid:17) is not necessary. Hence, af-
ter hierarchical text segmentation, the algorithm needs to
index substrings at each node on the segmentation tree.
This reduces the index size tremendously (in comparison
with indexing all possible substrings), and our index scheme
allows the retrieval of most of the meaningful substrings. If
only strings at the high levels are indexed, then nothing is
returned when searching for string at lower levels. If only
strings at the lowest levels are indexed, too many candidates
are returned for veri(cid:28)cation. Thus, at least, we need to in-
dex strings at several appropriate levels. Since the number
of all the strings on the segmentation tree is no more than
twice of the number of leaves, indexing all the strings is a
reasonable approach.
4.2 Query Models and Ranking Functions

Features based on selected subsequences (substrings in
names and partial formulae in formulae) should be used as
tokens for search and ranking. Extending our previous work
[25], we propose three basic types of queries for chemical
name search: exact name search, substring name search, and
similarity name search. Usually only exact name search and
substring name search are supported by current chemistry
databases [2]. We have designed and implemented novel
ranking schemes by adapting term frequency and inverse
document frequency, i.e. TF.IDF, to rank retrieved chemical
entities, i.e., names and formulae.

Figure 4: An Example of Hierarchical Text Segmen-
tation

Algorithm: HTS(s,IF ,P ,r):
Input:
A sequence s,
a set of independent frequent strings IF with corresponding
independent frequency IF reqs02IF ,
a set of natural segmentation symbols with priorities P , and
the tree root r.
Output:
The tree root r with a tree representation of s.
1. if s has natural segmentation symbols c 2 P
2.

segment s into subsequences < s1; s2:::; sn >
using c with the highest priority;
put each s0 2 fs1; s2:::; sng in a child node
r0 2 fr1; r2:::; rng of r;
for each subsequence s0 in r0 do

HTS(s0,IF ,P ,r0);

3.

4.
5.
6. else if Ls > 1
7.
8. else return;

DynSeg(s,IF ,r,2);

Figure 5: Algorithm 2: Hierarchical Text Segmen-
tation

and (cid:16)ethyl(cid:17), and then (cid:16)methyl(cid:17) into (cid:16)meth(cid:17) and (cid:16)yl(cid:17), (cid:16)ethyl(cid:17)
into (cid:16)eth(cid:17) and (cid:16)yl(cid:17), where (cid:16)meth(cid:17) means (cid:16)one(cid:17), (cid:16)eth(cid:17) means
(cid:16)two(cid:17), and (cid:16)yl(cid:17) means (cid:16)alkyl(cid:17).

4. MOLECULE INFORMATION

INDEXING AND SEARCH
We discuss two issues in this section:

indexing schemes
and query models with corresponding ranking functions. For
indexing schemes, we focus on how to select tokens for in-
dexing instead of indexing algorithms.
4.1

Index Construction and Pruning

Previous work has shown that small indexes that (cid:28)t into
the main memory have much better query response times
[7]. We propose two methods of index pruning for the chem-
ical name and formula indexing respectively. For chemical
formula indexing, we proposed the strategy in [25].

For chemical name indexing, a naive way of index con-
struction for substring search is to index each character
and its position, and during substring search, like phrase
search in information retrieval, chemical names containing
all the characters in the query string are retrieved and ver-
i(cid:28)ed whether the query substring appears in the returned
names. Even though the index size is quite small using this
approach, it is not used due to two reasons: 1) Each charac-
ter has too many matched results to return for veri(cid:28)cation,
which increases the response time of queries; 2) a matched
substring may not be meaningful after text segmentation,

740WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, ChinaFreqmin=10
Freqmin=20
Freqmin=40
Freqmin=80
Freqmin=160

900

800

700

600

500

400

300

200

100

s
g
n
i
r
t
s
b
u
s
 
f
o
 
r
e
b
m
u
N

Freqmin=10
Freqmin=20
Freqmin=40
Freqmin=80
Freqmin=160

80

70

60

50

40

30

20

10

)
c
e
s
(
 
e
m

i

i
t
 
g
n
n
n
u
R

0
0

5

10
20
Length of substring
(a) Discovered Sub-terms

15

25

30

0
0

1

3

2
5
Sample size of terms

4

6

7
x 104

(b) Algorithm Running Time

Figure 7: Independent Frequent Subsequence Mining for
chemical names

Figure 8: Examples of Hierarchical Text Seg-
mentation. Strings in rectangles with bold
boundary lines have semantic meanings.

De(cid:28)nition 7. SF.IEF: Given the collection of entity C,
a query q and an entity e 2 C, SF (s; e) is the subsequence
frequency for each subsequence s „ e, which is the total
number of occurrences of s in e, IEF (s) is the inverse entity
frequency of s in C, and de(cid:28)ned as

SF (s; e) =

f req(s; e)

jej

; IEF (s) = log

jCj

jfejs „ egj

;

(10)

where f req(s; e) is the frequency of s in e, jej = Pk f req(sk; e)
is the total frequency of all selected subsequences in e, jCj
is the total number of entities in C, and jfejs „ egj is the
number of entities that contain subsequence s. An entity
refers to a chemical name or formula.

Exact name search

An exact name search query returns chemical names with

documents where the exact keyword appears.

Substring name search

Substring name searches return chemical names that con-
tain the user-provided keyword as a substring with docu-
ments containing those names. If the query string is indexed,
the results are retrieved directly. Otherwise, the query string
is segmented hierarchically, then substrings at each node are
used to retrieve names in the collection, and the intersection
of returned results are veri(cid:28)ed to check if the query are con-
tained. This happens rarely since most frequent substrings
are indexed. The ranking function of substring name search
regarding a query q and a name string e is given as

score(q; e) = SF (q; e)IEF (q)=pjej:

(11)

Similarity name search

Similarity name searches return names that are similar
to the query. However, the edit distance for similarity mea-
surement is not used for two reasons: 1) Computing edit
distances of the query and all the names in the data set is
computationally expensive, so a method based on indexed
features of substrings is much faster and feasible in prac-
tice. 2) Chemical names with similar structures may have
a large edit distance. Our approach is feature-based simi-
larity search, where substring features are used to measure
the similarity. We design a ranking function based on in-
dexed substrings, so that the query is processed and the
ranking score is computed e(cid:30)ciently. Similar to substring
search, (cid:28)rst a query string is segmented hierarchically, then
substrings at each node are used to retrieve names in the col-
lection, and scores are computed and summed up. Longer
substrings are given more weight for scoring, and scores of
names are normalized by their total frequency of substrings.

Table 1: The most frequent sub-terms at each
length, F reqmin = 160

String

Freq Meaning

tetramethyl

295

tetrahydro

trimethyl
dimethyl

285

441
922

String
hydroxy
methyl
ethyl
thio
tri
di

Freq Meaning
803
1744
1269
811
2597
4154

three
two

The ranking function is given as
score(q; e) = X

W (s)SF (s; q)SF (s; e)IEF (s)=pjej; (12)

s„q

where W (s) is the weight of s, de(cid:28)ned as the length of s.

The chemical formula searches are similar and presented
in our prior work [25]. Conjunctive searches of the basic
name/formula searches are supported, so that users can de-
(cid:28)ne various constraints to search for desired names or for-
mulae. When a user inputs a query that contains chemical
name and formula searches as well as other keywords, the
whole process involves two stages: 1) name/formula searches
are executed to (cid:28)nd desired names and formulae, and 2) re-
turned names and formulae as well as other keywords are
used to retrieve related documents. We use TF.IDF as the
ranking function in the second stage, and the ranking scores
of each returned chemical name/formula in the (cid:28)rst stage
are used as term weights to multiple the TF.IDF of each
term when computing the ranking score in the second stage.

5. EXPERIMENTAL EVALUATION

In this section, our proposed methods are examined. We

present and discuss corresponding experimental results.
5.1

Independent Frequent Subsequence
Mining and Hierarchical
Text Segmentation

Since our chemical name tagging uses data mining with
features of domain knowledge. Some features are based on a
chemical lexicon. To construct a lexicon of chemical names,
we collected 221,145 chemical names from multiple chemical
webpages and online databases. We evaluate our algorithm
of IFSM using this lexicon with di(cid:27)erent threshold values
F reqmin = f10; 20; 40; 80; 160g. We (cid:28)rst tokenize chemical
names and get 66769 unique terms. Then we discover fre-
quent sub-terms from them. The distribution of sub-term’s

741WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, Chinai

i

n
o
s
c
e
r
P

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5
0.65

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

i

i

n
o
s
c
e
r
P

0.9

0.95

1

0.5

0.5

HCRF
all features
no POS
no RULE
no POS+RULE
1.5

1
2.5
Feature boosting parameter q

2

HCRF
all features
no POS
no RULE
no POS+RULE
0.7
0.8

0.75

0.85

Recall

(a) Avg precision v.s. recall

(b) Average precision

1

0.95

0.9

0.85

0.8

0.75

0.7

l
l

a
c
e
R

3

0.65

0.5

HCRF
all features
no POS
no RULE
no POS+RULE

2

1.5

1
2.5
Feature boosting parameter q
(c) Average recall

1

0.95

0.9

0.85

0.8

0.75

0.7

e
r
u
s
a
e
m
−
F

3

0.65

0.5

HCRF
all features
no POS
no RULE
no POS+RULE
1.5

1
2.5
Feature boosting parameter q

2

(d) Average F-measure

Figure 9: Chemical formula tagging using di(cid:27)erent values of feature boosting parameter (cid:181)

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.4

all features
no subterm
no lexicon
no subterm+lexicon

0.5

0.6

Recall

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

i

i

n
o
s
c
e
r
P

0.7

0.8

0.9

0.2

0.5

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

l
l

a
c
e
R

3

0.4

0.5

all features
no subterm
no lexicon
no subterm+lexicon

1
2.5
Feature boosting parameter q

1.5

2

(a) Avg precision v.s. recall

(b) Average precision

1

0.9

0.8

0.7

0.6

0.5

0.4

e
r
u
s
a
e
m
−
F

3

0.3

0.5

all features
no subterm
no lexicon
no subterm+lexicon

1
2.5
Feature boosting parameter q

1.5

2

(d) Average F-measure

all features
no subterm
no lexicon
no subterm+lexicon
2

1.5

1
2.5
Feature boosting parameter q
(c) Average recall

Figure 10: Chemical name tagging using di(cid:27)erent values of feature boosting parameter (cid:181)

3

3

Table 2: Sentence tagging average accuracy

Method
CRF,(cid:181) = 1:0

Recall
78.75% 89.12% 83.61%

Precision

F

Table 3: Formula tagging average accuracy
F

Method
String Pattern Match
CRF,(cid:181) = 1:0
CRF,(cid:181) = 1:5
HCRF,(cid:181) = 1:0
HCRF,(cid:181) = 1:5

Precision
41.70%
96.02%

Recall
58.57%
98.38%
86.05%
90.76%
90.92% 93.79% 92.33%
88.63%
92.30%
93.09% 93.88% 93.48%

96.29%

lengths with di(cid:27)erent values of F reqmin are presented in
Figure 7 (a) and the run time of our algorithm is shown
in Figure 7 (b). Most discovered sub-terms have seman-
tic meanings in Chemistry. We show some of the frequent
sub-terms with their meanings in Table 1. After IFSM, hier-
archical text segmentation is tested using the same lexicon
and tagged chemical names from online documents, and ex-
amples of the results are shown in Figure 8.
5.2 Chemical Entity Tagging

The data to test tagging chemical entities is randomly se-
lected from publications crawled from the Royal Society of
Chemistry [3]. 200 documents are selected randomly from
the data set, and a part of each document is selected ran-
domly to construct the training set manually. The train-
ing set is very imbalanced because of the preponderance of
non-chemical terms. We (cid:28)rst apply CRF and 10-fold cross-
validation for sentence tagging. We manually construct the
training set by labeling each sentence as content (content of
the documents) or meta (document meta data, including ti-

tles, authors, references, etc.). Results for sentence tagging
are presented in Table 2. Then sentence tags can be used as
features for chemical entity tagging at the term level.

For chemical formula tagging, we use 10-fold cross-validation

and a 2-level HCRF, where the two levels are the sentence
level and the term level. The sentence tags are used as fea-
tures for chemical formula tagging. At the term level, we
label each token as a formula or a non-formula. Several
methods are evaluated, including rule-based String Pattern
Match, CRFs with di(cid:27)erent feature sets, and HCRFs with
all features. Features are categorized into three subsets: fea-
tures using rule-based string pattern match (RULE), fea-
tures using part-of-speech tags (POS), and other features.
Four combinations are tested: (1) all features, (2) no POS,
(3) no RULE, and (4) no POS or RULE. Results of chemi-
cal formula tagging are shown in Table 3 and Figure 9. We
can see that the RULE features contribute more than the
POS features and HCRF has the highest contribution. In
comparison to CRFs, HCRFs only has an improvement of
1:15%. However, since the total error rate is just 7:67%, the
improvement is about 15% of the total error rate of 7:67%.
This means long-dependence features at the sentence level
has positive contributions. HCRF has a better performance
than CRF, but increases the runtime. Both for HCRF and
CRF using all features, the best F-measure is reached when
(cid:181) = 1:5, and in this case recall and precision are balanced.
We show the results using all features in Table 3.

For chemical name tagging, since a name may be a phrase
of several terms, we label each token as a B-name (begin-
ning of a name), or I-name (continuing of a name), or a
non-name. We use 5-fold cross-validation and CRF with
di(cid:27)erent feature sets. Features are classi(cid:28)ed into three sub-
sets: features using frequent sub-terms (sub-term), features
using lexicons of chemical names and WordNet [4] (lexicon),

742WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, China0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

i

g
n
n
u
r
p

 
x
e
d
n

i
 
r
e

t
f

a

 

o

i
t

a
R

0

0

20

40

# substrings
index size
similarity search time

0.66

0.64

0.62

0.6

0.58

0.56

0.54

0.52

0.5

0.48

o

i
t

a
r
 

n
o

i
t

l

a
e
r
r
o
c
 

e
g
a
r
e
v
A

Freqmin=10
Freqmin=20
Freqmin=40
Freqmin=80
Freqmin=160

0.5

0.45

0.4

0.35

0.3

l

o
i
t
a
r
 
n
o
i
t
a
e
r
r
o
c
 
e
g
a
r
e
v
A

Freqmin=10
Freqmin=20
Freqmin=40
Freqmin=80
Freqmin=160

10

25

30

Top n retrieved chemical names

15

20

60

80

100

Values of Freqmin

120

140

160

0.46

0

5

10

15

20

Top n retrieved chemical names

25

30

0.25

0

5

(a) Similarity name search

(b) Substring name search

Figure 11: Ratio of after v.s.
before index pruning

Figure 12: Correlation of name search results before and after
index pruning

Table 4: Name tagging average accuracy
F

Precision

Method, (cid:181) = 1:0
all features
no sub-term
no lexicon
no sub-term+lexicon

Recall
76.15% 84.98% 80.32%
74.82% 85.59% 79.84%
79.11%
74.73%
73.00%
78.08%

84.05%
83.92%

and other features. Four combinations are tested: (1) all
features, (2) no sub-term, (3) no lexicon, and (4) no sub-
term or lexicon. We test di(cid:27)erent values {0.5, 0.75, 1.0, 1.5,
2.0, 2.5, 3.0} for the feature boosting parameter (cid:181) for the
formula (or B-name) class. Note that when (cid:181) = 1:0, it is
the normal CRF, while when (cid:181) < 1:0, the non-formula (or
non-name) class gets more preference. To measure the over-
all performance, we use F = 2P R=(P + R) [18], where P is
precision and R is recall, instead of using error rate, since it
is too small for imbalanced data. Results of chemical name
tagging are shown in Table 4 and Figure 10. We can see
using all features has the best recall and F-measure, and
using features of frequent sub-terms can increase recall and
F-measure but decrease precision. Our system clearly out-
performs the tool for chemical entity tagging: Oscar3 [5] on
the same datasets. Oscar3 has a recall of 70.1%, precision
of 51.4%, and the F-measure is 59.3%.

We only evaluate HCRFs for chemical formula tagging, be-
cause we found that ambiguous terms, like (cid:16)C(cid:17), (cid:16)He(cid:17), (cid:16)NIH(cid:17),
appearing frequently in parts containing document meta
data, have di(cid:27)erent probabilities to be chemical formulae.
For example,(cid:16)C(cid:17) is usually a name abbreviation in the part
of authors and the references, but usually refers to (cid:16)Car-
bon(cid:17) in the body of document. Thus, HCRFs is evaluated
for chemical formula tagging to check whether the long de-
pendence information at the sentence level is useful or not.
HCRFs are not applied for chemical name tagging, because
we found that the major source of errors in chemical name
tagging is not due to term ambiguity, but due to limitations
of the chemical dictionary and incorrect text tokenizing.
5.3 Chemical Name Indexing

For chemical name indexing and search, we use the seg-
mentation based index construction and pruning. We com-
pare our approach with the method using all possible sub-
strings for indexing. We use the same collection of chemi-
cal names in Section 5.1. We split the collection into two
subsets. One is for index construction (37,656 chemical
names), while the query names are randomly selected from

the other subset. Di(cid:27)erent values of the frequency thresh-
old F reqmin = f10; 20; 40; 80; 160g to mine independent fre-
quent substrings are tested. The experiment results in Fig-
ure 11 show that most (99%) substrings are removed after
hierarchical text segmentation, so that the index size de-
creases correspondingly (6% of the original size left).
5.4 Chemical Name Search

After index construction, for similarity name search, we
generate a list of 100 queries using chemical names selected
randomly: half from the set of indexed chemical names and
half from unindexed chemical names. These formulae are
used to perform similarity searches. Moreover, for substring
name search, we generate a list of 100 queries using mean-
ingful and the most frequent sub-terms with the length 3-10
discovered in Section 5.1. We also evaluated the response
time for similarity name search, illustrated in Figure 11.
The method using HTS only requires 35% of the time for
similarity name search compared with the method using all
substrings. However, we did not test the case where the in-
dex using all substrings requires more space than the main
memory. In that case, the response time will be even longer.
We also show that for the same query of similarity name
search or substring name search, the search result using
segmentation-based index pruning has a strong correlation
with the result before index pruning. To compare the corre-
lation between them, we use the average of the percentage
of overlapping results for the top n 2 [1; 30] retrieved formu-
lae that is de(cid:28)ned as Corrn = jRn \ R0
nj=n; n = 1; 2; 3; :::,
where Rn and R0
n are the search results of before and after
index pruning, respectively. Results are presented in Figure
12. We can observe that for similarity search, when more
results are retrieved, the correlation curves decrease, while
for substring search, the correlation curves increase. When
F reqmin is larger, the correlation curves decrease especially
for substring search.
5.5 Term Disambiguation

To test the ability of our approach for term disambigua-
tion in documents, we index 5325 PDF documents crawled
online. We then design 15 queries of chemical formulae.
We categorize them into three levels based on their ambi-
guity, 1) hard (He, As, I, Fe, Cu), 2) medium (CH4, H2O,
O2, OH, NH4 ), and 3) easy (Fe2O3, CH3COOH, NaOH,
CO2, SO2 ). We compare our approach with the traditional
approach by analyzing the precision of the returned top-
20 documents. The precision is de(cid:28)ned as the percentage
of returned documents really containing the query formula.

743WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, Chinai

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

Formula
Lucene
Google
Yahoo
MSN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

Formula
Lucene
Google
Yahoo
MSN

i

i

n
o
s
c
e
r
P

Formula
Lucene
Google
Yahoo
MSN

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

i

i

n
o
s
c
e
r
P

4

6

8

10

12

14

16

Top n retrieved documents
(a) Hard Queries

18

20

0

0

2

4

6

8

10

12

14

16

Top n retrieved documents
(b) Medium Queries

18

20

0.8

0

2

4

6

8

10

12

14

16

18

20

Top n retrieved documents
(c) Easy Queries

Figure 13: Average Precision in Document Search using Ambiguous Formulae

Lucene [1] is used for the traditional approach. We also eval-
uate generic search engines, Google, Yahoo, and MSN using
those queries to show the ambiguity of terms. Since the in-
dexed data are di(cid:27)erent, they are not comparable with the
results of our approach and Lucene. Their results illustrate
that ambiguity exists and the domain-speci(cid:28)c search engines
are desired. From Figure 13, we can observe 1) the ambigu-
ity of terms is very serious for short chemical formulae, 2)
results of Google and Yahoo are more diversi(cid:28)ed than that
of MSN, so that chemical web pages are included, and 3)
our approach out-performs the traditional approach based
on Lucene, especially for short formulae.

6. CONCLUSIONS AND FUTURE WORK
We evaluated CRFs for chemical entity extraction and
proposed an extended model HCRFs. Experiments show
that CRFs perform well and HCRFs perform better. Exper-
iments illustrate that most of the discovered sub-terms in
chemical names using our algorithm of IFSM have semantic
meanings. Examples show our HTS method works well. Ex-
periments also show that our schemes of index construction
and pruning can reduce indexed tokens as well as the index
size signi(cid:28)cantly. Moreover, the response time of similar-
ity name search is considerably reduced. Retrieved ranked
results of similarity and substring name search before and af-
ter segmentation-based index pruning are highly correlated.
We also introduced several query models for name searches
with corresponding ranking functions. Experiments show
that the heuristics of the new ranking functions work well.
In the future, a detailed user study is required to evalu-
ate the results. Entity fuzzy matching and query expansion
among synonyms will also be considered.
7. ACKNOWLEDGMENTS

We acknowledge partial support from NSF grant 0535656.

8. REFERENCES

[1] Apache lucene. http://lucene.apache.org/.
[2] Nist chemistry webbook. http://webbook.nist.gov/chemistry.
[3] Royal society of chemistry. http://www.rsc.org.
[4] Wordnet. http://wordnet.princeton.edu/.
[5] World wide molecular matrix.

http://wwmm.ch.cam.ac.uk/wikis/wwmm.

[6] S. Chakrabarti. Dynamic personalized pagerank in

entity-relation graphs. In Proc. WWW, 2007.

[7] E. S. de Moura, C. F. dos Santos, D. R. Fernandes, A. S. Silva,

P. Calado, and M. A. Nascimento. Improving web search
e(cid:30)ciency via a locality based static pruning method. In Proc.
WWW, 2005.

[8] L. Dehaspe, H. Toivonen, and R. D. King. Finding frequent

substructures in chemical compounds. In Proc. SIGKDD, 1998.

[9] D. Freitag and A. McCallum. Information extraction using

hmms and shrinkage. In AAAI Workshop on Machine
Learning for Information Extraction, 1999.

[10] M. Kuramochi and G. Karypis. Frequent subgraph discovery. In

Proc. ICDM, 2001.

[11] J. La(cid:27)erty, A. McCallum, and F. Pereira. Conditional random

(cid:28)elds: Probabilistic models for segmenting and labeling
sequence data. In Proc. ICML, 2001.

[12] D. L.Banville. Mining chemical structural information from the

drug literature. Drug Discovery Today, 11(1-2):35(cid:21)42, 2006.

[13] V. I. Levenshtein. Binary codes capable of correcting deletions,

insertions, and reversals. Soviet Physics Doklady, 1966.

[14] W. Li and A. McCallum. Semi-supervised sequence modeling

with syntactic topic models. In Proc. AAAI, 2005.

[15] G. Luo, C. Tang, and Y. li Tian. Answering relationship queries

on the web. In Proc. WWW, 2007.

[16] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy

markov models for information extraction and segmentation. In
Proc. ICML, 2000.

[17] A. McCallum and W. Li. Early results for named entity

recognition with conditional random (cid:28)elds, feature induction
and web-enhanced lexicons. In Proc. CoNLL, 2003.

[18] R. McDonald and F. Pereira. Identifying gene and protein

mentions in text using conditional random (cid:28)elds.
Bioinformatics, 6(Suppl 1):S6, 2005.

[19] P. Murray-Rust, H. S. Rzepa, S. Tyrrell, and Y. Zhang.

Representation and use of chemistry in the global electronic
age. Org. Biomol. Chem., 2004.

[20] P. Murray-Rust, H. S. Rzepa, and M. Wright. Development of

chemical markup language (cml) as a system for handling
complex chemical content. New J. Chem., 2001.

[21] J. W. Raymond, E. J. Gardiner, and P. Willet. Rascal:

Calculation of graph similarity using maximum common edge
subgraphs. The Computer Journal, 45(6):631(cid:21)644, 2002.
[22] B. Settles. Abner: an open source tool for automatically
tagging genes, proteins, and other entity names in text.
Bioinformatics, 21(14):3191(cid:21)3192, 2005.

[23] D. Shasha, J. T. L. Wang, and R. Giugno. Algorithmics and

applications of tree and graph searching. In Proc. PODS, 2002.

[24] B. Sun, P. Mitra, H. Zha, C. L. Giles, and J. Yen. Topic

segmentation with shared topic detection and alignment of
multiple documents. In Proc. SIGIR, 2007.

[25] B. Sun, Q. Tan, P. Mitra, and C. L. Giles. Extraction and

search of chemical formulae in text documents on the web. In
Proc. WWW, 2007.

[26] W. J. Wilbur, G. F. Hazard, G. Divita, J. G. Mork, A. R.

Aronson, and A. C. Browne. Analysis of biomedical text for
chemical names: A comparison of three methods. In Proc.
AMIA Symp., 1999.

[27] P. Willet, J. M. Barnard, and G. M. Downs. Chemical similarity

searching. J. Chem. Inf. Comput. Sci., 38(6):983(cid:21)996, 1998.

[28] X. Yan and J. Han. Closegraph: Mining closed frequent graph

patterns. In Proc. SIGKDD, 2003.

[29] X. Yan, F. Zhu, P. S. Yu, and J. Han. Feature-based

substructure similarity search. ACM Transactions on Database
Systems, 2006.

[30] G. Yang. The complexity of mining maximal frequent itemsets

and maximal frequent patterns. In Proc. SIGKDD, 2004.

[31] Y. Zhao and G. Karypis. Hierarchical clustering algorithms for

document datasets. Data Mining and Knowledge Discovery,
2005.

744WWW 2008 / Refereed Track: Web Engineering - ApplicationsApril 21-25, 2008 · Beijing, China