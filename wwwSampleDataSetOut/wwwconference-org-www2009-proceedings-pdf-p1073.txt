Search shortcuts: Driving Users Towards Their Goals

Ranieri Baraglia

ISTI - CNR

Via G. Moruzzi 1
56124 Pisa, ITALY

r.baraglia@isti.cnr.it

Vreixo Formoso

University of A Coru√±a
Campus de Elvi√±a s/n
15017 A Coru√±a SPAIN
vformoso@udc.es

Fidel Cacheda

University of A Coru√±a
Campus de Elvi√±a s/n
15017 A Coru√±a SPAIN

Ô¨Ådel@udc.es

Raffaele Perego

ISTI - CNR

Via G. Moruzzi 1
56124 Pisa, ITALY

r.perego@isti.cnr.it

Victor Carneiro

University of A Coru√±a
Campus de Elvi√±a s/n
15017 A Coru√±a SPAIN

viccar@udc.es
Fabrizio Silvestri

ISTI - CNR

Via G. Moruzzi 1
56124 Pisa, ITALY

f.silvestri@isti.cnr.it

ABSTRACT
Giving suggestions to users of Web-based services is a com-
mon practice aimed at enhancing their navigation experi-
ence. Major Web Search Engines usually provide Sugges-
tions under the form of queries that are, to some extent,
related to the current query typed by the user, and the
knowledge learned from the past usage of the system. In this
work we introduce Search Shortcuts as ‚ÄúSuccessful ‚Äù queries
allowed, in the past, users to satisfy their information needs.
DiÔ¨Äerently from conventional suggestion techniques, our search
shortcuts allows to evaluate eÔ¨Äectiveness by exploiting a sim-
ple train-and-test approach. We have applied several Col-
laborative Filtering algorithms to this problem, evaluating
them on a real query log data. We generate the shortcuts
from all user sessions belonging to the testing set, and mea-
sure the quality of the shortcuts suggested by considering
the similarity between them and the navigational user be-
havior.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query for-
mulation, Search process

General Terms
Algorithms, Experimentation, Theory

Keywords
Search shortcut, model, evaluation

1.

INTRODUCTION

The main objective of a search engine is to help the user
fulÔ¨Åll his information need eÔ¨Éciently. Major search engines
usually provide suggestions in the form of queries that are
somehow related to the user information need, with the goal
to point the user in the right direction. Authors proposed
diÔ¨Äerent techniques to address this problem [1, 3]. The de-
sign of eÔ¨Äective and eÔ¨Écient algorithms for such suggestions
is a complex and challenging task, as well as the evalua-
tion of them.
In fact, human-based evaluation has been

Copyright is held by the author/owner(s).
WWW 2009, April 20‚Äì24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

traditionally used. Although very precise, its main incon-
venience is the non repeatability of the experiments, which
makes diÔ¨Écult an extensive comparison of such techniques.
In this work we introduce the Search Shortcut Discovery
Problem as a problem related with the use of query sug-
gestions in search engines, and the potential reductions ob-
tained in the user session length. We deÔ¨Åne an evaluation
methodology for this problem based on query logs that will
allow a straightforward, yet interesting, comparison of the
techniques that could be applied to this problem.

We have focused on the application of Collaborative Fil-
tering algorithms [2] to this problem, a technique based on
user preferences, that has been successful in several domains,
such as electronic commerce. Collaborative Filtering algo-
rithms can be divided in memory-based and model-based
approaches.

In order to apply these techniques to the query shortcuts
problem, we have to extract information from the query log
data. As the goal in the shortcuts problem is to recommend
queries for a given session, it seems reasonable to treat each
Query Session as a user and each Query as an item. Then,
the query ratings must be inferred from the information in
the query log. As a preliminary approach, in this work we
have given to the last query of a session a possitive rating
if the user has clicked on, at least, one result, or a nega-
tive rating otherwise. All remaining queries are considered
neutral.

2. SEARCH SHORTCUTS

The idea of Search Shortcuts can be easily explained with
a simple example. Suppose a (suÔ¨Éciently) high number of
users has queried the engine for q1, q2, q3, and Ô¨Ånally, after
asking for q4, they clicked on a result presented in the Search
Engine Result Page (SERP). We can assume that q4 is a
good query, since it was followed by a click. Therefore, we
can also consider q4 as relevant for users interested in topics
related to q1, q2, and q3. Whenever another user starts to
search for topics related to q1, q2, or q3, q4 will be proposed
as a shortcut. Obviously, the earlier a relevant shortcut is
shown during the user session, the more eÔ¨Äective it has to
be considered. Following this idea, a given algorithm can be
easily evaluated using the following function:

WWW 2009 MADRID!Poster Sessions: Wednesday, April 22, 20091073s`h`œÉt|¬¥ , œÉ|t

¬¥ =

P

n‚àítP

q‚ààh(œÉt|)

m=1

ÀÜq =`œÉ|t

¬¥

m

Àú f (m)

|h(œÉt|)|

(1)

Where:

‚Ä¢ œÉu =< q1 . . . qn > is a query session for a given user.
‚Ä¢ œÉt| is the head of œÉ up to t ‚â§ n is the sequence of the

Ô¨Årst t queries in œÉ, i.e. œÉt| =< q1, . . . , qt >

‚Ä¢ œÉ|t is the tail of œÉ from t ‚â§ n is the sequence of the

last n ‚àí t queries in œÉ, i.e. œÉ|t =< qt+1, . . . , qn >

‚Ä¢ h : S ‚Üí 2Q is the k-way shortcut function, taking
as argument a session and returning a set of queries of
cardinality less then k, i.e. |h (œÉ)| ‚â§ k.

‚Ä¢ f (m) is a monotonic increasing function.
‚Ä¢ The function [q = œÉm] = 1 if and only if the query q is

equal to the query œÉm.

3. EXPERIMENTS

As data set for our experiments we have used a subset
of the AOL query log, consisting of 3,558,412 records. To
perform the evaluation, we have divided our query log data
in two subsets: training and evaluation, by randomly chosing
a percentage of the sessions as the data to be used to train
the algorithms. Then, we fed the algorithms with the Ô¨Årst
two queries from each session in the evaluation set. The
evaluation is performed using both traditional metrics and
the similarity measure proposed in section 2.

The results, as can be seen in Fig. 1, clearly show the
diÔ¨Äerences between Memory- and Model-Based approaches
when applied to this problem. Memory-Based algorithms
showed pretty accurate results, but they cover a low fraction
of the data, thus resulting useless for the search shortcut
problem.

Results on other algorithms are a bit surprising, but we
can easily relate such results with both the limitations of
traditional metrics, and the way we are implicitly assign-
ing ratings to queries. Regarding this last issue, the simple
three-rating schema we have proposed leads to a rating ma-
trix where most queries have a neutral rating, thus biasing
the evaluation, specially on the MAE metric. On the other
hand, limitations on evaluation methodology are clearly ob-
served on classiÔ¨Åcation and rank accuracy metrics, where
all algorithms have obtained modest results. This is a prob-
lem related with both the sparsity and high volume of the
dataset, and the way these metrics are measured in an oÔ¨Ñine
dataset.

Anyway, by taking into account these limitations, we can
note the pretty good results of the Item-Mean, Trends-Based
and Personality Diagnosis. This shows that both techniques
can obtain enough information from sparse data, so further
experiments on these algorithms seem valuable.

4. CONCLUSIONS

In this work we have introduced the Search Shortcuts
problem, directly related with recommender systems and
query suggestion, and we have presented a well-deÔ¨Åned model
and evaluation framework, that allows the comparison and

Figure 1: Results in the predictive accuracy metrics
for several Collaborative Filtering algorithms.

evaluation of algorithms using an oÔ¨Ñine dataset. SpeciÔ¨Å-
cally, we have studied the application of Collaborative Fil-
tering techniques to the search shortcuts problem, evaluat-
ing several algorithms on a real query log data.

Our experiments have shown the limitations of traditional
algorithms, as the high volume and sparsity of the query log
data lead to poor results in most cases. SpeciÔ¨Åcally, tra-
ditional memory-based approaches present very low cover-
age, due to the fact that they are only based on a small
part of the information available. Traditional metrics and
evaluation methodologies have also shown some important
limitations. ClassiÔ¨Åcation accuracy metrics obtain valuable
results, but only if the details of the evaluation methodol-
ogy are taken into account. In particular, the usage of oÔ¨Ñine
and sparse datasets impose several limitations in the evalu-
ation, as items considered relevant by the algorithm cannot
be compared in many cases with real data.

Several limitations have also been observed in the way
used to extract information from the query log data. The
three level rating (positive, negative, neutral) does not per-
form as expected, especially because most queries are in
fact neutral. The way we map queries to items can also be
improved, by considering the terms that compose a query.
Techniques such as stemming or stopwords removal, can ef-
fectively reduce the data sparsity, and thus reduce the main
reason of the poor performance of most algorithms.

5. REFERENCES
[1] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza.

Improving search engines by query clustering. JASIST,
58(12):1793‚Äì1804, 2007.

[2] U. Shardanand and P. Maes. Social information

Ô¨Åltering: algorithms for automating ‚Äùword of mouth‚Äù.
In CHI ‚Äô95: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 210‚Äì217,
New York, NY, USA, 1995. ACM
Press/Addison-Wesley Publishing Co.

[3] Z. Zhang and O. Nasraoui. Mining search engine query

logs for query recommendation. In WWW ‚Äô06:
Proceedings of the 15th international conference on
World Wide Web, pages 1039‚Äì1040, New York, NY,
USA, 2006. ACM.

≈£≈£≈£≈£≈£,,9,WWW 2009 MADRID!Poster Sessions: Wednesday, April 22, 20091074