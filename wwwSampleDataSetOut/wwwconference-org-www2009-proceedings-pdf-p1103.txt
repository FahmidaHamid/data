Modeling Semantics and Structure of Discussion Threads∗

Chen Lin†, Jiang-Ming Yang‡, Rui Cai‡, Xin-Jing Wang‡, Wei Wang†, Lei Zhang‡

†School of Computer Science, Fudan University. {chen_lin, weiwang1}@fudan.edu.cn

‡Microsoft Research, Asia. {jmyang, ruicai, xjwang, leizhang}@microsoft.com

ABSTRACT
The abundant knowledge in web communities has motivated
the research interests in discussion threads. The dynamic
nature of discussion threads poses interesting and challeng-
ing problems for computer scientists. Although techniques
such as semantic models or structural models have been
shown to be useful in a number of areas, they are ineﬃ-
cient in understanding discussion threads due to the tempo-
ral dependence among posts in a discussion thread. Such de-
pendence causes that semantics and structure coupled with
each other in discussion threads. In this paper, we propose a
sparse coding-based model named SMSS to Simultaneously
Model Semantic and Structure of discussion threads.
Categories and Subject Descriptors
I.5.1 [Pattern Recognition]: Models - Statistical
General Terms
Algorithms, Experimentation
Keywords
Threaded discussion, sparse coding, reply reconstruction
1.

INTRODUCTION

Discussion threads have long been a popular option for
web users to exchange opinions and share knowledge, e.g.
thousands of web forum sites, mailing lists, chat rooms, and
so on. A discussion thread usually originated from a root
post by the thread starter. Fig. 1 gives an intuitive descrip-
tion of a thread1. It contains 7 posts. The ﬁrst post is a
piece of news about the release of “SilverLight 2.0”. Some
users comment on this post, i.e., the 2nd and 3rd posts are
about the “update time”; some users have further questions
and initiate sub-discussions, i.e., the 5th, 6th, and 7th posts
are about “Javascript communication”; others troll or com-
plain, i.e., the 4th post. As more users joining in and mak-
ing comments, the thread grows, forming a nested dialogue
structure as shown in the left part of Fig. 1. Furthermore,
discussion threads show rich complexity in the semantics.
Since users always response to others, previous posts aﬀect
later posts and cause the topic to drift in a thread. This is
shown in the right part of Fig. 1. The goal of this paper is
to model both the structure and semantics of a discussion
thread in a simultaneous way.

∗This work was done when the ﬁrst author visiting Microsoft
Research, Asia.
1http://developers.slashdot.org/comments.pl?sid=1000769
Copyright is held by the author/owner(s).
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

Figure 1: An example of the structure and semantics
of a discussion thread from Slashdot.
2. THE SMSS MODEL

expressed as a mixture of topics, as (cid:126)d(i) (cid:39)(cid:80)T

A discussion thread has the following four characteristics.
A discussion thread has several topics. Suppose
there are T topics and V words, the jth topic is described
as a distribution over the word space RV , as R is real num-
bers and (cid:126)x(j) ∈ RV , 1 ≤ j ≤ T . Then, each post (cid:126)d(i) is
· (cid:126)x(j),
where θ(i)
is the coeﬃcient of d(i) on topic (cid:126)x(j). To estimate
the topic space X = {(cid:126)x(1), . . . , (cid:126)x(T )}, in SMSS we minimize
the loss function (cid:107)D − XΘ(cid:107)2
F . Here the thread contains
L posts as D = { (cid:126)d(1), . . . , (cid:126)d(L)}; and the coeﬃcient matrix
Θ = {(cid:126)θ(1) . . . (cid:126)θ(L)}.

j=1 θ(i)

j

j

An individual post is related to a few topics. Al-
though one thread may contain several semantic topics, each
individual post usually concentrates on a limited number of
topics. Therefore, we assume (cid:126)θ(i) of each post is sparse and
introduce a regularizer (cid:107)(cid:126)θ(i)(cid:107)1 in SMSS.

larizer (cid:107)(cid:126)θ(i) −(cid:80)i−1

A post is related to its previous posts. Users usually
read current posts in a thread before they reply. Thus the
semantics of a reply post is related to its previous posts. In
SMSS we formally describe such reply structure as a regu-
k is the structural
coeﬃcient between the ith and kth posts. In other words,
(cid:126)θ(i) can be expressed as a linear combination of (cid:126)θ(k).

k · (cid:126)θ(k)(cid:107)2

F , where b(i)

k=1 b(i)

The reply relations are sparse.

In most situations,
users only intend to comment on one or two previous posts.
Again, in SMSS we introduce a regularizer to favor such
sparse structural coeﬃcients (cid:107)(cid:126)b(i)(cid:107)1.

Based on the above observations, the SMSS model is to
estimate the value of topic matrix X, the coeﬃcient matrix
Θ, and the structural coeﬃcients b for each post, by mini-

junkSilverlight 1.0JavascriptSilverlight 2.0 ReleasedAbout timeAs I still haven't installed Silverlight 1.0 or seen a site that requires it.Re:About timeSilverlight 1.0 should never have come out. Silverlight 1.0 vs Silverlight 2.0 is like comparing Flash to Flex...And nothing of valueJavascript communicationwas gained.We're looking for a replacement for canvas in IE. excanvas sucks. We could use flash, but the Javascriptflash interface is very slow...Re:Javascript communicationuse SVG, it IS XMLRe:Javascript communicationNo. SVG is no good for what we need. Also, its cross-browser support is actually poorer, and performance is abysmal.StructureSemanticsWWW 2009 MADRID!Poster Sessions: Wednesday, April 22, 20091103L(cid:88)

mizing the following loss function f :
(cid:107)(cid:126)θ(i)(cid:107)1

f = (cid:107)D − XΘ(cid:107)2

L(cid:88)

F + λ1

(cid:107)(cid:126)θ(i) − i−1(cid:88)

i=1

k=1

+λ2

i=1

k · (cid:126)θ(k)(cid:107)2
b(i)

F + λ3

L(cid:88)

i=1

(cid:107)(cid:126)b(i)(cid:107)1

(1)

M(cid:88)

Here, the optimization objective balances the four terms by
parameter λ1, λ2, and λ3. In this way, both the semantics
and the structure information are estimated simultaneously.
Furthermore, for a collection of M threads which shared the
same topics matrix X, we can optimize them together by
minimizing:

minimizeX,{Θ(t)},{(cid:126)bi}

f (n)(·)

(2)

The optimization problem is not jointly convex but can be
solved by iteratively minimizing the convex sub-problems.

n=1

3. APPLICATIONS

To demonstrate the eﬃciency of the proposed SMSS model,
we reconstruct the reply relationships among posts using the
semantics and structures estimated by SMSS.
3.1 Reply Reconstruction

Intuitively, posts with reply relations should have similar
terms. However, the term similarity is unreliable as posts
in discussion threads are usually very short. Our idea is
to integrate the revealed semantic topics and structure as
additional information in the similarity measure. Formally
the similarity of a given post j and a previous post i is the
combination of all the features, as:

sim(i, j) = sim( (cid:126)d(i), (cid:126)d(j)) + w1 · sim((cid:126)b(i),(cid:126)b(j))
+w2 · sim((cid:126)θ(i), (cid:126)θ(j))

(3)

Based on the similarity, we propose an approach to analyze
a thread with L posts. That is, for a new post we compute
the similarity between itself and all previous posts, rank the
similarity, choose the post with highest score as a candidate
parent.
In case that the candidate parent is not similar
enough to the new post, we assume this post initializes a
new discussion branch of the thread.
3.2 Experiment

In experiment, we adopt two forums, Apple Discussion2
and Slashdot3, as our data sources. These two forums are
carefully selected because they have provided clear reply re-
lations for evaluation. Since threads in Apple Discussion
are much shorter, we sample 2000 threads including 20000
posts. For Slashdot, we sample 100 threads which also con-
tains about 20000 posts. We manually write a wrapper to
parse these pages and extract the exact reply relations as
the ground truth. The evaluation metric is precision.

For comparison, we also adopt some naive methods such
as Nearest-Previous (NP), Reply-Root (RR), and Only Doc-
ument Similarity (DS). NP assigns each post to the nearest
previous post as the reply target; RR assigns each post to
the root post as the reply target; and DS assigns each post
to the post has most similar terms.

Moreover, we also compared our SMSS model with some
state-of-the-art models which can provide semantic topic
analysis, such as latent dirichlet allocation (LDA) [1] and

2http://discussions.apple.com/
3http://www.slashdot.org/

Table 1: Performance of reply reconstruction in all
posts v.s. high-quality posts

Method

Slashdot

Apple

All Posts Good Posts All posts Good Posts

NP
RR
DS
LDA
SWB
SMSS

0.021
0.183
0.463
0.465
0.463
0.524

0.012
0.319
0.643
0.644
0.644
0.737

0.289
0.269
0.409
0.410
0.410
0.517

0.239
0.474
0.628
0.648
0.641
0.772

LDA, (cid:126)θ(j)

the special words with background model (SWB) [2]. Sim-
ilar to Eq.3, we compute the post similarity by sim(i, j) =
sim( (cid:126)d(i), (cid:126)d(j))+w1·sim((cid:126)θ(i)
LDA) from LDA. While SWB
is an extension of LDA and allows words in documents to be
modeled as either originating from general topics, or from
post-speciﬁc “special” word distributions, or from a thread-
wide background distribution. We leverage both its topic
distribution (cid:126)θ(i)
SW B for
similarity computing, as sim(i, j) = sim( (cid:126)d(i), (cid:126)d(j)) + w1 ·
sim((cid:126)θ(i)

SW B and special-words distribution (cid:126)ψ(i)
SW B) + w2 · sim( (cid:126)ψ(i)

SW B, (cid:126)ψ(j)

SW B, (cid:126)θ(j)

SW B).

All the three methods (LDA, SWB, and SMSS) achieve
best average performance at w1 = 0.9 while the w2 is tuned
for SWB and SMSS respectively. The experiment results are
shown in Table 1.

From Table 1, we have four observations: (I) in Slashdot,
a certain number of posts reply to the thread root, few to the
nearest previous post; while in Apple Discussion, there are
almost equal number of posts replying to the nearest previ-
ous post and the root. This is because: discussion threads
in Apple Discussion follow a Question-Answering style. New
solutions and fresh questions in replies invoke a serial of dis-
cussions. However, threads in Slashdot are usually initial-
ized by a piece of news. Interesting aspects of the news and
brilliant replies arise branches of discussions. (II) SWB and
LDA show slight improvements to the baseline DS. This has
veriﬁed our assumption that topics are robust in modeling
the semantics; but topics are not capable enough to extract
reply relations. (III) In our experiment SWB achieves best
performance when w2 is very small. This is because posts
in threaded discussions are usually short.
It is very diﬃ-
cult to estimate a sound coeﬃcient for document speciﬁc
word distribution. (IV) SMSS demonstrates signiﬁcant im-
provement. The major diﬀerence between SMSS and former
approaches is that SMSS resolves the structure representa-
tion b(i) for post pi in each discussion thread. The best pa-
rameter of structural similarity is w2 = 0.9. This indicates
that, besides of semantic similarities, structure similarities
are more distinguishing in identifying reply relations. Fur-
thermore, we also analyze the performance for the posts of
diﬀerent quality. We deﬁne posts whose score is larger than
3 in Slashdot and the posts marked as “Helpful” or “Solved”
in Apple Discussion as good posts. The similarity based
methods have better performance for these posts with high
quality.
It makes sense since the posts with high quality
may cause more signiﬁcative replies.

4. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent

dirichlet allocation. Journal of Machine Learning
Research, 3(6):993–1022, 2003.

[2] C. Chemudugunta, P. Smyth, and M. Steyvers.

Modeling general and speciﬁc aspects of documents
with a probabilistic topic model. Advances in newral
information processing systems, 41(6):391–407, 1990.

WWW 2009 MADRID!Poster Sessions: Wednesday, April 22, 20091104