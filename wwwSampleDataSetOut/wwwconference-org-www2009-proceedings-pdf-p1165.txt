A Densitometric Analysis of Web Template Content

Christian Kohlschütter

L3S / Leibniz Universität Hannover

Appelstr. 9a, 30167 Hannover

Germany

kohlschuetter@L3S.de

ABSTRACT
What makes template content in the Web so special that we
need to remove it? In this paper I present a large-scale ag-
gregate analysis of textual Web content, corroborating sta-
tistical laws from the ﬁeld of Quantitative Linguistics.
I analyze the idiosyncrasy of template content compared to
regular “full text” content and derive a simple yet suitable
quantitative model.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Re-
trieval; G.3 [Probability and Statistics]: Distribution
Functions
General Terms
Theory, Experimentation, Measurement
Keywords
Content Analysis, Template Detection, Template Removal,
Web Page Segmentation, Noise Removal

1.

INTRODUCTION

In contrast to plain text, documents in the Web expose
some speciﬁc structural properties. A Web page’s text is not
limited to the actual “main content” but usually consists of
several additional segments. These provide further informa-
tion (site maps, headline lists, tables, “related article” links,
text-ads etc.) and are basically meant to augment the full-
text. This additional text provided seems only be partially
useful or probably even counterproductive for search and
classiﬁcation; the common solution to the problem is simply
erasing template content or at least ignoring it. However,
current approaches identify templates only heuristically or
by machine learning.

In this paper, the textual Web content of several large cor-
pora were subjected to a quantitative analysis. By deriving
a densitometric text model based upon techniques from the
ﬁeld of Quantitative Linguistics, it can be shown that the
text corpus exposes two fuzzy classes of text, covering full-
text and navigational information respectively. The propor-
tions of the two classes (in tokens) roughly is 1 : 2, whereas
“template text” can be divided in equal shares into these
classes (noisy, short navigational text hints vs.
frequently
used full-text).

Copyright is held by the author/owner(s).
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

2. THE BETA DISTRIBUTION MODEL

To understand the distinction between templates and main
content, a large-scale statistical classiﬁcation was performed
on the level of intra-document segments, under the assump-
tion that the segments are suﬃciently homogeneous (i.e.,
either template or main content). The analysis was con-
ducted on the representative Webspam UK-2007 dataset (on
the ham part, 356,437 out of 106 million crawled pages). As
a manual segmentation appears infeasible at corpus-scale,
the state-of-the-art BlockFusion segmentation algorithm was
employed, which utilizes the text density measure (b), re-
lating the number of tokens in a particular text block to
the occupied text “area” determined by word-wrapping the
character data at a ﬁxed line width wmax. It was shown that
the resulting block structure closely resembles a manual seg-
mentation [3].

Text density is a particularly useful measure when ana-
lyzing the Web’s quantitative structure. It does not depend
on the notion of “sentence”, which we could hardly deﬁne
for the Web’s content – many portions of text simply do
not contain sentences, nor anything meaningful that could
be separable by full stop (this is especially true for tem-
plate text). To reduce the impact of errors caused by a too
ﬁne-grained segmentation, the amount of text (= number of
tokens) contained in segments of a particular text density 
is examined at corpus-level. We can model this histograph-
ically by rounding: (cid:48)(b) = [(b)].

Figure 1 depicts the retrieved token-level count/density
distribution for the whole corpus. Apparently, two modal
scores are visible, at (cid:48) = 2 and (cid:48) = 12 respectively. This in-
dicates at least two classes of text within the corpus. The su-
perimposition of diﬀerent classes (“strata”) of text is known
in linguistics; from a theoretical perspective it may even be
the normal case [1]. To conﬁrm the presence of multiple
classes we need to ﬁnd a corresponding distribution func-
tion whose compound functions are already known in the
theory, e.g. the Beta distribution, which is the conjugate
prior of the binomial distribution.

In fact, an almost perfect ﬁt was achieved by combining
two beta distributions with a normal distribution (R2
. =
0.998, RMSE = 0.0021 for a1 = 68.03, b1 = 132.5; a2 =
4, 034, b2 = 54, 49; c = 0.015, d = 0.64; e = 78.87, f =
7.834, µ = 28.65, σ2 = 6.489; x scores (densities) have been
normalized by xnorm = 36 to [0 : 1] before ﬁtting):

f (x) = c · (d · fbeta(x, a1, b1) + (1 − d) · fbeta(x, a2, b2))

+ (1 − c) · ϕµ,σ2 (e · x + f )

(1)

WWW 2009 MADRID!Poster Sessions: Thursday, April 23, 20091165As the combination of three beta distributions, while adding

another parameter, resulted in a far less accurate ﬁt (R2 =
0.944, RM SE = 0.0031), I conclude that the distribution of
text densities can be divided into two fuzzy classes C1 and
C2; the transition from C1 to C2 follows the normal distribu-
tion, which means that for blocks with particular densities it
is rather undetermined to which class the contained text be-
longs. The distribution parameter d reveals that C1 roughly
covers one third of the tokens enclosed in the corpus and C2
covers two thirds; from Figure 1 we see that for 5 ≤ (cid:48) ≤ 10
the normal distribution dominates.

Rank

1
2
3
4
5
6
7
8
9
10

Term
sitemap
bookmark
accessibility

misc
skip

shipping

polls

aﬃliates
username

thu

ε

-0.33
-0.29
-0.29
-0.29
-0.28
-0.28
-0.28
-0.27
-0.27
-0.27

Term
spelled
thousands
temporarily
gave
tried
aimed
seem
eventually
unfortunately
obvious

ε

0.51
0.36
0.35
0.34
0.33
0.33
0.32
0.31
0.31
0.31

Table 1: The top-10 terms for π1 and π2

The resulting values are in the range of [−1; +1]; the ab-
solute score is the degree of typicality, the sign indicates
the direction of typicality (−1 means the term clearly be-
longs to C1, +1 states that the term clearly belongs to class
C2). In our setup, of the 2938 terms with w1∪2 ≥ 100, 589
terms (20%) expose a term typicality ε ≤ −0.05 (i.e., C1)
and 1255 terms (42.7%) a term typicality of ε ≥ +0.05 (i.e.,
C2). Table 1 shows the top-10 typical terms for C1 and C2
respectively. As one can see, C1 terms are very likely to ap-
pear in template blocks, whereas C2 terms are more likely
for full-text.

4. FULL STOP AND COMMON BLOCKS

To further conﬁrm the observed dichotomy of Web text,
two other features of “full text” and “template text” were con-
trasted with the text density distribution: 1. the presence
of full stop characters in the segment and 2. the frequency
of the segment. As full stops indicate complete sentences,
the number of tokens contained in segments with full-stop
should be much higher for C2 than for C1 (and vice versa).
Indeed, text density has a fairly high information gain for
predicting the occurrence of a full stop (0.711), which is
substantiated by a classiﬁcation accuracy of 91.4% using a
simple linear classiﬁer. Finally, the token-level distribution
of frequent templates [2] was analyzed (38,634 segments oc-
curing at least 10 times, representing 28% of all tokens).
A majority of the tokens in segments with (cid:48)(b) ≤ 5 are
contained frequent templates (63%). With high chance the
other 37% tokens in few-worded segments do also not de-
scribe the “main” conteent, and could be considered “tem-
plate” (23% of all tokens in the corpus). On the other hand,
many boilerplate templates contain full sentences ((cid:48)(b) ≥ 9;
20% of all tokens). While detecting these full-text templates
obviously requires global information (such as segment ﬁn-
gerprints etc.), identifying the low-density templates can be
performed on-the-ﬂy without corpus-level statistics, which
makes density-based template detection a clear choice as
an upstream ﬁlter within a sophisticated template removal
strategy.
5. REFERENCES
[1] Gabriel Altmann. Das Problem der Datenhomogenit¨at.

In Glottometrika 13. Brockmeyer, 1992.

[2] D. Gibson, K. Punera, and A. Tomkins. The volume

and evolution of web page templates. In WWW’05.
[3] Chr. Kohlsch¨utter and W. Nejdl. A Densitometric

Approach to Web Page Segmentation. In CIKM 2008.
[4] D. Lavalette. A general purpose ranking variable with
applications to various ranking laws. In Exact Methods
in the Study of Language and Text. 2007.

Figure 1: Density Distribution Model

3. TERM TYPICALITY

To make a statement on the meaning of the determined
two classes, the content of these classes, i.e., the term vocab-
ulary, needs to be analyzed. If the two classes are diﬀerent,
then the contained token vocabulary should also expose no-
ticeable diﬀerences. As we want to understand the peculiar-
ities of the two classes C1 and C2, which are roughly repre-
sented by the two partitions π1 ((cid:48) ≤ 8) and π2 ((cid:48) ≥ 9), the
partition-speciﬁc term document frequencies are compared.
We may expect that terms that are typical for C1 appear
much more often in π1 than in π2, and vice versa. I examine
this relationship by computing the corresponding document
frequency ratios. The normalized ratio follows a power law
distribution of the form y = c (x/(1− x))−a1 with a1 = 0.39
and c = 0.01 (R2 = 0.9468, RMSE = 0.0034). This type is a
generalization of Zipf’s law [4]. In our case, we can interpret
the ratio x/(1 − x) as the combination of two Zipﬁan sub-
sets, a top-ranked and a bottom-ranked one, which mutually
inﬂuence the curve. In fact the frequencies of the considered
terms apparently are Zipﬁan, too, and for both partitions
enough typical terms exist. To avoid over-interpreting the
impact of rarely occurring terms, the analysis is limited to
terms with a collection-wide document frequency w1∪2 of at
least 100. For these terms, I compute the term typicality
ε(t), which I deﬁne as the logarithmic ratio of the corre-
sponding document frequencies w1, w2 of the examined term
t in the two partitions. The ratio is normalized by the loga-
rithm to base N + 1 with N being the number of documents
in the corpus (i.e., the maximum document frequency):

ε(t) = logN +1

w2(t) + 1
w1(t) + 1

(2)

0510152025Text Density05x1061x1071.5x1072x107Number of wordsMeasuredClass 1 (Beta d.)Normal DistributionClass 2 (Beta d.)Complete FitWWW 2009 MADRID!Poster Sessions: Thursday, April 23, 20091166