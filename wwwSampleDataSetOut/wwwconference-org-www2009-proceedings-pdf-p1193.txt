Deriving Music Theme Annotations from User Tags

Kerstin Bischoff, Claudiu S. Firan, Raluca Paiu∗
L3S Research Center / Leibniz Universität Hannover

Appelstr. 9a

30167 Hannover, Germany

{bischoff,ﬁran,paiu}@L3S.de

ABSTRACT
Music theme annotations would be really beneﬁcial for sup-
porting retrieval, but are often neglected by users while an-
notating. Thus, in order to support users in tagging and to
ﬁll the gaps in the tag space, in this paper we develop algo-
rithms for recommending theme annotations. Our methods
exploit already existing user tags, the lyrics of music tracks,
as well as combinations of both. We compare the results for
our recommended theme annotations against genre and style
recommendations – a much easier and already studied task.
We evaluate the quality of our recommended tags against
an expert ground truth data set. Our results are promising
and provide interesting insights into possible extensions for
music tagging systems to support music search.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing; H.3.3 [Information Storage and
Retrieval]: Information Search and Retrieval

General Terms
Algorithms, Experimentation, Human Factors

Keywords
Collaborative Tagging, High-Level Music Descriptors, Theme
Tag Recommendations, Metadata Enrichment

1.

INTRODUCTION

Currently no available search engine supports music search
by sample music ﬁles, thus people are still constrained to
search for music using textual queries. In this context, sup-
porting users in providing meaningful tags for music tracks
becomes crucial – tags and other metadata (e.g. extracted
from ID3 tags) can be indexed and later be used to support
music search.
[1] discusses the problems arising from the
gaps existing between the types of tags employed by users for
tagging music data and the types of keywords used for mu-
sic search: theme-related words represent 5% of the tags for
songs, though when searching for music, 30% of the queries
are theme-related (e.g. “halloween party music”). One pos-
sibility to make users use keywords from the categories we
∗work supported by the PHAROS project funded by the
European Commission (FP6 - IST Contract No. 045035)

Copyright is held by the author/owner(s).
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

need is to unobtrusively recommend such tags and thus sup-
port them in the tagging process. Speciﬁcally, our goal is to
support users by recommending tags referring to usage con-
text information (“themes”). The “theme of a song” refers
to the context or situation which ﬁts best when listening to
the music track, e.g. at the beach, dinner ambiance, party
time, road trip, etc.Within an application recommendations
could be presented to the user, who can select the relevant
ones and add them to the music track. Recommended theme
tags can also be indexed to enrich the metadata for a speciﬁc
track and to automatically create theme-based playlists.

2. RELATED WORK

Several existing papers aim at automatically inferring ad-
ditional information from available content or (user gener-
ated) metadata. There are approaches that exploit user gen-
erated tags to recommend additional tags for pictures [6],
for blog posts [8] or personalized tags for Web pages [2]. For
music, enrichment recently focuses on deriving mood infor-
mation based on extracted accoustic data [3, 5, 7]. Estab-
lishing a basis for this, [4] studies the relationships between
moods and artists, genres and usage metadata. In contrast
to existing music metadata enrichment studies, we rely only
on social tags and no low-level features of the tracks.

3. DATA SET DESCRIPTION

For our experiments, we collected the AllMusic.com pages
covering music themes, genres and styles. We found 73
themes, 20 genres and 633 styles (more ﬁne-grained mu-
sic genre classes). From the pages corresponding to themes
/ genres / styles, we also gathered information related to
which music tracks fall into these categories. With this pro-
cedure, we ended up with 13,948 songs. Looking at the
songs identiﬁed in each of the categories, we have 1,164
track-theme, 1,521 track-genre, and 16,023 track-style as-
signments. For all these songs we also crawled the tags as-
sociated to these music tracks by users in Last.fm. We have
81,964 diﬀerent tags with their usage frequency. To investi-
gate whether lyrics can provide added value in the task of
theme and genre / style recommendation, we also obtained
the lyrics for the 13,948 tracks from www.lyricsdownload.com
and www.lyricsmode.com, if available.

4. RECOMMENDATION ALGORITHMS

For recommending themes, we base our solution on collab-
oratively created social knowledge – i.e. tags associated to
music tracks – extracted from Last.fm, as well as on lyrics

WWW 2009 MADRID!Poster Sessions: Friday, April 24, 20091193information. Based on already provided user tags, on the
lyrics of the music tracks, or on combinations of the two, we
built classiﬁers which try to infer other annotations corre-
sponding to themes appropriate for the songs. For compari-
son reasons, we additionally experiment with predictions for
music genres and styles. For building these classiﬁers, we use
the open source machine learning library Weka1. In the ex-
periments presented in this paper, we use the Na¨ıve Bayes
Multinomial implementation available in Weka. We have
one classiﬁer trained for the whole available set of classes
(i.e. either for themes or genres or styles). This classiﬁer
produces for every song in the test set a probability dis-
tribution over all classes (e.g. over all themes). Thus, one
or more classes (based on probabilities or on a given rank
number) can be then assigned to each song.

We experiment with three types of input features for the
diﬀerent sets of classiﬁers: (1) tags; (2) words from lyrics;
or (3) tags and words from lyrics. Depending on the type of
features used to train the classiﬁer and on the type of class
that the classiﬁer will assign to songs, we propose 9 methods
(3 types of output classes – themes, genres, styles – and 3
types of features – tags, lyrics, tags+lyrics). Each of these
algorithms uses a diﬀerent number of input features, as the
sets of AllMusic songs having theme, genre or style labels do
not overlap perfectly. We experimented with feature selec-
tion (e.g. Information Gain) but results showed that the full
set, though introducing noise, is better suitable for learning.
As we need a certain amount of input data in order to
be able to consistently train the classiﬁers, we discard those
classes having less than 30 songs assigned. For each of the
three types of classes, a classiﬁer learns a model based on
the presented features. Then the model is applied to any
new, unseen data. We can choose how many tags are rec-
ommended to the user based on the probabilities resulted
from the classiﬁcation or by setting an absolute threshold.

5. EVALUATION & CONCLUSIONS

The evaluation we perform aims to automatically mea-
sure the quality of our tag prediction algorithms. As ground
truth data we use the AllMusic.com data set, on which we
perform 10-fold cross-validation. Given a Last.fm music
track, we predict possible theme / genre / style annotations
and compare our output against the manual assignments of
AllMusic experts for the same song. We present the results
for all our experimental runs in Table 1. The evaluation
metrics we analyze are Hit-Rate at rank k (H@3, H@5),
showing whether a good descriptive tag is contained among
the top-k recommended tags; R-Precision (RP ), precision
at the total number of relevant tags; and Mean Reciprocal
Rank (M RR). We concentrate on the H@3 metric, as we
recommend three annotations to the users to choose from.
We consider three annotations a good compromise, provid-
ing enough suggestions and at the same time not overwhelm-
ing the users with too much information.

We observe that the best performing methods are those
using tags as input features for the classiﬁers. When com-
bining tags and lyrics as features, the corresponding meth-
ods perform much better than those based only on lyrics
and they sometimes also slightly outperform the tag-based
methods. Lyrics, in contrast to tags, introduce noise, as
many song texts contain all sorts of interjections (e.g. “hey”,

1http://www.cs.waikato.ac.nz/~ml/weka

Table 1: Overall experimental results

Features

Lyrics
Tags+Lyrics

e Tags
m
e
h
T
e Tags
r
n
e
G
e Tags
l
y
t
S

Lyrics
Tags+Lyrics

Lyrics
Tags+Lyrics

H@3 H@5 RP MRR
0.80
0.56
0.80
0.97
0.85
0.93
0.76
0.22
0.62

0.49
0.26
0.48
0.83
0.60
0.76
0.48
0.10
0.37

0.92
0.72
0.94
0.98
0.93
0.98
0.85
0.29
0.72

0.67
0.46
0.67
0.91
0.75
0.86
0.65
0.21
0.54

“oh”, “uh-huh”, etc.), slang or simply informal English. As
expected, the best results we obtain are for the genre-tag
recommendations: H@3 of 0.97 for the case of tags as fea-
tures. Styles do not perform as good as genres (H@3 of
0.76), mostly due to the fact that the AllMusic labels are
too ﬁne-grained to clearly distinguish between them. Given
the diﬃculty of agreeing on a single, appropriate music genre
taxonomy, some of these ﬁne distinctions may also be worth
discussing. For the case of theme recommendations, the best
results, H@3 of 0.80, are achieved for the algorithm both us-
ing only tags or a combination of tags and lyrics as features.
The results indicate a good performance of our algorithms
in correctly recommending themes or genre / style annota-
tions. Given the self-reinforcing nature of user generated
tags, the set of recommended theme tags will not only enrich
our future training set for learning, but will probably also
enable fully automatic theme tag assignment without user
interaction. Using our approach, music becomes searchable
by associated themes, providing a ﬁrst step towards eﬀec-
tively searching music by textual descriptive queries.

For the future we plan to investigate this issue further,
as well as mood classiﬁcation using metadata and how to
improve feature selection by automatic identiﬁcation of tag
types (e.g. Topic, Author, Usage context). Merging our ap-
proach with content-based methods trying to solve the same
task is also worth examining.

6. REFERENCES
[1] K. Bischoﬀ, C. S. Firan, W. Nejdl, and R. Paiu. Can all

tags be used for search? In CIKM, 2008.

[2] A. Byde, H. Wan, and S. Cayzer. Personalized tag

recommendations via tagging and content-based
similarity metrics. In ICWSM, 2007.

[3] Y. Feng, Y. Zhuang, and Y. Pan. Popular music

retrieval by detecting mood. In SIGIR, 2003.

[4] X. Hu and J. S. Downie. Exploring mood metadata:

Relationships with genre, artist and usage metadata. In
ISMIR, 2007.

[5] D. Liu, L. Lu, and H.-J. Zhang. Automatic mood

detection from acoustic music data. In ISMIR, 2003.

[6] B. Sigurbj¨ornsson and R. van Zwol. Flickr tag

recommendation based on collective knowledge. In
WWW, 2008.

[7] J. Skowronek, M. McKinney, and S. van de Par. A

demonstrator for automatic music mood estimation. In
ISMIR, 2007.

[8] S. Sood, S. Owsley, K. Hammond, and L. Birnbaum.

Tagassist: Automatic tag suggestion for blog posts. In
ICWSM, 2007.

WWW 2009 MADRID!Poster Sessions: Friday, April 24, 20091194