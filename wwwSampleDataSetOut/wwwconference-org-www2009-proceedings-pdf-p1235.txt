Bootstrapped Extraction of Class Attributes

∗
Joseph Reisinger

University of Texas at Austin
joeraii@cs.utexas.edu

Austin, TX

ABSTRACT
As an alternative to previous studies on extracting class attributes
from unstructured text, which consider either Web documents or
query logs as the source of textual data, A bootstrapped method
extracts class attributes simultaneously from both sources, using a
small set of seed attributes. The method improves extraction preci-
sion and also improves attribute relevance across 40 test classes.
Categories and Subject Descriptors
I.2.7 [Computing Methodologies]: Natural Language Processing
General Terms
Algorithms, Experimentation
1. EXTRACTION OF ATTRIBUTES
Motivation: Class attributes capture quantiﬁable properties (e.g.,
hiking trails, entrance fee and elevation), of given classes of in-
stances (e.g., NationalPark), and thus potentially serve as a skele-
ton towards constructing large-scale knowledge bases automatically.
Previous work on extracting class attributes from unstructured text
consider either Web documents [5] or query logs [2] as the extrac-
tion source. In this poster, we develop Bootstrapped Web Search
(BWS), a method for combining Web documents and query logs as
textual data sources that may contain class attributes. Web docu-
ments have textual content of higher semantic quality, convey in-
formation directly in natural language rather than through sets of
keywords, and contain more raw textual data. In contrast, search
queries are usually ambiguous, short, keyword-based approxima-
tions of often-underspeciﬁed user information needs. Previous work
has shown, however, that extraction from query logs yields signiﬁ-
cantly higher precision than extraction from Web documents [2].

BWS is a generic method for multiple-source class attribute ex-
traction that allows for corpora with varying levels of extraction
precision to be combined favorably. It requires no supervision other
than a small set of seed attributes for each semantic class. We test
this method by combining query logs and Web documents, lever-
aging their strengths in order to improve coverage and precision.
Combining Multiple Data Sources: Signiﬁcant previous work
has been done on attribute extraction across a wide variety of data
sources, e.g. news reports, query logs and Web documents. If ex-
traction from such domains yields high precision results, intuitively
it should be possible to obtain even more accurate attributes while
lowering bias by using a combination of data sources.
∗Contributions made during an internship at Google.

Copyright is held by the author/owner(s).
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

Marius Pa¸sca

Google, Inc.

Mountain View, CA

mars@google.com

Figure 1: Overview of the Web-search extraction procedure

def

We consider the general problem of extracting and ranking a set
of candidate attributes A using a ranked list of noisy seed attributes
S extracted from a different source. The only assumption placed
on S is that attribute precision correlates with rank on average.

= s−1Ps

In order to leverage noisy supervision, we introduce a simple
one-parameter smoothing procedure that builds on the assumption
that a seed’s rank is proportional to its precision. Candidate at-
tributes are ranked higher when they are more similar to a large
number of the most precise seed attributes. Intuitively, a candidate
attribute a ∈ A should be ranked highly overall if it has a high
average similarity to the most precise seed attributes.
Given A, the unordered set of extracted candidate attributes, and
S, the set of (noisy) ranked seed attributes extracted from a dif-
ferent source, each attribute a ∈ A is assigned a class-speciﬁc
i=0 Sim(a,Si), equal to its av-
extraction proﬁle, ρa(s)
erage similarity function Sim over the top s < |S| total seeds.
Given a set of candidate attributes and their associated extraction
proﬁles, a ranked list Aranked is constructed incrementally. At each
step s < |S|, the top α attributes ranked by their similarity to the
top s seeds, ρa(s), are added to Aranked, and removed from A.
Each evaluation of ρa(s) requires O(s) similarity calculations,
and hence O(|A||S|2) operations would be required to rank all can-
didate attributes. For efﬁciency we calculate ρa at a discrete set of
seed levels, δ = [5, 10, 20, 40, 60, 100].Aranked is then constructed
recursively, and at each step the top (δs−δs−1)/α attributes ranked
by ρ(δs) are added to Aranked (skipping duplicates). The parameter
α controls how strongly attribute ranking should prefer to empha-
size the number of seeds over the list rank; as α → ∞, more seeds
are used on average to score each attribute. Intuitively, this method
works because each seed level has a different peak where the best
results are obtained. By adjusting α, we can construct a list of at-
tributes with high precision across all ranks.
Extraction from Relevant Web Documents: As a speciﬁc appli-
cation of combining multiple textual data sources for attribute ex-
traction, we use attributes extracted from query logs in [3] as noisy
supervision for ﬁnding attributes in relevant Web documents. Web
documents relevant to a particular class are found by performing
search queries for each instance of that class and collecting the top

(cid:36)(cid:77)(cid:66)(cid:84)(cid:84)(cid:70)(cid:84)(cid:1)(cid:80)(cid:71)(cid:1)(cid:42)(cid:79)(cid:84)(cid:85)(cid:66)(cid:79)(cid:68)(cid:70)(cid:84)(cid:34)(cid:68)(cid:85)(cid:80)(cid:83)(cid:46)(cid:70)(cid:77)(cid:1)(cid:40)(cid:74)(cid:67)(cid:84)(cid:80)(cid:79)(cid:34)(cid:77)(cid:1)(cid:49)(cid:66)(cid:68)(cid:74)(cid:79)(cid:80)(cid:47)(cid:74)(cid:68)(cid:73)(cid:80)(cid:77)(cid:66)(cid:84)(cid:1)(cid:36)(cid:66)(cid:72)(cid:70)(cid:43)(cid:70)(cid:85)(cid:1)(cid:45)(cid:74)(cid:52)(cid:66)(cid:77)(cid:78)(cid:66)(cid:1)(cid:41)(cid:66)(cid:90)(cid:70)(cid:76)(cid:36)(cid:73)(cid:83)(cid:74)(cid:84)(cid:85)(cid:74)(cid:79)(cid:66)(cid:1)(cid:51)(cid:74)(cid:68)(cid:68)(cid:74)(cid:56)(cid:70)(cid:67)(cid:1)(cid:84)(cid:70)(cid:66)(cid:83)(cid:68)(cid:73)(cid:51)(cid:70)(cid:77)(cid:70)(cid:87)(cid:66)(cid:79)(cid:85)(cid:1)(cid:37)(cid:80)(cid:68)(cid:86)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:82)(cid:86)(cid:70)(cid:83)(cid:90)(cid:27)(cid:1)(cid:43)(cid:70)(cid:85)(cid:1)(cid:45)(cid:74)(cid:68)(cid:77)(cid:66)(cid:84)(cid:84)(cid:27)(cid:1)(cid:66)(cid:68)(cid:85)(cid:80)(cid:83)(cid:84)(cid:74)(cid:85)(cid:70)(cid:27)(cid:1)(cid:56)(cid:74)(cid:76)(cid:74)(cid:81)(cid:70)(cid:69)(cid:74)(cid:66)(cid:36)(cid:66)(cid:79)(cid:69)(cid:74)(cid:69)(cid:66)(cid:85)(cid:70)(cid:34)(cid:85)(cid:85)(cid:83)(cid:74)(cid:67)(cid:86)(cid:85)(cid:70)(cid:84)(cid:38)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:1)(cid:79)(cid:80)(cid:86)(cid:79)(cid:1)(cid:81)(cid:73)(cid:83)(cid:66)(cid:84)(cid:70)(cid:84)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:68)(cid:80)(cid:79)(cid:85)(cid:70)(cid:89)(cid:85)(cid:84)(cid:51)(cid:66)(cid:79)(cid:76)(cid:70)(cid:69)(cid:34)(cid:85)(cid:85)(cid:83)(cid:74)(cid:67)(cid:86)(cid:85)(cid:70)(cid:84)(cid:82)(cid:86)(cid:70)(cid:83)(cid:90)(cid:27)(cid:1)(cid:46)(cid:70)(cid:77)(cid:1)(cid:40)(cid:74)(cid:67)(cid:84)(cid:80)(cid:79)(cid:68)(cid:77)(cid:66)(cid:84)(cid:84)(cid:27)(cid:1)(cid:66)(cid:68)(cid:85)(cid:80)(cid:83)(cid:84)(cid:74)(cid:85)(cid:70)(cid:27)(cid:1)(cid:42)(cid:46)(cid:37)(cid:35)(cid:82)(cid:86)(cid:70)(cid:83)(cid:90)(cid:27)(cid:1)(cid:34)(cid:77)(cid:1)(cid:49)(cid:66)(cid:68)(cid:74)(cid:79)(cid:80)(cid:68)(cid:77)(cid:66)(cid:84)(cid:84)(cid:27)(cid:1)(cid:66)(cid:68)(cid:85)(cid:80)(cid:83)(cid:84)(cid:74)(cid:85)(cid:70)(cid:27)(cid:1)(cid:48)(cid:9359)(cid:68)(cid:74)(cid:66)(cid:77)(cid:1)(cid:39)(cid:66)(cid:79)(cid:1)(cid:52)(cid:74)(cid:85)(cid:70)(cid:68)(cid:66)(cid:83)(cid:70)(cid:70)(cid:83)(cid:81)(cid:73)(cid:80)(cid:85)(cid:80)(cid:84)(cid:68)(cid:73)(cid:66)(cid:85)(cid:1)(cid:83)(cid:80)(cid:80)(cid:78)(cid:81)(cid:74)(cid:68)(cid:85)(cid:86)(cid:83)(cid:70)(cid:84)(cid:36)(cid:80)(cid:78)(cid:81)(cid:86)(cid:85)(cid:70)(cid:1)(cid:84)(cid:74)(cid:78)(cid:74)(cid:77)(cid:66)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:85)(cid:80)(cid:1)(cid:84)(cid:70)(cid:70)(cid:69)(cid:84)(cid:68)(cid:66)(cid:83)(cid:70)(cid:70)(cid:83)(cid:66)(cid:72)(cid:70)(cid:66)(cid:88)(cid:66)(cid:83)(cid:69)(cid:84)(cid:73)(cid:70)(cid:74)(cid:72)(cid:73)(cid:85)(cid:67)(cid:74)(cid:83)(cid:85)(cid:73)(cid:81)(cid:77)(cid:66)(cid:68)(cid:70)(cid:78)(cid:80)(cid:87)(cid:74)(cid:70)(cid:1)(cid:83)(cid:80)(cid:77)(cid:70)(cid:84)(cid:112)(cid:77)(cid:78)(cid:80)(cid:72)(cid:83)(cid:66)(cid:81)(cid:73)(cid:90)(cid:81)(cid:83)(cid:80)(cid:112)(cid:77)(cid:70)WWW 2009 MADRID!Poster Sessions: Friday, April 24, 20091235documents returned. This approach is novel and is hypothesized
to yield better attributes than untargeted documents precisely be-
cause of the increase in relevancy. There are three main phases: 1)
document acquisition and noun-phrase extraction, 2) context vector
generation from extracted usage patterns and 3) attribute ranking
using distributional similarity. Figure 1 summarizes the approach
taken, using the class Actor as an example.

2. METHODS
WD: (Web Document based extraction) – This method uses a ﬁxed
set of linguistically motivated surface patterns (e.g. “the A of I”
or “I’s A” for instance I and attribute A) combined with a pre-
speciﬁed set of instance classes to extract attributes from a Web-
based textual corpus [3].
QP: (Query logs using Patterns) – This method uses the same pro-
cedure as WD, but is applied to query logs instead of Web docu-
ments, yielding higher precision in practice [2].
QL: (Query Logs using seeds) – A set of 5 manually speciﬁed
seed attributes for each class are used to automatically extract pat-
terns (syntactic contexts) that contain a seed and an instance from
the same class. These patterns are then used to ﬁnd other candi-
date attributes, i.e. non-seed noun-phrases that match the extracted
patterns. These candidate attributes are then ranked based on their
similarity across all contextual patterns. This method produces sig-
niﬁcantly more precise attributes than QP [3].
WS: (Web Search extraction) – The ﬁrst novel method proposed
in this poster; attributes are extracted from relevant documents re-
turned from search queries.
BWS: (Bootstrapped Web Search extraction) – The second method
proposed in this poster; it uses the top 100 high-precision attributes
extracted with QL as additional supervision for WS.

3. EVALUATION
Classes of Instances: Extraction speciﬁcity is controlled via a set
of instance clusters (corresponding to semantic classes) for which
we wish to obtain attributes. Obtaining such collections has been
studied extensively in previous work [1, 4]. In this poster we use
40 classes chosen manually to have broad coverage.
Data Sources: Our Web documents corpus is procured by retriev-
ing the top 200 search results for each instance using Google and re-
moving all non-html documents. The total (compressed) size of the
corpus is over 16GB. The query log data contains 50M anonymized
English queries submitted to Google.
Evaluation Methodology: During evaluation, each candidate at-
tribute extracted for a class is hand-labeled as one of three cate-
gories: vital (1.0), okay (0.5) and wrong (0.0; cf. [6]). Vital at-
tributes should appear in any complete list of attributes for the tar-
get class; okay attributes are useful but non-essential; wrong at-
tributes are incorrect.
Results: Our main results are two-fold. First, extraction from
top Web search results yields higher attribute precision than ﬁxed-
pattern Web extraction, but has lower precision than extraction from
queries, conﬁrming similar results using untargeted Web corpora
[3]. Second, the noisy attributes extracted from query logs can be
used as additional seed targets for Web search extraction, yielding
better precision than either method individually.

4. CONCLUSION

Extracting attributes from documents on the Web is difﬁcult due
to the presence of noise, however such sources are signiﬁcantly
more content rich than other, more high-precision attribute sources
such as query logs. In this poster we develop a conceptually simple

5

20

Precision (%)
10
50
76.3 72.1 64.3 53.1
96.5 90.9 85.6 76.5
56.5 53.5 50.4 41.8
96.5 78.3 62.5 43.6
97.8 94.8 88.3 76.5

20

Relative Recall (%)
5
50
10
5.3
11.2 19.2 31.3
11.6 23.5 44.3 100
3.5
2.1
11.9
11.6 13.7 17.8
9.3
9.3
21.0 41.8 76.1

6.7

QP
QL
WD
WS
BWS

Table 1: Comparative precision of attributes, as well as recall of
vital attributes relative to QL, measured at ranks 5, 10, 20 and
50 in the ranked lists of attributes extracted by various methods

Class
AircraftModel

CarModel

ChemicalElem

Company

Country

Drug

Empire

Movie

Painter

SearchEngine

Wine

WorldWarBattle

Attributes
weight, length, fuel capacity, wing span, history, speciﬁca-
tions, photographs, fuel consumption, cost, price
transmission, acceleration, top speed, gearbox, gas mileage,
owners manual, transmission problems, engine type, mpg, re-
liability
symbol, atomic number, mass, classiﬁcation, atomic structure,
freezing point, discovery date, number, physical properties,
atomic weight
headquarters, chairman, location, ceo, stock price, company
proﬁle, corporate ofﬁce, president, parent company, stock
quote
president, area, population, ﬂag, economy, religion, climate,
geography, culture, currency
side effects, dosage, price, color, withdrawal symptoms,
mechanism of action, mechanism, dangers, overdose, dose
ruler, size, collapse, founding, location, deﬁnition, chronology,
downfall, kings, end
director, cast, producer, genre, crew, synopsis, ofﬁcial site, re-
lease date, script, actors
paintings, biography, birthplace, works, artwork, bibliogra-
phy, autobiography quotations, quotes, biographies
quality, speed, market share, number of users, reliability, num-
ber, mission statement, phone book, algorithms, video search
vintage, style, color, taste, wine reviews, cost, style of wine,
wine ratings, fermentation, aging
date, result, location, combatants, images, importance, sum-
mary, timeline, casualties, survivors

Table 2: Top 10 attributes extracted by BWS for a few classes;
shown in italics if also found among top 10 in QL

seed-based method for leveraging high-precision low-coverage at-
tribute sources (e.g. query logs) in order to improve extraction from
high-coverage low-precision sources such as Web documents. This
approach yields signiﬁcantly higher precision than previous meth-
ods (both Web and query-log based) and improves coverage by mit-
igating the “search bias” inherent in query-log based extraction.

5. REFERENCES
[1] D. Lin and P. Pantel. Concept discovery from text. In Proceedings of

the 19th International Conference on Computational linguistics
(COLING-02), pages 1–7, 2002.

[2] M. Pa¸sca. Organizing and searching the World Wide Web of facts -

step two: Harnessing the wisdom of the crowds. In Proceedings of the
16th World Wide Web Conference (WWW-07), pages 101–110, 2007.
[3] M. Pa¸sca, B. Van Durme, and N. Garera. The role of documents vs.
queries in extracting class attributes from text. In Proceedings of the
16th International Conference on Information and Knowledge
Management (CIKM-07), pages 485–494, Lisbon, Portugal, 2007.
[4] K. Shinzato and K. Torisawa. Acquiring hyponymy relations from

Web documents. In Proc. of the 2004 Human Language Technology
Conference (HLT-NAACL-04), pages 73–80, 2004.

[5] K. Tokunaga, J. Kazama, and K. Torisawa. Automatic discovery of

attribute words from Web documents. In Proceedings of the 2nd
International Joint Conference on Natural Language Processing
(IJCNLP-05), pages 106–118, Jeju Island, Korea, 2005.

[6] E. Voorhees. Evaluating answers to deﬁnition questions. In

Proceedings of the 2003 Human Language Technology Conference
(HLT-NAACL-03), pages 109–111, Edmonton, Canada, 2003.

WWW 2009 MADRID!Poster Sessions: Friday, April 24, 20091236