A Class-Feature-Centroid Classiﬁer for Text Categorization

Hu Guan

Computer Science Dept.

Shanghai Jiao Tong University

800 Dongchuan Road

Shanghai 200240, China
guanhu@sjtu.edu.cn

Jingyu Zhou

Minyi Guo

Computer Science Dept.

Computer Science Dept.

Shanghai Jiao Tong University

Shanghai Jiao Tong University

800 Dongchuan Road

Shanghai 200240, China

zhou-jy@cs.sjtu.edu.cn

800 Dongchuan Road

Shanghai 200240, China

guo-my@cs.sjtu.edu.cn

ABSTRACT
Automated text categorization is an important technique
for many web applications, such as document indexing, doc-
ument ﬁltering, and cataloging web resources. Many dif-
ferent approaches have been proposed for the automated
text categorization problem. Among them, centroid-based
approaches have the advantages of short training time and
testing time due to its computational eﬃciency. As a result,
centroid-based classiﬁers have been widely used in many web
applications. However, the accuracy of centroid-based clas-
siﬁers is inferior to SVM, mainly because centroids found
during construction are far from perfect locations.

We design a fast Class-Feature-Centroid (CFC) classiﬁer
for multi-class, single-label text categorization.
In CFC,
a centroid is built from two important class distributions:
inter-class term index and inner-class term index. CFC
proposes a novel combination of these indices and employs
a denormalized cosine measure to calculate the similarity
score between a text vector and a centroid. Experiments on
the Reuters-21578 corpus and 20-newsgroup email collection
show that CFC consistently outperforms the state-of-the-art
SVM classiﬁers on both micro-F1 and macro-F1 scores. Par-
ticularly, CFC is more eﬀective and robust than SVM when
data is sparse.

Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Design Methodology—Clas-
siﬁer design and evaluation; Feature evaluation and selec-
tion; I.5.4 [Pattern Recognition]: Applications—Text pro-
cessing

General Terms
Algorithms, Experimentation, Performance

Keywords
centroid, text classiﬁcation, inter-class, inner-class, denor-
malized cosine measure

1.

INTRODUCTION

Automatic text categorization (TC) is a process of assign-
ing some class labels to text documents. In recent years, TC
has been widely used in many web applications, for instance,

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

query classiﬁcation in search engines [2], deep classiﬁcation
of web documents [5, 38], and Blog classiﬁcation [22]. For
these web applications, it is often required that TC can be
performed with a short training time and testing time, some-
times with incrementally updated data.

A number of TC techniques have been explored in the lit-
erature, for instance, Naive Bayes [6, 10], kNN [8], Neural
Network [4], centroid-based approaches [2, 3, 16, 34], De-
cision Tree (DT) [17, 35, 7], Rocchio [26], and SVM [12,
5]. Among them, the centroid-based classiﬁer is one of the
most popular supervised approaches, due to the computa-
tional eﬃciency. However, previous work [31] has found that
the performance of centroid-based classiﬁers is signiﬁcantly
lower than other approaches (e.g., SVM).

One of the reasons for the inferior performance of centroid-
based classiﬁers is that centroids do not have good initial val-
ues. To solve this problem, many methods have been using
feedback-loops to iteratively adjust prototype vectors, such
as Dragpushing method [34], Hypothesis Margin method [34],
and Weight Adjustment method [29]. These improved clas-
siﬁers perform competitively compared to SVM classiﬁers.

Motivated by previous weight-adjustment eﬀorts for centroid-

based classiﬁers, we design a Class-Feature-Centroid (CFC)
classiﬁer, which strives to construct centroids with better
initial values than traditional centroids. In our CFC classi-
ﬁer, we ﬁrst extract inter-class and inner-class term indices
from the corpus. Then both indices are carefully combined
together to produce prototype vectors. Our experimental re-
sults on the skewed Reuters-21578 corpus and the balanced
20-newsgroup corpus demonstrate that our CFC classiﬁer
has a consistently better performance than SVM classiﬁers.
In particular, CFC is more eﬀective and robust than SVM
when data is sparse. In summary, this paper has made fol-
lowing contributions:

• We propose a novel weight representation for centroid-
based classiﬁers, which incorporates both inter-class
term distribution and inner-class term distribution to
determine term weights in prototype vectors.

• In the testing phase, we adopt a denormalized cosine
measure for similarity calculation. Our experiments
demonstrate that this approach is more eﬀective for
CFC than normalizing prototype vectors, because the
discriminative capability of features is preserved.

The remainder of the paper is organized as follows. Sec-
tion 2 reviews centroid-based text categorization methods.
Section 3 elaborates the design of CFC. We evaluated the
performance of our CFC in Section 4. Section 5 summarizes

WWW 2009 MADRID!Track: Data Mining / Session: Learning201related work. Finally, Section 6 concludes our work with
future research directions.

2. CENTROID-BASED TC

In centroid-based TC, a text in a corpus is represented
with a Vector Space Model (VSM), where each text is con-
sidered as a vector in term space [1]. A prototype vector
(i.e., a centroid) is constructed for each category as a dele-
gate vector for all documents belonging to that class. When
classifying an unlabeled document, the vector representing
the document is compared with all prototype vectors, and
the document is assigned to the class whose prototype vector
is most similar [15].
2.1 Prototype Representation
In the vector space model, all documents in a class form
a lexicon set F = {t1, t2, ..., t|F|}, and a document is rep-
resented as a vector. Normalization by document length
is often performed when calculating a term’s weight and
scaling the vector to have L2-norm equal one is the most
commonly used method. A traditional prototype vector is a
delegate vector for each category, where a feature’s weight
should be a form of weight combination of all documents in
the category.

To avoid over-ﬁtting and high-computational complexity,
many dimension reduction methods have been proposed for
the term vector, such as stop words, stemming [37], word
clustering [19], and document frequency [39].
2.2 Centroid Construction

Given a class Cj of a corpus, there are two classical meth-

ods to create Cj’s prototype vector:

(1) Arithmetical Average Centroid (AAC):

−−−−−−−→
Centroidj =

1|Cj|

−→
d ,

X

−→
d ∈Cj

(1)

where the centroid is the arithmetical average of all docu-
ment vectors of class Cj. This is the most commonly used
initialization method for centroid-based classiﬁers.

2.3 Classiﬁcation

After centroids of diﬀerent categories are determined, an
unlabeled document is classiﬁed by ﬁnding the closest cen-
troid to the document vector. The category of this centroid
is then assigned to the test document. When the distance of
two vectors is measured as the their dot product, the testing
process is to calculate

(cid:4)

C

= arg max

j

( (cid:2)d • −−−−−−−→

Centroidj).

That is, the test document d will be labeled as class C
.
Besides the above method, other distances, such as Pear-
son Correlation Coeﬃcients [24], Euclidean-based similarity
computed by an exponential transformation, and Euclidean-
based similarity computed by division [3], have also been
employed in the literature.

(cid:4)

2.4 Motivation of Our Work

The performance of centroid-based classiﬁers depends strongly

on the quality of prototype vectors. To improve perfor-
mance, many studies have attempted using feedbacks to
adjust term weight in prototype vectors, such as Dragpush-
ing [32], Hypothesis Margin [34], and CentroidW [29]. Com-
bining Homogeneous [14] combined several centroid-based
classiﬁers with diﬀerent term distribution to improve the ac-
curacy of classiﬁcation. The performance of these adaptive
methods is generally better than the traditional centroid-
based methods. In particular, some of them can be compa-
rable to SVM classiﬁers on micro-F1 and macro-F1 evalua-
tions [32, 34].

The motivation of this work is from the observation that
all of the above adaptive methods start with the same initial
term weights obtained at the centroid construction phase.
During the training phase, term weights are adjusted to
get better prototype vectors. Diﬀerent from these previ-
ous approaches, we try to obtain good centroids during the
construction phase such that their classiﬁcation capability
is still competitive compared to those derived from adap-
tive methods. The following sections discuss details of our
centroid-based classiﬁer.

(2) Cumuli Geometric Centroid (CGC):

−−−−−−−→
Centroidj =

−→
d ,

X

−→
d ∈Cj

(2)

3. DESIGN

where each term will be given a summation weight [15].

Compared to other TC methods, centroid-based approaches

are more serious with the problem of inductive bias or model
misﬁt [18, 36] — classiﬁers are tuned to the contingent char-
acteristics of the training data rather than the constitutive
characteristics of the categories. Centroid-based approaches
are more susceptible to model misﬁt because of its assump-
tion that a document should be assigned to a particular
class when the similarity of this document and the class is
the largest. In practice, this assumption often doesn’t hold
(i.e., model misﬁt).

In fact, many researchers have found that a centroid-
based classiﬁer should give more weight to term distribu-
tions among the corpus, i.e., inter-class, inner-class and in-
collection distributions. Centroids considering characteris-
tics of term distribution have shown improved results [14].

Diﬀerent from previous approaches for constructing cen-
troids, CFC puts more emphasis on term distributions in
order to improve the quality of centroid. Speciﬁcally, CFC
ﬁrst extracts inter-class and inner-class term distributions
from the corpus, and then uses these distributions to de-
rive the centroid. In the rest of this section, we ﬁrst give
an overall description of CFC design, and then discuss our
feature extraction method, followed by a discussion of our
classiﬁcation method.

3.1 Overview

Similar to other centroid-based approaches, CFC adopts
the vector space model: all documents from a corpus form a
lexicon set F = {t1, t2, ..., t|F|}; and the centroid for class Cj
is represented by a term vector Centroidj = (w1j , w2j , ..., w|F|j),
where wkj (1 ≤ k ≤ |F|) represents the weight for term tk.
The main diﬀerence between CFC and other approach is
how term weight is derived. In CFC, weight for term tk of

WWW 2009 MADRID!Track: Data Mining / Session: Learning202class j is calculated as:

wij = b

j
ti

DF

|Cj | × log(

|C|
CFti

),

(3)

j
ti

is term ti’s document frequency in class Cj ,
where DF
|Cj| is the number of documents in class Cj , |C| is the total
number of document classes, CFti is the number of classes
containing term ti, and b is a constant larger than one.

DF

j
ti
|Cj |

In the above formula, the ﬁrst component b

is the
|C|
)
inner-class term index, and the second component log(
CFti
represents the inter-class term index. Both indices are dis-
cussed below.

3.2 Inner-Class Term Index

The inner-class distribution of a term can help classiﬁca-
tion. For instance, if a term appears many times in docu-
ments of category C, and then a test document containing
the term is more likely to be of category C.

After comparing diﬀerent forms for inner-class distribu-

DF

j
ti
|Cj |

tion, we ﬁnally choose b
, (b > 1) to be CFC’s the inner-
class term index. The advantage of this form is to limit the
inner-class feature weight within range (1, b]. The denomina-
tor |Cj| in the formula smoothes the diﬀerence of document
frequencies across categories.

The proposed inner-class term index can be easily com-
puted by counting the occurrence of terms while traversing
the corpus, which only incurs linear-time cost.

3.3 Inter-Class Term Index

Previous work [21] has studied many mathematical forms
for scoring a term’s distribution information.
Intuitively,
a good inter-class feature or term should distribute rather
diﬀerently among classes.
In other words, if a term only
appears in a few categories, and then the term is a discrim-
inative feature, thus a good feature for classiﬁcation. Con-
versely, if a term appears in every category, and then the
term is not a good inter-class feature.

|C|
CFti

Our method for extracting inter-class term index,

i.e.,
log(
), produces more discriminative features. Note that
such a form is similar to IDF, but the diﬀerence is that CFC
is counting the number of categories containing the term.
When a term occurs in every category, the value becomes
0 (because |C| = CFti ). When the term only occurs in
one category, the value becomes log(|C|).
In other cases,
the value falls between them. We can observe that such a
method favors rare terms and bias against popular terms.

The inter-class term index can be eﬃciently computed in
linear time. By counting lexicons while traversing the corpus
once, all inter-class term indices can be easily computed.

3.4 Testing Phase

After the prototype vector of each class is obtained, CFC
classiﬁes a document with a denormalized cosine measure,
i.e.,

(cid:4)

C

= arg max
(

j

−→
di • −−−−−−−→

Centroidj),

(4)

−→
−→
di is the document vector for document i. The term
di uses the standard normalized

where
weight for each feature in

cos α × ||−−−−−−−→

TF-IDF score. As Figure 1 showing, cos α is the stan-
dard similarity measure between two vectors, while we adopt
Centroidj||2 as the similarity measure between a
document vector and a prototype vector. Because the pro-
totype vector is not normalized, this similarity is called de-
normalized cosine measure in this paper.

Centroid

j

di

A

o

α

c o s α
||
α×
c o s

r o i d

||

2

j

C e n t

Figure 1: An illustration of denormalized cosine
measure.

This denormalized cosine measure preserves the discrimi-
native capability of prototype vectors and improves the ac-
curacy of classiﬁcation (see Section 4.3.1).

3.4.1 A Classiﬁcation Example

We use an example to illustrate how classiﬁcation is per-
formed in CFC and to demonstrate the diﬀerence between
CFC and AAC. Assume a corpus contains four categories
C1, C2, C3, and C4. Three feature terms appear in 14 docu-
ments, and each document only contains one term. Table 1
gives the term distribution of this corpus.

Table 1: Term distribution for an example corpus.

Feature
term1
term2
term3

C1
0
2
0

C2
0
1
0

C3
0
1
0

C4
2
1
7

Then, Table 2 shows prototype vectors for each category.

For CFC, b is set e − 1, and log uses function ln.

Table 2: Prototype vectors for the example corpus.
{0.2721,0.1361,0.9526}

Classiﬁer

C2

C3

C4

C1

{0,1,0}
{0,0,0}

{0,1,0}
{0,0,0}

{0,1,0}
{0,0,0}

{1.5445,0,2.0250}

AAC
CFC

Now, assume {0.6, 0.8, 0} is the document vector for a
document (cid:2)d. We then calculate the similarity between (cid:2)d and
all four categories and obtain (0.8000, 0.8000, 0.8000, 0.2721)
for AAC, and (0, 0, 0, 0.9267) for CFC.
AAC would assign a class label C1, C2, and C3 to (cid:2)d, since
the similarity for these three classes is 0.8000, the highest
value. For CFC, label C4 is assigned to (cid:2)d. For this pedagogic
example, we can observe that:

• AAC should favor popular words in a corpus, which

WWW 2009 MADRID!Track: Data Mining / Session: Learning203is exempliﬁed by the score of (0.8000, 0.8000, 0.8000,
0.2721) for (cid:2)d.

• CFC is more favorable to rare terms and biases against
popular terms. In the example, CFC gives (cid:2)d a score
of (0, 0, 0, 0.9267), because term1 only appears in cat-
egory C4 while term2 occurs in all of the categories.

This distinct trait of CFC contributes to the quite diﬀer-

ent classiﬁcation performance between AAC and CFC.

3.4.2 Discussion of Denormalized Measure

Enlarging cos α by a factor of prototype vector’s mag-
nitude may seem to be unfair to some categories. A pro-
totype vector with large norms could increase false posi-
tives, because documents from other categories may receive
higher similarity scores. On the other hand, prototype vec-
tors with small norms could have more false negatives. In
fact, such a phenomenon can happen for AAC or CGC (see
Section 4.3.1).

103

102

101

l

e
u
a
V
 
y
t
i
r
a

l
i

m
S

i

 
f

 

o
m
r
o
N

100

 

5

10

15

Norm of Prototype Vector
Max Similarity Value of Train Vectors
Max Similarity Value of Test Vectors

 

20

25

Category Number

30

35

40

45

50

Figure 2: Norms of prototype vectors, similarity
measure between documents (training and testing)
and prototype vectors of CFC for Reuters-21578 cor-
pus.

CFC, however, doesn’t suﬀer from the above problem of
incorrect classiﬁcations. We have found CFC’s similarity
measures do not change signiﬁcantly across diﬀerent cat-
egories. Figure 2 illustrates the comparison of the norms
of prototype vectors, and similarity measure between doc-
uments (divided into training set and testing set) and pro-
totype vectors for Reuters-21578 corpus. Even though the
variations for the norms of prototype vectors can be quite
signiﬁcant (the average is 40.3 and standard deviation is
2363.0) the maximums of similarity measure for training
documents and testing documents do not change signiﬁ-
cantly. The average and standard deviation are 8.96 and
5.69 for training set, and 8.17 and 3.88 for testing set. That
is, similarity measures are roughly on the same scale for all
categories.

4. EVALUATION

In this section, we evaluate our CFC classiﬁer on the
TC task by comparing CFC’s performance with previous

centroid-based approaches and the state-of-the-art SVM meth-
ods. Speciﬁcally, we compare the performance of four dif-
ferent approaches:

• AAC: centroid-based approach with arithmetical aver-

age centroids;

• CGC: centroid-based approach with cumuli geometric

centroids;

• CFC: our class-feature centroids;
• SVM: SVM-based tools. We adopted SVMLight1, SVM-

Torch2, and LibSVM3 in this paper.

We will use both skewed and balanced corpus for perfor-
mance evaluation. For the skewed corpus experiments, we
use the Reuters-21578 dataset. For balanced corpus, 20-
newsgroup dataset is used. In this paper, we focus on clas-
sifying multi-class, single-label TC task and remove multi-
label texts from the Reuters corpus.

The performance metrics used in the experiments are F1,
micro-averaging F1, and macro-averaging F1. F1 is a com-
bined form for precision (p) and recall (r), which is deﬁned
as

F 1 =

2rp
r + p

.

We used F1 to evaluate the classiﬁcation performance for in-
dividual category. The macro-averaging F1 (macro-F1) and
micro-averaging F1 (micro-F1) were used to measure the av-
erage performance for the whole corpus. Macro-F1 gives the
same weight to all categories, thus is mainly inﬂuenced by
the performance of rare categories for the Reuters-21578 cor-
pus, due to skewed category distribution. On the contrary,
micro-F1 will be dominated by the performance of common
categories for the Reuters-21578 corpus. Because the 20-
newsgroup corpus is a much balanced corpus, its macro-F1
and micro-F1 are quite similar.

In the rest of this section, we use ”μ-F1” and ”M-F1” to

represent micro-F1 and macro-F1, respectively.
4.1 Datasets and Experimental Settings

Reuters-21578. The Reuters-21578 dataset4 are based
on the Trinity College Dublin version. Trinity College Dublin
changed the original SGML text documents into XML for-
mat text documents. After removing all unlabeled docu-
ments and documents with more than one class labels, we
then retained only the categories that had at least one doc-
ument in both the training and testing sets and got a col-
lection of 52 categories.

Altogether, there are 6,495 training texts and 2,557 test-
ing texts left in this 52-category corpus. The distribution
of documents over the 52 categories is highly unbalanced.
After removing 338 stop words (also provided by Trinity
College dataset), and unigram terms that occur less than
three times, we get 11,430 unique unigram terms. Words
in titles are given ten times weight comparing to those in
abstracts.

1http://svmlight.joachims.org
2http://www.idiap.ch/∼bengio/projects/SVMTorch.html
3http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools
4http://ronaldo.cs.tcd.ie/esslli07/sw/step01.tgz

WWW 2009 MADRID!Track: Data Mining / Session: Learning20420-newsgroup. This dataset5 consists of 19,997 texts
(about one thousand text documents per category), and ap-
proximately 4% of the articles are cross-posted. The stop
words list [20] has 823 words, and we kept words that oc-
curred at least once and texts that had at least one term.

Altogether, there are 19,899 texts (13,272 training and
6,627 testing) left in the corpus. When parsing documents,
we only keep ”Subject”, ”Keywords”, and ”Content”. Other
information, such as ”Path”, ”From”, ”Message-ID”, ”Sender”,
”Organization”, ”References”, ”Date”, ”Lines”, and email ad-
dresses, are ﬁltered out. The total number of unigram terms
is 29,557 unigrams. Words in ”Subject” and ”Keywords” are
given ten times weight comparing to those in ”Contents”.

For both corpora, we used the tokenizer tool provided
in the Trinity College sample. IDF scores for TF-IDF are
extracted from the whole corpus. Stemming and word clus-
tering were not applied.

Parameter Settings. For all SVM tools (SVMLight,
SVMTorch, and LibSVM), the linear kernel and the de-
fault settings were used. All of SVM-based classiﬁers can
cope with a sparse multi-class TC task directly with one-
vs-others decomposition and default parameter values. For
the Reuters corpus, χ2-test method [39] was adopted to per-
form feature selection (top 9,000) for SVMLight (this setting
yields the best result in our experiments), while SVMTorch
classiﬁer used all 11,430 features. For 20-newsgroup, we
tuned parameter b for SVMLight for better performance.
In the experiments, the parameter b of CFC is set to e −

1.7, unless speciﬁed otherwise.
4.2 Overall Performance

We ﬁrst compare the overall performance of diﬀerent ap-
proaches. The results for both the Reuters and the 20-
newsgroup are shown in Table 3.

Table 3 shows that CFC performs the best among all clas-
siﬁers for the Reuters Corpus. Both micro-F1 and macro-
F1 values are above 0.99, which is signiﬁcantly better than
SVM-based classiﬁers. Two classical centroid-based classi-
ﬁers, AAC and CGC, perform the worst using normalized
cosines and happen to have the same results.

For the 20-newsgroup corpus, Table 3 shows that CFC
performs the best among all classiﬁers. CFC’s micro-F1 and
macro-F1 are 0.9272 and 0.9275, respectively, which are sig-
niﬁcantly better than all SVM classiﬁers. Again, centroid-
based AAC and CGC perform the worst.

Table 3: Overall performance comparison of diﬀer-
ent classiﬁers.

Classiﬁer

Reuters

CFC

SVMLight
SVMTorch*

LibSVM

AAC
CGC

μ-F1

0.9941
0.9210
0.9163
0.8999
0.8647
0.8647

M-F1
0.9960
0.8335
0.7875
0.7190
0.7344
0.7344

20-newsgroup
μ-F1
M-F1
0.9275
0.8297
0.8479
0.8425
0.8292
0.8292

0.9272
0.8304
0.8482
0.8416
0.8307
0.8307

*Previous work [33] has reported micro-F1 and macro-F1 for 20-
newsgroup corpus could reach 0.8891 and 0.8876, respectively.

5http://kdd.ics.uci.edu/databases/20newsgroups

s
e
i
r
o
g
e
t
a
C

 
f
o
 
r
e
b
m
u
N

50

45

40

35

30

25

20

15

10

5

0

 

0.1

0.2

0.3

0.4

0.5

0.6

F1 Scores

 

AAC
SVMTorch
SVMLight
CFC

0.7

0.8

0.9

1.0

Figure 3: F1 comparison of AAC, SVMTorch, SVM-
Light, and CFC for the Reuters corpus.

 

SVMLight
AAC
LibSVM
SVMTorch
CFC

18

16

14

12

10

8

6

4

2

s
e
i
r
o
g
e
a
C

t

 
f

o
 
r
e
b
m
u
N

 

0
0.5

0.6

0.7

0.8
F1 Scores

0.9

1.0

Figure 4: F1 comparison of AAC, SVMTorch, SVM-
Light, LibSVM, and CFC for the 20-newsgroup cor-
pus.

To give a more visualized comparison, Figure 3 and Fig-
ure 4 give the distributions of F1 scores for the Reuters cor-
pus and the 20-newsgroup corpus, respectively. From both
ﬁgures, we can observe that CFC has much more categories
within the range of [0.9, 1.0]. In fact, CFC consistently per-
forms better than other approaches for most categories. In
Figure 3, both SVMTorch and SVMLight have a few cate-
gories with zero F1 score. This is mainly for categories with
sparse data, which is discussed below.

4.2.1 Study on Sparse Data

In this study, we selected the bottom 10 classes from
Reuters corpus, where the number of training documents
varies from one to ﬁve. Previous work [7] has found that
most of conventional learning methods, such as SVM and
kNN, have poor performance for sparse training cases, re-
sulting in low macro-F1 scores, because macro-F1 treats ev-
ery category equally.

Table 4 illustrates F1 values for diﬀerent classiﬁers. Our

WWW 2009 MADRID!Track: Data Mining / Session: Learning205CFC performs the best with value one for all classes. I.e.,
all test documents are correctly classiﬁed. This is mainly
because CFC can eﬀectively exploit the discriminative ca-
pability of prototype vectors. This experiment can help to
explain CFC’s excellent macro-F1 values in the overall per-
formance comparison.

For SVMLight or SVMTorch, more than half of classes
have F1 values less than one, and both obtained zero for
category ”jet” and ”dlr”. The result could be caused by the
lack of training data.

For AAC, all F1 values are well below one because AAC’s
prototype vectors don’t have the same discriminative capa-
bility as CFC.

Table 4: Comparison of F1 metric for bottom 10
classes of Reuters corpus.

Class

Train Test

platinum

jet

potato

tea
cpu
dlr

nickel
fuel
lead

instal-debt

1
2
2
2
3
3
3
4
4
5

2
1
3
3
1
3
1
7
4
1

AAC
0.5714
0.1538
0.7500
0.3636
0.3333
0.3529
0.3333
0.6364
0.8000
0.1667

CFC

Light

Torch

1
1
1
1
1
1
1
1
1
1

0
0

0.8000
0.5000

1
0
1

1
0

0.8000
0.8000

1
0
1

0.4444
0.9000

1

0.4444

1
1

4.2.2 Study on Sufﬁcient Data

Reuters. To give an elaborated exhibition for categories
with suﬃcient training data, F1 values of the top ten cate-
gories from the Reuters corpus were listed in Table 5. For all
these ten categories, we observe that CFC consistently has
the best F1 scores. In many categories, CFC is signiﬁcantly
better than SVM classiﬁers.

Table 5: Comparison of F1 metric for top ten classes
of the Reuters corpus.

Class
earn
acq

crude
trade
mny-fx
interest
mny-sp

ship
sugar
coﬀee

Train Test
1076
2824
1594
695
119
251
75
249
87
204
81
189
28
115
107
36
25
97
90
22

AAC
0.9273
0.9035
0.8860
0.8951
0.7886
0.8025
0.6377
0.8537
0.8800
0.9565

CFC

0.9958
0.9943

1

0.9796
0.9884
0.9877
0.9655

1
1

0.9778

Light
0.9728
0.9353
0.8871
0.8589
0.7561
0.8662
0.8214
0.6229
0.9796
0.9778

Torch
0.9647
0.9266
0.8525
0.8947
0.7758
0.8701
0.8571
0.6349
0.9796
0.9778

20-newsgroup. Table 6 gives a closer look at CFC’s per-

formance in a balanced and suﬃcient dataset — 20-newsgroup
corpus. Similar to results from the Reuters corpus, we ob-
serve that CFC also consistently outperforms SVMLight,
SVMTorch, and LibSVM classiﬁers. For most categories,
the diﬀerence is signiﬁcant.

In summary, when the training data is suﬃcient, our CFC
classiﬁer can consistently perform better than SVM classi-
ﬁers for both corpus tested.

4.2.3 Discussion

We have attempted to improve SVMLight’s performance
for unbalanced training set by tuning parameter b, and such
attempts can slightly improve F1 scores. For 20-newsgroup,

Table 6: Comparison of F1 for all classes of 20-
newsgroup corpus.

# SVMLight
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

0.7446
0.7829
0.7773
0.7100
0.7838
0.8000
0.7557
0.8871
0.8910
0.9302
0.9381
0.9385
0.8142
0.8806
0.9216
0.8854
0.8615
0.9096
0.7471
0.6111

AAC
0.7616
0.7752
0.7890
0.7519
0.8050
0.8037
0.7907
0.8354
0.8750
0.9263
0.9170
0.8693
0.7829
0.9008
0.9132
0.7972
0.8038
0.9012
0.7370
0.5759

LibSVM SVMTorch
0.7847
0.8055
0.7724
0.7489
0.8438
0.7297
0.7412
0.8839
0.8986
0.9325
0.9457
0.9475
0.8318
0.9198
0.9354
0.8886
0.8719
0.9251
0.7732
0.6696

0.7814
0.8018
0.8049
0.7575
0.8397
0.8036
0.7585
0.8771
0.9164
0.9323
0.9399
0.9520
0.8158
0.8791
0.9453
0.8905
0.8694
0.9244
0.7687
0.7003

CFC

0.8359
0.9383
0.9293
0.9192
0.9469
0.9393
0.9257
0.9684
0.9572
0.9802
0.9894
0.9587
0.9450
0.9866
0.9790
0.9371
0.9102
0.9452
0.8168
0.7299

are:

Categories
3:comp.os.ms-windows.misc,
5:comp.sys.mac.hardware,
8:rec.autos,
11:rec.sport.hockey,
15:sci.space,
18:talk.politics.mideast,
20:talk.religion.misc.

9:rec.motorcycles,

12:sci.crypt,

16:soc.religion.christian,

1:alt.atheism,

6:comp.windows.x,

2:comp.graphics,
4:comp.sys.ibm.pc.hardware,
7:misc.forsale,
10:rec.sport.baseball,
14:sci.med,
17:talk.politics.guns,
and

13:sci.electronics,

19:talk.politics.misc,

when we tune parameter b (default value is 1.0), micro-F1
increases from 0.8124 to 0.8304, and macro-F1 rises from
0.8121 to 0.8297. However, tuning parameter b does not
work well for the Reuters corpus.

SVM-based classiﬁers are proved powerful when they are
working in a suitable environment.
In our experiments,
there are several factors that may contribute to the inferior
performance of SVM. First, the training samples for small
categories are seriously skewed when one-vs-others policy is
adopted. Second, no feature extraction methods, such as
latent semantic indexing (LSI) [11, 7] or linear discriminant
analysis (LDA) [11], are performed. Third, we use a quite
simple tokenizer with no stemming or word clustering. Fea-
tures are kept if they appear more times than a threshold,
i.e., based on term frequency. All of above factors could limit
SVM to ﬁnd the perfect hyperplane, thus lowering classiﬁ-
cation performance.

On the other hand, the better performance of CFC could
hint that CFC has a discriminative ability on those ”raw”
terms (without feature extraction and selection) for text
classiﬁcation. Previous work [25] has found that document
frequency is a powerful discriminative tool. In CFC, such
discriminative capability is preserved.

4.3 Study on Denormalized Cosine Measure

4.3.1 Normalize vs. Denormalize
CFC.
tor, i.e., cos α × ||−−−−−−−→

Our CFC classiﬁer adopts a denormalized prototype vec-
Centroidj||2. To study the eﬀects of this
design, we compare the performance of denormalized and
normalized prototype vectors for CFC and other centroid-
based classiﬁers. The results are illustrated in Table 7.

WWW 2009 MADRID!Track: Data Mining / Session: Learning206For CFC, Table 7 shows that both micro-F1 and macro-
F1 have signiﬁcant changes when switching from a normal-
ized form to the denormalized one. In fact, when prototype
vectors are normalized, CFC’s performance is even much
lower than AAC and CGC. This is because normalization
smoothes the prototype vectors, thus damages their discrim-
inative capability of enlarging selected term features in a
text vector.

Table 7: Comparison of normalized and denormal-
ized prototype vectors for the Reuters corpus.

Classiﬁer

normalized CFC 0.6058
CFC 0.9941
AAC 0.8647
denormalized AAC 0.5604
CGC 0.8647
denormalized CGC 0.8510

μ-F1 M-F1
0.5769
0.9960
0.7344
0.5454
0.7344
0.4408

AAC and CGC.

For AAC and CGC, Table 7 shows that denormalization
can cause signiﬁcant performance degradations. For AAC,
micro-F1 drops from 0.8647 to 0.5604 and macro-F1 fall from
0.7344 to 0.5454. For CGC, macro-F1 drops from 0.7344 to
0.4408. Here we select the CGC classiﬁer for an elaborated
illustration.

Table 8: Comparison of denormalized CGC and
CGC for the top ﬁve categories of the Reuters cor-
pus.

Class
earn
acq

crude
trade
mny-fx

Train Test
1076
2824
1594
695
119
251
75
249
204
87

Pre

0.9989
0.9750
0.9266
0.9412
0.7841

DPre
0.8227
0.8971
0.8110
0.8023
0.8800

F1

0.9273
0.9035
0.8860
0.8951
0.7886

DF1
0.8993
0.8939
0.8374
0.8571
0.8148

Note: ”Pre” means precision, ”DPre” means precision for denor-
malized CGC, and ”DF1” means F1 evaluation for denormalized
CGC.

Table 8 compares the original CGC approach and denor-
malized CGC for the top ﬁve categories. We can observe
that after denormalization, the precision for top four cate-
gories signiﬁcantly declines. The reason is that denormaliza-
tion results in larger norms for prototype vectors, as top cat-
egories have more documents. When calculating similarity,
a big norm increases false positives for those top categories,
thus lower precision. The top ﬁve categories represent nearly
80% of the whole corpus, so a larger number of false posi-
tives in these categories also correspond to the performance
deterioration of small categories.

On the other hand, Table 9 compares original CGC and
CGC using denormalized prototype vectors for the bottom
ten categories. The most prominent change after denormal-
ization is that recall for many categories have dropped to
zero. The reason is that the norms of these rare categories
are small, causing lower values of similarity measure for doc-
uments within these categories and increasing the number
of false negatives.

Though the F1 value of the top ﬁve categories are not
signiﬁcantly declining, those rare categories have signiﬁcant

drop on F1 evaluation, which results in signiﬁcant degrada-
tion of macro-F1.

Table 9: Comparison of denormalized CGC and
CGC for bottom ten categories of the Reuters cor-
pus.

Class

Train Test Recall DRecall

F1

DF1

0
0
0

0.6667

0
0
0

0.1429

0.5714
0.1538
0.7500
0.5455
0.3333
0.3529
0.3333
0.6364

0
0
0

0.8000

0
0
0

0.2500

1
1
1
1
1
1
1
1
1
1

platinum

jet

potato

tea
cpu
dlr

nickel
fuel
lead

1
2
2
2
3
3
3
4
4
5

2
1
3
3
1
3
1
7
4
1

0
0
Note: ”DRecall” means recall evaluation for denormalized CGC.

instal-debt

0
0

0.8

0.1667

In summary, a denormalized prototype vector can help
CFC to retain its discriminative capability. On the other
hand, the same denormalization actually hurts traditional
centroid-based classiﬁers, because of a quite diﬀerent method
for constructing centroid.

4.3.2 Denormalized Measure for 20-newsgroup

Norm of Prototype Vector
Max Similarity Value of Train Vectors
Max Similarity Value of Test Vectors

 

103

102

101

l

e
u
a
V
 
y
t
i
r
a

l
i

m
S

i

 
f

 

o
m
r
o
N

100
 
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20

Category Number

Figure 5: Norms of prototype vectors, similarity
measure between documents (training and testing)
and prototype vectors of CFC for 20-newsgroup cor-
pus.

Figure 5 illustrates the norms of prototype vectors for
the 20-newsgroup corpus, together with similarity measure
between documents and prototype vectors. Similar to the
Reuters corpus, we can observe that the norms of proto-
type vectors have great variations: the average is 113.8 and
standard deviation is 420.3. However, similarity measures
across categories do not have much variation. The average
and standard deviation are 18.1 and 35.7 for training set,
16.8 and 28.1 for testing set. As a result, the discriminative
features of prototype vectors are retained and their weights
are enlarged, which contributes to the eﬀectiveness of CFC.
It’s worth noting that variations of similarity scores in the
20-newsgroup corpus are more evident than the Reuters cor-
pus, even though 20-newsgroup is a fairly balanced corpus.
The reason is that the length of texts has great variations

WWW 2009 MADRID!Track: Data Mining / Session: Learning207in 20-newsgroup. For instance, the length of texts in class
”alt.atheism” varies from 1 KB to 51 KB, and this variation
of text length is quite common among all categories. As a
result, the number of selected features in text vectors has
much greater variations: 1 to 678 for 20-newsgroup and 1 to
44 for Reuters. Considering the unique terms (or features)
in 20-newsgroup and Reuters are 29,577 and 11,430, respec-
tively, texts in 20-newsgroup corpus may select up to 2.29%
features, while texts in Reuters may only use 0.38% features.
As a result, the similarity measure of 20-newsgroup is more
varied.
4.4 Study on Feature Weight

The ﬁrst construction, ln(

We carried out a series of experiments to study the fea-
ture weight for CFC. Table 10 illustrates the results of some
representative combinations of inner-class and inter-class in-
dices using the Reuters corpus.
|C|
CFt ), yields above 0.89 values
for both micro-F1 and macro-F1 metrics, indicating that
the inter-class term distribution works well for classiﬁcation.
The second and third formulas show that a simple combina-
tion of inner-class and inter-class distributions may actually
hurt the performance.

The fourth formula only uses inner-class index and yields
values above 0.99, indicating the inner-class index is highly
discriminative in the classiﬁcation. Combined with inter-
class term index, the ﬁfth formula shows slight improvement.
The sixth formula shows that changing parameter b can fur-
ther improve the results. We will discuss parameter b in the
next subsection.

Table 10: Performance study on diﬀerent forms of
feature weight.

No.

1

2

3

4

5

6

ln(1 + DF

DF j
t
|Cj|)

j

Formula
|C|
ln(
CFt )
× ln(
|C|
CFt )
t ) × ln(
DF j
t
|Cj| )
|Cj| ) × ln(
|Cj| × ln(

exp(
DF j
t

DF

j
t

|C|
CFt )

|C|
CFt )
|C|
CFt )

exp(
(e − 1)

μ-F1 M-F1

0.8909

0.8982

0.6676

0.4996

0.9711

0.8520

0.9930

0.9938

0.9941

0.9951

0.9941

0.9960

Note: |C| is the total number of categories in a corpus. |Cj| is the
number of documents within jth category. DF j
t is term t’s document
frequency within category Cj. CFt is number of categories containing
term t.

4.5 Study on Parameter b

In this section, we ﬁrst arrange the inner-class term index
solely in the prototype vectors to exhibit the eﬀect of the
inner-class term index for the Reuters corpus. Then, we
study the eﬀects of CFC’s parameter b.

4.5.1 Sole Inner-Class Term Index

In this experiment, we only use inner-class index as feature

j
t

DF

|Cj|

weight (i.e., b
) in a prototype vector. Figure 6 shows
that when b is between one and e, CFC’s performance is

generally good. When b is e− 1.8(≈ 0.918), the performance
is signiﬁcantly lower than e − 1.7 (≈ 1.018).

1

0.98

0.96

0.94

0.92

0.9

s
e
r
o
c
s
 
1
F

0.88

 

e−1.8

 

Micro F1
Macro F1

e−1.7

e−1.6
b

e−1.5

e−1.2 e−1.0

e

Figure 6: Sensitivity study parameter b of formula

DFt
|Cj |

b

for the Reuters corpus.

4.5.2 Study on CFC’s Parameter b

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

s
e
r
o
c
s
 
1
F

0.78

 

e−1.8

 

Micro F1
Macro F1

e−1.7

e−1.6
b

e−1.5

e−1.2 e−1.0

e

Figure 7: Sensitivity study of CFC’s parameter b for
the 20-newsgroup corpus.

Previous experiment hints that parameter b performs best
when slightly larger than one. In this experiment, we study
CFC’s performance with varying values of b. Figure 7 and
Figure 8 shows the results for the 20-newsgroup corpus and
the Reuters corpus, respectively. We can observe values
larger than one are signiﬁcantly better than e − 1.8. When
larger than one, increasing b lowers performance. So, e− 1.7
works well in practice and is used as the default value for
parameter b in our experiments.

5. RELATED WORK

A number of automated TC approaches train their models
from both positive and negative examples, such as Decision
Tree (DT) [17, 35, 7], Rocchio [26], and SVM [12, 5]. DT
employs a recursive growing and pruning branch method
until every leaf node has only one class label. The Roc-
chio method builds a model that rewards a document for its
closeness to positive examples and its distance from negative
examples. SVM selects support vectors in both negative and
positive examples to ﬁnd a hyperplane that optimizes deci-
sion surface. Diﬀerent from these approaches, CFC does

WWW 2009 MADRID!Track: Data Mining / Session: Learning2081

0.98

0.96

0.94

0.92

0.9

s
e
r
o
c
s
 

1
F

0.88

 

e−1.8

 

Micro F1
Macro F1

e−1.7

e−1.6
b

e−1.5

e−1.2 e−1.0

e

Figure 8: Sensitivity Study of CFC’s parameter b
for the Reuters corpus.

not use the negative and positive examples directly in its
model construction. Instead, the distributions of terms are
used to derive CFC’s model, i.e., the centroid. As a result,
constructing centroids in CFC only incurs linear-time cost.
Previous work extracts many diﬀerent features from doc-
uments, such as TF, IDF, information gain [13], mutual in-
formation [27, 23], Chi-square [39, 28], and odds ratio [30,
28]. Usually, feature selection metrics are functions of four
dependency tuples [30]:

1. (t, ci): presence of term t and membership in category

ci;

2. (t, ci): presence of term t and non-membership in cat-

egory ci;

3. (¯t, ci): absence of t and membership in category ci;

4. (¯t, ci): absence of t and non-membership in category

ci.

CFC counts the occurrence of a term in a category, i.e., only
the ﬁrst tuple, while some metrics (e.g., Chi-square and odds
ratio) also use other tuples. CFC’s inter-class term index is
most similar to IDF [25, 9], but diﬀer in the fact that CFC
counts the number of categories containing speciﬁc terms.
Unlike TF-IDF, CFC proposes a novel combination of inter-
class and inner-class indices for computing feature weights.
Many web applications [2, 38, 22] have used TC for vari-
ous purposes. Xue et al. [38] discussed the problem of clas-
sifying documents into a large-scale hierarchy, which ﬁrst
acquires a candidate category set for a given document, and
then performs classiﬁcation on a small subset of the origi-
nal hierarchy. Search engines can use TC to improve their
query classiﬁcation results [2]. Our CFC classiﬁer is fast on
training and classiﬁcation, and can be easily updated with
incremental data. Thus, CFC could be used in these web
applications, providing TC support.

6. CONCLUSIONS

We designed the Class-Feature-Centroid (CFC) classiﬁer
for text categorization and compared its performance with
SVM and centroid-based approaches. Experimental results
on Reuters-21578 corpus and 20-newsgroup email collection
show that CFC consistently outperforms SVM and centroid-
based approaches with both micro-F1 and macro-F1 evalu-
ations on multi-class, single-label TC tasks. Additionally,

when data is sparse, CFC has a much better performance
and is more robust than SVM.

Our CFC classiﬁer proposes a novel centroid incorporating
both inter-class and inner-class term indices. Both can be
eﬃciently computed from a corpus in linear time and can be
incrementally updated.

In the testing phase, CFC adopts a denormalized cosine
measure, instead of a normalized prototype vector. This
is to preserve prototype vectors’ discriminative ability and
enlarging eﬀect. Experimental results demonstrate that this
denormalized approach is more eﬀective for CFC.

We are applying the CFC approach to the multi-class,
multi-label TC task, preliminary results have been promis-
ing. We will continue this eﬀort in the future. Additionally,
we plan to investigate the performance of CFC for large-scale
web documents.

Source code for this work is available at http://epcc.

sjtu.edu.cn/~jzhou/research/cfc/.

Acknowledgment
We would like to thank Tao Yang and the anonymous re-
viewers for their insightful comments on this paper. This
work was supported in part by 863 Program of China (Grant
No. 2006AA01Z172, 2006AA01Z199, and 2008AA01Z106),
National Natural Science Foundation of China (Grant No.
60533040, 60811130528, and 60773089), and Shanghai Pu-
jiang Program (Grant No. 07pj14049).

7. REFERENCES
[1] M. Benkhalifa, A. Mouradi, and H. Bouyakhf.

Integrating external knowledge to supplement training
data in semi-supervised learning for text
categorization. Information Retrieval, 4(2):91–113,
2001.

[2] A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi,

V. Josifovski, and T. Zhang. Robust classiﬁcation of
rare queries using web knowledge. In Proceedings of
the 30th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 231–238, Amsterdam,
The Netherlands, 2007.

[3] Z. Cataltepe and E. Aygun. An improvement of

centroid-based classiﬁcation algorithm for text
classiﬁcation. IEEE 23rd International Conference on
Data Engineering Workshop, 1-2:952–956, 2007.

[4] R. N. Chau, C. S. Yeh, and K. A. Smith. A neural

network model for hierarchical multilingual text
categorization. Advances in Neural Networks, LNCS,
3497:238–245, 2005.

[5] S. Dumais and H. Chen. Hierarchical classiﬁcation of

Web content. In Proceedings of the 23rd Annual
International ACM SIGIR conference on Research and
Development in Information Retrieval, pages 256–263,
Athens, Greece, 2000.

[6] P. Frasconi, G. Soda, and A. Vullo. Text

categorization for multi-page documents: a hybrid
naive Bayes HMM approach. In Proceedings of the 1st
ACM/IEEE-CS joint conference on Digital libraries,
pages 11–20. ACM Press New York, NY, USA, 2001.

[7] S. Gao, W. Wu, C. H. Lee, and T. S. Chua. A

maximal ﬁgure-of-merit (MFoM)-learning approach to

WWW 2009 MADRID!Track: Data Mining / Session: Learning209robust classiﬁer design for text categorization. ACM
Transactions on Information Systems, 24(2):190–218,
2006.

[8] G. D. Guo, H. Wang, D. Bell, Y. X. Bi, and K. Greer.

Using kNN model for automatic text categorization.
Soft Computing, 10(5):423–430, 2006.

[9] B. How and K. Narayanan. An Empirical Study of
Feature Selection for Text Categorization based on
Term Weightage. WI 2004, pages 599–602, 2004.

[10] A. M. Kibriya, E. Frank, B. Pfahringer, and
G. Holmes. Multinomial naive bayes for text
categorization revisited. AI 2004: Advances in
Artiﬁcial Intelligence, 3339:488–499, 2004.

[11] H. Kim, P. Howland, and H. Park. Dimension

Reduction in Text Classiﬁcation with Support Vector
Machines. Journal of Machine Learning Research,
6(1):37–53, 2006.

[12] R. Klinkenberg and T. Joachims. Detecting Concept

Drift with Support Vector Machines. In Proceedings of
the Seventeenth International Conference on Machine
Learning, pages 487–494, 2000.

[13] C. K. Lee and G. G. Lee. Information gain and

divergence-based feature selection for machine
learning-based text categorization. Information
Processing and Management, 42(1):155–165, 2006.
[14] V. Lertnattee and T. Theeramunkong. Combining

homogeneous classiﬁers for centroid-based text
classiﬁcation. ISCC 2002, pages 1034–1039, 2002.

[15] V. Lertnattee and T. Theeramunkong. Eﬀect of term

distributions on centroid-based text categorization.
Information Sciences, 158:89–115, 2004.

[16] V. Lertnattee and T. Theeramunkong. Class

normalization in centroid-based text categorization.
Information Sciences, 176(12):1712–1738, 2006.

[17] D. Lewis and J. Catlett. Heterogeneous uncertainty

sampling for supervised learning. In Proceedings of the
Eleventh International Conference on Machine
Learning, pages 148–156, 1994.

J. Riedl. Grouplens: An open architecture for
collaborative ﬁltering of netnews. In Proceedings of
ACM Conference on Computer Supported Cooperative
Work, pages 175–186, Chapel Hill, NC, 1994.

[25] S. Robertson. Understanding inverse document

frequency: on theoretical arguments for IDF. Journal
of Documentation, 60:503–520, 2004.

[26] R. Schapire, Y. Singer, and A. Singhal. Boosting and
Rocchio applied to text ﬁltering. In Proceedings of the
21st International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 215–223, Melbourne, Australia, 1998.

[27] K. M. Schneider. Weighted average pointwise mutual

information for feature selection in text categorization.
Knowledge Discovery in Databases: PKDD 2005,
3721:252–263, 2005.

[28] F. Sebastiani. Machine learning in automated text

categorization. ACM Computing Surveys, 34(1):1–47,
2002.

[29] S. Shankar and G. Karypis. Weight Adjustment

Schemes for a Centroid Based Classiﬁer. Army High
Performance Computing Research Center, 2000.

[30] P. Soucy and G. W. Mineau. Feature selection
strategies for text categorization. Advances in
Artiﬁcial Intelligence, Proceedings, 2671:505–509,
2003.

[31] V. Tam, A. Santoso, and R. Setiono. A comparative

study of centroid-based, neighborhood-based and
statistical approaches for eﬀective document
categorization. 16th International Conference on
Pattern Recogniton, Iv:235–238, 2002.

[32] S. Tan. Large margin DragPushing strategy for

centroid text categorization. Expert Systems with
Applications, 33(1):215–220, 2007.

[33] S. Tan. Using hypothesis margin to boost centroid

text classiﬁer. In Proceedings of the 2007 ACM
Symposium on Applied Computing, pages 398–403,
Seoul, Korea, 2007.

[18] Y. Liu, Y. Yang, and J. Carbonell. Boosting to correct

[34] S. Tan. An improved centroid classiﬁer for text

inductive bias in text classiﬁcation. In Proc. of the
11th International Conference on Information and
Knowledge Management, pages 348–355, McLean, VA,
2002.

[19] S. Martin, J. Liermann, and H. Ney. Algorithms for

bigram and trigram word clustering. Speech
Communication, 24(1):19–37, 1998.

[20] A. K. McCallum. Mallet: A machine learning for

language toolkit. http://mallet.cs.umass.edu, 2002.

[21] E. Montanes, I. Diaz, J. Ranilla, E. F. Combarro, and

J. Fernandez. Scoring and selecting terms for text
categorization. IEEE Intelligent Systems, 20(3):40–47,
2005.

[22] X. Ni, G. Xue, X. Ling, Y. Yu, and Q. Yang.

Exploring in the weblog space by detecting informative
and aﬀective articles. In WWW, Banﬀ, Canada, 2007.

[23] Z. L. Pei, X. H. Shi, M. Marchese, and Y. C. Liang.

An enhanced text categorization method based on
improved text frequency approach and mutual
information algorithm. Progress in Natural Science,
17(12):1494–1500, 2007.

[24] P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and

categorization. Expert Systems with Applications,
35(1-2):279–285, 2008.

[35] S. Weiss, C. Apte, F. Damerau, D. Johnson, F. Oles,

T. Goetz, and T. Hampp. Maximizing text-mining
performance. IEEE Intelligent Systems, pages 63–69,
1999.

[36] H. Wu, T. Phang, B. Liu, and X. Li. A reﬁnement

approach to handling model misﬁt in text
categorization. In Proceedings of 8th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 207–216, Alberta, Canada, 2002.

[37] J. Xu and W. Croft. Corpus-based stemming using

cooccurrence of word variants. ACM Transactions on
Information Systems (TOIS), 16(1):61–81, 1998.

[38] G. Xue, D. Xing, Q. Yang, and Y. Yu. Deep

classiﬁcation in large-scale text hierarchies. In
Proceedings of the 31st Annual International ACM
SIGIR Conference, pages 627–634, Singapore, 2008.

[39] Y. Yang and J. O. Pedersen. A comparative study on

feature selection in text categorization. Machine
Learning, pages 412–420, 1997.

WWW 2009 MADRID!Track: Data Mining / Session: Learning210