Network-Aware Forward Caching

Jeffrey Erman, Alexandre Gerber, Mohammad T. Hajiaghayi, Dan Pei, Oliver Spatscheck

erman,gerber,hajiagha,peidan,spatsch}@research.att.com

AT&T Labs Research

New Jersey, USA

ABSTRACT
This paper proposes and evaluates a Network Aware Forward
Caching approach for determining the optimal deployment strategy
of forward caches to a network. A key advantage of this approach
is that we can reduce the network costs associated with forward
caching to maximize the beneﬁt obtained from their deployment.
We show in our simulation that a 37% increase to net beneﬁts could
be achieved over the standard method of full cache deployment to
cache all POPs trafﬁc. In addition, we show that this maximal point
occurs when only 68% of the total trafﬁc is cached.

Another contribution of this paper is the analysis we use to moti-
vate and evaluate this problem. We characterize the Internet trafﬁc
of 100K subscribers of a US residential broadband provider. We
use both layer 4 and layer 7 analysis to investigate the trafﬁc vol-
umes of the ﬂows as well as study the general characteristics of
the applications used. We show that HTTP is a dominant protocol
and account for 68% of the total downstream trafﬁc and that 34%
of that trafﬁc is multimedia. In addition, we show that multime-
dia content using HTTP exhibits a 83% annualized growth rate and
other HTTP trafﬁc has a 53% growth rate versus the 26% over all
annual growth rate of broadband trafﬁc. This shows that HTTP
trafﬁc will become ever more dominent and increase the poten-
tial caching opportunities. Furthermore, we characterize the core
backbone trafﬁc of this broadband provider to measure the distance
travelled by content and trafﬁc. We ﬁnd that CDN trafﬁc is much
more efﬁcient than P2P content and that there is large skew in the
Air Miles between POP in a typical network. Our ﬁndings show
that there are many opportunties in broadband provider networks
to optimize how trafﬁc is delivered and cached.

Categories and Subject Descriptors
D.4.8 [Performance]: Measurements—web caching

General Terms
Networking Optimization

1.

INTRODUCTION

Over the past decade, as the new “killer" Internet applications
emerge, new content delivery mechanisms are invented to meet the
demand of these new applications. This is evidenced by the fol-
lowing two examples. Web (text/image) is the ﬁrst “killer" Internet
application, thus HTTP protocol dominated Internet trafﬁc usage
in the ﬁrst several years of the Internet [10, 29]. Web caches and
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW  2009,  A pril  20–24,  2009,  Madrid,  S pain.
ACM 978-1-60558-487-4/09/04.

Content Distribution Networks (CDNs) were thus invented to im-
prove Web performance. When Peer-to-Peer (P2P) ﬁle sharing be-
came popular a few years later, it was shown to be dominant in
studies conducted in 2002 to 2005 for DSL, cable, and ﬁber broad-
band networks [7,15,22]. A lot of approaches have been proposed
to improve the delivery efﬁciency of P2P. With the recent emer-
gence of user-generated video, social networking, and TV/Movie-
on-demand services, most of which run on top of HTTP, we sus-
pect that HTTP protocol has become the dominant content delivery
protocol, especially for multimedia (video/audio) content. For ex-
ample, an informal report in a lightreading article in 2006 [1] in-
dicates that “network operators are reporting a rise in overall web
trafﬁc and a rise in HTTP video streaming". Furthermore, the mul-
timedia content we see today might be just the tip of the iceberg of
what is coming in the next decade as more providers join the busi-
ness and make more, higher-deﬁnition content available and more
subscribers access it. As such, we believe it is timely and impor-
tant to investigate the new opportunities for more efﬁcient delivery
mechanisms for HTTP trafﬁc again.

A natural solution is forward caching (proposed in the 90s),
which is to deploy HTTP caches within an Internet Service
Provider’s (ISP) network caching all cacheable HTTP trafﬁc ac-
cessed by the customers of the ISP. Caching makes intuitive sense
in that Internet content popularity is often very skewed thus offering
good opportunity for reusing. This is especially true for video con-
tent, as shown in recent studies [5,6,16]. Unfortunately, most large
US based ISPs currently do not operate forward caches within their
network. The main reason for this decision lies in the economics of
deploying forward caches. Forward caches are additional hardware
components (typically UNIX servers) which have to be purchased,
deployed and managed at a large number of locations. In the US
the bandwidth savings can often not justify the cost of such a server
deployment. To reduce the costs associated with forward caching
in an ISP, we believe that the most cost effective way of deploying
forward caches is to only deploy them in selected POPs (Point of
Presences) caching only selected expensive-to-deliver content. We
call this approach Network Aware Forward Caching.

To motivate and justify our solution, we ﬁrst provide a systematic
measurement study of the characteristics of the content transmitted
over the Internet today and shows the dominance of HTTP trafﬁc,
followed by the comparison of the efﬁciency of existing delivery
mechanisms (HTTP, CDN and P2P), and the cacheability of HTTP
content. We then formulate the cache placement problem, and pro-
pose and evaluate our heuristics.

To show how dominant HTTP content is compared to other types
of content transmitted over the Internet today, we characterize the
Internet trafﬁc of 100K subscribers of a US residential broadband
provider. Using both layer 4 and layer 7 analysis, we conﬁrm our

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 291hypothesis that HTTP is the dominant protocol, which contributes
to 68% of the total downstream peak trafﬁc. Furthermore, HTTP
has become the workhorse for data delivery: 80% of multimedia
content uses HTTP as its distribution protocol (mainly ﬂash video)
and ﬁle downloads via HTTP contributes 10% of downstream traf-
ﬁc in contrast to 0.3% for FTP. This is a drastic change since a few
years ago, when HTTP contributed to 9% of the total trafﬁc [22],
P2P was dominiant [7, 15, 22], and 63% of total residential volume
was user-to-user trafﬁc [7]. Our results show that recently the vol-
ume of HTTP content per subscriber is increasing much faster than
the 26% overall increase rate: multimedia streams over HTTP and
other HTTP trafﬁc exhibits a 83% and a 53% annualized growth
rate, respectively. This means that HTTP’s share will even increase
further.

Our study shows that among the existing delivery mechanisms,
CDNs are currently serving already a signiﬁcant portion (46%) of
the large ﬁle transfers in the network and are efﬁcient in bring-
ing the content closer to the content consumers, much better than
the existing P2P technologies or typical HTTP content providers
by a factor of 2 to 3 when comparing average bit-distance. Dis-
tance travelled on the network is strongly related to the network
cost of the trafﬁc. Furthermore, the distance traversed on a net-
work by different sources of trafﬁc to different points of presence
(POPs) varies signiﬁcantly. The trafﬁc to some remote POPs can
beneﬁt signiﬁcantly from better delivery mechanisms such as for-
ward caching.
In the HTTP trafﬁc cacheability study, we found
that 60% of the trafﬁc can potentially be reused overall because it
is requested more than once. However, only 33% of the content is
suited for an optimized delivery infrastructure when adding more
realistic constraints.

Finally, based on these observations, we introduce and evaluate
our new proposal, network aware forward caching, that increases
efﬁciency, reduces backbone trafﬁc and network costs and in-
creases end-user performance. Contrary to a simple all-or-nothing
forward caching deployment in a network, we argue that, by being
network aware, partial deployments of forward caches for a sub-
set of the POPs and a subset of the trafﬁc sources provides greater
beneﬁts per dollar invested. Indeed, we just showed the disparity
in efﬁciency and the fact that some sources, such as CDNs are al-
ready efﬁcient and, therefore, don’t need to be cached. In addition
to that observation about the differences in efﬁciency of each source
to each POP, we also note that an Internet Service Provider incurs
different costs based on the nature of the neighbor sending the traf-
ﬁc (e.g.
transit trafﬁc is more expensive than peering trafﬁc, and
customer trafﬁc even generates revenue), and we include this addi-
tional dimension in our decision process when selecting the content
that will be stored on the caches. We formulate the problem of de-
termining the optimal deployment, and shows that it is NP-hard.
We propose one pseudo-polynomial algorithm that solves the exact
problem, and a greedy heuristic algorithm that is much faster. Our
evaluation of the heuristic algorithm using realistic data shows that
the optimal deployment in terms of network costs occurs when only
68% of the total trafﬁc is cached. Moreover, that solution is clearly
senstive to the backbone costs and the caching costs.

The remainder of the paper is organized as follows. Section 2 de-
scribes our measurement methodology. Section 3 presents over-all
content composition results. Section 4 studies how current deliv-
ery mechanisms work, and Section 5 presents the content cacha-
bility results. Section 6 presents the formulation, algorithms, and
evaluation results for our network aware forward caching approach.
Section 7 reviews related work, and Section 8 concludes the paper.

2. MEASUREMENT METHODOLOGY

This section presents our network monitoring infrastructure and
data sets used in the analysis of this paper. To achieve our ob-
jectives in this paper of characterizing broadband trafﬁc usage and
evaluating our network aware caching approach, we obtained traces
from multiple vantage points of a US Broadband Providers network
to understand the different aspects concerning the delivery of con-
tent to a subscriber. In particular, the three types of data sets that
we utilize in our analysis are as follows: aggregated trafﬁc records
from broadband subscriber aggregation access points, aggregated
netﬂow records of the core backbone trafﬁc, and unsampled HTTP
header records from a single aggregation point. In the following
subsections we elaborate on each type data set. Our analysis is
based on a US Provider and networks in different geographic loca-
tions may not exhibit the same characteristics.

2.1 Access Trafﬁc

Our network monitoring infrastructure [11] consists of ﬁve net-
work monitors analyzing trafﬁc from 100K subscribers of a US
broadband provider. The monitors are diversely located on ﬁve
BRAS (Broadband Remote Access Servers; an aggregation point
of Digital Subscribers Lines, or DSLs) in three states (California,
Texas and Illinois). The subscriber data we analyzed was from the
week of February 25, 2007 to September 30, 2008.

For our study, network monitors summarized the observed traf-
ﬁc volumes every 5 minutes into ﬂow records. The ﬂow records
measure the number of packets and bytes for each identiﬁed appli-
cations class (described below). To reduce resource consumption,
the network monitors perform a combination of packet sampling
and ﬂow sampling when computing these ﬂow records [12]. The
5-minute ﬂow records are used to compute hourly summaries of the
aggregate trafﬁc from all the subscribers observed at the monitor.

Our application classiﬁcation relies on application header,
heuristic, and port number analysis to determine a ﬁnal classiﬁ-
cation of a ﬂow into an application class.

Overall, we classify our trafﬁc into 16 categories: Web (HTTP),
Multimedia (HTTP, RTSP, RMTP, etc), File Sharing (P2P), Games
(Quake, WoW), Net News (NNTP), Business (VPN, Databases),
VoIP (SIP), Chat (IM, IRC), Mail (POP3, IMAP), DNS, FTP, Secu-
rity Threats, Other, ICMP, Other-TCP and Other-UDP. (The exam-
ples in brackets are non-exhaustive of what we identify.) We base
these categories on a determination of the use of the data and not
explicitly on the protocol. This is most signiﬁcant for HTTP traf-
ﬁc, which we classiﬁed into either Web or Multimedia category.
Therefore, we separate HTTP trafﬁc by mime type–if the mime
type of a ﬂow is for a video or audio format we classify this ﬂow as
HTTP Multimedia instead of HTTP Web. In addition, Gnutella and
BitTorrent tracker-based HTTP trafﬁc is classiﬁed as those speciﬁc
P2P applications.

We adopt many of the same packet payload signatures described
by Sen et al. [25] and Karagiannis et al. [20]. We also use addi-
tional signatures to identify application subclasses. For instance, as
outlined above we use the mime type information in HTTP ﬂows
to further classify HTTP ﬂows. In addition, we extract from the
control channel the information needed to identify the data channel
of the FTP, RTSP, and Skype protocols.

Additional P2P trafﬁc was identiﬁed with other P2P speciﬁc
heuristics. For example, we use the announced port in the tracker
messages for BitTorrent to identify incoming encrypted ﬂows as
P2P. However, due to P2P applications such as BitTorrent using
encryption to obfuscate their protocols, we believe much of our un-
known TCP trafﬁc (in the Other-TCP class) is P2P as well. We
have based this inference on additional analysis we performed on

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 292the Other-TCP trafﬁc. In particular, we have found that based on
the ﬂow characteristics and the payload information in the ﬂows
this trafﬁc is consistent with encrypted P2P trafﬁc. Generating and
validation of additional signatures for encrypted P2P trafﬁc is left
as future work.

Lastly, we use some layer 4 port numbers to identify any remain-

ing trafﬁc not classiﬁed using signatures and heuristics.
2.2 Core Backbone Trafﬁc

Another type of data utilized in our analysis are aggregated Net-
Flow [9] records from the core backbone of the network. This data
allows us to measure the ingress and egress trafﬁc on the various
Peering, Transit or On-Net links of the network. The data used
in our analysis is from September 27, 2008 until October 5, 2008.
The data sets were obtained using a NetFlow collected on the back-
bone router of a US Broadband Provider. These traces provide the
amount of ingress and egress trafﬁc volume between core routers.
To minimize the performance impact on the routers, we used de-
terministic packet sampling with a rate of 1 packet out of 500. We
then use smart sampling [12] to further reduce the data volume.
2.3 HTTP Trafﬁc

To facilitate the study of HTTP cacheability, we analyzed the
HTTP header data from approximately 20K subscribers during
the week of January 27, 2008 to February 2, 2008. During our
analysis we correlated the HTTP requests with the actual TCP
ﬂows to obtain the actual ﬂow sizes. This step was taken be-
cause not all content sizes are available in the Content-Length
ﬁeld of the HTTP header. For instance, some objects such as dy-
namically generated pages or streaming content will sometimes
have a Content-Length of 0.
In our analysis we used from the
HTTP header ﬁelds the GET, POST, Cache-Control, Pragma, and
Content-Length ﬁelds of the HTTP header and the IP address of the
web server. All other information such as the subscriber IP address
and cookie values were not analyzed. In total, we analyzed approx-
imately 550 million requests representing 45 TB of trafﬁc with an
average request size of 91 KB. Unlike the smart-sampled records
in the previous two types of data sets we use, the HTTP data in our
analysis is unsampled and includes all requests.

3. BROADBAND TRAFFIC OVERVIEW

This section provides an overview of the growth and overall ap-
plication breakdown in the broadband data we studied. We show
that the volume and fraction of multimedia content delivered using
HTTP is increasing rapidly and that P2P trafﬁc as a percentage of
trafﬁc is actually decreasing. HTTP is more generally becoming the
protocol of choice for various kinds of activities such as software
distribution, multimedia and P2P applications.
3.1 Growth of Broadband Trafﬁc

Figure 1 shows that a large US broadband ISP saw a relatively
stable growth rate of the average downstream trafﬁc per subscriber
of 26% per year, with an above average growth rate for the last 2
years. An interesting observation when looking closer at the busy
hour trafﬁc per subscriber is that, while in the ﬁrst years of this
period, the average downstream trafﬁc per subscriber was grow-
ing faster than the downstream trafﬁc per subscriber, the opposite
can now be observed in the last 2 years. This indicates that syn-
chronous applications (e.g. instant watching multimedia streams)
are recently growing faster than asynchronous applications (e.g.
P2P ﬁle sharing). It is also important to keep in mind that the be-
havior during the busy hour is what really matters. The Internet
infrastructure is engineered for the peak demand and that generally

l

e
m
u
o
V
 
c
i
f
f

 

a
r
T
d
e
z

i
l

a
m
r
o
N

Average Peak Traffic
f(x)=1.0075*exp(0.0037x)
Average Traffic
g(x)=1.0183*exp(0.0044x)

 6
 5.5
 5
 4.5
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
2/17/20011/1/2002 1/1/2003 1/1/2004 1/1/2005 1/1/2006 1/1/2007 1/1/2008

Time

Figure 1: Weekly average downstream trafﬁc per subscriber
during the busy hour based on several million subscribers

l

e
m
u
o
V
 
c
i
f
f

 

a
r
T
d
e
z

i
l

a
m
r
o
N

 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
2/25/2007 4/1/2007

6/3/2007

8/5/2007

10/7/2007

12/9/2007

Time

HTTP Web
HTTP Multimedia
Multimedia Other
FileSharing

Other-TCP
Other-UDP
NetNews

Figure 2: Normalized Weekly trafﬁc per Application Class dur-
ing the Busy Hour

has spare capacity the rest of the time. The busy hour is deﬁned as
the 1-hour time span during a day which exhibits the largest aver-
age trafﬁc.

Figure 2 shows the weekly normalized application trafﬁc vol-
umes during the busy hour based on our aggregate BRAS trafﬁc
records. There are two major trends that can be seen. The ﬁrst
is that HTTP trafﬁc, both Web and Multimedia, shows consistent
growth during the busy hour over the observed period. In the ﬁg-
ure, HTTP Multimedia and HTTP Web trafﬁc exhibit a 83.1% and
a 52.9% annualized growth rate, respectively. The second is that
P2P trafﬁc has remained steady and shows a decline in its percent-
age share of the overall trafﬁc mix. These may be important obser-
vations as they contradict reports claiming that P2P trafﬁc is con-
tinuing to increase at dramatic rates. The growth of HTTP trafﬁc
and especially the HTTP Multimedia trafﬁc is the most signiﬁcant
cause of broadband subscribers trafﬁc growth.

3.2 Broadband Subscriber Trafﬁc Mix

Next, we look at the characterization of the overall application
mixture of the trafﬁc. Table 1 shows the application mixture of
the downstream and upstream trafﬁc at all ﬁve monitors during the
week of January 28, 2008 to February 3, 2008. The downstream
trafﬁc volumes are typically 3 times greater than the upstream traf-
ﬁc volumes during the busy hour. HTTP is the dominant protocol
and accounts for the largest volume of trafﬁc on the network and

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 293Table 1: Application Mix During the Busy Hour

Table 3: Multimedia Breakdown (includes HTTP Multimedia)

Class

HTTP Web
HTTP Multimedia
FileSharing
Other-TCP
Multimedia Other
Other-UDP
Games
NetNews
Business
Voip
Chat
Mail
Dns
Ftp
SecurityThreat
Other
ICMP

Downstream

Upstream

Busy Hour
41.8%
25.8%
9%
8.2%
4.7%
2.5%
1.4%
1.4%
1.1%
1%
0.6%
0.5%
0.5%
0.3%
0.2%
0%
0%

Average
39.8%
21.5%
12.3%
9.7%
4.7%
3%
1.3%
1.6%
1.7%
1.1%
0.5%
0.8%
0.4%
0.3%
0.4%
0%
0%

Busy Hour
23%
2.7%
25%
26.2%
2.1%
10.3%
3.2%
0.1%
2.2%
1.1%
0.5%
0.9%
0.6%
0.4%
0.5%
0%
0.3%

Average
17.5%
1.9%
27.6%
30.4%
1.7%
10.2%
2.3%
0.1%
2.8%
0.9%
0.4%
1.2%
0.4%
0.3%
0.7%
0.1%
0.3%

Table 2: HTTP Breakdown

Class
/http/video
/http/text-image
/http/download
/http/javascript
/http/audio
/http/other
/http/ﬂash
/http/https
/http/otherapp
/http/xml
/http/binary
/http/ofﬁce
/http/rss

% Busy Hour Trafﬁc % Average Trafﬁc
31.9%
25.9%
16.2%
5.8%
5.5%
4.4%
4.4%
2.8%
1.2%
1.1%
0%
0.1%
0%

30%
25.6%
18.9%
5.6%
4.4%
4.4%
4.1%
3.7%
1.5%
1%
0.2%
0.1%
0%

accounts for 68% of the downstream trafﬁc during the busy hour.
HTTP has also taken over ﬁle downloads and FTP is now only a
very small percentage of the overall trafﬁc.

P2P makes up at least 9% of the downstream trafﬁc during the
busy hour and 12.3% of all downstream trafﬁc. If we assume in part
that much of the TCP-Other trafﬁc is due to encrypted or uniden-
tiﬁed P2P trafﬁc, then P2P still makes up a maximum of 17% of
the downstream trafﬁc. As we have already noted, P2P’s percent-
age share of the overall trafﬁc has been decreasing. However, P2P
is still the dominant protocol in the upstream. Because P2P proto-
cols are designed to exploit subcribers’ sharing of data, this class of
trafﬁc has more symmetrical data ﬂows than Web/Multimedia pro-
tocols that are generally asymmetric [2]. The volume of P2P trafﬁc
has been quoted with various statistics in the media [4] and litera-
ture [19]. In many cases, the value quoted is the upstream volume.
Depending on the time and direction we could state a number be-
tween 17% to 58%. In a DSL environment, upstream link capacity
is dedicated per subscriber and shared backbone links have sym-
metric capacity. Therefore, upstream trafﬁc is not a problem and
the trafﬁc in the busiest direction in the busy hour is more of inter-
est (i.e., the downstream direction). However, for cable-based ISPs,
the P2P upstream volume may be more of a factor as subscribers
share the capacity of local cable lines.

Network News accounts for a surprising amount of the trafﬁc.
This is due to NetNews being used to download large ﬁles such as
movies, music, and software.

We turn next to the categories of HTTP and Multimedia to get a
better understanding of speciﬁc protocols and applications that are
used.

Table 2 shows the breakdown of the subcategories for HTTP traf-
ﬁc (HTTP Web in Table 1). When a ﬁle is requested using the
HTTP protocol, the HTTP header in the servers response includes
a Content-Type ﬁeld which contains the mime type of the ﬁle.
We have based these subcategories primarily on the mime types ex-

Class
/http/video
/http/audio
/multimedia/rtmp
/multimedia/rtsp
/multimedia/rtp
/multimedia/shoutcast
/multimedia/ms-streaming
/multimedia/other
/multimedia/realaudio
/multimedia/h323

% Busy Hour Trafﬁc % Average Trafﬁc
70.9%
12.3%
7.3%
5.5%
2.5%
0.5%
0.4%
0.1%
0.1%
0%

70.1%
10.4%
7.9%
6.6%
3%
0.8%
0.5%
0.2%
0.1%
0%

tracted from the HTTP stream. For example, an image ﬁle might
have the mime type of image/gif and would be classiﬁed as
/web/text-image. In our categories, we group similar mime types
together.

The /web/no-http subcategory is not based on mime type. This
category is for trafﬁc on a standard HTTP port that does not use the
HTTP protocol. This unidentiﬁed trafﬁc makes up 7.1% of the web
trafﬁc. Though there is no trafﬁc shaping to be evaded on the broad-
band network being studied, this could be the result of encrypted
P2P applications using a default HTTP port to avoid ﬁrewalls and
trafﬁc shaping on other ISP networks.

An interesting category is the /web/download. This trafﬁc is for
the download of compressed (e.g., .zip, .rar., .tar.gz) or executable
ﬁles (e.g., .exe). This shows that HTTP has replaced FTP as the dis-
tribution mechanism for these types of ﬁles and software patches.
HTTP download trafﬁc accounts for 10.2% of all trafﬁc, whereas,
the volume of FTP trafﬁc is a quite small, 0.3%, as shown in Ta-
ble 1.

Table 3 shows the breakdown of Multimedia trafﬁc (HTTP Mul-
timedia in Table 1) into subcategories. HTTP is used to provide
more than 80% of the multimedia data, substantially more than
the 20% of multimedia trafﬁc supported by traditional multimedia
streaming protocols such as RTSP and RTMP. Upon further inves-
tigation, we found that 85% of the /web/video is ﬂash video (ﬂv)
used by popular User Generated Content (UGC) sites like YouTube
to deliver video content. These UGC sites account for about 40% of
total multimedia trafﬁc. Some of the possible reasons why HTTP is
used to provide so much of the multimedia content on the Internet
could be the result of no license fee costs for streaming servers (as
required by traditional multimedia streaming protocols), compata-
bility between operating systems, ability to easily traverse ﬁrewalls,
and ease of integration into CDN services.

The conclusions we draw from this section are that HTTP is the
prevalent protocol on the Internet, accounting for 66% of the traf-
ﬁc and is the main driver of per subscriber broadband trafﬁc grow
today. It is very much the workhorse for data delivery and is very
versatile. HTTP has moved beyond its historical role in delivery of
web text and image content and is increasingly being used to han-
dle most of the Internet’s tasks, such as distribution of software, up-
dates, patches, and multimedia, and by P2P applications (gnutella,
torrent trackers, torrent distribution).

4. DELIVERY MECHANISMS

4.1 Efﬁciency of Content Distribution

An important consideration for the performance of a data dis-
tribution mechanisms is the distance the data travels to reach the
broadband subscriber. The distance traveled directly affects the ef-
ﬁciency of data delivery, and minimizing it is of interest to both the
broadband subscriber and the ISP. Average bit-distance traveled is
strongly related to the direct network costs associated with transfer-

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 294s
e

l
i

M

 
r
i

 

A
e
v
i
t

l

a
e
R

 1

 0.8

 0.6

 0.4

 0.2

 0

On-Net

All Traffic
CDN 1
CDN 2
CDN 3
Content Provider 1

Content Provider 2
Content Provider 3
Web
P2P

s
e

l
i

M

 
r
i

 

A
e
v
i
t

l

a
e
R

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 0.2

 0.4

 0.6

 0.8

 1

Percentage of POPs

s
e

l
i

M

 
r
i

 

A
e
v
i
t

l

a
e
R

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

Highest POP by ASN
Average POP by ASN
Lowest POP by ASN

 0.2

 0.4

 0.6

 0.8

 1

% of ASNs/Prefixs

Figure 3: Air Miles for Different Content
Providers

Figure 4: Average Air Miles Per POP

Figure 5: Air Miles By ASN/Preﬁx

ring the data. We conducted this part of our study using NetFlow
data from the backbone of a Tier-1 US ISP.

From the perspective of the broadband subscriber, increased
travel distances affects the load time of web pages and ﬁle down-
loads, and reduces throughput (e.g., TCP throughput is limited by
the round trip time). One method content providers use to enhance
the quality of their data delivery is to outsource it to a CDN to
place the data objects closer to the users, for fast, efﬁcient access.
From the perspective of the ISP, the network miles data travels re-
ﬂects the direct cost of delivery of the data over their backbone, so
shorter distances mean lower costs.

We can calculate the efﬁciency and speed of data transfer by
measuring the average bit-distance traveled assuming direct con-
nectivity, which we call Air Miles. We calculate the distance trav-
eled as the direct physical distance between two end points. Thus,
Air Miles can be calculated as: AM = sum of distance each bit trav-
els / total number of bits. In order to remove the impact of interdo-
main routing, we isolate the trafﬁc exchanged between customers
(On-Net trafﬁc), following intradomain routing from the source to
the destination. We would like to note that the best metric to use
would be layer 1 route miles; however, computation of this is difﬁ-
cult and average bit-distance metric is a close approximate of route
miles.

The data we used for our analysis is from September 27, 2008
until October 5, 2008. Note that for this data we only used L4
application mappings to obtain application information.

Figure 3 shows ON-Net air miles of different anonymized con-
tent providers, CDNs, P2P, and Web. The ALL trafﬁc represents
all the trafﬁc including P2P and Web. Overall, the distance trav-
eled by P2P applications are typically 25% longer than the distance
of HTTP trafﬁc. Currently, the CDN’s air miles are 2 to 3 times
lower than P2P and other content providers such as large web sites.
This indicates that CDNs are effective and are having a signiﬁcant
impact on the delivery of data.

There are several conclusions we can draw from this section.
The ﬁrst is that the current generation of P2P applications are quite
inefﬁcient in air miles. However, there have been some progress
recently to address this issue with P2P. For instance, the P4P Work-
ing Group has been working on P4P (Proactive network Provider
Participation for P2P) to use topology information from ISPs to op-
timize P2P trafﬁc on P2P networks. Xie et al. show where broad-
band subscribers experience increased throughput using P2P appli-
cations due to signiﬁcantly more data being served On-Net [8, 31].
The second conclusion is that CDNs are doing a good job of bring-
ing data closer to the end user. This allows the users to have a better
multimedia experience because these large multimedia ﬁles can be
served from a CDN and obtain higher bitrates. This allows more

and better quality content to be consumed and show that large ﬁles
are typically served from CDNs.
4.2 Air Mile Differences Between POPs

Figure 4 shows the distribution of relative air miles travelled to
POPs (Point of Presence) in the network. Figure 5 shows for the
POPs with the highest, lowest, and an average of all POPs the dis-
tribution of air miles to different ASNs. The main observation to
take away from graphs is that on the Broadband Providers network
there are many opportunities for optimizations. The trafﬁc going to
some POPs are signiﬁcantly more expensive, on average, to trans-
port than others. In addition, at each POP there are some ASN that
are signiﬁcantly more expensive, on average, to transport as well.
These observations help to motivate our proposed caching solution
later presented in Section 6.

5. CACHEABILITY

Many applications on the Internet have data ﬂows that could po-
tentially be reused to save network bandwidth because the same
content is being requested more than once. For instance, multiple
requests to the same web page could be cached and either served
from a local or network cache. Another example is multiple users
streaming the same video ﬁle, which could be served using multi-
cast or P2P.

The application classes of Web, Multimedia, P2P, and Network
News are the most likely candidates for transferring content that is
reusable. These trafﬁc classes were shown in Section 3.1 to repre-
sent 89% of the network trafﬁc during the busy hour. Other trafﬁc
classes such as VoIP, Chat and Business are less unlikely to contain
any reusable content in their data ﬂows but represent only 11% of
the network trafﬁc. The analysis in this section is based off of our
HTTP trafﬁc trace. By using these HTTP records we can look at
how much trafﬁc is reusable for Web and Multimedia because 95%
application classes are served using the HTTP protocol.
5.1 Time-To-Live Analysis

The Time-To-Live (TTL) length speciﬁed in Control-Cache
ﬁeld of the HTTP records indicates how long an object should be
kept in a cache before it needs to be refreshed.

Figure 6 shows the CDF of the TTL length speciﬁed by different
ﬁle sizes. We ﬁnd noticeable amounts of records with TTLs set as:
1 hour, 1 day, and 7 days.

We also looked at the impact CDNs have on the temporal char-
acteristics of the trafﬁc. To identify which requests are from a CDN
we employ a similar methodology as Huang et al. [17]. To facil-
itate this analysis we used dumps of the DNS tables used by the
subscribers at the broadband ISP. These dumps were collected on
February 2nd, 2008.

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 2950-1KB
1-5KB
5-10KB
10-100KB
100-500KB
500KB-1MB
1MB-10MB
10MB-100MB
100MB+

F
D
C

 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

All Requests
CDN Requests
Non-CDN Requests

F
D
C

 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

All Bytes
CDN Bytes
Non-CDN Bytes

F
D
C

 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

 1

 10

 100

 1000  10000 100000 1e+06  1e+07
TTL (minutes)

 1

 10

 100

 1000  10000 100000 1e+06  1e+07
TTL (minutes)

 1

 10

 100

 1000  10000 100000 1e+06  1e+07
TTL (minutes)

Figure 6: TTL of Cache Directives for
HTTP Requests

Figure 7: TTL of Cache Directives for Re-
quests

Figure 8: TTL of Cache Directives for
Weighted by Bytes

Greedy
Only No-Cache Directives
TTL Directives and No-Cache Directives

e
h
c
a
C
m
o
r
f
 

 

d
e
v
r
e
S
 
s
e

t
y
B
%

 

 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

Request
Bytes

%
o

 

i
t

a
R

 
t
i

 

H
e
h
c
a
C

 50

 40

 30

 20

 10

 0

 0

 100

 200

 300

 400

 500

Number of Requests (Millions)

1

2

5

10 25 50 75 100 250 500 750 1000 1500

Cache Size (GB)

Figure 9: Webcache Results

Figure 10: Caching Results using Different Cache Sizes

Table 4: Percentage of Bytes for Large Flow vs. Small Flows of
CDNs and Other
<1MB
8.0%
92.0%
100.0%

Overall
100MB +
8.0%
46.1%
53.9%
92.0%
100.0% 100.0%

Class
CDN
Other
Total

1-10MB
7.0%
93.0%
100.0%

10-100MB
18.2%
81.8%
100.0%

We use a two-fold approach to identify CDN requests. The ﬁrst
step in our analysis is to look up the DNS entry for each hostname
in the HTTP requests. If the hostname resolves to a CNAME that
belongs to a CDN provider then we label these requests as CDN
trafﬁc. In the second step, for the remaining trafﬁc we use the server
IP address to resolve the AS number. If the AS number is from a
known CDN provider then we label these requests as CDN trafﬁc
as well.

Figure 7 and Figure 8 show the CDFs of the TTL lengths when
weighted by requests and total bytes, respectively. When compar-
ing both graphs together we observed that while the distribution for
requests and bytes are similar for small TTLs. The TTLs for bytes
is smaller indicating that large ﬁle sizes have larger TTLs on aver-
age than smaller ﬁle sizes. A noticeable artifact in the graph is that
for the byte distribution of CDN requests over 50% of the bytes
have a TTL of 10 days. However, overall we found that there were
no other substantial differences in how the TTLs were set between
CDN and non-CDN trafﬁc that would indicate a difference in the
cachability of the CDN content.

Table 4 shows the percentage of all bytes in the downstream traf-
ﬁc broken down by source. We found that a signiﬁcant amount of
the large ﬁles are being served by CDNs. In particular, over 46% of
ﬂows greater than 100 Mbytes in size are originating from a CDN
today.

5.2 Cache Analysis

Not all HTTP requests are cacheable. This occurs for a vari-
ety of reasons–the web page may contain private information like

a cookie, or is dynamically generated.
In the HTTP 1.1 proto-
col there are two ﬁelds, Cache-Control and Pragma, in the
HTTP header that can be used to indicate cacheability [14]. The
Content-Control ﬁeld can be used by a server to indicate if
the document is not cacheable or the time the document can be
kept before it is stale. The Pragma ﬁeld can be used by the client
to indicate a request for a fresh copy of the ﬁle. For instance, hitting
the refresh button on most browsers causes the HTTP request to be
marked as no-cache when sent. We used both of these ﬁelds in
the HTTP records we analyzed.

If we remove from the HTTP trafﬁc the 42% that is marked non-
cacheable, in total, approximately 60% of all trafﬁc is potentially
reusable. We deﬁne content as reusable if there is more than one
request for a speciﬁc object during the period the object is valid
(e.g. not stale or modiﬁed). The results in Figure 9 assume that the
cache has an unlimited size. Thus, we do not assume removal of
items from the cache.

These assumptions are more general in some cases and pro-
vide a potentially more optimistic result than if explicitly studying
caching. Our methodology of identifying uncacheable documents
and using an unlimited cache size is similar to the approach taken
by Feldmann et al. who completed a comprehensive studies of
HTTP headers in relation to caching in 1999 [13].

We have tested three different algorithms for calculating
reusability. The ﬁrst, which we denote as “Greedy”, is where we
ignore the Content-control and Pragma directives including TTLs.
In our second algorithm, denoted as “No-Cache”, we only respect
the directives that indicate the content should not be cached or not
served from cache. The third algorithm, denoted as “TTL-Cache”,
takes into account all directives and also the TTL values assigned to
each request. We have chosen these three algorithms to take into ac-
count different hypothetical scenarios. The Greedy algorithm pro-
vides us with an upper bound for the potentially cacheable content,
and the TTL-Cache algorithm the lower bound if all the optional
parameters are followed explicitly. However, in reality, caches op-

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 296erate somewhere in between. For instance, some caches serve ob-
jects after they become stale.

Figure 9 shows the caches byte hit ratio as we process our data
set over time. We found with our Greedy algorithm that 92.2% of
requests and 67.9% of bytes could have been served from cache.
With the No-Cache algorithm we found 70.1% of requests and
37.4% of bytes could have been served from cache. Finally, with
the TTL-Cache algorithm 62.0% of the requests and 32.5% of the
bytes would be served from cache. Notice that when taking into
account the TTL of the objects that there is only a small 5% change
in the bytes served from cache.

We also found that the total cache size using the Greedy and
No-Cache algorithms was 17.4TB and 11.3TB, respectively. These
cache sizes may be economically infeasible to deploy in all scenar-
ios. However, in today’s environment where disk space is relatively
cheap and large ISPs have regional data centers (POPs) that serve
upwards of 150,000 to 1,000,000 subscribers, the deployment of a
10TB cache is both economically and technically feasible.

Our results allow us to estimate that 30% of Web and Multime-
dia content is reusable and that 70% of the trafﬁc during the busy
hour is web and multimedia trafﬁc, hence about 24% of network
trafﬁc could be optimized to take advantage of the fact it is be-
ing requested more than once. (We have calcuated the bandwidth
savings soley on caching Web and Multimedia trafﬁc, however, a
P2P-based cache could also deployed.)

In addition to simulating an unlimited cache size we imple-
mented a caching program to test the TTL-Cache algorithm with
various cache sizes. For our simulation, we used Least-Recently-
Used (LRU) as our caching policy. The choice of LRU was made
because most caching products utilize LRU. In industry this is be-
cause it is simple, understandable, and works just as well as any
other algorithm when you have enough disk space. Figure 10 shows
the results of our simluations using cache sizes varying from 1GB
to 1.5 TB in size. The parallel line in the graph shows the maxi-
mum amount cacheable bytes (32.5% calcualated previously) if we
had an unlimited cache size.

6.  NETWORK AWARE FORWARD

CACHING

Forward caching has been proposed in the 90s to address the
issues of improved client performance and reduced network cost
which we highlighted in the previous sections. A forward cache
is an HTTP cache deployed within an Internet Service Provider’s
(ISP) network caching all cacheable HTTP trafﬁc accessed by the
customers of the ISP. In contrast to CDNs a forward cache is de-
ployed for the customers beneﬁt and under the control of the ISP,
rather than for the beneﬁt of the content owner.

As forward caches are quite frequently collocated with cooperate
ﬁrewalls caching HTTP trafﬁc of the employees of the cooperation,
most large US based ISPs currently do not operate forward caches
within there network. The main reason for this decision lies in
the economics of deploying forward caches. Forward caches are
additional hardware components (typically UNIX servers) which
have to be purchased, deployed and managed at a large number of
locations. In the US the bandwidth savings can often not justify the
cost of such a server deployment.

To reduce the costs associated with forward caching in an ISP
we propose Network-Aware Forward Caching which is motivated
by the insights presented in the previous section. Our goal is to ﬁnd
the set of addresses at each POP that when cached maximizes the
cost savings for the network. In particular we noticed that some
trafﬁc is already originating close to the ISPs customers (e.g. CDN

trafﬁc) and, therefore, the beneﬁts of caching the content again us-
ing a forward cache in the ISPs POP is minimal both in terms of
performance and cost savings. On the other hand some trafﬁc tra-
verse expensive transit or backbone links and caching would be
both cost effective and improve performance. In a second dimen-
sion we also noticed that POPs themselves have a high variability
in terms of distance to HTTP sources as well as peering links. For
example, a remote POP might be far away from a CDN server,
whereas a metropolitan POP might be very close to a CDN server.
Considering these insights it becomes clear that the most cost
effective way of deploying forward caches is to only deploy them
in selected POPs that are caching only selected expensive-to-deliver
content that is requested frequently. This expensive content can be
identiﬁed using a metric like air miles as we have done. We call
this approach Network-Aware Forward Caching. In the remainder
of this section we will formally state the problem of how to place
caches and decide what content to cache, propose a solution and
evaluate our approach in a case study using data from a large tier
one US ISP.

6.1 Problem Formulation

We ﬁrst deﬁne the notations used for the formulation of for-
ward caching problem. The network has a set of POPs P =
{1, 2, 3, . . .}. The distance (air mileage) between POPs are deﬁned
as L = (li,j), where i, j ∈ P . The HTTP trafﬁc are downloaded
from a set of IP address sets S = {0, 1, 2, . . .}. For example, an
address set can be an address preﬁx (i.e., 100.200.0.0/24), or the
collection of addresses that belong to the same organization or au-
tonomous system (AS). Deﬁne V = (vi,j,s) as the monthly HTTP
trafﬁc volume from address set s that enter the network at ingress
POP j and leaves the network at egress POP i. The monthly transit
cost per unit volume for address set s is deﬁned T = (ts), where
ts > 0 for provider trafﬁc, ts < 0 for customer trafﬁc, ts = 0 for
peer trafﬁc.

Assume we have a total budget of N dollars to purchase and
deploy caches. Each cache costs γ dollars, has a disk space of b,
and can handle a trafﬁc throughput of e Mbps. We deﬁne a boolean
variable to denote whether to cache s at POP i: C = (ci,s), where
ci,s = 1 if yes, ci,s = 0 if not. We further deﬁne U = (ui,j,s)
as the monthly HTTP trafﬁc volume from s with ingress POP j
and egress POP i which cannot be possibly retrieved even from a
cache at s with inﬁnite computational power and disk space. We
deﬁne the disk space needed for caching address set s at POP i as
X = (xi,s). Note that X is different from U in that an object with
size x might have to downloaded twice due to TTL expiration, but
just needs x to store.

Cost

T C
P

We deﬁne the backbone cost unit as α dollars per mile-byte, and
transit cost unit as β dollars per byte. We can then compute the
backbone cost, transit cost (T C), and upfront caching cost (CC),
when caches are deployed. HTTP trafﬁc vi,j,s’s contribution to
backbone cost is α · li,j · ui,j,s when the objects in s are cached
at i (i.e.,ci,s = 1), and α · li,j · vi,j,s when the objects in s
are not cached at s(i.e.,ci,s = 0). Thus the total backbone cost
BC = α · P
∀i∈P,j∈P,s∈S li,j · ((1 − ci,s) · vi,j,s + ci,s · ui,j,s).
·
P
Similarly,
transit
∀i∈P,j∈P,s∈S ts · ((1 − ci,s) · vi,j,s + ci,s · ui,j,s).
The
total
thus
the number of cache units at POP i required by the compu-
tational power is (cid:3)P
the
number of cache units at POP i required by disk space is
(cid:3)P
∀s∈S ci,s · xi,s/b)(cid:4). The upfront caching cost at POP i is the
maximum of that required by computational power and that re-

∀s∈S,j∈P ci,s · vi,j,s/e(cid:4).

∀s∈S,j∈P ci,s · vi,j,s,

trafﬁc volume at POP i is

=

β

Similarly,

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 297P

P

∀s∈S,j∈P ci,s · vi,j,s/e,

quired by disk space. Thus the total upfront caching cost CC = γ ·
P
∀s∈S ci,s · xi,s/b)(cid:4).

∀i∈P (cid:3)max(
the problem is to ﬁnd ci,s such that
minimize: BC + T C + CC
subject to: CC ≤ N
P
After refactoring, the object function becomes:
∀i∈P,j∈P,s∈S vi,j,s · (α · li,j + β · ts) −
minimize:
P
∀i∈P,s∈S ci,s · P
j∈P (vi,j,s − ui,j,s) · (α · li,j + β · ts) − γ ·
(
P
P
∀s∈S ci,s · xi,s/b)(cid:4))
∀s∈S,j∈P ci,s · vi,j,s/e,
∀i∈P (cid:3)max(
P
j∈P (vi,j,s − ui,j,s) · (α · li,j + β · ts), which is
let Bi,s =
the beneﬁt of caching s at i, excluding the upfront cost. The objec-
tive function becomes:

P

P

maximize:

γ · X

(cid:3)max(

∀i∈P,s∈S ci,s · Bi,s−

X

∀i∈P

∀s∈S,j∈P

ci,s · vi,j,s/e,

X

∀s∈S

ci,s · xi,s/b)(cid:4)

(1)

P
subject to :
(cid:6)N/γ(cid:7) = N(cid:4)

∀i∈P (cid:3)max(

P

∀s∈S,j∈P ci,s · vi,j,s/e,

P

∀s∈S ci,s · xi,s/b)(cid:4) ≤

6.2 Algorithm
It is easy to see that our ﬁnal Formulation 1 of the problem for
the case that we have only one POP, i.e., |P| = 1, is as hard as
the knapsack problem. In the knapsack problem, given a set of n
different items, each with a weight and a value (beneﬁt), our goal
is to determine the set of items to include so that the total weight
is less than a given limit W and the total value is as large as possi-
ble. It is well-known that the knapsack problem is NP-hard though
it can be solved in pseudo-polynomial time1 using dynamic pro-
gramming. In addition, the problem has a polynomial-time 1 − -
approximation algorithm (an algorithm whose output has a value
at least 1 −  times the optimum solution) based on dynamic pro-
gramming, for arbitrary small constant  > 0.

P

First, let us observe that our problem formulation also has a
pseudo-polynomial-time dynamic programming algorithm. Con-
sider any POP i. For a content s, we denote its needed compu-
tational power by Cs =
j∈P vi,j,s and its needed disk space by
Ms = xi,s. Now we ﬁll in a table T [s, C, M ] which determines the
maximum beneﬁt that we can obtain from contents 1, 2, . . . , s with
at most C total computational power and M total disk space, where
(cid:3)C/e(cid:4) ((cid:3)M/b(cid:4)) is at most the maximum number of cache units
that we can afford in our total budget, i.e., N(cid:4)
. T [0, C, M ] = 0
for all feasible values of C and M. For s > 0, T [s, C, M ] =
max{T [s − 1, C, M ], T [s − 1, C − Cs, M − Ms] + Bi,s}, for
C ≥ Cs and M ≥ Ms, and ∞ otherwise. Next, deﬁne T (cid:4)i[U ] to be
T [|S|, e· U, b· U ] which is the maximum beneﬁt that we can obtain
by caching of contents in POP i with at most 0 ≤ U ≤ N(cid:4)
(as the
maximum of computational power or disk space) units of caches.
Finally having T (cid:4)i[U ]s, we compute our ﬁnal table T (cid:4)(cid:4)[i, U ] which
is the maximum beneﬁt that we can obtain from POPs 1, 2, . . . , i
with at most 0 ≤ U ≤ N(cid:4)
units of caches. T (cid:4)(cid:4)[0, U ] = 0 for
all affordable values of 0 ≤ U ≤ N(cid:4)
, and for i > 0, T (cid:4)(cid:4)[i, U ] =
max0≤j≤U{T (cid:4)(cid:4)[i−1, U−j]+T (cid:4)i[j]}. Therefore, the maximum of
our objective in Formulation 1 is max1≤U≤N(cid:2){T (cid:4)(cid:4)[|P|, U ]− γU}.
By using standard techniques analogous to those for the knap-
it is not hard to transform the above pseudo-
sack problem,
polynomial-time dynamic programming to a polynomial-time 1−-
approximation algorithm, for arbitrary small constant  > 0.

1In computational complexity theory, a numeric algorithm runs in
pseudo-polynomial time if its running time is polynomial in the
numeric value of the input (which is exponential in the length of
the input – its number of digits).

 1

)

Bis

i

s
B

(
 
t
i
f

e
n
e
B

 
l

t

a
o
T

 
f

 

 

o
%
F
D
C
C

 0.1

 0.01

 1e-05

 0.0001

 0.001
 0.01
% of i,s for ASNs

 0.1

 1

Figure 11: CCDF of Total Beneﬁt at Each i,s Pair

P

It is worth mentioning that in practice with large values, dynamic
programming approaches similar to the aforementioned one in this
section are time-consuming and not desirable. Due to this reason
we consider a well-known greedy heuristic for the knapsack prob-
lem which sorts the items in decreasing order of value per unit of
weight and then proceeds to insert them into the knapsack until
there is no longer space in the knapsack for more. This heuristic
for the knapsack problem is not only very fast and easy to imple-
ment but also gives a guaranteed approximation factor 2 for some
versions of knapsack. Below we generalize this greedy algorithm
for our purpose and report its evaluation results in the next section.
Our greedy heuristics is based on the idea that the total number of
caches n is within the range of [0, N(cid:4)]. Therefore, we can “guess"
and enumerate n. Thus the problem becomes:
∀i∈P,s∈S ci,s · Bi,s − γ · n

maximize:
subject to : n ≤ N(cid:4)
By enumerating over all n, γ · n is just a ﬁxed cost that can be
ignored for the maximization purposes. As we discussed in the
previous section, Bi,s is the beneﬁt of caching s at i. On the other
hand, the weight of content s to be cached on POP i is wi,s =
γ max(

∀j∈P vi,j,s/e, xi,s/b).2

Now, for a ﬁxed number n of caches, we have essentially a knap-
sack problem that we want to maximize the beneﬁt of selected el-
ements (i.e., the cache assignment) while our total weight is re-
stricted by the number n of caches. Thus inspired by the aforemen-
tion greedy algorithm for the knapsack problem, for a ﬁxed n, our
algorithm is to choose the most cost-efﬁcient (i, s) pair (i.e., cache
s at i) ﬁrst. A formal description of our algorithm is as follows:
1: for n = 0 to N(cid:4)
2:
3:
4:

ci,s = 0 for all i and s //clear all ci,s
for (i, s) pairs ranked by Bi,s/wi,s descendingly do

ci,s = 1 as long as the total number of used caches so far
is not more than n;

P

do

end for

5:
6: end for
7: ﬁnd the lowest (BC+T C+CC) across different n, and output

the corresponding C = (ci,s);

2Note that indeed, (cid:3)wi,s(cid:4) is the number of caches needed if we
cache content s on POP i alone. This weight might be smaller if
we cache other contents on POP i as well. However in some sense
we “over-provision" the caches, which is often needed in practice,
since we do not want to utilize the caches 100%.

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 298t
i
f

e
n
e
B
d
e
z

 

i
l

a
m
r
o
N

T $0 B $4
T $4 B $4
T $8 B $4

 2.5

 2

 1.5

 1

 0.5

 0

-0.5

-1

t
i
f

e
n
e
B
d
e
z

 

i
l

a
m
r
o
N

T $4 B $0
T $4 B $4
T $4 B $8

 2.5

 2

 1.5

 1

 0.5

 0

-0.5

-1

t
i
f

e
n
e
B
d
e
z

 

i
l

a
m
r
o
N

ASN with 0K Server Cost
ASN with 10K Server Cost
ASN with 20K Server Cost
ASN with 40K Server Cost
ASN with 60K Server Cost

 2.5

 2

 1.5

 1

 0.5

 0

 0

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9

 1

 0

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9

 1

 0

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9

 1

% of Deployment

% of Deployment

% of Deployment

Figure 12: Beneﬁts for Different Transit
Costs

Figure 13: Beneﬁts for Different Backbone
Costs

Figure 14: Beneﬁts for Different Server
Costs (ﬁxed $4 Transit, $4 Backbone, 400
Mbps, 4TB)

 

P
O
P
h
c
a
e
n

 

i
 

 

d
e
h
c
a
C
h
d
w
d
n
a
B

t

i

 
f

 

o
%

Max Net Benefit

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9

 1

% of POP

Figure 15: Coverage of Caches at each Pop at Maximum Net
Beneﬁt

6.3 Evaluation

We implemented a simulation program to evaluate our network-
aware caching approach using the heuristic we described in Sec-
tion 6.2.

To run the simulation we assume that transit (β) and backbone
(α) costs are approximately $4 Mbps/month [30] and the upfront
cost of a caching server is $20,000 which can be amortized over 36
month and costs approximately $555 a month to run. We assume
that this $20K server will be able to handle 400 Mbps of through-
put (e) and as have a disk size of 4 TB (b). We then vary these
parameters to study the sensitivity of the results.

In the simulation we base our network speciﬁc inputs P , S,
Vi,j,s, and Li,j from our Core Backbone Trafﬁc data set we pre-
viously analyzed in Section 4. In our analysis, we have chosen to
use AS numbers in S. However, our approach can use other levels
of detail such as preﬁxes if more granular measurements are avail-
able. To estimate the values of Ui,j,s and Xi,s we used our analysis
from Section 5 that calculated the values Ui,s and Xi,s for a single
BRAS in this network. In the simulation we use the U and X val-
ues from this BRAS as estimates that are scaled appropriately for
all other POPs.

Our simulation program calculates the net beneﬁt (Equation 1)
as the number of cache servers (n) increases. We ﬁnd the best de-
ployment solution by selecting n which maximizes the net beneﬁt.
In our program, the net beneﬁt is calculated in dollars saved. How-
ever, in our ﬁgures we plot the relative net beneﬁt. This is done

to preserve the anonymity of our data source. Figure 11 shows the
CCDF distribution of Bis. This shows that most of the beneﬁt can
be obtained from caching a small subset of the i, s pairs.

Overall, we ﬁnd in Figure 12 and Figure 13 that using our
stated assumptions for backbone, transit and cache costs and the
US Broadband Provider’s network data that the maximum bene-
ﬁts would occur when only 68% of the caches are deployed that
would have other otherwise been needed to cover all HTTP traf-
ﬁc. Our simulation results show that the relative net positive ben-
eﬁt increased from 0.501 to 0.688 (a 37% overall improvement in
beneﬁts) when the network-aware caching approach’s strategy was
compared to the beneﬁts of deploying caches to cover all POPs.
Figure 15 shows the distribution of cache servers to each POP based
on our maximal solution. This shows that in the our optimum solu-
tion 25% of POPs do not any postive net beneﬁt by having a cache
server place there. Only 15% of POPs have a maximumnet net
beneﬁt by having all HTTP trafﬁc covered.

Figures 12, 13, and 14 show selected results of varying each of
these factors by a couple magnitudes to see their overall affect on
the ﬁnal cost beneﬁt analysis.

Figure 12 shows that transit costs in our simulated network min-
imally affect the overall maximum net beneﬁt. This is due in our
case to the amount of transit trafﬁc that is cacheable is quite low.
However, in other network this may not be the case and therefore
have a large impact in the results.

Figure 13 shows that the backbone cost is a large factor inﬂuenc-
ing how much the net beneﬁt is. As the backbone cost increases
the cost beneﬁt curve is shifted upwards and maximum net beneﬁt
point shifts right.

Figure 14 shows the net beneﬁt cost as the cost of each cache
changes but all other parameters remain ﬁxed. When the server
cost is set to $0 per server the theoretical maximum beneﬁt ob-
tainable is depicted. As the server cost is increases the net beneﬁt
decreases and the optimum number of caches to deploy shifts to the
left. This follow intuition that as the cache cost increases that less
trafﬁc would have a positive net beneﬁt to cache.

7. RELATED WORK

Forward caches which are also known as proxy caches or for-
ward proxy caches have received a great deal of attention during
the dot com boom. As there exists a large volume of prior art on
various aspects of forward proxies we refer the interested reader
to [24] for a broader discussion of forward caching and focus the
related work discussion in this section more narrowly on the prob-
lem of cache placement and selective content caching.

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 299Proxy placement algorithms can be characterized in two classes.
First there are multiple proposals of how to optimize the cache
placement within a CDN. For example [3, 21, 27, 28] explores this
problem. Our goal differs in that we are interested in how to place
caches which cache only partial HTTP trafﬁc to optimize the cost
within an ISPs network and not how to optimize the placement of
caches within a CDN.

Another paper optimizing placement of proxy servers in a CDN
is [23]. This work does introduce a greedy algorithm similar in
spirit to our approach. However, it does not address the problem of
deciding which content to cache on which proxy while solving the
proxy placement problem in an access ISP. It rather focuses on the
optimal placement of proxies to server content from a given Web
server through a CDN.

A second line of work looks at proxy placement under vari-
ous topologies [3, 26] as well as proxy placement over multiple
ASes [18]. Again neither of this work considers the problem of se-
lective caching and cache placement within an access ISP to reduce
deployment cost.

As CDNs cache only content for which they are paid. We are
unaware of any work which explicitly restricts which content to
cache in a forward cache deployment to minimize the deployment
cost of the cache infrastructure. In addition to this new aspect of re-
stricting cachability on forward proxies we also provide a detailed
case study of our approach using real network traces and network
topology of a US broadband ISP.

8. CONCLUSION AND FUTURE WORK

This paper proposed and evaluated a Network Aware Forward
Caching approach for determining the optimal deployment strategy
of forward caches to a network. We ﬁnd that this is an NP-hard
problem. A key advantage of our approach is that we can reduce
the costs associated with forward caching to maximize the beneﬁt
obtained from their deployment. In our case study, we show in our
analysis that a 37% increase to net beneﬁts could be achived over
the standard method of a deploying caches to all POPs and caching
all trafﬁc. This maximal point occurs when only 68% of the total
trafﬁc is cached. At this point, we ﬁnd that 25% of POPs should
not have a cache and that only 15% of POP% should have all trafﬁc
cached.

Another contribution of this paper is the analysis we use to moti-
vate and evaluate this problem. We characterize the Internet trafﬁc
of 100K subscribers of a US residential broadband provider. We us
both layer 4 and layer 7 analysis to investigate the trafﬁc volumes
of the ﬂows as well study the general characteristics of the applica-
tions used. We show that HTTP is a dominant protocol and account
for 68% of the total downstream trafﬁc. In addition, we show that
multimedia content using HTTP exhibits a 83% annualized growth
rate and other HTTP trafﬁc has a 53% growth rate versus the 26%
over all annual growth rate of broadband trafﬁc. This shows that
HTTP trafﬁc will become ever more dominent and increase the
potential caching opportunities. Furthermore, we characterize the
core backbone trafﬁc of this broadband provider to measure the ef-
ﬁciency content and trafﬁc is delivered. We ﬁnd that CDN trafﬁc is
much more efﬁcient than P2P content and that there is large skew
in the Air Miles between POP in a typical network and shows many
opportunties in broadband provider networks to optimize how traf-
ﬁc is delivered and cached.

Several opportunities exists for future work. In this paper, we
have focused on the fastest growing and biggest component to
broadband ISP trafﬁc. However, additional work could be done
in to determine strategies for caching or increasing the delivery ef-
ﬁciency of other applications such as P2P and multicast.

9. REFERENCES
[1] Surveys: Internet trafﬁc touched by youtube. LightReading, Jan. 2006.
[2] N. Basher, A. Mahanti, A. Mahanti, C. Williamson, and M. Arlitt. A

comparative analysis of web and peer-to-peer trafﬁc. In Proc. WWW, Beijing,
China, April 2008.

[3] H. S. Bassali, K. M. Kamath, R. B. Hosamani, and L. Gao. Hierarchy-aware

algorithms for cdn proxy placement. Internet Computer Communications,
26-3:251–263, 2003.

[4] Cache Logic. Peer-to-Peer in 2005. http:

//www.cachelogic.com/home/pages/research/p2p2005.php,
2005.

[5] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and S. Moon. I tube, you tube,
everybody tubes: analyzing the world’s largest user generated content video
system. In Proc. IMC, San Diego, California, USA, October 2007.

[6] L. Cherkasova and M. Gupta. Analysis of enterprise media server workloads:

Access patterns, locality, content evolution, and rate of change. IEEE/ACM
Trans. Networking, 12(5), 2004.

[7] K. Cho, K. Fukuda, H. Esaki, and A. Kato. The impact and implications of the
growth in residential user-to-user trafﬁc. In Proc. ACM SIGCOMM, Pisa, Italy,
2006.

[8] D. R. Choffnes and F. E. Bustamante. Taming the Torrent: A practical approach

to reducing cross-ISP trafﬁc in P2P systems. In SIGCOMM’08, Seattle, USA,
August 2008.

[9] NetFlow Services and Applications. http://www.cisco.com/en/US/

docs/ios/solutions_docs/netflow/nfwhite.html, 2007.

[10] K. C. Claffy. Internet trafﬁc characterization. PhD thesis, University of

California, San Diego Supercomputer Center, San Diego, 1994.

[11] C. Cranor, T. Johnson, O. Spataschek, and V. Shkapenyuk. Gigascope: A

Stream Database for Network Applications. In SIGMOD’03, San Diego, USA,
June 2003.

[12] N. G. Dufﬁeld, C. Lund, and M. Thorup. Charging from sampled network

usage. In Proc. ACM SIGCOMM, San Francisco, CA, November 2001.

[13] A. Feldmann, R. Caceres, F. Douglis, G. Glass, and M. Rabinovich.

Performance of web proxy caching in heterogeneous bandwidth environments.
In Proc. IEEE INFOCOM, New York, NY, March 1999.

[14] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee. Hypertext

Transfer Protocol – HTTP/1.1. RFC 2068 (Proposed Standard), January 1997.
Obsoleted by RFC 2616.

[15] A. Gerber, J. Houle, H. Nguyen, M. Roughan, and S. Sen. P2P, the gorilla in

the cable. In National Cable and Telecommunications Association(NCTA) 2003
National Show, Chicago, IL, 2003.

[16] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. Youtube trafﬁc characterization: A

view from the edge. In Proc. IMC, San Diego, CA, 2007.

[17] C. Huang, A. Wang, J. Li, and K. W. Ross. Evaluating CDNs: How do Akamai

and Limelight Compare? In Microsoft Research Technical Report
MSR-TR-2008-106,, August 2008.

[18] M. Kamath, H. S. Bassali, R. B. Hosamani, and L. Gao. L.gao, policy-aware

algorithms for proxy placement. In in the internet, in: ITCOM 2001, 2001.

[19] T. Karagiannis, A. Broido, M. Faloutsos, and K. claffy. Transport Layer
Identiﬁcation of P2P Trafﬁc. In IMC’04, Taormina, Italy, October 2004.

[20] T. Karagiannis, K. Papagiannaki, and M. Faloutsos. BLINC: Multilevel Trafﬁc
Classiﬁcation in the Dark. In SIGCOMM’05, Philadelphia, USA, August 2005.

[21] B. Li, M. J. Golin, G. F. Italiano, X. Deng, and K. Sohraby. On the optimal

placement of web proxies in the Internet. In INFOCOM’99, March 1999.

[22] L. Plissonneau, J.-L. Costeux, and P. Brown. Analysis of peer-to-peer trafﬁc on

adsl. In Proc. PAM, pages 69–82, Boston,MA, 2005.

[23] L. Qiu, V. N. Padmanabhan, and G. Voelker. On the placement of Web server

replicas. In INFOCOM’01, Anchorage, USA, April 2001.

[24] M. Rabinovich and O. Spatschek. Web caching and replication.

Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2002.

[25] S. Sen, O. Spatscheck, and D. Wang. Accurate, Scalable In-Network

Identiﬁcation of P2P Trafﬁc Using Application Signatures. In WWW 2004, New
York, USA, May 2004.

[26] A.-E. M. Taha and A. E. Kamal. Optimal and near optimal web proxy

placement algorithms for networks with planar graph topologies. In ICDCSW
’03: Proceedings of the 23rd International Conference on Distributed
Computing Systems, page 911, Washington, DC, USA, 2003. IEEE Computer
Society.

[27] P. Trianﬁllou and I. Aekaterinidis. ProxyTeller: a proxy placement tool for

content delivery under performance constraints. In Fourth International
Conference on Web Information System Engineering, December 2003.

[28] S. Varadarajan, R. Harinath, J. Srivastava, and Z.-L. Zhang. Coverage-aware

proxy placement for dynamic content management over the internet. Distributed
Computing Systems Workshops, International Conference on, 0:892, 2003.

[29] C. Williamson. Internet trafﬁc measurement. IEEE Internet Computing, 5(6),

November 2001.

[30] C. Wilson. Cogent throws down pricing gauntlet, November 2008.
[31] H. Xie, Y. R. Yang, A. Krishnamurthy, Y. Liu, and A. Silberschatz. P4P: Portal

for (P2P) Applications. In SIGCOMM’08, Seattle, USA, August 2008.

WWW 2009 MADRID!Track: Performance, Scalability and Availability / Session: Performance 300