An Axiomatic Approach for Result Diversiﬁcation

Sreenivas Gollapudi
Microsoft Search Labs

Microsoft Research

sreenig@microsoft.com

Aneesh Sharma∗

Institute for Computational and Mathematical

Engineering, Stanford University

aneeshs@stanford.edu

ABSTRACT
Understanding user intent is key to designing an eﬀective
ranking system in a search engine.
In the absence of any
explicit knowledge of user intent, search engines want to di-
versify results to improve user satisfaction. In such a setting,
the probability ranking principle-based approach of present-
ing the most relevant results on top can be sub-optimal, and
hence the search engine would like to trade-oﬀ relevance for
diversity in the results.

In analogy to prior work on ranking and clustering sys-
tems, we use the axiomatic approach to characterize and
design diversiﬁcation systems. We develop a set of natu-
ral axioms that a diversiﬁcation system is expected to sat-
isfy, and show that no diversiﬁcation function can satisfy
all the axioms simultaneously. We illustrate the use of the
axiomatic framework by providing three example diversiﬁ-
cation objectives that satisfy diﬀerent subsets of the axioms.
We also uncover a rich link to the facility dispersion prob-
lem that results in algorithms for a number of diversiﬁcation
objectives. Finally, we propose an evaluation methodology
to characterize the objectives and the underlying axioms.
We conduct a large scale evaluation of our objectives based
on two data sets: a data set derived from the Wikipedia
disambiguation pages and a product database.

Categories and Subject Descriptors
H.3.3 [Information Systems]:
RetrievalInformation Search and Retrieval

Information Storage and

General Terms
Algorithms, Theory, Performance

Keywords
Search engine, Diversiﬁcation, Approximation Algorithms,
Axiomatic framework, Facility dispersion, Wikipedia

1.

INTRODUCTION

In the current search model, the user expresses her infor-
mation need with the use of a few query terms. In such a
∗Work done while author was an intern at Microsoft Search
Labs

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

scenario, the small number of terms often specify the intent
implicitly. In the absence of explicit information represent-
ing user intent, the search engine needs to “guess” the results
that are most likely to satisfy diﬀerent intents. In particu-
lar, for an ambiguous query such as eclipse, the search engine
could either take the probability ranking principle approach
of taking the “best guess” intent and showing the results, or
it could choose to present search results that maximize the
probability of a user with a random intent ﬁnding at least
one relevant document on the results page. This problem
of the user not ﬁnding any any relevant document in her
scanned set of documents is deﬁned as query abandonment.
Result diversiﬁcation lends itself as an eﬀective solution to
minimizing query abandonment [1, 9, 18].

Intuitively, diversiﬁcation implies a trade-oﬀ between hav-
ing more relevant results of the “correct” intent and having
diverse results in the top positions for a given query [6, 8].
Hence the twin objectives of being diverse and being rele-
vant compete with each other, and any diversiﬁcation system
must ﬁgure out how to trade-oﬀ these objectives appropri-
ately. This often results in the diversiﬁcation problem being
characterized as a bi-criteria optimization problem. Diver-
siﬁcation can be viewed as combining both ranking (pre-
senting more relevant results in the higher positions) and
clustering (grouping documents satisfying similar intents)
and therefore addresses a loosely deﬁned goal of picking
a set of most relevant but novel documents. This has re-
sulted in the development of a set of very diﬀerent objective
functions and algorithms ranging from combinatorial opti-
mizations [6, 18, 1] to those based on probabilistic language
models [8, 22]. The underlying principles supporting these
techniques are often diﬀerent and therefore admit diﬀerent
trade-oﬀ criteria. Given the importance of the problem there
has been relatively little work aimed at understanding result
diversiﬁcation independent of the objective functions or the
algorithms used to solve the problem.

In this work, we initiate an axiomatic study of result di-
versiﬁcation. We propose a set of simple properties that any
diversiﬁcation system ought to satisfy and these properties
help serve as a basis for the space of objective functions for
result diversiﬁcation. Generally, a diversiﬁcation function
can be thought of as taking two application speciﬁc inputs
viz, a relevance function that speciﬁes the relevance of doc-
ument for a given query, and a distance function that cap-
tures the pairwise similarity between any pair of documents
in the set of relevant results for a given query. In the con-
text of web search, one can use the search engine’s ranking

WWW 2009 MADRID!Track: Search / Session: Search UI381function1 as the relevance function. The characterization of
the distance function is not that clear.
In fact, designing
the right distance function is key for having eﬀective re-
sult diversiﬁcation. For example, by restricting the distance
function to be a metric by imposing the triangle inequality
d(u, w) ≤ d(u, v) + d(v, w) for all u, v, w ∈ U , we can exploit
eﬃcient approximation algorithms to solve certain class of
diversiﬁcation objectives (see Section 3).
1.1 Contributions of this study

In this work, we propose a set of natural axioms for result
diversiﬁcation that aid in the choice of an objective func-
tion and therefore help in constraining the resulting solution.
Our work is similar in spirit to recent work on axiomatiza-
tion of ranking and clustering systems [2, 12]. We study
the functions that arise out of the requirement of satisfying
a set of simple properties and show an impossibility result
which states that there exists no diversiﬁcation function f
that satisﬁes all the properties. We state the properties in
Section 2.

Although we do not aim to completely map the space
of objective functions in this study, we show that some di-
versiﬁcation objectives reduce to diﬀerent versions of the
well-studied facility dispersion problem. Speciﬁcally, we pick
three functions that satisfy diﬀerent subsets of the proper-
ties and characterize the solutions obtained by well-known
approximation algorithms for each of these functions. We
also characterize some of the objective functions deﬁned in
earlier works [1, 18, 8] using the axioms.

Finally, we do a preliminary characterization of the choice
of an objective (and its underlying properties) using the
natural measures of relevance and novelty. We posit that
diﬀerent subsets of axioms will exhibit diﬀerent trade-oﬀs
between these measures. Towards this end, we provide an
evaluation methodology that computes the measures based
on the disambiguation pages in Wikipedia2, which is the
largest public-domain evaluation data set used for testing a
diversiﬁcation system (see Section 5). Further, we consider
two distance functions - a semantic distance function and a
categorical distance function (see Section 4) - to test the ef-
fectiveness of the objectives under two diﬀerent application
contexts.
1.2 Related Work

The early work of Carbonell and Goldstein [6] described
the trade-oﬀ between relevance and novelty via the choice
of a parametrized objective function. Subsequent work on
query abandonment by Chen and Karger [8] is based on the
idea that documents should be selected sequentially accord-
ing to the probability of the document being relevant condi-
tioned on the documents that come before. Das Sarma et.
al. [18], solved a similar problem by using bypass rates of
a document to measure the overall likelihood of a user by-
passing all documents in a given set. Thus, the objective in
their setting was to produce a set that minimized likelihood
of completely getting bypassed.

Agrawal et. al., [1] propose a diversiﬁcation objective that
tries to maximize the likelihood of ﬁnding a relevant docu-
ment in the top-k positions given the categorical information
of the queries and documents. Other works on topical diver-
siﬁcation include [23]. Zhai and Laﬀerty [22, 20] propose a
1See [17] and references therein.
2http://en.wikipedia.org

risk minimization framework for information retrieval that
allows a user to deﬁne an arbitrary loss function over the set
of returned documents. Vee et. al., [19] proposed a method
for diversifying query results in online shopping applications
wherein the query is presented in a structure form using on-
line forms.

Our work is based on axiomatizations of ranking and clus-
tering systems [3, 12, 2]. Kleinberg [12] proposed a set of
three natural axioms for clustering functions and showed
that no clustering function satisﬁes all three axioms. Altman
and Tennenholtz [2] study ranking functions that combine
individual votes of agents into a social ranking of the agents
and compare them to social choice welfare functions which
were ﬁrst proposed in the classical work on social choice
theory by Kenneth Arrow [3].

One of the contributions of our work is the mapping of di-
versiﬁcation functions to those used in facility dispersion [15,
14]. The reader will ﬁnd a useful literature in the chapter
on facility dispersion in [16, 7].

2. AXIOMATIC FRAMEWORK

This section introduces the axiomatic framework and ﬁxes
the notation to be used in the remainder of the paper. We
are given a set U = {u1, u2, . . . , un} of n ≥ 2 of documents,
and a set (we’ll assume this to be ﬁnite for now) of queries
Q. Now, given a query q ∈ Q and an integer k, we want
to output a subset Sk ⊆ U of documents3 that is simulta-
neously both relevant and diverse. The relevance of each
document is speciﬁed by a function w : U × Q → R+, where
a higher value implies that the document is more relevant
to a particular query. The diversiﬁcation objective is in-
tuitively thought of as giving preference to dissimilar doc-
uments. To formalize this, we deﬁne a distance function
d : U × U → R+ between the documents, where smaller the
distance, the more similar the two documents are. We also
require the distance function to be discriminative, i.e.
for
any two documents u, v ∈ U , we have d(u, v) = 0 if and only
if u = v, and symmetric, i.e d(u, v) = d(v, u). Note that the
distance function need not be a metric.

We restrict attention to the set selection problem instead
of the search problem of selecting a ranked list as this is
clearly a simpler problem. In particular, the approach we
will take is to ﬁnd the best set and then rank it in order
of relevance. Formally, the set selection function f : 2U ×
Q × w × d → R can be thought of as assigning scores to all
possible subsets of U , given a query q ∈ Q, a weight function
w(·), a distance function d(·,·). Fixing q, w(·), d(·,·) and a
given integer k ∈ Z+ (k ≥ 2), the objective is to select a set
Sk ⊆ U of documents such that the value of the function f
is maximized, i.e. the objective is to ﬁnd

S

∗
k = argmax
Sk⊆U
|Sk|=k

f (Sk, q, w(·), d(·,·))

where all arguments other than the set Sk are ﬁxed inputs
to the function.

An important observation is that the diversiﬁcation frame-
work is underspeciﬁed and even if one assumes that the rel-
evance and distance functions are provided, there are many
possible choices for the objective function f . These functions
could trade-oﬀ relevance and similarity in diﬀerent ways, and

3We will denote the size of the set by the subscript, i.e.
|Sk| = k

WWW 2009 MADRID!Track: Search / Session: Search UI382one needs to specify criteria for selection among these func-
tions. A natural mathematical approach in such a situation
is to provide axioms that any diversiﬁcation system should
be expected to satisfy and therefore provide some basis of
comparison between diﬀerent objective functions.
2.1 Axioms of diversiﬁcation

We propose that f is such that it satisfy the set of axioms
given below, each of which seems intuitive for the setting of
diversiﬁcation. In addition, we show that any proper subset
of these axioms is maximal, i.e. no diversiﬁcation function
can satisfy all these axioms. This provides a natural method
of selecting between various objective functions, as one can
choose the essential properties for any particular diversiﬁca-
tion system. In section 3, we will illustrate the use of the ax-
ioms in choosing between diﬀerent diversiﬁcation objectives.
Before we state the axioms, we state the following notation.
Fix any q, w(·), d(·,·), k and f , such that f is maximized
by S∗

k = argmaxSk⊆U f (Sk, q, w(·), d(·,·)).

k, i.e., S∗

1. Scale invariance:

Informally, this property states
that the set selection function should be insensitive
to the scaling of the input functions. Consider the set
optimal set S∗
k. Now, we require f to be such that we
k = argmaxSk⊆U f (Sk, q, α · w(·), α · d(·,·)) for
have S∗
any ﬁxed positive constant α ∈ R, α > 0, i.e. S∗
k still
maximizes f even if all relevance and distance values
are scaled by some constant.

2. Consistency: Consistency states that making the out-
put documents more relevant and more diverse, and
making other documents less relevant and less diverse
should not change the output of the ranking. Now,
given any two functions α : U → R+ and β : U × U →
R+, we modify the relevance and weight functions as
follows:

w(u) =

w(u) + α(u)
w(u) − α(u)

, u ∈ S∗
, otherwise

k

(
(

d(u, v) =

d(u, v) + β(u, v)
d(u, v) − β(u, v)

, u, v ∈ S∗
k
, otherwise

The ranking function f must be such that it is still
maximized by S∗
k.

3. Richness: Informally speaking, the richness condition
states that we should be able to achieve any possible
set as the output, given the right choice of relevance
and distance function. Formally, there exists some w(·)
and d(·,·) such that for any k ≥ 2, there is a unique
S∗
k which maximizes f .

4. Stability: The stability condition seeks to ensure that
the output set does not change arbitrarily with the
output size, i.e., the function f should be deﬁned such
that S∗

k ⊂ S∗

k+1.

• d(u, v) for all u, v /∈ S.

6. Monotonicity: Monotonicity simply states that the
addition of any document does not decrease the score
of the set. Fix any w(·), d(·,·), f and S ⊆ U . Now,
for any x /∈ S, we must have

f (S ∪ {x}) ≥ f (S)

7. Strength of Relevance: This property ensures that
no function f ignores the relevance function. Formally,
we ﬁx some w(·), d(·,·), f and S. Now, the following
properties should hold for any x ∈ S:

(a) There exist some real numbers δ0 > 0 and a0 > 0,
such that the condition stated below is satisﬁed
after the following modiﬁcation: obtain a new
relevance function w(cid:48)(·) from w(·), where w(cid:48)(·)
is identical to w(·) except that w(cid:48)(x) = a0 >
w(x). The remaining relevance and distance val-
ues could decrease arbitrarily. Now, we must have

f (S, w

(cid:48)

(·), d(·,·), k) = f (S, w(·), d(·,·), k) + δ0

(b) If f (S \ {x}) < f (S), then there exist some real
numbers δ1 > 0 and a1 > 0 such that the follow-
ing condition holds: modify the relevance func-
tion w(·) to get a new relevance function w(cid:48)(·)
which is identical to w(·) except that w(cid:48)(x) =
a1 < w(x). Now, we must have

f (S, w

(cid:48)

(·), d(·,·), k) = f (S, w(·), d(·,·), k) − δ1

8. Strength of Similarity: This property ensures that
no function f ignores the similarity function. Formally,
we ﬁx some w(·), d(·,·), f and S. Now, the following
properties should hold for any x ∈ S:

(a) There exist some real numbers δ0 > 0 and b0 > 0,
such that the condition stated below is satisﬁed
after the following modiﬁcation: obtain a new dis-
tance function d(cid:48)(·,·) from d(·,·), where we in-
crease d(x, u) for the required u ∈ S to ensure
that minu∈S d(x, u) = b0. The remaining rele-
vance and distance values could decrease arbitrar-
ily. Now, we must have

f (S, w(·), d

(cid:48)

(·,·), k) = f (S, w(·), d(·,·), k) + δ0

(b) If f (S \ {x}) < f (S), then there exist some real
numbers δ1 > 0 and b1 > 0 such that the fol-
lowing condition holds: modify the distance func-
tion d(·,·) by decreasing d(x, u) to ensure that
maxu∈S d(x, u) = b1. Call this modiﬁed distance
function d(cid:48)(·,·). Now, we must have

f (S, w(·), d

(cid:48)

(·,·), k) = f (S, w(·), d(·,·), k) − δ1

5. Independence of Irrelevant Attributes: This ax-
iom states that the score of a set is not aﬀected by
most attributes of documents outside the set. Specif-
ically, given a set S, we require the function f to be
such that f (S) is independent of values of:

• w(u) for all u /∈ S.

Given these axioms, a natural question is to characterize
the set of functions f that satisfy these axioms. A somewhat
surprising observation here is that it is impossible to satisfy
all of these axioms simultaneously (proof is in appendix):

Theorem 1. No function f satisﬁes all 8 axioms stated

above.

WWW 2009 MADRID!Track: Search / Session: Search UI383Theorem 1 implies that any subset of the above axioms is
maximal. This result allows us to naturally characterize the
set of diversiﬁcation functions, and selection of a particular
function reduces to deciding upon the subset of axioms (or
properties) that the function is desired to satisfy. The fol-
lowing sections explore this idea further and show that the
axiomatic framework could be a powerful tool in choosing
between diversiﬁcation function. Another advantage of the
framework is that it allows a theoretical characterization of
the function which is independent of the speciﬁcs of the di-
versiﬁcation system such as the distance and the relevance
function.
3. OBJECTIVES AND ALGORITHMS

In light of the impossibility result shown in Theorem 1, we
can only hope for diversiﬁcation functions that satisfy a sub-
set of the axioms. We note that the list of such functions
is possibly quite large, and indeed several such functions
have been previously explored in the literature (see [8,18,1],
for instance). Further, proposing a diversiﬁcation objective
may not be useful in itself unless one can actually ﬁnd al-
gorithms to optimize the objective. In this section, we aim
to address both of the above issues: we demonstrate the
power of the axiomatic framework in choosing objectives,
and also propose reductions from a number of natural diver-
siﬁcation objectives to the well-studied combinatorial opti-
mization problem of facility dispersion [16]. In particular,
we propose three diversiﬁcation objectives in the following
sections, and provide algorithms that optimize those objec-
tives. We also present a brief characterization of the objec-
tive functions studied in earlier works [1, 18, 8]. We will use
the same notation as in the previous section and have the
f (Sk, q, w(·), d(·,·)), where
objective as S∗
f would vary from one function to another. Also, we as-
sume w(·), d(·,·) and k to be ﬁxed here and hence use the
shorthand f (S) for the function.
3.1 Max-sum diversiﬁcation

k = argmax Sk⊆U
|Sk|=k

A natural bi-criteria objective is to maximize the sum of
the relevance and dissimilarity of the selected set. This ob-
jective can be encoded in terms of our formulation in terms
of the function f (S), which is deﬁned as follows:

f (S) = (k − 1)

w(u) + 2λ

d(u, v)

(1)

X

u,v∈S

X

u∈S

where |S| = k, and λ > 0 is a parameter specifying the
trade-oﬀ between relevance and similarity. Observe that we
need to scale up the ﬁrst sum to balance out the fact that
there are k(k−1)
numbers in the similarity sum, as opposed
to k numbers in the relevance sum. We ﬁrst characterize the
objective in terms of the axioms.

2

Remark 1. The objective function given in equation 1 sat-

isﬁes all the axioms, except stability.

This objective can be recast in terms of a facility disper-
sion objective, known as the MAXSUMDISPERSION problem.
The MAXSUMDISPERSION problem is a facility dispersion
problem having the objective maximizing the sum of all pair-
wise distances between points in the set S which we show
to be equivalent to equation 1. To this end, we deﬁne a new
distance function d(cid:48)(u, v) as follows:

(cid:48)

d

(u, v) = w(u) + w(v) + 2λd(u, v)

(2)

Input : Universe U , k
Output: Set S (|S| = k) that maximizes f (S)
Initialize the set S = ∅
2(cid:99) do
for i ← 1 to (cid:98) k
Find (u, v) = argmaxx,y∈U d(x, y)
Set S = S ∪ {u, v}
Delete all edges from E that are incident to u or v

end
If k is odd, add an arbitrary document to S
Algorithm 1: Algorithm for MAXSUMDISPERSION

It is not hard to see the following claim (proof skipped):

Claim 1. d(cid:48)(·,·) is a metric if the distance d(·,·) consti-

tutes a metric.
Further, note that for some S ⊆ U (|S| = k), we have:

(cid:48)

(u, v) = (k − 1)

d

w(u) + 2λ

d(u, v)

X

u,v∈S

X

u,v∈S

X

u∈S

X

u,v∈S

using the deﬁnition of d(cid:48)(u, v) and the fact that each w(u)
is counted exactly k − 1 times in the sum (as we consider
the complete graph on S). Hence, from equation 1 we have
that

f (S) =

(cid:48)

d

(u, v)

But this is also the objective of the MAXSUMDISPERSION
problem described above where the distance metric is given
by d(cid:48)(·,·).

Given this reduction, we can map known results about
MAXSUMDISPERSION to the diversiﬁcation objective. First
of all, we observe that maximizing the objective in equa-
tion 1 is NP-hard, but there are known approximation algo-
rithms for the problem. In particular, there is a 2-approx-
imation algorithm for the MAXSUMDISPERSION problem [13,
11] (for the metric case) and is given in algorithm 1. Hence,
we can use algorithm 1, for the max-sum objective stated in
equation 1.
3.2 Max-min diversiﬁcation

The second bi-criteria objective we propose, maximizes
the minimum relevance and dissimilarity of the selected set.
This objective can be encoded in terms of our formulation
in terms of the function f (S), which is deﬁned as follows:

f (S) = min
u∈S

w(u) + λ min
u,v∈S

(3)
where |S| = k, and λ > 0 is a parameter specifying the
trade-oﬀ between relevance and similarity. Here is the char-
acterization of the objective in terms of the axioms:

d(u, v)

Remark 2. The diversiﬁcation objective given in equa-
tion 3 satisﬁes all the axioms except consistency and sta-
bility.

We proceed as before to link this objective to facility dis-
persion, and the dispersion objective that is relevant in this
case is MAXMINDISPERSION. The objective for the MAX-
MINDISPERSION problem is: g(P ) = minvi,vj∈P d(vi, vj),
which we now show to be equivalent to equation 3. As be-
fore, we combine the objective in equation 1 in terms of a
single metric, with which we can then solve the MAXMIN-
DISPERSION problem. To this end, we deﬁne a new distance

WWW 2009 MADRID!Track: Search / Session: Search UI384Input : Universe U , k
Output: Set S (|S| = k) that maximizes f (S)
Initialize the set S = ∅; Find
(u, v) = argmaxx,y∈U d(x, y) and set S = {u, v}; For
any x ∈ U \ S, deﬁne d(x, S) = minu∈S d(x, u);
while |S| < k do

Find x ∈ U \ S such that x = argmaxx∈U\S d(x, S);
Set S = S ∪ {x};

end

Algorithm 2: Algorithm for MAXMINDISPERSION

function d(cid:48)(u, v) as follows: Now, we show how to use this
algorithm for the bi-criteria objective given in equation 3.
In order to do this, we again need to combine our objective
function in terms of a single metric, with which we can then
solve the MAXMINDISPERSION problem. Hence, we deﬁne
a new distance function d(cid:48)(u, v) as follows:

(cid:48)

d

(u, v) =

1
2

(w(u) + w(v)) + λd(u, v)

(4)

It is not hard to see the following claim (proof skipped):

Claim 2. The distance d(cid:48)(·,·) forms a metric if the dis-

tance d(·,·) forms a metric.
Further, note that for some S ⊆ U (|S| = k), we have:
d(u, v) = f (S)

d

(cid:48)

(u, v) = min
u∈S

w(u) + λ min
u,v∈S

min
u,v∈S

from equation 3. This is also the objective from the MAX-
MINDISPERSION problem where the distance metric is given
by d(cid:48)(·,·). Hence, we can use algorithm 2 to approximately
maximize the objective stated in equation 3.

Again, we can map known results about the MAXMIN-
DISPERSION problem to equation 3, such as NP-hardness.
We describe a 2-approximation algorithm in algorithm 2
that was proposed in [15], and refer the reader to [15] for
further results.
3.3 Mono-objective formulation

The third and ﬁnal objective we will explore does not
relate to facility dispersion as it combines the relevance and
the similarity values into a single value for each document (as
opposed to each edge for the previous two objectives). The
objective can be stated in the notation of our framework in
terms of the function f (S), which is deﬁned as follows:

f (S) =

(cid:48)

w

(u)

(5)

X

u∈S

where the new relevance value w(cid:48)(·) for each document u ∈ U
is computed as follows:

(cid:48)

w

(u) = w(u) +

λ

|U| − 1

d(u, v)

X

v∈U

for some parameter λ > 0 specifying the trade-oﬀ between
relevance and similarity. Intuitively, the value w(cid:48)(u) com-
putes the “global” importance (i.e. not with respect to any
particular set S) of each document u. The axiomatic char-
acterization of this objective is as follows:

Also observe that it is possible to exactly optimize objec-
tive 5 by computing the value w(cid:48)(u) for all u ∈ U and then
picking the documents with the top k values of u for the set
S of size k.
3.4 Other objective functions

We note that the link to the facility dispersion problem
explored above is particularly rich as many dispersion objec-
tives have been studied in the literature (see [16,7]). We only
explore two objectives here in order to illustrate the use of
the framework, and also because other objectives share com-
mon algorithms. For instance, the MAXMSTDISPERSION
problem seeks to maximize the weight of the minimum span-
ning tree of the selected set. It turns out that algorithm 2 is
the best known approximation algorithm for this objective
as well, although the approximation factor is 4.

The axiomatic framework can also be used to characterize
diversiﬁcation objectives that have been proposed previously
(we note that the characterization might be non-trivial to
obtain as one needs to cast the objectives in our setting).
In particular, we point out that the DIVERSIFY objective
function in [1] as well as the MINQQUERYABANDONMENT
formulations proposed in [18] violate the stability and the
independence of irrelevant attributes axioms.

4. THE DISTANCE FUNCTION

The diversiﬁcation algorithm only partially speciﬁes the
framework, and to complete the speciﬁcation, we also need
to specify the distance and the relevance functions. We de-
scribe the relevance function later in the experiments, and
focus on the distance function here as it depends on the con-
tent of the data set being used, and might be of independent
interest. Speciﬁcally, we describe the distance function for
web pages and product hierarchies.
4.1 Semantic distance

Semantic distance is based on content similarity between
two pages.
Instead of using the whole of a page in the
similarity computation, we use simple sketching algorithms
based on the well known min-hashing scheme [5, 10] to com-
pute the sketch of a document and then apply Jaccard sim-
ilarity between sketches to compute the pairwise semantic
distance between the documents. We state this more for-
mally. Fix a hash function h that maps elements from
the universe U to a real number uniformly at random in
[0, 1]. Then the min-hash of a set of elements A is deﬁned as
M Hh(A) = argminx{h(x)|x ∈ A}. Therefore, M Hh(A) is
the element in A whose hash value corresponds to the min-
imum value among all values hashed into the range [0, 1].
This computation can be easily extended to multi-sets wherein
the min-hash of a bag A is computed as

M H(A) = argmin

x

{h(x, i)|x ∈ A, 1 ≤ i ≤ cx},

where cx is the frequency of element x in A. Thus, given
k hash functions h1,··· , hk, the sketch of a document d is
S(d) = {M Hh1 (d), M Hh2 (d), . . . , M Hhk (d)}. We can now
compute the similarity between two documents as

sim(u, v) =

|S(u) ∩ S(v)|
|S(u) ∪ S(v)|

Remark 3. The objective in equation 5 satisﬁes all the

axioms except consistency.

We note that Jaccard similarity is known to be a metric.
However, one issue that makes such a computation of sim(u, v)

WWW 2009 MADRID!Track: Search / Session: Search UI385ineﬀective is a large diﬀerence in lengths of u and v. One
simple approach to handle such random documents, would
be to discard documents that have a small sketch size.

Thus, one characterization of the semantic distance be-

tween two documents u and v could be
d(u, v) = 1 − sim(u, v)

4.2 Categorical distance

(6)

The semantic distance is not applicable in all contexts.
One scenario is when two ”intuitively” similar documents like
http://www.apache.org/ and http://www.apache.org/docs
actually might have very diﬀerent sketches. However, these
documents are ’close’ to each other with respect to the dis-
tance in the underlying web graph. However, computing
the pairwise distance of two web pages based on their web
graph connectivity can be very expensive. Taxonomies oﬀer
a succinct encoding of distances between pages wherein the
category of the page can be viewed as its sketch. There-
fore, the distance between the same pages on similar topics
in the taxonomy is likely to be small. In this context, we
use a weighted tree distance [4] as a measure of similarity
between two categories in the taxonomy. Distance between
two nodes u and v in the tree is computed as

l(u)X

i=1

l(v)X

i=1

d(u, v) =

1

2e(i−1) +

1

2e(i−1)

(7)

X

where e ≥ 0 and l(·) is the depth of the given node in the
taxonomy. This deﬁnition of a weighted tree distance re-
duces to the well-known tree distance (measured in path
length through the least common ancestor – lca(u, v)) when
e is set to zero and to the notion of hierarchically separated
trees (due to Bartal [4]) for greater values of e. Thus, nodes
corresponding to more general categories (e.g.,/Top/Health
and /Top/Finance) are more separated than speciﬁc cat-
egories (e.g., /Top/Health/Geriatrics/Osteoporosis and
/Top/Health/Geriatrics/Mental Health). We can extend
this notion of distance to the case where a document belongs
to multiple categories (with diﬀerent conﬁdences), one can-
not equate the categorical distance to the distance between
nodes in the taxonomy. Given two documents x and y and
their category information Cx and Cy respectively, we deﬁne
their categorical distance as

dc(x, y) =

u∈Cx,v∈Cy

min(Cx(u),Cy(v)) argmin

d(u, v)

(8)

v

where Cx(u) denotes the conﬁdence (or probability) of doc-
ument x belonging to category u.

5. EXPERIMENTAL EVALUATION

Recall that we used the axiomatic framework to character-
ize diﬀerences between diversiﬁcation objectives in section 3.
We now switch gears to investigate the other method of
distinguishing between various objectives, namely through
their experimental behavior. In this section, we character-
ize the choice of the objective function and its underlying
axioms using two well-known measures relevance and nov-
elty. We demonstrate the usefulness of the diversiﬁcation
framework by conducting two sets of experiments.

In the ﬁrst set of experiments, which we call semantic
disambiguation, we compare the performance of the three

diversiﬁcation algorithms using the set of Wikipedia disam-
biguation pages,4 as the ground truth. For instance, the
Wikipedia disambiguation page for jaguar5 lists several dif-
ferent meanings for the word, including jaguar the cat and
jaguar cars, along with links to their Wikipedia pages. The
titles of the disambiguation pages in Wikipedia (for instance,
jaguar in the above example) serve as the query set for our
evaluation. This is a natural set of queries as they have
been “labeled” by human editors as being ambiguous and the
search engine would want to cover their diﬀerent meanings
in the search results. The data set also has the advantage
of being large scale (about 2.5 million documents) and rep-
resentative of the words that naturally occur on the Web
and in web search (Wikipedia is one of the primary results
surfaced for many informational queries). In addition, un-
like query data for search engines, the Wikipedia data is in
public domain.

In the second set of experiments, which we call product dis-
ambiguation, we demonstrate the eﬃcacy of our algorithms
in diversifying product searches using the the categorical
distance function in Section 4.2. We use a product catalog
of about 41, 799, 440 products and 6808 product categories.
We use a logistic regression classiﬁer6 to classify the queries
into the product taxonomy.
5.1 Semantic disambiguation

We will refer to the queries drawn from the title of Wikipedia

disambiguation pages as ambiguous queries. Let us denote
the set of these titles by Q and the set of meanings or top-
ics (i.e. the diﬀerent Wikipedia pages) associated with each
disambiguation title q by Sq. Now, we posit that an ideal
set of diverse search results for query q would represent a
large selection of the topics in Sq (and not necessarily the
Wikipedia pages) within its top set of results.

To associate any search result page with a Wikipedia topic,
we compute the semantic distance between the web page
and the Wikipedia topic page using the distance function
described in Section 4.1. Thus, for a given query q, we com-
pute the topical distribution of a result page by computing
its distance to all pages in Sq. Let us denote the proba-
bility (distance normalized by the sum) of a document d
representing a particular topic s ∈ Sq by pq(x, s). We will
use this idea of coverage of a give topic to use the Wikipedia
data set for evaluating the eﬀectiveness of the diversiﬁcation
algorithm.

Recall from the framework that we view diversiﬁcation
as a re-ranking process for the search results, and we use
the search results for baseline comparison here. Thus, given
an ambiguous query q ∈ Q, we ﬁrst retrieve its top n re-
sults R(q) using a commercial search engine. Then we run
our diversiﬁcation algorithm to choose the top k diversiﬁed
results (the ordered list of diversiﬁed results is denote by
D(q)) from the set of n results and compare the set of top k
results, D(q) and Rk(q), in terms of relevance and novelty.
The details of the performance measurement for each of the
two measures are described next.
5.1.1 Novelty Evaluation
The idea behind the evaluation of novelty for a list is to
compute the number of categories represented in the list

4

5

6

http://en.wikipedia.org/wiki/Disambiguation_page

http://en.wikipedia.org/wiki/Jaguar_(disambiguation)

http://www.csie.ntu.edu.tw/~cjlin/liblinear/

WWW 2009 MADRID!Track: Search / Session: Search UI386L of top-k results for a given query q which we denote by
Noveltyq(L). We note that this measure is same as the S-
recall measure proposed in [21]. The list of topics for the
query q is given by the set Sq of Wikipedia disambiguation
pages for q.

in L, i.e. P

We compute the probability distribution over Sq for all
documents in the list L. To compute the representation or
coverage of a topic s ∈ Sq in the list L of search results,
we aggregate the conﬁdence on the topic over all the results
x∈L pq(x, s). If this sum is above a threshold
θ ∈ R we conclude that the topic s is covered by the S. The
fraction of covered topics gives us a measure of novelty of
the set S:

!

Noveltyq(L) =

1
|Sq|

I

pq(x, s) > θ

X

s∈Sq

 X

x∈L

where I (·) is the indicator function for an expression, eval-
uating to 1 if the expression is true, and 0 otherwise. Since
we are only interested in the diﬀerence in this value between
the two lists D(q) and Rk(q), we deﬁne fractional novelty:

Noveltyq(D(q)) − Noveltyq(Rk(q))

max`Noveltyq(D(q)), Noveltyq(Rk(q))´

FNq =

We note that the value FNq could also be negative, though
we would expect it to be positive for a diversiﬁed set of
results.
5.1.2 Relevance Evaluation
The relevance of a given document is often measured by
the likelihood of the document satisfying the information
need of the user expressed in terms of the search query.
Therefore, it could be viewed as a measure of how close
a document is to the query in the high-dimensional embed-
ding. To compute the overall eﬀectiveness of an ordering S,
we compute its relevance based on its relative distance to
the ideal ordering S

as

(cid:48)

˛˛˛˛ 1

rs

X

(cid:48)

s∈S

˛˛˛˛

− 1
(cid:48)
r
s

R(S, q) =

(9)

(cid:48)
s is the

where rs is the rank of the document s in S and r
rank of s in S

(cid:48) 7

The aim of this evaluation is to compare the relevance of
the diversiﬁed list D(q) with the original list Rk(q). To be
able to compare these lists, we need to know the relative
importance of each topic s ∈ Sq. For the list Rk(q) (or
L), we achieve this by using the search engine to perform a
site restricted search using only Wikipedia sites. From this
search, we produce a relevance ordering O on Sq by noting
the position of each s ∈ Sq. In the case of the diversiﬁed list
D(q), we compute the relevance of a topic s ∈ Sq as

X

Rel(s, q) =

1

pq(d, s),

d∈D(q)

pos(d)

where pos(d) is the 1-based rank of d in the list D(q). We
compute a relevance ordering O(cid:48)
for D(q) and use the func-
7Note that since we formulate the result diversiﬁcation as a
re-ranking problem, we assume that both S and S
contain
the same documents albeit in a diﬀerent order. One sim-
ple characterization of relevance could set the rank of each
document to its position in the ordered set of results.

(cid:48)

Figure 1: [Best viewed in color] The eﬀect of vary-
ing the value of the trade-oﬀ parameter λ, and the
threshold for measuring novelty on the output of the
search results from MAXSUMDISPERSION.

tion in Equation 9 to compute the relevance distance be-
tween the two lists. The value computed from applying
Equation 9 to these lists is the relevance score Relevanceq(L).
As before, we can compute the fractional diﬀerence in the
relevance score:

FRq =

Relevanceq(D(q)) − Relevanceq(Rk(q))

max (Relevanceq(D(q)), Relevanceq(Rk(q)))

5.1.3 Results
The parameters used for the experiments in this work as:
n = 30, k = 10. We choose n = 30 as relevance decreases
rapidly beyond these results. The other parameters for the
experiments are λ and θ, and the eﬀect of these parameters is
shown in Figure 1. Each point in this plot represents the av-
erage fractional diﬀerence in novelty given a value of λ and θ.
The average is computed over a 100 queries drawn randomly
from the set of the Wikipedia disambiguation pages. First
of all, note that the fractional diﬀerence is always positive
indicating that the diversiﬁcation algorithm does succeed in
increasing the novelty of the search results. Further, observe
that increasing the conﬁdence threshold θ has the eﬀect of
increasing the fractional diﬀerence between the search re-
sults. This indicates that the diversiﬁed search results have
a higher conﬁdence on the coverage of each category, and
consequently the upward trend is a further vindication of
increase in novelty of the diversiﬁed search results. Recall
that the λ value governs the trade-oﬀ between relevance and
diversity, and hence one would expect the novelty to increase
with λ. This trend is observed in the plot, although the
increase is marginal above a certain value of λ when the
trade-oﬀ is already heavily biased towards novelty.

Figure 2(a) plots the histogram of the fractional diﬀerence
in novelty as obtained over a 1000 queries drawn randomly
from the set of Wikipedia disambiguation pages. It is worth
noting that from the deﬁnition of fractional novelty that a
fractional diﬀerence value of 0.1, with 10 covered categories,
implies that the diversiﬁed search results covered one more
category than the vanilla search results. Hence, on an aver-
age, the diversiﬁed search results cover as many as 4 more
categories out of every 10 as compared to the original set of
search results. In fact, we can say about 75% of the queries

00.050.10.150.20.250.30.3500.10.20.30.40.50.60.70.80.91Fractional difference in noveltyThresholds for measuring noveltyNovelty for Max-sum as a function of thresholds and lambda0.10.20.40.60.812468WWW 2009 MADRID!Track: Search / Session: Search UI387produced more diverse results than the search results8. We
note that on the overall, MAXMINDISPERSION outperforms
the other two objectives.

(a) Novelty

(b) Relevance

Figure 2:
[Best viewed in color] The histogram of
fractional diﬀerence in novelty (a) and relevance (b)
plotted over a 1000 ambiguous queries.

Figure 2(b) plots the histogram of the fractional diﬀer-
ence in relevance as obtained over the same sample of 1000
queries. We note that the diversiﬁed set of search results
does as well as the search engine ranking in the majority
of the queries. Thus, the diversiﬁcation algorithm does not
suﬀer much in terms of relevance, while gaining a lot in nov-
elty. Within the three algorithms, MONOOBJECTIVE does
quite well on relevance on some queries, which is expected
due to the importance given to relevance in the objective.

In another set of experiments, we study the positional
variation of both relevance and novelty for all the three ob-
jective functions, i.e. we study the relevance and diversity
values by restricting the output ranking to top m ranks,
where m = {1, 2,··· , k}. We set λ = 1.0 and θ = 0.5 for
this experiment. Figure 3(a) plots the diversity at each po-
sition for the three objective functions and the search engine
results. We note that the number of topics covered increases
with position, and the results D(q) produced by all three al-
gorithms cover a larger number of categories even at the
high ranked positions (low m) compared to the search en-
gine. Although the diﬀerence in novelty between the search
engine results and the diverse results is noticeable, the three
diversiﬁcation objectives perform quite comparably to each
other. A similar plot for relevance is shown in Figure 3(b),
which plots the positional variation in the value of relevance.
We observe that all the orderings are equally relevant in the
higher positions (lower m) while diﬀerences appear in lower
positions. We note on the overall, the MONOOBJECTIVE
formulation does the best in terms of relevance as it has
the smallest distance to the ground truth. The MAXSUM-
DISPERSION algorithm comparatively produces the least rel-
evant result sets among the three diversiﬁcation objectives
as it is more likely to add a more diverse result with respect
to S than the other algorithms.
5.2 Product Disambiguation

In this set of experiments, we evaluate the performance
of the three objective functions using diﬀerent notions of
distance and relevance. Speciﬁcally, we use the distance
function described in Section 4.2 and the relevance score
based on the popularity of a product. We note that the
categorical distance can be tuned using the parameter e to

8The spike at 0 is due to queries where the search engine
results had no overlap with the Wikipedia topics.

(a) Novelty

(b) Relevance

Figure 3: [Best viewed in color] The positional varia-
tion in the novelty (a) and relevance (b) of the result
set.

eﬀectively distinguish between various features of a product.
For example, we would two diﬀerent CD players produced
by the same company to be closer to each other than to a
CD player from a diﬀerent company in a product taxonomy.
Our data set (obtained from a commercial product search
engine) consists of a set of 100 product queries and the top
50 results ranked according to popularity.

There is one drawback when we rank products based on
their popularity. In the case where the popularity is skewed
toward one or a few brands, the results can be dominated
by products belonging to that brand with (sometimes) slight
variations in the product descriptions. To observe the eﬀec-
tiveness of our formulation and the distance function, we
diversiﬁed the results for 100 queries using the MAXSUM-
DISPERSION algorithm and compared the ordering with the
results produced by a commercial product search engine.
The parameters in this experiment were as follows: n = 30,
k = 10, and λ = 1.0. Table 1 illustrates the diﬀerence be-
tween the orderings for the query cd player. Even though
the search engine results for cd player oﬀers some domi-
nant brands, it does not include the other popular brands
that the diversiﬁed results capture. Note that we do not
alter the relative positions of the popular brands in the di-
versiﬁed results.

Similar to novelty evaluation for semantic disambiguation,
we compute the number of categories represented in the list
L of top-k results for a given query q which we denote by
Noveltyq(L). To compute the representation of a category
in the result set, we require that the category is not the
descendant of any other category in the result set. The
fraction of covered topics gives us a measure of novelty of
the set S:

X

u,v∈L

Noveltyq(L) =

2

|L|(|L| − 1)

I (lca(u, v) /∈ {u, v})

02040608010012014016018020000.10.20.30.40.50.60.70.80.91Fractional NoveltyMaxSumMaxMinMono0100200300400500600700-1-0.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.100.10.20.30.40.50.60.70.80.91Fractional RelevanceMaxSumMaxMinMono01234567891012345678910Number of TopicsPositionSearch EngineMax-SumMax-MinMono Objective00.20.40.60.811.21.41.612345678910Relevance DistancePositionSearch EngineMax-SumMax-MinMono-ObjectiveWWW 2009 MADRID!Track: Search / Session: Search UI388Product Search Engine

Sony SCD CE595-SACD changer

Sony CDP CE375-CD changer
Sony CDP CX355-CD changer
Teac SR-L50- CD player/radio

Bose Wave Music System Multi-CD Changer
Sony RCD-W500C-CD changer/CD recorder

Sony CD Walkman D-EJ011-CD player

Sony S2 Sports ATRAC3/MP3 CD Walkman D-NS505

Diversiﬁed Results

Sony SCD CE595-SACD changer

Sony CDP CE375-CD changer
Teac SR-L50-CD player/radio

Bose Wave Music System Multi-CD Changer

Sony S2 Sports ATRAC3/MP3 CD Walkman D-NS505

COBY CX CD109-CD player

JVC XL PG3-CD player

Pioneer PD M426-CD changer

Sony Atrac3/MP3 CD Walkman D-NF430

Sony SCD XA9000ES-SACD player

COBY CX CD109-CD player

Yamaha CDR HD1500-CD recorder/HDD recorder

Table 1: The diﬀerence in the top 10 results for the query cd player from a commercial product search engine
and the diversiﬁed results produced by running MAXSUMDISPERSION with n = 30 and k = 10

represent the nodes in the taxonomy as well. Figure 4(b)
shows the positional variation in relevance for the product
search engine and the diverse results produced by MAXMIN-
DISPERSION. Surprisingly, the relevance of MONOOBJECTIVE
decreases relative to the other objectives.

6. CONCLUSIONS

This work presents an approach to characterizing diversi-
ﬁcation systems using a set of natural axioms and an em-
pirical analysis that qualitatively compares the choice of ax-
ioms, relevance and distance functions using the well-known
measures of novelty and relevance. The choice of axioms
presents a clean way of characterizing objectives indepen-
dent of the algorithms used for the objective, and the speciﬁc
forms of the distance and relevance functions. Speciﬁcally,
we illustrate the use of the axiomatic framework by studying
three objectives satisfying diﬀerent subsets of axioms. The
empirical analysis on the other hand, while being depen-
dent on these parameters, has the advantage of being able
to quantify the trade-oﬀs between novelty and relevance in
the diversiﬁcation objective. In this regard, we explore two
applications of web search and product search, each with
diﬀerent notions of relevance and distance. In each applica-
tion, we compare the performance of the three objectives by
measuring the trade-oﬀ in novelty and relevance.

There are several open questions that present themselves
in light of these results. In terms of the axiomatic frame-
work, it would be interesting to determine if the impossi-
bility proof still holds if the distance function is a metric.
Relaxations of the axioms (for instance, weakening the sta-
bility axiom) are also an avenue for future research. Another
direction of future research could be to explore the facility
dispersion link, and identify optimal objective functions for
settings of interest (such as web search, product search etc).

Acknowledgments
We would like to thank Kunal Talwar for helpful discussions,
and Panayiotis Tsaparas for providing data and helping us
with the classiﬁcation of product queries.

7. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.

Diversifying search results. In Proc. 2nd ACM Intl
Conf on Web Search and Data Mining, 2009.

[2] A. Altman and M. Tennenholtz. On the axiomatic

foundations of ranking systems. In Proc. 19th
International Joint Conference on Artiﬁcial
Intelligence, pages 917–922, 2005.

(a) Diversity

(b) Relevance

Figure 4: [Best viewed in color] The positional vari-
ation in the diversity (a) and relevance (b) of the
product result set averaged over 100 queries with
n = 30, k = 10, λ = 1.0 and θ = 0.5.

where I (·) is the indicator function and lca(·,·) returns the
least common ancestor of the two given nodes. Figure 4(a)
illustrates the positional variation in the novelty of both the
product search results ranked by popularity and the diverse
set of results produced by MAXMINDISPERSION. For the
relevance evaluation, we did not have any ground truth to
compare the diﬀerent orderings.
Instead, we posit that a
result is relevant to the query depending on how related the
result and the query categories are in the taxonomy. We
consider two categories to be completely related if one sub-
sumes the other. Given a query q, we compute the relevance
of a list L as

X

1 + d(lca(q, u), q)

,

1

Relevanceq(L)

=

1
|L|

u∈L

pos(u)

where pos(·) is rank of the result u and d(·,·) is computed
using Equation 7. In abuse of notation, we use q and u to

00.2123456Relevance DistancePosition00.050.112300.511.522.533.5412345678910Number of topicsPositionSearch EngineMax-SumMax-MinMono-Obj12345678910Position00.050.10.150.20.250.30.350.412345678910Search EngineMax-SumMax-MinMono-ObjWWW 2009 MADRID!Track: Search / Session: Search UI389[3] Kenneth Arrow. Social Choice and Individual Values.

[19] E. Vee, U. Srivastava, J. Shanmugasundaram, P. Bhat,

Wiley, New York, 1951.

[4] Yair Bartal. On approximating arbitrary metrices by

tree metrics. In STOC, pages 161–168, 1998.

[5] Andrei Z. Broder, Moses Charikar, Alan M. Frieze,
and Michael Mitzenmacher. Min-wise independent
permutations. Journal of Computer and System
Sciences, 60(3):630–659, 2000.

[6] J. Carbonell and J. Goldstein. The use of MMR,

diversity-based reranking for reordering documents
and producing summaries. Proceedings of the 21st
annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 335–336, 1998.

and S.A. Yahia. Eﬃcient Computation of Diverse
Query Results. IEEE 24th International Conference
on Data Engineering, 2008. ICDE 2008, pages
228–236, 2008.

[20] ChengXiang Zhai. Risk Minimization and Language

Modeling in Information Retrieval. PhD thesis,
Carnegie Mellon University, 2002.

[21] C.X. Zhai, W.W. Cohen, and J. Laﬀerty. Beyond

independent relevance: methods and evaluation
metrics for subtopic retrieval. Proceedings of the 26th
annual international ACM SIGIR conference on
Research and development in informaion retrieval,
pages 10–17, 2003.

[7] Barun Chandra and Magn´us M. Halld´orsson.

Approximation algorithms for dispersion problems. J.
Algorithms, 38(2):438–465, 2001.

[22] C.X. Zhai and J. Laﬀerty. A risk minimization

framework for information retrieval. Information
Processing and Management, 42(1):31–55, 2006.

[8] H. Chen and D.R. Karger. Less is more: probabilistic

[23] C.N. Ziegler, S.M. McNee, J.A. Konstan, and

models for retrieving fewer relevant documents.
Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 429–436, 2006.

[9] C.L.A. Clarke, M. Kolla, G.V. Cormack,

O. Vechtomova, A. Ashkan, S. B¨uttcher, and
I. MacKinnon. Novelty and diversity in information
retrieval evaluation. Proceedings of the 31st annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 659–666,
2008.

[10] Sreenivas Gollapudi and Rina Panigrahy. Exploiting

asymmetry in hierarchical topic extraction. In CIKM,
pages 475–482, 2006.

[11] R. Hassin, S. Rubinstein, and A. Tamir.

Approximation algorithms for maximum dispersion.
Operations Research Letters, 21(3):133–137, 1997.

[12] J. Kleinberg. An Impossibility Theorem for Clustering.

Advances in Neural Information Processing Systems
15: Proceedings of the 2002 Conference, 2003.

[13] B. Korte and D. Hausmann. An Analysis of the

Greedy Heuristic for Independence Systems.
Algorithmic Aspects of Combinatorics, 2:65–74, 1978.

[14] SS Ravi, D.J. Rosenkrantz, and G.K. Tayi. Facility

dispersion problems: Heuristics and special cases.
Proc. 2nd Workshop on Algorithms and Data
Structures (WADS), pages 355–366, 1991.

[15] S.S. Ravi, D.J. Rosenkrantz, and G.K. Tayi. Heuristic

and special case algorithms for dispersion problems.
Operations Research, 42(2):299–310, 1994.

[16] SS Ravi, D.J. Rosenkrantzt, and G.K. Tayi.

Approximation Algorithms for Facility Dispersion. In
Teoﬁlo F. Gonzalez, editor, Handbook of
Approximation Algorithms and Metaheuristics.
Chapman & Hall/CRC, 2007.

[17] Stephen Robertson and Hugo Zaragoza. On

rank-based eﬀectiveness measures and optimization.
Inf. Retr., 10(3):321–339, 2007.

[18] Atish Das Sarma, Sreenivas Gollapudi, and Samuel
Ieong. Bypass rates: reducing query abandonment
using negative inferences. In KDD ’08: Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 177–185,
New York, NY, USA, 2008. ACM.

G. Lausen. Improving recommendation lists through
topic diversiﬁcation. Proceedings of the 14th
international conference on World Wide Web, pages
22–32, 2005.

8. APPENDIX
8.1 Impossibility Result

Proof of Theorem 1. We start by ﬁxing functions w(·)
and d(·,·) such that f is maximized by a unique S∗
k for all
k ≥ 2. Such a set of functions always exist, from the richness
axiom. Now, ﬁxing a k, we can use the uniqueness property
and the stability axiom, to say that ∀y /∈ S∗

k+1, we have:

∗
k+1) > f (S

k ∪ {y})
∗

f (S

k+1 \ S∗

k ∪ {y}) ≥ f (S∗

Note that we have a strict inequality here, as otherwise the
monotonicity axiom would imply that there is no unique
k) for all y /∈ S∗
k+1, as we have that f (S∗
S∗
k.
From here on, we ﬁx one such y.
Let x = S∗
k (which follows from stability). Now,
we invoke the strength of relevance (b) axiom for S∗
k, in
conjunction with the independence of irrelevant attributes
axiom, to imply that the function value f (S∗
k+1) of the set
k+1 decreases by some δ > 0 if w(u) is ﬁxed for all u ∈ S
S∗
and w(x) = a0 for some a0 > 0. Next, we do the following
transformation: 1) increase d(y, u) for all u ∈ S∗
k to be equal
to twice their current value, and 2) scale all the relevance
and distance values by half.

k and S∗

Firstly, note that the ﬁrst step of this transformation is
consistent (in the sense deﬁned in the consistency axiom)
w.r.t both S∗
k+1. Also, the second step uses global
scaling, and hence from the scale invariance and consistency
axioms, we have that the output of f for size k and k + 1
should still be S∗
k+1 respectively. Further, from the
above comment about the decrease in f (S∗
k+1), we have that
k+1) − f (S∗
f (S∗
k) strictly decreases. In addition, we can use
k ∪ {y})
the strength of diversity (a) axiom to see that f (S∗
strictly increases. Repeating this process several times, we
are guaranteed to get a state where

k and S∗

f (S
which implies that S∗
tradiction to the richness axiom.

k ∪ {y})
∗

k+1) ≤ f (S
∗
k+1 is not unique, and is hence a con-

WWW 2009 MADRID!Track: Search / Session: Search UI390