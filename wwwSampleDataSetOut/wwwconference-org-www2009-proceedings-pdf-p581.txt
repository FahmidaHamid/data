Rapid Prototyping of Semantic Mash-Ups through

Semantic Web Pipes

Danh Le-Phuoc, Axel Polleres,

Manfred Hauswirth, Giovanni Tummarello

Digital Enterprise Research Institute,
National University of Ireland, Galway

Galway, Ireland

Christian Morbidoni

Univ. Politecnica delle Marche

Ancona, Italy

ABSTRACT
The use of RDF data published on the Web for applica-
tions is still a cumbersome and resource-intensive task due
to the limited software support and the lack of standard pro-
gramming paradigms to deal with everyday problems such as
combination of RDF data from diﬀerent sources, object iden-
tiﬁer consolidation, ontology alignment and mediation, or
plain querying and ﬁltering tasks. In this paper we present
Semantic Web Pipes that support fast implementation of Se-
mantic data mash-ups while preserving desirable properties
such as abstraction, encapsulation, component-orientation,
code re-usability and maintainability which are common and
well supported in other application areas.
Categories and Subject Descriptors:
D.1.1[Programming Techniques]: Applicative (Functional)
Programming; H.4.0[Information Systems Applications]
General Terms: Algorithms, Design
Keywords: RDF, Pipes, Semantic Web, Mash-up
1.

INTRODUCTION

Typical Semantic Web applications are data-intensive and
require the combination and integration of RDF data from
distributed data sources. The development of generic Web
applications is well understood and supported by many tra-
ditional computer science domains, such as classical database
applications. In current Web application development data
integration and access are typically dealt with by fairly so-
phisticated abstractions and tools, supporting rapid applica-
tion development and the generation of reusable and main-
tainable software components. The task of programming
such applications has become the task of combining exist-
ing components from well-established component libraries,
i.e., customizing and extending them for application-speciﬁc
tasks. Typically, such applications are built built relying on
a set of standard architectural styles which shall lower the
number of bugs and ensure code that is easy to understand
and maintain.

In contrast to that, data-intensive applications using RDF
are currently mostly custom-built with limited support for
reuse and standard functionalities are frequently re-implemented
from scratch. Cumbersome, resource-intensive and error-
prone tasks such as object identiﬁer consolidation, ontology
alignment and mediation ,or just plain querying and pro-
cessing tasks are unnecessarily repeated in a lot of software
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

systems. While the use of powerful tools such as SPARQL
processors, takes the edge oﬀ of some of the problems, a
lot of classical software development problems remain. Also
such applications are not yet built according to agreed ar-
chitectural styles which is mainly a problem of use rather
than existence of such styles. This problem though is well
addressed in classical Web applications. For example, before
the introduction of the standard 3-tier model for database-
oriented Web applications and its support by application
development frameworks, the situation was similar to a lot
the situation that we see now with RDF-based applications.
In this paper we propose a ﬂexible architectural style for
the fast development of reliable data-intensive applications
using RDF data. Architectural styles have been around for
several decades and have been the subject of intensive re-
search in other domains such as software engineering and
databases. We base our work on the classical pipe abstrac-
tion and extend it to meet the requirements of (semantic)
Web applications using RDF. The pipe concept lends it-
self naturally to the data-intensive tasks at hand by its in-
trinsic concept of decomposing an overall data-integration
and processing task into a set of smaller steps which can be
freely combined. This resembles a lot the decomposition of
queries into smaller subqueries when optimizing and gener-
ating query plans. To some extent, pipes can be seen as
materialized query plans deﬁned by the application devel-
oper. Besides, the intrinsic encapsulation of core function-
alities into small components, this paradigm is inherently
well suited to parallel processing which is an additional ben-
eﬁt for high-throughput applications which can be put on
parallel architectures or Grid environments.

The need for standard application development frame-
works stems from the fact that an increasing amount of RDF
data becomes available, for example, from widely used appli-
cations such as DBLP, DBpedia, blogs, wikis, forums, etc.
that expose their content in diﬀerent RDF-based formats
such as SIOC [8] or FOAF [9], in the form of RDF/XML, or
RDF statements embedded or extractable from HTML/XML
pages by technologies such as GRDDL [12] or RDFa [1]. De-
spite the existence of standards and de-facto standards for
publishing RDF, key problem in systems processing RDF
are that the data (i) is fragmented, (ii) may be incomplete,
incorrect or contradicting, (iii) partly follows ontologies, of-
ten with ontologies used wrongly or inconsistently, to name
a few, and thus needs to be “sanitized” before it can be
processed. A speciﬁcally cumbersome problem is the use of
diﬀerent identiﬁers denoting the same object which need to
be uniﬁed. The processing steps required are often similar

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management581across diﬀerent applications and can in fact be encapsulated
in customizable, reusable components which can be com-
bined via a small set of base operators.

The pipes metaphor for the Web has been pioneered by
Yahoo Pipes1, which support the implementation of cus-
tomized services and information streams through the com-
bination of data sources (usually RSS feeds) with simple op-
erators and processing constructs. Since the resulting Web
pipes are themselves data sources, they can be reused and
combined to form new pipes. Also, Web pipes are “live”:
they are computed on demand when requested via an HTTP
invocation, and thus reﬂect an up-to-date state of the system
(which can be detrimental as well in some scenarios where
caching would be applicable).

Being mainly targeted to work with RSS feeds (item lists)
though limits the applicability of Yahoo Pipes to the more
general graph based data model of RDF. To this end, we
come up with a pipes design, conceptual model and imple-
mentation that speciﬁcally targets graph based RDF data
and allows the developer to quickly prototype (semantic)
Web applications using RDF. Our system, called Seman-
tic Web Pipes (SWP), emphasizes Semantic Web data and
standards and enables the community-based generation of
reusable Semantic Web processing components. By basing
on common Web standards, pipes can be deployed on most
common Web application servers. That means pipes can
run on one machine or be distributed among an arbitrary
number of nodes.

SWP oﬀers specialized operators that can be arranged in
a graphical Web editor to perform the most important data
aggregation and transformation tasks without requiring pro-
gramming skills or sophisticated knowledge about Semantic
Web formats. This enables developers as well as end users
to create, share and re-use semantic mash-ups.
1.1 Motivating Example

To give a concise overview of how SWP works, we sketch
some of the main functionalities by giving a typical example
of Semantic Web data aggregations from multiple sources.

We aim to aggregate data about Tim Berners-Lee from
various sources on the Semantic Web (his FOAF ﬁle, DBLP,
and DBPedia entries, etc.). However, we cannot simply
merge these source, since all three sources use diﬀerent iden-
tiﬁers for Tim. So we need to normalize the data before
aggregation, for example by changing the URI used in DB-
Pedia and DBLP to match with his self-chosen URI (the
one used in his FOAF ﬁle). This job can be done by two
SPARQL CONSTRUCT queries. For DBLP we query:

CONSTRUCT {<http://www.w3.org/People/Berners-Lee/card#i> ?p ?o.

?s2 ?p2 <http://www.w3.org/People/Berners-Lee/card#i>}

WHERE
{{<http://dblp.l3s.de/d2r/resource/authors/Tim_Berners-Lee> ?p ?o}

UNION

{?s2 ?p2 <http://dblp.l3s.de/d2r/resource/authors/Tim_Berners-Lee>}}

For DBPedia we query:

CONSTRUCT {<http://www.w3.org/People/Berners-Lee/card#i> ?p ?o.

?s2 ?p2 <http://www.w3.org/People/Berners-Lee/card#i>}

WHERE
{{<http://dbpedia.org/resource/Tim_Berners-Lee> ?p ?o}

UNION
{?s2 ?p2 <http://dbpedia.org/resource/Tim_Berners-Lee>}}

The system should allow to encapsulate such queries in
functional blocks (e.g. using the C-operator later described),

1

http://pipes.yahoo.com/

which can then be connected to simple operators for fetching
the data and to the output, obtaining a simple pipe that
outputs the normalized RDF data.
In Figure 1 we show
a simple example of such a combination which solves our
simple use case: URIs are normalized via the C-operators
and then joined with Tim’s FOAF ﬁle.

We observe that most of this example might be done in
a single SPARQL query. However, besides the obvious ad-
vantage of modular design and reusability of basic building
blocks as well as whole pipes for aggregations in more ﬂex-
ible ways, we will see in the course of this paper that the
system oﬀers many more useful features beyond the capabil-
ities of single SPARQL queries. For instance, we allow the
user to plug RDFS inference in between certain blocks (e.g.,
applying materialization of inferred triples for Tim’s FOAF
data, but not on the imported data); or, we allow to com-
pute the input graphs for one SPARQL query by the output
of another, i.e., dynamic addition resources to a pipe, etc.

Figure 1: Workﬂow to aggregate Tim’s SW data

In the remainder of this paper, we will ﬁrst scetch the
basic concepts and operators allowed in SWP (Section 2).
Our implemented system that comprises an execution en-
gine and enables the creation and modiﬁcation of pipes in
a graphical editor is described in Section 3: once saved in
the editor, pipes are available at a stable URL and exe-
cutable by users through a simple HTTP call. We qual-
itatively compare SWP against other available RDF Web
application paradigma, namely single SPARQL queries and
ad hoc Java scripting in Section 4. Related works, as well as
an outlook to future work are presented in Sections 5 and 6.
2. BASIC CONCEPTS AND OPERATORS

The use cases we aim at addressing involve aggregation of
data available on the Web (expressed in several RDF seri-
alization formats, such as RDF/XML, RDFa, or microfor-
mats) as well as processing it in meaningful ways, that is,
”ﬁltering” and/or ”transforming” the original RDF data. To
this end Semantic Web Pipes introduce several base opera-
tors which can build up such a pipe that range from RDF
extraction from existing Web content to SPARQL query pro-
cessing or RDFS (and partial OWL) closure inference. In
the following, we will introduce the basic deﬁnition of such
operators and describe currently supported operators by ex-
ample. We emphasize that the generic operator model of
Semantic Web Pipes is extensible by new operators, some of
which we will sketch at the end of this section.

A Semantic Web pipe as we deﬁne it implements a prede-
ﬁned workﬂow that, given a set of RDF sources (resolvable
URLs), composes and processes them by means of pipelined
special purpose operators. We do not aim at replacing com-
plex workﬂow languages though, but rather promote a very
reduced acyclic data processing model. More speciﬁcally,
we only support two of the “classical” workﬂow patterns,
namely split and merge.

All other base operators have exactly one output and one
or more input parameters, each of which has a speciﬁc input

TBL on DBPLTBL on DBPediaTBL's FOAFRDF outputCONSTRUCT+U"CONSTRUCT for DBLP""CONSTRUCT for DBPedia"getRDFgetRDFCONSTRUCTgetRDFWWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management582type. The set of base operators which we consider suﬃcient
for most Semantic Web data aggregation tasks are shown
in Fig. 2. All inputs in a pipe are either string text, RDF
or other XML data. Parameters are named and may be in-
stantiated either exactly once (ﬁxed parameter ) or arbitrary
times (variable arity parameter ) marked by “*” in the graph-
ical representation in Fig. 2). As an example the SPARQL
CONSTRUCT- and SELECT- operators may have exactly
one default graph parameter (D), but an arbitrary number
of named graph parameters (N* ). Parameters can also ei-
ther have single values or sequences as input marked by “?”,
an example being variable substitutions for the SPARQL
CONSTRUCT- and SELECT- operators, see below. More
details on the supported operator are given in Section 2.1.
A pipe is a set of instances of the operators in Fig. 2, where:
• Each ﬁxed parameter input and all variable arity pa-
rameter inputs are linked to either (i) quoted literals
such as "<?xml version="1.0" ?> ..." for providing
ﬁxed string input (ii) a URL in angle brackets such
as <http://alice.example.org> denoting a Web re-
trievable data source that contains data in the required
input format, (iii) to the output of another pipe.

• All but one output (the “overall” output of the pipe)

are linked inputs of other operators.

• Links between inputs and outputs are acyclic.

By following these constraints, each pipe can itself be used
as an operator in another pipes. Apart from the graphical
format implicitly given in Fig. 2, in Section 3 below, we will
outline an XML format that allows to publish pipe “code” at
an arbitrary URL to be re-usable as operators in other pipes.
Furthermore, note that we do not constrain type mismatches
between links of outputs of one operator or pipe to another
operators: as described in Section 3, our system can handle
several types of errors, e.g., timeouts while retrieving some of
the sources or malformed source data, or ignoring non-RDF-
convertible input, while still producing a valid output based
on the remaining correct input data. The default behavior is
to treat such unavailable/malformed inputs as empty, and
likewise in certain cases, a wrong module input can cause
the pipe processing to halt with an empty output.

For simplicity, we limit our model to pipes that do not
contain cycles, leaving the study of cyclic pipes to future
work.2 We remark here that, as the current pipe engine is
based on a XML tree model (see Section 3), such cycles are
in fact not possible, at least within a single pipe.
2.1 Supported Operators

As mentioned above already, unlike fully-ﬂedged workﬂow
models, our current pipes model is a simple construction kit
that consists of only two of the typical workﬂow constructs,
namely split and merge, and a set of base operators which
we will outline in the following section.

The Merge Operator: RDF Merge
This operator takes a arbitrary number of RDF graphs as in-
puts (variable arity parameter G*), expressed in RDF/XML,
N3 [6] or Turtle [5] format, and produces an RDF graph that

2Such cycles would require conditional operators for termi-
nation conditions, etc., and would let us end up with a fully-
ﬂedged workﬂow language, which we do not intend to rein-
vent here.

Figure 2: Semantic Web pipe operators.

is composed of the merge of its inputs. The standard imple-
mentation of the merge operator simply standardizes blank
nodes apart, according to the RDF merge deﬁnition in [?].
The Split Operator
In order to enable reuse of outputs of other operators, we
provide a split operator that allows to connect a single out-
put to an arbitrary number of inputs of other operators. For
an example, we refer to Fig. 4 below.
The getRDF and getXML Operators
These two operators take a text parameter URL as input
which is supposed to retrieve a URL from the Web and con-
vert it to RDF or XML respectively. Depending on the
content type retrieved, for RDF extraction (getRDF) the
following attempts are made: RDF/XML, Turtle, N3 can
be consumed directly, HTML will be searched for RDFa or
well-known microformats (XFN, hCal, hCard, hReview) for
which a GRDDL extract is known. Likewise, explicitly men-
tioned GRDDL transforms will be executed. If none of these
attempts succeeds an empty graph is returned. XML ex-
traction (getXML) follows a simpler rule, retrieving HTML
and tidying3 it into XHTML, whereas other XML is left un-
touched, also possibly referenced XSL transformations are
not executed, as this can be done explicitly by the XSLT
operator covered next.
The XSLT Operator
This operator is useful for explicit execution of an XSL trans-
formation on a particular XML input ﬁle. In the event that
the XSL parameter is empty, the operator will attempt to
execute an XSL transformation dereferenced in the XML
input’s prolog, otherwise the XML is passed through un-
touched. This operator is particularly useful when custom
XML output formats are needed or when an input source
in a custom XML format shall be transformed to RDF, e.g.
when a GRDDL transform is not explicitly dereferenced.
The SPARQL CONSTRUCT and SELECT Operators
The ﬁnal and most essential operators in our framework are
the SPARQL CONSTRUCT and SELECT operators which
can help in aligning RDF data. Such queries can be used
to extract only relevant information from a bigger graph as
well as to ’align’ two graphs, by smushing4 identiﬁers or even
performing basic ontology mapping operations [20].

3

4

http://tidy.sourceforge.net/

http://esw.w3.org/topic/RdfSmushing

+UG*RDFRDF...Format...FormatFormatXSLTXML(c) general base operator(d) base operatorsXMLXMLXMLXSLgetRDFTextRDFURLgetXMLTextXMLURLRDFSRDFRDFGP1Pi*......(a) mergeGRDFRDF...(a) splitCONSTRUCTRDFRDFTextGRDF...N*Qv1vn...SELECTRDFRDFTextGRDF...N*Qv1vnXML/Text...XML/TextOWLRDFRDFGXML/TextXML/TextWWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management583The CONSTRUCT operator outputs the result of a con-
struct query Q given as textual input performed on the
SPARQL dataset [21] consisting of the default graph G and
named graphs given in optional N* parameters. The input
query allows full SPARQL CONSTRUCT syntax with the
following diﬀerences for FROM/FROM NAMED clauses:

(i) the dataset may be implicitly given in the inputs by us-
ing the G and N* parameters. For illustration of this dataset
generation, the CONSTRUCT operator corresponds to a na-
tive SPARQL CONSTRUCT query as follows:

Figure 3: A pipe which iterates over results of one
SPARQL operator and processes results in another.

(ii) Variables linked via inputs may be used in FROM/FROM

NAMED clauses, which we will explain in the following.

Variables occurring in the query in the CONSTRUCT,
WHERE, FROM or FROM NAMED clause can be linked to
inputs. This has the following eﬀect: Depending on whether
the input linked to variable v is a valid SPARQL result
form [21] or not, either the query Q iterates over all result
bindings for that very variable in that result form, replacing
the occurrences of v in Q with respective result bindings for
v in the input. This is handled by simply executing a re-
spective XPath/Xquery extracting the respective sequence
of bindings from the SPARQL result. If the same query re-
sult is linked (through the same split operator) to diﬀerent
variables input of the same subsequent query, then the iter-
ations are as per solution, whereas variables bound to solu-
tion sequences of from diﬀerent input queries are looped over
nestedly executing the respective query building a Cartesian
product. The results of these nested loops of CONSTRUCT
queries are merged into the same output RDF graph for the
CONSTRUCT operator.

The SELECT operator works analogously, but returning
a SPARQL result format XML document. Again results of
these nested loops are appended in the same SPARQL result
format XML document.

This semantics allows us to stack SPARQL queries into
each other. Thus pipes can emulate simple FOR-loops, with-
out oﬀering the full expressivity of procedural languages.

When a textual or other non-SPARQL result format input
is linked to a variable input parameter simply all occurrences
of the variable in the query Q are replaced with textual value
as an RDF plain literal in the query.

We illustrate the use of input variables in the SPARQL
query operator with a simple example in Fig. 3. This pipe
extracts the foaf:knows relations in Tim Berner’s Lee so-
cial “cloud” derived from RDF ﬁles linked in Tim Berners-
Lee’s own FOAF ﬁle via rdfs:seeAlso links. Such dynamic
generation of the dataset or iteration over the results of one
SPARQL query and processing them in another is not doable
in “plain” SPARQL but easy in pipes.

As a last example for the use of operators SPARQL, let
us turn to a pipe which uses the SPLIT operator, as shown
in Fig. 4. This pipe ﬁrst retrieves ISBN numbers of con-
tributions in books by Tim Berners-Lee from DBLP by a
respective SPARQL query. Then we assume to have RDF
graphs of book prices for Amazon and Barnes & Noble where
we search for these ISBN numbers and respective prices by
further SPARQL queries. The results of the ﬁrst query are

Figure 4: Tim’s Books: Using the Split operator.

split to the inputs of the subsequent queries. Note that we
used - for illustrative purposes - the same sketched query
(constructing price information in RDF per ISBN) for both
Amazon and Barnes & Noble in Fig. 4, but of course these
queries could look completely diﬀerent in practice. In fact,
neither Amazon’s nor and Barnes&Noble’s data is currently
available as one huge RDF graph, but it is in fact a more
likely assumption that such data will only become available
behind SPARQL endpoints in the future. In that case the
separate queries could be pushed separately to those two
endpoints, whereas a joint query on the merged data would
need an intelligence in the query-optimizer to split oﬀ the
distributed query (see e.g. [22]). Depending on how much
information on the endpoints is available to the outside, it
might be impossible for an automatic optimizer to come up
with the right partition of the query. In such scenarios, a hu-
man who has experience with each of the endpoints may still
be able to ﬁnd and design an optimized query plan manually
using the pipes paradigm.

The SELECT operator in conjunction with the XSLT-
operator can easily produce RSS feeds or other XML dialect
representations, thus acting as an adapter for non-Semantic
Web applications.

We restrict ourselves, at the moment, to a standard SPARQL

query engine (which is also what we currently deploy in our
implementation). In previous work though, we have shown
that “pure” SPARQL has several limitations which also af-
fect our envisioned use cases. For instance, SPARQL does
not allow complete mappings even between simple RDF vo-
cabularies [20] such as FOAF and vCard. Conceptually, we
could equally replace the existing SPARQL operators with
more expressive extensions such as SPARQL++ [20] or XS-
PARQL [2], which allow for aggregate functions or string
manipulations when creating new RDF terms in CONSTRUCT
queries, or for creating arbitrary XML directly out of RDF.
Such extensions would also further minimize the necessity of
intermediate applications of custom XSLT operators within
pipes.
The RDFS and OWL Operators
These two operators basically perform materialization of the
RDFS or OWL closure of the input graph by applying RDFS

CONSTRUCTgnRDFN*g1G"CONSTRUCT{  ... }  WHERE { ... }"CONSTRUCT{  ... }FROM GFROM NAMED g1...FROM NAMED gn WHERE { ... }...GQTBL's FOAFSELECT"SELECT ?g   WHERE { <http://www.w3.org/People/Berners-Lee/card#i>                    foaf:knows [rdfs:seeAlso ?G]}"getRDFCONSTRUCT?g"CONSTRUCT { ?s foaf:knows ?o }  FROM ?g  WHERE {  ?s foaf:knows ?o }"QGQRDF outputDBPediaSELECT"SELECT ?isbn ...  WHERE     {   ...swrc:isbn ?isbn ... dc:creator    <http://dblp.l3s.de/d2r/resource/authors/Tim_Berners-Lee>  }"getRDFRDF output+U"CONSTRUCT { ...?price ... }  WHERE { ... ?isbn  ... }"CONSTRUCTAmazongetRDFCONSTRUCTBarnes&NoblegetRDF?isbnQ?isbnQQGGGWWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management584Figure 5: Aggregate Tim’s data by OWL inference.

or partial OWL inference rules. Here, we mean ﬁnite ma-
terialization of entailed triples that can be done by simple
forward chaining rules. Although this method will not infer
e.g. all of the (inﬁnitely many) axiomatic triples, in most
cases a well enough approximation of RDFS which can be
implemented by common rules engines, such as Jena Rules5
or Datalog engines. Treating RDFS inference in pipes as a
separate operator (decoupled from, e.g., SPARQL rules eval-
uation) is a pragmatic approach, as we do not ﬁx at which
part of the chain inference shall be made. Other RDFS or
OWL fragments that can be approximated by materializing
ﬁnitely evaluatable inference rules, like ρdf [18] on the lower
end and the Horst’s pD* rule set[24] on the upper end, could
easily be plugged into our framework by similar operators,
currently we support the latter by employing Jena OWL
reasoner in the OWL operator.

The pipe in Fig. 5 for instance shows how a simple OWL
inference operator can be used instead of SPARQL queries
to achieve the same result as the pipe in Fig. 1. Note that
this formulation reveals another strength of pipes: Unlike
available SPARQL engines with partial OWL or RDFS rea-
soning support, where materialization always applies to all
or none of the graphs mentioned in a query, in a pipe the de-
signer can decide exactly where and on which intermediate
graphs inference is done. We consider this as an advantage
especially when dealing with large graphs, where one can
ﬁrst ﬁlter out relevant only triples in a SPARQL query and
then apply inference only on a reduced dataset in a pipe.

Clearly, the base operators described so far are only a sub-
set of the operators that can be thought useful in mashing
up information on the Semantic Web. Nonetheless, the con-
cept of an operator is general enough for more extensions
in the future. One such useful operator might be a gen-
eral wrapper for WSDL described Web service operations,
which, being deﬁned by inputs and outputs in speciﬁc XML
formats, can be easily mapped to the operator metaphor we
adopt here.
3. SYSTEM DESIGN & IMPLEMENTATION
This section describes and discusses the system design and
its prototypal implementation which is available for experi-
ments at http://pipes.deri.org.
3.1 System design

Obviously, our basic concepts and operators structure de-
scribed above ﬁts well into the popular pipes and ﬁlters
pattern and architecture style [11, 23]. Another reason to
choose the pipes and ﬁlters (or pipeline) structure is that
it has been commonly used and well investigated in parallel
programming environments such as [15, 14, 3], i.e. imple-
mentations of such modular pipes are parallelizable. The
most widely known and used instances of pipes and ﬁlter
architectures we refer to in our design are Unix shell scripts

Figure 6: Semantic Web Pipes Architecture

and programs [4]. Similar to Unix, Semantic Web Pipes
(SWP) support this architectural style by providing the no-
tation in Section 2 for connecting operators as components
(same as Unix processes) and by providing a run time exe-
cution engine for processing pipes.

We implemented the pipes and ﬁlter architecture style in
SWP by adopting the architecture depicted in Fig. 6. As
a typical property of pipes and ﬁlter systems mentioned
in [23], this architecture enables the pipe designer to see
overall input/output behavior of a pipe as a simple compo-
sition of the behaviors of the individual operators (ﬁlters).
Visually, a pipe is a workﬂow of connected operators.
Hence, similar to scientiﬁc workﬂow editors such as Kepler,
Taverna, etc6, we provide a lightweight Web-based work-
ﬂow editor called pipe editor (Fig. 7) for composing pipes
by drag&drop and wiring operators inputs and outputs to-
gether, inspired by the simpliﬁed graphical representation
chosen in Fig. 2. Using the pipe editor we aim at providing
a programmable Web environment suitable for non-expert
programmers. The designer does not need full programming
skills to construct an arbitrary control ﬂow of data he/she
wishes to aggregate. Moreover, the Web-based WYSIWYG
GUI enables designers to inspect the output of each single
operator composing the pipe.

On top of that, an important aspect of the graphical Web
editor is to foster reuse of previous pipe designs. Pipes once
created can be published and then browsed, searched and
re-used by other users. At any time, a new pipe can be
derived as a modiﬁcation of an existing one or an existing
pipe can be included as a functional block in a new one:
since the output of a published pipe is an HTTP-retrievable
RDF model or XML ﬁle, simple pipes can work as sources
for more complex pipes. Furthermore, as pipes can take
parameters as input (parametric pipes are discussed next in
Section 3.2), they can act within other pipes not only as
data sources but as full featured, custom operators.

After visually designed, a pipe is serialized and stored in a
custom XML format and can be later loaded into the editor
or run by the server-side execution engine. SWP uses a sim-
ple database to store these XML representations of pipes in
the pipes repository. When a pipe is invoked, the execution
engine reads its speciﬁcation from the repository and maps
the XML representation to concrete invocations of the op-
erators’ implementations for execution. The engine fetches
data from remote sources into an in-memory triple store,

5

http://jena.sourceforge.net/inference/

6

http://kepler-project.org/,http://taverna.sourceforge.net/

TBL on DBPLTBL's FOAFRDF output+UgetRDFgetRDFTBL on DBPediagetRDF+U<http://www.w3.org/People/Berners-Lee/card#i> owl:sameAs <http://dblp.l3s.de/d2r/resource/authors/Tim_Berners-Lee>. <http://www.w3.org/People/Berners-Lee/card#i> owl:sameAs<http://dbpedia.org/resource/Tim_Berners-Lee>.getRDFOWLWWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management585then executes the deﬁned workﬂow. Each intermediate op-
erator has its own RDF triples or XML infoset buﬀer where
it can load data from input operators, execute SPARQL
queries, materialize implicit triples of RDFS or OWL clo-
sures, ﬁlter and transform triples or XML trees, etc. SWP
naturally supports concurrent execution because each op-
erator can be implemented to run as a separate task and
potentially be executed in a distributed fashion in parallel
with other operators to improve scalability.

While it would be possible to store pipe descriptions them-
selves in RDF, our current ad hoc XML language is more
terse and legible. If an RDF representation of pipes will be
later needed, it can be possibly obtained via GRDDL.

A caching mechanism is deployed to avoid repeated fetch-
ing of remote resource and redundant execution of pipes
when the input data has not changed between invocations
of the pipe. Whenever content is fetched its hash value is
calculated. When no changes in the hash value are detected
the cached result is returned. Because it usually takes sig-
niﬁcant time to download data from remote resources, the
cache engine also stores a parsed copy of the input RDF to
a persistent triple store.

Circular invocations of the same pipe, which could cre-
ate denial of service, can be easily detected within the same
pipe engine, but not when diﬀerent engines are involved.7
Our solution for such cases relies on extra HTTP headers:
whenever a model is fetched coming from an another pipe en-
gine, an HTTP GET is performed putting an extra PipeTTL
(time to live) header. The TTL number is decremented at
each subsequent invocation. A pipe engine refuses to fetch
more sources if the PipeTTL header is ≤ 1.

Finally, thanks to HTTP content negotiation, humans can
use each Semantic Web Pipe directly through a convenient
Web user interface. The pipe output format depends on
the HTTP header sent in the request. For example, RDF-
enabled software can retrieve machine-readable RDF data,
while users are presented a richer graphical user interface to
browse the pipe results.
3.2 Implementation

To describe our prototype implementation, we refer to
the example pipe described in Section 1, that is available
online at http://pipes.deri.org:8080/pipes/pipes/?id=TBLonTheSW.
The pipe can be displayed and edited in the Web pipe editor,
as shown in Fig. 7, wiring operators in the GUI without
having to know any speciﬁc syntax, except for the SPARQL
language that is used to conﬁgure some of the functional
blocks.

The visual editor oﬀers some more features to speed up
the development process and help in keeping code consis-
tent. First, the output of each operator can be connected
only to those operators which allow a compatible data for-
mat as input. This type checking performed at design time
in the editor ensures consistency of the pipeline, avoiding de-
velopers to establish wrong connections between functional
blocks. Second, the data processed can at any step of the
pipeline be inspected by executing sub-parts of the pipe, of-
fering a useful tool for managing and debugging pipes, e.g.
individuate an operator with empty results due to a seman-
tically wrong query.

7Although cycles are disallowed within a pipe speciﬁcation,
such circular invocations could occur, by a pipe recursively
invoking itself as an operator.

Figure 7: Pipe Editor with Tim’s pipe

Pipes are represented and stored as XML documents, us-
ing an XML-based Domain Speciﬁc Language documented
on the project Web site8. The current format is simple and
easily extensible, thus enabling sharing and reuse of pipes
among diﬀerent system installations. While an in-depth dis-
cussion of the XML syntax is beyond the scope of this pa-
per, we give some syntactic “teaser” of the XML generated
to represent the example pipe in the following; here, Q1 and
Q2 stand for the CONSTRUCT queries shown in Section 1.

<merge>

<source><getRDF>

<url>http://www.w3.org/People/Berners-Lee/card#i</url>

</getRDF></source>
<source><construct>

<source><getRDF>

<url>http://dblp.l3s.de/.../Tim_Berners-Lee</url>

</getRDF></source>
<query> <![CDATA[ Q1 ]]> </query>

</construct></source>
<source><construct>

<source><getRDF>

<url>http://dbpedia.org/.../Tim_Berners-Lee</url>

</getRDF></source>
<query> <![CDATA[ Q2 ]]> </query>

</construct></source>

</merge>

In general, it can be useful to add parameters as input to
a pipe, for example to build a pipe that grabs information
about a city speciﬁed by a user or by an application using
the pipe REST API. In our system a pipe can declare mul-
tiple parameters and take their values as input via HTTP
GET requests. The following is typical example code of a
parametric pipe. As soon as this pipe is stored into the pipe
repository, it is reusable as a new operator.

<pipe>
<parameters> <parameter>

<id>name</id><label>City Name</label><default>London</default>
</parameter> </parameters>

<code>

<merge>

<source><getRDF>

<url>..sindice.com...lookup?keyword=${name}..</url>

</getRDF></source>
<source><getRDF>

<url>...geonames.org/search?name=${name}&...</url>

</getRDF></source>

</merge>
</code>

</pipe>

The default output of most pipe operators is RDF. How-
ever, using RDF/XML in light weight, client-side Ajax ap-
plications would be associated with signiﬁcant diﬃculties
and performance problems. Hence, the our implementa-
tion supports RDF/JSON output for client side processing
by Javascript. We also support other RDF formats such
as Turtle/N3, TRIG, TRIX, NTRIPLES which might be
more suitable to some applications than RDF/XML. Apart

8

http://pipes.deri.org/index.php/documentation

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management586from that, SWP allows user to visualize a pipe’s output
in a faceted browser by integrating SIMILE Exhibit 9, a
Javascript library that lets users explore the RDF output
interactively. Furthermore, users are able to specify param-
eters for pipes in the provided GUI and can directly check
the results in this faceted browser. As a complement solu-
tion to other mash-up platforms which are using XML/RSS
as input formats (see Section 5), SWP also provides RSS
output to feed into such engines; we provide some exam-
ple Yahoo’s pipes which consume RSS and JSON data from
SWP’s pipes on the SWP homepage.

Our current execution engine not only supports the base
operators from Figure 2 but also more advanced native op-
erators such as patching RDF graphs [17], smushing URIs
based on owl:sameAs relations.

We invite developers to contribute to the SWP open source
project (http://sourceforge.net/projects/semanticwebpipe). SWP
is not only aiming at Semantic Web developers for con-
tributing new native operator implementations but also shall
provide a framework to integrate oﬀ-the-self Semantic Web
data processing components as visual ready-to-use opera-
tors. For example, we are implementing wrappers to encap-
sulate available ”RDFizing” components and services (http://
simile.mit.edu/wiki/RDFizers, http://www.opencalais.com/, etc) which
are able to extract RDF data from popular formats such as
CSV, BibTEX, plain text,etc. These operators shall enable
users to access vast amount of data sources in their SWP
mash-ups.
4. EVALUATION

In this section we preliminarily evaluate the SWP frame-
work using three methodologies. First, we analyze how the
framework applies to three practical data mash-up scenar-
ios and a combination of metrics is used to compare SWP
with other SW data aggregation solutions. We then proceed
with a qualitative evaluation based on the cognitive dimen-
sions notations (CDs), a methodology that has previously
been used to evaluate related approaches. Finally, we dis-
cuss related aspects which are not covered by the previous
two points.
4.1 Test Case Studies

We start by preliminarily evaluating the complexity of so-
lutions to diﬀerent use case problems when implemented us-
ing Semantic Web Pipes as opposed to directly using SPARQL,
when possible, or implemented as custom Java programs.

For the Java paradigm we measure complexity using a
measure of Line of Code equivalent (LOCE) which is ob-
tained as sum of the number of lines of code in Java plus the
LOCE of complex SPARQL queries. For deriving a LOCE
equivalent for SPARQL queries, we make the assumption
that each triple pattern element or query operator is meant
to address a speciﬁc part of the data transformation, some-
thing that can be considered a lower bound to what a single
line of code in a general purpose language could do. For ex-
ample a query with two triple patterns joined over a common
variable is considered to be 3 LOCE.

For SWP we measure the number of operators and pro-
vide a complexity measure also in LOCE. In this case, we
calculate the pipes LOCE as 1 per block, plus one per each
simple parameter changed in a block from the default, one
per block to block interconnection, one per use of a pipe pa-

9

http://simile.mit.edu/exhibit/

Table 1: Complexities in LOCE. The numbers in
parenthesis represent how many LOCE are con-
sumed by SPARQL queries, if these are used inside
the implementation.

Java implemetation

Case 1
Case 2
Case 3

174 (30)
214 (49)
241 (43)

SPARQL
48 (48)

NA
NA

SWP
44 (30)
63 (49)
72 (43)

rameter inside block plus the LOCE of any SPARQL queries
possibly used inside pipe operators.

Let us note that while lines of code are a controversial
measure of algorithmic complexity, in our context their use
is limited to that of counting the number of atomic actions
that a developer would have to make to reach a desired
objective. With respect to this, these numbers help mak-
ing qualitative, rather than quantitative evaluations of the
framework’s usability.

Case 1: ”Tim Berners Lee on the Semantic Web”’
Plenty of data is available about persons like Tim Berners-
Lee on the Semantic Web: in this case study we want to ob-
tain a single RDF graph aggregated from his FOAF ﬁle with
personal data, DBLP entries with bibliographic information
and DBpedia data. This data cannot simply be merged since
the URIs used in the original sources do not match. This
case can be solved by all three compared approaches.

Case 2: Friend’s publications
Given one’s FOAF ﬁle and a conference name, it will use
data.semanticweb.org to see which friends published a paper
at that conference. The script will needs to resolve people’s
URIs to get the full name.

Case 3: SIOC Aggregation RSS feed
Given a SHA1 a user’s email, the output is a list of messages
that the user left on the internet in sites that expose SIOC [8]
data. To do this the pipe needs to execute a Sindice [19]
query which will return a set of documents likely containing
the description of messages left by the user on possibly mul-
tiple Web sites. It will then aggregate them and compose
an RSS feed.

The results of this implementation tests achieved by an

experienced developer are reported in table 1.

Not surprisingly, Java required the highest number of LOCE

by far when solving each of the scenarios. Each mash-up
operation required several instructions to properly instanti-
ate the frameworks/libraries which perform tasks like Web
data fetching, RDF processing, querying etc. Iterating over
results to perform further processing also required speciﬁc
programming which added to the count. We notice that the
LOCE of Java code are 3 to 4 times as many as the LOCE
of the SPARQL queries which are embedded.

With respect to the pure SPARQL query programming,
we notice that even in the case where this can be done in
a single expression, these queries tend to be unnecessarily
complex. As an example, let us have a look at following
uniﬁed query which “emulates” the pipe of Figure 1.

Squeezing the independent query blocks in one large UNION

query seems unintuitive compared to the clearly separated
blocks in a pipe. Also a query engine needs to decompose
these blocks automatically in its optimizer to achieve rea-
sonable performance. We deem it to be more natural to
write this query as a pipe – apart from saving LOCE – and
pipes may be viewed as visualization of a concrete, manually

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management587optimized “query plan”. Other tasks, especially thus involv-
ing iterations over results, are not expressible in SPARQL
alone. Although one could use a more complex language
such as XSLT, XQuery or XSPARQL [2], all of which are
Turing complete, to script those tasks, we deem again pipes
to be easier to design, maintain and comprehend.

CONSTRUCT{

<http://www.w3.org/People/Berners-Lee/card#i> ?p1 ?o1.
?s2 ?p2 <http://www.w3.org/People/Berners-Lee/card#i>.
<http://www.w3.org/People/Berners-Lee/card#i> ?p3 ?o3.
?s4 ?p4 <http://www.w3.org/People/Berners-Lee/card#i>.
?s5 ?p5 ?p6}

FROM NAMED <http://dblp.l3s.de/d2r/.../Tim_Berners-Lee>
FROM NAMED <http://dbpedia.org/resource/Tim_Berners-Lee>
FROM NAMED <http://www.w3.org/People/Berners-Lee/card>
WHERE {

{GRAPH <http://dblp.l3s.de/d2r/.../Tim_Berners-Lee> {

{<http://dblp.l3s.de/d2r/.../Tim_Berners-Lee> ?p ?o}
UNION
{?s2 ?p2 <http://dblp.l3s.de/d2r/.../Tim_Berners-Lee>}}}

UNION
{GRAPH <http://dbpedia.org/resource/Tim_Berners-Lee> {

{<http://dbpedia.org/resource/Tim_Berners-Lee> ?p3 ?o3}
UNION
?s4 ?p4 <http://dbpedia.org/resource/Tim_Berners-Lee>}}}

UNION
{GRAPH <http://www.w3.org/People/Berners-Lee/card>

{ ?s5 ?p5 ?p6 }}

}
Using the Pipes paradigm, the tasks are completed by
composing the available blocks. We observe that the low
LOCE is an indication of the minimal overhead of the frame-
work with respect to the problems. When SPARQL queries
are used inside pipes (case 2 and 3), these take most of the
complexity.
In the worse case, a little more than half a
LOCE is used for each LOCE of SPARQL used (case 3 with
43 SPARQL LOCE over a total of 72).

Finally, as more pipe operators are implemented, the ten-
dency is that there will be lesser need to use SPARQL queries.
For example we report that an implementation of case 1 can
be done without SPARQL, but an additional URI smushing
operator using only 18 LOCE.

The full details of this evaluation including the Java source
code, the SPARQL queries and the corresponding Semantic
Pipes are available online10.
4.2 Cognitive Dimensions of Notations

In this section we apply the Cognitive Dimension of no-
tations methodology (CDs) [7] to evaluate the usability of
Semantic Web Pipes. CDs, which have been notably used to
evaluate other Web mash-up approaches [13], is a subjective
test composed by a set of terms and concepts which over
time have established themselves as important by program-
mers. These concepts can be listed as follows (from Peyton
Jones et al. [16]):

learnt, how much of the rest can be inferred?

• Abstraction gradient What are the minimum and
maximum levels of abstraction? Can fragments be en-
capsulated?
• Consistency When some of the language has been
• Error-proneness Does the design of the notation in-
• Hidden dependencies Is every dependency overtly
indicated in both directions? Is the indication percep-
tual or only symbolic?
• Premature commitment Do programmers have to
make decisions before they have the information they
need?

duce “careless mistakes”?

10

http://pipes.deri.org/index.php/evaluations

component of a program relates to the whole?

• Progressive evaluation Can a partially-complete pro-
gram be executed to obtain feedback on “how am I
doing”?
• Role expressiveness Can the reader see how each
• Viscosity How much eﬀort is required to perform a
• Visibility and Juxtaposability Is every part of the
code simultaneously visible (assuming a large enough
display), or is it at least possible to compare any two
parts side-by-side at will? If the code is dispersed, is
it at least possible to know in what order to read it?

single change?

With respect to these dimensions, they derive the follow-

ing dimension assessment:

Abstraction Gradient: Semantic Web Pipes provide basic
building blocks which match the basic operations in Seman-
tic data mash-ups, e.g. fetch, add, query, transform. When
a pipe is completed, it can be encapsulated in a new single
drag and drop operator and in general at HTTP level can
be used by any software which fetches data as if it was a
static source.

Consistency: Mash-ups are developed in pipes using the
consistent paradigm of dragging/dropping/connecting/ con-
ﬁguring the blocks. Certain blocks, however, might have
internal conﬁgurations which might include additional lan-
guage complexity, e.g. writing SPARQL queries. Here, at
least, we rely on existing SW language paradigms, which we
try to simplify. Consistency is a key aspects of Semantic
Web Pipes which puts most, possibly all, the instruments
for semantic mash-up into a single, consistent, environment.
Error Proneness: Probably one of the most important ad-
vantages of the pipe paradigm is the ability to provide up-
front step-by-step debugging; block compositions are guided
preventing syntax mistakes. This lowers the entry barriers
for experimentations and increases the framework learning
curve.

Hidden Dependencies and Role Expressiveness: The con-
nection between one block and another is graphically dis-
played such that all dependencies are explicit.

Premature Commitment: Currently, the commitments re-
quired are limited to the use of standards in data representa-
tion and transport. To be processed, data must be available
either in RDF or in one the formats that are supported by
the operators installed in the runtime (e.g. microformats,
XML etc). For transport, pipes requires and provides HTTP
as a way to access/return data. These limitations are due to
the current runtime and can be overcome with by extending
the framework with conversion operators to support more
formats and protocols.

Progressive Evaluation: The visual pipe programming pa-
radigm provides excellent support for progressive evaluation:
each operator in the editor has a partial run button that
shows the debug output of the pipe up to that block.

Role expressiveness, Visibility and Juxtaposability: The
visual pipes programming mode emphasizes these features:
roles are visualized by connections between operators, vis-
ibility is limited by screen size only (and can be aided by
zoom functionality in the editor), code execution ﬂows can
be read both top down and bottom up.

Viscosity: Based on functional programming, pipes repre-
sent almost literally the principle of encapsulation and de-
coupling. A change in the format or location of one of a data

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management588source, for example, should only require a modiﬁcation at
the respective getXML/getRDF operator at the beginning
of the pipe.
4.3 Performance Issues

With respect to performance evaluation, exact performance
results not to be very signiﬁcative as they are mostly depen-
dant on the implementation.

It is however interesting to discuss in general the model
related aspects of the pipe execution performance and which
optimizations can be applied.

By deﬁnition, with respect to a single input/single output
scenario, e.g. a single, cold cached invocation of the execu-
tion framework, nothing can be more eﬃcient than a soft-
ware solution developed speciﬁcally for a speciﬁc problem at
hand. With respect to this, a purely programmed solution
would always be at least as fast as a solution composed by
pipelining predeﬁned operators.

Pipes however makes it easy and transparent to use many
of the optimization opportunities of the pipes and ﬁlters pro-
cessing model.

Firstly, the pipeline model allows execution of multiple
branches that in parallel, e.g. by diﬀerent threads and pro-
cessor cores.

Also the purely functional model of each operator, or block
composed by multiple operators, allow full or partial results
to be cached, reused and shared across multiple execution
threads. This caching happens both internally at execution
level, a process also known as memoization, and externally:
as the invocation happens using the REST model it would be
straightforward to arbitrarily increase the aggregated per-
formance of a Semantic Web Pipelining cluster simply by
using multiple servlets and roundrobin Web request routing
techniques.

We report that the current Semantic Web prototype, while
more concerned with ﬂexibility and functionalities than per-
formance, does currently implement memoization and inter-
nal multithread parallelization, e.g. in the FOR loop oper-
ator where a thread pool is used to execute the inner FOR
pipe branchs.
5. RELATED WORK

The term “pipeline” (or “pipe”) is well known in Computer
Science and denotes a chain of data processors where the
output of each of these processors is sent as an input to the
next one. The most famous implementation of this generic
concept is within the UNIX console [4].

The pipeline paradigm has been successfully applied to
XML transformation workﬂow deﬁnition and execution in
projects like Apache Cocoon11, and several languages, mostly
based on XML themselves, have been used by vendors to
represent such workﬂows12. A standardization eﬀort is cur-
rently ongoing within the W3C XML Processing Model Work-
ing Group, which recently produced the XProc language
speciﬁcation [25]. XProc relies on XSD, XSLT, XPath and
XQuery to perform several operations on XML documents,
e.g. validation, aggregation, transformation and ﬁltering,
and provides classical programming language operators as if-
then and for-each constructs. The XML language proposed
in this paper bases similar ideas but on diﬀerent technologies

11

12

http://cocoon.apache.org/
http://www.orbeon.com/ops/doc/reference-xpl-pipelines,

http://

www.oracle.com/technology/tech/xml/xdkhome.html

(e.g. SPARQL, RDFS, OWL) for querying and modifying
data, as RDF, although can be syntactically represented in
XML, has an inherently diﬀerent data model. While seman-
tic processing is in general more expressive and powerful, we
think that supporting pure XML processing, as we do with
the XSLT-operator, is crucial for applying Semantic Web
Pipes to real world scenarios.

The Yahoo Web Pipes framework, as mentioned earlier,
was inspiring for our work. It provides a rich set of oper-
ators and uses RSS as main data input and exchange for-
mat among operational blocks, but lacks functionality to
address our desired use cases. Other implementations of the
same idea have appeared recently: DAMIA13, from IBM,
is a partially Web based commercial product for composing
mash-ups of Web applications and data. Microsoft Popﬂy14
has similar features and provides a rich user interface plus
a number of predeﬁned wrappers for a variety of diﬀerent
data Web sources as Facebook, Geonames, Google Maps.

A particularly interesting application is Intel MashMaker,
recently released as a beta version and described in [13].
The basic idea is that of providing users with a graphical
interface to create, while they browse the Web, functional
programs where input data is taken from visited Web sites
and the output is a sort of spreadsheet presenting the ag-
gregated data. Although the idea is very interesting, the
impression is that the software, explicitly targeted to non
technical every-day users, is still too complex and requires
considerable eﬀort to create non-trivial mash-ups.

Concerning the Semantic Web world, the need for a cas-
cade of operators to process RDF repositories is also ad-
dressed by the SIMILE Banach project. Banach operates
inside the Sesame DB by leveraging its capabilities to have
a pipelined stack of operators which can both process data
and rewrite queries. Only few basic operators have been de-
veloped so far, but plenty of possibly useful operators are
discussed in the Web site.15 While some of these are al-
ready covered by the C-operator, others are not, but might
be considered in future releases of our pipe engine.

Finally, TopBraid has recently developed a module (called
SPARQLMotion16) of its Composer platform that imple-
ments an idea similar to the one discussed in this paper.
It provides a visual canvas where RDF sources and trans-
formation blocks, mostly based on the SPARQL language,
can be arranged in pipelines to produce the desired output.
If the functionalities provided seems comparable to the ones
implemented in our system, unlike Semantic Web Pipes, the
SPARQLMotion is a commercial, closed-source, desktop ap-
plication based on Eclipse RCP. As conﬁrmed by the pre-
viously mentioned projects, we believe that a Web based
approach better ﬁts the target scenario, being integrated in
an environment (the Web browser) that the user knows well
and uses every day, and fostering sharing and reuse of pipes
and operators. On the other hand, we believe a valuable
contribution of our project resides in its open-source nature,
that makes it possible to extend the framework with new
ad-hoc operators and provides architectural and implemen-
tation guidelines that can be of help for future research and
development in this ﬁeld by the Semantic Web community.

13

14

15

16

http://services.alphaworks.ibm.com/graduated/damia.html

http://www.popfly.com

http://simile.mit.edu/wiki/Banach

http://www.topquadrant.com/sparqlmotion/

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management5896. CONCLUSIONS AND FUTURE WORK
Semantic Web data is a general purpose representation
which needs to be tailored to particular application require-
ments and needs. In the absence of better tools, these trans-
formations are usually implemented in an ad-hoc manner
using general purpose programming languages. The nec-
essary solutions are often tedious to program, cumbersome
to debug and hard to maintain. To remedy this situation,
this paper presented Semantic Web Pipes (SWP), a concep-
tual framework for rapid prototyping of Semantic Web data
mash-ups.

Semantic Web Pipes have the additional beneﬁt that they
can be implicitly used as access points for linked data with-
out additional eﬀorts. A Semantic Web Pipe consumes on-
line data and each published pipe itself becomes a Semantic
Web source that can be used for other mash-ups. Live exe-
cution on invocation guarantees that the data returned by a
pipe reﬂects the latest state of the underlying Web data, a
major advantage of dynamic data transformations and work-
ﬂows as opposed to oﬄine processing of data which would
imply duplication and inconsistency, but at the cost of addi-
tional processing, which, however, is far outweighed by the
beneﬁts of SWP in our opinion.

Pipes can be stored, exported and reused, using a custom
XML format and developers can use SWP mash-up capa-
bilities inside their standalone Web applications by simply
importing the pipe execution engine as a jar archive. SWP
extends SPARQL by using workﬂows. We have illustrated
the advantages of pipes compared with “pure” SPARQL and
discussed several optimizations.

SWP are open source, which allows developers to extend
the framework with new operators as needed. Since its orig-
inal announcement SWP have received numerous contribu-
tions in terms of operators by third party developers. More-
over, on the project website, several mash-up demonstra-
tions created by the user community are available in such
diverse domains as health care & life sciences, social net-
working, online communities and more.
7. ACKNOWLEDGMENTS

The work presented in this paper has been funded by Sci-
ence Foundation Ireland under Grant No. SFI/08/CE/I1380
(Lion-2), by the European FP6 project inContext (IST-034718),
and by the European FP7 project ROMULUS (ICT-217031).
8. REFERENCES
[1] B. Adida, M. Birbeck, S. McCarron, S. Pemberton

(eds.). RDFa in XHTML: Syntax and Processing, Oct.
2008. W3C Rec.

[2] W. Akhtar, J. Kopecky, T. Krennwallner, A. Polleres.

XSPARQL: Traveling between the XML and RDF
worlds – and avoiding the XSLT pilgrimage.
ESWC2008, Tenerife, Spain, June 2008. Springer.

[3] G. R. Andrews. Foundations of Parallel and

Distributed Programming. Addison-Wesley Longman
Publishing,1999.

[4] M. J. Bach. The design of the UNIX operating system.

Prentice-Hall, 1986.

[5] D. Beckett, T. Berners-Lee. Turtle - Terse RDF Triple

Language. W3C Submission, Jan. 2008.

[6] T. Berners-Lee, D. Connolly. Notation3 (N3): A

readable RDF syntax. W3C Submission, Jan. 2008.

[7] A. F. Blackwell, et al. Cognitive dimensions of

notations: Design tools for cognitive technology. 4th
Int’l Conf. on Cognitive Technology (CT’01), London,
UK, 2001. Springer.

[8] U. Boj¯ars, et al. SIOC Core Ontology Speciﬁcation.

W3C Submission, June 2007.

[9] D. Brickley, L. Miller. FOAF Vocabulary Speciﬁcation

0.91, Nov. 2007. http://xmlns.com/foaf/spec/.
[10] J. Carroll, C. Bizer, P. Hayes, P. Stickler Named

graphs. Journal of Web Semantics 3(4):247–267, 2005.

[11] J. O. Coplien and D. C. Schmidt (eds.). Pattern

languages of program design. ACM
Press/Addison-Wesley Publishing, 1995.

[12] D. Connolly (ed.). Gleaning Resource Descriptions
from Dialects of Languages (GRDDL), Sept. 2007.
W3C Rec.

[13] R. Ennals, D. Gay. User-friendly functional

programming for web mashups. 12th ACM SIGPLAN
Int’l Conf. on Functional programming, 2007.

[14] I. Foster. Designing and Building Parallel Programs:

Concepts and Tools for Parallel Software Engineering.
Addison-Wesley Longman Publishing, 1995.

[15] D. C. Hyde, G. M. Schneider, C. H. Nevison.

Laboratories for Parallel Computing. Jones and
Bartlett Publishers, Inc., USA, 1994.

[16] S. P. Jones, A. Blackwell, M. Burnett. A user-centred

approach to functions in excel. ACM SIGPLAN
ICFP’03, New York, NY, USA, 2003. ACM.

[17] C. Morbidoni, A. Polleres, and G. Tummarello. Who

the FOAF knows Alice? RDF Revocation in DBin 2.0.
4th Italian Semantic Web Workshop (SWAP), 2007.
[18] S. Mu˜noz, J. P´erez, C. Gutierrez. Minimal deductive

systems for RDF. ESWC 2007, Innsbruck, Austria,
June 2007.

[19] E. Oren, R. Delbru, M. Catasta, R. Cyganiak,
H. Stenzhorn, G. Tummarello. Sindice.com: A
document-oriented lookup index for open linked data.
International Journal of Metadata, Semantics and
Ontologies 3(1):37–52, 2008.

[20] A. Polleres, F. Scharﬀe, R. Schindlauer. SPARQL++

for mapping between RDF vocabularies. ODBASE
2007, Vilamoura, Algarve, Portugal, Nov. 2007.

[21] E. Prud’hommeaux, A. Seaborne (eds.). SPARQL

Query Language for RDF, Jan. 2008. W3C Rec.

[22] B. Quilitz, U. Leser. Querying distributed RDF data
sources with SPARQL. ESWC 2008, Tenerife, Spain,
June 2008. Springer.

[23] M. Shaw, D. Garlan. Software architecture:

perspectives on an emerging discipline. Prentice-Hall,
Inc., Upper Saddle River, NJ, USA, 1996.

[24] H. J. ter Horst. Completeness, decidability and
complexity of entailment for rdf schema and a
semantic extension involving the owl vocabulary.
Journal of Web Semantics, 3(2):79–115, 2005.

[25] N. Walsh, A. Milowski. Xproc: An xml pipeline

language. W3C Working Draft, April 2008.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management590