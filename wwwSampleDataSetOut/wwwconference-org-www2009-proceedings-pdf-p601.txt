An Analysis of the Performance of Rule Engines

OpenRuleBench:

Senlin Liang

Paul Fodor

Hui Wan

Michael Kifer

Department of Computer Science

State University of New York at Stony Brook

Stony Brook, NY 11794, USA

{sliang,pfodor,hwan,kifer}@cs.stonybrook.edu

ABSTRACT
The Semantic Web initiative has led to an upsurge of the in-
terest in rules as a general and powerful way of processing,
combining, and analyzing semantic information. Since sev-
eral of the technologies underlying rule-based systems are al-
ready quite mature, it is important to understand how such
systems might perform on the Web scale. OpenRuleBench
is a suite of benchmarks for analyzing the performance and
scalability of diﬀerent rule engines. Currently the study
spans ﬁve diﬀerent technologies and eleven systems, but
OpenRuleBench is an open community resource, and contri-
butions from the community are welcome. In this paper, we
describe the tested systems and technologies, the methodol-
ogy used in testing, and analyze the results.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Performance
Evaluation; H.2.4 [Database Management]: Rule-Based
Databases; D.1.6 [Logic Programming]
General Terms
Experimentation, Performance, Languages
Keywords
OpenRuleBench, Semantic Web, Rule Systems, Benchmark

1.

INTRODUCTION

Rule systems have seen an upsurge of interest in the past
few years, as many in the academia and industry started to
regard the Semantic Web as a vast playing ﬁeld for rules. In
response, W3C created the Rule Interchange Format work-
ing group and tasked it with creation of a set of standards
for facilitating the exchange of rules among diﬀerent rule
systems.1 As this group moves towards producing several
candidate recommendations, developers and researchers can
greatly beneﬁt from a better understanding of the state of
the art in the rule systems technology, the overall landscape
of the available systems, their performance, and scalability.
Prolog has long had benchmarks for comparing the per-
formance of the diﬀerent implementations [21, 8]. More re-
cently, test suites have been developed for OWL [14, 19].
In contrast, rule engines do not enjoy the beneﬁt of a care-
fully constructed set of performance benchmarks. Instead,

1

http://www.w3.org/2005/rules/

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

comparative analysis of the diﬀerent engines is largely the
domain of hearsay and hastily constructed tests, often em-
ploying wrong syntax for competing systems [23, 5].

This paper is a step towards ﬁlling in the void. Over the
last eight months we have been studying and experimenting
with eleven diﬀerent systems—academic and commercial—
and created OpenRuleBench, a set of diverse benchmarks for
comparing and analyzing the performance of these systems.
The study touches upon ﬁve diﬀerent technologies: Prolog-
based, deductive databases, production rules, triple engines,
and general knowledge bases. It examines how the diﬀerent
systems scale for a number of common problem sets and, by
interpolation, how they or their successors might perform
on the Web scale.

Unlike Prolog and OWL whose syntax and features have
been standardized, our job was much harder. Not only the
syntax of one system often does not resemble that of an-
other, the features and the capabilities of the diﬀerent sys-
tems are often completely diﬀerent. While the data can
usually be generated programmatically, the rule sets almost
always had to be hand-crafted after careful study of the man-
uals. Given the diﬀerences in the capabilities of the systems,
some benchmarks are applicable only to some systems.

To gain better insight into the workings of the diﬀerent
systems, we conducted extensive correspondence with the
developers and users of each system, as evidenced by the
acknowledgments. In several cases our tests prompted the
developers to ﬁx hitherto unknown bugs, and we accepted
all patches that were given to us. We have made Open-
RuleBench, including the various rule sets, real-world data
and data generators, and scripts, freely available [20] to pro-
vide the community with a reference point for further inves-
tigation into the scalability of the various systems and to
ensure the veriﬁability of the results. To the best of our
knowledge, OpenRuleBench is the largest study of perfor-
mance of rule systems, and this paper reports only a subset
of the results, due to space limitations.

This paper is organized as follows. Section 2 describes
the systems used in this evaluation. Section 3 describes the
methodology used in the evaluation. Section 4 presents the
results of the study, and Section 5 concludes the paper.

2. SYSTEMS TESTED

The rule engines included in this study are quite diﬀerent
in their concepts, implementation, and intended use. To in-
troduce some order into this diversity and to give the reader
a better understanding of the nature of each system, we
classify the systems into ﬁve categories, based on their tech-

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management601nologies, and attempt to mitigate the inevitable ﬂaws of such
a categorization with short descriptions of each system. We
stress, however, that this paper is not a survey of the diﬀer-
ent rule systems and of their underlying technologies. For
the latter, the reader is referred to [6].

We should also remark that the snapshots of the systems
used in this study were produced at diﬀerent times. For
instance, in case of CYC, DLV, Ontobroker, and XSB we
used unreleased betas, which were kindly provided to us.
In all other cases we used the versions that were the latest
releases at the time of testing.

Prolog-based systems.
XSB2 is an open-source Prolog engine with a number of
important enhancements. Being a Prolog system, it is a
top-down inference engine at heart, and it provides a com-
plete environment for building applications. However, it can
also function as a deductive database through the mecha-
nism of tabling or memoing [30], which endows XSB with
certain elements of bottom-up inferencing. Memoing has
the eﬀect that knowledge base programming becomes much
more declarative. In great many cases, where Prolog nor-
mally goes into an inﬁnite loop, XSB will terminate. In par-
ticular, it will always terminate for Datalog, i.e., in case of
function-free Horn rules. Another important eﬀect of tabling
is that it may reduce the computational complexity of query
answering—sometimes from exponential to polynomial [31].
Many Prolog systems get their eﬃciency from the under-
lying highly optimized virtual machine, called WAM (War-
ren’s Abstract Machine) [1]. XSB has pioneered a modiﬁed
version of this abstract machine, SLG-WAM [24], which pro-
vides eﬃcient support for memoing.

One other important feature of XSB is that it supports
a declarative form of negation known as the well-founded
negation [29] (as opposed to the non-logical form of nega-
tion known as negation-as-failure [9], which is standard in
Prolog systems). Through a plug-in, XSB also supports an-
other declarative semantics for negation, the stable-model
semantics [13], but we did not include these tests in Open-
RuleBench, as only one other system (DLV) among the ones
we tested supports that form of negation.
XSB is implemented in the C language.
Yap3 is a highly optimized Prolog system, very similar to
XSB. Like XSB, it provides tabling and is based on SLG-
WAM. Its major limitation is that, unlike XSB, it supports
neither the well-founded nor stable-model negation. Yap has
many innovative features that are not directly related to our
tests, but we will mention one, which does: demand-driven
indexing. This means that the system creates indices on-the-
ﬂy when it judges that a certain index can speed up access
to large amounts of data. We shall see that this makes a big
diﬀerence for some of our tests.

Like XSB, Yap is an open-source system and it is written

in the C language.

Deductive databases.
DLV4 is a bottom-up rule system, which is unique among
the tested systems in that it is based on answer-set pro-
gramming [12]. Roughly this means that rules can have dis-

2

3

4

http://xsb.sourceforge.net/
http://www.dcc.fc.up.pt/~vsc/Yap/
http://www.dbai.tuwien.ac.at/proj/dlv/

junction in the consequent and the semantics for negation is
based on stable models. The well-founded negation is also
supported, however. Unlike XSB and Yap, DLV is a pure
query answering system:
it does not provide operators for
updating the knowledge base, and one cannot build a com-
plete application using DLV alone. Instead, applications are
to be built using a procedural language, such as Java, where
DLV plays the role of a knowledge base component.

DLV employs important optimizations, such as Magic Sets
[4], which endow it with certain beneﬁts of a top-down sys-
tem. It also employs a heuristic that reorders rule premises
in order to achieve better performance. DLV is implemented
in C++; it is free in binary form for non-commercial use, but
is not distributed with an open-source license.

IRIS5 is an open-source, bottom-up rule inference engine.
It is based on the well-founded semantics for negation and
provides the Magic Sets optimization. Like DLV, IRIS can-
not be used as a standalone system, but only as a knowledge
base component of an application written in a host language.
Since IRIS is written in Java, IRIS-based applications are in-
tended to be written in Java as well.

IRIS might be a system to watch, but it has not reached
the level of maturity that would allow a meaningful compar-
ison with other systems. Therefore, we omit further discus-
sion of IRIS in this paper.

Ontobroker6 is a bottom-up rule engine, which is similar
in many respects to DLV and IRIS. Like those systems, it
supports the well-founded negation, Magic Sets, and is in-
tended to be used as a knowledge base component within a
host language. Like IRIS, Ontobroker is written in Java and
is integrated with it. In addition, Ontobroker has a number
of features that diﬀerentiate it from other systems.

First, in addition to the Magic Sets optimization, Onto-
broker supports several others, including ﬁltering [17] and
cost-based optimization akin to database management sys-
tems [15, 11]. To evaluate a query, it ﬁrst builds a cost model
and then decides which optimizations to use, what order to
choose for joining the predicates in rule premises, and which
methods to use for each individual join (e.g., nested loops,
sort-merge). When necessary, Ontobroker builds the ap-
propriate indices to speed up query evaluation, and, when
multiple CPUs are available, it parallelizes the computa-
tion. Finally, unlike the other tested systems, Ontobroker’s
knowledge base language is based on F-logic [16].

Ontobroker has a commercial, non-open-source license,
but binary copies for research purposes can be obtained from
Ontoprise, Inc.

Like XSB and Yap, IRIS and Ontoprise support the use of
function symbols, which makes them Turing-complete. DLV
provides only a limited support for function symbols, but it
is being constantly enhanced [7].

Rule engines for triples.
Jena7 is a Java-based framework for building Semantic Web
applications. Among other things, it includes two rule en-
gines: a bottom-up engine and a top-down one. These infer-
ence modes can be used in tandem, albeit in a limited way.
Since the top-down engine fared much better in our tests,
we do not discuss the bottom-up engine any further.
5

http://sourceforge.net/projects/iris-reasoner
http://www.ontoprise.de/de/en/home/products.html
http://jena.sourceforge.net/

6

7

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management602The top-down engine roughly works the same way as in
XSB and Yap.
In particular, to ensure termination, Jena
employs tabling. However, unlike XSB and Yap, Jena is an
interpreted system; it does not have an underlying virtual
machine optimized speciﬁcally for rule-based query process-
ing.8 Jena is distributed under an open-source license.

SwiftOWLIM and BigOWLIM9 are bottom-up rule
engines for RDF [18] and a certain subset of OWL, called
OWL-Horst [25]. SwiftOWLIM supports a limited reper-
toire of query optimizations. For instance, caching of inter-
mediate results can be selectively turned oﬀ, which is anal-
ogous to selective tabling of predicates in XSB and Yap.

BigOWLIM is a commercial brother of SwiftOWLIM. The
main diﬀerence between the two engines is that the former
has much better performance when it is used to process
disk-bound data. BigOWLIM can also perform database-
style cost-based optimization similarly to Ontobroker (but
Ontobroker is a much more general and sophisticated sys-
tem). We did not test BigOWLIM because OpenRuleBench
focuses on main memory performance.

Apart from the above diﬀerences, Jena and SwiftOWLIM
have much in common. They are both implemented in Java,
are intended to be used as knowledge base components of ap-
plications written in Java, and they are designed to mainly
process RDF triples. Since both systems focus on ternary
relations,10 they can aﬀord full indexing on each of the ar-
guments and thus improve the performance of queries. Jena
and OWLIM also share a number of limitations. Being
triple-based, they support only unary and ternary predi-
cates. Neither system supports the well-founded negation
or even its simpler forms (like predicate-stratiﬁed negation
[2]), and the use of function symbols is not permitted. As a
result, several of our tests (e.g., those that use predicates of
higher arities) could not be used with these systems. These
tests could, in principle, be encoded using triples and used
with Jena and SwiftOWLIM. However, we decided that such
a comparison would be questionable: encoding n-ary rela-
tions using triples saddles triple-based systems with an over-
head that other systems do not have to deal with.

Both Jena and SwiftOWLIM are distributed under open-
source licenses, while BigOWLIM has a commercial license.

Production and reactive rule systems.
Drools11 is a production rule system—a bottom-up engine
where rules have actions in the consequent. When the ac-
tions are insertions of new facts, this is analogous to other
bottom-up systems for Datalog. However, actions can also
be deletions of facts or even calls to arbitrary Java methods.
In the last two cases, production rules have no logical se-
mantics and cannot be directly compared to other systems,
which we tested. Production rules can also have negation
in the rule premises. If used carefully, such negation can be
given logical semantics based on stratiﬁcation—a restricted
form of negation that lies in the intersection of the well-
founded and stable-model semantics.

One other feature that is common to all production rule
systems is that they are all based on (a version of) the Rete

8Jena programs are executed in JVM, but this is a general
purpose virtual machine, which is not optimized for rules.
9
10Or binary relations—depending on the point of view.
11

http://www.ontotext.com/owlim/index.html

http://www.jboss.org/drools/

algorithm [10]—a combination of the semi-naive bottom-up
computation [27] commonly used in Datalog systems with a
certain heuristic for common expression elimination [6].

Jess12 is also a production rule system. Like Drools, it
is mainly a bottom-up system, based on an enhanced ver-
sion of the Rete algorithm, but it also has certain top-down
inference capabilities.

To improve performance, both Drools and Jess use adap-
tive indexing techniques, which proﬁtably select the argu-
ments on which to build indices (full indexing can be too
expensive). For instance, join arguments are always indexed
and so are the intermediate results derived by the rules.

Prova13 Prova is a top-down reactive rule system, which
has special support for event-based processing and actions.
Operationally, Prova works similarly to Prolog, but it is an
interpreted system and is not based on a WAM. However,
unlike XSB and Yap, it does not have support for tabling
and it uses the non-logical version of negation-as-failure.

Among these systems, only Prova supports function sym-
bols. Although Prova appears to be quite diﬀerent from
Drools and Jess, these three systems are classiﬁed together
because their technologies revolve around various kinds of
actions. Incidentally, they are also all Java-based and are in-
tended as rule components inside Java applications.14 Drools
and Prova are distributed under open-source licenses. Jess
is free for research, but is not open-source.

We do not include Prova in subsequent comparisons, as it
appears to have been designed for event processing, not data
processing. As a result, it was not able to execute any of the
data-intensive benchmarks included in OpenRuleBench.

Knowledge-base systems.
CYC15 is an inference engine equiped with a vast body of
domain knowledge and heuristics for solving many complex
problems in classical and common-sense reasoning. Since
CYC is capable of much of the rule-based inferencing of the
kinds we benchmarked, we included it in our study. How-
ever, CYC’s reasoning capabilities go well beyond rules, and
the very optimizations that enable such reasoning put it at
a disadvantage when pure rule-based reasoning is involved.
As a consequence, CYC could not run most of our tests,
and we decided to omit this system from the comparative
analysis reported here.

3. METHODOLOGY

All tests were performed on a dual core 3GHz Dell Opti-
plex 755 with 4 gigabytes of main memory (of which only
3G are visible to applications). The machine was running
Ubuntu 7.10, a distribution of Linux with kernel 2.6.22.

The test suite itself consists of diﬀerent, carefully selected
rule sets, which are intended to test several well-known tasks
that rule systems are known to be good at. Since the tested
systems have very diﬀerent syntax, semantics, and capabil-
ities, the rules had to be manually (and often non-trivially)
adapted for each system. On the other hand, data sets were
12

http://herzberg.ca.sandia.gov/
http://www.prova.ws/

13
14Although Prova stands for Prolog+Java, it does not belong
in the same category as Prolog-based systems, since it is
vastly diﬀerent from them operationally, semantically, and
architecturally.
15

http://www.cyc.com/

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management603created with the help of data generators. Typically Open-
RuleBench uses data sets with sizes 50000, 100000, 250000,
500000, and 1000000 facts, but here we report the results for
only some of these data sets. The suite also includes “real-
world” benchmarks: the wine ontology, Mondial, WordNet,
and the DBLP database. Here the data sets are ﬁxed and
range from a few hundreds in the wine ontology to several
millions in the DBLP database. Finally, the test suite in-
cludes scripts for running the diﬀerent systems in batch,
measuring their run time, collecting the results, and creat-
ing LaTeX tables such as those included in Section 4.

In this paper, we describe and analyze the results obtained
large join tests,
from several diﬀerent categories of tests:
Datalog recursion, and default negation.16 Every one of the
systems has met a foe it could not beat—tests that it failed
because of a timed-out, or due to a crash. Three systems,
Yap, XSB, and Ontobroker, stood out both as the most
feature-full and also as (by far) the best-performing in the
entire bench. However, variation in the results among these
three for diﬀerent tests was signiﬁcant.

A few words about our methodology. As mentioned ear-
lier, the tested systems vary greatly in their capabilities, de-
grees of sophistication, and in what they do with their input
rule sets before cranking an inference engine. As our goal
was to compare technologies, not to run a beauty pageant
among systems, we tried to look into the potential of the
tested systems and often applied manual optimizations when
algorithms for such optimizations were known. Here are
some of the considerations that were factored into the test.

Indexing. A well-chosen set of indices may mean the dif-
ference between many hours and a few seconds.

Some systems, e.g., DLV, Ontobroker, Yap, and Jess ana-
lyze the rule sets and data and create useful indices on-the-
ﬂy. Others, like Jena and OWLIM, index on every argument.
Yet others, like XSB, oﬀer a repertoire of indexing tech-
niques, which must be speciﬁed manually. However, XSB’s
default indexing is rather simple-minded, and a naive user
might see wide variations in performance depending on the
characteristics of the data. Yet other systems always index
on the ﬁrst argument and do not provide any other options.
In some cases we had to signiﬁcantly rewrite the tests when
automatic indexing failed to do adequate job (for instance,
the LUBM-derived tests in case of Jess).

To reduce variability, we decided to use the most advanta-
geous indices for each system. For instance, when we knew
that XSB would perform best with certain indices present
then we would declare such indices manually. For Yap and
some others, on the other hand, we had to do nothing.

Optimization knobs. Some systems provide various op-
tions for optimization. For instance, in Ontobroker one can
enable or disable the parallel engine or choose specialized
inference engines. In XSB and Yap, predicates can be de-
clared as tabled in order to improve performance or ensure
termination, and there are diﬀerent tabling modes that one
can choose (XSB has four, for example). In SwiftOWLIM,
tabling can be turned oﬀ in some cases, although this did
not matter in our tests. Another example: IRIS, Ontobro-
ker, and DLV take advantage of the Magic Sets optimization.
In all cases, we tried to use the optimizations that we were
aware of, if we found them beneﬁcial. In several cases the
suggestions came from the system maintainers themselves.

16The full report [20] includes additional tests.

Query-based systems versus production rules. Most
of the tested systems are capable of taking queries and avoid-
ing the computation of intermediate results that are irrele-
vant to the given query. However, production rule systems,
such as Jess and Drools, are designed to compute all possi-
ble derivations and do not take queries into account. In such
cases, we changed the rule sets to remove the irrelevant rules
and thus to compensate for the lack of optimization.
Cost-based optimizers versus none. We have already
mentioned that Ontobroker and BigOWLIM can perform
cost-based optimization which, in particular, may reorder
predicates in the rule premises. Sometimes such reorder-
ing may drastically reduce the computation time and, in
some cases, even reduce the computational complexity of
the query. As with all the previous performance factors, we
tried to ﬁnd the most beneﬁcial order of the predicates for
each system and rewrite the rules manually.
Loading versus inference. Our main interest was in mea-
suring the time to do inference rather than loading of the
data sets. Most systems had clearly separated loading and
inference phases, so measuring the speed of inference alone
was easy to perform. However, some systems (for exam-
ple, DLV, IRIS, and OWLIM) combine loading and much of
the inference in one step. This made comparison with other
systems harder, since it forced us to run additional tests
to estimate the time of loading alone. This made our time
measurements for smaller tests less precise than we liked.
Choice of tests. Our tests are representative of database
and knowledge representation problems. They do not in-
clude actions, such as those found in production rule systems
and Prolog. Although half of the tested systems supports ac-
tions, they use diﬀerent paradigms. There is no obvious way
to translate between these paradigms in a way that would
be meaningful for performance tests.

We will now describe the tests in each category separately.

Large join tests.
These include database joins (Join1, Join2), LUBM-derived
tests, the Mondial, and the DBLP tests.

Join1 has a form of a non-recursive tree of binary joins,

which is expressed using these inference rules:17

a(X,Y) :- b1(X,Z), b2(Z,Y).
b1(X,Y) :- c1(X,Z), c2(Z,Y).
b2(X,Y) :- c3(X,Z), c4(Z,Y).
c1(X,Y) :- d1(X,Z), d2(Z,Y).

The base relations, c2, c3, c4, d1, and d2, were randomly
generated. We used two data sets: one with 50000 facts and
the other with 250000 facts. The queries are based on the
derived predicates, a, b1, b2, with diﬀerent bindings for the
variables: free-free, free-bound, and bound-free.

The test 5*Join1 was created by making ﬁve copies of
the above rule set with the predicate a(X,Y) renamed to a1,
..., a5, and then unioning the results using the rule

a(X,Y) :- a1(X,Y); a2(X,Y); a3(X,Y); a4(X,Y); a5(X,Y).

Join2 is another handcrafted pattern of joins, borrowed
from [5]. It produces a large intermediate result, but only a
small set of answers.
17All our examples use Prolog syntax.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management604ra(A,B,C,D,E) :- p(A),p(B),p(C),p(D),p(E).
rb(A,B,C,D,E) :- p(A),p(B),p(C),p(D),p(E).
r(A,B,C,D,E) :- ra(A,B,C,D,E),rb(A,B,C,D,E).
q(A) :- r(A,_,_,_,_).
q(B) :- r(_,B,_,_,_).
q(C) :- r(_,_,C,_,_).
q(D) :- r(_,_,_,D,_).
q(E) :- r(_,_,_,_,E).

DBLP is a database of publications in the ﬁelds of databases

and logic programming derived from the well-known Web-
based bibliography under the same name.20 The test con-
tains a single relation that has close to 2500000 facts about
more than 200000 publications. The only query in this test
is a 4-way join of parts of the same database relation:

q(Id,T,A,Y,M) :- att(Id,title,T), att(Id,year,Y),

att(Id,author,A), att(Id,month,M).

The content of the base relation is p(a0), . . . , p(a18), and
the query seeks all the facts for the predicate q.

LUBM-derived tests include three rule sets (Query1,
Query2, and Query9) that are adapted from the original
Lehigh benchmark, LUBM [14]. It is a university database
where the number of universities, departments, and students
can vary. We tested two data sets: one for 10 universities,
which has over 1000000 tuples and one for 50 universities,
which includes over 6000000 tuples.

Query1 retrieves students who take a particular gradu-
ate course. It is a join of two relations on an attribute that
has high selectivity (i.e., each tuple in one relation joins with
only a small number of tuples in the other relation). As a re-
sult, although the input database is large, the query answer
is very small. Query2 seeks those triples <s, d, u> where s
is a graduate student in department d, s has an undergrad-
uate degree from university u, and d is a department in u.
This query joins three unary and three binary relations, but,
again, the selectivity of the join attributes is high, so the ﬁ-
nal answer is small (around 100 tuples for the 50-university
data set). Query9 seeks those triples of the form <s, f, c>
where f is a faculty advisor of student s, and s takes course
c from f . This query joins three binary and three unary
relations, but the join attributes have lower selectivity than
in Queries 1 and 2. So, the query answer is relatively large
(more than 10000 for the 50-university data set).

Due to space limitations, we do not show the helper pred-
icates, which were introduced by the translation from the
corresponding LUBM tests. The interested reader can ﬁnd
the details in [20].

query1(X) :- takesCourse(X,graduateCourse0),

graduateStudent(X).

query2(X,Y,Z) :- graduateStudent(X), memberOf(X,Z),

undergraduateDegreeFrom(X,Y),
university(Y), department(Z),
subOrganizationOf_0(Z,Y).

query9(X,Y,Z) :- advisor(X,Y), teacherOf(Y,Z),
takesCourse(X,Z), student(X),
faculty(Y), course(Z).

Mondial18 is a geographical database derived from the
CIA Factbook.19 The base relations contain around 60000
facts, and each fact provides information about cities, provin-
ces, and countries around the world. The only query in this
test seeks certain statistical information about provinces in
China. The query is interesting because it produces an in-
termediate result (1676942 facts) that is orders of magnitude
larger than the ﬁnal results (888 facts). So, the query oﬀers
opportunities for optimization.

18

http://www.dbis.informatik.uni-goettingen.de/

Mondial/
19

https://www.cia.gov/library/publications/

Datalog recursion.
Recursion is perhaps the most important feature that distin-
guishes rule-based systems from traditional database man-
agement systems (SQL:1999 extensions notwithstanding).
Development of eﬃcient techniques for computing recursive
queries was a large subarea within the database and logic
programming research in the 80’s and 90’s. The recursive
tests in OpenRuleBench are intended to evaluate the per-
formance of such queries.

The tests include the classical transitive closure, the well-
known same-generation siblings problem, applications from
natural language processing based on WordNet,21 and a rule
representation of the well-known wine ontology.22

The transitive closure of a binary relation, par, is the
smallest transitive relation that contains par. It is written
in Prolog notation as follows:

tc(X,Y) :- par(X,Y).
tc(X,Y) :- par(X,Z), tc(Z,Y).

Traditional prologs have trouble computing the derived re-
lation tc, if the predicates in the second rule switch places.
However, even with the favorable order of the predicates
Prolog might go into an inﬁnite loop if the relation par rep-
resents a cyclic graph.

The tuples for the base relation par were randomly gen-
erated, and two types of data sets were generated: one with
cycles and the other without. In each case we ran tests for
two data sizes 50000 and 500000.

The same-generation problem seeks to ﬁnd all siblings
in the same generation, i.e., siblings that are equally remoted
from a common ancestor. Of course, the relation par can
represent an arbitrary graph, so the human analogy should
not be taken too far.

sg(X,Y) :- sib(X,Y).
sg(X,Y) :- par(X,Z), sg(Z,Z1), par(Y,Z1).

The base relations of par and sib were randomly generated.
As before, we considered both cyclic and acyclic data, and
for each type two data sizes were used: 6000 and 24000.

The WordNet tests include common queries from natu-
ral language processing based on WordNet, a semantic lex-
icon for the English language. These queries seek to ﬁnd
all hypernyms (words more general than a given word), hy-
ponyms (words more speciﬁc than a given word), meronyms
(words related by the part-of-a-whole semantic relation),
holonyms (words related by the composed-of relation), tro-
ponyms, same-synset, glosses, antonyms, and adjective-clusters.
The base facts were extracted from WordNet Version 3.0 and
20

http://www.informatik.uni-trier.de/~ley/db/
http://wordnet.princeton.edu
http://www.w3.org/TR/2004/REC-owl-guide-20040210/

21

22

the-world-factbook/

#WinePortal

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management605converted to the speciﬁc syntaxes of our test systems. The
database consists of about 115000 synsets (i.e., groups of se-
mantically equivalent words) containing over 150000 words
in total.
In most tests, the number of solutions is over
400000, and in some cases exceed 2000000. Due to space
limitation, we show only the hypernyms test here—see [20]
for a complete description of the tests. In these tests, choos-
ing a good index is of paramount importance, and we tried
to adjust the rules and set up indices to beneﬁt each indi-
vidual system.

hypernyms(W1,W2) :- s(S1,_,W1,_,_,_),

hypernymSynsets(S1,S2),
s(S2,_,W2,_,_,_).

hypernymSynsets(S1,S2) :- hypernym(S1,S2).
hypernymSynsets(S1,S2) :- hypernym(S1,S3),

hypernymSynsets(S3,S2).

The wine ontology is a rule-based representation of the
well-known OWL wine ontology.23 It has 815 rules and 654
facts. Since a ruleset this large cannot be included here, we
refer the reader to the sources on the OpenRuleBench Web
site [20]. Perhaps the most important feature of this test
is that many predicates are recursively dependent on each
other through chains of rules. This creates large cliques of
predicates with respect to the depends-on relationship. Such
cliques present special problems for top-down engines, and
we had to use special optimizations in order for the best of
our systems, Yap and XSB, to pass this test.

Default negation.
These tests include rules with default negation, not, in the
body. Three diﬀerent patterns of recursion through nega-
tion are considered: predicate-stratiﬁed negation [2], locally
stratiﬁed [22], and negation under the well-founded seman-
tics [29] for rule sets that are not even locally stratiﬁed.
The predicate-stratiﬁed negation is represented by a mod-
iﬁed same generation problem. Locally stratiﬁed negation
is represented by the well-known win-not-win example [29]
applied to acyclic data. The non-stratiﬁed case is obtained
from the same rule set when the data has cycles. In addition,
we include a rule set, referred to as MS, obtained from strat-
iﬁed rules by applying the Magic Sets transformation. The
resulting rule set is not predicate-stratiﬁed and most likely
not locally stratiﬁed either (this depends on the randomly
generated data).

The modiﬁed same generation test is as follows:

nonsg(X,Y) :- tc(X,Y).
nonsg(X,Y) :- tc(Y,X).
sg2(X,Y) :- sg(X,Y), not nonsg(X,Y).

The base relations of par and sib were randomly generated,
and there are cycles in the data. We ran the tests for two
data sizes: 6000 and 24000.

The win-not-win test consists of a single rule, where

move is the base relation:

win(X) :- move(X,Y), not win(Y).

When the base facts form a cycle: {move(1, 2), . . ., move(i,
i + 1), . . ., move(n − 1, n), move(n, 1)}, the rule set (together
with the data) is not locally stratiﬁed. When the data is
tree-structured: {move(i, 2 ∗ i), move(i, 2 ∗ i + 1) | 1 ≤ i ≤

23

http://www.w3.org/TR/2004/REC-owl-guide-20040210/

#WinePortal

n}, the rule set is locally stratiﬁed. We used three cyclic and
three tree-structured data sets with n = 100000, 250000, and
1000000.

The MS test was borrowed from [3]; it is a typical example
of when the magic transformation turns stratiﬁed negation
into a non-stratiﬁed one.

fb(X) :- magicfb(X),d(X),not ab(X),h(X,Y),ab(Y).
ab(X) :- magicab(X),g(X).
ab(X) :- magicab(X), b(X,Y),ab(Y).
magicab(Y) :- magicab(X),b(X,Y).
magicab(Y) :- magicfb(X),d(X),not ab(X),h(X,Y).
magicab(X) :- magicfb(X),d(X).

We used two data sets: one with 24000 facts for each of the
base relations b and h, and the other with 504000 facts.
Miscellaneous tests.
OpenRuleBench includes two other suits of tests: dynamic
indexing tests and database interface tests.

Dynamic indexing tests measure the time it takes to in-
sert and delete facts one-by-one in main memory. Some in-
dexing methods, such as the hash-based methods, are very
sensitive to skewed data patterns which cause excessive col-
lisions. Prolog semantics may also aﬀect the performance
because it is sensitive to the order of in which facts have
been inserted. Our tests show that XSB is particularly sen-
sitive to data skews and the order of insertions and deletions.
Details can be found in [20].

We also developed tests for evaluating interfaces to data-
base management systems, such as MySQL. However, in the
end we abandoned this eﬀort for two reasons. First, our
aim was to evaluate the diﬀerent technologies for perform-
ing rule-based inference per se. Database interfaces are im-
portant, but they are not inherent to the rules technology.
Second, the problem of pushing data to and from disk is not
unique to rule systems. The only issue that is truly unique
is the need to store and retrieve tree-structured data—facts
that include function symbols.24 However, only one system,
XSB, has support for storing this kind of facts, and even
that support is implemented in a naive way. We also note
that systems such as BigOWLIM and DLVDB [26] (which
we did not test) can deal with applications that do not ﬁt
in main memory.
4. RESULTS

Three systems, Yap, XSB, and Ontobroker (sometimes re-
ferred to as OB), were clear leaders in most of the tests with
one system holding an edge in some tests and another in
others. DLV was also a good performer overall, and in some
cases it did on a par or even better than the above systems.
Since a short summary cannot adequately describe our re-
sults, we will present them in a tabular form with a brief
In these tables, size means
analysis following each table.
the total size of all base relations; error means that the
system gave an error during the evaluation; timeout indi-
cates that evaluation did not ﬁnish within a set time limit
of 30 minutes. All times are given in seconds and exclude
the loading time. We note that Ontobroker and XSB are
signiﬁcantly slower in loading large data sets than Yap and
DLV. So, counting the loading time, XSB and Ontobroker
fall behind DLV and Yap on DBLP and LUBM tests.
24Some might argue that XML database do something similar
already.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management606query
size

ontobroker

xsb
yap
dlv
jena
owlim
drools

jess

a(X,Y)

b1(X,Y)

b2(X,Y)

50000
4.089
12.774
10.534
85.459
149.918
104.166
error
310.000

250000
28.385
timeout
timeout
838.781
timeout
1484.126
error
timeout

50000
0.213
0.122
0.109
7.177
2.910
1.999
27.414
12.000

250000
4.806
14.920
12.123
60.239
174.135
83.820
error
317.000

50000
0.019
0.013
0.013
0.820
0.799
0.038
2.111
1.000

250000
0.168
0.269
0.269
9.392
6.269
1.192
94.474
26.000

Table 1: Join1, no query bindings

Large join tests.
The execution times are shown in Tables 1, 2, and 3 (Join1),
Table 4 (5*Join1), Table 5 (Join2), Table 6 (LUBM-derived
queries), Table 7 (Mondial), and Table 8 (DBLP). Once
again, XSB, Yap, and Ontobroker contested the crown, out-
performing each other on some tests and falling behind on
others. DLV was sometimes close and sometimes even did
better than XSB or Yap. Other systems were usually one or
two orders of magnitude slower or could not run the tests at
all. Note that Join2, Mondial, and the DBLP tests are not
applicable to Jena and OWLIM because of the restrictions
on the predicate arity in those systems.

Interestingly, both XSB and Yap time out for query a(X,Y)
on a large dataset (50000 tuples in each base relation and
250000 tuples total—see the description of this test in Sec-
tion 3) in tests shown in Tables 1 and 4. The likely expla-
nation is that this is due to the fact that both XSB and Yap
are limited to only one kind of a join method, indexed nested
loops [15], while Ontobroker automatically chooses the most
appropriate method from a small repertoire of algorithms.
Here it uses the sort-merge join [15], which scales better than
nested loops. Interestingly, in Table 1 XSB and Yap do a lit-
tle better than Ontobroker for queries b1(X,Y) and b2(X,Y)
when smaller data sets are used, because of the initial over-
head (of sorting the relations) associated with sort-merge
joins. For larger datasets, this overhead gets amortized and
Ontobroker comes out on top. DLV scales better for a(X,Y)
than XSB and Yap, but it trails far behind Ontobroker.

For the tests in Tables 2 and 3 where one of the query
arguments is bound to a constant (=1), Jess, Drools, and
OWLIM are omitted, as they cannot take advantage of query
bindings and so their times are comparable to Table 1. For
these tests, the comparative results are essentially the same
as before, with Ontobroker doing the best overall and DLV
scaling better than XSB and Yap for queries a(1,Y) and
a(X,1). For the bound-free case in Table 2, XSB and Yap
do surprisingly well for queries b1(1,Y) and b2(1,Y). This
might be due to the overhead of the Magic Sets method,
which Ontobroker uses in order to take advantage of the
bindings in the query. For larger datasets this overhead gets
amortized.

Most interesting, however, is the observation that, for On-
tobroker, the times for the query a(1,Y) in Table 2 and for
the queries a(X,1) and b1(X,1) in Table 3 do not change
substantially when the dataset size increases from 50000 to
250000 tuples. And the time for b2(1,Y) and b2(X,1) even
goes down signiﬁcantly. All other systems report drastic
time increases or even time out! This unexpected behavior
is due to the fact that Ontobroker performs rule analysis and
determines that rule unfolding (compile-time substitution of
rule heads by rule bodies) followed by cost-based premise re-
ordering may be called for. The other systems, if they do
premise reordering at all, do not apparently precede this
step with unfolding, so reordering does not yield substantial
beneﬁts for them. The decrease in the execution time for b2

query
size

ontobroker

xsb
yap
dlv
jena

a(1,Y)

b1(1,Y)

b2(1,Y)

50000
0.035
0.013
0.021
0.287
0.381

250000
0.038
35.990
30.233
6.014
342.059

50000
0.013
0.000
0.007
0.014
0.026

250000
0.051
0.016
0.050
0.112
0.350

50000
0.070
0.000
0.004
0.008
0.014

250000
0.012
0.001
0.025
0.066
0.035

Table 2: Join1 with 1st argument bound.

query
size

ontobroker

xsb
yap
dlv
jena

a(X,1)

b1(X,1)

b2(X,1)

50000
0.030
5.033
0.814
0.665
74.741

250000
0.036
timeout
539.710
54.653
timeout

50000
0.011
0.048
0.014
0.023
1.179

250000
0.010
5.105
0.246
0.338
71.482

50000
0.028
0.004
0.004
0.008
0.333

250000
0.009
0.095
0.025
0.065
1.741

Table 3: Join1 with 2nd argument bound.

is due to a data skew (but Ontobroker is the only system
that takes advantage of this).

The 5*Join1 test would have been unremarkable if not
for another surprise from Ontobroker. This test directs the
systems to compute the same query, Join1, ﬁve times.
It
does so by renaming the derived predicates and fooling the
systems into thinking that these are ﬁve diﬀerent queries.
If we now compare Tables 1 and 4, we see that all systems
increased their running times by the factor of about ﬁve, but
Ontobroker’s times stay roughly the same! It turns out that,
after unfolding (the same optimization that gave Ontobroker
an edge in the Join1 tests) the system determines that all
ﬁve queries have common subexpressions (4-way joins), and
factors them out. So, for Ontobroker, Join1 and 5*Join1 are
essentially the same query.

For Join2, XSB and Yap take about the same time, while
Ontobroker comes in third. A possible explanation for On-
tobroker’s performance in this test (four times slower than
XSB and Yap) is that Join2 starts by producing large rela-
tions using the Cartesian product operation, and database
technology, on which many of Ontobroker’s optimizations
are based, does not have a good strategy for Cartesian prod-
ucts. In this situation, the well-optimized tabled SLG-WAM
platform gives XSB and Yap an advantage.

In the LUBM tests, Drools and Jess run out of memory
while loading the larger dataset (the 50univs data set has
over 6000000 tuples, claiming more than 600MB of memory).
XSB is the overall winner with Ontobroker coming close
second. Yap is slow on Query 1, but it bests Ontobroker
and sometimes XSB on other queries. Jena, and OWLIM
do very well on queries 1 and 9, but poorly on Query2. DLV,
on the other hand, does well overall: slightly behind OWLIM
on queries 1 and 9, but far ahead of both Jena and OWLIM
on Query 2.

The better performance of XSB may be explained by the
fact that we manually requested indexing on all arguments
(recall that XSB does not create suitable indices without
being asked), while Yap, Ontobroker, and others use heuris-

query
size

ontobroker

xsb
yap
dlv
jena
owlim
drools

jess

a(X,Y)

b1(X,Y)

b2(X,Y)

50000
3.647
63.841
52.202
415.237
709.440
671.996
error
error

250000
28.497
timeout
timeout
timeout
timeout
timeout
error
error

50000
0.163
0.634
0.522
28.276
9.794
15.803
error
65.000

250000
4.601
73.731
61.650
297.341
831.204
590.309
error
error

50000
0.029
0.060
0.054
2.582
1.212
0.820
16.059
6.000

250000
0.189
1.378
1.246
35.595
22.403
38.215
error
error

Table 4: 5*Join1, no query bindings

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management607system

time

yap
2.087

xsb
2.092

OB

dlv

jess

11.935

44.692

timeout

drools
error

system

time

OB
1.602

xsb
1.752

yap
2.447

drools
0.186

dlv

2.201

jess
error

Table 5: Times for Join2.

Table 8: Times for DBLP

query
size

ontobroker

xsb
yap
dlv
jena
owlim
drools

jess

Query1

Query2

Query9

10univs
0.013
0.000
0.106
0.251
0.017
0.110
0.001
0.000

50univs
0.025
0.001
0.607
3.188
0.018
1.050
error
error

10univs
0.596
0.142
0.139
1.041
5.282
4.420
13.537
1.000

50univs
2.704
0.968
0.938
7.388
63.782
195.480
error
error

10univs
1.243
0.162
0.417
1.260
2.390
1.000
22.483
2.000

50univs
8.799
1.124
3.300
7.068
14.889
3.130
error
error

Table 6: Times for LUBM tests

size

cyclic data
ontobroker

xsb
yap
dlv

drools

jess
jena
owlim

50000
no
6.129
2.725
2.066
19.655
120.416
55.000
16.332
7.656

50000
yes
19.145
7.081
13.026
73.837
error
184.000
47.674
38.457

500000
no
49.722
35.036
33.128
148.740
error
411.000
108.940
75.847

500000
yes
182.633
88.028
82.900
900.773
error
timeout
451.590
395.226

tics to determine which indices to create. We conjecture that
these heuristics might have overlooked better alternatives.

The Mondial test shows that XSB outperforms the other
systems by an order of magnitude. Again, this is likely the
eﬀect of a better manual choice of indexing versus heuristic
indexing of Yap, Ontobroker, and others.

The DBLP test is a 4-way self-join of a very large rela-
tion (see Section 3), which yields a small answer set. Good
indexing is a must in order to pass this test, and the usual
suspects, Ontobroker, XSB, DLV, and Yap, do well. But
the true surprise is that the best time (by an order of mag-
nitude!) belongs to none other than Drools—a system that
was lagging in other tests. On close examination, it ap-
pears that here Drools uses the same query plan as a well-
optimized database system would: ﬁrst it selects the relation
att on the second argument (thus reducing the size of the
relation ten-fold), then builds index on the join attribute,
and then computes the join. Other systems appear to be
computing the 4-way join of att with itself directly, albeit
using diﬀerent join algorithms and diﬀerent indices.

At this point it is instructive to revisit the issue of opti-
mization in order to better appreciate its impact on perfor-
mance. The Mondial test and XSB oﬀer a striking example.
In this test, XSB tables some predicates in order to avoid re-
computation and indices are used for faster access to data.
With these optimizations, Table 7 shows that XSB ﬁnishes
in just 0.004 seconds. However, if tabling is turned oﬀ, the
time jumps to 1.713 seconds, a 400-fold increase! With no
indexing, the time jumps further to 129.89 seconds—yet an-
other 76-fold increase. Altogether, these optimizations yield
a better than 30000 times speed-up!

Datalog recursion.
We examined the runtime for the query tc (transitive clo-
sure) and sg (same generation) with diﬀerent variable bind-
ings:
free-free, bound-free and free-bound. Again, Yap,
XSB, and Ontobroker did the best job, followed by OWLIM,
Jena, DLV, Jess, and Drools with performance lags ranging
from 30% to an order of magnitude.25 In addition, we looked
at the cases when the base relation par is an acyclic graph

25OWLIM, Jess, and Drools are omitted for queries with ar-
gument bindings, as these systems cannot beneﬁt from such
information.

system

time

xsb
0.004

yap
0.037

OB
0.042

jess
2.000

dlv

1.045

drools
3.476

Table 9: Transitive closure, no query bindings.

as well as when it is cyclic. These two sets of tests reveal
one interesting phenomenon, which is yet to be explained:
on the tc tests, XSB is typically 2-3 times faster and scales
better than Ontobroker, while on the sg tests Ontobroker is
both faster and scales better.

Yap is the best overall performer in the WordNet tests, fol-
lowed by XSB and Ontobroker. Ontobroker does marginally
better than XSB on the meronyms, holonyms, and troponyms
tests, while loosing by a wider margin on other tests.

For the Wine ontology, XSB is a notch ahead of Onto-
broker. Yap takes twice the time, but is far ahead of the
rest. However, both XSB and Yap run out of memory under
their default settings. Special optimizations (subsumptive
and batched tabling) and some reordering of rule premises
are required in order for these systems to pass the test. On-
tobroker’s all-around good performance (with no need for
manual intervention) is likely due to its premise-reordering
heuristic and its parallel engine, which yields a 20% speedup.

Default negation.
One can see from Table 17 that, for stratiﬁed rule sets, Yap
works best, followed by XSB then Ontobroker and DLV in
diﬀerent orders, depending on the test. Yap slips behind On-
tobroker on the modiﬁed same generation test with a large
dataset. The table indicates that all systems have scala-
bility problems in one case or another: Ontobroker might
time-out, XSB and DLV might take comparatively too long
to execute the second query, while Yap takes more than 10
times longer when going from 6000 to 24000 facts in case
of the modiﬁed same generation test (the Ontobroker’s test
shows that such a large jump is not inevitable).

Drools and Jess do well for the stratiﬁed win-not-win test
on smaller datasets, but Drools has a problem with larger
sets. Both fail for the modiﬁed same generation test.

Yap does not support rule sets that are not stratiﬁed lo-
cally, so it is not included in Table 18. The test shows that
XSB is signiﬁcantly faster and scales better than the other

size

cyclic data
ontobroker

xsb
yap
dlv
jena

50000
no
5.708
1.437
0.722
16.188
3.884

50000
yes
21.787
5.563
5.666
63.089
11.593

500000
no
47.804
18.532
9.026
107.629
35.383

500000
yes
180.758
55.447
31.454
111.990
120.466

Table 7: Times for Mondial

Table 10: Transitive closure, ﬁrst argument bound.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management608size

cyclic data
ontobroker

xsb
yap
dlv
jena

50000
no
0.015
0.016
0.036
0.025
0.792

50000
yes
0.054
0.015
0.048
0.068
0.884

500000
no
0.042
0.180
0.575
0.489
5.314

500000
yes
0.271
0.157
0.689
0.830
5.032

Table 11: Transitive closure, 2nd argument bound.

size

cyclic data
ontobroker

xsb
yap
dlv

drools

jess
jena
owlim

6000
no
1.402
2.359
1.875
20.274
104.884
64.000
21.007
8.666

6000
yes
1.926
3.408
3.148
31.346
error
error
37.692
13.314

24000
no
5.424
42.824
43.510
365.136
error
1517.000
387.268
174.968

24000
yes
5.309
44.487
43.452
438.008
error
error
415.376
195.825

Table 12: Same generation, no query bindings.

size

cyclic data
ontobroker

xsb
yap
dlv
jena

6000
no
0.857
1.338
0.619
11.976
6.827

6000
yes
1.835
3.245
1.398
29.887
15.959

24000
no
4.421
35.823
18.238
300.403
155.512

24000
yes
4.726
42.297
18.866
419.099
191.842

Table 13: Same generation, 1st argument bound.

size

cyclic data
ontobroker

xsb
yap
dlv
jena

6000
no
0.849
2.859
0.845
14.472
15.784

6000
yes
1.873
3.954
1.539
38.913
21.275

24000
no
4.874
55.855
19.178
373.920
251.355

24000
yes
5.339
51.406
19.450
506.422
254.322

Table 14: Same generation, 2nd argument bound.

Test
OB
yap
xsb
dlv

drools

jess
jena
owlim

hypernyms
4.968
0.244
1.524
33.65
error
error
7.613
5.713

hyponyms meronyms
0.512
0.216
0.688
7.460
error
4.060
2.023
1.158

4.862
0.236
1.868
35.920
error
error
14.291
5.164

holonyms
0.603
0.224
0.612
7.200
error
4.050
2.234
1.485

troponyms
0.227
0.216
0.340
1.390
0.911
0.094
0.059
0.123

Table 15: The WordNet tests.

System

time

OB
5.23

xsb
4.47

yap
12.05

dlv

20.19

jess
140

owlim
error

jena
error

drools
error

Table 16: The Wine ontology test.

test
size

ontobroker

xsb
yap
dlv

drools

jess

win-not-win

100000
1.327
0.231
0.103
0.691
1.228
1.000

500000
9.988
1.218
0.654
3.554
7.466
3.000

2000000
timeout
5.081
2.866
15.224
error
15.000

modiﬁed same gen
24000
24.963
90.928
44.605
444.873
error
error

6000
14.883
7.265
3.339
36.827
error
error

Table 17: Locally- and predicate-stratiﬁed negation.

test
size

ontobroker

xsb
dlv

win-not-win

50000
0.419
0.339
0.344

250000
3.754
1.416
1.879

1000000
17.237
5.647
8.361

MS

24000
0.236
1.381
0.189

504000
8.409
1.663
2.043

Table 18: Times for locally non-stratiﬁed rule sets.

two systems, while DLV does better than Ontobroker. We
should note that DLV and Ontobroker are both bottom-
up systems, and they use similar algorithms for the well-
founded negation, based on the alternating ﬁxpoint compu-
tation [28]. In contrast, XSB is a top-down system, and its
algorithm for the well-founded negation is radically diﬀer-
ent: it is based on SLG-resolution, and this might account
for the diﬀerence in performance.

5. CONCLUSIONS

We presented the results of a comprehensive study of the
performance and scalability for several well-known engines
for rule-based reasoning. Our objective was to compare ex-
isting technologies and evaluate their potential as semantic
engines for the Web. Our data, generators, and scripts are
available as part of OpenRuleBench [20], a benchmarking
suite provided as an open community resource for further
investigation into the performance of various rule engines.

Our results show that two technologies hold great promise:
the tabling Prolog technology, such as the one used in XSB
and Yap, and the deductive database technology used in
Ontobroker and DLV. Three systems, XSB, Yap, and Onto-
broker were clear winners in most of the tests. DLV also did
well in most of the tests.

The signiﬁcant variability in the test results among the
top-performing systems draws attention to three issues that
are critical for engine scalability and performance: indexing,
memory management, and heuristic query optimization. In-
dexing seems to have been more or less adequately addressed
in Ontobroker and Yap (and to a lesser extent in XSB),
but when it comes to memory management all of the tested
systems have met their challenge in one benchmark or an-
other (on larger data sets available in OpenRuleBench). The
third problem, query planning, has been addressed in some
bottom-up engines (DLV, Ontobroker), but further research
is needed to improve the algorithms.

Future work on OpenRuleBench might include additional
engines and tests. For instance, we would like to add tests
that involve function symbols in the data.

Acknowledgments. This work was supported, in part, by
a contract with Vulcan, Inc. We would like to thank Ben-
jamin Grosof for his encouragement, discussions, and many
helpful suggestions during the course of this project. We
are also grateful to Juergen Angele, Juergen Baier, and Ul-
rich Siebald for suggesting the Mondial, DBLP, and wine
benchmarks, and for the help with setting up and running
Ontobroker. Many thanks to: David Warren and Terrance
Swift—for assistance with the use of XSB; Vitor Santos
Costa and Michel Ferreira—for the help with Yap; Keith
Goolsbey, Douglas Miles, Douglas Lenat, Larry Lefkowitz,
Vinay Chaudhri, Ken Murray, and Bill Jarrold—for help-
ing out with CYC; Wolfgang Faber, Gerald Pfeifer, Nicola
Leone—for many insights into the use of DLV; Edson Tirelli,
Geoﬀrey De Smet, Jaroslaw Kijanowski, Mark Proctor, Anstis
Michael, and Ojwang Wilson — for assistance with Drools;

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management609Barry Bishop — for assistance with Iris; Andy Seaborne,
James Howison, Damian Steer, Dave Reynolds, and Adrian
Walker — for the help with Jena; Ernest Friedman-Hill,
Wolfgang Laun, Dane Wyrick, and Erick Wang—for assis-
tance with Jess; Damyan Ognyanoﬀ and Atanas Kiryakov
— for helping with OWLIM; Adrian Paschke and Alex Ko-
zlenkov — for the help with Prova.

6. REFERENCES

[1] H. A¨ıt-Kaci. Warren’s abstract machine: a tutorial
reconstruction. MIT Press, Cambridge, MA, 1991.
http://www.freetechbooks.com/warren-s-
abstract-machine-a-tutorial-reconstruction-
t397.html.

[2] K.R. Apt, H. Blair, and A. Walker. Towards a theory

of declarative knowledge. In J. Minker, editor,
Foundations of Deductive Databases and Logic
Programming, pages 89–148. Morgan Kaufmann, Los
Altos, CA, 1988.

[15] M. Kifer, A. Bernstein, and P.M. Lewis. Database

Systems: An Application Oriented Approach,
Complete Version (2nd Edition). Addison-Wesley
Longman Publishing Co., Inc., Boston, MA, 2005.

[16] M. Kifer, G. Lausen, and J. Wu. Logical foundations

of object-oriented and frame-based languages. Journal
of ACM, 42:741–843, July 1995.

[17] M. Kifer and E.L. Lozinskii. Implementing logic

programs as a database system. In IEEE 3-d Int’l
Conference on Data Engineering, pages 375–385,
February 1987.

[18] O. Lassila and R.R. Swick (editors). Resource

description framework (RDF) model and syntax
speciﬁcation. Technical report, W3C, February 1999.
http://www.w3.org/TR/1999/REC-rdf-syntax-
19990222/.

[19] L. Ma, Y. Yang, Z. Qiu, G. Xie, Y. Pan, and S. Liu.

Towards a complete OWL ontology benchmark. In
European Semantic Web Conference, pages 125–139,
2006.

[3] I. Balbin, G. S. Port, K. Ramamohanarao, and

[20] Openrulebench web site, 2008.

K. Meenakshi. Eﬃcient bottom-up computation of
queries on stratiﬁed databases. J. Log. Program.,
11(3-4):295–344, 1991.

[4] C. Beeri and R. Ramakrishnan. On the power of

magic. Journal of Logic Programming, 10:255–300,
April 1991.

[5] B. Bishop and F. Fischer. Iris - integrated rule
inference system. In International Workshop on
Advancing Reasoning on the Web: Scalability and
Commonsense (ARea 2008), June 2008.

[6] F. Bry, N. Eisinger, T. Eiter, T. Furche, G. Gottlob,

C. Ley, B. Linse, R. Pichler, and F. Wei. Foundations
of Rule-Based Query Answering. In Proceedings of
Summer School Reasoning Web 2007, Dresden,
Germany (3rd–7th September 2007), volume 4634 of
LNCS, pages 1–153. REWERSE, 2007.

[7] F. Calimeri, S. Cozza, G. Ianni, and N. Leone.

Computable functions in ASP: Theory and
implementation. In Int’l Conference on Logic
Programming, pages 407–424, December 2008.

[8] The China benchmark suite, 2001.

http://www.cs.unipr.it/China/Benchmarks/.

[9] K.L. Clark. Negation as failure. In H. Gallaire and

J. Minker, editors, Logic and Data Bases, pages
292–322. Plenum Press, 1978.

http://rulebench.projects.semwebcentral.org.

[21] Prolog benchmarking, 1985.

http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/lang/prolog/code/bench/0.html.

[22] T.C. Przymusinski. On the declarative semantics of

deductive databases and logic programs. In J. Minker,
editor, Foundations of Deductive Databases and Logic
Programming, pages 193–216. Morgan Kaufmann, Los
Altos, CA, 1988.

[23] Y Sure, S Staab, and J Angele. Ontoedit: Guiding

ontology development by methodology and
inferencing. In 1st International Conf. on Ontologies,
Databases, and Applications of Semantics, 2002.

[24] T. Swift and D.S. Warren. An abstract machine for

SLG resolution: Deﬁnite programs. In Int’l Logic
Programming Symposium, Cambridge, MA, November
1994. MIT Press.

[25] H.J. ter Horst. Combining RDF and part of OWL
with rules: Semantics, decidability, complexity. In
International Semantic Web Conference (ISWC),
pages 668–684, November 2005.

[26] G. Terracina, N. Leone, V. Lio, and C. Panetta.

Experimenting with recursive queries in database and
logic programming systems. Journal of the Theory and
Practice of Logic Programming, 8(2):129–165, 2008.

[10] C.L. Forgy. Rete: a fast algorithm for the many

[27] J.F. Ullman. Principles of Database and

pattern/many object pattern match problem.
Atriﬁcial Intelligence, 19(1):17–37, September 1982.

Knowledge-Base Systems, Volume 1. Computer
Science Press, Rockville, MD, 1988.

[11] H. Garcia-Molina, J.D. Ullman, and J. Widom.

[28] A. Van Gelder. The alternating ﬁxpoint of logic

Database Systems: The Complete Book. Prentice-Hall,
2008.

programs with negation. Journal of Computer and
System Sciences, 47(1):185–221, 1993.

[12] M. Gelfond and N. Leone. Logic programming and

[29] A. Van Gelder, K.A. Ross, and J.S. Schlipf. The

knowledge representation — the A-Prolog perspective.
Artiﬁcial Intelligence, 138(1–2):3–38, 2002.

well-founded semantics for general logic programs.
Journal of ACM, 38(3):620–650, 1991.

[13] M. Gelfond and V. Lifschitz. The stable model

[30] D.S. Warren. Memoing for logic programming.

semantics for logic programming. In Logic
Programming: Proceedings of the Fifth Conference and
Symposium, pages 1070–1080, 1988.

[14] Y. Guo, Z. Pan, and J. Heﬂin. LUBM: A benchmark

for OWL knowledge base systems. Journal of Web
Semantics, 2005.

Communications of ACM, 35(3):93–111, March 1992.

[31] D.S. Warren. Programming in tabled prolog.

Manuscript. http://www.cs.sunysb.edu/~warren/
xsbbook/, 1999.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Semantic Data Management610