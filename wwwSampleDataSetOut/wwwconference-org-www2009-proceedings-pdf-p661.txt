Extracting Key Terms From Noisy and Multi-theme

Documents

Maria Grineva
Institute for System

Programming

Maxim Grinev
Institute for System

Programming

Dmitry Lizorkin
Institute for System

Programming

Sciences

lizorkin@ispras.ru

of the Russian Academy of

of the Russian Academy of

of the Russian Academy of

Sciences

Sciences

maria.grineva@gmail.com

maxim@grinev.net

ABSTRACT
We present a novel method for key term extraction from text
documents. In our method, document is modeled as a graph
of semantic relationships between terms of that document.
We exploit the following remarkable feature of the graph:
the terms related to the main topics of the document tend to
bunch up into densely interconnected subgraphs or commu-
nities, while non-important terms fall into weakly intercon-
nected communities, or even become isolated vertices. We
apply graph community detection techniques to partition
the graph into thematically cohesive groups of terms. We
introduce a criterion function to select groups that contain
key terms discarding groups with unimportant terms. To
weight terms and determine semantic relatedness between
them we exploit information extracted from Wikipedia.

Using such an approach gives us the following two ad-
vantages. First, it allows eﬀectively processing multi-theme
documents. Second, it is good at ﬁltering out noise infor-
mation in the document, such as, for example, navigational
bars or headers in web pages.

Evaluations of the method show that it outperforms exist-
ing methods producing key terms with higher precision and
recall. Additional experiments on web pages prove that our
method appears to be substantially more eﬀective on noisy
and multi-theme documents than existing methods.

Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Re-
trieval—Information ﬁltering

General Terms
Algorithms, Experimentation, Measurement

Keywords
Wikipedia, community detection, graph analysis, keywords
extraction, semantic similarity

1.

INTRODUCTION

Key terms (sometimes referred to as keywords or key
phrases) are set of signiﬁcant terms in a text document that

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

give high-level description of its content for readers. Key
term extraction is a basic step for various tasks of natural
language processing, such as document classiﬁcation, doc-
ument clustering, text summarization and inferring a more
general topic of a text document [10].

Key term extraction is the core task of Internet content-
based advertising systems, such as Google’s AdSense and
Yahoo! Contextual Match. They ﬁnd relevant key terms on
a web page, and then display advertisements based on those
key terms. The quality of key terms extracted from a web
page is a crucial issue for such systems: 10% improvement
of key terms quality lead to nearly 10% higher click-through
rate, directly increasing the overall eﬀectiveness of a given
advertisement [25]. Such systems raise the following new
challenges for key term extraction techniques. First, they
need to extract key terms from web pages that are typically
noisy, i.e. overburden with information irrelevant to the
main topic of the page, such as navigational information
(e.g. side bars/menus), comments, future announces, etc.
Second, they need to extract key terms from portal home
pages which usually include many articles on diﬀerent topics
(for example, the homepage of BBC News 1). Thus, the key
terms extraction technique must be noise and multi-theme
stable.

State-of-the-art approaches for key terms extraction are
based on statistical learning that requires hand-created train-
ing set. In such approaches a document is treated as a set
of semantically independent terms that must be classiﬁed
as either positive or negative examples of keyphrases. The
classiﬁer is trained using statistics about term occurrence
patterns found in the training set. In this paper, we propose
a new approach to key terms extraction that is diﬀerent in
two points. First, instead of using statistics gathered from
a training set, which might be stale or domain-speciﬁc, we
use semantic information derived from a universal knowl-
edge base (namely, Wikipedia), which provides up-to-date
and extensive coverage for a broad range of domains. Sec-
ond, our method utilizes more information from within the
processed document by identifying and analyzing semantic
relatedness between terms in the document. We show that
our method produces key terms with quality not lower com-
pared to some state-of-the-art methods, at the same time
being more eﬀective on extracting key terms from noisy and
multi-theme documents.

The fundamental brick of our method is Wikipedia-based

1http://news.bbc.co.uk/

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics661semantic relatedness of terms. Wikipedia 2 is a free online
encyclopedia that has grown to become the largest online
repository of encyclopedic knowledge. It contains millions
of articles available for a large number of languages. As
for September 2008, English Wikipedia contains over 2.5
millions articles (over 6 millions if consider redirects).
In
addition to being the largest vocabulary ever existed, Wiki-
pedia is also a rich source of terms relationships expressed
via extensive number of cross-references and hierarchical
categories. Recent works on computing semantic related-
ness measure of terms using Wikipedia [18, 17, 6, 23, 26]
turned Wikipedia into a usable and exceptionally powerful
tool for our work and many other natural language process-
ing (NLP) and information retrieval (IR) applications. We
discuss these works in more detail in Section 2 of this paper.
The main ideas behind the proposed method are as fol-
lows. We generate candidate terms by detecting all Wikipe-
dia terms in the content of a document. At this step each
ambiguous term is assigned with its proper meaning using
one of the existing word sense disambiguation techniques
[12, 22, 11, 26]. The document is then modeled as a se-
mantic graph of the candidate terms. Semantic graph is a
weighted graph where vertices are terms, edge between a
pair of terms means that these two terms are semantically
related, the weight of the edge is the semantic relatedness
measure of the two terms. Analyzing semantic graphs con-
structed in this way for various documents we observed that
they have a remarkable property: the terms related to the
common topics tend to bunch up into dense subgraphs or
communities, and the most massive and densely intercon-
nected groups of terms typically correspond to the main
topics of the processed document! We exploit this prop-
erty to distinguish between key terms and non-important
terms in a document: we choose terms from the most dense
groups as key terms discarding terms from sparse groups
as irrelevant to the main topic of the document. To detect
communities in the semantic graph of a document we apply
Girvan-Newman network analysis algorithm that has been
proved to be highly eﬀective at discovering community struc-
ture in various computer-generated and real-world networks
[19].

Our method provides the following main advantages:
• No training. Instead of training the system with hand-
created examples, we use semantic information derived
from Wikipedia.

• Thematically grouped key terms. The output of the
method is groups of semantically related terms, and
each group relates to one of the main topics of the
document. Thematically grouped key terms can sig-
niﬁcantly improve further inferring of document topics
using, for example, spreading activation over Wikipe-
dia categories graph as described in [24]. Applying this
technique ([24]) to each group instead of all key terms
allows inferring topic more accurately.

• High accuracy. Our evaluation using human judg-
ments shows that our method produces key terms with
overall precision and recall higher than baseline (TFx-
IDF [21]) and several state-of-the-art systems (Yahoo!
keyword extractor, Wikify! [14] and TextRank [15]).

2www.wikipedia.org

• Noise and multi-theme stability. Noisy and multi-theme
documents are common among web pages. To test
noise and multi-theme stability of our method we have
conducted specialized evaluation on web pages. When
applied to web pages our method produces key terms
with substantially higher accuracy than the baseline
and state-of-the-art systems noted above.

2. BACKGROUND

The background of our method consists in the following
techniques: measure of terms semantic relatedness com-
puted over Wikipedia corpus; and network analysis algo-
rithm for detecting community structure in networks. We
discuss each of the techniques in the following subsections.
2.1 Computing semantic relatedness over Wi-

kipedia

A measure of semantic relatedness is computational mean
for calculating the association strength between terms. More
precisely it assigns a score for a pair of Wikipedia terms that
represents the strength of relatedness between the terms.
Terms semantic relatedness can be inferred from a dictio-
nary or thesaurus (for example, WordNet [16]), but here we
are interested in terms semantic relatedness derived from
Wikipedia. During the last three years, there have appeared
a good few of works on computing Wikipedia-based seman-
tic relatedness using diﬀerent methods [18, 17, 6, 23, 26].
Wikipedia-based semantic relatedness measure for two terms
can be computed using either the links found within their
corresponding Wikipedia articles [18, 17, 26], or Wikipedia
categories structure [23], or the article’s textual content [6].
See [17] for an insightful comparison of the many of existing
Wikipedia-based semantic relatedness measures. There is a
bunch of works on using Wikipedia-based semantic related-
ness to solve the many basic NLP/IR tasks such as: word
sense disambiguation [12, 22, 11, 26], document topic infer-
ring [24], document categorization [7], coreference resolution
[23].
In our method we use semantic relatedness to build
a semantic graph of candidate terms, which are Wikipedia
terms found in a document. The structure of the graph is
then analyzed to derive key terms. While our method does
not provide any requirements on the way of computing se-
mantic relatedness, the eﬀectiveness of our method depends
on the eﬀectiveness of the exploited semantic relatedness
measure. For the evaluation of our method in this paper we
use the semantic relatedness measure proposed in [26].
2.2 Detecting communities in networks

To automatically detect densely interconnected groups of
terms (communities) in semantic graph we need to apply a
proper graph clustering technique to such graph. It is cru-
cial that the number of communities is diﬀerent for diﬀerent
semantic graphs, and is not known in advance. Thus, a class
of clustering techniques that require some predeﬁned num-
ber of clusters for input (for example, diﬀerent variations of
k-means, minimum-cut graph partitioning algorithm) does
not ﬁt our task.

We turn to the hierarchical clustering approach focusing
at techniques that specialize in discovering a natural divi-
sion of networks into communities, when the number of com-
munities is uncertain. Fortunately, this ﬁeld has been well
studied. Many algorithms have been proposed and applied
with great success to social networks [27], citation networks

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics662[20, 4], purchasing network [3], biochemical networks [8] and
many others. However, to the best of our knowledge, there
are no applications of community detection algorithms to
the Wikipedia-based networks.

Among these algorithms the one invented by M. E. J.
Newman and M. Girvan [19] and its further optimization
for large networks [3] are most commonly used and have
been proved to be highly eﬀective at discovering community
structure in both computer-generated and real-world net-
work data. We use this algorithm in our method. This algo-
rithm iteratively removes edges from the network to split it
into communities. The edges removed being identiﬁed using
the graph-theoretic measure of betweenness, which assigns
a number to each edge that is large if the edge lies ”be-
tween” many pairs of nodes. To estimate the goodness of
the certain graph partition, the authors of [19] propose the
notion of modularity. Modularity [19] is a property of a net-
work and a speciﬁc proposed division of that network into
communities. It measures when the division is a good one,
in the sense that there are many edges within communities
and only a few between them. In practice, modularity val-
ues that fall in the range from about 0.3 to 0.7 indicate that
network has quite a distinguishable community structure.

3. RELATED WORK

There are a number of classical approaches to extracting
key terms. The simplest approach is to use a frequency cri-
terion (or TFxIDF model [21]) to select keywords in a docu-
ment. However, this method was generally found to lead to
poor results, and consequently other methods were explored.
The state-of-the-art in this area is currently represented by
supervised learning methods, where a system is trained to
recognize keywords in a text, based on lexical and syntactic
features. In this class of systems the most prominent one is
KEA [5]. In KEA the candidate terms are represented using
three features: TDxIDF, distance (the number of words that
precede the ﬁrst occurrence of the term, divided by the num-
ber of words in the document) and keyphrase-frequency (the
number of times a candidate term occurs as a key term in
the training documents). The classiﬁer is trained using the
naive Bayes learning algorithm. Thus, KEA analyses only
simple statistical properties of candidate terms and consid-
ers each candidate term independently from the other terms
in the document. In contrast to this, our method analyses
semantic term relationships in the document. Another dif-
ference is that KEA depends on the training set and may
provide poor results when the training set does not ﬁt well
the processed documents while our method does not require
training.

Recently a number of new methods have been proposed
that extend classical methods with new features computed
over Wikipedia corpus. For example [11] introduces node
degree feature that indicates how a candidate term is con-
nected via Wikipedia links to other candidate terms in the
document (i.e. the number of links between the Wikipedia
article corresponding to a candidate term and the Wikipe-
dia articles corresponding to other candidate terms). So
candidate terms with high node degree are those that have
many related terms in the document. Wikify! system [14]
introduces keyphraseness feature of a candidate term that
is deﬁned as the number of Wikipedia articles in which
the term appears and is marked up as a link divided by
the total number of Wikipedia articles where the term ap-

pears. This feature can be interpreted as probability that
the candidate term is selected as a key term in a Wikipe-
dia article as according to the Wikipedia editorial rules only
key terms should be used as links to other articles. Wik-
ify! uses keyphraseness as the only feature to select key
terms. Comparison of keyphraseness with TFxIDF demon-
strates improvements in both precision and recall by about
10% each. In our method we use keyphraseness to compute
community informativeness (see Section 4.5).

There is an alternative class of approaches that selects key
terms by analyzing syntactic or semantic term relatedness
in a document. In the approaches a document is modeled as
a graph of candidate terms in which edges represent a mea-
sure of term relatedness. Some graph analysis technique is
used to select key terms. Usually the graph is analyzed us-
ing a graph-based ranking algorithm (such as PageRank [2],
HITS [9], etc) to range candidate terms. Top N terms with
the highest scores are selected as key terms. For example,
in [15] the graph is constructed using a syntactic term relat-
edness (namely, co-occurrence relation) deﬁned as follows:
two terms are related if they co-occur within a window of
maximum N words. To rank candidate terms in the con-
structed graph Google’s PageRank algorithm [2] is applied.
The authors of this work reported that their method outper-
forms classical statistics-based approaches with respect to
precision and F-measure although the recall is lower then in
the statistics-based methods. Another variation of the same
approach was proposed in [7] as a part of a document cate-
gorization algorithm. In this work the graph is constructed
using a semantic term relatedness computed over an ontol-
ogy generated from Wikipedia as proposed in [1]. Candidate
terms are ranked with centrality scores computed using the
geodesic closeness measure. The author of the latter work
does not evaluate the proposed key term extraction method
itself but shows superior quality comparing their document
categorization method with statistics-based categorization
methods. Both of the methods described above are unsuper-
vised and does not require training. Our method also takes
the same approach based on analysis of semantic graph con-
structed from a document. But in our work we perform more
sophisticated graph analysis by applying Girvan-Newman
algorithm. It provides the following beneﬁts. First, as we
demonstrate in the evaluation section, our algorithm out-
performs those based on a graph-based ranking algorithm.
Second, the result of our method is semantically grouped
key terms that are useful for further processing (such as
document topic inference) or interpretation by humans.

4. METHOD FOR KEY TERMS EXTRAC-

TION

The method consists of the ﬁve steps that we describe
in detail in the following subsections: (1) candidate terms
extraction; (2) word sense disambiguation; (3) building se-
mantic graph; (4) discovering community structure of the
semantic graph; and (5) selecting valuable communities.

4.1 Candidate Terms Extraction

The goal of this step is to extract all terms from the doc-
ument and for each term prepare a set of Wikipedia articles
that can describe its meaning.

We parse the input document and extract all possible n-
grams. For each n-gram we construct its variations using

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics663diﬀerent morphological forms of its words. We search for all
n-gram variations among Wikipedia article titles. Thus, for
each n-gram a set of Wikipedia articles can be provided.

Constructing diﬀerent morphological forms of words al-
lows us not to miss a good fraction of terms. For instance,
”drinks”, ”drinking”, and ”drink” can be linked to the two
Wikipedia articles: ”Drink” and ”Drinking”.

It is a typical problem with traditional key term extrac-
tion techniques when nonsense phrases such as e.g. ”using”,
”electric cars are” appear in the result. Using Wikipedia
articles titles as a controlled vocabulary, allows us to avoid
this problem, all of the key terms produced by our method
are acceptable phrases.

4.2 Word Sense Disambiguation

At this step we need to choose the most appropriate Wi-
kipedia article from the set of candidate articles for each
ambiguous term extracted on the previous step.

It is an often situation in natural language when a word
is ambiguous, i.e. carries more than one meaning, for ex-
ample: the word ”platform” can be used in the expression
”railway platform”, or it can refer to a hardware architecture
or a software platform. The correct sense of an ambiguous
word can be selected based on the context where it occurs,
and correspondingly the problem of word sense disambigua-
tion is deﬁned as a task of automatically assigning the most
appropriate meaning (in our case, the most appropriate Wi-
kipedia article) to a word within a given context.

There are a number of works on disambiguating terms us-
ing Wikipedia [26, 11, 22, 12, 13]. For evaluation in this
paper we used the method described in [26].
In [26] au-
thors make use of Wikipedia disambiguation and redirect
articles to obtain candidate meanings of ambiguous terms.
For each ambiguous term disambiguation page contains all
of the meanings of the term, which are separate articles in
Wikipedia with their own link structure. For example, the
article ”platform (disambiguation)” contains 17 meanings of
the word ”platform”. Then in [26] semantic relatedness mea-
sure is used to pick the meaning that has the highest rele-
vance to the context where the ambiguous term appears.

The result of this step is a list of terms, where each term
is assigned with a single Wikipedia article that describes its
meaning.

4.3 Building Semantic Graph

At this step we build a semantic graph from a list of terms

obtained at the previous step.

Semantic graph is a weighted graph where each vertex
is a term, edge between a pair of vertices means that the
two terms corresponding to these vertices are semantically
related, the weight of the edge is the semantic relatedness
measure of the two terms.

Figure 1 shows semantic graph built from a news article
”Apple to Make ITunes More Accessible For the Blind”. This
article tells that the Massachusetts attorney general’s oﬃce
and the National Federation of the Blind reached an agree-
ment with Apple Inc. under which it will make its music
download service (ITunes) accessible to the blind consumers
using screen-reading software. In Figure 1 you can see that
terms related to Apple Inc. and Blindness constitute two
dominant communities, and terms like Time, Month, Mas-
sachusetts or Consumer fall into peripheral and weakly con-
nected communities.

An important observation is that disambiguation mistakes
(in Figure 1: Home Oﬃce, Free agent, Grocery store) tend
to fall into weakly connected communities or even become
isolated vertices in a semantic graph and not to adjoin to
dominant communities.

4.4 Discovering Community Structure of the

Semantic Graph

We use algorithm proposed by M. E. J. Newman and M.
Girvan [19] to discover community structure of the semantic
graph built on the previous step. The algorithm divides the
input graph into a number of subgraphs that are likely to
be dense communities.

We observed that semantic graphs constructed from an
average text document (one page news article, or a typical
academic paper) have modularity values between 0.3 and
0.5. That is an indication that application of the Girvan-
Newman algorithm to semantic graphs indeed makes sense
as they have quite a distinguishable community structure.

4.5 Selecting Valuable Communities

The goal of this step is to rank term communities in a
way that highest ranked communities would contain terms
semantically related to the main topics of the document (key
terms), and the lowest ranked communities contain not im-
portant terms, and possible disambiguation mistakes (terms
which meaning was chosen wrong on the second step).

Ranking is based on the density and informativeness of
communities. Density of a community is a sum of weights of
all inner-community edges divided by the number of vertices
in this community.

While experimenting with existing approaches discussed
in Section 3, we observed that using keyphraseness measure
of terms can help ranking communities in a proper way.
Keyphraseness measure gives higher values to the named
entities (for example, Apple Inc., Steve Jobs, Braille) than
to general terms (Consumer, Agreement, Information). We
compute keyphraseness measure of terms using Wikipedia
corpus as described in [11]: the number of Wikipedia arti-
cles in which the term appears and is marked up as a link
divided by the total number of Wikipedia articles where the
term appears. Informativeness of a community is a sum of
keyphraseness measure of all terms in a community divided
by the number of terms.

Eventually, the rank value assigned to each community is
its density multiplied by its informativeness. Communities
are then sorted according to this value, thus, we obtain a
sorted sequence of communities each assigned with its rank
value.

We have observed the following important feature of the
communities rank values.
In the sorted sequence commu-
nities rank values do not decrease evenly. Instead, there is
almost always a single evident decline. Figure 2 demon-
strates the decrease in community rank scores taken from
our example. Communities are numerated according to the
marks in Figure 1.
In Figure 2 such decline is observed
between second and third communities. According to the
semantic graph shown in Figure 1 this decline separates two
communities with terms related to the main topics of the
news article (Apple Inc. and Blindness) from other commu-
nities with less important terms.

On average, such a decline is 20 - 25 times more than the
diﬀerence between rank values of other neighbouring com-

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics664Figure 1: Semantic graph built from the news article ”Apple to Make ITunes More Accessible For the Blind”

munities in the sorted sequence. In section 5 we prove ex-
perimentally that this decline often separates communities
of important terms (that go before the decline) from commu-
nities of non-important terms (that go after the decline). It
makes sense to use this decline as an indicator when select-
ing valuable communities, thus, determining how many of
the communities to accept, while rejecting the rest of them.

5. EXPERIMENTAL EVALUATION

This section reports the experimental results comparing
our method with several baseline and state-of-the-art meth-
ods. Since these methods are not speciﬁcally designed to
handle noisy content and compound multi-theme documents,
we ﬁrst evaluate the methods on presumably single-topic
documents represented in plain text and being noise-free.
We then conduct twofold evaluation of the methods on web
pages containing noise and compound web pages with di-
verse topics.

Since there is no standard benchmark for evaluating the
quality of the extracted key terms, we conducted a human
study by asking annotators to manually select key terms
from a test collection. We found that reasonable examples
of single-topic noise-free documents are posts from technical
blogs obtained via RSS feeds. Our test collection consists of
252 posts from the following technical blogs: Geeking with
Greg by Greg Linden 3, DBMS2 by Curt Monash 4 and Stan-
ford Infoblog by people from Stanford Infolab 5. Twenty-two

3http://glinden.blogspot.com/
4http://www.dbms2.com/
5http://infoblog.stanford.edu/

Figure 2: Decline in community scores

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics665annotators took part in the human study. These included
5 developers having either M.Sc. or Ph.D. degree in com-
puter science, and 17 undergraduate students in the ﬁeld of
computer science. Each document was analyzed by 5 dif-
ferent annotators. Eventually, we considered a key term to
be valid if at least two of the participants identiﬁed this key
term in the document. For each document two sets of key
terms were built. To build the ﬁrst set, called uncontrolled
key terms, each annotator was instructed to identify from 5
to 10 key terms as they appear in the document. The sec-
ond set, called controlled key terms, is constructed on the
basis of the ﬁrst one by the same annotator as follows. All
the key terms from the ﬁrst set which have a correspond-
ing Wikipedia article have to be replaced with the title of
the article. The key terms that do not match Wikipedia
articles remain in the second set as they appear in the ﬁrst
one. Thus constructing the second set implies manual dis-
ambiguation of key terms against Wikipedia as an allowed
vocabulary. Doing that annotators were instructed to con-
sider Wikipedia redirects and the articles, which they direct
to as being the same term since they inherently represent
synonyms. These two sets of key terms are required so that
we can evaluate various methods based on uncontrolled or
controlled Wikipedia-based vocabulary using the same test
collection. As a result, we have got 2009 key terms for the
252 blog posts, with about 93% of them being Wikipedia
titles.

The techniques presented in the paper were implemented
with the following architectural design principles. For achiev-
ing the best computational eﬃciency, all necessary data con-
cerning Wikipedia articles names, Wikipedia link structure
and statistical information about the Wikipedia corpus (for
instance, terms keyphraseness) are kept in main memory.
With the recent Wikipedia being quite a large knowledge
base, our Wikipedia knowledge base takes 4.5 Gigabytes in
the main memory. A dedicated machine with 8Gb RAM
was used for the evaluation, and client applications access
the knowledge base via remote method invocation. With the
requirement for having access to similarity scores for virtu-
ally every term pair in Wikipedia, similarity scores are not
computed oﬄine in advance, but are rather computed on
demand on the ﬂy using Wikipedia link structure [26].

We have set up a web service upon this implementation,
so that our key terms extraction method is accessible online
via a simple web interface 6.

Before we proceed with the comparisons we consider ex-
perimental proof of the hypothesis stated in Section 4.5 -
that decline in community scores can be used as a criteria
to determine the number of term communities to be returned
as key terms.

5.1 What Does Decline in Communities Rank

Scores Mean?

The result of our method is a sequence of communities of
term with decreasing rank scores. As we mentioned in Sec-
tion 4.5, there is almost always an evident decline in com-
munities rank scores. What does this decline mean and how
it can help in determining the number of valuable commu-
nities? We presumed that this decline can serve as a border
between communities of important terms (with higher rank
scores) and the communities of non-important terms (with

6http://www.modis.ispras.ru:7005/demo/keywords/

lower rank scores), so, we conducted the following evaluation
to check this presumption.

For each document from our test collection we conducted
the following evaluation. We applied our method for a doc-
ument, thus obtaining a sorted sequence of communities of
terms. Then we, in serial, accepted every possible number
of communities: ﬁrst, we accepted only the single highest
ranked community, then two highest ranked communities,
and at least we ended up with all communities. For every
case we computed F-measure comparing the accepted terms
with the manually extracted terms for this document. We
then found out the maximum F-measure. Eventually, we
checked if the number of communities that give the maxi-
mum F-measure is indeed the number of communities that
go before the decline.

For our test collection we found out that the decline co-
incides with the maximum F-measure in 73%. Thus, we
consider decline as a good indicator for selecting important
communities. In the evaluation of precision, recall and F-
measure that follows in this section, we rely on the decline
instead of accepting some predeﬁned number of communities
(or key terms).
5.2 Evaluation of Key Terms

In this section, our method is evaluated against existing
methods on the collection of single-topic noise-free docu-
ments described above.

We chose the following methods for the comparison:

TFxIDF is a conventional baseline used in scientiﬁc litera-
ture for comparison of key term extraction algorithms.
For extracting candidate terms that are then ranked
using TFxIDF measure, the technique described in
Subsect. 4.1 was applied. Having the same candidate
term extraction technique for both methods makes the
comparison more illustrative. For obtaining the document-
frequency component required for the TFxIDF method,
the entire Wikipedia was used as a training corpus. As
the TFxIDF method allows merely ranking candidate
terms but provides no notion about the number of key
terms to be selected, the top-K terms are selected with
K equal to the average number of manually assigned
terms within the test set. Since the TFxIDF method,
as it is described above, returns key terms from Wiki-
pedia corpus we use the controlled key terms to eval-
uate the method.

Yahoo! Terms Extractor is chosen for comparison as be-
ing a state-of-the-art industrial tool for key term ex-
traction. Its implementation is available via open API 7.
It takes as input a text document and returns a list of
signiﬁcant words or phrases extracted from the docu-
ment. Since Yahoo! implements a method with uncon-
trolled vocabulary, we calculate precision, recall and
F-measure using the set of uncontrolled key terms.

Wikify! uses keyphraseness computed over Wikipedia to
select key terms (see Section 3 for details). Its imple-
mentation is available online as a demo8. The number
of key terms selected by the method is speciﬁed by
the special external parameter “density” as a fraction
of the overall number of words in a document being

7

8

http://developer.yahoo.com/search/content/V1/termExtraction.html

http://wikifyer.com/

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics666processed. Depending on a particular document size,
we speciﬁed the density value in a way to obtain the
number K of key terms equal to the average number of
manually assigned terms. As Wikify! uses controlled
vocabulary based on Wikipedia, we use the controlled
key terms to evaluate the method.

TextRank. As the last baseline, we implemented the Tex-
tRank approach to key terms selection as proposed
in [15] (see Section 3 for details). For adapting the
approach to the controlled vocabulary of Wikipedia
terms, only document terms which have corresponding
Wikipedia articles were chosen as vertices in the Tex-
tRank model, alternatively to all words passing syn-
tactic ﬁlters originally used in [15] due to uncontrolled
vocabulary. Since titles of Wikipedia articles are gen-
erally noun phrases, the modiﬁcation made to the ap-
proach fully corresponds to the recommendation given
on syntactic ﬁlters in the original TextRank proposal.
Further parameters were exactly those that performed
best in the TextRank experiments: the co-occurrence
window of 2 for term relationships and undirected treat-
ment of edges between vertices. As TextRank was
modiﬁed to work with Wikipedia-based vocabulary, we
use the controlled key terms to evaluate the method.

The methods outlined above were compared using the tra-
ditional measures of precision, recall and F-measure calcu-
lated with respect to key terms extracted by a method and
by human annotators. Namely, precision was calculated as a
fraction of terms automatically extracted by a method that
were also extracted by humans:

|{manually selected}T{machine-selected}|

|{machine-selected}|

,
precision =
with {manually selected} denoting the set of all terms iden-
tiﬁed for a document by humans, {machine-selected} denot-
ing the set of all terms extracted for the same document by
a method and |S| denoting the number of items in a set S.
Recall was calculated as the fraction of the manually ex-
tracted key terms that were also extracted by an automatic
method:

|{manually selected}T{machine-selected}|

|{manually selected}|

.

recall =

The weighted harmonic mean of precision and recall, F-

measure was calculated traditionally as:

F-measure =

2 · precision · recall
precision + recall

.

Table 1 summarizes the performance of the methods for
the test data set. The results presented in the table show
that the method presented in this paper (communities-based)
exhibits the best performance among all the other methods
considered in the experiment. Although TFxIDF method
was based on the same candidate terms extraction technique
as the communities-based method, TFxIDF method showed
the worst performance due to weaker consistency between
candidates ranking produced by the TFxIDF scheme and
human judgments. TextRank method shows the second to
the best performance, which conﬁrms our earlier assumption
that considering pair-wise semantic relationships between
candidate terms and treating them as a graph to analyze
improves the quality of the extracted key terms.

Performance, %

Method
TF×IDF
Wikify!
Yahoo! terms extractor
TextRank
Communities-based

Precision Recall F-measure
33.1
39.0
40.5
42.4
44.7

44.8
50.7
52.5
54.8
61.5

26.3
31.7
33.0
34.6
35.1

Table 1: Performance of diﬀerent key term extrac-
tion methods for noise-free data set

It is worth noting that while TextRank treats all candi-
date terms as a single graph when ranking the terms, the
communities-based method analyses each community inde-
pendently that allows more representatively choosing candi-
date terms from diﬀerent thematic groups.

5.2.1 Revision of Precision and Recall

However, we have revisited the measuring of precision and
recall according to the speciﬁcs of our method. The impor-
tant thing is that our method, on average, extracts more
terms than a human. More precisely, our method typically
extracts more related terms in each thematic group than
a human. For example, consider Figure 1, for the topic
related to Apple Inc. our method extracts terms: Music
download, Apple Inc., ITunes, IPod nano; while a human
typically identiﬁes less, and tends to identify named enti-
ties: Apple Inc., ITunes and IPod nano. That means that,
possibly, sometimes our method produces better terms cov-
erage for a speciﬁc topic than an average human. And this is
a reason that we measure the precision and recall in another
way also.

Each participant of the evaluation was asked to revisit his
key terms in the following way. For each document he was
provided with key terms extracted automatically for this
document. He had to review these automatically extracted
key terms and, if possible, extend his manually identiﬁed key
terms with some from the automatically extracted set. It
appeared that humans indeed found out relevant key terms
that they had not extracted before, and extended their key
terms.

After this revision we obtained 389 additional manually
selected key terms, that gives 1.2 additional terms per each
blog post in average. With the new manually selected terms
added, precision of the communities-based method becomes
46.1%, recall becomes 67.7%.
5.3 Evaluation on Web Pages

In this section key term extraction is evaluated in the pres-
ence of noise in the processed documents and performance
of diﬀerent methods is compared. As discussed in the in-
troduction, automatically extracting key terms from noisy
documents is an important task for content-targeted adver-
tisement.

Pages on the web provide a representative and practical
test data for performing the evaluation. Even with HTML
markup removed, most web pages contain plenty of text
irrelevant to the main content of the page such as menus
and navigation bars, comments, footers, etc., that have to
be ﬁltered out by an automatic method.

In the ﬁrst subsection below, the methods of key term ex-
traction are evaluated on web pages for noise stability. In the

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics667News Blogs Forums

Social

networks

Product
reviews

161

127

92

76

53

Table 2: Number of web pages of each kind consid-
ered for evaluation

Performance, %

Method
TF×IDF
Wikify!
Yahoo! terms extractor
TextRank
Communities-based

Precision Recall F-measure
23.2
29.1
31.3
32.7
41.2

34.6
39.8
40.4
43.9
60.7

17.4
22.9
25.6
26.1
31.2

Table 3: Performance of diﬀerent key term extrac-
tion methods for noisy data set

second subsection, key term extraction is evaluated for an
even more complicated area of compound pages, with each
page containing several articles that cover diverse topics.

5.3.1 Noise stability

For evaluating noise stability of key term extraction, we
collected 509 real-world web pages. In order to additionally
investigate performance of key terms extraction for various
kinds of web pages our test collection includes 5 kinds of
Web pages. Table 2 shows the detailed break down.

The selected web pages were processed by annotators in
the same manner as described in Section 5.
In order to
quantitatively evaluate the performance of the methods the
same measures of precision, recall and F-measure were used
as deﬁned in Section 5.2.

Table 3 summarizes the performance of diﬀerent methods
of key terms extraction for the noisy test set. Compared
to Table 1 considered in the previous section, it can be ob-
served that our communities-based method is more stable
to noise than other methods that have their performance
degrading faster. The reason for the observed tendency is
that noise tends to exhibit less semantic relatedness with the
main topic of a page and thus falls outside dominant commu-
nities in the communities-based method, while other meth-
ods have virtually no mechanism for distinguishing noisy
candidate terms from correct ones.

Table 4 shows performance of the communities-based method

for diﬀerent kinds of web pages considered in the experiment.
The results presented in the table show that key terms for
product reviews and technical blogs can be extracted with
better precision. This is probably due to these kinds of web
pages having more formal textual content and thus being
more stable to noise. Forums, news articles and social net-
works generally have less formal textual content and are
more aﬀected by noise that leads to somewhat worse per-
formance.

5.3.2 Multi-theme stability

For testing the ability of our method to correctly extract
key terms from compound texts consisting of several sep-
arate articles, we chose 50 web pages with diverse topics.
These included front web pages of popular news sites and

Performance, %

Kind of web pages Precision Recall F-measure
News
40.3
44.9
Blogs
37.0
Forums
37.9
Social networks
Product reviews
47.1

30.5
34.3
27.6
28.4
36.0

59.4
64.8
56.2
56.9
67.9

Table 4: Performance of the communities-based key
term extraction method for diﬀerent kinds of web
pages

Performance, %

Method
TF×IDF
Wikify!
Yahoo! terms extractor
TextRank
Communities-based

Precision Recall F-measure
11.4
15.0
25.3
17.6
36.0

9.2
11.1
20.8
15.4
28.7

15.0
23.3
32.2
20.5
48.3

Table 5: Performance of diﬀerent key term extrac-
tion methods for compound texts

home pages of Internet portals with lists of featured arti-
cles.
In addition to noisy content inherent to web pages,
each of the selected pages additionally contained from 2 to
10 separate articles that were generally focused at describing
diverse topics.

For obtaining the set of manually selected key terms that
properly cover the content of a web page, each annotator
was instructed to treat each article on a web page indepen-
dently and to select key terms for that article disregarding
the remaining content of the page.

Having this set of manually assigned key words, the fol-

lowing two experiments were conducted.

In the ﬁrst experiment, performance of the communities-
based method suggested in this paper was compared to the
performance of existing methods on the test set. Since ex-
isting methods have no ability to semantically separate key
terms obtained from diﬀerent articles on a single page, we
observed that candidate terms extracted by these methods
from one–two articles on a page tend to dominate over all the
candidate terms from the other articles. It results to incom-
plete sets of key terms for web pages with whole thematic
groups of important terms missing in the result. Table 5 il-
lustrates this tendency: diﬀerence in precision and recall be-
tween the communities-based method and existing methods
becomes even more signiﬁcant with compound texts consid-
ered.

In the second experiment, the ability of the communities-
based method to select relevant key terms on diﬀerent top-
ics was specially evaluated using an additional methodology.
In this experiment, communities of key terms produced by
the method were shown to annotators to evaluate the rel-
evance between these communities and the articles located
on a compound page. Since several articles on a single page
can have the same topic, e.g. Politics, Sport, Technology,
etc., there are generally fewer communities extracted by the
method than there are articles. However, we observed that
diﬀerent topics covered by articles usually have their corre-

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics668sponding communities extracted by the method. To obtain
a quantitative characteristics of such a correlation, each an-
notator was asked to revisit the pages he evaluated and to
verify whether each article has a corresponding community
of key terms extracted by the method. This additional user
study showed that for 78% articles presented on compound
pages the communities-based method was able to extract
relevant thematic communities.
5.4 Computational Efﬁciency

When experimenting with the implementation, we ob-
served that most computation time was consumed by (i) text
parser for extracting candidate terms from input document
and (ii) semantic graph construction that mainly consists in
obtaining similarity scores for candidate term pairs. Com-
pared to these preliminary steps, the running time for the
remaining steps of the algorithms is negligible, with both
community discovery and selection of valuable communities
being essentially linear [3] in the number of edges in the
semantic graph.

On average, it takes about 4 minutes to extract key terms

from 100 blog posts.

6. CONCLUSION

We presented a novel method for extracting key terms
from a text document. One of the advantages of our method
is that it does not require any training, as it works upon
the Wikipedia-based knowledge base. The important and
novel feature of our method is that it produces groups of
key terms, while each group contains key terms related to
one of the main topics of the document. Thus, our method
implicitly identiﬁes main document topics, and further cate-
gorization and clustering of this document can greatly ben-
eﬁt from that. From implementation viewpoint the novel
feature of our method is that, for the ﬁrst time, an algo-
rithm for detecting community structure of a network is ap-
plied to analyze a semantic graph of terms extracted from a
document.

Our experimental results show that our method produces
high-quality key terms comparable to the ones produced by
state-of-the-art systems developed in the area. Evaluation
proved that our method produces key terms with 67.7%
recall and 46.1% precision, that we consider being signiﬁ-
cantly high. We also conducted experiments for multi-theme
and noisy web pages with performance ﬁgures signiﬁcantly
higher than competitive methods. It allows us to conclude
that a promising application of our method is to improve
content-targeted advertising systems, which have to deal
with such web pages. The implementation of our method
is accessible online via a simple web interface 9.

7. ACKNOWLEDGEMENTS

We thank the developers and students in our department
who participated in the test creation, Alexander Boldakov
and Pavel Velikhov for valuable discussions.

8. REFERENCES
[1] S. Auer and J. Lehmann. What have innsbruck and
leipzig in common? extracting semantics from wiki
content. pages 503–517. 2007.

9http://www.modis.ispras.ru:7005/demo/keywords/

[2] S. Brin and L. Page. The anatomy of a large-scale

hypertextual web search engine. Comput. Netw. ISDN
Syst., 30(1-7):107–117, 1998.

[3] A. Clauset, M. E. J. Newman, and C. Moore. Finding
community structure in very large networks. Physical
Review E, 70:066111, 2004.

[4] D. J. de Solla Price. Networks of scientiﬁc papers.

Science, 169:510–515, 1965.

[5] E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin,

and C. G. Nevill-manning. Domain-speciﬁc keyphrase
extraction. pages 668–673. Morgan Kaufmann
Publishers, 1999.

[6] E. Gabrilovich and S. Markovitch. Computing

semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of The Twentieth
International Joint Conference for Artiﬁcial
Intelligence, pages 1606–1611, Hyderabad, India, 2007.

[7] M. Janik and K. J. Kochut. Wikipedia in action:

Ontological knowledge in text categorization.
International Conference on Semantic Computing,
0:268–275, 2008.

[8] S. A. Kauﬀman. Metabolic stability and epigenesis in

randomly constructed genetic nets. J Theor Biol,
22(3):437–467, March 1969.

[9] J. M. Kleinberg. Authoritative sources in a

hyperlinked environment. J. ACM, 46(5):604–632,
1999.

[10] C. D. Manning and H. Sch ˜Aijtze. Foundations of
Statistical Natural Language Processing. The MIT
Press, June 1999.

[11] O. Medelyan, I. H. Witten, and D. Milne. Topic

indexing with wikipedia. In Wikipedia and AI
workshop at the AAAI-08 Conference (WikiAI08),
Chicago, US, 2008.

[12] R. Mihalcea. Unsupervised large-vocabulary word

sense disambiguation with graph-based algorithms for
sequence data labeling. In HLT ’05: Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 411–418, Morristown, NJ, USA, 2005.
Association for Computational Linguistics.

[13] R. Mihalcea. Using wikipedia for automatic word

sense disambiguation. In Proceedings of NAACL HLT
2007, pages 196–203, 2007.

[14] R. Mihalcea and A. Csomai. Wikify!: linking

documents to encyclopedic knowledge. In CIKM ’07:
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge
management, pages 233–242, New York, NY, USA,
2007. ACM.

[15] R. Mihalcea and P. Tarau. TextRank: Bringing order
into texts. In Proceedings of EMNLP-04and the 2004
Conference on Empirical Methods in Natural Language
Processing, July 2004.

[16] G. A. Miller, C. Fellbaum, R. Tengi, P. Wakeﬁeld,
H. Langone, and B. R. Haskell. Wordnet: a lexical
database for the english language.
http://wordnet.princeton.edu/.

[17] D. Milne. Computing semantic relatedness using

wikipedia link structure. In Proceedings of the New
Zealand Computer Science Research Student
Conference (NZCSRSC), Hamilton, New Zealand,
2007.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics669[18] D. Milne and I. Witten. An eﬀective, low-cost measure
of semantic relatedness obtained from wikipedia links.
In Wikipedia and AI workshop at the AAAI-08
Conference (WikiAI08), Chicago, US, 2008.

[19] M. E. J. Newman and M. Girvan. Finding and

evaluating community structure in networks. Physical
Review E, 69:026113, 2004.

[20] S. Redner. How popular is your paper? an empirical

study of the citation distribution. The European
Physical Journal B, 4:131, 1998.

[21] G. Salton and C. Buckley. Term-weighting approaches

in automatic text retrieval. Inf. Process. Manage.,
24(5):513–523, 1988.

[22] R. Sinha and R. Mihalcea. Unsupervised

graph-basedword sense disambiguation using measures
of word semantic similarity. In ICSC ’07: Proceedings
of the International Conference on Semantic
Computing, pages 363–369, Washington, DC, USA,
2007. IEEE Computer Society.

[23] M. Strube and S. Ponzetto. WikiRelate! Computing
semantic relatedness using Wikipedia. In Proceedings

of the 21st National Conference on Artiﬁcial
Intelligence (AAAI-06), pages 1419–1424, Boston,
Mass., July 2006.

[24] Z. Syed, T. Finin, and A. Joshi. Wikipedia as an

Ontology for Describing Documents. In Proceedings of
the Second International Conference on Weblogs and
Social Media. AAAI Press, March 2008.

[25] W. tau Yih, J. Goodman, and V. R. Carvalho.

Finding advertising keywords on web pages. In WWW
’06: Proceedings of the 15th international conference
on World Wide Web, pages 213–222, New York, NY,
USA, 2006. ACM.

[26] D. Turdakov and P. Velikhov. Semantic relatedness
metric for wikipedia concepts based on link analysis
and its application to word sense disambiguation. In
Colloquium on Databases and Information Systems
(SYRCoDIS), 2008.

[27] S. Wasserman, K. Faust, and D. Iacobucci. Social

Network Analysis : Methods and Applications
(Structural Analysis in the Social Sciences).
Cambridge University Press, November 1994.

WWW 2009 MADRID!Track: Semantic/Data Web / Session: Mining for Semantics670