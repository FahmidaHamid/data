Sitemaps: Above and Beyond the Crawl of Duty

Uri Schonfeld

UCLA Computer Science Department

4732 Boelter Hall

Los Angeles , CA 90095

shuri@shuri.org

Narayanan Shivakumar

Google Inc.

1600 Amphitheatre Parkway

Mountain View, CA

shiva@google.com

ABSTRACT
Comprehensive coverage of the public web is crucial to web
search engines. Search engines use crawlers to retrieve pages
and then discover new ones by extracting the pages’ outgoing
links. However, the set of pages reachable from the publicly
linked web is estimated to be signiﬁcantly smaller than the
invisible web [5], the set of documents that have no incoming
links and can only be retrieved through web applications and
web forms. The Sitemaps protocol is a fast-growing web
protocol supported jointly by major search engines to help
content creators and search engines unlock this hidden data
by making it available to search engines. In this paper, we
perform a detailed study of how “classic” discovery crawling
compares with Sitemaps, in key measures such as coverage
and freshness over key representative websites as well as over
billions of URLs seen at Google. We observe that Sitemaps
and discovery crawling complement each other very well,
and oﬀer diﬀerent tradeoﬀs.

Categories and Subject Descriptors: H.3.3: Informa-
tion Search and Retrieval.

General Terms: Experimentation, Algorithms.

Keywords:
quality.

search engines, crawling, sitemaps, metrics,

1.

INTRODUCTION

How can search engines keep up with the application rich,
constantly changing, trillion URL scale web [17]? Search en-
gines utilize massive computing and networking resources to
run Web crawlers in order to build and maintain a frequently
updated, quality snapshot of the web. However, even with
such massive resources, crawlers still face huge challenges in
this task. In this paper we investigate the Sitemaps proto-
col, how it is being used, and how it can be harnessed to
better keep up with the web.

A Web crawlers starts by fetching a “seed” set of popular
URLs such as aol.com and yahoo.com that link to many Web
pages. It then proceeds by extracting their outgoing links,
adding them to a list of known URLs, and ﬁnally selecting a
set of URLs to retrieve next. The crawler repeats the above
steps until it detects it has a suﬃciently good set of Web
pages to index and serve. The crawler then continues to re-
crawl some of these pages at diﬀerent frequencies in order
to maintain the freshness of the document collection [1].

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

Crawlers still face at least two prominent challenges:
• Coverage: One of the problems crawlers face is ex-
posing the content “trapped” inside databases, behind
forms and behind other browser technologies. For ex-
ample, the growing use of rich AJAX applications where
pages are dynamically constructed based on user ac-
tions, make link extraction more diﬃcult. New ad-
vances in plug-in technologies (e.g., Google Gears, Sil-
verlight) that create custom interactions between the
website and the client browser make the problem even
worse. In general, data that is only available through
web forms, ﬂash applications, or downloaded javascript
programs together constitute the deep web [5]. How
can we help direct crawlers towards picking up these
islands of content?

• Freshness: Users today use search engines to search
for world events, viral videos or a new product an-
nouncement minutes after the event transpired (e.g.
http://www.google.com/trends). Considerable research
has gone into modeling and predicting the rate at which
the content of a website changes. [11]. But it is in-
creasingly challengingly expensive for even sophisti-
cated crawlers to keep a pulse on the rapidly changing
and growing web [17]. How can we help crawlers dis-
cover all new and modiﬁed valid URLs of a site in a
timely manner?

Sitemaps are an easy way for webmasters to inform search
engines about pages on their sites that are available for
crawling. In its simplest form, a Sitemaps ﬁle is an XML
ﬁle that lists a site’s URLs, along with additional metadata
detailing: when was the page last updated, how frequently
does the page change on average and how important it is rel-
ative to the other pages in the site. The purpose of Sitemaps
is to enable search engines to crawl the site more intelli-
gently. Using the Sitemaps protocol does not guarantee that
web pages will be included in the search engine’s index, but
it does provide them with hints that help Web crawlers do
a better job of crawling the site.

The value of such protocols lays primarily in their widespread

adoption by key web-players – search engines, web sites and
web site tools.
In 2006, Google, Yahoo and Microsoft co-
announced support for the Sitemaps protocol in an industry
ﬁrst [15]. Since then, billions of URLs have been published
on the web via Sitemaps and millions of websites support
Sitemaps. These include the US federal government, the US
state governments [28], and many popular websites such as
amazon.com, hp.com, cnn.com, wikipedia.org. A variety of
website tools providers including Go Daddy, Yahoo! Small

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 991Business, Google Search Appliance, IBM Portal, as well as
open-source projects like Plone CMS, Cold Fusion all sup-
port auto-generating Sitemaps.

To the best of our knowledge this paper provides the ﬁrst
analysis of a large data set collected from the Sitemaps pro-
tocol. Our main contributions are: (a) We oﬀer a case study
of how three diﬀerent websites with diﬀerent characteristics
organize and update Sitemaps; (b) We introduce metrics to
evaluate the quality of the Sitemaps provided by a website;
(c) We study how the web appears through Sitemaps crawl’s
and Discovery crawl’s perspectives in terms of freshness and
comprehensiveness; (d) We examine how Web crawlers can
use Sitemaps and we introduce crawling and refreshing al-
gorithms that make use of Sitemaps data.

The rest of this paper is structured as follows. Section 2
covers related work. Section 3 gives a detailed overview of
the Sitemaps protocol, high level statistics on how it is being
used by users, and how it ﬁts in Google’s crawling architec-
ture. Section 4 details a case study of three prominent sites
having very diﬀerent characteristics. Section 5 introduces
metrics to evaluate the quality of Sitemaps data and uses
these metrics to evaluate a large data set of real world web
data. Finally Section 6 examines diﬀerent ways in which
Sitemaps data can be incorporated inside Web crawlers.

2. RELATED WORK

Brandman et al [7] discussed how to make web servers
more friendly to crawlers, including the idea of exporting a
list of URLs along with additional metadata to oﬀer an ef-
ﬁcient communication mechanism between the crawler and
the web servers. In 2006, Google, Yahoo and Microsoft an-
nounced support for Sitemaps as a common standard for
websites based on a simple list of URLs [15]. A good overview
of the protocol itself was described in [27] by Ranjan et. al.
Other XML-based protocols similar to Sitemaps include
RSS, Atom, OAI-PMH. RSS and Atom are popular XML-
based web protocols used by websites and blogs typically
used to give recent updates and surfaced in RSS readers such
as google.com/reader. The Sitemaps protocol was designed
to support large sites and is ﬂexible in supporting diﬀerential
updates. [18]. OAI is an older sophisticated protocol used
by digital libraries [20]. Google supported each of the above
protocols in submissions. However as we see later, the frac-
tion of URLs coming in through RSS/Atom were smaller and
virtually none come from OAI [16]. We believe RSS/Atom
will continue to be popular for consumer-friendly updates
and Sitemaps will become increasingly popular as a simple
and crawler-friendly update mechanism. OAI will continue
to be popular in the library community as a mechanism to
submit rich metadata.

The problem of exposing the deep web has been studied
extensively. Recently, in [21] Madhavan et al. discuss the
problem of increasing the visibility of web sites that have
content hidden behind forms and inside data bases. They
introduced techniques to automatically ﬁll web forms in or-
der to expose some of the URLs hidden inside the deep web
of the site. While these techniques are complementary to
Sitemaps, such techniques do not utilize a web servers knowl-
edge, nor oﬀer provable guarantees on coverage, freshness or
eﬃciency.

Extensive research has been done on the problem of ap-
proximating the rate of change of web content [10, 11, 9, 26],
and developing crawling algorithms that improve the fresh-

ness of documents. In [24] it was shown that approximating
the rate of change is not always possible. On the other hand,
if the rate of change is known, optimal crawling schedules
are known for various metrics [9, 29]. The Sitemaps proto-
col uses the web server’s knowledge to oﬀer an alternative to
approximating change rates and thus is complementary to
the optimal scheduling techniques that are based on change
rates.
In this paper, we examine the quality of the data
supplied through the Sitemaps protocol, and how it diﬀers
from that available through Discovery crawl.

Much has gone into examining the web through crawl and
through random walks [10, 13, 3]. Our paper oﬀers an al-
ternative view of the web through Sitemaps data.

3. SITEMAPS PROTOCOL

Sitemaps ﬁles are XML ﬁles with a list of URLs with ad-

ditional metadata, as shown in the example below.

<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns=

"http://www.sitemaps.org/schemas/sitemap/0.9">

<url>

<loc>http://www.example.com/</loc>
<lastmod>2005-01-01</lastmod>
<changefreq>monthly</changefreq>
<priority>0.8</priority>

</url>
<url>

...

<url>

</urlset>

Within each <url> record:

• loc is a required ﬁeld, representing the URL of the web

page.

• lastmod is an optional ﬁeld in the W3C Datetime for-
mat (e.g., YYYY-MM-DD and HH-MM-SS if neces-
sary) representing the last time the URL was known
to change.

• changefreq is an optional ﬁeld representing how fre-
quently the page is likely to change. Valid values in-
clude always, hourly, daily, weekly, monthly, never.

• priority is an optional ﬁeld representing the relative

importance of this URL in the web site.

The full speciﬁcation of the protocol is available at http:
//www.sitemaps.org and the protocol is extensible to sup-
port new types of metadata. Examples of Google’s exten-
sions are available at http://www.google.com/webmasters.
For large sites, the protocol supports SitemapIndex ﬁles.
Conceptually, these index ﬁles list Sitemaps ﬁles. This al-
lows websites to break large Sitemaps into smaller Sitemaps
(under 10MBs compressed) that are more convenient to down-
load over HTTP. This technique also allows incremental
Sitemaps, Sitemaps ﬁles that only include URLs that are
either new or modiﬁed in a given time period, allowing sites
to provide updates without the need to rewrite large ﬁles.
More details about the speciﬁcation and examples are avail-
able at http://www.sitemaps.org.

In order to inform search engines of new or updated Sitemaps

ﬁles, along with where these ﬁles can be found, websites can
use one of the following techniques:

• Robots.txt ﬁle: Sitemaps can be published in the robots.txt

ﬁle of a website using the “Sitemaps:” directive. For ex-

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 992Format
XML Sitemap
Unknown
Url List
Atom
RSS

Percentage
76.76
17.51
3.42
1.61
0.11

Table 1: Breakdown of URLs Published by Submis-
sion Format

ample, at http://cnn.com/robots.txt you see the fol-
lowing line: “Sitemaps: http://www.cnn.com/sitemaps.
xml.” Crawlers can then use this URL to download
the corresponding Sitemaps ﬁle that contains CNN’s
URLs.

• HTTP request: Search engines may provide ping URLs
that can be used to notify them of new or updated
Sitemaps. In general, a ping URL can take the follow-
ing form: http://searchengineURL/ping?sitemap=-
sitemapUrl

• SearchEngine Submission Tools: Search engines may
also provide tools and APIs to submit Sitemaps. Google
Webmaster Tools are an example of such tools and
APIs (http://www.google.com/webmasters).

3.1 Sitemaps Usage

We now discuss some high level statistics on usage of
Sitemaps published through websites’ robots.txt, at the time
of writing this paper (October 2008). Sitemaps are dis-
tributed across many countries and top-level-domains:

• Approximately 35M websites publish Sitemaps, and
give us metadata for several billions of URLs. The top
ten TLDs for these websites are .com, .net, .cn, .org,
.jp, .de, .cz, .ru, .uk, .nl. And a long tail of TLDs
comprising the last 5% of websites.

• In Table 1 we list the percentage of URLs published,
broken down by submission format. Notice that XML
Sitemaps are most of the URLs. Websites also list a
simple text ﬁle with URLs as a UrlList, and submit
RSS/Atom feeds if they already support one. The Un-
known format includes Sitemaps that are misformat-
ted.

• 58% of URLs include a lastmodiﬁcation date, 7% in-
clude a change frequency ﬁeld, and 61% include a pri-
ority ﬁeld.

3.2 Sitemaps At Google

In Figure 1 we show how Sitemaps are incorporated into

Google’s crawling pipeline.

• Discovery: Discovery starts with a seed set of URLs,

and maintains a prioritized queue of URLs to be crawled.
Periodically,
it dequeues the next set of URLs and
passes this candidate set of to-be-crawled URLs to the
SpamFilter component.

• Sitemaps: The Sitemaps component takes the set of
URLs submitted to Google using ping or published on
websites’ robots.txt, and passes it on to the SpamFilter
component.

• SpamFilter: The Spam ﬁlter detects and removes the
spam links from the set of links it receives [14] before
passing the clean set of links to the web crawler.

Figure 1: Google Search Main Pipeline Architecture

• WebCrawler: The web crawler receives the next set
of URLs to be crawled, issues HTTP requests to web-
sites and retrieve the associated page content. URLs
are then extracted from the content and passed to the
Discovery component. The web crawler also handles
the periodic refreshing of pages. [11, 8].

• Indexer/Server: The Indexer selects a subset of the
crawled pages according to various quality metrics and
builds an index of these pages. The speciﬁc metrics
used will not be discussed in this paper, nor are they
pertinent to the discussions in the rest of the paper.

• Server: This component serves uses the index to pro-

duce responses to users’ search queries.

4. SITEMAPS CASE STUDY

In this section we study a few speciﬁc sites to give the
reader a ﬂavor for how Sitemaps are used under diﬀerent
scenarios and to motivate some of the metrics we introduce
in a later section. These sites publish their Sitemaps URL
through the special “Sitemaps:” directive in the robots.txt
ﬁle.

The sites have diﬀerent characteristics, including: (a) How
often their pages change; (b) How many web pages they
have; (c) If they have URL canonicalization (or duplication)
issues, e.g., how many distinct URLs render the same con-
tent; (d) If they use Sitemaps to be exhaustive or to focus
on speciﬁc content (e.g., recent content). In Section 5, we
study these properties across an aggregate set of sites.

4.1 Amazon.com

Amazon is a popular commercial website with several tens
of millions of URLs. The site is built on a service-oriented
architecture, where each page is constructed on the ﬂy by
calling potentially hundreds of services [19]. Each of these
services renders one component in the page such as product
information, customer reviews or the cover photo of a book.
With such a large and dynamic site, ﬁnding new content and
keeping the content fresh is clearly a challenge.

Amazon.com also suﬀers from URL canonicalization is-
sues, multiple URLs reference identical or similar content.
For example, our Discovery crawl crawls both

• http://.../B000FEFEFW?showViewpoints=1, and
• http://.../B000FEFEFW?filterBy=addFiveStar.

The two URLs return identical content and oﬀer little value,
since these pages oﬀer two “diﬀerent” views of an empty
customer review list. Simple crawlers cannot detect these
type of duplicate URLs without downloading all duplicate
URLs ﬁrst, processing their content and wasting resources
in the process. Sophisticated crawlers may use predictive

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 993(a) cnn.com

(b) pubmedcentral.nih.gov

Figure 2: Histogram of new URLs over time

techniques [4] to avoid crawling some of these kinds of pages.
Sitemaps usage. At the time of writing this paper, Ama-
zon publishes 20M URLs listed in Sitemaps using the“Sitemaps”
directive in amazon.com/robots.txt. They use a SitemapIn-
dex ﬁle that lists about 10K Sitemaps ﬁles each ﬁle listing
between 20K and 50K URLs. The SitemapIndex ﬁle con-
tains the last modiﬁcation time for each underlying Sitemaps
ﬁle, based on when the Sitemaps ﬁle was created. When we
examined the last-modiﬁcation dates of each Sitemaps ﬁle
over the past year, we noticed that new Sitemaps ﬁles are
added every day and do not change after they are added.
Each new Sitemaps ﬁle is composed of recently added or re-
cently modiﬁed URLs. When we examined the Sitemaps, we
saw that Amazon is primarily listing single product pages.
Also, Amazon Canada and Amazon UK maintain Sitemaps
in a similar manner on their own domains.

Conceptually, Amazon is using a “log structured”approach
by appending new Sitemaps each day to a SitemapIndex.
This aids diﬀerent crawls to update their index at diﬀerent
frequencies and yet read a small number of ﬁles containing
only the new URLs. For example, crawlerA and crawlerB
could choose to crawl content from Amazon once a day and
once a week respectively, and would only have to examine
the last day or week’s worth of Sitemaps. Furthermore, pro-
viding a set of canonical URLs that provide good coverage of
Amazon’s products, helps the crawlers prioritize what URLs
to crawl as well as pick what URL is the canonical URL that
should be used in the search results.

In Section 5 we will introduce several metrics for evaluat-
ing the content. For now, consider a simple metric of eﬃ-
ciency in terms of URLs crawled vs unique pages crawled.
The Discovery crawl of amazon.com is 63% eﬃcient, mean-
ing for every 100 URLs that we crawl, we collect 63 unique
pages. The Sitemaps crawl was 86% eﬃcient. To reiterate,
this is a simpliﬁcation and we’ll study this in more detail in
Section 5.

4.2 Cnn.com

CNN.com is a much smaller site than amazon.com. The
site covers breaking news in addition to including other con-
tent such as weekly polls and columns. When we exam-

ined some of the URLs in CNN, we observed that they have
slightly diﬀerent canonicalization issues. For example, we
see the same content in web viewable format and in print-
able format, and the crawlers may not automatically know
which of these URLs to crawl and index.

In ﬁgure 2(a) we see the counts of new URLs over time
as seen by Sitemaps crawl and Discovery crawl in www.cnn.
com. This graph shows that the website is very active with
many new URLs being added daily. While the number of
URLs found by Discovery and Sitemaps are similar, it seems
that Sitemaps ﬁnds these URLs somewhat faster since the
number of URLs appears front-loaded. However, it is clear
that Google adjusts the rate at which Discovery crawls the
site according to the site’s high change rate.
Sitemaps organization.
CNN adopts a diﬀerent ap-
proach from Amazon in creating Sitemaps. CNN publishes
multiple Sitemaps, each with a small numbers of URLs. The
Sitemaps ﬁle labeled “news”, includes the URLs of pages
most recently changed or added. The news-speciﬁc Sitemaps
changes multiple times each day and has 200–400 URLs. In
addition, the weekly Sitemaps ﬁle has 2500–3000 URLs and
the “monthly” has 5000–10000 URLs. These lists do not
overlap but rather complete each other. We also noticed
that CNN publishes unique and canonical URLs for the ar-
ticles.
In addition, CNN publishes hub pages on various
topics, and these are covered in a SitemapIndex ﬁle consist-
ing of additional Sitemaps where the URLs appear organized
alphabetically rather chronologically. We expect CNN orga-
nized their Sitemaps in this fashion, because they are focused
on helping web crawlers ﬁnd the latest news and articles of
that month. Additionally, it is pretty cheap for them to re-
produce new Sitemaps periodically, since they have a small
number of URLs.

4.3 Pubmedcentral.nih.gov

The pubmedcentral.nih.gov site oﬀers a huge archive of
medical journal articles, the earliest journal appearing on the
site was published in 1809! The underlying content changes
infrequently once published, and new articles are added pe-
riodically. The site has some URL duplication issues. For
example, two URLs found through our Discovery crawl were

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 994• http://.../articlerender.fcgi?artid=1456828 and
• http://.../articlerender.fcgi?&pubmedid=16085696

are actually the exact same content. However, the URLs
look diﬀerent and no crawler could automatically “guess”
these URLs were the same content without downloading
them ﬁrst.

In Figure 2(b) we plot the counts of new URLs over time
as seen by Sitemaps crawl and Discovery crawl for this do-
main. Notice here that URLs seen by Sitemaps occur in
large bursts, likely due to when pubmed’s Sitemaps genera-
tor runs and when our crawler ﬁnds the new Sitemaps.

Sitemaps organization. Pubmed organizes its Sitemaps
diﬀerently from CNN and Amazon. A SitemapIndex points
at 50+ Sitemaps ﬁles, each with around 30K URLs. The
SitemapIndex ﬁle is modiﬁed each day. One of the Sitemaps
consists of about 500 entries for the Table of Contents of
journals. All Sitemaps ﬁles indicate that change-rate of
URLs is monthly. However, we noticed in general that some
of the last modiﬁed metadata was inaccurate unlike CNN
and Amazon. For example, some Sitemaps were listed as re-
cently updated but the underlying content had not changed.
In other cases, the publication date of the last volume was
newer than the last modiﬁcation date in the Sitemaps ﬁle.

5. EFFICACY OF SITEMAPS

In this section we explore the possible beneﬁts of Sitemaps
to its users, meaning its beneﬁt both to websites and to web
crawlers. While we present our techniques in the context
of domains, they are equally applicable to hosts or smaller
units of a website.

5.1 Coverage

The ﬁrst aspect we explore is the notion of coverage, in-
cluding (a) how much of a domain is captured in Discovery
and Sitemaps, and (b) how much of the “useful” part of a
domain is captured. We introduce some metrics for this
evaluation and then discuss our experimental results.

5.1.1 Metrics

We can deﬁne many possible coverage metrics based on
diﬀerent parameters, including page content and the link
structure. In [12] Cho et al. present a coverage metric based
on PageRank – how much of a domain’s overall PageRank
is captured in the crawled document collection. This class
of metrics builds on the acceptance of PageRank as a proxy
for quality. We adapt this metric to allow us to compare the
coverage achieved by Discovery and Sitemaps. We further
introduce additional coverage metrics speciﬁcally suited in
the context of search engines.

For this paper, we adopt a simple deﬁnition of unique-
ness based on Shingles [22]. Consider all k-grams in a doc-
ument. The similarity of two documents is then deﬁned as
the percent of k-grams shared between the documents. Two
documents are considered near-duplicates if the percent of
shared k-grams is above some threshold value. The Shingles
technique allows identifying such near-duplicates eﬃciently.
The Shingles are produced by hashing each of the k-grams
and choosing some small subset of these hashes. The Shin-
gles can then be compared directly rather than comparing
the documents themselves. For a study of some of Shingling
techniques and alternate techniques, please read [22].

Let us ﬁrst deﬁne a few key measures to motivate some

coverage metrics. Consider how URLs propagate through
the search engine pipeline we discussed in Figure 1. For
a domain, the URLs can be classiﬁed into the following
states. Conceptually, a URL is more “useful” as it progresses
through each of the following sequence of states:

1. Seen: The set of URLs seen by the web crawler.
2. Crawled: The set of URLs the web crawler fetched.
3. Unique: The set of URLs after eliminating duplicates.
Indexed: The set of URLs the Indexer included in the
4.
index.

5. Results: The set of URLs the Server showed to the user

in response to a query.

6. Clicked: The set of URLs the user clicked on in the

results page.

For domain D, we deﬁne the following coverage metrics:

Coverage(D) =

U niqueCoverage(D) =

IndexCoverage(D) =

CrawledSitemaps(D)

Crawled(D)

|U niqueSitemaps(D)|

|U nique(D)|

|IndexedSitemaps(D)|

|Indexed(D)|

P ageRankCoverage(D) = P P ageRankSitemaps(D)

P P ageRank(D)

(1)

(2)

(3)

(4)

Where UniqueSitemaps(D) refers to the unique URLs in Sitemaps
in Domain D, and Unique refers to unique URLs known to
either Discovery or Sitemaps in Domain D. We use similar
notation for Indexed URLs and PageRank for URLs.

One additional metric is not directly related to coverage

but is useful in evaluation the quality of either crawl:

SignalT oN oise(D) =

|U nique(D)|

|Seen(D)|

(5)

These metrics combined together give us insights into the
quality of URLs in Sitemaps. SignalToNoise evaluates the
fraction of URLs that are worth further consideration (for
Discovery and Sitemaps), while UniqueCoverage, IndexCov-
erage and PageRankCoverage compare the URLs from Sitemaps
compared to what we know globally about the domain.

5.1.2 Coverage on pubmedcentral.nih.gov

Conceptually, archival domains such as pubmedcentral.
nih.gov can be exhaustively covered using Sitemaps because
it is easy to generate a list of the important URLs of this
domain and the content of these pages changes at a low rate.
We evaluate this domain carefully to consider what types of
URLs are left out of Sitemaps.

• Seen and Crawled: Discovery and Sitemaps together
crawled approximately 3 Million URLs from this do-
main. Sitemaps only contained 1.7 million URLs.

• Duplicates: One million of above missed URLs were
found to be near-duplicates when applying the Shin-
gling techniques discussed earlier. By contrast, Sitemaps
had only 100 duplicate URLs.

• Missing content: We manually examined a random
sample of 3000 URLs of the 300K URLs that Sitemaps
“missed”. Table 2 outlines the examination results .

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 995Type of missed content
Errors - bad certiﬁcates and “404s”
Redirects
Dup content – extra CGI parameter
Dup content – unmatched URLs
Table of contents
Image dominated page
Subset of another page
Missing content

%age
8%
10%
3%
8%
4%
30%
17%
20%

Table 2: Classifying missing content from pubmed-
central.nih.gov

• First 8% of the URLs generate errors and report
bad certiﬁcates but do not return the right HTTP
error codes.

• The next three lines of the table account for redi-
rect URLs and duplicate URLs that were not iden-
tiﬁed as duplicates, and add up to 21% of the
missed URLs. We include redirect URLs in this
category, because a crawler has to spend work
units to crawl the redirecting URL ﬁrst. Also the
duplicate URLs that are not identiﬁed as dupli-
cates are due to false negatives from the Shingling
algorithm [22].

• 51% of the URLs have content where a judgement
call needs to be made about the usefulness of the
URLs – will the domain or search engines want
some of these pages indexed?

• The other 20% of content appear to be missing

content that Discovery found and was not in Sitemaps.

The main observation from this study is that for an archival
domain, Discovery is 63% “eﬃcient” and Sitemaps is 99% ef-
ﬁcient in crawling the domain at the cost of missing a small
fraction of the content. In the next sections, we evaluate the
quality based coverage metrics as an aggregate over a set of
domains.

5.1.3 Experimental Results

In this section we will ﬁrst introduce our results and follow
with our observations. The dataset we chose to examine in
this section consists of all the known URLs starting with the
preﬁx “am” and belonging to the top level domain “.com”,
and were crawled sometime in 2008. This dataset covers
about 500 million URLs. In Figure 3 we plot UniqueCover-
age vs DomainSize (# URLs crawled) on a log-log plot of
URLs seen both by Sitemaps and Discovery. The size (and
color) of the dot represents the number of URLs in the do-
main. A color version of the paper is available online in [30].
We use color primarily to make it easier to diﬀerentiate the
domains that are close to each other on the graph.

Figure 4 similarly shows UniqueCoverage vs Coverage.
For example, the big outlying (yellow) dot in the center
of the graph represents amitymama.com with 6.4 million
URLs, with Coverage = 0.55 and UniqueCoverage = 0.64.

Finally, Figure 5(a) and Figure 5(b) plot PageRankCov-
erage vs DomainSize and IndexCoverage vs UniqueCover-
age, which together give us insights into the quality of the
Sitemaps URLs.

Putting all of this together, the above graphs allow us to
identify and make the following patterns and observations:

Figure 3: Unique Coverage vs Domain Size (URLs
seen)

Figure 4: Sitemaps UniqueCoverage vs Coverage

• Duplicate URLs: Figure 4 indicates that the percent
of duplicates inside Sitemaps is mostly similar to the
overall percent of duplicates.

• Coverage: Figure 3 shows how the coverage of domains
are distributed by their size. At least 46% of the do-
mains have above 50% UniqueCoverage and above 12%
have above 90% UniqueCoverage. There is no appar-
ent correlation between the size of the domain and
UniqueCoverage.

• Quality: Figures 5(a) and 5(b), show us that according
to both IndexCoverage and PageRankCoverage some
domains have very high quality Sitemaps coverage while
others are spammier. For example, in Figure 5(b) we
see that most domains are above the diagonal. This
indicates that for most domains, Sitemaps achieves a
higher percent of URLs in the index with less unique
pages. That is, Sitemaps crawl attains a higher utility.

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 996(a) PageRankCoverage vs Domain Size

(b) IndexCoverage vs Unique Coverage

Figure 5: Quality of URLs seen through Discovery and Sitemaps

5.2 Freshness

First Seen by Sitemaps

The second metric we wish to examine is that of fresh-
ness. Are sites using Sitemaps more likely to be fresh? We
introduce some metrics we use and discuss our experimental
results.

yes

no

Source ping? URL Count
yes
no
yes
no

12.7%
80.3%
1.5%
5.5%

5.2.1 Metrics

Figure 7: First Seen and Ping URL Count

Freshness can be deﬁned in many ways. One useful mea-
sure is the diﬀerence between the content of a page saved
on the search engine side and the content currently being
served by the web server. This value can then be aggregated
in diﬀerent manners to produce a single freshness score [9,
25].

In this paper we mainly focus on a simpler deﬁnition –
what is the time diﬀerence between the ﬁrst time Discovery
sees a URL and the ﬁrst time Sitemaps sees it. The alterna-
tive freshness measures mentioned above are complementary
to this deﬁnition.

For this purpose we deﬁne the SeenDelta measure, for a

period of time T and a set of URLs D:

SeenDelta(DT ) =

F irstSeenDiscovery(u) − F irstSeenSitemaps(u)

(6)

X

u∈DT

Where DT is the set of URLs in domain D ﬁrst seen in T by
both Sitemaps and Discovery, F irstSeenSitemaps(u) is the
time Sitemaps ﬁrst saw URL u and F irstSeenDiscovery(u)
is the time Discovery ﬁrst saw URL u. In the next section
we use this metric to evaluate the freshness of URLs seen
through Discovery and Sitemaps.

5.2.2 Experimental Results

We tracked URLs found through Discovery crawl and Site-
maps over a period of 6 months.
In Figure 6(a), for the
URLs in the domain pubmedcentral.nih.gov which were
seen both by Sitemaps and Discovery, we plot the day Sitemaps
ﬁrst saw the URLs compared to the day Discovery ﬁrst saw
the URLs. Each dot represents a cluster of URLs that
happen to fall on the same point in the graph, and the
size of the dot represents the size of the cluster. We see

that in the domain pubmedcentral.nih.gov Discovery and
Sitemaps perform equally well for many URLs. However,
there are some URLs seen signiﬁcantly later by the Dis-
covery crawl than by the Sitemaps crawl. In an archival site
such as Pubmed, a less frequent Discovery crawl makes sense
because Sitemaps can achieve better freshness at a poten-
tially lower price. Similarly, in Figure 6(b) we see the same
graph for the cnn.com domain. Since CNN’s site is an im-
portant and dynamic one, Discovery likely crawls at a high
frequency and the diﬀerence is less obvious.

Next, we study which of the two crawl systems, Sitemaps
and Discovery, sees URLs ﬁrst. We conduct this test over a
dataset consisting of over ﬁve billion URLs that were seen
by both systems. According to the most recent statistics
at the time of the writing, 78% of these URLs were seen
by Sitemaps ﬁrst, compared to 22% that were seen through
Discovery ﬁrst.

Finally, we study how usage of ping aﬀects Sitemaps and
Discovery. The use of ping requires more eﬀort to setup than
the use of robots.txt. Thus it is not surprising that only
14.2% of URLs are submitted through ping. But as can be
seen in Figure 7, the probability of seeing a URL through
Sitemaps before seeing it through discovery is independent
of whether the Sitemaps was submitted using pings or using
robots.txt.

6. SEARCH ENGINE USE OF SITEMAPS

In previous sections, we got a ﬂavor for the tradeoﬀs and
metrics involving the (a) quality of URLs and some of the
metadata, (b) freshness impact of diﬀerent crawls. In this
section, we consolidate some of our learnings and propose

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 997(a) pubmedcentral.nih.gov

(b) cnn.com

Figure 6: Firstseen by Discovery vs Sitemaps

how to integrate Sitemaps into existing web crawlers to com-
plement Discovery and gain the best of both worlds.

6.1 Crawling Sitemaps Files

We now discuss techniques to fetch Sitemaps and period-

ically refresh them.

• Finding Sitemaps: As discussed earlier, crawlers sup-
port several methods that webmasters can use to in-
dicate that a Sitemaps ﬁle is available (e.g., through
robots.txt). This step is cheap, since crawlers already
need to crawl robots.txt periodically in order to keep
up with the changes webmasters make and processing
requests made through the ping URL interface is cheap
as well.

• Downloading Sitemaps: This step is inexpensive be-
cause the number of Sitemaps corresponds roughly to
the number of hosts (in the tens of millions), which
is small compared to the number of URLs a crawler
processes. Many Sitemaps are of course larger than
most web pages (up to 10 MBs), even after compres-
sion is used. However, in our experience, websites and
crawlers alike would rather avoid the overhead of addi-
tional HTTP connections even at the price of greater
transfer sizes (up to a few MBs).

• Refreshing Sitemaps: As discussed earlier, the protocol
supports web servers supplying metadata as to when
Sitemaps change. For example, when a SitemapIn-
dex ﬁle is provided, it may supply additional meta-
data about the last date in which a Sitemaps ﬁle has
changed.
If the web servers were accurate and hon-
est, crawlers could rely solely on this information to
keep track of the Sitemaps ﬁles. But as our experi-
ments indicate, the metadata is not yet consistently
trustworthy enough. Hence we should apply the same
techniques Discovery crawl uses to keep a web archive
fresh to refresh Sitemaps. For an example consider [9].

6.2 Canonical URL Selection

As we saw in the case studies in Section 4 and our discus-
sions earlier, near duplicates and diﬀerent URLs with very
similar content are a well known reality on the web. Crawlers
deal with this problem by (a) clustering pages that have sim-
ilar content using techniques described in Section 5.1, and
(b) choosing a canonical URL for each of these clusters. This
URL is used to track the cluster and is returned in search
results. Choosing the canonical URL for each cluster is a
non-trivial problem that has not been extensively discussed
in the research community.

A possible reason for this is that there is no clear cut an-
swer. For example, the URL http://borders.com redirects
to the URL http://www.borders.com/online/store/Home.
Another example, the URL http://en.wikipedia.org/w/
index.php?title=Sitemaps&printable=yes points at a print-
able version of this URL http://en.wikipedia.org/wiki/
Sitemaps. As these examples may hint, a commonly used
good heuristic is to use the shorter URL as the canonical
URL, or the URL with the highest PageRank. An alterna-
tive is to use Sitemaps to give the power to webmasters, and
choose the URL listed in a Sitemap ﬁle if one is available as
the canonical URL.

6.3 Combining Sitemaps Feeds in Crawl Or-

der

Both web sites and web crawlers have limited resources
and crawlers need to prioritize which URLs to crawl. Typ-
ically, separate policies are used for the crawl of new URLs
and for the crawl that maintains the freshness of the cur-
rent document collection. In this section, we discuss how a
search engine crawler can work in the URLs seen through
Sitemaps with its existing crawling policies.

6.3.1 New Content

Signiﬁcant research has gone into studying the order in
which URLs should be crawled [12, 2, 23]. For example,
in [12] Cho et al. propose the RankMass crawling algorithm.

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 998Algorithm 6.1.

2 Ncrawl
2 Ncrawl

2 Ncrawl

SeenDiscovery(D)/CrawledDiscovery(D)

SeenSitemaps(D) / CrawledSitemaps(D)

sort(CrawlQSitemaps, SitemapScore)
sort(CrawlQDiscovery, DiscoveryScore)
if (utilitySitemaps < utilityDiscovery):

Function CrawlScheduleForDomain(Ncrawl,φ, D)
1: kS = 1
2: kD = 1
3: ∆ = φ ∗ 1
4: utilitySitemaps = utilityDiscovery = 0
5: for epoch in 0..∞ do
: Get Set of URLs Seen but Not Crawled in D
6: CrawlQSitemaps =
:
7: CrawlQDiscovery =
:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: Crawl additional URLs by priority if Quota permits
19: utilitySitemaps = utility(topKS, ∆)
20: utilityDiscovery = utility(topKD, ∆)
21: ∆ = φ ∗ min(|kS|, |kD|)
22: Update Crawled and Seen sets.

topKS = Crawl top kS of CrawlQSitemaps
topKD = Crawl top kD of CrawlQDiscovery

kD = kD + ∆
kS = kS − ∆

else:

kS = kS + ∆
kD = kD − ∆

Figure 8: Combined Crawling Algorithm Sitemaps
and Discovery

This algorithm produces a URL ordering for Discovery using
an approximation of the Personalized PageRank of pages yet
to be downloaded. In the case of Sitemaps URLs, however,
some pages may not have any incoming links despite being
“quality” pages.
In Figure 5(b), we saw that the quality
of pages seen through Discovery and Sitemaps varies from
domain to domain. For domains above the diagonal, it would
make sense to allocate more resources for Sitemaps URLs.
In this section we discuss how Sitemaps URLs should be
integrated into existing URL ordering policies, including (a)
How to order Sitemaps URLs, and (b) How to integrate
Sitemaps ordering with Discovery ordering?

Sitemaps Crawl Order. We wish to specify a priority
function, SitemapScore, which assigns each URL in the do-
main’s Sitemaps a score that indicates the importance of
crawling it relative to the other URLs in the domain. Given
a per-domain quota on the number of URLs we can crawl, we
can then simply choose the URLs with the highest scores for
crawl. We suggest several possible deﬁnitions for Sitemap-
Score:

• Priority: Set SitemapScore equal to the priorities sup-
plied by webmasters, and in the case priorities are not
supplied, all URLs can be assumed to have equal pri-
ority. This ranking system operates under the assump-
tion that the web master knows best.

• PageRank: Set SitemapScore equal to the PageRank of
the page, or to zero if no incoming links exist. This sys-
tem will give good results in domains that have good

PageRankCoverage, as we saw in Figure 5(a). How-
ever, it would not enable leveraging Sitemaps to crawl
new quality pages before the ﬁrst incoming link is seen.
• PriorityRank: A hybrid approach which combines the
two signals. The root of the domain is assumed to con-
tain an implicit link to the Sitemaps ﬁle. In addition,
the Sitemaps ﬁle is assumed to contain links to all of
its URLs, either uniformly weighted or weighted by
priority if this ﬁeld is provided. Over this augmented
graph, PageRank can be calculated for every URL in
the Sitemaps page. We call this PageRank score Pri-
orityRank. PriorityRank combines the two previous
scoring methods, taking the web masters’ preferences
into account and boosting the score of pages that al-
ready have incoming links from high ranking pages.
The idea of adding “virtual” links to allow PageRank
to be calculated is not new [6]. For example, it is used
to deal with dangling pages, pages with no outgoing
links.

Next we will present how to use these scoring functions

together with an existing crawling policy.

Prioritizing Between Discovery URLs and Sitemaps
URLs. Given two scoring functions, SitemapScore (like
above) and DiscoveryScore, we wish to choose some subset
of the uncrawled URLs to crawl next. For this discussion, we
assume that each domain is restricted by a quota, Ncrawl,
which sets the limit on the number of URLs that can be
fetched in a given time period (e.g. day, week, etc.). We
wish to use the scoring functions to choose the top kD of
the Discovery URLs and the top kS of the Sitemaps URLs
such that kS + kD ≤ Ncrawl. The question is, how do we
balance between kS and kD, given that that the scoring
functions are diﬀerent. We draw from Figure 5(b) a utility
metric to balance between these two queues. Speciﬁcally,
we propose the value IndexCoverage
U niqueCoverage which represents the
percent of URLs that make it to the index out of the total
number of URLs crawled.

Figure 8 presents an algorithm that uses this utility metric
to iteratively converge on the right balance between the two
URL sources for each domain. Conceptually, the utility of a
sample of the lowest priority URLs crawled in the last time-
period will determine how to adjust the balance for the next
one. Speciﬁcally,

• In Figure 8, lines 1-4 set the initial values of the kS, kD,
utility and ∆ parameters. The ∆ parameter controls
the size of the sample of URLs the utility is measured
over as well as the size of the adjustment to kS and kD.
The value φ controls how fast the balance is adjusted
and can be set through empirical evaluation (e.g., 0.1
is a slow converging default1). Lines 6-9 set and sort
the crawl queues for the current time period (epoch).
According to the previous epoch’s utility statistics, the
number of URLs to crawl from each queue are either
incremented or decremented by ∆ in lines 10-15. The
top kS and kD URLs are crawled from the respective
queues in lines 16-17 and additional URLs are crawled
if quota permits (line 18). The utility statistics are up-
dated in lines 19-20 using the utility function explained

1If a crawler retains additional histograms or has special
knowledge about a domain, φ can be optimized based on the
derivatives for functions SitemapScore and DiscoveryScore
using classical numerical techniques.

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 999shortly. Finally, the size of ∆, the Crawled set and the
Seen set are all updated according to the latest crawl
(lines 19-22).

• The utility function performs the following calcula-
tion. A subset of the list of URLs passed as the ﬁrst
parameter, topK, is chosen such that it includes the ∆
URLs with the lowest priority. The function returns
the number of URLs in this subset that were added
to the search engine index, divided by the size of the
subset, ∆. This function measures the expected incre-
mental contribution of adding resources to each of the
sources, that is, the derivative of the number of URLs
indexed by URLs downloaded. In this manner, good
balance can be found for domains depending on their
speciﬁc characteristics (Figure 5(b)).

6.3.2 Refresh Crawl

The purpose of a typical Refresh Crawl process is simple,
for a limited amount of resources, re-crawl the pages in your
document collection to achieve maximal freshness. Part of
this process typically requires, whether directly or indirectly,
approximating the rate at which the content changes.

In the case of the Refresh Crawl policy, integrating the
information provided by the Sitemaps feeds can simply be
done by using the “last modiﬁed date” meta data instead of
the approximate date. One caveat of this approach is that
the accuracy of this ﬁeld varies from site to site.
In this
case, using the historical data to approximate the accuracy
of this metadata can be used to decide which source is more
accurate, the history of crawling the URLs or the metadata
supplied in the Sitemaps.

7. CONCLUSIONS

In this paper we provided insight on how users use Sitemaps
today. We deﬁned metrics for evaluating the quality of
Sitemaps data and performed a large scale experimental
evaluation over real web data. Finally, we presented sev-
eral techniques of making use of Sitemaps data inside search
engines including a crawling algorithm that ﬁnds the right
balance between Discovery crawl and Sitemaps crawl.

8. REFERENCES

[1] H.T. Lee, D. Leonard, X. Wang, and D. Loguinov IRLBot:

Scaling to 6 billion pages and beyond. In Proc. 17th
WWW, 2008.

[2] R. Baeza-Yates, C. Castillo, M. Marin, and A. Rodriguez.

Crawling a country: Better strategies than breadth-ﬁrst for
Web page ordering. In Proc. 14th WWW, pages 864–872,
2005.

[3] Z. Bar-Yossef and M. Gurevich. Random sampling from a

search engine’s index. In Proc. 15th WWW, pages 367–376,
2006.

[4] Z. Bar-Yossef, I. Keidar and U. Schonfeld Do not crawl in
the DUST: diﬀerent URLs with similar text. In Proc.16th
WWW, pages 111–120, 2007.

[5] M.K. Bergman. The Deep Web: Surfacing hidden value.

Journal of Electronic Publishing, 7(1):07–01, 2001.

[6] M. Bianchini, M. Gori, and F. Scarselli. Inside PageRank.

ACM Transactions on Internet Technology (TOIT),
5(1):92–128, 2005.

[7] O. Brandman, J. Cho, H. Garcia-Molina, and

N. Shivakumar. Crawler-friendly Web servers. ACM
SIGMETRICS Performance Evaluation Review,
28(2):9–14, 2000.

[8] S. Brin and L. Page. The anatomy of a large-scale

hypertextual Web search engine. Computer Networks and
ISDN Systems, 30(1-7):107–117, 1998.

[9] J. Cho and H. Garcia-Molina. Synchronizing a database to
improve freshness. ACM SIGMOD Record, 29(2):117–128,
2000.

[10] J. Cho and H. Garcia-Molina. The evolution of the Web

and implications for an incremental crawler. In Proc. 26th
VLDB, pages 200–209, 2000.

[11] J. Cho and H. Garcia-Molina. Eﬀective page refresh policies
for Web crawlers. ACM Transactions on Database Systems
(TODS), 28(4):390–426, 2003.

[12] J. Cho and U. Schonfeld. RankMass Crawler: A crawler

with high PageRank coverage guarantee. In Proc. 33rd
VLDB, volume 7, pages 23–28.

[13] D. Fetterly, M. Manasse, M. Najork, and J.L. Wiener. A
large-scale study of the evolution of Web pages. Software
Practice and Experience, 34(2):213–237, 2004.

[14] D. Fetterly, M. Manasse, and M. Najork. Spam, damn
spam, and statistics: using statistical analysis to locate
spam Web pages. In Proc. 7th WebDB, pages 1–6, 2004.

[15] Google. Joint support for the Sitemap protocol. Available

online at: http://googlewebmastercentral.blogspot.com/
2006/11/joint-support-for-sitemap-protocol.html,
2006.

[16] Google. Retiring support for OAI. Available online at:

http://googlewebmastercentral.blogspot.com/2008/04/
retiring-support-for-oai-pmh-in.html, 2008.

[17] Google. We knew the web was big... Available online at:

http://googleblog.blogspot.com/2008/07/
we-knew-web-was-big.html, 2008.

[18] Microsoft Google, Yahoo. Sitemaps.org. Available online at:

http://sitemaps.org, 2008.

[19] J. Gray. A conversation with Werner Vogels. Available

online at: http://www.acmqueue.com/modules.php?name=
Content&pa=showpage&pid=388, 2006.

[20] Open Archive Initiative. Open archive. Available online at:

http://www.openarchives.org, 2008.

[21] J. Madhavan, D. Ko, L. Kot, V. Ganapathy, A. Rasmussen,

and A. Halevy. Google’s Deep Web crawl. In Proc. 34th
VLDB, 2008.

[22] G.S. Manku, A. Jain, and A.D. Sarma. Detecting

near-duplicates for Web crawling. In Proc. 16th WWW,
pages 141–150, 2007.

[23] M. Najork and J.L. Wiener. Breadth-ﬁrst crawling yields
high-quality pages. In Proc. 10th WWW, pages 114–118,
2001.

[24] A. Ntoulas, J. Cho, and C. Olston. What’s new on the
Web?: The evolution of the Web from a search engine
perspective. In Proc. 13th WWW, pages 1–12, 2004.

[25] C. Olston and S. Pandey. Recrawl scheduling based on

information longevity. 2008.

[26] S. Pandey and C. Olston. User-centric Web crawling. In

Proc. 14th WWW, pages 401–411, 2005.

[27] P. Ranjan and N. Shivakumar. Sitemaps: A content

discovery protocol for the Web. In Proc. 17th WWW, 2008.

[28] Reuters. Google, 4 states partner on government info
search. Available online at: http://www.reuters.com/
article/domesticNews/idUSN2946293620070430?sp=true,
2007.

[29] J.L. Wolf, M.S. Squillante, P.S. Yu, J. Sethuraman,

L. Ozsen, and L. Ozsen. Optimal crawling strategies for
Web search engines. In Proc. 11th WWW, pages 136–147,
2002.

[30] U. Schonfeld and N. Shivakumar. Sitemaps: Above and

beyond the crawl of duty. In Proc. 18th WWW, 2009.
Available online at: http:
//www.shuri.org/publications/www2009_sitemaps.pdf,
2009.

WWW 2009 MADRID!Track: XML and Web Data / Session: XML Extraction and Crawling 1000