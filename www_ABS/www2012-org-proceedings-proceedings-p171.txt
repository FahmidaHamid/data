search engines are an essential component of the web, but their web crawling agents can impose a signi cant burden on heavily loaded web servers. unfortunately, blocking or deferring web crawler requests is not a viable solution due to economic consequences. we conduct a quantitative measurement study on the impact and cost of web crawling agents, seeking optimization points for this class of request. based on our measurements, we present a practical caching approach for mitigating search engine overload, and implement the two-level cache scheme on a very busy web server. our experimental results show that the proposed caching framework can e ectively reduce the impact of search engine overload on service quality.
