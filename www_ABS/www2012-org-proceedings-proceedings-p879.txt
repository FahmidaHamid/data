latent dirichlet allocation (lda) is a popular topic modeling technique for exploring document collections. because of the increasing prevalence of large datasets, there is a need to improve the scal-ability of inference for lda. in this paper, we introduce a novel and  exible large scale topic modeling package in mapreduce (mr. lda). as opposed to other techniques which use gibbs sampling, our proposed framework uses variational inference, which easily  ts into a distributed environment. more importantly, this variational implementation, unlike highly tuned and specialized implementations based on gibbs sampling, is easily extensible. we demonstrate two extensions of the models possible with this scalable framework informed priors to guide topic discovery and extracting topics from a multilingual corpus. we compare the scalability of mr. lda against mahout, an existing large scale topic modeling package. mr. lda outperforms mahout both in execution speed and held-out likelihood.
