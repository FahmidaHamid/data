fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval, nearest neighbor classi cation and nearest neighbor regression. recently a number of machine learning algorithms have been proposed for representing the data to be searched as (short) bit vectors and then using hashing to do rapid search. these algorithms have been limited in their applicability in that they are suited for only one type of task   e.g. spectral hashing learns bit vector representations for retrieval, but not say, classi cation. in this paper we present a uni ed approach to learning bit vector representations for many applications that use nearest neighbor search. the main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand. this broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval. we propose a learning-to-rank formulation to learn the bit vector representation of the data. lambdarank algorithm is used for learning a function that computes a task-speci c bit vector from an input data vector. our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classi cation and retrieval datasets. it is scalable and learns a 32-bit representation on 1.46 million training cases in two days.
