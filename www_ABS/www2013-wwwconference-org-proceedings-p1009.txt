a frequent problem when dealing with data gathered from multiple sources on the web (ranging from booksellers to wikipedia pages to stock analyst predictions) is that these sources disagree, and we must decide which of their (often mutually exclusive) claims we should accept. current state-of-the-art information credibility algorithms known as  fact nders  are transitive voting systems with rules specifying how votes iteratively  ow from sources to claims and then back to sources. while this is quite tractable and often e ec-tive, fact nders also su er from substantial limitations; in particular, a lack of transparency obfuscates their credibility decisions and makes them di cult to adapt and analyze knowing the mechanics of how votes are calculated does not readily tell us what those votes mean, and  nding, for example, that a source has a score of 6 is not informative. we introduce a new approach to information credibility, latent credibility analysis (lca), constructing strongly principled, probabilistic models where the truth of each claim is a latent variable and the credibility of a source is captured by a set of model parameters. this gives lca models clear semantics and modularity that make extending them to capture additional observed and latent credibility factors straightforward. experiments over four real-world datasets demonstrate that lca models can outperform the best fact nders in both unsupervised and semi-supervised settings.
