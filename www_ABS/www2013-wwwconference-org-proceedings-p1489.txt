topic models have shown great promise in discovering latent semantic structures from complex data corpora, ranging from text documents and web news articles to images, videos, and even biological data. in order to deal with massive data collections and dynamic text streams, probabilistic online topic models such as online latent dirichlet allocation (olda) have recently been developed. however, due to normalization constraints, olda can be ine ective in controlling the sparsity of discovered representations, a desirable property for learning interpretable semantic patterns, especially when the total number of topics is large. in contrast, sparse topical coding (stc) has been successfully introduced as a non-probabilistic topic model for e ectively discovering sparse latent patterns by using sparsity-inducing regularization. but, unfortunately stc cannot scale to very large datasets or deal with online text streams, partly due to its batch learning procedure. in this paper, we present a sparse online topic model, which directly controls the sparsity of latent semantic patterns by imposing sparsity-inducing regularization and learns the topical dictionary by an online algorithm. the online algorithm is e cient and guaranteed to converge. extensive empirical results of the sparse online topic model as well as its collapsed and supervised extensions on a large-scale wikipedia dataset and the medium-sized 20newsgroups dataset demonstrate appealing performance.
