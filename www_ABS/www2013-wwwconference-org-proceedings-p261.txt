the development of solutions to scale the extraction of data from web sources is still a challenging issue. high accuracy can be achieved by supervised approaches but the costs of training data, i.e., annotations over a set of sample pages, limit their scalability. crowd sourcing platforms are making the manual annotation process more a ordable. however, the tasks demanded to these platforms should be extremely simple, to be performed by non-expert people, and their number should be minimized, to contain the costs. we introduce a framework to support a supervised wrapper inference system with training data generated by the crowd. training data are labeled values generated by means of membership queries, the simplest form of queries, posed to the crowd. we show that the costs of producing the training data are strongly a ected by the expressiveness of the wrapper formalism and by the choice of the training set. traditional supervised wrapper inference approaches use a statically de-(cid12)ned formalism, assuming it is able to express the wrapper. conversely, we present an inference algorithm that dynamically chooses the expressiveness of the wrapper formalism and actively selects the training set, while minimizing the number of membership queries to the crowd. we report the results of experiments on real web sources to con(cid12)rm the e ectiveness and the feasibility of the approach.
