a considerable portion of web content, from wikis to collaboratively edited documents, to code posted online, is revi-sioned. we consider the problem of attributing authorship to such revisioned content, and we develop scalable attribution algorithms that can be applied to very large bodies of revisioned content, such as the english wikipedia. since content can be deleted, only to be later reinserted, we introduce a notion of authorship that requires comparing each new revision with the entire set of past revisions. for each portion of content in the newest revision, we search the entire history for content matches that are statistically unlikely to occur spontaneously, thus denoting common origin. we use these matches to compute the earliest possible attribution of each word (or each token) of the new content. we show that this  earliest plausible attribution  can be computed e ciently via compact summaries of the past revision history. this leads to an algorithm that runs in time proportional to the sum of the size of the most recent revision, and the total amount of change (edit work) in the revision history. this amount of change is typically much smaller than the total size of all past revisions. the resulting algorithm can scale to very large repositories of revisioned content, as we show via experimental data over the english wikipedia.
