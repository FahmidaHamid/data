the enormity and rapid growth of the web-graph forces quantities such as its pagerank to be computed under missing information consisting of outlinks of pages that have not yet been crawled. this paper examines the role played by the size and distribution of this missing data in determining the accuracy of the computed pagerank, focusing on questions such as (i) the accuracy of pageranks under missing information, (ii) the size at which a crawl process may be aborted while still ensuring reasonable accuracy of pager-anks, and (iii) algorithms to estimate pageranks under such missing information. the (cid12)rst couple of questions are addressed on the basis of certain simple bounds relating the expected distance between the true and computed pager-anks and the size of the missing data. the third question is explored by devising algorithms to predict the pageranks when full information is not available. a key feature of the \dangling link estimation" and \clustered link estimation" algorithms proposed is that, they do not need to run the pagerank iteration afresh once the outlinks have been estimated.
