large web search engines have to answer thousands of queries per second with interactive response times. due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. to keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scala-bility. in particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. we propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. this intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. we propose and study several of ine and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. our experimental evaluation based on a large web crawl and real search engine query log shows signi cant performance gains for the best schemes, both in isolation and in combination with the other caching levels. we also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.
