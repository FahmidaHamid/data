we provide a novel method of evaluating search results, which allows us to combine existing editorial judgments with the relevance estimates generated by click-based user browsing models. there are evaluation methods in the literature that use clicks and editorial judgments together, but our approach is novel in the sense that it allows us to predict the impact of unseen search models without online tests to collect clicks and without requesting new editorial data, since we are only reusing existing editorial data, and clicks observed for previous result set con gurations. since the user browsing model and the preexisting editorial data cannot provide relevance estimates for all documents for the selected set of queries, one important challenge is to obtain this performance estimation where there are a lot of ranked documents with missing relevance values. we introduce a query and rank based smoothing to overcome this problem. we show that a hybrid of these smoothing techniques performs better than both query and position based smoothing, and despite the high percentage of missing judgments, the resulting method is signi cantly correlated (0.74) with dcg values evaluated using fully judged datasets, and approaches inter-annotator agreement. we show that previously published techniques, applicable to frequent queries, degrade when applied to a random sample of queries, with a correlation of only 0.29. while our experiments focus on evaluation using dcg, our method is also applicable to other commonly used metrics.
