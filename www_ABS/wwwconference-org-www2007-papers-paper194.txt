we consider the problem of dust di erent urls with similar text. such duplicate urls are prevalent in web sites, as web server software often uses aliases and redirections, and dynamically generates the same page from various di erent url requests. we present a novel algorithm, dustbuster, for uncovering dust; that is, for discovering rules that transform a given url to others that are likely to have similar content. dustbuster mines dust e ectively from previous crawl logs or web server logs, without examining page contents. verifying these rules via sampling requires fetching few actual web pages. search engines can bene t from information about dust to increase the e ectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as pagerank.
