previous studies have highlighted the high arrival rate of new content on the web. we study the extent to which this new content can be e ciently discovered by a crawler. our study has two parts. first, we study the inherent di culty of the discovery problem using a maximum cover formulation, under an assumption of perfect estimates of likely sources of links to new content. second, we relax this assumption and study a more realistic setting in which algorithms must use historical statistics to estimate which pages are most likely to yield links to new content. we recommend a simple algorithm that performs comparably to all approaches we consider. we measure the overhead of discovering new content, de- ned as the average number of fetches required to discover one new page. we show  rst that with perfect foreknowledge of where to explore for links to new content, it is possible to discover 90% of all new content with under 3% overhead, and 100% of new content with 9% overhead. but actual algorithms, which do not have access to perfect foreknowledge, face a more di cult task one quarter of new content is simply not amenable to e cient discovery. of the remaining three quarters, 80% of new content during a given week may be discovered with 160% overhead if content is recrawled fully on a monthly basis.
