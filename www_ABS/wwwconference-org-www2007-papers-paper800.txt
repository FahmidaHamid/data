current web search engines focus on searching only the most recent snapshot of the web. in some cases, however, it would be desirable to search over collections that include many different crawls and versions of each page. one important example of such a collection is the internet archive, though there are many others. since the data size of such an archive is multiple times that of a single snapshot, this presents us with signi cant performance challenges. current engines use various techniques for index compression and optimized query execution, but these techniques do not exploit the signi cant similarities between different versions of a page, or between different pages. in this paper, we propose a general framework for indexing and query processing of archival collections and, more generally, any collections with a suf cient amount of redundancy. our approach results in signi cant reductions in index size and query processing costs on such collections, and it is orthogonal to and can be combined with the existing techniques. it also supports highly ef cient updates, both locally and over a network. within this framework, we describe and evaluate different implementations that trade off index size versus cpu cost and other factors, and discuss applications ranging from archival web search to local search of web sites, email archives, or  le systems. we present experimental results based on search engine query log and a large collection consisting of multiple crawls.
