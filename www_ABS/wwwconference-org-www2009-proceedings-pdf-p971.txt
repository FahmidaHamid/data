much  of  the  information  on  the  web  is  found  in  articles  from online news outlets, magazines, encyclopedias, review collections, and  other  sources.    however,  extracting  this  content  from  the original  html  document  is  complicated  by  the  large  amount  of less typically  unrelated  material  such  as navigation  menus,  forms,  user  comments,  and  ads.    existing approaches tend to be either brittle and demand significant expert knowledge  and  time  (manual  or  tool-assisted  generation  of  rules or  code),  necessitate  labeled  examples  for  every  different  page structure  to  be  processed  (wrapper  induction),  require  relatively uniform  layout  (template  detection),  or,  as  with  visual  page segmentation  (vips),  are  computationally  expensive.   we introduce  maximum  subsequence  segmentation,  a  method  of global optimization over token-level local classifiers, and apply it to  the  domain  of  news  websites.    training  examples  are  easy  to obtain,  both  learning  and  prediction  are  linear  time,  and  results are excellent (our semi-supervised algorithm yields an overall f1-score of 97.947%), surpassing even those produced by vips with a hypothetical perfect block-selection heuristic.  we also evaluate against  the  recent  cleaneval  shared  task  with  surprisingly  good cross-task performance cleaning general web pages, exceeding the top   text-only   score  (based  on  levenshtein  distance),  87.8% versus 84.1%.
