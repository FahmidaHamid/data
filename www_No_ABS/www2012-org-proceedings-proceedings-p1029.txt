To evaluate users  engagement or satisfaction from a new service, feature, or product, providers usually perform bucket testing (also known as split testing, A/B testing, or 0/1 testing).
In particular, providers often choose a small subset of users to which a new service is o ered.
Based on these users  behavior, the overall satisfaction of users can be estimated.
If the satisfaction is high enough, the service is released to the entire user population.
For example, web page layouts signi cantly impacts users  engagement.
If a new page layout is considered, bucket testing is used to verify that the new layout is better than the existing one.
This example also demonstrates why the number of users exposed to the new layout should be considered carefully.
On one hand, it should be large enough so that the measurements are statistically valid.
On the other hand, it should be as small as possible to limit the potential damage in case the new page layout is  bad , i.e., to mitigate the number of users who  su ered  from the experiment in case the new layout is less successful than the existing one.
Accordingly, each bucket test is therefore given a budget, B, which represents the maximal number of users it is allowed to e ect.
In traditional bucket testing, users  individual interactions depend only on the user and the feature, in which case uniform sampling is optimal.
In social networks and services, however, the situation is more complex since users  satisfaction might depend on whether the service is also available to their friends.
For example, a messaging service may inherently be very useful but no user can enjoy it if none of his friends use it.
Thus, to measure users  engagement the service must be o ered to at least some of their friends as well.
Other examples include, content tagging, games, certain kinds of ads, event invitations, etc.
In [1], the authors suggest to set a parameter d >0 such that users  interactions can be measured only if at least d of their friends have also received the service.
We adopt this model as well.
Let the graph G(V, E) represent a social network, where each node corresponds to a user and {i, j}   E i  users i and j are connected (or  friends ).
We denote |V | = n and |E| = m. As in [1], we assume, for simplicity, that all the nodes in G have at least d neighbors.
Let f : V   [0, 1] be an arbitrary function over the users measuring the user s engagement or satisfaction.
The aim of the test is to estimate the mean value   = 1 i V fi.
For the algorithm to evaluate f on a core set of nodes A, it must pay |B| from n its budget B.
The set B   A is the d-closure (or fringe) set of A, which is the minimal set containing A such that all nodes in A have at least d neighbors in B.
(cid:2) two contradicting phenomena.
If set A is chosen uniformly at random, the measurement is unbiased but the fringe set B is expected to be roughly d times larger than A.
In other words, a large portion of the budget is squandered away on nodes for which the function f cannot be evaluated.
This results in a small number of evaluations, which increases the estimator variance.
On the other hand, if set A is chosen to contain a heavily connected subset of nodes, the fringe set B is not much larger than A and most of the budget can be used.
This, however, requires non uniform sampling, which increases the estimator variance.
The challenge is to balance between these e ects in order to minimize the variance, while not breaking the budged limitation.
An important di erence between our work and [1] is the restrictions put on the satisfaction function.
In [1] a random biased coin model generates the function f : V   {0, 1}.
While this model makes sense and indeed is very useful for analysis, we argue that it is restrictive.
The  rst generalization we propose here is that the function receives real values f : V   [0, 1].
Our rationale is that in many cases, measures of satisfaction take scalar values.
These include: the amount of time spent using an application, number of times it was launched, increase in activity or in resulting revenues, etc.
Moreover, a random model for f is not always justi ed.
Here, we consider any  xed or even adversarially chosen satisfaction function.
One can easily verify that the variance of estimating the mean of a random function f is always dominated by the variance obtained (over the random bits of the algorithm) for an adversarial choice of f .
Thus, our results hold for any model describing f , including the one used in [1].
Second, the authors of [1] suggest to choose the core set A according to di erent random walk procedures.
While this gives good results, we show that, in fact, there exist a virtually unlimited number of valid algorithms.1 We argue that any distribution over core subsets of nodes can be used.
If the probability of any node being in the sample is strictly positive, the obtained estimator Z for   is also unbiased.
The only di erence between di erent distributions is the variance this estimator exhibits.
Thus, for every network, one should choose the distribution which minimizes the estimator variance.
Since one must choose an algorithm before starting the bucket test, we bounds the variance from above for any f .
The rest of this work is organized as follows.
In Section 2 we present the meta algorithm which applies the speci c algorithms presented in Section 3.
Connections of our concept to the random walk approach of [1] is considered in Section
 duce the estimator variance is brie y described in Section 5.
Experimental setup and corresponding results are described in Section 6.
We discuss the results and conclude the work in Section 7.
We start by describing the meta-algorithm, which is identical for all distributions over core sets.
The meta algorithm is straightforward and is identical to the one in [1].
Accordingly, a given algorithm produces core sets from some
 core sets including single nodes and pairs of nodes.
distribution over subsets of graph nodes.
Then, for each selected core set it produces an unbiased estimator.
It continues to do so until it exhausts the budget B.
The output estimator is the mean of all estimators obtained during the process.
A formal description of the meta-algorithm is given in Algorithm 1.
Algorithm 1 Network Bucket Testing Meta-Algorithm Input: B, budget input: d, minimal degree threshold Input: q, such that q(i)   (cid:2) Input: G(V, E), input graph Input: Q, core set distribution over 2V Output: Y , estimation of   = 1 b   0 ; (cid:2)   1 ; L   1 n while True do (cid:2) A supp(Q) Q(A)(cid:0){i A} i V f (i) A(cid:2)   drawn according to core set distribution Q B(cid:2)   d-closure (fringe) set of A b   b + |B| if b > B then Z(cid:2)   (cid:2) i A end while return: Y   1 nq(i) f (i) ; L   (cid:2) ; (cid:2)   (cid:2) + 1 (cid:2) end if break

 (cid:2)=1 Z(cid:2)
 The  nal estimator Y is unbiased since each of the Z estimators are:
 = = (cid:3) f (i) q(i)
 n A supp(Q) (cid:3) (cid:3) i V i V
 n fi =   , (cid:3) i A
     (cid:3)
 nq(i) f (i) Q(A)(cid:0){i A} A supp(Q)     (1) where Q is the core set distribution over 2V , and q(i) denotes the probability that node i is in the chosen core set (for more details see Algorithm 1).
Note that if q(i) = 0, for some i, expression (1) is not valid.
Since the overall estimator Y is unbiased for any distribution, the goal is to reduce its variance.
Given the fact that core sets A(cid:2) are chosen i.i.d.
we have that Var (Y ) = Var (Z(cid:2)) =

 Var (Z) .
L(cid:3) (cid:2)=1

 (cid:2)
 To approximate the size of L, we compute the cost of this (cid:2)=1 |B(cid:2)|.
Assuming |B(cid:2)| (cid:6) Band experiment which is enforcing that the cost is less than the budget, we get that L   B/E (|B1|).
Note that since |B(cid:2)| are also i.i.d.
random variables, applying Cherno s inequality, it is easily shown that the value of L = (1   o(1))B/E (|B1|) with high probability.
Finally, we have that Var (Y )   1B E (|B|) Var (Z) .
(2) where we omit the subscript and use B and Z instead of B1 and Z1.
Since the budget B is  xed, the best choice of distribution over core subsets and estimators is the one minimizing E (|B|) Var (Z).
This quantity is highly related (cid:9)   function f is itself random, as assumed in [1], we can expect Var (Z) to be proportional to 1/|A|.
It remains to compute the variance Var (Z) = E (cid:8)
 (E (Z))2.
We start by calculating the second moment of the (cid:8) estimate Z,



 = (cid:9) (cid:3) (cid:3) (cid:3)
 (cid:3) (cid:3) j V i V j V i V where q (i, j)   (cid:2) = =
 n2
 n2 (cid:3) (cid:3) i A j A
 q(i)q(j) q(i, j) q(i)q(j)
 n2q(i)q(j) f (i)f (j) (cid:3)
 f (i)f (j) f (i)f (j) Q(A)(cid:0){{i,j} A(cid:2)} (cid:9) (cid:2) (cid:8)
 (3) A 2V Q(A)(cid:0){{i,j} A(cid:2)} is the probability that both nodes i and j are simultaneously included in a core set A.
Maximizing this expression over functions f f (i) =  n gives the worst variance possible.
such that It is easily veri ed that the maximal obtainable value is q(1) .
This
 bound, however, is overly pessimistic since it is obtained in the unrealistic case where f (1) =  n and all other values are f (i) = 0.
We therefore need to enforce that the non zero values of f are distributed over many nodes.
A natural way to achieve this is to limit ourselves to functions f such that f (i)   [0, 1].
q(1) , assuming w.l.o.g.
that maxi q(i) = 1 =  

 Proposition 1 Let W (i, j) = q(i, j) q(i)q(j) , (cid:3) {i,j} U W (i, j) .
and Then = U  = arg max |U| n (cid:3) {i,j} U  Var (Z)   W (i, j)/n
 .
(4) 2     (cid:9)   0 for all f .
(cid:8)

 Proof.
First, note that W is a positive semide nite ma-(cid:2) trix.
This is because f T W f = n2 Therefore, f T W f is a convex function of f de ned over the convex set f (i)   [0, 1] and f (i) =  n.
The maximal (cid:2) value of such functions is obtained in an extreme point of the body.
Let u  be this extreme point u (i)   {0, 1} and i u (i) =  n.
That is, for  n nodes we have u (i) = 1 and for (1    )n nodes u (i) = 0.
Setting U  = {i|u (i) = 1} completes the claim.
Computing the RHS of (4) amounts to  nding the heaviest submetrix of size  n of W .
The heaviest subgraph problem is notoriously hard [4][2].
It does, however admit scalable approximation algorithms that work well in practice [3][7].
Regardless, in our scenario, it is natural to assume that   is at least a small constant.
Therefore, a random choice of U  is expected to yields a  2 approximation factor to the optimal.
Moreover, if any algorithms improves on the random choice by a factor of t then the solution is guaranteed to by a t 2 approximation to the optimal.2
 For the sake of simplicity, we use a more relaxed bound which uses the spectral norm of W .
Applying the Cauchy-Schwarz inequality yields Var (Z)   1 n2  1(W )(cid:11)u (cid:11)2    2.
Here  1(W ) denotes the spectral norm of W (its largest eigenvalue).
Substituting (cid:11)u (cid:11)2 =  n we get that (cid:11)  1(W )     Var (Z)   (cid:10) (5)   .
n The largest eigenvalue  1(W ) can be easily calculated using various methods (e.g., the power iteration method).
In addition for this bound (expression (5)) being signi cantly easier to compute, we will see in Section 6.7 that it is also tight enough to provide valuable information.
As seen above, the variance of the estimator Y is proportional to E (|B|) Var (Z), where both E (|B|) and Var (Z) are complex functions of the core set distribution Q.
In the next section we describe speci c algorithms for e ciently drawing core subsets from a distribution Q and producing the probability vector q for every graph.
Our goal here is to reduce E (|B|) Var (Z) as much as possible.
i


 In order to describe the algorithm we require some addi-(i, j)   E} indicate tional notations.
Let Ni = {j   V : i = {{i}   Ni}.
We the set of neighbors of node i and N + denote by Ni,j = Ni   Nj (similarly N + i,j = N + j ) and M (i, j) = min{|Ni,j|, d  1}.
We also denote Q the distribution over subsets of nodes and Q(A) the probability of core set A being chosen.
Finally, let supp(Q) be the support of Q, i.e., A   supp(Q) i  Q(A) > 0.
Here the core sets are simply the nodes of the graph, supp(Q) = {{i} |i   V }.
In addition, the core set distribution Q is P r (A) = p(i), where p is some distribution de- ned over the nodes of G. Hence, q(i) = P r (i   A) = p(i).
Since each core set contains only one node and we randomly pick d of its neighbors to form the closure set, we clearly have |B|   d + 1.3 To compute the variance we note that W (i, j) = 1 q(i) for i = j and zero otherwise.
Since W is a diagonal matrix in this case, its top eigenvalue equals its maximal diagonal entry, we have that the spectral bound (5) reduces to (cid:10) Var (Z)  
 n
 mini p(i) (cid:11)      .
This is minimized using the uniform distribution p(i) = 1/n and gives Var (Z)   (1   ) .
In this case, it turns out, that the na ve spectral bound of (5) is tight.
The overall variance achieved by the na ve algorithm for any f and uniform node distribution is therefore Var (Ynaive) =
 (6)
 The  rst non trivial core set distribution can include any two nodes that are connected in the graph, namely {i, j}   supp(Q) if {i, j}   E. Setting the probability of choosing
 bors.
(cid:3) {i,j} E E (|B|)   2d   (cid:12)(cid:2) W (i, j) = p(i, j)M (i, j) (cid:13)(cid:12)(cid:2) p(i, j) (cid:13) .
k Nj p(j, k) k Ni p(i, k) A possible good assignment for p could be achieved by producing a maximal weighted matching on the graph G, where the weight of edge {i, j} is set to M (i, j), assigning probability 2/n for all edges in the matching and probability zero to all other edges.
Admittedly, not all graphs yield good maximal weighted matching, or even any matching that includes all nodes.
We therefore experimented with a simpler edge selection algorithm that applies to any graph.
.
i |i   V Here the core sets are the graph nodes and their neighbors supp(Q) = In addition, we assign di erent probabilities to each core set according Q(N + i ) = p(i), where p is an arbitrary distribution de ned over the nodes of G.
The node i will be referred to as the center of N + i .
Hence, a node i belongs to the core set A if one of its neighbors (or itself) is the center node of A.
Therefore we have (cid:14) (cid:15)
 (cid:12)(cid:2) W (i, j) = (cid:3) (p(i) + p(j)) (d   Mi,j ) (cid:2) {i,j} E (cid:13) .
(cid:13)(cid:12)(cid:2) k N p(k) + i,j k N + i p(k) k N + j p(k)

 In [1], the authors suggested generating the core sets according to a random walk, namely, start at any node and at each step move to one of the neighboring nodes uniformly at random.
One can theoretically consider a core set distribution Q that includes all length t paths in the graph.
The probability of a core set A is the probability of it being the set of nodes produced by the random walk.
Although it is computationally impossible to compute Q it is quite easy to sample from it simply by simulating the random walk.
In order to execute the meta algorithm one must also be able to compute q(i).
It is well known as per [6] that after a certain number of such steps, one reaches the stationary distribution.
According to the stationary distribution the (cid:2) probability of being at node i is proportional to its degree deg(i).
Therefore, the expected number of times a node is (cid:2) included in a length t random walk is t deg(i)/ j V deg(i).
Setting q(i) = t  deg(i)/ j V deg(i) completes the description of the algorithm.
It is worth noting that there is a slight di erence in notation between the ones in [1] and those used here.
There, a node can appear multiple times in the core set, so in a sense, it behaves more like a list than a set.
This is the reason the authors of [1] introduced the multiplicity of nodes in core sets into their estimator.
A similar view is also possible for the other variants of random walks proposed in [1].
The authors use random walks that try to balance the probabilities q(i).
This is possible using a Metropolis-Hastings random walk as used, for example, in [5].
Another option is to assign weights to edges and transition with probabilities proportional to edge weights.
It turns out that it is possible, in most cases, to assign such weights that the probability of visiting each node is roughly the same.
Again, computing or storing Q is computationally impossible but sampling from it is easy.
Setting q(i) = t/n and applying the meta algorithm is identical to the algorithms in [1].
One problematic aspect of using random walks is that it is impossible to compute the matrix W and hence impossible to analytically bound the variance for arbitrary unknown functions f .
Surprisingly, there is a way to overcome this problem.
In particular, one can simulate a very large num-(cid:5) ber of random walks and produce an empirical matrix W .
It is not hard to see that after a su cient number of sim-(cid:5)   W at least in the spectral ulations, we would have W sense.
Moreover, it can be shown using sampling argumen-(cid:5) tation, that O(n2 log(n)) simulations would su ce for W to be close enough to W to give similar bounds.
In cases where the support of the core set distribution is small, we can directly minimize the overall variance of the estimator.
That is,  nd values of Q that minimize E (|B|) Var (Z).
One obstacle in doing so is that an exact Proposition 1 we have that Var (Z)   (cid:2) expression for the variance Var (Z) is not available.
From {i,j} U  W (i, j)/n2.
over all elements of W , namely, Var (Z)   (cid:2) Alas, computing this value requires solving an NP-hard problem.
We therefore replace it with a simple bound which sums Although this bound is extremely na ve, it serves well as a surrogate to the actual value.
The second obstacle is that computing the closure set for a core set is also computationally hard.
For this problem we also use a simple bound which is the size of closure set achieved by a greedy algorithm, Bg.
Finally we are faced with minimizing  (Q) = E (|Bg|) {i,j} V W (i, j)/n2.
Since  (Q) is a complex function of the core set distribution we cannot hope to minimize it exactly.
We resort to minimizing  (Q) heuristically using Gradient decent method.
The results of using Gradient decent on Neighborhood algorithm core sets are presented in the Section 6.6.
(cid:2) {i,j} V W (i, j)/n2.
Evaluating algorithms  e ciency or preferring one algorithm to another is impossible in general.
The correct choice of algorithm heavily depends on a wide range of parameters.
While the authors of [1] report good results of their algorithms when applied to portions of the Facebook network, we observe that it does less well for others.
Likewise, our non trivial algorithms mostly outperform the na ve implementation but for some values of f and some graphs the na ve algorithm still prevails.
The number of variations possible in the graphs, satisfaction functions, algorithms, and measures of success, is practically endless.
Nevertheless, we tried to be as thorough as possible.
Our choices are described below.
The  rst crucial factor in the success of an algorithm is the Network it operates on.
In this work we examine 3 di erent graphs, one real and two synthetic.
DBLP: we used the Digital Bibliography and Library Projects (DBLP) entire database.
It contains data about authors of manuscripts.
We associated each node in the graph with an author, where an edge corresponds to co-authorship of at nodes, each with at least one edge (authors with no coauthors were discarded).
BA: a synthetic graph constructed according to the model of Albert and Barab asi [8].
We start with a ring graph of size 10.
Then we add nodes one at a time.
Each new node is connected by edges to 10 other nodes already in the graph with probabilities proportional to their degrees.
WS: a synthetic graph constructed according to the model of Watts and Strogatz [9].
To construct a network of n nodes, we start with an n node ring graph.
Then, we connect each node to 10 nodes to its right along the ring.
Finally, we reroute each edge to a random node with probability 1/2.
One immediate problem we encounter is that, due to our model, nodes with degree lower than d cannot be measured.
That is simply because, even if all their neighbors are chosen, they would not have d chosen neighbors.
One can think of several solutions for this issue.
For example, change the model by deciding that such nodes are still measurable if all their neighbors are chosen.
Alternatively, de ne the mean to not include those nodes and never measure their satisfaction.
However, since this is not the main point of the paper, we chose (as in [1]) to simply remove those nodes.
Note that after removing some small degree nodes, other nodes might become removable too.
Here, we simply continued removing those until all degrees in the graph were at least d. This process will be referred to as trimming.
As explained throughout the paper, one of the most crucial factors governing the variance of our estimation is the satisfaction function f .
While our proofs bound the variance for all functions f simultaneously, we experimented with only four kinds of such functions.
These were taken to represent extreme cases of network biases.
Uniform: samples uniformly, without replacement, exactly  n nodes from the graphs.
Then, set f (i) = 1 for chosen nodes and f (i) = 0 otherwise.
This serves mostly as a sanity check and as a baseline.
One cannot expect any meaningful social feature to truly have such a satisfaction function.
BFS: starts a Breadth First Search algorithm in an arbitrary node of the graph and assigns a value of f (i) = 1 to all nodes it encounters until it reaches a count of  n nodes.
Then, the rest of the nodes are set to f (i) = 0.
This function gives an extreme graph topological bias.
Degree Percentile: assigns the value of f (i) = 1 to all nodes in the top   percentile in terms of degree.
In other words, f (i) = 1 for the  n nodes whose degree in the graph is the highest.
Then, the rest of the nodes are set to f (i) = 0.
Here we simulate another extreme case of degree bias.
This is an important case for two reasons.
First, our algorithms are heavily in uenced by node degrees and so this choice of f might present a  di cult  scenario.
Second, in reality, social features are not independent of degree biases.
This is because the node degree usually relates to the user s activity or  socialness  in some sense.
This function is the extreme case of all satisfaction distributions that are positively correlated with the degree.
Degree Bias: proposed in [1] and gives a less extreme degree bias.
Nodes are chosen randomly with probability proportional to log(deg(i)).
The process terminates when we have picked  n nodes.
Then, we set f (i) = 1 for chosen nodes and f (i) = 0 otherwise.
Uniform BFS Degree Percentile Degree Bias Na ve Edge Edge-Matching Neighborhood Neighborhood-20 Neighborhood-40 Neighborhood-GD Simple-RW Metropolis-H-RW Matrix-Scaling-RW Triangle-Closing-RW











































 Table 1: The table gives the Normalized RMSE scores for the various algorithms and satisfaction functions.
The graph here is the DBLP graph and the satisfaction functions mean is   = 0.1.
Barab asi Uniform BFS Degree Percentile Degree Bias Na ve Edge Edge-Matching Neighborhood Neighborhood-20 Neighborhood-40 Neighborhood-GD Simple-RW Metropolis-H-RW Matrix-Scaling-RW Triangle-Closing-RW











































 Table 2: The table gives the Normalized RMSE scores for the various algorithms and satisfaction functions.
The graph contains n = 105 nodes and is generated according to the model of Albert and Barab asi [8].
As before, the satisfaction functions mean is   = 0.1.
All the algorithms we have examined produce their estimates according to the meta algorithm (see Algorithm 1).
Here we describe algorithms by the manner in which they choose core sets.
All Random Walk (RW) based algorithms are described brie y below (see [1] for more details).
We have included those to serve mostly as a baseline but also to validate their results on graphs other than Facebook.
These algorithms perform random walks on the network and collect nodes they encounter into the core set (see Section 4).
Below, we shortly recap each of the tested algorithms.
Na ve: each core set includes one single node chosen uniformly at random (see Section 3.1).
Edge: refers to core sets of size two.
An edge is chosen uniformly at random and the core set contains its supporting nodes (see Section 3.2).
Edge-Matching: a variant of the former Edge algorithm.
Here, the core sets are also pairs of nodes supported by edges.
The idea is to create a set of edges that behaves like a matching but is simpler to obtain.
The process proceeds as follows.
Start with an empty edge set Em.
For every node, Strogatz Uniform BFS Degree Percentile Degree Bias Na ve Edge Edge-Matching Neighborhood








 Metropolis-H-RW Matrix-Scaling-RW
 Triangle-Closing-RW 0.27 Neighborhood-20 Neighborhood-40 Neighborhood-GD Simple-RW
































 Table 3: The table gives the Normalized RMSE scores for the various algorithms and satisfaction functions.
The graph contains n = 105 nodes and is generated according to the model of Watts and Strogatz [9].
As before, the satisfaction functions mean is   = 0.1.
Naive Edge Edge Matching Neighborhood Neighborhood 20 Neighborhood 40 Neighborhood GD Simple RW Metropolis Hastings RW Matrix Scaling RW Triangle Closing RW




 Budget (percentage of n)

 i i pick the edge connecting it to its neighbor (in G) with the least degree with respect to Em.
In case of ties, pick the one maximizing Mi,j (for de nition see Section 3).
Add the picked edge to Em and continue.
The core sets are pairs of nodes supports of edges in Em chosen uniform at random.
Neighborhood: corresponds to a uniform distribution over the sets N + i .
This is achieved by selecting a node and its neighbors uniformly at random (see Section 3.3).
Neighborhood-k: a variant of the former Neighborhood algorithm.
Here, the algorithm also chooses nodes and their neighborhoods but avoids doing so for nodes of very high degree.
More accurately, Neighborhood-k chooses node i uniformly at random.
Tehn, if |N + |   k it returns N + otherwise it returns a singleton core set {i}.
Neighborhood-GD: here the core sets are still N + i .
However, the distribution Q over them is optimized using Gradient decent method (see Section 5) to reduce the overall estimation variance.
Simple-RW: a random walk algorithm which uses the standard transition probability, i.e., move from node i to j with probability 1/deg(i).
Metropolis-Hastings-RW: moves from node i to j with probability min(1/deg(i), 1/deg(j)), and stays in node i with the remainder probability.
This produces a uniform stationary distribution but tends to visit the same nodes many time.
Matrix-Scaling-RW: moves from node i to j with probability w(i, j).
The latter probabilities are computed by an iterative process that strives to make the stationary distribution as uniform as possible.
Triangle-Closing-RW: the transition probability between nodes i and j depends on the node h visited before i.
If {h, j} (cid:15)  E the transition probability is w (i, j).
If {h, j}   E this probability is increased by a factor     1 to be  w (i, j).
The weights w are chosen to produce a node distribution which is as close to a uniform distribution as possible.
(cid:5) (cid:5) (cid:5) Our main measure of success for an algorithm is the Root Mean Square Error (RMSE) of its outputs.
Assume we execute an algorithm t times and produce outcomes Y1, .
.
.
, Yt.
Figure 1: Estimate normalized RMSE for the DBLP graph and the Degree Bias satisfaction function plotted vs. the budget B.
(cid:2) t t   ( 1 i=1(Yi    )2)1/2.
We chose RMSE Since the satisfaction function f has mean   the normalized RMSE is given by 1 as our measure of success since it embodies both accuracy and reliability.
Note that since RMSE (squared) estimates E[(Yi    )2]/ 2, by Markov s inequality we also get con -dence intervals.
Tables 1 3 give the RMSE values achieved by the di erent algorithms for di erent choices of f .
Each combination of algorithm and satisfaction function was run 1000 times and the RMSE value is calculated according to Section 6.4.
For all tables we set the budget B to 1% of the network size.
This is a reasonable budget for actual bucket tests.
In Figure 1 the estimate normalized RMSE for the DBLP graph and the Degree Bias satisfaction function is plotted vs.
the budget B.
It is clearly visible that the RMSE decreases with the increase in budget.
To demonstrate the bene ts of the Gradient Decent optimization (see Section 5) we applied the Neighborhood algorithm to the DBLP graph.
As before, we iteratively trimmed the minimal degree in the graph to be 10.
This resulted in a graph containing n = 57285 nodes.
In Table 4 we provide several statistics for three di erent distributions over for all i   V .
The Neighborhood algorithm core sets, N +  rst is uniform (Uniform), the second is relative to 1/|N + | i (Degree) and the third is the probability Q(N + i ) assigned by the Gradient Decent procedure (GD).
i By examining the table we observe that the Gradient Decent optimization reduces  (Q), mainly by reducing the average closure set size.
In parallel, it also increases the e -ciency of the algorithm.
On the other hand it increases the largest eigenvalue when compared to that calculated for the Uniform distribution.
While it is hard to foresee the exact strategy that Gradient Decent optimization follows to reduce  (Q), Figure 2 may





 Neighborhood   spectral bound Neighborhood simulation (Uniform) Neighborhood simulation (BFS) Neighborhood simulation (Degree Percentile) Neighborhood simulation (Degree Bias)
 Degree Uniform












 Node Degree









 Budget (percentage of n)



 Figure 2: Probability distributions of the size of core sets for the DBLP graph.
The three distributions represent di erent distributions over the core set Ni+ .
Uniform, selects each with constant probability, 1/n.
Degree, selects Ni+ w.p.
proportional to
 output of the Gradient Decent optimization.
Figure 3: Estimate normalized RMSE upper bound plotted vs. the budget B for the DBLP graph using the Neighborhood Algorithm.
Along with the bounds we give simulation results for di erent satisfaction functions, namely, Uniform, BFS, Degree Percentile, and Degree Bias.
As expected, all simulation results reside below the theoretical bound.
provide some insights.
In Figure 2, the DBLP graph degree PDF is plotted for the three core set distributions (Uniform, Degree, and GD).
It is apparent that the GD optimization causes the graph degree PDF to drop much faster than those of the Uniform and Degree distributions.
It turns out that the GD optimization reduces the probabilities of higher degree nodes.
In fact, for the DBLP graph, it assigned a zero probably to any Neighborhood core set of size greater than
 Average core set size E (|A|) Average closure set size E (|Bg|) E ciency bound E (|A|) /E (|Bg|)
 n2  i,j W (i, j)  (Q)  1(W )
 n Uniform Degree


















 Table 4: Statistics for di erent distributions over Neighborhood algorithm core sets applied to the DBLP graph.
Being able to analytically bound the accuracy (RMSE) of a network bucket test is crucial for two main reasons.
Before the test, the administrator must choose the best algorithm to use.
After the test, he/she must supply error bounds on the resulting estimate.
Given the discussion following Proposition 1, this is a hard computational task.
However, using the spectral bound of Equation (5), designers can get a rough bound for this quantity.
To demonstrate the ben-e ts of this bound we use values from Table 4, derived for the aforementioned Neighborhood algorithm core set distributions, to calculate bounds for the DBLP graph.
The calculated bounds along with their corresponding simulation results are plotted in Figure 3 for   = 0.1.
In this paper we proposed and analyzed several algorithms for network bucket testing.
The achieved results are comparable or better than previous algorithms depending on the setup.
However, we argue that the contribution goes beyond that.
First, our algorithms are simple to program, provide unbiased estimates, e cient to execute, and analyzable.
Moreover, we can e ciently produce good error bounds for their performance.
This gives us the ability to choose the best algorithm for a network well before running the test.
In addition, the framework lets designers analyze a very large variety of algorithms.
For example, one can consider core sets of triangles in the graph.
Or, cover the graph with small tightly connected subgraphs and consider those as core sets.
Indeed, the possibilities are endless.
We hope the derivations also provide walk-through examples on how to analyze those.
An additional bene t which is not mentioned in the paper but is an immediate outcome of the Gradient Decent approach: That is, one can combine any number of di erent algorithms and consider the superset of their core sets.
Applying the Gradient Decent process to the core superset can automatically mix the di erent algorithms.
The resulting mixed algorithm is guaranteed to perform at least as good as the best single algorithm.
