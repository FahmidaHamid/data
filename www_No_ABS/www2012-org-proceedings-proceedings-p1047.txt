The Web evolved from a text-based system to the current rich and interactive medium that supports images, 2D graphics, audio and video.
The major media type that is still missing is 3D graphics.
As computer graphics technology has reached the point where 3D models can be rendered, often in real time on commodity desktop and mobile devices, at a  delity that is almost indistinguishable from the real thing, it should be possible to use 3D models rather than
 There have been a number of approaches over the last years to integrate 3D technologies on the Web and most of these systems and standards disappeared or barely survived (e.g., [23]).
We argue that this is because of the fact that research was focused mostly on 3D graphics and 3D graphics alone.
The focus of research did not include the search for user interaction techniques that are optimal for the hypertext-based Web interface.
However, what good is a realistic environment if one cannot interact with it?
As a result, hypertext (the ultimate product in symbolic communication) and interactive 3D graphics (the ultimate achievement in visual media) are at odds on the Web.
We believe that people can gain a lot from using integrated information spaces where hypertext and 3D graphics data are simultaneously available and linked.
This paper focuses on user interface design that supports the integrated exploration of such environments; the design, where users can browse the text, look through general information and search for more speci c information, and where they can also navigate freely through a 3D space, and examine and manipulate virtual 3D objects, to gain a better understanding of the data.
The objective of our user interface is to pair interactive
 of the Web to support all these user tasks.
We believe that this issue is of a great importance, since there are upcoming new open (WebGL) and proprietary (Stage3D) proposals for
 The cube (here in blue) is a three-dimensional solid object bounded by six square faces, facets or sides, with three meeting at each vertex.
A sphere is a perfectly round geometrical object (here in green).
Like a circle in two dimensions, a perfect sphere is completely symmetrical around its center, with all points on the surface laying the same distance from the center point.
This distance is known as the radius of the sphere.
The maximum straight distance through the sphere is known as the diameter of the sphere.
A pyramid is a polyhedron formed by connecting a polygonal base and a point, called the apex.
Each base edge and apex form a triangle.
++ The cube (here in blue) is a three-dimensional solid object bounded by six square faces, facets or sides, with three meeting at each vertex.
A sphere is a perfectly round geometrical object (here in green).
Like a circle in two dimensions, a perfect sphere is completely symmetrical around its center, with all points on the surface laying the same distance from the center point.
This distance is known as the radius of the sphere.
The maximum straight distance through the sphere is known as the diameter of the sphere.
A pyramid is a polyhedron formed by connecting a polygonal base and a point, called the apex.
Each base edge and apex form a triangle.
OORR Hypertext 3D Graphics Hypertext Mode 3D Mode A sphere is a perfectly round geometrical object (here in green).
Like a circle in two dimensions, a perfect sphere is completely symmetrical around its A pyramid is a polyhedron formed by connecting a polygonal base and a point, called the apex.
PPPyyyrrraaammmiiiddd Cube SSSppphhheeerrreee WWW 2012   Session: User Interfaces and Human FactorApril 16 20, 2012, Lyon, France10472.
FACTORS DRIVING THE DESIGN There were  ve main driving factors in designing our UI:
 A prerequisite to the e ective UI design is an understanding of the users and their tasks - the tasks for which that interface will actually be used.
In [16] we focused on an understanding of the fundamental tasks users may engage in while exploring Web-based 3D virtual environments.
We constructed a  3D Web Taskonomy , where we divided these tasks into hypermedia tasks, such as following hyperlinks, and virtual environment tasks, such as 3D navigation.
We also included a review of mouse-based 3D interaction techniques useful in the context of 3D Web.
One of our major goals was to provide an interface that meets the needs of both novice and experienced users.
We assumed that most users would be new to 3D interactions.
We therefore needed to design our UI in favor of novices.
This meant adhering to well-established UI conventions and making 3D navigation as simple as possible.
On the other hand, there are 3D knowledgeable users who can  nd the limitations and constraints of a novice interface frustrating.
As we did not want to diminish the 3D interaction experience in any way, we needed to provide these expert users with much more freedom with regards to the 3D task support.
Multimedia presentation was studied extensively within psychology, aiming at extracting principles that guarantee an e ective design and facilitate learning; the central claim of multimedia is that providing information in more than one medium of communication will lead to better understanding [21].
The theory based on Dual channel [26], Limited capacity [6], and Active processing [35] assumptions suggests that if active processing takes place in both visual and verbal cognitive subsystems, learning is improved; dual coding of information is more e ective than single coding; it is also critical that both visual and verbal representation are actively constructed, together with the connections between them.
Supporting multimedia theory, studies have shown that verbal and visual representations in combination are often more e ective than either in isolation [24, 10, 22].
On the other hand, Nielsen [25] warns that unconstrained use of multimedia can result in UIs that confuse users and make it harder for them to understand the information.
Therefore, we guided our design based on the basic principles for designing multimedia learning environments [21].
In this section we survey the work that has been done in the area of UI design for information spaces where hypertext and
 Intermedia [36] and Hyper-G [1] were probably the  rst hypermedia systems that integrated 3D documents.
These systems were window-oriented, which means that they used document clusters to form groups of related documents and all document types stayed separated in their own windows.
As a result, they were not bound to a particular UI metaphor.
In contrast, the Web is a multimedia document based hypermedia system.
Its user interface is based on single documents (HTML web pages) consisting of several parts of information of di erent types.
The documents are designed by a web designer, who is responsible for placement of texts and media elements, and the overall aesthetics of a site.
Currently, in order to view and interact with 3D graphics in a web browser, a special browser plugin (e.g.
VRML/X3D, Flash, Unity3D, Java3D) is required that allows the 3D scene and UI controls to be displayed within the web browser window.
These plugins usually provide users with the means for navigation through a 3D scene: on the one hand, they implement only one 3D navigation technique - the technique that is best suited for a given task; on the other hand, VRML/X3D browsers o er multiple methods of interaction based on examine,  y, walk and  y-to.
The  rst approach limits interaction for the sake of simplicity.
The second offers more freedom in terms of viewpoint control.
Our goal is to combine the most useful features of these approaches.
Another related work is the research on integrating perceptual and symbolic information in VEs, and the further work on Information-Rich VEs (IRVEs) [3, 4, 5, 28].
IRVEs combine the power of VEs and information visualization, augmenting VEs with additional abstract information such as text, numbers, or graphs.
IRVE applications show promise in increasing the utility of the VE experience [4].
In one of IRVE experiments evaluating depth and association cues between objects and their labels, Polys et al. [28] showed that screen-space interfaces outperformed object-space layouts.
Therefore, we decided to use solely screen-space techniques for displaying annotations in 3D.
Another closely related work was carried out by a group of researchers under the direction of Thomas Strothotte [13,
 tions.
In a study that strongly a ected our design, Sonnet et al. compared methods of associating text with its 3D model [32]; they evaluated the e ects of text positioning, connectivity, and visual hints on comprehension under three conditions: (a) annotations are attached to objects using translucent shapes; (b) annotations are located within the objects  shadows; (c) area showing the 3D model and text area are separated.
The authors suggest that setting a works well for short labels, while for extensive texts, setting c seems to be applicable because a user can explore a scene without any occlusions from the text.
Usability was another major driving factor for our design.
According to [19, 25] the main principles of Web usability are: websites should explain themselves; people do not read pages - they scan them; do not waste people s time; people are creatures of habit - use existing Web conventions; people tend to get  lost in space  - make it easy to go home, choose typography that communicates, and allow going back.
Currently, game industry leads the development of 3D interactive graphics and it is where many cutting edge interface ideas arise.
Based on observation of interfaces of popular 3D games, works on game design [30] and design guidelines for virtual environments [18, 31], we summarized the main 3D design principles: Text: keep it readable and let users select for details on demand; Navigation: minimize the number of navigation steps, simplify movement (keep movements planar, use collision detection), allow teleportation; Way nd-ing: provide overviews (maps) and history keeping.
In the previous section we described the main driving factors in designing the interface for accessing integrated information spaces, where hypertext and 3D graphics data are simultaneously available and linked.
Designing such user interface clearly presents some challenges as some factors contradict others.
For this purpose we have developed a dual-mode user interface (DMUI) that has two modes between which the user can switch anytime (see Figure 2): (1) Hypertext  don t-make-me-think  Mode driven by simple hypertext-based interactions, where a 3D scene is embedded in hypertext; (2) 3D  take-me-to-the-Wonderland  Mode, which immerses the hypertextual annotations into the 3D scene.
In the following we will describe in detail the two modes of the dual-mode user interface.
the next step in the Web s evolution, since it could bene t from graphics hardware and provide users with new and exciting experiences.
Nevertheless, while various approaches have been proposed (most notably VRML/X3D), they have never seen much serious widespread use.
One reason for the limited acceptance is the lack of 3D interaction techniques that are optimal for the hypertext-based Web interface.
Our approach to this problem is the hypertext mode of our interface (see Figure 2 (left)).
This mode introduces a level of 3D-based interactivity and couples it with well adapted hypertext-based interactions.
Our intention was to create, based on the Nielsen s [25] and Krug s [19] work on Web usability, a  don t-make-me-think  type of user interface.
In the following we will describe the components of the hypertext mode of DMUI: hypertextual information and the embedded viewing window, where the 3D content appears.
We de ne hypertextual information as an information set that can contain: textual information, non-textual information (e.g., static and animated images, audio, video), interactive information (e.g.,  ash interactive illustrations), navigational means (e.g., hyperlinks).
In the hypertext mode, hypertextual information is the primary information carrier.
It is possible to read it without any interaction with a 3D scene - the information is not embedded into the 3D scene, but rather presented in a concise form familiar to the Internet users.
Compared with standard hypertextual information that can be found e.g., on the Web, the hypertext mode of DMUI introduces two new user interface compo-nents/mechanisms: 3D-hyperlinks and hypersections.
In our user interface, hyperlinks constitute not only a mechanism for navigation between hypertext documents, but also for navigation within 3D scenes.
If a 3D scene contains a viewpoint node named viewpoint1, selecting a hyperlink connected to this viewpoint should smoothly animate the camera from its current position to the selected vantage point.
By introducing 3D links, we aim to provide users with the ability to view 3D content from di erent points of view with a single mouse click.
Visual cues are given as to where source anchors are located in a document.
We use light blue highlighting as the default color for  hypertext links  and light red highlighting as the default color for  3D links  (see Figure 2 (left)).
Both types of links can be embedded in hypersections.
Hypersections de ne sections of hypertextual information; they are analogous to HTML divisions that are often used to group block-elements to format them with styles.
The di erence is that hypersections are also designed to be: (1) Annotations of related 3D objects: Hypersections become annotations when the user switches to the 3D mode; (2) Links between hypertextual and 3D information: When a mouse cursor passes over a hypersection, the hypersection and the corresponding object in the 3D viewing window are automatically highlighted and the cursor changes its shape; the user is given visual cues as to what information is related to what object and where the related object is on the scene; (3) Navigational UI components: Pressing the middle mouse button over the hypersection animates the camera from its current position to the corresponding object.
We believe that 3D-hyperlinks and hypersections can greatly facilitate the interaction.
We wanted to make possible for users with little knowledge of 3D interaction techniques to browse a 3D scene simply by making a single mouse click.
The hypertext mode builds upon well established principles for including graphical resources in hypertext.
For example, in HTML we use the <img> tag to link to the image; the browser then displays the image where the tag occurs with the text  owing around the image (CSS is often used to de- ne the appearance and layout of the page).
Similarly, in the hypertext mode, the 3D viewing window is displayed by the browser  inline  with hypertextual information.
This window renders 3D scenes through which the users can navigate and in which they can interact with objects.
Hypertext browsers often draw hyperlinks in a di erent color so that users can easily  nd them.
Our interface provides three mechanisms to make certain that users could  nd se-lectable objects (objects in the scene linked to related hyper-textual information): (1) they can be identi ed using labels, short textual descriptions connected to their referent objects with lines extending into the virtual scene (these labels are also links to more comprehensive explanations displayed in the 3D mode); (2) cyclic temporary highlighting of all se-lectable objects allows users to view all objects of a possible interest in the visible area of the 3D scene; (3) when a mouse cursor passes over a selectable object, the object, its label and the related hypersection are automatically highlighted.
While designing the hypertext mode of the dual-mode UI we have tried to accommodate as broad audience as possible by o ering multiple ways to control the viewpoint, either by clicking selectable objects (easy for everybody), dragging the mouse across the 3D viewing window (more immersive, but also requiring some practice), or scrolling that gives people the option to see the whole scene in a guided tour.
We reserved a single left mouse button click in the 3D viewing window while the cursor is over a selectable object for targeted movement navigation [20, 33].
The click on an object of interest smoothly animates the camera from its current position to the selected object and optionally triggers a prede ned for the given object camera movement (e.g.
orbiting).
Such approach was used to preserve the primary interpretation of clicking in the web browser window as following a hyperlink.
The technique is easy to use, fast, and cognitive-friendly; it can also be easily integrated with other techniques [20].
On the other hand it has a major drawback: the target is always a selectable object.
The second possible interaction in the 3D viewing window is mouse dragging (moving the mouse while holding the left or right mouse button down) and is reserved for general movement navigation.
This approach should, if possible, emulate real world behaviors and take into account information about the scene and the task at hand.
For example, geographic VEs often employ a walking metaphor of camera motion where user positions are restricted to the 2D plane of the terrain; examine metaphor is often used to view di erent sides of objects and it is suitable for tasks where the user s goal is to view an object as though he or she were holding it.
If the user s goal can be reliably determined, the moding between the navigation techniques should be automated.
There are some problems inherent in using general movement techniques.
As they are designed to allow for unconstrained movement to any part of the VE, the user may move to unknown locations, look at things from awkward angles or miss seeing important features [11].
As a result, one cannot ensure that the user receives the intended message.
Like e.g., Galyean [12], we believe that empowering the author to bring some structure to the interaction experience can make VEs more suitable for the new-to-3D users.
Therefore, our design balances the exploration methods with an ability to guide the user, while at the same time maintaining a sense of pacing or  ow through the experience.
We reserved scrolling for speci ed trajectory movement navigation.
As users can navigate on a page by scrolling it, when the cursor hovers over a 3D scene, the mouse scroll wheel can also be used to navigate between the viewpoints de ned for this scene.
If the user wants to have more freedom in terms of viewpoint control, he or she can switch to the 3D mode using a button located on a browser s tool bar (in the test application, this button is in the UI s bottom left corner).
To avoid confusion, the state of the 3D environment (user s position, animations) is preserved when switching between UI modes.
Having a 3D graphics underlay invites interaction and having rich and immersive experiences.
Yet, for sake of simplicity, the hypertext mode limits interaction with that layer.
This can lead to a problem with supporting the feeling of immersion.
What is immersion and why do we need it?
Immersion is often explained as  the state of being absorbed or deeply involved .
It is critical to Virtual Reality and can best be attained by visually immersing a user with HMD or CAVE, by using stereo displays and head tracking.
However, immersion is also possible in desktop VEs, using desktop displays and common hardware for interaction (mouse and keyboard); as the user directly controls the interaction and focuses on it, he or she can be drawn into a 3D world [29].
The successful sensual immersion of the user in an imaginary
 interacting in a virtual environment.
Achieving a close to real-life experience in a virtual world, creating a feeling of being there is crucial to give a potential virtual visitor the sensation of what the site is really like.
Tan et al. [33] assert that the level of immersion that the user experiences greatly a ect the navigation task and performance.
The more immersed the user is, and the more easily the user can mentally integrate information acquired, the greater the chances of ef- cient navigation [29].
Certainly, much of a presence has to do with a quality of the presented material and the manner in which the user experiences it.
Immersion can take place while we are playing a well designed video game, watching a quality movie, or even while reading of good novel, in spite of the lack of visual or perceptual immersion.
Comparing to the hypertext mode, the 3D mode of the dual-mode UI was designed to make users feel more present in an environment - more immersed.
In this mode 3D graphics is the main information carrier.
It provides users with much more freedom with regard to the 3D task support - it was designed to support unconstrained interactive navigation through a 3D scene.
Furthermore, in this mode hyper-textual data relating to an environment is embedded into that environment.
The design of this, what we call  take-me-to-the-Wonderland  mode, was inspired by the work on IRVEs [5] and the work on annotating 3D illustrations [32].
In the following we will describe the components of the 3D mode of our interface: the viewing window, where the 3D content appears, hypertextual annotations, a dashboard designed to manage navigation, and a mini-map.
In the 3D mode of the dual-mode UI, 3D scenes appear in the viewing window that spans the entire available screen space.
A dashboard, a map and hypertextual information about an environment are rendered on top of the viewing window in screen-space, a 2D plane called HUD (head-up display) that overlays the 3D scene.
With regard to functionality, the viewing window in the 3D mode is similar to the one in the hypertext mode.
By default, it o ers the same navigation metaphors, the same behavior of selectable objects, etc.
However, this mode provides a user with more than one  default  way of interacting with the 3D scene - the user can choose to use other methods of navigation (e.g., change from walk to  y or examine).
One of the underlying premises of this research is that communicating information about 3D environments can be sig-ni cantly improved by attaching annotations to the environment s objects.
The 3D mode allows users to interactively recall and view the attached hypertextual information by clicking on labels connected to objects of interest during navigation.
In response, the hypertextual information is presented in screen-space adjacent to associated objects in scrollable annotation boxes [13].
In fact, these hypertextual explanations are hypersections from the hypertext mode of our UI.
Consequently, the annotation boxes may contain information from hypertext, through images and movies, to multimedia animations.
They may also contain 3D hyperlinks for navigation within the 3D scene.
Users may move and resize the annotation boxes, and toggle their visibility; they can also minimize them into labels.
To better support human attention, better maintain the  uency of work, and to improve workspace visibility, annotations are rendered as semitransparent user interface objects [17].
As we have already mentioned, for a given scene type and a task at hand, a designer should decide on the most intuitive mapping between input and interaction technique.
However, very often there is insu cient input DOF for the task and user input must be moded.
Therefore, the user has to be given explicit control of the di erent modes of navigation.
A graphical dashboard (in Figure 2 presented at the bottom of the viewing window) provides ready access to the most important 3D interaction tools.
By default, it should provide the methods of navigation based on examine, walk,  y and  y-to; it should also allow the user to switch between the viewpoints that are de ned for the 3D scene.
In addition to the di culties of controlling the viewpoint, there is a problem of way nding, especially in large virtual worlds.
This problem may manifest itself in a number of ways [9]: users may wander without direction when attempting to  nd a place for the  rst time, they may then have di culty relocating recently visited places, they are also often unable to grasp the overall structure of the space ( lost-in-cyberspace  problem).
Maps proved to be an invaluable tool for acquiring and maintaining orientation and position in a real environment and according to [9], this is also the case in a virtual environment.
In uenced also by computer games, we decided to include a mini-map to the 3D mode of our interface (see Figure 2).
It displays terrain, important locations and objects.
It dynamically updates the current position of the user with respect to the surrounding environment.
In the previous section we described in detail the dual-mode user interface design for information spaces combining hypertext with interactive 3D graphics.
To put our design into practice and evaluate it, we decided to build a testbed, a platform for the experimentation and for the assessment of both hypertext and 3D modes of our interface, a system that would allow to  nd a balance between 3D interaction techniques and well established hypertext interactions.
We developed a wiki-type authoring environment called Copernicus.
Its design was inspired by the popular MediaWiki (used to power e.g., Wikipedia); in addition to a classic hy-pertextual content, any page with an article can contain a
 In Copernicus, di erent types of information, from text, through images and video clips, to 3D graphics, can be easily collected, linked, and later made available as integrated information spaces in the hypertext based environment (the hypertext mode) or within the context of a virtual environment (the 3D mode).
It is important to note that Copernicus was used to create the virtual museum of Warcraft for the user study described in this article.
Copernicus was implemented using .NET as XAML Browser Application (XBAP), so it can be deployed on the Web.
Users with .NET framework can access Copernicus just like a Flash-enhanced web page using IE or Firefox on Windows.
The project s source code is available under GPL license.
We had several opportunities to observe novices interacting with the dual-mode user interface using Copernicus.
Most observations were made in primary and secondary schools (one of the objectives of DERI is to popularize the knowledge of science and promote engineering among young students) as well as at local science fairs.
We also observed individual users at their personal work spaces (mostly at DERI and NUIG campus).
These users were free to access and navigate any content they preferred; they then provide us with feedback on the positives and negatives of the system.
Figure 3: The youngest user of Copernicus.
The comments from the participants of this initial evaluation were extremely positive.
The study has shown that due to only a slight modi cation of hypertext-based interface paradigm, the users had no problems interacting with Copernicus.
The simplicity of the interaction techniques, essentially a single click in the hypertext mode and a drag action in the 3D mode, were immediately understood and usable by all our users.
To characterize the presented user interface in terms of its e ciency and usability, we conducted a user study, where we compared our dual-mode interface to two other, currently most common user interfaces for integrated information spaces, where text and 3D graphics data are simultaneously available and linked.
In the following we will describe our study in detail.
First we discuss the evaluation setup used in the experiment.
This discussion is followed by the description of the evaluated user interfaces and the procedure used in this evaluation.
Finally we present and discuss the results of the experiment.
20 students, researchers and members of sta  with normal or corrected-to-normal vision participated in the experiment.
our participants were female.
The participants ranged in age from 27 to 42, with 12 participants in the 27-30 range and 8 participants in 30-42 range.
8 participants were PhD students, 9 had higher education, and 3 were postdoctoral researchers.
All of the participants were familiar with hypertext navigation; 3 of them had no or very little experience with 3D interaction, 13 had some experience navigating 3D scenes (playing 3D games sporadically), and 4 had considerable experience in 3D navigation from playing 3D computer games.
Subjects were given gifts worth 15e/20$ for their participation.
Additionally, an iPod was awarded to one of the 5 best performing participants.
The experiment was conducted on the Intel Core 2 Extreme laptop computer equipped with 4GB of memory, GeForce
 LCD display running at 1920x1200 resolution.
Input devices were a standard 3-button optical mouse and a keyboard.
The computer operating system used was Microsoft s Windows 7.
The test application used for the evaluation was developed based on the Copernicus source code; the content for evaluation (see next section) was authored using our 3D wiki as well (visit http://copernicus.deri.ie to learn more).
For the purpose of this experiment we prepared a virtual museum featuring heroes, races, creatures, and weapons from the fantasy setting of the World of Warcraft (WoW) game; this choice was made to prevent the in uence of previously gathered knowledge.
The museum was divided into four exhibitions, one for the training session (heroes) and three for the main experiment (races, creatures and weapons).
Each exhibition conveyed integrated hypertextual and 3D visual information.
The virtual worlds created for the study were simple single oor museum-like environments, populated with 3D objects and images/painting (see Figure 4).
Below follows a description of each exhibition: Heroes of Warcraft - an exhibition of four notable characters from the Warcraft game.
The article consisted of about 480 words and 4 images.
The 3D scene consisted of three rooms populated by 5 objects and 5 paintings.
Races of Warcraft - an exhibition of the twelve playable races of the Alliance (i.e.
Humans, Night Elves, Dwarves, Gnomes, Draenei, and Worgen) and the Horde (Orcs, Trolls, Tauren, Forsaken, Blood Elves, and Goblins) factions from WoW.
The article consisted of about 1350 words and 12 images representing each race.
The 3D scene consisted of four rooms populated by 12 race objects and 12 paintings.
Creatures of Warcraft - an exhibition of common creatures that can be found in the World of Warcraft, such as bears, saber cats, drakes, and wolves.
The article consisted of about 920 words and 3 images; each creature was characterized by strength, agility, and intellect values.
The 3D scene consisted of nine rooms populated by 14 objects and 13 paintings.
Weapons of Warcraft - an exhibition of weapons (such as swords and axes) from the Warcraft universe.
The article consisted of about 1060 words; each of the 9 weapons was characterized by damage, bonuses (e.g.
to strength, agility, etc.
), and a price.
The 3D scene consisted of one room; all 9 objects were positioned in the center of the room.
Figure 4: The 3D scenes used in the evaluation.
According to the classi cation of virtual worlds [8], all our environments are dense (relatively large number of objects and cues in the space) and static (the positions and values of the objects do not change over time).
Moreover, the exhibition of weapons is a small world (a world in which all or most of the world can be seen from a single viewpoint), while all other environments are large (there is no vantage point from which the entire world can be seen in detail).
As we have already mentioned, the hypertext  don t-make-me-think  mode of the dual-mode user interface was inspired by the state-of-the-art practice of embedding 3D scenes as part of an HTML page.
The design of what we call 3D  take-me-to-the-Wonderland  mode was inspired by the work on IRVEs [5] and the work on annotating 3D illustrations [32].
To characterize the dual-mode user interface in terms of its e ciency and usability, we decided to compare it to these two inspirations that are currently user interfaces of choice for integrated information spaces, where text and 3D graphics data were simultaneously available and linked.
Hypertext UI - this interface was created by modifying the hypertext mode of the dual-mode UI.
Features like an ability to switch to 3D mode, hypersections, and 3D hyperlinks were disabled.
On the other hand, the dashboard UI component was added to the 3D viewing window (see Figure 5a).
(b) Weapons of Warcraft in the 3D UI (c) Races of Warcraft in the hypertext mode of the dual-mode UI (d) Races of Warcraft in the 3D mode of the dual-mode UI Figure 5: The exhibitions presented in the evaluated user interfaces.
of the dual-mode user interface.
Features like an ability to switch to the hypertext mode and 3D hyperlinks were disabled (see Figure 5b).
Dual-Mode UI - this interface integrates Hypertext UI and
 hypertext mode, to read about the collections and easily navigate through the rooms of the virtual museum using 3D hyperlinks and hypersections (see Figure 5c).
The same UI, while in the 3D mode, also allows users to experience the
 browser window; just like in the 3D UI, the user can walk through the rooms of the museum and click on object s labels to read more comprehensive explanations (see Figure 5d).
The user interface designs evaluated in this study di ered in the method used to integrate the textual information with the objects in the 3D scene.
On the other hand, the interfaces allowed for the same interactive exploration of 3D scenes; there were no di erences in the techniques that enabled a user to navigate the 3D scenes.
Movement was con ned to  walk  mode; guided tour navigation (scrolling over
 vent users moving through objects and walls.
Di erent possible measures could be used to determine the e ectiveness and usability of the evaluated interfaces.
In choosing tasks for the study, we looked for ones that are both valid (resemble a  real  act of browsing 3D content on the Web) and that are recognized for being able to detect signi cant di erences.
We decided to adopt tasks that were introduced by Chen et al. [7] and were later successfully used by Polys et al. [27, 28] to evaluate IRVEs.
Thus, the participants performed 4 types of tasks, representing various conditions a user is likely to experience on a 3D Web site: (1) Search for textual information and then search for visual information (S:H-3D).
Task 1 requires the users to  rst search for text information, and then to  nd the corresponding visual information in the 3D scene.
An example task is: Find the Horde race that uses Hawkstriders for mounts.
What other races are to the left and right of this race?
(2) Search for visual information followed by textual information (S:3D-H).
Task 2 is conceptually reversed, in that the users are required to  nd the visual information on the 3D scene  rst, and then to answer questions about the related text information.
An example task is: Find the sword which hilt/handle has a yellow dragon eye and ends with dragon claws.
What is the price of this weapon?
(3) Compare text information and derive visual information (C:H-3D) ( nd visual attributes of items with a given text criteria).
An example task is: Find the strongest creature in the museum.
What is the color of the creature s eyes?
(4) Compare visual information and derive textual information (C:3D-H) (search for textual attributes of items with a given visual criteria).
An example task is: There are two races with tails.
What are their mounts?
Like in [7, 27], the study measured relative e ectiveness of our user interfaces by both examining time taken to answer each question and correctness of answers.
In addition, we developed a questionnaire to measure participants  subjective impressions of the user interfaces.
The questionnaire contained continuous Likert scales regarding ease of use, learnability, e ciency, aesthetics, presentation and access to text, 3D navigation, way nding, immersion, and overall preference.
Subjects were also welcome to add any comments relevant to their experiences.
The test application also recorded the usage of di erent UI components (e.g., a number of selections in a viewpoint menu, the use of hyper-sections, 3D hyperlinks, etc.)
Each test session started with an introduction to the test application.
It s interface was divided into two parts: the window, where the user was presented with tasks and the browser window (1280x800), where the user could interact with the prepared exhibitions through the user interfaces evaluated in this study.
The introduction was followed by a training session (4 practice tasks for each interface) to allow the subject to get familiarized with the test application, the interfaces, and the test procedure.
The users were educated and guided on how to use the walk and go-to navigation metaphors, and the viewpoint menu for control in a virtual world; they were also introduced to the concepts of hyper-section and 3D hyperlink.
After the subjects indicated that they were satis ed, we proceeded with the actual trials.
The tasks in the main part of the evaluation were similar to the ones from the training session: for each exhibition-UI combination we asked 4 questions related to the content of the exhibitions (presentation of variables was counterbalanced by means of Latin square design).
For each question there was a choice of 4 answers from which the user had to choose 1 and only 1 answer.
The subjects were asked to complete the assigned tasks  as accurately and as fast as possible .
They were also told that it was more important to solve the tasks correctly rather than to be quick.
They were allowed to take a break between each set of questions.
The participants were video recorded during the tasks and notes were taken about their actions and comments.
After being presented with all 36 tasks (3 UI modes * 3 exhibitions * 4 tasks), the users were given the questionnaire and asked to directly compare the evaluated user interfaces.
Each evaluation session lasted approximately 120 minutes - here it is important to stress the fact that for most of the participants the experiment was not tiring and seemed much shorter (actually, some participants expected more questions).
We collected a total of 720 time and accuracy measurements (20 subjects * 3 UI modes * 3 exhibitions * 4 tasks), and 660 measurements of subjective impressions (20 subjects *
 results with analysis of variance (ANOVA).
With ANOVA we modeled our experiment as a repeated-measures 3x3x4 design (UI x Environment x Task).
Bonferroni procedure was used for evaluating the pairwise comparisons.
* Visit http://copernicus.deri.ie/www2012.htm to view experimental results and recordings from the test sessions.
Times for completion of each task were normalized on the overall average completion time.
Normalization was used to remove any e ects of base reading speed and 3D navigation experience among participants.
As overall accuracy was very high (0.985%), we decided to simply double the times of wrong answers.
Analysis of the task completion time revealed signi cant main e ects of all variables and their interactions (p<0.003).
Most importantly, it found signi cant main e ects of UI (F(2, 38)=44.32, p=.00000), interaction between UI and environment type (F(4, 76)=4.49, p=.0026), and interaction between UI and task type (F(6, 114)=25.66, p=0.0000).
Post-hoc comparisons of means revealed that the dual-mode UI condition resulted in the best overall task performance (p < 0.0001), while the hypertext UI condition was marginally worse than the 3D UI (p < 0.041).
To be more precise, executing tasks using the dual-mode UI was about 43% faster than using the hypertext UI (99s vs. 141s), and about 31% better than using the 3D UI (99s vs. 129s), while executing tasks using the 3D UI was about 9% faster than using the hypertext UI (129s vs. 141s).
Figure 6: Overall results.
Comparisons of means for each exhibition revealed that the dual-mode UI was signi cantly better than the hypertext UI (p < 0.015) for Races and Creatures.
It was also better than the 3D UI (p < 0.011) for Creatures.
Figure 6 illustrates the overall results of our experiment and the results for each exhibition with respect to task completion time (error bars denote 0.95 con dence intervals).
An interesting  nding, visible in Figure 6, is that the hypertext UI was worse than the 3D UI in large environments, and it was better in a small one (Weapons).
We believe this is because the small environment did not require much 3D navigation and users could not get lost in 3D space.
Figure 7: Interaction between UI and task type.
As we have already mentioned, we also found a signi cant main e ect of interaction between UI and task type (F(6,
 prisingly, since the task types di ered signi cantly (see Sec-
tion 5.5).
Post-hoc comparisons of means revealed that executing tasks using the dual-mode and hypertext UIs was signi cantly faster than using the 3D UI for S:H-3D and C:H-3D tasks (p<0.0001); using the dual-mode and 3D UIs was signi cantly faster than using the hypertext UI for S:3D-H and C:3D-H tasks (p<0.00006) - see Figure 7.
The average subject ratings with respect to ease of use (di -cult/easy), learnability (di cult/easy), e ciency (completing the tasks fast and accurately was: di cult/easy), aesthetics (non-aesthetic/aesthetic), presentation of text (con-fusing/clear), readability of text (hard/easy to read), search and access to text (di cult/easy), 3D navigation (compli-cated/simple), way nding (complicated/simple), immersion (not involved/really immersed), and overall preference are illustrated in Figure 8, together with standard deviations.
Analysis of the ratings revealed signi cant main e ects of UIs on all scores.
The dual-mode UI was perceived easier to use and more e cient than the hypertext and 3D UIs (p<0.0001).
It was also perceived as more aesthetic than the hypertext UI (p<0.003).
On the other hand, the hypertext UI was perceived as easier to learn than the 3D UI (p<0.045) and the dual-mode UI (p<0.0003).
Subjects perceived presentation, readability and access to text in the 3D UI as worse than in the dual-mode and hypertext UIs (p<0.0001).
In contrast, 3D navigation, way nding, and immersion in the hypertext UI were ranked lower than in the dual-mode and 3D UIs (p<0.0001).
Finally, the dual-mode UI was evidently preferred over the alternatives (p<0.0001).
These  ndings clearly support our analysis of task performance.
The results from this competitive user study suggest users performed better with the dual-mode user interface over alternatives, i.e. the hypertext and 3D UIs, on tasks, which we believe are representative of a variety of 3D Web applications.
The performance with the dual-mode UI was better because, except for switching costs (visible in C:3D-H), each mode of the interface could be used optimally for each task.
Hypertext mode was employed more often for both H-3D types of tasks, while 3D mode was a choice for 3D-H tasks.
The subjective comments also showed a preference for the dual-mode interface.
Moreover, the evaluation results can help to understand better the relationship between the hypertext and 3D UIs.
Like most controlled user-based studies, this one had some limitations that restrict the generality of our  ndings: although we tested three di erent virtual environments, we still managed to test only a small sample of possible uses of 3D content.
A viewpoint menu turned out to be very important navigation tool.
Interestingly, some users preferred alphabetic order of viewpoints, some preferred order based on the distance between the viewpoints; one subject noted that he would like to have categories in the menu.
On the other hand, it is not clear whether it would be useful for environments with a large number of points of interest.
A map also proved to be very important way nding aid to the majority of users.
We noticed that few users clicked on the map they expected instant teleportation to the selected rooms.
3 of the 4 participants with prior considerable experience in
 keyboard-based control of the camera, in addition to the mouse-only control we provided.
Some participants asked about search support (Ctrl-F), both for text and 3D.
One user did not like the grey highlighting of 3D objects in the 3D scene:  such highlighting makes colors and details less visible .
A few users criticized the label and annotation layout used in the experiment as we implemented a very simple layout scheme that places the label/annotation box on the left corner of a box that encloses the geometry s bounds (bounding box).
With regard to the dual-mode UI, one suggestion was to move the button for switching modes and position it in the corner of the 3D viewer (in the hypertext mode).
During the First Hypertext Conference in 1987, Andries van Dam gave a keynote speech and listed user-controlled 3D graphics as one of the key issues we should be looking at while researching and developing hypertext systems:  If a picture is worth a thousand words, a dynamic picture of time-varying objects is worth a thousand static ones.
We need dynamics at the nodes, not just static pictures and text.  Andries van Dam [34]
 The Web, today s largest and most important hypertext-based online information infrastructure, does not support
 posed (most notably VRML/X3D and now WebGL), there is still no clear design methodology for user interfaces that integrate hypertext and interactive 3D graphics.
We have presented a novel strategy for accessing 3D content on the Web.
We have introduced a user interface that has two modes between which a user can switch anytime: the driven by simple hypertext-based interactions hypertext  don t-make-me-think  mode, where a 3D scene is embedded in hypertext and the more immersive 3D  take-me-to-the-Wonderland  mode, which immerses the hypertextual annotations into the 3D scene.
Results from the competitive user study suggest users performed better with dual-mode user interface over alternatives.
ested in taking this work.
Firstly, our UI design is not yet grounded in any formal standard, as we have focused on the HCI perspective of the problem.
As the participants of the Declarative 3D for the Web Architecture W3C CG, we want to explore the possibilities of applying our design methodology in the HTML5 speci cation, since it does not yet de- ne how the integration of 3D imagery into HTML actually should look like.
Another future avenue that we intend to explore is collaborative web browsing.
Currently, browsing the Web is mostly an individual experience.
People visiting the same web page at the same time are not aware of each other.
DMUI supports this personal nature of browsing.
On the other hand, research on Collaborative Virtual Environments [2] has shown that their entertainment and social nature can provide virtual communities with richer content and greater interactivity and greatly support collaborative work.
We envision the Web (or rather part of it) evolving into a virtual space in which people, while being in the 3D mode of the dual-mode UI, can interact and communicate with each other through their avatars.
* This work has been supported by SFI under Grant No.
SFI/08/CE/I1380 and by EI under Grant No.
PC/2008/353.
Visit http://copernicus.deri.ie to learn more about DMUI.
