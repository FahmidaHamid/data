The recent boom in the amount of available semantic data increases the already high interest of the research community in the semantic technologies.
This growth can be largely contributed to the emerging web of Open Linked Data1.
More and more sources 1http://www.linkeddata.org/ Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
publish, along with the traditional human-readable content, structured and linked metadata records in formats such as RDF, RDFa and microformats2.
The increasing amount of semantic data brings along important technical challenges.
Except the basic need for the ability to store and access those data, the prevailing challenge is the retrieval from semantic data.
While there exist languages for querying semantic data, such as SPARQL for RDF data sets, they require a certain level of technical skills to formulate queries, as well as the knowledge of the data representation in the underlying knowledge bases.
It is only natural that the research community targets the question whether the natural language queries could be used instead of traditional structured query languages to tap the knowledge stored in semantic data sources.
The ad-hoc semantic search targets the challenge of answering keyword queries from structured knowledge bases.
While in information retrieval, the result for a keyword query is the ranked list of documents from a collection, in semantic search, the result comprise ranked list of entities, resources from the queried knowledge base.
As shown recently by Pound et al. [16], already today search engine users submit many queries that would be suitable for a semantic search; for example search for speci c entities, sets of entities or entity attributes.
Based on this observation, Pound et al.
described ad-hoc object retrieval for semantic search where a user formulates queries using keywords, much like in the web search, and they propose a classi cation of semantic ad-hoc queries into  ve categories: entity queries, type queries, attribute queries, relation queries and other keyword queries.
In this work, we focus on semantic type queries.
The task of answering semantic type query is, given an unstructured keyword query in natural language with the intent of retrieving objects of a give type and a semantic graph (knowledge base), to  nd objects/entities of the desired type.
For example, the following keyword queries are examples of semantic type queries: "Apollo astronauts who walked on the Moon", or "Arab states of the Persian Gulf".
To continue the example, the correct answer for the  rst query, using DBpedia as a knowledge base, would comprise  dbpedia:Neil Armstrong ,  dbpedia:Buzz Aldrin  and ten more astronauts represented by DBpedia entities.
In this paper, we propose a retrieval model for ad-hoc type queries called the SemSets model.
The approach can be described as trying to mimic the behaviour of a human trying to answer the query using a web search engine.
A human user would probably enter such a query to a web search engine and inspect several top-k results.
The user would search the text of the inspected documents to  nd desired set of entities.
Then, the user could rank the entities based on the quality of the information in retrieved documents, the query target and the user s knowledge and con dence.
Similarly, in our approach, we  rst search the documents con-2http://microformats.org/ ing activation technique to identify additional relevant entities in the knowledge base.
This corresponds to a user performing a web search and retrieving the top-k results.
Then, we check the membership of candidate resources in semantic sets constructed from the knowledge base.
This corresponds to the user inspection of the retrieved documents.
Finally, we evaluate the relevance of identi- ed semantic sets to a given query and rank the members of semantic sets accordingly.
The  nal step mimics user evaluation of the results, based on his/her knowledge.
The proposed approach combines information retrieval techniques, activation spreading over the link structure of a knowledge base and information about entity membership in semantic sets, de ned by the knowledge base.
The idea of combining the text information retrieval with activation spreading is well known (e.g.
[17]).
The main innovation of the proposed approach is the utilization of semantic sets in the process.
We propose two approaches for construction of semantic sets, groups of semantically related entities.
One approaches requires an expert user for the task, the second one is fully automatic.
The challenge of answering ad-hoc keyword type queries from knowledge bases is a new task, unexplored for the most part.
The novelty of the presented work is in the retrieval model itself, which exploits information about entity membership in semantic sets and in proposed methods for construction of semantic sets from a given knowledge base.
The main contributions of the paper are:   Retrieval model for the ad-hoc semantic type queries, with the goal of answering keyword queries for semantic list search from semantic data.
It combines information retrieval techniques, link evidence and information on the membership of entities in semantic sets to produce the results.
  Methods for semantic sets construction.
As shown in the evaluation section, the use of the information on the membership in semantic sets brings signi cant increase in the precision for the proposed retrieval model.
We show how an expert user, knowledgeable about the data set, can de ne such semantic sets; in addition we propose a fully automatic method for the construction of semantic sets.
When used in the proposed SemSets retrieval model, semantic sets constructed by both methods have very similar positive effects on the precision of the results.
  Evaluation results provide evidence of the method s ef -ciency.
Moreover, we use publicly available data sets in the evaluation, which makes our results reproducible and allows direct, head-to-head, comparison with other approaches to the semantic type retrieval task.
The organization of the rest of the paper is as follows.
We brie y discuss related work in Section 2, and in Section 3 we state the problem and describe the required preliminaries.
We propose the SemSets model in Section 4 and approaches to the SemSets construction in Section 5.
In Section 6 we describe our experimental setup and present the results of the evaluation.
Finally, we discuss future work in Section 7 before we conclude in Section 8.
The problem of providing natural language interfaces to semantic data, the area also addressed by this work, is currently in the centre of research attention, with several prototype systems already in existence.
Some were built for a speci c domain (e.g.
[5]), others are domain independent [9].
The dominant approach is to transform a user s keyword query to a formal semantic query by matching query segments to triples from the knowledge base [3, 9,
 of a free-text keyword query to entities of a given knowledge base.
Different strategies to this task can be found in the literature, from pattern-matching, bag-of-words and gazetteer approaches to deep linguistic analysis [19].
Work related to the problem of keyword queries analysis includes also annotation of the free text with resources from a knowledge base [15], segmentation of the keyword queries [7], as well as semantic query suggestion [12].
[11] and later by Damljanovic et al.
Learning from the user interaction can be employed in order to improve performance of the semantic ad-hoc retrieval system, cf.
Lopez et al.
[4], who extend work from [18] by allowing user feedback, query re nement, and query expansion.
In contrast to the most of other systems, PowerAqua [10] (building on [11]) is able to work with multiple heterogeneous ontologies.
It transforms the input keyword query into the intermediate triple form, similarly to the principle of other approaches, the intermediate format is than mapped to the candidate entities in distinct ontologies.
The authors in [13] and [16] study real query logs of a major search engine from a semantic search point of view; their study allowed for classi cation of semantic query types and for the de nition of the ad-hoc object retrieval from semantic data.
Their classi cation comprise also the ad-hoc semantic type search that is the main focus of this work and is largely uncovered by the previous work.
A methodology for the evaluation of ad-hoc retrieval is discussed in detail in [8].
The ad-hoc semantic type queries were the centre of attention in the list search track of the SemSearch 2011 challenge3.
The systems that have participated in the challenge addressed the problem of answering type queries and are the most related works to the approach presented in this paper.
An implementation of the SemSet model, presented in this paper, has been one of the participating systems and has won the challenge, achieving the highest precision scores in the evaluation.
The other notable approaches participating in the challenge were a) BM25MF model that is a modi ed version of BM25F model adopted for semantic data, allowing the  elds to have multiple values and b) the usage of the NLP parser to analyse the queries, with the goal of identifying the type of entities targeted by the query; only entities of the speci ed type are returned in the result set.
The description of the challenge and individual approaches can be found in [1].
In this section, we formulate the problem of ad-hoc object retrieval from a knowledge base.
First, we formalize the property graph data model that we use in the paper as for knowledge base representation.
The property graph is a formalism allowing us to reason about a knowledge base in terms of entities (vertices in the graph) that have attributes and are connected via labelled edges to other entities.
We then de ne the task of ad-hoc object retrieval from a property graph knowledge base.
Informally, the property graph data model is a multigraph data structure in which vertices and edges can have properties with values.
We can de ne the property graph as a tuple: G = (V, A, P, D, L,  , ) where V is a set of vertices, A is a multiset of directed edges (ordered pairs of vertices), P is the domain of properties, D is the domain of allowed property values for nodes, L is a domain of 3http://semsearch.yahoo.com/ function that maps nodes properties to their values (P(D) being the power set of D), and  : A   P   L is the function that maps edge properties to their values.
As the dominant approach to represent semantic data is through the RDF triples, we  rst discuss how to map triples to the property graph data structure.
To model triple statements, we assume that P contains at least two attributes: URI specifying resource identi er and predicate_type specifying the type of the relation.
From graph theory point of view, an RDF model is equivalent to a directed and labelled multigraph.
For our purposes, we need only one attribute on the edges of the property graph, which denotes a label of the relation.
Thus, for simplicity, in the following text we omit the attribute type and use the relation (cid:48) : A   L, instead of the original .
In order to illustrate how triples are represented using the property graph model, consider the following example using DBpedia data (for simplicity, we use pre x  dbpedia:  instead of the whole URI of the resource) of two triples: dbpedia:Trondheim , dbpedia-owl:populationTotal , 170936 dbpedia:Trondheim , dbpprop:city_of , dbpedia:NTNU The triples would translate into a property graph with two nodes V = {v1, v2}, one edge A = {(v1, v2)}, properties P = { URI, predicate, dbpedia-owl:populationTotal }, with  (v1, U RI) =(cid:48) dbpedia:Trondheim(cid:48),  (v2, U RI) =  dbpedia:dbpedia:NTNU ,  (v1, dbpedia-owl:populationTotal)=170936 and (cid:48)(v1, v2) =(cid:48) dbpprop:city_of (cid:48).
We adopt the de nition of the ad-hoc object retrieval task from [16], where the authors specify the task in terms of inputs, outputs and evaluation.
Input: Unstructured keyword query q and a property graph G.
Output: Ranked list of resource identi ers o = (o1, .
.
.
, ok), where resources are equivalent to the nodes of multigraph:  oi   o  vj   V :  (vj, U RI) = oi.
Evaluation: All the resources in o are labeled by an independent judge, knowledgeable about q and about all the necessary information on resources in o.
In our work we consider the task of ad-hoc object retrieval for semantic type queries, where a user s intents is to  nd members of a particular set of entities.
This section presents the SemSets retrieval model for semantic type queries.
Our approach consists of several successive phases and we structure the section accordingly.
First, we discuss the query analysis, which is a preprocessing step.
We then describe three scoring functions that contribute to the  nal scoring function of the model.
Where appropriate, we also discuss secondary data structures used by the model, derived from the underlying knowledge base represented as a property graph.
To facilitate the reading, we provide an example query, and for each step of the model, we provide intermediate results of the model scoring for the example query and the DBpedia data set.
The example query that we use through the rest of the section is:  Apollo astronauts who walked on the Moon .
The goal of the preprocessing phase is to analyse the keyword query and relate its segments to resources in the knowledge base.
The aim of the analysis is to identify the query segments and the principal entity of the query that belongs to the property graph.
The analysis can be exploited for re nement of the query, using query segmentation in the document retrieval model used in Section 4.2 and the usage of identi ed principal entity is described in Section 4.4.
Let pent(q, G) = v : v   V be the principal entity of the query, given the property graph G. For the sake of generality, we do not impose any restriction on the method for computation of the pent(q, G) function, any suitable method can be used.
In our experiments, we rely on a dictionary based annotation that links text fragments to the entities in a knowledge base [15].
Example of intermediate results.
Given the example query, the query analysis phase identi ed following entities mentioned in the query:  [[List of Apollo astronauts | Apollo astronauts]] who [[Base on balls | walked]] on [[Moon | the Moon]] .
The query segments identi ed as related to the entities of the knowledge base (DBpedia) are enclosed by symbols  [  and  ] , where the string segment following after the symbol  |  is the original text of the query and string segment before the  |  symbol refers to the entity of the DBpedia knowledge base.
In this case, the query analysis procedure identi ed correctly entities  List of Apollo astronauts  and  Moon , the third identi ed entity is incorrect.
The used query annotation method also assigns con dence values for the annotated entities[15]; the scores for the our example were: conf( List of Apollo astronauts ) = 0.962; conf( Base on balls )=0.619 and conf( Moon )=0.547.
Thus, the entity  List of Apollo astronauts  has been identi ed as the principal entity of the query.
The  rst approximation of the result ranking is done by the document retrieval method combined with the expansion of the entities based on the activation spreading over the edges of the knowledge base.
As we have vertices with properties instead of documents as the input, we  rst discuss the construction of documents for entities of the knowledge base.
Then, we describe the identi cation of candidate entities for the query answer, the  rst approximation of the result.
Entity documents.
We deal with unstructured keyword queries, it would be convenient to take advantage of the information retrieval models that are able to handle such queries.
However, information retrieval models are designed for the document retrieval, whereas we have a knowledge base composed of vertices with properties and edges (relations) between the vertices.
Our approach is to construct entity documents, i.e., a textual representations of entities (vertices) in the given property graph knowledge base.
We construct documents by concatenating a de ned subset of properties of each vertex in the knowledge base.
Let P (cid:48)   P be the subset of properties selected for documents construction (P (cid:48) := P would be the default, general setting).
We can de ne a document for a vertex v   V as a union of selected properties: (cid:91) doc(v, G) =  (v, p) : p   P (cid:48) We can then de ne the similarity between an entity in a knowledge base and a keyword query as: vsim(v, q, G) = sim(doc(v, G), q) where sim(doc(v, G), q) can be substituted by a suitable information retrieval model, e.g., probabilistic or vector-space model.
We de ne the rank of a vertex v for a given query q and a property graph G as: rank(v, q, G) = |{vi   V ; vsim(vi, q, G) > vsim(v, q, G)}| Candidate entity score.
In order to identify entities that are likely to be a part of the query answer we compute candidate scores for entities of the knowledge base.
We exploit the document retrieval model and combine it with the link evidence stored in the of the CS allowed us to bring more entities that match the query into the result set (i.e.  Buzz Aldrin  and  Eugene Cernan ); the result set is still far from optimal, as the optimal answer should comprise the twelve moon-walkers at the top ranks.
We now describe the core of the proposed SemSet model that also gives it the name.
Let us assume the existence of semantic sets that comprise entities from the knowledge base G, and let us assume that members of such a semantic set S (we will refer to a semantic sets as a SemSet through the rest of the paper) are semantically related.
We will refer to a set of all SemSets as S(cid:48).
We explain how to construct such semantic sets from the knowledge base in the Section 5.
After the construction of a candidate set C (Section 4.2), we identify the SemSets that have at least a fraction p of its members in the candidate set C(q, G, k): CSS(q, G, k, p, L (cid:48) ) =
 (cid:48) : |S   C(q, G, k, L(cid:48))|
   p (cid:27) (cid:26) As we are searching for the entities that belong to a semantic set, the members of SemSets in CSS are good candidates for the query answer.
However, in practice there often are multiple SemSets in CSS, in which case we have to distinguish how well a SemSets  t the input query.
To measure similarity of a SemSet S and a query q, we construct a document for S by concatenation of entity documents of SemSets members: (cid:91) v S sdoc(S, G) = doc(v, G) and we measure the similarity of a SemSet S and a query q as: ssim(S, q, G) = sim(sdoc(S, G), q) where sim(sdoc(S, G), q) is a document retrieval model.
Let b be the SemSet boost parameter.
We can then compute the SemSet Score of an entity in a knowledge base as: ) = 1 + (b   (cid:88) SS(v, q, G, k, p, b, L (cid:48) ssim(S, q, G)) v S;S  CSS(q,G,k,p,L(cid:48)) Example of intermediate results.
Let us assume the setting in which we use sets of entities belonging to Wikipedia categories as SemSets.
We can compute the CSS, set of candidate SemSets, which would (in this setting and the example query) be: CSS = { Category: People who have walked on the Moon, Category: Skylab program}.
As we can see, we have two candidate SemSets with different content, one of which comprises the correct answer entities.
Let us assume the setting, where we use additional Sem-Sets, formed by entities that include the same Wikipedia templates in their respective articles.
In this setting, the situation would be even more complex, having: CSS = { Category: People who have walked on the Moon, Category: Skylab program, Template: NASA Astronaut Group 3, Template: NASA Astronaut Group 5, Template: NASA Astronaut Group 2, Template: People who have walked on the Moon}.
To resolve the issue on which of the SemSets from CSS  ts the query best, we can compute the textual similarity of the query and the SemSet document.
The top three ranked items in CSS, in our example setting are the following: 1.
Template: People who have walked on the Moon, 2.
Category: People who have walked on the Moon , 3.
Template: NASA Astronaut Group 3.
In this way, we discover the best matching SemSets.
The ranking of the vertices after the computation of the SS score would comprise Figure 1: Example of SC computation for a simple graph.
Parameter k = 4, the nodes n1,n2,n3,n7 (in this order) are the top k matches for document similarity to the query.
property graph, exploiting the principle of the activation spreading.
Informally, we take the top-k vertices with the highest similarity to the query and expand from those vertices by the edges, as de ned in the property graph knowledge base G. Let top(k, q, G) = {v   V : rank(v, q, G) < k} be the top-k items from G with highest similarity values with respect to the query.
A vertex v   V is assigned a score equal to the sum of scores of top k items that link to it.
Activation is spreading one hop from the vertices retrieved by the information retrieval part.
We restrict the spreading only to one hop from the given vertices because of the mathematical properties of the network.
The semantic network used in our experiments is a small-world network with a small diameter.
Thus, allowing activation spreading more than one hop from the given vertices results in activation of a large part of the network.
The score for an item in top k is proportional to its rank.
We de ne a base score to be proportional to the similarity rank of an entity document to a query: (cid:26) 1   rank(v, q, G)/k   rank(v, q, G) < k SB(v, q, k, G) =
 Let L(cid:48)   L be the set of edge labels (e.g.
equivalent to predicate types in RDF model) used for expansion (again, L(cid:48) := L would be the default setting).
We de ne the candidate score SC as: SC (v, q, G, k, L (cid:48) ) = SB(v, q, k, G) + SB(i, q, k, G) (cid:88)  (i,v) A: (cid:48)((i,v)) L(cid:48) An example of candidate scores of a simple graph is depicted in Figure 1.
We consider vertices with SC greater than zero as the  rst approximation of the result and we will refer to this set as the candidate set C: C(q, G, k, L(cid:48)) = {v   V : SC (v, q, G, k, L(cid:48)) >
 Example of intermediate results.
To illustrate the process on our example query we provide intermediate results for this phase.
The top  ve entities with the highest vsim(v, q, G) for our example query about the Apollo astronauts are: 1.
The Wonder of It All (2007  lm) 2.
List of spacewalkers, 3.
Moon Landing (music drama), 4.
List of Apollo astronauts, 5.
Harrison Schmitt.
Although the result set contains entities somewhat related to the query, the only high-ranked entity that should be part of the optimal result is  Harrison Schmitt .
However, several of the entities retrieved are connected by edges to the entities that should be part of the optimal answer (e.g.
List of Apollo astronauts).
After computing the SC, using the spreading of activation (from the 10 top ranked vertices), the top ranked entities according to SC are: 1.
Astronaut, 2.
NASA, 3.
Moon, 4.
Apollo 15, 5.
Apollo 12,
 n1n5n4n2n3n6n7bScore = 1.0in bScore= 0CScore = 1bScore = 0.75in bScore=0CScore =0.75bScore =0 .5in bScore=0.75+0.25CScore =1.5bScore =0 in bScore=1CScore =1bScore =0in bScore=1+0.75+0.5CScore =2.25bScore =0 in bScore=0.75+05CScore =1.25bScore =0.25 in bScore=0CScore =0.25WWW 2012   Session: Semantic Web Approaches in SearchApril 16 20, 2012, Lyon, France134Figure 2: Block scheme of the SemSets model computation process.
The upper block describes of model phases, while the lower block represents the data sets used in distinct phases.
Figure 3: SemSets patterns in a property graph structure.
A SemSet is a) vertices having outgoing edge of the same label to a common vertex, b) vertices having an incoming edge of the same type from a single vertex.
entities  Neil Armstrong ,  Buzz Aldrin  and the ten more astronauts who have walked the moon on top ranks, followed by other NASA astronauts, forming a good answer for the given query.
In the case that no SemSets are identi ed in the candidate set C(q, G, k, L(cid:48)), we have only the basic SC score relying on the document similarity and link evidence to rank the resources.
When no SemSets are found, it often means that the information required by the query is either not covered by the knowledge base or is covered by the knowledge base but not by the used SemSets (e.g., in case the given query is not a semantic type query) Where no Sem-Sets are identi ed but a principal entity of the query is identi ed given the property graph G, the information required by the query might be covered by the knowledge base.
This might be an indication that another semantic retrieval model would be more suitable for the given query, e.g., in cases when the input query is not a semantic type query.
However, we extend the SemSet model by an optional step, to improve the scores in such cases.
We propose boosting the scores of items in C(q, G, k, L(cid:48)) based on the similarity with principal entity pent(q, G) identi ed in the query analysis phase.
In this step, we consider structural similarity of the nodes in the knowledge base.
We can write: (cid:48) SP (v, q, G, k, c, L ) = 1 + (c   struct_sim(v, pent(q, G), L (cid:48) )) where c is the boost parameter for the principal entity relatedness and struct_sim is a graph structural similarity measure, computed taking into accounts edges with labels from L(cid:48).
Namely, we utilize cosine similarity of two nodes, where the vector space is de ned by the nodes  neighbours, as encoded in the underlying knowledge base.
We consider this measure as it has proven ef ciency for computing semantic relatedness [14].
Example of intermediate results.
As the SemSets were iden-ti ed during the SemSets processing for our example query, the computation of the principal entity relatedness score is just optional.
However, for illustration, we continue with our example.
The vertex corresponding to the Wikipedia article  List of Apollo astronauts  has been identi ed as principal entity of the query.
If we compute the structural similarity of this vertex and vertices in candidate set C, the top ranked vertices would be: NASA Astronaut Group 3, NASA Astronaut Group 5, Eugene Cernan, List of Apollo astronauts, Jack Swigert , John Young (astronaut).
As shown in the evaluation section of the paper, using SP score can improve slightly the overall performance of the proposed model.
The  nal SemSet model score is simply the multiplication of the three scores introduced in previous subsections; the base score SC and SS and SP .
The block scheme of the phases and the data resources used are depicted in Figure 2.
Let v   V be a vertex from the property graph G, let q be the keyword semantic type query.
The parameters of the model are: k, b, c, p, L(cid:48), where k is the parameter used in 4.2 , de ning number of top k ranked results used for candidate set creation; b, c are boost parameters; p de nes the fraction of SemSet members required to identify the SemSet from the candidate set and L(cid:48) de nes set of edge types used in computations related to the graph structure.
Thus, the score is: SemSetModelScore(v, q, G, k, b, c, p, L ) = (cid:48) SC (v, q, G, k, L(cid:48)) SS(v, q, G, k, p, b, L(cid:48)) SP (v, q, G, k, c, L(cid:48))

 So far, we have assumed the existence of SemSets, without describing how to obtain them.
We  ll the gap in this subsection; we propose two methods for SemSets identi cation.
The  rst one relies on the judgement of an expert user knowledgeable of the data set.
As wez mentioned earlier, SemSets are sets of semantically related entities from the underlying knowledge base.
As the knowledge base contains information on semantic relations between the entities, we should be able to construct such SemSets from the knowledge base.
We de ne a SemSet in a property graph structure by following graph patterns: a) a set of vertices connected by an outgoing edge of the same label to a common vertex, or b) a set of vertices that have an incoming edge of the same label from a single vertex.
We depict both cases in Figure 3.
Let us use again the DBpedia data set to provide a few examples.
Members of Wikipedia categories are often a good example of a SemSet, e.g.
the vertex in the property graph representing Wikipedia category  Category: People who have walked on the Moon , comprising 12 astronauts is the SemSet that provides the answer for the example query we use in the paper.
Members of a category are vertices connected by an outgoing edge labelled  dcterms:subject  to the vertex representing the category.
The example of a SemSet of the second type, where a set of semantically related nodes have the incoming edge with the same label from a single vertex can be e.g., members of a music band in DBpedia, where the vertices representing band members have incoming edge labelled  dbpedia-owl:bandMember  from the vertex representing the band.
The provided examples form sets of nodes that are usually perceived as semantically related.
However, we can  nd examples of sets matching one of the two described patters which comprise members that do not match the human intuition of being semantically related; e.g.
members of a category  Category:1947 births  comprising people born in 1947, or the vertices representing persons connected by an incoming edge labelled  dbpedia-owl:deathPlace  to a vertex rep-SemSets model phasesDatasetsQueryAnalysisCandidate Set constructionSemSets Score BoostRelatedness Score BoostKnowledgeBaseResourceDocumentsSemSetsSemSetsDocumentsKeyword queryRanked list of resultsViVkVlVmLjLjLjViVkVlVmLjLjLja)b)WWW 2012   Session: Semantic Web Approaches in SearchApril 16 20, 2012, Lyon, France135resenting a city.
Although, the semantics of such sets is clear, a human judge would probably estimate the semantic relatedness of the set members to be quite low.
Another problem is a practical one.
The sheer number of possible SemSets can be impractical and dif cult to handle.
In theory, even if we constraint the minimal size of a SemSet to two members, there could be as much m SemSets for a graph G, where m is the number of its edges.
A very large number of SemSets would be inconvenient in practice.
The expert user with a good knowledge of the data set, its scheme and data itself can be helpful in identifying the SemSets that are useful.
Although, it would not be reasonable to judge all possible SemSets one by one, the expert s knowledge can be exploited with just a small effort.
We argue that not all the edge labels (types) (equivalents of RDF predicates) are equally helpful in construction of SemSets and the expert can identify the relations, labelled edges in the property graph, that are likely to de ne good SemSets.
E.g.
in our experiments with DBpedia data set, we have used SemSets de ned by category membership edges and edges de ning the inclusion of Wikipedia templates.
As documented in the evaluation section, just by using those two edge labels for the SemSets de nition, the improvement over the baseline has been signi cant.
More formally, let T (cid:48) be the subset of edge labels or types de ned by an expert for construction of SemSets.
In terms of the property graph, we can write that superset of SemSets S(cid:48) is: S(cid:48)(G) = {S; S   V :  v   V  t   T (cid:48) : (( i   S : (i, v)   A   (cid:48)((i, v)) = t)   ( i   S : (v, i)   A   (cid:48)((v, i)) = t))}
 Although the SemSets identi cation by involving the expert user as described above is feasible, it requires the knowledge of the data set.
An automatic method for identifying sets of semantically related entities would be suitable to give us the means of handling input data sets without prior knowledge of its schema or semantics.
The semantic relatedness should re ect how a human user would judge the relatedness of the given concepts, documents or terms.
It is, by its nature, a fuzzy concept, but the intuition is that the relatedness should be capturable or correlate well by some similarity measure.
Thus, we have generated all possible sets matching the described patterns (Figure 3), comprising more than two vertices (entities) of the property graph knowledge base.
We have than measured the similarity of SemSets members using similarity measures established in literature.
The motivation is to study, whether we can use some of established similarity measures to automatically  lter the SemSets and thus reduce their number.
The goals of this exercise are twofold.
The  rst goal is to measure whether such and automatically reduced set of all possible SemSets achieves competitive results when used in the proposed SemSets retrieval model.
The second goal is to study the suitability of different measures for the task and the correlations of distinct measures.
The data we work with, the property graph, provides us with graph topology and we have also textual information, entity documents constructed for each vertex of the knowledge base, based on its properties.
We have studied the textual similarity of the the SemSets as well as the structural similarities of its members.
For textual similarity, we have used the cosine similarity of the term vectors of the constructed entity documents.
As we deal with a set of elements, not a pair, we have computed the average of the pairwise cosine similarity of the set members.
To study structural similarity, we have used four measures, the  rst two measure how community like is a set of vertices, the rest were adopted from work studying semantic relatedness.
The  rst structural measure is con-Figure 4: Similarity scores distribution.
The y-axis is in log scale, depicting the number of SemSets having the de ned similarity score.
ductance that is de ned as the ratio of edges outgoing from the given set of nodes to the rest of the graph and total number of edges outgoing from the given set of vertices.
Formally, let ai,j be ele-j V aij, ment of the adjacency matrix of G and a(S) =(cid:80) (cid:80) i S the conductance can be de ned as: (cid:80)  (S) = i S,j S ai,j min(a(S), a(S)) In the standard de nition of conductance the lower the value is, the better the community structure is.
As the other studied measures have image of their functions in < 0, 1 > with the semantics that a higher value means higher similarity, we use value 1-conductance in our experiments to facilitate the comparison.
 (S) =(cid:80) The second studied structural measure is the internal density, expressing how clique-like is the subgraph generated by the given set of vertices, i.e., it is a ratio of edges within the given set and the number of all possible edges within the set.
We can de ne the internal density as follows: let ai,j be element of the adjacency matrix for G, sgn(x) is the signum function, the internal density is i,j S;i(cid:54)=j sgn(aij)/(|S| |S   1|).
Let us note that we are dealing with a multigraph structure which allows for multiple edges between two nodes.
The provided de nition counts multiple edges between two vertices as one and excludes loop edges (where edge s incoming and outgoing vertices are the same).
Thus, the image of the function is the interval < 0, 1 >.
The third and fourth structural similarity measures were averages of pairwise Cosine and Jaccard similarity.
The vectors used for the computation are the vectors of the vertices neighbours, connected by outgoing edges.
This similarity measures were inspired by the study of the semantic relatedness [14].
In Figure 4, we depict the distribution of the similarity measures scores.
The x-axis is the interval < 0, 1 >, which is the image of all the similarity function, y-axis is the number of SemSets having the similarity score.
The values of similarity scores were rounded to one decimal place for the sake of plot clarity.
The plot shows that majority of the SemSets have low similarity scores for all the studied measures; thus, a simple thresholding can reduce the number of the SemSets signi cantly.
We have used the sets with the average pairwise textual similarity higher than 0.1, with the observation that the achieved precision is very close to the precision of the method when the user de ned
 sures.
TCS - textual cosine similarity; IDens - internal density; COND - conductance; SJC - structural Jaccard similarity, SCS - Structural Cosine Similarity Sim.
meassures
 IDens


 IDens COND



























 labelled edges were used to produce the SemSets.
The evaluation setting and results are described in detail in Section 6.4.
We have studied the correlation of distinct similarity measures.
We have computed the Spearman s correlation coef cient (values ranging from 1 to 1, where 1 means a perfect linear dependency) for each pair of the studied measures.
The results are depicted in Table 1.
The correlation is quite signi cant for the textual cosine similarity of the term vectors of entity documents and the structural cosine similarity of the neighbourhood vectors.
The implication is that the measures can be used interchangeably.
One is based on the textual similarity, the other on the topological similarity.
Thus, in case, when we the relations are de ned poorly in the given data set, we can use the textual similarity.
On the other hand, when the data set lacks in textual content, the structural similarity can be exploited.
In this section, we present the evaluation of the SemSets retrieval model.
We  rst discuss the SemSearch 2011 data set, and we describe our experimental setup.
We then present the results achieved by SemSets model, using sets de ned by an expert user and we compare them against the baseline, showing an important improvement in the precision.
We discuss how the different steps of the SemSets model affect the  nal result.
We compare the scores achieved by using distinct partial scoring functions of the SemSets model and their combinations.
We compare the results achieved by using different retrieval models in step 4.2, we study results produced by using different edge labels to generate SemSets.
Finally, we compare the SemSets de ned by the expert user and the automatically constructed SemSets, when used in the SemSets retrieval model.
The results shows that the SemSets acquired automatically have almost the same positive effect as the ones de ned by the user.
Yahoo!
SemSearch 20114 was a research challenge designed for the ad-hoc object retrieval from semantic data, triple collection (namely, Billion Triple Challenge 2009 data set (BTC)5).
The list search track of SemSearch 2011 challenge has been especially designed for the task of answering keyword semantic type queries from a collection of triples and as such, was an obvious choice for the evaluation of the SemSets model.
The data set consist of 50 keyword queries selected from the query log of a web search engine and the relevance judgements for the resources from BTC for each query.
All of the selected queries were judged by human evaluators to be queries with the intent to retrieve list/set of entities.
In addition, all of those queries were followed, in the query log, by a user click on a link leading to a Wikipedia page.
The latter indicates that the results for the queries are covered by Wiki-4http://semsearch.yahoo.com/ 5http://vmlion25.deri.ie/ pedia and we have thus decided to use DBpedia, a data set containing extracted structured information from Wikipedia, as our primary knowledge base.
The relevance judgements for the queries were produced in course of SemSearch 2011 challenge, by using a crowd-sourcing solution for human intelligence computation, i.e., the Amazon s Mechanical Turk.
The evaluators are human workers who gain a  nancial reward for completing a given task.
Workers evaluated the results submitted by teams participating in the Sem-Search 2011 challenge.
The relevance judgements were performed on resources from the BTC collection.
Each answer has been evaluated by  ve workers and the resources were assigned the values: 0, meaning non-relevant resource to the given query; 1, meaning that the resource is partially relevant to the query, and 2, denoting that resources is part of the correct answer for the given query.
As the results were evaluated by humans and non-experts, the judge-ments are not perfect.
Assignment of the partial relatedness (score 1) can be considered a bit random, as the concept of the partial re latedness is very subjective to a particular individual.
Also several semantic errors are present, some resources that should belong to the correct answer for a query are judged with the score 0.
In addition, for several queries there are no resources with the score of 2, simply because the correct answers were not present in the evaluated data set.
Despite those imperfections, the SemSearch data set is the best available data set for the evaluation of the ad-hoc semantic type queries and we use it without any modi cations or quality improvements.
This allows us a head to head comparison of the SemSets model with other approaches to the ad-hoc list search.
In the following, we describe the setup used for the evaluation of the SemSets model.
Query analysis.
The goal of the query analysis is to identify principal entity mentioned in the input query, which is a part of the used knowledge base.
To map segments of the ad-hoc textual query to the resources in DBpedia data set, we use the Wikipedia miner toolkit 6.
It is an implementation of the method proposed by Milne et al. [15] and was primarily designed to annotate the text with the Wikipedia topics.
Mapping from Wikipedia topics to DBpedia resources is a straightforward process.
In case of multiple resources being identi ed within the query, we use the one with the highest relevance score as the principal entity.
The method used is similar to the query fragmentation proposed in [7].
Results of query analysis are exploited in document retrieval for the query fragmentation and in the computation of the principal entity relatedness.
Construction of candidate entities scores.
In order to generate set of candidate entities (cf.
Section 4.2), we need to construct the entity documents for the vertices of the property graph knowledge base.
To do that, we use concatenated entity properties with textual content, the main part of the entity documents being covered by the DBpedia English abstracts.
We construct the index of the document collection created from the knowledge base using the Lucene framework7.
To study the impact of the text similarity model used in this step on the SemSets model performance, we have performed experiments with multiple models: 1) the TF-IDF based, standard Lucene scoring function8, 2) the Lucene standard scoring function exploiting the query fragmentation, provided by the query analysis phase (terms of the query fragment related to the principal entity of the query must occur in the document), 3) language modelling scoring, and 4) a combination of (2) and (3), where normalized 6http://wikipedia-miner.sourceforge.net/ 7http://lucene.apache.org/
 the wikilink type, which represents the hyperlinks between Wikipedia articles, as the link types L(cid:48).
The reason for using the wikilink edges is because the links between Wikipedia articles usually de ne some kind of semantic relationship, even though we are not able to extract the type of the relationship automatically; i.e., there is no relation of another type between the two resources in DBpedia.
SemSets score computation.
For the initial set of experiments, we use SemSets de ned by the expert users.
In order to construct SemSets, we use the links connecting the resources of the knowledge base to the resources representing Wikipedia categories (i.e.
 subject  predicate in the DBpedia dataset).
In addition, we use the Wikipedia templates and inclusion relations between templates and related DBpedia resources.
The hypothesis is that the template inclusions often holds semantic information on the resources and could potentially improve the quality of the knowledge base and consequently the quality of the SemSets model results as well.
Computation of principal entity relatedness score.
This step corresponds to Section 4.4.
We use the principal entity identi ed in query analysis phase and we use wikilinks as the edges in the computation of the structural cosine similarity between the principal entity and the resources in the candidate set.
The rest of the parameters were determined empirically.
Result postprocessing.
In order to use the SemSearch 2011 data set for the evaluation, we have to perform a  nal postprocess-ing step.This is needed because the computed results from the described setup comprise only resources from DBPedia, whereas the SemSearch relevance judgements were produced for the BTC 2009 collection.
The BTC collection comprise a large part of DBPedia snapshot from 2009 (but not the complete DBPedia data set), in addition it also comprises triples from other sources.
In order to be able to use the SemSearch relevance judgements, we need to map resources produced as results by our experimental setup to the resources of the BTC.
We do it by  ltering the result set, removing all the DBpedia resources that are not part of the BTC collection.
We than expand the  ltered data set by the sameAs links that are part of the DBpedia collection and maps the DBpedia resources to the resources from other data sets.
We  lter the expanded result set, and again keep only the resources that are part of the BTC collection as well.
After this postprocessing step, we can use directly the Sem-Search 2011 relevance judgements to evaluate results produced by the SemSets model.
In the evaluation of the SemSets model, we used the experimental setup as described in Section 6.2.
We have used the top 100 ranked resources from the results for all the queries and computed the mean average precision (MAP) against the relevance judge-ments from the SemSearch 2011 challenge (using TREC evaluation toolkit9).
We performed the experiments with different con gura-tions of the SemSets model setup to study the impact of distinct parts of the SemSets model on the  nal result.
The results have been computed with and without the query analysis phase; for the candidate set construction (as described in Section 4.2), we have studied multiple text similarity scoring functions; for the SemSets score computation, we have studied different link types to de ne SemSets (category membership and templates inclusion) and we have studied the scores of distinct partial functions of the SemSets model.
The baseline.
To our knowledge, there is no prior work focusing directly on answering semantic type queries from semantic data.
9http://trec.nist.gov/trec_eval Figure 5: MAP scores for partial functions of SemSets model.
Table 2: SemSet model precision on SemSearch 2011 data set.
Thus, we have decided to use the sate-of-the-art retrieval method focusing on a similar task as a baseline for the comparison.
Probably the most related retrieval task is the ad-hoc entity retrieval from semantic data, where the task is to rank the entities in the semantic knowledge base, based on an unstructured keyword query.
According to the results of entity search track of SemSearch 2011 challenge, the best performing method for this task is the BM25MF [2] method, a modi cation of the popular BM25F information retrieval model.
The BM25MF method achieved mean average precision of
 SemSets model precision.
We have tuned the parameters of the SemSet model on the  rst 15 queries of the data set.
The con guration has been the following: a) Wikipedia miner tool for the query analysis (Section 4.1); b) the combination of the language modelling approach and Lucene scoring function with the query fragmentation based on the query analysis step for the text similarity computation was used for candidate entities score (Section 4.2); c) the category membership and template inclusion links were used to generate the SemSets for SemSets score computation (Section 4.3); d) the other SemSets model parameters were set as follows: k = 12; p = 0.7; b = c = 100.
The resulting MAP score of the experiment for this setup has been 0.2795.
Given the baseline of 0.1591, this is a signi cant improvement in the precision of answering type queries from semantic data and fully justify specialized retrieval model for such queries.
It is the most important result of the presented work.
For completeness, the precision values at various ranks are summarized in Table 2 In the rest of the section we discuss the effect of different setup con gurations on the MAP score.
Partial scores.
In order to study the effect of the three scoring functions that are part of the SemSets model, we have performed runs with the use of different combinations of the scoring functions.
The base SC function can be interpreted as a naive baseline, where we just retrieve documents constructed for resources of a knowledge base, given a keyword query.
The results are summarized in Figure 5.
From the results, it is obvious that the main improvement of the  nal score is brought by the employment of the SemSets score SS function, which boosts the scores of SemSets members, which are present in the candidate set.
The principal entity score SP brings also modest improvement when combined both with base SC   SP and also with SC   SS   SP .
From the aggregated results, it might intuitively seem that the improvements in score added by SS and SP are independent of each other.
This
 Lucene standard scoring, LucF - Lucene scoring function with query segmentation, LM is the language modeling approach and Comb.
is the combination of the two latter functions.
intuition can be veri ed by inspecting scores of distinct queries.
We compute the improvement of the SC   SS as SC   (SC   SS).
From the 50 input queries, there is 0 or negative improvement for
 in the evaluation data set, which leaves us with 18 queries with 0 or negative improvement over the naive baseline; the introduction of the SP lead to a positive improvement over the baseline in 11 cases (meaning that in 61.1% of the queries, where the SS score does not brings any improvement to the baseline, the SP does.)
This indicates that SP boost is complementary to the SS boost.
As the main increase in MAP score is contributed by exploiting the SemSets and query similarity to the candidate SemSets (CSS, cf.
Section 4.3), it is only appropriate to ask whether a simpler solution would not have similar results; we could just retrieve the SemSets documents with the highest similarity to the given query and then produce results by listing the resources belonging to appropriate SemSets.
The short answer is, that such a solution does not work well.
Our experiment with this solution resulted in the MAP score of 0.0998.
Thus, constructing  rst the candidate set based on query similarity to individual entities of the knowledge base and their linked neighbours brings the important improvement in precisions when combined with the SemSets principle.
Impact of the query analysis.
The query analysis provides information exploited in successive steps in the SemSets model.
The results of the query analysis are used for 1) query segmentation in candidate set generation and 2) for determining the principal entity of a query.
To measure how the information from the query analysis impacts the performance of the model, we executed runs with and without the query analysis part.
All the other settings remained the same as in the original con guration, as described above.
The MAP of the result set without query analysis was 0.2434, that is
 Impact of the text similarity score.
In order to measure the impact of the text similarity scoring on the SemSets model, we have executed runs with the four different retrieval models described in Section 6.2.
The results are depicted in Figure 6.
The important observation is that the improvements in the document retrieval model brings signi cant improvement to the overall performance of the SemSets model.
In addition, combination of different retrieval models, with good individual performance, was very bene cial in terms of overall MAP score improvement.
Impact of the SemSets generation.
To assess the importance of the SemSets quality used in the model, we have executed runs with different SemSets, constructed from the property graph.
We study the SemSets de ned by category membership and by the template Figure 7: Impact of used SemSets.
 No SemSets  - no SemSets were used,  Temps  - SemSets based on template inclusion relations,  Cats  - SemSets based on the category membership.
inclusion.
The SemSets de ned by the category membership are constructed from entities that are connected by the link of  subject  type to the same vertex.
The template inclusion data set comprise links between DBpedia resources and the Wikipedia templates that are included by those articles.
Figure 7 depicts the results.
Constructed SemSets So far, we have presented the results obtained using SemSets de- ned by an expert user and we have shown an important increase in the model precision when using information on entity membership in those SemSets.
As the requirement of the user involvement lacks generality, we have proposed an approach how to identify semantically related sets from the knowledge base automatically.
It is done by identifying candidate SemSets by matching two graph structural patters and computing similarity measures of their members.
The important question that has been left unanswered is how well do the automatically identi ed SemSets substitute the SemSets de ned by the expert, knowledgeable about the data set.
To answer this question, we have performed experiment where the automatically iden-ti ed SemSets with the average pairwise textual cosine similarity of the entity documents higher than 0.1 have been used (the threshold has been chosen empirically).
The MAP value of the result set obtained by using this setting was 0.238 (the MAP value with user de ned SemSets was 0.2795).
Although this result is quite satisfactory, the loss of 0.0415 in MAP score did not correspond to our intuition after the inspection of several individual queries.
Our interpretation of this loss is the quality of the relevance judge-ments, where because of three values for relevance were allowed (0-non relevant, 2-relevant, 1-partially relevant).
As argued before, the assessments of the partially relevance vary highly depending on the human evaluator and the partial relevance judgements are a bit random.
When we did the evaluation of the results against the relevance judgements stripped of the resources assessed as partially relevant, we achieved the MAP score of 0.2611 with automatically acquired SemSets and 0.2664 with a user de ned SemSets.
The conclusion is that we can use automatically identi ed SemSets and achieve results comparable to the setting with SemSets de ned by experts.
There are several opportunities for the future work.
As our experiments show, the SemSets model is quite ef cient for answering semantic type ad-hoc queries.
It is not, however, a perfect  t for all ad-hoc semantic queries, e.g., for entity queries or entity attribute queries.
The important question is whether we can determine, by analysing the input keyword query, the suitability of the SemSets
 so ef ciently, we could combine SemSets model with other models or systems for ad-hoc semantic search which are suitable for other query types.
We could thus construct a more robust system for semantic search, combining multiple approaches for different query types.
Similar approach is used in IBM Watson [6], where multiple models are used to analyse the given question, each suggesting an answer and the con dence of the model.
The  nal results are produced by synthesis of the results of distinct models and information on the con dence.
E.g.
the SemSets model can be used when the model s con dence on type of the query and the correctness of the answer is hight, otherwise other models can be used for the given question.
The intuition is that when no SemSets are identi ed in the candidate set or the text similarity of the best candidate Sem-Set is low, it is highly probable that the SemSets model is not a good retrieval model for the given query and other models might be used to compute the answer.
Although preliminary examination looks promising, this requires thorough examination and it will be the main focus of our future work on the topic.
For eight queries in total, out of  fty in SemSearch 2011 data set, our implementation of the SemSets model had MAP value of 0, meaning that no correct answer has been discovered by the model.
The main reasons are: a) absence of the relevant SemSet in the used data set (the SemSet model yields precise results only when the required answer for the query is well covered by the data set in use), b) failure to retrieve relevant entities in the candidate generation phase (this leaves a room for the improvement in the used document retrieval model).
The proposed method relies heavily on the traditional information retrieval and demonstrates the value of IR techniques for the processing of semantic data, especially for the ad-hoc querying.
The lesson learned is that the rich textual information should be an integral part accompanying the semantic data, at least for the data sets where the ad-hoc querying is desirable.
In this paper, we have proposed the SemSets retrieval model for answering semantic type queries from a semantic knowledge base.
The model combines and exploits document representation of knowledge base resources, relations between the resources and their membership in semantic sets to compute ranks of distinct resources give a user keyword query.
The approach is complementary to the other research efforts on ad-hoc object retrieval, it also showcases the importance of the traditional information retrieval methods for ad-hoc querying of the semantic data.
The proposed model has been evaluated using the SemSearch 2011 data set, especially designed for the semantic list search evaluation.
To our knowledge, the proposed model has state-of-the-art performance on this data set and it brings important improvement in retrieval precision for the given task, compared to the baseline, the state-of-the-art retrieval model for entity search in semantic data.
We have also proposed two approaches for the identi cation of the semantic sets from the knowledge base.
The  rst one relying on the involvement of an expert user, the second one fully automatic.
As shown by the experiments the automatic approach has almost the same positive effect on the SemSet model performance as the one guided by the expert user knowledge.
Acknowledgement.This work is supported by projects VEGA 2/0184/10, VEGA No.
2/0054/12, VENIS FP7-284984.
