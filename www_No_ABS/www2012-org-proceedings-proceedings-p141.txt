A rapidly increasing amount of structured data can be found on the Web today.
This development is triggered by the Linked Data movement, Semantic Web community efforts, and recently, also enjoys strong support from large companies including Google, Yahoo!
and Facebook, and governmental institutions.
The amount of Linked Data alone is in the order of billions of RDF triples, residing in hundreds of data sources [11].
In this paper, we aim at supporting the Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
exploitation of these structured Web data.
In particular, we aim at extending vertical search capabilities beyond internal data to also incorporate external Web data into the retrieval process.
We illustrate the problem behind it based on the following scenario: There is a company running a movie shopping website.
Users can search for movies on this website via form-based interfaces, and their requests are internally executed as structured queries against the company s dataset.
Now, the company aims to exploit the numerous Web data sources available as Linked Data, including data provided by a partner company with similar o erings and an encyclopedia dataset that contains additional movie related information.
The goal is to incorporate data from these external sources into the search processes.
However, the vocabularies and structure exhibited by these target data sources are di erent such that issuing the same structured queries (called seed queries) against these external sources may not produce any results.
Results satisfying the information needs behind these seed queries may exist but due to mismatches in structural and syntactical representation, they cannot be found.
In this paper, we study three di erent strategies that are applicable to this search scenario: (1) There are Information Retrieval (IR) solutions, which treat both the data and queries as bags of words [5, 20].
Because structure information is ignored during query processing, this strategy (called keyword search) often leads to nonempty results   albeit with varying quality.
(2) The alternative is to employ database solutions, where information needs are expressed as structured queries.
Given the richer representation of the information needs, the structure of the underlying data can be exploited and incorporated into the matching process.
While this can improve the quality of the results, this type of solutions requires upfront investment in data integration, i.e. computation of ontology and schema mappings and consolidation of data instances that refer to the same object (entity mappings) [8, 9, 13, 4].
Based on these mappings, results from external sources can be obtained via query rewriting [3, 23].
Integration e orts are needed whenever the data changes.
Clearly, integration on the Web is hard due to the large number of sources and their scale as well as their heterogeneity regarding di er-ences at the schema and data level, which is illustrated for our scenario in Figure 1.
Here, entities representing movies are displayed.
One can observe that three di erent representations of  Steven Spielberg  are used for the same real-world object.
Also, di erent labels are used to express the same attribute.
(3) As the third category, we elaborate on a hybrid solution, which combines the  exibility of unstructured IR solu-di erently at the schema level (e.g.
actors vs. starring ) and data level (e.g.
Spielberg, Steven vs. Steven Spielberg ).
tions (in the sense that no prior data integration is needed) and the expressiveness of database-style querying by incorporating the structure of the underlying data.
The idea is to start with a structured seed query speci ed for one particular source.
Based on the content and structure of the results obtained from this source, we construct an Entity Relevance Model (ERM) that can be seen as a compact representation of relevant results mirroring the underlying information need.
Instead of relying on upfront computed mappings for rewriting the structured seed query, we treat the seed query as a keyword query and submit it against external data sources to obtain additional results.
These candidates are obtained using a standard IR-based search engine.
Then, we create mappings between the structure of each candidate result and the structure of the ERM on the  y.
These mappings are used for an additional round of matching and ranking.
Candidates which more closely match the content as well as the structure captured by the ERM are ranked higher.
Thereby the structure of the ERM and of the result candidates are incorporated into the search process.
Since, the same similarity metrics for creating the mappings are reused for ranking, this on the  y integration comes for free.
As a result, this hybrid strategy not only takes structure information into account for more effective search, but also provides on the  y computed mappings that can support a pay-as-you-go integration paradigm where data integration is tightly embedded into the search process [16].
Contributions.
The contributions of this work can be summarized as follows: (1) We perform a systematic study of the two main prevailing strategies towards searching external heterogeneous data sources.
In particular, we show how to adopt the data integration approach to our scenario where the computation of entity mappings is challenging.
(2) To achieve the best of both worlds, we elaborate an a hybrid approach that does not rely on upfront data integration, but uses a query-speci c Entity Relevance Model (ERM) for searching as well as for computing mappings on the  y.
(3) Based on large-scale experiments using real-world datasets, we observe that the data integration approach consistently provides better results than keyword search.
The hybrid approach yields best results, outperforming keyword search by 120% and the data integration baseline by 54% on average in terms of Mean Average Precision.
Further, this hybrid approach is able to leverage upfront integration results, leading to additional quality improvement when pre-computed mappings are considered.
The qualitative di er-ences between these approaches are: Keyword search and the hybrid approach do not require upfront data integration.
Additionally, the hybrid approach provides on the  y computed mappings that can be used for a pay-as-you-go integration process that can exploit user feedbacks for quality improvement (as discussed in [16]).
Outline.
Section 2 de nes the research problem and gives an overview of existing solutions and brie y sketches our new approach.
This approach of relevance based on the  y mappings is presented in detail in Section 3.
Evaluation results are presented in Section 4.
Section 5 discusses the related work before we conclude in Section 6.
In this section, we present the setting of the addressed problem, and provide an overview of three di erent solutions.
The problem we address is situated in a Web data scenario.
The kind of Web data that is of most interest is RDF data.
For reasons of generality and simplicity, we employ a generic graph-based data model that omits speci c RDF features such as blank nodes.
In this model, entity nodes are RDF resources, literal nodes correspond to RDF literals, attributes are RDF properties, and edges stand for RDF triples: Data Graph.
The data is a directed and labeled graph G = (N, E).
The set of nodes N is a disjoint union of entities NE and literals NL, i.e. N = NE (cid:93) NL.
Edges E can be conceived as a disjoint union E = EE (cid:93) EL of edges representing connections between entities, i.e. a(ei, ej)   EE, i  ei, ej   NE, and connections between entities and literals, i.e. a(ei, ej)   EL, i  ei   NE and ej   NL.
Given this graph, we call the set of edges A(ei) = {a(ei, ej)   E} the description of the entity ei   NE, and each member a(ei, ej)   A(ei) is called an attribute of ei.
The set of distinct attribute labels of an entity ei, i.e. A(cid:48)(ei) = {a|a(ei, ej)   A(ei)}, is called the model of ei.
It is clear that this notion of data graph is su ciently general to capture not only RDF but also other types of Web data.
For instance, data in a relational database can be mapped to this model by representing tuple ids as entity nodes, other tuple values are literal nodes that are connected to the corresponding ids that are in the same tuple, and foreign key relationships are captured as connections between entity nodes.
Data Heterogeneity.
Web data reside in di erent datasets, each represented by a data graph.
Typically, real-world Web datasets exhibit heterogeneity at the schema and the data level.
At the data level, entities in di erent datasets, which refer to the same real-world object, may have di erent descriptions.
Di erences at the schema level occur when the same entity is represented in di erent datasets using attributes with di erent labels (di erent models).
As mentioned in the introduction, Figure 1 exempli es this het-a:Movie type Steven Spielberg a:Directors ea Amazon a: DBpedia db: IMdb i: Munich a:Title Daniel Craig, Eric Bana DVD a:Actors a:Binding 2005 a:ReleaseDate i:movie type Spielberg, Steven (I) i:directors ei E.T.
(1994) i:title Coyote, Peter i:actors i:producer Spielberg, Steven (I) db:Film type db:director ed 1941 (film) rdfs:label db:starring db:John_Candy_(actor) db:Steven_Spielberg WWW 2012   Session: Semantic Web Approaches in SearchApril 16 20, 2012, Lyon, France142erogeneity exhibited by real-world datasets.
Dealing with these types of heterogeneity requires data integration.
For this, a large body of work on schema alignment and entity consolidation (record linkage) can be leveraged to compute mappings between data sources [8].
While mappings of varying semantics have been proposed, the most basic and commonly used one asserts that two elements (schema elements or entities) are the same (i.e. same-as mappings).
Given this model of Web data, structured queries can be speci ed to search over such datasets.
The most commonly used language for querying RDF data on the Web is SPARQL [1].
One essential feature of SPARQL is the Basic Graph Pattern (BGP).
Basically, a BGP is a set of conjunctive triple patterns, each of the form predicate(subject, object).
They represent patterns because either predicate, subject or object might be a variable, or is explicitly speci ed as a constant.
Answering these queries amounts to the task of graph pattern matching, where subgraphs in the data graph matching the query pattern are returned as results.
Predicates are matched against edges in the data graph, whereas bindings to subjects and objects in the query are entity or literal nodes.
One particular form of BGP with high importance are so-called entity queries.
Essentially, they are star-shaped queries with the node in the center of the star representing the entity (entities) to be retrieved.
Figure 6 provides 3 examples.
According to a recent Web query logs study performed by Yahoo!
researchers, queries searching for entities constitute the most common type on the Web [18].
Also, most of the current Semantic Web search engines such as Sig.ma 1 and Falcons [5] focus on answering these queries.
For the sake of clarity, we also focus on this type of queries in this paper to illustrate the main ideas underlying our approach.
Later, we will point out how our approach can be extended towards supporting general graph patterns.
This however requires more complex algorithms for searching paths between entity nodes matching the query keywords (i.e. matching the seed query represented as keywords).
Problem.
Based on her knowledge about the schema and data of one particular source (e.g.
the one owned by the company in our scenario), it is possible for a programmer or expert user to specify complex entity queries that speci cally ask for information from this source.
It is however not trivial to exploit external datasets for this kind of entity search when they exhibit heterogeneity at the schema and data level as discussed before.
The problem we tackle is  nding relevant entities in a set of target datasets Gt given a source dataset Gs and an entity query qs adhering to the vocabulary of Gs.
Clearly, if all datasets exhibit the same schema and data representation, then qs can directly be used to retrieve information from Gt.
When this is not the case, the following di erent solutions can be applied.
Keyword Search (KW).
The  rst and most widespread solution to this end is to use keyword search over so called  bag-of-words  representations of entities [20, 5].
That is, the description of an entity is simply a bag of terms.
A query is also represented as terms, which is then matched against the term-based representation of the entities.
Clearly, this Figure 2: KW : A structured query (1) transformed into a keyword query (2) and matched against bag of words representations of entities (3).
Figure 3: QR: A query for Amazon is rewritten into a query for DBpedia with constants being replaced with variables, and the missing mapping results in an  empty  triple pattern.
approach is simple but also  exible in the sense that the same keyword query speci ed for Gs can also be used for Gt because results from Gt can be obtained when there are matches at the level of terms.
As illustrated in Figure 2, this approach ignores structure information and vocabulary mismatches.
Structured Query Rewriting (QR).
Another view on this retrieval problem is the database perspective.
Here, structure information in the entity descriptions is taken into account.
However, this also requires the query to be fully structured.
The strategy to query over multiple datasets and to deal with data heterogeneity here is to rewrite the structured seed query qs into a query qt that adheres to the vocabulary of the target dataset Gt   Gt.
For this, same-as mappings are computed using entity consolidation and schema mapping tools [8, 9, 13, 4].
Then, predicates and constants in qs referring to attributes and entities in Gs are replaced with predicates and constants representing corresponding attributes and entities in Gt.
While this strategy can exploit the  ne grained structure of data and query, it relies on upfront data integration, which is problematic in the Web scenario because Web datasets are heterogeneous and evolve quickly.
In our experiment on the datasets prepared for the Billion Triple Challenge2 for instance, we observe that state-of-the-art entity consolidation approaches [4] do not scale well to large datasets [21].
In particular, they are focused on the single-domain setting such that for these heterogeneous datasets (where many of them exhibit only small pairwise overlaps at the schema level), only a relatively small amount of correct mappings could be produced.
Thus, rewriting constants using entity mappings is especially challenging in this scenario.
In fact, it has been recognized that integration at the Web scale is too complex and resource-intensive to be performed completely upfront [16].
A more practical strategy to deal with this dynamic and large-scale environment is to perform integration as you go [16], i.e. at usage time as the system evolves.
In this regard, an alternative solution is to precompute schema mappings only.
Then, entity mappings 1http://sig.ma/ 2http://challenge.semanticweb.org/ a:Movie type  Rainer Werner Fassbinder  a:Directors ?x Amazon a:
 a:Theatrical ReleaseDate directors rainer werner fassbinder theatrical release date
 type movie e1
 IMdb i: Rainer Werner Fassbinder Veronika Voss i:released Itle veronika voss director rainer werner fassbinder released
 i:Itle i:director e2 i:movie Spielberg, Steven
 Schindlers Liste (1994) type Itle schindlers liste
 director spielberg steven type movie i:Itle i:director (3) (2) (1) e1 e2 db:director ?x DBpedia db: type ?y ?z Amazon a: Dbpedia db: a:Directors = db:director a:Title = db:name A:Actor = db:starring   =   Schema Alignment Tool Schema Amazon Schema DBpedia a:Movie type  Rainer Maria Fassbinder  a:Directors ?x Amazon a:
 a:Theatrical ReleaseDate time.
Figure 3 illustrate this: Schema mappings are used to rewrite the query, triple patterns for which no corresponding schema-level mappings exist are omitted, and constants are replaced with variables (instead of being replaced with constants that adhere to the vocabulary of the target source).
The resulting query captures only structure constraints of the original query and thus, produces possibly much more results than a query where constants are also rewritten.
To achieve that, a standard IR search engine can be leveraged to limit the results to only those, which match the constants expressed as keyword queries.
That is, the constants that have been replaced by variables in the  rst step, act as a keyword query in the second step to perform on the  y entity consolidation, i.e. to  nd entities in Gt, which match the entities in Gs as represented by the constants (such as  Rainer Maria Fassbinder 1982  in the example).
Our approach.
In this paper, we present a framework to address this problem of querying heterogeneous Web data using on the  y mappings computed in a pay-as-you-go fashion based on entity relevance models.
This framework is instantiated involving the following four steps.
(1) First, we compute an ERM from the results returned from the source dataset Gs using qs.
(2) Second, we treat qs as keywords and using a standard IR-based search engine, we obtain result candidates from the target datasets Gt.
(3) Then, a lightweight on the  y integration technique is employed, which maps the structure of result candidates to the structure of the ERM.
(4) Finally, the result candidates are ranked according to their similarity to the ERM using the mappings computed at runtime.
as label starring director released language type (2) et at i:title i:actors i:directors i:producer type k(as)





 w : Ps(w|as) world:0.2, on:0.2, wires:0.2, .
.
.
klaus:0.25, l owitsch:0.25, barbara:0.25,. .
.
rainer:0.33, werner:0.33, fassbinder:0.33
 german:1  lm:1 w : Pt(w|at) e:0.33, t:0.33, 1994:0.33 coyote:0.5, peter:0.5 spielberg:0.33, steven:0.33, i:0.33 spielberg:0.33, steven:0.33, i:0.33 movie:1 Figure 5: (1) ERM constructed from the entities e1, e2 of Figure 4.
The ERM has a  eld for each attribute with label as.
Each  eld is weighted with k(as) and has a language model Ps(w|as) de ning the probability of w occurring in  eld as.
(2) Representation of the entity ei of Figure 1 with language models for each attribute labeled at.
a probability to every word w in the vocabulary.
Considering the underlying statistical process that leads to the generation of query and document samples, the corresponding query and document language models can be reconstructed when the samples are large and representative.
A Maximum Likelihood Estimator is often used for this.
For example, given a document corpus C, the vocabulary of terms V , the language model P (w|D) representing document D   C can be estimated as follows: Figure 4: Example set Rs of two entities e1, e2 obtain for query qs n(w, D) P (w|D) =   |D| + (1    )P (w|C) (1) where n(w, D) is the count of word w in D, |D| is the document length, and P (w|C) is a background probability, which is used for smoothing controlled by the parameter  .
While a query Q may be too short as a sample, it has been shown that pseudo-relevance feedback (PRF) results obtained for it can serve as a representative sample of the information need, from which a query model (called relevance model) can be reconstructed.
Thus, instead of the query, a relevance model P (w|Q) is reconstructed of PRF results [14].
For ranking, a document D is considered relevant to a given query Q, if their probability distributions are close in  distance .
One way to achieve this is using the negative cross-entropy  H:
 w V


 P (w|Q) log P (w|D) (2) In this section, we present how the entity relevance model is constructed and discuss how this model can be exploited for ranking and relevance-based on the  y data integration.
We aim at building a model that captures the structure and content of entities relevant to the information need, which is expressed in the seed entity query qs.
The proposed model is called the Entity Relevance Model (ERM ).
The ERM builds upon the concept of language model, a statistical modeling technique frequently applied in Information Retrieval tasks.
We start with a brief overview of language models for IR (see [17] for more details).
Language Model.
The main idea here is to see documents and queries as samples from di erent probability distributions also called language models.
More precisely, a language model is a multinomial distribution, which assigns We adopt this modeling approach to the problem of searching structured Web data.
Our goal is to model both the structure and the content of entities.
The idea behind the ERM is to represent the attribute structure of entities by a set of language models, and each language model captures the content of the respective attribute.
Hence, instead of using language models to represent entire documents, we use them for modeling attribute values.
Entity Relevance Model.
The ERM = (Rs, As,Ps) is a composite model consisting of a set of entities Rs   NE, a set of attributes As   E, and a set of language models Ps.
Each Ps   Ps is associated with a weight de ned through the function k : Ps   [0, 1].
The entities Rs are obtained by submitting the query qs against the source dataset Gs and used as pseudo-relevance feedback.
As denotes the set of all distinct attribute labels that are associated with the entities Film type Rainer Werner Fassbinder director e1
 label World on Wires released Klaus L witsch starring type director e2
 Veronika Voss released label German language Barbara ValenEn starring tribute label as   As, we compute a corresponding language model Ps(w|as)   Ps and its weight k(as).
The language model Ps(w|as) speci es the probability of any word w   V occurring in the nodes of data graph edges with label as, where V is the vocabulary of all words.
Let N (ei, as) be the set of nodes that is connected with ei through edges with label as, i.e. N (ei, as) = {ej|as(ei, ej)   E}, we compute Ps(w|as) from all entity descriptions for ei   Rs as follows:

 ei Rs

 ej N (ei,as) n(w, ej) ej N (ei,as) |ej| ei Rs (3) Ps(w|as) = where n(w, e) denotes the count of word w in the node e and |e| is the length of e (the number of words contained in e).
The outer sum goes over the entities ei   Rs and the inner sum goes over all values ej of attributes with labels as.
Thus, entity descriptions, which do not have the attribute as, do not contribute to Ps(w|as).
In order to capture the importance of these attribute-speci c language models, we compute k(as) as the fraction of entities having an attribute with label as: k(as) = n(as, Rs) |Rs| (4) where the numerator denotes the number of entities having an attribute with label as and the denominator is the total number of entities in Rs.
In summary, an ERM can be seen as a query speci c model built from pseudo-relevance feedback entities retrieved for the seed query qs.
An example for an ERM constructed from two entities is illustrated in Figure 5 (1).
We tackle the problem of searching over heterogeneous data in a way similar to entity consolidation.
That is, given the results es   Rs from the source dataset obtained for the seed query, we aim at  nding entities in the target datasets which are similar to Rs.
We use the ERM as the model of those relevant results.
In particular, we estimate which entities et of Gt are relevant for the query qs by measuring their similarity to the ERM and rank them by decreasing similarity.
We model a candidate entity et analogously to the ERM: et = (At,Pt) where At = A(cid:48)(et) is the set of attributes of et and Pt is a set of language models.
Similar to the ERM, Pt contains a language model Pt(w|at) for each distinct attribute label at   At.
Let N (at) be the set of value nodes of the attribute at, i.e. N (at) = {ej|at(et, ej)   E}, Pt(w|at) is estimated as follows: Pt(w|at) = ej N (at) n(w, ej) ej N (at) |ej| (5) Here, the sum goes over all values e of attributes with label at, n(w, e) denotes that number of occurrences of w in e, and |e| denotes the length of e. Figure 5 (2) illustrates an example.
We calculate the similarity between the ERM and a candidate entity et by measuring the  distance  between a language model of ERM and a language model of et using the (negative) cross entropy  H.
We sum over these  distances  and weight each summand by k(as) and the parameter  (as):

 Sim(ERM, et) =
 as As  (as)   k(as)   H(Ps(w|as)||Pt(w|at) (6) The parameter   gives us the  exibility to boost the importance of attributes that occur in the query qs as follows:   1 b  (as) = if as /  qs if as   qs, b   1 (7)
 w V In particular, we apply this similarity calculation only when we know which attribute label as of ERM should be matched against which attribute at of et.
We address this problem in the next section and show how the ERM can be exploited to create on the  y schema mappings, i.e.
mappings between an attribute at and a  eld as of ERM .
Equation 6 applies to corresponding pairs of attribute and  eld.
If there is no mapping between as and at, then we use a  maximum distance .
This distance is computed as the cross entropy between Ps(w|as) and a language model that contains all words in the vocabulary but the ones in Ps(w|as).
For constructing the language models of the ERM and of the candidate entities, a maximum likelihood estimation has been used, which is proportional to the count of the words in an attribute value.
However, such an estimation assigns zero probabilities to those words not occurring in the attribute value.
In order to address this issue, Pt(w|at) is smoothed using a collection-wide model cs(w), which captures the probability of w occurring in the entire dataset Gs.
This smoothing is controlled by the Jelinek-Mercer parameter  .
As a result, the negative cross entropy  H is calculated over the vocabulary V of  eld as as: H(Ps||Pt) = Ps(w|as)   log(     Pt(w|at) + (1    )   cs(w) ) (8)
 We want to determine which attribute of an entity needs to be compared to a given  eld of the ERM constructed for qs.
The ERM is not only used for search, but also exploited for this alignment task.
The details for computing mappings between entity attributes at   At and ERM  elds as   As are presented in Algorithm (1).
The rational of the algorithm is that a  eld as is aligned to an attribute at when the cross entropy H between their language models is low, i.e. a mapping is established, if H is lower than a threshold t (normalized based on the highest cross entropy, line 12).
The algorithm iterates over n   r comparisons in worst case for an ERM with n  elds and an entity with r = |A(cid:48)(et)| attribute labels.
Note that n and r are relatively small (see Table 1 and Table 3) because this algorithm operates only on entities that are requested as part of the search process compared to full edge upfront integration that takes the entire schema into account.
Further, ranking requires the same computation (Equation 8) and thus the entropy values computed here are kept and subsequently reused for ranking.
Moreover, for a faster performance, ERM  elds having a weight of k(as) < c can be pruned due to their negligible in uence (see Section 4.6 and 4.7).
In addition, existing mappings can be reused to reduce the number of comparisons even further.
h   H(Ps(w|as)||Pt(w|at)) candM appings.add(at, h) candM appings := new OrderedByV alueM ap for all at   A(cid:48)(et) do Algorithm 1 On the  y Alignment Input: ERM , Entity et, Threshold t   [0, 1] Output: M appings A := {(as, at)|as   As, at   At   null}
















 end if 18: end for
 end for bestA   candM appings.f irstV alue worstA   candM appings.lastV alue if bestA < t   worstA then at   candM appings.f irstKey A.add(as, at) else A.add(as, null) end if // If not already aligned // see equation (8) // no mapping found

 In this section, we report on the experiments conducted with the three solutions discussed in Section 2.
We experimented with di erent parameter settings and observed that performance is stable when the employed parameters are in certain ranges (will be discussed in Section 4.6).
Results reported in the following are obtained using the con gura-tion: b = 10, c = 0.8, t = 0.75.
The smoothing parameter  , whose e ect on retrieval performance has been studied extensively for IR tasks, was set to 0.9, a common value used in literature.
We follow the Cran eld [6] methodology for the experiments on the search e ectiveness and adopt the same methodology to analyze the e ectiveness of mapping computation.
Our experiments were conducted with 3 RDF Web datasets, DBpedia 3.5.1, IMdb, and Amazon.
In every experiment, one of them serves as the source dataset and the other two represent the target datasets.
DBpedia is a structured representation of Wikipedia, which contains more than 9 million entities of various types, among them about 50k entities typed as  lms.
The IMdb and Amazon datasets are retrieved from www.imdb.com and www.amazon.com [23], and then transformed into RDF.
The IMdb dataset contains information about movies and  lms, whereas the Amazon dataset contains product information about DVDs and VHS Videos.
These three datasets are representative for our Web scenario because a vertical search application running one of these datasets (e.g.
the one owned by the company in our scenario) could bene t from incorporating the other two into the search process.
Further, the datasets exhibit the heterogeneity previously illustrated in Figure 1.
Table 1 gives details about each dataset.
Dataset #Entities Amazon IMdb DBpedia





 #Distinct Attribute Labels



 |A(e)|  StdDev.
Table 1: Dataset statistics
 Our goal is to  nd relevant entities in the target datasets Gt for a given query qs.
In this setting, we can determine the relevant entities in Gt by manually rewriting the query qs to obtain a structured query qt adhering to the vocabulary of Gt   Gt.
Figure 6 shows such a set of queries, one of the queries serves as the source query qs and the results of the other two queries capture the ground truth for the retrieval experiments.
We created three query sets, each containing 23 SPARQL BGP entity queries of di erent complexities, ranging from 2 to 4 triple patterns that produce a varying number of results, see Table 2.
The queries represent information needs like retrieve  movies directed by Steven Spielberg ,  movies available in English and also in Hebrew , or  movies directed by Rainer Werner Fassbinder, which were released in 1982 .
The last query is illustrated in Figure 6.
Rel.
Entities Amazon IMdb DBpedia max avg.
median min











 Table 2: Results per query and dataset.
Source Dataset Amazon IMdb DBpedia |ERM|   StdDev.
Table 3: Average number of  elds of an ERM.
We implement the strategies as discussed previously in Section 2.
Keyword Query (KW).
IR style keyword search on Web data has been proposed [20, 5] and implemented as an adoption of Lucene3, an IR engine, which applies a document and query length adjusted TF/IDF-based ranking function.
We use the Semplore implementation [5], which uses a virtual document for every entity description and use the concatenations of attribute labels and attribute values as document terms.
In the same way, we transform the structured query into a keyword query by using the concatenations of predicates and constants of the structured query as terms.
The resulting keyword query retrieves all virtual documents representing entity descriptions, which contain some of the corresponding terms.
Query Rewriting (QR).
This system is based on query rewriting using precomputed schema mappings.
We created same-as mappings with the tools Falcon-AO [13] and Aroma [7] using their default con gurations.
Table 4 shows the number of mappings between the datasets.
Then, to rewrite constants at runtime as discussed, we apply the KW baseline on top to limit the search results produced by the rewritten query to those that match constants formulated as a keyword query.
Datasets Falcon-AO[13] Aroma[7] Amazon-IMdb Amazon-DBpedia IMdb-DBpedia


 Table 4: Number of mappings.
Hybrid (ERM).
3 di erent versions are employed: (1) ERM computes mappings on the  y.
(2) ERMa relies entirely on the alignment computed upfront by Falcon-AO.
This version of ERM can be seen as a combination of our approach and query rewriting that mimics the QR baseline.
The precomputed mappings are used 3http://lucene.apache.org to obtained a rewritten query, which is processed to obtain results.
However instead of using keyword search on top, we use the ERM and apply our approach for ranking.
(3) ERMq combines these two approaches.
It uses pre-computed mappings and creates additional mappings on the  y for those attributes, which could not be mapped upfront.
We use the standard IR measures precision, recall, mean average precision (MAP) and mean reciprocal rank (MRR).
We retrieve the top  ve thousand entities using the initial keyword search, rank them, and compute the metrics based on the top one thousand entities returned by each system.
The results for six di erent retrieval settings are shown in Figure 7: First, we examine the scenario without prior data integration.
Here,  nding relevant entities in the target dataset is only possible with KW or ERM .
When comparing their results (Figure 7), we observe that ERM outperforms KW across all metrics and retrieval settings and improves over KW by 120% on average in terms of MAP.
Looking at the di erent retrieval settings, we can see that ERM performs best between IMdb and Amazon (i.e. when IMdb or Amazon are either source or target dataset), where MAP are
 datasets hold only entities from similar domains, movies and DVD/Videos, and describe them using similar attributes.
DBpedia seems to be the most di cult one, mainly due to its schema complexity: It is very heterogeneous, containing information about di erent types of entities.
Thus, whereas only one type have to be considered in the other datasets, identifying the relevant types out of a much larger set of possible candidates is also part of the retrieval problem here.
Further, entities in DBpedia often exhibit redundant attributes with same values, e.g.
name, title and rdfs:label, which leads to higher ambiguity during the computation of mappings.
Across all retrieval settings, ERM yields MAP above 0.5.
Also similarly good performance could be achieved for MRR and P@10, which consider the top of the ranked results.
The robustness of the retrieval performance of ERM can be observed in Figure 8, which shows the interpolated precision across the recall levels.
It can be observed that precision is fairly stable over di er-ent recall levels.
One exception is the setting with IMdb as the target and DBpedia as the source dataset (Figure 8(e)).
Here, performance decreases notably at recall levels above
 have much more relevant entities than others, and the rank of some entities obtained for these queries were relatively very low.
However, P@R, where R is the number of relevant entities, is still above 0.5 even for this setting (Figure 7(c)).
In the next scenario, we examine the performance in the presence of precomputed alignments.
Now, applying QR to retrieve entities is possible.
This system considerably outperforms KW .
Using pre-computed alignments with the hybrid approach, ERMa, yields slightly better performance than ERM on average (see Figure 7).
Both, ERM and ERMa outperform QR on average by 54%, respectively 59% in terms of MAP.
The performance of ERMa diverges from ERM most notably in two cases: ERMa is worse if IMdb and Amazon are involved.
It is better in the retrieval setting with IMdb and DBpedia.
This is because in the latter, the alignment problem that has to be solved as part of searching is more di cult due to the higher ambiguity and complexity introduced by DBpedia.
Thus, applying the rewritten query using precomputed mappings to produce candidate results yields better performance.
This e ect can be observed in Figure 8(e).
The strategy of combining the advantages of pre-computed mappings and computing alignments on the  y implemented by ERMq outperforms the others across all metrics (see Figure 7).
We assessed the mappings computed on the  y during the previously discussed experiments.
First, we collected all mappings and manually determined the ground truth based on the pooled mappings.
Since we operate on heterogeneous datasets, multiple correct mappings for one attribute are possible, e.g.
title in one dataset might correctly corresponds to title, name and label in another dataset.
Given this ground truth, we computed precision and recall of the mappings created between the  elds of an ERM and the attributes of an entity.
Table 3 shows the average size of an ERM and Table 1 provides the average description size of an entity.
Precision and Recall are here de ned as follows: P recision = (9) |{correct mappings}| |{created mappings}| |{correct mappings}| Recall = |{possible, correct mappings}| (10) where {possible, correct mappings} is the set of mappings, which could be established between the ERM and an entity as captured by the ground truth.
We computed precision and recall for each individual entity considered during search, averaged over the query and  nally over the entire query set.
Overall, mappings obtained for 115k entities and the ERMs are taken into account.
Figure 9(a) shows precision and recall for the di erent retrieval settings.
Averaging over all entities, precision is 0.46 and recall is 0.12.
However, we are primarily interested in the entities, which are actually relevant.
Therefore, we examine precision and recall only for these relevant entities.
Here, the average over all scenarios is 0.70 for precision and 0.30 for recall, as shown in Figure 9(a).
Figure 9(b) gives the average number of actual mappings created between the ERM and entities, and be-a:Movie type  Rainer Werner Fassbinder  a:Directors ?x db:director ?x db:Rainer_Werner_Fassbinder i:directors ?x  Fassbinder, Rainer Werner  Amazon a: DBpedia db: IMdb i:
 a:Theatrical ReleaseDate db:Film type
 db:released i:movie type
 i:year (b) Mean Reciprocal Rank (MRR) (c) R-Precision (P@R) (d) Precision at 10 (P@10) Figure 7: Retrieval performance between source dataset (S) and target dataset (T).
tween the ERM and relevant entities.
Clearly, better results can be achieved for relevant entities.
This is important for our search task, which is focused on  nding these relevant entities.
Intuitively, the search performance depends on the quality of the alignment.
We veri ed this by computing the Pearson correlation coe cient   between the search performance of the di erent settings captured by MAP, as reported in Figure 7(a), and the alignment quality in terms of precision and recall for relevant entities, as reported in Figure 9(a).
This yields  (MAP, Precision-Rel) = 0.98 and  (MAP, Recall-Rel) = 0.97, indicating strong dependency between quality of the mappings and search performance.
The hybrid approach relies on three parameters: b for boosting  elds (attributes) in the seed query (Equation 7), the alignment threshold t and the threshold c for pruning  elds of the ERM (Section 3.3).
We analyze the robustness of search e ectiveness in terms of MAP for the six retrieval scenarios by varying one parameter while keeping the others  xed at the levels we used for the experiments.
The results are shown in Figure 10.
We observed that boosting helps to improve the performance when dealing with similar datasets (i.e. Amazon and IMDB) but has a negative e ect when a di erent and diverse dataset like DBpedia is involved.
However, performance is rather insensitive to this parameter when b > 10 (thus we chose b = 10).
Regarding the alignment threshold t, we observed that performance is fairly stable when t is within the range [0.2, 0.8].
Pruning  elds has almost no e ect on e ectiveness.
To analyze the performance of the hybrid approach, we measured query execution time for ERM across all six retrieval scenarios, i.e. for a total of 138 queries.
Figure 11(a) shows the min, max and average time in seconds for each retrieval scenario.
The times reported cover all steps of the retrieval process, i.e. executing qs to obtain results for the source dataset, computing ERM , retrieving results for target datasets, computing models for each candidate entity, establishing mappings and ranking.
Such a retrieval process takes less than 13 secs on average for the above con gura-tion.
The performance can be improved by increasing the pruning parameter c as shown in Figure 11(b), which shows the min, max, and average query execution time over all six scenarios for di erent values of c. For these runtime experiments, we use a standard laptop with Intel Core 2 Duo
 MacOS 10.6, and implemented our approach using Java 6 and Lucene 3.0 for indexing and retrieval.
Computing the language models from the term-frequency vectors was performed at runtime.
These tasks can also be performed at indexing time.
Still, these preliminary results suggest that the hybrid approach is promising, given that not only search results but also on the  y mappings are obtained during the process.
(a) Runtime performance ERM (b) Prunning c Figure 11: Runtime performance analysis


 ERMa ERMq


 ERMa ERMq


 ERMa ERMq


 ERMa ERMq





 S=Amazon T=Dbpedia Amazon IMdb IMdb Dbpedia IMdb Amazon DBpedia Amazon DBpedia IMdb secs









 secs c max avg min (b) S=Amazon T=DBpedia (c) S=IMdb T=Amazon (d) S=IMdb T=DBpedia (e) S=DBpedia T=IMdb (f) S=DBpedia T=Amazon Figure 8: Precision-recall curves for source dataset (S) and target dataset (T).
(a) Precision and recall of the mappings created on the  y between ERM and entities, respectively relevant entities (Rel).
(b) Average number of mappings created on the  y between ERM and entities, respectively relevant entities (Rel).
Figure 9: Evaluation of the mappings created on the  y.
We have discussed related work throughout the paper.
Basically, there are two existing lines of approaches, one that is based on keyword search [2, 5, 20] and the other one is structured query rewriting [3, 23, 10].
The latter type of approaches uses precomputed mappings,  nds duplicates [23] or uses precomputed relaxations of the query constraints [10] to bridge di erences in syntactical representation.
The keyword search approaches rely on matches on the level of terms.
Beside the pure  bag-of-word  approaches [5, 20], a recent study showed that using a minimal structure by classifying attributes into important and unimportant  elds improves keyword search for entities [2].
Our approach represents a novel combination which combines the  exibility of keyword search with the power of structured querying.
Just like keyword search, it does not rely on precomputed mappings.
However, it is able to exploit the  ne-grained structure of query and results, which is the primary advantage of structured query rewriting.
In addition, it can leverage existing mappings created by alignment tools like [13, 7].
We presented the general idea of our approach and preliminary results in [12].
Our work leverages several ideas that are have been proposed for IR tasks.
In fact, the model underlying our approach originates from the concept of language models [17], which have been proposed for modeling resources and queries as multinomial distributions over the vocabulary terms, and for ranking based on the distance between the two models, e.g.
using KL-divergence [22] or cross entropy [14] as measures.
More precisely, the foundation of our work is established by Lavrenko et al.[14], who propose relevance-based language models to directly capture the relevance behind document and queries.
Also, structure information has been exploited for constructing structured relevance models [15] (SRM).
This is the one mostly related to ERM.
The difference is that while the goal of SRM is to predict values of empty  elds in a single dataset scenario, ERM targets searching in a completely di erent setting involving multiple heterogeneous datasets.
Thus, we build on well studied concepts and investigate them in a scenario di erent from the traditional IR settings.
Instead of searching documents using keyword queries, we show how to use structured language models to process structured queries against structured data residing in external Web datasets.
In this scenario, we also need to take structure mismatches (i.e. di erences at the schema level) into account and thus, propose on the  y integration to deal with this problem.
Recall Rel Precision Precision Rel
 Align./En+ty Avg.
Align./ Rel En+ty (b) Alignment threshold t (c) ERM pruning c Figure 10: Parameter sensitivity analysis.
The legend  A2D  stands for source dataset=Amazon (A), target dataset=DBpedia (D),  I2A  stands for source=IMdb (I), target=Amazon, etc.
