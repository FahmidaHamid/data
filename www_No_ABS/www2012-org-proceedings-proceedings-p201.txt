Consumers rely increasingly on user-generated online reviews to make, or reverse, purchase decisions [3].
Accordingly, there appears to be widespread and growing concern among both businesses and the public [12, 14, 16, 19, 20, 21] regarding the potential for posting deceptive opinion spam ctitious reviews that have been deliberately written to sound authentic, to deceive the reader [15].
Perhaps surprisingly, however, relatively little is known about the actual prevalence, or rate, of deception in online review communities, and less still is known about the factors that can in uence it.
On the one hand, the relative ease of producing reviews, combined with the pressure for businesses, products, and services to be perceived in a positive light, might lead one to expect that a preponderance of online reviews are fake.
One can argue, on the other hand, that a low rate of deception is required for review sites to serve any value.1 The focus of spam research in the context of online reviews has been primarily on detection.
Jindal and Liu [8], for example, train models using features based on the review text, reviewer, and product to identify duplicate opinions.2 Yoo and Gretzel [23] gather 40 truthful and 42 deceptive hotel reviews and, using a standard statistical test, manually compare the psychologically relevant linguistic di erences between them.
While useful, these approaches do not focus on the prevalence of deception in online reviews.
Indeed, empirical, scholarly studies of the prevalence of deceptive opinion spam have remained elusive.
One reason is the di culty in obtaining reliable gold-standard annotations for reviews, i.e., trusted labels that tag each review as either truthful (real) or deceptive (fake).
One option for producing gold-standard labels, for example, would be to rely on the judgements of human annotators.
Recent studies, however, show that deceptive opinion spam is not easily identi ed by human readers [15]; this is especially the case when considering the overtrusting nature of most human judges, a phenomenon referred to in the psychological
 ceptive reviews might still serve value, for example, if there remains enough truthful content to produce reasonable aggregate comparisons between o erings.
appear more than once in the corpus with the same (or similar) text.
However, simply because a review is duplicated does not make it deceptive.
Furthermore, it seems unlikely that either duplication or plagiarism characterizes the majority of fake reviews.
Moreover, such reviews are potentially detectable via o -the-shelf plagiarism detection software.
the nontrivial nature of identifying deceptive content, given below are two positive reviews of the Hilton Chicago Hotel, one of which is truthful, and the other of which is deceptive opinion spam:
 had a very nice stay!
The rooms were large and comfortable.
The view of Lake Michigan from our room was gorgeous.
Room service was really good and quick, eating in the room looking at that view, awesome!
The pool was really nice but we didnt get a chance to use it.
Great location for all of the downtown Chicago attractions such as theaters and museums.
Very friendly sta  and knowledgable, you cant go wrong staying here. 
 it being shabby I can t for the life of me  gure out what they are talking about.
Rooms were large with TWO bathrooms, lobby was fabulous, pool was large with two hot tubs and huge gym, sta  was courteous.
For us, the location was great across the street from Grant Park with a great view of Buckingham Fountain and close to all the museums and theatres.
I m sure others would rather be north of the river closer to the Magni cent Mile but we enjoyed the quieter and more scenic location.
Got it for $105 on Hotwire.
What a bargain for such a nice hotel.  Answer: See footnote.3 The di culty of detecting which of these reviews is fake is consistent with recent large meta-analyses demonstrating the inaccuracy of human judgments of deception, with accuracy rates typically near chance [1].
In particular, humans have a di cult time identifying deceptive messages from cues alone, and as such, it is not surprising that research on estimating the prevalence of deception (see Section 8.2) has generally relied on self-report methods, even though such reports are di cult and expensive to obtain, especially in large-scale settings, e.g., the web [5].
More importantly, self-report methods, such as diaries and large-scale surveys, have several methodological concerns, including social desirability bias and self-deception [4].
Furthermore, there are considerable disincentives to revealing one s own deception in the case of online reviews, such as being permanently banned from a review portal, or harming a business s reputation.
Recently, automated approaches (see Section 4.1) have emerged to reliably label reviews as truthful vs. deceptive: Ott et al. [15] train an n-gram based text classi er using a corpus of truthful and deceptive reviews the former culled from online review communities and the latter generated using Amazon Mechanical Turk (http://www.mturk.com).
Their resulting classi er is nearly 90% accurate.
In this work, we present a general framework (see Section 2) for estimating the prevalence of deception in online review communities.
Given a classi er that distinguishes truthful from deceptive reviews (like that described above), and inspired by studies of disease prevalence [9, 10], we propose a generative model of deception (see Section 3) that jointly models the classi er s uncertainty as well as the ground-truth deceptiveness of each review.
Inference for this model, which we perform via Gibbs sampling, allows us to estimate the prevalence of deception in the underlying review community, without relying on either self-reports or gold-standard annotations.
We further propose a theoretical component to the framework based on signaling theory from economics [18] (see Section 6) and use it to reason about the factors that in uence deception prevalence in online review communities.
In our context, signaling theory interprets each review as a signal to the product s true, unknown quality; thus, the goal of consumer reviews is to diminish the inherent information asymmetry between consumers and producer.
Very brie y, according to a signaling theory approach, deception prevalence should be a function of the costs and bene ts that accrue from producing a fake review.
We hypothesize that review communities with low signaling cost, such as communities that make it easy to post a review, and large bene ts, such as highly tra cked sites, will exhibit more deceptive opinion spam than those with higher signaling costs, such as communities that establish additional requirements for posting reviews, and lower bene ts, such as low site tra c.
We apply our approach to the domain of hotel reviews.
In particular, we examine hotels from the Chicago area, restricting attention to positive reviews only, and instantiate the framework for six online review communities (see Section 5): Expedia (http://www.expedia.com), Hotels.com (http://www.hotels.com), Orbitz (http://www.orbitz.com), Priceline (http://www.priceline.com), TripAdvisor (http: //www.tripadvisor.com), and Yelp (http://www.yelp.com).
We  nd  rst that the prevalence of deception indeed varies by community.
However, because it is not possible to validate these estimates empirically (i.e., the gold-standard rate of deception in each community is unknown), we focus our discussion instead on the relative di erences in the rate of deception between communities.
Here, the results con rm our hypotheses and suggest that deception is most prevalent in communities with a low signal cost.
Importantly, when measures are taken to increase a community s signal cost, we  nd dramatic reductions in our estimates of the rate of deception in that community.
In this section, we propose a framework to estimate the prevalence, or rate, of deception among reviews in six online review communities.
Since reviews in these communities do not have gold-standard annotations of deceptiveness, and neither human judgements nor self-reports of deception are reliable in this setting (see discussion in Section 1), our framework instead estimates the rates of deception in these communities using the output of an imperfect, automated deception classi er.
In particular, we utilize a supervised machine learning classi er, which has been shown recently by Ott et al. [15] to be nearly 90% accurate at detecting deceptive opinion spam in a class-balanced dataset.
A similar framework has been used previously in studies of disease prevalence, in which gold-standard diagnostic testing is either too expensive, or impossible to perform [9,
 prevalence of disease in the population using a combination of an imperfect diagnostic test, and estimates of the test s positive and negative recall rates.4

 notation.
Shaded nodes represent observed variables, and arrows denote dependence.
For example, f (xi) is observed, and depends on  ,  , and yi.
E[ f ] = E Our proposed framework is summarized here, with each step discussed in greater detail in the corresponding section:
 i=1 Assume given a set of labeled training reviews, Dtrain = {(xi, yi)}N train , where, for each review i, yi   {0, 1} gives the review s label (0 for truthful, 1 for deceptive), and xi   R|V | gives the review s feature vector representation, for some feature space of size |V |.
Similarly, assume given a set of labeled truthful development reviews, Ddev = {(xi, 0)}N dev i=1 , and a set of unlabeled test reviews, Dtest = {xi}N test i=1 .
Using the labeled training reviews, Dtrain, learn a supervised deception classi er, f : R|V |   {0, 1}.
By cross-validation on Dtrain, estimate the sensitivity (deceptive recall) of the deception classi er, f , as:   = Pr(f (xi) = 1| yi = 1).
(1) Then, use Ddev to estimate the speci city (truthful recall) of the deception classi er, f , as:   = Pr(f (xi) = 0| yi = 0).
(2) Finally, use f ,  ,  , and either the Na ve Prevalence Model (Section 3.1), or the generative Bayesian Prevalence Model (Section 3.2), to estimate the prevalence of deception, denoted  , among reviews in Dtest.
Note that if we had gold-standard labels, {yi}N test i=1 , the gold-standard prevalence of deception would be: N test(cid:88) i=1     =
 N test yi.
(3)

 In Section 2, we propose a framework to estimate the prevalence of deception in a group of reviews using only the output of a noisy deception classi er.
Central to this framework is the Prevalence Model, which models the uncertainty of the deception classi er, and ultimately produces the desired prevalence estimate.
In this section, we propose two competing Prevalence Models, which can be used interchangeably in our framework.
The Na ve Prevalence Model (na ve) estimates the prevalence of deception in a corpus of reviews by correcting the output of a noisy deception classi er according to the clas-si er s known performance characteristics.
Formally, for a given deception classi er, f , let  f be the number of reviews in Dtest for which f makes a positive prediction, i.e., the number of reviews for which f predicts deceptive.
Also, let the sensitivity (deceptive recall) and be known precisely.
However, imprecise estimates can often be obtained, especially in cases where it is feasible to perform gold-standard testing on a small subpopulation.
speci city (truthful recall) of f be given by   and  , respectively.
Then, we can write the expectation of  f as:     1 (cid:88) N test x Dtest (cid:88) =
 N test  [f (x) = 1] E [  [f (x) = 1]] x Dtest   + (1    )(1       ), =   (4) where   is the true (latent) rate of deception, and  [a = b] is the Kronecker delta function, which is equal to 1 when a = b, and 0 otherwise.
If we rearrange Equation 4 in terms of  , and replace the expectation of  f with the observed value, we get the Na ve Prevalence Model estimator:  na ve = .
(5)  f   (1    )     (1    ) Intuitively, Equation 5 corrects the raw classi er output, given by  f , by subtracting from it the false positive rate, given by 1    , and dividing the result by the di erence between the true and false positive rates, given by  (1 ).
Notice that when f is an oracle,5 i.e., when   =   = 1, the Na ve Prevalence Model estimate correctly reduces to the oracle rate given by f , i.e.,  na ve =  f =  .
Unfortunately, the Na ve Prevalence Model estimate,  na ve, is not restricted to the range [0, 1].
Speci cally, it is negative when  f < 1   , and greater than 1 when  f >  . Furthermore, the Na ve Prevalence Model makes the unrealistic assumption that the estimates of the classi er s sensitivity ( ) and speci city ( ), obtained using the procedure discussed in Section 4.2 and Appendix B, are exact.
always predicts the true, gold-standard label.
Ntest  *yif(xi) * * WWW 2012   Session: Fraud and Bias in User RatingsApril 16 20, 2012, Lyon, France203The Bayesian Prevalence Model (bayes) addresses these limitations by modeling the generative process through which deception occurs, or, equivalently, the joint probability distribution of the observed and latent data.
In particular, bayes models the observed classi er output, the true (latent) rate of deception ( ), as well as the classi er s true (latent) sensitivity ( ) and speci city ( ).
Formally, bayes assumes that our data was generated according to the following generative story:   Sample the true rate of deception:     Beta( )   Sample the classi er s true sensitivity:     Beta( )   Sample the classi er s true speci city:     Beta( )   For each review i:   Sample the ground-truth deception label: yi   Bernoulli(    ) (cid:26) Bernoulli( ) Bernoulli(1    )   Sample the classi er s output: f (xi)   if yi = 1 if yi = 0 The corresponding graphical model is given in plate notation in Figure 1.
Notice that by placing Beta prior distributions on  ,  , and  , bayes enables us to encode our prior knowledge about the true rate of deception, as well as our uncertainty about the estimates of the classi er s sensitivity and speci city.
This is discussed further in Section 4.2.
A similar model has been proposed by Joseph et al. [10] for studies of disease prevalence, in which it is necessary to estimate the prevalence of disease in a population given only an imperfect diagnostic test.
However, that model samples the total number of true positives and false negatives, while our model samples the yi individually.
Accordingly, while pilot experiments con rm that the two models produce identical results, the generative story of our model, given above, is comparatively much more intuitive.
While exact inference is intractable for the Bayesian Prevalence Model, a popular alternative way of approximating the desired posterior distribution is with Markov Chain Monte Carlo (MCMC) sampling, and more speci cally Gibbs sampling.
Gibbs sampling works by sampling each variable, in turn, from the conditional distribution of that variable given all other variables in the model.
After repeating this procedure for a  xed number of iterations, the desired posterior distribution can be approximated from samples in the chain by: (1) discarding a number of initial burn-in iterations, and (2) since adjacent samples in the chain are often highly correlated, thinning the number of remaining samples according to a sampling lag.
The conditional distributions of each variable given the others can be derived from the joint distribution, which can be read directly from the graph.
Based on the graphical representation of bayes, given in Figure 1, the joint distribution of the observed and latent variables is just: Pr(f (x), y,     ,   Pr(y |         ,   )   Pr(  ;  ,  ,  ) = Pr(f (x)| y,   )      ,     |  )   Pr(    |  ),   |  )   Pr(  (6) Table 1: Reference 5-fold cross-validated performance of an SVM deception detection classi er in a balanced dataset of TripAdvisor reviews, given by Ott et al. [15].
F-score corresponds to the harmonic mean of precision and recall.
metric performance Accuracy Deceptive Precision Deceptive Recall Deceptive F-score Truthful Precision Truthful Recall Truthful F-score Baseline Accuracy







 where each term is given according to the sampling distributions speci ed in the generative story in Section 3.2.
A common technique to simplify the joint distribution, and the sampling process, is to integrate out (collapse) variables that do not need to be sampled.
If we integrate out  ,  , and   from Equation 6, we can derive a Gibbs sampler that only needs to sample the yi s at each iteration.
The resulting sampling equations, and the corresponding Bayesian Prevalence Model estimate of the prevalence of deception,  bayes, are given in greater detail in Appendix A.
The next component of the framework given in Section 2 is the deception classi er, which predicts whether each unlabeled review is truthful (real) or deceptive (fake).
Following previous work [15], we assume given some amount of labeled training reviews, so that we can train deception classi ers using a supervised learning algorithm.
Previous work has shown that Support Vector Machines (SVM) trained on n-gram features perform well in deception detection tasks [8, 13, 15].
Following Ott et al. [15], we train linear SVM classi ers using the LIBSVM [2] software package, and represent reviews using unigram and bi-gram bag-of-words features.
While more sophisticated and purpose-built classi ers might achieve better performance, pilot experiments suggest that the Prevalence Models (see Section 3) are not heavily a ected by minor di erences in classi er performance.
Furthermore, the simple approach just outlined has been previously evaluated to be nearly 90% accurate at detecting deception in a balanced dataset [15].
Reference cross-validated classi er performance appears in Table 1.
Both Prevalence Models introduced in Section 3 can utilize knowledge of the underlying deception classi er s sensitivity ( ), i.e., deceptive recall rate, and speci city ( ), i.e., truthful recall rate.
While it is not possible to obtain gold-standard values for these parameters, we can obtain rough estimates of their values (denoted   and  , respectively) through a combination of cross-validation, and evaluation on a labeled development set.
For the Na ve Prevalence from six online review communities.
community # hotels # reviews Expedia Hotels.com Orbitz Priceline TripAdvisor Yelp Mechanical Turk













 Model, the estimates are used directly, and are assumed to be exact.
For the Bayesian Prevalence Model, we adopt an empirical Bayesian approach and use the estimates to inform the corresponding Beta priors via their hyperparameters,   and  , respectively.
The full procedure is given in Appendix B.
In this section, we brie y discuss each of the three kinds of data used by our framework introduced in Section 2.
Corpus statistics are given in Table 2.
Following Ott et al. [15], we excluded all reviews with fewer than 150 characters, as well as all non-English reviews.6
 Training a supervised deception classi er requires labeled training data.
Following Ott et al. [15], we build a balanced set of 800 training reviews, containing 400 truthful reviews from six online review communities, and 400 gold-standard deceptive reviews from Amazon Mechanical Turk.
Deceptive Reviews: In Section 1, we discuss some of the di culties associated with obtaining gold-standard labels of deception, including the inaccuracy of human judgements, and the problems with self-reports of deception.
To avoid these di culties, Ott et al. [15] have recently created 400 gold-standard deceptive reviews using Amazon s Mechanical Turk service.
In particular, they paid one US dollar ($1) to each of 400 unique Mechanical Turk workers to write a fake positive (5-star) review for one of the 20 most heavily-reviewed Chicago hotels on TripAdvisor.
Each worker was given a link to the hotel s website, and instructed to write a convincing review from the perspective of a satis- ed customer.
Any submission found to be plagiarized was rejected.
Any submission with fewer than 150 characters was discarded.
To date, this is the only publicly-available7 gold-standard deceptive opinion spam dataset.
As such, we choose it to be our sole source of labeled deceptive reviews for training our supervised deception classi ers.
Note that these same reviews are used to estimate the resulting clas-si er sensitivity (deceptive recall), via the cross-validation procedure given in Appendix B.
Truthful Reviews: Many of the same challenges that make it di cult to obtain gold-standard deceptive reviews, also apply to obtaining truthful reviews.
Related work [8, 11] has hypothesized that the relative impact of spam reviews
 brary: http://code.google.com/p/language-detection/.
7http://www.cs.cornell.edu/~myleott/op_spam Table 3: Signal costs associated with six online review communities, sorted approximately from highest signal cost to lowest.
Posting cost is High if users are required to purchase a product before reviewing it, and Low otherwise.
Exposure bene t is Low, Medium, or High based on the number of reviews in the community (see Table 2).
community Orbitz Priceline Expedia Hotels.com Yelp TripAdvisor posting cost exposure benefit High High High High Low Low Low Medium Medium Medium Low High is smaller for heavily-reviewed products, and that therefore spam should be less common among them.
For consistency with our labeled deceptive review data, we simply label as truthful all positive (5-star) reviews of the 20 previously chosen Chicago hotels.
We then draw a random sample of size 400, and take that to be our labeled truthful training data.
By training on deceptive and truthful reviews from the same 20 hotels, we are e ectively controlling our classi er for topic.
However, because this training data is not representative of Chicago hotel reviews in general, it is important that we do not use it to estimate the resulting classi er s speci city (truthful recall).
Accordingly, as speci ed in our framework (Section 2), classi er speci city is instead estimated on a separate, labeled truthful development set, which we draw uniformly at random from the unlabeled reviews in each review community.
For consistency with the sensitivity estimate, the size of the draw is always 400 reviews.
The last data component of our framework is the set of test reviews, among which to estimate the prevalence of deception.
To avoid evaluating reviews that are too di erent from our training data in either sentiment (due to negative reviews), or topic (due to reviews of hotels outside Chicago), we constrain each community s test set to contain only positive (5-star) Chicago hotel reviews.
This unfortunately dis-quali es our estimates of each community s prevalence of deception from being representative of all hotel reviews.
Notably, estimates of the prevalence of deception among negative reviews might be very di erent from our estimates, due to the distinct motives of posting deceptive positive vs. negative reviews.
We discuss this further in Section 9.
In terms of economic theory, the role of review communities is to reduce the inherent information asymmetry [18] between buyers and sellers in online marketplaces, by providing buyers with a priori knowledge of the underlying quality of the products being sold [7].
It follows that if reviews regularly failed to reduce this information asymmetry, or, worse, convey false information, then they would cease to be of (b) Priceline (c) Expedia (d) Hotels.com (e) Yelp (f) TripAdvisor Figure 2: Graph of Na ve estimates of deception prevalence versus time, for six online review communities.
Blue (a d) and red (e f ) graphs correspond to high and low posting cost communities, respectively.
value to the user.
Given that review communities are, in fact, valued by users [3], it seems unlikely that the prevalence of deception among them is large.
Nonetheless, there is widespread concern about the prevalence of deception in online reviews, rightly or wrongly, and further, deceptive reviews can be cause for concern even in small quantities, e.g., if they are concentrated in a single review community.
We propose that by framing reviews as signals voluntary communications that serve to convey information about the signaler [18], we can reason about the factors underlying deception by manipulating the distinct signal costs associated with truthful vs. deceptive reviews.
Speci cally, we claim that for a positive review to be posted in a given review community, there must be an incurred signal cost, that is increased by:
 review community, i.e., whether users are required to purchase a product prior to reviewing it (high cost) or not (low cost).
Some sites, for example, allow anyone to post reviews about any hotel, making the review cost e ectively zero.
Other sites, however, require the purchase of the hotel room before a review can be written, raising the cost from zero to the price of the room.
and decreased by:
 review community, i.e., the bene t derived from other users reading the review, which is proportional to the size of the review community s audience.
Review sites with more tra c have greater exposure bene t.
Observe that both the posting cost and the exposure bene t depend entirely on the review community.
An overview of these factors for each of the six review communities is given in Table 3.
Based on the signal cost function just de ned, we propose two hypotheses:   Hypothesis 1 : Review communities that have low signal costs (low posting requirements, high exposure), e.g., TripAdvisor and Yelp, will have more deception than communities with high signal costs, e.g., Orbitz.
  Hypothesis 2 : Increasing the signal cost will decrease the prevalence of deception.
The framework described in Section 2 is instantiated for the six review communities introduced in Section 5.
In particular, we  rst train our SVM deception classi er following the procedure outlined in Section 4.1.
An important step when training SVM classi ers is setting the cost parameter, C. We set C using a nested 5-fold cross-validation procedure, and choose the value that gives the best average balanced accuracy, de ned as 1 2 (sensitivity + speci city).
We then estimate the classi er s sensitivity, speci city, and hyperparameters, using the procedure outlined in Section 4.2 and Appendix B.
Based on those estimates, we then estimate the prevalence of deception among reviews in our test set using the Na ve and the Bayesian Prevalence Models.
Gibbs sampling for the Bayesian Prevalence Model is performed using Equations 7 and 8 (given in Appendix A) for 70,000 iterations, with a burn-in of 20,000 iterations, and a sampling lag of 50.
We use an uninformative (uniform) prior for  , i.e.,   = (cid:104)1, 1(cid:105).
Multiple runs are performed to verify the stability of the results.
- 6% - 4% - 2%



 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 - 6% - 4% - 2%



 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 - 6% - 4% - 2%



 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 - 6% - 4% - 2%



 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 - 6% - 4% - 2%



 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 - 6% - 4% - 2%



 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 (b) Priceline (c) Expedia (d) Hotels.com (e) Yelp (f) TripAdvisor Figure 3: Graph of Bayesian estimates of deception prevalence versus time, for six online review communities.
Blue (a d) and red (e f ) graphs correspond to high and low posting cost communities, respectively.
Error bars show Bayesian 95% credible intervals.
Estimates of the prevalence of deception for six review communities over time, given by the Na ve Prevalence Model, appear in Figure 2.
Blue graphs (a d) correspond to communities with High posting cost (see Table 3), i.e., communities for which you are required to book a hotel room before posting a review, while red graphs (e f) correspond to communities with Low posting cost, i.e., communities that allow any user to post reviews for any hotel.
In agreement with Hypothesis 1 (given in Section 6), it is clear from Figure 2 that deceptive opinion spam is decreasing or stationary over time for High posting cost review communities (blue graphs, a d).
In contrast, review communities that allow any user to post reviews for any hotel, i.e., Low posting cost communities (red graphs, e f), are seeing growth in their rate of deceptive opinion spam.
Unfortunately, as discussed in Section 3.1, we observe that the prevalence estimates produced by the Na ve Prevalence Model are often negative.
This occurs when the rate at which the classi er makes positive predictions is below the classi er s estimated false positive rate, suggesting both that the estimated false positive rate of the classi er is perhaps overestimated, and that the classi er s estimated speci city (truthful recall rate, given by  ) is perhaps underestimated.
We address this further in Section 8.1.
The Bayesian Prevalence Model, on the other hand, encodes the uncertainty in the estimated values of the classi- er s sensitivity and speci city through two Beta priors, and in particular their hyperparameters,   and  .
Estimates of the prevalence of deception for the six review communities over time, given by the Bayesian Prevalence Model, appear in Figure 3.
Blue (a d) and red (e f) graphs, as before, correspond to communities with High and Low posting costs, respectively.
In agreement with Hypothesis 1 (Section 6), we again  nd that Low signal cost communities, e.g., TripAdvisor, seem to contain larger quantities and accelerated growth of deceptive opinion spam when compared to High signal cost communities, e.g., Orbitz.
Interestingly, communities with a blend of signal costs appear to have medium rates of deception that are neither growing nor declining, e.g., Hotels.com, which has a rate of deception of   2%.
To test Hypothesis 2, i.e., that increasing the signal cost will decrease the prevalence of deception, we need to increase the signal cost, as we have de ned it in Section 6.
Thus, it is necessary to either increase the posting cost, or decrease the exposure bene t.
And while we have no control over a community s exposure bene t, we can increase the posting cost by, for example, hiding all reviews written by users who have not posted at least two reviews.
Essentially, by requiring users to post more than one review in order for their review to be displayed, we are increasing the posting cost and, accordingly, the signal cost as well.
Bayesian Prevalence Model estimates for TripAdvisor for varying signal costs appear in Figure 4.
In particular, we give the estimated prevalence of deception over time after removing reviews written by  rst-time review writers, and after removing reviews written by  rst- or second-time review writers.
In agreement with Hypothesis 2, we see a clear reduction in the prevalence of deception over time on TripAdvisor after removing these reviews, with rates dropping from   6%, to   5%, and  nally to   4%, suggesting that an increased signal cost may indeed help to reduce the prevalence of deception in online review communities.
Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 (b) TripAdvisor.
First-time reviewers excluded.
(c) TripAdvisor.
First-time and second-time reviewers excluded.
Figure 4: Graph of Bayesian estimates of deception prevalence versus time, for TripAdvisor, with reviews written by new users excluded.
Excluding reviews written by  rst- or second-time reviewers increases the signal cost, and decreases the prevalence of deception.
In this work we have made a number of assumptions, a few of which we will now highlight and discuss.
First, we note that our unlabeled test set, Dtest, overlaps with our labeled truthful training set, Dtrain.
Consequently, we will underestimate the prevalence of deception, because the overlapping reviews will be more likely to be classi ed at test time as truthful, having been seen in training as being truthful.
Excluding these overlapping reviews from the test set results in overestimating the prevalence of deception, based on the hypothesis that the overlapping reviews, chosen from the 20 most highly-reviewed Chicago hotels, are more likely to be truthful to begin with.
Second, we observe that our development set, Ddev, containing labeled truthful reviews, is not gold-standard.
Unfortunately, while it is necessary to obtain a uniform sample of reviews in order to fairly estimate the classi er s truthful recall rate (speci city), such review samples are inherently unlabeled.
This can be problematic if the underlying rate of deception is high among the reviews from which the development set is sampled, because the speci city will then be underestimated.
Indeed, our Na ve Prevalence Model regularly produces negative estimates, suggesting that the estimated classi er speci city may indeed be underestimated, possibly due to deceptive reviews in the development set.
Third, our proposal for increasing the signal cost, by hiding reviews written by  rst- or second-time reviewers, is not ideal.
While our results con rm that hiding these reviews will cause an immediate reduction in deception prevalence, the increase in signal cost might be insu cient to discourage new deception, once deceivers become aware of the increased posting requirements.
Fourth, in this work we have only considered a limited version of the deception prevalence problem.
In particular, we have only considered positive Chicago hotel reviews, and our classi er is trained on deceptive reviews coming only from Amazon Mechanical Turk.
Both negative reviews as well as deceptive reviews obtained by other means are likely to be di erent in character than the data used in this study.
The current research also represents a novel approach to a longstanding and ongoing debate around deception prevalence in the psychological literature.
In one of the  rst large-scale studies looking at how often people lie in everyday communication, DePaulo et al. [4] used a diary method to calculate the average number of lies told per day.
At the end of seven days participants told approximately one to two lies per day, with more recent studies replicating this general  nding [6], suggesting that deception is frequent in human communication.
More recently, Serota et al. [17] conducted a large scale representative survey of Americans asking participants how often they lied in the last 24 hours.
While they found the same average deception rate as previous research (approximately 1.65 lies per day), they discovered that the data was heavily skewed, with 60 percent of the participants reporting no lies at all.
They concluded that rather than deception prevalence being spread evenly across the population, there are instead a few proli c liars.
Unfortunately, both sides of this debate have relied solely on self-report data.
The current approach o ers a novel method for assessing deception prevalence that does not require self-report, but can provide insight into the prevalence of deception in human communication more generally.
At the same time, the question raised by the psychological research also mirrors an important point regarding the prevalence of deception in online reviews: are a few deceptive reviews posted by many people, or are there many deceptive reviews told by only a few?
That is, do some hotels have many fake reviews while others are primarily honest?
Or, is there a little bit of cheating by most hotels?
This kind of individualized modeling represents an important next step in this line of research.
In this work, we have presented a general framework for estimating the prevalence of deception in online review communities, based on the output of a noisy deception classi er.
Using this framework, we have explored the prevalence of deception among positive reviews in six popular online review communities, and provided the  rst empirical study of the magnitude, and in uencing factors of deceptive opinion spam.
We have additionally proposed a theoretical model of online reviews as a signal to a product s true (unknown) quality, based on economic signaling theory.
Speci cally, we have de ned the signal cost of positive online reviews as a func-






Jan- 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11






 Jan 09 Jul 09 Jan 10 Jul 10 Jan 11 Jul 11 community in which it is posted.
Based on this theory, we have further suggested two hypotheses, both of which are supported by our  ndings.
In particular, we  nd  rst that review communities with low signal costs (low posting requirements, high exposure) have more deception than communities with comparatively higher signal costs.
Second, we  nd that by increasing the signal cost of a review community, e.g., by excluding reviews written by  rst- or second-time reviewers, we can e ectively reduce both the prevalence and the growth rate of deception in that community.
Future work might explore other methods for manipulating the signal costs associated with posting online reviews, and the corresponding e ects on deception prevalence.
For example, some sites, such as Angie s List (http: //www.angieslist.com/), charge a monthly access fee in order to browse or post reviews, and future work might study the e ectiveness of such techniques at deterring deception.
This work was supported in part by National Science Foundation Grant NSCC-0904913, and the Jack Kent Cooke Foundation.
We also thank, alphabetically, Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bin Lu, Karthik Raman, Lu Wang, and Ainur Yessenalina, as well as members of the Cornell NLP seminar group and the WWW reviewers for their insightful comments, suggestions and advice on various aspects of this work.
