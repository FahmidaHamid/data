Information networks arise naturally in a wide range of domains.
Examples include biological networks, publication networks and social networks.
In these networks, fea-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
ture vectors are usually available which are associated with nodes.
Links represent relationships between the nodes.
Identifying communities in information networks is a crucial step to understand the network structures.
The community is de ned as a group of nodes which are densely connected inside the group, while loosely connected with the nodes outside the group.
Community detection in network data has been extensively studied in the literature [17, 19, 3, 18, 2, 21, 14].
Conventional approaches focus on detecting communities based upon linkage information.
They assume that the complete linkage information within the entire network is available.
However, in many real-world networks, such as terrorist-attack information networks, the complete linkages are very di cult or even impossible to obtain.
Instead, the complete linkage information is only available within a few small local regions.
We notice that a similar problem has also been studied in [13].
However, in this paper, we focus on incomplete information networks with local information regions.
For example, in work relation networks, it is usually impossible to obtain the complete linkage information among all the people.
But usually we can a ord to obtain the work relationships within a small number of local regions, such as groups or organizations.
These networks are called incomplete information networks in this paper.
The local regions with complete linkage information are called local information regions.
An incomplete information network with local information regions is shown in the upper left level of Figure 1.
Some real-world examples for community detection in incomplete information networks are listed as follows:   Terrorist-attack network.
Let us consider a terrorist attack activity networks within a period in a certain country.
Each node in the network represents a terrorist activity.
Terrorist attacks committed by the same terrorist organization are linked with each other.
Investigating the community structures within these networks is a challenging problem, since most of the connections/links between attacks are not clearly resolved.
Detecting the communities in these incomplete information networks is crucial for analyzing the structures of terrorist-attack activities.
  Food Web The food web of a large ecosystem is usually a highly complex network.
Each node in the network represents a living organism, while the links rep-Node attribute view for n1~ n9 t u p n
 n1 n3 n2 n5 n4 n6 n9 n8 n7 local information regions l g n i r e t s u c d e s a b _ e t u b i r t t
 d o h t e m r u
 n1 n3 n3 n2 n5 n4 n6 n9 n8 n7 n1 n3 n2 n5 n4 n6 n9 n8 n7 n5 n3 n1 n2 n4 n8 n7 n9 n6 Euclidean distance n1 n3 n2 n5 n9 n8 n7 n4 n6 Mahalanobis distance n5 n1 n3 n2 n4 n9 n8 n7 n6 Figure 1: Comparison of di erent clustering methods on incomplete information networks with missing edges.
resent the relations between them.
Usually, it is very di cult to resolve all of the links within a food web.
However, it is relatively easier to  gure out some local regions within the food web.
Discovering communities in these incomplete food webs can help us identify micro ecosystems and the corresponding living organisms of each micro ecosystem.
Finding communities in incomplete information networks is a challenging task.
Conventional graph-based clustering methods can not be directly applied to it.
The reason is that traditional graph clustering methods, such as normalized cut based methods [24] and modularity based methods [19], mainly focus on the topological structure of the network.
Since most of the links are absent in incomplete information networks, it is impossible to cluster the network with this kind of methods.
As shown in the middle level of Figure 1, if we cluster the nodes using the traditional attribute based methods such as k-means, the most likely result is that we place nodes with the most similar attributes in the same cluster.
However, the nodes which are densely connected in structure may not necessarily mean they have the most similar attributes, i.e., they may be only similar on a subset of the attributes.
For example, in the food web networks, a community usually stands for a micro ecosystem and can contain various kinds of living organisms, which can have very di erent attributes.
Recently, some new algorithms [31] which perform clustering based on both structures and the attributes of the network are proposed.
However, they can not be applied on incomplete information networks due to the absence of the complete linkage structure.
Given the assumption that the structure of the network has a close relation with attributes of each object in the information network, in this paper, we propose a novel approach for community detection in incomplete information networks.
To the best of our knowledge, this is the  rst attempt to formulate and address the incomplete information network problem.
The main idea of our approach is that, since the structure of the network has a strong relation with the attributes of the objects in the network, we can learn a global distance metric from the local information regions with complete linkage information.
Then, we use the global metric to measure the distance between any pair of nodes in the network.
Because the metric is learned from the structure of the network, the distance will re ect the hidden linkage structure in the network.
Finally, we propose a distance-based clustering algorithm to cluster the nodes in the incomplete information network.
The di erent clustering results are shown in Figure 1.
To summarize, this work contributes on the following aspects:   We identify and de ne the problem of community detection in incomplete information networks with local information regions, i.e., an incomplete information network that still has a few tiny local regions where the complete linkage information is available.
  In order to  nd a measurement, which can re ect the structural relation between the nodes in incomplete information networks, we cast the side information of the network into an optimization problem.
Then a metric, which can be used to measure the distance between any pair of nodes, is learned.
  Based on the learned metric, we devise a distance-based modularity function to evaluate the quality of the communities.
  Finally, we propose a distance-based algorithm DSHRINK which can discover the hierarchical and overlapped communities.
Moreover, in order to speedup the clustering process, an e ective strategy is also taken.
This paper is organized as follows.
We introduce the related work in Section 2.
The formal de nition of our problem is presented in Section 3.
In Section 4, we introduce how to make use of the side information to learn a global metric.
In Section 5, we explain the distance-based clustering algorithm.
The experimental results are presented in Section 6.
Finally, we conclude in Section 7.
Community detection in networks and graphs has been widely studied in recently years[16, 4].
Many approaches mainly focused on the topological structures based on various criteria including modularity [19], normalized cut [24], structural density [30] and partition density [3].
Given a graph, which is clustered into k communities, the modular-ity function Q is de ned as:
  # di

 (1) k !i=1" li
 where L is the number of edges in the graph, li is the number of edges between nodes within community i, and di is the sum of the degrees of the nodes in community i.
The optimal value which ranges from 0 to 1.
In general, maximizing Q is a NP-hard problem.
Hence, many heuristic approaches, which try to approximate the optimal modularity value, were proposed [10].
Such approaches include greedy agglomeration [19, 28], mathematical programming [1], spectral methods [25], simulated annealing [11], sampling techniques [22], etc.
However, modularity is not a scale-invariant measure, and therefor, by relying on its maximization, can not detect communities smaller than a certain size [8].
Besides, Palla et.
al.
[20] proposed a clique percolation method, which can detect overlapped communities, but is not suitable for detecting hierarchical structures.
Huang et.
al. [12] proposed a parameter-free algorithm SHRINK, which can not only discover overlapped and hierarchical communities but also the hub nodes and outliers among them.
Rosvall et.
al. [21] tried to compress the information of the graph by optimizing the minimum description length of the random walk and proposed a highly accurate algorithm namely Infomap.
Ahn et al. [3] insisted that link communities are fundamental building blocks, and the overlapped and hierarchical communities in networks are two aspects of the same phenomenon.
They proposed a link-based approach which reveals the real world communities e ectively.
Other link-based methods were also devised by [6].
There are also some graph clustering methods which based on attributes.
Tian et al. [26] proposed an OLAP-style aggregation approach to summarize large graphs by grouping nodes based on user-selected attributes and relationships.
This method achieves homogeneous attribute values within clusters but ignores the intra-cluster topological structures.
Tsai et al.
[27] proposed a feature weight self-adjustment mechanism for k-means clustering.
In that study,  nding the appropriate weight is modeled as an optimization problem which tries to minimize the separations within clusters and maximize the separations between clusters.
Since most of the attributes based methods mainly focus on the homogeneity of the clusters, the cohesive internal structure of the clusters can not be guaranteed.
Recently, some clustering methods based on both links [9] introduced the con-and attributes were also proposed.
nected k-center(CkC) problem, which checks whether an attributed graph can be partitioned or not by considering both attributes and the links.
Since the CkC problem is NP-complete, the authors proposed a constant factor approximation algorithm and a heuristic algorithm for the large data sets.
[31] proposed SA-Cluster, which is based on both structural and attribute similarities through a uni ed distance measure.
In that study, a graph is partitioned into k clusters so that each cluster contains a densely connected subgraph with homogeneous attribute values.
Then, in order to learn the degree of contributions of structural similarity and attribute similarity automatically, an e ective method was proposed.
In this section, we formally de ne our problem and introduce several related concepts.
De nition 1 (Information Network) An information network is denoted as G = (V, E, A), where V is the set of vertices, E   V  V is the set of edges, and A =&a1, a2, ..., a|V |  is the set of node attributes which describe the properties of vertices in V .
For convenience, we use A(v) to denote the attribute vector of node v and E(U) to represent the edges among nodes in
 De nition 2 (IIN) Incomplete Information Networks with Local Information Regions (IIN) are de ned as follows: given an information network G = (V, E, A) and a network G!
= (V !, E!, A!
), network G!
is called an incomplete information network with local information regions of G i  (1) V !
= V, E!
  E (2)  v!
  V !,  v   V, if v = v!, then A!(v!)
= A(v).
(3)  V !!
  V !, for  e   E(V !!
), then e   E!
(V !!
).
Speci cally, we call the local subnetwork g(V !
!, E!
!, A!!)
as the local information region denoted by L, where E!!
= E(V !!)
and A!!
= A(V !!
).
From De nition 2, we know that an incomplete information network with local information regions is a network G!
= (V !, E!, A!)
with a small set of connected regions V !!
  V !, where the edges in E!
  E(V !!)
are missing.
There can be many di erent types of incomplete information networks.
In this paper, we focus on the incomplete information network that has some local information regions, where the linkage structures are completely preserved as in De nition 2.
For the remaining of the paper, we will just refer to this type of  incomplete information network with local information regions  as incomplete information network.
Obviously, there can be more than one local information regions in an incomplete information network G!.
Furthermore, when we are talking about an incomplete information network, we assume that there is a corresponding information network, potentially.
A typical incomplete information network is shown in the upper left level in Figure 1.
De nition 3 (Dissimilar Node Pair) Given an information network G = (V, E, A) and its k clusters C1, C2, ..., Ck, where (k i=1 V (Ci) = V (G), any pair of nodes (vi, vj) is a dissimilar node pair i  (1) vi   Cm   vj   Cn   m )= n(1   m, n   k); (2) E({vi, vj}) =  .
We denote the dissimilar node-pair set as D.
For an information network G, if C = C1, C2, ..., Ck is the set of clusters which are based on the linkage structures of G, we formalize our community detection problem as: given an incomplete information network G!
of G and the dissimilar node-pair set D, based on some similar criteria, the objective is to  nd the set C !
which should be as similar as possible to C.
In this section, we address the problem of how to learn a global metric which is used to measure the distance between any pair of nodes in an incomplete information network G!.
This goal is achieved by solving an optimization problem, which makes use of the side information getting from the link relations of G!
and the dissimilar node-pair set D.
De nition 4 (Structure Similarity) Given a network G = (V, E), for any pair of nodes vi, vj   V , the structure similarity between node vi and vj is de ned as where  (v) is the set containing v and its neighbors.
| (vi)    (vj)| )| (vi) (vj)| s(vi, vj) = (2) nodes ui, vi in local information region Li by Equation (2).
Then, we de ne the similar node-pair set S as a 3-tuple set about the structure similarity as follows: Equations (5) similar to [29]: g(M) = !
(ui,vi) D -ui, vi-M (si-ui, vi-M)2   w (7) max
 h(M) = !
(ui,vi) S
 S = {(ui, vi, si)|si = s(ui, vi), ui, vi   V (Li)} (3) s.t.
Based on the similar node-pair set S and the dissimilar node-pair set D, our objective is to  nd a metric by which, the similar nodes should be close together and the dissimilar nodes should be far away from each other.
Moreover, the extent of closeness between any pair of similar nodes should be based on the structure similarity between them.
Inspired by [29], this objective can be achieved by learning a distance metric.
Let the matrix M  Rm m represent the distance metric.
Then, the distance between any two nodes ui, vi   V is de ned by dM(ui, vi) = -ui   vi-M =)(ui   vi)T M(ui   vi) (4) In order to make sure the distance metric de ned by Equation (4) satis es non-negativity and the triangle inequality, we constraint M to be positive semi-de nite.
Now, we can formalize our objective as an optimization problem as follows: min
 s.t.
!
(ui,vi) S !ui,vi D (si-ui   vi-M)2 -ui   vi-M   w (5)
 where w is a constant.
We notice that our objective function (5) is a linear function of M. Further more, both of the constraints given in Equations (5) are convex.
Hence, our optimization problem is convex, which enables us to compute the global optimal resolution.
Despite our optimization problem falls into the category of convex programming, it does not fall into any special class of convex programming, e.g., quadratic programming and semi-de nite programming.
Hence, the global solution can only be solved by a generic approach.
We also notice that the learned optimal M can appear in two forms, which are diagonal matrix and full matrix.
In order to get the diagonal form of M, we give the equivalent Equations (5) similar to [29]: f(M) = f(M11, ..., Mnn) = !ui,vi S s2 i -ui   vi-2 M   log    !
(ui,vi) D (6) -ui,  vi-M    Minimizing Equation (6) can be resolved by using the Newton-Raphson method.
Furthermore, in order to keep the semi-de nite characteristics of M, we replace the Newton update H  1 !f by  H  1 !f , where   is the a step-size parameter optimized via a line-search which gives the largest downhill steps subject to Mii   0 [29].
In order to get the full matrix of M, we give the equivalent The reason for giving the transformation of the original optimization problem is for e ciently  nding the global optimal full matrix M by using gradient descent and the idea of iterative projections [5].
We  rst use a gradient ascent on g(M) to optimize (7).
Then, we project the intermediate results to hold the constraints (7).
The similar tricks are also used in [29].
Besides, we notice that in Equations (5) and (7), w is a constant whose value is not important.
This is because the distance between any pair of nodes in network G!
is a relative variable.
Changing the value of w only makes the distance between any pair of nodes ui and vi change from -ui   vi-M to w2-ui   vi-M.
Hence, we choose w = 1 in this paper.
For the convenience of discussion, we denote the incomplete information network as G = (V, E, A, M) in the rest of the paper.
By optimizing our objective function, we have learned a matrix M in section 4.
In other words, we have gotten a metric which can be used to measure the distance between any two nodes in graph G.
Inspired by the density-based clustering approaches, e.g., [30, 12], which cluster nodes from the higher density to lower density, in this section, we propose a distance-based clustering approach DSHRINK which can detect the overlapped and hierarchical communities hidden in the graph.
The distance-based clustering approach DSHRINK places the nodes which have the shorter distance with each other into the same cluster, and the nodes which have the longer distance between them into di erent clusters.
In order to evaluate the quality of clusters, we de ne the distance-based modularity as follows: De nition 5 (Distance-based Modularity) Given an incomplete information network G = (V, E, A, M) and its cluster C = {C1, C2, ..., Ck}, the distance-based modularity Qd is de ned as (8) Qd = i k !i=1" DI
 i
 i = .u,v Ci where k is the number of clusters, DI dM(u, v) is the sum of distance between any pair of nodes within cluster Ci, DC between any node in cluster Ci and any node in the net-i = .u Ci,v V dM(u, v) is the sum of distance work G, and DT =.u,v V dM(u, v) is the sum of distance between any two nodes in the network G.
Obviously, in contrast to the original modularity de ned by Newman [19], the value range of Distance-based Modu-larity is [ 1, 0].
If Qd = 0, it means all the nodes are either domly.
The smaller value of Qd means the better quality of clustering.
Similar to [7, 12], if we combine any two modules Cs and Ct, the distance-based modularity gain 0Qd achieved from the combination can be computed by
 d   QCs   QCt =

 st


 s DC t
 (9) C l u s t e r s



 .
.
.
i C .
.
.
N o d e S e t 1 i v 2 i v .
.
.
j v i .
.
.
D i s t a n c e M a p , 1 v , ) j v j d ( ( i )
 , 2 v , ) j v j d ( ( i )
 .
.
.
, N v , ) jN d ) j v ( ( i where DU dM(u, v) is the sum of distance between any two nodes in modules Cs and Ct respectively.
st = .u Cs,v Ct According to Equation (9), we compute the gain of distance-based modularity 0Qd for combing j clusters C1, C2, ..., Cj into a new community by


 st  .s,t {1,...,j},s&=t 2DC
 s DC t (10) Before addressing our algorithm in detail, we give the following de nitions.
De nition 6 (Nearest Neighbor) Given an incomplete information network G = (V, E, A, M), the nearest neighbor set for  v   V is de ned as NN(v) = {y|y = arg min x dM(v, x), x   V   x )= v}.
(11) De nition 7 (Mutual Nearest Neighbor) Given an incomplete information network G = (V, E, A, M), any pair of nodes u, v   V is said to be mutual nearest neighbor, denoted by u   v, i  v   NN(u)   u   NN(v)   dM(u, v) =  , where    R +.
De nition 8 (Local Community) Given an incomplete information network G = (V, E, A, M), we call the subgraph C(v) = (V !, E!, A!, M,  ) of G as a local community i  (1) v   V !
; (2)  u   V !,  v   V !
  (u   v); (3) {u|u   V !
  u   v v /  V !}
=  .
   R + is the radius of the local community C(v).
The distance-based shrinking approach DSHRINK is presented in Figure 3.
Our approach can be divided into two phases.
At the  rst phase, we compute the distance between any pair of nodes in graph G and store the distance as a 3-tuple (vi, vj, dM(vi, vj)) into a map structure (see Figure 2).
For any vi   V , the sum of the distance between node vi and any other node vj   V is saved in ST i .
Since
 i , computing ST
 i in advance can speed up computing DC s when computing Qd.
For the same purpose, the total distance DT between any pair of node is also be computed.
s =.vi Cs,vj  V dM(vi, vj) =.vi Cs At the second phase, (1) we  rst begin at an arbitrary node and span the node to a local community based on Definition 8.
All the nodes, which are in the local community, will be tagged as  visited .
Then, we choose the next un-visited node in graph G and repeat the above step.
This process will not stop until all the nodes are visited.
(2) Secondly, for each local community discovered by the  rst step, we view each single node in it as a community.
Then
 Figure 2: Data structure used in DSHRINK which means the combination of the communities can decrease the total distance-based modularity Qd, we shrink the local community as a super node.
Otherwise, the local community will not be shrunk.
(3) Thirdly, we tag all the nodes including super nodes and the common nodes as  unvisited  and repeat the  rst and second steps.
The above steps will be repeated many times until shrinking any local maximal community can not decrease the Qd any more.
Finally, the nodes condensed in a super node form a community, and di erent super nodes stand for di erent communities.
According to the de nition of local community, we know that the order of traversing the nodes in the incomplete information network G does not change the  nal members of the local community.
Moreover, from the clustering process, we know that the local community, in which nodes have a shorter distance, will be shrunk at the prior or the same iteration than the local community, in which nodes have a longer distance.
Single nodes, which have not been shrunk to any other super nodes, are viewed as hubs or outliers depending on how many communities they are close to.
If we want to form the overlapped communities, the hub nodes will be placed into more than one communities.
Otherwise, each hub node will only be placed into the community which makes the most decrease of Qd by adding the hub node.
If we view the distance between each pair of nodes as the structure similarity, the above shrinking process is similar to [12].
Approximation It is possible to speed up the clustering process by allowing some approximation in the determination of the local community.
We de ne -approximate mutual nearest neighbor and -approximate local community as follows: De nition 9 (-approximation Mutual Nearest Neighbor) Given an incomplete information network G = (V, E, A, M), any pair of nodes u, v   V is said to be -approximation mutual nearest neighbor in G, denoted by u   v, i  (v   NN(u)  | dM(u, v)   dM(v, x)|  )   (u    NN(v)  | dM(u, v)   dM(u, y)|  ), where x   NN(v), y   NN(u),  R +.
Obviously, -approximate mutual nearest neighbor is an extended version of mutual nearest neighbor.
De nition 10 (-approximation Local Community) Given an incomplete information network G = (V, E, A, M), C(v) = (V !, E!, A!, M, ) is a subgraph of network G. C(v) is said to be a -approximation local community of G i  (1) Input: G = (V, E, A, M) : Incomplete information network.
Output: C = {C1, C2, ..., Ck} : Cluster set.
HO : Hubs and outliers.
Process:



 Initialize each vi   V as a community and put it in C; for each vi   V do for each vj   V   vi #= vj do Compute dM(vi, vj ) according to Equation 4; Store (key(vi, vj ), value(dM(vi, vj ))) with ascending order into the distance map.
i + = dM(vi, vj); DT + = dM(vi, vj ); end end



 9 while true do


 for each vi   V do if vi.visited then continue Span a local community C(vi) according to De nition 8; for each vj   V (C(vi)) do vj .visited = ture; end C   C   C(vi); end Qd.descrease = f alse; for each Cj   C do Compute &Qd according to Equation 10; if &Qd < 0 then



















 vs   V (Cj ); C   (C   Cj )   vs; Qd+ = &Qd; Qd.descrease = true; vs.visited = f alse; end if !
(Qd.descrease) then break; return C, HO; end end Figure 3: The Description of DSHRINK v); (3) {u|u   V !
  u   v   V !
; (2)  u   V !,  v   V !
  (u     v   v /  V !}
=  ; (4) let f(r) = {r|r = dM(s, t), s   t   s    V !
  t   V !
}, |Max(f(r))   Min(f(r))|  ; (5)when (3) and (4) can not be held at the same time, (4) is prior to (3) to be guaranteed.
, r  R +.
We note that this relaxation of the de nition of local community can greatly speed up the clustering process.
In order to take advantage of -approximation local community, the only di erence in DSHRINK is to span a -approximation local community instead of a local community in step (12).
When we span the -approximation local community (> 0), the  nal clustering result may rely on the visiting sequence of the nodes.
In this paper, we give priority to the shorter distance nodes among all of the -approximation neighbours when spanning the -approximation local communities.
Our experimental results show that the  nal clustering e ect is almost not a ected by the order of the visiting sequence of nodes by taking the above strategy.
Furthermore, given an appropriate parameter , we  nd that this relaxation does not a ect the practical quality of the communities obtained.
Table 1: Summary of experimental data sets Dataset # Nodes # Links # Attributes # Classes











 In this section, we use two real-world data sets to validate the e ectiveness and e ciency of our approach.
All the experiments are conducted on a machine with Intel 8-core 2.7 GHz processors and 28GB memory.
DBLP-A Dataset: DBLP-A is the data set extracted from DBLP database1 which provides bibliographic information on computer science journals and proceeding.
We extract paper information from 16 top conferences which cover 6 research  elds including Arti cial Intelligence, Information Retrieval, Computer Vision, etc.
We create the coauthor network by choosing authors, who published at least 2 papers during 2000   2010, as the nodes of the network.
Any pair of authors who have coauthored are linked in this coauthor network.
This coauthor network contains
 with a bag-of-words which extracted from the paper titles published by him/her.
We  rst apply the standard text pre-processing such as stemming, stop words removal.
Then we reduce the dimension of the bag-of-words to 100 by PCA and use them as the features of the corresponding node.
In addition, the number of coauthors and publications are also used as features of the nodes.
DBLP-B Dataset: We also extract paper information from 16 top conferences of 6 research  elds such as Algorithms & Theory, Natural Language Processing, Bioinfor-matics, etc.
The same setups with DBLP-A are also used here to build the coauthor network as our second data set, called DBLP-B.
We summarize our data sets in Table 1.
ation In order to simulate the incomplete information networks with local information regions, we use the following experiment setting.
If we perform random sampling on the nodes, the sampled network usually ends up being sparsely connected, without local information regions.
In this paper, we use the snowball sampling [23] to sample a group of connected local region at a time.
We randomly sample one node and use BFS to include its neighboring nodes into the sampled region until a  xed number of nodes are sampled.
We repeat this process until a number of local regions are sampled.
Then we assume the links within the local informative regions are available to the algorithms, while the remaining links in the network are removed.
In order to control the total number of nodes being sampled, we introduce a parameter p, called sample ratio, i.e., the ratio of the nodes in the network being sampled into the local region.
In addition, we introduce another parameter q, called local information region size, to control the size of each local information region.
In detail, we  rst randomly choose a node 1http://www.informatik.uni-trier.de/ ley/db/ Md+DSHRINK Mf+DSHRINK


 y t i r u



 y t i r u
 kmeans Md+DSHRINK Mf+DSHRINK






 sample size (p%) (a) DBLP-A












 sample size (p%) (b) DBLP-B Figure 4: Accuracy comparison between di erent methods (q% = 0.3%).
kmeans Md+DSHRINK Mf+DSHRINK



 y t i r u






 y t i r u
 kmeans Md+DSHRINK Mf+DSHRINK


 local information region size (q%)







 (a) DBLP-A


 local information region size (q%) (b) DBLP-B Figure 5: Accuracy comparison between di erent methods (p% = 10%).
in the network.
We then include q% nodes from its neighbors using BFS search.
Common neighbors of any pair of nodes in the sampled region are further included into the sampled local region.
The above sampling process continues until we sample p% of the nodes in the network.
In addition to the local regions, we sample the same number of nodes and use them to generate dissimilar pairwise constraints.
In the sampled group, the pairs of nodes that are in di erent classes are then used as the dissimilar node-pair set D. More concretely, for DBLP-A and DBLP-B datasets, we choose the pair of authors, whose research  elds are not overlapped as the dissimilar node pair.
In order to measure the e ectiveness of our approach, we adopt Purity to evaluate the quality of the communities generated by di erent approaches.
The de nition of purity is as follows: each cluster is  rst assigned with the most frequent class in the cluster, and then the purity is measured by computing the number of the instances assigned with the same labels in all clusters.
Formally: max j |Ci   lj| (12) where {C1,       , Ck} is the set of clusters, lj is the j-th class Purity =
 n k !i=1 label.
The value of purity ranges from 0 to 1.
The community structure generated by each compared method will be evaluated using the true label of each node such that the higher purity value means the higher accuracy of the method.
Since each author can have multiple research areas as its class labels.
We computed the purity of the clustering results based on each label separately, and the average results over 6 labels are reported.
In order to demonstrate the e ectiveness and e ciency of our approach, we compare our approach with the following methods:   Kmeans: We use the default Euclidean metric to measure the distance between any node xi and the centroid xk.
The K value used in the dataset of DBLP-A and DBLP-B is 6, which is the same number of clusters with the ground truth.
  Md+ DSHRINK: We learn a diagonal Mahalanobis matrix Md and use it as the input of M for DSHRINK.
  Mf + DSHRINK: We learn a full Mahalanobis matrix Mf and use it as the input of M for DSHRINK.
) e s ( e m
 i




 ) e s ( e m
 i





 approximation tolerance(b) (a) Mf +DSHRINK

 approximation tolerance(b) (b) Md+DSHRINK Figure 6: The computation time with the di erent values of b.
The variation of purity scores under di erent values of p% is given in Figure 4.
In this experiments, q% = 0.3% is used.
Since the accuracy of KMeans is not a ected by the number of sampling nodes, the purity value of the KMeans is a horizontal line in all cases.
We notice that the purity scores of Mf +DSHRINK and Md+DSHRINK ascend quickly with the increasing number of local information regions sampled.
Especially, when p% > 2%, the purity values of Mf +DSHRINK and Md+DSHRINK exceed kmeans over all data sets.
That is because,  rstly, the learned Ma-halanobis matrix M rescales all of the nodes into a new feature space, where the similar nodes are closer, and the dissimilar nodes are further away than the original Euclidean space.
Secondly, DSHRINK can automatically detect the most appropriate number of communities by minimizing the distance-based modularity.
Since the number of communities in the networks is unknown, Mf +DSHRINK and Md+DSHRINK have more advantage for discovering the most appropriate community structures than Kmeans.
Another observation is that, in most of the cases, with the same value of p%, the purity scores of Mf +DSHRINK are a little higher than Md+DSHRINK in both DBLP-A and DBLP-B data sets.
This demonstrates that the full Mahalanobis matrix performs better rescaling function for separating the similar nodes from the dissimilar nodes than diagonal Ma-halanobis matrix in DBLP-A and DBLP-B data sets.
However, this principle dose not not always hold.
For instance, in DBLP-A data set, the purity score of Mf +DSHRINK is less than Md+DSHRINK when p% = 8%.
In Figure 5, given a speci ed value of p% = 10%, we also present the changes of purity scores with the di erent value of q%.
We notice that, on the one hand, for a speci ed p% = 10%, the larger value of q% makes more similar node pairs be captured in each local information region, but fewer local information regions get chosen in the whole incomplete information network.
On the other hand, with the smaller value of q%, fewer similar node pairs can be captured in each local information region, but more local information regions can be sampled.
Since both the number of local information regions and the number of similar node pairs can a ect the learning of the metric,  nding the balance point of q% is critical for achieving a better clustering result.
From Figures 5 (a) to (b), we know that the balance point of q% can be gotten between 0.5% to 0.7%.
We notice that, computing the optimal Mahalanobis matrix and the distance between any pair of nodes can be accomplished in advance before clustering process.
In this part, we mainly focus on the clustering process and test how -approximation local community speeds up the clustering process and a ects the quality of clusters.
The distance here is relative and changeable according to di erent values of w in Equation (5).
Hence, discussing the value of  is meaningless with a special value of w. Fortunately, we  nd the top k nearest nodes of each node is a good base for us to compute the appropriate  value.
In order to compute an appropriate  value, we average the sum of distance between each node and its corresponding top k nearest nodes as follows: d = .N i=1.j T opK(i) dM(vi, vj) k|N| (13) where T opK(i) is the set of node index, whose distance to vi ranks in the top k among all of the nodes to vi, and |N| is the total number of nodes in the incomplete information network.
In this paper, we choose k = 10 and give the value of  as  = d   b.
For a speci ed incomplete information network G and a metric M, the value of d is a constant.
Therefor, changing the value of b is equivalent to change the value of .
In Figures 6(a) to (b), we have illustrated the variation in the e ciency of di erent b values for Mf +DSHRINK and Md+DSHRINK.
We observe that the computation time decreases quickly with the increasing value of b.
It is because relaxing the de nition of local community to a certain extent can decrease the iteration times in the clustering process.
We also  nd that the purity score are not changed dramatically with di erenct b values.
In this paper, we presented the  rst approach for community detection in incomplete information networks with local information regions.
While the traditional community detection algorithms make the assumption of the full knowledge of linkage information, they can not solve the problem In order to resolve this problem, we explored the metric learning idea and learned a global metric from the side information of the incomplete information network.
Moreover, we proposed the distance-based modularity function.
Based on this function, we further devised a distance-based clustering algorithm DSHRINK.
In order to speed up the clustering process, some helpful approximation strategies were also proposed.
Experimental results illustrated the e ectiveness and e ciency of our approach.
The  rst author is supported by National Natural Science Foundation of China through grants 60933005, 60873204 and the National High-Tech Program through grant
 NSF through grants IIS 0905215, DBI-0960443, and Google Mobile 2014 Program.
