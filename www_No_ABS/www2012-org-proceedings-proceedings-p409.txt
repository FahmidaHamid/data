The World Wide Web has grown in size exponentially for many years, and this has been accompanied by search engines becoming the preferred way that users  nd and access information online.
Hundreds of millions of queries are issued to the major search engines everyday, almost all of them in the form of  keywords .
Over years of usage, the keyword-search functionality has become the standard convention expected by users and supported by all major search engines.
This has resulted in users becoming adept at reducing their search needs speci cation to the bare minimum set of keywords needed to help the search engine  nd relevant results.
There have been some attempts amongst search engine practitioners to induce users to provide queries in natural text, most notably by Powerset [2], but by and large all search engines expect users to express their search need via a small set of keywords.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Kunal Punera RelateIQ Mountain View, CA 94041 kunal.punera@utexas.edu Given these state of affairs, there has been a considerable amount of work on extracting as much information as possible from user queries in order to determine their intent [7, 9, 22, 29].
Once this intent is extracted it can be used to provide relevant results improving the search experience [4], to detect whether a query has a commercial intent [22], to select a useful set of advertisements [8], and even to learn from the user s interaction with the search engine [24].
Challenges in extracting the intent information from queries arise from scalability issues since any potential solution must scale to hundreds of millions of queries a day, as well as instrumentation issues since the only user actions search engines observe are user clicks [16, 24].
However, the main dif culty arises from the brevity, and associated ambiguity, of the keywords in the query.
Keywords can sometimes be ambiguous when considered without the surrounding context; as in the oft-cited example of the word  jaguar , which can denote the car as well as the animal.
On the other hand, considering all keywords in a query together to maintain the context can result in sparsity issues; for example, determining the characteristics (e.g., advertisement click-through-rate) for tail queries such as  jaguar xj12 95 6.0l engine mount  is largely infeasible given the rarity of the query [22].
In this paper we seek to solve these issues by enriching search queries with information about the hidden structure underlying them.
In particular, our goal is to develop methods that can automatically determine that the tail query  jaguar xj 12 95 6.0l engine mount  can be described using the <Brand,Model,Year,Part> pattern.
We refer to such patterns as query templates and their constituents, such as Brand and Model, as attributes.
Such an enrichment can help in inferring and catering to the user intent behind the query.
For example, we can provide custom search experiences for certain templates, such as returning, on the search results page, the availability and prices for vehicles queried via the <Brand,Model,Year> query template.
Moreover, these enrichments can help us in dealing with data-sparsity by learning query characteristics (such as click-through-rates, search dif culty) at the template level instead of the individual query level.
In fact, in this paper we demonstrate that using query templates can improve performance over the state-of-the-art method for estimating Query-Advertisability.
Many past works have proposed methods to extract information in web search queries.
Some of these analyze queries to obtain seg-mentations [5, 19], while others extract named entities from queries [15,
 the tasks, such as manually labeled seed data, or use ancillary information such web search click-through data, both of which might be expensive or dif cult to obtain.
Some recent works, such as Agarwal et al. [3] and Sarkas et al. [25] focus on detecting templates in queries, while Szpektor et al. [27] use detected templates for improving query recommendations.
However, these works as-given as inputs in form of database relations or entity hierarchies.
Given that search engines  eld queries on a wide array of domains (with user interests following a long-tail [11]) and the terminology used in these queries is constantly in  ux (eg., new movies are released every week), it would be prohibitively expensive to create and maintain these attributes and their vocabularies.
Hence, in this work we consider the more realistic setting of extracting templates while constructing attributes and their vocabularies at the same time, without requiring any editorial/manual intervention.
At its very core this problem can be looked at as one of grouping keywords into attributes, and hence text-mining algorithms such as Spherical k-Means and LDA can be used [6, 10].
While these approaches are promising, they are not designed to take into account constraints/properties speci c to how users construct queries.
For example, for any given domain, users are known to employ only a few con gurations of attributes as query templates [3], whereas both these methods allow queries to be generated from arbitrary combinations of attributes.
Our proposed approach uses this knowledge to reduce the search space of attribute-word vocabularies and template-attribute con gurations.
In Section 3 we discuss this and other important properties of the problem and how they are incorporated in our model.
While this adds signi cant complexity to the modeling process, we give a procedure to estimate the parameters of the model.
In Section 5 we empirically demonstrate that, without any supervision, manual or external (eg. click-through data), our approach is able to extract meaningful templates and attributes from short keyword queries.
plates in queries as well as learning the attributes and their vocabularies.
As far as we know this is the  rst work to tackle this problem without requiring any seed input, external data (eg. click-through data), or manual supervision.
queries are formulated by users.
We then give a generative model for queries that takes these properties into account.
infer the query templates and attributes in our model from observed search queries.
that our approach  nds better query templates and attributes than two state-of-the-art approaches based on LDA and k-Means.
To ensure representative and robust results we repeat our experiments on real-world queries from three different domains.
tected by our approach we consider the problem of predicting the Advertisability of tail queries.
Our experiments demonstrate that using automatically inferred query templates can signi cantly improve upon the state-of-the-art [22].
In the next section we present our formulation of the problem of extracting templates and attributes from observed web search query data.
Our proposed solution for this problem is presented in Section 3.
We postpone a detailed discussion of related work to Section 4, where we can better contrast our proposed approach to it.
In that section we also describe two existing approaches that can be adapted to extract query templates; these serve as strong baselines in the empirical evaluation of our approach using labeled ground truth in Section 5.
In Section 5.5 we evaluate the templates discovered by our model on the task of predicting query advertisability.
Finally, we summarize the paper in Section 6.
In this section we formulate the problem of extracting query templates from web search queries.
In this paper our goal is to extract, with a completely unsupervised process, a set of domain-speci c query patterns that most search queries in a domain conform to.
For example, we would like our approach to discover that many search queries in the Automobiles domain can be described using the <Brand, Model, Year> pattern.
We refer to such patterns as query templates and their constituents as attributes.
Here the attribute Brand denotes a placeholder for brand/manufacturer of the vehicle and can stand-in for words such as Honda, Toyota, Ford, etc.
Similarly, attribute Model can stand-in for words such as Accord, Civic, etc., and, Year denotes the year when the car was manufactured.
Extracting attributes from query-logs and enriching queries with the templates they conform to can facilitate many search engine operations.
For example, due to sparsity issues search engines struggle with obtaining robust estimates of various properties, such as advertisement click-through rates, of tail queries.
With the ability to discover that the query  jaguar xj12 95 6.0l engine mount  corresponds to query template <Brand, Model, Year, Parts>, we can smooth the ad-click-through estimates of tail queries with the aggregated estimates of the corresponding templates.
Other applications include building custom search solutions for some query templates and using the extracted templates for improved query recommendations [18, 27].
Here we note that some past works [3, 5, 15, 19, 23, 20, 25, 27] have proposed methods to extract similar information from web search queries.
However, these approaches require either direct supervision of the tasks, such as manually labeled seed data, or use ancillary information such web search click-through data (more details in Section 4).
The manually labeled seed data takes the form of attributes and their vocabularies in [3, 25] and entity classes or hierarchies in [5, 27], all of which are expensive to create and maintain in a dynamically changing environment like web search.
While the ancillary information needed in the form of entities [23] and text of documents [19] clicked in response to queries, can be dif cult to obtain.
Therefore, in this work we restrict ourselves to the setting of extracting templates while constructing attributes and their vocabularies at the same time, without requiring any editorial/manual intervention.
From our inspection of search query logs we observed that different users with the same search intent issue variations of a query to the search engine.
However, at a fundamental level they follow a common process for generating these queries from a common underlying schema, as shown in Figure 1.
For example, for the search intent  nd information about the 6.0l engine mount of a 1995 jaguar xj12  some users might formulate the query  jaguar xj 12 95 6.0l engine mount  while others might use  jaguar xj 95 en gine mount .
Both these queries can be thought of as being generated from the query template <Brand, Model, Year, Parts> with the latter query containing fewer terms from the attributes Model and Parts.
Similar observations hold for other domains of web search such as Entertainment, Travel, etc.
In each case while we do not know the underlying schema   the templates, attributes, and vocabularies are hidden   and do not observe the underlying generative process, we do see the generated query load.
Our approach in this paper is to mathematically construct a realistic generative process for the queries so that we can reconstruct (infer) the hidden template-structure used to generate them (as shown in Figure 1).
In this section we formally describe our proposed generative model and give an algorithm to perform inference on it.
We also describe how our modeling process satis es the properties mentioned in Section 2.2.
Our generative model works as follows.
We start with a pool of template con gurations,1 called candidate pool, whereby each con guration consists of a set of attributes.
For example, say we have 3 attributes, Model, Year and Brand.
Then the candidate pool of template con gurations can be any subset of {<Model>, <Brand>, <Year>, <Model, Brand>, <Model, Year>, <Brand, Year>, <Model, Brand, Year>}.
Note that the empty template con guration is not allowed in the pool.
In a real-world setting, the candidate pool can also be constructed by a domain expert.
Given the candidate pool we let the model choose T template con gurations to construct vector (cid:126)  where  t denotes the template con guration at index t.2 Then each query q picks a template index tq which leads to its template con guration  tq .
This way the queries can pick from only those template con gurations which are present in (cid:126)  (while the candidate pool has many more con gura-tions), helping our model enforce PROPERTY I mentioned above.
One way to think of this is that the candidate pool denotes the set of template con gurations which are appropriate for the domain.
Then the model chooses T template con gurations from the candidate pool, (cid:126) , to best explain the generation of queries.
By varying the value of T we can control the trade-off between data likelihood and over tting.
Lastly, given a template con guration for a query, from each attribute in the con guration we generate a few words and then arrange them to create the query.
More speci cally, each attribute has an associated Poisson distribution to determine the number of words it contributes (PROPERTY III), and a Multinomial distribution over the vocabulary to decide which words it contributes.
In our running example, in our learnt model we expect the Poisson parameter for the attribute Year to be smaller than that for the attribute Parts.
Moreover, we expect that the Multinomial distribution associated with attribute Brand has a much larger probability of generating the word  honda  than, say, the attribute Year.
The priors for all distributions are chosen to be their conjugates; Dirichlet for Multinomial and Gamma for Poisson.
We enforce PROPERTY II within the inference process described in Section 3.2.
Formally, the parameters of the model are:     Dirichlet( ) {multinomial distribution over all the template con gurations in the candidate pool}  t   Multinomial ( ) {con guration for template index t}     Dirichlet( ) {vector of size equal to the number of template indices, say T , that queries are allowed to chose from tq   Multinomial ( ) {template index for query q}  a   Dirichlet( ) {multinomial word distribution for attribute a}  a   Gamma(g1, g2) {Poisson parameter for the number of words that attribute a contributes towards a query when the attribute is present in the template for the query}
 tions, and query templates interchangeably
 placement, i.e., a con guration can make into (cid:126)  at two different indices.
In other words, for t (cid:54)= t(cid:48),  t may be equal to  t(cid:48).
Figure 1: Search queries are assumed to be generated from a common hidden underlying structure.
Our goal in this paper is to devise an algorithm to use the observed queries to reconstruct the parameters of the generative process as well as the underlying template structure The simplest generative process would be single-attribute template model whereby each template has a single unique attribute, and each attribute is associated with a set of words (a word distribution).
When constructing a query a user picks a template, and then picks words from the associated attribute.
This, however, is an unrealistic process since we have seen in the examples above that most queries have words from different attributes of a domain, such as Brand, Year etc.
Hence, we consider multi-attribute template models whereby each template is associated with multiple attributes.
Here, each attribute has its own word distribution from which the words are chosen to instantiate the query.
Besides having multi-attribute templates we list some additional properties that we desire in our model.
These properties bring the generative model closer to reality.
Moreover, they are also critical for the model performance since queries are short and sparse, and so searching a more constrained/restrictive but realistic model space is likely to be more robust and perform better.
PROPERTY I: While most queries are different, we believe that a large number of queries in a given domain can be captured by only a few templates.
In other words, our model must not allow arbitrary distributions of attributes as templates; instead we constrain the number of distinct templates that can be formed in the model.
PROPERTY II: Once a template for generating a query is picked, we force each attribute in the template to generate at least one word.
The intuition behind this is that since queries are short, with 2-3 words on average, if an attribute is not contributing any words towards a query, it is not required in the template.
PROPERTY III: Each attribute has a speci c word distribution as well as a distinct tendency for the number of words it contributes in a query.
For example, the Year attribute in the Automobiles domain is likely to contribute 1 word (e.g., 1995), while the Parts attribute typically generates more words (e.g., 6.0l engine mount).
Following this discussion we can formulate our problem as follows: PROBLEM DEFINITION: Given a set of queries, extract the underlying schema (templates, attribute, and their vocabularies) and learn the parameters of the generative process in a completely unsupervised manner while respecting the properties mentioned above.
Accounting for these properties signi cantly increases the technical complexity of our model over the state-of-the-art baseline methods such as LDA [6] and Spherical k-Means [10] that are not designed to enforce these properties.
However, in Section 3.2 we show that it is still feasible, mathematically and computationally, to perform inference in our model.
Moreover, we empirically show in Section 5 that these properties help our approach signi cantly improve performance over the baselines.
lowing process:

 Dirichlet( ).
to T:  t   Multinomial ( ).
(a) Sample the word distribution prior:  a   Dirichlet( ).
(b) Sample the Poisson parameter:  a   Gamma(g1, g2).
(a) Sample template index tq   Multinomial ( ), which leads to template con guration  tq (b) For each attribute a    tq : i.
Sample ni,a   Poisson( a).
ii.
Place attribute a at ni,a number of positions in vector (cid:126)zq.
(c) For each position j in query q, sample word wq,j from attribute zq,j, i.e., wq,j|zq,j,  zq,j   Multinomial ( zq,j ).
Other probabilistic models could be modi ed to extract templates (such as LDA [6]).
In Section 4.1.1 we describe these alternative models in detail and show how our model differs from them.
In Section 5 we empirically compare them against each other.
Here we describe how we perform inference on our model.
Due to paucity of space we skip many intermediate steps of algebra; see the longer version [21] for detailed derivation of the expressions.
Recall that (cid:126)  denotes the set of T template con gurations (i.e., (cid:126)  = { t}T t=1) and (cid:126)  denotes the Poisson parameters for all attributes A, i.e.,   = { a}|A| a=1.
Also, let (cid:126)w, (cid:126)z, (cid:126)t denote the sequence of words, attributes and templates over all queries.
Given the hyper-parameters, we  rst derive the collapsed representation of the joint distribution over the known and latent variables.
The joint distribution is then used for inference in Section 3.2.1.
P ( (cid:126)w, (cid:126)z, (cid:126)t, (cid:126) , (cid:126) ,  ,  ,   |  ,  ,  , g1, g2) Integration over the latent variables (cid:126) ,  ,  ,   gives: P ( (cid:126)w, (cid:126)z, (cid:126)t, (cid:126)  |  ,  ,  , g1, g2) = (cid:0)P ( (cid:126)w|(cid:126)z,  ) P ( | )(cid:1)   (cid:0)P ((cid:126)t| ) P ( | )(cid:1)  (cid:0)P ((cid:126)z|(cid:126) , (cid:126)t, (cid:126) ) P ((cid:126) |g1, g2)(cid:1)   (cid:0)P ((cid:126) | ) P ( | )(cid:1) (cid:90) (cid:0)P ((cid:126)t| ) P ( | )(cid:1) d  (cid:90) (cid:0)P ( (cid:126)w|(cid:126)z,  ) P ( | )(cid:1) d    (cid:90) (cid:0)P ((cid:126)z|(cid:126) , (cid:126)t, (cid:126) ) P ((cid:126) |g1, g2)(cid:1) d(cid:126)    (cid:90) (cid:0)P ((cid:126) | ) P ( | )(cid:1) d  TERM I(cid:90) (cid:0)P ( (cid:126)w|(cid:126)z,  ) P ( | )(cid:1) d  =  
 We compute each of these terms in turn.
 ( (cid:126)na + (cid:126) ) |A|(cid:89) = where na(w) denotes the number of times word w is assigned to attribute a, (cid:126)na = {n(w) w=1, and  ((cid:126) ) = a }V .
a=1  ((cid:126) ) (cid:81)dim (cid:126)   ((cid:80)dim (cid:126)  k=1  ( k) k=1  k) TERM II(cid:90) (cid:0)P ((cid:126)t| ) P ( | )(cid:1) d  =  ( (cid:126)nt + (cid:126) )  ((cid:126) ) where (cid:126)nt = {nt}T t=1 denotes the vector of counts of queries assigned to each template index, and T denotes the total number of template indices that queries are allowed to choose from.
(cid:90) (cid:0)P ((cid:126)z|(cid:126) , (cid:126)t, (cid:126) ) P ((cid:126) |g1, g2)(cid:1) d(cid:126)  (cid:90) |Q|(cid:89) (cid:18) 1 (cid:89) (cid:0)Poisson(nq,a| a) nq,a!
(cid:1)(cid:19) q nq!
a (tq ) = Gamma( a|g1, g2) d(cid:126)  where nq denotes the length of query q and nq,a denotes the number of words from attribute a in the query.
Since Gamma is conjugate prior for Poisson, the above integration can be simpli ed to: Q nq,a   1)!
Q nq,a)(g1   1)!
g1 (g1 +(cid:80) (g2 + Na)(g1+(cid:80) (cid:19) (cid:89) (cid:18) |Q|(cid:89)
 nq!
(cid:19) (cid:18) g2 = a A q where Na denotes the number of queries with attribute a in their templates.
TERM IV(cid:90) (cid:0)P ((cid:126) | ) P ( | )(cid:1) d  =  ( (cid:126)n  + (cid:126) )  ((cid:126) ) where the ith element of n  is the number of template indices, from
 Inference
 Our goal is to infer the attribute assignment of words and templates assignment for the queries.
Mathematically speaking, we want to infer the distribution P ((cid:126)z, (cid:126) , (cid:126)t| (cid:126)w), which can be written as: P ((cid:126)z, (cid:126) , (cid:126)t| (cid:126)w) = P ((cid:126)z, (cid:126) , (cid:126)t, (cid:126)w) P ( (cid:126)w) = (cid:80) P ((cid:126)z, (cid:126) , (cid:126)t, (cid:126)w) (cid:126)z,(cid:126)t,(cid:126)  P ((cid:126)z, (cid:126) , (cid:126)t, (cid:126)w) where we omit the hyper-parameters for convenience.
Clearly, the denominator in the above expression is a summation over a large number of combinations and is dif cult to compute.
Hence, we use Gibbs sampling to perform this inference [13, 14].3 Under the Gibbs sampling procedure, the full conditional distributions (P (zi|(cid:126)z i, (cid:126) , (cid:126)t, (cid:126)w), P ( i|(cid:126)z, (cid:126) i, (cid:126)t, (cid:126)w), P (ti|(cid:126)z, (cid:126) , (cid:126)t i, (cid:126)w)) are used to simulate P ((cid:126)z, (cid:126) , (cid:126)t| (cid:126)w).
To derive these conditionals, we use the joint distribution P ((cid:126)z, (cid:126) , (cid:126)t, (cid:126)w) (computed earlier): P ((cid:126)z, (cid:126) , (cid:126)t, (cid:126)w| ,  ,  , g1, g2) = P ( (cid:126)w|(cid:126)z,  ) p((cid:126) | ) p((cid:126)t| ) p((cid:126)z|(cid:126) , (cid:126)t, g1, g2) Next we give derivation of the conditional distributions for different latent variables, i.e., zi,  i, ti.
As mentioned above, these conditionals are then used to perform the Gibbs sampling for inferring the query templates and attributes.
The overview of our complete approach is given in Section 3.3.
say (cid:126)x, are estimated by computing: P ((cid:126)x| (cid:126)w) = P (xi|(cid:126)x i, (cid:126)w) = P ((cid:126)x i, (cid:126)w) = P ((cid:126)x, (cid:126)w) P ((cid:126)x, (cid:126)w) (cid:82) X P ((cid:126)x, (cid:126)w)dxi .
Here we simply give the expressions for the conditional distributions and give the intuition behind them; please refer to the longer version [21] for complete derivations.
CONDITIONAL FOR z.
Ideally, we would like to compute (P (zi|(cid:126)z i, (cid:126) , (cid:126)t, (cid:126)w) where zi is the ith element of attribute sequence z.
But in our case zi s are not sampled independently as this can result in one of the attributes contributing zero words to the query (we do not allow this under PROPERTY II mentioned above).
Hence, the sampling is done on per-query basis, say (cid:126)zq, which consists of the attributes for every word in the query.
Hence, it is convenient to compute the conditional in terms of (P ((cid:126)zq|(cid:126)z q, (cid:126) , (cid:126)t, (cid:126)w).
One can show that (derivation in the long version [21]): P ((cid:126)zq = (cid:126)vq|(cid:126)z q, (cid:126) , (cid:126)t, w) = 1 nq (cid:81) (g1+(cid:80) (g1+(cid:80) (g2+Na 1) (g2+Na) Q ni,a 1 nq,a+vq,a)!
Q ni,a nq,a ) Q ni,a nq,a+vq,a) Q ni,a 1 nq,a)!
a (cid:126)vq (g1+(cid:80) (g1+(cid:80) where (cid:126)vq denotes a new attribute sequence for query q, Na denotes the number of queries with attribute a in their templates, ni is the length of query i, ni,a is the number of terms in query i from attribute a, and nq,a and vq,a denote the number of terms from attribute a in the old query con guration q and the new con guration v, respectively.
CONDITIONAL FOR t.
Recall that (cid:126)t denotes the vector consisting of the template indices of all queries.
We compute the conditional by deriving the probability of updating the template index of query j.
P (tj = k|(cid:126)t j, (cid:126)z, (cid:126)w, (cid:126) )   P ((cid:126)z|(cid:126) ,(cid:126)t) P ((cid:126)z j|(cid:126) ,(cid:126)t j ) P ((cid:126)t) P ((cid:126)t j ) We computed P ((cid:126)z|(cid:126) ,(cid:126)t) P ((cid:126)z j|(cid:126) ,(cid:126)t j ) earlier.
Next we look at P ((cid:126)t) P ((cid:126)t j ) .
P (tj = k, (cid:126)t j) P ((cid:126)t j) = (cid:80) n(k) t, j +  k k n(k) t, j +  k and n(k) where n(k) plate index k, with and without query j.
Hence, n(k) t, j denote the number of queries assigned tem-t, j + 1.
t = n(k) t CONDITIONAL FOR  .
Next we compute the conditional for  , i.e., P ( j|(cid:126) j, (cid:126)z, (cid:126)w, (cid:126)t).
Recall that (cid:126)  denotes the vector of T template con gurations selected by the model, from which each query is assigned a con guration.
Note that by updating the index j of (cid:126)  to any con guration c (i.e., set  j = c), we indirectly update the template of each query that is pointing to  j (i.e., queries which have tq = j).
Hence, P ( j = c|(cid:126) j, (cid:126)z, (cid:126)w, (cid:126)t)   P ((cid:126)z|(cid:126) ,(cid:126)t) P ((cid:126)z j| j ,(cid:126)t) P ((cid:126) ) P ((cid:126) j ) where (cid:126)z j denotes the attribute sequence, excluding all the queries whose template is pointing to  j con guration.
We can compute P ((cid:126)z|(cid:126) ,(cid:126)t) P ((cid:126)z j|(cid:126) j ,(cid:126)t) as we computed it earlier.
Next we look at P ((cid:126) ) P ((cid:126) j ) P ( j = c,  j) P ( j) = (cid:80) n(c)  , j +  c c(cid:48) n(c(cid:48))  , j +  c(cid:48) and n(c)  , j denote the number of elements in vector (cid:126)  that where n(c)   have attribute con guration c, with and without including element j.
Hence, n(c)   = n(c)  , j + 1.
Above we have described the model and the update equations (i.e., conditionals).
Here we summarize how the conditionals are used to perform the inference.
The procedure begins with a random initialization of the queryword-attribute assignment (cid:126)z, the query-template assignment (cid:126)t, and the set of T template con gurations (cid:126) .
Then we iterate over queries and the template set using the conditionals derived in Section 3.2.2 to update the (cid:126)z, (cid:126)t, and (cid:126)  vectors.
At each iteration we compute the likelihood of the observed query data given the current learnt model parameters ( ,  ,  , (cid:126) ).
The procedure ends with an assignment of each query to a template, and each word in the query to an attribute.
Some existing methods can be adapted to tackle the problem of discovering templates; in this section we describe two such approaches, LDA, and k-Means, and highlight the ways in which our proposed model differs from them.
We end this section with a survey of some past works that are broadly related to our problem setting.
Latent Dirichlet Allocation is a popular model for unsupervised discovery of document topics [6, 14].
Before showing how it can be applied for template extraction, we brie y describe the generative model here.
In the LDA model, each topic has an associated   distribution over the vocabulary.
To construct document d,  rst a multinomial distribution over the topics, denoted by  d is sampled from a Dirichlet prior.
The ith word in the document is picked by choosing a topic from this multinomial distribution, and then sampling a word from the   distribution associated with the topic.
In our scenario each query can be thought of as a document.
By applying LDA we can  nd the attributes (i.e., the topics) and their associated vocabularies that have been used to generate the queries.
These attributes can then be used to construct the query templates.
Here we note that there are some fundamental differences between the LDA model and our proposed generative model.
First, LDA allows each query to pick an arbitrary topic distribution, while we constrain the query to conform to one of the  nite templates (i.e., attribute con gurations) as is required by PROPERTY I of the problem de nition in Section 2.2.
Second, even if the topic distribution for a query has a high probability for a topic, LDA does not require that topic to contribute a word in the query.
In contrast, our model forces each attributes in the template to contribute at least one word to the query enforcing PROPERTY II of the problem de nition.
As discussed in detail in Section 2.2 these properties bring the generation closer to reality, and reduce the model search for our approach to a more constrained and realistic space; this is particularly helpful since queries are short and sparse.
Incorporating these properties in our model adds signi cant technical complexity.
For example, for enforcing PROPERTY I we maintain a  nite pool of allowed con gurations ((cid:126) ).
All other latent variables (e.g., (cid:126)z, (cid:126)t) depend on this (cid:126)  vector, and as a result, when (cid:126)  is updated in the inference process, all other latent variables have to be updated accordingly.
Similar, enforcing PROPERTY II means that we cannot sample the attributes for words (z s) independently.
Instead, the sampling is done on a per-query basis.
In our experiments we justify this added complexity by comparing LDA with our proposed model and showing that our approach results in much better template extraction performance.
The problem of extracting attributes can be framed as a problem of grouping the query words, and hence any text-clustering approach can be used.
In this section we describe how Spherical k-Means [10] can be applied.
We represent each word wu as a vocabulary-sized vector, where each element v of the vector contains the number of times words wu and wv occur together in queries.
By running Spherical k-Means on these vectors, we put together those words into a cluster which have similar co-occurrence behavior as other words, e.g., words such as accord and toyota should have similar co-occurrence behavior with respect to brand words and years, and should fall into the same cluster.
Hence, these clusters act like attributes for our scenario; using them we construct query templates.
Many past works have tackled problems related to modeling query keywords.
While a full survey is not possible due to lack of space, we discuss some key works that can help us put our work into context.
The problem of assigning an attribute to each word in a query has been explored in [3, 25].
Agarwal et al. [3] proposed an approach based on random-walk on the tripartite graph of queries, sites, and templates.
Sarkas et al. [25] assumed that  structured  data is given in the form of tables.
Then queries are annotated by mapping a query to a table and the attributes of this table.
Both these works, however, assume that the attributes and their vocabularies are given as input to the algorithms.
In contrast our work  nds both the query templates as well as the attributes and their vocabularies in a completely unsupervised manner.
The problem of named entity recognition in web queries (eg.  nd-ing movie names) is related to our problem with entities playing the role of attributes.
In [15], Guo et al. give a nice semi-supervised approach to extract entities from queries while ensuring that the model topics and pre-de ned classes align.
In [30] the approach learns a topic-model using click-through data under some supervision from humans and uses the model to resolve ambiguities among named entities.
In another related work [20] a weakly-supervised extraction framework is given for extracting named entities from web search queries starting from a seed set.
Another set of work seek to obtain useful segmentations of web queries via learning from labeled examples [5] or using click-through data [19].
Our work differs from these in the focus   we focus assigning all words to attributes, not just named entities   and based on the fact that our proposed approach works with just the query-set and does not need manual intervention or data click-through interactions.
Finally, there are recent works that seek to exploit query templates for accomplishing search related tasks.
In [27] Szpektor et al. used entity hierarchies to mine query templates, which were used to improve query recommendation algorithms by de ning better relationships between queries.
Similarly, Jain et al. [18] use many heuristics to de ne relationships between queries, and query templates could be used as one such signal.
Hence, our work is complementary to these works that seek to improve web mining algorithms via query templates and could act as input into them.
In this section we analyze the performance of our approach (QT-GEN) on real-world search engine queries.
We also provide an empirical comparison with the state-of-the-art alternatives described in Section 4.1.
We begin with a description of the experimental methodology and then proceed to describing the results.
We end the section with an application of our approach to the task of predicting query-advertisability.
Domain Automobiles Travel Movies Attribute Brand Model Year Parts Specs Vehicle_type Tasks Brand Location Quali ers Tasks Names Genres Quali ers Vocabulary Vocab-size Honda, Toyota .
.
.
Civic, Camry .
.
.
engine, tires .
.
.
mpg, 250hp .
.
.
car, sedan .
.
.
purchase,  ights .
.
.
hilton, southwest .
.
.
hawaii, SFO .
.
.
cheap, discount .
.
.
tickets, reviews .
.
.
avatar, brad pitt .
.
.
horror, bollywood .
.
.
free, online .
.
.
Table 1: Ground Truth: attributes and vocabularies created manually.
The learned attributes output by QTGEN and baselines are evaluated in terms of their match to this ground truth.
We  rst describe the construction of the query dataset and ground truth.
We then describe the implementation details of our approach, QTGEN, and of the competing baselines.
In order to obtain robust results and reduce the effect of any one topic we perform our empirical evaluation using queries from multiple different domains: Automobiles, Travel, and Movies.
This is the standard methodology in this area.
Topical classi cation of queries is a well-studied problem and we use a state-of-the-art multi-label classi er [28] to classify a randomly sampled set of 100 million Yahoo search queries executed in September of 2010 into the above domains.
These queries were suitably anonymized and care was taken to remove all personally identi able information was removed.
From these domain-speci c queries we construct datasets for our two evaluation tasks.
First, we construct three domain-speci c query-sets of roughly 1000 queries each; the size was picked so that we could manually construct the ground truth.
Second, for the large-scale automatic evaluation on the query-advertisability task we construct three datasets with 43793, 83387, and 15050 tail queries from the Automobiles, Travel, and Movies domains, respectively.
For this task we also obtain the sponsored search impressions and clicks from the search logs.
In order to construct the ground truth, templates underlying the queries in these query-sets were manually extracted.
This resulted in the construction of 6 attributes for Automobiles domain and 4 each for the Travel and Movies domains, each of which was populated with the words likely to be generated from them; some words were labeled as being generated from multiple attributes.
Some details about the ground truth are given in Table 1.
Note that the ground truth construction was completed before the outputs of any of the approaches under evaluation were seen by the editors.
QTGEN: This is an implementation of the approach outlined in Section 3.
Each run of QTGEN is parameterized by the following settings: number of attributes (k), number of iterations (N), and values of model parameters  , g1, and g2.
Please see the description of our model for details of these parameters.
In our empirical analysis we perform many experiments by varying values for these parameters, but unless mentioned otherwise, the values are set to k = 5, N = 100,   = 0.1, g1 = 4, and g2 = 0.2; these parameter values were tuned using a 10% validation set.
LDA: For this baseline approach we used the Mallet [1] implementation of Latent Dirichlet Allocation, which has been described in detail in Section 4.1.1.
The parameters values are set to k = 5,   = 50 and   = 0.01.
baselines on the Automobiles domain.
Each mark on the curves (from left to right) is produced by evaluating the top-N learnt attributes.
K-MEANS: This baseline approach is described in detail in Section 4.1.2.
In this implementation the initialization is done via farthest  rst approach.
For our experiments, k was set to 5 (set using a 10% validation dataset) and distance measure between vectors was computed using cosine similarity [10].
Truth Above we described the process through which the templates that generate queries were manually extracted for three domains.
In this section we evaluate the performance of QTGEN, LDA, and K-MEANS in successfully retrieving the attributes that form these templates.
Before proceeding to the results we describe the metrics we use to evaluate the outputs of the various approaches.
Traditional clustering evaluation measures such as pairs-based ones (e.g., Adjusted RAND [17]) and entropy-based ones (e.g., normalized mutual information [26]) are not suited for the tasks we consider in this paper.
In designing an evaluation scheme relevant to our problem setting, we  rst attempt to understand how the extracted templates/attributes are likely to be used.
Most applications of interest need the attributes to be returned in some order, and since none of the approaches under evaluation ranks the attributes, we assume each approach returns attributes in the decreasing order of the number of queries that generate words from it.
Once the order is set, we want that each learned attribute contain words from only one attribute from ground truth.
Moreover, we want that for each attribute in the ground truth, all its words be covered by one learnt attribute.
To ful ll these requirements each ground truth attribute must be mapped to a unique learnt attribute.
In a real-life application, this mapping would be constructed by a human expert when she studies the attributes output by the system.
In order to evaluate numerous runs of our approach and baselines objectively, we construct this mapping automatically by evaluating all possible mapping in terms of the total AUC [12] between the learnt and the ground truth attributes, and picking the one with the maximum score.
Once the mapping is  xed, we go over the learnt attributes in the order established and evaluate them in terms of net PRECISION and CORRECTRECALL.
We say that a word is correctly placed if the learnt attribute and the ground truth attribute it belongs to are mapped to each other.
Hence, PRECISION(N) is the fraction of words in the  rst N learnt attributes (in the algorithm s ordering) that are correctly placed.
Similarly, CORRECTRECALL(N), is the fraction of words in ground truth attributes mapped to the  rst N learnt attributes that are correctly placed.
Intuitively, it can be seen that PRECISION measures the accuracy of the system while COR-RECTRECALL measures the correct coverage.
Figure 3: PRECISION vs. CORRECTRECALL curves of QTGEN and baselines on the Travel domain.
Each mark on the curves (from left to right) is produced by evaluating the top-N learnt attributes.
Figure 4: PRECISION vs. CORRECTRECALL curves of QTGEN and baselines on the Movies domain.
Each mark on the curves (from left to right) is produced by evaluating the top-N learnt attributes.
We plot PRECISION and CORRECTRECALL of all approaches on the three domains in Figures 2, 3, and 4.
The PRECISION is plotted on the y-axis while the CORRECTRECALL is on the x-axis.
The markers on each curve indicate the performance values at different N, with markers on the left indicating values for lower N. As we can see, in general, at higher values of N, PRECISION tends to decrease while CORRECTRECALL monotonically increases.
First we observe that attributes found by QTGEN match the ground truth very precisely in their word compositions.
For example, in the Automobiles domain, the top-3 learnt attributes have > 60% of the words that were assigned to them and were also manually labeled as belonging to these attributes.
Similar results are also seen for the other two domains.
Being this precise makes the learnt attributes easier for downstream human users to interpret as well as easier for automated algorithms to use.
We also observe that for all domains and operation points, the attributes found by QTGEN dominate those found by LDA and K-MEANS in terms of CORREC-TRECALL.
The runners-up in terms of PRECISION is clearly the K-MEANS approach; in fact it equals QTGEN in terms of PRECISION for the Movies domain.
The second observation about the results concerns the CORREC-TRECALL of the learnt attributes.
As we can see, upon learning 5 attributes our approach consistently  nds around 50% of the words in the matched ground truth attributes.
On the Automobiles dataset, QTGEN vastly outperforms the baseline approaches, while on the other two domains the algorithms are roughly comparable in terms of CORRECTRECALL.
In fact, LDA, and to a lesser degree K-MEANS, consistently operate at a lower PRECISION and slightly higher CORRECTRECALL pro le.
In many applications producing output with high CORRECTRECALL is very important, however, we feel that the the low PRECISION of attributes learnt by LDA and K-MEANS will make them too obscure to be used automatically by algorithms or manually by human editors.
different number of iterations on the Automobiles domain.
Figure 6: PRECISION vs. CORRECTRECALL curves of QTGEN using different number of attributes on the Automobiles domain.
Finally, we note that the results indicate that learnt attributes that are heavily used also score high in terms of PRECISION and COR-RECTRECALL.
This is completely true in the case of the Automobiles domains and to a slightly lesser degree for the other two domains.
This can be seen in the curves of QTGEN in Figures 2, 3, and 4 which are constructed by considering attributes in the de- creasing order of their usage.
As we can see for the performance curve of QTGEN on Automobiles domain (Figure 2), the PRECISION drops monotonically, indicating that the most precise attributes are the most heavily used, and the gaps between successive marks on the x-axis become smaller, indicating the same trend for the COR-RECTRECALL of learnt attributes.
It is also clear from the  gures that the attributes learnt by LDA and K-MEANS display this desirable property to a much lesser degree.
To summarize, we have seen that QTGEN outperforms the baselines in terms of obtaining interpretable attributes that cover a large fraction of the query-terms correctly.
The parameter setting used to report this performance was tuned over a 10% held-out validation dataset.
Next, we show the effect of variation of these parameters on the performance of QTGEN in terms of PRECISION and COR-


We start by studying the effect of the number of model iterations.
In Figure 5 we have plotted the scores of attributes identi ed by QT-GEN after a  xed number of iterations.
These performance numbers are plotted by averaging the result of multiple runs with random initializations, but we do not show the con dence intervals to reduce clutter.
Note that the results of this experiment on all three domains were very similar and hence we show them only for the Automobiles domain.
From the results displayed in Figure 5 we make two observations.
First, it can be easily seen that while the performance of QTGEN does improve with increasing iterations, after very few iterations (>50) the performance differences become indistinguishable.
The only statistically signi cant differences in performance are between Figure 7: PRECISION vs. CORRECTRECALL curves of QTGEN run with different values of   on the Automobiles domain.
the #iters = 10 and #iters = 25 curves and the rest.
This fast convergence of QTGEN can be attributed to the fact that it imposes a lot of structure derived from domain knowledge on the model.
For instance, in our model queries are required to be generated from one of a few attribute con gurations and each attribute is required to generate at least one word.
These restrictions reduce the number of choices that must be evaluated and hence the convergence is achieved in fewer iterations.
Second, we observe that QTGEN converges faster to a precise characterization of attributes that are used more frequently than it does for other less frequently used attributes.
In Figure 5 we have annotated each mark of a converged attribute with the ground truth attribute that it maps to.
As we can see, QTGEN converges to the  nal characterization of the attribute that maps to Model  rst.
This is because a model-name is speci ed in nearly every query in our data.
Next most commonly used attribute is Year, which, in addition, also has a small vocabulary and is hence found in few iterations as well.
Finally, Brand and Parts that participate in fewer queries are found.
In Figure 6 we plot the performance of QTGEN when it learns templates by allowing for different number of attributes.
As we can see the performance for most settings are very similar showing that with any given number of attributes QTGEN is able to learn attributes that closely map to ground truth attributes.
However, the main difference is in the number of ground truth attributes that are covered by the learnt attributes.
This difference is primarily represented in the CORRECTRECALL values that QTGEN is able to achieve with the lower settings of number of attributes.
Often, in unsupervised learning scenarios, one of the hardest parameters for a domain expert to set is the true number of clusters / attributes in the data.
These results show that QTGEN has the ability to  nd appropriate attributes as long as the number of desired attributes is set to a reasonable value.
In these experiments we report on the results of tuning the Dirichlet parameter   that acts as a prior to the attribute-word multinomial distributions and the Gamma distribution parameters g1 and g2 that are used to generate the Poisson distributions for each attribute.
The   parameters controls the extent to which the modeling procedure relies on the observed data as opposed to the priors.
The smaller the value of  , the more the multinomial distribution of words associated with each attribute is tuned to the data.
In Figure 7 we plot the performance of QTGEN when run with different settings of  .
It is clear from the results that for a wide range of parameters values the results stay stable.
Only in the case when the value of   is very high do the results deteriorate since the system now relies on the prior too much and ignores the evidence of the data.
The Gamma distribution parameters g1 and g2 control the Poisson parameters which in turn control the propensity of each attribute to generate words in the query.
We performed experiments with
 Brand Model Year Parts Specs Words most frequently used in queries honda, toyota, chevy, ford, jeep, nissan, dodge, mustang, ranger truck, camaro, corvette, civic, bmw, accord, silverado, yukon, ram 2010, 2007, 2008, 2006, 2004, 2005, engine, 2011, 2009, custom parts, accessories, couple, rims, reviews, problems, tires, manual, seat owners, belt,  oor, door, coupe (a) Query Template Brand Model Year Brand Model Year Parts Brand Model Year Parts Specs Model Year Parts Brand Model Parts (b) Frequency Ad-clicks









 (a) 2 % (b) 5 % Table 2: Structure extracted by QTGEN for the Automobiles domain.
Table (a) shows the attributes found along with the most popular words in them.
The attribute name is the ground truth attribute it matched with in our evaluation.
Table (b) shows the top-5 frequently used query templates found by QTGEN in the Automobiles domain.
The frequency indicates the fraction of the query traf c that is generated from this template.
The Ad-clicks is fraction of all ad clicks on Automobiles domain queries that are on queries generated from this template.
settings that forced attributes to generate very few words per query and others that allowed attributes to generate more.
Our results were remarkably similar across these settings and we do not show them here due to paucity of space.
Our results in this section show that QTGEN is extremely robust to changes in parameters values and performs well for all values in a reasonable range.
Here we present a qualitative evaluation of the attributes and templates found by QTGEN and discuss potential applications.
In Table 2 we have shown the template structure uncovered by QTGEN.
Table 2(a) shows the attributes found along with the most popular words in their vocabularies.
The attribute name in the table is the ground truth attribute that matched it.
As is clear most of the words are grouped together into coherent attributes.
Moreover, these attributes and their vocabularies closely match those in the manually constructed ground truth in Table 1.
In Table 2(b) we have listed the  ve most popular query templates, and the fraction of queries in our dataset covered by them, as detected by QTGEN.
The results make intuitive sense.
The most popular query template found by QTGEN is  Vehicle  queries (that covers 33% of queries in our set) and describes the vehicle of interest using the three most important attributes, Brand, Model, and Year.
These users are most probably researching an automobile for a purchase or lease.
Beyond this template the users are querying for more detailed information on speci c vehicles: to do this they  rst establish the identity of the vehicle using some combination of Brand, Model, and Year, and then express their information need via attributes Part and Spec.
These  Parts  queries all together are about as frequent as the  Vehicle  queries in our dataset.
In the  nal column of Table 2 we list the fraction of all sponsored search advertisement clicks (in the Automobiles domain in our dataset) due to queries that are generated from these templates.
As we can see these query templates demonstrate a large difference in their tendency to attract ad-clicks from users.
For example, the  Parts  queries contribute a larger fraction of ad-clicks while occurring a smaller fraction of times than the  Vehicle  queries.
Hence, knowledge gathered by QTGEN on the template from which the query has been generated should be useful for inferring its advertis-ability [22].
Other potential applications of extracting the attributes and query templates can be in generating special case search expe-(c) 10 % (d) 50 % Figure 8: Clicks vs Impressions curves for models trained using varying amounts of data in the Travel domain.
% of Training data

 Approaches








 -0.83%




 Table 3: The %-improvement in the AUC of predicting the query-advertisability for different approaches.
The two approaches using SMOOTH and QT are explained in this section.
The improvements are measured over the TERM baseline.
The numbers in bold represent improvements that are statistically signi cant at   = 0.01 riences, such as returning on the search results page the price and availability of the automotive part plugged into the query template.
The results from Table 2 show that the presence of certain extracted templates and attributes is positively correlated with the click-through-rate (CTR) on sponsored search advertisements.
Motivated by these observations, in this section we conduct experiments to validate the usefulness of templates extracted by QTGEN for the problem of predicting Query-Advertisability of tail queries [22].
In brief, the search query frequencies are skewed as a power-law and a large fraction of queries are unique (or occur very few times).
This makes predicting the CTR of sponsored search advertisements on these queries very challenging.
One way is to predict the Ad-vertisability for these tail queries in an attempt to help the search engine decide whether to show advertisements for them.
In [22] we proposed an approach for this task that was shown to outperform state-of-the-art baselines.
Due to paucity of space we refer the reader to the original paper [22] for details of the proposed approach.
Here we only describe how we enhance this approach using query templates learnt by QTGEN.
The approach proposed in [22] is based on learning word-speci c scores that are then combined to predict the query-advertisability; we call this approach TERM.
We then learn the query templates for the training set of these tail queries and hence the assignment of each term to an attribute.
We then augment the query with these learnt attributes and learn the attribute-speci c scores.
Our two enhancements of the TERM baseline are as follows:
 the attribute it is assigned to by adding (a fraction of) the attribute rare terms and in cases where very little data is available to reliably estimate term-speci c scores.
The weight to be given to the attribute impressions is tuned on a validation set and varies with number of training data points.
Finally, only the smoothed term-speci c scores are then combined into the query-advertisability score.
as above.
In addition, however, we combine term-speci c scores as well as attribute-speci c scores to learn the query-advertisability score.
The methodology for combining these scores is exactly the same as in [22].
As before, in order to remove the effects of particular topics and obtain robust results, the experiments were performed on 43793,
 Movies domains, respectively, randomly sampled from the Yahoo!
query logs.
Most of these queries occurred once with less than 5% occurring twice.
Each of the approaches orders the queries in terms of their predicted advertisability scores and we report the AUC [12] of the clicks-vs-impressions plots (see Figure 8).
Queries in each domain were evaluated separately since the presence of exclusive templates makes the predicted scores a little dif cult to compare.
The AUC values reported in Table 3 are weighted average across the domains, with tests in each domain averaged over 100 runs with randomly selected training points.
40% of queries in each domain were put into the training set, 10% for tuning the SMOOTH parameter, and rest for testing.
Care was taken to ensure that the queries used in the test data had occurred later in time than the ones used for training and validation.
To simulate low-data situations we sub-sampled the training data to 2%, 5%, 10%, and 50%.
The averaged AUC results are in Table 3 and results from the Travel domain are plotted in Figure 8.
The main points to note are that we get huge percentage improvements from the addition of templates found by QTGEN in situations when very little training data is present.
This is because attribute-speci c scores are estimated over a larger number of terms making them more robust than term-speci c scores, and using them to smooth the latter improves accuracy signi cantly.
Moreover, note that even the presence of certain attributes (SMOOTH + QT) gives the algorithm an additional accuracy boost over just using SMOOTH; this was as expected from the observations in Table 2.
While the improvements are overwhealm-ing for smaller training set sizes, the effect remains until at least 50% of the training data is available.
To conclude, the main goal of this experiment was for us to verify that QTGEN  nds good query template assignments by showing that these assigned attributes help in estimating query-advertisability.
As the plots in Figure 8(a) and 8(b) show, the very strong baseline [22] is essentially random in very low-data situations.
However, the accuracy signi cantly improves with the addition of query templates.
Moreover, even in settings with more data the bene t of using query templates persists.
In this paper we focused on the goal of automatically enriching short keyword search queries by  nding domain-speci c query templates in a completely unsupervised manner.
More speci cally, we gave a generative model based approach that  nds query templates as well as query attributes and their vocabularies, without any human intervention.
We empirically demonstrated the performance of our approach by comparing it against two state-of-the-art approaches on real datasets.
Finally, we showed an application of query templates to computational advertising.
In particular, we sig-ni cantly improved the performance of an approach for predicting query advertisability by supplementing query keywords with their attributes and template information.
