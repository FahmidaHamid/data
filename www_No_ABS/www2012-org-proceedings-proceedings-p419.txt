The amount of information available on the Internet on any topic is ever increasing.
Hence the amount of information relevant to any topic is also on the increase.
Ranking plays a crucial part in selecting relevant information.
Ranking has traditionally been posed as a problem of matching queries to documents.
Subsequent approaches to ranking emphasized document quality/authority in addition to matching.
With the availability of complex data and information  nding requirements, there is a realization that several aspects are important to ranking.
In this paper, we address multi-objective ranking.
Multi-objective ranking is relevant to several applications   product ranking, video ranking, etc.
We consider ranking   hoo!
Labs, Bangalore This work was conducted when the authors were with Ya-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
of comments on news articles.
In a typical scenario, users comment on news articles.
Users also rate the comments with  thumbs up  and  thumbs down .
The goal of ranking is to increase user engagement   roughly measured by the fraction of  thumbs up .
A simple strategy is to rank comments by the number of thumbs up they receive.
This does not work well since new comments do not get su cient ratings.
Hence we resort to other measures like the quality of comments or reputation of commenters.
So ranking of comments has to balance several objectives   actual rating, recency, commenter reputation, comment quality etc.
  to arrive at a single ranking.
Example 1.
We use Figure 1 as a running example in this paper.
This illustrates the ranking of 8 comments to a news article in Yahoo News.
We consider three ranking aspects: current rating, commenter s reputation, and comment quality.
For each of these aspects, each comment gets a score.
These scores are incomplete because of the following: new comments have insu cient ratings, some commenters have no history of commenting, and we do not calculate quality scores for very short comments.
We can construct a graph for each aspect.
The nodes of the graph are comments and there is an edge between any pair of nodes if both the
 nodes have scores.
See Figures 1(a), 1(b), and 1(c).
It is well-known such multi-aspect data can be have con icts in the sense di erent aspects can have con icting preferences or even be  cyclic .
Example 2.
Comments 2 has higher rating compared to comment 7 but the latter has a higher quality compared to the former.
See Figures 1(a) and 1(c).
In Figure 1(h) (which represents the harmonic  ow as discussed in Section 2.2), comment 2, 3, 1, and 4 form a cycle: 4 is better than 2, 3 is better than 1, 1 is better than 4, and 4 is better than 1.
2 Such cyclic preferences are inherent in social choice data.
In other scenarios, the cyclic preferences are artifacts of incomplete data and it is possible to make the data more complete at a cost.
For example, in case of comments, it is possible to obtain more ratings for a comments (or pairs of comments) by ranking it (or them) high.1 So if we identify critical dependencies and get more data (by displaying the comments, for example), we can resolve the critical con icts and arrive at globally consistent ranking faster.
This is related to active learning using explore-exploit or editorial strategies but in the Hodge framework.
ple, In product ranking, popularity information for a product can be determined by showing the product more often.
mation about relative preference between comments 1 and

 preference.
Contributions: The above examples highlight the following requirements in ranking applications.
(1) Ability to fuse the preference data from multiple aspects.
(2) Need to identify and resolve con icts.
Our contributions are,   A technique for aggregating preference data from multiple aspects.
  An active exploration framework for identi cation of critical missing data for fast resolution of ranking con icts.
We exploit Hodge decomposition framework for ranking of preference data and identi cation of critical missing data.
Hodge decomposition is a global framework which requires processing of the information about all comments of an article.
Since new comments arrive steadily, we need a mechanism to incrementally update the ranking rather than running the ranking calculations from the scratch.
In this case, online updates are more e cient.
Hence we propose   A technique for online update of ranking models.
Paper organization: This paper is organized as follows.
Section 1.1 discusses the problem de nition and multi-objective ranking.
Section 2 reviews Hodge decomposition.
Section 3 outlines the proposed technique for ranking data aggregation and Section 4 describes the use of Hodge decomposition on the aggregated matrix.
While Hodge decomposition provides a global ranking, the quality of this ranking may not be high.
Section 5 proposes an active learning framework for increasing the quality of global ranking.
Online updates for Hodge ranking are discussed in Section 6.
Section 7 provides experimental validation of the proposed techniques.
Section 8 places this work in context with respect to related work.
The paper closes with concluding remarks.
The general problem tackled in this work can be de ned as follows.
We are given n objects to be ranked.
(The notation in the following sections is summarized in Table 1.)
In this paper, these are the comments to a single article.
There are k criteria for comparisons (rating, author-reputation, readability, freshness, textual similarity with the article, collaborative factors, etc.).
For each comparison criterion r, we have a preference score matrix Yr with Yr ij denoting the rank-score di erence between comment i and comment j according to the criteria r. Note that the matrix Yr is skew-symmetric (Yr ij =  Yr ji).
As mentioned in Example 1, the entries observed or observable are sparse.
The sparsity is measured by a parameter p which is the fraction of the total entries observed.
And the support set for criteria r is denoted by Sr which is a symmetric binary matrix.
In this paper, we address the following.
Rank Aggregation: We  nd a ranking preference matrix Y(w) which respects each of the criteria Yr (in a pre-decided weighted manner).
See Section 3.
(a) Yrating (b) Yreputation (c) Yquality (d) Y(w) (e) Ranking (f) grad  ow of Y(w) (g) curl  ow of Y(w) (h) harmonic  ow of Y(w) (j) grad  ow of Y(w) after active learning (k) curl  ow of Y(w) after active learning (i) Mod-i ed Ranking Figure 1: Example: Rank aggregation for ranking a set of eight comments.
Vector of 1 s Unit vector with dimension i set to 1 Number of comparison criteria Number of comments to rank Sparsity ratio in preference data Residue matrix Individual ranking-score vector Binary support matrix of Sr Gradient component Curl Flow component Harmonic Flow component e ei k n p
 s Sr


 Y (Yr) Preference scores (for criterion r) Aggregated Yrs using weight vector w Y(w) Preference weight for criterion r (cid:3)Y(cid:3)F,S Frobenius norm of Y on support set S  r Global ranking: Once we  nd the  consensus  preference matrix, we  nd the  optimal  global ranking consistent with it (Section 4).
Active Learning: In active learning we identify the pair of comments, for which if more data is acquired, the global inconsistency maximally reduces (Section 5).
Online Update: Here we compute the ranking incrementally as in online learning framework (Section 6).
In this section, we review the combinatorial Hodge decomposition as described in [16] for arriving at globally consistent rankings from pairwise preference information Y.
The decomposition also provides a quality score ( residue ) for the global ranking as well as a resolution of the residue to insigni cant (curl ) and signi cant (harmonic) components.
A globally consistent ranking is characterized a single score function on comments.
Suppose s   Rn is the vector of ranking scores for the comments such that si > sj implies that comment i is better than comment j.
Then preference matrix induced by s is given by Xij = sj   si This can also be written in matrix form as X = esT   seT The RHS can be viewed as a gradient operator on vectors s, grad : Rn   Rn n and the X is a rank-2, skew-symmetric matrix can be viewed as a gradient  ow.
While the matrix X is cycle free, the data matrix Y can have con icts.
We de ne the optimal global ranking XG of Y as
 (cid:3)Y   XG(cid:3)2 minimize subject to XG = esT   seT , eT s = 0.
(1) (2) (3) The objective function is the Frobenius-norm of the residue after subtracting a gradient  ow from the data matrix Y, weighted with S. The norm is de ned as (cid:3)Y   X(cid:3)2
 Sij (Yij   Xij )2 (4) (cid:2)  i,j This constrained-linear least squares regression problem  nds the globally consistent ranking closest to Y in L2 sense.
Since  s + constant  is also a valid solution, the second constraint is added to get a unique solution without changing the ranking order.
We can solve similar problem with minimizing L1-norm in the objective function or a slightly modi ed nuclear-norm minimizing, matrix completion problem as given in [10].
However, the rankings obtained by either approaches are very similar to each other.
Hence, we continue with L2-norm for the experiments.
The inconsistency in data is characterized by the residue obtained after extracting the     optimal global component.
The residue R G is a divergence-free cyclic  ow.
The consistency quality of observed data is characterized using the cyclicity ratio given by
 Cp =  (cid:3)2 (cid:3)R (cid:3)Y(cid:3)2



 (5) (6) (7) Example 4.
Figure 1(f ) shows the gradient  ow of the graph in Figure 1(d).
The ranking derived from this  ow is
 shown in Figure 1(e).
In this section, we look deeper into the inconsistencies and split them into local and global inconsistencies of the data.
The local inconsistency is a curl  ow which is combination of triangular cyclic  ows.
And the global inconsistency is an orthogonal (in L2 sense)  ow which is locally acyclic but globally cyclic.
This  ow is called the harmonic  ow.
The local and global components are separated by solving another optimization problem, (cid:3)R minimize     XC(cid:3)2
   subject to XC = curl

 where the 3-dimensional tensor   is of size n   n   n and satis es  (i, j, k) =  (j, k, i) =  (k, i, j) =    (i, k, j) =    (j, i, k) =    (k, j, i).
The operator curl applied to a matrix Y gives a n  n  n tensor   such that .
= curl(Y)ijk  (i, j, k) .
= Yij + Yjk + Yki (8) and its dual operator curl* applied to a 3-dimensional tensor   gives a matrix XC such that (cid:2) XC (i, j) =  (i, j, k) k Here, the entry corresponding to an edge (i, j) is the sum of all the triangular  ows which share that particular edge.
If Yij is cost of traversing the edge (i, j), then for XG any two paths between same source and destination nodes would cost exactly the same.
In other words, XG satis es the zero-curl condition, The optimization problem (6) is a linear least squares regression for the matrix XC which is constrained to the space of three-dimensional n n n tensors.
Where, curl   of curl is dual of the curl operator de ned in Expression (8).
The solution XC denotes the curl  ow which comprises of the local-inconsistencies and can be written as sum of triangu-    XC denoted by lar inconsistencies.
The residue  ow R XH is the divergence-free, curl-free  ow called the harmonic  ow.
The matrix XH is a locally consistent but globally inconsistent  ow which comprises of cycles of size   4.
The global inconsistency usually occurs due to some missing Y values.
In case of complete S, the global inconsistency is 0 as all the large cycles are decomposed into gradient  ow and smaller triangular  ows.
Example 5.
Figures 1(g) and 1(h) show the curl and harmonic  ows of the graph in Figure 1(d).
The cycles in the harmonic  ow (in Figure 1(h)) is of length 4.
Recall that cycles of length 4 or more are called globally inconsistent.
The curl component (Figure 1(g)) has cycles of length 3 and these are called locally inconsistent.
Note that our proposed active learning technique (in Section 5) tries to eliminate
 global inconsistencies.
Similar to  cyclicity ratio  (see Expression 5), we de ne norm-ratios for curl and harmonic components as (cid:3)XC(cid:3)2 (cid:3)Y(cid:3)2



 (cid:3)XH(cid:3)2 (cid:3)Y(cid:3)2



 and (9) to quantify local and global inconsistencies.
Therefore, we have decomposed the aggregated preference data Y into
 (10) which is known as the Hodge decomposition of a matrix into (a) curl-free gradient  ow (b) divergence-free curl  ow and (c) curl & divergence-free harmonic  ow.
Hodge decomposition provides a way of decomposing a single preference matrix.
As described earlier in Section 1.1, we have several preference matrices Yr for r = 1, 2,    , k   each one being partial and possibly noisy.
These are aggregated to get an incomplete matrix Y(w).
(We extract the most appropriate individual ranking score s from Y(w).)
In this section, we describe the initial preprocessing step of aggregating the ranking data from various criteria.
Given n objects (comments) to be ranked, there are k criteria for comparisons (rating, recency, author-reputation, readability, etc).
For each comparison criteria r, preference score for every pair (i, j) may not be available or meaningful.
We denote the sparse support for each criteria with a symmetric binary matrix Sr with Sr ij = 1 for every available preference score (i, j).
The sparsity is measured by a parameter p denoting the fraction of total entries observed.
The actual value of the di erence in ranking score between a pair of comments is given by a skew-symmetric matrix Yr.
In the most consistent case Yr is a rank 2 skew-symmetric matrix with e (vector of ones) as one of its generators as explained in the previous section.
We now describe our procedure to aggregate the incomplete preference data from di erent criteria to get a balanced ij = 0 or Sr aggregate preference measure.
Firstly, we observe that an edge (i, j) with Yr ij = 0 in a preference matrix, can either have the corresponding Sr ij = 1.
The distinction between the two being that a 0 of  rst kind denotes lack of information for that edge, while the second kind indicates that the two comments i and j are equal in preference scores according to the criteria r. (The latter is called a  structural zero .)
Hence, we say that an edge (i, j) belongs to the support set Sr if Yr ij value is known with high con dence and it denotes the preference measure between i and j.
Drawing parallel from the rank-aggregation of ranking lists [5], we combine the individual preference matrix Yr into a combination preference matrix given by r=k(cid:2) Y(w) = r=1 wrYr.
(11) The weights w are learned by solving the following optimization problem: r=k(cid:2) minimize w r=1 subject to  r(cid:3)Y(w)   Yr(cid:3)2 F,Sr r=k(cid:2) wrYr, Y(w) = eT w = 1, w   0.
r=1 (12) The objective function is the weighted sum of the Frobe-nius distances of the aggregated matrix from the individual components.
See Equation (4).
And the parameters  r are chosen from prior con dence or importance assigned to respective criteria.
The  r can also be tailored to speci c user preferences using declared interests or learned from user behavior history.
Note that each edge (i, j) may belong to multiple or one or none of the comparison criteria support sets Sr. Let  S (Note that  S does denote the union of support sets Sr.
not depend on w.) Thus,  Sij = 1 implies that for at least one criteria r, the preference information for edge (i, j) is available.
Due to the non-uniformity of the sparsity patters of di erent criteria, the weights have to carefully normalized over the available information for each edge belonging to  S.
The solution of this constrained-convex optimization problem gives us a balanced aggregate preference measure Y(w).
Example 6.
Figure 1(d) shows the preference graph cor-
responding to Y(w).
In this section, we show how the ideas discussed in the previous sections  t together.
to arrive at the aggregated preference matrix Y(w).
optimization problem (3).
problem (6).
residues (see Expressions (5) and (9)) while the quality of the ranking is measured by comparing the sum of the Kendall -  distances from individual rankings.
See Section 7 for details.
Example 7.
Figure 1(f ) shows the preference graph corresponding to this approximating preference matrix.
We refer to this approximating preference relationship as the gradient  ow of the aggregate preference matrix.
The ranking obtained as a result is shown in Figure 1(e).
We now move to the analysis of the residue.
One component captures local inconsistencies in the aggregate preference relationship while the other component captures global inconsistencies.
Figure 1(g) shows the preference graph corresponding to the local inconsistencies, which is referred to as the curl  ow.
These are in the form of short cyclic preferential relationships amongst the comments indicated as triangular cyclic components in the  gure.
The preference graph pertaining to the global component (referred to as the harmonic  ow) is shown in Figure 1(h) and exhibit preferential chains of longer cycles.
In our example, the harmonic component is {1   4   2   3   1} and {6   1   4   2   7   3   6}.
Having obtained the Hodge decomposition into three components, we will now look at each of it closely.
Ideally, with complete data and no noise, we would expect Y(w) to be of the form of gradient  ow and extracting a global ranking-score s from it would be straight forward.
But as most settings with real online data, the noise in behavior of large user base is inevitable.
And with complete noisy data, even though the global inconsistencies are zero, the local inconsistencies are unavoidable.
In this work, we do not focus on methods for reducing the local inconsistencies.
In our setting, the sparsity adds another twist leading to a nonzero harmonic  ow and thereby a globally inconsistent component.
Our next step is to eliminate the global inconsistencies in the harmonic component which are caused by the sparsity of data.
For this, we will select edges for active-learning and use editorial or explore-exploit methods to obtain the preference data for these comment-pairs.
Evaluating all the absent edges would solve our problem.
However, the evaluation process is either very expensive (for editorial) or takes a long time (to increase ratings using explore-exploit methods).
Hence, it is vital to choose the edges intelligently to eliminate the harmonic-component as cheaply and quickly as possible.
And the experimental evidence suggests that the global inconsistencies can be reduced substantially with relatively very few edge evaluations.
Among the various di er-ent methods for choosing edges for evaluation, we compare the following three methods:

 pleted by its addition,
 completed by its addition, Section 7 shows the experimental results of these three strategies for reducing the harmonic component.
Example 8.
As we saw earlier in this example that there is an uncertainty in the preference relationships between comments 1 and 2.
Having a direct preferential relationship between comments 1 and 2 is more reliable than to infer it through second order relationships.
To resolve this, we evaluate this edge directly and introduce the correct preference explicitly which is 1   2.
Figure 1(i) shows the updated ranking order after the introduction of this edge.
Figure 1(j) shows the gradient  ow component after the addition of the new edge and Figure 1(k) shows the updated curl  ow of the
 updated aggregate preference matrix.
In this section, we describe how to update the rank when new information/data arrives/is obtained in the form of a new comment or an update from active learning iteration.
One would predict the existing order to not change substantially by addition of a new comment.
Hence, the new comment can be expected to be inserted in the existing order with few changes in the order of  nearby  comments.
In order to de ne the notion of  nearby  comments, we will  rst look at an alternate formulation for the optimization problem (3).
Given the n   n preference data matrix Y, we consider a directed graph on n nodes with weighted edges from node j to node i if Yij > 0.
Let A denote the adjacency matrix of this graph, A corresponds to the positive part of the sparse support S. Let D = Ae, where e, as before, is the vector of 1s.
Then the Laplacian matrix of the graph is given by G = D   A.
Substituting the gradient  ow constraint in the objective function of the optimization problem (3) and di erentiating with respect to s, we get the following equivalence: s arg min (cid:3)Y grad(s)(cid:3)F,S   {s : (D A)s = Gs =  (Y)} (13) where   is a constant vector which linearly depends on Y, in fact,  i is the total sum of the weights of edges leaving and entering the node i with appropriate signs.
Therefore,  (Y) = Ye (14) It is known that for a connected component with nc nodes, the corresponding block Laplacian has rank nc   1.
Hence, for a graph with c connected components, the Laplacian G is rank de cient with rank n  c. For simplicity, we assume we have a single component, and the procedure described can be extended to multiple components by repeating the same for each block.
As in the previous formulation, the solution to the linear system is not unique under translation.
Thus we replace the  rst row in G with a row of all 1s to get  G and the corresponding entry in   with 0 to get  .
The solution to this system is unique, and for multiple components we can do the same for each by replacing  rst rows of each block with a row of appropriate 1s and 0s.
  The key insight of our online update formulation is that from a data matrix Y can be the optimal ranking score s computed by splitting the matrix as Y = Y1 + Y2, com  puting the individual ranking scores s 2 to get the   e ective score s
 L.H.S.
of the linear system, it is important to note that both the problems have to be solved with same support set S even if certain entries of individual Y1 or Y2 are 0 (more speci cally, structural zeros).
  1 and s   1 + s   = s a new comment is added.
This adds a new node to the graph.
The edges added will be based on the scores like author-reputation, readability, etc.
which can be calculated instantly for the new comment.
Let the new node be n + 1 and suppose it is connected to d of the old nodes denoted by the neighborhood set Nd.
The binary vector indicating this connectivity is denoted by g   Rn and corresponding weights ynew.
Then, the new system of equations is given by: (cid:4) (cid:3) (cid:3) (cid:4)  G + Dg  g  gT d snew =  old +  inc  n+1 (15) Here,  g is g with  rst entry set to 1, Dg is diagonal matrix of g and  inc is the increment to   after appending of ynew to the data Y.
Let sg = s   g denote the entry-wise (Hadamard/Schur) product of s and g, and  sg = be the mean of these d nonzero values to give  y = sg    sgg.
Now we split the new problem into two problems, both with the new support,  rst with the Y entries for all the old edges and si    sg for the new edges between i   Nd and the new node.
The second part with 0 for all the old edges and the remaining di erence of new preference values for the new edges.
Thus we write the new preference matrix as: gT s d (cid:4) (cid:8) + (cid:3) (cid:5) Y  y (cid:6)(cid:7)  yT
 Yaug
 (cid:6)(cid:7)  yT  y
 Yinc (cid:4) (cid:8) (16) (cid:3) (cid:4)
  ynew ynew
 = (cid:3) (cid:5) (cid:4) where,  y = ynew    y.
The  rst system is very similar to the older problem with only few new edges, and the second system is sparse with a large number of 0s and a very few nonzero values.
The new system of equations for the  rst problem is given by: (cid:3) (cid:3) (cid:4)  G + Dg  g  gT d saug =  old + sg    sgg
 Here,  g is g with the  rst entry set to 1 to maintain the uniqueness condition of  rst row.
The solution for this problem has a closed form solution which can be written as: (cid:3) (cid:4) (17) (18)   aug = s s    e n  where, the scalar   =  sg n + 1 .
Note that the form of s   aug guarantees the  rst condition of zero-sum to hold.
The second problem is more involved and the ease of solving it depends on the structure of the existing graph and the new edges added.
The new system is given by:  inc   sg +  sgg (cid:3) (cid:4) (cid:4) (cid:3) sinc =  n+1 (19)  G + Dg  g  gT d The L.H.S.
is same as  rst problem and the R.H.S.
is sparse with all 0s except the neighbors of the new node and the last entry.
As expected, the in uence of the few nonzero edges will diminish as we go further away from the new node.
The  nal ranking score is given by sum of the two solutions   new = s   aug + s   inc.
s (20) To solve the second problem in online fashion, we will main 1.
For this, it is vital to note tain and update the inverse  G that the new matrix can be written as a rank d + 1 update of an invertible matrix.
(ei   en+1)(ei   en+1)T  Gnew = (cid:3) (cid:4) + (cid:2) (cid:4) i Nd (cid:3) +
  1


 (21) where, U and V are both n (d+1) matrices.
The  rst term  G+, which is  G appended with a column and row of 0s and 1 on diagonal, is easily invertible given the inverse matrix  1 from previous iteration.
The second term is the sum of
 d rank-1 updates for each edge added to the graph and the third term denotes the adjustment required to  Gnew(1, n) to have  rst row of 1s for the uniqueness condition and to  1  Gnew(n, n) to getd .
And the new matrix inverse  G new is given by the Sherman-Morrison-Woodbury identity [12] for inverse of low-rank updates as:  1 new =  G  1 +

  1
 Id + VT  G  1

  1 + .
(22) (cid:10) 1 (cid:9) Thus to solve the second problem, we compute the updated inverse which can be used for further calculations.
This computation involves inverting a small (d + 1)   (d + 1) matrix and involves O(n2d) operations compared to O(n3) in any other direct method using LU or QR decompositions.
However, a similar incremental update of LU factors can be used, but LU is highly dependent on ordering of nodes, in the worst case of the new comment being connected to node
 that this updated inverse could be used directly to solve the original new system.
However, the bene t of splitting the problem lies in the  rst split problem requiring O(n)   time for computing the closed form solution s aug, and the second split problem exploiting the sparseness in R.H.S.
to   compute s inc with a factor of O(n/d) faster.
We can further speed up the calculations at the expense of accuracy of the ranking scores ignoring small entries while solving the second problem.
Since the ranking order is robust to small changes   in the ranking score s new, we would expect small error in   s new to not disturb the  nal ordering.
And the complete problem can be resolved at regular intervals to avoid error buildup.
In the case of an edge update from active-learning described in previous section, an edge (i, j) gets added to the graph.
The Laplacian matrix gets updated by a rank-1 matrix as  Gnew =  G + (ei   ej)(ei   ej)T .
And the Sherman-Morrison-Woodbury identity reduces the inverse update to  1 new =  G
 (cid:11)  1(ei   ej)(ei   ej)T  G  1

 (cid:12) .
This update requires no matrix-inversion and the 3 matrix-vector multiplication can be done carefully using sparsity of (ei   ej), reducing the complete updating procedure to O(n2)  ops.
Thus we have described a method to update the hodge-rank for incremental addition of nodes or edges to the data.
(23) (24) In this section, we provide experimental validation of the proposed methods.
Dataset: We use 200 articles along with 50 comments for each article from Yahoo!
News.
The comments have the following associated information: time stamp, commenter, and rating (number of thumbs up and thumbs down) along with comment text.
The articles appeared during Sep 2011.
Ranking aspects: We use three ranking criteria namely, rating, reputation of author, and quality of the comment.
The reputation information for the commenters is the average rating obtained during the three month period June
 commenters in the dataset.
For comment quality, we use the framework of [4].
The rating information is given by the fraction of thumbs up to the total number of rating for a comment.
The above information is available in the form of absolute scores.
We calculate the preference information as (scorej   scorei).
Note that the ratings of a old comment and new comment are not commensurate since the old comment could have accumulated a large number of ratings and the new comment may not have had that chance.
So we calculate the preference information only between  commensurate  comments.
Metrics: We use Kendall   rank correlation coe cient to characterize the distance between two rankings.
Kendall   is de ned as (number of concordant pairs)   (number of discordant pairs) n(n 1)   values are in the range [ 1, 1] with 1 indicating reverse order and +1 indicating same order.
Hence higher values indicate better performance.
If R1, R2,  , Rk are the individual rankings and RA is the aggregated rank, then we use the Q-metric de ned below.
k(cid:2)

 k   (RA, Ri) i=1 to characterize the quality of RA.
A higher values of Q(RA) indicate better aggregation.
To show the sizes of various components (like residue, harmonic and curl), we use the three norm-ratio measures de- ned in Expressions (5) and (9).
Note the lack of readily available golden data in this task.
It is close to impossible to editorially rank a set of comments.
The best approach is to do live tests.
In this paper, we use the above measures as proxies for live tests.
Baselines: For rank aggregation, we use simple mean and weighted mean.
In simple mean, the scores are averaged  rst and the ranking derived from the scores is used.
In weighted mean, the weights obtained from the optimization problem (12) are used for averaging and the ranking induced by the weighted scores is used.
We  rst use optimization problem (12) described in Section 3 to calculate the aggregated preference matrix and the globally consistent Hodge rank is computed using the optimization problem (3).
(We set all  r = 1 in equation (12)).
Further, two baseline rankings are computed by simple mean and weighted mean of the individual partial ranking lists of Table 2: Q-metric from individual rankings Simple Mean Weighted Mean (% Gain) (% Gain) A.m



 Sparsity Hodge Rank














 p












































 Simple Mean Weighted Mean









 .
= p
 .
= p
 .
= p i s g n k n a r l i a u d v d n i i r o f
 (a) (b) Simple Mean Weighted Mean Hodge Rank







 n i i n a
 l a n o i t c a r


 Sparsity Ratio  p 





 Sparsity Ratio  p 



















 Simple Mean




















 Weighted Mean


 (a) Kendall  from individual rankings Figure 2: (left) and Fractional Rise in Total Kendall  (right).
(b) Histogram for Fractional Rise for 200 articles.
each criteria.
We then compare the quality of Hodge rank to the baseline ranks using the total Kendall -  of these ranks from the individual partial rankings lists.
Table 2 illustrates the typical quality, Q( ), values for Hodge rank, simple mean rank and weighted mean rank.
This is shown for the 200 article dataset ( A.avg ) as well as for four arbitrarily chosen articles.
In the latter, we average over 25 instances of randomly chosen 50 comments for each article.
The table also shows the gain of the baselines compared to Hodge rank.
It can be seen that Hodge rank (c) Norm Ratio Curl Flow .
= p





 (a) Norm Ratio Harmonic Flow


























 Random Max.
  Max.
Wtd.
 














 (d) Histogram for 200 articles








































 .
= p .
= p



































 Number of Iterations Figure 3: Active Learning.
(a) Norm-Ratio Harmonic Component.
(b) Cyclicity Ratio.
(c) Norm-Ratio Curl Component.
(d) Histogram of Number of Iterations.
is up to 30% better than the baselines.
Also, the weighted average on par or slightly better than simple average but not uniformly so   as can be seen from Table 2 and Figure 2.
Figure 2 shows the Quality scores averaged over 200 articles in di erent formats.
In Figure 2(a), we show the typical dependence of the Kendall -  values of the three rankings for di erent sparsity p in data, ranging from p = 0.2 to p = 0.6.
Figure 2(b) shows the typical fractional gain in these values for Hodge rank.
As expected, as the sparsity in data reduces (p increases) the performance of simple and weighted mean ranks catch up with the Hodge Rank as seen in Figure 2(a).
The histogram for the fraction gain in Kendall -  from individual rankings for Hodge rank is given in Figure 2(b) for three di erent sparsity values.
Note that for majority of the articles, Hodge rank shows increasing improvement over the mean ranks with sparsity (the histogram shifts towards 0 as p increases).
However, for about 3   5% of the articles, Hodge Rank performs worse than the mean rankings.
In second part of the experiments, we solve the optimization problem (6) to separate the global and local inconsistencies.
The harmonic component is then used to identify the edges for active learning to reduce the global inconsistency.
Three strategies as described in Section 5 are used in parallel and at each iteration the unknown data value for the respective edge is added to the individual Yr matrix.
Figure 3(a)-(c) show the norm-ratio of the harmonic component, the cyclicity ratio and the norm-ratio of curl compo-nent2 as a function of number of active learning iterations for three di erent values of p.
(For p = 4, the norm ratio of the harmonic component becomes 0 in two iterations.
Hence we do not show the Cyclicity ratio and the norm ratio of curl  ow beyond two iterations.)
As can be seen, the weighted maximum heuristic performs the best in reducing the harmonic component.
At the same time, the cyclicity ratio reduces con rming that additional data improves the rating.
Further decrease in norm-ratio of curl component indicates that additional data does not just transfer inconsistency from global component to local component.
Figure 3(d) shows the histograms of number of active learning iterations required to eliminate the harmonic component for
 quired reduces with sparsity (increase in p).
Finally, we investigate the computation times for online updates for addition of edges from active-learning iterations or addition of nodes as a result of receiving new comments.
Computation times for computing the Hodge rank for 10 active-learning iterations is shown in left plot of Figure 4.
The right plot of Figure 4 illustrates the computation time required for incrementally updating the Hodge rank as number of comments increase from 50 to 60, one at a time.
Comments and ratings form a key component of the Social web and are one of the primary contributors to its success.
There is a signi cant volume of research e ort in the recent years focused on comments and ratings.
User studies in [18, 11] illustrate the importance of comments in blogging communities.
In [24], Witschge explores public online debates in the form of comments on strong sociopolitical issues and in [21] Schuth et al. study the discussion structure in comments associated with online news articles.
In [20], Edge Addition x 10 3 Node Addition ) c e s ( e m
 i









 Online Update Full Fresh Update




 New Comment Addition







 ) c e s ( e m
 i






 Edge Addition from Active Learning Figure 4: Computation Times for Online Updates the authors study comments associated with blog posts and [14] utilizes the comments associated with blog posts for summarization of the blog article.
Ratings and Quality: A number of recent e orts are in the direction of understanding the quality of user-contributed comments.
In [19], Mishne et.
al. utilize a language modeling approach to detect link spam in comments made on blogs.
The quality measure for comments used in our experiments is based on the work of Chen et al. [4].
In [4], Chen et al. presents a supervised approach to determine reputation of users in the context of comments and ratings To this end they editorially judge the quality of comments according to a pre-de ned set of guidelines.
Utilizing this as a training set, they learn regression models to characterize the quality of a comment.
The predicted reputation of a user as an author of comments is obtained as mean quality score of the comments posted by them.
The authors also introduce a support-based reputation computation.
This is de ned as the average rating a user receives from all the users for comments made by him in a certain category.
However, this is di cult to compute empirically as in practice a user receives ratings for his comments only from a small subset of users.
To mitigate this issue, the authors propose a latent factor model to predict the ratings of a comment.
This model is personalized in the sense that it predicts the rating that a particular user will provide to a comment made by another user in a certain context.
They show that this tensor model can be marginalized easily to obtain the support based reputation score for the users in a certain context.
With the increase in user participation on commenting, ranking/recommendation of comments becomes an important problem to facilitate e cient consumption of existing comments on a web site.
[22] presents a detailed study of user comments in YouTube videos.
The authors examine the comments and their ratings in YouTube videos to investigate the relationships between the language and sentiment expressed and the ratings of the comments.
Treating highly vs lowly rated comments as two classes, they also learn a discriminative model to predict whether a given comment is likely to be highly or lowly rated.
[13] proposes a rating prediction approach to user comments on the social news ag-gregator site, Digg.
In Digg, users can rate (provide thumbs up and thumbs down votes) to each comment (associated with a submitted story).
The authors consider the di er-ence of the total thumbs up and total thumbs down as the aggregate rating of a comment.
They adopt the learning to rank approach to predict the ratings of comments.
To this end they compute various features pertaining to each comment, based on the commenting activity of the commenter (user reputation) and content of the comment.
The various user and content based features adopted in this work is similar to what we use in our work.
They utilize historical data of comment ratings to train support vector regression models based on these features.
As the rating received by a comment is biased by the posting time of the comment (older comments get more visibility and hence more ratings), the paper also presents experimental results where they explicitly account for this bias while training the ranking model.
In [1] Agarwal et al. presents a framework for recommendation of user comments so as to surface comments (in an online news service) to users (in a personalized fashion) which they are likely to rate positively.
They adopt a generalized linear model framework where given a user and a comment, the mean rating for it consists of several factors like rater bias, comment popularity, author reputation.
Additionally they also include factors representing author-rater a nity and rater-comment a nity.
They estimate the latent factors using MLE in a Bayesian setting where they learn appropriate priors for the factors by pooling data across similar users and comments.
Multi-objective Ranking: In the recent years, the challenge of optimizing for multiple criterion in learning to rank paradigm has attracted a lot of attention in the research community [23, 6].
Dai et al. in [6] adapts the conventional learning to rank paradigm to simultaneously optimize for freshness and relevance in web search.
To this end they extend the Divide and Conquer ranking framework [3].
The training phase involves clustering queries into several clusters based on the retrieval features computed from the top ranked results (according to a reference ranking model) for these queries.
The clustering employed is a soft-clustering where each query is associated with all the clusters with di erent association weights.
They learn multiple ranking models for each of these clusters where they incorporate the notion of freshness into the traditional letor approach by generating hybrid labels based on relevance and freshness judgments (similar to Dong et al. [8]).
While serving results for a query, they determine the a nity (association weights) of a query to each of the clusters.
Then the results are scored according to each of the pre-trained models and the scores are combined using these association weights to obtain the  nal ranking of the results.
Rank aggregation: Rank aggregation is a widely studied problem in the domain of social choice and voting theory [2].
In the context of the web, one of the early works on rank aggregation is seen in [9].
In [9] they illustrate the use of rank aggregation for aggregating results from multiple search engines.
They rely on the Condorcet criterion for mitigating problems of spam, where their suggested approach satis es the extended Condorcet criterion.
While, Kemeny optimal aggregation satis es this criterion,  nding a kemeny optimal is known to be NP hard.
The authors introduce a concept of locally Kemeny optimal which states that an aggregated permutation is locally Kemeny optimal if one can not decrease the Kendall   measure (of the aggregated permutation with the component rank orders) by swapping adjacent elements in the aggregated order.
In essence, their approach is to start with any rank aggregation technique and do a local Kemenization.
The locally Kemeny optimal aggregate can be obtained by computing the Hamiltonian path in the majority graph.
In the context of rank aggregation, there have been some vised [17] rank aggregation.
In [17], Liu et al. propose a framework which utilizes the ordinal information and a labeled training data to learn a rank aggregation model in a supervised setting.
To this end they adapt the Markov Chain based rank aggregation approach (which is non-convex) into that of semi-de nite programming.
They show encouraging results on the OSHUMED dataset and also in a rank aggregation task of web search results from six di erent commercial search engines.
In [10], Gleich et al. adopt a matrix completion approach towards rank aggregation.
They utilize the scores from the component rankers to generate pairwise preference matrices of the items to be ranked.
They suggest various aggregation schemes (based on arithmetic mean, geometric mean, etc.)
to generate a composite pairwise matrix from the component rankings.
The  nal ranking is obtained as a rank-2 skew symmetric completion and approximation of the composite matrix.To this end they utilize a singular value projection based algorithm.
Results on a synthetic and the Net ix dataset illustrate the e cacy of their method.
We have adopted Combinatorial Hodge Theory based approach towards rank aggregation.
The application of Hodge Theory for ranking was  rst proposed in [15, 16].
In our rank aggregation framework, the comments are associated with cardinal scores based on quality, reputation and ratings.
Unlike traditional rank aggregation methods which consider ordinal orderings based on these scores, the Hodge decomposition approach allows us to leverage these cardinal scores in a more direct fashion to establish the pairwise preference graph (see Section 3).
Further, as discussed in Section 3 the pairwise preference graphs obtained using these individual aspects (rating, quality, and author reputation) are incomplete.
The Hodge decomposition framework in essence generalizes the well-known Borda count [7] scheme to scenarios where the ranking data is incomplete.
The framework, besides providing an elegant approach for rank aggregation, also provides some notion of a con dence measure on the quality of the global ranking that is obtained.
This is expressed in terms of the curl  ow and the harmonic  ow components of the decomposition (see Section 2.2).
In this work, we adopt an active learning based approach to minimize the harmonic  ow component (global inconsistencies) and obtain a more globally consistent ranking.
Multi-objective rank aggregation is becoming essential in several applications.
In this paper, we use the framework of Hodge decomposition for rank aggregation problems.
We propose a novel technique for aggregating individual preference functions and experimentally show that the proposed approach is superior to commonly used baselines.
We then formulate the problem of reducing global inconsistencies and propose techniques for identifying local observations which can maximally reduce global inconsistencies.
We  nally show how we can perform the decompositions in an online fashion.
One of our future directions is to formulate the theory for choosing the edges optimally.
