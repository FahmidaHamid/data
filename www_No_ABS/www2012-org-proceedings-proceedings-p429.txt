Much of today s media is consumed online, and media Web sites   whether online outlets of media companies with traditional forms of distribution (newspapers, TV stations, etc.)
or portals of Web media companies   vie for the attention of consumers.
New engaging user experiences enable sites to increase their user base as well as understand their users better, making them more attractive to advertisers.
A common feature of many present-day media sites is their attempt to enrich user engagement by soliciting and sharing user generated content (UGC).
One popular form of UGC-centric engagement stems from comments on content that media sites solicit from users.
Users are empowered to write comments and are able to rate others  comments, reply to comments (thereby generating discussion threads), etc.
each user a personalized ranked list of stories (that the user has yet to consume) that may engage the user to the point of commenting on them.
An illustration of the planned experience is presented in Figure 1.
This is an extreme form of content recommendation, as we go beyond engaging the user to click on or to consume information - we hope the user will be passionate enough about the presented stories to actually generate content.
Furthermore, there is a time-critical dimension to our task   stories should be recommended to the users soon after they are published, as freshness is a major ingredient in a story s attractiveness.
Technically, we tap each user s propensity to comment on stories of certain topics, as well as collaborative  ltering (CF) signals that rely on co-commenting patterns of users (tendency of di erent users to comment on the same stories).
Note that when trying to recommend fresh stories, a trade-o  emerges.
Fresh stories have few comments, leading to lower contribution of the co-commenting signal.
This is a form of the item cold-start problem in recommendation systems.
On the other hand, waiting for more commenters to comment on the story helps in identifying additional users who would  nd the story engaging, but in the meantime the story will have  aged .
Another signal we tap in our experiments is social data.
One of our datasets sports a social network between its users, prompting us to investigate whether and to what extent do stories, on which a user s friends have commented, engage the user to the point of commenting.
We apply a memory based recommendation algorithm to the signals above, as well as several  avors of a latent factor model (LFM).
The LFM  avors deviate from each other by the loss function applied while learning them.
Our experiments, on two di erent datasets with thousands of stories and users, achieve encouraging results.
The contributions of this paper are along two axes.
First, with respect to the application domain, we measure the relative strengths of three signals (textual tags, co-commenters, and friends) as predictors of a person s tendency to comment on stories.
Our  ndings show that:   Textual tags provide a signal that is already surpassed by roughly 5 co-commenters in the tested datasets.
Furthermore, combining tags and co-commenter information improves on what each signal can achieve alone.
  We study the tradeo  between recommendation accuracy and the number of comments available per story.
  The contribution of social signals to commenting activity prediction is lower than either of the other signal types.
Second, with respect to the art of recommendation systems, we  nd that:   One can improve the accuracy of latent factor models (LFMs) by tailoring the loss function to the speci c application at hand rather than using the standard squared error loss function.
  In cases where uniformly unattractive items can be easily identi ed, i.e. stories on which almost no users will comment, the AUC metric may fail to measure the personalization capabilities of competing recommendation schemes.
We therefore introduce a novel variant metric, called strati ed AUC, that measures the accuracy of ranking items in separate bins that correspond to the items  global popularity.
This isolates the uniformly unattractive items and prevents them obscuring the quality of the evaluated recommendation scheme.
The rest of this paper is organized as follows.
Section 2 surveys related work.
Section 3 presents the LFM and memory based recommendation algorithms we evaluated.
Section 4 describes the two datasets in our experiments, and Section 5 reports our results.
We conclude in Section 6.
Since the appearance of Web 2.0 and, with it, user generated comments, various studies explored di erent aspects of user comments, showing that comments can provide many insights about Web documents.
Mishne and Glance [16] presented a study of weblog comments and their relation to blog posts, for example, exploring the relation between the weblog popularity and users  commenting patterns.
Siers-dorfer at al. [26] studied user comments on YouTube videos and gave an analysis of the dependencies between comments, views, comment ratings and topic categories along with a predictor of comments  ratings.
Since user comments often express apparent feelings of users, the study of user comments has been also associated with sentiment analysis (see survey by Pang and Lee [17]).
For example, a study by Pavlou and Dimoka [19] that demonstrated the role of user comments in building sellers  benevolence and credibility in online marketplaces.
Other studies utilized user comments for the completion of various tasks: user comments on blog posts were utilized for automatic blog summarization [9] and for clustering of blog documents [11].
Yee et al. [31] demonstrated the potential of Youtube user comments to annotate videos by incorporating comments into the search index, which yielded up to a 15% increase in search accuracy.
Studying the content sharing website Digg, Rangwala and Jamali [21] de ned a co-participation network between users based on Digg s comment information and used this data to predict the popularity of online content linked at Digg.
A recent study by Chen et al. [3] focused on user reputation in a comment rating environment and showed that the quality of a comment judged editorially is almost uncorrelated with the ratings that it receives, but can be predicted using standard text features.
Several past studies focused on user comments on news articles.
Li et al.
[12] employed user comments for dynamically suggesting related stories to a given news article, showing that suggestions improve when the content of user comment threads is considered.
A recent study by Park et al.
[18] utilized user comments in order to identify the political orientation of news articles by uncovering user sentiments expressed in their comments.
Other studies [28, 29] explored the distributions of comments on news from various news agents and tried to predict the comments  volume for news stories.
Schuth et al. [25] examined relations between news comments by extracting discussion structures ( replies-to  relations) from  at comment threads.
Hsu et al. [8] presented a machine learning approach for ranking comments based on the expressed preferences of a social web community, which can be used to promote high-quality comments and  lter out low-quality ones.
A recent study by Agar-[2] added personalization to comment ranking and used a latent factor model to rank comments according to personalized preferences of speci c users.
The work presented in this paper adds to the line of user generated comments research by describing a model that predicts, for a given user, the most probable news stories for commenting and thus, provides personalized recommendations of stories to comment on.
Our work lies within the  eld of recommender systems [23].
Broadly speaking, recommender systems are based on two di erent strategies.
The content  ltering approach creates a pro le for each user or product to characterize its nature.
As an example, a news article pro le may include its publisher, author, subject tokens, categories, tags, textual attributes, etc.
Similarly systems may employ user pro les including demographic information or answers to a suitable questionnaire.
The resulting pro les allow programs to associate users with matching products.
Much more details can be found in Ricci et al. [23].
An alternative to content  ltering relies only on past user behavior e.g., previous transactions or product ratings  without requiring the creation of explicit pro les.
This approach is known as Collaborative Filtering (CF), a term coined by the developers of the  rst recommender system - Tapestry [7].
CF analyzes relationships between users and interdependencies among products, in order to identify new user-item associations.
A major appeal of CF is that it is domain free, yet it can address aspects of the data that are often elusive and di cult to pro le using content  ltering.
A common division of CF is between a class of  memory-based  methods, and a class of  model-based  methods [1].
Memory-based methods are essentially heuristics that generate recommendations by directly referring to the raw user feedback data, or to values immediately derived from it.
A known example is the item-item recommendation method [14, 24].
Model-based methods initially construct a compact model for representing the inherent structure essential for relating user with items.
Consequently, recommendations are made by referring to the created model.
The models often follow a latent factor representation (or, LFM), such as the widely used matrix factorization method [10].
LFM tries to explain the user behavior by characterizing both items and users on factors automatically inferred from the patterns of user-item interactions.
While generally being more accurate than content based techniques, CF su ers from what is called the  cold start  problem, due to the inability to address products or users new to the system.
This often leads to favoring hybrid methods [1] that combine content and collaborative  ltering together.
The methods proposed in this paper are indeed such a hybrid.
Most works in the recommenders  eld deal with entertainment and media product.
More relevant to us are the works dealing with recommending textual items.
Such textual items, like news articles, are characterized by high volatility and churn rate, while o ering an opportunity of content analysis of their text.
One such work [4] describes Google news recommendation engine, where the authors use a blend of three separate CF methods in a highly scalable fashion.
Moving closer to the UGC domain, several recent works [5, 13] addressed question recommendation in community ques tion answering sites.
Table 1: Summary of Notations T (s) The set of textual tags associated with story s C(s) The set of users that commented on story s

 u  
 u Su s u c t rus  x The set of all users in the data The set of stories a user u commented on The set of stories a user u did not commented on A set of positive-negative story pairs for user u Index letter for a story Index letter for a user receiving recommendations Index letter for a user as a commenter Index letter for a textual tag An a nity score for user u and story s Vector representation of x in the K-dimensional latent space (where x can be a user, story, tag)

 We begin by presenting some notations used throughout this section.
For a given story s, we denote the set of textual tags associated with it by T (s).
Similarly, we denote the set of users that commented on s by C(s).
Let U denote the set of all users in the data.
For each user u   U , let S+ u be the set of stories that u commented on, also referred to as the   positive examples of u.
Similarly, let S u be the set of stories that u did not comment on, also called the negative examples of u.
As part of our LFM learning procedure (section 3.2) we train our models on the same number of positive and negative examples for each user u.
To this end, we pair each positive example s+   S+ u with a sampled negative example
   s u .
Generally, negative examples are drawn uniformly   from S u , unless otherwise stated.
We denote by Su the set of positive-negative story pairs (s+, s ) over which the LFMs are trained with respect to a user u.
As for the LFM parameters, we use a bar notation to distinguish vectors in the latent space from the scalars they represent.
For example,  u   R K is the vector representation of user u and  s   R K is the vector representation of story s. Table 1 summarizes these notations for easy referencing.
  We treat the problem of commenting recommendation as a ranking problem.
Given a user u, the goal is to rank the test stories so that if story s1 is ranked higher than s2 then u is more likely to comment on s1 than on s2.
The ranking of test stories is accomplished by using a prediction model that given a user u and a story s, generates a score ru,s re ecting how likely is u to comment on s. We addressed this problem with two approaches, a memory based approach and a more complex latent factor model in several variations, some of which are novel with respect to recommendation system usage.
The rest of this section describes the methods that were implemented in the two approaches.
The memory based approach directly leverages the notion of co-occurrence.
This is done by calculating a co-occurrence similarity measure for a given user and all story features (textual tags and given commenters) and averaging this measure over all features of a given story.
(cid:2) ru,s = +
 |{t   T (s)}| t T (s) |{c   C(s), c (cid:5)= u}|
 nu,t  (cid:2) nunt c C(s),c(cid:3)=u nu,c  nunc (1) (cid:3) K(cid:2) (cid:4) The variable nu,t is the number of stories with tag t that user u had commented on, nu is the total number of stories that u had commented on and nt is the total number of stories with tag t. Similarly, nu,c is the number of stories which both users u and c have commented on.
Implementation-wise, we build an inverted index [15] over training period stories, along with their tags and commenting users.
This results in posting lists per tag and commenter, whose posting elements contain (and are sorted by) story ID.
The index allows us to quickly compute co-occurrence (intersection) counts on the  y, by running two-term conjunctive queries against it.
K and  s   R The basic idea behind latent factor models is to represent each user u and story s in a K-dimensional latent space, so that, if  u   R K are the representations of u and s in the latent space, then the score ru,s is calculated by the inner product of the representation vectors, plus an additive user bias ru,s = (cid:6) u,  s(cid:7) + bu =  uk  sk (2) where bu   R is a user bias parameter.
Note that while the user biases have no e ect on the ranking of stories for a  xed user, we still  nd them improving results as their inclusion leads to a more accurate training of the latent factors.
+ bu k=1 In many recommender system settings, items are directly modeled based on user interactions with them (during the training period).
Our setting is di erent because of the ephemeral nature of news stories.
Hence, we assume no user interactions are available for test stories (see also the related design of the train-test split in Section 4).
Therefore latent space representation for stories ( s) would not be a distinct parameter of the model.
Rather, we assign latent space parameters to the story features (textual tags and commenters) and construct the stories  representations through their features.
This is feasible since the same features do appear with other stories during training.
Formally, each textual tag t and commenting user c are represented by factor vectors,  t   R K , respectively.
The representation of a story s is given by averaging the representations of its features K and  c   R (cid:2)  s =
 |{t   T (s)}|  t +
 |{c   C(s)}|  c (3) c C(s) Note that we chose to use two di erent latent representations for each user:  u as a user receiving a recommendation (see equation (2)) and  c as a commenter (being a feature of a story).
This is done in order to allow the model to address the di erent roles a user assumes.
In our experiments, we used a value of K = 10 for the latent space dimensionality.
Other values we experimented with (20,50), did not produce any observable gain in performance.
(cid:2) t T (s) (cid:2) u U   The parameters of the latent factor models were learned by employing a stochastic gradient descent algorithm with early stopping (see, e.g., [27]).
The algorithm attempts to minimize the cost function (cid:2) loss(u, s + , s   ) (4) cost = {s+,s } Su where loss(u, s+, s the rest of this subsection.
) is one of the loss function described in The training procedure is an iterative process and a typical number of iterations is 20.
At each iteration, we iterate over all users u   U .
For each user, we train the model over all positive-negative story pairs in Su.
Recall that the story pairs in Su include all the positive examples in S+ u and for each s+   S+
   u .
Negative examples are sampled uniformly except for the case of the last loss function (see  Rank Loss  below).
u a sampled negative example s Following is a high level pseudocode description of the training procedure: while validation performance is improving do for all u   U do for all {s+, s  }   Su do calculate the gradient  (loss(u, s+, s change model parameters by  (loss(u, s+, s ))     )) end for end for end while We used a learning rate of   = 0.1.
The model s latent factor parameters are regularized by penalizing their squared magnitude with a regularization coe cient   = 0.1 (also known as Tikhonov regularization).
Following is a description of the various loss functions used in our experiments.
Squared Error (SE) The most common loss function in recommendation systems latent factor models is the squared error (e.g.
[10]).
Since recommendation systems commonly try to predict a user-item rating (e.g., movie ratings), the SE loss function aims at minimizing the distance of predictions from target ratings; in our case it aims at getting the score of positive examples close to 1 and of negative examples close to 0 se loss(u, s   + , s ) = (1   ru,s+ )
 + (0   ru,s  )
 (5) Classi cation Error (CE) While observing the SE loss function, one may argue that it is unjusti ed to penalize positive examples with scores higher than 1.
The same holds for negative examples with scores lower than 0.
The  Classi cation Error  deals with that by using a hinge loss that better meshes with misclas-si cation.
Thus, the loss term is ce loss(u, s , s +   ) =|1   ru,s+|+ + |ru,s |+ (6) where |x|+ = x if x   0 and 0 otherwise.
Bayesian Personalized Ranking (BPR) Rendle et al. [22] suggested a loss function that is designed for ranking problems.
The method treats pairs consisting of a positive and a negative example.
The goal is to score the The loss term is bpr loss(u, s   ) = log ( (ru,s+   ru,s  )) + , s (7) where  (x) is the sigmoid function
 1+e x .
Rank Loss (RL) Bengio et al.
[30] suggested a loss function that utilizes a di erent sampling method for negative examples, which we adopt here for recommendation purposes.
For each positive example s+   S+ u , negative examples are drawn with   replacement until a negative example s u that violates the desired ranking is encountered.
A negative example
   s u is considered violating, with respect to a positive example s+   S+ u , if its score is within a margin of 1 from the positive example s score or higher, that is, if it satis es ru,s+ (cid:2) 1 + ru,s  .
The model is then updated based on such violating pair of examples.
No update is performed if no violating example could be found after |S u | draws.
The number   of non-violating negative examples drawn while sampling is used to estimate the total number of negative examples that violate the desired ranking for a given positive example.
We denote this as rank in the following formulation rank = total number of negative examples number of negative examples drawn We used a binary rankloss function after other suggested variations [30] we experimented with did not improve performance.
rankloss(rank) = 1 if rank > 1, 0 otherwise dataset type Newsvine Yahoo!
Original Train Test Original Train Test stories





 users





 comments





 Table 2: Data statistics for the Newsvine and Yahoo!
datasets before and after splitting and preprocess-ing.
s e i r o t s f o #







 # of comments Figure 2: The distribution of stories by their number of comments in the Newsvine dataset (after prepro-cessing).
Then the loss function for user u and positive-negative  }   Su becomes story pair {s+, s ) = rankloss(rank) (1 ru,s+ +ru,s  ) (8)   , s rl loss(u, s +

 In order to evaluate our methods, we examined two different datasets of news articles, comments and users.
We proceed to describe how each of the datasets was obtained and preprocessed.
The  rst dataset was crawled from the Newsvine news site1.
Newsvine is a community-powered, collaborative journalism news website, owned by msnbc.com.
Users can write articles, seed links to external content, and discuss news items submitted by both users and professional journalists.
We have chosen to crawl the Newsvine site, among dozens of other available news sites, since: (1) Newsvine is relatively easy to crawl due to the static HTML nature of its content pages; and (2) its registered users constitute a social network that is publicly visible.
Our crawler fetched all news articles published from May 18, 2011 till September 27, 2011.
Each fetched article was parsed in order to extract its title, content, editorial tags, time of publication and set of associated comments.
From each comment we extracted its commenter ID.
Finally, the set of friend IDs of each commenter was obtained.
As shown in Table 2, the Newsvine dataset contains 111,893 articles with 3,009,247 comments and 221,809 users.
Furthermore, its social network contains 106,705 (symmetric) friendship relations, i.e. undirected friendship edges.
1www.newsvine.com s e i r o t s f o #







 # of comments Figure 3: The distribution of stories by their number of comments in the Yahoo!
dataset (after pre-processing).
The second dataset was a set of articles obtained from Yahoo!
News2.
Yahoo!
News is an Internet-based news ag-gregator provided by Yahoo!.
Articles in Yahoo!
News come from news services, namely Associated Press (AP), Reuters, Fox News, CNN.com, BBC News, and others.
Similarly to Newsvine, Yahoo!
News also allows users to comment on news articles.
Our set of articles was sampled so as to contain articles with varying number of comments.
As shown in Table 2, this dataset contains 25,527 articles with 1,664,917 comments and 320,425 users.
Since the  rst dataset was crawled from the Newsvine website we could not obtain any click data that can validate 2news.yahoo.com In order to compare the two datasets properly we did not use any click data on the Yahoo!
dataset as well.
Each article (in both datasets) was later represented as a set of tags and a list of commenters (ordered by their commenting time on the article).
The set of tags contained both editorial tags (only available for the Newsvine dataset) and named entities which were extracted from the article s title and content.
In order to keep the article s representation as compact as possible, we chose to only extract named entities, such as names of people and places, and not its entire set of terms.
In order to identify named entities, we applied a very simple heuristic that looked for sequences of words beginning with a capital letter, except for sequences of length one appearing at the beginning of a sentence.
The results of this simple yet fast heuristic su ced for our needs.
Then, each dataset was split into a training set, containing 60% of the articles, and a test set, containing the remaining
 order of their publish dates, where older articles constituted the training set and newer ones the test set.
Finally, users and stories that did not meet certain conditions were  ltered out, as hereby described.
Users with less than ten comments in the training set were removed, being an inappropriate target of recommendation.
Users without comments in the test set were also removed, as they cannot be evaluated properly.
In addition, we  ltered out from the training set stories without comments, and from the test set stories with less than  ve comments.
We required stories in the test set to have at least  ve comments due to the o ine nature of our evaluation, as will become clearer in Section 5.
Table 2 summarizes the number of stories, users and comments for the training and test sets of each dataset.
Figures 2 and 3 illustrate the distribution of stories by their number of comments in the Newsvine and Yahoo!
datasets, respectively, after the preprocessing phase.
We trained  ve di erent models over the training set data: the Memory Based model (MB) and Latent Factor Models with four error function variants (see Section 3).
Once all models were trained, they were used to generate, for each user, a score for all stories in the test set.3 Then, test stories were ranked for each user according to the assigned scores.
We used the Area Under ROC Curve (AUC) measurement in order to evaluate the rankings.
In general, the AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative one [6].
Eventually, we averaged the achieved AUC values over all users.
Table 3 shows the obtained mean AUC values for both datasets.
Overall, all methods perform quite well on the Newsvine dataset, with a minor advantage to BPR with
 are more signi cant; RL is the best performing method with


 menters were excluded from the test for that user, since the  rst 5 commenters per story are treated as known com-menters and comprise the co-commenting features of a story.
Method Newsvine AUC Yahoo!
AUC














 Table 3: Obtained AUC values for the various methods over the Newsvine and Yahoo!
datasets.
Strati ed AUC The AUC metric, while common in the literature, is susceptible to easily-forecasted elements.
Whenever a considerable portion of the ranked elements are easily classi ed as positive or negative, the AUC value increases and its power of discrimination between better and worse methods deteriorates.
In our case, many stories are predictably unpopular, i.e. it is easy to identify many stories that are unlikely to be commented on by any user.
We call these easy negatives.
As our emphasis is on evaluating the methods by their ability to personalize recommendations, we would like to deemphasize the masking e ect that easy negatives have on the AUC metric.
In a broader sense, we would like to promote methods that are good at personalization, and demote methods that are merely able to predict the universal popularity of stories.
To demonstrate that the AUC metric is biased toward popularity prediction, we measured the success of a popularity oracle that ranks (in hindsight) the test stories, uniformly for all users, by the number of comments they eventually received.
Obviously this oracle does no personalization at all, as it recommends to all users the same order of stories4.
We also reiterate that it is an o ine algorithm that  learns  from the test data and thus naturally over ts.
Indeed, the ranking produced by the  popularity oracle  achieved very high AUC values, with 89.7% AUC on Newsvine and 85.8% AUC on Yahoo!, handily beating all other methods (compare with the values in Table 3).
In order to mitigate the in uence of the popularity bias in our evaluation and to refocus it on personalization, we propose a novel strati ed-AUC performance measurement (sAUC).
The sAUC metric calculates the probability of a random positive example to be ranked higher than a random negative example of (approximately) equal popularity.
More speci cally, sAUC partitions the test stories into popularity bins (see Figures 2 and 3), and computes AUC separately within each bin.
These bin-speci c AUC values are then averaged proportionally to the number of positive examples in each bin.
In order to suit the sAUC measure better, we retrained our latent factor models with an emphasis on discriminating between positive and negative stories within the same popularity bin.
To this end, when training the model on a positive example and a negative example (see Section 3), we sampled the latter from the same popularity bin as the positive one.
Table 4 displays sAUC values for both datasets.
As evident, the  popularity oracle  loses its power when eval-
ranking of test stories, i.e. no other such ranking of stories achieves a better AUC.
Newsvine sAUC Yahoo!
sAUC




  popularity oracle 











 Table 4: Obtained strati ed-AUC values for the various methods over the Newsvine and Yahoo!
datasets.
In contrast to the classic AUC measurement, here the  popularity oracle  ranking achieves the poorest results.
uated with sAUC, while the other methods still achieve fair performance, with BPR attaining the best performance of
 Yahoo!.
It is interesting to note that while all the latent factor models outperform the memory based approach, there is still no clear winning method for both datasets.
Namely, while BPR achieves the best AUC and sAUC on Newsvine, its performance on the Yahoo!
dataset is inferior to the other LFMs.
This promotes a possible invstigation of the di erences between the two datasets and a research of better adjusted LFM loss functions in future research.
As described in section 3, our methods utilize two types of data signals, textual tags and co-commenting patterns, as predictors of a person s tendency to comment on stories.
In this section we attempt to quantify the impact of each signal alone, and of their combination on the recommendation accuracy.
In order to achieve this goal, we created three variations for each one of our prediction models: Tags.
This variation takes into account the textual tags signal alone while completely ignoring the co-commenting signal.
Co-Commenting.
This variation takes into account the co-commenting signal solely while completely ignoring the textual tags signal.
Furthermore, we used this variation in order to investigate the tradeo  between recommendation accuracy and the number of comments available per story.
In order to accomplish this task we tried di erent values (one to  ve) of given comments for test stories.
Tags Co-Commenting Both


 Number of given commenters

 Figure 4: Evaluating the AUC metric on a varying number of given commenters.
The LFM model is trained and tested using the BPR loss function over the Newsvine dataset.
Tags Co-Commenting Both


 Number of given commenters

 Figure 5: Evaluating the AUC metric on a varying number of given commenters.
The LFM model is trained and tested using the RL loss function over the Yahoo!
dataset.
s






 Tags Co-Commenting Both


 Number of given commenters

 Both.
Taking into account both of the co-commenting and textual tags signals.
Similarly to the Co-Commenting variation, we tried di erent values (one to  ve) of given comments for test stories.
Figure 6: Evaluating the strati ed-AUC metric on a varying number of given commenters.
The LFM model is trained and tested using the BPR loss function over the Newsvine dataset.
s








 Tags Co-Commenting Both


 Number of given commenters

 Figure 7: Evaluating the strati ed-AUC metric on a varying number of given commenters.
The LFM model is trained and tested using the CE loss function over the Yahoo!
dataset.
Figures 4-7 show the results of applying the three di erent variations over the prediction models that achieved the best results in the previous subsection.
More speci cally, Figures
 sAUC measures on the Newsvine dataset.
Figure 5 shows results for RL when evaluating the AUC measure on the Yahoo!
dataset.
Finally, Figure 7 shows results for CE when evaluating the sAUC measure on the Yahoo!
dataset.
A careful examination of the four  gures implies that when using 5 given commenters, the co-commenting signal provides a signal that is equivalent or higher than the textual tags signal.
Interestingly, this rea rms the  ndings by Pil aszy and Tikk [20] in the very di erent domain of movies.
Furthermore, combining tags and co-commenting information improves on what each signal achieves alone, even for as little as two given commenters.
As mentioned in Section 4, the Newsvine site has a dedicated social network among its users.
Furthermore, the Newsvine friendship relations are publicly crawlable.
We thus examined whether tapping the co-commenting patterns of a user s friends can help improve our personalized recommendation for the user.
We created a subset of the Newsvine dataset that includes only users with at least one friend (and stories commented by such users, etc.).
Data statistics on the resulting dataset are shown in Table 5.
type Train Test stories users



 comments friendships



 Table 5: Social data statistics for the Newsvine dataset.
We experimented with multiple approaches for representing the social data in the memory based and LFM models.
In all tested approaches, adding social data had a negligible e ect on our ability to predict users  comments.
We therefore only report below the results of the best variation among those tested.
The approach that obtained the maximal recommendation accuracy was an LFM based on modifying each user representation to include information about his friends.
We denote the friends of user u by f riends(u), and represent each friend f by latent factor vector  f   R K .
The vector representation of u,  u (see Section 3.2) is now replaced by (cid:2) the vector  uF  uF =  u +
 |f riends(u)|  f (9) f f riends(u) We also tried an alternative approach of enhancing the story representation based on friendship relationships, which resulted in inferior performance.
Table 6 shows the obtained results when using the tags, co-commenting and social signals, compared to using only the tags and co-commenting signals.
The results are reported for the BPR loss function, which achieved the best results for the Newsvine dataset (in accordance with the previous subsection).
As can be seen from the results, the improvement in prediction due to the addition of the social signal is negligible: 0.1% when using the AUC metric and 0.4% when using the strati ed-AUC metric.
Signals Tags & Co-Commenting Tags & Co-Commenting & Social AUC sAUC



 Table 6: Performance e ect of adding the social signal on the Newsvine dataset.
Obviously, our inability to extract strong signals from the social data is not a proof that such signals does not exist.
Furthermore, the above experiment was done on a single dataset.
However, with those grains of salt in mind, we posit the following:   Commenting activity indicates very strong engagement and passion about a story, and friendship (as re ected in social networks) is not a strong indication of the friends sharing that level of topical passion.
  A nity trumps friendship - collaborative  ltering techniques that (implicitly) identify like-minded users from the population at large, are better at representing one s passions than one s (explicit) selection of friends.
Note that one s selection of friends is not task-oriented, i.e.
people do not mark online friendships for information  ltering purposes.
Thus, the expressed friendships are not tuned to compete against task-oriented CF algorithms.
Still, when unwillingly competing in the information  ltering race, they lose.
  Related to the previous point, it could very well be that the major attraction of a social networks in a site like Newsvine lies in one s ability to keep informed of friends  activities and interests, without necessarily sharing those same interests (or at least not at the same level of passion).
In other words, one may appreciate knowing that a friend has commented on a story, even if that story is of low personal interest.
In this study, we propose a method for personalized recommendation of news stories for commenting.
This is achieved by predicting the stories that are most probable for commenting by a given user.
We combine content-analysis (utilizing stories textual tags) together with collaborative  lter-ing (utilizing users  co-commenting patterns) in a memory based and latent factor model (LFM) approaches.
With respect to the application domain, it is shown that when using 5 given commenters, co-commenting provides a signal that is equivalent or higher than the textual tags signal.
Furthermore, combining both signals improves on what each signal achieves alone.
Moreover, prediction accuracy grows with the number of comments available per story.
Finally, our attempts to combine a third signal, that of social relationships among users, did not result in an observable gain in prediction accuracy.
With respect to the art of recommendation systems, we exemplify how one can improve the accuracy of latent factor models by tailoring the loss function to the speci c application at hand, rather than using the standard squared error loss function.
Moreover, we demonstrate the bias of the AUC metric in cases where one can easily identity items that are not suitable for most users.
Therefore, we suggest a variation of the AUC metric, to which we call strati ed AUC.
The strati ed-AUC metric compares the ranking of items in bins corresponding to their global popularity.
We plan to deploy our model on live tra c data, which will allow a more realistic evaluation of the methods described in this paper.
Such an online setting will bring us new challenges and opportunities.
It will necessitate the replacement of the static train-test split by a more realistic incremental evaluation, where the amount of information per story gradually increases over time.
Such an evaluation will incorporate the temporal popularity of stories and the aging of old stories.
No less important is the opportunity to better identify stories regarded as  negative  for the user.
As previously explained, we currently draw negative examples from those stories on which the user has not commented.
In the future we will be exposed to additional signals allowing us a more re ned identi cation of negative stories.
We would like to discard stories which were never presented to the user (e.g., she was inactive during their serving time).
Then, we should distinguish between stories presented to user but not viewed (clicked) by her and stories viewed by the user without initiating commenting activity.
Such considerations impact not only the evaluation procedure, but also the modeling methodology.
Another aspect that was not addressed in this work is the discrimination between  types  of commenters; while many users have a rich commenting history, a large portion of the users rarely comment and their commenting history is very poor.
A future research may want to examine the possibility of modeling users di erently according to their commenting habits.
In additions, our preliminary attempts to employ friendship connections to the recommendation model promote further investigation of the bene t of using social signals vs. users  behavioral similarity.
We would like to acknowledge Michal Aharon, Yoad Lustig and Yoelle Maarek for their contribution to this work in many fruitful discussions.
