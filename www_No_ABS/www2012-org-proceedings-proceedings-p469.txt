The Linked Open Data (LOD) community is currently bringing structured data to the Web by publishing data sets using RDF and by interlinking related concepts coming from di erent data sets.
As the LOD movement gains momentum, linking traditional Web content to the LOD cloud is giving rise to new possibilities for online information processing.
For instance, linking textual content to LOD concepts opens the door to automated text enrichment (e.g., by providing additional information coming from the LOD cloud for the entities appearing in the text), as well as to streamlined information retrieval and integration (e.g., by using links to retrieve all text articles related to a given concept from the LOD cloud).
Automatizing the process of extracting entities from natural language text and linking those entities to the correct structured concept(s) in the LOD cloud is currently drawing a lot of attention (see the Related Work section below).
It represents however a daunting task, as entity matching is Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
known to be extremely challenging even in relatively simple contexts, since parsing and disambiguating natural language text is still extremely di cult for machines nowadays.
The current matching techniques used to relate an entity extracted from text to corresponding entities from the LOD cloud can be broadly classi ed into two groups: Algorithmic Matching: Given the scale of the problem (that could potentially span the entire HTML Web), many e orts are currently focusing on designing and deploying scalable algorithms to perform the matching automatically on very large corpuses.
Manual Matching: While algorithmic matching techniques are constantly improving, they are still at this stage not as reliable as humans.
Hence, many organizations are still today appointing individuals to manually link textual elements to concepts.
For instance, the New York Times employs a whole team whose sole responsibility is to manually create links from news articles to NYT identi ers1.
This paper represents a step towards bridging the gap between those two classes of techniques by parsimoniously using human workers to guide the automated linking process.
We introduce a new system, called ZenCrowd, which gracefully combines algorithmic and manual matching.
ZenCrowd takes advantage of algorithmic matching techniques to routinely link entities, but attempts to improve the automatic results by involving human workers.
Our solution systematizes and automatizes manual matching techniques by dynamically creating micro matching tasks and by publishing them on a popular crowdsourcing platform.
To operate the system e ciently, we develop a probabilistic framework to decide how to incorporate manual matching, and to more effectively integrate inconsistent results obtained by arbitrary sets of human workers on the crowdsourcing platform.
ZenCrowd does not focus on the algorithmic entity extraction or entity matching problems per se (we use state of the art techniques for both automated entity extraction and entity matching, but do not directly innovate on that front).
However, we believe that we make a number of key contributions at the interface of algorithmic and manual matching, and discuss in detail how to most e ectively and e ciently combine the two approaches using both theoretical models and experimental results.
The contributions of this paper include:   a new system architecture supporting both algorithmic and manual matching approaches in concert 1see http://data.nytimes.com/ taking advantage of scoping mechanisms and modern crowdsourcing platforms   new techniques to evaluate the combined performance of algorithmic and manual matching on sets of entities   a new probabilistic reasoning framework to dynamically assess the results of arbitrary human workers operating on a crowdsourcing platform, and to e ectively combine their (con icting) output taking into account the results of the algorithmic matching, uniqueness constraints, and identity links from the LOD cloud   an empirical evaluation of our system in a real deployment over di erent countries showing that ZenCrowd combines the best of both worlds, in the sense that our combined approach turns out to be more e ective than both algorithmic and manual matching techniques for online entity linking.
The rest of this paper is structured as follows: We review the state of the art in entity linking, entity matching and crowd-sourcing systems in Section 2.
Section 3 gives an overview of the architecture of our system, including its algorithmic matching interface, its probabilistic reasoning engine, and its templating and crowdsourcing components.
We describe our formal model to combine both algorithmic and crowd-sourcing results using probabilistic reasoning in Section 4.
We introduce our evaluation methodology and discuss results from a real deployment of our system in Section 5, before concluding in Section 6.
Entity Linking.
Entities have recently become  rst-class citizens on the Web.
A large amount of online search queries are about entities [29], and search engines exploit entities and structured data to build their result pages [17].
In the  eld of Information Retrieval (IR) a lot of attention has been given to entities: At TREC2, the main IR evaluation initiative, the task of Expert Finding, Related Entity Finding, and Entity List Completion have been studied [2, 3].
Along similar lines, we evaluated [12] Entity Ranking in Wikipedia at INEX3 recently.
The problem of assigning URIs to entities (i.e., entity linking), which is the focus of our paper, has been widely studied by the database and the Semantic Web research communities.
A related e ort has for example been carried out in the context of the OKKAM project4, which suggested the idea of an Entity Name System (ENS) to assign identi ers to entities on the Web [7].
The ENS could integrate techniques from our paper to improve matching e ectiveness.
The  rst step in entity linking consists in extracting entities from textual content.
Several approaches developed within the NLP  eld provide high-quality entity extraction for persons, locations, and organizations [8, 4].
State of the art techniques are implemented in tools like Gate [11], the Stanford parser [21] (which we use in our experiments), and Extractiv5.
Once entities are extracted, they still need to be disambiguated and matched to semantically similar but 2http://trec.nist.gov 3https://inex.mmci.uni-saarland.de/ 4http://www.okkam.org 5http://extractiv.com/ syntactically di erent occurrences of the same real-world object (e.g.,  Mr.
Obama  and  President of the USA ).
Classical matching approaches are based on string similarities ( Barack Obama  vs.  B.
Obama ) such as the edit distance [24], the Jaro similarity [18], or the Jaro-Winkler similarity [30].
More advanced techniques, as for instance Group Linkage [28], compare groups of records to  nd matches.
A third class of approaches uses semantic information.
Reference Reconciliation [14], for example, builds a dependency graph and exploits relations to propagate information among entities.
In [10], we build disambiguation graphs based on the transitive closures of equivalence links in networks containing uncertain information.
Our present work focuses on a very di erent topic and aims at correctly linking isolated entities to external entities using an e ective combination of algorithmic and manual matching techniques.
The  nal step in entity linking is that of deciding which links to retain in order to enrich the entity.
Systems performing such a task are available as well (e.g., Open Calais6, DBPedia Spotlight [26]).
Relevant work aims for instance at enriching documents by automatically creating links to Wikipedia pages [27], which can be seen as entity identi ers.
While previous work selects URIs from a speci c corpus (e.g., DBPedia, Wikipedia), our goal is to assign entity identi ers from the LOD cloud7 instead.
To the best of our knowledge, this paper is the  rst to propose a principled approach based on crowdsourcing techniques to improve the quality of automated entity linking algorithms.
Ad-Hoc Object Retrieval.
Another task related to the one we are addressing in this paper is Ad-hoc Object Retrieval (AOR) [29], where systems need to retrieve the correct URIs given a keyword query representing an entity.
Such a task has been evaluated in the context of the Semantic Search workshop in 2010 and 2011 using a set of queries extracted from a commercial search engine query log and crowdsourc-ing techniques to create the gold standard.
Most of the proposed systems for this task (see for example [6]) exploit IR indexing and ranking techniques over the RDF dataset used at the Billion Triple Challenge 2009.
Similarly to such tasks, our dataset is composed of a large set of triples coming from LOD datasets, while our queries consist of entities extracted from news articles and the gold standard is manually created by experts.
In addition to those e orts, we selectively exploit the crowd to improve the accuracy of the task.
Crowdsourcing.
Crowdsourcing is a relatively recent technique that is currently being investigated in a number of contexts.
In the IR community, crowdsourcing techniques have been mainly used to create test collections for repeatable relevance assessment [1, 19, 20].
The task of the workers is to judge the relevance of a document for a given query.
Studies have shown that this is a practically relevant approach, which produces reliable evaluation collections [5].
The database community is currently evaluating how crowd-sourcing methods can be used to build RDMS systems able to answer complex queries where subjective comparison are needed (e.g.,  10 papers with the most novel ideas ) [15].
Crowdsourcing can also be used for basic computational operations such as sort and join [25].
6http://www.opencalais.com/ 7http://linkeddata.org/ been used by Finn et al [16] to annotate entities in Twitter.
Their goal is simpler than ours as they ask human workers to identify entities in text and assign a type (i.e., person, location, or organization) to the identi ed entities.
Our goal is, instead, to assign entity identi ers to large amounts of entities on the Web.
The two approaches might be combined to obtain high quality for both extraction and linking.
ZenCrowd is a hybrid platform that takes advantage of both algorithmic and manual matching techniques simultaneously.
Figure 1 presents a simpli ed architecture of our system.
We start by giving an overview of our system below in Section 3.1, and then describe in more detail some of its components in Sections 3.2 to 3.5.
ZenCrowd takes as input sets of HTML pages (that can for example be provided by a Web crawler).
The HTML pages are then passed to Entity Extractors that inspect the pages and identify potentially relevant textual entities (e.g., persons, companies, places, etc.)
mentioned on the page.
Once detected, the entities are fed into Algorithmic Matchers that attempt to automatically link the textual entities to semantically similar entities from the LOD cloud.
As querying the Web of data dynamically to match each entity would incur a very high latency, we build a local cache (called LOD Index in Figure 1) to locally retrieve and index relevant information from the LOD cloud.
Algorithmic matchers return lists of top-k links to LOD entities, along with a con dence value for each potentially relevant link.
The results of the algorithmic matchers are stored in a Probabilistic Network, and are then combined and analyzed using probabilistic inference techniques.
ZenCrowd treats the results of the algorithmic matchers in three di erent ways depending on their quality.
If the algorithmic results are deemed excellent by our Decision Engine, the results (i.e., the links connecting a textual entity extracted from an HTML page to the LOD cloud) get stored in a local database directly.
If the results are deemed useless (e.g., when all the links picked by the matchers have a low con dence value), the results get discarded.
Finally, if the results are deemed promising but uncertain (for example because several algorithmic matchers disagree on the links, or because their con dence values are relatively low), they are then passed to the Micro-Task Manager, which extracts relevant snippets from the original HTML pages, collects all promising links, and dynamically creates a micro-matching task using a tem-plating engine.
Once created, the micro-matching task is published on a crowdsourcing platform, where it is handled by collections of human workers.
When the human workers have performed their task (i.e., when they have picked the relevant links for a given textual entity), workers results are fed back to the Probabilistic Network.
When all the matching results are available for a given HTML page, an enriched HTML page containing both the original HTML code as well as RDFa annotations linking the textual entities to their counterpart from the LOD cloud is  nally generated.
The Entity Extractors receive HTML as input, and extract named entities appearing in the HTML content as output.
Entity Extraction is an active area of research and a number of advances have recently been made in that  eld (using for instance third-party information or novel NLP techniques).
Entity extraction is not the focus of our work in ZenCrowd.
However, we support arbitrary entity extractors through a generic interface in our system and union their respective output to obtain additional results.
Once extracted, the textual entities are inspected by algorithmic matchers, whose role is to  nd semantically similar entities from the LOD cloud.
ZenCrowd implements a number of state of the art matching techniques (see Section 5 for an example) that take advantage of the LOD Index component to e ciently  nd potential matches.
Each matcher also implements a normalized scoring scheme, whose results are combined by our Decision Engine (see below Section 4).
The LOD index is a declarative information retrieval engine used to speed up the matching process.
While most LOD data sets provide a public SPARQL interface, they are in practice very cumbersome to use due to the very high latency (from several hundreds of milliseconds to several seconds) and bandwidth consumption that they impose.
Instead of querying the LOD cloud dynamically for each new entity, ZenCrowd caches locally pertinent information from the LOD cloud.
Our LOD Index engine receives as input a list of SPARQL endpoints or LOD dumps as well as a list of triple patterns, and iteratively retrieves all corresponding triples from the LOD data sets.
The information thus extracted is cached locally in two ways: in our e cient analytical query engine [31] o ering a SPARQL interface and in an inverted index to provide e cient support for unstructured queries.
Instead of using heuristics or arbitrary rules, ZenCrowd systematizes the use of probabilistic networks to make sensible decisions about the entities.
All evidences gathered both from the algorithmic matchers and the crowd are fed into a scalable probabilistic store, and used by our decision engine to process all entities accordingly.
Our probabilistic models are described in detail in Section 4.
The micro-task manager,  nally, is responsible for dynamically creating human computation tasks that are then published on a crowdsourcing platform.
Whenever an entity match is deemed promising by our Decision Engine (see below for details), it is sent to the crowd for further examination.
The micro-task manager dynamically builds a Web page to be published on the crowdsourcing platform using three resources: i) the name of the textual entity ii) some contextual information generated using a template and the original HTML document from which the entity was extracted and iii) the current top-k matches for the entity from the Probabilistic Network.
We experimented with various templates to extract contextual information from the HTML pages, including named entity providing just the text corresponding to the extracted entity, and text snippet extracting the surrounding text around each occurrence of the entity in the HTML page (we report  ndings for those two templates in Section 5).
Once created and published, the matching micro-tasks can be selected by workers on the crowdsourc-ing platform, who are then asked to select the relevant links them by extracting textual entities appearing in the pages and linking them to the Linked Open Data cloud.
ZenCrowd uses both algorithmic matchers and human workers to generate high quality results.
(if any) for the entity, given its name, the contextual information from the original HTML text, and the various candidate matches linking to their online description in the LOD cloud.
Once performed, the results of the micro-matching tasks are sent back to the Micro-Task Manager, which in turns inserts them in the Probabilistic Network.
ZenCrowd exploits probabilistic models to make sensible decisions about candidate matches.
We describe below the probabilistic models used to systematically represent and combine information in ZenCrowd, and how those models are implemented and handled by our system.
We start by giving an overview of probabilistic networks  rst.
Message Passing Schemes We use factor-graphs to graphically represent probabilistic variables and distributions in the following.
Note that our approach is not bound to this representation we could use series of conditional probabilities only or other probabilistic graphical model but we decided to use factor-graphs for their illustrative merits.
We give below a brief introduction to factor-graphs and message-passing techniques.
For a more in-depth coverage, we refer the interested reader to one of the many overviews on this domain, such as [23].
Probabilistic graphical models are a marriage between probability theory and graph theory.
In many situations, one can deal with a complicated global problem by viewing it as a factorization of several local functions, each depending on a subset of the variables appearing in the global problem.
As an example, suppose that a global function g(x1, x2, x3, x4) factors into a product of two local functions fA and fB: g(x1, x2, x3, x4) = fA(x1, x2)fB(x2, x3, x4).
This factorization can be represented in a graphical form by the factor-graph depicted in Figure 2: A simple factor-graph of four variables and two factors.
Figure 2, where variables (circles) are linked to their respective factors (black squares).
Often, one is interested in computing a marginal of this (cid:88) (cid:88) (cid:88) global function, e.g., (cid:88) where we introduce the summary operator (cid:80) g(x1, x2, x3, x4) = g2(x2) = x1 x3 x4  {xi} to sum over all variables but xi.
Such marginals can be derived in an e cient way by a series of simple sum-product operations on the local function, such as: g(x1, x2, x3, x4)  {x2} (cid:32)(cid:88) (cid:33)(cid:32)(cid:88) (cid:88) (cid:33) g2(x2) = fA(x1, x2) fB(x2, x3, x4) .
x1 x3 x4 Interestingly, the above computation can be seen as the product of two messages  fA x2 (x2) and  fB x2 (x2) sent respectively by fA and fB to x2 (see Figure 2).
The sum-product algorithm [23] exploits this observation to compute all marginal functions of a factor-graph in a concurrent and e cient manner.
Micro Matching TasksHTMLPagesHTML+ RDFaPagesLOD Open Data CloudCrowdsourcingPlatformZenCrowdEntityExtractorsLOD IndexGet EntityInputOutputProbabilistic NetworkDecision EngineMicro-Task ManagerWorkers DecisionsAlgorithmicMatchersx1x2x3x4fAfB fA-x2(x2) fB-x2(x2)WWW 2012   Session: Entity LinkingApril 16 20, 2012, Lyon, France472rect should be proportional to the fraction of good workers indicating the link as correct.
Taking into account both observations, and mapping the value 0 to Incorrect and 1 to Correct, we write the following function for the factor: (cid:40) 0.5 (cid:80) (cid:80) (wi=Good   ci=l)
 i
 i (wi=Good) lf (w1, .
.
.
, wm, c1, .
.
.
, cn, l) = if  wi   {w1, .
.
.
, wm} wi = Bad SameAs Constraints where 1(cond) is an indicator function equal to 1 when cond is true and 0 otherwise.
SameAs constraints exploit the fact that the resources identi ed by the links to the LOD cloud can themselves be interlinked (e.g., http://dbpedia.org/resource/Fribourg is connected through an owl:sameAs link to fbase:Fribourg in the LOD cloud).
Considering that the SameAs links are correct, we de ne a constraint on the variables connected by such links; the factor sa() connecting those variables puts a constraint forbidding assignments where the variables would not be set to the same values: (cid:26) 1 if  (li, lj)   {l1, .
.
.
, ln} li = lj sa(l1, .
.
.
, ln) = 0 otherwise We enforce the constraint by declaring sa() = 1.
This constraint considerably helps the decision process when strong evidences (good priors, reliable clicks) are available for any of the URIs connected to a SameAs link.
When not all SameAs links should be considered as correct, further probabilistic analyses (e.g., on the transitive closures of the links as de ned in idMesh [10]) can be put into place.
Many LOD datasets are curated manually, or are generated from manually-curated databases such as Wikipedia.
In those datasets, a given entity is represented by exactly one URI (i.e., the datasets do not contain duplicate entities).
When several links from such a dataset appear in an entity graph, we can thus rule out all con gurations where more than one of those links are considered as Correct.
The corresponding factor u() is declared as being equal to 1 and is de ned as follows: (cid:26) 0 if  (li, lj)   {l1, .
.
.
, ln}|li = lj = Correct u(l1, .
.
.
, ln) = 1 otherwise
 Given the scheme above, we can reach a sensible decision by simply running a probabilistic inference method (e.g., the sum-product algorithm described above) on the network, and considering as correct all links with a posterior probability P (l = Correct) > 0.5.
The Decision Engine can also consider a higher threshold   > 0.5 for the decisions in order to increase the precision of the results.
Our computations always take into account prior factors capturing a priori information about the workers.
As time passes, decisions are reached on the correctness of the various links, and the probabilistic network iteratively accumulates posterior probabilities on the reliability of the workers.
Actually, the network gets new posterior probabilities on the reliability of the workers for every new link decision that is Figure 3: An entity factor-graph connecting two workers (wi), six clicks (cij), and three candidate links (lj).
Workers, We start by describing the probabilistic graphs used to combine all evidences gathered for a given entity.
Consider an entity e extracted from an HTML page.
Our probabilistic graph stores all candidate matches for the entity coming from the LOD cloud.
The candidate matches are stored as a list of potential links lj linking to the LOD cloud.
Each link has a prior probability distribution plj computed from the algorithmic matchers.
Each link can also be examined by human workers wi performing micro-matching tasks and performing clicks cij to express the fact that a given link corresponds (or not) to the entity from his/her perspective.
links, and clicks are mapped onto binary variables in our model.
Workers accept two values {Good, Bad} indicating whether they are reliable or not.
Links can either be Correct or Incorrect.
As for click variables, they represent whether the worker considers that the entity is the same as the one represented by the link (Correct) or not (Incorrect).
We store prior distributions representing a priori knowledge obtained for example through training phases or thanks to external sources for each workers (pwi()) and each link (plj()).
The clicks are observed variables and are set to Correct or Incorrect depending on how the human workers clicked on the crowdsourcing platform.
A simple example of such an entity graph is given in Figure 3.
Clicks, workers, and links are further connected through three factors described below.
Link factors lfj() connect each link to its related clicks and the workers who performed those clicks.
Examining the relationships between those three classes of variables, we make two key observations: i) clicks from reliable workers should weight more than clicks from unreliable workers (actually, clicks from consistently unreliable workers deciding randomly if a given link is relevant or not should have no weight at all in our decision process) and ii) when reliable workers do not agree, the likelihood of the link being cor-w1w2l1l2pw1( )pw2( )lf1( )lf2( )pl1( )pl2( )l3lf3( )pl3( )c11c22c12c21c13c23u2-3( )sa1-2( )WWW 2012   Session: Entity LinkingApril 16 20, 2012, Lyon, France473reached.
Thus, the Decision Engine can decide to modify the priors of the workers by taking into account the evidences accumulated thus far in order to get more accurate results in the future.
This corresponds to a learning parameters phase in a probabilistic graphical model when some of the observations are missing.
Several techniques might be applied to this type of problem (e.g., Monte Carlo methods, Gaussian approximations).
We use in the following a simple Expectation-Maximization [9, 13] process, which looks as follows: - Initialize the prior probability of the workers using a training phase during which workers are evaluated on k matches whose results are known.
Initialize their prior reliability to #correct results/k.
If no information is available or no training phase is possible, start with P (w = reliable) = P (w = unreliable) = 0.5 (maximum entropy principle).
- Gather posterior evidences on the reliability of the workers P (w = reliable|li = Correct/Incorrect) as soon as a decision is reached on a link.
Treat these evidences as new observations on the reliability of the workers, and update their prior beliefs iteratively as follows: k(cid:88) P (w = reliable) = Pi(w = reliable|li)k  1 i=1 where i runs over all evidences gathered so far (from the training phase and from the posterior evidences described above).
Hence, we make the prior values slowly converge to their maximum likelihood to re ect the fact that more and more evidences are being gathered about the mappings as we reach more decisions on the links.
This technique can also be used to identify and blacklist unreliable workers dynamically (see Section 5.2 for an illustration).
The framework described above actually creates a gigantic probabilistic graph, where all entities, clicks, and workers are indirectly connected through various factors.
However, only a subset of the variables need to be considered by the inference engine at any point in time.
Our system updates the various priors iteratively, but only instantiates the variables useful for reaching a decision on the entity currently examined.
It thus dynamically instantiates entity factor-graphs, computes posterior probabilities for the links, reaches a decision, updates the priors, and stores back all results before de-instantiating the graph and moving to the next entity.
Dataset Description.
In order to evaluate ZenCrowd, we created an ad-hoc test collection8.
The collection consists of 25 news articles written in English from CNN.com, NYTimes.com, washington-post.com, timeso ndia.indiatimes.com, and swissinfo.com, which were manually selected to cover global interest news
 http://diuf.unifr.ch/xi/zencrowd/.
(10), US local news (5), India local news (5), and Switzerland local news (5).
After the full text of the articles has been extracted from the HTML page [22], 489 entities were extracted from it using the Stanford Parser [21] as entity extractor.
The collection of candidate URIs is composed of all entities from DBPedia9, Freebase10, Geonames11, and NYT12, summing up to approximately 40 million entities (23M from Freebase, 9M from DBPedia, 8M from Geon-ames, 22K from NYT).
Expert editors manually selected the correct URIs for all the entities in the collection to create the ground truth for our experiments.
Crowdsourcing was performed using the Amazon MTurk13 platform where
 $0.01, consisted of selecting the correct URIs out of the proposed  ve URIs for a given entity.
ZenCrowd is a relatively sophisticated system involving many components.
In the following, we present and discuss the results of a series of focused experiments, each designed to illustrate the performance of a particular feature of our system or of related techniques.
Though many other experiments could have been performed, we believe that the set of experiments presented below gives an particularly accurate account of the performance of ZenCrowd.
We start by describing a relatively simple base-con guration for our experimental setting below.
Candidate Selection: LOD Indexing, Entity Matching and Ranking.
In order to select candidate URIs for an entity, we adopt IR techniques similar to those that have been used by participants of the Entity Search evaluation at the Semantic Search workshop for the AOR task, where a string representing an entity (i.e., the query) is used to rank URIs that identify the entity.
We build an inverted index over 40 million entity labels in the considered LOD datasets, and run queries against it using the entities extracted from the news articles in the test collection.
Unless speci ed otherwise, the top 5 results ranked by TF-IDF are used as candidates for the crowdsourcing task.
Micro-Task Generation.
We dynamically create a task on MTurk for each entity sent to the crowd.
We generate a micro-task where the entity (possibly with some textual context) is shown to the worker who has then to select all the URIs that match the entity, with the possibility to click on the URI and visit the corresponding webpage.
If no URI matches the entity, the worker can select the  None of the above  answer.
An additional  eld is available for the worker to leave comments.
Evaluation Measures.
In order to evaluate the e ectiveness of our methods we compare, for each entity, the selected URIs against the ground truth which provides matching/non-matching information for each candidate URI.
Speci cally, we compute (P)recision, (R)ecall, and (A)ccuracy which are de ned as follows: We consider as true positives (tp) all 9http://dbpedia.org/ 10http://www.freebase.com/ 11http://www.geonames.org/ 12http://data.nytimes.com/ 13http://www.mturk.com select the URI, true negatives (tn) the cases where both the ground truth and the approach do not select the URI for the entity, false positives (fp) the cases where the approach selects a URI which is not considered correct by the ground truth, and false negatives (fn) the cases where the approach does non select a URI that is correct in the ground truth.
Then, Precision is de ned as P = tp/(tp + f p), Recall as R = tp/(tp + f n), and Accuracy as A = (tp + tn)/(tp + tn + f p + f n).
In the following, all the  nal matching approaches (automatic, agreement vote, and ZenCrowd) are optimized to return high precision values.
We decided to focus on precision from the start, since from our experience it is the most useful metric in practice (i.e., entity linking applications typically tend to favor precision to foster correct information processing capabilities, and do not care if some of the entities end up being not linked).
Entity Extraction and Linkable Entities.
We start by evaluating the performance of the entity extraction process.
As described above, we use a state of the art extractor (the Stanford Parser) for this task.
According to our ground truth, 383 out of the 488 automatically extracted entities can be correctly linked to URIs in our experiments, while the remaining ones are either wrongly extracted, or are not available in the LOD cloud we consider.
Unless stated otherwise, we average our results over all link-able entities, i.e., all entities for which at least one correct link can be picked out (we disregard the other entities for several experiments, since they were wrongly extracted from the text or are not at all available in the LOD data we consider and thus can be seen as a constant noise level in our experiments).
Candidate Selection.
We now turn to the evaluation of our candidate selection method.
As described above, candidate selection consists in the present case in ranking URIs using TF-IDF given an extracted entity14.
We focus on high recall for this phase (i.e., we aim at keeping as many potentially interesting candidates as possible), and decided to keep the top-5 URIs produced by this process.
Thus, we aim at preserving as many correct URIs as possible for later matching steps (e.g., in order to provide good candidate URIs to the crowd).
We report on the performance of candidate selection in Table 1.
As we can observe, results are consistent with our goal since all interesting candidates are preserved by this method (Recall of 1 for the linkable entities set).
Then, we examine the potential role of the highest con -dence scores in the candidate selection process.
This analysis helps us decide when crowdsourcing an entity matching task is useful and when it is not.
In Figure 4, we report on the average recall of the top-5 candidates when classifying results based on the maximum con dence score obtained (top-1 score).
The results are averaged over all extracted entities15.
BM25F as a ranking function.
ually de ning a transformation function.
Table 1: Performance results for the candidate selection approach.
All Entities Linkable Entities GL News US News IN News SW News All News























 Figure 4: Average Recall of candidate selection when discriminating on max relevance probability in the candidate URI set.
As expected, we observe that high con dence values for the candidates selection lead to high recall and, therefore, to candidate sets which contain many of the correct URIs.
For this reason, it is useful to crowdsource entity matching tasks only for those cases exhibiting relatively high con dence values (e.g., > 0.5).
When the highest con dence value in the candidate set is low, it is then more likely that no URI will match the entity (because the entity has no URI in the LOD cloud we consider, or because the entity extractor extracted the entity wrongly).
On the other hand, crowdsourcing might be unnecessary for cases where the precision of the automatic candidate selection phase is already quite high.
The automatic selection techniques can be adapted to identify the correct URIs in a completely automatic fashion.
In the following, we automatically select top-1 candidates only (i.e., the link with the highest con dence), in order to focus on high precision results as required by many practical applications.
A di er-ent approach focusing on recall might select all candidates with a con dence higher than a certain threshold.
Figure 5 reports on the performance of our fully automatic entity linking approaches.
We observe that when the top-1 URI is selected, the automatic approach reaches a precision value of 0.70 at the cost of low recall (i.e., fewer links are picked).
As later results will show, crowdsourcing techniques can improve both precision and recall results over this automatic entity linking approaches in all cases.
Entity Linking using Crowdsourcing with Agreement Vote.
We now report on the performance of a state of the art crowdsourcing approach based on agreement voting: the 5 automatically selected candidate URIs are all proposed to 5 di erent workers who have to decide which URI(s) is (are) correct for the given entity.
After the task is completed,




















 Recall of Top
 Max Matching Probability the automatic approach.
the URIs with at least 2 votes are selected as valid links (we tried various thresholds and manually picked 2 in the end since it leads to the highest precision scores while keeping good recall values for our experiments).
We report on the performance of this crowdsourcing technique in Table 2.
The values are averaged over all linkable entities for di erent document types and worker communities.
Table 2: Performance results for crowdsourcing with agreement vote over linkable entities.
US Workers

















 Indian Workers

















 GL News US News IN News SW News All News The  rst question we examine is whether there is a di er-ence in reliability between the various populations of workers.
In Figure 6 we show the performance for tasks performed by workers located in USA and India (each point corresponds to the average precision and recall over all entities in one document).
On average, we observe that tasks performed by workers located in the USA lead to higher precision values.
As we can see in Table 2, Indian workers obtain higher precision and recall on local Indian news as compared to US workers.
The biggest di erence in terms of accuracy between the two communities can be observed on the global interest news.
the sentences containing the entity are shown to the worker (snippets).
Surprisingly, we could not observe a signi cant di erence in e ectiveness caused by the di erent textual contexts given to the workers.
Thus, we focus on only one type of context for the remaining experiments (we always give the snippet context).
Figure 7: Crowdsourcing results with two di erent textual contexts Entity Linking with ZenCrowd.
We now focus on the performance of the probabilistic inference network as proposed in this paper.
We consider the method described in Section 4, with an initial training phase consisting of 5 entities, and a second, continuous training phase, consisting of 5% of the other entities being o ered to the workers (i.e., the workers are given a task whose solution is known by the system every 20 tasks on average).
In order to reduce the number of tasks having little in u-ence in the  nal results, a simple technique of blacklisting of bad workers is used.
A bad worker (who can be considered as a spammer ) is a worker who randomly and rapidly clicks on the links, hence generating noise in our system.
In our experiments, we consider that 3 consecutive bad answers in the training phase is enough to identify the worker as a spammer and to blacklist him/her.
We report the average results of ZenCrowd when exploiting the training phase, constraints, and blacklisting in Table 3.
As we can observe, precision and accuracy values are higher in all cases when compared to the agreement vote approach.
Table 3: Performance results for crowdsourcing with ZenCrowd over linkable entities.
US Workers

















 Indian Workers

















 GL News US News IN News SW News All News Figure 6: Per document task e ectiveness.
A second question we examine is how the textual context given for an entity in uences the worker performance.
In Figure 7, we compare the tasks for which only the entity label is given to those for which a context consisting of all Finally, we compare ZenCrowd to the state of the art crowdsourcing approach (using the optimal agreement vote) and our best automatic approach on a per-task basis in Figure 8.
The comparison is given for each document in the test collection.
We observe that in most cases the human intelligence contribution improves the precision of the automatic approach.
We also observe that ZenCrowd dominates




















 Precision / Recall Matching Probability Threshold

















 Precision / Recall Top
 Results


















 Recall Precision
 India




















 Precision Document Simple Snippet ing platforms (all we can do is prevent some workers from receiving any further task through blacklisting), obtaining perfect linking results is thus in general unrealistic for non-controlled settings.
As another consequence, augmenting the numbers of workers performing a given task is not always bene cial: Figure 9 right shows how the average precision of ZenCrowd when (virtually) employing the available top-k workers for a given task.
As can be seen from the graph, the quality of the results gets worse after a certain value of k, as more and more mediocre workers are picked out.
As a general rule, we observe that limiting the number of workers to 4 or 5 good workers for a given task gives the best results.
Figure 8: Comparison of three matching techniques.
the overall performance (it is the best performing approach in more than 3/4 of the cases).
Ef ciency.
Finally, we brie y comment on the e ciency of our approach.
In its current implementation, ZenCrowd takes on average 200ms to extract an entity from text, 500ms to select and rank candidate URIs, and 500ms to generate a micro-matching task.
The decision process takes on average 100ms.
Without taking into account any parallelization, our system can thus o er a new entity to the crowd roughly every second, which in our opinion is su cient for most applications (e.g., enriching newspaper articles or internal company documents).
Once on the crowdsourcing platform, the tasks have a much higher latency (several minutes to a few hours), latency which is however mitigated by the fact that entity matching is an embarrassingly parallel operation on crowd-sourcing platforms (i.e., large collections of workers can work in parallel at any given point in time).
Looking back at the experimental results presented above, we  rst observe that crowdsourcing entity matching is useful to improve the e ectiveness of an entity linking system.
State of the art crowdsourcing techniques can improve precision by 6%.
ZenCrowd takes advantage of a probabilistic framework for making decisions and performs even better, leading to performance improvement ranging between 4% and 35% over the manually optimized agreement vote approach, and on average of 14% over our best automatic matching approach.
In both cases, the improvement is statistically signi cant (t-test p < 0.05).
A more general observation is that entity linking is a challenging task, which can rapidly become impossible when errors are made at the entity extraction or candidate selection phases.
Analyzing the population of workers on the crowdsourcing platform (see Figure 9 left), we observe that the number of tasks performed by a given worker is Zipf-distributed (i.e., few workers perform many tasks, while many workers perform a few tasks only).
Also, we observe that the average precision of the workers is broadly distributed between [0, 1].
As workers cannot be selected Figure 9: Distribution of worker precision and task precision with top k workers.
As the LOD movement gains momentum, linking traditional Web content to the LOD cloud is getting increasingly important in order to foster automated information processing capabilities.
Current tools rely either on fully automatic techniques or on the sole work of human experts.
In this paper, we have presented ZenCrowd, a system based on a probabilistic framework leveraging both automatic techniques and punctual human intelligence feedback captured on a crowdsourcing platform.
ZenCrowd can be used in combination with automatic entity extraction, ranking, and matching techniques to improve the overall linking accuracy.
As our approach incorporates a human intelligence component, it typically cannot perform entity linking tasks in real-time.
However, we believe that it can still be used in most practical settings, thanks to the embarrassingly parallel nature of entity matching in crowdsourcing environments.
In conclusion, ZenCrowd provides a reliable approach to entity linking, which exploits the trade-o  between large-scale automatic entity linking and high-quality human annotations, and which according to our results improves the precision of the results by 4% to 35% over a state of the art and manually optimized crowdsourcing approach, and on average by 14% over our best automatic matching approach.
As future work, we plan to focus on making the entire linking process more e ective and e cient by adopting methods to automatically select the best LOD data sets for a given entity dynamically instead of considering a  xed dataset for all cases.
We also plan to focus on a qualitative analysis of our approach by looking into its implications on the end-user side.
Finally, we plan to investigate a few sociological aspects by analyzing the long-term behavior of the human workers.
Precision Document Agr.
Vote ZenCrowd Top
 Top
 Worker





 Worker Precision Number of Tasks
 Workers
 Workers



















 Precision Top
 workers This work was supported by the Swiss National Science Foundation under grant number PP00P2 128459.
