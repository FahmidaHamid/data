Preference aggregation is the problem of combining multiple preferences over objects into a single consensus ranking.
This problem is crucially important in many applications, such as information retrieval, collaborative  ltering and social choice.
Across various domains, the preferences over objects are expressed in several di erent ways, ranging from full and partial rankings to arbitrary comparisons.
For instance, in meta-search an issued query is sent to several search engines and the (often partial) rankings returned by them are aggregated to generate more comprehensive ranking results.
On the other hand, in online gaming the goal is typically to estimate the rank/skill of the players that participate in 1-on-1 games as well as tournaments.
The resulting evidence of players  skill thus comes in the form of pairwise Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Richard S. Zemel University of Toronto
 Toronto, ON M5S 2E4 zemel@cs.toronto.edu comparisons as well as partial tournament rankings, with many observations of the form  a beat b  and  b beat a .
Given the underlying correspondence between ranking and permutation, considerable work in machine learning has exploited probabilistic models on permutations, many of which originate in statistics and psychology.
Mallows [18] and Plackett-Luce [22, 17] are particularly popular models, each with many extensions [9, 23, 16].
However, research has largely concentrated on learning a consensus ranking based on a set of observed full, or partial rankings.
These models are thus inadequate for problems where preferences are expressed in other forms, and where inconsistencies exist in the observed preferences, such as  a beat b ,  b beat c , and  c beat a .
In this paper we address this problem by developing a  ex-ible probabilistic model over pairwise comparisons.
Pairwise comparisons are the building blocks of almost all forms of evidence about preference and subsume the most general models of evidence proposed in literature.
Our model can thus be applied to a wide spectrum of preference aggregation problems and does not impose any restrictions on the type of evidence.
The score-based approach that we adopt allows for rapid learning and inference, which makes the model applicable to large-scale problems with hundreds of thousands of preferences.
Experiments on a meta-search task with Microsoft s LETOR4.0 [14] data sets show that our model outperforms existing state-of-the-art methods designed speci -cally for this task.
We assume a set of M items X = {x1, ..., xM} and a set of N agents.
Each agent n generates a list of preferences for items in X.
The preferences can be in the form of full or partial rankings, ratings, relative item comparisons, or combinations of these.
All of these forms can be converted to a set of partial pairwise preferences, which in most cases will be neither complete nor consistent.
We use {xi (cid:2) xj} to denote the preference of xi over xj.
We allow the same pairwise preferences to occur multiple times, and use the pairwise count matrix Cn(i, j) : M   M to count the number of times preference {xi (cid:2) xj} is produced by the agent n, with Cn(i, j) = 0 if {xi (cid:2) xj} is not expressed by n. The most straightforward way to convert rankings into pairwise preferences is through binary comparisons.
Given two rankings rni and rnj assigned by n to xi and xj we set Cn(i, j) = I[rni < rnj ] where I is an indicator function, similarly Cn(j, i) = I[rni > rnj ].
This representation, however, completely ignores the strength of preference expressed by ranking {1, 200, 300} will have the same count matrix as the ranking {1, 2, 3}, but the  rst ranking expresses signi cantly more con dence about the ordering of the items than the second one.
To account for this we instead use Cn(i, j) = (rnj  rni)I[rni < rnj ] and Cn(j, i) = (rni rnj)I[rni > rnj].
In this form we assume that ranking {rni = 1, rnj = 200} is equivalent to observing the pairwise preference {xi (cid:2) xj} 199 times, whereas ranking {rni = 1, rnj = 2} is equivalent to observing {xi (cid:2) xj} only once.
This method of accounting for preference strength is not new and the reader can refer to [8, 11] for more extensive treatment of this and other approaches for converting rankings to pairwise matrices.
A ranking of items in X can be represented as a permutation of X.
A permutation   is a bijection   : X   {1, ..., M} mapping each item xi to its rank  (i), and xi =  1(i).
Given the observed (partial) preference instance consisting of count matrices {C1, ..., CN} the goal is to come up with a single ranking   of items in X that maximally satis es this instance.
Most preference aggregation problems  t this framework.
For instance in meta-search X is the set of documents retrieved for a given query.
Each search engine n generates either partial or complete ranking of the documents in X.
As before we can let Cn(i, j) = (rnj   rni)I[rni < rnj] if documents xi and xj are both ranked by the search engine n and set Cn(i, j) = 0 otherwise.
In collaborative  ltering X is the set of movies/songs/books etc., and an instance of the rank aggregation problem aims to infer the consensus ranking of movies given the (partial) ratings of N users [8,
 this problem.
We can de ne Cn(i, j) = (lni  lnj )I[lni > lnj ] where lni and lnj are the ratings assigned to movies xi and xj by user n. If n did not rate either xi or xj we set Cn(i, j) to 0.
Finally, we note that in some settings, there are multiple preference aggregation problems.
In meta-search for example, the same set of N search engines are the agents for multiple queries, returning a set of partial rankings of the M documents for each query.
We use S((cid:2)) and C((cid:2)) = {C ((cid:2))

 to denote the scores and pairwise counts for each query (cid:3).
Relevant previous work in this area can be divided into two categories: permutation based and score based.
In this section we brie y describe both types of models.
Permutation based models work directly in the permutation space.
The most common and well explored such model is the Mallows [18] model.
Mallows de nes a distribution over permutations and is typically parametrized by a central permutation   and a dispersion parameter     (0, 1]; the probability of a permutation   is given by: P ( | ,  ) =
 Z( ,  )  d( , ) (1) where d( ,  ) is a distance between   and  .
For rank aggregation problems inference in this model amounts to  nding the permutation   that maximizes the likelihood of the observed rankings.
For some distance metrics, such as Kendall s tau (the di erence between the proportion of item pairs in the correct versus incorrect order w.r.t.
 ), the partition function Z( ,  ) can be found exactly.
However,  nd-ing the central permutation   that maximizes the likelihood is typically very di cult and in many cases is intractable [21].
Recent work extends the Mallows model to de ne distributions over partial rankings [16].
Under partial rankings the partition function can no longer be computed exactly, so these authors introduced a new sampling approach to estimate it.
When M is large, however, this sampling approach is typically very slow, which makes the model impractical for many large scale online problems such as meta-search where aggregation has to be done very quickly.
Furthermore, both the proposed pairwise model and the sampling approach rely on the assumption that all pairwise preferences are consistent, which is often violated in real-world preference aggregation problems.
Several other generalizations of the Mallows model such as the CPS model [23], the Aggregation model [12] and the Cranking model [13] have recently been explored.
Due to space limitations we only discuss the CPS model here.
CPS de nes a sequential generative process, similar to the Plackett-Luce model described below, which draws the items without replacement to form a permutation; the probability of a given permutation   is: P ( | ,  ) = d(r,  )) (2) exp( 
 i=1
 r 1:i Z(i,  ) where  1:i is a set of permutations where the  rst i posi-
tions are  xed to  ; Z(i,  ) s are the normalizing constants   P ( | ,  ) = 1.
For several distance that ensure that metrics such as Spearman s rank correlation and footrule d(r,  ) as well as Kendall s tau, the summation over (M   i)!
elements, can be found in O(M 2), allowing the normalizing constants Z(i,  ) to be computed in polynomial time.
However, during inference one must still consider nearly all of the M !
possible permutations to  nd an optimal  .
A greedy approximation avoids this search, which reduces the complexity to O(M 2), but provides no guarantee with respect to the optimal solution.
r 1:i In general, due to the extremely large search space (typically M !
for M items) and the discontinuity of functions over permutations, exact inference in permutation based models is often intractable.
Thus one must resort to approximate inference methods, such as sampling or greedy approaches, often without guarantees on how close the approximate solution will be to the target optimal one.
As the number of items grows, the cost of  nding a good approximation increases signi cantly, which makes the majority of these models impractical for real world applications where data collections are extremely large.
The score based approach described in the next section avoids this problem by working with real valued scores instead.
In score based approaches the goal is to learn a set of real valued scores (one per item) S = {s1, ..., sM} which are then used to sort the items.
Working with scores avoids the discontinuity problems of the permutation space.
Early score based methods for rank aggregation in meta search are heuristic based.
For example, BordaCount [1] and median rank aggregation [7] derive the item scores by of pairwise wins.
In statistics a very popular pairwise score model is the Bradley-Terry [3] model:  Cn(i,j) (3)  
 P (Cn|S) = exp(si) i(cid:3)=j exp(si) + exp(sj) exp(si) where exp(si)+exp(sj ) can be interpreted as the probability that item xi beats xj in the pairwise contest.
In logistic form the Bradley-Terry model is very similar to another popular pairwise model, the Thurstone model [25].
Extensions of these models include the Elo Chess rating system [6], adopted by the World Chess Federation FIDE in 1970, and Microsoft s TrueSkill [5] rating system for player matching in online games, used extensively in Halo and other games.
Furthermore, the popular learning-to-rank model RankNet [4] is also based on this approach.
The key assumption behind the Bradley-Terry model is that the pairwise probabilities are completely independent of the items not included in the pair.
A problem that arises from this assumption is that if a given item xi has won all pairwise contests, the likelihood becomes larger as si becomes larger.
It follows that a maximum likelihood estimate for si is   [20].
As a consequence the model will always produce a tie amongst all undefeated items.
Often this is an unsatisfactory solution because the contests that the undefeated items participated in, and their opponents  strengths, could be signi cantly di erent.
To avoid some of these drawbacks, the Bradley-Terry model was generalized by Plackett and Luce [22, 17] to a model for permutations: P ( |s) =
 i=1
 exp(s 1(i)) j=i exp(s 1(j)) (4) The generative process behind the Plackett-Luce model assumes that items are selected sequentially without replacement.
Initially item  1(1) is selected from the set of M items and placed  rst, then item  1(2) is selected from the remaining M   1 items and placed second and so on until all M items are placed.
Note that here inference can be done quickly by doing simple gradient descent on scores, which is a clear advantage over most permutation based models.
The Plackett-Luce generalization relaxes the independence assumption of the Bradley-Terry model but this model is only applicable to consistent full or partial rankings (or consistent pairwise preferences) which signi cantly limits its application.
Recently several score based approaches have been developed to model the joint pairwise matrix [8, 11].
In these methods the preferences expressed by each of the N agents are combined into a single preference matrix Y : M   M , which is then factorized by a low rank factorization such as: Y = Se T   eST The resulting scores S are then used to rank the items.
The main drawback of this approach is that by combining all preferences into a single Y the individual user information is lost.
Consequently outlier agents with preferences substantially deviating from the consensus can signi cantly in uence both Y and the resulting scores.
A supervised score based rank aggregation approach was also recently introduced [15].
In this model the ground truth (a) (b) Figure 1: Figure 1(a) displays the count matrix with the contests won by each of the 3 items x1, x2 and x3 after their ranking {r1 = 1, r2 = 2, r3 = 3} is converted to pairwise counts using the rank di erence method.
A count is displayed in each (xi, xj) entry if ri < rj, and the size of the square represents the count magnitude.
Figure 1(b) shows the same matrix for the ranking {r1 = 30, r2 = 20, r3 = 1}.
preferences are used to create a pairwise constraint matrix, and the scoring functions is then optimized to satisfy as many of these constraints as possible.
The scoring function is based on a Markov Chain which makes the resulting constrained optimization problem non-convex.
To solve it the authors employ a number of approximations transforming the problem into a semide nite programming problem (SDP), which is solved using an SDP solver.
The main drawback of this approach is that it is computationally very intensive and requires expensive operations such as matrix inverse and constrained optimization.
In this section we develop a new score based model for pairwise preferences, the Multinomial Preference Model (MPM).
A key motivating idea behind our approach is that when absolute preferences such as rankings are converted into pairwise counts using the rank di erence approach described above, we interpret the resulting counts as conveying two forms of information: a binary preference, simply based on which item is ranked higher, and a con dence, based on the magnitude of the rank di erence.
Consider for example three items x1, x2 and x3 with ranks r1 = 1, r2 = 2 and r3 = 3 respectively.
Figure 1(a) shows the resulting count matrix after these ranks are converted to pairwise Item x1 is preferred to both x2 and x3 with preferences.
C(1, 2) = r2   r1 = 1 and C(1, 3) = r3   r1 = 2, x2 is preferred only to x3 with C(2, 3) = r3   r2 = 1, andx 3 is not preferred to any item.
Note that preference {x1 (cid:2) x3} where both items are at the extremes of the ranking has the largest rank di erence and consequently the biggest count.
Now consider the second example with partial ranking r1 = 30, r2 = 20 and r3 = 1 yielding the pairwise count matrix shown in Figure 1(b).
Comparing this with the previous example we see that the preference {x3 (cid:2) x1} with items at the extremes of the ranking also has the highest count, however in this case we are signi cantly more certain of it.
The count C(3, 1) = 29 is considerably higher than the highest count from the previous example, strongly indicating that x3 should be placed above x1.
The two examples demonstrate more evidence to conclude that xi (cid:2) xj is correct.
In MPM we model the count matrix C as an outcome of multiple draws from the joint consensus distribution Q over pairwise preferences de ned by the scores.
For instance in the second example above after observing C we can infer that P (x3 (cid:2) x1) should have the most mass under Q.
We use B to denote the random variable distributed as Q.
A draw from Q can be represented as a vector bij of length M  (M   1) (all possible pairs), with 1 on the entry corresponding to preference {xi (cid:2) xj} and zeros everywhere else, i.e., a one-hot encoding.
Given S we de ne the consensus distribution as follows: Definition 1.
The consensus distribution Q = {P (B = bij|S)}i(cid:3)=j is a collection of pairwise probabilities P (B = bij|S), where P (B = bij|S) = exp(si sj ) k(cid:2)=l exp(sk sl ) .
Q de nes a multinomial distribution over pairwise preferences.
Parametrization through S controls the shape of Q, lending considerable  exibility in distributions over preferences, which can be tailored to many di erent problems.
To generate the observed aggregated counts C we assume
 that T independent samples are drawn from Q where T = i(cid:3)=j C(i, j) so that: C(i, j) =
 t=1 I[B = b(t) ij ] ij ] is 1 if preference {xi (cid:2) xj} was sampled where I[B = b(t) on the t th draw and 0 otherwise.
Under this model the probability of the observed counts is given by:
 =


 i(cid:3)=j C(i, j)!
i(cid:3)=j i(cid:3)=j C(i, j)!
i(cid:3)=j C(i,j) P (B = bij|S)
 exp(si   sj) k(cid:3)=l exp(sk   sl) !C(i,j) (5) (a) (b) Figure 2: Graphical model representation of MPM and its   extension.
MPM, in the Bradley-Terry model there is no joint interaction amongst scores and pairs are modeled independently so a single preference is su cient to push the score to in nity.
In the base MPM model it is di cult to judge the model s con dence for a given score combination.
Aside from the relative score magnitudes, it is hard to measure the uncertainty associated with the score assigned to each item and the aggregate ranking that the scores impose.
Such a measure can be very useful during inference and can in uence the decision process.
For instance, it can be used to further  lter and/or reorder the items in the aggregate ranking.
Moreover, for problems where the accuracy is extremely important, the recommender system can inform the user if the produced ranking has high/low degree of uncertainty.
To address this problem we introduce a set of  variance  parameters   = { 1, ...,  M},  i > 0  i.
Each  i models the uncertainty associated with the score si inferred for the item xi.
The consensus distribution now becomes: P (B = bij|S,  ) =
 exp((si   sj)/( i +  j)) k(cid:3)=l exp((sk   sl)/( k +  l)) (7) Note that in MPM the pairwise probabilities depend on the entire item set X and the observed counts matrix is modeled jointly.
The magnitude of the score si is directly related to the count C(i, j).
When the scores are  tted via maximum likelihood the gradient of the log probability with respect to si is given by:   log(P (C|S))

  si C(i, j)   j =
 j

 C(j, i)
   log( !
(6) k(cid:3)=l esk sl )  si Note that when xi is strongly preferred to other items the  rst term in Equation 6 will be large leading to an increase in si.
This will in turn raise the probability of preferences where xi beats the other items.
Raising the probability for some preferences must simultaneously lower it for others since the probabilities always sum to 1.
The second term, the derivative of the partition function, accounts for this.
The scores thus compete with each other and the ones with the most positive/negative evidence get pushed to the extremes.
This is exactly the e ect we wanted to achieve because it will allow us to accurately model the count matrices as illustrated by the toy examples above.
In contrast with Note that the probability of xi beating xj decreases (increases) if the variance for either xi or xj increases (decreases).
Through   we can e ectively express the variance over the preferences for each item xi and translate this variance into uncertainty over pairwise probabilities.
Moreover, measures such as the average  ,   = 1 i=1  i, can be
 used to infer the variance for the entire aggregate ranking produced by the model.
In this setting  s can either be learned in combination with scores via maximum likelihood or set using some update rule.
The generative process for MPM with both S and   parameters is shown in Figure 2(a).
The assumption in MPM that the preferences generated by the N agents are independent and identically distributed is likely to be false in many domains.
Often one would expect to  nd preferences which either completely or partially deviate from the general consensus.
For example in collaborative  ltering most people tend to like popular movies such as Harry Potter and Forrest Gump, but in almost all cases one can  nd a number of outlier users who would give these movies low ratings.
Assuming that the preferences of the outliers have the same distribution as the consensus, as is especially if the outliers are severe.
To introduce the notion of outliers into our model we de- ne an additional set of  adherence  parameters   = { 1, ...,  N},  n   [0, 1].
Here we assume that each agent n has its own distribution over preferences Qn whose adherence to the global consensus distribution Q (see De nition 1) is described by  n.
Associated with each agent n is a random variable Bn   Qn, where we de ne Qn as: Qn = {P (Bn = bij|S,  ,  n)}i(cid:3)=j P (Bn = bij|S,  ,  n) =
 exp( n(si   sj )/( i +  j)) k(cid:3)=l exp( n(si   sj)/( i +  j)) (8) Note that if  n = 0, Qn becomes a uniform distribution indicating that the preferences of the n th agent deviate completely from the consensus (is an outlier), and will not be modeled by it.
Values between 0 and 1 indicate di erent degrees of agreement, with  n = 1 indicating complete agreement.
Hence, by introducing  n we make the model robust, allowing it to control the extent to which each agent s preferences are modeled by the scores, e ectively eliminating the outliers.
n=1 = = i(cid:3)=j Tn!
i(cid:3)=j Cn(i, j)!
In the generative process we now assume that at each of the T draws an agent n is picked at random and a preference is generated from Qn; Figure 2(b) demonstrates this process.
Under this process the probability of the observed instance C = {C1, ..., CN} is given by:





 where Tn = i(cid:3)=j Cn(i, j) is the total number of preferences generated by agent n. The preferences are modeled by a mixture of N multinomials that share the same score vector S but di er in the adherence parameter  n.
Both S and   can be e ciently learned by maximizing the log likelihood, and the consensus ranking can then be obtained by sorting the scores.
P (Bn = bij|S,  ,  n)
 e n(si sj )/( i+ j ) k(cid:3)=l e n(sk sl)/( k+ l )

 !Cn(i,j) i(cid:3)=j Cn(i, j)!
Cn(i,j) Tn!
i(cid:3)=j (9) n=1

 As noted above, in many preference aggregation problems the input typically consists of several preference instances {C((cid:2))}, and the goal is to infer a separate set of scores S((cid:2)) and variances  ((cid:2)) for each instance (cid:3).
The log likelihood of the entire corpus under the model is given by: L({C((cid:2))}|{S((cid:2))},{ ((cid:2))},  ) =




 log (cid:2) n=1
 T ((cid:2)) n !
i(cid:3)=j C((cid:2)) n (i, j)!
i(cid:3)=j P (B((cid:2)) n = bij|S((cid:2)),  
 ((cid:2)),  n) ((cid:3)) n (i,j) (10) Here   is shared across the instances and the original MPM model is recovered by setting     1.
When two of the three parameters {S((cid:2)),  ((cid:2)),  } are  xed it is not di cult to show that L is concave with respect to third parameter.
Therefore simple gradient descent can be used to e ciently  nd globally optimal setting.
Furthermore, even though joint optimization is no longer convex, in the experiments we found that by using gradient descent jointly good local optimum solutions can still be found very e ciently.
eters The above problem can be considered unsupervised, as the adherence parameters  , the consensus scores and the variances are inferred from the observed preferences.
This produces a predicted ranking for a given set of observed preferences by sorting the inferred scores, without ever utilizing any known consensus rankings or relevance labels in the data.
For problems such as meta search we often have access to labeled training instances {C((cid:2))} for which we have the ground-truth orderings {L((cid:2))} of the items {X ((cid:2))}.
In this section we describe an approach to incorporate this information into the Multinomial Preference Model.
Each  n models the adherence of the n th agent to the consensus.
For the labeled examples the consensus is explicitly given by L((cid:2)).
This allows us to exactly compute the adherence of each agent to the consensus based on the match between the preferences given by the agent and the ground truth rankings.
Using this we can set  n to the average distance between the preferences of n th agent and the ground truth labels:

 n )  n = |{C((cid:2))}| (cid:2) (11) where D is a normalized distance metric between preferences, such as Kendall s tau.
Note that as above,  n   1(  0) indicates that the preferences of agent n agree with (deviate from) the consensus across the training examples.
When training examples are available the inference proceeds as follows:  rst training examples are used to set  ; then keeping    xed the scores and the variances are optimized on the test examples by maximizing the log likelihood.
For meta search aggregation problem we use the LETOR [14] benchmark datasets.
These data sets were chosen because they are publicly available, include several baseline results, and provide evaluation tools to ensure accurate comparison between methods.
In LETOR4.0 there are two meta search data sets, MQ2007-agg and MQ2008-agg.
MQ2007-agg contains 1692 queries with 69623 documents and MQ2008-agg contains 784 queries and a total of 15211 documents.
Each query contains several lists of partial rankings of the documents under that query.
There are 21 such lists in MQ2007-agg and 25 in MQ2008-agg.
These are the outputs of the search engines to which the query was submitted.
In addition, in both data sets, each document is assigned one of three relevance levels: 2 = highly relevant, 1 = relevant and 0 = irrelevant.
Finally, each dataset comes with  ve precomputed folds with 60/20/20 splits for train-ing/validation/testing.
The results shown for each model are the averages of the test set results for the  ve folds.
The MQ2007-agg dataset is approximately 35% sparse, meaning that for an average query the partial ranking matrix of documents by search engines will be missing 35% of its entries.
MQ2008-agg is signi cantly more sparse with the sparsity factor of approximately 65%.
Precision










 BordaCount CPS-best
 Bradley-Terry Plackett-Luce  -MPM
 BordaCount CPS-best
 Bradley-Terry Plackett-Luce  -MPM



































































































































 The goal is to use the rank lists to infer an aggregate ranking of the documents for each query which maximally agrees with the held-out relevance levels.
To evaluate this agreement we use standard information retrieval metrics: Normalized Discounted Cumulative Gain (N@K) [10], Precision (P@K) and Mean Average Precision (MAP) [2].
Given an aggregate ranking  , and relevance levels L, NDCG is de- ned as:


 i=1
 log(i + 1) (12) N DCG( , L)@K = where L( 1(i)) is the relevance level of the document with rank i in  , and GK (L) is a normalizing constant that ensures that a perfect ordering has an NDCG value of 1.
The normalizing constant allows an NDCG measure averaged over multiple queries with di erent numbers of documents to be meaningful.
Furthermore, K is a truncation constant and is generally set to a small value to emphasize the utmost importance of getting the top ranked documents correct.
MAP only allows binary (relevant/not relevant) document assignments, and is de ned in terms of average precision

 k=1 P @k   L( 1(k))
 k=1 L( 1(k)) AP ( , L) = (13) where M is the number of documents; and P @k is the precision at k: Pk i=1 L( 1(i)) k P @k = MAP is then computed by averaging AP over all queries.
To compute P@k and MAP on the MQ datasets the relevance levels are binarised with 1 converted to 0 and 2 converted to 1.
All presented NDCG, Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website.
To investigate the properties of MPM we conducted extensive experiments with various versions of the model.
Through these experiments we found that the supervised   version (see Section 4.3) had the best performance; below we refer to this model as  -MPM.
Note that the training data are only used in  -MPM to set the values of the adherence parameters  .
Then the scores and the variances on each test query are found via maximum likelihood, and the scores are sorted to produce a predicted ranking.
This is similar to the framework used by the CPS model [23] where the training data is used to estimate the   parameter.
In all experiments we did not take the variances into account during the sort.
We compare the results of  -MPM against the best methods currently listed on the LETOR4.0 website,1 namely the BordaCount model and the best of the three CPS models (combination of Mallows and Plackett-Luce models) on each of the MQ datasets.
We also compare with the Bradley-Terry and Plackett-Luce models, as well as the singular value decomposition based method SVP [8].
These models cover most of the primary leading approaches in rank aggregation research.
The Bradley-Terry model is  t using the same count matrices Cn that are used for MPM.
For all models we found that 100 steps of gradient descent was enough to obtain the optimal results.
To avoid constrained optimization we reparametrized the variance parameters as  i = exp( i) and optimized  i instead.
This reparametrization was done for all the reported experiments.
Inference with MPM is extremely fast: a MATLAB implementation took   0.8 (  0.005 seconds per query) to make a full pass through Fold 1 (156 queries, 2874 documents) of the MQ2008-agg dataset, and   4.0 seconds (  0.012 seconds per query) to make a full pas through Fold 1 (336 queries,
 agg and MQ2007-agg datasets are shown in the top and bottom halves of Table 1 respectively.
For each data set we conducted a paired T-test between  -MPM and the best baseline at each of the 5 truncations for NDCG and precision as well as MAP, the statistically signi cant results at the 0.05 level are underlined.
From the table we see that the  -MPM models signi cantly outperforms the baselines on the MQ2007-agg dataset on both NDCG and MAP metrics.
On MQ2008-agg  -MPM is also the best model, signi cantly improving over the baselines on truncations 2-4 for NDCG and 2,3,5 for Precision.
1research.microsoft.com/en-us/um/beijing/projects/letor/ (14) The results for MPM together with the baselines on MQ2008-probabilistic matrix factorization model; statistically signi cant results are underlined.
Bradley-Terry Plackett-Luce








































 (a) (b) (c) Figure 4: Plots of NDCG at truncations 1, 5 and 10; in this setting all the missing ratings were repeatedly imputed by one of the constants shown on the x-axis and the rankings given by each method were evaluated using NDCG (Equation 15).
All the di erences are statistically signi cant.
Figure 3: Top row: normalized  , found by the supervised procedure outlined in Section 4.3, for training Fold 1 of MQ2007-agg.
Bottom row: learned   on the same Fold.
Here white = 1 and black = 0.
Figure 3 shows the adherence parameters   set based on the labeled training examples, together with the one learned in an unsupervised fashion by doing gradient descent on both S and   simultaneously.
From the  gure we see many similarities in the two vectors, indicating that the model is able to capture the notion of  outliers  which correlates closely with the training labels.
There are however a number of differences, such as the  rst three components being switched from on to o  in the learned  .
In our experiments we found that setting   using the training labels consistently produced better performance.
For collaborative  ltering experiments we used the Movie-Lens dataset, a collection of 100,000 ratings (1-5) from 943 users on 1682 movies.
This data set was chosen because it provides demographic information such as age and occupation for each user, as well as movie information such as genre, title and release year.
Each user in this data rated at least 20 movies but the majority of ratings for each movie are missing and the rating matrix is more than 94% sparse.
We formulate the preference aggregation as follows: given users  ratings the goal is to come up with a single ranking of the movies that accurately summarizes the majority of user preferences expressed in the data.
This ranking could be used as an initial recommendation for a new user who has not provided any ratings yet, as well as in a summary page.
Note that the aggregation can be further personalized by only aggregating over users that share similar demographic and/or other factors with the target user.
To convert ratings into preferences we can either sort them (resolving ties), to obtain a partial ranking for each user, or use the pairwise method to obtain the count matrices Cn, where Cn(i, j) = (lni   lnj )I[lni > lnj ] if movies xi and xj were rated by user n and 0 otherwise.
We use the sort method for the permutation based Plackett-Luce model and use the rating di erence method for the pair based Bradley-Terry and MPM models.
In collaborative  ltering and in most other applications the primary goal of aggregation is to recommend items to a new or existing user.
Items ranked in the top few positions are of particular interest because they are the ones that will typically be shown to the user.
Intuitively a top ranked item should have ratings from many users (high support) and most who rated it should prefer it to other items (strong preference).
Consequently NDCG suggests itself as a good metric to evaluate the rankers for this problem because of its emphasis on the top ranked items and the truncation level structure.
Unlike meta search the ground truth ratings are not available for most collaborative  ltering data.
To get around this problem we complete the rating matrix by imputing the missing ratings for every user.
We investigate two methods of imputing the ratings: a user independent method, where all the missing ratings are  lled in by the same value, and a user dependent method, where for every user n the missing ratings are predicted by a probabilistic matrix factorization model (PMF) [24].
The reason for choosing PMF was that it has shown excellent performance users that rated it (#u) and the total number of pairwise contests that the movie won (#won) and lost(#lost) across all users.
Bradley-Terry #u #won #lost Plackett-Luce #u #won #lost MPM #u #won #lost Pather Panchali Wallace & Gromit Casablanca Close Shave Rear Window .
.
.
Children of Corn Lawnmower Man 2 Free Willy 3 Kazaam Best of the Best 3









 Shawshank Red.
Usual Suspects Star Wars














 .
.
.
Barb Wire Robocop 3 Gone Fishin  Highlander III Ready to Wear on collaborative  ltering tasks such as the Net ix challenge.
After completing the rating matrix we compute the NDCG value for every user by sorting the items according to scores: N DCG( , Ln)@K = GK (Ln) i=1 log(i + 1)


 (15) Here   is the aggregated ranking obtained by sorting the items according to scores, and  1(i) is the index of the item with rank i in  ; Ln is a (completed) vector of ratings for user n. GK is the normalizing constant and represents the maximum DCG value that could be obtained for n: GK(Ln) =
 log(i + 1) (16)
 i=1 where   is a permutation of Ln with the ratings sorted from largest to smallest.
In this form if for a given user n an item in position i in   has a rating lower than the rating Ln( 1(i)) of the i th highest rated item by n, the corresponding term in the NDCG summation will decrease exponentially with the di erence between Ln( 1(i)) and Ln( 1(i)).
We use this metric (averaged across all users) to evaluate the performance of the models.
We compare the results of MPM to the Bradley-Terry and Plackett-Luce models, the two best baselines on the meta search task.
For all models we found that 100 steps of gradient descent was enough to reach convergence.
The NDCG results from the user dependent rating imputation method are shown in Table 2.
From this table we see that MPM outperforms the best baseline, Plackett-Luce, on all truncations except 1 with statistically signi cant gains at truncations 5-10.
This is likely due to the fact that in MPM the score magnitude is directly related to the number of observations.
The model has a strong bias to put movies with a large number of observations at the extremes of the ranking.
The NDCG plots for the user independent rating imputation method are shown in Figure 4.
The plots show NDCG at truncations 1, 5 and 10 for the three methods, when each of the values in {0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5} was used to  ll the missing ratings.
Here, the value 3.5 was chosen as the upper boundary because it is the average rating for the MovieLens data set.
A number of studies have shown that users tend





























 Star Wars Raiders of the L.
Godfather Silence of the L.
Shawshank Red.
.
.
.
Cable Guy Striptease Very Brady Jungle2Jungle Island of Dr.
to rate items that they like so the average of the observed ratings is typically signi cantly higher than the average of the the unobserved ones [19].
From the  gure we see that MPM signi cantly outperforms both the Bradley-Terry and Plackett-Luce models.
The di erences are especially large when low values are imputed for the missing ratings.
This indicates that the Bradley-Terry and Plackett-Luce models place items that were rated by very few users (low support) at the top of the list.
This causes the imputed ratings to dominate the numerator in the NDCG summation making the results very sensitive to the magnitude of the imputed rating.
This e ect can also be observed from Table 3 which shows the top and bottom 5 movies generated by each model together with statistics on the number of users that rated each movie and the number of pairwise contests lost and won by the movie (summed across all users).
For a given user n and movie i with rating lni we  nd the number of pairwise wins by counting the number of pairs {i, j} with lni > lnj; losses are found in a similar way.
From the table we see that the Bradley-Terry model places the movie Pather Panchali at the top of the list.
This movie is only rated by 8 out of 943 users and even though most users who rated it preferred it to other movies (#lost is low) there is still very little evidence that this movie represents the top preference for the majority of users.
Due to its pairwise independence assumption the Bradley-Terry model is always likely to place movies with few ratings near the top/bottom of the list.
The Plackett-Luce model partially  xes this problem by considering items jointly, and places the frequently rated movie Shawshank Redemption  rst.
However the model does not fully eliminate the problem, placing the very infrequently rated Wallace & Gromit (also ranked second by Bradley-Terry) in the second spot.
Part of the reason for this comes from the fact that the Plackett-Luce is a permutation based model and as such cannot model the strength of preferences, treating the preferences given for example by ratings {5, 2, 1} the same as {5, 4, 3}.
On the other hand for the Multinomial Preference Model we see that the position of the item is related to both the number of observed preferences and the strength of those preference.
The top three movies are all rated by more than 400 users and are strongly preferred by the majority of those users.
A more severe pattern can be observed for the bottom 5 (b) Figure 5: 5(a) shows the number of ratings versus the learned variance  i for each movie xi.
5(b) shows the rank for each movie obtained after sorting the scores versus the learned  i.
movies.
Both Bradley-Terry and Plackett-Luce place movies rated by less than 30 users in the bottom 5 positions labeling them the worst movies in the entire data set.
This selection has very little evidence in the data and has a high probability of being wrong if more ratings are collected.
For MPM all of the bottom 5 movies are rated by more than 50 users with 3 out of 5 movies rated by more than 90 users.
In addition to the retrieval accuracy we investigated the properties of the learned variance parameters  .
Figure 5(a) shows the learned variances together with the number of ratings for each movie.
Note that the variance is inversely proportional to the number of ratings so as the number of ratings increases the model becomes increasingly more certain in the preferences decreasing the variance.
In Figure 5(b) we plot   against the aggregate rank for each movie.
The general pattern is clear: the variance decreases towards the extremes of the ranking, indicating that the model is more certain in the movies that are placed near the top and near the bottom of the aggregate ranking.
As shown above, this is due to the fact that the movies at the extremes of the ranking have many comparisons, allowing accurate inference of strong negative or positive preferences.
The plot however, also shows outliers, which are the movies placed in the middle of the aggregrate ranking with low vari-ance/high con dence.
After further inspection we found that each such movie had many positive as well as negative preferences.
Examples of these include Sabrina (#u:190 #won:10190 #lost:12347), Mrs. Doubt re (#u:192 #won:13251 #lost:17551) and Ghost (#u:170 #won:11785 #lost:14452).
Note that all three movies were rated by more than 150 users and overall were neither strongly preferred nor strongly disliked.
The model thus correctly placed them in the middle of the ranking with strong con dence.
Moreover, note that it is impossible to express this con dence with scores alone since all the movies in the middle of the ranking have similar scores.
The variances thus provide additional information about the decisions made by the model during the aggregation, which could be very useful for post processing and evaluation.
We have introduced a new probabilistic model over preferences based on a multinomial generative process.
Preferences over items are expressed through real valued scores resulting in a convex optimization problem during inference which can be solved e ciently with standard gradient based techniques.
Modeling the general partial pairwise preferences makes the model applicable to a wide range of preference aggregation problems.
Empirically we have shown that our approach outperforms existing preference aggregation methods on two unrelated problems: meta search and collaborative  ltering.
Future work includes developing supervised extensions of the model that can more directly utilize the labeled training data available in problems such as meta search.
Another interesting direction is to investigate how the learned variances can be used to improve the  nal ranking.
Finally, we also plan to explore mixtures of the MPM distributions where each mixing component is parametrized by its own set of scores.
The mixture could be trained to learn di erent user preference types and used for personalized recommendation.
