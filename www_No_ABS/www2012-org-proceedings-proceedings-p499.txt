Search queries are often ambiguous (e.g.,  of ce  can be a workplace or a software) and/or underspeci ed (e.g.,  harry potter  can be a book, a  lm or the main character) [9].
To accommodate such different user needs (or user intents) given a query, research in search result diversi cation has received much attention recently (e.g., [1, 10, 14, 15, 24]).
TREC began a diversity task in the Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Web track in 2009 [6]1, and NTCIR concluded its  rst INTENT task for mining intents and selectively diversifying search results in 2011 [26]2.
These tasks evaluate a  at list of possibly diver-si ed but relevant documents, although other approaches such as dynamic presentation [2] may also be useful.
The main challenge in diversity evaluation is the balancing between diversity and relevance.
That is, we want search engines to cover as many intents as possible in the  rst Search Engine Result Page (SERP), but we also want as many relevant documents as possible.
Moreover, if we know that some intents for a given query are more likely than others, we might want to allocate more space within the SERP to the popular intents.
Furthermore, we probably want documents that are highly relevant to each intent rather than those that are partially relevant.
We need  good  evaluation metrics that re ect these requirements, in order to achieve the goal of providing a single  entry-point  SERP that is useful to as many users as possible.
In light of the above considerations, Sakai and Song [21] conducted an extensive study of different diversity metrics in terms of discriminative power [18] and intuitiveness, given the premises that intent probabilities and per-intent graded relevance assessments are available with the diversity test collection.
Discrimina-tive power is the proportion of statistically signi cant differences one can get out of a given experimental environment and therefore a measure of how reliable a metric is.
(Details will be given in Section 4.1.)
Sakai and Song also discussed intuitiveness by manually examining pairs of ranked lists, and showed that a family of metrics called D(cid:2)-measures [21] have several advantages over  -nDCG [8] and Intent-Aware (IA) metrics [1].
More speci cally, they highlighted the following limitations of  -nDCG and IA metrics:
 graded relevance (although intent probabilities were later incorporated [7, 9]).
tend to reward non-diversi ed systems that focus on popular intents [7], and have relatively low discriminative power.
tween 0 and 1.
Sakai and Song [21] also showed that Expected Reciprocal Rank (ERR) [5] and Graded Average Precision (GAP) [16] have low dis-criminative power both as traditional IR merics and as IA metrics for diversity evaluation.
1http://plg.uwaterloo.ca/~trecweb/ 2http://research.microsoft.com/en-us/people/ tesakai/intent2.aspx measures, the manual analysis by Sakai and Song [21] suggested that D(cid:2)-nDCG, a member of the D(cid:2)-measure family, may be less intuitive than  -nDCG when the intents are navigational.
Conversely,  -nDCG seemed less intuitive than D(cid:2)-nDCG when the intents are informational.
The original de nitions of navigational and informational intents by Broder [3] are: Navigational The immediate intent is to reach a particular site.
Informational The intent is to acquire some information assumed to be present on one or more web pages.
Thus, according to these de nitions, there is basically only one web page that the user wants to see when the intent is navigational, while the user may be happy to see many relevant pages (minus duplicate information) when the intent is informational.
 -nDCG works well for navigational intents precisely because of its  , which discourages retrieval of multiple relevant documents for each intent.
Whereas, D(cid:2)-nDCG works well for informational intents, precisely because nDCG (normalised Discounted Cumulative Gain) [12] was designed to cumulate pieces of information across multiple relevant documents.
According to a study by Jansen, Booth and Spink [11], over 80% of their Dogpile metasearch queries were informational, and about 10% were navigational, although multi-intent queries were outside the scope of their study.
The objective of this paper is to explore ways to incorporate the explicit knowledge of informational and navigational intents into diversity evaluation, and to design diversity metrics that are more intuitive than D(cid:2)-measures and  -nDCG.
We propose new diversity evaluation metrics called DIN(cid:2)-measures and P+Q(cid:2), as well as a simple method for comparing the intuitiveness of a given pair of metrics quantitatively.
Our main experimental  ndings are: (a) In terms of discriminative power [18] which re ects statistical reliability, the proposed metrics, DIN(cid:2)-nDCG and P+Q(cid:2), are comparable to intent recall and D(cid:2)-nDCG, and possibly superior to  -nDCG; (b) In terms of preference agreement (described in Section 4.2) with intent recall, P+Q(cid:2) is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises diversity; and (c) In terms of preference agreement with effective precision (also described in Section 4.2), DIN(cid:2)-nDCG is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises relevance.
Moreover, DIN(cid:2)-nDCG may be the most intuitive as a metric that considers both diversity and relevance.
In addition, we demonstrate that the randomised Tukey s Honestly Signi cant Differences test [4] that takes the entire set of available runs into account is substantially more conservative than the paired bootstrap test [18] that only considers one run pair at a time, and therefore recommend the former approach for signi cance testing when a set of runs is available for evaluation.
The remainder of this paper is organised as follows.
Section 2 discusses previous work related to this study and de nes existing diversity metrics.
Section 3 de nes our proposed metrics, and Section 4 describes how we evaluate diversity evaluation metrics in terms of discriminative power and intuitiveness.
Section 5 describes our experiments and reports on discriminative power and intuitiveness results.
Finally, Section 6 concludes this paper.
This section summarises prior art related to this study.
Section 2.1  rst de nes some traditional graded-relevance IR metrics on top of which diversi ed IR metrics have been designed.
Section 2.2 de nes these existing diversity metrics, namely,  -nDCG, IA metrics and D(cid:2)-measures.
Section 2.3 summarises previous  ndings from comparing different diversity metrics.
We  rst de ne a popular version of nDCG.
Let g(r) denote the gain value at rank r in a system s ranked list.
Following a popular practice, we let g(r) = 7 if the document at r is highly relevant (L3), g(r) = 3 if it is relevant (L2), and g(r) = 1 if it is partially relevant (L1).
Otherwise g(r) = 0.
The cumulative gain at rank r is de ned as cg(r) = (r) denote the (cumulative) gain at rank r in an ideal ranked list, obtained by listing up all relevant documents in descending order of relevance levels.
nDCG at document cutoff l can be de ned as: k=1 g(k).
Also, let g (r) and cg (cid:2)r     (cid:2)l (cid:2)l nDCG@l = r=1 g(r)/ log(r + 1) r=1 g (r)/ log(r + 1)   .
(1) Let J(r) = 0 if a document at rank r is nonrelevant to the query and J(r) = 1 otherwise.
Let C(r) = k=1 J(k).
Then the blended ratio at rank r, a graded-relevance version of precision, is de ned as: (cid:2)r C(r) +  cg(r) BR(r) = (2) where   (  0) is a user persistence parameter which is set to 1 throughout this study.
Then Q-measure [18, 20] is de ned as: r +  cg (r)   Q-measure = L(cid:3) r=1

 J(r)BR(R) (3) where L is the size of the ranked list.
Note that   = 0 reduces Q-measure to Average Precision.
Since we are interested in evaluation with a small document cutoff to evaluate the  rst SERP, we use a document-cutoff version of Q-measure, Q@l, which replaces the R with min(l, R) and the L with l in Eq.
3 to ensure that the maximum value achievable is 1.
As can be seen, both nDCG and Q are de ned based on cumu-lating gains discounted by ranks, and are inherently suitable for informational queries where more relevant documents means better user satisfaction.
But there also exist metrics that are more suitable for navigational queries, for which obtaining exactly one (highly) relevant document is suf cient.
ERR and P+ [17] are examples of such metrics.
ERR assumes that the user is dissatis ed with documents from ranks 1 to r   1 and is  nally satis ed with one at rank r, and that the satisfaction probabilities are proportional to the gain values.
Whereas, P+ assumes that the user stops examining the ranked list at the preferred rank (rp), which contains one of the most relevant documents within the ranked list and is closest to the top of the list.
In this paper, as we are interested in evaluation with a small document cutoff, we de ne rp after truncating the ranked list at the cutoff3.
Formally, P+ is de ned as [17]: rp(cid:3) +

 = C(rp) r=1 J(r)BR(R) (4)
 L1-relevant document at rank 1, and two L2-relevant documents at ranks 5 and 10, and one L3-relevant document at rank 20.
Then, in our setting, rp = 5 as we ignore the document at rank 20.
list, and P + = 0 otherwise.
Sakai [17] showed that metrics for navigational topics (such as P+) generally have lower discriminative power than those for informational topics (such as Q) as the former generally rely on fewer data points, i.e., retrieved documents treated as relevant.
Similarly, as was mentioned earlier, Sakai and Song [21] reported somewhat negative results for ERR in terms of discriminative power.
Q-measure, P+ and ERR can be seen as members of the Nor-malised Cumulative Utility (NCU) metrics family [20].
An NCU metric is de ned as a combination of the user s stopping probability distribution across document ranks and a utility function given a particular stopping rank.
Q-measure s probability distribution is uniform across all relevant documents; that of P+ is uniform across all relevant documents retrieved between ranks 1 and rp.
Both metrics measure the utility by means of the aforementioned blended ratio.
Whereas, both ERR and a rank-biased version of NCU [20] use stopping probabilities that depend on the number of relevant documents previously seen.
(cid:2)m  -nDCG is an extension of nDCG towards diversity evaluation.
It views both query intents and documents as sets of nuggets.
The main idea is to discount the gains according to  nuggets already seen  before discounting by ranks.
The strength of the novelty-biased discount is controlled by   (which is set of 0.5 throughout this paper as we use the of cial  -nDCG values from the TREC
 r is relevant to the nth nugget and 0 otherwise; let Cn(r) = (cid:2)r k=1 Jn(r), i.e., the number of documents observed within top r that contained the nth nugget.
Then the novelty-biased gain is n=1 Jn(r)(1   )Cn(r 1), where m is the de ned as NG(r) = total number of nuggets for the query.
 -nDCG is de ned by replacing the raw gain values in Eq.
1 with the novelty-biased gains.
Unlike the IA metrics and the D((cid:2))-measures discussed below, the original  -nDCG [8] can handle neither intent likelihood nor per-intent graded relevance.
Leenanupab, Zuccon and Jose [13] have proposed to adjust the value of   per topic, which may improve the intuitiveness of  -nDCG.
However, this approach does not change the above two limitations.
Given the intent probabilities P (i|q) for intent i and query q, i Pr (i|q) = 1, as well as per-intent graded relevance as-where sessments, an IA version of a given metric M is given by (cid:2)
 Pr (i|q)Mi (5) (cid:3) i where Mi is the per-intent (or local) version of metric M. For example, nDCG-IA is computed as follows: (1) De ne an ideal ranked list for each intent; (2) For each intent, compare the system output with the local ideal list and compute the local nDCG (nDCGi); (3) Finally, apply Eq.
5.
Both  -nDCG and IA metrics are imperfect metrics in that they are not guaranteed to range between 0 and 1: computing an ideal list for  -nDCG is NP-complete; it is generally not possible for a single system output to be ideal for all intents at the same time.
We now de ne the D-measures, which are free from the aforementioned limitations of  -nDCG and the IA metrics.
Given the intent probabilities Pr (i|q) and per-intent graded relevance assessments, where gi(r) is the gain value for document at rank r for intent i, we  rst de ne the global gain at rank r as: (cid:3) GG(r) = i Pr (i|q)gi(r) .
(6)   We then de ne a single ideal list (in contrast to the IA metrics which de ne an ideal list for every intent) by sorting all relevant documents by the global gain, and denote the ideal global gain at rank r by GG (r).
Finally, by replacing the raw gains of metrics such as nDCG and Q-measure with the global gains, D-measures (D-nDCG, D-Q etc.)
can be computed.
Note that there is no NP-complete problem involved here.
Sakai and Song [21] proposed to plot D-measures against intent recall (a.k.a.
subtopic recall [28], the proportion of intents covered by a ranked list) to visualise the trade-off between relevance and diversity.
In addition, to obtain a single-value metric, they proposed to compute the D(cid:2)-measures in addition: D(cid:2)-measure =  I-rec + (1    )D-measure (7) where   is a parameter.
Throughout this paper, we let   = 0.5: intent recall and D-nDCG/Q are highly correlated with each other and therefore D(cid:2)-nDCG/Q are not so sensitive to the choice of   [21].
To date, there are only a few studies that compared the reliability and usefulness of different diversity metrics.
Clarke et al. [7] compared diversity metrics including  -nDCG, a similar metric called Novelty and Rank-Biased Precision (NRBP) and an IA version of ERR (ERR-IA) in terms of discriminative power.
Somewhat surprisingly, their results suggested that intent recall, a simple set-based diversity metric, is more discriminative than others.
However, their experiments were limited to uniform intent probabilities and binary per-intent graded relevance assessments from the TREC 2009 Web track test collection [6].
Sakai and Song [21] compared D((cid:2))-measures with  -nDCG and a variety of IA metrics including ERR-IA, using uniform and nonuniform intent probabilities and graded per-intent relevance assessments added to the TREC 2009 Web collection.
They compared the metrics in terms of discriminative power and intuitiveness: their results suggested that D(cid:2)-measures are the most promising diversity metrics among the existing ones.
Also, as was mentioned earlier, their intuitiveness analysis suggested that while  -nDCG may sometimes be more intuitive than other metrics for navigational intents, D(cid:2)-measures may be more intuitive for informational intents, which is the main motivation of this study.
Moreover, as Sakai and Song s intuitiveness analysis was somewhat subjective and anecdotal, we propose a simple method for quantifying the relative intuitiveness of diversity metrics in this present study.
Using the Amazon Mechanical Turk framework and the TREC
 et al. [23] examined the predictive power of diversity metrics such as  -nDCG: if a metric prefers one ranked list over another, does the user also prefer the same list?
While our method for quantifying the relative intuitiveness of diversity metrics was partially inspired by the side-by-side approach of Sanderson et al., their work and ours fundamentally differ in the following aspects: (1) While Sanderson et al. treated each subtopic (i.e., intent) as an independent topic to examine the relationship between user preferences and metric preferences, we aim to measure the intuitiveness of metrics with respect to the entire (ambiguous or underspeci ed) topic in terms of diversity and relevance; (2) While Sanderson et al. used the Mechanical Turkers, we use very simple evaluation metrics that represent diversity or relevance as the gold standard in order to quantify the intuitiveness.
Sanderson et al. found that intent recall (called  cluster recall  in their paper) is as effective as other diversity metrics in predicting user preferences, despite its simplicity.
They also reported that diversity metrics agreed well with user analysis relied on only 18 navigational subtopics.
This section proposes new diversity metrics that rely on the explicit knowledge on whether an intent is informational or navigational.
Relevance to informational intent i
 Relevance to navigational Intent j



 First relevant doc for intent j ( at rank 2) First among the most relevant in ranked list (at rank 4) Doc at rank 1 Doc at rank 2 Doc at rank 3 Doc at rank 4 Doc at rank 5 Our  rst proposal, DIN-measures4, are identical to D-measures in the way the globally ideal ranked list is de ned.
The only difference is that systems do not receive any credit for returning multiple relevant documents for each navigational intent.
For example, consider a ranked list shown in Figure 1 for a query with exactly one informational intent i and exactly one navigational intent j. Suppose that, as the  gure shows, the document at rank 1 is L1-relevant to i, the document at rank 2 is L3-relevant to i and L1-relevant to j, and so on.
While existing diversity measures such as  -nDCG and D-nDCG consider the document at rank 4 as relevant to j, DIN-measures treats this document as nonrelevant to j because a relevant document has already been found at rank 2 for this navigational intent.
(As this example shows, even navigational intents may have multiple relevant documents in the test collection.)
Note that this is similar to how the binary-relevance Reciprocal Rank evaluates a ranked list: only the  rst relevant document matters.
Formally, let {i} and {j} denote the sets of informational and navigational intents for query q, and let isnew j(r) = 1 if there is no document relevant to the navigational intent j between ranks 1 and r   1, and isnew j(r) = 0 otherwise.
We rede ne the Global Gain as:
 (cid:3) (cid:3) isnew j(r)Pr (j|q)gj (r) .
Pr (i|q)gi(r) + (r) =
 i j (8) This should be compared with the original Global Gain (Eq.
6) which does not distinguish between informational and navigational intents.
It can be observed that GG DIN simply ignores redundant relevant documents for navigational intents.
Now, DIN-nDCG, for example, can be de ned as: (cid:2)l r=1 GG DIN (r)/ log(r + 1) (cid:2)l (r)/ log(r + 1) r=1 GG   .
(9) DIN -nDCG@l = Similarly, DIN-Q can be de ned as: DIN -Q@l =
 min(l, R) l(cid:3) r=1 J(r)DIN -BR(R) (10) where DIN -BR(r) = (cid:2)r (cid:2)r C(r) +   r +   k=1 GG DIN (k) k=1 GG (k)   .
(11) Note that only the system s global gains (numerators in Eqs.
9 and 11) have been modi ed, and the ideal global gains (denominators) remain unchanged.
This means that, unlike D-measures, the maximum possible value of a DIN-measure may be less than one.
We regard this as a cost of improving the intuitiveness of diversity metrics while keepling them simple.
Just like D-measures, DIN-measures can be combined with intent recall (or I-rec) to boost diversity relative to relevance (Recall Eq.
7).
We call the resultant metrics DIN(cid:2)-metrics.
In this paper,
 tional intents.
Figure 1: An example ranked list for a query with one informational intent and one navigational intent.
we examine DIN(cid:2)-nDCG and DIN(cid:2)-Q: the latter uses the cutoff version of Q-measure as was described in Section 2.1.
Our second proposal is to extend the IA approach of Agrawal et al. [1], so that two different metrics are used for informational and navigational intents, respectively.
A natural choice would be to use two metrics that share a similar user model: in this paper, we use Q@l for informational intents, and P+ for navigational intents, and call the resultant metric P+Q: P +Q@l = Pr (i|q)Qi@l + Pr (j|q)P + j .
(12) (cid:3) (cid:3) i j j Here, for example, Qi@l means Q@l computed for intent i based on an ideal list de ned particularly for this intent.
Recall also that, in this paper, the preferred rank rpj for each P + is de ned after truncating the ranked list at l, and therefore rpj   l holds (See Section 2.1).
Let us go back to Figure 1: P+Q is computed for this example as follows.
For the informational intent i, Q@l is computed by taking the relevant documents at ranks 1, 2 and 5 into account: recall that Q assumes that the user is equally likely to stop examining the ranked list at any of these three ranks.
Whereas, for the navigational intent j, we  rst determine rp: in this example, rp = 4 (not
 and the document at rank 4 is the  rst document whose relevance level is L3.
Then, P + for j is computed: recall that it assumes that the user is equally likely to stop examining the ranked list at ranks
 intent probabilities into account.
Note that, in this particular example, P+ is the same as Q for intent j and therefore P+Q is the same as Q-IA, the Intent-Aware version of Q.
Whereas, if the document at rank 2 in Figure 1 was (say) L3-relevant for j, then the document at rank 4 would be ignored and P+Q would be less than Q-IA.
Just like the IA metrics, the maximum value of P+Q is usually below 1: a single system output is almost never ideal for all intents at the same time.
Again, we regard this as a cost of improving the intuitiveness of diversity metrics while keepling them simple.
Furthermore, we consider combining P+Q with I-rec to empha-sise diversity, and call the resultant metric P+Q(cid:2).
Note that Sakai and Song [21] did not consider the combination of IA metrics with I-rec, although it is possible5.
dant, as  -nDCG already has a mechanism for emphasising diversity, namely the parameter  .
  t(z) = z w = (z1   z, .
.
.
, zN   z);  / count = 0; for b = 1 to B do { N where z and   are mean and standard deviation of z; obtained by sampling with replacement from w; w b = bootstrap sample of size N t(w b) = w b    b/ N where w b and  b are mean and standard deviation of w b; if( |t(w b)|   |t(z)| ) count + +; } ASL = count /B; foreach pair of runs (r1, r2) do count (r1, r2) = 0; for b = 1 to B do { create matrix X b whose row t is a permutation of row t of X  b = maxi x b max for every t   T ; i ; min  b = mini x b i where is the mean of i-th column vector of X b; i x b  b   min if( max foreach pair of runs (r1, r2) {  b > |x(r1)   x(r2)| where x(ri) is the mean of the column vector for run ri in X ) count (r1, r2) + +; } foreach pair of runs (r1, r2) do ASL(r1, r2) = count (r1, r2)/B; Figure 2: Algorithm for obtaining the Achieved Signi cance Level with the two-sided, paired bootstrap test given two runs r1 and r2, Topic Set T (|T| = N) and Metric M [18].
foreach pair of runs (r1, r2) do  (r1, r2) = |w b(cid:3)|;   = maxi,j  (ri, rj ); if( |t(w) b(cid:3)| is the B -th largest value in {|t(w) b|}) Figure 3: Algorithm for estimating the performance   required for obtaining a signi cant difference at   with the paired bootstrap test [18].
This section describes two methods for comparing the  goodness  of diversity metrics: discriminative power [18], which represents the statistical reliability of a metric, and intuitiveness test, which is our new proposal.
Given a test collection with a set of runs, discriminative power is measured by conducting a statistical signi cance test for every pair of runs and counting the number of signi cant differences.
In this paper, we use two different signi cance tests that rely on computer power and thereby require fewer assumptions than classical tests such as thet test.
The  rst is the paired bootstrap test which was the signi cance test originally used for measuring discrimina-tive power [18].
The second is the randomised version of Tukey s Honestly Signi cant Differences (HSD) test [4].
The bootstrap test is conducted for every run pair independently.
That is, the statistical signi cance at   (i.e., Type I error probability: note that this is unrelated to  -nDCG) for a run pair is tested without taking the other runs into consideration.
However, pairwise tests conducted in this fashion for k run pairs inevitably results in the family-wise error rate of 1   (1    )k [4]: this is the probability of detecting at least one signi cant difference for a pair of runs that are in fact no different from each other.
Note that this problem applies to all pairwise signi cance tests.
In contrast, the randomised Tukey s HSD test takes the entire set of runs into account to judge whether each run pair is signi cantly different or not.
Thus this test is naturally more conservative, i.e., researchers are less likely to  nd signi cant differences that are not  real.  We chose to use this test along with the original bootstrap test because of this advantage, and also because the two tests are similar in spirit in that they rely on modern computational power instead of making many statistical assumptions.
(Smucker, Allan and Carterette [25] recommend the randomisation test for pairwise signi cance testing.)
Figure 4: Algorithm for obtaining the Achieved Signi cance Level with the two-sided, randomised Tukey s HSD given a performance value matrix X whose rows represent topics and columns represent runs [4].
foreach pair of runs (r1, r2) with a signi cant difference at   do  (r1, r2) = |mean(r1)   mean (r2)|;   = mini,j  (r1, r2); Figure 5: Algorithm for estimating the performance   required for obtaining a signi cant difference at   with the ran-domised Tukey s HSD test.
Let tT,i denote the i-th topic from a topic set T of size N, and let M (t, rj) denote the value of a metric M for a topic t and a run rj.
A paired bootstrap test for a given run pair (r1, r2) can be performed as shown in Figure 2:  rst, a vector z of per-topic performances differences are obtained, and we set up a null hypothesis (H0) saying that these values were sampled from a distribution whose population mean is zero; then, to construct an empirical distribution that obeys H0, a shifted vector w is prepared and B bootstrap samples are obtained from it; then, for every trial b, the studentised statistic of z (i.e., t(z)) is compared with the corresponding statistic for the bootstrap sample (t(w b)); in this way, we obtain the Achieved Signi cance Level (ASL; a.k.a.
p-value), which represents how likely z would be under H0.
As in any other signi cance testing, H0 is rejected if ASL <  .
Based on the bootstrap test, Sakai [18] also showed how to estimate the performance   required in order to achieve statistical signi cance at   given the topic set size N: the algorithm is shown in Figure 3.
For example, if we have B = 1, 000 bootstrap samples and   = 0.05, we  nd the 50-th largest |t(w b)| and record the corresponding non-studentised mean |w  b| for every run pair.
These values represent the borderline  s between signi cance and nonsigni cance.
Finally, to be conservative, we take the maximum value observed across all run pairs.
In contrast to pairwise tests such as the bootstrap test, the main idea behind Tukey s HSD is that if the largest mean difference observed is not signi cant, then none of the other differences should be signi cant either.
Given a set of runs, the null hypothesis is that there is no difference between any of the systems.
Following Carterette [4], we perform randomised Tukey s HSD as shown in Figure 4: from a given matrix X whose element at (row i, column j) represents the performance of the j-th run for the i-th topic, we create B new matrices X b by permutating each row at random; then, for every run pair, we compare the performance   of this run pair with the largest performance   observed within X b.
Finally, the ASL value is computed in a way similar to Figure 2, but for each run pair.
foreach pair of runs (r1, r2) do foreach topic t do {  M1 = M1(t, r1)   M1(t, r2);  M2 = M2(t, r1)   M2(t, r2);  M GS = M GS (t, r1)   M GS (t, r2); if(  M1    M2 < 0 ){ // M1 and M2 disagree Disagreements + +; if(  M1    M GS   0) ) // M1 and MGS agree if(  M2    M GS   0) ) // M2 and MGS agree Correct 1 + +; Correct 2 + +; } } Intuitiveness(M1|M2, M GS ) = Correct 1/Disagreements; Intuitiveness(M2|M1, M GS ) = Correct 2/Disagreements; Figure 6: Algorithm for computing the intuitiveness of metrics M1 and M2 based on preference agreement with M GS .
Using the results of the randomised Tukey s HSD tests, we also try to estimate the performance   required to achieve a statistical signi cance at   for a given topic set size as shown in Figure 5: we simply take the smallest observed   from all the run pairs that were found to be signi cantly different.
It has been pointed out that discriminative power is not useful when, for example, the  metric  in question sorts systems alphabetically by the system name as this produces perfectly consistent judgments regardless of the data used (e.g., [22]).
However, we are interested in metrics that are strictly functions of a ranked list of items (i.e. system output) and a set of judged items (i.e. right answers).
We are not interested in a  metric  that knows that (say) one ranked list is from Google and that the other is from Bing, and uses this knowledge to say which is better than the other.
Moreover, note that, by means of discriminative power, we are measuring the robustness of metrics to variations in the choice of topics and therefore the reliability of experiments: we are not discussing which particular differences are actually perceptible to the user.
We do believe, however, that signi cance testing is one useful tool for making  real  improvements that may eventually add up to produce user-perceptible differences.
Sakai and Song [21] manually examined the actual ranked lists of documents to compare the intuitiveness of different diversity metrics, but this paper proposes a simple method for quantifying the intuitiveness.
Suppose we want to compare two diversity metrics M1 and M2.
We deliberately choose a simple Gold Standard metric M GS that should represent the intuitiveness, or the most important property that the diversity metrics should satisfy.
For the purpose of search result diversi cation, the two most important properties are diversity and relevance.
In the present study, we use intent recall (I-rec at l) to represent diversity, and effective precision (Ef-P at l) to represent relevance.
Here, Ef-P is the proportion of documents that are effectively relevant to at least one intent: for informational intents,  effectively relevant  just means relevant; for each nagi-vational intent, it means that only the  rst relevant document is counted as relevant and other  redundant  relevant documents are ignored.
For example, the Ef-P for the example shown in Figure 1 (Section 3) is 3/5 = 0.8, as the document at rank 4 is treated as nonrelevant.
Note that these two gold standards are set retrieval metrics based on binary relevance: as different diversity metrics employ different rank-based discounting and different ways to de- ne graded relevance, the gold standards should be chosen so that they are as agnostic to these differences as possible.
Given M1, M2 and M GS (i.e., either I-rec or Ef-P), we measure the relative intuitiveness of the two diversity metrics in terms of preference agreement with M GS as shown in Figure 6.
In this pseu-docode, Disagreement is the number of ranked list pairs for which the two diversity metrics disagreed with each other as to which list is better; Correct 1 is the number of ranked list pairs from the disagreements, for which M1 agrees with the  correct judgment  of M GS , and so on.
In the pseudocode, note that if  M GS is zero (i.e., the gold standard says that the two ranked lists are tied), this case is counted as a  correct  case.
We found that ties actually occur quite often with  crude  metrics such as I-rec.
Note also that we focus on the disagreements between M1 and M2 rather than the entire set of ranked list pairs.
(We have a total of 4,560 pairs: 24 topics   190 run pairs.)
This is because we already know that different diversity metrics are generally highly correlated to one another [21].
Thus, Figure 6 enables us to discuss  which metric is more intuitive than the other  assuming that the gold standard truly represents intuitiveness.
We can expect metrics such as D(cid:2)-measures, DIN(cid:2)-measures and P+Q(cid:2) to show good intuitiveness results when I-rec is used as the gold standard, since these metrics directly depend on I-rec by means of Eq.
7 and the like.
Also, we can expect DIN(cid:2)-measures and P+Q(cid:2) to show good results when Ef-P is used as the gold standard, since these metrics all rely on the basic idea of ignoring redundant documents for navigational intents6.
In short, it would not be surprising if our proposed metrics do well in our intuitiveness experiments.
The contribution here, however, is that we are able to quantify exactly how much some of these metrics outperform the other metrics, including the popular  -nDCG.
The above method considers diversity (I-rec) and relevance (Ef-P) one at a time.
However, what we really want are intuitive evaluation metrics that consider both.
We therefore extend the algorithm shown in Figure 6 to handle two gold-standard metrics M GS and M GS (which in this paper are I-rec and Ef-P): in this case, Correct 1 is incremented only if M1 agrees with M GS and with
 , and so on.
For evaluating different diversity metrics in terms of discrimina-tive power and intuitiveness, we used the graded-relevance version of the TREC 2009 Web track diversity test collection with Category A runs [6].
The original TREC data has binary per-intent relevance assessments, but this version contains L3 (highly relevant), L2 (relevant) and L1 (partially relevant) documents for each intent, which were de ned based on judgements from multiple assessors [21]7.
From the of cial 50 topics, we selected those that had at least one navigational subtopic (i.e., intent), which resulted in 24 topics.
Some statistics of this data set are shown in Table 1.
As shown in the table, our data set contains 68 informational and 31 nagivational intents, with a total of 2,635 relevant documents for the informational intents and 198 for the navigational intents; we use the uniform and nonuniform intent probabilities of Sakai and
  rst relevant document for each navigational document as relevant, P+Q(cid:2) goes down to the preferred rank rp as was discussed in Section 3.2.
on request.
#Topics #Intents Mean and Range of #Intents/topic Intent probabilities for n intents #Relevant Mean and Range of #Intents/document #Runs Table 1: Test collection statistics.
Approx.
one billion Web pages (ClueWeb09).
24, with at least one navigational intent (17 faceted; 7 ambiguous).
99, with at least one relevant document (68 informational; 31 navigational) 4.1 [1, 6] (all); 2.8 [1, 5] (informational); 1.3 [1, 3] (navigational) across 24 topics.
Uniform: j-th intent has the probability 1/n; Nonuniform: j-th intent has the probability 2n j+1/


 1.19 [1, 4] across 2,223 unique relevant summed across topics.
k=1 2k.
(cid:2)n Song [21], and the 20 sampled runs from the same study, which gives us 190 run pairs.
Following previous work [17, 18, 20, 21], we used B = 1, 000 for the bootstrap tests.
On the other hand, as we had no previous experience in using the randomised Tukey s HSD, we determined the value of B through a preliminary experiment: Smucker, Allan and Carterette [25] used B = 100, 000 for their pairwise ran-domisation test but we thought that a fewer number of trials may suf ce.
Figure 7(b) shows the ASL curves [18] for D(cid:2)-nDCG with the uniform intent probabilities based on the randomised Tukey s HSD test for different values of B: the y-axis represents the ASL and the x axis represents the 190 run pairs sorted by the ASL.
The graphs are somewhat cluttered but that is exactly the point: for example, the curve for B = 5, 000 almost completely overlaps with that for B = 10, 000.
Based on these results, we use B = 5, 000 for randomised Tukey s HSD.
For reference, Figure 7(a) shows a similar set of graphs for the bootstrap test: it can be observed that B = 1, 000 is probably suf cient, and that much lower ASLs are obtained compared to Tukey s HSD.
This demonstrates the fact that pairwise tests may  nd  signi cant differences that are not substantial.
Note that the family-wise error rate (See Section 4.1) for any pairwise tests at   = 0.05 with 190 run pairs is 1   0.95190 > 0.9999.
(a) Bootstrap




















































































 (b) Randomised Tukey s HSD












































































































 Figure 7: Effect of B on the accuracy of the ASL curve for D(cid:2)-nDCG@10 (Uniform Pr (i|q)).
y-axis: ASL (i.e., p-value); x-axis: run pairs sorted by ASL.
For computing all evaluation metrics, we used a version of the NTCIREVAL toolkit [19]8.
The only exception was  -nDCG: we
 tools/ntcireval-en.html.
at http://research.nii.ac.jp/ntcir/ used the of cial  -nDCG values from TREC (with   = 0.5) as implementing this metric requires a greedy approximation of the ideal ranked list [8].
For all metrics, we used the document cutoff of l = 10 as we are interested in evaluating the  rst SERP, the entry-point page for different user intents.
Figures 8 and 9 show the ASL curves of some selected diversity metrics, based on the bootstrap test and the randomised Tukey s HSD, respectively.
Parts (a) of these  gures show the results with the uniform intent probability distribution:  -nDCG and I-rec are included here as these two metrics do not utilise intent probabilities.
Parts (b) of these  gures show the results with the nonuniform distribution: D-nDCG, DIN-nDCG and P+Q are included here to highlight the effect of combining these metrics with I-rec and thereby obtaining D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2).
We want metrics that are discriminative, i.e., those that are closer to the origin in the  gures.
Tables 2 and 3 cut Figures 8 and 9 in half at   = 0.05 to quantify discriminative power and the performance   required for achieving statistical signi cance with 24 topics.
For example, Tables 2(a) shows that the discriminative power of I-rec according to the bootstrap test at   = 0.05 is (100/190) = 52.6% (i.e., 100 signi cantly different run pairs were found) and the   required for achieving statistical signi cance is around 0.20.
First, by comparing the bootstrap and the randomised Tukey s HSD results (i.e., Figure 8 vs.
Figure 9 and Table 2 vs. Table 3), it can be observed that:   The relative performances of the different metrics are generally similar with these two tests, although it is not clear why P+Q does relatively well with the randomised Tukey s HSD (Figure 9(b)) but not with the bootstrap test (Figure 8(b)).
  The randomised Tukey s HSD is substantially more conservative than the bootstrap test, as it is clear from the contrast between Figures 8 and 9.
For example, at   = 0.05, the discriminative power of I-rec according to the bootstrap is
 Tukey s HSD is only 26.8% (Table 3(a)): that is, about half of the signi cant differences obtained with the bootstrap test are not signi cant with the randomised Tukey s HSD.
(This set of signi cant differences obtained by the randomised Tukey s HSD is a true subset of the set of signi cant differences obtained by the bootstrap test.)
  The performance  s as estimated with the randomised Tukey s HSD are similar to the corresponding values based on the bootstrap test.
For example, with the uniform setting, the performance   required for achieving a statistical signi -cance with P+Q(cid:2) given 24 topics is 0.15 according to both tests (Tables 2(a) and 3(a)).
D#-nDCG DIN#-nDCG
 I-rec (cid:626)-nDCG

















































































 (b) Bootstrap; nonuniform D#-nDCG DIN#-nDCG
 D-nDCG DIN-nDCG








































































































 Figure 8: ASL curves based on the bootstrap test.
y-axis: ASL (i.e., p-value); x-axis: run pairs sorted by ASL.
(a) Randomised Tukey s HSD; uniform D#-nDCG DIN#-nDCG
 I-rec (cid:626)-nDCG

















































































 (b) Randomised Tukey s HSD; nonuniform D#-nDCG DIN#-nDCG
 D-nDCG DIN-nDCG


















































































 Figure 9: ASL curves based on the randomised Tukey s HSD.
y-axis: ASL (i.e., p-value); x-axis: run pairs sorted by ASL.
The above observations suggest that the randomised Tukey s HSD is a good alternative to the pairwise bootstrap test for the purpose of comparing evaluation metrics.
Also, given a set of available runs, researchers are encouraged to make use of all of these runs in sig-ni cance testing, as focussing on a particular set of runs (by means of a pairwise test) may often lead to wrong conclusions [4].
Next, by comparing the different metrics in terms of discrimina-tive power as shown in Figures 8 and 9 and Tables 2 and 3, it can be observed that:   DIN(cid:2)-nDCG and P+Q(cid:2) are comparable to I-rec and D(cid:2)-nDCG in terms of discriminative power (Figures 8 and 9)9.
   -nDCG may be slightly less discriminative than the above best metrics (Figures 8(a) and 9(a)).
This difference is evident particularly when   is large (e.g.,     0.1).
(Again, this   is the Type I Error probability, not the redundancy parameter of  -nDCG.)
  Combination with I-rec dramatically boosts the discrimina-tive power of all diversity metrics (e.g., compare P+Q(cid:2) with P+Q in Figures 8(b) and 9(b));
 inative power, because it is based on fewer data points (i.e., documents treated as relevant) than D(cid:2)-nDCG when not identical to it.
Table 2: Discriminative power / performance   of diversity metrics based on the bootstrap test at   = 0.05 .
Table 3: Discriminative power / performance   of diversity metrics based on the randomised Tukey s HSD test at   = 0.05.
(a) uniform
 I-rec P+Q(cid:4)
 D(cid:4)-nDCG
 DIN(cid:4)-nDCG 50.0% 0.15 D(cid:4)-Q
 DIN(cid:4)-Q
  -nDCG

 D-nDCG
 DIN-nDCG





 (a) uniform D(cid:4)-nDCG
 D(cid:4)-Q
 DIN(cid:4)-nDCG 26.8% 0.17
 I-rec P+Q(cid:4)
 DIN(cid:4)-Q
  -nDCG

 D-nDCG


 DIN-nDCG



 (b) nonuniform P+Q(cid:4)
 D(cid:4)-nDCG
 DIN(cid:4)-nDCG 50.5% 0.16 D(cid:4)-Q
 DIN(cid:4)-Q

 D-nDCG
 DIN-nDCG





 (b) nonuniform D(cid:4)-nDCG
 P+Q(cid:4)
 D(cid:4)-Q
 DIN(cid:4)-nDCG 25.8% 0.16 DIN(cid:4)-Q



 D-nDCG
 DIN-nDCG



 ---

---Table 4: Comparison of signi cantly different run pairs (ran-domised Tukey s HSD at   = 0.05; uniform setting).
P+Q(cid:4)



  -nDCG D(cid:4)-nDCG DIN(cid:4)-nDCG



 I-rec  -nDCG D(cid:4)-nDCG DIN(cid:4)-nDCG   D-Q and DIN-Q are the least discriminative metrics among the ones we examined (e.g., see bottom of Table 3(b)).
Moreover, in the tables, DIN(cid:2)-Q is never more discriminative than DIN(cid:2)-nDCG, and D(cid:2)-Q is never more discriminative than D(cid:2)-nDCG.
(For these reasons, DIN((cid:2))-Q and D((cid:2))-Q are not shown in the two  gures.)
The above observations suggest that DIN(cid:2)-nDCG and P+Q(cid:2) are promising as metrics that explicitly takes into account whether each intent is informational or navigational.
The high discriminative power comes mostly from the simple I-rec metric.
Note, however, that these results only suggest that DIN(cid:2)-nDCG and P+Q(cid:2) are statistically reliable and consistent: they say nothing about whether they are right or wrong.
Hence we discuss the intuitiveness of these metrics in Section 5.4.
Based on the above results, we hereafter focus our attention to DIN(cid:2)-nDCG and P+Q(cid:2) as well as D(cid:2)-nDCG and  -nDCG for comparison purposes.
Table 4 provides a further analysis of some of the results from Table 3(a), i.e., the randomised Tukey s HSD results with the uniform setting.
The table shows the degree of overlap between the sets of signi cantly different pairs for I-rec,  -nDCG, D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2).
For example, it can be observed from the rightmost column that I-rec and P+Q(cid:2) have 48 run pairs in common, and that these two metrics obtained 3 + 48 = 51 signi cant differences and 48 + 2 = 50 signi cant differences, respectively.
(These correspond to the discriminative power values of 51/190 = 26.8% and 50/190 = 26.3% in Table 3(a).)
The main message this table (uniform setting).
Values higher than 0.9 are shown in bold for convenience.
I-rec  -nDCG D(cid:4)-nDCG DIN(cid:4)-nDCG --- -nDCG D(cid:4)-nDCG DIN(cid:4)-nDCG .74/.80 .91/.93 .79/.83 --.92/.94 .80/.94 .99/.99 -P+Q(cid:4) .94/.93 .78/.84 .95/.95
 Table 6: Intuitiveness based on preference agreement.
For each metric pair, the higher score is shown in bold.
Disagreements are shown in parentheses.
(a) Baseline: I-rec ( diversity ) D(cid:4)-nDCG DIN(cid:4)-nDCG .597/.995 .607/.996  -nDCG D(cid:4)-nDCG (236) -DIN(cid:4)-nDCG -(242)
 (19) -P+Q(cid:4)
 (246) .908/1 (120) .907/1 (118) P+Q(cid:4) .646/.654 (246) .800/.625 (120) .831/.593 (118) P+Q(cid:4) .390/.654 (246) .708/.625 (120) .737/.593 (118) (b) Baseline: Effective Precision ( relevance ) D(cid:4)-nDCG DIN(cid:4)-nDCG .623/.733 .616/.748  -nDCG D(cid:4)-nDCG (236) -DIN(cid:4)-nDCG -(242) .474/.842 (19) (c) Baseline: I-rec AND Effective Precision D(cid:4)-nDCG DIN(cid:4)-nDCG .398/.729 .397/.744  -nDCG D(cid:4)-nDCG (236) -DIN(cid:4)-nDCG -(242) .474/.842 (19) conveys is that these metrics are quite similar to each other when averaged across topics.
Table 5 shows the Kendall s   and (the symmetric version of)  ap proposed by Yilmaz, Aslam and Robertson [27] for ranking the 20 runs by the aforementioned  ve metrics.
 ap compares the similarity of two run rankings based on pairwise swaps just like   , but is more  top-heavy.  It can be observed that the rankings by I-rec, D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2) all resemble each other, quite naturally as the  (cid:2)  represents linear combination with I-rec.
Perhaps what is more interesting is that the ranking by DIN(cid:2)-nDCG and P+Q(cid:2) are actually identical as indicated by the   and  ap values of 1, despite the different approaches they employ (See Section 3).
Tables 4 and 5 show how similar the  ve diversity metrics are on average; below we focus on individual cases where they differ.
Table 6 show the intuitiveness scores for  -nDCG, D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2) computed using the preference agreement algorithm shown in Figure 6: Part (a) uses I-rec as the gold-standard and therefore represents how the diversity metrics favour diversi- ed results like they should; Part (b) uses Ef-P as the gold-standard and therefore represents how they favour the result with more relevant documents like they should (while ignoring redundant relevant documents for navigational intents).
Part (c) computes the intuitiveness scores by requiring that the diversity metrics agree with both I-rec and Ef-P. For example, Table 6(a) shows that, if we compare  -nDCG and D(cid:2)-nDCG in terms of diversity, there are
 is only .597, that for D(cid:2)-nDCG is .995.
This means that, given a pair of ranked lists for which  -nDCG and D(cid:2)-nDCG disagree with each other, D(cid:2)-nDCG is far more likely to agree with I-rec on the preference than  -nDCG.
The relative results can be summarised as follows:   In terms of diversity (Part (a)), D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2) outperform  -nDCG; P+Q(cid:2) outperforms D(cid:2)-nDCG and DIN(cid:2)-nDCG; and therefore P+Q(cid:2) is the winner (note that P+Q(cid:2) agrees 100% with I-rec in the rightmost column of Table 6(a)).
  In terms of relevance (Part (b)), D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2) outperform  -nDCG; D(cid:2)-nDCG outperforms P+Q(cid:2); DIN(cid:2)-nDCG outperforms D(cid:2)-nDCG and P+Q(cid:2); and therefore DIN(cid:2)-nDCG is the winner.
  In terms of both diversity and relevance (Part (c)), D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2) outperform  -nDCG; D(cid:2)-nDCG outperforms P+Q(cid:2); DIN(cid:2)-nDCG outperforms D(cid:2)-nDCG and P+Q(cid:2); and therefore DIN(cid:2)-nDCG is the winner.
Recall that these results should be regarded with a grain of salt:  rst, it is not surprising that D(cid:2)-nDCG, DIN(cid:2)-nDCG and P+Q(cid:2) behave similarly to I-rec (See Eq.
7); second, it is not surprising that DIN(cid:2)-nDCG behaves similarly to Ef-P, since they both look at the  rst retrieved relevant document for every navigational intent.
However, note that our results say much more: they suggest that P+Q(cid:2) may be the most intuitive  diversity-oriented  diversity metric, and that DIN(cid:2)-nDCG may be the most intuitive  relevance-oriented  diversity metric and the best metric that takes both diversity and relevance into account.
Moreover, they quantitatively show the advantages of these metrics over  -nDCG.
Note also that our simple approach to discussing intuitiveness based on preference agreement is applicable to any pair of evaluation metrics provided that an appropriate gold-standard metric can be de ned.
In this study, we proposed new evaluation metrics called DIN(cid:2)-measures and P+Q(cid:2) which incorporate the explicit knowledge of informational and navigational intents into diversity evaluation.
Like Intent-Aware metrics and D(cid:2)-measures, these metrics can handle intent probabilities and per-intent graded relevance.
(Recall that  -nDCG can handle neither.)
We also proposed a simple method for comparing the intuitiveness of a given pair of metrics quantitatively.
Our main experimental  ndings are: (a) In terms of discriminative power, the proposed metrics, DIN(cid:2)-nDCG and P+Q(cid:2), are comparable to intent recall and D(cid:2)-nDCG, and possibly superior to  -nDCG; (b) In terms of preference agreement with intent recall, P+Q(cid:2) is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises diversity; and (c) In terms of preference agreement with effective precision, DIN(cid:2)-nDCG is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises relevance.
Moreover, DIN(cid:2)-nDCG may be the most intuitive as a metric that considers both diversity and relevance.
In addition, we demonstrated that the randomised Tukey s Honestly Signi cant Differences test (described by Carterette [4]) that takes the entire set of available runs into account is substantially more conservative than the paired bootstrap test that only considers one for signi cance testing when a set of runs is available for evaluation.
Finally, limitations to the present study include the following:
 imperfect metrics, in that they do not range fully between 0 and 1.
However, we regard this as a cost of taking into account the distinction between informational and navigational intents and yet keeping the metrics simple to undertand and to compute.
lieve that our approach and user-based studies such as the work by Sanderson et al. [23] are complementary.
Note that it is not straightforward to conduct a user study for diversity metrics, as a diversi ed SERP is intended for a population of users sharing the same query but having different intents, as opposed to a small group of participants.
to expand our experiments by adding graded relevance assessments to the latest TREC web diversity test collections and/or utilising the NTCIR INTENT test collections which come with intent probabilities obtained through assessor voting [26].
of whether each intent is informational or nagivational, there is another aspect that is available in the TREC diversity test collections which we did not consider, namely, the distinction between ambiguous and faceted topics [6].
Clarke, Kolla and Vechtomova [9] have brie y discussed this in the context of extending  -nDCG.
However, the challenge here would be how to keep the evaluation metric simple and intuitive.
