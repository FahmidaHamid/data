The Markovian model for Web user behavior posits that when a user is browsing a Web page P , the next page she visits depends only on P and not on how the user arrived at P .
This assumption is central to some of the most widely used Web algorithms and systems including Google s PageRank [15] and other forms of link analysis [13].
Markovian user models have also been proposed for advertising [1] and in fact, many systems used for behavioral targeting of advertisements use an even simpler zeroth order model in which the next page visited is drawn from a probability distribution that is independent of the user s current position.
The central question we examine in this paper is: how valid are these simple (Markovian, as well zeroth order) models?
Using data from a variety of different sources, we establish that the Markovian model (and by corollary the zeroth order model) is too simplistic to capture the behavior of Web users.
Our data includes browser trails on  Flavio Chierichetti was supported in part by the NSF grant CCF-
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
large networks including Yahoo!
as well data from user behavior within a page, using mouse and eye-tracking.
In other words, user behavior on the Web is rather more intricate than the simple models underlying the most commonly used algorithms on the Web.
Given a set of hyperlinked Web pages, we may view each page as a state in a Markov chain [12].
Each hyperlink is a potential transition of the Markov chain modeling a user following that hyperlink.
The transition probabilities of the Markov chain represent the probabilities of the user following each hyperlink, if she is at the page containing that link.
In some of the settings we study, we will naturally model a slot within a page as a state, when considering user actions within the page.
In these settings we study the drift of the user s mouse or eyes over the page as transitions between states.
Here too (as we will detail) one could naturally model the user s behavior using a Markov model.
Given this view of pages visited as states in a Markov chain, we next extend the notion of a Markov chain (in a manner routine in probability theory) to a richer class of user models that includes as special cases the Markovian and zeroth order models.
Consider the probability that a user at stage (page) i goes next to page j.
If for all j, the transition probability is independent of i, then we say the user model is zeroth order.
If instead it is uniquely determined by i then we say the user model is Markovian; we will sometimes refer to this as the  rst order model.
Thus in the Markovian model, the probability of going from i to j varies with i, but depends only on i and not on how the user arrived at state i.
More generally for k > 1 we say the user model is of order k if it is the smallest integer such that the probability of going to page j is determined by the sequence sk, sk 1, .
.
.
, s2, i of the last k states (pages) visited by the user.
Thus in a second order user model, the transition probabilities depend on the current page i as well as the previous page the user visited prior to arriving at i.
Intuitively the larger k is, the greater the in uence of the user s historical trail on her behavior.
The zeroth order model even ignores which pages are linked to which others; it simply views the user s next page as drawn from a  xed probability distribution independent of her current position.
The ( rst order) Markovian model does take into account the user s current state (page) i and thus can take into account the links out of i; it ignores states visited prior to i and in this sense may be considered memoryless.
Classic Web algorithms such as PageRank use this model.
Some prior work [20, 14] offers weak evidence in support of users  behavior being Markovian.
We know of no prior work that has examined whether the assumption implicit in PageRank and other algorithms.
Aside from these applications, we believe the question  how memoryless are users  browsing habits  is of interest in its own right, as an important step in understanding user behavior.
We bring together two ingredients in addressing this question.
First, we study a number of data sets each consisting of a large number of user trails; in some of these data sets the user trails visit Web pages.
In others, each trail follows the user through other discrete steps abstracted as states.
Second we appeal to a powerful result from statistics that considers the error in a model s power to explain given data as a function of increasing k.
Informally the result [16] asserts that the magnitude of the error undergoes a sharp drop from order k  1 to the true order k. This enables us to ask the question: what is the smallest k at which a given set of user trails is adequately explained by the model?
The rest of the paper is organized as follows.
In Section 3 we introduce our notation and the basics of Markov chains.
We prove that several natural questions are computationally or information-theoretically hard to answer in Sections 4.1 and 5.2.
The description of our novel algorithms for optimally estimating higher order Markov chains is given in Section 4.
Finally we present our extensive experimental study using a diverse collection of large scale Web data sets in Sections 7 and 8.
Estimating the order of a Markov chain has been extensively studied by the statistics community [7, 16].
Multiple order estimators that are asymptotically consistent as the data size tends to in nity are known [7]; however, to the best of our knowledge, their  nite sample convergence has not been investigated carefully.
In fact, in Section 4.1 we show that the number of samples required for distinguishing between order 1 and order k Markov chains grows extremely rapidly in the worst case.
Variable order Markov chains (VOMC) were introduced in [4] though similar ideas were considered earlier in context-dependent data compression by Rissanen [18].
Ron et al. [19] gave a polynomial time algorithm that learns a VOMC such that the probability distribution of the emitted state sequences has small Kullback  Leibler divergence from those generated by the true source.
Dalevi et al. [8] extended the recent order estimation algorithm of Peres and Shields [16] to VOMCs and conducted experiments with DNA sequences comparing the accuracy of several algorithms.
There has been some work on empirically modeling user browsing patterns with  rst [20, 14], second [22, 21], and higher order [17] Markov chains.
Borges [3]  t VOMC to session logs and Deshpande and Karypis [10] studied the compression and pruning of higher order Markov models.
However experimental evaluation has generally been limited to web access logs of a few relatively small web sites, e.g., a computer science department s or a merchant s web site, raising issues with the homogeneity, representa-tiveness, and insuf cient scale of the data.
For a thorough overview of sequence prediction algorithms applied to learning web request patterns we refer the reader to the excellent survey of Davison [9].
First order Markov chains [2, 6] and variable order hidden Markov models [5] have often been applied to context-aware search, document re-ranking, and query suggestion as well.
general kth order Markov chain.
Let v1, .
.
.
, vn be the elements of the state space S.
of order k is a process (Xi) vk+1, .
.
.
, v1, it holds that (ORDER k MARKOV CHAIN).
A Markov chain i=1 such that for each t   k and   Pr [Xk+1 = vk+1 | Xk = vk, .
.
.
, X1 = v1] = Pr [Xt+1 = vk+1 | Xt = vk, .
.
.
, Xt k+1 = v1].
In other words, the next state in a kth order Markov chain depends on the identity of the k states leading up to the current state.
Note that the traditional Markov chain is order 1 according to this de nition since the next state depends only on the current state.
Along similar lines, one can also de ne a zeroth order Markov chain where the next state distribution is independent of the current state.
Next, we de ne the order of an element.
DEFINITION 2.
Let u be an element of a Markov chain of any order.
Then u has order k if for each t   k + 1 and vk+1, .
.
.
, v1, it holds that Pr [Xt+1 = vt+1 | Xt = u, Xt 1 = vt 1, .
.
.
, X1 = v1] = Pr [Xt+1 = vt+1 | Xt = u, Xt 1 = vt 1, .
.
.
, Xt k+1 = vt k+1].
represented by(cid:80)n (cid:80)ki Observe that if a higher order Markov chain has elements v1, .
.
.
, vn respectively of orders k1, .
.
.
, kn, then the Markov chain can be j=1 nj 1 probability vectors: for each element vi, and for each possible sequence of length at most ki leading to vi, store the probability vector representing the next transition to be taken in the chain.
i=1 Now, given a set of paths where each path (sometimes called a trail) is a sequence of states, a natural computational question is: what is the order of the underlying stochastic process that generates these paths, or more speci cally, what is the order of a generic element vi?
This is the Markov chain order estimation problem.
as a  rst Markov chain on the larger state space S(cid:48) = (cid:83)k Note that a kth order Markov chain on a state space S can be seen i=1 Si.
Also, a sequence of traces generated by M can be interpreted as a sequence of traces generated by M(cid:48): a trace (a1, .
.
.
, ai) on M, can be seen as a trace ((a1), .
.
.
, (a1, .
.
.
, ak), (a2, .
.
.
, ak+1), .
.
.
, (ai k+1, .
.
.
, ai)) on M(cid:48).
Given an integer k   1 and a set T = {T1, .
.
.
, Tt} of traces, we wish to compute the kth order Markov chain that maximizes the probability of observing T ; such a Markov chain is called a Maximum Likelihood Estimate (MLE) for T .
Without loss of generality we assume that all trails start and end with a special reset state R that represents the unobservable components of the users  trails.
we will show how the k   2 case reduces to the k = 1 case.
We will continue the discussion assuming k = 1, and at the end An easy algorithm to compute the Maximum Likelihood Markov chain (of order 1) is the following.
For each sequence of the form Ti =(cid:0)xi,1, xi,2, .
.
.
, xi,|Ti|(cid:1), increase each of the counters

 In this section we describe the background material necessary for understanding higher order Markov chains.
First we de ne a Cxi,1 xi,2 , Cxi,2 xi,3 , .
.
.
, Cxi,|Ti| 1 xi,|Ti| .
Each of the counters starts at 0.
Observe that if there are n states x1, .
.
.
, xn plus the special reset state R, then the number of counters will be (n + 1)2.
Ma b = .
(1) (cid:80) Ca b states c Ca c Observe that the ratio is well-de ned iff at least one trace passes through state a.
We will consider M to be a matrix whose rows are indexed by the source states and the columns are indexed by the destination states.
The reset state R will index the  rst row and the  rst column.
Equation (1) can be easily extended to the higher order case using our observation from Section 3.
LEMMA 3.
The Markov chain M given by (1) is a Maximum Likelihood Estimate.
PROOF.
We prove for  rst order Markov chains; the proof easily extends to higher order Markov chains using the observation from Section 3.
Ti = (cid:0)xi,1, xi,2, .
.
.
, xi,|Ti|(cid:1).
Let Ca b be the number of times Let N be the MLE Markov chain for traces T1, .
.
.
, Tt, with that the states a and b were consecutive in a trace.
The probability of observing the traces T1, .
.
.
, Tn with Markov
 t(cid:89) (cid:89) chain N is given by (cid:1) = (cid:0)Nxi,j xi,j+1 |Ti| 1(cid:89) (cid:89) Take any state a and consider its product Pa =(cid:81) We have P =(cid:81) j=1 i=1 xj xi (cid:1)Cxi xj .
(cid:0)Nxi xj (cid:0)Na xj (cid:1)Ca xj .
xj a Pa.
Observe that Pa is the likelihood of a multi-nomial distribution.
We conclude the proof by stating the following fact: FACT 4.
Let n1, .
.
.
, nk be positive integers, and consider the function f (p1, .
.
.
, pn) = pn1
   pn2
       pnk k .
The function f, given the constraints pi   0, for i = 1 .
.
.
, k, , for i=1 pi = 1, is uniquely maximized at pi = ni(cid:80)k j=1 nj and(cid:80)k i = 1, .
.
.
, k.
.
Therefore, M = N.
By Fact 4, the maximum likelihood is attained by setting Na b = Ca b c Ca c (cid:80)
 While the Maximum Likelihood Estimate is easy to compute, we now show that it is impossible to reconstruct the unknown Markov chain to any good approximation, unless we are given a very large number of samples.
We will start by showing that learning a kth order Markov chain is not feasible even just (i) in an approximate fashion, (ii) at sta-tionarity, and (iii) if states are chosen uniformly at random out of an arbitrary support.
That is, (i) if we allow some slack in learning the transition probabilities, and (ii) if the slack is not worst-case but, rather, it is averaged over states in a way that mimics how the user travels around the Markov chain, and  nally (iii) even if the transition probabilities are uniform (as opposed to chosen so to make life harder for an algorithm)   the number of samples needed to learn the Markov chain grows exponentially in k.
The next construction also shows that making any guess on the order of the Markov chain (even allowing the three relaxations above) is impossible unless we are given a very large number of samples.
LEMMA 5.
There exists a Markov chain M of order k = O(1), satisfying point (iii) above, for which the average expected (cid:96)1 distance of the best guess of the next step distribution is  (1), unless  (cid:0)nk(cid:1) steps of the Markov chain have been observed.
Furthermore, guessing whether the order of the Markov chain is k or k   1 with probability  (1) is impossible unless   steps are observed.
PROOF.
Let n  3 be a multiple of k, and create a Markov chain with k layers with (n   3)/k states each, plus three extra states.
Let Li be the set of states of the ith layer, i = 1, .
.
.
, n 3 k , and let the three extra states be R (the reset state), A, and B.
The transition probabilities are de ned as follows:   if we are at node R, the next node to be visited will be chosen (cid:17) (cid:16) k 1
 n uniformly at random in L1;   from each node v   Li, i   {1, 2, .
.
.
, k   2, k   1}, the next node to be visited will be chose uniformly at random from Li+1;   if we are at a node vk   Lk, and the history up to that point is (v1, v2, .
.
.
, vk), the next node will be f (v1, .
.
.
, vk) with probability 1, where f is a function chosen uniformly at random (when constructing the Markov chain) between those with domain L1   L2       Lk and codomain {A, B};    nally, if we are at either A or B, the next state will be the reset state R with probability 1.
k k (cid:16) (cid:1)k(cid:17) (cid:16) k  (cid:0) n Observe that, given that f is chosen uniformly at random, if we happen to be at a node vk   Lk, with a history (v1, .
.
.
, vk), for the  rst time, it will be impossible for us to guess whether the distribution of the next node is degenerate in favor of A or B.
Therefore, regardless of which distribution we guess for the next step, it will have average (cid:96)1-distance to the actual one of at least 1.
Let H = {(v1, .
.
.
, vk) | vi   Li, i = 1, .
.
.
, k}.
Now, if we only observe o k  (cid:0) n steps in the Markov chain, there will be a fraction of 1   o(1) histories in H that we will not have seen.
Since each such history is equally likely, if we only observe o steps, our best guess to the distribution of the next step, if we are at a state vk   Lk, will have an average (cid:96)1-distance to the actual one of at least 1   o(1).
The main claim then follows by observing that any walk will be in a state of Lk a fraction  (k 1) =  (1) of the time.
For the second claim, suppose that an adversary chooses f either (a) uniformly at random from the set of functions L1   L2       Lk   {A, B}, or (b) uniformly at random from the functions of that set that satisfy f (x, v2, v3, .
.
.
, vk) = f (y, v2, v3, .
.
.
, vk), for each x, y   L1, and for each vi   Li, i = 2, .
.
.
, k.
(cid:1)k(cid:17) Observe that, with high probability (over the random choice of f), choice (a) produces a Markov chain of order k, and choice (b) produces a Markov chain of order at most k   1.
Now, unless a sub-walk v2, .
.
.
, vk, vi   Li, i = 2, .
.
.
k, is repeated twice, it will be impossible for the algorithm to distinguish between choices (a) and (b).
The probability that one such sub-walk is repeated at least twice is at most o(1), if the number of observed steps is o (cid:16)  nk 1 (cid:17) .
The above result shows that learning Markov chains (and their order) is quite costly in terms of how many steps are needed, even under assumption (iii), i.e., the transition probabilities of the Markov chain are not very small.
We show that, if we drop assumption (iii), then there is no function of n and k that upper bounds the number of steps needed to distinguish between n-states Markov chains of order k and a Markov hard that it cannot be solved with a number of steps upper bounded by any  nite function of n and k.
We sketch an argument of why this is the case.
Consider a Markov chain on n states such that the probability of transitioning from any state xi (cid:54)= x1, to any state xj, is exactly 1 n .
The transition from x1 to any other state xi, i   2, is uniform, regardless of the history.
The adversary makes a single choice: either (a) the probability of transitioning from state x1 to itself is pmin (cid:28) 1 n , regardless of the history; or (b) the probability of transitioning from state x1 to itself is n if the history is not a run of k continuous x1 s, pmin (cid:28) 1 and 1 2   pmin otherwise.
Now observe that if the adversary makes choice (a), then the If, on the other hand, the adversary Markov chain has order 1.
makes choice (b), the order of the Markov chain is k.
To guess whether the choice is (a) or (b) we have to traverse a (k + 1)-long sequence of x1 s.
Since the probability of following such a trail in the next k + 1 steps is at most 1 min , it follows that if we have less than o(cid:0)n   p (cid:1) steps to learn from we are n   pk+1 not able to distinguish between choices (a) and (b), i.e., we cannot distinguish between Markov chains of order k and order 1 with fewer steps.
Crucially, the lower bound does not depend on just n and k, but rather on the minimum nonzero probability in the Markov chain.
 k 1 min

 So far we have only hinted at one issue of higher order Markov chains: their state space can be quite large.
In this section, we deal with this general issue in two different ways.
First, we consider the variable order Markov chain estimation problem.
We change the MLE problem de nition to allow each state to have a different order, but we insist on  nding the MLE of the Markov chain under the constraint that the sum of the orders of the states is bounded by some value.
A solution to this problem can be used to obtain a more parsimonious assignment of  memory  to states.
As a byproduct, such a solution can be used to classify states in those that, roughly speaking, bene t from a  deeper  memory, and those that can be reasonably represented with a shorter one.
In fact, we present such a classi cation in Section 8.3.
Then, we consider a different problem that tackles more directly the issue of the bit-size of a higher order Markov chain.
We start from the observation that the highest cost in the memory representation of a Markov chain is not given by the identi ers of the states1, but rather by the probability distributions that each state has on its out-neighbors.
We therefore de ne the  compressed MLE  problem: if we are allowed to keep in memory at most t probability distributions, what is the maximum likelihood estimate for the higher order Markov chain?
We show that the variable order Markov chain MLE problem is solvable ef ciently in polynomial time; on the other hand, we show that the compressed MLE problem is NP-hard.
In this section we propose a dynamic programming algorithm (Algorithm 1) for solving the variable order Markov chain MLE problem.
tory  can be seen as a state.
In the algorithm description, Pk(vi) is the product of the maximum likelihood probabilities of the trace steps going out of vi, if we  x at k the order of node vi.
Such kth order maximum likelihood probabilities at vi can be computed exactly as in the uniform-order MLE algorithm.
Algorithm 1 for solving the variable order Markov chain problem.
let
 and A[0, .
.
.
, K], B[0, .
.
.
, K] be two vectors of size K + 1.
order, target total the B[j + k]   max (A[j]   Pk(vi), B[j + k]) .
for j = 0, .
.
.
, K   k do Initialize every element of B to 0.
for k = 0, .
.
.
, K do


 4: for all states vi do






 12: while i   0 do



 Let k    j  be such that A[k ] = A[j ]   Pj k  (vi).
Choose a history of length j    k  for state vi.
j    k  i   i   1 The algorithm itself is a modi cation of classical dynamic programming algorithms.
Thus, we do not provide a detailed analysis here and sketch its correctness instead.
After having iterated over states v1, .
.
.
, vi at Line 4, A[j], j =
 .
.
., Pji (vi), with the constraint that j1 +   + ji = j.
A standard dynamic programming induction can be employed to show that at Line 10, the value of A[j ] is the maximum possible likelihood, given the total order constraint.
The last part of the algorithm just unwinds the computation and reconstructs an order assignment that guarantees the maximum likelihood A[j ].
Here, we prove that the compressed MLE problem is NP-hard.
Our proof works regardless of the order of the (possibly, variable order) Markov chain.
The NP-hardness proof works as long as each state in the chain has positive order.
LEMMA 6.
The compressed MLE problem, for any order k  
 PROOF SKETCH.
We reduce from the Edge-Partition into Triangles problem, shown to be NP-hard by Holyer [11].
Given an undirected graph G = (V, E), the problem asks whether the edge set E, |E| = m, can be partitioned into m For each e = {v, w}   E we create two traces (xe, xv) and (xe, xw).
Let T be the set of traces.
We ask whether there exists a Markov chain for T , using at most m 3 + 2 distinct probability distributions over the out-neighbors, with likelihood at least p = (3m) 3 triangles.
 2m.
First, suppose that a partition of E into triangles exists, i.e., let
 such that e   s. The out-distribution of the reset state R will be uniform over {xe | e   E}, i.e., for each e   E, the probability of transitioning from R to xe will be 1 m .
Furthermore, for each s   S, and for each e   E such that e   s, we assign to xe the uniform out-distribution with support {xv | v   s}.
(cid:1), |S| = m S   (cid:0)V
 probability 1.
An easy calculation shows that the probability that the above Markov chain produces the input traces T is exactly p. Also, the number of distinct out-distributions is 1 + |S| + 1 = m On the other hand, it can be shown that every Markov chain M satisfying the requirements and guaranteeing a likelihood of at least 3 +2 different out-distribution: one having p, must contain exactly m support {R}, one having support {xv | v   V }, and each of the remaining m each e   E, there must exist exactly one Si containing it; these properties imply that the Si s induce a partition into triangles of the edges of G.
(cid:1).
Furthermore, for




 In this section we characterize the stationary distribution of chains derived from trails and its connection to a prefetching problem.
Again, assume that we are given a sequence of traces T = {T1, .
.
.
, Tt}, and that we compute via Equation (1) the Maximum Likelihood Estimate M for T .
Let (cid:96)i be the number of times that state xi was visited in the input traces.
Let (cid:96)R be the number of i=1 (cid:96)i be the total number of visits traces.
Finally, let L = (cid:96)R +(cid:80)n to states in the input traces.
(cid:16) (cid:96)R (cid:17) LEMMA 7.
Let the vector   be   = Then,  M =  .
PROOF.
Observe that (cid:96)i, i = 1, .
.
.
, n, is equal to n(cid:88) n(cid:88) j=1 j=1 n(cid:88) n(cid:88) n(cid:88) j=1 Furthermore, (cid:96)R is equal to (cid:96)R = CR R + Therefore, Let   =     M. Consider the xi-coordinate of  , for i =
 MR xj = CR R + Mxj R.
j=1 t(cid:88) (cid:96)i = (|Ti| + 1) (cid:96)R + i=1 i=1  
 (cid:18) (cid:96)k k=1 CR xi n(cid:88)   MR xi + (cid:80)n (cid:32) Cxk R +(cid:80)n (cid:18) Cxk xi n(cid:88) j=1 CR xj Cxk xi
 (cid:96)k
 (cid:19)   + k=1
 (cid:96)R
 (cid:96)R
 n(cid:88)  xi = = + = k=1 CR xi
   Mxk xi (cid:19) (cid:33) j=1 Cxk xj = (cid:96)i
 =  xi .
The same derivation gives  R =  R.
Therefore,     M =   =  , and the claim is proved.
We observe that the Markov chain M is irreducible.
This will allow us to claim that the   of Lemma 7 is the only stationary distribution of M.
L , (cid:96)1 L , (cid:96)2 L , .
.
.
, (cid:96)n
 .
 S(cid:48) the last state of   is xi (cid:96)i = Cxi R + Cxi xj = CR xi + Cxj xi .
 (xi) = OBSERVATION 8.
The Markov chain M is irreducible.
PROOF.
Since a state is part of M iff it was reached by at least one input trace starting from R and ending in R, and since each input trace has positive probability of being followed in M, it holds that M is irreducible.
COROLLARY 9.
The vector   of Lemma 7 is the unique stationary distribution of M.
we let  (xi) =(cid:80) a  rst order Markov chain M(cid:48) on state space S(cid:48) = {R}  (cid:83)k Now consider a Markov chain M of order k on states S = {x1, .
.
.
, xn}, plus the  reset  state R. Such a chain can be seen as i=1 Si A sequence of traces generated by M, can be interpreted as a sequence of traces generated by M(cid:48).
By Corollary 9, there exists a stationary distribution  (cid:48) for M(cid:48).
Recall that  (cid:48)( ), for some     S(cid:48), is the fraction of time that is spent on the multi-state   by a random walk.
Analogously, if  (cid:48)( ), we have that  (xi) is the fraction of time that is spent on state xi in a random walk over the kth order Markov chain M.
Let (cid:96)  be the number of times that the multi-state   in the Markov chain M(cid:48) is visited by the input traces, let L be equal to the sum of the (cid:96) s plus the number of traces, and let (cid:96)i be the number of times that state xi in M is visited by the input traces.
Then the last state of   is xi  S(cid:48) (cid:88) (cid:96)  = (cid:96)i.
(cid:80) Therefore, we obtain that the fraction of time spent on state xi in a random walk over the kth order Markov chain M is equal to: (cid:88)  S(cid:48) last state of   is xi (cid:48)   ( ) = last state of   is xi  S(cid:48)
 (cid:96)  = (cid:96)i
 .
We now use what we have developed so far in this section to solve the following prediction problem: suppose the user s browser is able to ask a content provider which page it should prefetch so to maximize the probability that, when the user clicks on a new link, the browser will be able to show the new page without performing other network operations.
Which page should be suggested?
Given a Markov chain, the best page to prefetch is easy: given the user history up until that point, prefetch the state (page) that has largest probability of being clicked on (breaking ties arbitrarily).
With the following observation, we obtain the stationary ef -ciency of the best algorithm with a given stationary Markov chain.
By stationary ef ciency we mean the (asymptotic) fraction of times at which the page that was prefetched happens to be the one that the user clicked on.
Again we state our result in terms of a  rst order Markov chain, but, as already noted, the higher order case reduces to the  rst order case.
LEMMA 10.
If   is the unique stationary distribution of M, then the ef ciency of the best prefetching algorithm for M is (cid:18) (cid:88) x (cid:19)  (x)   max y M (x, y) .
PROOF.
The probability of prefetching the right page, if we are at state x is exactly maxy M (x, y).
At stationarity, we will spend a fraction  (x) of the time at x.
Hence, the statement follows.
We observe that, if M is a maximum likelihood estimate obtained from a set of traces T , then the terms  (x) and M (x, y) can be computed directly from the traces.
We use four data sets for our experiments.
The  rst two deal with user behavior patterns across different pages in a website whereas the last two deal with user behavior patterns on a single page such as the search results page (SERP) or a content page.
In all our data, we append a generic reset state to each trail and prepend a sequence of length k reset states so that the trails are all connected to one another, the underlying chain is ergodic, and reset will help  forget  the history across different trails.
Yahoo.
This dataset, called Yahoo, is an anonymized sample of all user transitions that occurred in all Yahoo!
websites.
We restrict our attention to US-generated traf c and to the top 59 Yahoo!
sites including yahoo.com, Mail, News, Sports, Finance, and so on; we also include a catchall outside state to capture transitions that leave the Yahoo!
websites.
The data was collected in July 2009 and consists of a large set of randomly sampled 1.1 billion cookies.
The average length of the trails is around 46.
A record for a cookie is of the form (cid:104)a, t, b(cid:105)i=1 where a, b are the names of the Yahoo!
sites and t is the time at which the user left state a and entered state b, measured in seconds; the information about a was obtained using the HTTP referrer.
We break the record for a cookie into trails whenever consecutive elements of the record cannot be pieced together, i.e., bi 1 (cid:54)= ai or either ai or bi is the outside state.
Note that this can happen due to one of several reasons: the referrer string was not recorded properly, the user typed a URL into the browser address location, or a bookmark was used to directly jump into a website.
New York Times.
This dataset, called NYTimes, consists of a sample of user transitions that occurred in New York Times (nytimes.
com) and recorded using the Yahoo!
browser toolbar from September 2011.
The data consists of about 25,000 user trails, where each trail is identi ed by its anonymized cookie.
The average length of the trails is around 9.
A record for a cookie is similar to Yahoo, except that a and b are URLs in NYT.
We map these URLs into one of 40 topics, where these topics were manually selected from the New York Times website and by looking at the URLs themselves.
The topics will be the states of the Markov chain.
Example topics are Science, Politics, Sports, World, etc.
We used simple handcrafted URL-based mapping rules to map the URLs to one of these
 cessfully mapped to a valid topic.
The remaining were mapped into a generic other state.
As in Yahoo, we also have an outside state to capture transitions from or to non-NYT sites.
Mousetracking.
This dataset, called MouseTrack, consists of events such as mouse scroll, focus, and click, captured for a random sample of users visiting the Yahoo!
SERP.
This capturing was enabled by appropriately instrumenting the SERP and using the JavaScript mouseover and mouseout events on speci c DOM elements in the SERP.
The number of states in this dataset is 270 and includes states such as res::i (the ith search result), logo::logo (the Yahoo!
logo), ads_horiz_bot (the horizontal ads at the bottom), etc.
Note that these states are automatically extracted from the name of the corresponding DOM element in the raw HTML.
The data was recorded for 10 days in August 2011 and consists of about 2.34M trails.
The average length of each trail is around 7.
The trail consists of elements of the form (cid:104)te, t(cid:96), a(cid:105), where a is the state, and te is the time when the mouse entered the DOM element an t(cid:96) is the time when the mouse left the DOM element (or the DOM element was clicked).
We use the timestamps to construct the actual trail.
Note that unlike the previous two data sets, this dataset captures the user behavior on a single page.
Also, by the construction of SERP, a majority of the user movements have an orientation (top to bottom) and mostly self-avoiding (i.e., states are not typically revisited).
Eyetracking.
This dataset, called EyeTrack, consists of eye gaze movements collected as part of a controlled experiment involving about 32 participants.
In each treatment of the experiment, 8 random news articles were rendered on a 2   4 grid, where the positions are numbered row-major from left to right.
Each participant was exposed to about 18 treatments and their task was to click on one article of their choice to read.
All the participant s activities, in particular, their eye movements and gaze patterns, were recorded using a Tobii 1750 Eye Tracker (sampling rate 50Hz, 17  monitor,
 to obtain pauses and abrupt changes in the eye position.
We associate the eye position with one of the 8 cells in the grid (thus, the number of states in this dataset is 8).
The resulting data consists of 521 trails, where each trail consists of elements of the form similar to MouseTrack.
The average length of the trails is around 68.
This dataset is closer to MouseTrack in the sense that it is derived from user behavior on a single page, but is different in that there is no obvious top to bottom or left to right orientation .
In fact, as we will see, the lack of orientation is heavily re ected in the behavior.
In this section we present the results of our various algorithms and measurements on the four data sets that we discussed in Section
 browsing behavior across multiple pages and next we present the results on the order of the chain that captures the behavior within a single page.
Then, we focus on variable order Markov chains and the effect of representing and compressing the state space.
We then investigate the robustness of  ndings by subjecting the data to several natural constraints and modi cations.
Finally, we conclude with an application of our methods: to predict the next state visited by the user.
We implemented the basic algorithm for Markov chain order estimation.
Recall that this algorithm simply involves maintaining various counters to count the number of transitions, for a given length of the history.
This algorithm is naturally parallelizable in Map-Reduce, which is very important for studying large data sets such as MouseTrack and Yahoo.
We present our results by computing the MLE matrix for various order chains and then computing the log-likelihood of the input for each of these orders.
We use k = 1, .
.
.
, 5 for Yahoo (due to the size of the data) and k = 1, .
.
.
, 8 for the other three data sets.
While reporting the performance, to convey the main idea, we report the relative improvement over the log-likelihood  t at k = 1, i.e., the usual Markov chain.
This way, we can clearly see the value in using a chain of higher order to describe the observed trails.
Figure 1 shows the relative log-likelihood improvement for order k Markov chains over the k = 1 chain.
As we can see, the curves are concave and appear to saturate at k = 3 for Yahoo and k  
 Yahoo and   13% for NYTimes.
This suggests that the browsing behavior across websites is de nitely not Markovian but can be captured reasonably well by a not-too-high order Markov chain.
Thus, the cross-site browsing behavior appears to have limited but nontrivial history.
If we examine the popular higher-order states in both the data sets we  nd that they are quite intuitive.
For example, in Yahoo, the (Mail, Mail) or (yahoo.com, Registration, Mail), or (Mail, Figure 2: Log-likelihood  t for mouse movements in Mouse-Track and eye movement in EyeTrack.
News) are popular higher-order states that determine the next transition.
Likewise, for NYTimes, popular higher order states include (Business, Search), (NYregion, US, World), and (Opinion, Blog, Opinion).
We then turn to the browsing behavior on a single page.
To this end, we use the mousetracking data (MouseTrack) on SERP and the eyetracking data (EyeTrack) on news articles.
Figure 2 shows the relative improvements in log-likelihood.
As we see, unlike Section 8.1, the behavior is markedly opposite: the curves are convex for EyeTrack and convex up to order 6 for MouseTrack.
This suggests that the single-page browsing behavior is not only highly non-Markovian but also cannot be represented by a low-order Markov chain.
Thus, users clearly (perhaps subconsciously) remember their browsing pattern and the states they have visited.
Even though at a high level both EyeTrack and MouseTrack exhibit similar behavior, there are subtle differences.
The plot for MouseTrack shows that the improvement is diminishing after k  
 the states) and it is reasonable that mouse movements on about 6 or 7 search results are probably suf cient to determine the user s next course of action.
In contrast, the EyeTrack plot shows no signs of  attening.
This is due to the inherent two-dimensional browsing task the users were subjected to.
Middle states such as 2, 3, 5, 6 have to be revisited many times in order to move from the left side to the right side.
So, a lot of history might be needed in order to determine the next course of action by the user.
In this section we study the performance of the algorithm for variable order estimation.
We ran this algorithm on the four data sets and Figure 3 shows the results.
To make comparison with the  xed order Markov chain easier, we interpret the x-axis as a fractional order, i.e., it is the sum of the history lengths of all states divided by the number of states.
For completeness, we also include the range k   (0, 1] and show the performance of the  xed order Markov chain.
Clearly, variable order Markov chains are very powerful and even with very limited total history, they can exceed the performance of  xed order chains, even with larger total history.
This is only modestly true for EyeTrack, once again suggesting that the user behavior is more complicated in this case.
In course of building the variable order chains, it is illustrative to study which states bene t from having a lot of history.
For EyeTrack, we see that the  middle  states 3, 2, 7, 6 bene t a lot from history.
For MouseTrack, the search results (in increasing order from 1, .
.
.
, 10) bene t from history.
For these two cases, the bene t is more polarized.
In the optimal solution, these states demand a lot of history before other states get some amount of history.
For Yahoo and NYTimes, the situation is quite different.
The history gets spread evenly among the more popular states: e.g., Mail, News, Sports in Yahoo and World, US, Blog, Opinion in NY-Times.
This once again suggests a marked behavioral difference between these two activities.
In this section we study the transition table of a  xed order chain.
First, we focus on the support sizes as a function of the order.
Recall that an order k Markov chain can have O(nk)-sized support; hence, it is useful to measure the support size of the kth order chain as a fraction of this maximum.
Figure 4 shows the relative sizes.
Clearly, MouseTrack is quite ef cient in terms of support whereas EyeTrack requires relatively more values in the support.
The support sizes for Yahoo and NYTimes are comparable and lie somewhere in between.
Next, we study the effect of pruning some of the entries in the transition table.
This pruning is done at the counting stage, before
 Figure 4: Support size of the higher-order chain relative to the maximum support size.
the transition matrix is normalized.
For brevity, we only show the results for NYTimes; the other are similar.
The top panel of Figure 5 shows the performance hit in log-likelihood when entries below a certain count are removed from the table (e.g., the curve for the legend > 4 denotes normalizing the matrix after removing all counts of at most 4).
The bottom panel of Figure 5 shows the declining support size after pruning.
It is clear from the  gures that even aggressive pruning can result in signi cant space savings while not compromising adversely on the quality of the representation.
In this section we perform various analysis to study how robust are our  ndings.
Train-test split.
First, we focus on computing the MLE estimator on a dataset that is different from the dataset on which the log-likelihood evaluation is done.
We choose the two large data sets Figure 5: Effect of pruning on NYTimes.
Yahoo and MouseTrack for this purpose.
We split the data into two equal-sized partitions, train and compute the MLE on one partition, and evaluate it on the other partition.
The results are shown in Figure 6.
There is not much change in terms of the relative log-
Figure 7: Fixed order chains with self-loops removed for NY-Times and MouseTrack.
likelihood improvements over k = 1 between the original results and the new results for Yahoo (and NYTimes) but for Mouse-Track (and EyeTrack), there was a marked difference for higher order states.
This suggests that page browsing patterns, even at an aggregate level, are hard to learn and utilize.
Exploring this discrepancy further is an interesting direction for future research.
Removing self-loops.
Next, we focus on removing self-loops in the data and see how it would affect the  ndings.
Self-loops are natural in all the data sets and in some applications, it is important to consider the process without self-loops since other stochastic models can be used to capture the dwell time on a particular website.
Figure 7 shows the results.
The improvements are almost halved for NYTimes and nearly unchanged for MouseTrack.
The former is intuitive since users might browse similar categories repeatedly across different web pages.
The latter happens since single-page browsing is less likely to have too many self-loops and hence the impact of removing them is minimal.
Removing short trails.
Finally, we study the impact of removing trails that are too short.
Note that by including trails that are very short, we are actually downplaying the performance of higher-order chains.
Hence, if we remove them, we should see an improvement in their performance.
Figure 8 shows the results for NYTimes and MouseTrack, after removing trails of length at most 5.
The effect of having longer history is quite dramatic suggesting that longer trails can actually bene t a lot more from them.
From a different point of view, we also study the impact of sessionizing: breaking up long trails into smaller trails if the consecutive time interval is more than 30 minutes.
Figure 9 shows the effect of such a sessionization on Yahoo.
The effect, as seen, is minimal: trails spanning more than a session would not have bene ted from history in the  rst place and hence this is to be expected.
Figure 8: Fixed order chains with trails of length at most 5 removed for NYTimes and MouseTrack.
