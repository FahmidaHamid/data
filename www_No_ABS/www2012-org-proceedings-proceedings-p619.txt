There is no such thing as an isolated piece of knowledge.
Bits of information are interconnected in giant networks, and we are daily navigating and  nding paths through such networks.
Browsing the Web is an important example, but by far not the only one: we follow leads in citation networks to  nd work that is related to our own research; when we reason or try to  nd explanations for the phenomena around us, we are implicitly disentangling a network of relations between concepts, with the goal of  nding a path of connections between the  cause  and the  effect ; and we constantly look things up in cross-referenced dictionaries and encyclopedias, be it in the form of books or online resources such as Wikipedia.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Jure Leskovec Computer Science Department Stanford University jure@cs.stanford.edu




 -e














 Figure 1: A human example path between the concepts DIK-DIK and ALBERT EINSTEIN.
Nodes represent Wikipedia articles and edges the hyperlinks clicked by the human.
Edge labels indicate the order of clicks, the framed numbers the shortest-path length to the target.
One of several optimal solutions would be (cid:2)DIK-DIK, WATER, GERMANY, ALBERT EINSTEIN(cid:3).
This last example is particularly interesting, since Wikipedia is not just a regular website but a rich network representing human knowledge as well as the connections between single pieces of knowledge, by means of hyperlinks.
This distinguishes Wikipedia-browsing from navigation on typical Web resources.
By observing humans  nding their ways between articles in Wikipedia, we are watching them navigate a large information network, using their mental maps of relationships in order to  nd the paths that connect concepts.
There are two aspects analytic and pragmatic of this view of human way nding in information networks.
From an analytic perspective, it is important to understand what strategies and clues people use to  nd paths in the Wikipedia information network.
In particular, as humans are navigating information networks, they might switch between various strategies.
The interplay between the topical relatedness of concepts and the underlying network structure could give us important insights about the methods used by ef cient information seekers.
Also, the latter often face trade-offs: there may be way nding strategies that are safe but also inef cient; on the other hand, by trying to  nd only the shortest paths, the searcher might get lost more easily.
From a pragmatic perspective, there is useful information in the trail an information seeker has navigated so far, even before reaching the target.
We see such trails playing an important role in the development of methods that can analyze the path taken so far and provide information seekers with navigational aids.
One useful direction for this is in predicting what piece of information the information seeker is trying to locate.
Another is in automatically detecting if the user has gotten lost.
Given that human navigation of information networks is so ubiquitous, a better understanding of the methods according to which humans  nd connecting paths spaces [15], more intuitive and navigable link structures [8], and new intelligent information navigation systems [16].
Present work.
These broad issues suggest a wide range of interesting open questions.
We take a step in this direction by computationally analyzing how people navigate to speci c target pages in the Wikipedia information network.
As a tool we use the online human-computation game Wikispeedia [24, 23], in which players (i.e., information seekers) are given two random articles and aim to solve the task of navigating from one to the other by clicking as few hyperlinks as possible.
Players have no knowledge of the global network structure but must rely solely on the local information they see on each page the outgoing links connecting the current article to its neighbors and on their expectations about which articles are likely to be interlinked.
In this respect, the task humans are trying to solve at each visited article is that of guessing which of the outgoing links to follow in order to eventually reach the target article.
What makes our study unique is that we have been collecting detailed data on more than 30,000 instances of human way nding in an information network describing general human knowledge (the data came from around 9,400 distinct IP addresses).
This allows us to computationally analyze human way nding on a large scale.
Even more important, for every instance we know the starting article and the given target article the user is trying to reach.
Hence, we do not have to infer or guess the information need of the information seeker, but can base our methods on the ground truth instead.
To illustrate the dynamics of the Wikispeedia game, as well as potential reasoning schemes and classes of strategies humans might use, Fig. 1 gives the example of a human path between the start article DIK-DIK and the target ALBERT EINSTEIN.
(We call such a pair a mission.)
Note that using the browser s back button is allowed.
In the example, the information seeker clicked from ELECTRON to ATOM, but backed up after not  nding the link to the target that he/she had expected there.
We call the sequence including ATOM and the back-click the full path, while referring to (cid:2)DIK-DIK, WATER, ELECTRON, QUANTUM MECHANICS, ALBERT EINSTEIN(cid:3) as the effective path.
The shortest-path length (SPL) from every article to the target is shown in squares in the picture.
If a click decreases the SPL, we call it lucrative.
Note that, in the example, not every click is lucrative; rather, the information seeker makes progress at  rst, but then orbits at a distance of 2 from ALBERT EINSTEIN, before  nally gravitating towards it with the choice of QUANTUM MECHANICS.
We also emphasize the special role the article on WATER plays in the example.
It connects to many parts of the network hence we call it a hub and marks the transition between getting away from the animal kingdom and homing in on the realm of physics.
Despite the lack of global knowledge, humans are good at connecting the dots: the median human game path is only one click longer than the median optimal solution.
We explain this effect by showing that certain properties of Wikipedia s hyperlink structure make it easily navigable.
For instance, our Wikipedia graph (we use a version containing about 4,000 articles and 120,000 links [27]) has a skewed degree distribution (median/mean/max degree 19/26/ 294) and contains a few high-degree hubs that contribute to every thing being connected to everything else by short chains (median/ mean/max shortest-path length 3/3.2/9; note that this is the case although no  meta pages , such as category indices, are available to players).
This makes our network a typical  small world .
Our analysis shows that people commonly  nd their way in it by  rst locating a hub and by constantly decreasing the conceptual distance to the target thereafter.
While approaching the target through a series of conceptually very related articles is safer and often humans  preferred solution (cf.
the example of Fig. 1), it is typically not the most ef cient: we  nd that thinking  out of the box  often allows information seekers to  nd shorter paths between concepts at the risk of getting lost.
A strategy that is both popular and often successful is to connect concepts in terms of their geographical commonalities.
In the above example, (cid:2)DIK-DIK, AFRICA, EUROPE, GERMANY, ALBERT EINSTEIN(cid:3) would have been such a solution.
Following this analysis, we formulate a task that captures some of the key motivating issues discussed above.
We show how information from a short pre x of the navigation path can be used to predict what the information seeker is looking for.
We design a ranking-based machine learning model and an ef cient parameter estimation algorithm.
Our method is informed by the lessons learned in our analysis and is trained on real human paths.
The experimental evaluation shows that it can predict humans  intended targets with high accuracy.
Overall, our results provide insights into how people navigate and solve the task of way nding in information networks.
From the practical perspective, our  ndings can be applied in order to make better sense of observed human search paths.
Our performance on the target prediction task suggests that features of the underlying path can provide useful information beyond simply predicting the next action of the user.
We therefore think that results of our research can be incorporated into intelligent systems to facilitate human information browsing and navigation.
The work related to our explorations here can be separated into three parts: Web click-trail analysis, systems that aid users in Web navigation, and decentralized search in networks.
Next, we briefly review each of these three lines of related work.
Information retrieval has focused on analyzing Web-browsing click trails of millions of users mainly for the purpose of improving Web search results.
Click trails can be used as endorsements to rank search results more effectively [4, 20], trail destination pages can themselves be used as search results [26], and the concept of tele-portation can be used to navigate directly to the desired page [21].
Similarly, large-scale studies of Webpage revisitation patterns [2] focus on how often users revisit the same page, while ignoring how people get there.
In contrast, our work focuses on understanding how people reach information by navigating through networks.
Another important difference is that, in our case, we know the exact target of human search and can thus quantitatively analyze the strategies people use when navigating information spaces, as well as their ef ciency.
Observational and laboratory studies have conducted small-scale controlled experiments about users  thought processes during Web search by having them think aloud as they search [14], and about their interaction with Web information [18].
These studies spawned sophisticated descriptive models, like information scent [5] and information foraging [17], which uses the metaphor of how animals forage for food.
Other analogies, such as orienteering [15] and berrypicking [3], have also been used to describe users  informa-tion-seeking strategies.
Systems like ScentTrails [16] and guided tours [22] have been proposed to create annotations to indicate where other users have navigated in the past, all with the goal of helping people  nd information faster.
Our present work differs in two important ways: First, our goal is not to formulate an analogy for human way nding, but rather to analyze it computationally using a large-scale collection of real search traces.
Second, we ad-
 1
  2
  3
  4
 e g a t n e c r e p
  5



 number of clicks

 Figure 2: Distribution of game length, according to different path-length metrics.
Black circles: shortest possible paths.
Blue X s: effective human paths (i.e., ignoring back-clicks).
Red dots: complete human paths (i.e., including back-clicks).
Green plus signs: complete human paths, corrected for dropout rates.
path-length metric shortest possible paths human, effective human, incl.
back-clicks human, dropout corrected mode median mean











 Table 1: Summary statistics of the distributions of Fig. 2.
dress the task of predicting the actual target of human search, not just the next action [7].
The last line of related work can be traced back to Milgram s small-world experiment [13] and the algorithmic problem of decentralized search in networks [10].
Decentralized search considers a scenario in which a starting node s is trying to send a message to a given target node t by forwarding the message to one of its neighbors, where the process continues in the same way until eventually t is reached.
This process has been investigated both experimentally as well as through simulations [6, 11, 1, 9, 19].
Each game of Wikispeedia may be considered an instance of decentralized search in a network, where players try to navigate between given start and target pages using only the local information provided on the current page (i.e., players can only follow hyperlinks of the current page).
In the small-world experiment, search is in a sense even more decentralized, since each node i.e., human on the path independently forwards the message and then forfeits control.
While in Wikispeedia the information seeker also has only local knowledge about the unknown network, he/she stays in control all the way and can thus form more elaborate strategies than in the multi-person scenario.
Moreover, as previous empirical studies of search behavior had very few completed paths (e.g., only 384 [6]), we work with more than 30,000 completed chains.
Our study is unique in several respects.
We collected large-scale data about human navigation in a network of real-world concepts, where we know the precise target node humans are trying to reach.
We focus on computationally investigating and modeling how humans navigate information networks and what strategies they use.
This allows us to build accurate predictive models of where the users are trying to navigate.
The Wikipedia graph is an example of a  small world  in which most pairs of nodes are connected by short chains, with a mean/ median/max shortest-path length (SPL) across all pairs of 3.2/3/9.
A natural  rst question to ask is, How good are humans at  nding such short chains?
e g a t n e c r e p




 All games of SPL 3 Pyramid   Bean Brain   Telephone Asteroid   Viking Theatre   Zebra





 number of clicks Figure 3: Distribution of game length for four speci c missions with an optimal solution of 3 clicks.
We recorded between 216 and 376 paths per mission.
The gray curve shows the length distribution for all games with an optimal solution of 3 clicks.
Fig. 2 gives a good impression of how the paths found by humans compare to optimal solutions (summary statistics of the distributions in the  gure are provided in Table 1).
The red line shows the distribution of human path lengths (where clicks that were later undone and back-clicks are counted as regular clicks), while effective paths were used for the blue line.
For each human game we also computed an optimal solution, and the resulting path length distribution is plotted as a black line.
We make three observations:
 for optimal solutions.
While the distribution of optimal path length is tight around 3 clicks, the human distribution exposes a heavy tail.
line in Fig. 2) are typically not much longer than shortest paths.
Both mode and median search times differ from optimal by just 1 click (3 vs. 4 clicks), mean search time by 2 clicks (2.9 vs. 4.9 clicks).
(See Table 1.)
clicks (the red line in Fig. 2), the mode search time is still 4, and the mean and median search times are 1 click more than for effective paths (5 vs. 4, and 5.8 vs. 4.9 clicks).
That is, humans click back on average once every other game.
Two questions arise: First, what is the reason for the large variance in human search time?
Second, why is human search still so ef cient on average?
The  rst question permits two potential answers.
Either some missions are inherently harder than others, or some information seekers are better than others.
Some missions have longer optimal solutions than others, so necessarily some games are inherently harder.
However, even when restricting ourselves to missions of a  xed SPL, the numbers stay virtually unchanged (e.g., for games with a SPL of 3 clicks, the mode/mean/median is 4/5/6.0, as opposed to 4/5/5.8 for all games).
Of course, even among missions of a  xed SPL, some are harder for humans because the lucrative links might be less obvious.
To control such effects, we posted four missions all of SPL 3 on the game website with increased frequency.
This allows us to  nd out how different humans perform on the exact same task.
The search time distributions for the four frequent missions are plotted in Fig. 3.
We see that for each separate mission there is considerable search time variance, but also that some missions allow for shorter games on average than others.
This leads us to conclude that both hardness of mission and individual skill play a roll in explaining the large search time variance.
e t a r t u o   p o r d


 path pos.
(incl.
back clicks)

 Figure 4: Dropout rate as a function of path position (with
 probability of around 10%.
 1
  2
  3
 ) r (
 y t i l i b a b o r p k n i l
  4





 rank r



 Regarding the second question, too, Why is human search so ef cient on average? several answers are conceivable.
One might argue that the ef ciency of observed games is caused by a sampling bias.
In studies that collect data from human volunteers, one always faces the problem of participants dropping out before  n-ishing the task assigned.
In our case, this might result in a bias towards observing shorter chains than what we would observe by forcing participants to  nish all tasks, since the longer the game takes, the more likely the subject is to give up at some point.
For instance, 54% of all games in our data set were canceled before  n-ishing.
Fig. 4 shows that the dropout rate Ri, i.e., the probability of giving up at the i-th step, is roughly constant at around 10%.
Figure 5: Link probability P(r) as a function of rank r. Given r, consider all node pairs (u,v) such that v is the node that is r-th closest to u among all nodes.
Then P(r) is de ned as the fraction of these nodes for which u links to v. Blue: P(r).
Red: P(r) + , with  = 0.005.
Black: ideal slope of  1 (not a  t; only for orientation).
Also note the red, upper curve in Fig. 5: after adding a small constant  = 0.005 to P(r), the plot looks considerably more like the required power law.
We take this as an indication that there is slight underlinking in the Wikipedia graph: if every node linked to even its furthest fellow nodes with a small background probability , then Wikipedia could become even more easily navigable (at least under the TF-IDF distance measure).
In the previous section, we have argued that human search in the Wikipedia network is made possible by the statistical properties of its link structure.
Next we turn our attention to a detailed analysis of how people actually exploit these properties.
In our analysis, we investigate how some key quantities of articles and clicks change as games progress from the start towards the target article.
To facilitate the analysis, we restrict ourselves to all games whose start and target articles are optimally connected by exactly 3 clicks and consider only effective paths.
Fig. 6 contains a graphical summary of the  ndings we are about to discuss.
Each sub gure tracks one quantity along game paths; each curve is computed from all games of the same effective path length, the leftmost curve representing games of length 8, the next one games of length 7, etc.
(to avoid clutter, we consider only games of a maximum length of 8 clicks).
The x-axes show the human-path distance, i.e., the number of clicks to the target on the effective path (i.e., paths may be thought of as running from left to right), while the y-axes represent the mean of the respective quantity over all games, alongside 95% con dence intervals.
The bold gray curves plot the given quantity for the average optimal solution.
To compute it, we found an optimal solution for every human game instance and averaged.
We refer to the  gure in row r and column c as plot (r,c).
Making progress is easiest far from and close to the target.
Plot (1,1) shows how the shortest-path length (SPL) to the target changes as a function of human-path distance.
Necessarily, the shorter the game, the steeper the curve.
Additionally, all curves share a typical anatomy: with the  rst click, the information seeker gets signi cantly closer to the target on average, then the curve  attens out and becomes steeper again towards the endgame.
In short games, the players blasts straight through to the target, making progress with nearly every step, while in long games the player Using dropout rates, we can correct for the aforementioned bias and compute an ideal search time histogram, for the hypothetical case that participants never give up [6].
The result is shown as the green line in Fig. 2.
Although longer games are more frequent under the ideal than under the observed distribution, the distributions still look similar qualitatively, with mode 4 and a power-law like tail.
The median search time is only 1 click higher (6 vs. 5 clicks), and mean search time rises by 3 clicks (8.9 vs. 5.8 clicks).
We conclude that the observed human ef ciency in Wikispeedia play is not explicable by a sampling bias alone.
Instead, we conjecture that, even without knowing the set of all existing links, the Wikipedia graph is ef ciently navigable for humans because they have an intuition about what links to expect.
Clearly, the probability of two articles linking to each other is higher the more related they are.
This can lead to ef cient navigation even in the absence of global knowledge.
In particular, Liben-Nowell et al. [11] have shown analytically that short search times (technically de ned as polylogarithmic in the number of nodes) can be expected under their model of  rank-based friendship , viz., if the probability of a node linking to its r-th closest fellow node decays as 1/r.
Intuitively, such a scenario is desirable because it constitutes an appropriate mix of many short and a few long-range links.
The latter are helpful for getting somewhat close to the target, while the former are necessary for fully reaching it.
We strive to investigate whether the Wikipedia graph satis es rank-based friendship.
Humans may tap into all their knowledge and reasoning skills during play, so it is hard to formalize their node distance measure.
In the present analysis, we therefore coarsely approximate the human by a standard text-based distance measure and de ne the similarity of two articles as the cosine of their TF-IDF vectors [12] (and distance as one minus similarity).
Fig. 5 plots the link probability P(r) as a function of rank r. The black line was added to show an ideal slope of  1, as postulated by the rank-based friendship model.
Note that, although P(r) does not fully follow a power law, the overall slope of the curve comes close to  1, which leads us to conclude that Wikipedia is conducive to ef cient navigation because its links represent an appropriate mix of long and short-range connections across concept space.
SPL to target outdegree lucrative degree


 lucrative ratio










 rel.
info gain










 prob.
of lucr.
click












 cat.
tree dist.
to target TF IDF dist.
to target TF IDF dist.
to next























 Figure 6: The evolution of article properties along search paths, for games of optimal length 3.
Only games of between 3 and 8 clicks are shown.
Each colored line represents games of the same length.
The x-axis shows the distance-to-go to the target, the y-axis the average value of the respective property (with
 based on optimal solutions for the considered human paths.
goes through a phase of inef cient circling around the target before  nally gravitating towards it.
Another perspective of the same phenomenon is afforded by plot (2,3), which shows the fraction of times humans picked a lucrative link, i.e., one that led them closer to the goal in terms of SPL.
We observe a down up pattern in the curves: information seekers are more likely to make progress with the  rst click than with the second.
Later on, in the endgame, clicks become again ever more likely to be lucrative.
In long games, the phases of progress in the opening and endgame are separated by a phase of stagnation where the probability of picking a good link stays roughly constant, a manifestation of the circling effect described above.1 Hubs are crucial in the opening.
The initial progress with the  rst click is afforded by leaping to a  hub  article, i.e., a high-degree node that is easily reachable from all over the graph and that has connections to many regions of it.
This makes sense intuitively, since a good hub gives the information seeker more options to continue the search, and is demonstrated by plots (1,2), (1,3), and (2,1).
While the start article has an average degree of only about 30 (cf.
plot (1,2)), the  rst click leads to an article with an average degree of between 80 and 100.
After the sudden degree increase with the  rst click, the quantity decreases slowly as the target is approached.
Note that the shorter the game, the higher the degree of the hub (and of any given position, for that matter).
This could mean (1)
 achieve the optimal path length (the blue curve) is due to the fact that players might have later undone clicks taken from articles along the effective path by means of the browser s back button, such that they may have taken suboptimal links while still achieving the optimal effective path length.
y t i l a u q b u h







 path length Figure 7: Hub quality as a function of search time (with 95% con dence intervals).
Hub quality is de ned as the degree of the second article, divided by the degree of the maximum-degree neighbor of the start article.
that better information seekers pick better hubs, or (2) that some missions are easier because the start articles have links to better hubs.
While the availability of good hubs certainly helps, Fig. 7 demonstrates that the  rst alternative plays a role as well.
We plot the ratio deg(u2)/deg(u  ) of the degree of the second article and that of the highest-degree neighbor of the start article, averaged over all games of the respective length.
The quantity decreases with increasing game length, implying that better information seekers tend to start games with relatively higher-degree hubs.
Let the term  lucrative degree  stand for the number of outgoing links that decrease the SPL to the target.
Plot (1,3) shows that, just like the plain degree, the lucrative degree, too, increases sig-ni cantly with the  rst click the hub article typically offers more lucrative options than the start article.
Also, the mean lucrative degree then decreases as the games continue (necessarily, since there are more articles far from the target than close to it).
We do not see a correlation between the hub s lucrative degree and game length.
However, the start article itself has higher lucrative degree for very short games than for longer ones, an indicator that some games are inherently easier than others, even if the optimal number of clicks is held  xed.
This certainly is a factor in the aforementioned negative correlation between search time and hub degree: if there are many good hubs it is easier to  nd one of them.
An interesting additional insight is afforded by looking at how the average of the ratio of lucrative degree and degree changes during games (cf.
(2,1)).
The resulting quantity, which we call  lucrative ratio , corresponds to the probability of getting closer to the target when randomly choosing an outgoing link.
While both degree and lucrative degree achieve their maximum with the second article, their ratio drops drastically between the  rst and second articles.
From this we conclude that the second article is a true hub, in that it does not only have many outlinks leading closer to the target, but has even more that lead further away from it, i.e., that it has connections into many different regions of the graph.
Conceptual distance to the target decreases steadily.
Plots (3,1) and (3,2) show that articles get ever more related textually to the target as the latter is approached (in other words, textual distance decreases).
We verify this using two distinct measures of conceptual relatedness, (1) the cosine of the TF-IDF vectors of the two respective articles, as in Section 3, and (2) the number of edges that have to be traversed in the category tree that comes with our Wikipedia version, in order to reach one article from the other ( category tree distance ).
The fact that the conceptual distance to the target decreases strictly along paths corroborates our conjecture from Section 3 that humans approximately perform a decentralized search using a distance measure between concepts.
Also, note that the very intuition that the distance between concepts along the path Wikispeedia [24].
Big leaps  rst, followed by smaller steps.
While plots (3,1) and (3,2) track the textual distance between the current article and the target, plot (3,3) does so for the distance between the current and the next articles.
This  textual step size  is monotonically decreasing:  rst, information seekers make big leaps, with adjacent articles being rather unrelated (e.g., when jumping to the hub); then, as they home in on the target, they straddle ever smaller  gaps .
This progression is possible because Wikipedia s link structure trades off long versus short-range connections in a favorable manner, as laid out in our discussion of rank-based friendship in Section 3.
We also see the aforementioned circling effect for long games again: between the initial getting-away and the  nal homing-in, both the textual distance to the target and the textual step size stagnate, as the player stumbles around on the graph.
Clicks are most predictable far from and close to the target.
Finally, consider plot (2,2), which attempts to capture the agreement between different humans.
Consider a target article t. For each article u, we de ne a click probability distribution over u s outlinks, which counts for each outlink how often it was taken when humans were searching for the target t (with add-0.1 smoothing, to mitigate the effect of zero counts).
The entropy of this distribution provides us with a measure of how predictable human clicks are, lower entropy meaning higher predictability.
We let the term  information gain  refer to the difference between the prior entropy of the uniform click distribution before observing any clicks and the posterior entropy given all game data.
It measures how much more predictable clicks at a given article u are after seeing the game data than before.
 Relative information gain  is the ratio of information gain and prior entropy, or in other words, the percentage-wise decrease in uncertainty afforded by observing the game data.
This quantity exposes a characteristic pattern, as shown in plot (2,2).
The relative information gain at the start article is typically around 23% on average and much lower (around 10%) for the following article.
The leap to the hub is much more predictable than the ways in which people continue from there.
(This is compounded by the fact that, given a start article, not all humans choose the same hub, such that for each hub we have fewer samples than for the start article and the respective click distribution stays more uniform, resulting in higher posterior entropy and thus lower information gain.)
As information seekers approach the target, their behavior becomes again more coherent and predictable, with information gain increasing.
Comparison of human with shortest paths.
To conclude our discussion of typical human search paths, we compare them to the optimal solutions found by a shortest-path algorithm (the bold gray curve in each plot).
Most of the curves are qualitatively similar to those for human paths, which follows from the structural constraints imposed by the link graph.
However, there are quantitative differences with respect to all quantities we investigate.
For instance, for shortest paths, too, the average degree goes up with the  rst click, but this is purely statistically so because the shortest-path  nder is more likely to pick high betweenness-centrality nodes, which in turn tend to have high degree; note that nonetheless the hub has about 20 fewer outlinks than for optimal humans.
The lucrative degree of the hub is about 3.5 for optimal solutions found by humans, while it is and only 2 for solutions found by the shortest-path  nder.
The relative information gain is nearly zero for the second article, much smaller than the 10% typical for humans.
The reason is that shortest paths are often entirely different from human paths, such that the second article itself is often one that humans never picked.
Since the information gain is computed solely based on human paths, the entropy at the second article stays very uniform (i.e., information gain close to zero).
The curves for TF-IDF similarity to the target and to the next article are qualitatively similar to those for human paths, in that the distance values decrease as games progress.
This is due to the fact that closeness in the Wikipedia graph is correlated with textual similarity (cf.
Fig. 5).
Therefore, as the graph distance to the target decreases, so does the textual distance (plots (3,1) and (3,2)).
Note, however, that the decrease is much more pronounced for human paths: humans explicitly navigate according to the content of articles, while the shortest-path  nder does so only because it is implicitly constrained by the statistical properties of the hyperlink graph.
Given the  ndings of the previous section, degree and similarity seem to be the most important factors in human way nding in Wikipedia.
We hypothesize that humans navigate more strongly according to degree in the early game phase, when  nding a good hub is important, and more strongly according to textual similarity later on, in the homing-in phase.
The goal of this section is to test this hypothesis.
We conduct the following experiment to gauge the trade-off between similarity and degree.
Consider only the games with an optimal solution of 3 clicks.
Then divide the set of all human trajectories into subsets according to the number of clicks taken by the player (the maximum length we consider is 8).2 Each of these subsets is divided into balanced training (70%) and test (30%) sets.
For each training set and each path position in the training set, we train a logistic regression classi er, using two features (and a constant bias term), representing degree and similarity to the target, respectively.
The positive examples consist of all human clicks contained in the respective training set.
The negative examples have to be contrived (since we have no ground truth of clicks a human will never make).
We do so by randomly (with replacement) sampling clicks that were never observed, until there are as many negative as there are positive examples.
Once the classi ers for all combinations of path length and path position have been trained, we inspect the resulting feature weights to infer how important each feature is in humans  click choice at each position.
Before presenting the results, we add some notes about the two features.
When regression is used for the purpose of feature analysis, it is important to have uncorrelated features.
The natural choice for similarity would be the TF-IDF cosine that we have also used in previous sections of this paper.
However, this similarity measure is highly correlated with degree: the higher a node s degree, the higher its average TF-IDF similarity with all other articles.
This happens because high-degree articles are typically long, and long articles are more likely to have some text overlap with the target article.
(The effect is noticeable even in the face of the length-normalization implicit in cosine similarity.)
On the contrary, no such correlation with degree is exhibited by the category tree distance.
We therefore adopt the latter to quantify similarity in our regression analysis.
To be able to compare the weights for features that can take on very different values, we also have to normalize.
We do so by adopting a rank-based approach.
Consider an article u and a given feature.
The neighbors of u get values from the inter-
back-clicks themselves are neglected, we do consider the forward clicks that the player undoes later on.
length 6








 length 4


 length 7

















 length 5



 length 8









 Figure 8: Logistic regression weights for classifying human vs.
nonhuman clicks (with standard errors).
Green: textual similarity.
Red: degree.
There is one plot per human path length; the x-axes show path positions, the y-axes weights.
val [0,1], such that the highest-ranking neighbor, according to the feature, gets value 1, and the lowest-ranking neighbor value 0.
Fig. 8 plots the resulting weights for the two features.
There is one plot for each game length between 3 and 8.
The x-axes show path positions, and each data point represents one feature weight.
The red curves are the degree and the green ones the similarity weights.
The weight of the bias term was omitted from the plots, since it is not informative.
Note that, for visibility s sake, we do not show the weights for the last click.
There, similarity becomes a nearly perfect indicator for the target article, since the target has maximum similarity with itself, so the similarity feature gets a very large weight, and the interesting part of the plots would get squished and hard to read.
Interpreting the plots, our expectation is con rmed.
Both features obtain positive weights everywhere, which means that both high degree and high similarity with the target are characteristics of the click choices made by humans.
More interesting, as hypothesized, degree dominates in the beginning of games, but as games progress, similarity becomes ever more important, superseding degree starting with the second or third click.
Furthermore, similarity starts dominating earlier in more ef cient games.
We emphasize that the purpose of this experiment is an analysis of the  tted feature weights, not maximizing the accuracy of the classi ers.
Still, to justify our conclusions, we need to show that the classi ers perform better than chance (50%) on a statistically sig-ni cant level.
Evaluating the classi ers on the held-out test set, we  nd that this is the case.
Accuracy is similar for all game lengths.
It drops from around 90% for the  rst path position to about 65% for the second and then stays in the regime of between 55% and 65%.
When maximum accuracy is the goal, more powerful features, such as TF-IDF cosine, perform better, but as mentioned earlier, feature correlation does not permit us to use this feature in our analysis.
The main  nding of the previous section is that in the opening of games it is common to navigate through hubs.
Next we take a closer look at the strategies players adopt in endgames, in order to home in on the target.
In the present analysis, we de ne an endgame as the last 3 articles (i.e., 2 clicks) of a path.
To make sure the endgames we analyze do not contain artifacts from the game openings, we consider only games of a full length of at least 5 articles (i.e., 4 clicks).
We also neglect all games above the length threshold of 20 articles.
The d a e h r e v o n a e m



 one group per target category Figure 9: Overhead with respect to optimal solutions, for single-category (red) and most popular multi-category (blue) strategies, with one group per target category.
The green bars show means over all games of the respective target category.
From left to right: PEOPLE, MUSIC, IT, LANGUAGE


 endgame strategy corresponding to an endgame (cid:2)un 2,un 1,un(cid:3) is de ned as (cid:2)C(un 2),C(un 1),C(un)(cid:3), where C(u) is u s top-level category in the hierarchy that comes with our Wikipedia version.
For instance, the full category of DIK-DIK is SCIENCE/BIOLOGY/ MAMMALS, and C(DIK-DIK) = SCIENCE.
All 14 values C can take on are listed in the caption of Fig. 9.
We divide the set of all Wikispeedia games into subsets according to target categories, such that all games with target articles from the same category are placed in the same subset.
For each target category, we observe between 29 and 104 distinct strategies, out of the possible 142 = 196.
For all target categories, the distribution over strategies is highly nonuniform, with most games following one of only a few top strategies.
As a consequence, for each target category, the top 10 strategies typically cover between
 ticles within each category are also nonuniform; e.g., 14% of all instances of GEOGRAPHY are UNITED STATES and 6.1% UNITED
 In 12 out of the 14 target categories, the most popular strategy is the one that consists of the target category only, which we call the  simple  strategy: people tend to approach the target through articles from the same category as the target.
In the remaining two categories, the simple strategy has very high rank, too: it is second most frequent when the target is from DESIGN AND TECHNOLOGY, and fourth most frequent when it is from PEOPLE.
In the former case, the more frequent strategy is (cid:2)GEOGRAPHY, GEOGRAPHY, DESIGN AND TECHNOLOGY(cid:3); in the latter case, the three more frequent strategies are (cid:2)GEOGRAPHY, GEOGRAPHY, PEO-PLE(cid:3), (cid:2)GEOGRAPHY, CITIZENSHIP, PEOPLE(cid:3), and (cid:2)GEOGRAPHY, HISTORY, PEOPLE(cid:3).
In these examples, GEOGRAPHY seems to play a prominent role.
And indeed this is a general property of human paths.
To demonstrate this, we count, for each category c, how often articles from it appear in endgames in which the target is not also of category c. We  nd that GEOGRAPHY accounts for 20% of articles in endgames of which GEOGRAPHY is not the target.
The next most common categories according to this metric are SCIENCE (7.5%) and HISTORY (5.1%).
One might argue that certain categories are a priori more likely, since they contain more articles.
We can correct for this bias by considering the ratio of the above-introduced frequency and the a-priori category frequency, i.e., the number of articles in the category, divided by the overall number of articles.
In the resulting ranking, GEOGRAPHY is still top, now followed by CITIZENSHIP (mostly about politics and culture) and RELIGION.
This  nding might imply that humans organize their knowledge strongly cepts with their countries of origin.
Previously, we saw that simple single-category endgames are typically most popular with players.
Next, we investigate how ef- cient they are compared to other, more complex strategies.
Let l be the length (number of clicks) of a human game, and l  the number of clicks in an optimal solution.
We de ne the overhead of a game as (l  l )/l , i.e., the percentage of the optimal solution length that the information seeker needed extra.
For each endgame strategy, we compute the mean overhead over all games of that strategy.
As a baseline, we consider the mean overhead across all games of the given target category.
We  nd that the overhead of the simple strategy is on average (over the 14 target categories) 12% higher than that of the mean game; i.e., games using the simple strategy are typically worse than average.
Now consider, instead of the simple strategy, the most frequent multi-category strategy.
Averaged across all target categories, its overhead is 18% smaller than that of the mean game; i.e., games using the most frequent multi-category strategy are typically considerably better than average.
Fig. 9 shows that this is not only true on average but for nearly every target category taken by itself.
One of only three categories for which the simple strategy is, on the contrary, most ef cient is GEOGRAPHY (the rightmost group in the bar chart).
This is in tune with our previous  ndings: since GEOGRAPHY plays a prominent role even when it is not the target, it makes sense that it allows for ef cient paths when it is.
Our interpretation of these  ndings is that information seekers face a trade-off between ef ciency and simplicity.
Whilst reaching the target through very related articles from the same category is conceptually simple, the steps taken this way can be small and prolong the game.
It often pays off to think out of the box or to think in geographic terms.
In the previous sections, we have conducted an in-depth analysis of how people navigate Wikipedia towards a given target article.
Our next goal is to apply the lessons learned, in order to design a learning algorithm for predicting an information seeker s target, given only a pre x of a few clicks.
Our method explicitly takes the characteristic features of human search into account and is trained on real human trajectories.
There are many potential use cases of such a method.
For instance, an intelligent browsing interface could use the algorithm for tracking the user s goal and adapt accordingly, e.g., by suggesting useful shortcuts, thus making human search more ef cient.
Given the scope of this paper, we evaluate our method only in the context of Wikipedia, but we believe it is general enough to extend to other search scenarios, if appropriate features are used.
Human Markov model.
We cast our task as a ranking problem.
Given the observed path pre x q, rank all articles t according to how plausible they are as targets of the current search.
At the heart of our approach is a Markov model of human search, the parameters   of which are learned.
To make the prediction, we order candidate targets t according to a ranking function g(t|q;  ), de ned as the likelihood of t given the pre x q, i.e., as P(q|t;  ).
Let q = (cid:2)u1, ...,uk(cid:3) be a pre x of k   1 clicks.
Given target t and model parameters  , the probability of seeing q is obtained by multiplying the local click probabilities, and we aim to  nd the most likely target, i.e., argmax t P(q|t;  ) = argmax t P(u1) k 1Y i=1 P(ui+1|ui,t;  ), (1) where P(u1) = 1/N is constant, with N the number of articles (since start articles are picked randomly).
Note that we will work with the pre x log-likelihood L(t|q;  ) := logP(q|t;  ) instead.
P(ui+1|ui,t;  ) = In our analysis of humans, we saw that people trade off features differently at different steps.
We mimic this by learning a separate set of weights for each step, such that   = ( 1, ...,  k 1) is in fact a collection of weight vectors, with  i being the weights for step i.
We next propose and test two alternative models of click probability P(ui+1|ui,t;  ), each with its own model  tting algorithm.
Binomial logistic model.
The  rst, simpler model is similar in spirit to the regression of Section 4.2 (but using stronger features), where we  t a model to predict whether humans would pick a given link.
The model speci es, for any given click triple (ui,ui+1,t) separately, the probability that a human would choose it.3 Formally, we de ne the binomial logistic model as  ( (cid:5)
 v (ui)  ( (cid:5) (2) where  (x) = (1 + e x) 1; f(ui,ui+1,t) is a feature vector for the click from ui to ui+1 given target t; and  (ui) the set of ui s neighbors.
The model parameters   are  tted as in standard logistic regression using gradient descent.
Learning-to-rank model.
Since the task is to rank target candidates, we also explore a different setup in which we  t   explicitly to optimize a ranking objective we refer to as cumulative recipro-
cal rank.
This metric is de ned as (cid:2)(r) := r j=1 1/ j, where r is the rank of the true target [25].
Minimizing this objective implies ranking the true target as high as possible; additionally, cumulative reciprocal rank has the desirable property of putting more emphasis on the top of the ranking (e.g., (cid:2)(20)  (cid:2)(1) (cid:7) (cid:2)(120)  (cid:2)(101)), and is therefore a sensible choice for evaluating rankings.
i f(ui,ui+1,t)) i f(ui,v,t)) Notice that, unlike assumed by the simplistic binomial logistic model, humans really face a multinomial choice at each step.
Therefore, we now represent click probabilities in a multinomial logistic model: , P(ui+1|ui,t;  ) = exp( (cid:5)
 v (ui) exp( (cid:5) i f(ui,ui+1,t)) i f(ui,v,t)) .
(3) t Another advantage of this framework is that we may use other features in addition to the likelihood.
Some important factors depend on the entire pre x and cannot be encoded naturally into the Markov model (e.g., How often did the player not take a direct link to the target although this was possible?
), which considers only local clicks.
Thus, in the learning-to-rank setup, our  nal ranking function g consists of a linear combination of pre x log-likelihood and those additional pre x-global features.
As our prediction, we select the target t that maximizes g, i.e., g(t|q;  ,  ) = argmax  1F1(q,t) + ... +  mFm(q,t), argmax (4) where F1 = L(t|q;  ), and F2, ...,Fm are the pre x-global features, the details of which are provided below (note that they do not depend on  ).
We  t   and   using an approach inspired by a method proposed recently by Weston et al. [25].
The algorithm minimizes cumulative reciprocal rank via stochastic gradient descent, using a novel sampling trick to speed up learning.
In our case, this is necessary since we would otherwise have to iterate over all target candidates
 gistic regression instead, but this is not possible, since the degree of ui and hence the number of classes is variable.
t to Weston et al. s paper regarding the details of the framework and restrict ourselves to highlighting how we adapt their algorithm: The learning algorithm requires computing the derivative of g g(t|q;  ,  ) = Fj(q,t) is obvi-  g(t|q;  ,  ) is trickier.
We forgo a derivation and simply with respect to   and  .
While     j ous,   state that    i g(t|q;  ,  ) =  1 L(t|q;  )    i h f(ui,ui+1,t) P =  1 v (ui) P(v|ui,t; ) f(ui,v,t) i .
That is, the likelihood gradient with respect to the weights for pre x position i is equal to the difference of the feature vector of click i and the expected feature vector under the current weights  .
Features for learning.
So far, we have only described the abstract framework of our algorithms but have not yet discussed the concrete features we use.
As mentioned, our choice of features is inspired by the results of our study of human behavior: if we design features that capture the characteristics of human paths, the algorithms can learn weights to predict targets under which the seen pre x resembles human behavior most.
Speci cally, these are the entries of the likelihood feature vector f(ui,ui+1,t) (recall that we learn a separate weight vector for each i):
 related to the target as human games proceed (Fig. 6 (3,2));
 comes ever smaller (Fig. 6 (3,3)); 3. deg(ui+1): humans commonly navigate through hubs in the beginning, so we expect the weight for this feature to be large for early and small for later steps i; 4. deg(ui)  deg(ui+1): if ui already is a hub, there is no more need for the player to search for another one, which can be captured by this interaction feature;


 the shortest-path length to t, then it is less likely to be chosen by a player;
 textually,  nding a hub is less important; typically also smaller; if it decreases the textual similarity to t.
In the learning-to-rank approach, we additionally have the following pre x-global features (cf.
(4)):
 t yet did not;

 The last two pre x-global features are similar to likelihood features 7 and 8, but here they can modify the ranking function explicitly rather than merely via the likelihood term.
We expected the  rst pre x-global feature to receive a large negative weight, guided by the intuition that humans would always go directly to the target as soon as this is possible.
However, a weight of nearly zero is learned for this feature, which indicates that information seekers often miss the best links because they do not expect them and hence do not notice them in the often long article text.
This emphasizes the usefulness of the task of target prediction: if the algorithm can




 accuracy


 cumulative reciprocal rank k = 2 k = 3 k = 4






 n : total #articles on path n : total #articles on path Figure 10: Performance of our target prediction algorithms, for varying pre x lengths k (indicated by color).
Left: accuracy (higher is better).
Right: cumulative reciprocal rank (lower is better).
Bold solid: multinomial ranking model.
Thin solid: binomial logistic regression.
Dashed: TF-IDF baseline.
m @ c e r p g n i l b i s






 n = 5, k = 4 n = 6, k = 4 n = 7, k = 4

 m

 Figure 11: Sibling precision of our target prediction algorithms.
Bold solid: multinomial ranking model.
Thin solid: binomial logistic regression.
Dashed: TF-IDF baseline.
infer the intended target, it could highlight the most useful links so they become more salient to the user.
Evaluation.
For training our models, we use pre xes of 3 clicks, taken from human games of at least 4 clicks.
The weights converge quickly for the multinomial ranking model, after seeing a few thousand examples, which takes only some minutes.
We also compare our algorithms to a baseline that simply predicts as the target the article with the largest TF-IDF similarity to the last article of the pre x.
We use a test set not seen during training to evaluate the algorithms on two tasks: (1) given a pre x q = (cid:2)u1, ...,uk(cid:3) and a choice of two targets, pick the true target t; the false target is picked randomly from the set of articles that ever occurred as targets and have the same SPL fromu k as t; (2) given q, rank the set of all articles such that the true target is ranked high.
The second task is much harder but also more useful than the  rst; e.g., if the method is to be implemented for an intelligent browsing tool, reasonable targets must be picked from all candidates, not just from a set of two.
In the  rst task, we use accuracy as a metric; in the second, we measure ranking loss according to cumulative reciprocal rank, the objective we also use for training.
While this captures ranking quality objectively, it might be overly strict; e.g., if t = WINE, then predicting BEER is much better than, say, GASOLINE.
We account for this by measuring  sibling precision@m , which is the same as precision@m, with the difference that not only t but all articles from the same category as t are counted as relevant (we use the leaves of the hierarchy of our Wikipedia version as categories).
We vary two parameters of the test pre xes: k, the number of articles in the pre x; and n, the length of the entire human path.
The results are summarized in Fig. 10 and 11.
In all plots, the bold solid line the binomial logistic regression (BLR) model, and the dashed line the TF-IDF baseline.
First note that MR is at least as good as, and often better than, both BLR and TF-IDF according to every metric.
Now consider Fig. 10.
As expected, our methods work better when pre xes are longer (cf.
the order of the bold curves) and when full paths are shorter (cf.
the slopes of the curves).
Notably, on the task of picking the correct one of two targets, MR achieves an accuracy of 80% when 3 clicks are seen, regardless of whether the entire game is 4, 5, or 6 clicks long.
Interestingly, while BLR has higher accuracy on the binary task, the simple TF-IDF baseline achieves better ranking performance.
We take this as an indicator that MR combines the better properties of both.
Finally, consider Fig. 11, which shows sibling precision@m. For the sake of brevity, we display only the case k = 4, but in relative terms the results are the same for all pre x lengths.
The preci-sion@30 of MR is 20% for n = 5, which means that 6 of the top 30 targets are of the same category as the true target, when we see 3 clicks and the full game has 1 more click.
Even when there are 2 (3) more clicks, we still see 5 (3) top-ranked articles that are very close to the true target (for comparison, in a random ranking, precision is only 1% on average).
This property of the ranking algorithm is desirable, since in a real-world application making a close enough guess might often be nearly as good as predicting the exact target.
Finding paths connecting different concepts like linking causes to effects is a task the human race has been performing for mil-lennia.
We formalize this task in a human-computation game of way nding between the concepts of Wikipedia.
We study more than 30,000 goal-directed human search paths and identify aggregate strategies people use when navigating information spaces.
As information spaces become more complex, it is increasingly important to understand how humans navigate them and to assist them in locating the desired information.
This is the second focus of our paper, where we build a predictive model of human way nding that can be applied towards intelligent browsing interfaces.
The view of human way nding as a navigation task on Wikipedia points to a broad range of interesting issues, and our goal in this paper has been to start exploring the foundations for reasoning about these questions.
We anticipate further investigations in determining why people give up navigating and characterizing un nished way nding tasks.
Given the insights we offer here, another interesting direction is in automatically designing information spaces that humans  nd intuitive to navigate and identifying individual links which could make the Wikipedia network easier to navigate.
Overall, we hope that this perspective can contribute to the development of new functionality in the continuing evolution of how we use and navigate the Web.
This research has been supported in part by a Microsoft Faculty Fellowship, NSF grants IIS-1016909, IIS-1149837, and CNS-1010921, the Albert Yu & Mary Bechmann Foundation, IBM, Microsoft, Samsung, and Yahoo.
