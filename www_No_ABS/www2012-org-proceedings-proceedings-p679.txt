Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses.
By distributing tasks or questions to large numbers of Internet users, these  crowd-sourcing  systems have done everything from answering user questions (Quora), to translating books, creating 3-D photo tours [29], and predicting the behavior of stock mar-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
kets and movie grosses.
Online services like Amazon s Mechanical Turk, Rent-a-Coder (vWorker), Freelancer, and Innocentive have created open platforms to connect people with jobs and workers willing to perform them for various levels of compensation.
On the other hand, crowd-sourcing systems could pose a serious challenge to a number of security mechanisms deployed to protect Internet services against automated scripts.
For example, electronic marketplaces want to prevent scripts from automating auction bids [23], and online social networks (OSNs) want to detect and remove fake users (Sybils) that spread spam [32, 34].
Detection techniques include different types of CAPTCHAs, as well as machine-learning that tries to detect abnormal user behavior [10], e.g.
near-instantaneous responses to messages or highly bursty user events.
Regardless of the speci c technique used, they rely on a common assumption, that the malicious tasks in question cannot be performed by real humans en masse.
This is an assumption that is easily broken by crowd-sourcing systems dedicated to organizing works to perform malicious tasks.
Through measurements, we have found surprising evidence showing that not only do malicious crowd-sourcing systems exist, but they are rapidly growing in both user base and revenue generated.
Because of their similarity with both traditional crowd-sourcing systems and astrotur ng behavior, we refer to them as crowdturf-ing systems.
More speci cally, we de ne crowdtur ng systems as systems where customers initiate  campaigns,  and a signi cant number of users obtain  nancial compensation in exchange for performing simple  tasks  that go against accepted user policies.
In this paper, we describe a signi cant effort to study and understand crowdtur ng systems in today s Internet.
We found significant evidence of these systems in a number of countries, including the US and India, but focus our study on two of the largest crowdtur ng systems with readily available data, both of which are hosted in and targeted users in China.
From anecdotal evidence, we learn that these systems are well-known to young Internet users in China, and have persisted despite threats from law enforcement agencies to shut them down [5, 9, 20].
Our study results in four key  ndings on the operation and effectiveness of crowdtur ng systems.
First, we used detailed crawls to extract data about the size and operational structure of these crowd-tur ng systems.
We use readily available data to quantify both tasks and revenue  owing through these systems, and observe that these sites are growing exponentially in both metrics.
Second, we study the types of tasks offered and performed in these sites, which include mass account creation, and posting of speci c content on OSNs, microblogs, blogs, and online forums.
Tasks often ask users to post advertisements and positive comments about websites along with an URL.
We perform detailed analysis of tasks trying to start
 Customer
 Agent
 Worker

 Work Flow Cash Flow





 Customer Agent Customer Agent Leader Leader Leader Workers Mailing List / IM Group (a)Distributed Structure Workers Public Web Service (b) Centralized Structure Figure 1: Work and cash  ow of a crowdtur ng campaign.
Figure 2: Two different crowdtur ng system structures.
information cascades on microblogging sites, and study the effectiveness of cascades as a function of the microblog social graph.
Third, we want to evaluate the end-to-end effectiveness of crowd-tur ng campaigns.
To do so, we created accounts on one of our target systems, and initiated a number of benign campaigns that provide unsolicited advertisements for legitimate businesses.
By bouncing clicks through our redirection server, we log responses to advertisements generated by our campaigns, allowing us to quantify their effectiveness.
Our data shows that crowdtur ng campaigns can be cost-effective at soliciting real user responses.
Finally, we study and compare the source of workers on crowdtur ng sites in different countries.
We  nd that crowdtur ng workers easily cross national borders, and workers in less-developed countries often get paid through global payment services for performing tasks affecting US-based networks.
This suggests that the continuing growth of crowdtur ng systems poses a real threat to U.S.-based online communities such as Facebook, Twitter, and Google+.
This study is the one of the  rst to examine the organization and effectiveness of large-scale crowdtur ng systems on the Internet.
These systems have already established roots in other countries, and are responsible for producing fake social network accounts that look indistinguishable from those of real users [34].
A recent study shows that similar types of behavior are also on the rise in the US-based Freelancer site [24].
Understanding the operation of these systems from both  nancial and technical angles is the  rst step to developing effective defenses to protect today s online social networks and online communities.
In this section, we introduce the core concepts related to crowd-tur ng.
We start by de ning crowdtur ng and the key players in a crowdtur ng campaign.
Next, we present two different types of systems that are used to effect crowdtur ng campaigns on the Internet: distributed and centralized.
Measurements of a distributed crowdtur ng system show that it is signi cantly less popular with users than centralized systems.
Thus we focus on understanding centralized crowdtur ng systems in the remainder of our paper.
The term crowdtur ng is a portmanteau of  crowd-sourcing  and  astrotur ng.  Astrotur ng refers to information dissemination campaigns that are sponsored by an organization, but are obfuscated so as to appear like spontaneous, decentralized  grassroots  movements.
Astrotur ng campaigns often involve spreading legally grey, or even illegal, content, such as defamatory rumors, false advertising, or suspect political messages.
Although astro-tur ng predates the Internet, the ability to quickly mobilize large groups via crowd-sourcing systems has drastically increased the power of astrotur ng.
We refer to this combined threat as crowd-tur ng.
Because of its use of real human users, crowdtur ng poses an immediate threat to existing security measures that protect online communities by targeting automated scripts and bots.
Crowdtur ng campaigns on the Internet involve three key actors:
 tur ng campaign.
The customer is responsible for paying for the monetary costs, and are typically are either related to or themselves the bene ciaries of the campaign.
ning and management.
The agent is responsible for  nding, managing, and distributing funds to workers to accomplish the goals of the campaign.
form speci c tasks in exchange for a fee.
Each campaign is structured as a collection of tasks.
For example, a campaign might entail generating positive sentiment for a new restaurant.
In this case, each task would be  post a single (fake) positive restaurant review online.  Workers who complete tasks generate submissions that include evidence of their work.
The customer/agent can then verify that the work was done to their satisfaction.
In the case of the restaurant review campaign, submissions are screenshots of or URLs pointing to the fake reviews.
Ideally, there is a one-to-one mapping between tasks and submissions.
However, not all tasks may be completed, and submissions may be rejected due to lack of quality.
In these cases, the number of submissions will not match the number of tasks for a given campaign.
The process for a crowdtur ng campaign is shown in Figure 1.
Initially, a customer brings the campaign to an agent and pays them to carry it out (1).
The agent distributes individual tasks among a pool of workers (2), who complete the tasks and return submissions back to the agent (3).
The agent passes the submissions back to the customer (4), who evaluates the work.
If the customer is satis ed they inform the agent (5), who then pays the workers (6).
Crowdtur ng systems are instances of infrastructure used to connect customers, agents, and workers to enable crowdtur ng campaigns.
These systems are generally created and maintained by agents, and help to streamline the process of organizing workers, verifying their work, and distributing payments.
We have observed two different types of crowdtur ng systems in the wild: distributed and centralized.
We now describe the differences between these two structures, highlighting their respective strengths and weaknesses.
Crowdtur ng systems are similar to crowd-sourcing systems like Amazon s Mechanical Turk, with the exception that they accept tasks that are unethical or illegal, and that they can utilize distributed infrastructures.
Distributed Architecture.
Distributed crowdtur ng systems are organized around small instant message (IM) groups, mailing lists, or chat rooms hosted by group leaders.
As illustrated in Fig-organizes the workers.
The advantage of distributed crowdtur ng systems is that they are resistant to external threats, like law-enforcement.
Individual forums and mailing lists are dif cult to locate, and they can be dissolved and reconstituted elsewhere at any time.
Furthermore, sensitive communications, such as payment transfers, occur via private channels directly between leaders and workers, and thus cannot be observed by third parties.
However, there are two disadvantages to distributed systems that limit their popularity.
The  rst is lack of accountability.
Distributed systems do not have robust reputation metrics, leaving customers with little assurance that work will be performed satisfactorily, and workers with no guarantees of getting paid.
The second disadvantage stems from the fragmented nature of distributed systems.
Prospective workers must locate groups before they can accept jobs, which acts as a barrier-of-entry for many users.
To test this, we located 14 crowdtur ng groups in China hosted on the popular Tencent QQ instant messaging network.
Despite the fact that these groups were well advertised on popular forums, they only hosted  2K total users.
Over the course of several days of observation, each group only generated 28 messages per day on average, most of which was idle chatter.
The conclusion we can draw from these measurements is that distributed crowdtur ng systems are not very successful at attracting workers.
As we will show in Section 3, centralized systems attract orders of magnitude more campaigns and workers.
Centralized Architecture.
Centralized crowdtur ng systems, illustrated in Figure 2b, are instantiated as websites that directly connect customers and workers.
Much like Amazon s Mechanical Turk, customers post campaigns and offer rewards, while workers sign up to complete tasks and collect payments.
Both customer and workers register bank information associated with their accounts, and all transactions are processed through the website.
Centralized crowdtur ng websites use reputation and punishment systems to incentivize customer and workers to behave properly.
The primary role of the agent in centralized architectures is simply to maintain the website, although they may also perform veri cation of submissions at the behest of customers.
The advantage of centralized crowdtur ng systems is their simplicity.
There are a small number of these large, public websites, making them trivial to locate by customers and workers.
Centralized software automates campaign management, payment distribution, and maintains per-worker reputation scores.
These features streamline centralized crowdtur ng systems, and reduce uncertainties for all involved parties.
The disadvantage of centralized crowdtur ng systems is their susceptibility to scrutiny by third parties.
Since these public sites allow anyone to sign up, they are easy targets for in ltration, which may be problematic for crowdtur ng sites that operate in legally grey-areas.
On the other hand, this disadvantage made it possible for us to crawl and analyze several large crowdtur ng websites.
We begin our analysis of crowdtur ng systems, by analyzing the volume of campaigns, tasks, users, and total revenue processed by the largest known systems.
We  rst describe the representative systems in our study along with our data gathering methodology.
We then present detailed results addressing these questions.
While a number of crowdsur ng systems operate across the global Internet, the two largest and most representative systems are hosted on Chinese networks.
Their popularity is explained by the fact that China has both the world s largest Internet population (485M) [25] and a moderately low per-capita income ( $3,200/year) [21].
Crowd-tur ng sites in China connect dodgy PR  rms to a large online user population willing to act as crowd-sourced labor, and have been used to spread false rumors and advertising [4, 20, 5].
This  Shui Jun  (water army), as it is commonly known, has emerged as a force on the Chinese Internet that authorities are only beginning to grapple with [9, 2].
This con uence of factors makes China an ideal place to study crowdtur ng.
In this section, we measure and characterize the two largest crowdtur ng websites in China: Zhubajie (ZBJ, zhubajie.
com) and Sandaha (SDH, sandaha.com).
All data on these sites are public, and we were able to gather all data on their current and past tasks via periodic crawls of their campaign histories.
Zhubajie and Sandaha.
The  rst site we crawled is Zhubajie (ZBJ), which is the largest crowd-sourcing website in China.
As shown in Table 1, ZBJ has been active for  ve years, and is well established in the Chinese market.
Customers post many different legitimate types of jobs to ZBJ, including requests for freelance design and programming, as well as Mechanical Turk-style  human intelligence tasks.  However, there is a subsection of ZBJ called  Internet Marketing  that is dedicated solely to crowdtur ng.
ZBJ also has an English-language version hosted in Texas (witmart.
com), but its crowdtur ng subsection only has 3 campaigns to date.
Unlike ZBJ, Sandaha (SDH) only provides crowdtur ng services, and is four years younger than ZBJ.
Crawling Methodology.
We crawled ZBJ and SDH in September, 2011 to gather data for this study.
We crawled SDH in its entirety, but only crawled the crowdtur ng section of ZBJ.
Both sites are structured similarly, starting with a main page that links to a paginated list of campaigns, ordered reverse chronologically.
Each campaign has its own page that gives pertinent information, along with links to another paginated list of completed submissions from workers.
All information on both sites is publicly available, and neither site employs security measures to prevent crawling.
Our crawler recorded details of all campaigns and submissions on ZBJ and SDH.
Campaigns are characterized by a description, start and end times, total number of tasks, total money available to pay workers, whether the campaign is completed, and the number of accepted and rejected submissions.
It also includes details for each submission entered by workers, including the worker username and UID, a submission timestamp, one or more screenshots and/or URLs pointing to content generated by the worker, and a  ag marking the submission as either accepted or rejected after review.
Both ZBJ and SDH make the complete history of campaigns available on their sites, which enables the crawler to collect data dating back to each site s inception.
Table 1 lists the total number of campaigns on each site, as well as the percentage that were usable for our study.
Data on some campaigns is incomplete because the customer deleted them or made them private.
Other data could be missing because either the campaign only provided partial information (e.g.
no task count or price per task), or the campaign was still ongoing at the time of our crawl.
Incomplete campaigns only account for 8% of the total on ZBJ and 12% on SDH, and thus have little impact on our overall results.
For clarity, we convert all currency values on ZBJ and SDH (Chinese Yuan) to US Dollars using an exchange rate of 0.1543 to 1.
General Statistics.
Table 1 shows the high-level results from our crawls.
ZBJ is older and more well-established than SDH, hence it has attracted more campaigns, workers, and money.
Campaigns on both sites each include many individual tasks, and task Active Since Zhubajie (ZBJ) Nov. 2006 Sandaha (SDH) March 2010 Total Campaigns (%)



 Total Workers



 Total Tasks



 Total Submissions (%)



 Total Accepted (%)



 Total Money

 Money for Workers

 Money for Website (%)

 Table 1: General information for two large crowdtur ng websites.
h t n o
 r e p s n g a p m a
 i














 h t n o
 r e p s r a l l o































 -

-

-

-

-

-

-

-

-

-











Acpt.
ZBJ Subm.
ZBJ Task ZBJ Acpt.
SDH Subm.
SDH Task SDH



 Tasks/Submissions per Campaign













 Submissions per Worker

 Figure 3: Campaigns, dollars per month.
Figure 4: Tasks, submissions per camp.
Figure 5: Submissions per worker.
count is almost three orders of magnitude greater than number of campaigns.
The number of submissions generated by workers in response to tasks is highly variable: on ZBJ only 36% of tasks receive submissions, whereas on SDH 130% of tasks receive submissions (i.e. there is competition among workers to complete the same tasks).
Roughly 50% of all submissions are accepted.
Most importantly, more than $4 million dollars have been spent on crowdtur ng on ZBJ and SDH in the past  ve years.
Both sites take a 20% cut of campaign dollars as a fee, resulting in significant pro ts for ZBJ, due to its high volume of campaigns.
Furthermore, Figure 3 shows that the number of campaigns and total money spent are growing exponentially.
The younger SDH has a growth trend that mirrors ZBJ, suggesting that it will reach similar levels of pro tability within the next year.
These trends indicate the rising popularity of crowdsur ng systems, and foreshadow the potential impact these systems will have in the very near future.
Figure 4 illustrates the high level breakdown of tasks and submissions on ZBJ and SDH.
There are three lines corresponding to each site: tasks per campaign, submissions per campaign, and accepted submissions per campaign.
Campaigns on ZBJ tend to have an order of magnitude fewer tasks than those on SDH.
Although both sites only accept  50% of submissions, the overabundance of submissions on SDH means that the number of accepted submissions closely tracks the required number of tasks, especially for campaigns with >100 tasks.
Campaign Types.
Crowdtur ng campaigns on ZBJ and SDH can be divided into several categories, with the  ve most popular listed in Table 2.
These  ve campaign types account for 88% of all campaigns on ZBJ, and 91% on SDH.
 Account registration  refers to the creation of user accounts on a target website.
Unlike what has been observed by prior work [24], these accounts are almost never used to automate the process of spamming.
Instead, customers request this service to bolster the popularity of  edgling websites and online games, in order to make them appear well traf cked.
Four campaign types refer to spamming in speci c contexts: QQ instant-message groups, forums, blogs, and microblogs (e.g.
Twitter).
Customers in China prefer to pay workers directly to generate content on popular websites, rather than purchasing accounts from workers and spamming through them.
Note, that QQ and forums represent a larger percentage of campaigns because their existence predates microblogs, which have only become popular Campaign Type Account Reg.
Forum Post QQ Group Microblog Blog Post Forum Post QQ Group
 Blog Post Microblog Num of Campaigns









 $/Camp.
$71 $16 $15 $12 $12 $48 $48 $47 $49 $49 $/Task Monthly Growth
 $0.35
 $0.27 $0.70

 $0.18
 $0.23
 $0.19
 $0.13
 $0.21 $0.19

 $0.27

 Table 2: The top  ve campaign types on ZBJ and SDH.
in China in the last year [25].
The last column of Table 2 shows the average monthly growth in number of campaigns, and shows that microblogs campaigns are growing faster than all other top-5 categories in both ZBJ and SDH.
As the popularity of social networks and microblogs continues to grow, we expect to see more campaigns targeting them.
Finally,  Q&A  involves posting and answering questions on social Q&A sites like Quora (quora.com).
Workers are expected to answer product-related questions in a biased manner, and in some cases post dummy questions that are immediately answered by other colluding workers.
Worker Characteristics.
We now focus our discussion on the behavior of workers on crowdtur ng websites.
Figure 5 shows that the total number of submissions per worker (including both accepted and rejected submissions) varies across the worker population, and even between ZBJ and SDH.
Roughly 40% of SDH workers only complete a single task, compared to 20% on ZBJ.
The average worker on both sites complete around 5-7 tasks each.
Figure 5 also reveals that a small percentage of extremely proli c workers (especially on SDH) generate hundreds, even thousands, of submissions.
Figure 6 plots the percentage of submissions from top workers ordered from most to least proli c.
The distribution is highly skewed in favor of these career crowdturfers, who are responsible for generating  75% of submissions.
We now examine the temporal aspects of worker behavior.
Figure 7 plots the time difference between a campaign getting posted online, and the  rst submission from a worker.
On SDH, 50% of campaigns become active within 24 hours, whereas on ZBJ (with its larger worker population) 75% of campaigns become active within
 ramp up: up to 15 days on ZBJ, and 30 days on SDH.
As we discuss s n o s s m b u
 i f o











 Top Workers (%) i s n o s s m b u
 i f o

















 Time (Days) ) % i ( s n o s s m b u
 i












 Hours in the Day Figure 6: Submissions by top workers.
Figure 7: Time to  rst response.
Figure 8: Daily submissions.
Price per Submission ($)









 y e n o
 f o











 Top Workers (%)



 Total Money per Worker ($)

 Figure 9: Submission prices.
Figure 10: Money per worker.
Figure 11: Money earned by top workers.
in Section 3.3, these slow moving campaigns have very speci c requirements that cannot be met by the vast majority of workers.
Figure 8 shows the correlation between time of day and number of submissions on ZBJ and SDH.
Most submissions happen during the workday and in the evening.
Slight drops around lunch and dinner are also visible.
This pattern con rms that submissions are generated by human beings, and not automated bots.
We now explore the monetary reward component of crowdturf-ing systems.
As is common on crowd-sourcing systems like Mechanical Turk, workers on ZBJ and SDH make a tiny fee for each accepted submission.
As shown in Figure 9, the vast majority of workers on ZBJ and SDH earn $0.11 per submission, although  20% of submissions command higher prices than this.
Workers must complete many submissions in order to earn substantial pay, leading to the proli c submission habits of career crowdturfers seen in Figure 6.
Note that this is a very different model from bid-for-tasks systems like the recent Freelancer study [24].
The total amount of money earned by most workers on ZBJ and SDH is very small.
As illustrated in Figure 10, close to 70% of workers earn less than $1 for their efforts.
The remaining 30% of workers earn between $1 and $100, making crowdtur ng a potentially rewarding part-time job to supplement their core income.
For a very small group of workers (0.4%), crowdtur ng is a full-time job, earning rewards in the $1,000 dollar range.
Not surprisingly, the distribution of monetary rewards matches this distribution.
As seen in Figure 11, the top 5% of workers take home 80% of the proceeds on ZBJ and SDH.
Clearly, a hardcore contingent of career crowdturfers is taking the bulk of the reward money by quickly completing many submissions.
Task Pricing.
The goal and budget of each crowdtur ng campaign affects the number and price of tasks in that campaign.
Figure 12 plots the correlation between the number of tasks in a campaign, versus the price per submission the customer is willing to pay.
The vast majority of campaigns with 1K-10K tasks call for generating numerous  tweets  on microblog sites.
We examine these tasks in more detail in Section 4.
Although the vast majority of campaigns call for many tasks with low price per submission, Figure 12 reveals that there is a small minority of well paying tasks.
In many cases, these campaigns only include a single task that can earn an accepted submission  $100 dollars.
We examined the 158 outlying tasks that earned  $10 and determined that they include a large range of very strange campaigns, some prominent examples include: ceive a percentage of the sales.
mid scheme to receive a large payment.
  Pyramid Schemes: Workers recruit their friends into a pyra-  Commissioned Sales: Workers sell products in order to re  Dating Sites: Workers crawl OSNs and clone the pro les of   Power-Users: These tasks call for a single worker who owns a powerful social network account, well-read blog, or works for a news service to generate a story endorsing the customer.
attractive men and women onto a dating site.
In this section, we study the broader impact of crowdtur ng by measuring the spread of crowdturf content on microblogging sites.
We gather data from Sina Weibo, the most popular microblogging social network in China that has the same look and feel as Twitter.
We study Weibo for two reasons.
First, as shown in Table 2, mi-croblogging sites and Weibo in particular are very popular targets for crowdtur ng campaigns.
Second, the vast majority of information on Weibo (i.e.  tweets  and user pro le information) is public, making it an ideal target for measurement and analysis.
We begin by introducing Weibo and our data collection methodology.
Next, we examine properties of crowdtur ng tasks and workers on Weibo.
Finally, we gauge the success of campaigns across the social network by analyzing the spread of crowdtur ng content.
Founded in August 2009, Sina Weibo is the most popular mi-croblogging social network in China, with more than 250 million users as of October 2011 [1].
Weibo has functionality identical to Twitter: users generate 140 character  tweets,  which can be replied to and  retweeted  by other users.
Users may also create directed relationships with other users by following them.
We focus our study of Weibo campaigns from ZBJ, because ZBJ has the most microblogging campaigns by far.
Of the 4,061 mi-




s k s a
 f o r e b m u







 Price Per Submission ($) i # s n g a p m a
 o b e
 i













 March Apr May Jun Jul Aug Months in the year 2011






 Weibo Accounts Submissions
 Number of   per Worker

 Figure 12: # of tasks vs. submission price.
Figure 13: Weibo campaigns in 2011.
Figure 14: Submissions and accounts.
All Users Workers Customers















 Customers Workers All Msgs.
Ratio of Followers to Users Followed

 1000 10000 100000 1e+06 1e+07 Messages per Campaign





 Pay per Tweet Pay per Retweet






 Maximum Crowdturf Cascade Depth Figure 15: Follow rates for Weibo users.
Figure 16: Messages per campaign.
Figure 17: Crowdtur ng cascades.
croblogging campaigns on ZBJ, 3,145 target Weibo.
As shown in Figure 13, the number of Weibo campaigns on ZBJ mirrors Weibo s rapid growth in popularity in 2011.
The goal of crowdtur ng campaigns on Weibo is to increase the customer s reach, and to spread their sponsored message throughout the social network.
These goals lead to three task types:  pay per tweet,   pay per retweet,  and  purchasing followers.  The most common task type is retweeting, in which the customer posts a tweet and then pays workers to retweet it.
Alternatively, customers may pay workers to generate their own tweets, laden with speci c keywords and URLs, or to have their accounts follow the customer s for future messages.
To increase the power of their campaigns, customers prefer workers who use realistic, well-maintained Weibo accounts to complete tasks.
Customers may not accept submissions from poor quality, e.g.
easily detected or banned, Sybil accounts.
Conversely, workers who control popular accounts with many followers can earn more per task than worker accounts with average popularity.
Data Collection.
Understanding the spread of crowdtur ng content on Weibo requires identifying information cascades [19].
Each cascade is characterized by an origin post that initiates the cascade, and retweets that further propagate the information.
Cascades form a directed tree with the origin post at the root.
In crowd-tur ng cascades, the origin post is always generated by a customer or a worker, but retweets can be attributed to workers and normal Weibo users.
Each campaign is a forest of cascade trees.
We crawled Weibo in early September, 2011 to gather data on the spread of crowdturf content.
The crawler was initially seeded with URLs that matched campaigns already found on ZBJ, and used simple content analysis to determine if each worker submission was an origin post or a retweet in order to differentiate between  pay to tweet  and  pay to retweet  tasks.
In the latter case, the crawler fetched the origin post using information embedded in the retweet.
Our crawler targets the mobile version of the Weibo site because it lists all retweets of a given origin post on a single page, including the full path of multi-hop retweets.
The crawler recorded the total number of tweets, followers, and users followed by each user involved in crowdtur ng cascades.
Unfortunately, Weibo only divulges the  rst 1K followers for each user, so we are unable to fully reconstruct the social graph.
Overall, our crawler collected 2,869 campaigns involving 1,280 customers.
These campaigns received submissions from more than
 users.
Among these, 2% of worker accounts were inaccessible, and were presumably banned by Weibo for spamming.
0.08% of the non-worker user accounts were inaccessible, and all customer accounts remained active.
 Pay per tweet  campaigns initiated 25,000 cascades, while  pay per retweet  campaigns triggered 5,000 cascades.
We ignore  purchase followers  campaigns, since they do not generate crowdtur ng cascades.
To get a baseline understanding of normal Weibo user accounts, we performed a snowball crawl of Weibo s social graph in October

 We begin by examining and comparing the characteristics of Weibo accounts controlled by workers and customers to those of normal Weibo users.
As shown in Figure 14, the number of accounts controlled by each worker follows the same trend as submissions per worker.
This is intuitive: workers need multiple accounts in order to make multiple submissions to a single campaign.
Hence, professional crowdturfers who generate many submissions need to control a commensurate number of accounts.
In absolute terms, we observe 14,151 accounts controlled by 5,364 ZBJ workers.
The top 1% of workers each control  100 accounts, but the average worker controls only  6 accounts.
Comparison to Normal Accounts.
We now compare characteristics of worker s and customer s accounts to normal users.
We  nd that each account type tweets with the same frequency.
This suggests that workers and customers are both careful not to overwhelm their followers with spam tweets.
Previous work on Sybil detection on OSNs showed that follow rate is an effective metric for locating aberrant accounts [31].
A user s follow rate is de ned as the ratio of followers to users followed.
Sybils often attempt to gain followers by following many other users and hoping they reciprocate.
Thus Sybils have follow rates <1, e.g.
they follow more users than they have followers.
Figure 15 shows the follow rates for different Weibo account types.
Surprisingly, normal users have the lowest follow rates.
Most worker accounts have follow rates  1, allowing them to eas-u o
 i r e p n g a p m a
 r e p .
g s
 .
g v






 Top 25% All Bottom 25%







 Time (Days) 1e+07 1e+06

 s e g a s s e
 f o #

 i s n g a p m a
 l a r i
 f o #







 n tl y V ir a l s i s t e n o







 # of Campaigns Viral All Median

 Money per Campaign ($) Figure 18: Message creation over time.
Figure 19: Cost of Weibo campaigns.
Figure 20: Viral campaigns per customer.
ily blend in.
This may represent a conscious effort on the part of workers to make their Weibo accounts appear  normal  so that they will evade automatic Sybil detectors.
Customers tend to have follow rates >1.
This makes sense, since customers tend to be commercial entities, and are thus net information disseminators rather than information consumers.
Much work has studied how to optimize information dissemination on social networks.
We analyze our data to evaluate the level of success in crowdtur ng cascades, and whether there are factors that can predict the success of social crowdtur ng campaigns.
Campaign Analysis.
We start by examining the number of messages generated by crowdtur ng campaigns on Weibo.
We de- ne a message as a single entry in a Weibo timeline.
A tweet from a single user generates f messages, where f is their number of followers.
The number of messages in a campaign is equal to the number of messages generated by the customer, workers, and any normal users who retweet the content.
Total messages per campaign represents an upper bound on the audience size of that campaign.
Since we have an incomplete view of the Weibo social graph, we cannot quantify the number of duplicate messages per user.
Figure 16 shows the CDF of messages generated by Weibo campaigns.
50% of campaigns generate  146K messages, and 8% manage to breach the 1M-message milestone.
As expected, workers are responsible for the vast majority of messages, i.e. there are very few retweets.
Considering the low cost of these campaigns, however, these raw numbers are nonetheless impressive.
Next, we want to examine the depth of crowdtur ng cascades.
Figure 17 plots the depth of cascades measured as the height of each information cascade tree.
Pay-per-tweet campaigns are very shallow, i.e. worker s tweets are rarely retweeted by normal users.
In contrast, pay-per-retweet campaigns are more successful at engaging normal users: 50% reach depths >2, i.e.
they include at least one retweet from a normal user.
One possible explanation for the success of pay per retweet is that normal users may place greater trust in information that is retweeted from a popular customer, rather than content authored by random worker accounts.
Next, we examine the temporal dynamics of crowdtur ng campaigns.
Figure 18 shows the number of messages generated per hour after each campaign is initiated.
The  all  line is averaged across all campaigns, while the top and bottom-25% lines focus on the largest and smallest campaigns (in terms of total messages).
Most messages are generated during a campaigns   rst hour (10K on average), which is bolstered by the high-degree of customers (who tend to be super-nodes), and the quick responses of career crowdturfers (see Figure 7).
However, by the end of the  rst day, the message rate drops to  1K per hour.
There is a two order of magnitude difference between the effectiveness of the top and bottom-25% campaigns, although they both follow the same falloff trend after day 1.
Factors Impacting Campaign Success.
We now take a look at factors that may affect the performance of crowdtur ng cascades.
The high-level question we wish to answer is: are there speci c ways to improve the probability that a campaign goes viral?
The  rst factor we examine is the cost of the campaign.
Figure 19 illustrates the number of messages generated by Weibo campaigns versus their cost.
The median line, around which the bulk of campaigns are clustered, reveals a linear relationship between money and messages.
This result is intuitive: more money buys more workers, who in turn generate more messages.
However, Figure 19 also reveals the presence of viral campaigns, which we de- ne as campaigns that generate at least two times more messages than their cost would predict.
There are 723 viral campaigns scattered randomly throughout the upper portion of Figure 19.
This shows that viral popularity is independent of campaign budget.
We look at whether speci c workers are better at generating viral campaigns.
We found that individual workers are not responsible for the success of viral campaigns.
The only workers consistently involved in viral campaigns are career crowdturfers, who tend to be involved in all campaigns, viral or not.
Surprisingly, a small number of customers exhibit a consistent ability to start viral campaigns.
Figure 20 plots the total number of campaigns started by each customer vs.
the number that went viral, for all customers who started at least 1 viral campaign.
The vast majority of customers initiate  3 campaigns, which makes it dif cult to claim correlation when one or more go viral.
However, the 20 customers (1.5%) in the highlighted region do initiate a sig-ni cant number of campaigns, and they go viral  50% of the time.
Since many of these customers do not actively participate in their own campaigns, this suggests that campaigns go viral because their content is of interest to Weibo users, perhaps because they are related to customers such as well-known actors or performers.
Our next step to understanding crowdsur ng systems involves a look from the perspective of a paying customer on ZBJ.
We initiate a number of benign advertising campaigns on different platforms and subjects.
By redirecting the click traf c through a measurement server under our control, we are able to analyze the clicks of workers and of users receiving crowdturf content in real-time.
We begin by describing our experimental setup before moving on to our  ndings, and conclude with a discussion of practical lessons we learned during this process.
Methodology.
Figure 21 depicts the procedure we use to collect real-time data on crowdtur ng clicks.
The process begins when we post a new campaign to ZBJ that contains a brief description of the tasks, along with a URL ( Task Info  in Figure 21) that workers can click on to  nd details and to perform the tasks.
The task details page is hosted on our measurement server, and thus any worker <a>Task Info</a> Measurement Server Online Store Redirection Spam Creation <a>Visit my store!</a> Target Networks s n o s s m b u
 i i f o








 Weibo Forums







 Time (Hours) r u o
 r e p   f o r e b m u





 Submissions Clicks Peak Submissions: 60



 Time (Hours)

 Figure 21: Crowdtur ng data collection.
Figure 22: Response time of ZBJ workers.
Figure 23: Long campaign characteristics.
who wants to accept our tasks must  rst visit our server, where we collect their information (i.e.
IP, timestamp, etc).
Referring workers to task details on external sites is a common practice on ZBJ, and does not raise suspicion among workers.
Workers that accept our tasks are directed to post spam messages that advertise real online stores to one of three target networks: Weibo, QQ instant message groups, and discussion forums.
The posted messages urge normal users to click embedded links ( Visit my store!  in Figure 21) that take them to our measurement server.
The measurement server records some user data before transparently redirecting them to the real online store.
We took care to preserve the integrity of our experimental setup.
Because some Chinese Internet users have limited access to websites hosted outside of mainland China, we placed our measurement server in China, and only advertised legitimate Chinese e-commerce sites.
In addition, we also identi ed many search engines and bots generating clicks on our links, and  ltered them out before analyzing our logs.
Campaign Details.
In order to experiment with a variety of topics and venues, we posted nine total campaigns to ZBJ in October
 campaigns (iPhone4S, Maldives, and Raf e), and targeted each at three distinct networks.
We discuss a fourth campaign, OceanPark, later in the section.
The  rst campaign promotes an unof cial iPhone dealer who imports iPhones from North America and sells them in China.
We launched this campaign on October 4, 2011, immediately after Apple of cially unveiled the iPhone 4S.
In the task requirements, we required workers to post messages advertising a discount price from the dealer on the iPhone 4S ($970).
The second campaign tried to sell a tour package to the Maldives (a popular tourist destination in China).
The spam advertises a 30% group-purchase discount offered by the seller that saves $600 on the total trip price ($1542 after discount).
The third campaign tells users about an online raf e hosted by a car company.
Anyone could participate in the raf e for free, and the prizes were 200 prepaid calling cards worth $4.63 each.
All campaigns shared the same set of baseline requirements.
Each campaign had a budget of $15 on each target network, and workers had a time limit of 7 days to perform tasks.
The desired number of tasks was set to either 50 or 100, depending on the campaign type.
Submissions were not accepted if the content generated by the worker was deleted by spam detection systems within 24 hours of creation.
These baseline requirements closely match the expected norms for campaigns on ZBJ (see Figure 4 and Table 2).
We applied additional requirements for campaigns on speci c networks.
For campaigns on the QQ instant messaging network, workers were required to generate content in groups with a minimum of 300 members.
For campaigns on user discussion forums, workers were only allowed to post content on a prede ned list of forums that receive at least 1,000 hits per day.
Each campaign type had additional, variable requirements.
For Maldives and Raf e campaigns, the price per task was set to $0.154, meaning 100 submissions would be accepted.
However, the price for iPhone4S tasks was doubled to $0.308 with an expectation of 50 submissions.
iPhone 4S tasks were more challenging for two reasons.
On Weibo, workers were required to tweet using accounts with at least 3,000 followers.
On QQ, workers needed to spam two groups instead of one.
Finally, on forums, the list of acceptable sites was reduced to only include the most popular forums.
Table 3 lists the high level results of from our crowdtur ng campaigns, including 9 short campaigns and the  OceanPark  campaign.
Seven of the short campaigns received suf cient submissions, and six were completed within a few hours (Time column).
Interestingly, workers continued submitting to campaigns even after they were  full,  in the hopes that earlier submissions would be rejected, and they would claim the reward.
In total, the short campaigns garnered 894 submissions from 224 distinct workers.
Figure 22 shows the response times of workers for campaigns targeting different networks.
We aggregate the data across campaign types rather than networks because workers  ability to complete tasks is based on the number of accounts they control on each network.
More than 80% of submissions are generated within an hour for Weibo and forum campaigns, and within six hours for QQ.
The  Msgs  column lists the number of messages generated by each campaign.
For Weibo campaigns, we calculate messages using the same methodology as in Section 4.
For QQ campaigns, messages are calculated as the number of users in all QQ groups that received spam from our workers.
We cannot estimate the number of messages for forums because we do not know how many users browse these sites.
We can understand the effectiveness of different crowdtur ng strategies by comparing the number of messages generated to the number of clicks (responses by normal users,  Clicks  column in Table 3).
We see that QQ campaigns are the most effective, and generate more clicks than Weibo campaigns despite generating only
 messages pop-up directly on users  desktops, leading to more views and clicks.
Tweets on Weibo, on the other hand, are not as invasive, and may get lost in the  ood of tweets in each user s timeline.
Forums perform the worst of the three, most likely because admins on popular forums are diligent about deleting spammy posts.
Finally, we try to detect the presence of Sybil accounts (multiple accounts controlled by one user) on crowdtur ng sites.
Column  W/IP  in Table 3 compares the number of distinct workers (W ) to the number of distinct IPs (IP ) that click on the  Task Info  link (see Figure 21) in each campaign.
If W >IP , then not all ZBJ workers clicked the link to read the instructions.
This suggests that multiple ZBJ worker accounts are controlled by a single user, who viewed the instructions once before completing tasks from multiple iPhone4S Maldives Raf e Network Weibo
 Forums Weibo
 Forums Weibo
 Forums OceanPark Weibo Subm.
Time Msgs.
45min














 6hr 3day 3h 4h 4h 2h 6day 1day 4day Clicks W/IP



















 Table 3: Results from our crowdtur ng campaigns.
accounts.
Our results show that W >IP for 66% of our campaigns.
Thus, not only do crowdturfers utilize multiple accounts on target websites to complete tasks (Figure 14), but they also have multiple accounts on crowdtur ng sites themselves.
Long Campaigns.
The campaigns we have analyzed thus far all required  100 tasks, and many were completed within about an hour by workers (see Figure 22).
These short campaigns favor career crowdturfers, who control many accounts on target websites and move rapidly to generate submissions.
To observe the actions of less proli c workers, we experimented with a longer campaign that required 300 tasks.
This campaign included an additional restriction to limit career crowdturfers: each ZBJ worker account could only submit once.
The goal of the campaign was to advertise discount tickets to an ocean-themed amusement park in Hong Kong on Weibo.
This campaign is listed as OceanPark in Table 3.
Figure 23 plots the number of worker submissions and clicks from Weibo users over time for the OceanPark campaign.
Just as in previous experiments, the  rst 100 submissions were generated within the  rst few hours.
Clicks from users on the advertised links closely track worker submission patterns.
Overall, 191 submissions were received on day one, 11 more on day two, and 2  nal submissions on day four, for a total of 204 submissions.
This indicates that there are  200 active Weibo workers on ZBJ: if there were more, they would have submitted to claim one of the 97 incomplete tasks in our campaign.
Discussion.
Our real-world experiments demonstrate the feasibility of crowd-sourced spamming.
The iPhone4S and Maldives campaigns were able to generate 491 and 218 click-backs (respectively) while only costing $45 each.
Considering that the iPhone
 $1,542, just a single sale of either item would be more than enough to recoup the entire crowdtur ng fee.
The cost per click (CPC) of these campaigns are $0.21 and $0.09, respectively, which is more expensive than observed CPC rates ($0.01) for traditional display advertising on the web [30].
However, with improved targeting (i.e. omitting underperformers like forum spam) the costs could be reduced, bringing CPC more in line with display advertising.
Our Maldives campaign is a good indicator of the effectiveness of crowdtur ng.
The tour website listed 4 Maldives trips sold to 2 people in the month before our campaign.
However, the day our Maldives campaign went live, 11 trips were sold to 2 people.
In the month after our campaign, no additional trips were sold.
While we cannot be sure, it is likely that the 218 clicks from our campaign were responsible for these sales.
In previous sections, we focused on the crowdtur ng market in China.
We now take a global view and survey the market for crowd-Website Amazon Turk (US) ShortTask
   MinuteWorkers (US) MyEasyTask (US) Microworkers (US) Paisalive (India) Campaigns.
% Crowd-tur ng





 Tasks $ per Subm.
$0.241

 $0.149

 $0.175


 $0.01 Table 4: Details of U.S. and Indian crowd-sourcing sites.
Data encompasses one month of campaigns, except ShortTask which is one year.
tur ng systems in the U.S. and India.
Additional crawls conducted by us, as well as prior work from other researchers, demonstrates that crowdtur ng systems in the U.S. are very active, and are supported by an international workforce.
Mechanical Turk.
Although prior work has found that 41% of tasks on Mechanical Turk were spam related in 2010 [15], our measurements indicate that this is no longer the case.
We performed hourly crawls of Mechanical Turk for one month in October 2011, and used keyword analysis to classify tasks.
As shown in Table 4, crowdtur ng now only accounts for only 12% of campaigns.
Other U.S. Based Sites.
However, the drop in crowdtur ng on Mechanical Turk does not mean this problem has gone away.
Instead, crowdtur ng has just shifted to alternative websites.
For example, recent work has shown that 31% of the jobs on Freelancer over the last seven years were related to search engine optimization (SEO), Sybil account creation, and spam [24].
Many SEO products are also available on eBay: trivial keyword searches turn up many sellers offering bulk Facebook likes/fans and Twitter followers.
To con rm this  nding, we crawled four U.S. based crowd-sourcing sites that have been active since 2009.
Since they do not provide information on past tasks, we crawled MinuteWorkers, MyEasy-Task, and Microworkers once a day during the month of October
 one year, hence we only crawled them once.
As shown in Table 4, keyword classi cation reveals that between 70-95% of campaigns on these sites are crowdtur ng.
We manually veri ed that the remaining campaigns were not malicious.
The types of campaigns on these sites closely matches the types found on Freelancer, i.e. the most prevalent campaign type is SEO [24].
Sites like ShortTask, Microworkers, and MyEasyTask  ll two needs in the underground market.
First, they do not enforce any restrictions against crowdtur ng.
This contrasts with Mechanical Turk, which actively enforces policies against spammy jobs [6].
Second, these sites enable a truly international workforce by supporting a wide range of payment methods.
Amazon requires workers to have U.S. bank accounts, or to accept cheques in Indian Rupees, and hence most  turkers  are located in the US (46.8%) and India (34%) [14].
However, alternative crowd-sourcing sites support payments through systems like Paypal and E-Gold, which makes them accessible to non-U.S. and non-Indian workers.
For example, Microworkers come from Indonesia (18%), Bangladesh (17%), Philippines (5%), and Romania (5%) [12].
Freelancers are also located in the United Kingdom and Pakistan [24].
Paisalive.
We located one crowdtur ng site in India called Paisalive that takes globalization even further.
As shown in Table 4, Paisalive is very small and the wages are very low compared to other services.
However, the interesting feature of Paisalive is that it is email based: workers sign up on the website, and afterwards all task requests and submissions are handled through email.
This design is geared towards enabling workers in rural populations constrained by low-bandwidth, intermittent Internet connectivity.
Crowd-sourcing Research.
Since coming online in 2005, Amazon s Mechanical Turk has been scrutinized by the research community.
This includes studies of worker demographics [14,
 chanical Turk to conduct user studies [18].
The characteristics of Micro Workers have also been thoroughly studied [12].
OSN Spam and Detection.
Researchers have identi ed copious amounts of fake accounts and spam campaigns on large OSNs like Facebook [8], Twitter [11, 32], and Renren [34].
The growing threat posed by this malicious activity has spurred work that aims to detect and stop OSN spam using machine learning techniques [3, 33, 31].
This body of research has focused on analyzing and defending against the outward manifestations of OSN spam.
In contrast, our work identi es some of the underlying systems used by attackers to generate spam and evade security measures.
Opinion Spam.
Spam that attempts to in uence the opinions and actions of normal people has become more prevalent in recent years [16].
Researchers have been working on detecting and characterizing fake product reviews [22, 17], fake comments on news sites [4], and astroturf political campaigns on Twitter [27].
The authors of [26] created a model to help classify deceptive reviews generated by Mechanical Turk workers.
These works reaf rm our results, that crowdtur ng is a growing, global threat on the web.
In this paper, we contribute to the growing pool of knowledge about malicious crowd-sourcing systems.
Our analysis of the two largest crowdtur ng sites in China reveals that $4 million dollars have already been spent on these two sites alone.
The number of campaigns and dollars spent on ZBJ and SDH are growing exponentially, meaning that the problems associated with crowdtur ng will continue to get worse in the future.
We measure the real-world rami cations of crowdtur ng by looking at spam dissemination on Weibo, and by becoming active customers of ZBJ.
Our results reveal the presence of career crowd-turfers that control thousands of accounts on OSNs, and manage them carefully by hand.
We  nd that these workers are capable of generating large information cascades, while avoiding the security systems that are designed to catch automated spam.
We also observe that this spam is highly effective, driving hundreds of clicks from normal users.
Finally, our survey of crowdtur ng sites in the U.S. and elsewhere demonstrates the global nature of this problem.
Unscrupulous crowd-sourcing sites, coupled with international payment systems, have enabled a burgeoning crowdtur ng market that targets U.S. websites, fueled by a global workforce.
As part of ongoing work, we are exploring the design and quantifying the effectiveness of both passive and active defenses against these systems.
Acknowledgments The authors wish to thank Zengbin Zhang for his help on ZBJ experiments.
This project is supported by NSF under IIS-0916307, and also supported by WCU (World Class University) program under the National Research Foundation of Korea and funded by the Ministry of Eduation, Science and Technology of Korea (Project No: R31-30007).
