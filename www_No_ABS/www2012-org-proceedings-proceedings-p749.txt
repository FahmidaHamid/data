The popularity of social networking websites has grown dramatically over the last few years, creating enormous collections of user-generated content online.
Photo-sharing sites have become particularly popular: Flickr and Facebook alone have amassed an estimated 100 billion images, with over 100 million new images uploaded every day [18].
People use these sites to share photos with family and friends, but in the process they are creating immense public archives of information about the world: each photo is a record of what the world looked like at a particular point in time and space.
When combined together, the billions of photos on these sites combined with metadata including timestamps, geo-tags, and captions are a rich untapped source of information about the state of the world and how it is changing over time.
Recent work has studied how to mine passively-collected data from social networking and microblogging websites to make estimates and predictions about world events, including tracking the spread of disease [11], monitoring for  res and emergencies [9], predicting product adoption rates and election outcomes [16], and estimating aggregate public mood [5, 22].
In most of these studies, however, there is either little ground truth available to judge the quality of the estimates and predictions, or the available ground truth is an indirect proxy (e.g.
since no aggregate public mood data exists, [22] evaluates against opinion polls, while [5] compares to stock market indices).
While these studies have demonstrated promising results, it is not yet clear when crowd-sourcing data from social media sites can yield reliable estimates, or how to deal with the substantial noise and bias in these datasets.
Moreover, these studies have largely focused on textual content and have not taken advantage of the vast amount of visual content online.
In this paper, we study the particular problem of estimating geo-temporal distributions of ecological phenomena using geo-tagged, time-stamped photos from Flickr.
Our motivations to study this particular problem are threefold.
First, biological and ecological phenomena frequently appear in images, both because photographers take photos of them purposely (e.g.
closeups of plants and animals) or incidentally (a bird in the background of a family portrait, or the snow in the action shot of children sledding).
Second, for the two phenomena we study here, snowfall and vegetation cover, large-scale (albeit imperfect) ground truth is available in the form of observations from satellites and ground-based weather stations.
Thus we can explicitly evaluate the accuracy of various techniques Coarsened satellite map Map estimated by Flickr photo analysis Figure 1: Comparing MODIS satellite snow coverage data for North America on Dec 21, 2009 with estimates produced by analyzing Flickr tags (best viewed on screen in color).
Left: Original MODIS snow data, where white corresponds with water, black is missing data because of cloud cover, grey indicates snow cover, and purple indicates no signi cant snow cover.
Middle: Satellite data coarsened into 1 degree bins, where green indicates snow cover, blue indicates no snow, and grey indicates missing data.
Right: Estimates produced by the Flickr photo analysis proposed in this paper, where green indicates high probability of snow cover, and grey and black indicate low-con dence areas (with few photos or ambiguous evidence).
for extracting semantic information from large-scale social media collections.
Third, while ground truth is available for these particular phenomena, for other important ecological phenomena (like the geo-temporal distribution of plants and animals) no such data is available, and social media could help  ll this need.
In fact, perhaps no community is in greater need of real-time, global-scale information on the state of the world than the scientists who study climate change.
Recent work shows that global climate change is impacting a variety of  ora and fauna at local, regional and continental scales: for example, species of high-elevation and cold-weather mammals have moved northward, some species of butter ies have become extinct, waterfowl are losing coastal wetland habitats as oceans rise, and certain  sh populations are rapidly declining [23].
However monitoring these changes is surprisingly dif cult: plot-based studies involving direct observation of small patches of land yield high-quality data but are costly and possible only at very small scales, while aerial surveillance gives data over large land areas but cloud cover, forests, atmospheric conditions and mountain shadows can interfere with the observations, and only certain types of ecological information can be collected from the air.
To understand how biological phenomena are responding to both landscape changes and global climate change, ecologists need an ef cient system for ground-based data collection to give detailed observations across the planet.
A new approach for creating ground-level, continental-scale datasets is to use passive data-mining of the huge number of visual observations produced by millions of users worldwide, in the form of digital images uploaded to photo-sharing websites.
Challenges.
There are two key challenges to unlocking the ecological information latent in these photo datasets.
The  rst is how to recognize ecological phenomena appearing in photos and how to map these observations to speci c places and times.
Fortunately, modern photo-sharing sites collect a rich variety of non-visual information about photos, including metadata recorded by the digital camera   exposure settings and timestamps, for example   as well as information generated during social sharing   text tags, comments, and ratings, for example.
Many sites also record the geographic coordinates of where on Earth a photo was taken, as reported either by a GPS-enabled camera or smartphone, or input manually by the user.
Thus online photos include the ingredients necessary to produce geo-temporal data about the world, including information about content (images, tags and comments), and when (timestamp) and where (geotag) each photo was taken.
The second challenge is how to deal with the biases and noise inherent in online data.
People do not photograph the Earth evenly, so there are disproportionate concentrations of activity near cities and tourist attractions.
Photo metadata is often noisy or inaccurate; for example, users forget to set the clock on their camera, GPS units fail to  nd  xes, and users carelessly tag photos.
Even photos without such errors might be misleading: the tag  snow  on an image might refer to a snow lily or a snowy owl, while snow appearing in an image might be arti cial (as in an indoor zoo exhibit).
This paper.
In this paper we study how to mine data from photo-sharing websites to produce crowd-sourced observations of ecological phenomena.
As a  rst step towards the longer-term goal of mining for many types of phenomena, here we study two in particular: ground snow cover and vegetation cover ( green-up ) data.
Both are critical features for ecologists monitoring the earth s ecosystems.
Importantly for our study, these two phenomena have accurate  ne-grained ground truth available at a continental scale in the form of observations from aerial instruments like NASA s Terra earth-observing satellites [12, 19] or networks of ground-based observing stations run by the U.S. National Weather Service.
This data allows us to evaluate the performance of our crowd-sourced data mining techniques at a very large scale, including thousands of days of data across an entire continent.
Using a dataset of nearly
 can potentially be a reliable resource for scienti c research.
An example comparing ground truth snow cover data with the estimates produced by our Flickr analysis on one particular day (December
 sparse in places with few photographs, while the satellite data is missing in areas with cloud cover, but they agree well in areas where both observations are present.
This (and the much more extensive experimental results presented later in the paper) suggests that Flickr analysis may produce useful observations either on its own or as a complement other observational sources.
To summarize, the main contributions of this paper include:   introducing the novel idea of mining photo-sharing sites for geo-temporal information about ecological phenomena,   introducing several techniques for deriving crowd-sourced observations from noisy, biased data using both visual and textual tag analysis, and   evaluating the ability of these techniques to accurately measure these phenomena, using dense large-scale ground truth.
A variety of recent work has studied how to apply computational techniques to analyze online social datasets in order to aid research sociology and human interaction, such as how friendships form [8], how information  ows through social networks [21], how people move through space [6], and how people in uence their peers [4].
The goal of these projects is not to measure data about the physical world itself, but instead to discover interesting properties of human behavior using social networking sites as a convenient data source.
Crowd-sourced observational data.
Other studies have shown the power of social networking sites as a source of observational data about the world itself.
Bollen et al [5] use data from Twitter to try to measure the aggregated emotional state of humanity, computing mood across six dimensions according to a standard psychological test.
Intriguingly, they  nd that these changing mood states correlate well with the Dow Jones Industrial Average, allowing stock market moves to be predicted up to 3 days in advance.
However their test dataset is relatively small, consisting of only three weeks of trading data.
Like us, Jin et al [16] use Flickr as a source of data for prediction, but they estimate the adoption rate of consumer photos by monitoring the frequency of tag use over time.
They  nd that the volume of Flickr tags is correlated with with sales of two products, Macs and iPods.
They also estimate geo-temporal distributions of these sales over time but do not compare to ground truth, so it is unclear how accurate these estimates are.
In contrast, we evaluate our techniques against a large ground truth dataset, where the task is to accurately predict the distribution of a phenomenon (e.g.
snow) across an entire continent each day for several years.
Crowd-sourced geo-temporal data.
Other work has used online data to predict geo-temporal distributions, but again in domains other than ecology.
Perhaps the most striking is the work of Ginsberg et al [11], who show that by monitoring the geospatial distribution of search engine queries related to  u symptoms, the spread of the H1N1  u can be estimated several days before the of cial statistics produced by traditional means.
DeLongueville et al [9] study tweets related to a major  re in France, but their analysis is at a very small scale (a few dozen tweets) and their focus is more on human reactions to the  re as opposed to using these tweets to estimate the  re s position and severity.
In perhaps the most related existing work to ours, Singh et al [24] create geospatial heat maps (dubbed  social pixels ) of various tags, including snow and greenery, but their focus is on developing a formal database-style algebra for describing queries on these systems and for creating visualizations.
They do not consider how to produce accurate predictions from these visualizations, nor do they compare to any ground truth.
Citizen science.
While some volunteer-based biology efforts like the Lost Ladybug Project [3] and the Great Sun ower Project [2] use social networking sites to organize and recruit volunteer observers, we are not aware of any work that has attempted to passively mine ecological data from social media sites.
The visual data in online social networking sites provide a unique resource for tracking biological phenomena: because they are images, this data can be veri ed in ways that simple text cannot.
In addition, the rapidly expanding quantity of online images with geo-spatial and temporal metadata creates a  ne-scale record of what is happening across the globe.
However, to unlock the latent information in these vast photo collections, we need mining and recognition tools that can ef ciently process large numbers of images, and robust statistical models that can handle incomplete and incorrect observations.
We use a sample of nearly 150 million geo-tagged, timestamped Flickr photos as our source of user-contributed observational data about the world.
We collected this data using the public Flickr API, by repeatedly searching for photos within random time periods and geo-spatial regions, until the entire globe and all days between January 1, 2007 and December 31, 2010 had been covered.
We applied  lters to remove blatantly inaccurate metadata, in particular removing photos with geotag precision less than about city-scale (as reported by Flickr), and photos whose upload timestamp is the same as the EXIF camera timestamp (which usually means that the camera timestamp was missing).
For ground truth we use large-scale data originating from two independent sources: ground-based weather stations, and aerial observations from satellites.
For the ground-based observations, we use publicly-available daily snowfall and snow depth observations from the U.S. National Oceanic and Atmospheric Administration (NOAA) Global Climate Observing System Surface Network (GSN) [1].
This data provides highly accurate daily data, but only at sites that have surface observing stations.
For denser, more global coverage, we also use data from the Moderate Resolution Imaging Spectroradiometer (MODIS) instrument aboard NASA s Terra satellite.
The satellite is in a polar orbit so that it scans the entire surface of the earth every day.
The MODIS instrument measures spectral emissions at various wavelengths, and then post-processing uses these measurements to estimate ground cover.
In this paper we use two datasets: the daily snow cover maps [12] and the two-week vegetation averages [19].
Both of these sets of data including an estimate of the percentage of snow or vegetation ground cover at each point on earth, along with a quality score indicating the con dence in the estimate.
Low con dence is caused primarily by cloud cover (which changes the spectral emissions and prevents accurate ground cover from being estimated), but also by technical problems with the satellite.
As an example, Figure 1 shows raw satellite snow data from one particular day.
Our goal is to estimate the presence or absence of a given ecological phenomenon (like a species of plant or  ower, or a meteorological feature like snow) on a given day and at a given place, using only the geo-tagged, time-stamped photos from Flickr.
One way of viewing this problem is that every time a user takes a photo of a phenomenon of interest, they are casting a  vote  that the phenomenon actually occurred in a given geospatial region.
We could simply look for tags indicating the presence of a feature   i.e. count the number of photos with the tag  snow    but sources of noise and bias make this task challenging, including:   Sparse sampling: The geospatial distribution of photos is highly nonuniform.
A lack of photos of a phenomenon in a region does not necessarily mean that it was not there.
  Observer bias: Social media users are younger and wealthier than average, and most live in North America and Europe.
  Incorrect, incomplete and misleading tags: Photographers may use incorrect or ambiguous tags   e.g.
the tag  snow  may refer to a snowy owl or interference on a TV screen.
  Measurement errors: Geo-tags and timestamps are often incorrect (e.g.
because people forget to set their camera clocks).
A statistical test.
We introduce a simple probabilistic model and use it to derive a statistical test that can deal with some such sources of noise and bias.
The test could be used for estimating the presence of any phenomenon of interest; without loss of generality we use the particular case of snow here, for ease of explanation.
Any given photo either contains evidence of snow (event s) or does not contain evidence of snow (event  s).
We assume that a given photo taken at a time and place with snow has a  xed probability P (s|snow) , of containing evidence of snow; this probability is less than 1.0 because many photos are taken indoors, and outdoor photos might be composed in such a way that no snow is visible.
We also assume that photos taken at a time and place without snow have some nonzero probability P (s|snow) of containing evidence of snow; this incorporates various scenarios including incorrect timestamps or geo-tags and misleading visual evidence (e.g.
man-made snow).
Let m be the number of snow photos (event s), and n be the number of non-snow photos (event  s) taken at a place and time of interest.
Assuming that each photo is captured independently, we can use Bayes  Law to derive the probability that a given place has snow given its number of snow and non-snow photos, P (snow|sm,  sn) = P (sm,  sn|snow)P (snow)  pm(1   p)nP (snow) P (sm,  sn) `m+n = m P (sm,  sn) where we write sm,  sn to denote m occurrences of event s and n occurrences of event  s, and where p = P (s|snow) and P (snow) is the prior probability of snow.
A similar derivation gives the posterior probability that the bin does not contain snow, `m+n  qm(1   q)nP (snow) m P (sm,  sn) P (snow|sm,  sn) = where q = P (s|snow).
Taking the ratio between these two posterior probabilities yields a likelihood ratio, P (snow|sm,  sn) P (snow|sm,  sn) = P (snow) P (snow) q   p  m  1   p  n 1   q .
(1) This ratio can be thought of as a measure of the con dence that a given time and place actually had snow, given photos from Flickr.
A simple way of classifying a photo into a positive event s or a negative event  s is to use text tags.
We identify a set S of tags related to a phenomenon of interest.
Any photo tagged with at least one tag in S is declared to be a positive event s, and otherwise it is considered a negative event  s.
For the snow detection task, we use the set S={snow, snowy, snowing, snowstorm}, which we selected by hand.
The above derivation assumes that photos are taken independently of one another, which is generally not true in reality.
One particular source of dependency is that photos from the same user are highly correlated with one another.
To mitigate this problem, instead of counting m and n as numbers of photos, we instead let m be the number of photographers having at least one photo with evidence of snow, while n is the numbers of photographers who did not upload any photos with evidence of snow.
The probability parameters in the likelihood ratio of equation (1) can be directly estimated from training data and ground truth.
For example, for the snow cover results presented in Section 4, the learned parameters are: p = p(s|snow) = 17.12%, q = p(s|snow) =
 photo containing snow, whereas about 1 in 700 people take a photo containing evidence of snow at a non-snowy place.
Figure 1 shows a visualization of the likelihood ratio values for the U.S. on one particular day using this simple technique with S={snow, snowy, snowing, snowstorm}.
High likelihood ratio values are plotted in green, indicating a high con dence of snow in a geospatial bin, while low values are shown in blue and indicate high con dence of no snow.
Black areas indicate a likelihood ratio near 1, showing little conference either way, and grey areas lack data entirely (having no Flickr photos in that bin on that day).
The con dence score in the last section has a number of limitations, including requiring that a set of tags related to the phenomenon of interest be selected by hand.
Moreover, it makes no attempt to incorporate visual evidence or negative textual evidence   e.g., that a photo tagged  snowy owl  probably contains a bird and no actual snow.
We use machine learning techniques to address these weaknesses, both to automatically identify speci c tags and tag combinations that are correlated with the presence of a phenomenon of interest, and to incorporate visual evidence into the prediction techniques.
Learning tags.
We consider two learning paradigms.
The  rst is to produce a single exemplar for each bin in time and space consisting of the set of all tags used by all users.
For each of these exemplars, the NASA and/or NOAA ground truth data gives a label (snow or non-snow).
We then use standard machine learning algorithms like Support Vector Machines and decision trees to identify the most discriminative tags and tag combinations.
In the second paradigm, our goal instead is to classify individual photos as containing snow or not, and then use these classi er outputs to compute the number of positive and non-positive photos in each bin (i.e., to compute m and n in the likelihood ratio described in the last section).
Learning visual features.
We also wish to incorporate visual evidence from the photos themselves.
There is decades of work in the computer vision community on object and scene classi cation (see [27] for a recent survey), although most of that work has not considered the large, noisy photo collections we work with here.
We tried a number of approaches, and found that a classi er using a simpli ed version of GIST augmented with color features [14,28] gave a good trade-off between accuracy and tractability.
Given an image I, we partition the image into a 4   4 grid of 16 equally-sized rectangular regions.
In each region we compute the average pixel values in each of the red, green, and blue color planes, and then convert this color triple from sRGB space to the CIELAB color space [15].
CIELAB has a number of advantages, including separating greyscale intensity from the color channels and having greater perceptual uniformity (so that Euclidean distances between two CIELAB color triples are approximately proportional to the human perception of difference between the colors).
For each region R we also compute the total gradient energy E(R) within the grayscale plane Ig of the image,
 = || Ig(x, y)|| pIx(x, y)2 + Iy(x, y)2,

 (x,y) R (x,y) R where Ix(x, y) and Iy(x, y) are the partial derivatives in the x and y directions evaluated at point (x, y), approximated as, Ix(x, y) = Ig(x + 1, y)   Ig(x   1, y), Iy(x, y) = Ig(x, y + 1)   Ig(x, y   1).
For each image we concatenate the gradient energy in each of the
 values for each of the 16 bins), to produce a 64-dimensional feature vector.
We then learn a Support Vector Machine (SVM) classi er from a labeled training image set.
We now turn to presenting experimental results for estimating the geo-temporal distributions of two ecological phenomena: snow

 Mean active Flickr users / day
 Approx.
city area (km2)

 User density (avg users/unit area) 112.4

 Mean daily snow (inches)

 Snow days (snow>0 inches) Number of obs.
stations












 Figure 2: Top: New York City geospatial bounding box used to select Flickr photos, and locations of NOAA observation stations.
Bottom: Statistics about spatial area, photo density, and ground truth for each of the 4 cities.
and vegetation cover.
In addition to the likelihood ratio-based score described in Section 4 and machine learning approaches, we also compare to two simpler techniques: voting, in which we simply count the number of users that use one of a set S of tags related to the phenomenon of interest at a given time and place, and percentage, in which we calculate the ratio of users that use one of the tags in S over the total number of users who took a photo in that place on that day.
We  rst test how well the Flickr data can predict snowfall at a local level, and in particular for cities in which high-quality surface-based snowfall observations exist and for which photo density is high.
We choose 4 U.S. metropolitan areas, New York City, Boston, Chicago and Philadelphia, and try to predict both daily snow presence as well as the quantity of snowfall.
For each city, we de ne a corresponding geospatial bounding box and select the NOAA ground observation stations in that area.
For example, Figure 2 shows the the stations and the bounding box for New York City.
We calculate the ground truth daily snow quantity for a city as the average of the valid snowfall values from its stations.
We call any day with a nonzero snowfall or snowcover to be a snow day, and any other day to be a non-snow day.
Figure 2 also presents some basic statistics for these 4 cities.
All of our experiments involve
 2010; we reserve the  rst two years for training and validation, and the second two years for testing.
Daily snow classi cation for 4 cities.
Figure 3(a) presents ROC curves for this daily snow versus non-snow classi cation task on New York City.
The  gure compares the likelihood ratio con -dence score from equation (1) to the baseline approaches (voting and percentage), using the tag set S={snow, snowy, snowing, snowstorm}.
The area under the ROC curve (AUC) statistics are 0.929, 0.905, and 0.903 for con dence, percentage, and voting, respec- tively, and the improvement of the con dence method is statistically signi cant with p = 0.0713 according to the statistical test of [29].
The con dence method also outperforms other methods for the other three cities (not shown due to space constraints).
ROC curves for all 4 cities using the likelihood scores are shown in Figure 3(b).
Chicago has the best performance and Philadelphia has the worst; a possible explanation is that Chicago has the most active Flickr users per day (94.9) while Philadelphia has the least (43.7).
These methods based on presence or absence of tags are simple and very fast, but they have a number of disadvantages, including that the tag set must be manually chosen and that negative correlations between tags and phenomena are not considered.
We thus tried training a classi er to learn these relationships automatically.
For each day in each city, we produce a single binary feature vector indicating whether or not a given tag was used on that day.
We also tried a feature selection step by computing information gain and rejecting features below a threshold, as well as adding the likelihood score from equation (1) as an additional feature.
For all experiments we used feature vectors from 2007 and 2008 for training and tested on data from 2009 and 2010, and used a LibLin-ear classi er with L2-regularized logistic regression [10].
Table 1 presents the results, showing that information gain (IG) and con -dence scores (Conf) improve the results for all cities, and that the classi er built with both IG and Conf generally outperforms other classi ers, except for Boston.
Figure 3(c) shows ROC curves from different classi ers for NYC and Figure 3(d) compares ROC curves for the 4 cities using the classi er using both feature selection and con dence.
Note that the machine learning-based techniques substantially outperform the simple likelihood ratio approach (compare Figures 3(b) and (d)).
Predicting snow quantities.
In addition to predicting simple presence or absence of a phenomenon, it may be possible to predict the degree or quantity of that phenomenon.
Here we try one particular approach, using our observation that the numerical likelihood score of equation (1) is somewhat correlated with depth of snow (R2=0.2972)   i.e., that people take more photos of more severe storms (see Figure 4).
Because snow cover is temporally correlated, we  t a multiple linear regression model in which the con dence scores of the last several days are incorporated.
The prediction on day t is then given by,

 i=0  i log(conft i) +   if conft   1 otherwise where conft represents the likelihood ratio from equation (1) on day t, T is the size of the temporal window, and the   and   pa-Table 1: Daily snow clasi cation results for a 2 year period (2009 2010) for four major metropolitan areas.
Features Accuracy Precision Recall F-Measure Baseline Tags Tags+Conf.
Tags+IG Tags+IG+Conf.
Tags Tags+Conf.
Tags+IG Tags+IG+Conf.
Tags Tags+Conf.
Tags+IG Tags+IG+Conf.
Tags Tags+Conf.
Tags+IG Tags+IG+Conf.
Boston



 Chicago























 Philadelphia































 (b) (c) (d) Figure 3: ROC curves for binary snow predictions: (a) ROC curves for New York City, comparing likelihood ratio con dence score to voting and percentage approaches, (b) ROC curves for 4 cities using the likelihood scores, (c) ROC curves from SVM classi ers with different features for New York City, and (d) ROC curves for 4 cities using the logistic regression (LibLinear) classi er with tags, information gain and con dence features.
(Best viewed in color.)
Figure 4: Time series of actual daily snow (top) and score estimated from Flickr (bottom) for New York City, 2007 2010.
rameters are learned from the training data.
We found that increasing T generally improves performance on the 4 cities, but that no additional improvement occurred with T > 3.
We can measure the error of our predictions with the root-mean-squared error between the time series of our predictions and the actual snow data (following [16]).
We achieve an RMS error of between about 1 and 1.5 inches across the 4 cities; Philadelphia has the largest error (1.44), followed by Boston (1.26), New York (1.15), and Chicago (1.06).
As an example, Figure 5 presents a visual comparison of the prediction time series versus the actual snow time series for Chicago.
An alternative way of evaluating the snow quantity estimates is to view it as a multi-way classi cation task.
We follow an existing snowfall impact scale [25] and quantize daily snow quantity into 7 buckets: no snow, 0-1 inches, 1-4 inches, 4-10 inches, 10-20 inches,
 predict the snow ranges for the four cities using the numbers of snow and non-snow users.
We include the numbers of users from the previous three days as extra features.
We use a Naive Bayesian classi er [17], which performed best on this task.
These multi-way classi cation results are better than a majority class baseline, with
 for New York, 84.0% for Boston, and 83.7% for Chicago (versus baselines of 80.5%, 85.1%, 75.6%, and 72.9%, respectively).
Predicting snow for individual cities is of limited practical use because accurate meteorological data already exists for these highly Figure 5: Comparing time series of actual daily snowfall (in mm) for Chicago with estimates using Flickr, for Jan 2009 Dec
 show actual values.
populated areas.
In this section we ask whether phenomena can be monitored at a continental scale, a task for which existing data sources are less complete and accurate.
We use the photo data and ground truth described in Section 4, although for the experiments presented in this paper we restrict our dataset to North America (which we de ned to be a rectangular region spanning from 10 degrees north, -130 degrees west to 70 degrees north, -50 degrees west).
(We did this because Flickr is a dominant photo-sharing site in North America, while other regions have other popular sites   e.g.
Fotolog in Latin America and Renren in China.)
The spatial resolution of the NASA satellite ground truth datasets is 0.05 degrees latitude by 0.05 degrees longitude, or about 5  
 nonuniform because lines of longitude get closer together near the poles.)
However, because the number of photos uploaded to Flickr on any particular day and at any given spatial location is relatively low, and because of imprecision in Flickr geo-tags, we produce estimates at a coarser resolution of 1 degree square, or roughly 100   100km2.
To make the NASA maps comparable, we downsample them to this same resolution by averaging the high con dence observations within the coarser bin.
We then threshold the con dence and snow cover percentages to annotate each bin with one of three ground truth labels:   Snow bin, if con dence is above 90 and coverage above 80,   Non-snow bin, if con dence is above 90 and coverage is 0,   Unknown bin, otherwise.
Our goal is to predict whether or not each geospatial bin had snow-cover on each day, given the photos from Flickr.
Retrieving snow or non-snow bins.
In many real applications, ecologists would be satis ed in  nding bins for which the phenomenon is present, rather than actually classifying all bins.
It is thus useful to view this problem as a retrieval task, in which the Figure 6: Precision and recall curves for retrieving snow (top) and non-snow (bottom) instances, where an instance is a single geo-spatial bin on a single day, using different techniques: (a) comparing the voting, percentage, and statistical con dence estimation techniques, (b) comparing different temporal smoothing strategies, (c) using classi ers to reject falsely-tagged snow images using visual and textual features.
goal is to identify bins likely to contain the phenomenon, or likely not to contain it.
We thus turn to evaluating the performance of our estimation techniques using precision-recall curves, where precision =

 recall =

 , where R is the set of retrieved bins and G is the set of correct bins according to the ground truth.
Precision-recall curves are also easier to interpret in situations where the classi cation baselines are so high, as in our case.
Figure 6(a) shows precision-recall curves for retrieving bins and days containing snow (top) and those not containing snow (bottom).
In total, these curves involve classifying about 7 million exemplars (each of which is a single geospatial bin on a single day), of which 11.0% have ground truth.
82.2% of the bins with ground truth are no-snow bins, while snow bins account for 17.8%.
We observe that the con dence method performs signi cantly better than the other two methods for retrieving snow bins, achieving about 98% precision at 0.2% recall, and about 80% precision at 1% re call.
For retrieving non-snow bins the three techniques are almost the same, and all three perform better than the random baseline.
While the precisions in these curves are high, the recall values are alarming low.
The main reason for this is that large areas of North America, particularly most of Canada and Alaska, have sparse populations resulting in a very limited number of photos uploaded in these areas.
We showed in the last section that accurate snow estimates can be inferred for highly populated cities; the low recalls here are because of low photographic density in much of the continent.
Restricting to speci c subsets signi cantly increases the density of observations: for example, the average number of photos per bin over our four years of data is nearly ten times larger for the northeast US compared to all of North America (70,398 vs 8,134).
The performance is signi cantly better in these more densely populated areas; for example, in the Northeast US the precision is 96.3% at a recall of 19.5% for snow retrieval, and 99.9% precision at 9.1% (b) (c) recall for non-snow retrieval.
Moreover, recall would naturally improve as our dataset grows; our sample of 150 million images is less than 3% of the photos on Flickr, and thus the recall would improve signi cantly if we had access to the entire dataset.
Temporal smoothing.
For many phenomena (including snow), the existence of an event on one day is strongly correlated with its existence on the next day.
Thus one way of addressing the sparsity of Flickr photos in some locations is to propagate evidence forward and backward in time.
To do this, we apply a Gaussian  lter on the Flickr con dence values for each bin in an attempt to achieve better recalls.
We vary the degree of smoothing by using Gaussians with different variance values.
We tried smoothing with many different parameters, including smoothing both forward and backwards in time, or in only one direction.
Figure 6(b) shows curves for several of the best combinations that we found, including the raw con -dence score (blue X s), 3 days before and after with variance 1.0 (brown triangles), 2 days before with variance 0.5 (red squares), 3 days before with variance 1.0 (blue circles), 5 days before with variance 5.0 (purple stars), and 3 days after with variance 1.0 (yellow + s).
We  nd that temporal smoothing three days before and after with variance 1.0 signi cantly improves performance for both snow and non-snow retrieval, increasing snow retrieval precision by about 7 percentage points at 1% recall.
Voting.
Voting performs worse than the statistical con dence given by the Bayesian likelihood ratio, but it is an interesting technique to study in more detail because of its simplicity.
Voting simply counts the number of users who have annotated at least one photo in a given bin and day with a snow-related tag.
Figure 7 plots precision versus the number of votes for snow retrieval.
The shape of these curve illustrates why crowd-sourced observations of the world can be reliable, if enough people are involved: as the number of votes for snow increases, it becomes progressively less likely that these independent observations are coincidental, and more likely that using the voting method.
Table 2: Taxonomy of manually-labeled false-positive photos (which have at least one snow-related tag despite being taken at a snowless time and place according to the ground truth).
Class little or distant man made no snow snow not sure Description photos with trace amount of snow or snow in the distance photos with snow made by humans (e.g.
at a ski slope) photos without visible snow photos with signi cant snow other photos # of photos




 they are caused by the presence or absence of an actual phenomenon.
It is interesting to notice that when there are 7 or more snow voters, snow prediction precision becomes 100%, while the same is true for non-snow prediction when the number of non-snow voters reaches 33 if there are no snow voters in the bin.
Case study of false positives.
To understand the failure modes of estimating attributes about the world from Flickr photos, we performed a case study of false positives   bins and days in which our Flickr mining predicted the presence of snow, but the NASA ground truth indicated that there was no snow cover.
In particular, we studied snow false positives at the operating point at which the likelihood ratio method gives a precision of 74.1% and a recall of
 total predictions are made (each corresponding to a single geospa-tial bin on a single day), 2,208 of which have valid ground truth.
Of these 2,208 bins, 1,636 (74.1%) are correctly classi ed, while the 572 false positive bins have a total of 1,855 photos tagged with one of the snow terms (despite the fact that they were taken at places and times in which the NASA satellite did not record snow).
We manually examined these 1,855 false positive photos and classi ed them into 5 different classes according to their visual content, as shown in Table 2.
Nearly 60% of these photos do actually appear to contain some snow; of these, 33% either show trace amount of snow or snow in the distance (usually on a distant mountain peak), and 8.6% have man-made snow that would not show up on the NASA maps (like in a zoo or ski slope), while only about 16% include a signi cant amount of natural snow.
About 40% of the photos tagged with a snow-related term do not appear to contain any snow at all; these are caused by mis-tagged images or snow-related tags that are used to describe something else (like the interference on a TV screen).
Figure 8 shows some sample false positives from each class.
For images that seem to contain natural snow, there are several possible explanations for why the ground truth does not indicate (a) (c) (b) (d) Figure 8: Sample photos that were not taken at a place and time with snow according to the ground truth, but that were uploaded with a snow-related tag: (a) photo with trace amounts of snow, (b) photo with distant snow, (c) photo with man made snow, and (d) photo with no snow (but with a  snowy egret ).
snow cover at that time and place.
One is that the satellite passes over at an unknown time of day, so it is possible that snowfall occurred after the satellite s observation was taken.
Another cause are photos with incorrect time stamps or geo-locations; we assume that such errors occur frequently, although it is hard to quantify the frequency just by looking at the photos.
Other photos clearly contain snow, but the amount is so little that it might not be visible from the satellite (e.g.
Figure 8(a)), or the snow is so far in the distance that it is in a different geospatial bin (e.g.
Figure 8(b)).
There are some cases where the Flickr evidence for snow is overwhelming, but the NASA ground truth does not indicate snow.
This could be caused by the timing issue described above, or by satellite resolution and con dence issues.
For example, on February 21,
 marked as a no-snow bin in the ground truth because the vast majority of it has zero snow coverage according to the satellite, but there is a small area within the bin that has low con dence (due to cloud cover) and probably corresponds to a snow squall.
Machine learning for tag selection.
Many of the above error modes can be addressed by training classi ers on textual tag and visual images features.
As discussed in Section 3.1, we are interested in two learning paradigms: the  rst is to learn combinations of tags that classify geospatial bins well according to the NASA ground truth, while the second task is to reduce false positives by rejecting photos that are tagged with a snow term but do not actually contain snow.
In the  rst task, we want to learn to classify whether a given bin contains snow on a given day, based on a binary feature vector encoding the set of tags used by all users in that bin on that day.
We tried four different classi ers to address this problem: REP-Tree, a fast decision tree learner which builds a decision tree using information gain and variance and prunes it using reduced-error pruning [13], Support Vector Machines (SVMs) [7], Discrimina-tive Multinomial Naive Bayes (DMNB) [26] and LibLinear clas-si er with L2-regularized logistic regression [10].
To reduce the large number of features (a total of 404324 tags), we compute information gain and keep all features (13442 tags) with information gain greater than zero.
Figure 9 presents ROC curves for this task, showing that the learned classi er outperforms the likelihood ratio from equation (1), and that feature selection with information gain the photo is classi ed as not snow.
If summer is not present, then the next few layers look at tags like  mountain,   clouds,   ski,   geese,  and  egret.  Machine learning to suppress false positives.
Finally, we consider using the photo classi er as a  lter while computing the likelihood ratios of Section 3.1, in order to reject photos that are marked with a snow tag but do not contain snow, using both visual and textual features.
For the textual features, we use the decision tree classi- er just described.
For visual features, we trained an SVM using the GIST-like visual features described in Section 3.1, on the same hand-labeled dataset of about 2,000 images explained above.
As with all other experiments, the training and testing sets were kept separate by training on data from 2007-2008 and testing on data from 2009-2010.
For the photos in these latter two years, we use our decision tree to try to  lter out false positives (photos tagged  snow  but not containing snow), and then recompute the likelihood ratio con dence score.
We  nd that using a classi er to reject false positives based on tags increased precision by nearly 10 percentage points, as shown in Figure 6(f): at 1% recall, precision increased from about 84% to to about 93% for snow retrieval.
For the visual features, we  nd a signi cant but more modest improvement, from about 84% to 86% at this level of recall.
Another important measure of the ecological state of the planet is vegetation cover.
We perform greenery versus no greenery predictions similarly to snow and no snow predictions using the Flickr con dence threshold method discussed in Section 3.1.
As with snow, the ground truth is obtained from down-sampling and thresh-olding the NASA MODIS greenery data which has the same resolution as the snow cover data with similar coverage and quality (con dence) values.
The Flickr greenery con dence values of bins are obtained in a similar way as with snow, except that we use a different set of target tags, including  tree,   trees,   leaf,   leaves , and  grass.  One important difference between the NASA greenery and snow datasets is that the greenery data is an average of daily observations spanning 16 days.
Thus our goal is to predict the geospatial distribution of greenery for each 16-day period of the year.
We require a bin to have no less than 50% greenery coverage and above middle quality to be considered as a ground truth bin.
We report experiments using two different de nitions of non-green bins: those having less than 1% coverage, and those having less than 5% coverage.
For the 50% and 1% threshold combination, 25.6% of the bins with ground truth are greenery bins, while for the 50% and 5% threshold combination, 15.8% of the bin with ground truth are greenery bins.
As shown in Figure 11, both curves outperform a random baseline for greenery prediction, but the estimates are not as accurate as those observed during in snow predictions.
There seem to be several reasons for this drop in performance.
One is that the boundary between greenery and no greenery seems more vague than the snow/no-snow boundary.
Moreover, the greenery ground truth data has a much coarser temporal resolution (16 days).
Finally, it s less clear which tags should be used to estimate greenery; using color analysis of the visual content of images may be a better approach, which we leave for future work.
We also tried a learned classi er to predict greenery/non-greenery bins based on the set of tags used by all users in each bin and on each day.
We used the LibLinear classi er [10] because it performed well in case of snow classi cation.
Figure 12 presents the ROC curve for this classi cation task, showing an equal-error rate of about 91.6%.
Figure 9: ROC curves for classifying whether a geo-bin has snow on a given day, comparing the LibLinear classi er with various tag features to the con dence method using hand-selected tags.
Figure 10: ROC curve for classifying whether photos contain snow, using decision trees with various features: AllTags includes all tags, IntersTags excludes tags corresponding to spe-ci c geographic areas, and AllTags+Time and IntersTags+Time include the month of the year as an additional feature.
and using the con dence ratio as an additional feature all improve performance.
Next we try the second learning paradigm, in which our goal is to examine photos that have a snow-related tag, and use the other tags as well as visual features to decide whether or not they actually contain snow.
For example, the classi er might learn that a photo with  snowy  should be discarded if it also contains the tag  egret,  since that photo is likely of a bird and not of actual snow.
For training these classi ers, we had a human judge evaluate 1,855 images and to annotate them as to whether or not they actually contain evidence of snow.
We used decision trees for this task because it is easy to understand and interpret what features the classi er is using.
In initial experimentation, we found that many of the most discriminative features were place names, like  sandiego  or  canada.  These geographic tags are understandably strongly correlated with snowfall, but we would like our classi er to base its decisions on the content of an image (because, for example, climate change might cause snowfall in San Diego some day, and we would like our classi er to be able to detect this).
To avoid selecting these tags, we  rst divide North America into four regions (northeast, northwest, southeast, southwest) and get the intersection of the sets of tags used in these four regions.
We then use only this set of intersected tags ( Inter-sTags ) for building the decision tree.
Besides tags, we also tried including the photo s timestamp month as an additional feature.
