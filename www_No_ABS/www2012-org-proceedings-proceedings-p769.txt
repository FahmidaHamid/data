Micro-blogging services such as Twitter, Tumblr and Weibo, have become very important tools for online users to share breaking news and interesting stories.
They are even used for organizing  ash mobs and protest groups.
For example, Twitter was used extensively in a number of events and emergencies, ranging from elections, earthquakes and tsunamis to playing an instrumental role in facilitating political upheavals in the Middle East.
Key Questions: In addition to its use as a content sharing platform, micro-blogging services like Twitter, along with other location sharing services such as Foursquare, Gowalla, and Facebook Places are nowadays supporting location services.
That is, users are able to specify their location in messages, either explicitly, by letting users choose their place, or implicitly, by enabling geo-tagging functionality.
This presents an exciting opportunity to answer a range of questions:
 ographic locations?
What is the inherent geographic variability of content?
people?
How does this vary across regions?
discover patterns in users  usage of micro-blogging services.
There exists a considerable body of research addressing these issues [12, 15, 7, 5, 4].
However, the analysis of data still poses a considerable challenge due to its size and due to the integration of a range of di erent attributes.
To our knowledge this is the  rst paper to address both scale, location and language modeling in an integrated fashion.
That is, we customize the model to be su ciently sparse to allow for a large scale in terms of users and locations.
Furthermore, we design an accurate and scalable inference algorithm.
Our algorithm allows us to discover language patterns and to extract users  interests from geo-tagged messages.
We data and the diversity of language variations used on Twitter.
In addition, there are many factors to in uence the language used in a tweet with a particular location.
For example, words used in a tweet certainly depend on the author and the location where the tweet is written.
A user in New York City might be interested in entirely di erent matters compared to a user in Beijing.
Moreover, the choice of words is clearly in uenced by the topic of the tweet.
Finally, location speci c language will cause the same event to be reported quite di erently in di erent locations (e.g.
a soccer game between Brazil and Italy being reported quite di erently in those two countries).
Thus, di erent geographical regions have di erent language variations and topics have di erent chances of being discussed in these regions.
It turns out that users tend to appear only in a handful of geographic locations [5].
This is useful in improving location accuracy in estimates.
The arising challenge is how to best integrate all these strands of information into a single model.
Prior work falls into two groups: Some work only models certain aspects of the problem described above while ignoring the remainder.
For instance [17] investigated how location information can be used to better understand patterns in social photo sharing services.
A Gaussian mixture model and a probabilistic topic model are combined to learn clusters of locations and latent topics.
However, no regional language models are learned and user preferences are also not taken into account.
Thus, models developed for such data are usually limited and cannot easily be applied to content-rich social media.
Similarly [5] proposed a two component Gaussian mixture model to study the mobility of users in a number of location sharing services.
However, their model does not incorporate content at all.
At the other end of the spectrum we  nd rather complex models, however, without the ability to scale to industrial size.
For instance [7] propose a model to predict locations of users in Twitter.
Their model has a global topic matrix and each region has di erent variation of this matrix.
However, the inference algorithm is complex.
Furthermore, the problem of over-parametrization makes it nontrivial to perform inference accurately.
Furthermore, previous models ignore user preferences.
Our Contribution: We propose a model that is both  exible enough to embed all reasonable components of content and geographical locations, as well as user preference modeling.
Moreover, it scales to real-world datasets to handle millions of documents and users.
In this paper, we address the problem of modeling geographical topical patterns on Twitter by introducing a novel sparse generative model.
It utilizes both statistical topic models and sparse coding techniques to provide a principled method for uncovering di erent language patterns and common interests shared across the world.
Our approach is vital for applications such as user pro ling, content recommendation and topic tracking and the method can be easily extended in a number of ways.
We show that interesting topics can be identi ed by the model and we demonstrate its e ectiveness on the task of predicting locations of new messages and outperform nontrivial baselines.
The main contributions are as follows:   An additive generative model of content and locations that incorporates multiple facets of micro-blogging environments in an integral fashion.
  Sparse coding techniques and Bayesian treatments are smoothly embedded in our modeling, resulting in an e cient and e ective implementation.
  Our model outperforms several state-of-the-art algorithms in the task of location predictions and it demonstrates interesting patterns in real-world datasets.
The paper is organized as follows.
In Section 2 we will brie y discuss some recent related work in terms of geographical modeling in micro-blogging environments.
In Section 3 we proceed with detailed description of the proposed model with implementation notes.
In Section 4 we compare our model with several state-of-the-art algorithms in a number of tasks and demonstrate its e ectiveness.
Finally, we conclude in Section 5 with discussions and future work.
We brie y review two lines of related research.
The  rst is a range of papers which use geographical language modeling in general while the second is a set of works which are speci cally tuned for Twitter data.
We are particularly interested in models and approaches that combine geographical modeling and language modeling to discover topics from geographical regions.
We summarize some of representative work here:   Mei et al. [12] propose a model based on Probabilistic Latent Semantic Indexing (PLSA) [11].
It assumes that each word is either drawn from a universal background topic or from a location and time dependent language model.
Inference is performed via EM.
However, the mixture coe cients between the background topic and other spatio-temporal topics ones is tuned manually.
Since the model uses PLSA, no prior distribution is (or could be) assumed.
Evaluation is carried out by showing anecdotal results.
  Later, Wang et al. [15] introduce a fully Bayesian generative model to incorporate locations.
Rather than working with real latitudes and longitudes, they have a  xed number of region labels and they assume that each term is associated with a location label.
For each word in a document, a topic assignment is  rst generated according to a multinomial distribution.
Then the term and the location are generated dependent on this topic assignment, according to two di erent multinomial distributions.
The inference is performed by Variational EM.
Again the evaluation is limited to anecdotal results.
  Sizov [13] propose a similar model to [15].
Rather than using a multinomial distribution to generate locations they replace it with two Gaussian distributions for generating latitude and longitude respectively.
For inference, this work uses Gibbs Sampling and the evaluation is done by showing anecdotal results, by measuring Deviation Information Criteria (a model complexity criterion similar to BIC), as well as classi cation accuracy using manually labeled data.
One of the drawbacks of the work is that they only use data from Flickr restricted to the greater London area.
al. [15].
However, they introduce the notion of global topics and local topics where more general terms are grouped into global topics and terms related to local events going to local topics.
The inference is performed by Gibbs Sampling.
Hao et al. [10] evaluate their model based on anecdotal results and some heuristic measurements.
  Yin et al. [17] propose a model is similar in spirit to Eisenstein et al. [7].
The terms and the location of a particular document are generated by a latent region.
The location is generated from a region by a normal distribution and the region is sampled from a multi-nomial distribution.
The prior is also placed into the model, however the inference is done by MAP-style EM rather than a fully Bayesian fashion.
The model is evaluated using perplexity and by showing anecdotal results.
  Wing and Baldridge [16] use an even simpler approach where documents are assigned to geodesic grids and thus a supervised learning method is utilized, essentially yielding to build na ve Bayes classi ers on geodesic grids.
Although there exists such attempts of modeling language patterns and geographical locations, most prior work does not consider users at all.
A second line of work covers models directly designed to work on Twitter data.
For instance, Eisenstein et al. [7] propose a model utilizing the correlations between global and local topics.
In their model, each author is assigned a latent region variable and an observed GPS location.
Terms and the actual GPS location are both conditioned on the latent region variable.
The topics to generate terms are local topics, which are derived from global topics.
The inference is done by Variational EM and the evaluation is done by measuring the accuracy of predicted location and showing anecdotal results.
Finally, Cho et al. [5] studied the problem of human mobility in location sharing services.
Their  ndings include that users tend to appear in a very limited number of places (e.g., o ce and home).
They demonstrated that it might be e ective enough to use a two component Gaussian mixture model to estimate users  locations.
It has been an active research area to incorporate different information sources into topic modeling.
For example, Chemudugunta et al. [3] propose a method to combine corpus-wide topics and document-speci c language patterns together by using a  switch  variable for each term in the document, becoming a popular scheme in topic modeling literature.
We use a  switch-free  approach in this work and therefore reduce the number of variables used in the model.
Last, for general patterns and analysis of social location sharing services, please refer to Cheng et al. [4].
We now introduce our model that addresses the problems raised in the previous sections.
We start with an overview of the basic components in Section 3.1 by discussing generative models without explicit switch variables.
This allows us to describe the basic aspects of our model in Section 3.2.
In order to learn more discriminative features, in Section 3.3, we impose L1 penalty on certain parts of our model, resulting Table 1: Notation Usage global region distribution Symbol Size  0

  user U   R user-dependent region distribution  0
  geo R   K region-dependent topic distribution  user U   K user-dependent topic distribution  0

  geo R   V region-dependent term distribution K   V a global topic matrix

  

 mean location of a latent region covariance matrix of a latent region global term distribution in a sparse modeling approach.
For geographical modeling, non-informative prior distributions are discussed in Section

 Our model is closely related to the Sparse Additive Generative model (SAGE).
The basic idea of the SAGE model is that the outcome variable is generated by the mixture of all components without any explicit indicator variable.
The key di erence to traditional mixture models is that the mixture occurs not in terms of the expectation parameters (i.e. the distribution) but in terms of the natural parameters of the exponential family model.
Such a model has the advantage that it can easily take a large number of aspects into account without having to infer a complex indicator variable distinguishing the set of causes.
To be more concrete, we take language modeling as an example.
Suppose we have a vocabulary V where each term v is generated by a background language model  0, a per-user background language model  u and a regional language model  g.
A conventional mixture model would attempt to represent the joint in uence of the three components by a linear combination of the associated densities.
Denote by p(v| ) an exponential family model of the form p(v| ) = exp ( v   g( )) where g( ) = logXv exp ( v) Here g( ) is often referred to as the log-partition function as it ensures that the distribution is properly normalized.
In particular for the discrete distribution  (v| ) is well-de ned for all choices of  .
We now combine the factors via P (v| 0,  u,  g) := p(v| 0 +  u +  g) (1) Unlike in traditional topic models, the formalism above does not require an indicator variable to specify which component to use in generating v.
In addition to additive modeling, di erent language models can be constructed in such a way as to incorporate more discriminative terms.
More specifically, in our model we choose  0 to denote the (baseline) log frequency of v in the dataset while other components are used to model the di erences between the baseline and the background model.
This idea is explored in [18, 6] to model topics.
Here, we extend it to model regions and topics jointly and to propose an e cient inference procedure.
We start the discussion with some notations in our model.
Each tweet d = {wd, ld, ud} consists of three parts: Here of word assumption, ld is a real-valued pair ld = {l0, l1}, representing the latitude and longitude where this tweet is written and ud is the user id for the author of the tweet.
For simplicity, we assume that all the tweets in our dataset are generated by a  xed vocabulary V and a  xed user base U.
Moreover, we assume that the geographical locations have been clustered into R latent regions.
Each region r   R is characterized by a mean location  r and a covariance matrix  r.
We assume that there are three types of language models: a) a background language model  0, b) a per-region language model  geo and c) a topical language model  .
All these language models are over the vocabulary V. Each tweet is in uenced by these three factors simultaneously.
Before describing the generative process of our model, on a high level, our model encodes the following intuitions:   Words used in a tweet depend on both the location and topic of the tweet.
  Di erent geographical regions have di erent language variations.
Topics have di erent chances to be discussed in di erent regions (e.g.
bull ghts in India are unlikely to occur; likewise Spaniards are unlikely to discuss Divali).
 0  user  user  0  

 r l z w
 Da

  0  geo  geo
 Figure 1: A graphical representation of our model   Draw a latent region index rd   p(rd| 0 +  user u )   Users tend to appear in a handful geographical loca-  Draw a topic index tions.
For each tweet, the model generates the location, the topic and terms in the tweet consecutively.
In our model, all locations are categorized into R latent regions.
For each tweet, we  rst choose from which latent region this tweet is written.
To generate the region index r, we utilize a multinomial model as follows: zd   p(zd| 0 +  user u +  geo r )   Draw a location ld = {l0, l1}   N ( r,  r) (2)   For each token w in wd draw w   p(w| 0 +  geo r ,  zd) This generative process applies to all tweets in the corpus.
The graphical representation of the generation process is shown in Figure 1.
As discussed in Section 3.1, the bene t of our approach is to learn discriminative features from data, rather than obtaining redundant ones in di erent components of the model.
In order to achieve this goal, we also impose prior distributions over certain parts of our model.
More speci cally, for the following components in the model, we impose zero-mean Laplace distributions.
The rationale is that users in certain regions are likely to draw their words either from a location independent distribution or from a small, i.e. sparse corpus of additional terms which are more prevalent in a given location rather than globally.
Likewise, we assume that topics consist of a background distribution of generic words plus a sparse set of additional words which are characteristic for the particular topic.
Note that we do not require these words to be unique.
That is, the word  jaguar  might for instance be more prevalent in the  animals  and in the  cars  topic.
However, we do not expect it to be prevalent in a large number of topics beyond what a background language model would indicate.
P `r| 0,  user u   = p`r| 0 +  user u   Here  0 is a global distribution over latent regions and  u is a user dependent distribution over latent regions for user u.
Each location ld is drawn from a latent region r by a region-dependent multivariate normal distribution ld   N ( r,  r).
(3) Once the region and the location is generated, a topic z is selected dependent on both the latent region and the author of tweet: (4) P `z| 0,  user u ,  geo r   = p z| 0 j +  user u,j +   geo r,j   Here  0 is a global distribution over topics,  user is a user-is a regional dependent distribution over topics and   distribution over topics.
The intuition is that the topic is heavily in uenced where this tweet is written and user preferences.
After generating the topic index z each word w in the tweet is generated by drawing from the aggregate distribution: u geo r P `w|z,  0,  geo r ,  z  = p`w| 0 +  geo r +  zd  .
In this case  0 parametrizes a global distribution over terms,  geo describes the a region-dependence and     RK V is a topic matrix where each row is a distribution over terms.
With the above speci cation the generative story for a single tweet d can be expressed as follows: (5)  0 r   L(0,  0)  geo z   L(0,  l) v   L(0,  0)  0  z,v   L(0,  t)  user u,r   L(0,  u)  user u,z   L(0,  u)  geo r,v   L(0,  l)  geo r,z   L(0,  r) where L( , b) is a Laplace distribution with mean   and scale parameter b.
A zero-mean Laplace prior has the same e ect as placing an L1 regularizer on these components, resulting in a sparse solution to the model.
Here, a sparse modeling approach does not only encourage more discriminative features to be learned, but also leads to a more e cient learning algorithm, which will be introduced below.
We use ISTA [2] algorithm to do sparse optimization in our work.
Note that besides Laplace distributions used in this paper, other distributions could be employed, too.
For instance using a normal distribution as prior on all elements amounts to a latent Gaussian process induced by the parameters.
Before we proceed with the inference algorithm, we introduce the following shorthands to simplify our notation: P (zd = k|  0,  user P (w = v|zd,  0,  geo u r ,  geo r ) =  u,r,k ,  ) =  r,z,v P (r = t |  0,  user u ) =  u,t We treat topic assignments z and latent region assignments r as latent variables and all other variables as model parameters.
A mixture between EM and a Monte Carlo sampler is utilized to e ectively learn all parameters for the model along the lines of [14].
In the E-step, we sample latent region assignments and topic assignments by  xing all other parameters by Gibbs sampling.
In the M-step, we optimize model parameters by  xing all latent region assignments and topic assignments.
We iterate this until convergence.
More speci cally, in the E-step, we iteratively draw latent region assignments and topic assignments for all tweets.
For each tweet, a latent region r is  rstly drawn from the following distribution, conditioned on the old topic assignments: r   P (ld| j ,  j )    u,j    u,j,k   Nd Yi=1  j,k,v (6) where P (ld| j,  j ) is the pdf function for a multivariate normal distribution and k is the old topic assignment.
After r is sampled, we sample the topic assignment z for the same tweet, conditioned on the newly sampled r: z    u,r,k   Nd Yi=1  r,z,v (7) where r is the new region index.
In the M-step, we maximize the log likelihood of the model with respect to model parameters by  xing all region and topic assignments obtained in the E-step.
For geographical modeling, the maximum likelihood estimation (MLE) of parameters can be obtained in closed form:  j =  Nj  j = Sj = =
 #(d, j)
 Xd=1
 #(d, j)   1 I(rd = j)ld (8)
 Xd=1 (ld    j )T (ld    j ) (9) where #(d, j) is the number of tweets assigned to region j.
Indeed,  j is set to the sample mean and  j is set to the sample variance.
For other parameters, unfortunately, no closed-form solutions exist.
Therefore, we adopt gradient-based optimization methods to maximize the likelihood.
Let L be the likelihood of the model.
The gradients of model parameters can be obtained as follows.
For  0 and  user, we have:  0 t (L) =
 Xu=1 d(u, t)  
 Xu=1 d(u) u,t  user u,t (L) = d(u, t)   d(u) u,t (10) where d(u, t) is the number of tweets produced by user u are assigned to the region t and d(u) is the total number of tweets generated by user u.
For the global topic distribution  0, user topic distributions  user and regional topic distributions  geo, we have:  0 k(L) =
 Xu=1 d(u, k)  

 Xu=1 Xt=1 d(u, t) u,t,k (11)  user u,k (L) = d(u, k)  
 Xt=1 d(u, t) u,t,k (12)   geo t,k (L) =
 Xu=1 d(u, t, k)  
 Xu=1 d(u, t) u,t,k (13) where d(u, k) is the number of tweets produced by user u assigned to the topic k and d(u, t, k) is the number of tweets written by the user u in the region t assigned to the topic k.
For the global language model  0, regional language models  geo and topical language models  , we have:  0 v(L) =
 Xt=1 n(t, v)  

 Xt=1 Xk=1 n(t, k) t,k,v (14)   geo t,v (L) = n(t, v)  
 Xk=1 n(t, k) t,k,v (15)  k,v(L) =
 Xt=1 n(t, k, v)  
 Xt=1 n(t, k) t,k,v (16) where n(d, v) is the number of times term v appearing in tweet d, n(t, k) is the number of terms associated to the topic k in region t, n(t, v) is the number of times term v appearing region t, n(t, k, v) is the number of terms v assigned to the topic k appearing in the region t. These gradients have an intuitive interpretation as the di erence of the true counts and their expected counts.
In the previous section, we use a point estimate of regional means and covariance matrices in each M-step based on samples obtained in the E-step.
However, this process is not very stable since only one sample of regional assignments instability would be to draw multiple samples per tweet and to use a set of samples for estimation purposes.
However, this would introduce an inner loop in the E-step for each tweet, thus signi cantly increasing sampling time.
Instead, we apply a Bayesian treatment to mean vectors and covariance matrices and do not estimate them explicitly in M-step.
The standard practice in multivariate normal distribution is to endow them with a set of conjugate parameters, that is, with a Gauss-Wishart prior.
This is computationally expensive.
A cheaper (and equally reliable) approach is to place a non-informative Je rey s prior over the values of the mean parameters, that is     Unif.
and a Je rey s distribution over the values of the covariance matrices to penalize large covariance matrices:
 The same treatment is also used in [1, 8].
By imposing these prior distributions, we can e ectively integrate out   and  , resulting in a collapsed Gibbs sampler for locations, similar to [9].
More speci cally, we sample r from the following distribution: r   T    Nr, Sr (n + 1) n(n   2) , n   2   u,j u,j,k Nd Yi=1  j,k,v (17) Here T (a, b, n) is a multivariate Student-T distribution with the location as a, the scale matrix as b and n degree of freedom.
Here,  Nr and Sr are sample mean and sample respectively, as de ned in (8).
Sampling r does not require us to re-estimate the values of mean and covariance matrix in the M-step and hence reduce the computation cost of the inference algorithm.
Several implementation notes warrant a detailed discussion here.
Firstly, the bottleneck of sampling z is to evaluate many exponential functions as we expand Equation (7): exp 0 i=1 exp 0 k +  user u,k +   i +  user geo r,k   geo r,i   u,i +   Nd
 exp 0 wi +   j=1 exp 0
 geo r,wi +  k,wi  geo r,j +  k,j 
 The key to speed up the sampling procedure here is to reduce the number of exponential functions to be evaluated.
We rewrite the above equation as: j +   i=1 k +  user u,k +   exph 0 geo r,k + wi +  geo r,wi +  k,wi    log
 Xi=1 exp 0 i +  user u,i +   Nd Xi=1 0 r,i   geo  Nd log
 Xj=1 exp 0 j +   geo r,j +  k,j i (18) The logarithm of a sum of components can be e ciently computed as logPi exp(xi) = m+log[Pi exp(xi m)] where m is the maximum element in xi and can be cached since they are constant in the E-step.
Therefore, we only need to calculate one exponential function for sampling z per tweet, which signi cantly reduces the computational cost.
The second technique to speed up the inference algorithm is to e ciently calculate gradients (14), (11), and (10).
A na ve calculation would lead to a very ine cient implementation.
Taking the gradients of   as an example, the expanded form of gradients is as follows:
 Xt=1 n(t, k, v)  
 Xt=1 n(t, k) exp( 0 v +  
 i=1 exp( 0 geo t,v +  k,v) i +   geo t,i +  k,i) where the second part of the gradients, which is the expected counts, requires the calculation for all the possible combinations of topics and latent regions.
However, because of sparse modeling in Section (3.3), we can e ectively calculate the second parts by utilizing the sparsity of the model as follows: n(k, v)   exp( 0 v) n(t, k)
 Xt=1 exp( 0
 Ct,k v)h exp(     
 Xt=1 Xt=1
 n(t, k) n(t, k)
 Ct,k
 Ct,k geo t,v )   1)i t,v )h exp( k,v)   1i geo exp( 0 v) exp(  i +   i=1 exp( 0 where Ct,k = PV geo t,i +  k,i).
The gradients are decomposed into three parts.
The  rst part is a global term for all terms and therefore can be calculated once and geo t,v are not cached.
The second part only exists for those   zero.
Similarly, the third part is nonzero only when both geo t,v and  k,v are not zero.
Thus, if we employ a reasonable   L1 regularizer on both regional and topical language models, most of those elements would be driven to zero and therefore the second and third parts can be very e ciently calculated.
Similar decomposition also works for other gradients.
The last but not the least important technique is how to initialize the model.
Di erent initialization values of parameters can lead to signi cantly di erent results.
Here, we use the following initialization steps.
Again, taking language models as an example, we  rstly initialize  0 as log frequencies of terms in the whole corpus and  geo as log frequencies of terms in region r minus the same term in  0.
Then, we initialize   as all zero and optimize over   by  xing  0 and  geo.
Similar strategy can be also applied to   and   values.
For latent regions, we initialize them by a K-Means algorithm.
r

 In this section, we demonstrate the e ectiveness of our model on real-world datasets.
We compare our model with several state-of-the-art models.
Our dataset is a sample of the Twitter Firehose stream1, issued to Yahoo!.
In Twitter, two types of location information are associated to tweets:
 graphical locations, each tweet is associated to a real-valued latitude and longitude vector.
For Twitter Places, we convert them into real-valued latitudes and longitudes.
After doing this, we remove all tweets without locations.
We also preprocess all the remaining tweets by detecting whether 1https://dev.twitter.com/docs/streaming-api/methods 2http://blog.twitter.com/2010/06/twitter-places-more- context-for-your.html s m k ( r o r r
 e g a r e v










 Baseline Topics Topics + Region Full Model (w/o Bayesian)









 The number of latent regions ) s m k ( r o r r
 e g a r e v














 Topics Topics+Region Full Model Topics(Bayesian) Topics+Region(Bayesian) Full Model(Bayesian)


 The number of latent regions



 Figure 2: The comparison of location prediction on Yahoo!
dataset.
The X-axis is the number of latent regions and Y-axis is the average Euclidean distance in kilometers (kms) between predicted locations and true locations.
Figure 3: The comparison of non-Bayesian models and Bayesian models on the task of location prediction on Yahoo!
dataset.
The X-axis is the number of latent regions and Y-axis is the average Euclidean distance in kilometers (kms) between predicted locations and true locations.
a tweet is in English.
This step is done by a dictionary based method.
We randomly sample 10,000 users from the dataset, with their full set of tweets between January 2011 and May 2011, resulting 573,203 distinct tweets.
The size of the dataset is signi cantly larger than the ones used in some similar studies (e.g, [7, 17]).
In addition to demonstrating that our model can discover interesting topics and users  geographical patterns, we also wish to show that our model can be used in a quantitative fashion.
Here, we focus on the task of location prediction for tweets.
Di ering from the work done by Eisenstein et al. [7] where their aim is to predict the location for a user and the way they de ned the location of a user may not be very appropriate (the  rst location shown in their dataset), our goal is to predict the location for each new tweet, based on the words used in the tweet and its authors  information.
Based on our statistics, only 1%   2% of tweets have either geographical locations (including Twitter Places) explicitly attached, meaning that we cannot easily locate a majority of tweets.
However, it has been shown (e.g., [5, 4]) that geographical locations can be used to predict users  behaviors and uncover users  interests and therefore it is potentially invaluable for many perspectives, such as behavior targeting and online advertisements.
In addition to our dataset, we also apply our model to an open source datasest3, denoted as CMU dataset, and compare the best reported results.
Evaluation Metric: For each new tweet, we predict its location as  ld.
We calculate the Euclidean distance between predicted value and the true location and average them over the whole test set 1 is the Euclidean distance function and N is the total number of tweets in the test set.
N P Dis( ld, ld) where Dis(a, b) Baselines: The following methods are used as baselines in our dataset to compare with the full model proposed in Section (3).
  Yin et al. [17]: Their method is essentially to have a global set of topics shared across all latent regions.
There is no regional language models in the model.
Besides, no user level preferences are learned in the model.
The prediction is done by two steps: 1) choosing the region index that can maximize the test tweet likelihood, and 2) use the mean location of the region as the predicted location.
We re-implemented their method in our work.
This method is denoted as Baseline.
  Our model without  geo,  user and  user: This is essentially very similar to Baseline.
The only di erence is that Baseline is under PLSA formalism and our model is in SAGE formalism.
We denote this method as Topics.
  Our model without  user and  user: This variation of our model can learn regional language models while user preferences are still missing here.
We denote this method as Topics + Region.
For the comparison on the CMU dataset, we compare with:   Eisenstein et al. [7]: The model is to learn a base topic matrix that can be shared across all latent regions and a di erent topic matrix as the regional variation for each latent region.
No user level preferences are learned in the model.
The best reported results are used in the experiments.
  Eisenstein et al. [6]: The original SAGE paper.
The best reported results are used in the experiments.
  Wing and Baldridge [16]: Their method is essentially to learn regional language models per explicit regions.
The best reported results are used in the experiments.
For our model, the prediction is conducted in two steps.
Firstly, a region index that can maximize the likelihood of test tweet is chosen.
Next, the mean location of the corresponding region is used as the predicted location.For Bayesian treatment of geographical modeling discussed in Section (3.5), the mean vectors are estimated after the whole inference algorithm  nishes.
Experimental Results: Firstly, we show the basic comparison between our model and other baselines discussed above on the Yahoo!
dataset.
The results are shown





 ) s m k ( r o r r
 e g a r e v


 Baseline Topics (Bayesian) Topics + Region (Bayesian) Full Model (Bayesian) Baseline Topics (Bayesian) Topics+Region (Bayesian) Full Model (Bayesian)






 ) s m k ( r o r r
 e g a r e v




 The Number of Topics













 The number of latent regions Figure 4: The comparison of models with di erent number of topics by  xing the number of latent regions (as 400) on Yahoo!
dataset.
The X-axis is the number of topics and Y-axis is the average Euclidean distance in kilometers (kms) between predicted locations and true locations.
Figure 5: The comparison of models with randomly selected users on Yahoo!
dataset.
The X-axis is the number of latent regions and Y-axis is the average Euclidean distance in kilometers (kms) between predicted locations and true locations.
in Figure (2).
In this experiment, we  x the number of topics to 50 for all models.
For all models, we adopt a  ve-fold cross validation setting.
The numbers reported here are averaged across di erent folds.
One major impression is that the average error decreases as the number of latent regions increases, although it becomes  at after 500 latent regions.
This makes sense because we predict the locations based on the mean locations of latent regions.
Therefore, the more regions the model has, the more  exible the prediction would be.
As we discussed above, Topics method is very similar to Baseline method and therefore, not very surprisingly, the performance of these two models is approximately the same.
For Topics + Region model, the performance is signi cantly better over Baseline model and Topics model.
The main reason might be that regional language models learn special terms for di erent regions and therefore these terms become discriminative when we perform location predictions.
Moreover, our sparse modeling approach also contributes to learned discriminative terms in regional language models.
By incorporating user regional preferences ( user), our full model performs the best on the Yahoo!
dataset.
This partially validates that users might have stable mobility patterns in their usage of micro-blogging environments and therefore we can learn this pattern through their historical content.
Indeed, Cho et al. [5] found that users who frequently use location sharing services demonstrate surprisingly stable patterns and they successfully used a two-component Gaussian mixture model to predict users  locations in the future.
Note that the full model used in this experiment is the one without Bayesian geographical modeling that is discussed in Section (3.5).
The next set of experiments is to show whether the Bayesian treatment of geographical modeling can lead to additional improvements of predication performance.
As we previously discussed, non-Bayesian modeling in locations may lead to unstable results.
The experimental setting follows the one used above and results are shown in Figure (3).
Two observations can be made from the  gure.
Firstly, all models with Bayesian modeling lead to signi cantly improvements over their non-Bayesian counterparts.
The second observation is that, although Bayesian modeling can improve the performance, major improvements still comes from whether certain components are  on  or  o .
In short, Bayesian modeling in locations enjoys better predictive performance and a more e cient inference algorithm, as discussed in previous sections.
All previous experiments are the ones with  xed topics and di erent latent regions.
Here we show how the predictive performance varies for di erent number of topics.
The basic setting remains the same as the previous two sets of experiments and the results are shown in Figure (4).
The main observation is that the performance does not change too much as the number of topics varies.
As we mentioned before, all these models make predictions based on the mean vectors of latent regions.
Therefore, a  xed number of regions will limit the predictive power of these models and hence the performance is sort of bounded in a range.
In other words, enlarging the number of topics does not give models the  exibility to learn regions well.
Another interesting experiment is not to randomly sample tweets but randomly sample users.
In this setting, all users in the test set are never shown in the training set and therefore we do not have su cient user preference data.
This setting might be more realistic in Twitter because the majority of users never use geo-related features and hence it is highly likely that some users will adopt this feature in the future.
In order to e ectively predict locations, we use the following strategy to learn a  prior  distribution for users.
Taking  user as an example, since the test user is not in our training set, we optimize over  user by  xing all other parameters on the  y.
Therefore, the obtained values for this user is essentially the prior regional distribution for this user, without any tweets observed.
After having this prior distribution, we can e ectively predict locations as usual.
We do this optimization for users on the  y for all other user-related parameters.
The results are shown in Figure (5).
The main observation from the  gure is that the performance from all models is signi cantly worse than the experiments with randomly selected tweets.
This partially validates that all these models su er from certain di culties for  new  users and  new  content.
However, the relative improvement of performance remains the same as previous experiments, suggesting that our model can learn reasonable prior distributions for users, in order to achieve better predictive performance.
For the CMU dataset, we download their dataset and run our model on it.
Note that previous models (e.g., [7, 16]) are designed to predict the locations for users.
In our case, we can do  ner grained predictions on tweet level.
To make fair





 [[7]]





 [[16]]





 [[6]]





 Topics





 Topics + Region Full Model











 Table 2: Comparison of models on CMU dataset.
All numbers are Kilometers.
For [7, 16, 6], the median number reported in the paper is used.
We do not rerun their models and only report numbers from corresponding papers.
comparisons, two strategies can be applied here: 1) obtain the predicted location for each tweet and take the mean locations over them and 2) obtain the dominant region index for tweets by the same user and use the mean value for it as the prediction.
In our experiments, we have tried both strategies and found no signi cant di erence between them.
Therefore, we only report the results from the  rst strategy.
The results are shown in Table (2).
Firstly, we see that our full model outperforms all previous models signi cantly.
In addition, as the number of latent regions increases, the predictive performance increases, which also validates the results in our Yahoo!
dataset.
Here is some analysis why our model outperforms others.
For [7] and [6], they used a topic-variation matrix per region, which might be too expensive to be applied over a large number of regions while the authors in those papers found that their model peaks at around 50 regions and 10 topics and the predictive performance deteriorates otherwise for excessive number of parameters, resulting in over tting.
In our case, we use global topics and background topics to factor out common words.
In addition, we use two signals: regional topic distribution and regional word unigrams.
For [7, 6], their model has a single location for all tweets per user.
On the contrary, our model assumes that each user has a distribution over regions and each tweet is associated with a region, thus we can accommodate user movements.
Also, their models used a two-stage training which does not enable the language model to in uence how many regions are needed.
However, we use a joint training procedure for both regions and topics and we re-sample the user regions in our training phase where their models assume that regions assignments are given at the  rst place.
In this section, we take one run of our full model on Yahoo!
dataset as an example to demonstrate what kinds of topics can be obtained.
Firstly, we show some samples of regional language models.
As we see in the previous section, these language models play a vital role in location predictions.
Since in our model, regions are latent variables and do not correspond to cities or regions in the real-world.
It might be di cult to demonstrate topics.
Here, we assign the mean vectors of latent regions to nearest existing cities and manually pick 5 cities as an example, shown in Table (3).
Terms are the ones with largest magnitudes in  geo.
It is very interesting to see that most top ranked terms are actually the name of these locations.
Remember that our method is fully unsupervised.
In addition, we can see that top ranked terms in di erent regions vary signi cantly.
Another interesting observation is that users tend to tweet with their locations when they are in airports.
This can be seen Entertainments lady bieber album music beats artist video listen itunes apple produced movies #bieber lol new songs Sports yankees match nba football giants wow win winner game weekend horse #nba Politics obama election middle east china uprising egypt russian tunisia #egypt afghanistan people eu Table 4: Examples of  , global topic matrix.
The terms are top ranked terms in each language model.
in region  United States->California->San Francisco  and  United Kingdom->England->London .
In addition to geographical language models, we also show some examples from the global topic matrix  .
These language models are designed so that broader topics will be captured here.
The examples are shown in Table (4).
Again, these topics are manually picked and the  title  of these topics is assigned by the authors of the paper since these topics are learned without any explicit labels.
We can see that these topics are relatively broad, compared to regional language models and widely discussed across regions.
Some topics might have captured recent unrest in the Middle East.
In this paper, we address the problem of modeling geographical topical patterns on Twitter by introducing a novel sparse generative model, which utilizes both statistical topic models and sparse coding techniques to provide a principled method for uncovering di erent language patterns and common interests shared across the world.
Our approach is vital for applications such as behavior targeting, user pro ling, content recommendation and topic tracking and the method can be easily extended in a number of ways.
We show that interesting topics can be identi ed by the model and we demonstrate its e ectiveness on the task of predicting locations of new messages and outperform nontrivial baselines.
Main contributions of this work include a) a sparse additive model of content and locations that incorporate multiple facets of micro-blogging environments without switch variables, b) sparse coding techniques and Bayesian treatments are smoothly embedded in our modeling, resulting in an e cient and e ective implementation and c) outperforms several state-of-the-art algorithms in the task of location predictions and demonstrate interesting patterns from real-world datasets.
For future work, we wish to model human mobility explicitly by introducing user level regional United States->New York->Brooklyn brooklyn ave  atbush avenue mta prospect 5th #brooklyn spotlight carroll bushwick museum broadway madison vanderbilt coney slope eastern subway new york pkwy #viernesnayobon #mets otsego greenwich starbucks United States->California->San Francisco sfo francisco san airport international millbrae terminal  ight burlingame bart mateo boarding bayshore telecommute landed heading bay airlines united bound  ying #sfo camino groupon caltrain moon tsa baggage california engineer valley United States->Pennsylvania->Philadelphia philadelphia #philadelphia phl #jobs market others #job street philly walnut septa chestnut the cherry sansom arch spruce citizens locust btw temple pennsylvania rittenhouse passyunk bitlyetq7a6 bookrenters pike international United Kingdom->England->London winds lhr hounslow terminal the cloudy mph ickenham bath heathrow temperature airport car only airways uxbridge sun splendid fair london british lounge tothers harmondsworth speedbird whens for stars day  ight dominos navigation brunel Australia->New South Wales->Sydney sydney #sydney bondi george street mascot domestic syd surry station cnr platforms harbour darlinghurst qantas hoteloxford eddy haymarket terminal wales australia chalmers uts pitt #marketing junction darling centre #citijobs citigroup druitt Table 3: Examples of  geo, geographical language models.
The terms are top ranked terms in each language model.
components.
considered for the task of location prediction.
In addition, temporal factors should also be
We thank Marco Pennacchiotti and Ana-Maria Popescu for helpful discussions.
In addition, we thank Twitter to provide the dataset and the permission for publication of this work.
We also appreciate the reviews from anonymous reviewers.
This material is based upon work supported in part by the National Science Foundation under Grant Numbers IIS-0545875 and IIS- 0803605.
