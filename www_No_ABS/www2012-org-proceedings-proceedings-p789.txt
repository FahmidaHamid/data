While integrating  opinions  from multiple sources, a system is often required to resolve con icts.
This arises in a variety of settings, but one common setting is that of an information integration system, in which multiple sources provide information about the same real-world entity.
If di erent, incompatible values are provided for the same attribute, the sources are said to con ict, and the process of resolving the con ict to assign a value is referred to as  corroboration  or  truth  nding .
This task is frequently challenging, as illustrated by the following example.
Consider a restaurant aggregator that seeks to compile a database of restaurants from three feeds, s1, s2, and s3.
Let e be phone attribute of one business,  Truya Sushi .
For example, both sources s1 and s2 might report that the current value of e is  555-1234 , while source s3 reports the current value as  555-4444 .
However, suppose a single value must be chosen to show  rst for  Truya Sushi .
There exist two broad classes of opinion aggregation techniques   (a) meta-learning approaches [2, 14], which would employ classi cation/regression with the values provided by each source as covariates, and (b) graph propagation approaches [6, 15], which propagate object properties to source properties and vice-versa via incident edges.
Most graph propagation approaches assume source independence, but recently, the source-independence assumption was relaxed by [1] which considered copying between sources, and by [3] which considered internal dependence between data items.
However, all work of which we are aware of has ignored an exceptionally important factor in estimating the value of an unknown variable from con icting sources, the history of updates, as we now illustrate.
Temporal Information Example: Consider two histories for e shown in Table 1 and 2.
Each scenario shows the values given for e by s1   s3 at two times in the past, t1 and t2.
Sources need not report values at the same time, this is for simplicity of presentation.
We now turn to the additional inferences that can be made.
The main idea is to change from modeling a single opinion to try to ascertain the latest true update to e. To illustrate this idea, consider the  rst scenario, in which source s3 has steadily asserted the  555-4444  number, while source s1 started with  555-4444  and t1 t2 s1 s2 s3





 t3 = now


 Table 1: Scenario 1 Source t1 t2 s1 s2 s3





 t3 =now


 Table 2: Scenario 2 s2 started with  555-8566 , but then both s1 and s2 evolved to  555-1234 .
In this scenario, given the greater accuracy scores for sources s1 and s2, it is reasonable to output  555-
that  555-4444  is an old value and s3 has never updated.
However, consider scenario two.
In this scenario, at previous time slices, s3 agreed with s1 and s2.
In this case, we argue that much more weight should be given to s3 s opinion, since it has either received a new update not yet seen by s1 and s2 or has accepted an update from a wildly inaccurate source.
This illustrates the need to consider what is the last true update, rather than the majority alone.
Other uses of history integration include tracking mobile units [17], determining the rate of change of a variable or attribute, estimating the typical lag introduced by each source s, etc.
In this paper, however, we focus on the estimation of Z, and leave these applications to future work.
Challenges: The history aggregation problem is hard due to three main challenges:
 entities.
E.g.
a restaurant changes its phone number, but a stream retained its old phone number.
a restaurant changes its street name to Picasso street, but a stream s update read Pic.
st..
bitrary time.
E.g.
one stream sent the update within minutes of actual change, while other stream sent an update after a week.
The above three challenges occur in practice due to the fact that the information streams are manually updated and may be reformatted.
Contributions: To our knowledge, this paper is the  rst to consider the implications of a value update history for truth corroboration.
corroboration as determining the value of an unknown history of real-world updates Z, based on the observation streams Sk for several sources k = 1, .
.
.
, n. In our model, we assume that updates in a stream Sk are noisy versions of an update in the real world from the past, and this is modeled using a latent mapping vector Ok.
Furthermore, we model the fact that streams can miss or repeat the same real world update by imposing a Marko-vian structure on the mapping vectors.
(i) mappings Ok corresponding to stream Sk, and (ii) true stream Z.
We show that when Z is known, then mappings can be optimally estimated using dynamic programming approach.
We also show that when Ok is known then Z can be estimated using particle  ltering method (and exact inference is possible under some natural functional assumptions).
Based on this we give two inference algorithms, (i) EMA that alternates inference of the two hidden variables by  xing one and optimizing the other, converging to a local optima, and (ii) Gibbs that samples a set of possible mapping vectors, and infers the best Z for the sampled set.
cluding one of estimating NFL scores via Twitter), the performance of our joint inference algorithms, and show that our model outperforms existing consensus based approaches that are either history-agnostic, or use history in a naive way.
Outline: Rest of the paper is organized as follows.
We describe the history integration problem in Sec.
2.
We present our model in Sec.
3 and the inference algorithms in Sec.
4 and 5.
Experimental evaluation is discussed in Sec.
6 and 7.
Finally we conclude by discussing related work in Sec.
8.
The state changes that occur in any real-world entity can be succinctly described through a temporal sequence.
We de ne a  temporal sequence  as follows.
Definition 1 (Temporal Sequence).
A temporal sequence   is a sequence of pairs [(v1, t1), .
.
.
,( vm, tm)], such that the following constraints hold:  i   [1, m), vi (cid:4)= vi+1  i   [1, m), ti < ti+1 (1) (2) We use the following notations: (i)  V (i) =v i is the ith value in the sequence, (ii)  T (i) = ti is the time corresponding to the ith value, (iii) | | (= m) is the number of entries in  , and (iv)  (1 : i) is all the entries of   starting from the  rst pair onwards to the ith pair.
Let Z = [(v1, t1), .
.
.
,( vm, tm)] be the temporal sequence that represents the state changes of an entity.
Note that since Z captures the changes in the entity value, consecutive values Z V (i) andZ V (i + 1) are di erent.
We call Z as an entity sequence.
Let n streams make independent observations of Z and publish their views in the form of temporal sequences S1, .
.
.
, Sn.
We call them as stream sequences.
The stream sequence S can have errors w.r.t Z obtained by applying one or more of the following four operations on Z.
OP1 Missing updates: Stream S may not report some of the updates in Z.
OP2 Noisy/Erroneous observations: A stream S may report a valueS (j) that is di erent from the observed value Z(i).
OP3 Duplicate observations: Stream S may report di er-ent noisy observations of the same value Z(i) multiple times (as say, S(j) andS (j + 1)).
OP4 Lagged observations: Stream S may report an observation Z(i) that occurred in the past (i.e., Z T (i)   ST (j)).
Nevertheless, we assume that updates in Z are reported in the same order byS .
ated using the (j   1)th update and the time of O[j]th update of Z.
Thus probability distribution over ST k (j) k (j   1) and Z T (Ok[j]), and is de-depends only on ST k (j   1), Z T (Ok[j])).
k (j)|ST noted as P (ST
 erated using the value of O[j]th update of Z.
Thus k (j) depends only on probability distribution over SV Z V (Ok[j]), and is denoted as P (SV k (j)|Z V (Ok[j])).
Figure 1 illustrates this generative probabilistic model pictorially.
Given this generative process for S, we can write the joint probability distribution of P (S, Z) as follows.
(cid:2)
 (cid:2)
 = P (Z)P (Ok|Z)P (ST k |Ok, Z)P (SV k |Ok, Z) (5)
 (domain knowledge)
   n(cid:3) k=1   n(cid:3) k=1   n(cid:3) |Sk|(cid:3) j=1 |Sk|(cid:3) j=1 |Sk|(cid:3) k=1 j=1 P (Ok[j]|Ok[j   1], Z) (miss/repeat)
 k (j)|ST k (j   1), ZT (Ok[j])) (lag)
 k (j)|ZV (Ok[j]))] (noise) The key assumptions made in our generative model and equation 5 is that given the complete history of the entity (Z) and the mapping of observed variables to Z (essentially O), the stream update values (SV ) are independent of each other.
This is true in most, but not all, practical scenarios.
For eg., some sources might be copying each other (see [3]), and hence be correlated, but we assume, for the model s simplicity, that copying sources have been detected, and removed from the analysis.
Recall that the history integration problem is to compute Z (cid:2) = arg maxZ{P (S, Z)}, where P (S, Z) is given by Equation 5.
Here we describe our inference techniques to compute Z (cid:2).
This involves  nding the set of updates Z V as well as the times when these updates occurred Z T .
We start this section by describing a special case, where the mapping vector O is known.
Then using the solution for the special case, we develop two approximate inference algorithms for the entire problem   Gibbs, an approach based on Gibbs Sampling [4], and EMA, an approach based on Expectation-Maximization.
In all approaches, the overall idea is to it-eratively  nd a mapping vector O (that represents miss-ing/repeat updates) based on the current estimates of Z, and then conditioned on the current O optimize for Z V and Z T (accounting for lags and noise).
Assume that we know the set of mapping vectors O then the problem is to maximize P (Z|S, O), where Z(i) forms the hidden state.
Furthermore, since each Ok maps the updates in a stream Sk to the updates in Z, the hidden states in Z form a HSMM with transitions dictated by the prior on Z and the emission from Z(i) given by the corresponding stream reports Sk(j), where Ok[j] =i .
Note that though this special case of problem with  xed O is similar to the one of HSMM inference, it has many distinguishing characteristics.
First, not every hidden state Z(i) emits an observation; we have to deal with missing values.
Moreover, some hidden states emit multiple observations (when multiple stream updates are mapped to the same Z(i)).
Second, Z(i) values are not discrete   in many of our experiments the Z V values are modeled using Gaussian distributions, and in general the transition and emission probabilities can be any distributions from the exponential family.
We address missing updates by allowing hidden states to emit a special null observation that has equal probability of being emitted by each of the hidden states.
To address multiple emissions, we think of the multiple observations as a single observation emitted with probability equal to the product of the emission probability of each observation (as they are conditionally independent given the hidden state).
Once missing/multiple updates are addressed, in the discrete case, Z can be inferred exactly using standard Viterbi algorithm for HMM(HSMM) inference.
In the continuous case, one can use the general technique of particle  lter-ing, also known as sequential monte carlo, for inferring the hidden values of Z.
However, it is not possible to perform exact inference for general prior distributions of Z, and one can use sequential importance sampling [5] to perform an approximate inference of Z.
Here we consider the problem when the mapping vectors O are unknown.
In the Gibbs algorithm, we begin by sampling a random O from the prior P (O|Z); this prior depends on the length of Z alone.
Since we do not know |Z|, we generate sample O s for various values of |Z| and we pick the one with the maximum likelihood.
Starting with this initial O, we iteratively  nd the Z, with the maximum P (Z|S, O) using techniques from Sec.
4.1 for a  xed O, and then  nd a new O by sampling from the posterior distribution of P (O|S, Z) as follows.
For all streams k: P (Ok|O k, S, Z)   P (Ok|Z)   P (Sk|Z, Ok) (6) The above sampling distribution can be used to sample Ok for all streams k. Z can be recomputed from the sampled O and the process can be repeated either until convergence or until a maximum number of iterations are performed.
The Gibbs algorithm can take many steps to converge.
An alternative is to use the Expectation Maximization technique.
When Z is known, we can estimate an optimal O, such that P (O|S, Z) is maximized.
This can be done independently for all streams, as P (O|S, Z) is k (j)|Z V (Ok[j]))   P (ST k (j)|Z T (Ok[j]))   n(cid:2) |Sk|(cid:2) k=1 j=1
  P (Ok[j]|Ok[j   1], Z) (7) where we used equation 5 to get above factorization.
We can use Viterbi algorithm [12] to get the best mapping (see algorithm 1).
EstimateMapping algorithm exploits the optimal substructure property and runs in O(m   (cid:2) 2) time, where(cid:2) is the length of Z and m denotes the number of updates in a stream.
Ok = EstimateMapping(Z, Sk) gives mapping for { ,   model missing update and   models noise and lag} Let  (i) = log P (O[1] = i|Z) Let  (j, i, k) = log P (O[j] = i|O[j   1] = k, Z) Let  (j, i) = log P (S(j)|Z(i)) l = |Z|, m = |S| for i = 1 to l do c[1, i] =  (i) +  (1, i) end for for j = 2 to m do for i = 1 to l do r = arg maxr{c[j   1, r] +  (j, i, r) : r   [1, i]} c[j, i] = c[j   1, r] +  (j, i, r) +  (j, i) d[j, i] = r end for end for O[m] = arg maxi{c[m, i] : 1   i   l} for j = m   1 to 1 do O[j] = d[j + 1, O[j + 1]] end for Return O stream k. The EMA algorithm takes an initial Z and then alternates between  nding O and Z iteratively.
These iterations are repeated until a  xed point is reached.
The EMA algorithm works with the intuition that in any iteration, the perturbations in O would improve the likelihood of the subsequent Z.
Note that EMA algorithm aims at  nding local maxima Z, O for the probability distributions P (Z, S, O).
The key problem with the EMA algorithm is that it can get stuck in a local optima, and its performance is critically dependent on the quality of initial Z.
One way to alleviate this is to choose a good initial Zinit using Gibbs and then run the EMA algorithm starting from Zinit; we denote this hybrid algorithm as Gibbs + EMA.
In this section, we describe an instantiation of our general model, where we assume natural functional forms for the various probabilities.
Our choices allow us to tractable solve the history integration problem.
We show in our experiments that this speci c model works well on a number of datasets.
We  rst describe the distributions used to model domain knowledge, missing/repeat updates, lags and noise (from Equation 5).
We then present tractable algorithms for estimating the most likely Z (cid:2) given O as well as for sampling from the posterior distribution of P (O|S, Z).
Finally we conclude this section with a brief note on parameter learning for our natural instantiation.
Domain Knowledge [P (Z)]: We assume that the prior on Z factorizes such that Z V (i) only depends on Z V (i   1), the previous di erent value in Z, and Z T (i) only depends on the time of the previous update Z T (i   1).
We use the Exponential distribution to model the intervals between consecutive updates: P (Z T (i)  Z T (i  1) =  )   exp( Z    ).
When Z V values are continuous, we use a Gaussian prior Z V (i)   N(Z V (i   1),  Z).
The  rst element, Z(1), of Z is considered to have a uniform prior.
Missing/Repeated Updates [P (Ok[j]|Ok[j 1], Z)]: The Instantiation Algorithm 2 Estimating Z T k (j) : k,  j, Ok[j] = |Z|} ZT (|Z|) =min{S T {If no mapping for last element of |Z|, then set |Z| = |Z|  1 and rerun algorithm} for i = |Z|  1 to 1 do if  k,  j, Ok[j] (cid:5)= i then {No mapping vector, cant estimate time of update} ZT (i) = ZT (i + 1)   ZT (i) = min{ST if ZT (i)   ZT (i + 1) then k (j) : k,  j, Ok[j] = i} else ZT (i) =Z T (i + 1)   end if end if end for Return ZT di erence between consecutive indices of a mapping vector Ok[j] andO k[j + 1] determine whether an update in Z is repeated/missed.
We model the distance between Ok[j] and Ok[j + 1], independent of Z, using a Poisson distribution: P (Ok[j + 1]   Ok[j] = x)   e  x/x!.
This is to model the fact that a stream typically gets one out of every   update.
k (j   1), Z T (Ok[j]))]: We assume that Lags [P (ST k (j   1) and model the lag, i.e.
k (j) is independent of ST
 k (j)  Z T (Ok[j])|, using an Exponential distribution that
 k (j)   penalizes streams with large reporting delays: P (ST Z T (Ok[j]) =  )   exp( k    ).
k (j)|ST k (j)|Z V (Ok[j]))]: There is no one model for Noise [P (SV the way values in the stream are misreported by streams.
this is very application speci c and depends on the kind of data being reported.
We present the following two example instantiations.
When values being reported are continuous (like in the weather readings or trading volume), one can k (j)   N(Z V (Ok[j]),  k).
When the use Guassian noise: SV reported values are discrete (like phone numbers, or football scores), one may use following simple noise model: SV (j) is the same asZ V (O[j]) with probability pk, and di erent with probability 1   pk.
In this section, we provide e cient algorithms to extend Sec.
4.1 for  nding Z given O on the speci c distributions instantiated in the previous section.
Since the prior on Z factorizes into independent terms containing only Z T and Z V , we can optimize for the times and values independently.
Finding Z T : When the lag follows an Exponential distri-k (j)   Z T (Ok[j]) =  )   exp( k    ), we bution, i.e. P (ST can exactly compute the time of the ith update in Z, by solving the following minimization (cid:3) min{ z[Z T (|Z|)   Z T (1)] + k (j)   Z T (i)]} (8)  k[ST  i, k, j Ok[j]=i Subject to following constraints Z T (i   1) < Z T (i) < Z T (i + 1) Z T (i)   min{ST k (j) :  k, j, Ok[j] = i} In general, Algorithm EstimatingZ T provides a valid solution to Z T that satis es the required constraints.
Under as shown in the lemma below.
Lemma 5.1.
Algorithm 2 always returns a feasible solution to Equation 8, and the solution is guaranteed to be optimal when    0 and  z    k,  k.
If  z >  k then we can use Z T as initial solution and sample Z T1 (i) uniformly in the range [Z T1 (i  1), Z T (i)] and pick the solution that minimizes Equation 8.
Finding Z V for continuous values: When Z V follows k (j)   Z V (Ok[j]) =  )   an Gaussian distribution, i.e. P (SV exp( k     2), we can exactly compute the values of the ith update in Z, by solving the following equation.
(cid:3)  k, j Ok[j]=i  k   SV k (j) (9)  z[Z V (i+1) + Z V (i-1)] + Z V (i) = (cid:4) 2 z +  k, j Ok[j]=i  k
 where   = 1/  Above equation is obtained by taking the Equation 7 on log scale and taking its derivative w.r.t Z V (i) and equating it to 0.
This is similar to Kalman Filter, and we can estimate the values in Z using an iterative algorithm.
Finding Z V for discrete values: For discrete values, in the absence of a prior on Z, estimating Z V (i) corresponds to k (j) = v) is  nding v such that the probability maximized.
We can show that v is just the majority update amongst all the stream updates that map to i Ok[j]=i P (SV (cid:5) Parameter learning.
Given Z and stream observation Sk, we can compute the error incurred by the stream.
The standard deviation of the error is our new estimate of  k.
Similarly the lag for all the observations can be used to compute most likely  k and the number of missing updates can be used to compute most likely  k.
This falls naturally from our underlying assumption that stream s noise, lag and missing characteristics are independent.
We experimentally evaluate the performance of our model on four datasets.
These datasets from diverse domains exhibit varied stream characteristics and thus validate the wide applicability of our model.
Twitter Dataset.
We used Twitter api to extract all the tweets posted on Twitter regarding NFL1 games, such as, Jets vs Charger, Raven vs Jaguar, Packers vs Vikings.
These games were played in October, 2011.
From the collected tweets, we removed all the tweets that did not mention the game score in the format \d+ - \d+.
We also discarded tweets containing words such as  predict  as these tweets were found to be speculative and were not a representative of the actual game score.
The resulting dataset consists of tweets from 20 users for Jets vs Chargers game, 37 users for Raven vs Jaguar game, 23 users for Packers vs Vikings game.
The extracted tweets were of the order of number of users, as users rarely posted score more than once.
Our goal in the experiment is to construct the game score timeline 1http://www.n .com/ # updates per game Num streams (users) per game # tweets per update per game Relevant tweets per game Non-relevant tweets per game Average stream lag (sec) # Missing update per stream






 Table 3: Twitter dataset characteristics aggregated for all games.
with this dataset.
Even thought the dataset looks small, it is extremely challenging due to fact that the underlying streams (users in this case) are missing a lot of updates, they have a large lag and many irrelevant tweets (see Table 3).2     Climate Dataset.
We consider the temperature data provided by Earth System Research Laboratory, USA.
The data, which is available for public download3, consists of monthly temperature means for all the locations on Earth from 2001 to 2010.
The dataset consists of 10512 gridded lat-locations (with a grid resolution of 2.5 itude) and a temperature series of length 120 (one entry per month).
longitude x 2.5 Our goal here is to estimate the series of mean temperatures for the 120 months between 2001 and 2010 of a given location based on its neighboring locations.
This is a reasonable goal due to high spatial auto-correlation present in the Earth science data.
The neighboring locations can be viewed as noisy streams with no lag and missing update.
We simiulate missing updates by deleting temperature reading from all stream at random points in time.
Random lags is simulated by adding  k   r to the time of the update, i.e.
k (i) = max{ST k (i  1), 20  i} +  k   r, where r is a random
 number between [0, 1] and  k is set randomly to an integer in the range [20, 100].
Note that this dataset is challenging as the stream s noise (unknown to our model) varies due to several environmental factors such as topology, height, latitude, longitude of the selected locations.
NASDAQ Dataset.
Our third dataset is NASDAQ s volume trading data.4 The dataset contains 5 minute updates of the trading volume that occurred from Jan 2008 to Dec
 dataset, using Guassian noise, Exponential lag and Poisson missing/repeated updates as described in Section 5.
Stream generation parameters are not known to our algorithms.
This dataset is interesting for two reasons: (1) It is very hard to come up with a prior for the volume trading data.
As a result the output of the model is completely dependent on the stream emissions.
(2) Trading data has several changes with small amplitude.
Baseline models can be tempted to combine several di erent updates together due to the close proximity of values.
Our model does not su er from this as stream observations can have random lags and they are penalized if two seemingly di erent updates (in terms of time) are being grouped together.
involving a team which have the same name as a NFL team (Winnipeg Jets and New York Jets).
3http://www.esrl.noaa.gov/psd/data/ 4http://www.eoddata.com/products/historicaldata.aspx thetically generated data.
We consider Z V = [1, 2, .
.
.
, m], where m is varied from 10 to 100 and consider 5-15 streams.
Unlike NASDAQ and Climate data, the time of updates (Z T ) is randomly initialized to non-regular spaced intervals (e.g.
Z T = [1, 3, 10, 38, 39, .
.
.]).
The stream generation process is as follows: A stream would pick elements of the Z vector sequentially and could perform the following three operations: a) Simulate missing update: Ignore the picked element and move to the next element with Bernouilli probability = pmissk, b) Simulate independent error: Add Gaussian noise with precision  k > 1, c) Simulate Lag: Publish the noisy update after lag governed by Uniform distribution in the range [1   10].
Note that streams for synthetic data di ers from NASDAQ data in terms of the lag and the missing update distributions.
Dataset Summary.
The above four datasets present different challenges to the model.
Twitter dataset presents an interesting aspect of how the model performs for sparse datasets.
On the other hand, the climate dataset presents challenges since the underlying noise is di cult to model5.
NASDAQ dataset presents challenges due to the small variability between adjacent updates.
Finally, with the synthetic dataset we present more extensive analysis of our model under several di erent conditions.
In this section, we present our results over the four datasets.
The main objective of our experiments is to show that our model can be adapted to run quite nicely for several different domains and under di erent conditions.
The second objective is to show that in comparison to an intuitive baseline model it performs better as it models all the parameters that a stream can exhibit while reporting the updates.
We compare our algorithm against three baseline approaches.
Our: We use the hybrid algorithm Gibbs + EMA that is described in Section 4 which runs a few iterations of Gibbs to  nd a Z and then runs EMA.
B1 (PickLastUpdate): This baseline picks the last update across all streams as the true state of entity at that time.
This algorithm performs a merge sort of updates based on ascending time order.
B2 (PickMajorityUpdate): This algorithm picks the majority of the stream updates at any given time.
In order to compute the merge stream it uses the following procedure: Consider that a stream published an update at time t. We collect the last update for all streams that was published at time t or before it.
The majority value amongst these updates is set in the merge sequence at time t. This is done for all t.
B3 (OptimalAlignment): This algorithm is based on the intuition presented in [17], where streams can have  xed lags.
The algorithm considers the optimal sequence alignment strategy to evaluate all possible alignments and then  xes O to compute Z that maximizes the joint-likelihood.
over missing points on Earth such as over Oceans, etc are extremly sophisticated and take into account tens of variables.
Our







 Precision Recall F-measure # Last correct





 Table 4: Precision recall of our model vs the best baseline model averaged over three games.
Baseline 3 performs better than the other two baselines for the selected datasets and hence for the sake of clarity we only present results with this baseline.
We treat each user as an independent stream providing updates about the game score.
For two teams playing a game, users can post score of team1  rst (7-0) or team2  rst (0-7), hence while computing Z, we consider both permutations of the scores and pick the con guration with least noise.
We used P oisson(  = 5) to model missing updates and Exponential(  = 10) to model stream lag.
The merging strategy is to consider the majority element amongst the stream updates that map to same index in Z (since scores can be thought of as discrete values).
Once the best Z is obtained, model parameters are recomputed and streams with relatively small likelihood P (S|Z, O) are discarded and Z is recomputed.
This is repeated for 2-3 rounds.
Figure 2 and 3 shows updates computed by our algorithm along with the corresponding mapping tweets.
Even though dataset was very sparse, with many streams having one or two updates, it was able to retain the key updates that constitute the real timeline of the game along with an accurate mapping of the tweets.
We also discovered that the model is able to correctly discard the set of tweets that were not related to the game score.
Comparison with Baseline models.
Next we compare the timeline predicted by our model with baseline models B2 and B3 (using the majority merging strategy).
We de ne precision as the fraction of updates output by the algorithm that are correct.
Similarly, we de ne recall as the fraction of actual updates of true timeline that are output by the algorithm.
Additionally, we also check whether the last update output by each algorithm is correct.
Table 4 shows the performance of our model in comparison with the best baseline model aggregated for all the selected game datasets.
Overall we see that our model has higher precision and F1 over the best baseline model.
We also observe that the last update of Z presented by our model is actually correct whereas this is not the case for the baseline.
This happens because after the end of the game, some tweets mention the number of wins and losses for a team, and the baseline model would consider that as an update.
On the contrary, our model correctly discards such updates from Z.
We selected several locations randomly for di erent latitudes and longitudes and considered their temperature series.
For each selected location, we considered 8 adjacent neighbors to it as 8 streams.
Typically locations are roughly reso-250 kms apart despite being neighbors due to 2.5 lution.
The mean of absolute temperature di erence between 8 neighbors to a location is 1.8 (on average) with a mean standard deviation of 0.93.
Our model is run with   strip, 7-0 hole.
Need to score now.
#jets (13:06): Lmaooooo dammmmmn Keller coughed up da ball #Chargers score thanks
 (14:10): Mark Sanchez to Plaxico Burress for a 3yd score to bring the #Jets closer to the #Chargers...14-10 (14:09): Sanchez #Jets to Burress.
score TD but trail #Chargers 14-10.
#Chargers!!!
You re up 21-10 and now you re losing 24-21 and the #Jets are ready to score again??
SMH.
(16:45): 27-21 JetsRT "@bonniblucakes: What was the score #jets ???"
(14:26): #Chargers score 21-10 (15:47): Sanchez to Burress again, 3-yard score, #Jets in front 24-21, 8:41 left (16:17): #Jets blank #Chargers in second half, score 17 unanswered to win, 27-21.
What a game.
Score




 Figure 2: Jets vs Chargers game score output by our algorithm along with the corresponding tweets.
(21:16): How the **** is the score still 0-0 ?
C Mon #ravens (21:18): 0-0.
Ravens hvn t showed up though.
Defense got a fumble tho.
Score (21:23): #Jaguars score the first points of the game with the field goal, up 3-0 over #Ravens with 1:42 left in the first quarter (21:23): #Jaguars score first 3-0 over #Ravens.
(23:39): **** score is 9-0?
What is this baseball?
Need more offense #ravens and #jaquars.
This is the #NFL -_-(22:51): #Jaguars up 9-0.
game, that s a threescore lead.
In this (23:47): Baltimore #Ravens finally score a TD.
#Jaguars lead cut to 9-7 with 2:02 left.
(23:52): 7-9 (01:58): Just checked the #Ravens score en
 route to work...
to JACKSONVILLE?!
Are you kidding me?!
Woeful.
Utter disgrace.
(00:59): #Jaguars defeat the #Baltimore #Ravens by a score of




 Figure 3: Raven vs Jaguars game score output by our algorithm along with the corresponding tweets.
Gaussian(  = 1) noise, Exponential( k = 10) lag, Poisson ( k = 0.1) missing update.
We also consider a prior on length of Z, as we aim to get a update vector of same length as true Z.
Once model computes Z, stream parameters are re-estimated and few more such rounds of algorithm is run.
We  rst compare the quality of a Z output by Gibbs + EMA and baseline B3 with respect to the true temperature series Z (cid:2) as the number of missing updates per stream increases.
Quality is computed as the sum of (Z V (m)   Z (cid:2)V (m))2 over all times (months) m for which Z outputs an update.
Figure 4(a) shows Gibbs+EMA outperforms the baseline B3.
Figures 4(b) shows the performance of our model in comparison with the best baseline B3 over the NASDAQ.
We plot the log of negative log-likelihood due to scale of the values, and so lower value implies that model has higher likelihood.
The likelihood of the true Z that generated the data is also plotted for absolute comparisons.
We see that Gibbs + EMA performs statistically signi cantly better than the best baseline (paired ttest, p <0 .01, 99% CI).
Moreover, the average absolute error for last element of Z (i.e.
current state of the entity) was at least 10 times lower for Gibbs + EMA compared to the best baseline B3.
Figure 4(c) shows the performance of Gibbs + EMA in comparison to B3 over the synthetic dataset.
Like in the NASDAQ data, we observe that our model performs much better than B3.
We perform further controlled experiments on the synthetic data to better evaluate our algorithm.
Comparison between Gibbs vs Gibbs+EMA: Figure 5 shows the model performance of the Gibbs and Gibbs+EMA algorithms for various values of |Z (cid:2)|.
We see that Gibbs + EMA slightly improves the performance of the model in comparison to stand alone Gibbs.
(cid:4) (cid:6) i (Z V (i)   Z (cid:2)V (i))2(cid:7) 1 E ect of variation in length of Z and the number of streams: Figure 6(a) plots the quality of the Z output by Gibbs + EMA as the number of streams increases.
Each line corresponds to a true Z (cid:2) of di erent lengths.
Quality is
 measured over the values as can clearly see that the absolute error in the prediction increases as the length ofZ (cid:2) increases.
This intuitively makes sense as we expect the error to increase for higher length Z (as more iterations might be required for mixing of O and inference).
We also see that as more streams are added, the error sharply goes down.
This also makes sense as adding more streams decreases the probability of missing updates.
Additionally, we observe that even though the underlying streams imperfect their aggregation is quite robust to noise, lags and missing updates.
E ect of One Good Stream: In several practical scenarios, there is one (or a few) good stream(s); i.e. streams with small lag, low error and small miss probability.
We simulate such a scenario by using a goodness criteria (g), such that,  g, have a few good streams miss updates with probability e  g, and noise with standard deviation lags with parameter e  g.
The rest of the streams are bad   miss updates with e probability picked uniformly between 0 and 1, have lags with parameter 10 and noise with standard deviation 1.
We call  g and as g is increased, streams stream goodness to be e Best Baseline




 r o r r e d e r a u q s n a e m t o o r




 Number of missing updates ) d o o h i l e k i l g o l   ( g o l






 True Our Best baseline

 Length of Z vector
 ) d o o h i l e k i l g o l   ( g o l







 True Our Best baseline

 Length of Z vector

 (a) Variance of our model s Z w.r.t.
true Z over the climate dataset.
(b) Model comparison over NASDAQ dataset.
(c) Model comparison over Synthetic dataset.
Figure 4: Performance on Climate, NASDAQ and Synthetic data ) d o o h i l e k i l g o l   ( g o l





 Gibbs + EMA Gibbs


 Length of Z

 Figure 5: Performance of Gibbs sampling algorithm vs Gibbs+EMA.
are even better.
In this scenario, we run our algorithm and compute the mean squared error of the output with the true Z.
Figure 6(b) shows the mean squared error on log scale.
For the sake of clarity, we do not plot the performance of the baseline models as they either perform equivalent or worse as compared to our model.
We observe here that as stream goodness is increases the error decreases.
The result also indicates that as more streams turns good then the error further decreases (log-linear).
E ect of One Good Parameter: Another special case is when one stream is good on only one of the parameters (noise, lags or missing/repeated updates).
To simulate this, we consider the stream goodness parameter (as discussed in previous result) and make one stream miss update with a very small probability, other with a small lag and a third with small noise.
Figure 6(c) shows the performance of the model in comparison where one stream has all the goodness parameters.
We see that when goodness is low then the errors are relatively close.
But when goodness increases then the gap between errors increase.
This happens because, we observe that if a stream which is less noisy but misses a lot of updates, then algorithm relies more on the updates presented by other stream, whereas a stream which is good in all the criteria practically dictates the inference of Z.
The history integration problem is most related to the following three  elds   temporal data integration, multiple sequence alignment and reconciliation of values in static data.
We review each of these  elds.
Temporal Data Within temporal data integration, perhaps the work most closely related to ours is [17] that studies inference for the purpose of mobile location tracking.
They also model the hidden variable (user s actual location), and have multiple observations streams, but with  xed (yet unknown) lags.
That model makes sense for the mobile location setting, since the lag of sensor s reading to the real world would be  xed, but might not be correct for scenarios discussed in this paper, when a single source may have varying lags for di erent observations .
Due to the assumption of  xed lags, their optimization problem is technically simpler, and can be solved by trying out all possible variations of the  xed lag (i.e. the Ok mapping).
This strategy would be too ine cient for our problem as the number of possible Ok mapping vectors is exponential in |Sk|, the number of observations in the source stream.
Apart from mobile domain, HSMM based models have been used in several major applications such as speech recognition, handwriting recognition, network tra c modeling, and functional MRI brain mapping.
Zheng et al. [16] presents in detail how inferencing is done in HSMM (also its variants) and presents a summary of its applications in several domains.
Temporal data is often aggregated in sensor networks with two critical di erences: (i) Lags for sensor reading are assumed to be known (Ok is known), and (ii) data is often aggregated (rather than integrated) as the domain for the hidden variable (e.g.
temperature) is often continuous, e.g.
[8].
Finally, a recent paper [7] considers the problem of dedu-plicating entities from a temporal stream of updates.
While their techniques model the evolution of an entity s attribute value, their focus is to cluster the temporal updates corresponding to the same entity, and not compute the correct value of the entity s attribute.
Multiple Sequence Alignment The goal of multiple sequence alignment [9, 10, 11, 13] is to construct an alignment of symbols in n sequences by adding gaps between consecutive characters in the sequence.
There is a penalty for adding gaps, and a penalty when two sequences have di ering characters at some position; an optimal alignment minimized the total pairwise penalty across all sequences.
While the history integration problem seems very similar to the alignment problem (since we are trying to align stream updates to real world updates using a mapping vector), there are key differences between the two.
For instance, we explicitly model time in our problem.
One could think of modeling time im-r o r r e d e r a u q s n a e m t o o r







 Z length = 3 Z length = 5 Z length = 10 Z length = 50



 Number of Streams
  5  10  15  20 ) r o r r e d e r a u q s n a e m t o o r ( g o l  25
 1e 1 1e 2 1e 3 1e 5

 Number of good streams

 ) r o r r e d e r a u q s n a e m t o o r ( g o l
  2  4  6  8  10  12


 One good parameter One good stream


 log(goodness) (a) E ect of varying length of Z and number of streams on absolute error.
(b) E ect of good stream on the squared error, where stream goodness varies from 1e   1 to 1e   5.
(c) One good stream vs One good parameter per stream Figure 6: Experiments on Synthetic data plicitly in the alignment problem by discretizing times and replaying the same value Z V (i) for all times between Z T (i) and Z T (i + 1), but this no longer is  exible enough to model all kinds of missing updates and lags.
Reconciliation of Static Data We also mention here some of the static techniques that do not look at historical updates for integration.
Thus they cannot directly be applied for history integration, but we mention them for the sake of completeness.
In his seminal paper, Kleinberg introduced the hubs and authorities framework (HITS), where each node is associated with an authority and a hub score [6].
Each directed edge is deemed as an endorsement of authority in one direction and of connectivity ( hubness ) in the opposite direction.
The graph structure is then used to propagate these scores till an equilibrium is attained.
Recently, Yin et al. [15], proposed the TruthFinder algorithm speci cally focused on opinion aggregation for binary opinions following an approach similar to HITS.
However, unlike HITS, the predicate truth scores are computed by probabilistic aggregation of agent opinions assuming independence as in [2].
This paper also proposes simple heuristics for handling dependencies between predicates and was shown to be more e ective than a simple voting strategy.
In this paper, we studied the problem of merging historical information from multiple sources.
Unlike prior work, which assumes explicit mapping between source values and the real values they observe, we model the mapping as a hidden unknown variable.
We then perform inference to compute an estimate of the history of true updates, together with their mapping to the source values.
We presented two approximation algorithms for this inference task, and evaluate their performance against several baseline methods that either ignore history, or use it in a naive way.
These experiments show that our techniques are able to approximate both the unknown history and the  nal value signi cantly more accurately than baseline techniques.
