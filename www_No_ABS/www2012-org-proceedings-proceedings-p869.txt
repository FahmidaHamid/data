Summarization of opinions is crucial in helping users digest the many opinions expressed on the web.
Previous studies have primarily focused on the task of generating highly structured summaries.
This could be a simple sentiment summary such as  positive  or  negative  on a topic of interest [17, 16, 26, 29] or a multi-aspect summary such as battery life: 1 star, screen: 3.5 stars, etc for an mp3 player [14, 24, 22, 13, 9].
While structured summaries can be useful in conveying the general sentiments about a person, product, or service, such summaries lack the level of detail that an unstructured (textual) summary could o er, often forcing users to go back to the original text to get more information.
Textual summaries are thus critical in conveying key opinions and reasons for those opinions at di erent granularities (i.e. entity level or topic level).
Unfortunately, generating textual opinion summaries is a hard task.
First, the summaries have to be representative of the key opinions (to avoid bias) and then, it has to be Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Table 1: Example of micropinion summaries on two di erent topics given a constraint of 10 words.
Mp3 Player Y Very short battery life.
Big and clear screen.
(8 words) Restaurant X Good service.
Delicious soup dishes.
Very noisy at nights.
(9 words) readable so that it can be easily understood by the reader.
Further, with the increased use of handheld devices for various online activities such as shopping and  nding places to eat, the conciseness or compactness of such summaries is also crucial.
Indeed, there are many scenarios where concise summaries would be very bene cial.
Consider shopping sites where there could be hundreds of reviews per product.
In this case, a concise pros and cons summary consisting of a list of short opinion phrases would help convey critical information about a product, thus facilitating decision making.
Such concise summaries are also suitable as tweets that are automatically generated based on blogs or news articles.
In this paper, we explore the task of generating a set of very concise phrases, where each phrase (micropinion) is a summary of a key opinion in text.
The ultra-concise nature of the phrases allows for  exible adjustment of summary size according to the display constraints.
Our emphasis on generating concise abstractive summaries (rather than extrac-tive summaries), makes this a unique summarization problem which has not been previously studied.
In Table 1, we show examples of envisioned micropinion summaries.
On the surface, our summarization task appears to be similar to a keyphrase extraction problem.
However, since the goal is to help users digest the underlying opinions, there are some important aspects that are unique to this task.
In traditional keyphrase extraction, the goal is primarily to select a set of phrases to characterize documents.
Thus, phrases such as battery life and screen from a set of reviews about a phone may be selected as candidate keyphrases.
For our task, such keyphrases are meaningless without the associated opinions.
In addition, since we want readers to understand the opinions in the summary, the phrases in the summary need to be fairly well-formed and grammatically sound.
Consider a phrase such as  short battery life  in contrast to one such as  life short battery .
Even though both phrases contain the same words, the ordering is di erent, changing their meaning, where the former is readable but the latter would make no sense to the reader.
This readability aspect is less of a concern in traditional keyphrase extraction as the phrases are only used to  tag  documents.
to generating micropinion summaries for di erent display constraints.
Our idea is to start with a set of high frequency seed words from the input text, gradually forming meaningful higher order n-grams.
At each step the n-grams are scored based on their representativeness (measured based a modi ed mutual information function) and readability (measured based on an n-gram language model).
We frame this problem as an optimization problem, where we attempt to  nd a set of concise, non-redundant phrases (not necessarily occurring in the original text) that are readable and represent the major opinions.
We propose heuristic algorithms to solve this optimization problem e ciently.
Evaluation results using a set of product reviews shows that our approach is e ective in generating micropinion summaries and outperforms other summarization approaches.
Further, the proposed approach is lightweight and general, requiring no linguistics or domain knowledge.
It can thus be used in a variety of domains and could even be used with other languages.
The dataset and demo related to this work can be found at http://timan.cs.uiuc.edu/downloads.html.
Research in opinion summarization has been quite extensive with much of the focus being on generating structured summaries of opinions.
Early opinion summarization systems focused on predicting the overall sentiment class (positive or negative) on a given piece of text [17, 16, 26, 29].
In later years, this de nition was generalized to a multi-point rating scale where ratings are predicted on a set of aspects [14, 24, 22, 13, 9, 8, 27].
In contrast to structured summarization approaches, studies addressing unstructured summarization of opinions are notably fewer and abstractive approaches are less common than extractive approaches.
To the best of our knowledge, no previous work has studied the generation of ultra-concise and abstractive summaries using an optimization framework such as ours with special emphasis on compactness, readability and representativeness.
The works closest to ours are perhaps the work of Brana-van et.
al [1] and Ganesan et.
al [7] (Opinosis) where both attempt to generate concise summaries of opinions.
Branavan et.
al implemented a hierarchical Bayesian model reusing existing pros and cons phrases to train their keyphrase extraction model.
Unlike their approach, ours is unsupervised and domain independent where we try to generate concise and informative phrases using only the existing text and a publicly available n-gram model.
While Opinosis is also unsupervised, it uses a graph data structure that relies on the structural redundancies in text to discover informative phrases.
In our method, we do not rely on structural redundancies but rather grow seed words from the input text into longer and meaningful phrases.
With this, it is possible to generate new phrases that are more concise to convey essential opinions.
Carenini et.
al [2, 3] compared both the use of extractive and abstractive summarization methods for opinion summa-rization.
A key di erence between our abstractive approach and theirs is that we focus on generating ultra-concise summaries (set of short phrases) using a domain-independent and lightweight approach, while they focus on generating paragraph level summaries made up of full length sentences using a complex natural language generation architecture.
Other types of textual opinion summaries include contrastive summarization [10, 18] where the summaries are meant to highlight contradicting sentence pairs in opinionated text.
Note that these summaries are not meant to summarize key opinions in text.
As discussed in Section 1, while there may be some key di erences between our task and traditional keyphrase extraction, conceptually, the tasks are quite similar.
Thus, presumably many of the keyphrase extraction approaches [25, 30, 6, 15] can be used for the task of micropinion generation.
Most of the keyphrase extraction approaches are highly supervised and rely on various linguistics annotations in order to  nd candidate phrases.
Our approach is not only unsupervised and domain independent, but we also do not rely on linguistics annotations to discover meaningful phrases, which makes our approach more general.
Further, our optimization framework which gives special emphasis to the readability and representativeness aspects, has not been explored by any of the keyphrase extraction approaches.
The goal of our task is to generate a compact and informative summary using a set of micropinions.
A micropinion is a short phrase (between 2 and 5 words) summarizing a key opinion in text.
The minimum phrase length of 2 is based on the observation that a meaningful opinion is often targeted towards an object [12] (e.g.
clear screen vs. clear ).
The maximum phrase length of just 5 is to allow for  ex-ibile adjustment of summary size according to the display requirements.
For example, a small phone may have stricter display requirements than a full sized PDA.
Thus, compared with most existing work in text summarization, a unique aspect of our goal is to maximize information conveyed in the given constraints.
Formally, given a set of sentences Z = {zi}n i=1 from an opinion document, our goal is to generate a micropinion summary, M = {mi}k i=1, where |mi|  [2 , 5] words and each mi   M conveys a key opinion from Z.
Note that while we require mi to use words that have occurred at least once in Z, we do not require mi to be an exact subsequence of any of the sentences in Z.
This makes our task setup more of an abstractive summarization problem.
In contrast to the predominantly popular extractive summarization task, this raises a new interesting challenge in how to compose concise and meaningful summaries using only words from the original text.
This requirement is not restrictive since user generated content such as opinions have the bene t of volume and with this there would be many choices of words to describe a major opinion.
Intuitively, we would like the generated micropinion summary, M = {mi}k i=1 to be: (1) representative (i.e., each mi should re ect the major opinions in the original text), (2) readable (i.e., each mi should be well formed according to the language s grammatical rules), and (3) compact (i.e., M should use as few words as possible to convey the major opinions).
Thus, in theory, we can solve this new summa-rization problem by enumerating all possible summaries and evaluating each one to see how well it satis es these three criteria, which suggests that we can formulate micropinion summarization as the following optimization problem:
 = arg maxM ={m1,...,mk} k(cid:2) i=1 [Srep(mi) + Sread(mi)] (cid:3)k i=1 |mi|   ss Srep(M )    rep Sread(M )    read sim(mi, mj)    sim , i, j   [1, k] where
 tativeness of mi;
 ity of mi; 3. sim(mi, mj) is a similarity function measuring the sim ilarity of mi and mj; 4.  ss and  sim are two user adjustable parameters to control the maximum length of the summary and the amount of redundancy allowed in the summary; 5.  rep and  read are minimum representativeness and readability thresholds to improve e ciency of the algorithm.
The rationale for this optimization formulation is the following: First, the objective function captures the intention of optimizing both representativeness and readability which ensures that the summaries re ect opinions from the original text and are also reasonably well-formed.
Second, the compactness is captured by setting a threshold on the maximum length of the summary and a threshold on the similarity between any two phrases in the summary so as to minimize redundancy.
These parameters are indeed necessary to accommodate di erent needs of the user or the application.
For example, when devices have very small screens, the user may desire smaller summaries with good coverage of information (i.e. less redundancies).
On the other hand, with bigger screens, users may desire longer summaries and may be willing to tolerate more redundancies in order to get more detailed information.
In both cases, these application or user speci c requirements can be easily adjusted using the  ss and  sim thresholds.
Finally, we use two thresholds for minimum representativeness and readability respectively which ensures e ciency of the algorithm by reducing the exploration of non-promising candidate expansion paths.
In turn, this also ensures a summary to be both representative and readable (i.e., avoid  skewed tradeo s ).
Clearly, the main challenge now is to de ne the representa-tiveness function Srep(mi), the readability function Sread(mi), and the similarity function sim(mi, mj).
In this paper, we focus on studying how to de ne Sread(mi) and Srep(mi) as they represent interesting new challenges raised by mi-cropinion summarization.
For sim(mi, mj), we simply use the Jaccard similarity [21] to measure and eliminate redundancy since it is not the focus of our study.
In Section 3.1, we will explain the proposed method to measure the representativeness of mi, Srep(mi), based on the modi ed pointwise mutual information of the words in mi.
This captures how well mi aligns with major opinions in the original text.
Then in Section 3.2, we describe how we estimate readability, Sread(mi), of phrases based on a general n-gram language model with the assumption that mi is more readable if mi is more frequent according to the n-gram model.
In general, opinions are often quite redundant and may contain contradicting viewpoints.
Hence, generating a few highly representative phrases is a challenge.
Since we are mainly interested in summarizing the major opinions in text, a representative summary would be one that can accurately bring to surface the most common complaint, praise or critical information.
For example, assuming we have 10 sentences in the input document that talks about  battery life being short  and one about  battery life being excellent , by our de nition, the former would be the representative opinion phrase.
In determining the representativeness of a phrase mi, we have de ned two key properties of a highly representative phrase: (1) the words in each mi, should be strongly associated within a narrow window in the original text and (2) the words in mi should be su ciently frequent in the original text.
The  rst property ensures that only a set of related words are used in the generated phrases to avoid conveying incorrect information.
As an example, if we were to generate a phrase containing the word short, it is important that short is used with the right set of words or we may convey in-the phone formation not present in the original text (e.g.
is short instead of battery life is short).
While this is not a problem for methods that use existing n-grams from the input document, it is important for our method as we form potentially new n-grams from seed words.
This  rst property of strong association can be captured by computing the pointwise mutual information (PMI) of words in mi based on its alignment with the original text.
PMI was shown to be the best metric to measure strength of association of word pairs [23].
For a set of strongly associated words to be considered representative, these words should also be significant in the input text.
The second property thus rewards n-grams containing words that occur frequently in the original text.
Formally, suppose m = w1...wn is a candidate phrase.
We de ne Srep(m) as follows: Srep(w1...wn) = n(cid:2) i=1
 n pmilocal(wi) (1) where pmilocal(wi) is a local pointwise mutual information function de ned as: pmilocal(wi) = i+C(cid:2) j=i C


 pmi(cid:3) (wi, wj ), i (cid:2)= j (2) where C is a contextual window size.
The pmilocal(wi) measures the average strength of association of a word wi with all its C neighboring words (on the left and on the right).
So, for the phrase short battery life, assuming C = 1, for short we would obtain the average PMI score of short with  battery  and for battery we would obtain the average PMI of battery with  short  and battery with  life .
When this is done for each wi   m, this would give a good estimate of how strongly associated the words are in m, which is the rationale for Equation 1.
To capture the second property, we use a modi ed PMI scoring referred to as pmi(cid:3) where the pmi(cid:3) between two words, wi and wj, is de ned as: pmi(cid:3) (wi, wj ) = log2 p(wi, wj )   c(wi, wj ) p(wi)   p(wj ) (3) where c(wi, wj) is the frequency of two words co-occurring in a sentence from the original text within the context window of C (in any direction) and p(wi, wj) is the corresponding joint probability.
We later show the in uence of C on the generated summaries.
The co-occurrence frequency, integrated into our PMI scoring to reward frequently occurring words from the original text.
The problem with the original PMI scoring is that it yields in high scores for low frequency words.
By adding c(wi, wj) into the PMI scoring, we ensure that low frequency words do not dominate and moderately associated words with high co-occurrences have relatively high scores.
For a summary to be readable, it will have to be fairly well-formed and grammatically sound according to the language s grammatical rules.
The readability aspect is an important requirement in any summarization task as this allows a reader to easily digest information.
In extractive summarization, the readability of a summary is less of a problem as existing sentences or phrases are reused to form summaries.
In our approach, we do not reuse sentences or phrases directly from Z, but rather attempt to generate new phrases using existing words from Z.
Hence, there is no guarantee that the generated phrases would be well-formed and readable.
For example, without readability scoring, it will be hard to distinguish the grammatical di erence between the phrases clear is screen and screen is clear.
Without any form of supervision, measuring the readability of a phrase is di cult.
We address this problem by leveraging the publicly available Microsoft N-gram service1 [28], to score all our system generated phrases.
The abundance of textual content on the web which includes blogs, news articles, user reviews, tutorials, etc makes an n-gram model estimated based on all such content an approximate judge of how readable the system generated phrases are.
The intuition is that, if a generated phrase occurs frequently on the web, then this phrase is more likely readable.
This approximation to determining readability is fair, since the web as a corpus, is extremely large and there would be enough evidence to segregate a well-constructed phrase from a poorly constructed one.
Speci cally, we use the Microsoft s trigram language model trained on the body text of documents to obtain conditional probabilities of the candidate phrases.
In scoring each phrase, we  rst obtain the conditional probability of di er-ent sets of trigrams in the phrase.
These scores are combined and averaged to generate the  nal readability score.
Suppose m = w1...wn is a candidate phrase, Sread(m) is thus de ned as follows: n(cid:4) k=q Sread(wi...wn) =   log2

 P (wk|wk q+1...wk 1) (4) where q represents the n-gram order of the model used and in our case, q = 3.
K represents the number of conditional probabilities computed.
Equation 4 is essentially the chain rule used to compute the joint probability in terms of conditional probabilities, which is then averaged.
Averaging the scores accounts for the di erences in phrase lengths and enables us to set cuto s to prune non-promising candidates.
The optimization formulation involves several parameters to be empirically set.
Some of these have to be set in an application-speci c way based on tradeo s between multiple criteria of summarization.
The  ss and  sim parameters 1http://web-ngram.research.microsoft.com are required to enable users or the application to control the desired total length of a summary and the amount of redundancy allowed in the summary.
These values can be set, for example based on the screen size of a mobile device if the summary is to be displayed on such a device; smaller  ss for shorter summaries and smaller  sim for less redundant summaries.
Note that it does not make much sense for a system to automatically set these two parameters due to their inherent dependency on user preferences.
 rep and  read mainly help with the e ciency of the optimization algorithm and also helps ensure the minimum rep-resentativeness and readability of phrases; we will later show that these two values can be set to reasonably small values and has little e ect on performance unless the thresholds are too restrictive.
With this setup, the remaining challenge is to solve the optimization problem e ciently, which we will discuss in the next section.
Since we want to compose new phrases using words from the original text, the potential solution space for our optimization problem, is huge.
In practice, it is infeasible to enumerate all the possible summary candidates and score each one of them.
In this section, we propose a greedy algorithm to solve the optimization problem by systematically exploring the solution space with heuristic pruning so that we only touch the most promising candidates.
At a high level, we start with a set of high frequency uni-grams from the original text.
We then gradually merge them to generate higher order n-grams as long as their readability and representativeness remain reasonably high.
This process of generating candidates stops when an attempt to grow an existing candidate leads to phrases that are low either in readability or representativeness (i.e. does not satisfy  rep or  read).
Speci cally, the input to our summarization algorithm is a set of sentences from an opinion containing document.
For example, all review sentences about the iPhone.
We denote these sentences as Z = {zi}n i=1.
The output is a micropinion summary with a set of n-gram phrases M = {m1, ..., mk}, where the number of micropinions is determined based on the constraints of the optimization problem.
The summa-rization algorithm consists of the following three steps: Step 1.
Generation of seed bigrams: The  rst step takes the original text, Z as input and generates a set of promising bigrams based on combinations of high frequency unigrams.
Step 2.
Generation of scored n-grams: The second step takes the seed bigrams as input and further grows them into a set of promising n-grams by concatenating bigrams that share an overlapping word.
While generating n-grams, we also compute their representativeness and readability scores Srep and Sread, and prune all the cases where any of these scores is below the corresponding threshold.
We further check redundancy of the generated candidates, and if two phrases have a similarity higher than the  sim threshold, we would discard the one with a lower combined score of Srep + Sread.
Step 3.
Generation of micropinion summary: The  nal step is to sort all the candidate n-grams based on their objective function values (i.e., sum of Srep and Sread) and generate a micropinion summary M by gradually adding phrases with the highest scores to our summary until the  ss.
We will now focus on elaborating steps 1 and 2 since step 3 is straightforward.
As redundancies are inherent in opinionated documents, this property can be leveraged to shortlist a set of seed words that can be used to generate higher order n-grams.
This way, we avoid having to try every combination of words.
Our assumption is that, if a word is not frequent in the original text, it is unlikely a good candidate word to be included in any phrase of a micropinion summary (presumably we will have other better candidate words to work with).
Assuming that the input text is a set of sentences, we shortlist a set of high frequency unigrams from the input text, where the unigrams should have counts larger than the median count (after ignoring words with frequency of
 input size.
The low threshold serves as an initial reduction in search space; further reduction happens when Srep and Sread are computed later.
Each of these high frequency unigrams is then paired with every other unigram to form bigrams.
For example, if we have the words  battery  and  life , the bigrams generated would be  battery life  and  life battery .
We then compute the representativeness score Srep of each bigram and keep only those bigrams whose Srep passes the threshold  rep.
Although the combination of words could be quite random, the Srep function helps prune invalid combinations (as it demands co-occurrences of two words within a window C of the original text).
tion Using the seed bigrams described in the previous section, we attempt to generate higher order n-grams that will  nally serve as candidate micropinions.
If there are a large number of seed bigrams (for large input documents), the starting seeds can further be shortlisted by their representativeness scores.
For example, we can use only the top 500 seeds as the starting point.
Algorithm 1 outlines the steps involved in candidate micropinion generation.
return candList   {candList   p} Algorithm 1 GenerateCandidates(p)



 5: end if

 8: end if






 end if 16: end for if N otM irror(p, bigram) then newP hrase   M erge(p, bigram) Score(newP hrase) GenerateCandidates(newP hrase) First, for any incoming phrase, p, a check is done to determine if p is a promising phrase.
This is done by checking the Sread(p) and Srep(p) scores with the corresponding thresholds (line 3).
If the score constraints are not ful lled, then p will not be further expanded.
This step ensures that we explore only reasonable phrases, thus improving e ciency.
If p ful lls the score constraints, then a check is done to determine if it can become a candidate micropinion (line 6).
This is based on the similarity between p and the existing candidate phrases from the pool of candidates.
If p is not similar to any of these phrases, then p will automatically be added to this candidate pool.
If p is similar to a phrase X in the pool, p will replace X if Srep(p) + Sread(p) > Srep(X) + Sread(X).
In other words, at any given time, we will have a set of non-redundant candidate phrases.
Once the validity of a phrase has been determined, the algorithm proceeds recursively in a depth  rst fashion in an attempt to expand p to a higher order n-gram (line 9-16).
We expand phrases using concepts used for pattern growth as shown in [19].
In particular, we impose a merge requirement between a candidate phrase p and a bigram, b (from the set of seed bigrams) as follows: (1) the ending word in p should overlap with the starting word in b and (2) p should not be a mirror of b.
With this, all seed bigrams that satisfy this requirement will be merged with p. Consider a phrase very long battery and a seed bigram battery life.
The overlapping word battery, connects the two phrases and since one is not a mirror of the other, the two phrases can be merged to form very long battery life.
Such a pattern growth approach eliminates the need for an exhaustive search and it also avoids exploration of n-grams that are extremely random and unlikely useful.
The newly merged phrases are then scored for their readability and representativeness prior to being further expanded (line 13).
To evaluate this micropinion generation task, we leverage user generated reviews from CNET2 for 330 di erent products.
Each product has 5 associated reviews at the minimum.
The CNET review structure is such that a user writes a full length review about a product followed by a brief summary about the pros and cons (PC).
The PCs are usually a set of short phrases such as  bright screen, fast download, etc  where these phrases tend to summarize the full reviews and hence are quite redundant.
In evaluating our summa-rization task, we ignore the PCs and use only the full reviews for a product to make the summarization task more realistic (i.e we eliminate obvious redundancies).
Thus, for a given product X, we generate a review document, rx, where rx holds all sentences from the full reviews related to X.
On average, there were 500 sentences per review document.
Out of the 330 review documents generated, we use only 230 documents for evaluation as 100 were withheld to train one of the baseline approaches.
While the PCs seem ideal as the gold standard summary, in some cases the PCs are just an enumeration of features that the user likes or dislikes and contain no speci c opinions.
For example,  Pros: battery, sound ; Cons: hard disk, screen .
Since we need a set of opinion phrases that are more complete such as  improved battery life; crystal clear sound  , we leverage these PCs to help human summarizers compose meaningful summaries.
Speci cally, for each product X, we  nd the top 10 high frequency phrases from the PCs which are then presented as hints to the human sum-marizers.
Since we have a large number of review documents to summarize, such hints help the human summarizer with topic coverage, thus reducing bias in the summaries.
Two 2http://www.cnet.com product presented to them and then compose a set of phrases summarizing key opinions on the product.
Out of the 330 products, 165 were assigned to one summarizer and the remaining to the other.
With this, for each rx, we obtained a corresponding human summary, hx.
In demonstrating the e ectiveness of our approach, we need to quantify to what extent our summaries are representative of key opinions and how readable these summaries are so that they can be understood by human readers.
One way of assessing the quality of summaries is to measure how well system summaries resemble human composed summaries.
Keyphrase extraction tasks are typically evaluated based on the number of overlapping keyphrases between system generated keyphrases and the gold standard ones.
This requires exact matches which is unlikely, as there could be many ways to describe one opinion and subtle variations may result in an unfair  no match .
We thus chose to use ROUGE [11], an evaluation method based on n-gram overlap statistics found to be highly correlated with human evaluations.
ROUGE was also the standard measure used in the DUC 2004 summarization task3 on generating very short summaries such as headline summaries (< 10 words).
ROUGE is ideal for our task as it does not demand exact matches but it can measure both repre-sentativeness and readability of summaries.
As an example, ROUGE-1 measures the overlap of unigrams between system summaries and human summaries, thus measuring repre-sentativeness.
Higher order ROUGE-N (N > 1) captures the match of subsequences, which measures the  uency or readability of summaries.
In our experiments, we primarily use ROUGE-1, ROUGE-2 (bigram overlap) and ROUGE-SU4 (skip-bigram with maximum gap length of 4).
In addition to the automatic ROUGE evaluation, we also performed a manual evaluation to assess the potential utility of the micropinion summaries to real users.
Speci cally, two assessors were asked to read the micropinion summaries presented to them (using the original reviews as reference) and then  ll out a questionnaire assessing the summary on several aspects related to its e ectiveness.
Note that these assessors are di erent from those that composed the gold-standard summaries.
The questionnaire consisted of three key questions rated on a scale from 1 (Very Poor) to 5 (Very Good).
The questions are as follows: Grammaticality: Are the phrases in the summary readable with no obvious errors?
Score (1) - None of the phrases are readable or comprehensible; Score (5) - I don t see any issues with the phrases in the summary.
Non-redundancy: Are the phrases in the summary unique with unnecessary repetitions such as repeated facts or repeated use of noun phrases?
Score (1) - All the phrases mean the same thing; Score (5) - The phrases are very unique, summarizing very di erent topics or issues.
Informativeness: Do the phrases convey important information regarding the product?
This can be positive/negative opinions about the product or some critical facts about the product.
Score (1) - None of the phrases contain useful or accurate information about the product.
Score (5) - All the phrases contain accurate opinions or critical information about the product.
Note that the  rst two aspects, grammaticality and non-redundancy are linguistic questions used at the 2005 Document Understanding Conference (DUC) [4].
The last aspect, informativeness, has been used in other summarization tasks [5] and is key in measuring how much users would learn from the summaries.
For this evaluation, we used the micropinion summaries for 70 di erent products that were randomly selected from our dataset.
The assessors were not informed about which method was used to generate the summaries.
To assess how well our approach compares with existing approaches, we use three representative baselines.
As our  rst baseline, we use TF-IDF, an unsupervised language-independent method commonly used for keyphrase extraction tasks.
To make the TF-IDF baseline competitive, we limit the n-grams in consideration to those that contain at least one adjective (i.e.
favoring opinion containing n-grams).
Note that the performance is much worse without this selection step.
Then, we used the same redundancy removal technique used in our approach to allow a set of non-redundant phrases to be generated.
For our second baseline, we use KEA4 [30], a highly cited, state-of-the-art keyphrase extraction method.
KEA builds a Naive Bayes model with known keyphrases, and then uses the model to  nd keyphrases in new documents.
The KEA model was trained using the 100 review documents withheld from our dataset (explained in Section 5.1).
With KEA as a baseline, we would gain insights into the applicability of existing keyphrase extraction approaches for the task of micropinion generation.
Note that since KEA uses training data and our method does not, the comparison is, strictly speaking, an unfair comparison.
As our  nal representative baseline, we use Opinosis [7], an abstractive summarizer designed to generate textual summary of opinions.
Opinosis was shown to be more e ective in generating concise opinion summaries compared to extrac-tive approaches like MEAD [20].
We turned o  the optional collapse feature in Opinosis which attempts to merge several short phrases into longer ones to simulate the task of micropinion generation.
All other settings were set at their default values.
In addition to these baselines, we also performed a run to examine the bene t of our strategy of composing potentially new phrases as opposed to relying solely on phrases that occurred in the original text.
For this run (WebNgramseen), we force our search algorithm to return n-grams that have occurred at least once in the reviews.
To give a fair comparison, all the n-grams in each of our baseline are 2-5 words long.
Our approach is referred to as WebNgram in all our experiments.
By default, the e ciency parameters in our approach are set as follows: minimum readability score,  read =  2 (log of probabilities); minimum representativeness score,  rep = 4.
As will be shown later, the performance of summarization is not very sensitive to the settings of these parameters.
The contextual window size is set to C = 3, which is the opti-3http://duc.nist.gov/pubs.html#2004
 **p-value < 0.001  ss T df
 Opinosis WebNGramseen WebNGram


















 % Improvement












  ss T df
 Opinosis WebNGramseen




 WebNgram over TfIdf WebNgram over KEA WebNgram over Opinosis +13.25* +39.90** +36.61** +40.00** +37.32** +37.77** WebNgram over TfIdf -25.36 +22.38* +22.74** +29.31** +27.33** +25.37** WebNgram over KEA +7.48 +8.19 +3.41 +9.17 +5.02 +4.89 WebNgram over Opinosis


















 % Improvement

















 +65.55** +77.80** +74.22** +77.41** +75.52** +75.07** +39.33* +56.59** +52.95** +58.18** +55.66** +52.34** +27.73 +33.57** +19.71* +29.95** +26.09** +29.19** mal setting.
The user adjustable parameter for redundancy control (using Jaccard) is set to  sim = 0.40, for reasonable diversity in phrases.
Comparison of summarization strategies.
First, we assess the e ectiveness of our approach (WebNgram) in comparison with other representative approaches (KEA, Opinosis, TF-IDF).
The performance comparison is shown in Table 2 for di erent  ss settings.
Since the summary size is constrained, we are primarily interested in the gain in recall as the precision is proportional to recall when summary length is  xed.
First, based on Table 2, we see that overall, WebNgram has the highest ROUGE-1 and ROUGE-2 scores, outperforming all baseline methods.
The statistical signi cance of improvements (based on Wilcoxon test) is indicated in Table 2.
As the summary size increases, Web-Ngram consistently gains both in terms of ROUGE-1 and ROUGE-2.
This shows that representative phrases are being included in the summary and these phrases are also readable as shown by the consistent gain in ROUGE-2 (the other baseline methods do not gain as much with ROUGE-2).
Next, we see that the ROUGE-1 scores of Opinosis and WebNgram are quite similar, but the ROUGE-2 scores are very di erent.
The similarity in ROUGE-1 scores indicates that Opinosis is able to capture similar topics and opinions as WebNgram.
However, the ROUGE-2 score of Opinosis indicates that the generated phrases are less  uent.
Through manual inspection, we found that Opinosis generates many short phrases (< 4 words), generally fragmented and thus less  uent.
This explains the lower ROUGE-2 scores.
This is further con rmed by the average number of phrases generated by Opinosis which is higher (i.e more fragmented) than that of WebNgram (shown in Table 3).
One possible reason as to why Opinosis has problems generating longer and more complete phrases is lack of structural redundancies between sentences in the CNET reviews.
Amongst all the approaches, TF-IDF performs the worst as shown in Table 2.
This is likely due to insu cient redundancies of meaningful n-grams.
When there are subtle di erences in common expressions, it is often di cult to discover redundant n-grams.
This is where our method excels as we do not directly rely on the structure of the input text, but rather expand high frequency seed words into longer and meaningful phrases.
Finally, notice that KEA does only slightly better than TF-IDF even though KEA is a supervised approach.
While KEA is suitable for characterizing documents, such a supervised approach proves to be insuf- cient for the task of generating micropinions.
It might be that the model needs a more sophisticated set of features for Table 3: Average # of generated phrases in summary.
 ss





 Web Ngram





 Opinosis
 Kea

















 it to generate more meaningful summaries.
Note that varying the size of training data had minimal e ect on KEA.
Seen N-grams vs. System Generated N-grams.
In our candidate generation approach, we form longer n-grams from shorter ones using the procedure described in Section 4.
One may argue that with su ciently redundant opinions, searching the restricted space of all seen n-grams may be su cient for generating micropinion summaries and thus such a search procedure may not be necessary.
We thus performed a run by forcing only seen n-grams to appear as candidate phrases.
The results are shown in Table 2 under WebNgramseen.
From this, it is clear that this approach yields lower performance compared to using system generated n-grams (WebNgram), suggesting that our search algorithm actually helps discover useful new phrases.
When opinions are not su ciently redundant, observed n-grams tend to be less representative than our system generated n-grams which are constructed by combining related words and phrases (e.g.
big and clear screen from  ..the phone has a big screen..  and  ..the screen was clear.. ).
This is one good example of why a more abstractive approach is suitable for generating such concise opinion summaries.
Well-formedness of phrases.
Intuitively, a good summary phrase is one that is fairly well-formed and clearly conveys the intended meaning.
Thus, a few readable phrases is more desirable than many fragmented phrases.
Consider two micropinion summaries, M = {very clear, screen is} and M(cid:3) {very clear screen}.
In this example, it is obvious that M(cid:3) is a more desirable summary than M .
Thus, we now look into the average number of phrases generated for di erent summary sizes which is shown in Table 3.
We can see that the WebNgram approach generates the fewest phrases for any given  ss constraint.
This shows that the WebNgram phrases are longer on average (i.e. more well-formed) and also readable as previously shown by the ROUGE-2 scores and further validated by our manual evaluation.
KEA seems =  ss WebNgram
 Opinosis Web +Bias Ngram



















 Table 5: Recall scores with the use of biasing ( ss =



 WebNgram WebNgram+Bias change








 to favor very short phrases and on average, KEA generates the most number of phrases.
Opinion coverage and e ect of summary biasing.
An important point to note is that each of the baselines used (KEA, Opinosis and TF-IDF) has some form of opinion biasing builtin; KEA with the training examples from human composed summaries, Opinosis through selection of phrases with certain POS tags and TF-IDF through selection of adjective containing n-grams.
In our current model, we do not use any form of biasing to evaluate the e ectiveness of our method as is.
To estimate the actual coverage of opinions, we count the average number of opinion words in the summary using a general set of adjective words from General In-quirer5 (1,614 positive and 1,982 negative).
The results are reported in Table 4.
Considering only the base WebNgram model and the other 3 baselines discussed earlier, Opinosis has the highest number of opinion words.
However, notice that even without opinion speci c re nements, WebNgram has comparable number of opinion words to that of Opinosis and TF-IDF (which have builtin biasing).
To gain insights into how opinion speci c re nements can help our summaries, we explore a simple heuristics biasing.
Speci cally, in our  nal candidate selection step we only considered phrases that contain at least one adjective.
This run is called WebNgram+Bias.
From Table 4 and Table 5, it is clear that the addition of opinion biasing improves agreement with human summaries and the average number of opinion words in the summaries is also much higher than the base model.
This suggests that our model can be further re ned to generate task speci c summaries.
E ectiveness of two-part scoring.
So far, it has been assumed that both components of our scoring function, Srep and Sread are required to generate reasonable summaries.
To test this hypotheses, we generate summaries with di erent scoring components turned o  at a given time.
The ROUGE scores are reported in Table 6.
Overall, the performance is lowest when Sread is not used (row 1).
Without Sread it is likely that ungrammatical phrases with high representative-ness scores appear as good candidates, thus resulting in poor performance.
While Sread is important, by itself, it does not perform as well (row 2) as when used in conjunction with Srep (row 3).
This is because Sread favors highly readable phrases but these phrases may not be representative of the underlying opinions.
Thus, it is clear that Sread and Srep work in synergy to generate meaningful summaries.
Stability of parameters.
It is critical to understand the e ect of nonuser dependent parameters in our model (namely 5http://www.wjh.harvard.edu/ inquirer/ Table 6: Performance of scoring components ( ss =
 Recall F-score Rouge





 +Srep-Sread -Srep+Sread











 +Srep+Sread 0.219




 Table 7: Results of manual evaluation with  ss = 25.
Score 1 (Very Poor) to 5 (Very Good).
WebNgram
 Human Question Grammaticality Non-redundancy Informativeness Avg.
Dev.
Avg.
Dev.
Avg.
Dev.
Overall





  read,  rep and C), so that these parameters can be set correctly for a new dataset.
The primary function of  read and  rep is to prune non-promising candidates, thus improving e ciency and as a result also ensuring minimum readability and representativeness.
Without these two parameters, we will still arrive at a solution, but the time to convergence would be much longer and the results could be skewed (e.g.
very representative but not readable).
Figure 1 (a) and (b) show how di erent settings of  read and  rep a ect the overall performance.
The values in Figure 1 (a) are averaged across  rep   [1, 5] and C   [2, 6]; The values in Figure 1 (b) are averaged across  read   [ 4, 1] and C   [2, 6].
Notice that these two parameters are actually stable across di erent values and do not a ect performance except in extreme conditions (thresholds that are too high).
When  read    1, phrases are expected to have high readability scores from the start and this requirement is too restrictive in  nding good candidates.
Similarly, when  rep   5, the candidates are expected to have extremely high representativeness scores at every point, and again restricting discovery of good candidates.
The fact that  read and  rep do not a ect performance (except when the thresholds are too high) suggests that the objective function already ensures phrases to be both representative and readable.
It is thus safe to set these values to be small enough (between 2 and 4 for  read; between 1 and 4 for  rep) to ensure reasonable e ciency and meaningful summaries.
Note that the low  min  curves as seen in Figure 1 (a), (b) and (c) is caused by the extreme values,  rep = 5 and  read =  1.
The third parameter, C is a window size used to compute the representativeness score of a phrase.
The requirement is that two words in a candidate phrase should occur within a context window of size C in the original text (see Section 3.1).
Figure 1 (c) shows performance at di erent C values (averaged across  rep   [1, 5] and  read   [ 4, 1]).
On average, the best performance is achieved when C = 3, which is quite reasonable.
Intuitively, when C is large, certain important words that are spread out can now be seen in context (e.g.
the Nokia phone that I bought was cheap).
At the same time, wrong pairs of words may also be considered related and this is evident with lower performance when C > 3.
To further show that the suggested parameter settings would hold true on new datasets, we obtained the optimal parameter values tuned on the 100 review documents with-



avg.
max min (a)


  rep

 avg.
max min (b)
 subjective and a score above 3 is actually quite encouraging.
The WebNgram summaries that had informativeness scores between 3-4 mostly had meaningful and representative phrases along with some false positives that did not have any real information (e.g I bought this for Christmas).
The informativeness aspect can be improved in di erent ways, one of which is through a much stricter selection of phrases.
This is something we would like to study in detail in the future.
1 2 3 4  read


 avg.
max min (c)





 Figure 1: ROUGE-2 with varying  read,  rep and C.
Labeled as (a), (b) and (c) respectively.
held to train KEA.
The values are: C = 3,  read =  4 and  rep = 4.
Note that all these values comply with the suggested settings.
In fact, the  rep and C values are the same as our default settings.
The only di erence is that  read =  4.
This new  read value on our evaluation dataset, did not show any signi cant di erence in terms of performance (ROUGE-1 gain:-0.004, ROUGE-2 gain: +0.002).
The only di erence was in e ciency, which in this case was slower due to the more relaxed setting.
It is thus clear, that the e ciency parameters have little e ect on performance of summarization as long as the values are not too restrictive.
Manual evaluation.
To assess potential utility of the mi-cropinion summaries to real users, a subset of the summaries were manually evaluated using the procedure described in Section 5.3.
The average grammaticality, non-redundancy and informativeness scores (along with respective standard deviation scores) for three methods are reported in Table 7.
The results on human summaries serves as an upper bound for comparison.
A score below 3 is considered  poor  and a score above 3 is considered  good .
Earlier, we showed that TF-IDF had the lowest ROUGE scores amongst all the approaches, indicating that the summaries may not be very useful (see Table 2).
The scores assigned by the human assessors on the TF-IDF summaries agree with this conclusion.
On average, TF-IDF summaries received poor scores (below 3) on all three dimensions compared to WebNgram and human summaries.
WebNgram s average scores are above 3 on all dimensions and are quite close to human scores.
In terms of grammaticality, WebNgram summaries received an average score of 4.16, which means that the Web-Ngram phrases are mostly grammatically sound with a few exceptions.
This number actually correlates well with our ROUGE-2 scores discussed earlier.
Next, the average non-redundancy score of 3.92 tells us that the WebNgram phrases are fairly unique with a few cases of overlapping facts or opinions.
Although our  sim setting requests a diverse set of phrases, some of the redundancies were caused by the di erent ways in which the same information can be conveyed.
For example, the phrases Excellent sound quality and Great audio may seem di erent but actually mean the same thing.
Such di erences can be resolved with the use of opinion or phrase normalization or clustering of similar words and concepts prior to summarization.
While the average informativeness scores of WebNgram and humans are quite close, notice that these scores are just slightly above

 In this paper, we proposed an unsupervised summariza-tion approach to generate ultra-concise summaries of opinions.
We frame the problem as an optimization problem with an objective function capturing representativeness and readability along with constraints that ensures compactness of a generated summary.
We use modi ed mutual information to capture representativeness and model readability with an n-gram language model.
We propose a heuristic search algorithm to solve the optimization problem e ciently.
Our evaluation using a set of user reviews shows that our summaries can convey essential information with minimal word usage and is more e ective than other competing methods.
Human assessors found the summaries to be very readable, fairly non-redundant and informative.
Compared with existing approaches, our approach is unsupervised, lightweight and practical with no reliance on any linguistic annotations (e.g.
POS tagging or syntactic parsing) and is not designed or optimized for a speci c domain.
It only uses the existing text and a web scale n-gram model to generate meaningful summaries.
Also, since we do not use any opinion speci c re nements, our idea and approach can be easily used to generate ultra-concise summaries in other application areas such as Twitter comments, Facebook status updates, blogs, user comments and so on.
In the future, we would like to evaluate our method in these other domains and study further re nements of the summarization framework.
This material is based upon work supported by a Microsoft Research Grant, by the National Science Foundation under Grant Number CNS 1028381, and by an AFOSR MURI Grant FA9550-08-1-0265.
