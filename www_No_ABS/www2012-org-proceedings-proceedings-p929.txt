Nearest neighbor (NN) methods are widely used in several web applications such as web objects (e.g., documents, and images) classi cation, and retrieval.
In retrieval applications, availability of large volume of web data makes it possible to retrieve very similar objects for almost any given query object, and these similar objects are nearest neighbors under some suitably de ned similarity measure.
Similarly, NN methods such as k-Nearest Neighbor (kNN) classi ers achieve high accuracy as very similar objects often be-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
long to the same category.
These classi ers are attractive due to their simplicity and speed independence in the number of classes1.
Since NN methods work well in very large data set scenarios, the ability to handle large volume of data during training and testing is a necessity.
For example, in a naive implementation, linear search over the entire training data is needed, and this is expensive for web scale applications.
Therefore, development of algorithms, data structures (e.g., KD-Trees) and representations (e.g., bit vectors) that facilitate fast search are of paramount importance.
Furthermore, task-speci c performance (e.g., accuracy in classi cation tasks, and ranking measures such as normalized discounted cumulative gain (NDCG), precision@k in retrieval tasks) achieved is dependent on distance metric or similarity measure that is used in  nding the neighbors.
Although Euclidean distance is a useful metric in some applications, it is often not optimal.
Hence, learning a task-speci c distance metric is also necessary.
Keeping above requirements in mind, we propose a uni ed framework in which task-speci c fast NN methods that achieve high performance can be developed.
We demonstrate our method on classi cation and retrieval tasks, and experimental results show superior performance over state-of-the-art NN classi cation and retrieval methods.
In recent years, there has been a  urry of research activity on nearest neighbor methods for web classi cation and information retrieval applications.
To motivate our approach, we brie y discuss several popular methods and differentiate them along three key aspects (capturing the above mentioned requirements): 1) speed (training and testing), 2) learning (representations/similarity measure with or without additional information (e.g., labels of objects, pairwise similar or dissimilar objects), and 3) optimizing a task-speci c performance measure during learning.
In Table 1 we present a summary of various methods, and Figure 1 shows different con gurations to which these methods adhere to.
We observe that all the existing methods fall short in covering at least one of the aspects, and we attempt to address this issue.
Hashing methods are approximate nearest neighbor search methods, and are popular due to their speed.
In these methods data points (objects) are represented as bit vectors, and such a representation helps in  nding the neighbors quickly2.
Similarity between two objects is measured by the Hamming distance, and nearest
 models have been quite successfully used in the web context, the dependence of model and test time complexities on the number of classes makes them less attractive when the number of classes is very large.
can be made constant with respect to the size of the search set.
For example, with 32 bit representation, one can build an inverted index with 232 = 4294967296 bins Real vector Bits Bits Bits Random projections Learned projections Random projections Real vector Learned projections Learned projections Task-specific learned projections Data vector Data vector Data vector Data vector Data vector (a) LSH (b) Metric learning (c) Metric learning
 (d) Spectral Hashing (e) This paper Figure 1: Comparison of various approaches with respect to our algorithm.
neighbors are identi ed as the points that are within a user spec-i ed Hamming distance from a given query.
Locality Sensitive Hashing (LSH) [1] is a simple method ( gure 1(a)) in which bit vector representation for a data point (object) is obtained from projecting the data vector on several random directions, and converting the projected values to {0, 1} by thresholding.
This method does not make use of data to learn the representation.
Learning is important because it is useful to  nd compact and better representations, which in turn results in improved speed and task-speci c performance.
Hence, attempts have been made to develop better hashing methods.
Sophisticated hashing methods such as Semantic Hashing [19] and Spectral Hashing [23] ( gure 1(d)) learn compact bit vector representations (codes) with the desirable property that similar objects have similar codes.
In Semantic Hashing, each object in the training database is represented by a compact binary code, and the code is computed using a feed forward neural network, with the network weights learned by minimizing data reconstruction error.
In Spectral Hashing [23], k-bit compact binary codes are obtained by minimizing the sum of weighted Hamming distance between data points that are similar, and the weight represents a similarity score computed using the euclidean distance in a suitable input feature space.
A key disadvantage of these methods is that they do not make use of any task-speci c information such as object labels, query-document pair relevancy score, etc.
(when available); and, they do not explicitly optimize task-speci c performance measures.
So these methods suffer on actual task-speci c performance such as classi cation accuracy, NDCG, etc.
There are several popular methods that learn a distance metric (e.g., Mahalanobis distance).
These methods make use of additional information and learn the distance metric by a optimizing task-speci c objective function.
Methods such as Neighborhood component analysis (NCA) [9] and large margin nearest neighbor (LMNN) [22] ( gure 1(b)) classi cation learn the metric using class label information, keeping nearest neighbor classi cation as the goal.
Metric learning has also been seen as a special case of ranking problem, from the view point that good neighbors appear at the top of the list and bad neighbors appear at the bottom.
McFee et al. [16] propose a structural SVM learning framework ( gure 1(b)) for learning the metric where various ranking measures such as precision@k, NDCG, etc., can be optimized (referred as MLR in Table 1).
Chechik et al. [6] propose an online algorithm where each bin contains the data points that were mapped to the corresponding bit vector.
for scalable image similarity (OASIS) that learns a similarity measure ( gure 1(b)) with the property of scoring the more relevant pair of objects higher; it uses labeled information in the form of relative similarity of different pairs of objects.
A major drawback with these methods is that they do not provide bit vector representations.
Therefore,  nding nearest neighbors is expensive unless the input representation is sparse (which is not the case in many applications).
Also, learning is computationally expensive with NCA, LMNN and MLR approaches3.
Therefore, they are not suitable for very large-scale problems.
Jain et al. [12] propose a method ( gure 1(c)) that applies LSH on a learned metric (referred as M+LSH in Table 1).
However, this method does not use task-speci c objective function for learning the metric; more importantly, it does not learn the bit vector representation directly.
Although LSH can be applied on the projected data using a metric learned via NCA or LMNN, any such independent two stage method will be sub-optimal in getting a good bit vector representation.
We propose a uni ed framework (Section 3) to develop fast nearest neighbor methods for various applications such as classi cation, retrieval, regression, and user recommendations.
In our method we learn the bit vector representation directly by explicitly optimizing task-speci c performance.
This is done by solving a learning-to-rank problem.
Our approach has several advantages.
  Using bit vector representation helps in  nding nearest neighbors fast using Hamming distance as the measure.
  High performance is achieved by direct bit vector learning and explicit optimization of task-speci c performance measure.
  It is easy to adapt the method for different applications via choices of performance measure and ranking.
LambdaRank [5] can be used to optimize various non-smooth task-speci c performance measures (e.g., precision@k and nearest neighbor classi cation accuracy) (Section 3).
de niteness (PD) are used.
Chechik et al. [6] argue that PD constraints are not necessary for large datasets, and they do not use such constraints.
In NCA, the computational cost is quadratic in the number of training points; there are no positive de nite constraints since the projection matrix is learned directly.
Learned function Query / Document (a) (b) h Weight matrix W v + W2j W1j v1 v2 bj WDj vD (c) Figure 2: (a) The algorithm we are proposing learns a function that takes a query or a document as input and computes a bit vector as output.
(b) The function multiplies the input vector v by the matrix W , adds the biases bj to the result, and then applies the sigmoid function to compute a vector h whose components are real values between 0 and 1.
The elements of W and the biases bj are the learnable parameters of the function.
(c) The sigmoid is applied component-wise to the result of the linear projection.
Learning Representation Task-Speci c No bit vector No Yes Yes Method
 Fig. 1(a) Spectral Fig. 1(d) Semantic Yes Fig. 1(d)
 Fig. 1(b)
 Fig. 1(b)
 Fig. 1(b)
 Fig. 1(b)
 Fig. 1(c) Ours Fig. 1(e) Yes Yes Yes bit vector bit vector No No real vector Yes (C) real vector Yes (C) real vector Yes (IR) real vector Yes (IR) bit vector No bit vector Yes
 Yes Yes classi cation and regression (Section 6).
We also show how the speed of our method can be signi cantly improved further with a multistage (cascaded) implementation; here, speed improvement is achieved by allocating fewer bits in earlier stages, and reducing the nearest neighbor search space progressively (Section 6).
Our algorithm learns a function that takes a query or document vector as input and computes its corresponding bit vector representation as output (see  gure 2a).
Here we describe at a high level how the function is learned, and after learning, how it is used for nearest neighbor search.
Training data: As mentioned before, we adopt a learning-to-rank approach.
So each training example has the form (query, document list), where document list speci es how its elements should be ordered with respect to that query.
In this paper we assume that both the query and the document are the same type of data, using the same representation (e.g., both are images of the same size, or both are text documents that use the same bag-of-words representa-tion)4.
Therefore the same learned function can be applied to both a query and a document to compute the corresponding bit vector.
Parameterization of the function: The function applies linear projections to the input vector, followed by the sigmoid function to map the output of the linear projections into real numbers in the range [0, 1] (see  gure 2b and 2c).
Making the function differen-tiable allows for gradient descent learning.
At test time, when a proper bit vector is required for fast nearest neighbor search, the output of the sigmoid function is thresholded at 0.5 to convert its value from a real number in [0, 1] into 0 or 1.
The linear projection coef cients are the learnable parameters of the function.
They are represented as a matrix of size (number of inputs)   (number of bits).
See section 3 for more details.
Learning: LambdaRank [5] is the algorithm we use for learning.
It is based on the RankNet algorithm [4], which learns a pairwise ranking function.
Given a triplet (query, document1, doc-ument2), the ranking function computes the probability that document 1 should be ranked higher than document 2 for that query.
The parameters of the function are learned iteratively using the gradient of the log probability of the true pairwise ranking for a set of training triplets.
While RankNet learning is limited to pairwise ranking, Lamb-Table 1: Comparison of Methods (see text for details).
The abbreviations C, IR and R stand for classi cation, information retrieval and regression tasks respectively, and PD stands for positive de -nite matrix constraint.
Compact bit vector representation is preferable for faster nearest neighbor search during training and testing.
Constraints such as positive de nite affect training speed.
Task-speci c performance optimization is important to get improved performance.
Jain et al. [12] use information theoretic criterion for distance metric learning.
However, LSH can be combined with any other task-speci c distance metric learning methods such as NCA, LMNN, etc.
  From an optimization view point, our formulation and implementation (Section 4) are simple and scalable.
We observed that our algorithm, parallelized over 8 cores, completes learning in 2 days on a 1.46 million image dataset.
At test time on the same dataset, 100000 query searches over 1.46 million vectors with a 32-bit representation using an in verted index are completed in 1.37 seconds (13.7 microseconds per query on average) on a single core of an Intel Xeon
 We conduct detailed experimental study (Section 5) on several benchmark datasets for two applications: 1) classi cation and 2) information retrieval.
Comparisons with state-of-the-art NN methods show that our method performs signi cantly better.
We discuss applicability of our framework to other problems such as hierarchical daRank can optimize more general, listwise ranking evaluation scores.
Suppose LambdaRank is optimizing a ranking function for some
 query and the document are two distinct types of inputs, but we do not consider it here.
of LambdaRank de nes the gradient to be that function s RankNet gradient for a pair of documents, multiplied by the change in the score s value | S| if those two documents swapped positions in the full listwise ordering computed by the current ranking function.
Optimizing with the LambdaRank gradient has been shown empirically to converge to a local optimum of various non-smooth IR evaluation scores [7].
Adapting LambdaRank to learn a bit vector representation requires several modi cations.
These are 1) the parameterization of the ranking function, 2) the procedure for  nding pairwise swaps that give nonzero | S|, and 3) the de nition of appropriate evaluation scores for various tasks.
In addition, we show how to apply LambdaRank to non-retrieval tasks such as classi cation and regression with bit vectors.
Details are in section 3.
After learning, the bit vector representation is used to  nd nearest neighbors for a query vector within a set of documents.
The query is  rst converted into a bit vector using the learned function, with the output of the sigmoid function thresholded at 0.5.
The bit vectors for the documents to be searched can be precomputed and stored in memory.
The query bit vector is then compared against the document bit vectors in the search set with Hamming distance as the metric.
Documents with the lowest Hamming distances are returned as neighbors.
Two important choices that need to be made in bit vector-based nearest neighbor search are 1) the type of thresholding to use on the Hamming distance, and 2) how to resolve ties in the ranking.
These choices are task-speci c, so we discuss them in section 3.
Now we explain how the training data for different tasks are constructed to  t the learning-to-rank framework.
Then we de ne the function for computing the bit vector representation, how it is used for ranking, and how it is learned from data using the RankNet and LambdaRank algorithms.
Learning requires triplets, each consisting of a query and two documents, along with the desired pairwise ranking.
Depending on the task (e.g.
retrieval, nearest neighbor classi cation, nearest neighbor regression, etc.
), how these triplets are created differs.
NN Classi cation: Given a classi cation dataset consisting of (input, label) pairs, we de ne triplets as follows: the input for which the class label needs to be predicted is the query.
Inputs in the training set that are used as candidate neighbors are the documents.
Given a query belonging to a particular class, we want documents of the same class to get ranked higher than documents of all other classes.
We will denote input vectors by v and their labels by l.
So three class-labeled inputs (v1, l1), (v2, l2), and (v3, l3), where l1 = l2 and l1 (cid:2)= l3 are turned into a triplet (v1, v2, v3) where v1 is the query, v2 and v3 form the document pair.
Retrieval: A retrieval dataset may already be in the desired query-document format, where each query has a corresponding list of documents ordered by a relevance label.
In such a case, it is clear how to construct triplets.
In image retrieval applications (such as the ones used in our experiments) we may simply be given for each query a set of documents that are equally relevant (i.e. the relevance label is either 0 or 1).
Here we can follow the same strategy as in the classi cation setting and create triplets of the form (v1, v2, v3) where v1 is the query, v2 is a relevant document for the query, and v3 is an irrelevant document.
Let v   (cid:4)D be the D-dimensional input vector (either a query or a document) for which we want to compute the bit vector.
First we de ne a function for computing a real-valued B-dimensional vector h whose components have values between 0 and 1, h   [0, 1]B, from v. The bit vector is subsequently computed from h by rounding.
The jth component of h is given by hj = 1 + exp      
 bj +

 i=1 Wijvi   , (1) where vi is the ith component of h, Wij is a learnable weight parameter, bj is a learnable bias, and 1/(1+exp( ( )) is the sigmoid function.
The function for computing the entire h vector is param-eterized in terms of a D   B matrix W and the B bias parameters ( gure 2b).
To simplify notation we omit bias terms from the equations in the rest of the paper without loss of generality5 The bit vector b is computed by rounding each component of h to 0 or 1.
Note that h is differentiable with respect to W , but b is not because of the non-smoothness of rounding.
We want to formulate an algorithm for learning W within a learning-to-rank framework.
To do this we  rst need to de ne a ranking function suited for learning a bit vector representation.
It takes a query and a document as inputs and computes a scalar score with which the document can be ranked.
Given a query vq and a set of documents {vd1 , vd2 , ..., vdn} (all in (cid:4)D), the score can be computed for each query-document pair, followed by a sort, to rank the documents with respect to the query.
Let bq   {0, 1}B be the bit vector for the query and bi   {0, 1}B be the bit vector for the ith document.
The ranking score si for the ith document is the Hamming distance between bq and bi.
Documents are ranked in ascending order of the score.
For gradient-based learning we need the ranking score to be dif-ferentiable with respect to W .
The score si does not meet this requirement because rounding is not a differentiable function.
So we compute an alternative score  si using h (from equation 1) instead.
Since h is real-valued, Hamming distance is no longer appropriate.
One alternative is to use Euclidean distance, i.e. let the score for the ith document be the Euclidean distance between hq and hi.
When the components of hq and hi are exactly binary, the score will be the same as Hamming distance.
One drawback is that the score can also be zero even when hq and hi are not binary, but still identical.
For example, if the components of both vectors are all 0.5, then the score will be zero.
A second alternative is the following:  si =
 j=1 hqj(1   hij) + (1  hqj)hij, (2) where hqj and hij are the jth component of hq and hi, respectively.
It  relaxes  Hamming distance to non-binary values, and becomes equal to Hamming distance for binary values.
It is not a proper distance metric   two identical vectors will not give a score of zero unless they are binary.
We use this relaxed Hamming score in our experiments.
ement 1 to the input vector and de ning W to be a (D + 1)   B matrix where the extra row corresponds to the biases.
gradient as follows: As mentioned before, LambdaRank is based on RankNet.
Now we describe RankNet in some detail because it is required for Lamb-daRank.
Also it serves as an important baseline in our experiments to demonstrate the usefulness of task-speci c optimization.
So far we have formulated a ranking function that incorporates the smooth version of the bit vector representation and is differen-tiable with respect to W .
Now we plug this ranking function into the RankNet algorithm to learn W .
Given a query vq and a pair of documents, vd1 and vd2, let vd1  vd2 denote the event that vd1 is ranked higher than vd2.
RankNet adopts a probabilistic view that allows for uncertainty in the pairwise ranking.
The ranking scores of the two documents do not deterministically imply an ordering.
Instead they de ne a probability that vd1 is ranked higher than vd2: P (vd1  vd2 ) =
 1 + exp( s1    s2) , (3) where  s1 and  s2 are the ranking scores for vd1 and vd2, respectively (as computed by equations 1 and 2).
Note that if  s1 <  s2, then P (vd1  vd2 ) > 0.5.
Intuitively if one document is closer to the query than the other, it has a higher probability of being ranked higher than the farther one.
And if  s1 <<  s2 (document 1 is much closer to the query than document
 Consider the simple case where our training set consists of only one triplet (vq, vd1 , vd2 ).
Let T12 be the target probability that vd1 is ranked higher than vd2 with respect to vq.
For example, T12 = 1 if vd1 should be ranked higher than vd2, and 0 if the opposite is true.
The cost function used by RankNet for learning is: C =  T12 log P (vd1 vd2 ) (1 T12) log(1 P (vd1 vd2 )).
(4) C is the cross entropy between the two Bernoulli distributions P (vd1  vd2 ) and T12.
The parameters of the ranking function are learned via gradient descent on C. C is a smooth differentiable function of W , and an expression for  C  W can be derived analytically.
Equation 4 considers only a single training triplet, but it can be applied to the case where multiple training triplets are available by simply minimizing the average cost of all the triplets.
Note that with RankNet only the way in which the triplets are de ned differs from task to task.
The training cost function and the optimization procedure stay the same across tasks.
In contrast, LambdaRank allows a different cost function for each task, which makes it possible to customize the learning.
We begin with a general description of LambdaRank, and then describe the adaptations speci c to learning a bit vector representation for classi cation and retrieval in sections 3.5.1, 3.5.2, and
 We keep the parameterization of the ranking function and the de nition of the ranking score the same as before, but change the learning algorithm to LambdaRank ( gure 3).
This affects two things   1) the objective function used for learning, and 2) the procedure for computing its gradient.
For simplicity we present LambdaRank here as a modi ed version of RankNet, but the underlying ideas are more general.
Consider again a RankNet training triplet (vq, vd1 , vd2 ) where vq is a query, and vd1 and vd2 are two documents to be ranked with respect to vq.
Suppose that we want the learning to optimize the ranking function for an evaluation score S. S can be a listwise ranking score, e.g.
NDCG.
Then LambdaRank modi es the RankNet


 (5) where C is the RankNet cost function (equation 4) and W is the weight matrix to be learned.
| S| is the absolute difference in the value of S due to swapping the positions of vd1 and vd2 in the ordering of all documents, with respect to vq, computed by the current ranking function.
Note that LambdaRank learns on triplets, as before, but now only those triplets that produce a nonzero change in S by swapping the positions of the documents contribute to the learning.
Given a query,  rst all documents are ordered with respect to it using the ranking function given by the current W .
Evaluating this ordering will give some score S1.
Now pick any two documents and swap their positions in the ordering.
Evaluating this new ordering will give some score S2.
LambdaRank then computes the RankNet gradient for those two documents and the query, and multiplies it by |S1 S2|.
Only the score function S needs to be changed from task to task in LambdaRank.
We now describe aspects of LambdaRank training that are spe-ci c to our approach.
Score function for classi cation:
 Suppose that for a query vq, L documents {vd1 , vd2 , ..., vdL} are selected as neighbors (as explained later).
Let lq and ldi be the class labels of the query and the ith document, respectively.
The score function SC for nearest neighbor classi cation of a single query is: SC = [M AJORIT Y (ld1 , ld2 , ..., ldL ) =l q], (6) where the M AJORIT Y ( ) function picks the most frequent label in {ld1 , ld2 , ..., ldL} and [ ] is Iverson notation denoting an output 1 if the argument is true and 0 otherwise.
One disadvantage of the above score is that it does not re ect incremental progress towards the correct answer.
For example, if the correct label is three votes short of becoming the majority, then a swap that increases its vote count by 1 will not change the above score.
But such a swap moves the result closer to the correct answer and therefore should be used.
To allow for such swaps, we use the following the score:
 [ldi = lq].
(7) i=1 Note that without the M AJORIT Y ( ) function, even swaps that would make all the neighbors have the same label as the query would be allowed.
This is more stringent than necessary for nearest neighbor classi cation.
A score function with a looser requirement may give better accuracy, but we have not yet explored that.
Thresholding for neighbor selection: There are two types of thresh-olding that can be used on the Hamming distance.
An absolute threshold k selects all documents with Hamming distance   k from the query as neighbors.
A relative threshold k selects only those documents in the k nearest nonempty Hamming distance bins from the query as neighbors.
Consider an example where the nearest documents from the query appear at say, distances 4, 7, 12, 17, 20, etc.
and k = 3.
With an absolute threshold, no documents would be selected, which makes nearest neighbor classi cation ambiguous.
But with a relative threshold, documents at distances 4, 7, and
 set of neighbors will always be nonempty, we use it for nearest neighbor classi cation.
tion, then Hammming distance can take on only B + 1 possible values (0 to B).
If the number of documents being ranked is much greater than B + 1, then there will be a lot of ties in the ranking.
For nearest neighbor classi cation, we do not break ties.
Instead we use all the documents from the selected bins to vote on the class label.
In section 5 we consider two retrieval tasks where the relevance label of a document is binary (either 0 or 1), and accuracy is measured using precision (# of retrieved documents that are relevant divided by # of retrieved documents).
We de ne the score function for this particular setting.
Again consider a query vq, and L documents {vd1 , vd2 , ..., vdL} retrieved for that query with an absolute threshold k. Let ldi be the relevance label for the ith document.
The score function SR for retrieval is:

 ldi .
(8) i=1 We do not normalize the score by L in order to prevent queries that have a small number of retrieved documents from contributing disproportionately to the gradient.
Resolving ties: During learning, we do not break ties in the ranking.
But during testing, tie-breaking may be needed.
For example, consider a task that requires retrieving exactly 100 documents for a given query.
If the nearest Hamming distance bin to the query contains 110 documents, then some re-ranking procedure would be needed to select exactly 1006.
In such an application a bit vector representation is still useful for rapidly shortlisting a small set of documents for the re-ranker.
Note that tie-breaking is needed for any approach that uses bit vectors, not just ours.
A key step in the per-query gradient computation of LambdaRank is to identify only those pairwise swaps that have nonzero | S| (steps 3 and 4 in  gure 3).
At  rst glance,  nding such swaps may appear to be a very expensive computation that requires sorting all the documents in the training set for every query.
But in the case of learning a bit vector representation, a fast approximation turns out to be possible.
Given a query, there are only a  xed number of Hamming distance bins that the documents can belong to.
For ef ciency, we use swaps only among those documents that belong to a small subset of bins closest to the query.
Finding documents that belong to the nearest bins can be done ef ciently using a heap structure, without having to compute the full Hamming distance and sorting over the entire training set.
Subsampling: To signi cantly speed up the per-query gradient computation, we subsample the set of documents from which document pairs are selected for each query.
A noisy estimate of the gradient can be computed cheaply from a small subset of the full document set.
This is helpful when the training set is large and computing the gradient from the full set is too expensive.
In the case of RankNet, for each query we use only a small subset of the full training set to generate pairs for that query.
Most of our experiments (section 5) use only 100 randomly chosen documents per query to generate triplets.
and select the top 100.
LambdaRank learning algorithm: Training inputs: - A ranking metric M (e.g.
NDCG) to be optimized.
- A set of training examples where the ith example contains:   (cid:3)D,
 q
 , ..., vi dN } all in (cid:3)D.
d1 - Number of bits B.
- Learning rate parameters: step-size  , momentum m.
Training outputs: D   B weight matrix W .
Initialization: Wij are sampled from zero-mean Gaussian with small (10 3) variance,  W = zero matrix.
Weight update computed using the ith training case:

 q using equation 1.
(a) Compute real-valued representation hk for vi dk using equation 1.
(b) Compute score  sk using equation 2 from hq and hk.
in ascending order of the scores  sk.
Let SO be the value of the evaluation score S for O.
For an ef cient approximation, see sections 3.5.3 and 4.
a such that swapping the positions of (vi a and vi for which |SO   SO(cid:2)| > 0.
a new ordering O(cid:2) b )   V compute P (vi
 a , vi b , vi b equation 3.
) selected from Li ) in O results in  vi b ) using a with respect to W .
 W of the cost function C (equation 4)



 Figure 3: Summary of LambdaRank learning algorithm.
).
For LambdaRank, we restrict the number of bins from which documents are considered for swaps to be one-third of the number of bits.
For the tasks considered here, swaps only happen between a document in one of the top k bins and a document outside of the top k bins, but still within the restricted set.
We can make this even more ef cient by considering only a subset of the full training set (Li in step 2 of  gure 3) to populate these bins.
How much subsampling can be done without degrading accuracy depends on the dataset, but in our experiments we have seen that good results can be achieved even with 10  subsampling.
Gradient descent: The weight matrix W is updated by averaging gradient estimates given by a set of queries and taking a step along the average gradient.
Averaging can reduce the noise in the updates.
The average is typically computed over 100 queries.
One can easily parallelize this computation by splitting the set of queries across multiple cores and then averaging together the gradients computed at all cores.
 1, 10 We use a  xed step size to update W .
The best value depends on the dataset   for the datasets in section 5 we have tried values in the  4].
On some datasets we have observed signi cant range [10 sensitivity in the accuracy to the step size value.
We set the step size to the highest possible value that does not produce large oscillations in the objective function value during optimization.
We have not yet tried any second-order optimization methods like conjugate gradient to improve convergence speed.
Such methods may help in RankNet training, but are unlikely to be useful cannot be directly evaluated.
As in [11], we maintain an exponentially decaying sum of the previous gradients which is added to the current gradient to compute the weight update.
The decay factor is set to 0.8, so the effect of the gradient computed at a particular step persists for several steps afterwards.
Performance is evaluated on two types of tasks: 1) nearest neighbor classi cation and 2) retrieval.
We present results for four clas-si cation and two retrieval datasets.
The evaluation metric for classi cation is the number of incorrect label predictions on a test set.
Given a test case, its bit vector is compared against the bit vectors for all the training cases.
Neighbors are selected using a relative threshold k   those training cases that fall within the nearest k nonempty Hamming distances to the test case bit vector are returned.
The predicted label is then picked by a majority vote among the neighbors.
For retrieval we use the same precision metric as in Weiss et al.
[23].
Given a query, its bit vector is compared against the bit vectors for all the documents in the search set.
Documents are selected using an absolute threshold   those documents that are less than a pre-speci ed Hamming distance threshold from the query bit vector are retrieved.
We consider binary relevance labels here7   a document is either relevant for a query or not.
So precision for a single query is computed as follows: Precision = # of relevant documents in the retrieved set # of documents in the retrieved set .
(9) This quantity is then averaged over a held-out set of queries to get a single precision value.
Tables 2 and 3 summarize the datasets.
The datasets span a range of training set sizes (60K to 1.45 million), input dimensionality (128 to 47K), and in the case of classi cation, number of classes (7 to 101).
They include a number of different types of data   images (MNIST, INRIA SIFT 1M, Tiny Images Subset), text documents (MCAT, RCV), and geospatial measurements (Covertype).
We brie y describe each dataset: MNIST8 is a set of handwritten images of the digits 0 to 9.
The images are grayscale and of size 28   28.
MCAT contains text documents that belong to a subtree of the Reuters Corpus Volume 1 (RCV1) dataset [14].
A document is represented as a bag-of-words with a vocabulary size of 11429.
Only about 0.58% of the word counts are nonzero, so the representation is sparse.
Covertype9 [3] is a dataset of geospatial measurements that are used to predict the forest covertype at various locations in the US.
RCV1 Subset10 contains only those documents in the RCV1 dataset that do not have multiple labels associated with them.
As in MCAT, a document is represented as a bag-of-words, but with a much
 8http://yann.lecun.com/exdb/mnist/
 10http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ binary.html\#rcv1.binary Dataset Train set Test set

 Covertype
 size



 size



 Input Number of dim.
classes





 Table 2: Classi cation datasets.
Dataset

 Tiny Images Subset Train set size
 # of test queries
 # of test docs to search
 Input dim.
Table 3: Retrieval datasets.
larger vocabulary size of 47236 and 101 classes.
The representation is 0.14% sparse.
INRIA SIFT1M11 is a web image dataset designed for evaluating retrieval algorithms.
It consists of 128-dimensional SIFT features [15] computed for images collected from the web.
Given a query image s SIFT feature vector, the relevant images to retrieve are de- ned to be its 50 nearest neighbors, according to Euclidean distance, in the SIFT feature space.
Tiny Images Subset is derived from the Tiny Images dataset [21], which contains 80 million images collected from the web.
Various text queries were given to popular image search engines and the results were downloaded.
The subset we use here contains only the top-ranked images for the query terms, so it is likely to be less noisy than the full 80 million set.
The images are represented using 512 dimensional GIST feature vectors [17].
The retrieval task is de ned in the same way as in INRIA SIFT1M: the goal is to retrieve the
 image s GIST feature vector.
We compare against other methods that compute a bit vector representation.
For retrieval, Spectral Hashing is a state-of-the-art method.
Binary LSH is a commonly used baseline in the literature, so we compare against it as well.
We use the Matlab implementation of Spectral Hashing by the authors of that paper12.
For classi cation, we again use Spectral Hashing and LSH as baselines.
However these methods were not intended to be used for classi cation, so they cannot be taken as strong baselines.
Therefore we decided to compare also against nearest neighbor methods that do not use bit vectors.
The simplest one is kNN classi cation with L2 distance.
State-of-the-art learning methods are NCA and LMNN.
LMNN does not scale to datasets with more than a few tens of thousands of training cases [22], so we cannot run it on Cover-type, MCAT, and RCV1 Subset.
For MNIST we quote the LMNN classi cation error from [22].
For NCA, we use the implementation in the Matlab Toolbox for Dimensionality Reduction13.
Note that one can always apply binary LSH on top of a metric learning method like NCA or LMNN to construct bit vectors.
But such a two-stage approach will at best give the same accuracy as the 11http://corpus-texmex.irisa.fr/

 _Dimensionality\_Reduction.html mation of it.
So we compare against the accuracy of the real-valued metric directly, i.e. the best-case result for the two-stage approach.
On the RCV1 Subset, Spectral Hashing required doing an eigenvalue decomposition of the 47236   47236 data covariance matrix and selecting the top eigenvectors.
For 32 and 64 bits, Matlab was not able to perform this computation due to excessive memory use, even with 32GB of RAM.
Those results are not given.
In NCA, the dimensionality of the output of the linear projection matrix can be made less than the input dimensionality.
The dimensionality we chose is 60 for MNIST, 500 for MCAT, 54 for Covertype, and 500 for RCV.
On MNIST 60 gave the same accuracy as bigger values.
On Covertype the input dimensionality is already small (54), so a lower number was not tried.
On MCAT and RCV, we chose 500 to keep the training times reasonble.
Table 4 shows the test set error rates for the four classi cation datasets.
We use a relative threshold of k = 3 for all methods that use bit vectors.
LambdaRank achieves the lowest classi cation error on three out of four datasets.
Among bit vector methods, LambdaRank is 48.9% better than others (averaged over four datasets), with RankNet being the closest competitor.
LambdaRank is on average 26.8% better than the best RankNet result.
Only on the Covertype dataset does RankNet perform comparably to Lamb-daRank.
So task-speci c optimization by LambdaRank improves accuracy, and as MNIST and Covertype results show, the improvement can be substantial (+54% and +42%, respectively).
Recall from section 3.5.1 that there can be ties in the Hamming distance ranking.
A relative threshold k only guarantees that the number of neighbors used to classify a given test case is at least k.
The actual number can be much larger than k if there are many ties.
All the four bit vector methods compared in table 4 share this property.
The better accuracy of LambdaRank despite this commonality implies that simply having a large number of neighbors to classify a test case is not suf cient for high accuracy, and that learning a good bit vector representation is also crucial.
The bene t of LambdaRank can also be measured in terms of the bit  compression  it gives with respect to the other methods while matching their best error.
In many cases LambdaRank needs signi cantly fewer bits.
To get a rough idea of how big the compression factor is, we linearly interpolate between the datapoints in table 4 to determine the number of bits needed by LambdaRank to achieve a particular error rate.
On average the ratio of the number of bits needed by LSH, Spectral Hashing and RankNet to achieve their best error rate divided by the number of bits needed by Lamb-daRank to achieve the same error rate is 7.1 , 4.1 , and 2.0 , respectively.
Table 5 shows the precision results computed on the test sets of INRIA SIFT1M and Tiny Images Subset.
Again, LambdaRank gives the best results on both datasets.
It gives 1.2  and 2.66  better precision than Spectral hashing on the two datasets.
The improvements over RankNet are again substantial for both datasets: +6.5% and +13.6%.
This adds further evidence to the usefulness of task-speci c learning.
The training time of our algorithm scales well to large datasets.
On Tiny Images Subset (1.46 million training cases), learning with
 tel Xeon 2.50GHz machine.
Figure 4 shows the convergence be-No.
of bits LSH Spectral Rank Net hashing Lambda Rank Non-bit vector methods
















































 Covertype














































 --kNN, L2: 3.09

 kNN, L2: 3.67
 kNN, L2: 6.25
 kNN, L2: 29.67
 Table 4: Classi cation error (%) on the test sets of MNIST, MCAT, Covertype, and RCV1.
No.
of
 Spectral hashing Rank Net INRIA SIFT1M (Precision  10


 Tiny Image Subset (Precision  10





 bits





 Lambda Rank  4)


  5)











 Table 5: Precision at Hamming distance < 2 from the query on the test sets of INRIA SIFT1M and Tiny Images Subset.
haviour.
Even after one pass through the data, the precision on the test set has already reached 90% of its  nal value.
The memory needs of the learning algorithm are minimal.
The weight matrix, its gradient, and the bit vectors for the training set (computed using the current weight matrix) account for most of the memory use, and all of these together takes up much less memory than the training data itself (e.g.
for Tiny Images Subset these variables take up 0.2% of the memory occupied by the training data).
Table 6 shows the CPU times needed for  nding the nearest neighbors for the test set of different datasets by the different algorithms.
Note that nearest neighbor classi cation with bit vectors is substantially faster than with real-valued representations, even without using an inverted index.
This is because the distance calculation for bit vectors is done using bitwise XOR on chunks of 8 bits, and the 8-bit result is converted into a Hamming distance with a lookup table.
The distances from the 8-bit chunks are then added together to get the full distance.
So computing the Hamming dis-y r e u q m o r f
 < e c n a t s i d g n i m m a
 t a n o i s i c e r
 14.5 hours 2 days








 Epochs


 Figure 4: Precision on the test set as a function of the number of passes through the Tiny Images Subset training data.
Note that most of the improvement happens in the  rst pass.
Dataset Lambda

 Covertype RCV1 Subset Rank




 kNN








 Table 6: CPU time (seconds) required to  nd the nearest neighbors for the entire test set by different methods on the various datasets.
tance between two 256-bit vectors requires only 256 bitwise XORs, 32 accesses into a lookup table and 32 adds.
For INRIA SIFT1M and Tiny Images Subset, the number of bits in the learned representation is small enough to build an inverted index that  ts into 4GB RAM.
We use subroutines from the Spectral Hashing software (see footnote 11) to build the inverted index and use it for search.
The CPU times for  nding the nearest neighbors on the test sets of INRIA SIFT1M and Tiny Images Subset with a 32-bit representation and an inverted index are 0.10s and

 search time per query of 10 microseconds for INRIA SIFT1M and
 on the same 32-bit representation is much slower than the inverted index.
The total CPU times for  nding the nearest neighbors on the test sets of INRIA SIFT1M and Tiny Images Subset are 63.09s and 1674.40s, respectively.
Learning nonlinear features: Currently the ranking function does not contain any learnable nonlinear features of the input vector.
Including such features is easy and can make the function more  ex-ible and accurate.
For example, one can use hidden units from the neural network literature to learn nonlinear features.
Cascade architecture: A common way to speed up search is to use a multistage, cascade architecture where a fast  rst stage search rules out a large fraction of the search set, followed by increasingly slower stages that only need to search the set retrieved by the previous stage.
Such an architecture can also be used in our case   different ranking functions can be cascaded in increasing order of the number of bits, with each stage searching only among the neighbors found by the previous stage.
We have tried a two stage classi er on the MNIST dataset with an 8-bit ranking function  rst and then a 256-bit classi er.
The search set size for the 256-bit classi er reduces by an order of magnitude with almost no loss in accuracy.
Note that with LambdaRank it is possible to modify the training of the initial  ltering stages such that they are explicitly trained to  lter (and not classify).
We have not yet explored this option.
Hierarchical classi cation: If a hierarchy over classes is given in a classi cation task, then the score function for LambdaRank can be modi ed to incorporate this information.
Instead of binary relevance (1 for a document with the same label as the query, 0 otherwise), now we use multiple relevance levels to represent varying degrees of similarity between two classes.
The similarity between two classes can be de ned in many ways, e.g.
with a monotonically decreasing function of the height of the common ancestor of the two classes in the hierarchy.
For a query vq and L documents {vd1 , vd2 , ..., vdL} retrieved from the top K nonempty Hamming distance bins, we can modify the original classi cation score function (equation ) as follows: SHier = F (lq, ldi ).
(10)
 i=1 where lq and ldi are the labels of the query and the ith document, respectively, and F (a, b) is a function that speci es the similarity between classes a and b.
Regression: As mentioned before, a bit vector representation for nearest neighbor regression can be learned using our approach.
In the case of RankNet, pairwise ranking of a document pair with respect to a query can be done using the absolute difference between the query target and a document target.
The document with the closer target to the query in the pair should get ranked higher.
For LambdaRank we need to de ne a score function.
Consider a query vq, and L documents {vd1 , vd2 , ..., vdL} retrieved from the top K bins for that query.
Let tq and tdi be the regression target values for the query and the ith document.
The score can be the mean squared error between tq and the document targets: SRegression =

 (tq   tdi )

 i=1 (11) As a preliminary experiment, we have trained a 64-bit RankNet model on the SARCOS dataset14 (see section 2.5 of [18]).
It achieves a better standardized mean squared error than a linear regression model.
Experiments with LambdaRank is left as future work.
One potential application of nearest neighbor regression is in Collaborative Filtering.
Neighborhood based models have been shown to be useful for predicting the rating a user would give to an item [2].
For example, in a user-based neighborhood model, given a user-item pair for which to predict a rating, a bit vector-based representation can be used to retrieve similar users who have rated the same item.
The predicted rating is then computed as a weighted average of the ratings of the retrieved users.
We discussed several popular methods closely related to our work in the introduction, and empirically compared with representative methods.
Here, we present other related work.
Hashing Methods Kernel LSH [13] is a recent scheme that generalizes LSH, and is useful when similarity measure is given directly via a kernel function (without explicit knowledge of the underlying transformation), or the underlying transformation is in nitely 14http://www.gaussianprocess.org/gpml/data/ optimization method to optimize the codes for both preserving similarity as well as minimizing search time.
The main drawback of these hashing approaches is that they cannot be directly used in applications where we are not given a similarity metric but rather class/relevance labels that indicate which data points are similar or dissimilar to each other.
Metric Learning Methods There are other methods that learn the distance metric by optimizing task speci c performance measure.
Classi cation task oriented methods such as NCA and LMNN discussed before fall under this category.
Other approaches include Fisher s linear discriminant analysis (FLDA), maximally collapsing metric learning algorithm (MCML) [8], relevance component analysis (RCA) [20], etc.
While FLDA and MCML use class label information, RCA assumes that a set of chunklets is available, where each chunklet is a set of examples which belong to the same class (but, the class information is unknown; hence, RCA can be seen as a weaker form of supervised learning).
Both FLDA and RCA involve matrix inversion (in the dimension of input space), and use projections on eigen vectors for nearest neighbor classi- cation.
Therefore, they are computationally expensive for high dimensional data.
MCML method tries to collapse all examples belonging to the same class into a single point, and keep examples belonging to other class far away.
Like LMNN, MCML uses positive de nite matrix constraints during training.
All these methods are not scalable, and are limited to classi cation application.
To summarize, our work can be seen as merging two streams of work, one for learning a task-speci c metric from labeled data, and the other for learning bit vector representations for doing fast search.
To our knowledge this is the  rst attempt that combines the two types of learning into a uni ed framework.
We have presented a uni ed approach to learning a bit vector representation for nearest neighbor search in different tasks.
The key contribution is the ability to customize the learning algorithm to the task at hand by modifying the score function used in Lamb-daRank training.
What makes the uni ed approach possible is that the differences across the tasks are abstracted out of the core nearest neighbor search problem and pushed into the score function in LambdaRank.
As a result the same algorithm can be easily adapted to different tasks to learn accurate representations for each.
Experimental results clearly demonstrate that our algorithm 1) outperforms other methods for nearest neighbor classi cation and retrieval, and 2) scales well to large text and image datasets, making it particularly useful for web applications.
