Graph pattern matching is being increasingly used in various applications, e.g., software plagiarism detection, protein interaction networks, social networks and intelligence analysis [18, 25, 26].
Graph matching is typically de ned in terms of subgraph isomorphism (see, e.g., [15] for a survey).
Hence the problem is np-complete [27].
Furthermore, subgraph i-somorphism is often too restrictive to catch sensible matches in emerging applications such as social networks [5, 13].
To reduce the complexity and capture the need of novel applications, graph simulation [16] has been adopted for pattern matching [5, 13].
It is less restrictive than subgraph isomorphism, and can be determined in quadratic time [16].
We say that a graph G matches a pattern Q, via graph sim-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Figure 1: Pattern query and fragmented data graph ulation, if there exists a binary relation M   Vq   V , where Vq and V are the two sets of nodes in Q and G, respectively, such that (1) for each (u, v)   M , u and v have the same label; and (2) for each node u in Q, there existsv in G such that (a) (u, v)   M , and (b) for each edge (u, u(cid:2)) in Q, there exists an edge (v, v (cid:2)) in G having (u(cid:2), v(cid:2))   M .
Graph simulation (and its extensions) play a critical role for the analysis of social positions/roles in social networks [5, 13, 12].
When evaluating a query on a large dataset, one wants to partition and distribute the data to multiple machines, so that the query can be e ciently evaluated in parallel, as advocated by, e.g., MapReduce [11] and Pregel [21].
Moreover, it is common to  nd distributed real-life datasets, such as Facebook, Yahoo Flickr, Twitter, and Apple AppStore, that are stored at data centers, which typically host a cluster of tens to thousands of machines [1].
Hence, a natural question raised is how to evaluate graph pattern matching, in terms of graph simulation, on distributed data.
To our knowledge, no such distributed algorithms are in place yet.
Example 1: Consider a real-life example for social matching, taken from [24], that recruits people from a social system to set up a team to develop a new software product.
Ideally such a team consists of members with the following roles: (1) project manager (PM), (2) business analyst (BA), (3) software architect (SA), (4) user interface designer (UD), (5) software developer (SD), and (6) software tester (ST).
All candidates are stored in a social system in the form of data graph G1 as shown in Fig. 1, where (1) a node is a person labeled with her expertise (with subscripts to distinguish one from another), and (2) an edge (A, B) indicates the relationship that B worked well under the supervision of A in previous projects.
The relationships of team members are particularly important as the success of the software product heavily relies on their collaboration.
To identify the proper candidates from G1, pattern query Q1 is designed that requires (1) all SA, BA and UD worked well under the same PM, (2) UD worked well under BA such that the user interface could clearly re ect the idea of BA, (3) SD worked well under SA and ST, and (4) ST further worked well under SD as they are mostly related in the development.
F5 (separated by dotted cycles), and it is distributed over a cluster of  ve machines, one fragment on each machine.
Observe the following.
(1) When graph simulation is used, a sensible match containing all nodes in F3, F4 and F5 are iden-ti ed.
In contrast, when subgraph isomorphism is adopted, it imposes too strict constraints such that no matches can be found.
(2) Furthermore, to build a well-organized team, one has to query over all these fragments over di erent machines to avoid missing any potential candidates.
This highlights the need for developing distributed algorithms for graph simulation.
There are a few cheap solutions.
(1) A naive one simply collects all parts of a data graph into one machine, and calls a centralized algorithm, e.g., [16].
(2) Another one is to build a MapReduce [11] or Pregel [21] system, and to delegate most of the work to the system.
There are, however, several problems for these two solutions.
Obviously the  rst solution does not make use of the distributed facilities at all.
Graph simulation needs recursive computations, as illustrated by its recursive de -nition given before.
MapReduce is typically not  t for this kind of graph algorithms, which needs a series of chained MapReduce invocations [7, 21].
Pregel utilizes a message-passing model, which is typically  t for graph algorithms.
However, if we simply delegate the computation tasks to Pregel, it may involve too many rounds of computations.
As one knows, a good solution for graph simulation must exploit the nature of graph simulation itself.
As observed in [21], graph algorithms often exhibit poor data locality and hence, may incur prohibitive overhead on network tra c.
One may verify that to make G1 match Q1, it essentially requires the complete information of the subgraph consisting of fragments F3, F4 and F5 (, even the entire data graph in the worst case).
That is, graph simulation has poor data locality (more sophisticated analyses are available in Section 3).
This further brings challenge for developing distributed algorithms for graph simulation.
2 Contributions & Roadmap.
To this end, we develop algorithms and optimization techniques for evaluating graph simulation in a distributed setting.
Our contributions can be summarized as follows.
(1) We study fundamental properties of graph simulation (Section 3).
We show that connected components in data graph can be treated separately, and the  nal matches are simply the union of those matches for all single components.
We also show that graph simulation has poor data locality.
Nevertheless, we identify cases when data locality could be exploited to facilitate the evaluation of graph simulation.
(2) We give an analysis of a large class of distributed algorithms for graph simulation (Section 4).
These distributed algorithms are captured by a message-passing model, which is  exible enough to express a broad class of algorithms [19], and is typically  t for the evaluation of graph algorithms [21].
We also identify three complexity measures: visit times, makespan and data shipment, for the analyses of this class of distributed algorithms, and show that these measures are controversial with each other.
As e ciency (makespan) remains the dominant factor, we make a decision to sacri ce visit times and data shipment for makespan when designing distributed algorithms.
(3) We propose distributed algorithms which exploit the properties of graph simulation and the analyses of distributed algorithms (Section 5).
The algorithms guarantee the following.
(a) The total computation cost at all machines is comparable to what is needed by the best-known centralized algorithm [16].
Moreover, the number of rounds of computation is bounded by a constant 4.
(b) The total data shipment is at most |G| + 4|B| + |Q||G|+(k 1)|Q|, where |G| and |Q| are the sizes of data graph and pattern graph, respectively, and |B| is the total number of nodes with edges across di er-ent fragments (boundary nodes), and k is the total number of machines.
(c) Each machine except the coordinator is visited at most g + 2 times, where g is the maximum number of machines at which a connected component resides.
The coordinator is visited with 2(k   1) extra times due to the need for scheduling data shipment and assembling the  nal result.
We also develop e ective optimization techniques.
(4) Using both real-life data (Google and Amazon) and synthetic data, we conduct an extensive experimental study (Section 6).
We  nd that our distributed algorithms for graph simulation scale well with large data graphs (e.g., with
 are e ective, reducing 1/5 of running time in average.
Related work.
There has been a host of work on graph pattern matching, via subgraph isomorphism (e.g., [18, 25, 26]; see [3, 15] for surveys) and via graph simulation [16] and its extensions [13, 12].
Nevertheless, none of these investigates the problem in a distributed fashion.
Distributed query processing has been studied for relational data [17] and XML [8].
There has also been recent work on distributed graph processing to manage large-scale graphs [11, 21].
However, to the best of our knowledge, no previous work has studied distributed computation of graph simulation [16] and its extensions[13, 12].
Close to this work is strong simulation [20], in which the locality property of strong simulation allows us to develop a simple yet e ective algorithm to  nd matches in distributed graphs.
In contrast, we show that graph simulation has poor locality, and hence the simple algorithm does not work here.
Message-passing model has been recently adopted for various famous distributed systems, e.g., Pregel [21].
Our distributed algorithms follows this model, which is  exible enough to express a broad class of algorithms [19], and is typically  t for the evaluation of graph algorithms [21].
In this section, we  rst present basic graph notations.
We then introduce the problems of graph pattern matching and its distributed counterpart, in terms of graph simulation.
We specify both pattern graphs and data graphs as follows.
Let   be a (possibly in nite) set of labels.
Graphs.
A node-labeled directed graph (or simply a graph) is de ned as G(V , E, l), where (1) V is a  nite set of nodes; (2) E   V   V is a  nite set of edges, in which (u, u(cid:2)) denotes an edge from nodes u to u(cid:2); and (3) l is a labeling total function that maps each node u in V to a label l(u) in  .
The size of G, denoted as |G|, is the total number of its nodes and edges, i.e., |V | + |E|.
We also denote G as (V, E) when it is clear from the context.
Intuitively, the function l() speci es node attributes, e.g., keywords, blogs, comments, ratings, names, emails, companies [4]; and the label set   denotes all such attributes.
Output: The maximum match M in G for Q.
such that postG(w)   sim(u) =   do sim(v) := sim(v) \ {w};


 Figure 2: Centralized algorithm HHK Subgraphs.
Graph H(Vs, Es, lH ) is a subgraph of graph G(V, E, lG) if (1) for each node u   Vs, u   V and lH (u) = lG(u), and (2) for each edge e   Es, e   E. That is, subgraph H only contains a subset of nodes and a subset of edges of G. We also denote subgraph H as G[Vs] if Es is exactly the edges that appear in G over Vs.
Descendants.
We say that node v is a descendant of node u in G if v is reachable from u, i.e., there is a directed path from u to v. We use desc(G, u) to denote the subgraph that contains the set of all descendants of u in G, including u itself, and the set of edges in G on those descendants exactly.
We next review the notion of graph simulation [16].
Consider a pattern graph Q(Vq, Eq) and a data graph G(V, E).
A binary relation R   Vq   V is said to be a match if (1) for each (u, v)   R, u and v have the same label, i.e., lQ(u) = lG(v); and (2) for each edge (u, u(cid:2))   Eq, there exists an edge (v, v(cid:2)) in E such that (u(cid:2), v(cid:2))   R.
Note that an empty binary relation is a match.
Graph G matches pattern Q via graph simulation, denoted by Q   G, if there exists a total match relation M , i.e., for each u   Vq, there exists v   V such that (u, v)   M .
Intuitively, simulation preserves the labels and the child relationship of a graph pattern in its match.
Simulation was proposed for the analyses of programs [16], and studied for schema extraction from semi-structured data [2].
Simulation and its extensions were recently introduced for social networks [5] and for graph pattern matching [13, 12].
Graph pattern matching.
The problem is to  nd, given any pattern graph Q and data graph G, the maximum match in G for Q if P   G. It was known that the following result holds [16, 13], by which the problem is well de ned.
Proposition 1: Given any pattern graph Q and data graph G, there exists a unique maximum match of G for Q no matter whether Q   G or not [16, 13].
Algorithm HHK.
We next present an algorithm for graph simulation in [16], denoted by HHK and shown in Fig. 2.
For each node u in Q, the set sim(u) contains candidate nodes in G, initially all nodes in G with the same label as u (line 1).
By the de nition, if (u, v)   Eq (v   postQ(u), successors of u), but there exist no nodes w(cid:2)   sim(v) such that (w, w(cid:2))   E (w(cid:2)   postG(v)), then w cannot be matched to v, and hence is removed from sim(u).
This process is repeated until there are no more changes (lines 2 3).
Finally, the maximum match is assembled and returned (lines 4 5).
Example 2: Consider pattern graph Q1 and data graph G1 shown in Fig. 1.
The maximum match computed by HHK is
 (SD, SDh), (ST, ST1), .
.
.
, (ST, STh)}.
Note that here PM1 cannot match PM since no child of PM1 is labeled with UD, and it is similar for the other false matches.
Remark.
(1) Graph simulation is computable in quadratic time [16].
Algorithm HHK does not run in quadratic time, but it is simple and easy to be understood.
Its further re ne-ment leads to a quadratic algorithm [16], the best available algorithm as long as time complexity is concerned [23].
(2) Algorithm HHK correctly computes the maximum match M in data graph G for pattern graph Q, and Q   G i  for each node v in Q, there exists a node u in G with (v, u)   M .
Hence we focus on computing the maximum match.
We now introduce graph fragmentation, followed by the problem of distributed graph pattern matching.
(cid:2)k i  (1) We say (G[V1], .
.
.
, G[Vk]) is a partition of graph G(V , E) i=1 Vi = V ; and (2) for any i (cid:5)= j   [1, k], Vi  Vj =  .
We also say node u in subgraph G[Vi] (1   i   k) is a boundary node if there exists an edge (u, v) in G from u to v in G[Vj ] such that i (cid:5)= j and 1   j   k. To maintain the completeness of G, for each boundary node u in a partition G[Vi], we maintain locally a set Bu of labeled nodes v : j such that there exists an edge from u to v in subgraph G[Vj ].
Fragmented graph.
A fragmented graph F of data graph G is denoted as (F1, .
.
.
, Fk), where (1) for each i   [1, k], Fi = (G[Vi], Bi) is a fragment of G placed at a separate machine Si, (2) (G[V1], .
.
., G[Vk]) is a partition of G, and (3) Bi (i   [1, k]) is the union of the sets Bu of labeled nodes of all boundary nodes u in subgraph G[Vi].
Example 3: Consider the fragmented data graph G1 in Fig. 1 that consists of  ve fragments F1, .
.
.
, F5.
Formally,

 }), F5 = (G[V5],  ), where (1) V1

 = {SA2}, and V5 = {SD1, ST1, .
.
.
,SD h, STh}, respectively; and (2) BPM1 = {BA1 : 2}, BSA1 = {SD1 : 2}, BPM2 = {SA2 :
 Remark.
(1) The fragmented graph has the same number of nodes and edges as the original graph except that the children of boundary nodes are labeled with IDs of the fragments in which they are located.
(2) Graph partition is np-complete in general [14], and is not the focus of this work.
Here we allow arbitrary fragmentation of data graphs.
Distributed graph pattern matching.
We now de ne the graph pattern matching problem in a distributed setting.
Given pattern graph Q, and fragmented graph F = (F1, .
.
., Fk) of data graph G, in which each fragment Fi = (G[Vi], Bi) (i   [1, k]) is placed at a separate machine Si, the distributed graph pattern matching problem is to  nd the maximum match in G for Q, via graph simulation.
In this section, we study fundamental properties of graph simulation, which help us design distributed algorithms.
We consider pattern graph Q(Vq, Eq) and data graph G(V, E).
We  rst show the impacts of connected components (CCs) on the evaluation of graph simulation.
Proposition 2: Let pattern Q consist of h CCs Q1, .
.
.
, Qh.
For any data graph G, if Mi is the maximum match in G for Qi, then
 i=1 Mi is the maximum match in G for Q.
(cid:2)h are always connected in the sequel.
(cid:2)h Proposition 3: Let data graph G consist of h CCs G1, .
.
., Gh.
For any pattern Q, if Mi is the maximum match in Gi for Q, then i=1 Mi is the maximum match in G for Q.
2 Proposition 3 implies that when a CC Gi (i   [1, h]) is only located in a single fragment, we can simply compute the maximum match Mi in Gi for Q locally.
However, this strategy does not work when a CC is across multiple fragments at di erent machines.
We need a better solution.
To do this, we  rst introduce the following notions.
Consider a binary relation R   Vq   V .
We use R(G) to denote the subgraph H(Vs, Es) of G, in which (1) v   Vs i  there exists u   Vq with (u, v)   R, and (2) (v, v(cid:2))   Es i  (i) (v, v (cid:2))   E and (ii) there exist u, u(cid:2)   Vq with (u, v)   R, (u(cid:2), v(cid:2))   R and (u, u(cid:2))   Eq.
We also use R(Q) to denote the subgraph Q[Vq,s] of Q in which u   Vq,s i  there exists v   V with (u, v)   R. Intuitively, R(Q) and R(G) are the subgraphs of Q and G, respectively, that play a role in R.
Theorem 4: Consider any binary relation R   Vq   V on pattern graph Q(Vq, Eq) and data graph G(V, E) that contains the maximum match M in G for Q.
If Mi is the maximum match in R(G)i for Q, then i=1 Mi is exactly M , where R(G) consists of h CCs R(G)1, .
.
.
, R(G)h.
(cid:2)h By Theorem 4, we can utilize subgraph R(G), instead of the entire G, to compute the maximum match M if R is guaranteed to contain M .
Note that even if G is connected, R(G) might be highly disconnected, by removing useless nodes and edges from G. This enhances the possibility of treating each CC separately for evaluating graph simulation.
Remark.
Note that  nding all pairwise disconnected components is linear-time equivalent to  nding strongly connected components, which is in linear time [9].
We then study the impacts of data locality on graph simulation.
For distributed algorithms, one way to maximize parallelization is to explore  what can be computed locally  [22].
However, as observed in [21], graph algorithms often exhibit poor locality and hence, may incur prohibitive overhead on network tra c.
This is indeed rather challenging for graph simulation, illustrated by an example below.
Example 4: Consider pattern Q1 and data graph G1 of Fig. 1.
Let Gs be the CC of G1 containing node PM2.
Then to decide whether Q1   Gs, we have to ship all subgraphs of Gs to a single site to reassemble Gs.
Indeed, (1) the match graph of Q1 and Gs is the entire Gs; and (2) removing any node or edge from Gs makes Q1 (cid:5)  Gs.
This tells us that graph simulation has poor data locality.
Theorem 5: For any binary relation R   Vq  V on pattern graph Q(Vq, Eq) and data graph G(V, E) that contains the maximum match M in G for Q, (1) match (u, v)   R is in M i  it is in the maximum match in subgraph desc(R[G], v) for subgraph desc(R[Q], u); and (2) if there exists a cycle in desc(R[Q], u), there must exist a cycle in desc(R[G], v) as well.
By Theorem 5, whether node v in G can be mapped to node u in Q can be reduced to the sub-problem of checking whether it belongs to the maximum match in desc(R[G], v) for desc(R[Q], u), in which desc(R[G], v) and desc(R[Q], u) are connected subgraphs of G and Q, respectively.
Moreover, a cycle in Q must match a cycle in G.
Indeed, the poor data locality of simulation is caused by the cycles in Q.
Data locality.
We next formally de ne data locality that may avoid unnecessary data shipment when evaluating pattern query Q on data graph G, via graph simulation.
We say that a node v in data graph G can be determined locally if checking whether v matches any node u in Q involves only the nodes v (cid:2) in G that have distance dist(v, v(cid:2)) bounded by a constant factor determined by Q only.
Similarly, we say that data graph G can be determined locally if all nodes in G can be determined locally.
Theorem 6: Checking whether node v in data graph G matches node u in pattern graph Q can be determined locally i  subgraph desc(Q, u) is a dag.
Example 5: Consider again pattern graph Q1 and data graph G1 of Fig. 1.
Observe that the cycle SD/ST/SD in Q1 matches the cycle SD1/ST1/ .
.
.
/SDh/STh in G1.
This makes G1 impossible to be determined locally since h can be arbitrarily large and cannot be bounded by Q1.
On the contrary, nodes BA and UD in G1 can be determined locally since the involved nodes in G1 have a distance bounded by the longest shortest distance in Q1.
Now let us consider a fragmented graph F = (F1, .
.
.
, Fk) of data graph G, in which each fragment Fi = (G[Vi], Bi) (i   [1, k]) is placed at a separate machine Si.
Corollary 7: A fragment Fi can be determined locally if all its boundary nodes can be determined locally.
Summary.
(1) We can treat each connect component in a data graph separately when evaluating graph simulation.
(2) Theorem 6 tells us how to check whether a node in data graph G can be determined locally or not.
(3) Corollary 7 tells us that the key for distributed pattern matching is to determine the matches of boundary nodes.
In this section, we investigate the complexities of a large class of distributed algorithms for graph simulation, which guides us the design of distributed algorithms.
We consider pattern graph Q(Vq, Eq) and fragmented graph F = (F1, .
.
., Fk) of data graphG (V, E), where each fragment Fi = (G[Vi], Bi) (i   [1, k]) is placed at a separate machine Si.
We  rst present the computational model for a large class of distributed algorithms for graph simulation.
We consider distributed algorithms in a pure message-passing (sharing nothing) model.
The model consists of a cluster of identical machines, in which one machine can directly send arbitrary number of messages to another one, and those machines co-work with each other by local computations and message-passing.
Note that this model is  ex-ible enough to express a broad class of algorithms [19], and is typically  t for the evaluation of graph algorithms [21].
The class of distributed algorithms that we consider work in the following fashion.
A user initiates a pattern query Q at an arbitrary machine, referred to as the coordinator, in the a cluster of k identical machines.
Then query Q is ter, after which those machines cooperate with each other, through message passing, to compute the maximum match M in data graph G for Q, via graph simulation.
Finally, the maximum match M is collected and presented to the user at the coordinator.
Here the fragmented graph of G is placed at k machines, one fragment at each machine, i.e., the number of fragments is exactly the number of machines.
Note that this should not be treated a restriction as an arbitrary number of fragments can be merged into a single fragment.
Complexity measures.
There are a variety of complexity measures on the performance of distributed algorithms [19], closely related to their computation models.
We consider three measures: (1) visit times, the maximum visiting times of a machine in the cluster which indicates the complexity of interactions, (2) makespan, the time cost measuring the completion time, from the time when the query is initiated to the time when the maximum match M is completely assembled, and (3) data shipment, the size of the total messages among distinct machines in the cluster during the computation.
Intuitively, one wants to minimize visit times, makespan and data shipment in the same time.
As will be seen shortly, these three measures, however, are controversial with each other, which advocates a well-balanced strategy when designing distributed algorithms.
Speci cations.
To help analyze the complexity of the class of distributed algorithms, we need to further specify the following: (1) the local information available at each machine, (2) the messages exchanged among machines, and (3) the local computations executed on single machines.
(1) The local information at each machine Si (i   [1, k]) consists of (a) pattern graph Q, (b) subgraph Gs,i of data graph G and (c) a marked binary relation Ri   Vq   V .
The pattern graph Q is broadcasted to all machines, and kept unchanged during the computation.
The subgraph Gs,i is initially the local fragment Fi = (G[Vi], Bi), and is updated once receiving messages containing subgraphs from other machines.
For each node pair (u, v)   Ri, it is marked as true, false or unknown, denoting that (u, v) is (a) in the maximum match M , (b) not in M , or (c) undetermined.
Relation Ri can be updated by either messages or local computations.
(2) When machine Si sends a message to another machine Sj , the message only consists of the local information (Q, Gs,i, Ri) available at machine Si.
Here we do not allow information coding [10] since it is orthogonal to the analysis of the data shipment of distributed algorithms.
(3) At each machine Si, local algorithms that given the local information (Q, Gs,i, Ri), compute an updated Ri by utilizing the de nition of graph simulation.
We require that the local algorithms execute only local computations without involving message-passing during the computation, and they run in time of a polynomial of |Q| and |Gs,i|.
Note that this is to help analyze the makespan problem [28] of distributed algorithms, and should not be treated as a restriction at all.
Remark.
Under the model and speci cations, we can express a large class of distributed algorithms for graph simulation, including all the ones that come into our mind.
We next present our  ndings on the class of distributed algorithms for evaluating graph simulation queries, which satisfy the computation model and speci cations given above.
Figure 3: Example for complexity analyses Our  rst set of  ndings are shown below.
Proposition 8: The optimal data shipment of the class of distributed algorithms is |G|   1, and the bound is tight.
2 Surprisingly, a simple distributed algorithm, referred to as naiveMatchds, can achieve the optimal data shipment.
Given pattern graph Q and fragmented data graph G, the algorithm simply collects all fragments of the data graph to the coordinator, and then it calls a standard centralized algorithm of graph simulation, e.g., HHK [16], to compute the maximum match in G for Q.
Note that the total data shipment in the process is bounded by |G| 1 since the subgraph at the coordinator contains at least one node.
Proposition 9: The optimal visit times of the class of distributed algorithms are 1, and the bound is tight.
Again a simple distributed algorithm, referred to as naiveMatchvt, achieves the optimal visit times.
Given pattern graph Q and fragmented data graph G, the algorithm visits k machines sequentially, where the last visited one is the coordinator.
In the process, each time when the algorithm visits a machine, it collects the local fragment, and sends all the fragments collected so far to the next machine.
Finally, all fragments of G reside at the coordinator, and it calls a centralized algorithm of graph simulation, e.g., HHK [16], to compute the maximum match in G for Q.
Note that each machine is visited one and only once in the process.
While there are e cient optimal algorithms for data shipment and visit times separately, the problem of  nding a minimum makespan is much harder, as shown below.
Proposition 10: The minimum makespan problem of the class of distributed algorithms is np-complete [28].
We next present our second set of  ndings that these three complexity measures are controversial with each other.
We illustrate this with the following example.
Example 6: Consider pattern graph Qo and the fragmented data graph Go in Fig. 3, in which each fragment is a CC without any boundary nodes.
Here nodes in Qo are only allowed to match nodes in Go with the same labels by ignoring their subscripts.
We also assume w.l.o.g.
that F1 = (G[V1],  ) lies at the coordinator.
One can verify the following: (1) For algorithm naiveMatchds, the (optimal) data shipment is |G|   1 = 8h, while the visit times are h.
(2) For algorithm naiveMatchvt, the (optimal) visit times are 1, while the data shipment is 4h(h + 1).
Observe that all the computational workload in these algorithms is mostly laid at a single machine, the coordinator.
Essentially, no computing power is used in parallel at all.
(3) Ideally, a distributed algorithm would work as follows.
(i) After receiving Q on each machine, a local algorithm is called to compute the local match in the fragment.
Fk) with Fi = (G[Vi], Bi) placed at machine Si (i   [1, k]).
Output: The maximum match M in G for Q.
Coordinator SQ.
case (1): upon receiving Mis and CCs from all k machines do

 3. send the assignment to all k participating machines; case (2): upon receiving Mb,is from all k machines do

 Machine Si.
case (3): upon receiving pattern graph Q do



 case (4): upon receiving the assignment of data shipment do
 case (5) upon receiving all the assigned CCs Ci do

 Figure 4: Distributed algorithm disHHK (ii) Here since each fragment is a CC, all local match results are simply sent to and assembled at the coordinator.
Recall Proposition 3 in Section 3.
In this way, the computation is maximally parallelized, and the makespan is minimized, accordingly.
However, one may verify that the data shipment is 30h, and the visit times are h. In contrast, the optimal data shipment and visit times are 8h and 1, respectively.
Summary.
We  nd that data shipment, visit times and makespan of a large class of distributed algorithms for graph simulation are controversial with each other.
As e ciency remains the dominant factor, there needs a well-balanced strategy between makespan and the other two measures.
In this section, we present the distributed algorithms that exploit the properties of graph simulation (Section 3) and the analyses of distributed algorithms (Section 4).
We consider pattern graph Q(Vq, Eq) and fragmented graph F = (F1, .
.
., Fk) of data graphG (V, E), where each fragment Fi = (G[Vi], Bi) (i   [1, k]) is placed at a separate machine Si.
The distributed algorithm, referred to as disHHK, follows the computation model and speci cations given in Section 4.
It is initiated at the coordinator SQ where the query Q is issued, and it consists of  ve stages (shown in Fig. 4).
Stage 1: Coordinator SQ simply broadcasts pattern query Q to all the k participating machines.
Stage 2: The main objective is to (partially) evaluate Q in each fragment at local machines in parallel (case (3)).
As a byproduct, the local evaluation  lters out useless nodes and edges in the fragment, and hence reduces the data shipment.
It also breaks a large fragment into smaller CCs, which further reduces the sizes of CCs across di erent machines.
Stage 3: The objective is to ship those CCs across di erent machines to single machines (case (1) and case (4)).
This involves two important, but controversial, issues: minimizing data shipment and makespan.
We provide a solution that (a) both minimizes the makespan with performance guarantees and (b) minimizes the data shipment with heuristics in the same time.
Stage 4: The objective is to compute the maximum matches in those CCs originally across di erent machines in parallel, by making use of those partial matches on these components computed locally at Stage 1 (case (5)).
Stage 5: Finally, the match results on all machines are sent to and assembled at the coordinator (case (5) and case (2)).
Correctness.
The correctness of disHHK can be easily ver-i ed by the analyses of graph simulation in Section 3.
Proposition 11: Given any pattern graph Q and fragmented graph F of data graph G, algorithm disHHK computes the maximum match in G for Q.
Performance.
The algorithm guarantees the following.
(1) The total computation cost is comparable to the one of the best-known centralized algorithm [16].
And it invokes four rounds of message-passing and local evaluation only.
(2) The total data shipment is at most |G| + 4|B| + |Q||G|+ (k 1)|Q|, where |B| is the total number of boundary nodes.
(3) Each machine except coordinator SQ is visited at most g + 2 times, where g is the maximum number of machines at which a CC resides at the end of Stage 2.
Coordinator SQ is visited with 2(k   1) extra times since it needs to schedule the data shipment and assemble the  nal result.
Remark.
(1) We have decided to sacri ce the visit times and data shipment for the bene ts of the makespan, a decision based on the analyses of Section 4.
(2) As one may notice, most stages are run in parallel except for case (1) at Stage 3.
As will be seen soon, its computation cost is really low, and would not cause a bottleneck.
In what follows, we describe each stage in more detail.
As shown by the analyses of Section 3, special care needs to be paid on boundary nodes.
To do this, we introduce a notion ofpartial match relation .
We consider a fragment Fi = (G[Vi], Bi) at machine Si (i   [1, k]).
Partial match.
A binary relation R   Vq   Vi is said to be a partial match if (1) for each (u, v)   R, u and v have the same label; and (2) for each edge (u, u(cid:2)) in Eq, (a) there exists a node v(cid:2)   Bv in Bi having the same label as u(cid:2) if v is a boundary node, or (b) there exists an edge (v, v(cid:2)) in G[Vi] such that (u(cid:2), v(cid:2))   R, otherwise.
The di erence between a partial match and a match given in Section 2 is the latter deals with boundary nodes.
When there are no boundary nodes involved, they are equivalent.
Note that (u, v) in a partial match might not appear in the maximum match in G for Q since the matches on boundary nodes are checked partially only with their directed neighbors.
We illustrate this with an example below.
Example 7: Consider pattern graph Q1 and data graph G1 in Example 1.
Pair (SA, SA1) is in the maximum partial match P M1 in fragment F1 for Q.
However, it does not belong to the maximum match M in G for Q.
Algorithm localHHK.
To compute the partial match, we propose algorithm localHHK, a revision of algorithm HHK that further deals with boundary nodes.
Given pattern graph Q and fragment Fi = (G[Vi], Bi) (i   [1, k]), it computes the maximum partial match in the fragment for Q.
Due to space limitations, its detail is omitted here.
as Proposition 1 and algorithm HHK in Section 2.2.
Corollary 12: For any pattern graph Q and fragment Fi, (1) there is a unique maximum partial match; and (2) algorithm localHHK computes the maximum partial match.
2 We next illustrate with an example how local evaluation also  lters out useless nodes and edges in data graphs, which reduces data shipment and computations.
Example 8: Consider again pattern graph Q1 and fragments F1 and F2 in data graph G1 in Example 1.
Recall that the maximum partial matches P M1 = {(SA, SA1)} and P M2 =  .
Hence, instead of the connected component consisting of fragments F1 and F2, only node SA1 in F1 will be considered in the following stages.
Thus a plenty of unnecessary nodes and edges are  ltered out at this stage.
After the local evaluation of partial match is done, we have a partial match P Mi on each machine Si (i   [1, k]).
Let P M be i=1 P Mi, and M be the maximum match in G for Q.
It is easy to verify that M   P M .
(cid:2)k Now let us consider subgraph P M (G) consisting of nodes and edges of G that play a role in P M , as de ned in Section 3.1.
Theorem 4 tells us that those matches in P M involved with those CCs residing at single machines must belong to the maximum match M .
Due to the poor data quality of graph simulation, those CCs of P M residing at multiple machines need to be gathered into single machines.
A challenging task here is how to schedule the data shipment such that both the data shipment and the makespan are minimized.
Note that the visit times have been  xed in the algorithm, and hence are not involved.
To do this, we introduce the following problem.
The scheduling problem.
Consider subgraph P Mi(G) at machine Si.
Each CC of P Mi(G) is identi ed by its boundary nodes, its size (e.g., the number of nodes and edges) and its location Si.
These information of all the CCs involved with boundary nodes are sent to the coordinator, at which those CCs of P M (G) across di erent machines are merged.
Hence, we have a set {C1, .
.
.
, Cl} of CCs of P M (G).
For each j   [1, l], Cj is associated with k + 1 costs: (1) for each i   [1, k], the data shipment Cj.di if Cj is shipped to Si; and (2) the computation cost Cj.c.
Recall that the computation cost of Cj is a polynomial of |Q| and |Cj|.
Formally, the scheduling problem is de ned as follows.
Given l CCs C1, .
.
.
, Cl, and an integer k,  nd an assignment of the CC to k identical machines, so that both the makespan and the total data shipment are minimized.
Approximation hardness.
To minimize the makespan, the key is to distribute those connected components evenly on those k machines.
As the minimum makespan problem is np-complete (Proposition 10), we focus on approximate solutions here.
We  rst look for chances that minimize both the makespan and the data shipment.
We say that the scheduling problem is approximatable within ( ,  ) if there exists a ptime algorithm such that given any instance of the problem, the algorithm produces a scheduling solution such that the data shipment is bounded by   times of the optimal data shipment and the makespan is bounded by   times of the optimal makespan.
Input: Connected components C1, .
.
.
, Cl and an integer k.
Output: An assignment of C1, .
.
.
, Cl to k identical machines.
if the total load of Ci and its optimal machine   avgc then Assign Ci to its optimal machine; Figure 5: Scheduling algorithm dSchedule The result is, however, negative due to the controversial nature of these two measures as shown in Section 4.2.
Theorem 13: The scheduling problem is not approximable within ( , max(k   1, 2)) for any   > 1.
Approximation algorithm.
This motivates us to look for solutions that have performance guarantees for the makespan only.
We propose an approximation algorithm dSchedule that has a heuristic for minimizing the data shipment, and a performance guarantee for the makespan.
Theorem 14: Algorithm dSchedule produces an assignment of the scheduling problem such that the makespan is within a factor (2   1/k) of the optimal one.
We now present the details of dSchedule, shown in Fig. 5.
Given connected components C1, .
.
.
, Cl and an integer k as inputs, the algorithm  nds an assignment for these CCs.
It  rst computes the average computation avgc cost of all CCs (line 1).
Given a CC Cj, the optimal machine Sjo to which Cj is assigned is the one on which the largest part of Cj resides, i.e., Cj.djo is the largest among {Cj .1, .
.
.
, Cj.k}.
If the total load of Ci and its optimal assigned machine together is equal or less than the average computation cost avgc, then Ci is assigned to its optimal machine (lines 2-4).
For the rest CCs Cj, schedule Cj to the machine that has the least amount of load (lines 5-6).
Remark.
(1) A heuristic is used to minimize the data shipment, by shipping CCs to their corresponding optimal machines w.r.t.
the data shipment (lines 1-4).
(2) A greedy approach (lines 5-6) is adopted to guarantee the performance of the makesapn, along the same lines as the one for the standard makespan problem [28].
(3) The algorithm runs in O(kl), and is very e cient.
Hence, its evaluation could not cause a bottleneck.
Example 9: Consider again pattern graph Q1 and data graph G1 in Example 1.
Algorithm dSchedule  nds an assignment for the two connected components C1 consisting of a single node SA1, and C2 consisting of the entire fragments F3, F4 and F5.
In this case, (1) C1 is simply assigned to S1, and (2) C2 is assigned to S5.
That is, these CCs are assigned to the machines with optimal data shipment.
Similar to algorithm localHHK, algorithm re neHHK is also a revision of algorithm HHK.
The key di erences between re neHHK and HHK lie in that (1) partial matches evaluated at stage 1 are treated as initial candidate matches, and (2) we process the matches of boundary nodes  rst, in which way the total computation cost of re neHHK and localHHK is comparative to the one of HHK.
We next present optimization techniques for algorithm disHHK, by means of data locality and query minimization.
For each partial match P Mi (i   [1, k]) in fragment Fi for Q, we further determine whether those matches in P Mi with boundary nodes, computed by localHHK, belong to the maximum match M in the entire data graph G for Q.
It is based on an application of Theorems 5 and 6 in Section 3.
This reduces both computations and data shipments.
Consider the matches (u, v) for a boundary node v in a partial match P Mi in fragment Fi = (G[Vi], Bi) for Q.
To determine whether (u, v) belongs to the maximum match M in G for Q, it su ces to determine whether for each child u(cid:2) of u in Q, there is a child v (cid:2) of v such that (u(cid:2), v(cid:2)) is in M .
For each child node j : v(cid:2) in Bv of Bi, if v(cid:2) matches a child u(cid:2) of u in P Mi, match (v(cid:2), u(cid:2)) is further checked by lazy evaluation at machine Sj as follows.
Let Cv(cid:2) be the connected component of P Mj such that v(cid:2) is in Cj , and let P Mi,v(cid:2) be the set of matches in P Mj that involve nodes in Cj .
We have two cases to consider: Case 1: when there are no boundary nodes in Gj .
For this case, we simply check whether node u(cid:2) belongs to subgraph P Mi,v(cid:2) (Q).
If the answer is  yes , then match (v(cid:2), u(cid:2)) is a true match, i.e., (v(cid:2), u(cid:2)) belongs to the maximum match M in G for Q.
Otherwise, it is a false match.
The correctness of this approach is guaranteed by Theorem 5.
Case 2: when there are boundary nodes in Cj, but subgraph desc(Q, u(cid:2)) of pattern graph Q is a dag.
For this case, we need to check whether all nodes in desc(Q, u(cid:2)), including u(cid:2) itself, matches no boundary nodes in Cj.
If the answer is  yes , then match (v(cid:2), u(cid:2)) is a true match, i.e., (v(cid:2), u(cid:2)) belongs to the maximum match M in G for Q.
Otherwise, it is an unknown match.
The correctness of this approach is guaranteed by Theorems 5 and 6.
We next illustrate the bene ts of this optimization technique with the following example.
Example 10: Consider pattern graph Q1 and data graph G1 in Fig. 1, and the partial match results in Example 7.
One can verify that (1) boundary nodes SA1 and SA2 can be determined by this optimization technique, while nothing can be done for boundary node PM2.
(1) For node SA1, its only child SD1 is located in fragment F2.
The partial match P M2 is empty.
Hence, a false match decision is sent back to machine S1, and this further helps determine that (SA, SA1) is a false match.
(2) For node SA2, its only child SD1 is located in fragment F5.
The subgraph P M5(G) contains no boundary nodes, and SD belongs to P M5(Q).
Hence, a true match decision is sent back to machine S4, and this further helps determine that (SA, SA2) is a true match.
After these are done, fragment F3 is the only part of G that needs to be further evaluated.
To check the matches in F3, we simply ship fragment F4 to machine S3, instead of shipping F3 and F4 to machine S5 as shown in Example 9.
That is, our approach could potentially save a large amount of unnecessary data shipments and computations.
Remark.
(1) This approach exploits the partial match results at other machines, and the checking is simple and e -cient.
(2) Only a small amount of data shipment is incurred.
The only involved data shipment is the triggers of the lazy evaluation and the decisions (true, false, or unknown).
Note that the evaluation is done at machine Sj, not Si.
This is why the data shipment incurred is small.
Minimizing pattern graphs.
Given pattern graph Q, we compute a minimized pattern graph Qm such that for any data graph G, G matches Q i  G matches Qm, via graph simulation.
The algorithm runs in quadratic time, and is taken from [6].
Note that Q is typically small.
We next illustrate the bene ts of minimizing pattern graphs with an example below.
Example 11: Consider pattern graph Qo in Fig.3.
The minimized equivalent pattern graph Qmo of Qo is a compact representation of Qo, by merging (1) nodes A1, A2, (2) nodes C1, C2, and (3) nodes D1, D2, D3, D4.
It only consists of four nodes and four edges.
Hence, Qmo is much smaller than Qo.
It is easy to see that both the data shipment and computation cost of evaluating Qmo on Go are much smaller that those of evaluating Qo on Go.
We have implemented a version of disHHK that supports these optimizations, referred to as disHHK+.
As will be seen in Section 6, disHHK+ signi cantly outperforms disHHK.
We next present an experimental study of our algorithms disHHK and disHHK+.
Using both real-life and synthetic data, we conducted four sets of tests to evaluate: (1) the makespan, (2) the data shipment, (3) the visit times of our algorithms, and (4) the e ectiveness of algorithm localHHK.
Experimental setting.
We use the following datasets.
Real-life data.
We used two real-life datasets1.
(a) Google records a Web graph with 875,713 nodes and 5,105,039 edges where nodes are URLs and an edge from URLs x to y indicates that there exists a hyperlink from x to y.
(b) Amazon contains a product co-purchasing network with 548,552 nodes and 1,788,725 edges in which nodes are products and an edge from products x to y represents that people buy y with high probability when they buy x.
Synthetic graph generator.
We adopted the graph-tool li-brary2 to produce both pattern and data graphs.
It is controlled by three parameters: the number n of nodes, the number n  of edges, and the number l of node labels.
Given n,   and l, the generator produces a graph with n nodes, n  edges, and the nodes are labeled from a set of l labels.
Algorithms.
We implemented the following algorithms, all in Python: (1) algorithms disHHK and disHHK+, and (2) optimal algorithms naiveMatchds and naiveMatchvt (Section 4).
The experiments were run on a cluster of 16 machines, all with 2 Intel Xeon E5620 CPUs and 64GB memory, that are connected by kilomega network.
Each test was repeated over 5 times and the average is reported here.
Experimental results.
In all the experiments, we  xed l = 200, k = 16, and set   ( q) = 1.20 by default.
All dataset-s are partitioned with a hashing function hash(ID) mod k, and distributed over all participating machines.
This partition approach has been commonly used in large-scale data process systems, such as MapReduce [11] and Pregel [21].
Exp-1: Makespan.
In the  rst set of tests, we evaluated the performance of disHHK, disHHK+, naiveMatchds and naiveMatchvt.
We did not report naiveMatchvt here as it is always much slower than naiveMatchds.
1http://snap.stanford.edu/data/index.html 2http://projects.skewed.de/graph-tool/ ) s


 x ( e m i t d e s p a l





 naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic) naieMatchds(oogle) naieMatchds(Amaon) dis(oogle) dis(oogle) dis(Amaon) dis(Amaon) ) s


 x ( e m i t d e s p a l










 naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic) naieMatchds(oogle) naieMatchds(Amaon) dis(oogle) dis(oogle) dis(Amaon) dis(Amaon)



 (a) Vary |Vq| (b) Vary  q naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic) naieMatchds(oogle) naieMatchds(Amaon) dis(oogle) dis(oogle) dis(Amaon) dis(Amaon)



 )


 x ( t n e m p i h s a t a d f o # dis(ynthetic) dis(ynthetic) naieMatchds(ynthetic) dis(oogle) dis(oogle) naieMatchds(oogle) dis(Amaon) naieMatchds(Amaon) dis(Amaon) )


 x ( t n e m p i h s a t a d f o #







 )


 x ( t n e m p i h s a t a d f o #















 ) s


 x ( e m i t d e s p a l
 dis(ynthetic) dis(ynthetic) naieMatchds(oogle) dis(oogle) dis(oogle) naieMatchds(Amaon) dis(Amaon) dis(Amaon)









 (c) Vary |V |( d) naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic) naieMatchds(oogle) dis(oogle) dis(oogle) naieMatchds(Amaon) dis(Amaon) dis(Amaon) ) s


 x ( e m i t d e s p a l




 naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic)



 (d) Vary a naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic) )


 x ( t n e m p i h s a t a d f o # 37x103 32x103 27x103 22x103 17x103


















 s e m i t t i s i v l a t o t f o # dis(ynthetic) dis(ynthetic) dis(oogle) dis(oogle) dis(Amaon) dis(Amaon)


 (f) Vary |Vq| (h) Vary |V |( d)







 s e m i t t i s i v l a t o t f o # (g) Vary aq dis(ynthetic) dis(oogle) dis(Amaon) dis(ynthetic) dis(oogle) dis(Amaon)







 s e m i t t i s i v l a t o t f o # s e m i t t i s i v l a t o t f o # dis(ynthetic) dis(oogle) dis(Amaon) dis(ynthetic) dis(oogle) dis(Amaon)













 (i) Vary a dis(ynthetic) dis(ynthetic)














 ) s


 x ( e m i t d e s p a l



 )


 x ( t n e m p i h s a t a d

 dis(ynthetic) dis(ynthetic) naieMatchds(oogle) dis(oogle) dis(oogle) naieMatchds(Amaon) dis(Amaon) dis(Amaon)






 (e) Vary k naieMatchds(ynthetic) dis(ynthetic) dis(ynthetic) naieMatchds(oogle) dis(oogle) dis(oogle) naieMatchds(Amaon) dis(Amaon) dis(Amaon) f o #

 s e m i t t i s i v l a t o t f o #












 (j) Vary k dis(ynthetic) dis(oogle) dis(Amaon) dis(ynthetic) dis(oogle) dis(Amaon)







 (k) Vary |Vq| (l) Vary aq (m) Vary |V |( d) (n) Vary a (o) Vary k Figure 6: Evaluation on makespan, data shipment and visit times (1) To evaluate the impacts of pattern graphs Q, we  xed data graphs G, e.g., Google with 875,713 nodes, Amazon with 548,552 nodes and synthetic data with 108 nodes, while varying (a) the number |Vq| of nodes in Q from 3 to 15 and (b) the density  q of Q from 1.05 to 1.20, respectively.
The results are reported in Figures 6(a) and 6(b), respectively.
One can  nd the following.
(a) All these algorithms scale well with Vq and  q on large data graphs, except naiveMatchds.
While it took disHHK and disHHK+ less than
 es, and was much slower than disHHK and disHHK+.
(b) disHHK+ is faster than disHHK.
Indeed, the running time of disHHK+ is consistently about [3/4, 4/5] of the time taken by disHHK, a signi cant reduction.
(c) Finally, the elapsed time of all algorithms increases when |Vq| or  q increases.
(2) To evaluate the impacts of data graphs G, we  xed pattern graphs Q with |Vq| = 9, while varying the number |V | of nodes of G (Google from 5 104 to 5 105, Amazon from 103 to 104 and synthetic data from 107 to 108), and the density   of G on synthetic data from 1.05 to 1.20, respectively.
The results are reported in Figures 6(c) and 6(d), respectively, where  d is a constant such that it is 5   104, 103 and 107 for Google, Amazon and synthetic data, respectively.
One can  nd the following.
(a) All algorithms scale well on the large data graphs except naiveMatchds, which took over 300s even on the smallest synthetic graphs with 107 nodes.
Hence we did not report its running time on synthetic data (b) disHHK+ is consistently faster graphs in Figure 6(c).
than disHHK, e.g., it took disHHK 217s on synthetic data graphs with |V | = 108 while it was only 176s for disHHK+.
Indeed, the running time of disHHK+ is consistently about [3/4, 4/5] of the time taken by disHHK, the same as the case above when varying pattern graphs.
(c) Finally, the elapsed time of all algorithms increases when |V | or   increases.
(3) To evaluate the impacts of the number k of participating machines, we  xed both data graphs G (with the same setting for data graphs as (1)) and pattern graphs Q (with the setting for pattern graphs as (2)), while varying k from
 We  nd the following.
(a) The elapsed time of all algorithms decreases when k increases.
(b) The elapsed time of disHHK and disHHK+ decreases faster than the one of naiveMatchds when k increases.
The elapsed time of disHHK on synthetic data was reduced from 1081s with k = 2 to
 reduced from 1603s with k = 2 to 1521s with k = 16.
And (c) the running time of disHHK+ is consistently about [3/4,
 when varying pattern or data graphs.
Exp-2: Data shipments.
In the second set of tests, using the same setting as Exp-1, we evaluated the total data shipments of disHHK, disHHK+, naiveMatchds and naiveMatchvt.
We did not report naiveMatchvt here since it always triggered much more data shipments than naiveMatchds.
(1) We tested the impacts of Q using the same setting as Exp-1(1).
The results are reported in Figs.
6(f) and 6(g).
(a) The total data shipments of all algorithms are not sensitive to the size of pattern graphs, (b) although naiveMatchds achieves the theoretical optimal data shipment, disHHK and disHHK+ trigger similar amount of data shipments as naiveMatchds, and (c) they even trigger less data shipments than naiveMatchds on large and sparse data graphs, e.g., when |Vq| > 11 on synthetic data or  q   1.125 on Amazon.
(2) We tested the impacts of G using the same setting as Exp-1(2).
The results are reported in Figs.
6(h) and 6(i).
(a) It is obvious that the total data shipments of all algorithms increase while |V | or   increases.
(b) disHHK and disHHK+ shipped less data than naiveMatchds on large and sparse data graphs, e.g., when     1.125 on synthetic data.
And (c) disHHK+ always shipped less data than disHHK.
(3) We tested the impacts of k using the same setting as Exp-1(3).
The results are reported in Figure 6(j).
(a) The total data shipments of all algorithms increase when k increases.
This is obvious since there are more boundary nodes when k increases when  xing data graphs.
And (b) shipments as naiveMatchds.
Exp-3: Visit times.
In the third set of tests, using the same setting as Exp-1, we evaluated total visits times of all algorithms.
We did not report naiveMatchvt here as it is always equal to the number k of participating machines.
The impacts of Q, G and k are reported in Figs.
6(k) and
 (a) The total visit times of all algorithms decrease when |Vq| or  q increases.
This is because when the size of pattern graphs increases, there are less matches in data graphs.
(b) The visit times of all algorithms obviously increase when |V |,   or k increases.
(c) disHHK and disHHK+ have more visit times than naiveMatchds, and disHHK+ has [30%, 53%] more visit times than disHHK, as expected.
Indeed, this is a price that has to be paid in exchange for e ciency.
Exp-4: E ectiveness of localHHK.
Adopting the same setting as Exp-1, we also evaluated the e ectiveness of localHHK, measured by the number of boundary nodes that are  ltered out by localHHK.
The results are shown below:



 # of total boundary nodes # of  ltered boundary nodes Google Amazon Synthetic

 It shows that localHHK eliminates a large portion of boundary nodes for disHHK and disHHK+, e.g., it cuts off 44%, 54% and 33% boundary nodes on Google, Amazon and synthetic data, respectively.
This means that localHHK indeed plays a considerable role in our algorithms.
Summary.
From these experiments, we  nd the following.
(1) disHHK and disHHK+ are e cient and scale well on large and dense data graphs, and considerably outperform naiveMatchds (optimal data shipment alone) and naiveMatchvt (optimal visit times alone).
(2) Our optimization techniques are e ective, reducing the running time by
 ment and visit times for makespan.
However, disHHK and disHHK+ even ship less data than naiveMatchds when data graphs are large and sparse.
Recall that real-life graphs are often large and sparse.
disHHK and disHHK+ indeed have more visit times than naiveMatchvs, a price that has to be paid in exchange for improving e ciency and minimizing data shipments.
(4) localHHK e ectively  lters out [33%, 54%] unnecessary boundary nodes for disHHK and disHHK+.
We have proposed evaluation algorithms for graph simulation in a distributed setting.
To our knowledge, we are among the  rst to settle this problem.
We have also veri- ed, both analytically and experimentally, the e ectiveness of our algorithms and optimization techniques.
Several topics are targeted for future work.
First, we are to extend our algorithms to deal with skewed graph partitions.
Second, we are experimentally verifying our algorithms using MapReduce and Pregel platforms [11, 21]).
Finally, we are to explore indexing techniques and distributed incremental methods to speed up the computation, in response to the dynamic changes of real-life data graphs.
Acknowledgments.
Shuai is supported in part by NGFR
 mental Research Funds for the Universities, and the Young Faculty Program of MSRA.
Tianyu is a contact author.
