Humans are more e ective than computers for many tasks, such as identifying concepts in images, translating natural language, and evaluating the usefulness of products.
Thus, there has been a lot of recent interest in crowdsourcing, where humans perform tasks for pay or for fun [1, 20].
Crowdsourced applications often require substantial programming: A computer must post tasks for the humans, collect results, and cleanse and aggregate the answers provided by humans.
In many cases, a computer program is required to oversee the entire process.
For instance, say we have 1,000 photos of a restaurant, and we wish to  nd the one that people think is most appealing and best describes the restaurant.
(We wish to use the photo in an advertisement, for example.)
Since it is not practical to show one Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
human all 1,000 photos, we need to break up the problem.
For example, as a  rst step, we may show 100 people 10 photos each, and ask each one of them to select the best one in their set.
Then we can select the 100 winners, partition them into 10 new sets, and ask 10 humans to evaluate, and so on.
What we have described is a crowdsourcing algorithm, analogous to traditional algorithms, except that the base operations or comparisons (e.g., compare 10 photos) are done by humans as opposed to by the computer.
Note that we use the term program to refer to the computer code that runs the entire process (e.g.,  nding human workers, displaying photos to the workers, paying workers, and so on).
We reserve the term algorithm to refer to the logic that performs a speci c function of interest (e.g.,  nding the max).
In this paper we study an important crowdsourcing algorithm, one that  nds the best or maximum item in a set, just as illustrated in the previous example.
This max algorithm is a fundamental one, needed in many applications.
For instance, the max algorithm can be used to  nd the most relevant URL for a given user query.
(The results can subsequently be used as a gold standard for evaluating search engine results.)
As another example, a max algorithm can be used in an entity resolution application, e.g., to  nd the best Facebook pro le that matches a target person.
The main challenge in developing an e ective max algorithm is in handling user mistakes or variability.
Speci -cally, humans may give di erent answers in a comparison task (i.e., pick a di erent item as the best), either because they make a mistake and pick the wrong item, or because the comparison task is subjective and there is no true best item.
We can compensate by asking multiple humans to perform each task (comparison in our setting), but these extra tasks add cost and/or latency.
In this paper we address these quality-cost-latency tradeo s and explore di erent types of max algorithms, under di erent evaluation criteria.
There is a substantial body of work in the theory community that explores algorithms with  faulty  comparisons (e.g., [2, 3, 4, 7, 8, 9, 16, 21]), and that work is clearly applicable here.
However, those works use relatively simple models that often do not capture all of the crowdsourcing issues (such as the monetary cost required to execute a crowd-sourcing algorithm).
For example, these works assume that operators (in our case humans) compare only two items at a time, or have a  xed probability of making an error.
It is not obvious how to extend the proposed techniques to the common case in crowdsourcing where humans compare more than two items at a time and the error is related to how similar the items are.
ments algorithms like max in a real crowdsourcing system and considers all practical issues.
Their algorithm is highly specialized to a speci c crowdsourcing scenario and it is not obvious how to generalize it to handle other scenarios, e.g., di erent types of item comparisons or di erent types of human workers.
For our work, we have taken a middle-of-the-ground approach: We formulate a fairly detailed framework that captures, for instance, human tasks that involve sets of items of di erent sizes, and di erent human error models.
Within this framework, we are able to study, not just a single algorithm or approach, but a wide range of families of param-eterized max algorithms, and we are able to study how to tune these algorithms for best performance.
Instead of just asking the question  For one given error model (or for one particular crowdsourcing system), what algorithm is best,  we ask the more general question  For what type of workers and tasks does each possible family of max algorithm work best.  Instead of just asking the question  How does algorithm X perform when humans can compare two items,  we ask  What is the impact on cost and answer quality of the number of items a human is given to compare each time.  Instead of just showing that  Algorithm X works well in a given setting,  we show how to tune each family of max algorithms (e.g., how many repetitions of each tasks are needed at each stage of the process).
To answer these questions we really need a model that is relatively rich and detailed, but yet not as rigid as an actual implementation.
As we will see, for some of our studies we resort to simulations, since the models are not amenable to detailed analysis.
However, in some of our scenarios, we are able to develop analytical expressions that describe performance.
Since both our simulations and analysis share the same underlying framework, when we can obtain analytical expressions, we are also able to validate them with simulations.
This redundancy gives us greater con dence in both the simulations and the analysis.
Contributions   We develop a framework for reasoning about max algo-Our main contributions are: rithms (Section 2).
  A max algorithm often asks multiple workers to perform the same comparison (task), so it needs a rule to aggregate the responses.
We formalize the most popular aggregation rule and explore some of its key properties (Section 3.1).
  We present several families of parameterized max algorithms.
The parameters for each family include the number of items compared by each human task, and the number of repetitions for each comparison (Sections 3.2 and
 ecution, e.g., we may initially perform fewer repetitions and in later stages perform more.
  We study strategies for tuning a max algorithm, i.e., for  nding the parameter values that lead to lowest overall error, given appropriate constraints (Section 4).
  As part of our framework, we develop a number of models for worker error and user cost/payments (Sections 5.1 and
   We experimentally evaluate our algorithms and strategies under various error and cost models (Section 6).
Our problem s input is a set of items E = {e1, e2, .
.
.
, e|E|}.
There is a smaller than relation <, which forms a strict ordering for the items in E. That is, for any two items ei, ej   E (with i 6= j) exactly one of the following is true: either ei < ej or ej < ei.
(We assume that ei 6= ej for i 6= j.)
The item ei   E for which ej < ei,  j   {1, 2, .
.
.
, |E|} \ {i} is called the maximum item in E. Our goal is to recover the maximum item in E, given a set of constraints (our problem is formally de ned in Section 2.4).
(Note that the < relation is only a formalism used to quantify e ectiveness.
The max algorithms themselves do not use the < relation.)
To determine the maximum item, an algorithm uses a comparison operator termed Comp().
Operator Comp(S, r, R) asks r humans to compare the items in set S   E and combines the responses using aggregation rule R. (One simple rule R for example is to pick the item with the most votes, described in detail in Section 3.1.)
Each of the r workers selects the item from S that he believes to be the maximum and R provides the overall winner.
We use a probabilistic model to describe a worker.
In particular, our model gives a vector of probabilities ~p = [p1, p2, .
.
.
, p|S|], where p1 is the probability that the worker returns the maximum item, p2 is the probability he returns the second best, and so on.
For example, if ~p = [0.8, 0.1, 0.1], then a worker selects the maximum item in S with probability 0.8, and each of the non-maximum items with 0.1 probability.
We study other error models in Section 5.1.
In the crowdsourcing platforms we examine, human work is compensated.
We denote the compensation of any human performing a comparison of |S| items by Cost(|S|).
We consider various types of cost functions in Section 5.2.
The total cost of Comp(S, r, R) is the sum of its compensations, which is equal to r   Cost(|S|).
Crowdsourcing algorithms are executed in steps.
A batch of comparisons C1 is submitted during the  rst step to the marketplace and after some time the human answers are returned.
Then, depending on these answers, an appropriate set C2 of comparisons is selected for the second step.
For the selection of the Ci comparisons all human answers from previous steps (1, 2, .
.
.
, i   1) can be considered.
Since the operations in a step are likely to be executed concurrently, the number of steps an algorithm requires is a(n) indicator/proxy of the execution time of the algorithm.
The number of steps is an especially good indicator when the number of human workers is very large.
Thus, we want the execution time proxy (number of steps) to be kept to a minimum.
Our work focuses on parameterized families of max algorithms, although many more algorithms can be considered.
The input of a max algorithm is a set of items E and the output is an item from E that is believed to be the maximum in E. To obtain a max algorithm A from a parameterized family of max algorithms A, one needs to instantiate a number of parameters: A = A(parameters).
We examine in detail two intuitive families of max algorithm in Section 3.
The quantities we are interested in are: Time(A, E), the Execution Time: The number of steps required for a max algorithm A to complete (and return one item) for input E.
Cost(A, E), the (Monetary) Cost: The total amount of money required for A to complete: [r   Cost(|S|)].
PTime(A,E) i=1 PComp(S,r,R) Ci Quality(A, E), the Quality of the Result: The probability that max algorithm A returns the maximum item from input E.
Given a family of algorithms A, our goal is to  nd parameters that  optimize performance.  The optimization can be formulated in a variety of ways.
Here, we focus on maximizing the quality of the result, for a given cost budget B and a given time bound T .
The human models are an input to the problem, and thus, for example, Quality(A, E) and Cost(A, E) take into account how humans act and how they are compensated.
Problem 1 (Max Algorithm).
maximize A=A(parameters) Quality(A, E) subject to Cost(A, E)   B Time(A, E)   T (1) Other possible formulations of the problem are:   Given a desired quality of the result and a budget, optimize the execution time, or   Given a desired quality of the result and an execution time bound, optimize the (monetary) cost.
The strategies we propose in Section 4 can be adjusted to address these problems, but we do not consider them in our current work.
Although we de ne our optimization problem using the Quality(A, E) metric, we propose alternative metrics in Section 5.3.
These metrics describe how close the returned item is to the maximum item in E, while Quality(A, E) describes whether A returns the maximum item in E or not.
For our evaluations (Section 6) we use both Quality() and the additional metrics.
This section explores the two most natural families of pa-rameterized max algorithms that can be used to solve the max algorithm problem (Problem 1).
The parameterized families of max algorithms we propose operate in steps and their parameters are the following: ri: the number of human responses seeked at step i, and si: the size of the sets compared by Comp() at step i (one set can have fewer items).
We assume a human cannot compare more than m items, thus si   {2, 3, .
.
.
, m}.
We refer to the ri and si values used by one algorithm by {ri} and {si} respectively.
First, we present an intuitive human responses aggregation rule (Section 3.1) that is a key component of every max algorithm.
Then, we describe the bubble max algorithms (Section 3.2) and the tournament max algorithms (Section 3.3).
This section describes the most common human responses aggregation rule: the plurality rule (denoted by RP).
In our complete report [19] we present another popular rule (the majority rule) and compare it to the plurality rule.
When comparison Comp(S, r, RP) is performed, one of the items in S with the most votes is selected: If l items have received n responses each and there is no other item with more responses, then one of the l items (selected at random) is considered to be the maximum item in S. The l items are called winners.
To understand the impact of aggregation rules on Quality(), we de ne AggrQuality(s, r; ~p, R): Definition 1.
AggrQuality(s, r; ~p, R) is the probability that R returns the maximum of s items, assuming we collect r human responses, and each response follows a probabilistic distribution ~p.
To calculate AggrQuality(s, r; ~p, RP) (for R = RP), we assume S = {e1, e2, .
.
.
, es}, with |S| = s and es < es 1 < .
.
.
< e1.
Thus, if ~p = [p1, p2, .
.
.
, ps], then pi is the probability that a human response selected item ei as the maximum in S. We note that the number of received human responses for items e1, e2, .
.
.
, es follows a multinomial distribution with parameter ~p.
Thus, since r votes are provided, the probability that ki votes have been issued for item ei (for i = 1, 2, .
.
.
, s) is i .
Based on these probabilities, we can compute the overall quality of the aggregation rule as follows: k1! k2! ... ks!
  Qs i=1 pki r!
AggrQuality(s, r; ~p, RP) = s
 l=1
 l r
 n=1

    

 Pi   L ki+ln=r r!
(n!
)l Qj   L kj!
Y z L pkw w z Y pn w   L     , where L is the set of all sets L that are subsets of {1, 2, .
.
.
, s} with |L| = l and 1   L. Also,  L = {1, 2, .
.
.
, s} \ L. (The derivation can be found in our complete report [19]).
In the following, Comp(S, r) will be considered equivalent to Comp(S, r, RP).
In all known cases, AggrQuality() is non-decreasing on r (although there is no formal proof since 1785 [6], when the problem was  rst stated).
The family of Bubble max algorithms (denoted by AB) is named after the bubble-sort algorithm, because of its similarities to it.
Algorithm A = AB({ri}, {si}) is described in Algorithm 1.
In summary, A operates on E as follows.
If E only has one item, this item is returned by the max algorithm A. Otherwise, a comparison with s1 random items from E is performed and a winner w is declared.
The s1 random items are removed from E. If E is now empty, w is returned.
Otherwise, in every step i, si   1 items from E are chosen and unioned with {w} forming S  i, ri, R) is  executed  and a winner w is declared, while the items of S  i are removed from E. When E becomes empty, the last winner w is returned by the max algorithm.
i. Comp(S  To formally de ne the max algorithm problem (Problem 1) we introduce a few new variables in this section: xi (the size of the output of step i), si (the size of the smallest set that if E == {e} then return e



 w   Comp(S  i   2 ; while E 6=   do 1, r1) ; Si   random subset of E of size min(si   1, |E|) ; E   E \ Si ;
 w   Comp(S  i   i + 1 ; i   Si   {w} ; i, ri, R) ;












 return w is compared at step i), and xi (the number of items that are compared within a set of size si at step i).
For example, in step i, we can have an input of xi 1 = 12 items and si = 5.
In this case, we will have 2 sets of size si = 5 compared.
Thus, xi = 10 items are compared within sets of size si.
The remaining xi 1   xi = si = 2 items are included in a smaller set of size si to be compared.
Now we are able to express the following three quantities: x0 i=j i=1 j=2 sj  1 i=1 s1 x0 i=1 si   1.
(ri   Cost(si)).
 QTime(A,E)   QTime(A,E) Time(A, E): the smallest k for which |E| + k  Pk Cost(A, E): PTime(A,E) Quality(A, E): AggrQuality(si, ri; ~pi, R) + +PTime(A,E) AggrQuality(si, ri; ~pi, R).
For this family of max algorithms Quality(A, E) is the summation of the probabilities of the maximum item taking part in the ith comparison for the  rst time, multiplied by the probability that it is propagated to all next steps.
Our goal is to select the appropriate sequences {ri} and {si} in order to maximize Quality(), given constraints on Cost() and Time().
We formalize the optimization problem derived from Equation (1) for the bubble max algorithms in our complete report [19].
The Tournament max algorithms (AT) are named after tournament-like algorithms that declare best teams in sports.
Given the parameters described at the beginning of this section (sequences {ri} and {si}), we obtain a max algorithm A = AT({ri}, {si}).
A operates on some input E and returns some item from E, as described in Algorithm 2.
Algorithm 2: AT({ri}, {si}) operating on E






 i   1 ; Ei   E ; while |Ei| 6= 1 do partition Ei in non-overlapping sets Sj, with |Sj| = si (the last set can have fewer items); i   i + 1 ; Ei   Sj{C(Sj , ri, R)} ; return e   Ei In summary, A operates on E as follows.
Set Ei contains the items that are compared at step i (E1 = E).
At step i, Ei is partitioned in non-overlapping sets (Sj, for j = 1, 2, .
.
.
,   |Ei|  ) of size si (|Sj| = si) and the winners si of each comparison Comp(Sj , ri, R) form set Ei+1.
If Ei+1 has exactly one item, this item is the output of A. Otherwise, the process is repeated for one more step.
To formally de ne the max algorithm problem (Problem 1) we need to be able to express three quantities: Time(A, E): the minimum k for which xk = 1 (more details in our complete report [19]).
sj sj j=1 j=1 xj 1 (cid:0) xj ri  (cid:16)  xj 1 Cost(A, E): PTime(A,E) Quality(A, E): QTime(A,E)     Cost(sj) + Cost(sj)(cid:17).
AggrQuality(sj , rj; ~pj , R)+ AggrQuality(sj , rj; ~pj , R)(cid:1), which is the product of xj 1 the probabilities that the maximum item in each step is propagated to the next step.
We again aim to select appropriate sequences {ri} and {si} in order to maximize Quality(), given constraints on Cost() and Time().
We formalize the optimization problem derived from Equation (1) for the tournament max algorithms in our complete report [19].
This section describes strategies that optimize max algorithms with parameters {ri} and {si} that run in steps (like the ones we described in Section 3).
Our strategies are not guaranteed to be optimal, but they heuristically determine parameter choices ({ri} and {si}) that improve performance while satisfying the (budget and time) constraints.
We present the strategies by increasing complexity.
The  rst strategy we examine is the one most commonly used in practice today: The practitioner decides how big the sets of items are and how many human responses he seeks per set of items.
ConstantSequences emulates this behaviour and selects parameters {ri} and {si} among the constant sequences search space: For all i we have ri = r and si = s. This way, we only have two parameters (r and s) to determine.
ConstantSequences operates as follows.
It goes through all acceptable s values (from 2 to m), determines if the execution time constraint is satis ed,  nds the maximum possible r for each s that keeps the cost below the budget B, and computes Quality().
Finally, ConstantSequences returns the  optimal  probability of returning the maximum item and the r and s that give this  optimal  probability ( p,  r,  s).
Proposition 1.
If AggrQuality(s, r; ~p, R) is non-decreasing on r, ConstantSequences returns the optimal selection of r and s for constant sequences (for the bubble and tournament max algorithms).
The proof can be found in our complete report [19].
As mentioned in Section 3.1, AggrQuality() is assumed to be non-decreasing in the political philosophy literature for the plurality rule [14], and thus, we are con dent that Con-stantSequences returns the optimal r and s values.
RandomHillclimb builds on top of ConstantSequences and operates as follows.
It  rst determines the optimal r and s values using ConstantSequences; the s value does not change after this point for any step.
Then, it attempts to adjust the r values across the steps, such that Quality() increases.
A random source step c is selected and rc (repetitions at step c) is decreased by 1.
Thus, some additional comparisons can be applied in other steps.
We check each possible other step t and we attempt to add all the additional comparisons to this target step.
If by  moving  the repetitions from c to t we manage to improve the quality of the tournament we adjust the sequence {ri}.
We perform this greedily, i.e., we make the adjustment for the step t that maximizes the improvement in Quality() every time.
If at some point we fail to improve the performance of the max algorithm, we stop.
AllPairsHillclimb is an extension of RandomHillclimb with one important di erence: This strategy considers all possible steps as sources c (not just a random one).
We again greedily change the r s by keeping each time the source c and the target t that maximize the improvement in Quality(A, E).
AllSAllPairsHillclimb is a generalization of the All-PairsHillclimb.
The di erence is that all possible s s are considered (not only the one that ConstantSequences returns).
This optimization can only lead to improvements for Quality().
We observe the signi cance of these improvements in our experiments in Section 6.
Algorithm 3: VaryingS(x0, ~p, B, T , A) // global optimal values ( p, { ri}, { si})   (0.0, N U LL, N U LL) ; (p, {ri}, {si})   AllSAllPairsHillclimb(x0, ~p, B, T, A) ; u   1 ;  ru   r1 ;  su   s1 ; b1   budget consumed from the selection of  r1 and  s1 on an input of size x0 ; while xu > 1 do u   u + 1 ; (p, {ri}, {si})   AllSAllPairsHillclimb(xu 1, ~p, B   bu 1, T   (u  

  ru   r1 ;  su   s1 ; bu   budget consumed from the selection of  r1,  r2, .
.
.
,  ru and  s1,  s2, .
.
.
,  su on an input of size x0 ;













  p   Quality(A({ ri}, { si}), E) ; return ( p, { ri}, { si}) ; VaryingS is a generalization of AllSAllPairsHillclimb and it is the only strategy we examine that allows di erent s values in di erent steps.
VaryingS operates as seen in Algorithm 3.
The parameters for step 1 are the same as the parameters the AllSAllPairsHillclimb returns for step 1 If the parameters for steps 1, 2, .
.
.
, u   1 (lines 1 5).
have been decided, the parameters for step u are decided as follows: ru and su are set equal to the parameters for step 1 of AllSAllPairsHillclimb with the non-consumed budget and execution time and the appropriate number of input items (lines 8 12).
The process stops when the selection of {si} returns exactly one item in some step (line 7).
After all the selections have occurred, Quality() is computed and returned (lines 13 14).
Max algorithms can be used by various applications and each application gives rise to di erent human behaviours.
Also, each application may focus on di erent evaluation metrics.
To understand the quality/time/cost tradeo , we explore various concrete error and compensation worker models (still a fraction of what our framework supports) in Sections 5.1 and 5.2, and propose various evaluation metrics in Section 5.3 (extending the already de ned Quality(A, E) metric).
In our complete report [19], we also explain how our proposed models can be applied in other domains.
This section describes various models for human errors.
We present our models from the most detailed one to the simplest one.
As we have seen in Section 2.1, a set of items S is given to a human and the error model assigns probabilities to each (possible) response of a human.
We assume there are no malicious workers, and thus p1   1 |S| (i.e., the probability of returning the maximum item in S is at least as high as the probability of returning a random item from S).
Our complete report [19] describes how an appropriate model (and parameters) can be selected in practice.
Let the input set of items of each comparison be S = {e1, e2, .
.
.
e|S|}, with ei being the ith best item in E (or equivalently, e|S| < e|S| 1 < .
.
.
< e1).
As a reminder, a human response has probability pi of returning item ei.
The rank of an item is de ned as follows: Definition 2 (Item s Rank).
The rank of item ei   S ,denoted as rank(ei, S), is de ned as the number of items in S that are better than ei plus 1.
For example, rank(e1, S) = 0 + 1 = 1 and rank(ei, S) = i.
The proximity error model Proximity Error Model takes one parameter p   [ 1 |S| , 1] and assumes there is a distance function d( ,  ) that compares how di erent two items are.
Formally, d(ei, ej)   (0, 1) (since ei 6= ej for i 6= j).
When d(ei, ej) is close to 0, ei is similar to ej.
On the contrary, when d(ei, ej) is close to 1, ei and ej are very di erent.
A worker returns ei with probability: p1 = p pi = (1   p)   1 d(ei,e1)

 j=2[1 d(ej ,e1)] , i   {2, 3, .
.
.
, |S|}.
    The intuition is the following: Humans  nd it hard to distinguish similar items.
Thus, the closer one item is to the maximum item in S, the higher its probability of being selected by the human as the maximum.
Order-Based Error Model This model is similar to the previous model with one di erence: The distance function
 worker returns e1 with probability p1 = p and returns ei with probability pi = (1   p)   |S| | rank(ei,S) rank(e1,S)|


 j=2 [|S| | rank(ej ,S) rank(e1,S)|] for i = 2, 3, .
.
.
, |S|.
Probabilities pi depend on the order of item ei in this model.
Linear Error Model This model assumes that the human worker is able to determine the maximum item in S with a probability that depends on the number of items |S| to be examined.
Intuitively, the more items to examine, the harder the task becomes.
The probability that a worker selects the maximum item is p1 = 1   pe   se   (|S|   2), where pe, se are model parameters.
E ectively, p1 = 1   pe when |S| = 2 and it decreases by se for each additional item in S. When the worker fails to return the maximum item (with probability pe + se   (|S|   2), he returns a random (but not the maximum) item from S. Thus, each item in {e2, e3, .
.
.
, e|S|} has probability 1 pe se (|S| 2) to be selected.
Values pe and se take values in such a way that pe + se   (|S|   2)   [0, 1   1
 for all values of |S| used in practice.
Constant Error Model This model assumes that the human worker is able to determine the maximum item from S with probability p   [ 1 |S| , 1], for any S (with |S|   2).
In the event that the worker is not able to determine the maximum item from S (which occurs with probability 1 p), he returns one random non-maximum item from S. Thus, each item in {e2, e3, .
.
.
, e|S|} has probability 1 p |S| 1 to be selected.
This section describes the models we explore for human work cost.
The function Cost( ) we de ned in Section 2.1 takes as input the number of items that are compared at a time.
Constant Cost Model  xed price.
That is, Cost(|S|) = c for some constant c.
In this model, each task has a In this model, each task has a price Linear Cost Model that depends on |S|.
If the human worker works on two items, he is compensated with c, if he works on three items, he is compensated with c + sc, if he works on four items, he is compensated with c + 2   sc, and so on.
Formally, Cost(|S|) = c + sc   (|S|   2), for some constants c and sc.
We de ned Quality(A, E) as the main optimization criterion, but there are other metrics of interest.
Quality(A, E) describes how often A returns the maximum item in E.
Other metrics evaluate how often A returns one of the top-k items in E, or how close the returned items are to the maximum item.
This section describes some standard metrics we have used for our evaluations.
We note that we only optimize for the Quality(A, E) metric, but evaluate max algorithms using the metrics we de ne in this section also.
We de ne the metrics of interest in the context of simulations.
We simulate the application A(E) (max algorithm A operates on input set E) E times and then obtain the following values: Top-k is the fraction of the E simulations for which A(E) belonged in the top-k items of E (i.e., rank(A(E), E)   k).
i=1 ranki
 Mean Reciprocal Rank (MRR) is 1 , where ranki is the rank of the returned item in the ith simulation.
Both metrics yield values in [0, 1]: Values close to 0 indicate that A returns items that are not close to the maximum item, while values close to 1 indicate that A returns items that are close to the maximum.
Also, both metrics have a probabilistic meaning: For the top-k metric for example, if we have a signi cantly large number of simulations, we converge to the probability that A returns an item in the top-k items.
Finally, we experimentally observe exceptionally low variance values for our metrics (lower than 10 3 in all cases), and only report the expected metric values in Section 6.
This section summarizes various experiments we have performed, organized by results.
We  rst explore which max algorithms and which strategies perform best.
Surprisingly, we see that the same algorithm and strategy perform best under all combinations of human models and parameters we have tested (Section 6.1).
Nevertheless, we explore in more detail:   the e ect of di erent parameters (like B, T , m, p, pe, se, c, and sc) on the performance of the max algorithms (Section 6.1),   characterizations of the  optimal  solutions (Section 6.2),   the e ectiveness of our proposed optimization strategies, by comparing our heuristics to exhaustive search (Section 6.3), and   the sensitivity of our algorithms to inaccurate estimations of the crowdsourcing system error model parameters (Section 6.4).
Additionally, in our complete report [19]:   We show that even though obtaining the top-1 item is hard,  nding one of the top-2 or top-3 items is signi -cantly easier in practice,   We show that performing more repetitions in a single tournament is more e ective than performing multiple tournaments (same input) with fewer repetitions each (and at the end combining results), and   We show that relaxing the time bound constraint can improve the quality of the result by 50% in some cases.
Most of our experimental results were analytically calculated (and con rmed with simulations).
In a few cases, we resorted to simulations only, since analysis was not possible.
When we use simulations, we point out why analysis was hard and how our simulations were run.
This section compares the e ect of various strategies to the quality of the result Quality() for the two parameterized max algorithms we have de ned in Section 3 (bubble and tournament).
We experimented with various human models and parameters: budget (B), time bound (T ), error model parameters (p, pe, se), and cost model parameters (c, sc).
All parameters tested gave qualitatively similar results.
For succinctness, we present one selection of human models and parameters.
Figure 1 shows Quality() on the y-axis and the budget B on the x-axis, for each of the  ve strategies we described in Section 4 (ConstantSequences, RandomHillclimb, All-PairsHillclimb, AllSAllPairsHillclimb, and VaryingS).
) ( y t i l a u



 Bubble ConstantSequences RandomHillclimb AllPairsHillclimb AllSAllPairsHillclimb VaryingS



 Budget B



 Budget B

 u a l i t y ( )

 Figure 1: The e ect of the strategies (middle) on the tournament (left) and bubble (right) max algorithms.
The graph on the left refers to the tournament algorithm and the graph at the right to the bubble algorithm; the legend in the middle is common for both graphs.
To generate Figure 1 we used |E| = 100, the order-based error model with parameter p = 0.78 (one of the many values we tried in the range [0.6, 1]), the linear cost model with parameters c = 1 and sc = 0.1, and allowed sets S participating in some Comp() to have size at most m = 10.
Finally, we set the time bound T to in nity and varied the budget B between 20 and 100 with a step of 20.
We observe in Figure 1 that there are  discontinuities  (jumps) for Quality() as budget B increases.
These discontinuities exist because: (1) B needs to be increased by a particular amount to increase ri for some step i (e.g., a tournament algorithm performs r1     |E|   comparisons at step 1, thus to increase r1 by 1 requires an additional budget for   |E|   new comparisons), and (2) the ri increase does not s1 have the same Quality() impact all times (e.g., increasing any ri from 1 to 2 has no impact to Quality() by the plurality rule properties, but increasing ri from 2 to 3 increases Quality()).
s1 From Figure 1, we obtain the following: Result 1: As expected, as the budget B increases, the quality of the result Quality() increases (for all strategies).
Result 1 states the intuitive fact that the more money one is willing to spend, the higher the quality of the result is.
The increase is almost linear for the range of budgets we examined.
Furthermore, we do not observe topping o  of gains for the values of budget B we present, but of course it takes place at higher values.
When comparing the  ve strategies with each other we obtain the following: Result 2: For all budgets examined and for both the bubble and the tournament max algorithms there is a clear ordering of the strategies from best to worst: VaryingS, AllSAllPairsHillclimb, AllPairsHillclimb, RandomHillclimb, and Con-stantSequences.
Thus, VaryingS is the strategy that should be used in all cases in practice.
Most applications today use strategies like ConstantSe-quences, i.e., ri = r and si = s for all steps i.
Result 2 suggests that the quality of the result improves when the number of repetitions and the set size are allowed to vary across steps.
Interestingly, comparing to ConstantSequences, Vary-ingS increases Quality() by  100% for the same budget in some cases (e.g., for the tournament algorithm for B = 40)!
Thus, applications can bene t by our strategies in two ways: either increase Quality() for a  xed budget, or produce a desired Quality() for a lower budget.
Comparing the two max algorithms (bubble and tournament) we obtain the following: Result 3: Tournament max algorithms perform better than bubble max algorithms for the same budget.
The performance di erence between the tournament and the bubble max algorithms can be explained in the following way.
The maximum item has to  survive  (not erroneously be dropped by a comparison Comp()) by all the comparisons it participates in for it to be the output of the max algorithm.
In bubble max algorithms the worst case scenario is that the maximum item survives a very long chain of comparisons ( (|E|)), while in the tournament max algorithms the number of comparisons is small in all cases ( (log |E|)).
Finally, it is important to note that in no combination of human models and parameters that we tested, the bubble max algorithms performed better than the tournament max algorithms.
Now, observing each of the families of max algorithms separately, we observe that: Result 4: For very small or very large budgets B, the di erences between the  ve strategies are not signi cant.
For middle values of B though we observe large di erences between the strategies.
We generally observe large di erences between the strategies for B   |E| 2 shifted appropriately as the cost parameters change.
Thus, it is  indi erent  which strategy is used for very small or very large values of B, but for the middle values VaryingS should be used when possible.
The di erences between the Quality() values the strategies achieve, can be explained if we think of each strategy as a search in the parameter space of sequences {ri} and {si}.
For small values of B the di erences between the resulting Quality() values are not signi cant, because there are not many choices of {ri} and {si} permitted by budget B.
As B increases, there are more parameter choices and the differences between the strategies  performance increase: Some strategies are able to determine good parameter choices and some are not.
For very high values of B the di erences between the performance of the algorithms decreases again.
There are many {ri} and {si} choices and all strategies perform very well: If there is enough budget, one can increase ri s signi cantly and observe high Quality() for any strategy.
Browsing through the resulting {ri} and {si} from the various strategies for large B s indicates that the actual selected ri and si values may be signi cantly di erent, but the Quality() values are very close.
We also note that the di erences between strategies are not signi cant when p1 is close to 1.
This section explores how the ri values are distributed across the steps, when all steps have the same si.
We experiment with the AllPairsHillclimb strategy and the tournament max algorithm.
i r





 i



 Figure 2: Exploring how AllPairsHillclimb algorithm distributes repetitions (ri s) across steps.
Figure 2 shows the ri values on the y-axis achieved for each step i (x-axis).
For simplicity, we used si = 10 for each step i and observed the ri s.
We used |E| = 10,000 (thus, there are exactly 4 steps for the tournament max algorithm), in nite time bound (T =  ), the constant error model with p = 0.8, and the constant human cost model with c = 1.
We used various budgets (from B = 1,500 up to B = 10,000).
From Figure 2 we observe that: Result 5: Balanced numbers of repetitions across steps are bene cial when the budget allows it.
Thus, when ri s are unbalanced, it is because the budget does not allow more repetitions to be allocated in some steps.
We observe in Figure 2 that when the budget is low (B = 1,500) all 4 steps are assigned one repetition each (ri = 1 for all i s).
As budget increases (B = 4,000 or B = 5,500) mainly the last steps are bene ted from more repetitions.
(The number of comparisons performed in the initial steps is very high, and thus it costs a lot to allocate comparisons in them.)
As we increase the budget even more (B = 10,000), we observe that the repetitions across steps are again balanced (approximately 10 repetitions per step for all steps).
Interestingly, there no are clear patterns for the {si} sequence, and we explain why in our full report [19].
This section evaluates how e ective our strategies are with respect to an exhaustive search of the parameter space.
In order to evaluate our strategies, we perform an exhaustive search over the space of {ri} and {si}.
The exhaustive search is very expensive, and thus we focus on a small instance of the problem: We consider |E| = 10, m = 4, and set a maximum value for ri to 5.
We use the linear error model (with pe = 0.2 and se = 0.04), and the linear cost model (with c = 1 and sc = 0.2).
We set the budget B to 10, and the time bound T to  .
For the exhaustive search, we  nd all {ri} and {si} that lead to Cost(A, E) below B and for each A = AT({ri}, {si}) we simulate the value (averaging over 500,000 simulation runs) for each of the following metrics: MRR and top-k for k = 1, 2, and 3.
(As mentioned earlier, Quality() and top-1 are the same: They both measure the probability that an algorithm returns the maximum item.)
We only evaluate the VaryingS strategy for the tournament max algorithms, which is the best strategy (Result 2).
e u l a v c i r t e m



 top-1 top-2 top-3 MRR metric Figure 3: Comparing the VaryingS strategy for the tournament max algorithms with an exhaustive search over the parameter space.
Figure 3 shows the distribution of the metric values using box/whiskers plots and evaluates the e ectiveness of Vary-ingS.
Each box/whiskers plot shows the range of metric values observed as we exhaustively examine all {ri} and {si} values (that meet the budget constraint).
The top bar represents the maximum value observed, the bottom bar is the lowest value.
The bottom of the box is the  rst quartile (minimum value above the 25% percent of the observed values), the top of the box is the third quartile (minimum value above 75% percent of the observed values), and the line in the middle of the box is the median value observed.
The x-axis presents the metrics we examined (top-k and MRR).
The y-axis presents the metric values.
For each metric, we present the achieved value by VaryingS using the   symbol.
By observing the range of metric values the exhaustive search achieves, we see that: Result 6: For all metrics, there is a wide range of values that are achieved by di erent parameter selections.
Thus, simple strategies are not likely to work well.
Observing the performance of the VaryingS strategy, we see that: Result 7: Even though VaryingS optimized over only one objective function (top-1 or Quality()), it produces very good results in all metrics we examined.
Thus, it is enough to only optimize over Quality() using VaryingS, to obtain high quality results.
Result 7 suggests that using VaryingS to tune algorithms is enough for any metric of interest.
Modeling humans is a very di cult task, and thus it is vital for an algorithm to be tolerant in erroneous estimations of the error model parameters.
This section explores how sensitive our algorithms are to the error model parameters.
For our study, we assume that a crowdsourcing marketplace has error model parameters ~P , but the algorithm A = AT({ri}, {si}) that is run was generated by strategy VaryingS with parameters ~P   6= ~P .
We have experimented with all the error models we have described in Section 5.1 obtaining similar results.
We now describe one of the many experiments we have performed.
For this experiment, we used |E| = 1,000 items with the linear cost model (c = 1 and sc = 0.2), our budget was B = 1,400, the time bound was T =  , and m was set to 10.
We used the linear error model with parameters pe = 0.15 VaryingS strategy, assuming that the crowdsourcing marketplace had parameters p  e = se = 0.02.
In Figure 4 we see the di erence  pe = p  e   pe (x-axis) and the Quality() achieved by the algorithm A (y-axis) produced with the corresponding  pe (i.e., the algorithm returned by VaryingS when p  e = pe +  pe = 0.15 +  pe).
e   (0, 0.3) and s  ) ( y t i l a u



  0.1

  pe Figure 4: Sensitivity of a tournament algorithm to the linear error model parameter pe.
We see that: Result 8: As expected,the highest value for Quality() is observed for  pe = 0.
Negative and positive values of  pe (i.e., p  e 6= pe) give slightly (but not signi cantly) smaller values for Quality().
Thus, small parameter errors do not signi cantly impact the performance of the algorithms.
Because of Result 8, the estimations of the actual parameters of the error models followed in practice do not need to be perfectly accurate.
Thus, the cost for estimating the model parameters of a marketplace can be kept to a minimum with small accuracy losses in the algorithm s application.
A lot of work has been done in the context of our problem.
In summary, comparing to previous work, our paper is di erent in at least three aspects: Error/cost models: The error and cost models we consider are more general than previously considered error models.
Notion of Steps: Crowdsourcing environments force algorithms to be executed in steps, making the design of algorithms harder.
Number of items compared: We allow the  exibility of comparing  any  number of items per operation.
All other techniques we are aware of only perform binary comparisons.
In the rest of this section we present work related to our paper.
Resilient algorithms Some related work to ours has extensively examined so-called resilient algorithms.
These algorithms assume that some items from the input are corrupted (e.g., because of memory faults).
The goal is to produce the correct result (for computing the maximum item in a set for example) for the uncorrupted items in the input.
One of the  rst papers in this area is [9], where resilient algorithms for sorting and the max algorithm problem are provided.
An overview paper on resilient algorithms is [13], which presents work done in resilient counting, resilient sorting, and the resilient max algorithm problem.
Along these lines [10, 11] attempt to understand for a given algorithm, how many corruptions can be tolerated for it to run correctly.
In our work we assume that no item from the input may be corrupted, but that humans (who are e ectively  comparators ) can make mistakes.
Also, only binary comparisons are considered in [9, 10, 11, 13]; we consider any comparison of size 2 or more.
Sorting/Max algorithms with errors Another line of work similar to ours involves sorting networks, in which some comparators can be faulty.
One of the  rst works in this area [21] proposes sorting algorithms that deal with two scenarios of faults: (1) up to k comparators are faulty and (2) each comparator has a probability   (  is small) of failing and yet, the result is retrieved correctly with probability greater than 1  (  is small).
In [4] sorting networks that return the correct result with high probability are constructed from any network that uses unreliable comparators (each comparator can be faulty with some probability smaller than

 multiplied by the depth of the original network.
In [16] the max algorithm problem is considered under two error models: (1) up to e comparisons are wrong, and (2) all  yes  answers are correct and up to e  no  answers are wrong.
The max (and the sorting) problem is also considered in [3] under a di erent error model: If the two items compared have very similar values (their absolute di erence is below a threshold), then a random one is returned; otherwise, the correct item is returned.
The max algorithm problem is solved using 2   n
 model used in [16] is given in [2] where up to a fraction of p of the questions are answered erroneously at any point.
In [2] it is proved that if p   1 2 , then the maximum item may not be retrieved; if p < 1 2 , then the maximum item can be (1 p)n (cid:17) binary comparisons (where n is retrieved with  (cid:16) the size of the input).
Even more elaborate error models are considered in [2]: For example, the number of wrong answers we have received may be at most p fraction of the number of questions only at the end of the process (and for example the  rst 5 questions may all be answered incorrectly).
In this model if p   1 n 1 , then the maximum item may not be found; if p < 1 n 1 , then the maximum item can be found in n   1 questions.
The complexity of four di erent problems is explored in [7,
 ing, (3) merging, and (4) selecting the kth best item.
The error model considered is the following: For any comparison of items there is a probability p that the correct result is returned and 1   p that the wrong result is returned.
Again, this line of work also does not have the notion of steps, nor the complexity of the error models we are exploring.
Also, the only type of comparisons that has been used is that of two items per comparison.
Crowdsourcing Marcus et al. [15] consider the problems of sorting and joining datasets in real crowdsourcing system, considering many practical issues.
Their work implements max algorithms (as instances of sorting algorithms) by splitting the input set of items E in non-overlapping sets of equal size and asking workers to sort these sets.
Our work on the contrary allows us to answer more general questions regarding di erent scenarios, such as di erent types of human workers and compensation schemes, di erent algorithms and the tournament algorithm in the setting described in [15].
Tournament algorithms have been used in crowdsourcing to determine the correct answer to a di cult task [18].
Answers provided by n workers to a task (not necessarily a comparison) are compared (in pairs) by workers.
The winners of the pairwise comparisons are compared with each other in the next step.
This process continues for a  xed number of steps, after which the majority answer is returned.
The intuition is that workers are better in identifying the correct answer (when comparing it to other answers) than in producing the correct answer.
Sorting in steps Work has also been done in sorting in a particular number of rounds (or steps as we de ned them).
H aggvist and Hell prove bounds on the complexity of sorting n items in r rounds [12].
In the same spirit, [5] answers the following question: How many processing units are required if one wants to sort n items in r rounds?
In these two works there is no uncertainty about the data nor the comparisons.
Voting systems Voting systems have been used to declare a winner (maximum item in our case) according to votes (comparisons to other items in our case).
One work in this line of research is [17], which declares a winner such that it creates the least disagreement with the votes of the crowd.
Furthermore, de Condorcet [6] and List and Goodin [14] provide useful insights for the plurality rule from the standpoint of political philosophy.
We investigated methods for retrieving the maximum item from a set in a crowdsourcing environment.
We developed parameterized families of algorithms to retrieve the maximum item and proposed strategies to tune these algorithms under various human error and cost models.
We concluded, among other things, the following:   The type of worker errors impact results accuracy, but not what is the best algorithm/strategy.
  It pays o  to vary the size of a task.
  It pays o  to vary/optimize the number of repetitions.
  Finding the maximum item with high accuracy is expensive, unless workers are very reliable.
However, it is much easier (costs less) to  nd an item in the top-2 or top-3.
Our results explore multiple models and shed light on various aspects of realistic crowdsourced max algorithms.
The models we used assume uniform and independent worker errors.
Of course, in a real crowdsourcing environment these assumptions may not hold.
We believe though it is important to  rst understand the tradeo s between quality/time/cost in more tractable scenarios.
Furthermore, our algorithms, optimized for the simpli ed model, may very well be good starting points in a more realistic setting, and can then be experimentally  ne tuned.
Natural extensions of our work include the retrieval of the top-k items, from a set and sorting sets of items both from the modeling perspective and from the practitioners  viewpoint.
