People are more inclined than ever to express their opinion online and news articles are typical pieces that drive tra c in the form of comments posted by users on news sites.
Such comments contain rich information such as entities (e.g., Person, Organization), and often express an agreeing or disagreeing opinion on the content of the article.
Yet, the most common dimensions used to recommend articles to users Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
are popularity (e.g., most shared and most commented), re-cency, and (manual) editors  picks (based on daily hot topics).
We propose to account for users  comments in choosing articles to recommend.
While recommended restaurants or products are ones that received mostly positive reviews, news articles that drive most tra c may be ones that are most diverse in the sentiment or entities expressed in their comments.
We propose to explore the e ect of sentiment and entities mentioned in comments on news diversity.
We are interested in a common scenario in which a user is reading a news article and related articles are recommended on-the- y.
Such recommendations are typically chosen based on overlap in content with the current article, re-cency, user activity (such as emailing and commenting) and on that day s editor s picks.
Figures 1, 2 and 3 show screenshots of related articles on popular news sites, Al Jazeera English, CNN, and the BBC, respectively.
As it can be seen on the examples, a variety of tabs that re ect di erent semantics such as  Most Viewed  and  Most Watched  enable the exploration of articles related to the one currently being browsed.
Our discussions with Al Jazeera editors and journalists led to the conclusion that none of the existing news sites uses the content of users  comments to select the set of related articles to recommend.
We conjecture that the use of sentiment and entities expressed in comments will enable a good diversi cation of recommended articles.
We use that as a basis to de ne a novel news recommendation problem that relies on achieving a balance between relevance and diversity.
Relevance is computed with respect to an article currently browsed by the user.
Diversity is based on a pairwise distance between articles according to user-generated content in the form of entities and sentiment contained in comments.
To the best of our knowledge, our work is the  rst formalization of the problem of real-time recommendation of diverse news articles that accounts for relevance to an input article and for users  comments to achieve diversity between recommended articles.
Given a set of articles Ra that are relevant to an input article a, we would like to identify a subset Sa   Ra of size k such that the audience s comments for the collection of articles Sa are most attractive among all possible subsets of Ra of size k. We capture di erent ways of expressing news attractiveness by incorporating entities and sentiment in a pairwise distance between articles.
Our problem then becomes  nding candidate articles that are within a relevance distance r from a given article a and among those,
 Jazeera English Figure 2: Editor s Picks on CNN Figure 3: News Popularity on the BBC identifying the k most diverse articles according to a diversity measure that captures coverage of di erent entities or user sentiment in comments associated to recommended articles.
Section 2 contains our formalism, the di erent diversity functions that we propose to explore, a de nition of our problem and a study of its complexity.
Given the real-time nature of our recommendations and the high complexity of our problem [1], we propose to adapt Locality Sensitive Hashing (LSH) techniques [11] to  nd candidate nearest neighbors of an input article a. LSH pre-computes buckets of similar articles based on pre-de ned hash functions.
Diversity is achieved in a second stage by looking for a subset of k articles among related ones that maximizes a diversity distance.
This problem refers to the dispersion problem under MAX-MIN criterion known to be NP-hard [13].
Hence, we adopt Ravi et al. algorithm [13] similar to Ganzalez s algorithm [10] that achieves a performance guarantee of 2, i.e., returns a set of k articles that 2-approximately maximize overall diversity.
Obtaining a performance guarantee less than 2 is proven to be NP-hard [10,
       In applications with a large number of similar articles, the size of precomputed buckets will be large and will a ect the performance of  nding candidate articles in dLSH.
To overcome this problem, we develop dLSH , an algorithm that pushes diversity into building buckets.
The intuition behind dLSH is to reduce the size of candidate articles by precomputing only potentially interesting articles.
For each bucket, it precomputes and stores at most k articles that (d-approximately) maximize the diversity of the bucket.
retrieves at most L  k articles where L is the num-dLSH ber of buckets thereby reducing the complexity of dLSH.
Section 3 contains a detailed description of our algorithms.
Our user study on real opinion articles on Al Jazeera English and on articles from Reuters compared the use of entities and sentiment contained in users  comments for diversifying recommended articles to diversi cation based on articles  content and showed the superiority of users  comments.
Our performance experiments show the superiority of our algorithms when compared to naive ones without compromising precision.
In particular, in many cases, pushing diversity into LSH buckets (dLSH ) results in better diversity and execution time improvement with a small relevance loss.
Section 4 provides a detailed summary of our   experiments.
Finally, related work is given in Section 5, and conclusion and future work in Section 6.
We de ne a set A of articles and a set U of users.
Each article a   A has a unique identi er aid and a set of users aU   U who posted comments on a.
A comment is of the form [aid, uid, text] where uid is a unique user identi- er.
Without loss of generality, we assume a user posts at most one comment per article.
The set of all comments on an article a form a discussion denoted by discuss(a).
A news article a could also be characterized by a set of attributes such as topic, date, authors, length, and nature (e.g., opinion article, survey).
Similarly, a user u could carry demographics information such as geographic location, gender, age and occupation.
We are interested in two kinds of distances between articles.
Relevance distance determines dissimilarity between two articles, an input article a and an article in A, and is used to  nd a set of articles related to a. Diversity distance, on the other hand, determines how  di erent  two articles are.
In this section, we start with a de nition of pairwise relevance, then we de ne  ve di erent distances to measure diversity between two articles.
The four former diversities rely on user comments on articles.
The latter is de ned on articles  content and will serve as a basis for comparison to assess the e ectiveness of using comments to achieve diversity between retrieved articles (see Section 4.5).
Given an article a and a threshold radius r, the set of articles relevant to a is de ned as the set of all articles ai   A i.e., drel(ai, a)   r.
within relevance distance r from a, For example, the distance between two articles (represented by their sets of features x and y) could be de ned as drel(ai, a) = 1  Jaccard(x, y).
Jaccard coe cient measures the similarity between two sets as the size of the intersection divided by the size of the union of the sets.
In this paper, features are extracted by running Open Calais(OC) 1 on an article and extracting entities mentioned in them.
There-1 http://www.opencalais.com 2fore, drel(ai, a) = 1   Jaccard(OC(ai), OC(a)).
We de ne diversity between two articles ai and aj as a function of discussions engaged by users through their comments on those articles.
features: Using Entities: For each article a, we characterize its discussion, discuss(a), with a set of topics (e.g., Politics, Sport, etc.)
and named entities (Persons, Countries, Cities) to which it refers.
Recall that discuss(a) is a unique document containing the set of all comments posted on a.
Features are extracted by running Open Calais(OC) on discuss(a).
The pairwise diversity distance between two articles is de ned as the Jaccard ratio between feature sets extracted from i.e., ddiv(ai, aj ) = 1   their corresponding discussions, Jaccard(OC(discuss(ai)), OC(discuss(aj ))).
The intuition behind this distance is that news readers are more likely to reveal and amplify subtle di erences (e.g., journalist s leaning toward a given topic) between two similar articles (e.g.
same topic, same persons, etc.)
than any content-based diversi cation technique will do.
Using Sentiment: Diversity between two articles ai and aj is a function of sentiment extracted from their comments.
Given an article a, each comment ci   discuss(a) is assigned a sentiment value si   [ 1, 1] which states whether the comment expresses a negative (si < 0), positive (si > 0), or neutral (si = 0) opinion.
A typical method for extracting sentiment from text is to use a dictionary of positive and negative words and count the number of occurrences of those words in the text.
Such dictionaries (e.g., UPitt 2, SentiStrengh 3) and sentiment extraction implementations based on them [4], are widely available.
Each article a is assigned a unique sentiment score obtained by averaging the scores si of all comments ci   discuss(a).
Diversity between two articles is given by: ddiv(ai, aj) = |sai   saj|.
Another option to use sentiment for diversity would be to   compute two sentiment scores (s+ a ) for each article a.
  s+ a (resp.
s a ) is the percentage of positive (resp.
negative) comments posted on a.
Here, diversity between ai and aj could be de ned as the Euclidean distance over their senti-(cid:2) ment coordinates ddiv(ai, aj ) = ai   s+ (s+ aj )2 + (s a , s ai   s     aj )2.
The intuition is that two articles are diverse if they drive di erent users  sentiments.
In other words, the more polarized two articles are, the more diverse they are considered.
Using user IDs: Each article a is assigned a set of user identi ers user IDs(a) who posted at least one comment on a.
Every user is considered only once even if she posted more than one comment.
Diversity between two articles ai and aj is a function of the overlap in users commenting on them, i.e.
ddiv(ai, aj ) = 1   Jaccard(user IDs(ai), users IDs(aj)).
The intuition here is that similar users tend to read similar articles.
Indeed, the similarity of two articles read by exactly the same group of users is likely to be higher than the similarty of articles that interest di erent users.
Using user locations: Each article a is assigned a set of Countries user countries(a) extracted from the pro les of users in user IDs(a).
Every country is considered 2http://mpqa.cs.pitt.edu/#subj lexicon 3http://sentistrength.wlv.ac.uk/ only once even if many users from that country posted a comment on a. Diversity between two articles ai and aj is a function of the overlap in countries from where comments are issued on both of them, i.e. ddiv(ai, aj) =
 The intuition here is that articles that interest users from the same region of the world are more likely to be similar than articles that interest users from di erent regions.
Here, we de ne the diversity between two articles ai and aj as a function of entities extracted from their content.
This de nition will serve as a basis in our experiments to assess the e ectiveness of comment-based diversity in  nding attractive articles.
In particular, each article a is assigned a set of features (topics, persons, and locations) extracted from its content using Open Calais.
Diversity between two articles ai and aj is a function of the overlap in features cited in both of them, i.e. ddiv(ai, aj) = 1   Jaccard(OC(ai), OC(aj )).
Thus, two news articles on US election may be similar because they refer to the same topic (Politics) and location (USA), but still di erent (diverse) if one is about Mitt Romney and the other on Barack Obama.
The relevance Rel(R) of a set of articles R with respect to an input article a is de ned as Avgai R(1   drel(ai, a)) Set diversity answers the following question: given a set R, if we were to pick a set S   R containing the k  most distinct  (as measured by the diversity distance) articles, how distinct those articles would be?
The pairwise k-diversity divk(R) of a set R is de ned as max S R,|S|=k min ai,aj S ddiv(ai, aj ) In above formulation, diversity is de ned as the maximum over any set of size k, of the minimum pairwise distance between articles.
Here, ai plays the role of the  aggregate  or  centroid , and diversity measures the minimum distance to the centroid.
Our goal is to return a set Sa of k most diverse articles among a set Ra formed by articles whose relevance distance to an input article a is at most r. That is, for a distance r and given a point a, we would like to  nd a set of k points within distance r from a (according to the relevance distance) that maximizes their pairwise k-diversity (according to the diversity distance).
Given the complexity of this problem [1], we de ne an approximate version where the goal is to  nd a set of k points that d-approximates their pairwise k-diversity (i.e., the k-diversity is at least 1/d times the best possible).
Finally, we can de ne the bi-criterion approximate version of the problem, where for approximation factors c and d, our goal is to  nd a set of k points S within distance cr from a such that divk(S)   1/d   divk(Ra), where Ra is the set of points within distance r from a.
A simple way of solving this problem (for c = 1 and d = 2) would be to (i)  nd the set Ra of all points within relevance distance r from a, and (ii) 2 approximate divk(Ra).
The former task can be done using LSH algorithm in O(ne) time 3for some exponent e   1[11], while the latter task can be done using e.g., the 2-approximate GMM algorithm [10] in O(|Ra|k) time.
A nice feature of the latter algorithm is that it works for any diversity distance function satisfying metric properties (i.e., symmetry and triangle inequality).
Our  rst solution is to implement and combine such above algorithms into a single algorithm we call dLSH and that runs in O(ne + |Ra|k) time.
As, the O(ne + |Ra|k) time can be too large in some applications.
we present another algorithm (dLSH ) to improve the time to O(L2), where L is the number of locality-sensitive hash functions needed to return all points within distance (approximately) r from a.
Typically, we have L << n.
Our approach has two steps: i. identify the set Ra of candidate articles which are within a relevance distance r from an input article a, and ii.
 nd among Ra the subset Sa of articles that maximizes diversity.
We propose two algorithms dLSH and dLSH .
dLSH is a direct application of Locality-Sensitive Hashing (LSH) [11] on top of which we implemented a greedy algorithm to solve the maximiza-  tion of minimum pairwise distances problem.
, we pushed diversity into LSH by precomputing the k most diverse elements of each LSH bucket.
This makes the algorithm faster than dLSH.
Furthermore, we show in [1] that dLSH
 achieves a 6-approximation of dLSH.
    In dLSH We start with an overview of the LSH algorithm [11].
Essentially, the algorithm attempts to  nd all candidate articles composing Ra.
This is done as follows: during an o ine process, LSH creates l hash functions g1 .
.
.
gl, and their corresponding hash tables A1 .
.
.
Al. Each function gi is a combination of k hash functions hj (i.e., gi = h1.
.
.
.
hk.)
Then each article a is stored in bucket gi(a) of Ai for all i = 1 .
.
.
L (lines 1-4, Algorithm 1).
The hash functions have the property that, for any article a, we have Ra   A1(g1(a))   .
.
.
  Al(g1(a)) It should be noted that functions with the above property (for low values of k and l) are known to exist only for speci c relevance distance functions, such as the Euclidean distance and dot product (see [5] for more details).
At runtime, for any input article a, LSH simply recovers all points in A1(g1(a))   .
.
.
Al(gl(a)) (lines 1-3, Algorithm
 Algorithm 1 dLSH: o ine process Input: G = {g1, .
.
.
, gL}: set of L hashing functions, D: s: size of precomputed diverse collection of articles.
buckets.
Output: A = {A1, .
.
.
, AL}: set of L hash arrays.
Ai[gi(a)]   Ai[gi(a)]   {a} for all gi   G do As pointed in Section 2, relevance distance between two articles ai and aj both characterized by a set of features is de ned as: drel(ai, aj) = 1   Jaccard(OC(ai), OC(aj )).
An e cient way to estimate the Jaccard coe cient is to use min-hash functions based on min-wise independent permutations.
A min-hash function h    H could be de- ned as follows: Let OC(ai) be the set of features (topics, persons, and locations) extracted from article ai, E = OC(a1)  .
.
.
OC(an) the union of sets of features of all articles, SE the set of all permutations of E, and   a permutation uniformly chosen at random over SE .
h (a) = M in{ (xi), xi   OC(a)} Where  (xi) is the index of the entity xi within permutation  .
Such min-hash functions guarantee the following property [7]: P r(h (ai) = h (aj)) = Jaccard(N E(ai), N E(aj)) Based on H , we de ne each hash function gj as a composition of K min-hash functions h i, gj(a) = [h 1(a)   h 2(a)     h k(a)] %  where   is a prede ned prime.
Enabling Diversity: Greedy Max-Min Algorithm.
To enable diversity we need an algorithm that identi es a subset of k most diverse articles among a set of n articles.
According to our de nition in Section 2.2, given a set S of n articles, and a diversity distance ddiv, the most diverse subset of k articles is the one that maximizes the minimum pairwise (diversity) distance.
This problem refers to the dispersion problem under MAX-MIN criterion known to be NP-hard [13].
To solve this problem, we adopted the greedy algorithm proposed by Ravi et al. [13] similar to Ganzalez s algorithm [10] that achieves a performance guarantee of 2 in the case of distances satisfying the triangular inequality.
Note that obtaining a performance guarantee less than 2 is proven to be NP-hard [10, 13].
Algorithm 2 GMM: Greedy MaxMin algorithm Input: D: set of articles, ddiv: distance function, k: number of requested articles ddiv(c, d) mizes the min pairwise distance Output: C: set of k articles which x-approximately maxi-



4: repeat
  nd e|e   D   C    f   D   C, minx C{d(e, x)}   miny C{d(f, y)} C   C   {e}


 Algorithm 2 optimizes for the maximal minimum pairwise distance.
It starts by adding to the result set the two articles with the maximum pairwise distance (lines 2-3, Algorithm
 mizes the minimum pairwise distance with articles already selected (lines 4-6).
The algorithm stops when the number of articles in the result set is k (line 7).
Algorithm 3 describes the way diverse recommendations of related articles are processed.
Given an input article a, a distance radius r, and hash tables {A1, .
.
.
AL} created in the o ine step; the algorithm does the following: First, it retrieves all candidate articles that are hashed into the same buckets as a (lines 1-3).
Second, it  lters those articles that are out of the ball B(a, r) (lines 4-6).
Finally, the algorithm runs GMM to pick the d-approximately k most diverse articles (lines 7-8).
Algorithm 3 dLSH: online process Input: G = {g1, .
.
.
, gL}: set of L hashing functions, A = {A1, .
.
.
, AL}: set of L hash arrays, D: collection of articles, a: input article, r: radius distance, k:number of requested articles.
within a distance r from a.
Output: Sa: set of d-approximately k most diverse articles







 Ra   Ra   {pj} {Remove irrelevant answers from result set} if disdiv(a, pj) (cid:15) r then      : Pushing diversity into LSH buckets
 As pointed earlier, the main drawback of dLSH is that in several cases the O(n + |Ra| k) time complexity is too high, especially in applications where the data distribution results in many dense buckets.
To overcome this problem, we present dLSH , an algorithm that considers diversity earlier in the recommendation process.
The intuition behind dLSH is to reduce the size of Ra by storing only potentially interesting articles in hash tables A1, .
.
.
AL. Suppose that we request k articles that d-approximately maximize the diversity of Ra.
For each bucket Ai[j], we pre-(cid:5) compute and store the set A i[j] of at most k articles that (d-approximately) maximize the diversity of Ai[j].
This is done by using GMM algorithm during the o ine process of dLSH (lines 5-7, algorithm 4).
The online process of is similar to the one of dLSH (see Algorithm 3) dLSH retrieves at most L   k articles stored except that dLSH (cid:5) in buckets A i(gi(a)), i = 1 .
.
.
L. Here again, GMM is used to  nd the d-approximately k most diverse articles among those L   k article.
By doing so, the complexity in time drops to O(Lk), where L is the number of locality-sensitive hash functions and k is the number of desired articles.
     

 We conducted di erent experiments to assess the e -ciency and e ectiveness of our proposals.
Implementation setup Our algorithms are implemented in JDK 6 with 2GB virtual memory.
All experiments were conducted on an Intel dual core workstation with 4GB Memory, running Windows 7 OS (64-bit).
Obtained results are the average of three separate runs.
  Algorithm 4 dLSH Input: G = {g1, .
.
.
, gL}: set of L hashing functions, D: s: size of precomputed diverse : o ine process collection of articles.
buckets.
Output: A = {A1, .
.
.
, AL}: set of L hash arrays.
for all gi   G do





 Ai[gi(a)]   Ai[gi(a)]   {a} Ai[j]   GM M (Ai[j], s) for j = 0; j < size(Ai); j + + do Datasets We conducted our experiments on two di erent datasets: a collection of AlJazeera English (AJE) opinion articles and a collection of Reuters news articles.
  AJE opinion articles4 This dataset contains 2040 articles published between April 24th, 2010 and February
 (389k in total) posted by 35k di erent users from 179 di erent countries.
We characterized each article by its features (topics, persons, and locations) extracted using Open Calais.
On average, each article had around 7.5 di erent features distributed as follows: 1.42 top ics, 2.58 cited persons, and 3.5 cited locations.
  Reuters news articles5 We crawled 13K news articles published by Reuters between December 4th, 2010 and December 20th, 2011.
We ran Open Calais 6 to identify article features.
For each article, we identi- ed an average of 5.19 features with 1.6 topics, 1.85 cited persons, and 1.74 cited locations.
Note that user comments are not available for Reuters articles.
We introduce in this section three algorithms that we implemented and used in the experiments to evaluate di erent aspects of our diversi cation proposals.
Exact Near Neighbors Algorithm Referred to as r N N is an algorithm that given an input article a and a distance radius r, scans the whole data collection and retrieves all those articles within a distance r from a.
Approximate k Nearest Neighbor Algorithm Referred to as k N N is an algorithm that given an input article a and a list of candidate relevant articles Ra (generated by LSH), ranks articles in Ra according to their relevance to a and picks the top k among them.
This algorithm is approximate because in relies of Ra rather than the whole data collection.
MMR Algorithm M M R is an iterative algorithm that strives to reduce redundancy while maintaining relevance of retrieved articles[8].
M M R is based on the concept of Marginal Relevance (MR) that is a linear combination of the relevance of an articles wrt an input one and its novelty wrt already selected ones.
The MR score of article ai w.r.t.
4http://www.aljazeera.com/indepth/opinion/ 5http://www.reuters.com 6 http://www.opencalais.com
 given by: M R(ai) =     sim(ai, a)   (1    )   maxaj Ssim(ai, aj) Then, at each iteration, the algorithm picks the article not yet selected whose MR score is maximal.
Our  rst experiment (Section 4.3) showed that dLSH exhibits a strong performance in terms of execution time compared to a brute force r N N. dLSH is 22 times faster in nearest neighbor retrieval and 30.38 times faster in recommending diverse related articles compared to MMR.
This result makes our approach suitable for a real-time recommendation scenario.
Second, exploiting user-generated data, comments in our case, to diversify articles leads to a good balance between relevance and diversity (Section 4.4).
The comparison result of dLSH and MMR strengthens this observation as it shows clearly that dLSH preserves relevance while providing an acceptable diversi cation compared to MMR.
Moreover, we found that the diversity gain induced by dLSH when compared to k N N, is much more important than relevance loss especially in the case of low values of r. For instance, dLSH@r = 0.5 under sentiment-based di-versi cation increases recommendation diversity by 37.46% while average relevance drops down by only 5% compared to the TopK lists.
Finally, the user study (Section 4.5) revealed that users prefer sentiment-based diversity.
We ran both LSH and an exact brute-force algorithm, r N N, to request the set of nearest neighbor articles, Ra, that are within a relevance distance r of an input article a where a is instantiated to every article in the AJE and In each run, the relevance distance r Reuters collections.
took each of these values: {0, 0.2, 0.4, 0.6, 0.8}.
For each input article a, r N N accesses all articles in the corresponding collection and retrieves those within distance r from a.
For LSH, the parameters are set as follows: L = 1, K = 2, and   = 307 (i.e. g(p) = h 1(p)   h 2(p)%307).
So, for each article a, the algorithm  rst retrieves all articles stored in the bucket (A [g(a)]), then it  lters articles out of the ball (Ra).
Figure 4: exact r N N vs. LSH @r In Figure 4, we show the evolution of the average execution time per 1000 input articles in both Reuters (left-hand side) and AJE (right-hand side).
Not surprisingly, LSH is much faster than r N N. On average, LSH is 4.5 times faster for Reuters, 22.5 times faster for AJE.
This gain is relatively stable for all values of r. The di erence in gain achieved in the two collections is due to the distribution of articles in the distance space.
By analyzing the two collections, we observed that AJE articles are scattered whereas Reuters has many dense regions representing similar articles.
The presence of dense regions in Reuters is re ected by the generation of a small number of LSH buckets each of which with a high  lling rate.
In contrast, in the sparse AJE collection, LSH generates a large number of buckets with a very low  lling rate.
From this observation, it becomes obvious that Reuters requires more processing (to  lter out irrelevant articles) than AJE.
However, as expected, Table 1 shows that the gain in execution time a ects recall.
In fact, by considering the set of articles returned by r N N as our ground truth, we computed recall@r for each collection as the ratio between the number of articles returned by LSH and the number of articles returned by r N N. For instance, Table 1 shows that for r = 0, LSH returns the same results as r N N for both collections.
For this speci c value of r, we observed an average of 0.28 nearest neighbor articles for each AJE article and 38.3 nearest neighbors for each article in the Reuters collection.
We can also see that the recall curves are drastically decreasing as r increases.
This is due to the o ine process in LSH during which only articles with relatively highly similarity are hashed into the same bucket.
For example, when we request the set of nearest neighbors within a distance r = 0.4, only 30% of them are returned by LSH in both collections.
To improve the recall score, one can reduce K and increase L hence creating simpler hash functions.
However, for the data collection sizes we have, this will result in a signi cant increase in time necessary to  lter out irrelevant articles.
Table 1: LSH recall@r Dataset/ r
 Reuters















 We evaluate the e ciency and e ectiveness of dLSH algorithms.
As we did not have access to user and dLSH comments on Reuters, we used AJE only.
  Evaluation Method   For each input article a, we identify the set of candidate related articles Ra using dLSH or dLSH .
The radius r takes one of these values: 0.5, 0.6, 0.7, and 0.8.
For each set of candidate answers we compute two sets divK and topK.
The former is the set of k articles that d-approximately maximizes diversity generated by dLSH whereas the latter is the set of k most relevant articles within Ra generated using an k N N algorithm (both de ned in section 2.2.)
Basically, the approximate k N N algorithm accesses all articles in Ra, ranks them in terms of their relevance to a, and returns the top K among them.
Here also, we varied k to take di erent values ranging from 3 to 20 for both divK and topK.
For each set (divK and topK), we report its diversity and average relevance scores obtained in both dLSH and/or dLSH .
Thus, we can estimate the loss of relevance implied by each diversi cation technique as   well as the impact of pre-computing divK of each dLSH bucket.
In what follows, we  rst report the results of dLSH in terms of relevance and diversity of recommendations   compared to k N N. Then, we report the impact of dLSH on results obtained by dLSH.
  6we argue that it is preferable to substitute users to their locations (extracted from ip addresses) as the average number of locations per article is much smaller than the number of users who commented on that article, hence improving execution time.
Finally, there is no obvious relationship between the radius r and the diversity gain achieved by the di erent techniques.
For instance, one would expect that diversity scores increase as the value of r increases.
That is not the case here because the diversity of divK sets is optimized using one of the diversi cation techniques we proposed (using entities, sentiments, user ids, or locations) whereas diversity scores reported in Table 2 re ects the content diversity of divK sets.
Comparing dLSH to approximate k N N An important issue of diversi cation in general, is to control the loss in relevance implied by diversi cation.
We now show that dLSH leads to a substantial increase in diversity of divK sets while maintaining a reasonable relevance compared to topK sets (Table 2).
As diversity is measured using di erent distances in di erent spaces (entities, sentiments, users, and locations), scores are not comparable.
For instance, the same set divK of k articles may get a high diversity score in the space of user s locations and a low score in the space of sentiments.
In order to enable the comparison of di erent diversi cation techniques, we consider content-based diversity as the ground truth.
For every input article a, we retrieve its corresponding divK sets using entities, sentiments, users, and locations for diversi cation.
Then, for every divK set, we report its actual diversity score measured in terms of articles content (as de ned in Section 2.1.3.)
This allow us to check if there is any correlation between the diversity of articles obtained in di erent comment spaces and the diversity of articles in the content space.
More speci cally, we check whether a set of articles diverse in terms of sentiments, entities, users, or user s locations is also diverse in term of articles  content (less redundancy and more novelty).
Table 2 reports the gain/loss in diversity/relevance achieved by dLSH compared to approximate k N N. For every radius value r, we report the average of scores obtained at di erent values of k   {3, 5, 10, 15, 20}.
Table 2: dLSH vs. r N N: Percentage of Gain/Loss in Diversity/Relevance @r Radius



 Entity-based rel.
div.
-6.37
 -5.38
 -8.8

 -20.79 div.
Sentiment-based UserID-based rel.
-3.84 -4.42 -7.88 -19.44 rel.
-5.07 -4.73 -8.47 -20.33 div.
geoLocation-based div.
rel.
-3.87 -4.34 -7.87 -19.5 Not surprisingly, we notice that all diversi cation techniques achieve a positive gain in diversity of divK sets compared to topK sets returned by r N N algorithm.
This means that there is a clear correlation between comment diversity as acquired through entities, sentiments, users, or locations from one side and content diversity from another.
To con rm this observation, we note that maximizing the diversity with any of the techniques we proposed results in dropping the average relevance of divK sets compared to topK sets.
Also, it seems that the best compromise between diversity and relevance is achieved in the case of r = 0.5 where the gain in diversity is maximized in all cases (entities, sentiments, users, and locations) while the loss in relevance is minimized in almost all cases.
There are several observations to be made regarding the pairwise comparison of the four diversi cation techniques.
First, The sentiment-based approach outperforms other approaches as it implies 19.4% of diversity gain and -9.6% of relevance loss in average.
This con rms the existance of a strong correlation between content diversity and opinion (sentiment) diversity, i.e., two diverse articles are more likely to generate di erent sentiment values than two similar articles.
Second, userID-based and geoLocation-based diver-si cation techniques achieve almost the same results.
This observation is con rmed in Figure 5 where one can see that these two techniques lead to almost the same curves.
Thus, Figure 5: Diversity of di erent diversi cation techniques Figure 5 summarizes results presented in the last line (r = 0.8) of Table 2 breakdown by value of k. Black (resp.
grey) curves correspond to divK (resp.
topK) while continuous (resp.
dashed) line curves correspond to diversity (resp.
relevance).
From this  gure we observe the following.
First, in all diversi cation techniques and for all values of k, dLSH achieves a real content diversity compared to approximate k N N. The diversity achieved is more important when using entities (A) and sentiments (B) than in the two remaining cases (C and D).
Second, the average relevance of divK sets is not a ected by the number of requested articles (k).
This unexpected behavior is due to the o ine step of dLSH algorithm where only relatively highly similar articles are hashed into the same bucket.
That is, for a given bucket Ai[j] whose size is m, and any two di er-ent numbers K1, K2 such that K1   m, K2   m we have relevance(divK1)   relevance(divK2).
Note that this observation holds for topK sets as well.
This further corroborates our conclusion made in Nearest Neighbor retrieval experiment (section 4.3) related to the recall scores achieved by LSH Comparing dLSH to MMR In the previous experiments, we investigated the impact of dLSH on the relevance and diversity of recommendation compared to T opk approach.
We have shown that in most of cases, dLSH implies a signi cant gain in diversity against a relatively small loss in relevance.
In order to have a com-
versi cation approach called Maximal Marginal Relevance (MMR) [8] (see section 4.1).
The evaluation process may be summarized as follows.
For each input article, we requested di erent lists of divK articles using dLSH and M M R. All parameters (K, r, and  ) are varied to capture the di erent trends of each algorithm.
In the case of dLSH, we reported the results of the four diversity distances proposed.
Figure 6 summarizes the results of comparing dLSH to MMR in terms of execution time ( gs.6(a), and (b)), relevance ( gs.6(c),(c ),(e), and (g)), and diversity ( gs.6(d), (d ), (f), and (h)).
In  gs.6 (c), (c ), (d) and (d ), we plotted relevance and diversity scores achieved by dLSH under di erent diversity distances and MMR when the number of requested articles was K = 10.
The x-axis corresponds to the di erent values of dLSH radius r in  gures (c) and (d) whereas it corresponds to the MMR parameter   in  gurs (c ) and (d ).
Not surprisingly,  gs.
6 (c) and (c ) show that dLSH relevance curves decrease as the radius r increases while the MMR curve slightly increases as   increases.
One should expect that the relevance of MMR @  = 0.9 (  0.347) should be greater than the relevance achieved by dLSH @r = 0.5 (  0.525).
However, contrarily to MMR where scores are obtained by averaging relevance of recommendation lists of all articles in the dataset collection, dLSH scores are obtained only from those articles having at least K + 1 nearest neighbors within a radius r. This results in a higher relevance in the case of dLSH that is always greater than or equal to r. In the remaining  gs, we decided to plot results of dLSH under di erent diversity distances and values of r along with results of M M R@0.7.
Setting MMR parameter   to 0.7 is a reasonable choice as it provides a good balance between relevance and diversity.
Figs 6 (e) and (g) strengthen the previous remark and show clearly that relevance@k (number of requested articles) obtained by dLSH (with r = 0.7) under all diversity distances is substantially greater than relevance@k achieved by MMR.
In contrast to that,  gs.
6 (f) and (h) show that MMR achieves a better diversity compared to dLSH.
In fact, as reported before, AJE collection is so scattered that there re many articles that don t have any relevant article (within a reasonable distance radius).
However, unlike dLSH which may return an empty list of recommendation for this kind of articles, MMR will anyways recommend a list of articles that minimize the intra-redundancy even if none of these articles is relevant enough to the input article.
To summarize this experiment part, we can say that dLSH is a good  t in cases where a minimum relevance threshold should absolutely be satis ed while RM M is better in cases where we are more  exible with the relevance.
Yet, another interesting result concerns the time performance of dLSH.
Figs 6 (a) and (b) present the execution time (in milliseconds) necessary to the di erent algorithms in order to  nd recommendation lists for 100 input articles.
In the former  g, we  xed r = 0.8 and   = 0.7 and varied the number of requested articles K whereas in the latter  g, we  xed K = 10 and   = 0.7 and varied the radius r of dLSH.
From  g 6 (a) we can see that dLSH is much more e cient than MMR.
For instance, when we requested
 sentiment-based diversi cation which was 30.38x times faster than MMR while the worst gain is recorded by userID-based dLSH which was only 7.48x times faster.
The slowness of userID-based diversi cation is due to the high number of unique users commenting on each article making the distance calculation more expensive.
However, one can use location-based instead which is 24.68x faster than MMR and achieves almost the same relevance and diversity scores than userID-based diversi cation.
In cases where we requested more articles (K = 15 and K = 20), the gain achieved by dLSH was even bigger.
This gain is due to the nature of MMR which for a given input article, scans the collection of articles K times, at each iteration it updates the MR scores of the hole dataset and picks only the T op1.
Comparing dLSH to dLSH* This section reports results of comparing dLSH to dLSH .
The comparison is conducted to measure the impact of pre-computing diverse buckets on the diversity and relevance of recommended related articles.
As pointed before, LSH algorithm reduces signi cantly the execution time compared to the exact r N N algorithm.
However, when the size of data collections is important LSH may become slow.
In such cases, one can use dLSH to reduce the execution time.
    Table 3: Execution time(dLSH   vs. dLSH) Tech Entities Sentiments Users Locations Gain 27.49x 21.34x 22.82x 5.5x       Table 3 summarizes the gain in execution time obtained by dLSH over dLSH.
For instance, in the case of entity-based diversi cation, dLSH is 27.49 times faster than dLSH whereas in the case of sentiment-based diversi cation, dLSH is only 5.5 times faster.
The di erence in gain depends on the nature of diversity distance used.
From a computational point of view, it is clear that Euclidean distance used in sentiment-based diversi cation is much faster than Jaccard correlation used in the Entity (UserId and location)-based diversi cations.
Thus, reducing buckets size in dLSH leads to a higher gain in the case of entity, users, and location based diversi cation.
Table 4: dLSH vs. dLSH: diversity/relevance gain     Tech Entities Sentiments Users +3.41% -1.42% Rel.
gain Div.
gain +58.49% -19.91% -9.83% +11.73% Locations +4.52% -17.84%       The main concern of dLSH is the way it impacts relevance and diversity compared to dLSH.
For instance, one would expect that diversity scores of dLSH divK sets be lower than diversity scores of dLSH divK sets (because dLSH relies on subsets of dLSH buckets).
We will see that this observation is not hold when the diversity is measured on articles content.
Table 4 presents the overall gain and/or loss in relevance and diversity caused by dLSH when compared to dLSH.
These overall scores are obtained by averaging scores of each diversi cation technique obtained at di er-ent values of k = {3, 5, 10, 15, 20} and r = {0.5, 0.6, 0.7, 0.8}.
It is worth to notice that diversity gains presented in the  gure are calculated over content-based diversity achieved by dLSH and dLSH did well in the case of entity-based diversi cation as the diversity of returned sets increases by 58.50% while the relevance decreases .
As we can see, dLSH      



 r e p ) s m ( e m i t e g a r e v a s e i r e u q












 r e p ) s m ( e m i t e g a r e v a s e i r e u q

















 (a) Execution time @k, r=0.8, (cid:644)=0.7









 (c) Relevance@r , k=10
























 (c(cid:859)) MMR Relevance@ (cid:644), k=10



















 (b) Execution time @r, k=10, (cid:644)=0.7



 (d) Diversity@r, k=10



 (d(cid:859))MMR Diversity@ (cid:644), k=10


 Entity based Sentiment based User ID based








 (e) Relevance@k, r=0.7, (cid:644)=0.7





 (g) Relevance@k, r=0.6, (cid:644)=0.7













 (h) Diversity@k, r=0.6, (cid:644)=0.7





 (f) Diversity@k, r=0.7, (cid:644)=0.7 geoLocation based Figure 6: Comparing dLSH to MMR in terms of execution time, relevance, and diversity     by only 1.42% compared to sets returned by dLSH.
That is, there s a real bene t to use dLSH rather than dLSH for this kind of diversi cation.
In the case of sentiment-based diversi cation, the results are encouraging although the advantage of using dLSH is less noticeable as the resulting 11.7% gain in diversity is almost paid with a loss of 9.8% in terms of relevance.
In contrast to that, we observe in the two remaining diversi cation techniques, namely user-based and location-based, that dLSH leads to a diversity decrease and a relevance increase.
Moreover, the derived loss in diversity (user-based: -19.9%, location-based:-17.8%) is much more important than the gain in relevance obtained ((user-based: 3.4%, location-based: 4.5%)).This unexpected results lead to think that there is no obvious relationship between dLSH and dLSH for two reasons: (i.)
The diversity is maximized for some comment-based distance its quality is measured in terms of content.
(ii.)
The 6-approximation doesn t hold in the case of Jaccard-based distances which doesn t satisfy the triangular inequality.
   
 To evaluate the quality of our diversi cation techniques, we conducted a user study using Amazon Mechanical Turk system (AMT).
The goal of this survey is twofold: (i.)
Compare lists of related articles generated by dLSH and M M R.
(ii.)
Compare di erent diversity distances to each other.
For this purpose, we randomly picked two articles from two main topics: Politics and Business-Finance.
For each input article we generated four related articles lists each of size 5.
Three lists are generated using dLSH with entity, sentiment, and userId- diversity distance whereas the fourth list is generated using M M R.
location-based diversi cation is intentionally omitted because it usually generates the same lists than userID-based diversi cation.
Comparative experiment.
We design a survey including two questions of interest: In question Q1, we ask workers to rate the overall diversity of one list of articles (let say list A) wrt.
another list (say list B) using  ve comparative measures: articles in list A are signi cantly more diverse, somewhat more diverse, equally diverse, somewhat less diverse, or signi cantly less diverse compared to articles in list B.
In question Q2, we simply ask them to say which list of articles they do prefer.
We build for each input article 6 di erent groups of HITs each of which composed of 11 identical HITs (the total number of HITs is 132).
In the  rst three groups we compare di-versi ed list of articles (say, divK lists) generated by our di-versi cation techniques (entity-based, sentiment-based, and userID-based) to the list generated by M M R (say, mmrK lists.)
The three remaining groups are devoted to compare divK lists two by two.
In order to make the user study realistic, we omitted to provide any extra information about related articles but their titles while the input article was presented with its title, key features (topic, persons, locations), a small abstract, and a Read more link.
As a consequence, all workers answers will rely on the titles only.
Results To quantify the responses given by workers, we use two metrics called Mean Response Volume (MRV) and Mean Weighted Response (MWR) [9].
For a given question, MRV measures the number of responses received per option divided by the total number of responses whereas MWR provides an aggregated quantitative measure about workers responses.
To aggregate workers responses, we number options from 1 (worst) to 4 or 5 (best) according to the number of options provided.
Overall, the user study revealed that only 28.5% of workers preferred (in Q2) the list of related articles they identi- ed as being more diverse than the other one (in Q1).
This leads to think that users are somehow reluctant to diversity as they don t understand the reason for which certain articles are recommended to them.
An explanation e ort should be undertaken here to overcome this problem (e.g., display the related features under article titles).
Figure 7-(a) shows the overall MRV of each option in response to Q1 in HITs comparing divK lists to mmrK lists.
In the case of economics input article, more than 65% of workers found dLSH related articles (signi cantly or somewhat) more diverse than M M R list while in the case of politics input article only 30% of workers said that dLSH
 contradictory results are mainly due to the fact that users are restricted to article titles to judge the diversity of lists.
It turns that in the former case, all the  ve articles in the M M R list contained the word China in their title while the input article was about The asset crisis of emerging economies.
dLSH did better in this case, as the list of articles it generated contained more varied titles.
In the latter case, we think that the actual (content) diversity of articles was better re ected in the titles in both M M R and dLSH lists.
Therefore, a majority of workers noticed that M M R list is more diverse than dLSH lists.
Figure 7-(B) reports the MWR breakdown by diversi cation technique.
It shows that the three diversi cation techniques produce lists that are more diverse (score (cid:15) 3) compared to mmrK lists in the case of economics input article.
In the case of politics input article, userID and entity-based diversi cation got almost the same diversity scores than M M R while sentiment-based lists were de nitely less diverse.
Figure 7: Diversity of dLSH vs. M M R In terms of pairwise comparison of dLSH diversi cation techniques, results are as follows: In the case of economics article sentiment-based diversi cation was preferred to both userId-based (55% vs. 45% of votes) and entity-based (73% vs.27% of votes), and user-based diversi cation was preferred to the entity-based (82% vs. 18% of votes).
In the case of politics article, entity-based diversi cation did almost as well as sentiment-based diversi cation, both ahead userID-based technique.
Overall, the most preferred diversi- cation was sentiment-based distance which surprisingly got a good results in both economics and politics input articles.
That is, people tend more than ever to express their sentiment on news articles of di erent topics, in particular those directly impacting their lives.
Search result diversi cation has received extensive attention in both research and industry.
Carbonell and Goldstein introduced the concept of diversity in text retrieval and sum-marization [8].
Authors used two similarities to de ne the Maximal Marginal Relevance criterion.
The  rst similarity is used to evaluate document relevance given a query and the second to estimate document novelty.
Since then, several diversi cation techniques have been proposed which fall into two main approaches: content-based and intent-based.
In the former [8, 12, 6] the goal is to reduce the redundancy of a set of results by selecting the most relevant document and greedily adding documents which maximize relevance and minimize redundancy.
In the latter [3, 14] the goal is to cover subtopics (intents) behind queries.
This approach is particularly e ective in the case of ambiguous queries.
It starts by identifying the set of all possible query subtopics and uses probabilities with which the query belongs to each subtopic to rank documents.
In recommender systems, diversi cation was approached through two paradigms: local and aggregate.
Local diversity [18, 17, 15, 16] aims at increasing the intra-diversity of a set of items within a recommendation list and is usually achieved using attributes [18, 17] or explanations [16].
Unlike local diversity, aggregate diversity aims at increasing diversity [2] so that users do not see the same top k items.
Jain et al. address the problem of providing diversity in K-nearest neighbor queries [12].
Authors present an algorithm (MOTLEY) which relies on a user-tunable parameter to control the minimun diversity that any pair of results should satisfy.
The main idea consists in  nding the set of candidate answers which satis es the diversity constraint, then access these candidates in a smart way (using an R-tree) to pick the set of k nearest neighbors.
Besides the fact that we are using social data to recommend diverse news, there are some major di erences between our work and that of Jain et al. i.
In our solution, only those items which are within a relevance distance are candidates, whereas [12] consider items which satisfy a diversity constraint, i.e., MOTLEY ii.
Unlike MOTLEY provides no guarantee on relevance.
that uses R-trees to retrieve the set of k-NN, we are using LSH techniques [5] that are faster and more robust under high dimensionality [11].
We addressed the problem of providing diverse news recommendations related to an input article.
We leveraged user-generated data to re ne lists of related articles.
In particular, we explored di erent diversity distances that rely on the content of user comments on articles such as sentiments and entities.
Given the real-time nature of our recommendations, we applied locality-sensitive hashing techniques to  nd candidate articles that are within a relevance distance r from an input article a.
Then, we adapted a greedy algorithm to identify the k articles that collectively 2-approximately maximize diversity.
Our user study showed that user-generated data achieves a good balance between relevance and diversity compared to other diversi cation techniques.
We also showed with an extensive set of experiments on Al Jazeera English and Reuters collections, that using LSH makes our proposals e ective in an real-time environment.
We would like to investigate the relationship between LSH parameters L and K and the relevance of nearest neighbor articles retrieved.
The intuition we have is that the diversity of two articles can be a function of the number of times (  = 1 .
.
.
L) they fall into the same bucket (i.e.
number of hash functions that hash both articles to the same bucket).
The lower that number, the higher the diversity.
