Con icts among information sources are commonplace: Twitter users debate the e ects of healthcare reform, Wikipedia Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
authors provide di ering populations for the same city, online retailers o er discordant descriptions of the same product,  nancial analysts disagree on the future price of securities, and medical blogs prescribe di erent courses of treatment.
Consequently, we need a means of discerning which of the asserted claims are true, especially on the web, where three of our four experimental datasets (from current, real problems in information credibility) originate.
Presently this is addressed by simple or weighted voting or, with more sophisticated fact nder algorithms (e.g.
[4, 18, 14]), transitive voting, but these methods tend to be ad hoc and di cult to analyze and extend.
Latent Credibility Analysis is a new method of approaching the credibility problem by instead modeling the joint probability of the sources making claims and the unseen (latent) truth of those claims.
Finding the probability that a particular claim is true is then performed via inference in a probabilistic graphical model using one of the many extant exact and approximate inference algorithms.
Unlike those of fact nders, the resulting credibility decisions and the parameters capturing the credibility of the sources are distributions and probabilities with clear semantics: for example, in the SimpleLCA model we reason that a claim is likely to be true because the probability that everyone who asserted it was lying (as given by the Honesty parameters of the sources) is relatively small.
This transparency is important both when we need to explain the model s decisions to users (who might otherwise distrust the system itself) and when we adapt an LCA model to real-world problems; in our experiments, we are able to formulate reasonable priors and anticipate (to a degree) the most appropriate, best performing models by understanding the domain.
Such clarity is a common trait of probabilistic models, but a substantial improvement over fact nders, where the closest analog to priors is typically the number of  votes  each claim is initialized with; further, fact nders in general have few, if any, other tunable parameters that can be adjusted, and where present (like the Investment fact nder s  growth rate  value [13]) they tend to be both ad hoc and opaque it is rarely possible to anticipate what values are suitable for a particular problem before evaluating them on labeled data.
LCA models are also much simpler to modify on a more substantial level: there is a straightforward path from a  generative story  about why sources assert the claims that they do to the joint distribution, and augmenting this core (e.g.
to incorporate the idea that observed attributes of the sources, like academic degrees, in u-ence their credibility) is as simple as  nding a product across several independent components.
Even in experiments ig-1009norant of such factors and using the fact nders  standard unsupervised setting, LCA models substantially outperform fact nders in establishing the credibility of city population, book authorship, stock predictions, and predictions of the Supreme Court of the United States.
Perhaps surprisingly, this needn t come at an exorbitant cost: two of our models scale linearly, as fact nders do, and the remaining two, while not linear time, nonetheless proved tractable even over relatively large (tens of thousands of sources and claims) datasets in our experiments.
In the remainder of this paper we  rst provide a more detailed description of fact nders.
We subsequently discuss the fundamentals of LCA before introducing, in order of increasing sophistication, four speci c LCA models: Sim-pleLCA, GuessLCA, MistakeLCA, and LieLCA, and then explore the performance of these models in comparison to fact nders in our experiments.
A fact nder takes as its input a list of assertions of the form  source s asserts claim c  and a list of disjoint mutual exclusion sets of claims [14].
Exactly one of the claims in each mutual exclusion set is true, and this is what the fact nder endeavors to identify.
This is done via an iterative transitive voting system: starting from some initial belief score in all the claims, the algorithm calculates the trustworthiness of each information source (e.g.
a Wikipedia editor, a  nancial analyst, a website, a classi er, etc.)
based on the claims it makes, and then in turn calculates the belief of the claims based on the trustworthiness of the sources asserting it; this process then repeats for a  xed number of iterations or until convergence.
claims, T i(s) =(cid:80) asserting it, Bi(c) = (cid:80) Fact nders are di erentiated by their various update rules, whereby the trustworthiness of sources and belief in claims is calculated.
For example, the  Sums  fact nder is derived from Hubs and Authorities [9], where source trustworthiness can be considered the  hub  score and claim belief the  authority  score; at each iteration i we calculate the trustworthiness of each source as the sum of the belief in its c:s c Bi 1(c), and then the belief score of each claim as the sum of the trustworthiness of the sources s:s c T i(s).
Of course, fact nders can be considerably more complex and varied; in the Investment and PooledInvestment [13] algorithms, sources  invest  their credibility in the claims they make, and claim belief is then non-linearly grown and apportioned back to the sources based on the size of their  investment .
Several fact nders have probabilistic elements.
TruthFinder [19] calculates claim belief as 1  (cid:81) s:s c 1   T (s), with the idea that T (s) is the probability that s tells the truth, so the probability that a claim is wrong is the probability that all the (independent) sources are liars.
However, these semantics are problematic: the pseudoprobabilities over all the claims in a mutual exclusion set will not sum to 1 and cannot be readily normalized since the trustworthiness of a source is calculated as the arithmetic mean of those claims it makes.
[17] explicitly seeks to create a fact nder with an (approximate) Bayesian justi cation, but relies on substantial assumptions, the most important being that P (s   c|T rue(c))   P (s   c), i.e. the probability a source asserts a claim is independent of the truth of that claim (which does not hold in practice).
[21] is something of an anomaly, as it, like Latent Credibility Analysis, models the credibility problem as a graphical model (a Bayesian network), but specializes in situations where the truth is a collection of entities (e.g.
identify all the authors of a book) and the model has the advantage of reasoning about these directly; other approaches (including LCA) instead simply treat these as binary claims (is  John Smith  an author of  Book  or not?).
More importantly, the model makes an implicit assumption (as noted by the authors) that each source is predominately honest, which often does not hold in real data (e.g.
vandalism in Wikipedia).
Additionally, some fact nders have incorporated aspects beyond source trustworthiness and claim belief into their update rules.
3-Estimates [4] adds parameters to attempt to capture the  di culty  of a claim, an idea also present in our LCA models.
Fact nders have also been applied to instances where the claims are not extracted in a prior step but rather snippets of textual  evidence  are e ectively clustered using similarity metrics, as applied by the Apollo system to tweets [10] or to news articles by [16].
AccuVote [3] attempts to identify source dependence (one source copying another) to give greater credence to more  independent  sources, an aspect that is important in certain domains (e.g.
blog postings, which are routinely derivative) and could be incorporated in future LCA models, although we do not consider it here.
Finally, frameworks have been created capable of extending any fact nder.
[13] applies declarative prior knowledge (in the form of  rst-order logic) to fact nders by using linear programming to constrain claim beliefs; in our experiments, we use this method in an extremely simple form to apply supervision to fact nders (our constraints are of the type  claim c is true ), which are otherwise wholly unsupervised algorithms.
For LCA models, declarative constraints may be enforced by one of several methods for constraining the posterior distributions of probabilistic models, such as Posterior Regularization [5] or Constraint Driven Learning [1].
Further, [14] introduces generalized fact nders, which adapt the bipartite unweighted graphs of standard fact nders to weighted, k-partite graphs, allowing such factors as source features (e.g.
 source s has a doctorate in a relevant  eld ) and uncertainty in information extraction to be incorporated, essentially changing how votes  ow throughout the network.
LCA models naturally support these forms of prior knowledge and data in a principled way, as we will discuss shortly, and can incorporate many others (such as priors over the honesty of sources and real-valued features) that generalized fact nders cannot.
A Latent Credibility Analysis model is a probabilistic model in Alaska ).
Note that  s, (cid:80) where the true claim c in each mutual exclusion set of claims m is a (multinomial) latent variable, ym.
An observed assertion is the probability of c as claimed by s, bs,c, typically {0, 1} (e.g.
 John claims Obama was born in Hawaii ), but distributional claims are also possible (e.g.
 John is 95% certain Obama was born in Hawaii and 5% certain he was born c m bs,c = 1.
Every source s also has a [0, ) con dence in his assertions over the claims in m, ws,m, again typically {0, 1} (0 if the source makes no assertion about m, 1 if it does), but other values may be used to express degrees of con dence with straightforward seman-
s c m ym bs,c ws,m Hs Dg/m/s








   Examples / De nition An information source Amazon.com; Dan Rather A claim President Barack Obama born in 1953 A mutually exclusive (ME) set of claims Claimed Birth Years of Barack Obama The true claim in m President Barack Obama was born in 1961 The (observed) probability of c asserted by s
 [0, ) con dence of s in the distribution asserted over all c   m 0; 1; 4.5
 The probability s makes an honest, accurate assertion
 The probability s knows ym (global, per-ME set, or per-source) = {s} Set of all sources s = {c} Set of all claims c = {m} Set of all mutual exclusion sets m = {bs,m} |S| |C| matrix of all observed assertions b = {m} |S| |M| matrix of all assertion con dences w = {ym : m   M} Set of all true claims
 Set of all latent true claims
 Set of all observed true claims (labels) = B   { all other features } Set of all observations (including B) e.g.
{Hs : s   S}   {Dm : m   M} Set of all latent model parameters Table 1: LCA Notation tics: as can be seen from the joint distributions of our LCA models, a ws,m of 0.5 causes assertions made by s about claims in m to a ect the log-likelihood only half as much as sources with ws,m = 1, and ws,m = 2 is equivalent to making the same assertions twice.
This can be useful if, for example, a source expresses abundant or reduced con dence in his assertion, e.g.
 John is 50% con dent that Obama was born in Hawaii with 95% probability... , comparable in function and purpose to belief and plausibility in Dempster-Shafer theory [20, 15] and uncertainty in subjective logic [8,
 Since we are not interested in modeling why a source decides to make an assertion about the claims in a mutual exclusion set (and with what con dence), the con dence matrix W = {ws,m} is taken as a given constant rather than an observation.
Our observations are the assertion matrix B = {bs,c}, together with whatever observed features (such as attributes of the sources) are relevant to the particular model; we will collectively refer to these observed variables as X.
Similarly, we will refer to our latent variables as Y = {ym}, and the model parameters (in the models we describe later these include the honesty of each source and the  di culty factor  of identifying the true claim) as  . Finally, when we write the joint probabilities, we assume all mutual exclusion sets contain at least two claims; this is a notational convenience, since any uncontested claim must be true (there is no alternative) and the probability of a source asserting it is thus 1 and it does not a ect the joint probability.
As an example, consider a problem with two mutual exclusion sets, mp = Obama s Birthplace  and md = Obama s Birthdate , where we observe a source sj =  John  make a single assertion ch =  Obama was born in Hawaii .
Then bsj ,ch = 1,  c mp\ch bsj ,c = 0, wsj ,mp = 1, and wsj ,md = 0 (rendering the values of {bsj ,c : c   md} irrelevant).
Latent variables ymp and ymd are Obama s true birthplace and birthdate, respectively, so ymp =  Hawaii  and ymd =  August 4th, 1961 .
Information credibility problems can be classed as unsupervised or semi-supervised; in the unsupervised case, we are only given observations X and none of the ym are known, so YU = Y and YL =   (YU and YL are the sets of unlabeled [latent] and labeled [observed] true claims, respectively).
Alternatively, when semi-supervised, we know the true claims in some mutual exclusion sets, YL   Y , already and only need to determine the remaining YU = Y \ YL.
In both cases, our goal is to infer:
 (cid:82) (cid:80) (cid:82)   P (YU , YL, X| )P ( )   P (YU , YL, X| )P ( )
 This is the distribution over the possible true claims for each mutual exclusion set where the true claim is not already known, given the observations and true claims already iden-ti ed.
In our experiments we solve this approximately, by using EM [2] to  nd the maximum a posteriori (MAP) point estimate of the parameters,   = argmax  P (X| )P ( ), and then simply calculating: P (YU|X, YL,     ) = (cid:80) P (YU , X, YL| ) P (YU , X, YL| )
 The expectation and maximization update rules used to  nd the maximum a posteriori point estimate   are: Expectation   Step :  m : P (ym = c|X,  t) = (cid:80) P (ym = c, X| t) v m P (ym = v, X| t) Maximization   Step :  t+1 = argmax   EY |X, t [log(P (X, Y | )P ( ))] In LCA models, the E-step is always easy, since the ym values are independent given the observations X and the parameters  t at iteration t. The M-step can be more di cult:
 immediately derive a joint distribution over ym and X: P (ym, X|Hs) |m| 1 .
From this intuitive idea, we can  (Hs)bs,ym (cid:89) (cid:32) (cid:18) 1   Hs c m\ym  ws,m (cid:19)bs,c (cid:18) 1   Hs (cid:19)(1 bs,ym )(cid:33)ws,m |m| 1 = P (ym) = P (ym) (Hs)bs,ym |m| 1 by noting that(cid:80) Here, P (ym) is our prior probability of ym being the true claim in m, and ws,m will be 1 if the source asserts (with full certainty) a claim in m, or 0 if the source says nothing about m. In the second equation we have simpli ed the expression bs,c = 1   bs,ym .
Observing that all sources make their assertions independently and taking   = {Hs} we can write the full joint as: c m bs,c = 1, so(cid:80) (cid:32) (cid:18) 1   Hs (cid:19)(1 bs,ym )(cid:33)ws,m P (Y , X| ) =(cid:89) P (ym) (Hs)bs,ym (cid:89) c m\ym |m| 1 m s The expected log-likelihood maximized in the M-step is then EY |X, t [log(P (X, Y | )P ( ))] = P (Y |X,  t) log (cid:88) log(P ( )) + (cid:18) 1   Hs |m| 1 P (ym) (cid:32)(cid:89) (cid:19)(1 bs,ym )(cid:33)ws,m(cid:33) (cid:32) m = log(P ( )) + P (ym|X,  t) log(P (ym))  (cid:89) s (Hs)bs,ym (cid:32) (cid:88) (cid:18) m
 (cid:88) ym (cid:88) (cid:18) 1   Hs (cid:19)(cid:19)(cid:33) s + ws,m bs,ym log(Hs)+(1 bs,ym ) log |m| 1 Finding the derivative with respect to each Hs    ,    Hs EY |X, t [log(P (X, Y | )P ( ))] =  P (Hs)  1 P (Hs) (cid:80) (cid:80) m ym  Hs + P (ym|X,  t)ws,m(bs,ym   Hs) Hs   (Hs)2 Now we can maximize each Hs independently in our M-step using gradient ascent to  nd the new, maximizing  t+1.
However, when the priors P (Hs) are uniform (so  P (Hs) = 0), the gradient simpli es, allowing us to set it to 0 and solve the resulting equation explicitly for the new maximizing value of Hs at the stationary point:  Hs (cid:80) (cid:80) Hs = m ym (cid:80) P (ym|X,  t)ws,mbs,ym m ws,m As we would intuitively expect, we thus estimate the honesty of a source, that is, the probability that it provides the true claim, as essentially the expected proportion of true claims made by the source given our current parameters.
Figure 1: A plate diagram of a basic SimpleLCA model with observed assertions as the sole features (X = B).
in SimpleLCA,  t+1 can be calculated in closed form provided that P ( ) is uniform; otherwise, gradient ascent must be used.
Where this can be done parameter-by-parameter, the time required for the M-step scales linearly in the number of parameters; in MistakeLCA and LieLCA, joint gradient ascent requires a number of steps increasing linearly in the number of dimensions [12] (since the Lipschitz constant and squared diameter both increase linearly) while the cost to compute the gradient and the function value also increase linearly (provided the number of assertions per source and claims per mutual exclusion set remains constant), yielding O(| |2) complexity.
However, even on our largest experiments, MistakeLCA and LieLCA took no more than  
 than suggested by this worst case quadratic bound.
Exact runtimes varied, but for concreteness LieLCA took approximately 20 minutes on the population dataset, 30 minutes on the stock dataset (per time interval), and from 25-80 minutes on the books dataset (single-threaded on a 3GHz Core
 one minute, and 3-4 minutes, respectively.
SimpleLCA, as with all our models, is a joint distribution that re ects a  story  of how sources decide which claims to assert.
For both this and subsequent LCA models, we assume that each bs,c   {0, 1} and each ws,m   {0, 1}; this matches our experimental domains (where sources assert a single claim in a mutual exclusion set with full certainty) and simpli es the equations for the joints by avoiding a cumbersome normalization factor.
If these assumptions are relaxed, the joint  distributions  as written will no longer be distributions and must be normalized.
In SimpleLCA, each source s has a probability of being honest, Hs.
A source then decides to assert the true claim c in mutual exclusion set m with probability Hs; otherwise, it chooses uniformly at random from the other claims in m m 2Ms 2Sws,mbs,cc 2mymHs1012This closed form update rule also means that SimpleLCA with uniform honesty priors is as fast as fact nders in practice, making it extremely scalable.
When alternative priors are used, gradient ascent requires about twice as much time per EM iteration, but even on our largest datasets this was a matter of seconds.
SimpleLCA is indeed quite simple.
But it s also clear that, for sources, identifying the truth in some mutual exclusion sets is much harder than in others; for example, a source who merely guessed randomly would be assigned an honesty of
 sets of size 2, and 0.25 if size 4.
In GuessLCA, a source has a probability of knowing and telling the truth, Hs.
Thus, with probability Hs, it asserts the true claim.
However, with probability 1  Hs, it guesses c m Pg(c|s) = 1).
claim c with probability Pg(c|s) (where(cid:80) This gives us the joint probability: P (X, Y | ) = (cid:89) P (ym)(cid:89) (cid:89) m s c m\ym (Hs + (1   Hs)Pg(ym|s))bs,ym ws,m ((1   Hs)Pg(c|s))bs,cws,m ing it; (cid:80) (cid:81) This joint can be easily understood by considering the marginal case for each m   M ; the probability that the source asserts the true claim (bs,ym = 1) is then just Hs + (1  Hs)Pg(ym|s), the probability of knowing the truth plus the chance of not knowing the truth and (fortunately) guess-c m bs,c = 1    c(cid:54)=ym bs,c = 0, so the product (.
.
.
)bs,c = 1 is moot.
Conversely, the probability of asserting an untrue claim (bs,c(cid:54)=ym = 1) can be similarly found as the probability of not knowing the truth and guessing c, (1   Hs)Pg(c|s).
c m\ym Omitting the intermediate steps for brevity, we  nd that the gradient of the expected log-likelihood with respect to Hs simpli es to    Hs EY |X, t [log(P (X, Y | )P ( ))] =  P (Hs)  1 P (Hs) (cid:88) (cid:88)  Hs +   m ym   P (ym|X,  t)ws,m bs,ym Hs + Pg (ym|s)
 bs,ym   1
 +   Like SimpleLCA, the gradient with respect to each Hs is independent of the other parameters   \ Hs, allowing us to maximize the expected log-likelihood in the M-step using gradient ascent parameter-by-parameter, which is very fast in practice.
The guess distribution Pg(c|s) is provided to the model as a prior; we could, for example, set Pg(c|s) to the distribution of sources asserting the claims in m under the assumption that a guessing source chooses randomly according to the distribution of  votes  it observes at the time.
This mitigates sources becoming trusted by asserting obvious or well-known claims: the assessed probability of guessing these will then be high (because a large majority of sources already assert them, and we assume that guessers tend to go with the crowd), so the model is free to set Hs low as the observation can be e ectively explained away by (1  Hs)Pg(ym|s); conversely, a source asserting a true claim with a low probability of being guessed will be attributed to a high Hs.
GuessLCA thus rewards getting hard claims right and penalizes getting easy claims wrong.
GuessLCA does require that this  di culty  information be provided a priori rather than learned by the model, and while in most domains the distribution of guesses is easy to approximate (e.g.
if the sources tend to guess with the crowd, probably the most prevalent behavior in practice, we can use the distribution of the number of assertions made by other sources for each alternative within the mutual exclusion set, and if the sources are believed to guess randomly we use a uniform prior over the possibilities) this cannot capture the latent di culty implied by, for example, the disagreement of two highly honest sources (since honesty itself is latent).
More signi cantly, the model assumes that no source will do worse than guessing even if Hs = 0, a source still has a Pg(c|s) probability of guessing the correct claim c.
This assumption is violated when sources are systematically wrong.
This may be due to intentional deception, or, more commonly, a recurring mistake: for example, there are multiple ways of de ning the population of a city (metro area, city limits, etc.)
and some Wikipedia editors consistently use de nitions that disagree with the  truth  (census data).
To overcome these problems, MistakeLCA models di -culty explicitly, as the probability of an honest source making a mistake.
For a source to assert the true claim it must both intend to tell the truth with probability Hs and must know what the truth is with probability D. D may be global (in which case all sources have probability Dg of knowing the truth across all mutual exclusion sets) or tied to each mutual exclusion set (in which case sources have probability Dm of knowing the truth in a particular mutual exclusion set); this results in two variants of the model, which we will refer to as MistakeLCAg and MistakeLCAm.
A source thus asserts the true claim c with probability HsD, but otherwise, with probability 1   HsD, chooses another claim c   m \ c according to Pe(c|c, s).
Recall that, in GuessLCA, our guessing probability Pg was not conditioned on the true claim, but Pe speci es the distribution of mistakes a source will make given that c is true, with Pe(c|c, s) = 0.
Like Pg, Pe is provided as a prior, but conditioning on the true claim means that it can also encode very useful information about similar or easily confused claims; for example, if there are three claims about a person s age, 35, 45, and 46, Pe(45|46, s) and Pe(46|45, s) would both be high.
The joint probability is given by: P (X, Y | ) = (cid:89) P (ym)(cid:89) (cid:89) m s c m\ym (HsD)bs,ym ws,m (Pe(c|ym, s)(1   HsD))bs,cws,m
  (.
.
.)
 Hs =  (.
.
.)
 Dm =  P (Hs) (cid:88)  Hs (cid:88) + m ym  1 P (Hs) P (ym|X,  t)ws,m  P (Dm) (cid:88)  Dm (cid:88) + s ym  1 P (Dm) P (ym|X,  t)ws,m (cid:18) bs,ym   DmHs Hs   DmH 2 s (cid:18) bs,ym   DmHs Dm   D2 mHs (cid:19) (cid:19) The gradient for Dg is identical, except that we sum over all mutual exclusion sets as well as all sources.
Since all Hs are linked by D, we must optimize all parameters jointly in the M-step.
MistakeLCA makes no distinction between intentional lies caused by a lack of honesty and  honest mistakes  that occur with probability (1 D); we can imagine that the former case is governed by a distribution over possible lies, whereas the latter results in guessing.
In LieLCA, a source asserts the true claim c if it it is both honest and knows the answer (with probability HsD).
A dishonest source who knows the truth, however, chooses a lie c with probability (1  H)DPl(c|c, s), where Pl is the distribution over possible lies given the truth (Pl(c|c, s) = 0).
Finally, any source who does not know the truth guesses a claim c with probability (1  D)Pg(c|s).
The D parameters may be per-source, per-mutual exclusion set, or global, resulting in LieLCAs, LieLCAm, and LieLCAg variants.
The joint probability is thus: P (X, Y | ) = (cid:89) P (ym)(cid:89) (cid:89) m s c m\ym (HsD + (1   D)Pg(ym|s))bs,ym ws,m ((1   Hs)DPl(c|ym, s) + (1   D)Pg(c|s))bs,cws,m The gradients of the expected log-likelihood with respect to Hs and D can be found as:  (.
.
.)
 Hs + = (cid:88) (cid:88)   (cid:88) ym m c m\ym  P (Hs)  1 P (Hs)  Hs P (ym|X,  t)ws,m   bs,ym D (1   D)Pg(ym|s) + DHs bs,ym DPl(c|ym, s) (1   D)Pg(c|s) + D(1   Hs)Pl(c|ym, s)  (.
.
.)
 Dg = (cid:88) +  P (Dg)  1 P (Dg)  Dg P (ym|X,  t)ws,m   bs,ym (Hs   Pg(ym|s)) HsDg + (1   Dg)Pg(ym|s) bs,c((1   Hs)Pl(c|ym, s)   Pg(c|s)) (1   Dg)Pg(c|s) + Dg(1   Hs)Pl(c|ym, s) (cid:88) (cid:88) ym m,s + c m\ym (cid:80) m,s is replaced by (cid:80) s and (cid:80) Again, the gradients for Dm and Ds are identical, except m, respectively.
It is interesting to note that LieLCAs is a special case since each pair of (Hs, Ds) parameters may be optimized independently of the others, with the same linearly scaling complexity as Sim-pleLCA and GuessLCA; otherwise, like MistakeLCA, the parameters must be optimized jointly.
It is important to note that we are abusing language somewhat here; in LiarLCA, a  lie  is an intentional, incorrect assertion by a source who knows the truth, but it need not imply malice or an intent to deceive.
A Wikipedia editor who (perhaps out of ignorance) accurately lists the population of cities by their greater metro area rather than by their city limits when the latter is held to be the true measure would not normally be considered a liar, even though the model considers their assertions to be  lies  (and in this particular case those  lies  may be quite informative since we know they will be drawn from values strictly greater than the true population such that Pl(c|c, s) > 0 i  c   c).
Given that we have presented a series of increasingly complex models it might be tempting to think of these hierarchically along the lines of SimpleLCA   GuessLCA   MistakeLCA   LieLCA.
However, this is incorrect: it is easy to see that there are some worlds that SimpleLCA can model (a source with an honesty of 0 who always asserts the wrong claim) that, for example, GuessLCA cannot (at worst a source will still sometimes guess the truth).
We can similarly observe that the Hs parameters have subtly di erent meanings in each model: in SimpleLCA, it is simply the probability that a source asserts the correct claim; in GuessLCA, it is the probability that it both knows and asserts it; and in MistakeLCA and LieLCA, it is the probability the source intends to tell the truth.
Such distinctions are of practical importance: because each model tells a different story with di erent semantics, we should not expect, for instance, that the more sophisticated LieLCA will necessarily outperform the SimpleLCA model given su cient data (as we might if SimpleLCA were indeed subsumed by LieLCA); rather, we expect that relative performance will depend on which model more closely re ects the actual behavior of sources within a particular domain.
That said, our experiments showed that, indeed, some models appear to be more plausible than others, and the more complex models are vulnerable to over tting: in particular, GuessLCA performs substantially better than SimpleLCA overall and is competitive with MistakeLCA and LieLCA, especially where these models over t (e.g.
on the stocks dataset).
A key bene t of LCA is its  exibility and transparency relative to fact nders.
Bayesian priors over the parameters, claims, and other phenomena (such as the mistake distribution, Pe) provide a straightforward way of encoding domain knowledge, but many extensions are also possible.
The modularity of LCA can be illustrated by an example: consider a case where we have features Xf (such as the quality of a source s website, his academic degrees, years of experience, etc.)
associated with the credibility of our sources.
By assuming that these features are independent from the     1014sources  assertions given their credibility, we can create a new model by simply concatenating two joint distributions: P (X, Y | ) = PLCA(Xb, Y | )Pf (Xf| ), where PLCA(Xb, Y | ) is an LCA model over observed assertions Xb and Pf (Xf| ) is the probability of observing features Xf given the credibility of the sources (captured by parameters  ).
Additionally, LCA models (and fact nders) will normally only give credibility to claims that are known to exist and asserted by at least one source (an unknown alternative obviously cannot be explicitly considered in the set of possibilities m, and the models infer a distribution over the possible values of ym   m).
However, we can easily create a new  none of the above  claim u and assign it a prior probability P (u); believing one of the known, asserted claims will then depend on the evidence outweighing our prior inclination towards doubt.
We evaluate our models on two unsupervised datasets, book authorship [19] and city populations [13], and two semi-supervised datasets, stock predictions and U.S. Supreme Court decision predictions1.
Our evaluation compares our four basic LCA models with several top-performing fact nders found in the literature: TruthFinder [19], Investment, PooledInvestment, and Average-Log [13], Sums [9], 3-Estimates [4], as well as simple voting (choose the claim with the most sources asserting it).
For Investment and Pooled-Investment we used the same values for g as [13], 1.2 and
 LCA) until convergence (within 50 iterations in our experiments).
Additionally, we supplement our real-world experiments with synthetic data from sampled from SimpleLCA joint distributions to more carefully analyze the relative performance of the LCA models in a controlled context.
The books dataset [19] is a collection of 14,287 claims of the authorship of various books by 894 websites, with an evaluation set of 605 true claims collected by examining the books  covers.
We used uniform priors for the parameters P ( ).
For for the claim priors P (c) and guess priors Pg(c|s) we used  voted  priors corresponding to the distribution of sources asserting each claim relative to the number (cid:80) of sources asserting any claim within the mutual exclusion |{t:wt,m=bt,c=1}| v m|{t:wt,m=bt,v =1}| .
Finally, the mistake and lie pri-set: ors Pe(c|c, s) were also  voted , computed as Pl(c|c, s) = (cid:80) |{t:wt,m=bt,c=1}| v m\c|{t:wt,m=bt,v =1}| [for c (cid:54)= c]); this is the proportion of sources asserting c relative to the total number of sources asserting any claim in m other than c. For simplicity, the distributions are the same for all sources s. For LieLCAs, LieLCAm, and MistakeLCAm, the Ds or Dm parameters in the model are much more variable than a single global Dg (which tends to be high), resulting in greater emphasis on the voted Pe priors and making voted claim priors P (c) effectively redundant; to correct this, we instead use uniform claim priors on these models.
The results are shown in Table 2; we calculate con dence intervals with the simplifying assumption that the predic-
datasets are available at http://lotho.cs.illinois.edu/data/ Unfortunately, we are unable to release the stock predictions data due to licensing restrictions.
tion over each mutual exclusion set is independent from the others.
The only fact nder to do better than any of the LCA models is PooledInvestment, still more than 3% below LieLCAs.
The LieLCAs generative story  ts especially well with what we know about online booksellers a priori: some sources will consistently corrupt, abbreviate or omit authors names (in other words, they consistently  lie  with a low Hs), while others  guess  by copying prevailing sources since they tend not to research the information themselves (low Ds).
The population dataset [13] contains 44,761 claims about the population of a city in a speci c year made by 171,171 Wikipedia editors in infoboxes, with an evaluation set of 274 true claims identi ed from U.S. census data.
Our evaluation set is marginally smaller than [13] because when an editor made multiple claims about the population of a city in the same year, we kept only the most recent edit and discarded the rest; this resulted in some true claims becoming uncontested and thus eliminated from the evaluation set.
Our priors remained the same as before, except that the claim priors followed the distribution of the number of revisions a claim was present in, rather than the number of sources asserting it, as per [13].
Additionally, we noticed that some models could achieve better results if we knew exactly when to stop them prior to convergence (which is not possible given the unsupervised setting); Investment is the most extreme example of this, as at 20 iterations its accuracy is 86.86%, but it ultimately converges to 75.55%.
There is a wide variance in the the cities in this dataset; some, like Ventura, California are relatively contentious (49 edits asserted a population of 105,000 in 2006, while 68 asserted 106,744), while in others things are more lopsided (in Spring eld the split was 202 edits vs. 10).
As a consequence, some cities can be considered much  harder  than others, since an overwhelming majority for one option over the others means both that the answer is well-known and that an editor needs only follow the crowd to identify it.
Given this, we would expect those models that are capable of capturing this variable di culty to perform the best, and this matches our experiments exactly: GuessLCA (which attributes greater honesty [Hs] to sources that assert true but hard-to-guess claims and less to those that assert false, easy-to-guess claims) and LieLCAm and MistakeLCAm (which model the variable di culty of each city directly with Dm parameters) are the best performing among the LCA models.
TruthFinder also does quite well, but the opaque nature of fact nders precludes an explanation why, or a prediction of the domains where it might similarly perform well in the future.
LieLCAm s top performance, however, is a result of having both Dm parameters to model latent di culty (e.g.
as demonstrated by incorrect assertions by highly honest sources) and guessing priors to incorporate the more obvious situations of lopsided and even votes where the di culty is apparent even without having an estimate of the honesty of the sources involved.
We took the set of stocks that were in the S&P 500 Index on January 1st, 2000 (the index changes composition over time) and followed them through February 1st, 2012.
Our results average predictive accuracy across 10 dates, at July
 Voting Sums
 TruthFinder Average-Log Investment PooledInvestment SimpleLCA GuessLCA MistakeLCAg MistakeLCAm LieLCAg LieLCAm LieLCAs Books Unsupervised













 Populations Unsupervised













 Stocks Semi-Supervised













 Supreme Court Semi-Supervised













 Values are percent accuracy (proportion of true claims correctly identi ed) and 95% con dence interval.
The best LCA models outperform the best fact nders with statistical signi cance in the Books, Stocks and Supreme Court datasets.
Table 2: Experimental Results (N/A: Not Available).
each of these dates is the present time and interpret stock analysts  buy or sell predictions as claims about whether each stock will yield a return higher or lower than the baseline S&P 500 return over the next 60 days.
For example, when we pretend that the date is July 1st, 2011 and are considering Microsoft stock we know the buy or sell recommendations analysts have made over the previous two weeks (in late June), and the latent truth we seek to identify is, of course, whether or not the stock will actually outperform the S&P 500 over the next 60 days.
As a technical detail, stocks are assumed to be bought piecemeal over a week, starting on the subsequent day, and then sold piecemeal over a week, starting 60 days later (this reduces the day-to-day price variance).
At each of these dates, we also know which recommendations analysts made more 60 days ago were proven true, and this observed truth of whether each stock went up or down is our labeled data.
Similarly, the remainder of the predictions (those recommendations made in the last 60 days) are e ectively unlabeled data, since we do not know if they will be proven true yet.
In total, there are approximately 4K distinct analysts and 80K distinct stock predictions, and our evaluation set consists of 560 true claims about stocks where analysts disagreed.
One thing we can quickly observe is that analysts are, in fact, usually wrong, as re ected by the 47.14% accuracy of voting.
We therefore used uniform claim priors, which are a better alternative to the voted priors of our previous experiments; all other priors remain the same.
Given the di culty of the problem (as the oft-cited e cient market hypothesis that consistent risk-adjusted returns relative to the market are impossible would suggest [11]) we would expect no analyst to be especially good (otherwise they would presumably be running a hedge fund) nor any stock to be especially easy to predict; modeling these features, then, would o er little bene t but substantial risk of over tting, as we observe in LieLCAm, MistakeLCAm, and LieLCAs, the three lowest-performing LCA models.
Conversely, LieLCAg, balancing the overall di culty of stock prediction with each source s ability (captured by Hs), does the best (Dg essentially serves as a latent, universal cap on how accurate any analyst can be at the task).
Amusingly, the (aptly-named) Investment is the only fact nder to do better than 50%, although it surpasses only one LCA model (MistakeLCAm).
Given the practical importance of this domain, a natural question to ask is if these models would work in practice as an investment strategy, given the   58% accuracy of LieLCAg.
It is important to observe, however, that we considered only binary outperform and underperform labels and, critically, not how much would have been gained (or lost) on each stock; overall excess return relative to the market as a whole is likely to be minor.
Furthermore, since the market changes over time, there is no guarantee that a strategy that works on historical data would continue to work in the future, nor can we easily quantify this risk (and unexpected, unlikely events can collectively pose a major hazard to any strategy, e.g.
the collapse of Long-Term Capital Management [6]).
Finally, we considered the FantasySCOTUS project; here, 1138 people (largely law students) have made predictions about the outcome of 53 U.S. Supreme Court cases that have already been decided, and 24 that have not been.
Using the same priors as the Books experiment (based on voting), we evaluated with 10-fold cross-validation.
Within each fold, Investment, PooledInvestment, SimpleLCA and GuessLCA were tuned by nested 4-fold cross-validation.
For Investment and PooledInvestment, the growth parameter (from 1 to 2 in increments of .1), was tuned, whereas for SimpleLCA and GuessLCA the parameter priors P (Hs) were tuned over sets of 10 possible Beta distributions.
Since the votes for most cases are nearly tied, we concluded that most sources did little better than guessing, and selected Beta distributions biased toward 0 for GuessLCA (such that the prior on the probability of doing better than guessing is low), and biased towards 1/2 for SimpleLCA (such that the prior probability of asserting the truth is near random).
The other fact nders were not tuned because they lacked tunable parameters; LieLCA and MistakeLCA results are omitted because 1016the experiments were not feasible; 10-fold cross-validation with 4-fold nested cross-validated tuning across 10 possible distributions of the priors of P (Hs) and P (D) is 4000 times as expensive as a normal run (and running a greatly reduced cross-validation regimen with just a few alternative priors for each parameter would underestimate performance relative to our other LCA results).
This is a tradeo  for the greater sophistication of the LieLCA and MistakeLCA models: not only are there an additional set of parameters (the D s) to select priors for, the M-step requires a substantially more expensive optimization (up to about 200 times as expensive as that for SimpleLCA or GuessLCA as previously discussed; a single, normal run of LieLCA on this dataset takes 20-30 minutes).
However, we note that this cross-validated tuning is parallelizable, and a real-world implementation could handle the task by splitting it over a cluster of machines.
In our experimental results, our understanding of the domains allowed us to regularly anticipate which models would be most appropriate: in the books domain, the propensity of di erent booksellers to copy each others  claims ( guessing ) or systematically disagree with the truth ( lying , e.g.
an idiosyncratic way of abbreviating author names) suggested that LieLCAs was the best  t.
For Wikipedia population claims, LieLCAm and MistakeLCAm captured the widely varying di cultly of identifying the true population among the cities.
In predicting stocks we could expect LieLCAm and MistakeLCAm to not work because predicting stocks is more-or-less uniformly challenging across companies and per-company di culty parameters merely worsens the chance of over tting.
Finally, in the Supreme Court domain, we know that historically some sources have been much more accurate than others, but given the even split of votes in most cases it s clear that other sources (a majority) are more-or-less guessing; here we would expect LieLCAm (which models both guessing and varying di culty amongst mutual exclusion sets) to perform best, although it s similarly clear why GuessLCA outperforms SimpleLCA.
However, these are qualitative judgements, and while they certainly help us narrow down the set of potential models, it is not always clear precisely which should be used, particularly when partial supervision is not available to empirically estimate performance; e.g.
in city populations it is not obvious why MistakeLCAm outperforms LieLCAm.
Arguably, since both of these models do well (and are presumably both reasonably good approximations to the collection of highly varied processes that sources really do follow in generating claims) we could acknowledge that either would be a satisfactory choice.
Still, we also wanted brie y investigate the idea of model  t quantitatively, empirically observing how well these models perform given varying quantities of data and a precise knowledge of how the data were really generated (as opposed to real word datasets, where we are left to speculate using our knowledge of the domain).
To do so, we generated data using the SimpleLCA joint distribution with the intent of obtaining a simple underlying process that would allow us to focus on the models  behavior.
We ran two sets of experiments using a SimpleLCA model to generate data; SimpleLCA does not incorporate guessing, mistake or lie prior probabilities, so in the  rst set we give GuessLCA, MistakeLCA and LieLCA uniform probabilities.
In the second set, however, we generate these priors randomly2, with the idea that this will give some insight into the e ect of a poor model choice when mixed with a bad (random and independent of reality) priors.
In each experiment we had 100 sources and 100 mutual exclusion sets, each containing between 2 and 5 claims (selected uniformly at random).
The number of claims made by each source was  xed at 3, 5, 10, or 20, and increasing this e ec-tively increased the amount of data provided to the models.
To mitigate statistical noise, every experiment was repeated 100 times with 100 di erent generated datasets, and the re ported accuracies are an average of those runs (and, within each experiment, the same 100 randomly-generated datasets were used to test each model).
The distribution of Hs was Beta(7, 3); this prior over Hs was used in all models in both experiment sets, despite Hs having somewhat di erent semantics in each model (the intent is to observe performance when the models do not  t the data in a well-understood way).
The results of our synthetic experiments may be found in Table 3.
There are a number of interesting phenomena that we may observe in these results:   Surprisingly, with uniform priors, two of the models (GuessLCA and LieLCAg) consistently outperform Sim-pleLCA on data generated by a SimpleLCA process.
In SimpleLCA, the model tends to conclude that, given a disagreement between sources, one is perfectly honest (Hs = 1) and the other is constantly wrong (Hs = 0).
Other models avoid this with guessing, such that even the worst source can always make a lucky guess, which prevents the model from disregarding their claims entirely.
  With su cient data this over tting is avoided entirely.
  MistakeLCAg versus MistakeLCAm: the latter fares quite poorly in all experiments, while the former does quite well, re ecting a substantial di erence in the models in practice despite a similar joint distribution.
MistakeLCAg s global Dg parameter controls the frequency sources make mistakes, again creating an alternative explanation for a source s error other than complete dishonesty (since some of their inaccuracy will be attributed to  honest mistakes  rather than dishonesty).
  MistakeLCAm, by contrast, has far more freedom to set its 100 Dm parameters to extreme values (over t-ting).
  With randomized priors handicapping the other models, SimpleLCA leads the pack, as expected.
uniformly for each claim and then normalizing over the mutual exclusion set for Pg(c|s) and normalizing over the claims in the mutual set excluding ym for Pl(c|ym, s) and Pe(c|ym, s).
This results in a rather complex distribution: for example, given two claims A and B, the probability of a a+b , where a is the value drawn for guessing A is taken as claim A, and b is the value drawn for claim B. Marginalizing out b gives Pg(A) = a(log(a + 1)   log(a))).
Claims per Source
 Uniform

 SimpleLCA GuessLCA MistakeLCAg MistakeLCAm LieLCAg LieLCAm LieLCAs




































 Randomized























 Table 3: Performance of LCA Models with Synthetic Data from a SimpleLCA Process.
Each experiment was run over 100 random datasets and the results averaged.
  With randomized priors, MistakeLCAm su ers from worsening performance as more assertions are made in each mutual exclusion set, increasing the Dm gradients relative to those of Hs and pushing Dm to lower values (it is easier to  explain away  bad assertions by decreasing the Dm for the mutual exclusion set than decreasing the Hs for many sources).
This then places greater weight on the (random) mistake priors.
  The other models prove remarkably robust given their completely incorrect priors, although its clear that this does cap the possible performance of GuessLCA and LieLCAm a bit, whereas MistakeLCAg and LieLCAg can simply set a high Dg, eliminating or reducing their in uence, respectively.
In our real-world data, SimpleLCA was often among the least accurate LCA models; the synthetic results here suggest that, indeed, even in an arti cial best-case scenario other models are able to perform almost as well.
However, SimpleLCA remains easy to implement, easy to understand, and very tractable, and so should not be discounted entirely.
It is also apparent that MistakeLCAm may face severe di culty in some cases; whereas LieLCAm can believe a source will assert the correct claim by guessing even if the Dm parameter for the relevant mutual exclusion set is 0, MistakeLCAm has no  safety valve  of this sort: if Dm is 0, the source must always get the claim wrong (this creates a sort of perverse  anti-vote , whereby the claim with the fewest assertions is likely to be believed).
This danger manifests itself in the high variance we see in the model s real-world performance; while the top performer in the population domain, it is also the lowest performer in the stocks domain.
Care must therefore be taken to ensure that MistakeLCAm is a reasonably good  t to the domain, whereas the other models are much more forgiving.
Our synthetic experiments are limited in scope, but they do inform our approach to real-world problems.
MistakeLCAm can sometimes yield the best results, but LieLCAm has a similar generative story and is a less variable choice that This research was sponsored in part by the Army Research Laboratory (ARL) under agreement W911NF-09-2-
tions are those of the authors and do not necessarily re ect the view of the ARL.
can do well in the same domains without MistakeLCAm s risk of over tting.
A second lesson is that these models can be remarkably resistant to bad priors (when the underlying process generating the data is simple), and uniform priors are a good choice even if the generating process is quite di erent from the model being applied.
GuessLCA in particular does quite well with uniform priors in our synthetic experiments, and, moreover, performs consistently well in the real-world experiments, too.
This consistency is partly due to its simplicity (little danger of over tting) and partly because it manages to at least approximately model the important  di culty  aspect of claims; not as precisely as the more sophisticated LieLCA or MistakeLCA models, of course, but also without their computational cost.
LieLCA and MistakeLCA are, on the other hand, more appropriate where the behavior of sources is well understood (e.g.
the books domain) and where partial supervision can be used to avoid over tting (e.g.
the stocks domain).
Latent Credibility Analysis is a  exible and powerful approach to modeling the information credibility problem; although we have really only begun to explore its potential in our experiments so far, we have nonetheless seen that the performance of LCA models surpasses that of fact nders on both semi-supervised and unsupervised real-world datasets, often substantially.
GuessLCA in particular is promising due to its consistently strong performance and tractability, scaling linearly with the size of the problem as fact nders do, although other, more expressive (and expensive) LCA models can achieve better results when used judiciously.
Future work should extend the LCA framework, capturing phenomena such as source dependency and real-valued claims that will allow it to model an even wider range of domains; for now, however, LCAs are a new approach to credibility that is already both semantically appealing and of substantial practical utility.
