Given a set of skill requirements (called task T ), a set of experts who have expertise in one or more skill, along with a social or professional network of the experts, the team formation problem is to identify a competent and highly collaborative team.
This problem in the context of a social network was  rst introduced by [18] and has attracted recent interest in the data mining community [15, 2, 12].
A closely related and well-studied problem in operations research is the assignment problem.
Here, given a set of agents and a set of tasks, the goal is to  nd an agent-task assignment minimizing the cost of the assignment such that exactly one agent is assigned to a task and every task is assigned to some agent.
This problem can be modeled as a maximum weight matching problem in a weighted bipartite graph.
In contrast to the assignment problem, the team formation problem considers the underlying social network, which for example models the previous collaborations among the experts, while forming teams.
The advantage of using such a social network is that the teams that have worked together previously are expected to have less communication overhead and work more e ectively as a team.
The criteria explored in the literature so far for measuring the e ectiveness of teams are based on the shortest path distances, density, and the cost of the minimum spanning tree of the subgraph induced by the team.
Here the density of a subgraph is de ned as the ratio of the total weight of the edges within the subgraph over the size of the sub-graph.
Teams that are well connected have high density values.
Methods based on minimizing diameter (largest shortest path between any two vertices) or cost of the spanning tree have the main advantage that the teams they yield are always connected (provided the underlying social network is connected).
However, diameter or spanning tree based objectives are not robust to the changes (addition/deletion of edges) in the social network.
As demonstrated in [12] using various performance measures, the density based objective performs better in identifying well connected teams.
On the other hand, maximizing density may give a team whose subgraph is disconnected.
This happens especially when there are small groups of people who are highly connected with each other but are sparsely connected to the rest of the graph.
Existing methods make either strong assumptions on the problem that do not hold in practice or are not capable of incorporating more intuitive constraints such as bounding the total size of the team.
The goal of this paper is to consider 1077the team formation problem in a more realistic setting and present a novel formulation based on a generalization of the densest subgraph problem.
Our formulation allows modeling of many realistic requirements such as (i) inclusion of a designated team leader and/or a group of given experts, (ii) restriction on the size or more generally cost of the team (iii) enforcing locality of the team, e.g., in a geographical sense or social sense, etc.
In fact most of the future directions pointed out by [12] are covered in our formulation.
The  rst work [18] in the team formation problem in the presence of a social network presents greedy algorithms for minimizing the diameter and the cost of the minimum spanning tree (MST) induced by the team.
While the greedy algorithm for minimizing the diameter has an approximation guarantee of two, no guarantee is proven for the MST algorithm.
However, [18] impose the strong assumption that a skill requirement of a task can be ful lled by a single person; thus a more natural requirement such as  at least k experts of skill s are needed for the task  cannot be handled by their method.
This shortcoming has been addressed in [12], which presents a 2-approximation algorithm for a slightly more general problem that can accommodate the above requirement.
However, both algorithms cannot handle an upper bound constraint on the team size.
On the other hand, the solutions obtained by all these algorithms (including the MST algorithm) can be shown to be connected subgraphs if the underlying social graph is connected.
Two new formulations are proposed in [15] based on the shortest path distances between the nodes of the graph.
The  rst formulation assumes that experts from each skill have to communicate with every expert from the other skill and thus minimizes the sum of the pairwise shortest path distances between experts belonging to di erent skills.
They prove that this problem is NP-hard and provide a greedy algorithm with an approximation guarantee of two.
The second formulation, solvable optimally in polynomial time, assumes that there is a designated team leader who has to communicate with every expert in the team and minimizes the sum of the distances only to the leader.
The main shortcoming of this work is its restrictive assumption that exactly one expert is su cient for each skill, which implies that the size of the found teams is always upper bounded by the number of skills in the given task, noting that an expert is allowed to have multiple skills.
They exploit this assumption and (are the  rst to) produce top-k teams that can perform the given task.
However, although based on the shortest path distances, neither of the two formulations does guarantee that the solution obtained is connected.
In contrast to the distance or diameter based cost functions, [12] explore the usefulness of the density based objective in  nding strongly connected teams.
Using various performance measures, the superiority of the density based objective function over the diameter objective is demonstrated.
The setting considered in [12] is the most general one until now but the resulting problem is shown to be NP hard.
The greedy algorithms that they propose have approximation guarantees (of factor 3) for two special cases.
The teams found by their algorithms are often quite large and it is not straightforward to modify their algorithms to integrate an additional upper bound constraint on the team size.
Another disadvantage is that subgraphs that maximize the density under the given constraints need not necessarily be connected.
Recently [2] considered an online team formation problem where tasks arrive in a sequential manner and teams have to be formed minimizing the (maximum) load on any expert across the tasks while bounding the coordination cost (a free parameter) within a team for any given task.
Approximation algorithms are provided for two variants of coordinate costs: diameter cost and Steiner cost (cost of the minimum Steiner tree where the team members are the terminal nodes).
While this work focusses more on the load balancing aspect, it also makes the strong assumption that a skill is covered by the team if there exists at least one expert having that skill.
All of the above methods allow only binary skill level, i.e., an expert has a skill level of either one or zero.
We point out that many methods have been developed in the operations research community for the team formation problem, [5, 9, 21, 20], but none of them explicitly considers the underlying social or professional connections among the experts.
There is also literature discussing the social aspects of the team formation [10] and their in uence on the evolution of communities, e.g., [4].
Now we formally de ne the Team Formation problem that we address in this paper.
Let V be the set of n experts and G(V, W ) be the weighted, undirected graph re ecting the relationship or previous collaboration of the experts V .
Then non-negative, symmetric weight wij   W connecting two experts i and j re ects the level of compatibility between them.
The set of skills is given by A = {a1, .
.
.
, ap}.
Each expert is assumed to possess one or more skills.
The non-negative matrix M   Rn p speci es the skill levels of all experts in each skill.
Note that we de ne the skill level on a continuous scale.
If an expert i does not have skill j, then Mij = 0.
Moreover, we use the notation Mj   Rn 1 for the j th column of M , i.e. the vector of skill levels corresponding to skill j.
A task T is given by the set of triples {(aj,  j,  j)}p j=1, where aj   A, specifying that at least  j and at most  j of skill aj is required to  nish the given task.
Generalized team formation problem.
Given a task T , the generalized team formation problem is de ned as  nding a team C   V of experts maximizing the collaborative compatibility and satisfying the following constraints:   Inclusion of a speci ed group: a predetermined group of experts S   V should be in C.
  Skill requirement: at least  j and at most  j of skill aj is required to  nish the task T .
  Bound on the team size: the size of the team should be smaller than or equal to b, i.e., |C|   b.
  Budget constraint: total budget for  nishing the task is bounded by B, i.e., Pi C ci   B, where ci   R+ is the cost incurred on expert i.
  Distance based constraint: the distance (measured according to some non-negative, symmetric function, dist) between any pair of experts in C should not be larger than d0, i.e., dist(u, v)   d0,  u, v   C.
trast to existing methods, we also allow an upper bound on each skill and on the total team size.
If the skill matrix is only allowed to be binary as in previous work, this translates into upper and lower bounds on the number of experts required for each skill.
Using vertex weights, we can in fact encode more generic constraints, e.g., having a limit on the total budget of the team.
It is not straightforward to extend existing methods to include any upper bound constraints.
Up to our knowledge we are the  rst to integrate upper bound constraints, in particular on the size of the team, into the team formation problem.
We think that the latter constraint is essential for realistic team formation.
Our general setting also allows a group of experts around whom the team has to be formed.
This constraint often applies as the team leader is usually  xed before forming the team.
Another important generalization is the inclusion of distance constraints for any general distance function1.
Such a constraint can be used to enforce locality of the team e.g.
in a geographical sense (the distance could be travel time) or social sense (distance in the network).
Another potential application are mutual incompatibilities of team members e.g.
on a personal level, which can be addressed by assigning a high distance to experts who are mutually incompatible and thus should not be put together in the same team.
We emphasize that all constraints considered in the literature are special instances of the above constraint set.
Measure of collaborative compatiblity.
In this paper we use as a measure of collaborative compatibility a generalized form of the density of subgraphs, de ned as density(C) := assoc(C) volg(C) = Pi,j C wij Pi C gi , (1) where wij is the non-negative weight of the edge between i and j and volg(C) is de ned as Pi C gi, with gi being the positive weight of the vertex i.
We recover the original density formulation, via gi = 1,  i   V .
We use the relation, assoc(C) = vold(C)   cut(C, V \C), where di = Pn j=1 wij is the degree of vertex i and cut(A, B) := Pi A,j B wij.
Discussion of density based objective.
As pointed out in [12], the density based objective possesses useful properties like strict monotonicity and robustness.
In case of the density based objective, if an edge gets added (because of a new collaboration) or deleted (because of newly found incompatibility) the density of the subgraphs involving this edge necessarily increases resp.
decreases, which is not true for the diameter based objective.
In contrast to density based objective, the impact of small changes in graph structure is more severe in the case of diameter objective [12].
The generalized density that we use here leads to further modeling freedom as it enables to give weights to the experts according to their expertise.
By giving smaller weight to those with high expertise, one can obtain solutions that not only satisfy the given skill requirements but also give preference to the more competent team members (i.e. the ones having smaller weights).
Problem Formulation.
Using the notation introduced above, an instance of the team formation problem based on the generalized density can be formulated as max
 assoc(C) volg(C) subject to : S   C (2)  j   {1, .
.
.
, p}  j   volMj (C)    j, |C|   b volc(C)   B dist(u, v)   d0,  u, v   C, Note that the upper bound constraints on the team size and the budget can be rewritten as skill constraints and can be incorporated into the skill matrix M accordingly.
Thus, without loss of generality, we omit the budget and size constraints from now on, for the sake of brevity.
Moreover, since S is required to be part of the solution, we can assume that dist(u, v)   d0,  u, v   S, otherwise the above problem is infeasible.
The distance constraint also implies that any u   V for which dist(u, s) > d0, for some s   S, cannot be a part of the solution.
Thus, we again assume wlog that there is no such u   V ; otherwise such vertices can be eliminated without changing the solution of problem (2).
Our formulation (2) is a generalized version of the classical densest subgraph problem (DSP), which has many applications in graph analysis, e.g., see [19].
The simplest version of DSP is the problem of  nding a densest subgraph (without any constraints on the solution), which can be solved optimally in polynomial time [13].
The densest-k-subgraph problem, which requires the solution to contain exactly k vertices, is a notoriously hard problem in this class and has been shown not to admit a polynomial time approximation scheme [16].
Recently, it has been shown that the densest subgraph problem with an upper bound on the size is as hard as the densest-k-subgraph problem [17].
However, the densest subgraph problem with a lower bound constraint has a 2-approximation algorithm [17].
It is based on solving a sequence of unconstrained densest subgraph problems.
They also show that there exists a linear programming relaxation for this problem achieving the same approximation guarantee.
Recently [12] considered the following generalized version of the densest subgraph problem with lower bound constraints in the context of team formation problem: max
 assoc(C) volg(C) (3) subject to : volMj (C)    j,  j   {1, .
.
.
, p} where M is the binary skill matrix.
They extend the greedy method of [17] and show that it achieves a 3-approximation guarantee for some special cases of this problem.
[8] recently improved the approximation guarantee of the greedy algorithm of [12] for problem (3) to a factor 2.
The time complexity of this greedy algorithm is O(kn3), where n is j=1 kj is the minimum the number of experts and k := Pm number of experts required.
ity.
Direct integration of subset constraint.
The subset constraint can be integrated into the objective by directly working on the subgraph G  induced by the vertex set V   = 1079max
 assocS(A) volg(A) +  S
 pen(A) := V \S.
Note that any C   V that contains S can be written as C = A   S, for A   V  .
We now reformulate the team formation problem on the subgraph G .
We introduce the notation m = |V  |, and we assume wlog that the  rst m entries of V are the ones in V  .
The terms in problem (2) can be rewritten as assoc(C) = assoc(A) + assoc(S) + 2 cut(A, S), = vold(A)   cut(A, V \A) + assoc(S) + 2 cut(A, S) = vold(A)   cut(A, V  \A) + assoc(S) + cut(A, S) volg(C) = volg(A) + volg(S) Moreover, note that we can write: cut(A, S) = voldS (A), where dS i = Pj S wij denotes the degree of vertex i restricted to the subset S in the original graph.
Using the abbreviations,  S = assoc(S),  S = volg(S), assocS(A) = vold(A) cut(A, V  \A)+ S +voldS (A), we rewrite the team formation problem (2) as subject to : kj   volMj (A)   lj,  j   {1, .
.
.
, p} dist(u, v)   d0,  u, v   A, where for all j = 1, .
.
.
, p, the bounds were updated as kj =  j  volMj (S), lj =  j  volMj (S).
Note that here we already used the assumption: dist(u, s)   d0,  u   V,  s   S.
The constraint, A 6=  , has been introduced for technical reasons required for the formulation of the continuous problem in Section 4.2.
The equivalence of problem (GDSP) to (2) follows by considering either S (if feasible) or the set A    S, where A  is an optimal solution of (GDSP), depending on whichever has higher density.
To the best of our knowledge there is no greedy algorithm with an approximation guarantee to solve problem (GDSP).
Instead of designing a greedy approximation algorithm for this discrete optimization problem, we derive an equivalent continuous optimization problem in Section 4.
That is, we reformulate the discrete problem in continuous space while preserving the optimality of the solutions of the discrete problem.
The rationale behind this approach is that the continuous formulation is more  exible and allows us to choose from a larger set of methods for its solution than for the discrete one.
Although the resulting continuous problem is as hard as the original discrete problem, recent progress in continuous optimization [14] allow us to  nd a locally optimal solution very e ciently.
In this section we present our method, Formation Of Realistic Teams (FORTE, for short) to solve the team formation problem, which is rewritten as (GDSP), using the continuous relaxation.
We derive FORTE in three steps: i.
Derive an equivalent unconstrained discrete problem (4) of the team formation problem (GDSP) via an exact penalty approach.
ii.
Derive an equivalent continuous relaxation (6) of the unconstrained problem (4) by using the concept of Lo-vasz extensions.
iii.
Compute the solution of the continuous problem (6) using the recent method RatioDCA from fractional programming.
A general technique in constrained optimization is to transform the constrained problem into an equivalent unconstrained problem by adding to the objective a penalty term, which is controlled by a parameter     0.
The penalty term is zero if the constraints are satis ed at the given input and strictly positive otherwise.
The choice of the regularization parameter   in uences the tradeo  between satisfying the constraints and having a low objective value.
Large values of   tend to enforce the satisfaction of constraints.
In the following we show that for the team formation problem (GDSP) there exists a value of   that guarantees the satisfaction of all constraints.
Let us de ne the penalty term for constraints of the team formation problem (GDSP) as j=1 max{0, volMj (A)   lj} j=1 max{0, kj   volMj (A)} Pp +Pp +Pu,v A max{0, dist(u, v)   d0} A 6=  
    
 Note that the above penalty function is zero only when A satis es the constraints; otherwise it is strictly positive and increases with increasing infeasibility.
The special treatment of the empty set is again a technicality required later for the Lovasz extensions, see Section 4.2.
For the same reason, we also replace the constant terms  S and  S in (GDSP) by  S unit(A) and  S unit(A) respectively, where unit(A) := 1, A 6=   and unit( ) = 0.
The following theorem shows that there exists an unconstrained problem equivalent to the constrained optimization problem (GDSP).
Theorem 1.
The constrained problem (GDSP) is equivalent to the unconstrained problem min
 volg(A) +  S unit(A) +   pen(A) assocS(A) (4)   volg (A0)+ S for   > vold(V ) assocS (A0) , where A0 is any feasible set of problem (GDSP) such that assocS(A0) > 0 and   is the minimum value of infeasibility, i.e., pen(A)    , if A is infeasible.
assocS (A) Proof.
We de ne spvol(A) := volg (A)+ S unit(A) .
Note that maximizing (GDSP) is the same as minimizing spvol(A) subject to the constraints of (GDSP).
For any feasible subset A, the objective of (4) is equal to spvol(A), since the penalty term is zero.
Thus, if we show that all minimizers of (4) satisfy the constraints then the equivalence follows.
Suppose, for the sake of contradiction, that A (6=  , if S =  ) is a minimizer of (4) and that A  is infeasible for problem (GDSP).
Since  S   0 and gi > 0,  i, we have under the given condition on  , volg(A ) +  S +   pen(A ) assocS(A )     maxA V assocS(A)   >   pen(A ) assocS(A )       vold(V ) > volg(A0) +  S assocS(A0) , which leads to a contradiction because the last term is the objective value of (4) at A0.
good as the one of f .
This holds because We will now derive a tight continuous relaxation of problem (4).
This will lead us to a minimization problem over Rm, which then can be handled more easily than the original discrete problem.
The connection between the discrete and the continuous space is achieved via thresholding.
Given a vector f   Rm, one can de ne the sets RL(f ) = = m 1 Xi=1 Xi=1 m 1 Ai := {j   V |fj   fi} , (5) by thresholding f at the value fi.
In order to go from functions on sets to functions on continuous space, we make use of the concept of Lovasz extensions.
Definition 1.
(Lovasz extension) Let R : 2V   R be a set function with R( ) = 0, and let f   Rm be ordered in ascending order f1   f2           fm.
The Lovasz extension RL : Rm   R of R is de ned by RL(f ) = m 1 Xi=1 R(Ai+1) (fi+1   fi) + R(V )f1.
Note that RL(1A) = R(A) for all A   V , i.e. RL is indeed an extension of R from 2V to RV (|V | = m).
In the following, given a set function R, we will denote its Lovasz extension by RL.
The explicit forms of the Lovasz extensions used in the derivation will be dealt with in Section 4.3.
In the following theorem we show the equivalence for GDSP.
A more general result showing equivalence for fractional set programs can be found in [7].
Theorem 2.
The unconstrained discrete problem (4) is R(Ai+1)(fi+1   fi) + f1R(V  ) R(Ai+1) assocS(Ai+1) +
 assocS(V  ) assocS(Ai+1)(fi+1   fi) assocS(V  )f1   min j=1,...m R(Aj) assocS(Aj) m 1 Xi=1 (cid:16) assocS(Ai+1)(fi+1   fi) + assocS(V  )f1(cid:17) = min j=1,...m R(Aj) assocS(Aj) assocL S (f ) The third step follows from the fact that f is non-negative (f1   0) and ordered in ascending order, i.e., fi+1   fi  
 S (f ) is non-negative, the  nal step implies that RL(f ) assocL S (f )   min j=1,...m R(Aj) assocS(Aj) .
(7) Thus we have RL(f ) assocL S (f ) min f  RV   +   min

 assocS(A) .
From inequality (7), it follows that optimal thresholding of
 f   yields a set that is a minimizer of problem (4).
equivalent to the continuous problem min f  RV   + volL g (f ) +  S unitL(A) +   penL(f ) assocL S (f ) Corollary 1.
The team formation problem (GDSP) is equivalent to the problem (6) if   is chosen according to the condition given in Theorem 1.
(6) Proof.
This directly follows from Theorems 1 and 2.
2 for any     0.
Moreover, optimal thresholding of a mini-mizer f     Rm + ,
 Ai={j V  |f   i }, i=1,...,m min j  f   volg(Ai) +  S +   pen(Ai) assocS(Ai) , yields a set A  that is optimal for problem (4).
Proof.
Let R(A) = volg(A) +  S unit(A) +   pen(A).
Then we have min

 assocS(A) = min

 assocL
   min f  RV   + RL(f ) assocL S (f ) , where in the  rst step we used the fact that RL(f ) and assocL(f ) are extensions of R(A) and assoc(A), respectively.
Below we  rst show that the above inequality also holds in the other direction, which then establishes that the optimum values of both problems are the same.
The proof of the reverse direction will also imply that a set minimizer of the problem (4) can be obtained from any minimizer f   of (6) via optimal thresholding.
While the continuous problem is as hard as the original discrete problem, recent ideas from continuous optimization [14] allow us to derive in the next section an algorithm for obtaining locally optimal solutions very e ciently.
We now describe an algorithm for (approximately) solving the continuous optimization problem (6).
The idea is to make use of the fact that the fractional optimization problem (6) has a special structure: as we will show in this section, it can be written as a special ratio of di erence of convex (d.c.) functions, i.e.
it has the form R1(f )   R2(f ) S1(f )   S2(f ) min f  RV + := Q(f ), (8) where the functions R1, R2, S1 and S2 are positively one-homogeneous convex functions2 and numerator and denominator are nonnegative.
This reformulation then allows us to use a recent  rst order method called RatioDCA [14, 7].
In order to  nd the explicit form of the convex functions, we  rst need to rewrite the penalty term as pen(A) = We  rst show that the optimal thresholding of any f   Rm + yields a set A such that 1A has an objective value at least as
 f ( x) =  f (x),     0.
pen1(A) = Pp pen2(A) = Pp j=1volMj (A) +Pp j=1min{lj,volMj(A)}+Pp  Pu,v Amax{0, dist(u, v)   d0}.
j=1kj unit(A), j=1min{kj,volMj(A)} Using this decomposition of pen(A), we can now write down the functions R1, R2, S1 and S2 as RatioDCA [14, 7], a method for the local minimization of objectives of the form (8) on the whole Rm.
Given an RatioDCA [14] Minimization of a non-negative ratio of one-homogeneous d.c functions over Rm +
 2: repeat
 + ,  0 = Q(f 0) R1(u)+ lS2(u) (cid:10)u, r2(f l) +  ls1(f l)(cid:11) f l+1 =arg min + , kuk2 1 u Rm where r2(f l)    R2(f l), s1(f l)    S1(f l)  l+1 = Q(f l+1) R1(f ) = volL   (f ) +   max i R2(f ) =   penL S1(f ) = volL 2 (f ) {fi}
 d (f ) + volL dS (f ) +  S max i {fi} 5: until | l+1 l|  l <   S2(f ) = cutL(f ).
where   := g +  Pp j=1 Mj,   :=  S +  Pp denotes the Lovasz extension of pen2(A), and j=1 kj, penL 2 (f ) h (f ) = h(hi)m volL cutL(f ) = 1 i=1, f i , where h   Rn, i,j=1 wij |fi   fj|.
Lemma 1.
Using the functions R1, R2, S1 and S2 de ned above, the problem (6) can be rewritten in the form (8).
The functions R1, R2, S1 and S2 are convex and positively one-homogeneous, and R1   R2 and S1   S2 are nonnegative.
Proof.
The denominator of (6) is given as assocL d (f )   cutL(f ) + volL S (f ) = dS (f ) +  S unitL(f ), and the nu-volL merator is given as volL g (f ) +  S unitL(A) +   penL(f ).
Using Prop.2.1 in [3] and the decomposition of pen(A) introduced earlier in this section, we can decompose penL(f ) = penL
 given as penL j=1 kj maxi{fi}, and let penL
 plicit form is not necessary, as shown later in this section).
The equality between (6) and (8) then follows by simple rearranging of the terms.
Mj (f )+Pp

 j=1 volL The nonnegativity of the functions R1   R2 and S1   S2 follows from the nonnegativity of denominator and numerator of (6) and the de nition of the Lovasz extension.
Moreover, the Lovasz extensions of any set function is positively one-homogeneous [3].
Finally, the convexity of R1 and S1 follows as they are a non-negative combination of the convex functions maxi{fi} and h(hi)m i=1, f i for some h   Rn.
The function S2(f ) = cutL(f ) is well-known to be convex [3].
To show the convexity of R2, we will show that the function pen2(A) is sub-modular3.
The convexity then follows from the fact that a set function is submodular if and only if its Lovasz extension is convex [3].
For the proof of the submodularity of the  rst two sums one uses the fact that the pointwise minimum of a constant and a increasing submodular function is again sub-modular.
Writing Duv := max{0, dist(u, v)   d0}, the last sum can be written as  Pu,v A Duv =  Pu A,v V   Duv + Pu A,v V  \A Duv.
Using (dD)i = Pj Dij, we can write its Lovasz extension as   voldD (f ) + 1
 which is a sum of a linear term and a convex term.
The reformulation of the problem in the form (8) enables us to apply a modi cation of the recently proposed

 initialization f0, the above algorithm solves a sequence of convex optimization problems (line 3).
Note that we do not need an explicit description of the terms S1(f ) and R2(f ), but only elements of their sudi erential s1(f )    S1(f ) resp.
r2(f )    R2(f ).
The explicit forms of the subgradients are given in the appendix.
The convex problem (line 3) then has the form min f  Rm +  l
 m Xi,j=1 wij |fi   fj| + hf, ci +   max i {fi}, (9) where c =  r2(f l) ls1(f l).
Note that (9) is a non-smooth problem.
However, there exists an equivalent smooth dual problem, which we give below.
Lemma 2.
The problem (9) is equivalent to min k k 1  ij = ji min v Sm PRm + (cid:18) c    l

 2 (cid:13)(cid:13)(cid:13)(cid:13)
 ,
 A     v(cid:19)(cid:13)(cid:13)(cid:13)(cid:13) denotes the projection on the positive orthant and Sm is the where A : RE 7  RV with (A )i := Pj wij( ij    ji), PRm simplex Sm = {v   Rm | vi   0,Pm Proof.
First we use the homogenity of the objective in the inner problem to eliminate the norm constraint.
This yields the equivalent problem i=1 vi = 1}.
+ min u Rn +   max i ui +

 kuk2 2 + hu, ci +  l
 n Xi,j=1 wij|ui   uj|.
We derive the dual problem as follows:  l
 min u Rn + = min u Rn n Xi,j=1 +n max k k 1  ij = ji wij |ui   uj| +   max ui +

 kuk2 2 + hu, ci  l
 n Xi,j=1 wij (ui   uj)  ij + max v Sn   hu, vi +

 kuk2 2 +(cid:28)u, c + kuk2 2 + hu, cio  l
 A  +  v(cid:29) , = max k k 1  ij = ji min u Rn + v Sn

 where (A )i := Pj wij( ij    ji).
The optimization over
 (x)k2

 u has the solution u = PRn into the objective and using that hPRn we obtain the result.
(x), xi = kPRn ( c    l + + +
 ing recent scalable  rst order methods like FISTA [6], which has a guaranteed convergence rate of O( 1 k2 ), where k is the number of steps done in FISTA.
The main part in the calculation of FISTA consists of a matrix-vector multiplication.
As the social network is typically sparse, this operation costs O(m), where m is the number of non-zeros of W .
RatioDCA [14], produces a strictly decreasing sequence f l, i.e., Q(f l+1) < Q(f l), or terminates.
This is a typical property of fast local methods in non-convex optimization.
Moreover, the convex problem need not be solved to full accuracy; we can terminate the convex problem early, if the current f l produces already su cent descent in Q.
As the number of required steps in the RatioDCA typically ranges between 5-20, the full method scales to large networks.
Note that convergence to the global optimum of (8) cannot be guaranteed due to the non-convex nature of the problem.
However, we have the following quality guarantee for the team formation problem.
Theorem 3.
Let A0 be a feasible set for the problem (GDSP) and   is chosen as in Theorem 1.
Let f   denote the result of RatioDCA after initializing with the vector 1A0 , and let Af   denote the set found by optimal thresholding of f  .
Either RatioDCA terminates after one iteration, or produces Af   which satis es all the constraints of the team formation problem (GDSP) and assocS(Af   ) volg(Af   ) +  S > assocS(A0) volg(A0) +  S .
Proof.
RatioDCA generates a decreasing sequence {f l} such that Q(f l+1) < Q(f l) until it terminates [14].
We have Q(f 1) < Q(1A0 ), if the algorithm does not stop in one step.
As shown in Theorem (2) optimal thresholding of f 1 yields a set Af that achieves smaller objective on the corresponding set function.
Since the chosen value of   guarantees the satisfaction of the constraints, Af has to be feasible.
Recall that our team formation problem based on the density objective is rewritten as the following GDSP after integrating the subset constraint: max
 assocS(A) volg(A) +  S (10) subject to : kj   volMj (A)   lj,  j   {1, .
.
.
, p} dist(u, v)   d0,  u, v   A Note that here we do not require the additional constraint, A 6=  , that we added to (GDSP).
In this section we show that there exists a Linear programming (LP) relaxation for this problem.
The LP relaxation can be solved optimally in polynomial time and provides an upper bound on the optimum value of GDSP.
In practice such an upper bound is useful to check the quality of the solutions found by approximation algorithms.
Theorem 4.
The following LP is a relaxation of the Generalized Densest Subgraph Problem (10).
max t R, f  RV   ,  RE  m Xi,j=1 wij ij + 2 DdS, fE + t S (11) subject to : tkj   hMj, f i   tlj,  j   {1, .
.
.
, p}  u, v : dist(u, v) > d0 fu + fv   t, t   0,  ij   fi,  ij   fj,  (i, j)   E 
 hg, f i + t S = 1.
where V   = V \S, E  is the set of edges induced by V  .
Proof.
The following problem is equivalent to (10), because (i) for every feasible set A of (10), there exist corresponding feasible y, X given by y = 1A, Xij = min{yi, yj}, with the same objective value and (ii) an optimal solution of the following problem always satis es X   j }.
ij = min{y  i , y 
 hg, yi +  S max y {0, 1}V  
 subject to : kj   hMj, yi   lj, yu + yv   1, Xij   yi, Xij   yj,  j   {1, .
.
.
, p}  u, v : dist(u, v) > d0  (i, j)   E  Relaxing the integrality constraints and using the substitution, Xij =  ij max t R, f  RV   ,  RE  t , we obtain the relaxation: t and yi = fi
 hg, f i + t S subject to : tkj   hMj, f i   tlj,  j   {1, .
.
.
, p}  u, v : dist(u, v) > d0 fu + fv   t, t   0,  ij   fi,  ij   fj,  (i, j)   E 
 Since this problem is invariant under scaling, we can  x the scale by setting the denominator to 1, which yields the equivalent LP stated in the theorem.
Note that the solution f   of the LP (11) is, in general, not integral, i.e., f   /  {0, 1}V   .
One can use standard techniques of randomized rounding or optimal thresholding to derive an integral solution from f  .
However, the resulting integral solution may not necessarily give a subset that sat-is es the constraints of (10).
In the special case when there are only lower bound constraints, i.e., problem (3), one can obtain a feasible set A for problem (3) by thresholding f   (see (5)) according to the objective of (10).
This is possible in this special case because there is always a threshold f   i which yields a nonempty subset Ai (in the worst case the full set V  ) satisfying all the lower bound constraints.
In our experiments on problem (3), we derived a feasible set from the solution of LP in this fashion by choosing the threshold that yields a subset that satis es the constraints and has the highest objective value.
Note that the LP relaxation (11) is vacuous with respect to upper bound constraints in the sense that given f   Rm that does not satisfy the upper bound constraints of the LP (11) one can construct  f , feasible for the LP by rescaling f without changing the objective of the LP.
This implies that 1083one can always transform the solution of the unconstrained problem into a feasible solution when there are only upper bound constraints.
However, in the presence of lower bound or subset constraints, such a rescaling does not yield a feasible solution and hence the LP relaxation is useful on the instances of (10) with at least one lower bound or a subset constraint (i.e.,  S > 0).
We now empirically show that FORTE consistently produces high quality compact teams.
We also show that the quality guarantee given by Theorem 3 is useful in practice as our method often improves a given sub-optimal solution.
Since we are not aware of any publicly available real world datasets for the team formation problem, we use, as in [12], a scienti c collaboration network extracted from the DBLP database.
Similar to [12], we restrict ourselves to four  elds of computer science: Databases (DB), Theory (T), Data Mining (DM), Arti cial Intelligence (AI).
Conferences that we consider for each  eld are given as follows: DB = {SIG-


For our team formation problem, the skill set is given by A ={DB, T, DM, AI}.
Any author who has at least three publications in any of the above 23 conferences is considered to be an expert.
In our DBLP coauthor graph, a vertex corresponds to an expert and an edge between two experts indicates prior collaboration between them.
The weight of the edge is the number of shared publications.
Since the resulting coauthor graph is disconnected, we take its largest connected component (of size 9264) for our experiments.
Directly solving the non-convex problem (6) for the value of   given in Theorem 1 often yields poor results.
Hence in our implementation of FORTE we adopt the following strategy.
We  rst solve the unconstrained version of problem (6) (i.e.,   = 0) and then iteratively solve (6) for increasing values of   until all constraints are satis ed.
In each iteration, we increase   only for those constraints which were infeasible in the previous iteration; in this way, each penalty term is regulated by di erent value of  .
Moreover, the solution obtained in the previous iteration of   is used as the starting point for the current iteration.
In this section we perform a quantitative evaluation of our method in the special case of the team formation problem with lower bound constraints and gi = 1  i (problem (3)).
We evaluate the performance of our method against the greedy method proposed in [12], refered to as mdAlk.
Similar to the experiments of [12], an expert is de ned to have a skill level of 1 in skill j, if he/she has a publication in any of the conferences corresponding to the skill j.
As done in [12], we create random tasks for di erent values of skill size, k = {3, 8, 13, 18, 23, 28}.
For each value of k we sample k skills with replacement from the skill set A = {DB, T, DM, AI}.
For example if k = 3, a sample might contain {DB, DB, T}, which means that the random task requires at least two experts from the skill DB and one expert from the skill T.
In Figure 1, we show for each method the densities, sizes and runtimes for the di erent skill sizes k, averaged over 10 random runs.
In the  rst plot, we also show the optimal values of the LP relaxation in (11).
Note that this provides an upper bound on the optimal value of (GDSP).
We can obtain feasible solutions from the LP relaxation of (GDSP) via thresholding (see Section 5), which are shown in the plot as LPfeas.
Furthermore, the plots contain the results obtained when the solutions of LPfeas and mdAlk are used as the initializations for FORTE (in each of the   iteration).
The plots show that FORTE always produces teams of higher densities and smaller sizes compared to mdAlk and LPfeas.
Furthermore, LPfeas produces better results than the greedy method in several cases in terms of densities and sizes of the obtained teams.
The results of mdAlk+FORTE and LPfeas+FORTE further show that our method is able improve the sub-optimal solutions of mdAlk and LPfeas signi cantly and achieves almost similar results as that of FORTE which was started with the unconstrained solution of (6).
Under the worst-case assumption that the upper bound on (GDSP) computed using the LP is the optimal value, the solution of FORTE is 94%   99% optimal (depending on k).
In this experiment, we assess the quality of the teams obtained for several tasks with di erent skill requirements.
Here we consider the team formation problem (GDSP) in its more general setting.
We use the generalized density objective of (1) where each vertex is given a rank ri, which we de ne based on the number of publications of the corresponding expert.
For each skill, we rank the experts according to the number of his/her publications in the conferences corresponding to the skill.
In this way each expert gets four di erent rankings; the total rank of an expert is then the minimum of these four ranks.
The main advantage of such a ranking is that the experts that have higher skill are given preference, thus producing more competent teams.
Note that we choose a relative measure like rank as the vertex weights instead of an absolute quantity like number of publications, since the distribution of the number of publications varies between di erent  elds.
In practice such a ranking is always available and hence, in our opinion, should be incorporated.
Furthermore, in order to identify the main area of expertise of each expert, we consider his/her relative number of publications.
Each expert is de ned to have a skill level of 1 in skill j if he has more than 25% of his/her publications in the conferences corresponding to skill j.
As a distance function between authors, we use the shortest path on the unweighted version of the DBLP graph, i.e. two experts are at a distance of two, if the shortest path between the corresponding vertices in the unweighted DBLP graph contains two edges.
Note that in general the distance function can come from other general sources beyond the input graph, but here we had to rely on the graph distance because of lack of other information.
In order to assess the competence of the found teams, we use the list of the 10000 most cited authors of Citeseer [1].
Note that in contrast to the skill-based ranking discussed above, this list is only used in the evaluation and not in the construction of the graph.
We compute the average inverse , where k is the size of the team and Ri is the rank of expert i on the Citeseer list of 10000 most cited authors.
For authors not contained rank as in [12] as AIR := 1000  Pk
 Ri i=1
 from the LP (LPfeas), and FORTE initialized with LPfeas and mdAlk, averaged over 10 trials.
All versions of (FORTE) signi cantly outperform mdAlk, and LPfeas both in terms of densities and sizes of the teams found.
The densities of FORTE are close to the upper bound on the optimum of the GDSP given by the LP.
on the list we set Ri = 10001.
We also report the densities of the teams found in order to assess their compatibility.
We create several tasks with various constraints and compare the teams produced by FORTE, mdAlk and LPfeas (feasible solution derived from the LP relaxation).
Note that in our implementation we extended the mdAlk algorithm of [12] to incorporate general vertex weights, using Dinkel-bach s method from fractional programming [11].
The results for these tasks are shown in Table 1.
We report the upper bound given by the LP relaxation, density value, AIR as well as number and sizes of the connected components.
Furthermore, we give the names and the Citeseer ranks of the team members who have rank at most 1000.
Note that mdAlk could only be applied to some of the tasks and LPfeas failed to  nd a feasible team in several cases.
As a  rst task we show the unconstrained solution where we maximize density without any constraints.
Note that this problem is optimally solvable in polynomial time and all methods  nd the optimal solution.
The second task asks for at least three experts with skill DB.
Here again all methods return the same team, which is indeed optimal since the LP bound agrees with the density of the obtained team.
Next we illustrate the usefulness of the additional modeling freedom of our formulation by giving an example task where obtaining meaningful, connected teams is not possible with the lower bound constraints alone.
Consider a task where we need at least four experts having skill AI (Task
 of size seven where only four members have the skill AI.
The other three experts possess skills DB and DM and are densely connected among themselves.
One can see from the LP bound that this team is again optimal.
This example illustrates the major drawback of the density based objective which while preferring higher density subgraphs compromises on the connectivity of the solution.
Our further experiments revealed that the subgraph corresponding to the skill AI is less densely connected (relative to the other skills) and forming coherent teams in this case is di cult without specifying additional requirements.
With the help of subset and distance based constraints supported by FORTE, we can now impose the team requirements more precisely and obtain meaningful teams.
In Task 4, we require that Andrew Y. Ng is the team leader and that all experts of the team should be within a distance of two from each other in terms of the underlying coauthor graph.
The result of our method is a densely connected and highly ranked team of size four with a density of 3.89.
Note that this is very close to the LP bound of 3.91.
The feasible solution obtained by LPfeas is worse than our result both in terms of density and AIR.
The greedy method mdAlk cannot be applied to this task because of the distance constraint.
In Task 5 we choose Bernhard Schoelkopf as the team leader while keeping the constraints from the previous task.
Out of the three methods, only FORTE can solve this problem.
It produces a large disconnected team, many members of which are highly skilled experts from the skill DM and have strong connections among themselves.
To  lter these densely connected members of high expertise, we introduce a budget constraint in Task 6, where we de ne the cost of the team as the total number of publications of its members.
Again this task can be solved only by FORTE which produces a compact team of four well-known AI experts.
A slightly better solution is obtained when FORTE is initialized with the infeasible solution of the LP relaxation as shown (only in this task).
This is an indication that on more di cult instances of (GDSP), it pays o  to run FORTE with more than one starting point to get the best results.
The solution of the LP, possibly infeasible, is a good starting point apart from the unconstrained solution of (6).
Tasks 7, 8 and 9 provide some additional teams found by FORTE for other tasks involving upper and lower bound constraints on di erent skills.
As noted in Section 5 the LP bound is loose in the presence of upper bound constraints and this is also the reason why it was not possible to derive a feasible solution from the LP relaxation in these cases.
In fact the LP bounds for these tasks remain the same even if the upper bound constraints are dropped from these tasks.
By incorporating various realistic constraints we have made a step forward towards a realistic formulation of the team formation problem.
Our method  nds qualitatively better teams that are more compact and have higher densities than those found by the greedy method [12].
Our linear programming relaxation not only allows us to check the solution quality but also provides a good starting point for our non-convex method.
However, arguably, a potential downside of a density-based approach is that it does not guarantee connected components.
A further extension of our approach could aim at incorporating  connectedness  or a relaxed version of it as an additional constraint.
Task 1: Unconstrained (LP bound: 32.7) Task 2: DB 3 (LP bound: 29.8) Task 3: AI 4 (LP bound: 16.6)
 Task 4: distG(u, v)  2, S={Andrew Ng} (LP bound: 3.91)
 Task 5: distG(u, v)  2, S={B.Schoelkopf} (LP bound: 6.11)
 Task 6: distG(u, v)  2, S={B.Schoelkopf}, Pi ci  255 (LP bound: 2.06) Task 7: 3 DB 6,
 (LP bound: 11.3) Task 8: 2 DB 5,



 (LP bound: 10.7)
 Task 9:
 (LP bound: 19)
 #Comps: 1 (2) Density: 32.7
 Jiawei Han (54), Philip S. Yu (279) #Comps: 1 (3) Density: 29.8
 Jiawei Han (54), Philip S. Yu (279) (+1) #Comps:

 Michael I. Jordan (28), Jiawei Han (54), Daphne Koller (127), Philip S. Yu (279), Andrew Y.
Ng (345), Bernhard Schoelkopf (364) (+1) #Comps: 1 (4) Density: 3.89 AIR: 14.2 Michael I. Jordan (28), Sebastian Thrun (97), Daphne Koller (127), Andrew Y. Ng (345) mdAlk #Comps: 1 (2) Density: 32.7
 Jiawei Han (54), Philip S. Yu (279) #Comps: 1 (3) Density: 29.8
 Jiawei Han (54), Philip S. Yu (279) (+1) #Comps:

 Michael I. Jordan (28), Jiawei Han (54), Daphne Koller (127), Philip S. Yu (279), Andrew Y.
Ng (345), Bernhard Schoelkopf (364) (+1)


 LPfeas #Comps: 1 (2) Density: 32.7
 Jiawei Han (54), Philip S. Yu (279) #Comps: 1 (3) Density: 29.8
 Jiawei Han (54), Philip S. Yu (279) (+1) #Comps:

 Michael I. Jordan (28), Jiawei Han (54), Daphne Koller (127), Philip S. Yu (279), Andrew Y.
Ng (345), Bernhard Schoelkopf (364) (+1) #Comps: 1 (6) Density: 3.5
 Michael I. Jordan (28), Geo rey E. Hinton (61), Sebastian Thrun (97), Daphne Koller (127), Andrew Y. Ng (345), Zoubin Ghahramani (577) #Comps: 2 (11,1) Density: 3.54 AIR: 3.94 Jiawei Han (54), Christos Faloutsos (140), Thomas S. Huang (146), Philip S. Yu (279), Zheng Chen (308), Bernhard Schoelkopf (364), Wei-Ying Ma (523), Ke Wang (580) (+4) #Comps: 1 (4) Density: 1.24 AIR: 1.82 Alex J. Smola (335), Bernhard Schoelkopf (364) (+2) LP+FORTE: #Comps: 2 (2,2) Density: 1.77 AIR: 2.73 Robert E. Schapire (293), Alex J. Smola (335), Bernhard Schoelkopf (364), Yoram Singer (568) #Comps: 1 (10) Density: 9.52 AIR: 4.96 Haixun Wang (50), Jiawei Han (54), Philip S. Yu (279), Zheng Chen (308), Ke Wang (580) (+5) #Comps: 3 (1,12,3) Density: 7.4 AIR: 5.06 Michael I. Jordan (28), Jiawei Han (54), Daphne Koller (127), Philip S. Yu (279), Zheng Chen (308), Andrew Y. Ng (345), Bernhard Schoelkopf (364), Wei-Ying Ma (523), Divyakant Agrawal (591) (+7) #Comps: 3 (2,2,2) Density: 6.17 AIR: 1.53 Didier Dubois (426), Micha Sharir (447), Divyakant Agrawal (591), Henri Prade (713), Pankaj K. Agarwal (770) (+1) Table 1: Teams formed by FORTE, mdAlk and LPfeas for various tasks.
We list the number and sizes of the found components, the (generalized) maximum density as well as the average inverse rank (AIR) based on the Citeseer list.
Finally, we give name and rank of each team member with rank at most 1000.
Experts who do not have the skill required by the task but are still included in the team are shown in italic font.
We gratefully acknowledge support from the Excellence Cluster MMCI at Saarland University funded by the German Research Foundation (DFG) and the project NOLEPRO funded by the European Research Council (ERC).
The subgradient of S1(f ) is given by s1(f ) = d + dS +  SImax(f ), where Imax(f ) is the indicator function of the largest entry of f .
For the subgradient of R2, using Prop.
of the form min{lj, volMj (A)},   (cid:0)t(lj ,Mj )(f )(cid:1)i=  
 lj   volMj (Ai+1) volMj (Ai)   lj, volMj (Ai+1) > lj Mij volMj (Ai+1)   lj volMj (Ai) < lj .
De ning Duv := max{0, dist(u, v)   d0}, an element of the subgradient of the second term of R2 is given as dD   p(f ), where (dD)i = Pj Dij and p(f )i   nPm j=1Dijuij | uij =  uji, uij   sign(fi   fj)o, where sgn(x) := +1, if x > 0; In total, we obtain for the 1 if x < 0; [ 1, 1], if x = 0.
subgradient r2(f ) of R2(f ), r2(f ) =  Pp j=1t(lj ,Mj )(f )+ Pp j=1t(kj ,Mj )(f )+ (p(f ) dD).
