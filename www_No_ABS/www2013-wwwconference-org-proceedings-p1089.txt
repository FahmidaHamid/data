An increasing number of applications on the World Wide Web rely on combining link and content analysis (in different ways) for subsequent analysis and inference.
For example, search engines, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
like Google, Bing and Yahoo!
typically use content and link information to index, retrieve and rank web pages.
Social networking sites like Twitter, Flickr and Facebook, as well as the aforementioned search engines, are increasingly relying on fusing content (pictures, tags, text) and link information (friends, followers, and users) for deriving actionable knowledge (e.g.
marketing and advertising).
In this article we limit our discussion to a fundamental inference problem   that of combining link and content information for the purposes of inferring clusters or communities of interest.
The challenges are manifold.
The topological characteristics of such problems (graphs induced from the natural link structure) makes identifying community structure dif cult.
Further complicating the issue is the presence of noise (incorrect links (false positives) and missing links (false negatives).
Determining how to fuse this link structure with content information ef ciently and effectively is unclear.
Finally, underpinning these challenges, is the issue of scalability as many of these graphs are extremely large running into millions of nodes and billions of edges, if not larger.
Given the fundamental nature of this problem, a number of solutions have emerged in the literature.
Broadly these can be classi- ed as: i) those that ignore content information (a large majority) and focus on addressing the topological and scalability challenges, and ii) those that account for both content and topological information.
From a qualitative standpoint the latter presumes to improve on the former (since the null hypothesis is that content should help improve the quality of the inferred communities) but often at a prohibitive cost to scalability.
In this article we present CODICIL1, a family of highly ef cient graph simpli cation algorithms leveraging both content and graph topology to identify and retain important edges in a network.
Our approach relies on fusing content and topological (link) information in a natural manner.
The output of CODICIL is a transformed variant of the original graph (with content information), which can then be clustered by any fast content-insensitive graph clustering algorithm such as METIS or Markov clustering.
Through extensive experiments on real-world datasets drawn from Flickr, Wikipedia, and CiteSeer, and across several graph clustering algorithms, we demonstrate the effectiveness and ef ciency of our methods.
We  nd that CODICIL runs several orders of magnitude faster than those state-of-the-art approaches and often identi es communities of comparable or superior quality on these datasets.
This paper is arranged as follows.
In Section 2 we discuss existent research efforts pertaining to our work.
The algorithm of CODICIL, along with implementation details, is presented in Section 3.
We report quantitative experiment results in Section 4, and
 Link-structure
 studies in Section 5.
We  nally conclude the paper in Section 6.
Community Discovery using Topology (and Content): Graph clustering/partitioning for community discovery has been studied for more than  ve decades, and a vast number of algorithms (exemplars include Metis [15], Graclus [6] and Markov clustering [27]) have been proposed and widely used in  elds including social network analytics, document clustering, bioinformatics and others.
Most of those methods, however, discard content information associated with graph elements.
Due to space limitations, we suppress detailed discussions and refer interested readers to recent surveys (e.g.
[9]) for a more comprehensive picture.
Leskovec et al.
compared a multitude of community discovery algorithms based on conductance score, and discovered the trade-off between clustering objective and community compactness [16].
Various approaches have been taken to utilize content information for community discovery.
One of them is generative probabilistic modeling which considers both contents and links as being dependent on one or more latent variables, and then estimates the conditional distributions to  nd community assignments.
PLSA-PHITS [5], Community-User-Topic model [29] and Link-PLSA-LDA [20] are three representatives in this category.
They mainly focus on studies of citation and email communication networks.
Link-PLSA-LDA, for instance, was motivated for  nding latent topics in text and citations and assumes different generative processes on citing documents, cited documents as well as citations themselves.
Text generation is following the LDA approach, and link creation from a citing document to a cited document is controlled by another topic-speci c multinomial distribution.
Yang et al. [28] introduced an alternative discriminative probabilistic model, PCL-DC, to incorporate content information in the conditional link model and estimate the community membership directly.
In this model, link probability between two nodes is decided by nodes  popularity as well as community membership, which is in turn decided by content terms.
A two-stage EM algorithm is proposed to optimize community membership probabilities and content weights alternately.
Upon convergence, each graph node is assigned to the community with maximum membership probability.
Researchers have also explored ways to augment the underlying network to take into account the content information.
The SA-Cluster-Inc algorithm proposed by Zhou et al. [30], for example, inserts virtual attribute nodes and attribute edges into the graph and computes all-pair random walk distances on the new attribute-augmented graph.
K-means clustering is then used on original graph nodes to assign them to different groups.
Weights associated with attributes are updated after each k-means iteration according to their clustering tendencies.
The algorithm iterates until convergence.
Ester et al. [8] proposed an heuristic algorithm to solve the Connected k-Center problem where both connectedness and radius constraints need to be satis ed.
The complexity of this method is dependent on the longest distance between any pair of nodes in the feature space, making it susceptible to outliers.
Biologists have studied methods [13, 26] to  nd functional modules using network topology and gene expression data.
Those methods, however, bear domain-speci c assumptions on data and are therefore not directly applicable in general.
Recently G nnemann et al. [12] introduced a subspace clustering algorithm on graphs with feature vectors, which shares some similarity with our topic.
Although their method could run on the full feature space, the search space of their algorithm is con ned by the intersection, instead of union, of the epsilon-neighborhood and the density-based combined cluster.
Furthermore, the construction of both neighborhoods are sensitive to their multiple parameters.
While decent performance can be achieved on small and medium graphs using those methods, it often comes at the cost of model complexity and lack of scalability.
Some of them take time proportional to the number of values in each attribute.
Others take time and space proportional to the number of clusters to  nd, which is often unacceptable.
Our method, in contrast, is more lightweight and scalable.
Clustering/Learning Multiple Graphs: Content-aware clustering is also related to multiple-view clustering, as content information and link structure can be treated as two views of the data.
Strehl and Ghose [23] discussed three consensus functions (cluster-wise similarity partitioning, hyper-graph partitioning and meta-clustering) to implement cluster ensembles, in which the availability of each individual view s clustering is assumed.
Tang et al. [24] proposed a linked matrix factorization method, where each graph s adjacency matrix is decomposed into a  characteristic  matrix and a common factor matrix shared among all graphs.
The purpose of factorization is to represent each vertex by a lower-dimensional vector and then cluster the vertices using corresponding feature vectors.
Their method, while applicable to small-scale problems, is not designed for web-scale networks.
Graph Sampling for Fast Clustering: Graph sampling (also known as  sparsi cation  or  ltering ) has attracted more and more focus in recent years due to the explosive growth of network data.
If a graph s structure can be preserved using fewer nodes and/or edges, community discovery algorithms can obtain similar results using less time and memory storage.
Maiya and Berger-Wolf [17] introduced an algorithm which greedily identi es the node that leads to the greatest expansion in each iteration until the user-speci ed node count is reached.
By doing so, an expander-like node-induced sub-graph is constructed.
After clustering the subgraph, the unsampled nodes can be labeled by using collective inference or other trans-ductive learning methods.
This extra post-processing step, however, operates on the original graph as a whole and easily becomes the scalability bottleneck on larger networks.
Satuluri et al. [22] proposed an edge sampling method to preferentially retain edges that connect two similar nodes.
The localized strategy ensures that edges in the relatively sparse areas will not be over-pruned.
Their method, however, does not consider content information either.
Edge sampling has also been applied to other graph tasks.
Karger [14] studied the impact of random edge sampling on original graph s cuts, and proposed randomized algorithms to  nd graph s minimum cut and maximum  ow.
Aggarwal et al. [1] proposed using edging sampling to maintain structural properties and detect out-liers in graph streams.
The goals of those work are not to preserve community structure in graphs, though.
We begin by de ning the notations used in the rest of our paper.
Let Gt = (V,Et,T ) be an undirected graph with n vertices V = v1, .
.
.
, vn, edges Et, and a collection of n corresponding term vectors T = t1, .
.
.
,t n. We use the terms  graph  and  network  interchangeably as well as the terms  vertex  and  node .
Elements in each term vector ti are basic content units which can be single words, tags or n-grams, etc., depending on the context of underlying network.
For each graph node vi   V, let its term vector be ti.
Our goal is to generate a simpli ed, edge-sampled graph Gsample =
 ent content and link structure.
Gsample should possess the following properties:   Gsample has the same vertex set as Gt.
That is, no node in the network is added or removed during the simpli cation process.
  |Esample| (cid:3) |Et|, as this enables both better runtime performance and lower memory usage in the subsequent clustering stage.
  Informally put, the resultant edge set Esample would connect node pairs which are both structure-wise and content-wise similar.
As a result, it is possible for our method to add edges which were absent from Et since the content similarity was overlooked.
The main steps of the CODICIL algorithm are:

 with bias, retaining only edges that are relevant in local neighborhoods.
The constructed content graph and simpli ed graph have the same vertices as the input graph (vertices are never added or removed), so the essential operations of the algorithm are constructing, combining edges and then sampling with bias.
Figure 1 illustrates the work  ow of CODICIL.
From the term vectors T , content edges Ec are constructed.
Those content edges and the input topological edges Et are combined as Eu which is then sampled with bias to form a smaller edge set Esample where the most relevant edges are preserved.
The graph composed of these sampled edges is passed to the graph clustering algorithm which partitions the vertices into a given number of clusters.
The pseudo-code of CODICIL is given in Algorithm 1.
CODICIL takes as input 1) Gt, the original graph consisting of vertices V , edges Et and term vectors T where ti is the content term vector for vertex vi, 1   i   |V| = |T |, 2) k, the number of nearest content neighbors to  nd for each vertex, 3) normalize(x), a function that normalizes a vector x, 4)  , an optional parameter that speci es the weights of topology and content similarities, 5) l, the number of output clusters desired, 6) clusteralgo(G, l), an algorithm that partitions a graph G into l clusters and 7) similarity(x, y) to compute similarity between x and y.
Note that any content-insensitive graph clustering algorithm can be plugged in the CODICIL framework, providing great  exibility for applications.
Lines 2 through 7 detail how content edges are created.
For each vertex vi, its k most content-similar neighbors are computed2.
For each of vi s top-k neighbors vj, an edge (vi, vj) is added to content edges Ec.
In our experiments we implemented the T opK subroutine by calculating the cosine similarity of ti s TF-IDF vector and each other term vector s TF-IDF vector.
For a content unit
 larity above a given global threshold, but this tended to produce highly imbalanced degree distributions.
l, Ec   Ec   (vi, vj) foreach vj   T opK(vi, k,T ) do end for Algorithm 1 CODICIL Input: Gt = (V,Et,T ), k, normalize( ),     [0, 1], clusteralgo( , ), similarity( ,  ) Returns: C (a disjoint clustering of V)





 7: end for
 relevant ones





 \\ i contains vi s neighbors in the edge union  i   ngbr(vi,Eu) for do = similarity(ngbr(vi,Et), ngbr( j,Et)) i   normalize(simt simnormt i) for j = 1 to | i| do simc i   normalize(simc simnormc for j = 1 to | i| do simij       simnormt  )   simnormc \\Sort similarity values in descending order.
Store the corresponding node IDs in idxi [vali, idxi]   descsort(simi) for j = 1 to ij   similarity(ti, t j ) Esample   Esample   (vi, vidxij )



 24: end for


 end for (cid:4) (cid:2)(cid:3)| i|




 ij + (1   simt ij
 to | i|   do i) j ij c, its TF-IDF value in a term vector ti is computed as
 (cid:3) tf (c, ti)   log
 tf -idf (c, ti) = (cid:6)|T | (cid:5) j=1 tf (c, tj) (cid:7) .
(1) The cosine similarity of two vectors x and y is cosine(x, y) = .
(2) x   y (cid:8)x(cid:8)2   (cid:8)y(cid:8)2 The k vertices corresponding to the k highest TF-IDF vector cosine similarity values with vi are selected as the top-k neighbors of vi.
tion
 Line 9 takes the union of the newly-created content edge set Ec and the original topological edge set Et.
In lines 10 through 24, a sampled edge set Esample is constructed by retaining the most relevant edges from the edge union Eu.
For each vertex vi, the edges to retain are selected from its local neighborhood in Eu (line 13).
We compute the topological similarity (line 14) between node vi and its neighbor  j as the relative overlap of their respective topological neighbor sets, I = ngbr(vi,Et) and J = ngbr( j,Et), using similarity (either cosine similarity as in Equation 2 or Jaccard
 Term vectors T Content edges Ec
 Topological edges Et
 Edge union Eu Edge subset Esample
 Clustering C Vertices V Figure 1: Work  ow of CODICIL

 (3) coef cient as de ned below): jaccard(I, J) =

 .
(cid:6)|(cid:3)x| i=1 xi |(cid:4)x| |(cid:3)x|(cid:8) After the computation of the topological similarity vector simt i  nishes, it is normalized by normalize (line 15).
In our experiments we implemented normalize with either zero-one, which simply rescales the vector to [0, 1]: zero-one((cid:4)x) = (xi   min((cid:4)x))/(max((cid:4)x)   min((cid:4)x)) (4) or z-norm3, which centers and normalizes values to zero mean and unit variance: xi      

 ,   = i=1 ,   = z-norm((cid:4)x) = 1|(cid:4)x|   1 (xi ) (5) Likewise, we compute vi s content similarity to its neighbor  j by applying similarity on term vectors ti and t j and normalize those similarities (lines 16 and 17).
The topological and content similarities of each edge are then aggregated with the weight spec-i ed by   (line 18).
In lines 20 through 23, the edges with highest similarity values are retained.
As stated in our desiderata, we want |Esample| (cid:3) |Et| and therefore need to retain fewer than | i| edges.
Inspired by [22], we choose to keep (cid:10)(cid:3)| i|(cid:11) edges.
This form has the following properties: 1) every vertex vi will be incident to at least one edge, therefore the sparsi cation process does not generate new singleton, 2) concavity and monotonicity ensure that larger-degree vertices will retain no fewer edges than smaller-degree vertices, and 3) sublinearity ensures that smaller-degree vertices will have a larger fraction of their edges retained than larger-degree vertices.
Finally in lines 25 through 27 the sampled graph Gsample is formed with the retained edges, and the graph clustering algorithm clusteralgo partitions Gsample into l clusters.
The proposed CODICIL framework can also be easily extended to support community detection from other types of graph.
If an input graph has weighted edges, we can modify the formula in line 18 so that simij becomes the product of combined similarity and original edge weight.
Support of attribute graph is also straightforward, as attribute assignment of a node can be represented by an indicator vector, which is in the same form of a text vector.
vantage of being both shift and scale invariant as well as outlier insensitive.
They experimentally found it best among six simple combination schemes discussed in [10].
When computing cosine similarities across term vectors t1, .
.
.
,t |T |, one can truncate the TF-IDF vectors by only keeping m elements with the highest TF-IDF values and set other elements to 0.
When m is set to a small value, TF-IDF vectors are sparser and therefore the similarity calculation becomes more ef cient with little loss in accuracy.
We may also be interested in constraining content edges to be within a topological neighborhood of each node vi, such that the search space of T opK algorithm can be greatly reduced.
Two straightforward choices are 1)  1-hop  graph in which the content edges from vi are restricted to be in vi s direct topological neighborhood, and 2)  2-hop  graph in which content edges can connect vi and its neighbors  neighbors.
Many contemporary text search systems make use of inverted indices to speed up the operation of  nding the k term vectors (documents) with the largest values of Equation 2 given a query vector ti.
We used the implementation from Apache Lucene for the largest dataset.
.
To avoid expensive computation of the exact Jaccard similarity, we estimate it by using minwise hashing [3].
An unbiased estimator of sets A and B s Jaccard similarity can be obtained by h(cid:8)
 h   i=1 I(min( i(A)) = min( i(B))) , (6) jaccard(A, B) = where  1,  2,  ,  h are h permutations drawn randomly from a family of minwise independent permutations de ned on the universe A and B belong to, and I is the identity function.
After hashing each element once using each permutation, the cost for similarity estimation is only O(h) where h is usually chosen to be less than |A| and |B|.
Similar to Jaccard coef cient, we can apply random projection method for fast estimate of cosine similarity [4].
In this method, each hash signature for a d-dimensional vector x is h(x) = sgn (x, r), where r   {0, 1}d is drawn randomly.
For two vectors x and y, the following holds: P r[h(x) = h(y)] = 1   arccos (cosine(x, y))   .
(7)
 Lines 3 7 of CODICIL are a preprocessing step which compute for each vertex its top-k most similar vertices.
Results of this one-(cid:2)   k. Its complexity time computation can be reused for any k
 largest dataset Wikipedia this step completed within a few hours.
We now consider the loop in lines 11 24 where CODICIL loops through each vertex.
For lines 14 and 16 we use the Jaccard estimator from Section 3.3.2 for which runs in O(h) with a constant number of hashes h. The normalizations in lines 15 and 17 are O(| i|) and the inner loop in lines 21 23 is O( edges by weight in line 20 is O(| i| log | i|).
The size of  i, the union of topology and content neighbors, is at most n but on average much smaller in real world graphs.
Thus the loop in lines
 (cid:3)| i|).
Sorting The overall runtime of CODICIL is the edge preprocessing time, plus O(n2 log n) for the loop, plus the algorithm-dependent time taken by clusteralgo.
We are interested in empirically answering the following questions: to better clustering than using graph topology only?
  Do the proposed content-aware clustering methods lead   How do our methods compare to existing content-aware   How scalable are our methods when the data size grows?
clustering methods?
Three publicly-available datasets with varying scale and characteristic are used.
Their domains cover document network as well as social network.
Each dataset is described below, and Table 1 follows, listing basic statistics of them.
A citation network of computer science publications4, each of which labeled as one of six sub elds.
In our graph, nodes stand for publications and undirected edges indicate citation relationships.
The content information is stemmed words from research papers, represented as one binary vector for each document.
Observe that the density of this network (average degree 2.74) is signi cantly lower than normally expected for a citation network.
The static dump of English Wikipedia pages (October 2011).
Only regular pages belonging to at least one category are included, each of which becomes one node.
Page links are extracted.
Cleaned bi-grams from title and text are used to represent each document s content.
We use categories that a page belongs to as the page s class labels.
Note that a page can be contained in more than one category, thus ground truth categories are overlapping.
From a dataset of tagged photos5 we removed infrequent tags and users associated with only few tags.
Each graph node stands for a user, and an edge exists if one user is in another s contact list.
Tags that users added to uploaded photos are used as content information.
Flickr user groups are collected as ground truth.
Similar to Wikipedia categories, Flickr user groups are also overlapping.
4http://www.cs.umd.edu/projects/linqs/ projects/lbc/index.html 5http://staff.science.uva.nl/~xirong/index.
php?n=DataSet.Flickr3m In terms of strawman methods, we compare the CODICIL methods with three existing content-aware graph clustering algorithms, SA-Cluster-Inc [30], PCL-DC [28] and Link-PLSA-LDA (L-P-LDA) [20].
Their methodologies have been brie y introduced in Section 2.
When applying SA-Cluster-Inc, we treat each term in T as a binary-valued attribute, i.e. for each graph node i every attribute value indicates whether the corresponding term is present in ti or not.
For L-P-LDA, since it does not assume a distinct distribution over topics for each cited document individually, only citing documents  topic distributions are estimated.
As a result, there are
 score on those documents using their corresponding ground-truth assignments.
Previously SA-Cluster-Inc has been shown to outperform k-SNAP [25] and PCL-DC to outperform methods including PLSA-PHITS [5], LDA-Link-Word [7] and Link-Content-Factorization [31].
Therefore we do not compare with those algorithms.
Two content-insensitive clustering algorithms are included in the experiments as well.
The  rst method,  Original Topo , clusters the original network directly.
The second method samples edges solely based on structural similarity and then clusters the sampled graph [22], and we refer to it as  Sampled Topo  hereafter.
Finally, we also adapt LDA and K-means6 algorithm to cluster graph nodes using content information only.
When applying LDA, we treat each term vector ti as a document, and one product of LDA s estimation procedure is the distribution over latent topics,  ti, for each ti (more details can be found at the original paper by Blei et al. [2]).
Therefore, we treat each latent topic as a cluster and assign each graph node to the cluster that corresponds to the topic of largest probability.
We use GibbsLDA++7, a C++ implementation of LDA using Gibbs sampling [11] which is faster than the variational method proposed originally.
Results of this method are denoted as  LDA .
There are several tunable parameters in the CODICIL framework,  rst of which is k, the number of content neighbors in the T opK subroutine.
We propose the following heuristic to decide a proper value for k: the value of k should let |Ec|   |Et|.
As a result, k is set to 50 for both Wikipedia (|Ec| = 150, 955, 014) and Flickr (|Ec| = 722, 928).
For CiteSeer, we experiment with two relatively higher k values (50, |Ec| = 103, 080 and 70, |Ec| = 143, 575) in order to compensate the extreme sparsity in the original network.
Though simplistic, this heuristic leads to decent clustering quality, as shown in Section 4.5, and avoids extra effort for tuning.
Another parameter of interest is  , which determines the weights for structural and content similarities.
We set   to 0.5 unless otherwise speci ed, as in Section 4.7.
The number of hashes (h) used for minwise hashing (Jaccard coef cient) is 30, and 512 for random projection (cosine similarity).
Experiments with both choices of similarity function are performed.
As for m, the number of nonzero elements in term vectors, we let m = 10 for Wikipedia and Flickr.
This optional step is omitted for CiteSeer since the speedup is insigni cant.
We combine the CODICIL framework with two different clus-
in C or C++.
7http://gibbslda.sourceforge.net/ 1093to the number of independent components within the graph [18].
For CiteSeer we see an increase in the number of components as a result of topological simpli cation whereas for Flickr (similarly for Wikipedia) the number of components is unchanged.
Our hypothesis is that for datasets like CiteSeer this will have a negative impact on the quality of the resulting clustering.
We further hypothesize that our content-based enhancements will help in overcoming this shortfall.
Note that the sum of eigenvalues for the complete spectrum is proportional to the number of edges in the graph [18] so this explains why the plots for the original graphs are slightly above those for the simpli ed graph even though the overall trends (e.g.
spectral gap, relative changes in eigenvalues), except for the number of components, are quite similar for both datasets.
We are interested in comparison between the predicted clustering and the real community structure since group/category information is available for all three datasets.
Later in Section 5 we will evaluate CODICIL s performance qualitatively.
While it is tempting to use conductance or other cut-based objectives to evaluate the quality of clustering, they only value the structural cohesiveness but not the content cohesiveness of resultant clustering, which is exactly the motivation of content-aware clustering algorithm.
Instead, we use average F-score with regard to the ground truth as the clustering quality measure, as it takes content grouping into consideration and ensures a fair comparison among different clusterings.
Given a predicted cluster p and with reference to a ground truth cluster g (both in the form of node set), we de ne the precision rate as .
The F-score of p on g, denoted as F (p, g), is the harmonic mean of precision and recall rates.
|p g| |p| and the recall rate as |p g| |g| For a predicted cluster p, we compute its F-score on each g in the ground truth clustering G and de ne the maximal obtained as p s F-score on G. That is: F (p, G) = max g G F (p, g) .
(8) The  nal F-score of the predicted clustering P on the ground truth clustering G is then calculated as the weighted (by cluster size) average of each predicted cluster s F-score: (cid:8) |p| |V| F (p, G) .
(9)




 Original Topo Sampled Topo (a) Citeseer


 ) e l a c s g o l ( s e u l a v n e g i e n a i c a l p a
 h p a r
 ) e l a c s g o l ( s e u l a v n e g i e n a i c a l p a
 h p a r

 Wikipedia
 Flickr CiteSeer

 |Et|






 |CCmax|


 # Uniq.
Content Unit Avg |ti|





 # Class


 Table 1: Basic statistics of datasets.
# CC: number of connected components.
|CCmax|: size of the largest connected component.
Avg |ti|: average number of nonzero elements in term vectors.
# Class: number of (overlapping) ground truth classes.
tering algorithms, Metis8 [15] and Multilevel Regularized Markov Clustering (MLR-MCL)9 [21].
Both clustering algorithms are also applied on strawman methods.
ture In this section we investigate the impact of topological simpli cation (or sampling) on the spectrum of the graph.
For both Cite-Seer and Flickr (results for Wikipedia are similar to that of Flickr) we compute the Laplacian of the graph and then examine the top part of its eigenspectrum ( rst 2000 eigenvectors).
Speci cally, in Figure 2 we order the eigenvectors from the smallest one to the largest one (on the X axis) and plot corresponding eigenvalues (on the Y axis).
Original Topo Sampled Topo
 p P

 (b) Flickr Figure 2: Eigenvalues of graph Laplacian before and after sim-pli cation The multiplicity of 0 as an eigenvalue in such a plot corresponds 8http://glaros.dtc.umn.edu/gkhome/metis/ metis/download 9http://www.cse.ohio-state.edu/~satuluri/ research.html This effectively penalizes the predicted clustering that is not well-aligned with the ground truth, and we use it as the quality measure of all methods on all datasets.
In Figure 3 we show the experiment results on CiteSeer.
Since it is known that the network has six communities (i.e. sub elds in computer science), there is no need to vary l, the number of desired clusters.
We report results using Metis (similar numbers were observed with Markov clustering).
For PCL-DC, we set the parameter   to 5 as suggested in the original paper, yielding an F-score of

 ing based on topology alone results in a performance well below the state-of-the-art content-aware clustering methods.
This is not surprising as the input graph has 438 connected components and therefore most small components were randomly assigned a prediction label.
Although such approach has no impact on topology-based measures (e.g.
normalized cut or conductance), it greatly spoils the F-score measure against the ground truth.
Moreover, topology-based simpli cation further deteriorates the clustering performance as it creates even more connected components, as we projected in Section 4.4.
Neither is LDA able to provide a competitive result, as it is oblivious to link structure embedded in the dataset.
Surprisingly though, K-means only manages to produce a very unbalanced clustering (the largest cluster always contains more than 90% of all papers) even after 50 iterations, and its F-score (averaged over  ve runs) is only 0.336.
Quality on Citeseer e r o c s -




















-r a
 a a o o
 -
i m
 m c c s s -
-g p e c c i i
 l
 i l a a a n n
 u
 n e n r r e e s
 a d s d d , , t e r -
n c l

 o o p p o o , , k k k k = = = =







 Figure 3: F-score of Metis on CiteSeer On the other hand, our content-aware approaches (using Metis as the clustering method) were able to handle the issue of disconnection as they also include content-similar edges.
For both similarity measures, the F-scores are within 90% range of PCL-DC, and it outperforms PCL-DC when k increases to 70.
While achieving the quality that is comparable with existing methods, the CODICIL series are signi cantly faster.
PCL-DC takes 234 seconds on this dataset and SA-Cluster-Inc requires 306 seconds.
LDA  nishes in 40 seconds.
In contrast, the sum of CODICIL s edge sampling and clustering time never exceeds 1 second.
Therefore, the CODICIL methods are at least one order of magnitude faster than state-of-the-art algorithms.
For the Wikipedia dataset, we were unable to run the experiment on SA-Cluster-Inc, PCL-DC, L-P-LDA, LDA and K-means as their memory and/or running time requirement became prohibitive on this million-node network.
For example, storing 10,000 centroids alone in K-means requires 54 GBs).
Figures 4a and 4c plot the performances using MLR-MCL and Metis, respectively.
Since category assignments as the ground truth are overlapping, there is no gold standard for the number of clusters.
We therefore varied l in both clustering algorithms.
Our content-aware clustering algorithms constantly outperforms Sampled Topo by a large margin, indicating that CODICIL methods are able to simplify the network and recover community structure at the same time.
CODICIL methods  F-scores are also on par or better than those of Original Topo.
Figure 5a shows the performances of various methods with MLR-MCL on Flickr, where SA-Cluster-Inc, PCL-DC, LDA and K-means can also  nish in a reasonable time (L-P-LDA still takes more than
 lar to results on CiteSeer, CODICIL methods again lead the baselines by a considerable margin.
The F-scores of SA-Cluster-Inc, LDA, and K-means never exceed 0.2, whereas CODICIL methods  F-scores are often higher, together with Original & Sampled Topo.
Readers may have noticed that for PCL-DC only three data points (l = 50, 75, 100) are obtained.
That is because its excessive memory consumption crashed our workstation after using up 16 GBs of RAM for larger l values.
We also observe that while PCL-DC generates a group membership distribution over l groups for each vertex, fewer than l communities are discovered.
That is, there exist groups of which no vertex is a prominent member.
Furthermore, the number of communities discovered is decreasing as l increases (45, 43 and 39 communities for l = 50, 75, 100), which is opposite to other methods  trends.
All three clusterings  F-scores are less than 0.25.
Similarly, multiple runs of K-means (K is set to 400, 800, 1200, and 1600) can only identity roughly 200 communities.
The running time on CiteSeer has already been discussed, and here we focus on Flickr and Wikipedia.
For CODICIL methods, the running time includes both edge sampling and clustering stage.
The plots  Y-axes (running time) are in log scale.
We  rst report scalability results on Flickr (see Figure 5b).
For SA-Cluster-Inc, the value of l (the desired output cluster count), ranging from 100 to 5000, does not affect its running time as it always stays between 1 and 1.25 hours with memory usage around
 in the number of latent topics (i.e. l) speci ed, climbing up from
 running time with three l values (50, 75, 100) is 0.5, 2.0 and 2.8 hours, respectively.
As for our content-aware clustering algorithms, running them on Flickr requires less than 8 seconds, which is three to four orders of magnitude faster than SA-Cluster-Inc, PCL-DC and LDA.
Original Topo takes more than 10 seconds, and Sampled Topo runs slightly faster than CODICIL methods.
Original Topo, Sampled Topo and all CODICIL methods  n-ished successfully.
The running time is plotted in Figures 4b and 4d.
When clustering using MLR-MCL, our methods are at least one order of magnitude faster than clustering based on network topology alone.
For Metis, CODICIL is also more than four times faster.
The trend lines suggest our methods have promising scalability for analysis on even larger networks.
So far all experiments performed  x   at 0.5, meaning equal weights of structural and content similarities.
In this subsection we track how the clustering quality changes when the value of   is varied from 0.1 to 0.9 with a step length of 0.1.
On Wikipedia (Figure 6a) and Citeseer (Figure 6b), F-scores are greatest around   = 0.5, supporting the decision of assigning




 e r o c s -


Original Topo Sampled Topo CODICIL w/ Jaccard CODICIL w/ Cosine
 Num of Output Clusters (a) F-score of MLR-MCL on Wikipedia




 e r o c s -


Original Topo Sampled Topo CODICIL w/ Jaccard CODICIL w/ Cosine
 Num of Output Clusters (c) F-score of Metis on Wikipedia ) s d n o c e s ( e m i
 ) s d n o c e s ( e m i




 Original Topo Sampled Topo CODICIL w/ Jaccard CODICIL w/ Cosine
 Num of Output Clusters (b) Running time of MLR-MCL on Wikipedia

 Original Topo Sampled Topo CODICIL w/ Jaccard CODICIL w/ Cosine

 Num of Output Clusters (d) Running time of Metis on Wikipedia Figure 4: Experiment Results on Wikipedia equal weights to structural and content similarities.
Results differ on Flickr where F-score is constantly improving when   increases (i.e. more weight assigned to topological similarity).
In Section 3.3.1 we discuss the possibility of constraining content edges within a topological neighborhood for each node vi.
Here we provide a brief review on how the qualities of resultant clusterings are impacted by such constraint.
For the sake of space, we focus on the F-scores on Wikipedia and Flickr.
Figures 7a and 7b show F-scores achieved on Wikipedia, using different Ec constraints.
Full means no constraint and T opK subroutine searches the whole vertex set V, whereas 1-hop constrains the search to within a one-hop neighborhood, and likewise for 2-hop.
Plots of full and 2-hop almost overlap with each other, suggesting that searching within the 2-hop neighborhood can provide suf ciently strong content signals on this dataset.
For Flickr (Figures 7c and 7d), interestingly 2-hop and 1-hop have a slight lead over full.
This may be an indication that in online social networks, compared with information networks, content similarity between two closely connected users emits stronger community signals.
An interesting observation on the biased edge sampling is that it always results in an improvement in running time.
However, sampling just the topology graph results in a clear loss in accuracy whereas content-conscious sampling is much more effective with accuracies that are on par with the best performing methods at a fraction of the cost to compute.
We observe this for all three datasets.
We also  nd that for probabilistic-model-based methods (PCL-DC, L-P-LDA and LDA) as well as K-means, their running time is at least linear in l, the desired number of output clusters, which becomes a critical drawback in face of large-scale workloads.
As the network grows, the number of clusters also increases naturally.
Plots on CODICIL methods  running time, on the other hand, suggest a logarithmic increase with regard to the number of clusters, which is more affordable.
In this section, we demonstrate the bene ts of leveraging content information on two Wikipedia pages:  Machine Learning  and  Graph (Mathematics) .
In the original network,  machine learning  has a total degree of 637, and many of its neighbors (including  1-2-AX working memory task ,  Wayne State University Computer Science Department ,  Chou-Fasman method , etc.)
are at best peripheral to the context.
When we sample the graph according to its link structure only, 119 neighbors are retained for  machine learning .
Although this eliminates some noise, many others, including the three entries above, are still preserved.
Moreover, it also removes during the process many neighbors which should have been kept, e.g.
 naive Bayes classi er ,  support vector machine , and so on.
e r o c s -


Original Topo Sampled Topo SA-Cluster-Inc

 K-means CODICIL w/ Jaccard CODICIL w/ Cosine




 Num of Output Clusters (a) F-score of MLR-MCL on Flickr




 ) s d n o c e s ( e m i
 Original Topo Sampled Topo SA-Cluster-Inc

 CODICIL w/ Jaccard CODICIL w/ Cosine




 Num of Output Clusters (b) Running time of MLR-MCL on Flickr Figure 5: Experiment Results on Flickr




 e r o c s -

CODICIL w/ METIS w/ Jaccard CODICIL w/ METIS w/ Cosine CODICIL w/ MLR-MCL w/ Jaccard CODICIL w/ MLR-MCL w/ Cosine





 e r o c s -





e r o c s -
CODICIL w/ METIS w/ Jaccard CODICIL w/ METIS w/ Cosine CODICIL w/ METIS w/ Jaccard CODICIL w/ METIS w/ Cosine CODICIL w/ MLR-MCL w/ Jaccard CODICIL w/ MLR-MCL w/ Cosine
   (c) Varying   on Flickr


     (a) Varying   on Wikipedia (b) Varying   on Citeseer Figure 6: Effect of Varying   on F-score (Avg.
# Clusters for Wikipedia: 29,414, Avg.
# Clusters for Flickr: 1,911) The CODICIL framework, in contrast, alleviates both problems.
Apart from removing noisy edges, it also keeps the most relevant ones.
For example,  AdaBoost ,  ensemble learning ,  pattern recognition  all appear in  machine learning s neighborhood in the sampled edge set Esample.
Perhaps more interestingly, we  nd that CODICIL adds  neural network , an edge absent from the original network, into Esample (recall that it is possible for CODICIL to include an edge even it is not in the original graph, given its content similarity is suf ciently high).
This again illustrates the core philosophy of CODICIL: to complement the original network with content information so as to better recover the community structure.
Similar observations can be made on the  Graph (Mathematics)  page.
For example, CODICIL removes entries including  Eric W.
Weisstein ,  gadget (computer science)  and  interval chromatic number of an ordered graph .
It also keeps  clique (graph theory) ,  Hamiltonian path ,  connectivity (graph theory)  and others, which would otherwise be removed if we sample the graph using link structure alone.
We have presented an ef cient and extremely simple algorithm for community identi cation in large-scale graphs by fusing content and link similarity.
Our algorithm, CODICIL, selectively retains edges of high relevancy within local neighborhoods from the fused graph, and subsequently clusters this backbone graph with any content-agnostic graph clustering algorithm.
Our experiments demonstrate that CODICIL outperforms state-of-the-art methods in clustering quality while running orders of magnitude faster for moderately-sized datasets, and can ef ciently handle large graphs with millions of nodes and hundreds of millions of edges.
While simpli cation can be applied to the original topology alone with a small loss of clustering quality, it is particularly potent when combined with content edges, delivering superior clustering quality with excellent runtime performance.
This work is sponsored by NSF Award #1111118  SoCS: Collaborative Research: Social Media Enhanced Organizational Sense-making in Emergency Response  and NSF Award #1240651  CCF: EAGER: Collaborative Research: Scalable Graph Mining and Clustering on Desktop Supercomputers .
