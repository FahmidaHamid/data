Web information representation is getting more sophisticated, thanks to information extraction and semantic Web e orts.
Much structured and semistructured data now supplements unstructured, free-format textual pages.
In verticals such as e-commerce, the structured data can be accessed through forms and faceted search.
However, a large number of free-format queries remain outside the scope of verticals.
As we shall review in Section 2, there is much recent research on analyzing and annotating them.
Here we focus on a speci c kind of entity search query: some words (called selectors) in the query are meant to occur literally in a response document (as in traditional text Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
Soumen Chakrabarti IIT Bombay soumen@cse.iitb.ac.in search), but other words hint at the type of entity sought by the query.
Unlike prior work on translating well-formed sentences or questions to structured queries using deep NLP, we are interested in handling  telegraphic  queries that are typically sent to search engines.
Each response entity must be a member of the hinted type.
Note that this problem is quite di erent from  nding an-in swers to well-formed natural language questions (e.g., Wolfram Alpha) from structured knowledge bases (perhaps curated through information extraction).
Also observe that we do not restrict ourselves to queries that seek entities by attribute values or attributes of a given entity (both are valuable query templates for e-commerce and have been researched).
In our setup, some responses may only be collected from diverse, open-domain, free-format text sources.
E.g., typical driving time between Paris and Nice (the target type is time duration), or cricketers who scored centuries at Lords (the target type is cricketers).
Figure 1: Example of a collective, joint query interpretation and entity ranking problem; includes a query containing di erent possible hint and selector words, partially matching types with member entities and corpus snippets The target type (or a more general supertype, such as sportsperson in place of cricketer ) may be instantiated in a catalog, but the typical user has no knowledge of the catalog or its schema.
Large catalogs like Wikipedia or Freebase evolve  organically .
They are not designed by linguists, and they are not minimal or canonical in any sense.
Types have overlaps and redundancies.
The query interpreter should take advantage of specialized types whenever available, but otherwise gracefully back o  to broader types.
Figure 1 shows a query that has at least two plausible hint word sets: {team, baseball} (correct) and {world, series} (in-
also a large literature on topic-independent intent discovery [10, 18] as well as topic-dependent facet [30] or template [1] inference.
The problem of disambiguating named entities mentioned in queries is super cially similar to ours, but is technically In Figure 2, query word ymca may refer quite di erent.
to di erent entities, but additional query word lyrics hints at type music, whereas address hints at type organization.
Note that the query text directly embeds a mention of an entity, not a type.
Disambiguating the entity (usually) amounts to disambiguating the type contrast Figure 2 with Figure 1.
A given mention usually refers to only a few entities.
In contrast, misinterpreting the hint often pollutes the entity response list beyond redemption.
Delaying a hard choice of the target type, or avoiding it entirely, is likely to help.
correct).
Hint words partially match descriptions of types in a catalog, which lead to member entities.
Potential response entities are mentioned in document snippets (one shown), which in turn partially match selector words (world, series, losing, 1998).
Given a limited number of types to choose from, a human will  nd it trivial to pick the best.
However, a program will  nd it very challenging to decide which subset of query words are type hints, and, even after that, to select the best type(s) from a large type catalog.
This query interpretation task is one part of our goal.
We posit that corpus statistics provide critical signals for query interpretation.
For example, we might bene t from knowing that San_Diego_Padres rarely co-occurs with the word  hockey , which can be known only from the corpus.
Query interpretation should ideally be done jointly with ranking entities from the corpus, because it involves a delicate combinatorial balance between the hint-selector split, and the (rather noisy) signals from the quality of matches between type descriptions and hint words, snippets and other words, and mentions of entities in said snippets.
Although query typing has been investigated before [38, 5], to the best of our knowledge this is the  rst work on combining type interpretation with learning to rank [21].
In Section 4, we present a natural, generative formulation for the task using probabilistic language models.
In Section 5 we present a more  exible and powerful max-margin discrim-inative approach [19, 7].
In Section 6, we report on experiments involving 709 queries, Figure 2: Disambiguating named entities in queries.
over 200,000 types, 1.5 million entities, and 380 million evidence snippets collected from over 500 million Web pages.
The entity ranking accuracy of a reasonable query interpreter will be between the  lower bound  of a generic system that makes no e ort to identify the target type (i.e., all catalog entities are candidates), and the upper bound of an unrealistic  perfect  system that knows the target type by magic.
Our salient experimental observations are:   The generative language model approach improves entity ranking accuracy signi cantly beyond the lower bound wrt MAP, MRR and NDCG.
  The discriminative approach is superior to generative; e.g., it bridges 43% of the MAP gap between the lower and upper bounds.
  In fact, if we discard the entity ranks output from our system, use it only as a target type predictor, and issue a query with the predicted type, entity ranking accuracy drops.
  Our discriminative approach beats a recent target type   NLP-heavy techniques are not robust to telegraphic prediction algorithm by signi cant margins.
queries.
Our data and code will be made publicly available at http://www.cse.iitb.ac.in/~soumen/doc/CSAW/.
Interpreting a free-format query into a structured form has been explored extensively in the information retrieval (IR) and Web search communities, with several recent dedicated workshops1.
A preliminary but critical structuring step is 1ciir.cs.umass.edu/sigir2010/qru ciir.cs.umass.edu/sigir2011/qru strataconf.com/stratany2011/public/schedule/detail/21413 sysrun.haifa.il.ibm.com/hrl/smer2011 For entity disambiguation, Guo et al. [15] proposed a probabilistic language model through weak supervision that learns to associate, e.g., lyrics with music and address with organization.
Pantel et al. [25, 26] pushed this farther by exploiting clicks and modeling intent.
Hu et al. [17] addressed a similar problem.
None gave a discriminative max-margin formulation, or uni ed the framework with learning to rank.
Given that the database community uses SQL and XQuery as unambiguous, structured representations of information needs, and that the NLP community seeks to parse sentences to a well-de ned meaning, there also exists convergent database and NLP literature on interpreting free-format (source) queries into a suitable target  query language .
Naturally, much of this work seeks to identify types, entities, attributes, and relations in queries.
Although the theoretical problem is challenging [14], a common underlying theme is that each token in the query may be an expression of schema elements, entities, or relationships: this leads to a general assignment problem, which is solved approximately using various techniques, summarized below.
Sarkas et al. [33] annotated e-commerce queries using schema and data in a structured product catalog.
In the context of Web-extracted knowledge bases such as YAGO [35], Pound et al. [29, 28] set up a collective assignment problem with a cost model that re ects syntactic similarity between query fragments and their assigned concepts, as well as semantic coherence between concepts [20].
Sarkas, Pound and others, like us, handle  telegraphic queries  that may not be well-formed sentences.
DEANNA [39] solved the collective assignment problem using an integer program.
It is capable of parsing queries as complex as  which director has won the Academy Award for best director and is married to an actress that has won the Academy Award for best ac-km.aifb.kit.edu/ws/jiwes2012
 to query syntax and often fails on telegraphic queries.
All these systems interpret the query with the help of a fairly clean, structured knowledge base.
[33, 29, 28, 39] do not give discriminative learning-to-rank algorithms that jointly disambiguate the query and ranks responses.
IBM s Watson [24] identi es candidate entities  rst, and then scores them for compatibility with likely target types.
In this work, we do not assume that a knowledge base has been curated ahead of time from a text corpus.
Instead we assume entities and types have been annotated on spans of unstructured text.
Accordingly, we step back from sophisticated target schemata, settling for three basic relations (instanceOf, subTypeOf, and mentionOf, see Figure 1) that link a structured entity catalog with an unstructured text corpus (such as the Web).
On the other hand, we take the  rst step toward integrating learning-to-rank [21] techniques with query interpretation.
Closest to our goal are those of Vallet and Zaragoza [38] and Balog and Neumayer (B&N) [5].
Vallet and Zaragoza  rst collected a ranked list of entities by launching a query without any type constraints.
Each entity belongs to a hierarchy of types.
They accrued a score in favor of a type from every entity as a function of its rank, and ranked types by decreasing total score.
B&N investigated two techniques.
In the  rst, descriptions of all entities e belonging to each type t were concatenated into a super document for t, and turned into a language model.
In the second (similar in spirit to Vallet and Zaragoza), the score of t was calculated as a weighted average of probabilities of entity description language models generating the query, for e   t.
These approaches [5, 31] use long entity descriptions, such as found on the Wikipedia page representing an entity, but not a corpus where entity mentions are annotated.
The corpus documents may well not be de nitional, and yet remarkably improve entity ranking accuracy, as we shall see.
None of [38, 5, 31] attempt a segmentation of query words by purpose (target type vs. literal matches).
A  telegraphic  entity search query q expresses an information need that is satis ed by one or more entities.
Query q is a sequence of |q| words.
The jth word of query q is denoted wq,j , where j = 1, .
.
.
,| q|, and subscript q in wq,j is omitted if clear from context.
We will interchangeably use q (as a query identi er) and (cid:2)q (to highlight that it is a sequence of words).
Unlike full, well-formed, grammatical sentences or questions, telegraphic queries resemble short Web search queries having no clear subject-verb-object or other complex clausal structure.
Some examples of natural telegraphic entity search queries and possible natural language  translations  are shown in Figure 3.
Q denotes a set of queries.
The catalog (T ,E , +, +), is a directed acyclic graph of type nodes t   T , with edges representing the  is-subtype-of  transitive binary relation  +.
Each type t is described by one or more lemmas (descriptive phrases) L(t), e.g., Austrian physicists.
Q1 Woodrow Wilson was president of which university?
Q2 Which Chinese cities have many international companies?
Q3 What cathedral is in Claude Monet s paintings?
Q4 Along the banks of what river is the Hermitage Museum located?
woodrow wilson president university chinese international companies many city claude monet cathedral paintings hermitage museum banks of river Q5 At what institute was Dolly dolly clone institute cloned?
Q6 Who made the  rst air rst airplane inventor plane?
Figure 3: Natural language queries and typical telegraphic forms, with potential type description matches underlined.
Each entity e in the catalog is also represented by a node connected by  is-instance-of  edge(s) to one or more most speci c type nodes, and transitively belongs to all super-types; this relation is represented as  +.
An entity e may be a candidate for a query q.
The set of candidate entities for query q is called Eq   E .
In training data, an entity e may be labeled relevant (denoted e+) or irrelevant (denoted e ) for q. Eq is accordingly partitioned into E +
 q ,E  q .
The corpus is a set of free-format text documents.
Each document is modeled as a sequence of words.
Entity e is mentioned at some places in an unstructured text corpus.
A  mention  is a token span (e.g., Big Apple) that gives evidence of reference to e (e.g., New York City).
The mention span, together with a suitable window of context words around it, is called a snippet.
The set of snippets mentioning e is called Se.
c   Se is one snippet context supporting e.
In the Wikipedia corpus, most mentions are annotated manually as wiki hyperlinks.
For Web text, statistical learning techniques [20, 16] are used for high-quality annotations.
Here we assume mentions to be correct and deterministic.
Extending our work to noisy mentions is left for future work.
Given the success of generative techniques in corpus modeling [8], IR [41] and entity ranking [3, 4], it is natural to propose a generative language model approach to joint query interpretation and response ranking.
As is common in generative language models, we will  x an entity e and generate the query words, by taking the following steps:

 which will be called hint words;

 from these snippets.
Our goal is to rank entities by probability given the query, by taking the expectation over possible types and hints.
Given entity e, we  rst pick a type t such that e  + t, and describe t in the query (with the expectation that the system will infer t, then instantiate it to e as a response).
So the basic question looks like:  if the answer is Albert
 (cid:5) Einstein, what type (among scientist, person, organism, etc.)
is likely to be mentioned in the query, before we inspect the query?  (After we see the query, our beliefs will change, e.g., depending on whether the query asks  who discovered general relativity?  vs.  which physicist discovered general relativity? ) So we need to design the prior distribution Pr(t|e).
Recall that there may be hundreds of thousands of ts, and tens of millions of es, so  tting the prior for each e separately is out of the question.
On the other hand, the prior is just a mild guidance mechanism to discourage obscure or low-recall types like  Austrian Physicists who died in 1972 .
Therefore, we propose the following crude but e cient estimate.
From a query log with ground truth (i.e., each query accompanied with a t provided by a human), accumulate a hit count Nt for each type t. At query time, given a candidate e, we calculate Pr(t|e) = Nt +   t(cid:2):e +t(cid:2) (Nt(cid:2) +  ) e  + t , , (1)
 otherwise where     (0, 1) is a tuned constant.
Suppose the query is the word sequence (wj, j = 1, .
.
.
,| q|).
For each position j, we posit a binary switch variable zj   {h, s}.
Each zj will be generated iid from a Bernoulli distribution with tuned parameter     (0, 1).
If zj = h, then word wj is intended as a hint to the target type.
Otherwise wj is a selector sampled from snippets mentioning entity e.
The vector of switch variables is called (cid:2)z.
|q| |q| The number of possible partitions of query words into hints and selectors is 2 .
By de nition, telegraphic queries are short, so 2 is manageable.
One can also reduce this search space by asserting additional constraints, without compromising quality in practice.
E.g., we can restrict the type hint to a contiguous span with at most three tokens.
Given (cid:2)q and a proposed partition (cid:2)z, we de ne two helper functions, overloading symbols s and h: Hint words of q: Selector words of q: h((cid:2)q, (cid:2)z) = {wq,j : zj = h} s((cid:2)q, (cid:2)z) = {wq,j : zj = s}.
(2) (3) With these de nitions, in the exhaustive hint-selector partition case, (cid:2)z is the result of |q| Bernoulli trials with hint probability     (0, 1) for each word, so we have Pr((cid:2)z) =  |h((cid:2)q,(cid:2)z)|   is tuned using training data.
(1    ) |s((cid:2)q,(cid:2)z)|.
(4) In this paper we will consider strict partitions of query words between hints and selectors, but it is not di cult to generalize to words that may be both hints and selectors.
Assuming each query word has a purpose, the full space grows to 3 , but assuming contiguity of the hint segment again reduces the space to essentially O(|q|).
|q| Globally across queries, the textual description of each type t induces a language model.
We can de ne the exact form of the model in any number of ways, but, to keep implementations e cient, we will make the commonly used assumption that hint words are conditionally independent of each other given the type.
Each type t is described by one or more lemmas (descriptive phrases) L(t), e.g., Austrian physicists.
Because lemmas are very short, words are rarely repeated, so we can use the multivariate Bernoulli [23] distribution derived from lemma (cid:5):(cid:6)Pr(w|(cid:5)) = (cid:7)

 if w appears in (cid:5), otherwise (5) Following usual smoothing policies [41], we interpolate the smoothed distribution above with a background language model created from all types: (cid:5) t T (cid:2)w appears in (cid:5) ; (cid:5)   L(t)(cid:3) (cid:6)Pr(w|T ) = ; (6)
 in words, the fraction of all types that contain w. (cid:2)B(cid:3) is 1 if Boolean condition B is true, and 0 otherwise.
We splice together (5) and (6) using parameter     (0, 1): Pr(w|(cid:5)) = (1    )(cid:6)Pr(w|(cid:5)) +  (cid:6)Pr(w|T ).
(7) The probability of generating exactly the hint words in the query is Pr(h((cid:2)q, (cid:2)z)|(cid:5)) = (1   Pr(w|(cid:5))), Pr(w|(cid:5)) (8) (cid:8) (cid:8) w h((cid:2)q,(cid:2)z) w(cid:3) h((cid:2)q,(cid:2)z) where w ranges over the entire vocabulary of type descriptions.
In case of multiple lemmas describing a type, Pr( |t) = max (cid:3) L(t) Pr( |(cid:5)); (9) i.e., use the most favorable lemma.
All  tted parameters in the distribution Pr(w|(cid:5)) are collectively called  .
The selector part of the query, s((cid:2)q, (cid:2)z), is generated from a language model derived from Se, the set of snippets that mention candidate entity e. For simplicity we use the same kind of smoothed multivariate Bernoulli distribution to build the language model as we did for the type descriptions.
Note that words that appear in snippets but not in the query are of no concern in a language model that seeks to generate the query from distributions associated with the snippets.
Suppose corpusCount (e) is the number of mentions of e in the corpus C, and corpusCount (e, w) be the number of mentions of e where w also occurs within a speci ed snippet window width.
The unsmoothed probability of generating a query word w from the snippets of e is corpusCount (e, w) corpusCount (e) = |{s   Se : w   s}| corpusCount (e) .
(10) (cid:6)Pr(w|e) = As before, we will smooth the above estimate using an corpus-level, entity-independent background word distribution esti-mate:(cid:6)Pr(w|C) = And now we use the interpolation
 Pr(w|e) = (1    )(cid:6)Pr(w|e) +  (cid:6)Pr(w|C), (11) (12) where     (0, 1) is a suitable smoothing parameter.
The  tted parameters of the Pr(w|e) distribution are collectively called  .
Similar to (8), the selector part of the query is
 In standard text search, top-ranking URLs are accompanied by a summary with matching query words highlighted.
In our system, top-ranking entities need to be justi ed by explaining to the user how the query was interpreted.
Specifically, we need to show the user the inferred type, and the inferred purpose (hint or selector) of each query word.
Pr(t, (cid:2)z|e, (cid:2)q)   Pr(e, t, (cid:2)q, (cid:2)z) = Pr(e) Pr(t|e)Pr((cid:2)z|e, t) Pr((cid:2)q|e, t, (cid:2)z)   Pr(e) Pr(t|e)Pr((cid:2)z) Pr((cid:2)q|e, t, (cid:2)z) (16) approximating Pr((cid:2)z|e, t)   Pr((cid:2)z) as before.
Now we can report arg maxt,(cid:2)z Pr(t, (cid:2)z|e, (cid:2)q) as the explanation for e. It is also possible to report marginals such as Pr(t|e, (cid:2)q) or Pr(zj|e, (cid:2)q) this way.
As often happens, a generative formulation starts out feeling natural, but is soon mired in a number of questionable assumptions and tuned hyper parameters.
In recent times, this story has played out in many problems, such as information extraction [32] and learning to rank [21], where generative language models were proposed earlier, but the latest algorithms are all discriminatively trained.
The above formulation has several potential shortcomings:   The modeling of Pr(t|e) is necessarily a compromise.
  Pr(zj) is assumed to be independent of q and e, and   In the interest of computational feasibility, the language models for both types and snippets are simplistic.
Phrase and exact matches are di cult to capture.
  Hyper parameters  ,  ,  ,   can only be tuned by sweeping ranges; no e ective learning technique is obvious.
  As often happens with complex generative models, the scales of probabilities being multiplied (15) are diverse and hard to balance.
iid.
These assumptions may not be the best.
q ,E  Instead of designing conditional distributions as in Section 4, here we will design feature functions, and learn weights corresponding to them by using relevant and (samples of) irrelevant entity sets E + q associated with each query q, as is standard in learning to rank [21].
The bene t is that it is much safer to incrementally add highly informative but strongly correlated features (such as exact phrase match, match with and without stemming, etc.)
to discriminative formulations.
Standard notation used in structured max-margin learning uses  (x, y)   R d as the feature map, where x is an observation and y is the label to be predicted.
A model     R d is  tted so that      (x, ycorrect) >      (x, yincorrect).
Once   gets  xed via training, given a new text instance xtest, inference is the process of  nding arg maxy      (xtest, y).
In our case, we use the notation  (q, e, t, (cid:2)z) for the feature map.
q gives us access to the sequence of words in the query, and is the analog of x above.
e gives us access to the snippets Se that support e, and is the analog of y above.
t and (cid:2)z are latent variable [40] inputs to the feature map whose role will be explained shortly.
Figure 4: Plate diagram for generating a query q from a candidate entity e. Only (wq,j : j = 1, .
.
.
,| q|) are observed variables.
  represents the type description language model and   represents the entity mention snippets language model.
(zq,j : j = 1, .
.
.
,| q|) are the hidden switch variables.
T is the hidden type variable.
(cid:8) (cid:8) generated with probability Pr(s((cid:2)q, (cid:2)z)|e) = Pr(w|e) w s((cid:2)q,(cid:2)z) w(cid:3) s((cid:2)q,(cid:2)z) (1   Pr(w|e)), (13) except here w ranges over all query words.
A plate diagram for the process generating a query (cid:2)q is shown in Figure 4.
Vertices are marked with random variables E, T, Z, W whose instantiations are speci c values e, t, (cid:2)z, w   q.
The hidden variables of interest are the binary Z   {h, s}, for selecting between type hint (h) and selector (s) words; and T , the type of one query.
Each query picks one hidden value t, and a vector of |q| size for Z, denoted (cid:2)z.
The only observed variables are the |q| query words (wj : j = 1, .
.
.
,| q|).
Also,  ,  ,  ,   are hyper-parameters tuned globally across queries.
In the end we are interested in arg maxe Pr(e|(cid:2)q), where Pr((cid:2)q, t, (cid:2)z|e) Pr(e|(cid:2)q)   Pr(e, (cid:2)q) = Pr(e) Pr((cid:2)q|e) = Pr(e) (cid:9) (cid:9) (cid:9) (cid:9) t,(cid:2)z t,(cid:2)z t,(cid:2)z = Pr(e)   Pr(e) = Pr(e) Pr(t|e)Pr((cid:2)z|e, t) Pr((cid:2)q|e, t, (cid:2)z) t,(cid:2)z (14) Pr(t|e)Pr((cid:2)z) Pr((cid:2)q|e, t, (cid:2)z) (cid:11)(cid:12) (cid:10)(cid:11)(cid:12)(cid:13) Pr(t|e) Pr((cid:2)z) (cid:13) (cid:10) Pr(h((cid:2)q, (cid:2)z)|t) (15) (cid:13) (cid:10) Pr(s((cid:2)q, (cid:2)z)|e) (cid:11)(cid:12) .
(4) (9) (13) To get from (14) to (15) we make the simplifying assumption that the density of hint words in queries is independent of the candidate entity and type.
As mentioned before, adding over t, (cid:2)z is feasible for telegraphic queries because they are short.
The prior Pr(e) may be uninformative (i.e., uniform), or set proportional to |Se| [22], or use shrunk estimates from answer types in the past.
We use Pr(e) = |Se|/ (cid:5) e(cid:2) |Se(cid:2)|.
If we allow a query word to represent both a type hint and a selector, the clean separation after (15) no longer works, but it is possible to extend the framework using a soft-OR expression.
We omit details owing to space constraints.
1103(cid:15) (cid:14) Guided by the generative formulation in Section 4, we partition the feature vector as follows: , (17)  1(q, e),  2(t, e),  3(q, (cid:2)z, t),  4(q, (cid:2)z, e)  (q, e, t, (cid:2)z) = where   1(q, e) models the prior for e.
   2(t, e) models the prior Pr(t|e).
   3(q, (cid:2)z, t) models the compatibility between the type    4(q, (cid:2)z, e) models the compatibility between the selec-hint part of query words and the proposed type t.
tor part of query words and Se.
In Section 4.5 we used Pr(e) = |Se|/ e(cid:2) |Se(cid:2)| as a prior probability for e.
It is natural to make this one element in  1.
But the discriminative setup allows us to introduce other powerful features.
|Se| does not distinguish between snippets that match the query well vs. poorly.
Let IDF(w) be the inverse document frequency [2] of query word w, and IDF(q) = w q IDF(w).
c   q is the set of query words found in snippet c, with total IDF(c   q) = w c q IDF(w).
Then the match-quality-weighted snippet support for e is characterized as IDF(c   q),  1(q, e)[ ] = (cid:9) (cid:5) (cid:5) (18)
 IDF(q) |q|
 c Se (cid:5) |q| where 2 IDF(q) normalizes the feature across diverse queries.
Another feature in  1 relates to negative evidence.
If there are other words present, a query that directly mentions an entity is hardly ever answered correctly by that entity; Tom Cruise could not be the answer for the query tom cruise wife.
Another (0/1) element in  1 is whether a description ( lemma ) of e is contained in the query.
In our experiments, the model element in   corresponding to this feature turns out a negative number, as expected.
(cid:16)(cid:16){e : e  + t}(cid:16)(cid:16)/|E|.
In our experi-We have already proposed one way to estimate Pr(t|e) in Section 4.1.
This estimate a natural element in  2.
We can also help the learner use the generality or speci city of types, measured as this feature: ments, the element of   corresponding to this feature also got negative values, indicating preference of speci c types over generic ones.
This corroborates earlier observation regarding the depth of desired types in a hierarchy [5].
Given the input parameters of  3((cid:2)q, (cid:2)z, t), we compute the hint word subsequence h((cid:2)q, (cid:2)z) as in (2).
Now we can de ne any number of features between these hint words and the given type t, which has lemma set L(t).
  A standard feature borrowed from (9) is Pr(h((cid:2)q, (cid:2)z)|t).
  Unlike in the generative formulation, we can add synthetic features.
E.g., a feature that has value 1 if (cid:5) matches the subsequence h((cid:2)q, (cid:2)z) exactly.
  In Section 4, the size of h((cid:2)q, (cid:2)z) was drawn from a binomial distribution controlled by hyper parameter  .
features of the form(cid:7) To model more general distributions, we use binary |h((cid:2)q, (cid:2)z)| < k otherwise

 for k = 1, .
.
., to capture the belief that smaller number of hint words is preferable.
Now consider q and its selectors s((cid:2)q, (cid:2)z)   q as word sets (no duplicates), and the snippets Se supporting candidate entity e.  4(q, (cid:2)z, e) will include feature/s that express the extent of match or compatibility between the selector words and the snippets.
We need to characterize and then combine two kinds of signals here:   The rarity (hence, informativeness) of a subset of s((cid:2)q, (cid:2)z)   The number of supporting snippets [22] that match a that match in snippets, and given word set.
(A third kind of signal, proximity [27, 37, 36], is favored indirectly, because snippets have limited width.
A more re ned treatment of proximity is left for future work.)
A snippet c   Se, interpreted as a subset of query words q, covers s((cid:2)q, (cid:2)z) if c   s((cid:2)q, (cid:2)z).
Otherwise c   s((cid:2)q, (cid:2)z).
Recall every snippet c has an IDF(c) = w c q IDF(w).
We propose two features: (cid:5)

 IDF(s((cid:2)q, (cid:2)z)) IDF(s((cid:2)q, (cid:2)z)) |{c : c   s((cid:2)q, (cid:2)z)}| c s((cid:2)q,(cid:2)z) = and

 c s((cid:2)q,(cid:2)z) IDF(q) IDF(c).
(19) (20) (cid:9) (cid:9) |q|
 We found the separation above to be superior to collapsing covering and non-covering snippets into one sum.
Another useful feature was the fraction of snippets c such that c = q (exactly matching all query words).
With a wrong choice of hint-selector partition (cid:2)z, or a wrong choice of type t, even a highly relevant response e could score very poorly.
Therefore, any reasonable scoring scheme should evaluate e under the best choice of t, (cid:2)z. I.e., the score of e should be      (q, e, t, (cid:2)z).
max t:e +t,(cid:2)z (21) (Note that t ranges over only those types to which e belongs.)
In learning to rank [21], three training paradigms are commonly used: itemwise, pairwise and listwise.
Because of the added complexity from the latent variables t, (cid:2)z, here we discuss itemwise and pairwise training.
Pairwise linear discrimination [19] remains an e ective approach for learning to rank.
Listwise training is left for future work, as is the use of nonlinear models like boosted regression trees.
In itemwise training, each response entity e is one item, which can be good (relevant, denoted e+) or bad (irrelevant, denoted e ).
Following standard max-margin methodology, we want  q, e+ :  q, e  : max t,(cid:2)z      (q, e+, t, (cid:2)z)   1    q,e+ , and      (q, e , t, (cid:2)z)   1 +  q,e  , max t,(cid:2)z (23) where  q,e+ ,  q,e    0 are the usual SVM-style slack variables.
Constraint (23) is easy to handle by breaking it up (22) 1104into the conjunct:  q, e , t, (cid:2)z :      (q, e, t, (cid:2)z)   1 +  q,e  .
(24) However, (22) is a disjunctive constraint, as also arises in multiple instance classi cation or ranking [7].
A common way of dealing with this is to modify constraint (22) into  q, e+ : u(q, e+, t, (cid:2)z)     (q, e+, t, (cid:2)z)   1    q,e+ (25) (cid:9) t,(cid:2)z (cid:9) where u(q, e, t, (cid:2)z)   {0, 1} and  q, e+ : u(q, e+, t, (cid:2)z) = 1.
t,(cid:2)z This is an integer program, so the next step is to relax the new variables to 0   u(q, e, t, (cid:2)z)   1 (i.e., the (t, (cid:2)z)-simplex).
Unfortunately, owing to the introduction of new variables u(    ) and multiplication with old variables  , the optimization is no longer convex.
Bergeron et al. [7] propose an alternating optimization: holding one of u and    xed, optimize the other, and repeat (there are no theoretical guarantees).
Note that if   is  xed, the optimization of u is a simple linear program.
If u is  xed, (cid:5) the optimization of   is comparable to training a standard SVM.
The objective would then take the form  q,e+ + e E  q | + |E 
 q | (cid:9) (cid:5) (cid:12) (cid:12)2 e+ E+

  q,e  (26) +

 q q q Q Here C > 0 is the usual SVM parameter trading o  training loss against model complexity.
Note that u does not appear in the objective.
In our application,  (q, e, t, (cid:2)z)   (cid:2)0.
Suppose     (cid:2)0 in some iteration (which easily happens in our application).
In that case, to satisfy constraint (25), it su ces to set only one element in u to 1, corresponding to arg maxt,(cid:2)z    (q, e, t, (cid:2)z), and the rest to 0s.
In other words, a particular (t, (cid:2)z) is chosen ignoring all others.
This severely restricts the search space over u,   in subsequent iterations and has greater chance of getting stuck in a local minima.
To mitigate this problem, we propose the following annealing protocol.
The u distribution collapse reduces entropy suddenly.
The remedy is to subtract from the objective (to be minimized) a term related to the entropy of the u distribution: (cid:9) (cid:9)
 u(q, e+, t, (cid:2)z) log u(q, e+, t, (cid:2)z).
(27) q,e+ t,(cid:2)z Here D   0 is a temperature parameter that is gradually reduced in powers of 10 toward zero with the alternative iterations optimizing u and  .
Note that the objective (27) is convex in u,   and  .
Moreover, with either u or    xed, all constraints are linear inequalities.
1: initialize u to random values on the simplex
 3: while not reached local optimum do


 Figure 5: Pseudocode for discriminative training.
 x u and solve quadratic program to get next   reduce D geometrically  x   and solve convex program for next u Very little changes if we extend from itemwise to pairwise training, except the optimization gets slower, because of the sheer number of pair constraints of the form:  q, e+, e  : max      (q, e+, t, (cid:2)z)   max      (q, e , t, (cid:2)z) The itemwise objective in (26) changes to the pairwise ob-jectice (cid:12) (cid:12)2

 +


 q | |E 
 q | e+ E+ q ,e E  q t,(cid:2)z   1    q,e+,e  .
(cid:9)  q,e+,e  .
t,(cid:2)z (cid:9) q Q (28) (29) ).
).
(30) For clarity,  rst we rewrite (28) as      (q, e+, t, (cid:2)z)  q, e+, e  : max t,(cid:2)z   1    q,e+,e  + max t(cid:2),(cid:2)z(cid:2)      (q, e , t(cid:8), (cid:2)z(cid:8) Then we pull out t(cid:8), (cid:2)z(cid:8) :  q, e+, e , t(cid:8), (cid:2)z(cid:8)      (q, e+, t, (cid:2)z) : max t,(cid:2)z Finally, we use a new set of u variables to convert this to an alternating optimization as before:   1    q,e+,e  +      (q, e , t(cid:8), (cid:2)z(cid:8) (cid:9) u(q, e+, t, (cid:2)z)     (q, e+, t, (cid:2)z)   1    q,e+,e  +      (q, e , t(cid:8), (cid:2)z(cid:8) t,(cid:2)z ).
 q, e+, e , t(cid:8), (cid:2)z(cid:8) : These enhancements do not change the basic nature of the optimization.
The space of (q, e, t, (cid:2)z) and especially their discriminative constraints can become prohibitively large.
To keep RAM and CPU needs practical, we used the following policies; our experimental results are insensitive to them.
  We sampled down bad (irrelevant) entities e  that   For empty h((cid:2)q, (cid:2)z) =  ,  3(q, (cid:2)z, t) provides no signal.
In such cases, we allow t to take only one value: the most generic type Entity.
were allowed to generate constraint (28).
This is even simpler in the discriminative setting than in the generative setting; we can simply use (21) to report arg maxt,(cid:2)z      (q, e, t, (cid:2)z).
Extending the above scheme, each entity e scores each candidate types t as score (t|e) = max(cid:2)z      ( , e, t, (cid:2)z).
This induces a ranking over types for each entity.
We can choose the overall type predicted by the query as the one whose sum of ranks among the top-k entities is smallest.
An apparently crude approximation would be to predict the best type for the single top-ranked entity.
But k > 1 can stabilize the predicted type, in case the top entity is incorrect.
(We may want to predict a single type as a feedback to the user, or to compare with other type prediction systems, but, as we shall see, not for the best quality of entity ranking, which is best done collectively.)
Our type and entity catalog was YAGO [35], with about
 on mentions of these entities in Wikipedia2 was applied [12] over a Web corpus from a commercial search engine, having
 billion entity annotations, average 16 annotations per page.
These were then indexed [13].
The index supports semistructured queries speci ed by:   an answer type t from among the 200,000 YAGO types,   a bag of words and phrases in a IDF-WAND (weak-and) operator [11], and   a snippet window width.
A DAAT [11] query processor returns a stream of snippets at most as wide as the given window width limit, that contain a mention of some entity e  + t and satis es the WAND predicate.
In case of phrases in the query, the WAND threshold is computed by adding the IDF of constituent words.
Our query processor is implemented using MG4J [9] in Java, with no index caching.
Basic keyword WAND queries take a few seconds over 500 million documents.
Setting t = Entity, the root type, and asking for a stream of all entities in qualifying snippets, slows down the query by a small factor.
A discriminative snippet scoring and aggregation technique [34] achieves entity ranking accuracy superior to recent approaches.
We use 709 entity search queries collected from many years of TREC and INEX competitions, along with relevant and irrelevant entities.
Two paid masters students, familiar with Web search engines, read the full TREC/INEX description of entity search queries and wrote out queries they would naturally issue to a commercial search engine.
They also selected the best (as per their judgment) type from YAGO for each query, as ground truth.
The distribution of types is heavy-tailed, with 69% of the atypes in this list occurring only once and top four atypes accounting for one third of queries.
The atypes towards top are mostly generic (location, person, etc.
), while those toward the bottom are more speci c (Brooklyn Dodgers players, Dilbert characters etc.
).
This data is publicly available at bit.ly/WSpxvr.
Launching the queries with the known types resulted in 380 million snippets supporting candidate entities; these are also available on request.
We also performed type prediction (Section 5.6.3) on dataset provided in [5].
Since this dataset does not contain ground truth of relevant entities for each query, we did not test entity ranking.
The ranking accuracy of a reasonable query interpreter algorithm in our framework will lie between two baselines: Generic: The generic baseline assumes zero knowledge of query types, instead using t = Entity, the root/s of the type hierarchy in the catalog.
 Perfect : The  perfect  baseline assumes complete (human-provided) knowledge of the type and uses it in the semistructured query launched over the catalog and annotated corpus.
Of course, even perfect  may perform poorly in some queries, because of lack of support for relevant entities in the corpus, snippets incorrectly or not annotated (both false positive and negative), incorrect absence of paths between types and entities in the catalog, or some inadequacy of the type-constrained entity ranker.
It is also possible for an algorithm (including ours) to perform worse than generic on some queries, by choosing a particularly unfortunate type, but obviously it should do better than generic on average, to be useful.
As is standard in entity ranking research, we report NDCG at various ranks, mean reciprocal rank (MRR, not truncated) and mean average precision (MAP) at the entity (not document) level.
Space constraints prevent us from de n-ing these; see Liu [21] for details.
For Discriminative, C is tuned by 5-fold cross validation at the query level.
For Generative, we swept over  ,  ,  ,   in powers of 10 (e.g.
 4, .
.
.
, 1).
 5, 10

     Generic Generative Discriminative Perfect

  
  


 Figure 6: Generic, generative, discriminative and  perfect  accuracies.
For our techniques to be useful, they must bridge a substantial part of the gap between the generic lower bound and the perfect upper bound.
Figure 6 con rms that Generative bridges 28% of the MAP gap between generic and perfect, whereas discriminative is signi cantly better at 43%.
MRR and NDCG follow similar trends.
All gaps are statistically signi cant at 95% con dence level (indicated by  ).
Figure 6 is aggregated over all queries.
Figure 7 focuses on average precision disaggregated into queries, comparing discriminative against generic.
While some queries are damaged by discriminative, many more are improved.
Failure analysis revealed residual (t, (cid:2)z) ambiguity, coupled with lack of  + or  + paths in an incomplete catalog to be the major reasons for losses on some queries.
Even though there is some ground yet to cover to reach  perfect  levels, these results show there is much hope for automatically interpreting even telegraphic queries.
Figure 8 shows that discriminative with our entropy-based annealing protocol performs signi cantly (marked with  )
 A type prediction may be less than ideal, and yet entity prediction may be  ne.
One can take the top type predicted by B&N, and launch an entity query (see Section 6.1.2) with that type restriction.
To improve recall, we can also take the union of the top k predicted types.
The result is a ranked list of entities, on which we can compute entity-level MAP, MRR, NDCG, as usual.
In this setting, both B&N and our algorithm (discriminative) used YAGO as the catalog.
Results for our dataset (Section 6.2) are shown in Figure 10.
k MAP MRR %Q better %Q worse













 Generic 0.323 0.432  






   Figure 10: B&N-driven entity ranking accuracy.
We were surprised to see the low entity ranking accuracy of B&N (which is why we recreated very closely their reported type ranking accuracy on DBpedia).
Closer scrutiny revealed that the main reason for lower accuracy was changing the type catalog from DBpedia (358 types) to YAGO (over 200,000 types).
Entity ranking accuracy is low because B&N s type prediction accuracy is very low on YAGO: 0.04 MRR, 0.04 MAP, and 0.058 NDCG@10.
For comparison, our type prediction accuracy is 0.348 MRR, 0.348 MAP, and
 signal: if we switch o  snippet-based features  4, our accuracy also plummets.
The moral seems to be, large organic type catalogs provide enough partial and spurious matches for any choice of hints, so it is essential (and rewarding) to exploit corpus signals.
A minimally modi ed B&N that uses the corpus may replace Wikipedia entity descriptions with corpus-driven descriptions, i.e., a pseudo-document made up of all snippets retrieved for a particular entity from the corpus.
As we see in Figure 11, ranking accuracy improves marginally.
This indicates that in the case of Web-scale entity search, an imperfectly annotated corpus can prove to be more useful than a small human-curated information source.
k MAP MRR %Q better %Q worse













 Generic 0.323 0.432  






   Figure 11: B&N-driven entity ranking accuracy with corpus-driven entity description.
On an average, B&N type prediction, followed by query launch, seems worse than generic.
This is almost entirely because of choosing bad types for many, but not all queries.
There are queries where B&N shows a (e.g., MAP) lift beyond generic, but they are just too few (Figure 12).
Figure 7: MAP of discriminative minus map of generic, compared query-wise between generic and discriminative.
Below zero means discriminative did worse than generic on that query.
Queries in (arbitrary) order of discriminative AP gain.
better than the scheme proposed by Bergeron et al.[7].
This may be of independent interest in multiple instance ranking and max-margin learning with latent variables.
Bergeron (26) Entropy (27)

  
  


 Figure 8: Bene ts of annealing protocol.
B&N [5] proposed two models, of which the  entity-centric  model was generally superior.
Each entity e was associated with a textual description (e.g., Wikipedia page) which induced a smoothed language model  e.
B&N estimate the score of type t as (cid:5) Pr(q|t) = e +t Pr(q| e) Pr(e|t), (31) where Pr(e|t) was set to uniform.
Note that no corpus (apart from the one of entity descriptions) was used.
The output of B&N s algorithm (hereafter,  B&N ) is a ranked list of types, not entities.
We implemented B&N, and obtained accuracy closely matching their published numbers, using the DBpedia catalog with 358 types, and 258 queries (di erent from our main query set and testbed).
B&N Discr(k = 1) Discr(k = 5) Discr(k = 10)



 Figure 9: Type prediction by B&N vs. discrimina-tive.
We turned our system into a type predictor (Section 5.6.3), and also used DBpedia like B&N and compared type prediction accuracy on dataset provided in [5].
Results are shown in Figure 9 after including the top k returned types.
At k = 1, our discriminative type prediction matches B&N, and larger k performs better, owing to stabilizing consensus from lower-ranked entities.
Coupled with the results in Section 6.4.6, this is strong evidence that our uni ed formulation is superior, even if the goal is type prediction.
Discr 0.422 0.437 Figure 14: Entity ranking accuracy using DBpedia types.
We also tried to use the Web interface to send a sample of our telegraphic queries and their well-formed sentence counterparts to DEANNA [39] and receive back the interpretation.
We manually inspected their output.
Some anecdotes are shown in Figure 15.
The queries are from Figure 3.
None of the telegraphic queries was successfully interpreted.
The well-formed questions saw partial success.
QID Well-formed Q1 Missing target type

 Telegraphic Empty Incorrect fragments Empty Incorrect, missed Wikipedia type  list of cities in China  Incorrect (painting) Incorrect fragments Incorrect fragments target type

 Q6 No target type Figure 15: DEANNA interpretations of some of our queries.
Incorrect fragments Empty Empty

 We initiated a study of generative and discriminative formulations for joint query interpretation and response ranking, in the context of targeted-type entity search needs expressed in a natural  telegraphic  Web query style.
Using
 lion documents annotated at 8 billion places with over 1.5 million entities and 200,000 types from YAGO, we showed experimentally that jointly interpreting target type and ranking responses is superior to a two-phase interpret-then-execute paradigm.
Our work opens up several directions for further research.
Our notion of selectors can be readily generalized to allow mentions of entities as literals [15, 26] in the query.
More sophisticated training using bundle methods may further improve the discriminative formulation.
Finally, modeling list-wise [21] losses, and/or exploring more powerful nonlinear scoring functions (e.g., via boosting) may also help.
Figure 12: 2-stage entity ranking via B&N does boost accuracy for some queries, but the overall effect is negative.
Joint interpretation and ranking also damages some queries but improves many more.
The bene cial role of the corpus is now established, but is joint inference really necessary, if a good query type interpreter were available?
To test this in a controlled setting, we run our system, throw away the ranked entity list, and only retain the predicted type (Section 5.6.3), then launch a query restricted to this type (Section 6.1.2) and measure entity ranking accuracy.
Joint 2-stage 2-stage  
  
 2-stage (k = 1) (k = 5) (k = 10)  
  
  
  
 Figure 13: Joint inference improves entity ranking quality compared to 2-stage.
Figure 13 shows that the result is signi cantly (shown by  ) less accurate than via joint inference, even after tuning k, which indicates that no single inferred type may retain enough information for the best entity ranking, and that joint inference is indeed vital.
A plausible counterargument to the above experiments is that, by moving from only 358 DBpedia types to over 20,000 YAGO types, we are making the type prediction problem hopelessly di cult for B&N, and that this level of type re nement is unnecessary for high accuracy in entity search.
We modi ed our system to use types from DBpedia, and correspondingly re-indexed our Web corpus annotations using DBpedia types.
As partial con rmation of the above hypothesis, the entity ranking accuracy using B&N did increase substantially.
However, as shown in Figure 14, the entity ranking accuracy achieved by our discriminative algorithm remains unbeaten.
Also compare with Figure 6   whereas B&N improves by coarsening the type system, our discriminative algorithm seems to be degraded by this move.
[24] J. W. Murdock, A. Kalyanpur, C. Welty, J.
Fan, D. A.
Ferrucci, D. C. Gondek, L. Zhang, and H. Kanayama.
Typing candidate answers using type coercion.
IBM Journal of Research and Development, 56(3/4):7:1 7:13, 2012.
[25] P. Pantel and A. Fuxman.
Jigs and lures: Associating web queries with structured entities.
In ACL Conference, pages
 [26] P. Pantel, T. Lin, and M. Gamon.
Mining entity types from query logs via user intent modeling.
In ACL Conference, pages 563 571, Jeju Island, Korea, July 2012.
[27] D. Petkova and W. B. Croft.
Proximity-based document representation for named entity retrieval.
In CIKM, pages

 [28] J.
Pound, A. K. Hudek, I. F. Ilyas, and G. Weddell.
Interpreting keyword queries over Web knowledge bases.
In
 [29] J.
Pound, I. F. Ilyas, and G. Weddell.
Expressive and  exible access to Web-extracted data: a keyword-based structured query language.
In SIGMOD Conference, pages

 [30] J.
Pound, S. Paparizos, and P. Tsaparas.
Facet discovery for structured Web search: a query-log mining approach.
In SIGMOD Conference, pages 169 180, 2011.
[31] H. Raviv, D. Carmel, and O. Kurland.
A ranking framework for entity oriented search using Markov random  elds.
In Joint International Workshop on Entity-Oriented and Semantic Search, pages 1:1 1:6, Portland, OR, 2012.
ACM.
Located with SIGIR Conference.
[32] S. Sarawagi.
Information extraction.
FnT Databases, 1(3),
 [33] N. Sarkas, S. Paparizos, and P. Tsaparas.
Structured annotations of Web queries.
In SIGMOD Conference, 2010.
[34] U. Sawant and S. Chakrabarti.
Features and aggregators for web-scale entity search.
arXiv 1303.3164, 2013.
[35] F. M. Suchanek, G. Kasneci, and G. Weikum.
YAGO: A core of semantic knowledge unifying WordNet and Wikipedia.
In WWW Conference, pages 697 706.
ACM Press, 2007.
[36] K. M. Svore, P. H. Kanani, and N. Khan.
How good is a span of terms?
exploiting proximity to improve Web retrieval.
In SIGIR Conference, pages 154 161.
ACM, 2010.
[37] T. Tao and C. Zhai.
An exploration of proximity measures in information retrieval.
In SIGIR Conference, pages

 [38] D. Vallet and H. Zaragoza.
Inferring the most important types of a query: a semantic approach.
In SIGIR Conference, pages 857 858.
ACM, 2008.
[39] M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath, V. Tresp, and G. Weikum.
Natural language questions for the Web of data.
In EMNLP Conference, pages 379 390, Jeju Island, Korea, July 2012.
[40] C.-N. J. Yu and T. Joachims.
Learning structural SVMs with latent variables.
In ICML, pages 1169 1176.
ACM,
 [41] C. Zhai.
Statistical language models for information retrieval: A critical review.
Foundations and Trends in Information Retrieval, 2(3):137 213, Mar.
2008.
