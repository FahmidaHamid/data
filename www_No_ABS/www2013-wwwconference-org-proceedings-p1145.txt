There is a large and increasing quantity of structured data available on the Web.
Traditional information retrieval approaches based on keyword search are user-friendly but can-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
not exploit the internal structure of data due to their bag-of-words semantic.
For searching information on the Data Web we need similar user friendly approaches i.e. keyword-base interfaces, which leverage the internal structure of the data.
Also, Question Answering is a specialized form of information retrieval.
A Question Answering system attempts to extract correct answers to questions posed in natural language.
Using the structure of data in retrieval process has two prominent advantages.
Firstly, it approaches the information retrieval systems to question answering systems.
Secondly, it enables us to easily integrate information from di erent datasets.
In this paper we present an approach for question answering over a set of interlinked data sources.
We have to deal with two challenges: A  rst challenge is that information for answering a certain question can be spread among di erent datasets employing heterogeneous schemas.
This makes the mapping of the input keywords to resources more challenging when compared to querying a single dataset.
The second challenge is constructing a formal query from the matched resources across di erent datasets by exploiting links between the di erent datasets on the schema and instance levels.
In order to address these challenges, our approach resembles a horizontal search, where query segments derived from an input query are matched against all available datasets.
We employ a Hidden Markov Model (HMM) to obtain the optimal input query segmentation and disambiguation of possible matches in a single step.
We test di erent bootstrapping methods for the HMM parameters using various distributions (Normal, Zipf, Uniform) as well as an algorithm based on Hyperlink-Induced Topic Search (HITS).
Our proposed functions for HMM parameters produce the best results for both segmentation and disambiguation.
Then, we construct a formal query (expressed in SPARQL) using the disambiguated matches by traversing links in the underlying datasets.
By taking links between the matched resources (including owl:sameAs links) into account we obtain the minimum spanning graph covering all matches in the di erent datasets.
As a test bed for evaluating our approach we used the Sider 1, Diseasome [8]2 and Drugbank [31]3 datasets published in RDF.
Sider contains information about drugs and their side e ects.
Diseasome contains information about diseases and genes associated with these diseases.
Drugbank 1http://sidee ects.embl.de/ 2http://diseasome.kobic.re.kr/ 3http://www.drugbank.ca/ 1145is a comprehensive knowledge base containing information about drugs, drug target (i.e. protein) information, interactions and enzymes.
As it can be seen in Figure 1 the classes representing drugs in Drugbank and Sider are linked using owl:sameAs and diseases from Diseasome are linked to drugs in Drugbank using possible Drug and possible Disease target.
Diseases and side e ects between Sider and Disea-some are linked using the owl:sameAs property.
Note that in this  gure the dotted arrows represent the properties between classes inside a dataset.
Our approach can answer queries with the following three characteristics:   Queries requiring fused information: An example is the query:  side e ects of drugs used for Tuberculosis .
Tuberculosis is de ned in Diseasome, drugs for curing Tuberculosis are described in Drugbank, while we  nd their side e ects in Sider.
  Queries targeting combined information: An example depicted in Figure 2 is the query:  side e ect and enzymes of drugs used for ASTHMA .
Here the answer to that query can only be obtained by joining data from Sider (side e ects) and Drugbank (enzymes, drugs).
  Query requiring keyword expansion: An example is the query  side e ects of Valdecoxib .
Here the drug Valdecoxib can not be found in Sider, however, its synonym Bextra is available via Sider.
To the best of our knowledge our approach is the  rst approach for answering questions on interlinked datasets by constructing a federated SPARQL query.
Our main contributions can be summed up as follows:   We extend the Hidden Markov Model approach for dis-ambiguating resources from di erent datasets.
  We present a novel method for constructing formal queries using disambiguated resources and leveraging the interlinking structure of the underlying datasets.
  We developed a benchmark consisting of 25 queries for a testbed in the life-sciences.
The evaluation of our implementation demonstrates its feasibility with an f-measure of 90%.
This paper is organized as follows: In the subsequent section, we present the problem at hand in more detail and some of the notations and concepts used in this work.
Section 3 presents the proposed disambiguation method in detail along with the evaluation of the bootstrapping.
In section 4, we then present the key steps of our algorithm for constructing a conjunctive query.
Our evaluation results are presented in the section 5 while related work is reviewed in the section 6.
We close with a discussion and future work.
In this section, we introduce some crucial notions employed throughout the paper and describe the main challenges that arise when transforming user queries to formal, conjunctive queries on linked data.
Figure 1: Schema interlinking for three datasets i.e.
DrugBank, Sider, Diseasome.
Figure 2: Resources from three di erent datasets are fused at the instance level in order to exploit information which are spread across diverse datasets.
An RDF knowledge base can be viewed as a directed, labeled graph Gi = (Vi, Ei) where Vi is a set of nodes comprising all entities and literal property values, and Ei is a set of directed edges, i.e.
the set of all properties.
We de ne linked data in the context of this paper as a graph G = (V =(cid:83) Vi, E =(cid:83) Ei) containing a set of RDF knowledge bases, which are linked to each other in the sense, that their sets of nodes overlap, i.e. that Vi   Vj (cid:54)=  .
In this work we focus on user-supplied queries in natural language, which we transform into an ordered sets of keywords by tokenizing, stop-word removal and lemmatiza-tion.
Our input query thus is an n-tuple of keywords, i.e.
Q = (k1, k2, ..., kn).
Challenge 1: Resource Disambiguation.
In the  rst step, we aim to map the input keywords to a suitable set of entity identi ers, i.e. resources R = {r1, r2...rm}.
Note, that several adjacent keywords can be mapped to a single resource, i.e. m   n. In order to accomplish this task, the input keywords have to be grouped together to segments.
For each segment, a suitable resource is then to be determined.
The challenge here is to determine the right segment granularity, so that the most suitable mapping to identi ers in the underlying knowledge base can be retrieved for constructing a conjunctive query answering the input query.
For example, the question  What is the side e ects of drugs is transformed to the 4-keyword used for Tuberculosis?  Diseasome Sider Drugs sameAs Disease Drug Side Effect Genes enzymes Drug interactions references targets DrugBank Diseasome Drug Asthma ?v0 side effect sameAs a ?v2 ?v3 Disease Drug Side Effect a a a ?v1 enzyme Enzymes a Sider DrugBank 1146tuple (side, e ect, drug, Tuberculosis).
This tuple can be segmented into ( side e ect drug ,  Tuberculosis  ) or ( side e ect ,  drug ,  Tuberculosis  ).
Note that the second segmentation is more likely to lead to a query that contains the results intended by the user.
In addition to detecting the right segments for a given input query, we also have to map each of these segments to a suitable resource in the underlying knowledge base.
This step is dubbed entity disambiguation and is of increasing importance since the size of knowledge bases and schemes heterogeneity on the Linked Data Web grows steadily.
In this example, the segment  drug  is ambiguous when querying both Sider and Diseasome because it may refer to the resource diseasome:Tuberculosis describing the disease Tuberculosis or to the resource sider:Tuberculosis being the side e ect caused by some drugs.
Challenge 2: Query Construction.
Once the segmentation and disambiguation have been completed, adequate SPARQL queries have to be generated based on the detected resources.
In order to generate a conjunctive query, a connected subgraph G(cid:48) = (V (cid:48), E(cid:48)) of G called the query graph has to be determined.
The intuition behind constructing such a query graph is that it has to fully cover the set of mapped resources R = {r1, ..., rm} while comprising a minimal number of vertices and edges (|V (cid:48)| + |E(cid:48)|).
In linked data, mapped resources ri may belong to di erent graphs Gi; thus the query construction algorithm must be able to traverse the links between datasets at both schema and instance levels.
With respect to the previous example, after applying disambiguation on the identi ed resources, we would obtain the following resources from di erent datasets: sider:sideEffect, diseasome:possibleDrug and diseasome:1154.
The appropriate conjunctive query contains the following triple patterns:
 2.
?v1 ?v2 .
3.
?v2 ?v3 .
The second triple pattern bridges between the datasets Drug-bank and Sider.
owl:sameAs sider:sideEffect In this section, we present the formal notations for addressing the resource disambiguation challenge, aiming at mapping the n-tuple of keywords Q = (k1, k2, ..., kn) to the m-tuple of resources R = (r1, ..., rm).
Definition 1 (Segment and Segmentation).
For a given query Q = (k1, k2, ..., kn), the segment S(i,j) is the sequence of keywords from start position i to end position j, i.e., S(i,j) = (ki, ki+1, ..., kj).
A query segmentation is an m-tuple of segments SG(Q) = (S(0,i), S(i+1,j), ..., S(l,n)) with non-overlapping segments arranged in a continuous order, i.e. for two continuous segments Sx, Sx+1 : Start(Sx+1) = End(Sx) + 1.
The concatenation of segments belonging to a segmentation forms the corresponding input query Q.
Definition 2 (Resource Disambiguation).
Let the seg-(0,i), S2 (i+1,j), ..., Sx mentation SG(cid:48) = (S1 (l,n)) be the suitable segmentation for the given query Q.
Each segment Si of SG(cid:48) is  rst mapped to a set of candidate resources Ri = {r1, r2...rh} from the underlying knowledge base.
The aim of the disambiguation step is to detect an m-tuple of resources (r1, r2, ..., rm)   R1   R2   .
.
.
  Rm from the Cartesian product of the sets of candidate resources for which each ri Valid Segments side e ect drug tuberculosis Samples of Candidate Resources
 1. drugbank:drugs 3. sider:drugs 1. diseases:1154
 4. diseasome:possibledrug
 Table 1: Generated segments and samples of candidate resources for a given query.
Data: q: n-tuple of keywords, knowledge base Result: SegmentSet: Set of segments
 2 start=1; 3 while start <= n do
 i = start; while S(start,i) is valid do SegmentSet.add(S(start,i)); i++; end start++;




 10 end Algorithm 1: Naive algorithm for determining all valid segments taking the order of keywords into account.
has two important properties: First, it is among the highest ranked candidates for the corresponding segment with respect to the similarity as well as popularity and second it shares a semantic relationship with other resources in the m-tuple.
Semantic relationship refers to the existence of a path between resources.
The disambiguated m-tuple is appropriate if a query graph [capable of answering the input query] can be constructed using all resources contained in that m-tuple.
The order in which keywords appear in the original query is partially signi cant for mapping.
However, once a mapping from keywords to resources is established the order of the resources does not a ect the SPARQL query construction anymore.
This is a fact that users will write strongly related keywords together, while the order of only loosely related keywords or keyword segments may vary.
When considering the order of keywords, the number of segmentations for a query Q consisting of n keywords is 2(n 1).
However, not all these segmentations contain valid segments.
A valid segment is a segment for which at least one matching resource can be found in the underlying knowledge base.
Thus, the number of segmentations is reduced by excluding those containing invalid segments.
Algorithm 1 shows a naive approach for  nding all valid segments when considering the order of keywords.
It starts with the  rst keyword in the given query as  rst segment, then adds the next keyword to the current segment and checks whether this addition would render the new segment invalid.
This process is repeated until we reach the end of the query.
The input query is usually short.
The number of keywords is mainly less than 6 4; therefore, this algorithm is not expensive.
Table 1 shows the set of valid segments along with some samples of the candidate resources computed for the previous example using the naive algorithm.
Note that  side e ect drug ,  side ,  e ect  are not a valid segments.
4http://www.keyworddiscovery.com/keyword- stats.html?date=2012-08-01
 The second challenge addressed by this paper tackles the problem of generating a federated conjunctive query leveraging the disambiguated resources i.e. R = (r1, ..., rm).
Herein, we consider conjunctive queries being conjunctions of SPARQL algebra triple patterns5.
We leverage the disam-biguated resources and implicit knowledge about them (i.e.
types of resources, interlinked instances and schema as well as domain and range of resources with the type property) to form the triple patterns.
For instance, for the running query which asks for a list of resources (i.e. side e ects) which have a speci c characteristic in common (i.e. caused by drugs used for Tuberculosis ).
Suppose the resources identi ed during the disambiguation process are: sider:sideEffect, Diseasome:possibleDrug as well as Diseasome:1154.
Suitable triple patterns which are formed using the implicit knowledge are:
 ?v2 .
2.
?v1 3.
?v2 ?v3 .
owl:sameAs sider:sideEffect The second triple pattern is formed based on interlinked data information.
This triple connects the resources with the type drug in the dataset Drugbank to their equivalent resources with the type drug in the Sider dataset using owl:sameAs link.
These triple patterns satisfy the information need expressed in the input query.
Since most of common queries commonly lack of a quanti er, thus conjunctive queries to a large extend capture the user information need.
A conjunctive query is called query graph and formally de- ned as follows.
Definition 3 (Query Graph).
Let a set of resources R = {r1, ..., rn} (from potentially di erent knowledge bases) be given.
A query graph QGR = (V (cid:48), E(cid:48)) is a directed, connected multi-graph such that R   E(cid:48)   V (cid:48).
Each edge e   E(cid:48) is a resource that represents a property from the underlying knowledge bases.
Two nodes n and n(cid:48)   V (cid:48) can be connected by e if n (resp.
n(cid:48)) satis es the domain (resp.
range) restrictions of e. Each query graph built by these means corresponds to a set of triple patterns.
i.e.
QG   {(n, e, n(cid:48))|(n, n(cid:48))   V 2   e   E}.
In this section we describe how we use a HMM for the concurrent segmentation of queries and disambiguation of resources.
First, we introduce the notation of HMM parameters and then we detail how we bootstrap the parameters of our HMM for solving the query segmentation and entity disambiguation problems.
Hidden Markov Models: Formally, a hidden Markov model (HMM) is a quintuple   = (X, Y, A, B,  ) where:   X is a  nite set of states.
In our case, X is a subset of the resources contained in the underlying graphs.
  Y denotes the set of observations.
Herein, Y equals to the valid segments derived from the input n-tuple of keywords.
RDF and SPARQL speci cations, such as graph pattern, triple pattern and RDF graph.
  A : X   X   [0, 1] is the transition matrix of which each entry aij = is the transition probability P r(Sj|Si) from state i to state j;   B : X   Y   [0, 1] represents the emission matrix.
Each entry bih = P r(h|Si) is the probability of emitting the symbol h from state i;     : X   [0, 1] denotes the initial probability of states.
Commonly, estimating the hidden Markov model parameters is carried out by employing supervised learning.
We rely on bootstrapping, a technique used to estimate an unknown probability distribution function.
Speci cally, we bootstrap6 the parameters of our HMM by using string similarity metrics (i.e., Levenshtein and Jaccard ) for the emission probability distribution and more importantly the topology of the graph for the transition probability.
The results of the evaluation show that by using these bootstrapped parameters, we achieve a mean reciprocal rank (MRR) above 84%.
Constructing the State Space: A-priori, the state space should be populated with as many states as there are entities in the knowledge base.
The number of states in X is thus potentially large given that X will contain all RDF resources contained in the graph G on which the search is to be carried out, i.e. X = V   E. For DBpedia, for example, X would contain more than 3 million states.
To reduce the number of states, we exclude irrelevant states based on the following observations: (1) A relevant state is a state for which a valid segment can be observed (we described the recognition of valid segments in Section 2.1).
(2) A valid segment is observed in a state if the probability of emitting that segment is higher than a given threshold  .
The probability of emitting a segment from a state is computed based on the similarity score which we describe in Section 3.1.
Thus, we can prune the state space such that it contains solely the subset of the resources from the knowledge bases for which the emission probability is higher than  .
In addition to these states, we add an unknown entity state (UE) which represents all entities that were pruned.
Based on this construction of state space, we are now able to detect likely segmentations and disambiguation of resources, the segmentation being the labels emitted by the elements of the most likely sequence of states.
The dis-ambiguated resources are the states determined as the most likely sequence of states.
Extension of State Space with reasoning: A further extension of the state space can be carried out by including resources inferred from lightweight owl:sameAs reasoning.
We precomputed and added the triples inferred from the symmetry and transitivity property of the owl:sameAs relation.
Consequently, for extending the state space, for each state representing a resource x we just include states for all resources y, which are in an owl:sameAs relation with x.
Our bootstrapping approach for the model parameters A and   is based on the HITS algorithm and semantic relations between resources in the knowledge base.
The rationale is that the semantic relatedness of two resources can de ned in
 the QALD benchmark 2012 training dataset.
1148terms of two parameters: the distance between the two re sources and the popularity of each of the resources.
The distance between two resources is the path length between those resources.
The popularity of a resource is simply the connectivity degree of the resource with other resources available in the state space.
We use the HITS algorithm for transforming these two values to hub and authority values (as detailed below).
An analysis of the bootstrapping shows signi cant improvement of accuracy due to this transformation.
In the following, we  rst introduce the HITS algorithm, since it is employed within the functions for computing the two HMM parameters A and  .
Then, we discuss the distribution functions proposed for each parameter.
Finally, we compare our bootstrapping method with other well-known distribution functions.
Hub and Authority of States.
Hyperlink-Induced Topic Search (HITS) is a link analysis algorithm that was developed originally for ranking Web pages [13].
It assigns a hub and authority value to each Web page.
The hub value estimates the value of links to other pages and the authority value estimates the value of the content on a page.
Hub and authority values are mutually interdependent and computed in a series of iterations.
In each iteration the authority value is updated to the sum of the hub scores of each referring page; and the hub value is updated to the sum of the authority scores of each referring page.
After each iteration, hub and authority values are normalized.
This normalization process causes these values to converge eventually.
Since RDF data forms a graph of linked entities, we employ a weighted version of the HITS algorithm in order to assign di erent popularity values to the states based on the distance between states.
We compute the distance between states employing weighted edges.
For each two states Si and Sj in the state space, we add an edge if there is a path of maximum length k between the two corresponding resources.
Note that we also take property resources into account when computing the path length.The weight of the edge between the states Si and Sj is set to wi,j = k   pathLength(i, j), where pathLength(i, j) is the length of the path between the corresponding resources.
The authority of a state can now wi,j   hub(Si).
The hub wi,j   auth(Si).
These de nitions of hub and authority for states are the foundation for computing the transition and initial probabilities in the HMM.
be computed by: auth(Sj) = (cid:80) value of a state is given by hub(Sj) = (cid:80) Si Si Transition Probability.
To compute the transition probability between two states, we take both, the connectivity of the whole of space state as well as the weight of the edge between the two states, into account.
The transition probability value decreases with increasing distance between states.
For example, transitions between entities in the same triple have a higher probability than transitions between entities in triples connected through auxiliary intermediate entities.
In addition to edges representing the shortest path between entities, there is an edge between each state and the unknown entity (UE) state.
The transition probability of state Sj following state Si is denoted as aij = P r(Sj|Si).
Note that the condition(cid:80) P r(Sj|Si) = 1 holds.
Sj The transition probability from the state Si to UE is de- ned as: aiU E = P r(U E|Si) = 1   hub(Si) Consequently, a good hub has a smaller probability of transition to UE.
The transition probability from the state Si to the state Sj is computed by: aij = P r(Sj|Si) = auth(Sj) auth(Sk)   hub(Si) (cid:80)  aik>0 Here, the probability from state Si to the neighboring states are uniformly distributed based on the authority values.
Consequently, states with higher authority values are more probable to be met.
(cid:80) Initial Probability.
The initial probability  (Si) is the probability that the model assigns to the initial state Si in the beginning.
The initial probabilities ful ll the condition  (Si) = 1.
We denote states for which the  rst keyword  Si is observable by InitialStates.
The initial states are de ned as follows:  (Si) = auth(Si) + hub(Si) (auth(Sj) + hub(Sj)) (cid:80)  Sj InitialStates In fact,  (Si) of an initial state is uniformly distributed on both hub and authority values.
Emission Probability.
Both the labels of states and the segments contain sets of words.
For computing the emission probability of the state Si and the emitted segment h, we compare the similarity of the label of state Si with the segment h in two levels, namely string-similarity and set-similarity level:   The string-similarity level measures the string similarity of each word in the segment with the most similar word in the label using the Levenshtein distance.
  The set-similarity level measures the di erence between the label and the segment in terms of the number of words using the Jaccard similarity.
Our similarity score is a combination of these two metrics.
Consider the segment h = (ki, ki+1, ..., kj) and the words from the label l divided into a set of keywords M and stop-words N , i.e. l = M   N .
The total similarity score between keywords of a segment and a label is then computed as follows: j(cid:80) bih = P r(h|Si) = t=i argmax ( (mi, kt)) mi M |M   h| + 0.1   |N| This formula is essentially an extension of the Jaccard similarity coe cient.
The di erence is that we use the sum of the string-similarity score of the intersections in the numerator instead of the cardinality of intersections.
As in the Jaccard similarity, the denominator comprises the cardinal-ity of the union of two sets (keywords and stopwords).
The di erence is that the number of stopwords is down-weighted by the factor 0.1 to reduce their in uence since they do not convey much supplementary semantics.
Viterbi Algorithm for the K-best Set of Hidden States.
The optimal path through the HMM for a given sequence (i.e. input query keywords) generates disambiguated resources which form a correct segmentation.
The Viterbi algorithm or Viterbi path [28] is a dynamic programming
 given input sequence.
It discovers the most likely sequence of underlying hidden states that might have generated a given sequence of observations.
This discovered path has the maximum joint emission and transition probability of the involved states.
The sub-paths of this most likely path also have the maximum probability for the respective sub sequence of observations.
The naive version of this algorithm just keeps track of the most likely path.
We extended this algorithm using a tree data structure to store all possible paths generating the observed query keywords.
Thus, our implementation can provide a ranked list of all paths generating the observation sequence with the corresponding probability.
After running the Viterbi algorithm for our running example, the disambiguated resources are: {sider:sideE ect, dis-easome:possibleDrug, diseases:1154} and consequently the detected segmentation is: {side e ect, drug, Tuberculosis}.
We evaluated the accuracy of our approximation of the transition probability A (which is basically a kind of uniform distribution) in comparison with two other distribution functions, i.e., Normal and Zip an distributions.
Moreover, to measure the e ectiveness of the hub and authority values, we ran the distribution functions with two di erent inputs, i.e. distance and connectivity degree values as well as hub and authority values.
Note that for a given edge the source state is the one from which the edge originates and the sink state is the one where the edge ends.
We ran the distribution functions separately with X being de ned as the weighted sum of the normalized distance between two states and normalized connectivity degree of the sink state: Xij =   distance(Si Sj ) + (1   )  (1  connectivityDegreeSj ).
Similarly, Y was de ned as the weighted sum of the hub of the source state and the authority of the sink state: Y =     hub(Si) + (1    )   (1   authorithysj ).
In addition, to measuring the e ectiveness of hub and authority, we also measured a similar uniform function with the input parameters distance and connectivity degree de ned as:   connectivitydegree(Si) aij = distance(Si   Sj) distance(Si   Sk) (cid:80)  Sk>0 Given that the model at hand generates and scores a ranked list of possible tuples of resources, we compared the results obtained with the di erent distributions by looking at the mean reciprocal rank (MRR) [29] they achieve.
For each query qi   Q in the benchmark, we compare the rank ri assigned by di erent algorithms with the correct tuple of resources and set M RR(A) = 1|Q| .
Note that if (cid:80)
 ri qi the correct tuple of resources was not found, the reciprocal rank was assigned the value 0.
We used 11 queries from QALD2-Benchmark 2012 training dataset for boot-strapping7.
Figure 3 shows the M RR achieved by bootstrapping the transition probability of this model with 3 di er-ent distribution functions per query in 14 di erent settings.
Figure 4 compares the average M RR for di erent functions employed for bootstrapping the transition probability per setting.
Our results show clearly that the proposed function is superior to all other settings and achieves an MRR of approximately 81%.
A comparison of the MRR achieved
 http://www.sc.cit-ec.uni-bielefeld.de/qald-2 Figure 4: Comparison of di erent functions and settings for bootstrapping the transition probability.
Uni stands for the uniform distribution, while Zip stands for the Zip an and Norm for the normal distribution.
when using hub and authority with that obtained when using distance and connectivity degree reveals that using hub and authority leads to an 8% improvement on average.
This di erence is in Zip an and Normal settings trivial, but very signi cant in the case of a uniform distribution.
Essentially, HITS fairly assigns quali cation values for the states based on the topology of the graph.
We bootstrapped the emission probability B with two distribution functions based on (1) Levenshtein similarity metric, (2) the proposed similarity metric as a combination of the Jaccard and Levenshtein measures.
We observed the M RR achieved by bootstrapping the emission probability of this model employing those two similarity metrics per query in two settings (i.e. natural and reverse order of query keywords).
The results show no di erence in M RR between these two metrics in the natural order.
However, in the reverse order the Levenshtein metric failed for 81% of the queries, while no failure was observed with the combination of Jaccard and Levenshtein.
Hence, our combination is robust with regard to change of input keyword order.
For bootstrapping the initial probability  , we compared the uniform distribution on both   hub and authority   values with a uniform distribution on the number of states for which the  rst keyword is observable.
The result of this comparison shows a 5% improvement for the proposed function.
Figure 5 shows the mean of M RR for di erent values of the threshold   employed for prunning the state space.
A high value of   prevents inclusion of some relevant resources and a low value adds irrelevant resources.
It can be observed that the optimal value of   is in the range [0.6, 0.7].
Thus, we set   to 0.7 in the rest of our experiments.
The goal of query graph construction is generating a conjunctive query (i.e. SPARQL query) from a given set of resource identi ers i.e., R = {r1, r2, ...rm}.
The core of SPARQL queries are basic graph patterns, which can be viewed as a query graph QG.
In this section, we  rst discuss the formal considerations underlying our query graph
 the number of both the number of free variables and the number of triple patterns in QG.
Note that is each triple pattern, the subject si (resp.
object oi) should be included in the domain (resp.
range) of the predicate pi or be a variable.
Otherwise, we assume the relevance probability of the given triple pattern to be zero: (si /  domain(pi))   (oi /  range(pi))   Pr(si, pi, oi) = 0.
Forward Chaining.
One of the prerequisites of our approach is the inference of implicit knowledge on the types of resources as well as domain and range information of the properties.
We de ne the comprehensive type (CT ) of a resource r as the set of all super-classes of explicitly stated classes of r (i.e., those classes associated with r via the rdf:type property in the knowledge base).
The comprehensive type of a resource can be easily computed using forward chaining on the rdf:type and rdfs:subClassOf statements in the knowledge base.
We can apply the same approach to properties to obtain maximal knowledge on their domain and range.
We call the extended domain and range of a property p comprehensive domain (CDp) and comprehensive range (CRp).
We reduce the task of  nding the comprehensive properties (CPr r(cid:48) ) which link two resources r and r(cid:48) to  nding properties p such that the comprehensive domain (resp.
comprehensive range) of p intersects with the comprehensive type of r resp r(cid:48) or vice-versa.
We call the set OPr (resp.
IPr) of all properties that can originate from (resp.
end with) a resource r the set of outgoing (resp.
incoming) properties of r.
To construct possible query graphs, we generate in a  rst step an incomplete query graph IQG(R) = (V (cid:48)(cid:48), E(cid:48)(cid:48)) such that the vertices V (cid:48)(cid:48) (resp.
edges E(cid:48)(cid:48)) are either equal or subset of the vertices (resp.
edges) of the  nal query graph V (cid:48)(cid:48)   V (cid:48) (resp.
E(cid:48)(cid:48)   E(cid:48)).
In fact, an incomplete query graph (IQG) contains a set of disjoint sub-graphs, i.e. there is no vertex or edge in common between the sub-graphs: IQG = {gi(vi, ei)| gi (cid:54)= gj : vi   vj =     ei   ej =  }.
An IQG connects a maximal number of the resources detected beforehand in all possible combinations.
The IQG is the input for the second step of our approach, which transforms the possibly incomplete query graphs into a set of  nal query graphs QG.
Note that for the second step, we use an extension of the minimum spanning tree method that takes subgraphs (and not sets of nodes) as input and generates a minimal spanning graph as output.
Since in the second step, the minimum spanning tree does not add any extra intermediate nodes (except nodes connected by Figure 5: Mean MRR for di erent values of  .
generation strategy and then describe our algorithm for generating the query graph.
The output of this algorithm is a set of graph templates.
Each graph template represents a comprehensive set of query graphs, which are isomorphic regarding edges.
A query graph A is isomorphic regarding its edges to a query graph B, if A can be derived from B by changing the labels of edges.
Pr(si, pi, oi), thus rendering Pr(QG|R) =(cid:81)n A query graph QG consists of a conjunction of triple patterns denoted by (si, pi, oi).
When the set of resource iden-ti ers R is given, we aim to generate a query graph QG satisfying the completeness restriction, i.e., each ri in R maps to at least one resource in a triple pattern contained in QG.
For a given set of resources R, the probability of a generated query graph Pr(QG|R) being relevant for answering the information need depends on the probability of all corresponding triple patterns to be relevant.
We assume that triple patterns are independent with regard to the relevance probability.
Thus, we de ne the relevance probability for a QG as product of the relevance probabilities of the n containing triple patterns.
We denote the triple patterns with (si, pi, oi)i=1...n and their relevance probability with i=1 Pr(si, pi, oi).
We aim at constructing QG with the highest relevance probability, i.e.
arg max Pr(QG|R).
There are two parameters that in u-ence Pr(QG|R): (1) the number of triple patterns and (2) the number of free variables, i.e. variables in a triple pattern that are not bound to any input resource.
Given that  (si, pi, oi) : Pr(si, pi, oi)   1, a low number of triple patterns increases the relevance probability of QG.
Thus, our approach aims at generating small query graphs to maximize the relevance probability.
Regarding the second parameter, more free variables increase the uncertainty and consequently cause a decrease in Pr(QG|R).
As a result of these considerations, we devise an algorithm that minimizes
 index over the neighborhood of nodes and using exploration for  nding paths between nodes.
Generation of IQGs.
After identifying a corresponding set of resources R = {r1, r2, ...rm} for the input query, we can construct vertices V (cid:48) and primary edges of the query graph E(cid:48)(cid:48)   E(cid:48) in an initial step.
Each resource r is processed as follows: (1) If r is an instance, CT of this vertex is equivalent to CT (r) and the label of this vertex is r. (2) If r is a class, CT of this vertex just contains r and the label of this vertex is a new variable.
After the generation of the vertices for all resources that are instances or classes, the remaining resources (i.e., the properties) generate an edge and zero (when connecting existing vertices), one (when connecting an existing with a new vertex) or two vertices.
This step uses the sets of incoming and outgoing properties as computed by the forward chaining.
For each resource r representing a property we proceed as follows:   If there is a pair of vertices (v, v(cid:48)) such that r belongs to the intersection of the set of outgoing properties of v and the set of incoming properties of v(cid:48) (i.e. r   OPv   IPv(cid:48) ), we generate an edge between v and v(cid:48) and label it with r. Note that in case several pairs (v, v(cid:48)) satisfy this condition, an IQG is generated for each pair.
  Else, if there is a vertex v ful lling the condition r   OPv, then we generate a new vertex u with the CTu being equal to CRr and an edge labeled with the r between those vertices (v, u).
Also, if the condition r   IPv for v holds, a new vertex w is generated with CTw being equal to CDr as well as an edge between v and w labeled with r.
  If none of the above holds, two vertices are generated, one with CT equal to CDr and another one with CT equal to CRr.
Also, an edge between these two vertices with label r is created.
This policy for generating vertices keeps the number of free variables at a minimum.
Note that whenever a property is connected to a vertex, the associated CT of that vertex is updated to the intersection of the previous CT and CDp (CRp respectively) of the property.
Also, there may be different options for inserting a property between vertices.
In this case, we construct an individual IQG for each possible option.
If the output of this step generates an IQG that contains one single graph, we can terminate as there is no need for further edges and nodes.
Example 1.
We look at the query: What is the side effects of drugs used for Tuberculosis?.
Assume the resource disambiguation process has identi ed the following resources:
 CD={diseasome:disease}, 2. diseasome:1154 CT={diseasome:disease}
 CD={sider:drug}, (type property) CR={drugbank:drugs} (type instance) (type property) CR={sider:sideeffect} After running the IQGs generation, since we have only one resource with the type class or instance, just one vertice is generated.
Thereafter, since only the domain of possibleDrug intersects with the CT of the node 1154, we generate: (1) a new vertex labeled ?v0 with the CT being equal to CR =drugbank:drugs, and (2) an edge labeled possibleDrug from 1154 to ?v0.
Since, there is no matched node for the property sideEffect we generate: (1) a new vertex labeled ?v1 with the CT being equal to sider:drug, (2) a new vertex labeled ?v2 with the CT being equal to sider:sideeffect, (3) an edge labeled sideEffect from ?v1 to ?v2.
Figure 6 shows the constructed IQG, which contains two disjoint graphs.
Figure 6: IQG for Example 1.
Connecting Sub-graphs of an IQG.
Since the query graph QG must be a connected graph, we need to connect the disjoint sub-graphs in each of the IQGs.
The core idea of our algorithm utilizes the Minimum Spanning Tree (MST) approach, which builds a tree over a given graph connecting all the vertices.
We use the idea behind Prim s algorithm [3], which starts with all vertices and subsequently incrementally includes edges.
However, instead of connecting vertices we connect individual disjoint sub-graphs.
Hence, we try to  nd a minimum set of edges (i.e., properties) to span a set of disjoint graphs so as to obtain a connected graph.
Therewith, we can generate a query graph that spans all vertices while keeping the number of vertices and edges at a minimum.
Since a single graph may have many di erent spanning trees, there may be several query graphs that correspond to each IQG.
We generate all different spanning graphs because each one may represent a speci c interpretation of the user query.
To connect two disjoint graphs we need to obtain edges that qualify for connecting a vertex in one graph with a suitable vertex in the other graph.
We obtain these properties by computing the set of comprehensive properties CP (cf.
Section 4.1) for each combination of two vertices from different sub-graphs.
Note that if two vertices are from different datasets, we have to traverse owl:sameAs links to compute a comprehensive set of properties.
This step is crucial for constructing a federated query over interlinked data.
In order to do so, we  rst retrieve the direct properties between two vertices ?v0 ?p ?v1.
In case such properties exist, we add an edge between those two vertices to IQG.
Then, we retrieve the properties connecting two vertices via an owl:sameAs link.
To do that, we employ two graph patterns: (1) ?v0 owl:sameAs ?x.
?x ?p ?v1.
(2) ?v0 ?p ?x.
?x owl:sameAs ?v1.
The resulting matches to each of these two patterns are added to the IQG.
Finally, we obtain properties connecting vertices having owl:sameAs links according to the following pattern: ?v0 owl:sameAs ?x.
?x ?p ?y.
?y owl:sameAs ?v1.
Also, matches for this pattern are added to the IQG.
For each connection discovered between a pair of vertices (v, v(cid:48)), a di erent IQG is constructed by adding the found edge connecting those vertices to the original IQG.
Note that the IQG resulting from this process contains less unconnected graphs than the input IQG.
The time complexity
 vertices).
Example 2.
To connect two disjoint graphs i.e. Graph
 to obtain edges that qualify for connecting either the vertex 1154 or ?v0 to either vertex ?v1 or ?v2 in Graph 2.
Forward chaining reveals the existence of two owl:sameAs connections between two vertices i.e. (1) 1154 and ?v2, (2) ?v0 and ?v1.
Therefore, we can construct the  rst query graph template by adding an edge between 1154 and ?v2 and the second query graph template by adding an edge between ?v0 and ?v1.
The two generated query graph templates are depicted in Figure 7.
Figure 7: Generated query graph templates.
Our approach was implemented as a Java Web application which is publicly available at http://sina-linkeddata.aksw.
org.
The algorithm is knowledge-base-agnostic and can thus be easily used with other knowledge bases.
Experimental Setup.
The goal of our evaluation was to determine how well (1) our resource disambiguation and (2) our query construction approaches perform.
To the best of our knowledge, no benchmark for federated queries over Linked Data has been created so far.
Thus, we created a benchmark of 25 queries on the 3 interlinked datasets Drugbank, Sider and Disea-some for the purposes of our evaluation8.
The benchmark was created by three independent SPARQL experts, which provided us with (1) a natural-language query and (2) the equivalent conjunctive SPARQL query.
We selected these three datasets because they are a fragment of the well interlinked biomedical fraction of the Linked Open Data Cloud9 and thus represent an ideal case for the future structure of Linked Data sources.
We measured the performance of our resource disambiguation approach using the Mean Reciprocal Rank (MRR).
Moreover, we measured the accuracy of the query construction in terms of precision and recall.
To compute the precision, we compared the results returned from the query construction method with the results of the reference query provided by the benchmark.
The query construction is initiated with the top-1 tuple returned by the disambiguation approach.
All experiments were carried out on a Windows 7 machine with an Intel Core2 Duo (2.66GHz) processor and 4GB of RAM.
For testing the statistical signi cance of our results, we used a Wilcoxon signed ranked test with a signi cance level of

 Projects/lodquery
 instances in Sider and the 4772 drug instances in Drugbank Results.
The detailed results of our evaluation are shown in Figure 9.
We ran our approach without and with OWL inferenc-ing during the state space construction.
When ran without inferencing, our approach was able to disambiguate 23 out of 25 (i.e. 92%) of the resources contained in the queries without mistakes.
For Q9 (resp.
Q25), the correct disambiguation was only ranked third (resp.
 fth).
In the other two cases (i.e. Q10 and Q12), our approach simply failed to retrieve the correct disambiguation.
This was due to the path between Doxil and Bextra not being found for Q10 as well as the mapping from disease to side effect not being used in Q12.
Overall, we achieve an MRR of 86.1% without inferencing.
The MRR was 2% lower (not statistically signi cant) when including OWL inferencing due to the best resource disambiguation being ranked at the second position for three queries that were disambiguated correctly without inferencing (Q5, Q7 and Q20).
This was simply due to the state space being larger and leading to higher transition probabilities for the selected resources.
With respect to precision and recall achieved with and without reasoning, there were also no statistically signi cant di erences between the two approaches.
The approach without reasoning achieved a precision of 0.91 and a recall of 0.88 while using reasoning led to precision (resp.
recall) values of 0.95 (resp.
0.90).
Although performance was not (yet) the primary focus of our work, we want to provide evidence, that our approach can be used for real-time querying.
Overall the pros and cons of using inferencing are clearly illustrated in the results of our experiments.
On Q12, our approach is unable to construct a query without reasoning due to the missing equivalence between the terms disease and side effect.
This equivalence is made available by the inference engine, thus making the construction of the SPARQL query possible.
On the downside, adding supplementary information through inferencing alters the ranking of queries and can thus lead to poorer recall values as in the case of Q20.
Figure 8 shows the runtime average of disambiguation and query construction with and without inferencing during the state space construction for three runs.
As it can be expected, inferencing increases the runtime, especially when the number of input keywords is high.
Despite carrying out all computations on-the- y, disambiguation and query construction terminate in reasonable time, especially for smaller number of keywords.
After implementing further performance optimizations (e.g.
indexing resource distances), we expect our implementation to terminate in less than 10s also for up to 5 keywords.
Several information retrieval and question answering approaches have been developed for the Semantic Web over the past years.
Most of these approaches are adaptations of document retrieval approaches.
Swoogle [5], Watson [4] and Sindice [24], for example, stick to the document-centric paradigm.
Recently, entity-centric approaches, such as Sig.Ma [23], Falcons [2], SWSE [11], have emerged.
However, the basis for all these services are keyword indexing and retrieval relying on the matching user keywords and indexed terms.
Examples of question answering systems are PowerAqua [16] and OntoNL [12].
PowerAqua can automatically combine information from multiple knowledge bases at runtime.
The input is a natural language query
 query construction with (+) and without reasoning in the disambiguation phase in logarithmic scale.
and the output is a list of relevant entities.
PowerAqua lacks a deep linguistic analysis and can not handle complex queries.
Pythia [26] is a question answering system that employs deep linguistic analysis.
It can handle linguistically complex questions, but is highly dependent on a manually created lexicon.
Therefore, it fails with datasets for which the lexicon was not designed.
Pythia was recently used as kernel for TBSL [25], a more  exible question-answering system that combines Pythia s linguistic analysis and the BOA framework [7] for detecting properties to natural language patterns.
Exploring schema from anchor points bound to input keywords is another approach discussed in [22].
Querying Linked datasets is addressed with the work mainly treat both the data and queries as bags of words [2, 30].
[10] presents a hybrid solution for querying linked datasets.
It run the input query against one particular dataset regarding the structure of data, then for candidate answers, it  nds and ranks the linked entities from other datasets .
Our approach is a prior work as it queries all the datasets at hand and then according to the structure of the data, it makes a federated query.
Furthermore, our approach is independent of any linguistic analysis and does not fail when the input query is an incomplete sentence.
Segmentation and disambiguation are inherent challenges of keyword-based search.
Keyword queries are usually short and lead to signi cant keyword ambiguity [27].
Segmentation has been studied extensively in the natural language processing (NLP) literature e.g., [18]).
NLP techniques for chunking such as part-of-speech tagging or name entity recognition cannot achieve high performance when applied to query segmentation.
[17] addresses the segmentation problem as well as spelling correction and employs a dynamic programming algorithm based on a scoring function for segmentation and cleaning.
An unsupervised approach to query segmentation in Web search is described in [21].
[32] is a supervised method based on Conditional Random Fields (CRF) whose parameters are learned from query logs.
For detecting named entities, [9] uses query log data and Latent Dirichlet Allocation.
In addition to query logs, various external resources such as Web pages, search result Figure 9: Accuracy results for the benchmark.
snippets, Wikipedia titles and a history of the user activities have been used [19, 21, 1, 20].
Still, the most common approach is using the context for disambiguation [15, 6, 14].
In this work, resource disambiguation is based on the structure of the knowledge at hand as well as semantic relations between the candidate resources mapped to the keywords of the input query.
We presented a two-step approach for question answering from user-supplied queries over federated RDF data.
A main assumption of this work is that some schema information is available for the underlying knowledge base and resources are typed according to the schema.
Regarding the disambiguation, the superiority of our model is related to the transition probabilities.
We achieved a fair balance between the quali cation of states for transiting by re ecting the popularity and distance in the hub and authority values and setting a transition probability to the unknown entity state (depending on the hub value).
This resulted in an accuracy of the generated answers of more than 90% for our test-bed with life-science datasets.
This work represents a  rst step in a larger research agenda aiming to make the whole Data Web easily queryable.
For scaling the implementation, a  rst avenue of improvements is related to the performance of the system, which can be improved by several orders of magnitued thorough including better indexing and precomputed forward-chaining.
Acknowledgments We would like to thank our colleagues from AKSW research group for their helpful comments and inspiring discussions during the development of this approach.
This work was supported by a grant from the European Union s 7th Framework Programme provided for the project LOD2 (GA no.
