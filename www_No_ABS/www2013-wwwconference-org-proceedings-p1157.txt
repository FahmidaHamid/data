The advancement of the internet in the past decade created a platform for a new form of labor markets known as crowdsourcing markets where cognitive work can be distributed to hundreds of thousands of geographically disparate workers.
In contrast to traditional procurement mar  on Human Computation (HCOMP) 2011 Preliminary results were presented at the AAAI workshop Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
kets that rely on specialized contractors, requesters in crowd-sourcing markets typically outsource large quantities of simple tasks to anonymous, unspecialized workers.
Typical examples of crowdsourcing tasks include image labeling, sentiment analysis, content generation, listing veri cation, image moderation, transcription, and other forms of tasks that are impossible, di cult or too expensive to automate.
There are several crowdsourcing platforms that provide workers with non-monetary incentives like entertainment [9], educational opportunities [5], information [27], and altruism [17] in exchange for their e orts.
Despite the success of these platforms, it is often di cult to engineer non-monetary incentive schemes for tedious and repetitive work.
Therefore an overwhelming majority of crowdsourcing tasks are performed in exchange for payments [1, 4, 8, 2, 6, 3, 7].
In such markets, implementing a campaign successfully requires pricing and allocating tasks e ectively.
Designing e ective pricing and allocation schemes presents a challenging problem due to requesters  constraints and the realities of crowdsourcing markets.
Requesters often face task completion deadlines and budget constraints, and must account for dramatic elasticity in the workforce supply.
Furthermore, there is a large variance in e ort required to complete di erent tasks, which also largely depends on the skills and background of workers who are often based in multiple geographical locations.
Despite this di culty in pricing tasks, most crowdsourcing markets provide requesters surprisingly limited tools for pricing tasks e ectively.
In this paper we address the problem of pricing and allocating tasks in crowdsourcing markets.
We develop a theoretical framework and design mechanisms that work well in practice and have provable guarantees.
In addition, we describe a platform which we implemented that enables re-questers to automate the process of pricing in crowdsourc-ing markets using the mechanisms we present here as well as other pricing schemes.
The framework is primarily designed for tasks where the quality of the worker s performance does not yield additional utility to the requester once above a certain quality threshold.
Although there are crowdsourc-ing tasks that do not fall into this category like predicting future events, designing a logo, or writing an introduction to an academic paper, a large fraction of the work in crowd-sourcing markets typically  ts this criterion.
For such tasks requesters often use various methods to ensure the threshold quality is met like injecting gold standards [29], majority voting, and more sophisticated cross-validation methods [31,
 questers  approval in crowdsourcing platforms, we assume 1157requesters have access to such veri cation schemes and fo- cus on e ciently pricing and allocating tasks, independent of their quality.
We take a mechanism design approach to the pricing problem and enable workers to bid on work by expressing their cost for performing tasks and the number of tasks they wish to perform.
Although most crowdsourcing platforms do not provide workers such level of expressiveness, existing APIs make this feature easy to integrate into most platforms as we further describe.
This relatively minor modi cation enables designing powerful mechanisms.
The mechanisms we present here are designed for two main objectives: maximizing the number of tasks performed under budget, and minimizing payments for a given number of tasks.
We consider requesters that impose a deadline for the completion of tasks and workers who arrive i.i.d.
according to some known distribution and can strategically misreport their cost or number of tasks they wish to perform.
We therefore design incentive compatible mechanisms that ensure that the allocation and pricing are such that it is in every worker s best interest to bid truthfully.
The problem of designing mechanisms for pricing tasks in crowdsourcing markets has been addressed using di erent models and di erent techniques, such as bargaining between requesters and workers to minimize work [26] and recently using bandit algorithms to maximize tasks [34].
While both are natural approaches, they leave room for frameworks that allow better theoretical guarantees as used here.
In [24] the authors study an orthogonal problem and present an algorithmic framework for matching workers with with re-questers based on their skills.
The approach in this paper advocates for eliciting workers costs via incentive compatible protocols.
A di erent approach is that of developing a model for workers  e ort and learning its parameters from data as done in [25, 13].
The problem of designing mechanisms for procurement has been extensively studied by the algorithmic game theory community over the past decade.
The earlier line of frugality  rst suggested in [10] focuses on minimizing payments for complex objective functions.
Recently, the budget feasibility framework has been initiated in [32], where the goal is to design incentive compatible mechanisms that maximize a requester s objective under a budget.
The framework has been adapted to various settings [33, 19, 16, 15, 18] and we follow it in this paper.
In our model we account for the online arrival of workers, which raises a signi cant challenge.
There is substantial literature on online mechanism design where workers arrive according to a given distribution [21, 22, 23, 12, 11].
In our case we consider mechanisms for buying items (rather then selling) from strategic agents which requires di erent machinery.
In [14] the authors study online procurement though the emphasis there is on a di erent model of posted prices.
In our model we consider a single requester and multiple workers.
The requester has a task completion deadline T   N+ by which his tasks need to be allocated.
Each worker arrives at some time step t   {1, .
.
.
, T} i.i.d.
according to some distribution.
Each worker ai associates a cost ci   R+ for performing a single task posted by the requester and a number of tasks she can complete vi   N+.
The requester does not know ci, vi, or the total number of workers that will appear by T .
We assume the requester knows the distribution of the arrival of workers.1 The requester either aims to maximize the number of tasks allocated under some budget B   R+, or alternatively complete L   N+ tasks while mini-i N vi   100L.
mizing payments, in which case we assume (cid:2) One can adversarially assume that the workers know B, L, T and the objective of the requester.
When allocated  wi tasks at pi per task, the worker s utility is simply  wi(pi   ci).
We assume that workers are rational in that they wish to maximize their utility.
The protocol.
At the time of her arrival, each worker ai submits a bid indicating her cost per task and the maximal number of tasks she wishes to perform, denoted as bi and wi respectively, which do not necessarily correspond to ci, vi.
The mechanism must decide how many tasks to allocate to the worker and the price for each task upon her arrival, after processing her bid.
The bid on the maximal number of tasks is abiding and if a worker completes less than the number of tasks allocated to her, the requester does not pay the worker.
(cid:3) i(p i   ci), where pi, p (cid:3) Our goal is to design mechanisms that perform well.
A mechanism is simply an algorithm that decides how many tasks each worker performs and how she is paid.
Since workers may report false costs, we will seek incentive compatible (truthful) mechanisms for which reporting the true costs is a dominant strategy.
Formally, a mechanism is incentive compatible if for every ai   N with cost and limit ci, vi and bid bi, wi, and every set of bids by N \ {ai} we have xi(pi   ci)   x (cid:3) i are the payments and (cid:3) xi, x i indicate the number of tasks ai is allocated, when bidding ci, vi and bi, wi, respectively.
Since an incentive compatible mechanism guarantees that the bids are truthful, its performance over the bids can be compared against a theoretically optimal algorithm that knows workers  true values.
Maximizing Tasks.
Under this objective we are given a  xed budget by the requester and seek to maximize the expected number of tasks performed without exceeding the budget, where the expectation is over the random arrival order of the workers and the randomization of the mechanism.
This design objective is known as budget feasiblity where the mechanism must be designed so that the sum of its payments (and not workers  costs) does not exceed the budget [32].
To quantify the performance of the mechanism we compare its solution with the optimal o ine solution: the solution that would have been obtainable if all workers  true values were known in advance and we could pay each worker exactly her cost.
Note that this is the most demanding benchmark possible.
A mechanism is O(g(n))-competitive if the ratio between the benchmark and the expected value guaranteed by the mechanism is O(g(n)).
Ideally, we would like our mechanism to be O(1)-competitive.
Minimizing Payments.
A complementary objective to maximizing expected number of tasks under budget is that of minimizing expected payments for a given number of tasks.
Ideally, we would like the total payments the mechanism makes to be comparable to the minimal cost required
 know the median of this distribution.
incentive compatible mechanism can perform well under this benchmark.2 We therefore set our objective for minimizing payments as follows.
Given a  xed number of tasks to perform, L, we say that a mechanism for minimizing payments is  competitive if it allocates L tasks in expectation and is guaranteed to pay no more than the minimum cost required to complete  L tasks in the o ine scenario when all costs are known.
Here as well our goal is to design mechanisms that are O(1)-competitive.
The common approach to achieve desirable outcomes in online settings is to observe a fraction of the input and use it as a sample to make an informed decision on the rest of the input.
We will use a similar strategy, though rather than rejecting workers whose bids are used as a sample, we will allocate tasks to workers in the sample.
When our objective is to maximize the number of tasks performed, our mechanism dynamically learns an appropriate price for a task as it allocates tasks to workers.
In the case of minimizing payments, the requester can specify a budget for allocating tasks during the sampling phase.
In essence, both mechanisms sample the input until the median time step   the time step at which each worker appears with probability 1/2   and use the sample to estimate a threshold price.
The mechanisms then use the threshold price to decide which bids to accept or reject.
Estimating appropriate threshold prices is an important building block in both mechanisms, and we begin by describing this procedure and its properties.
Intuitively, the threshold price is the lowest single price we can o er which many workers will accept.
The procedure for computing threshold prices presented below is a variant of the proportional share mechanism introduced in [32], which serves as the basis for designing procurement mechanisms under budget [33, 19, 16, 15, 18].
Note that this is an o ine procedure that has access all bids.
GetThreshold input: bids {(b1, w1), .
.
.
,( bm, wm)}, Budget B 1. initialize: sort bids s.t.
b1   b2   .
.
.
  bm; set i = 1 2. while bi   B(cid:2) j<i  wj +1 set: p = bi,  wi = min{wi,(cid:6) B i = i + 1; p (cid:7)   (cid:2) j<i  wj}, output: p
 all required tasks at some small cost , while every other workers has some very high costs P .
It is not hard to show that any incentive compatible mechanism that completes all required tasks must pay at least P in such a case, thus making the ratio between minimal cost and the total payments of an incentive compatible mechanisms unbounded.
(cid:2)i 1 (cid:7)   (cid:2) When tasks are priced at p = bi the remaining budget is B   p j=1  wj, and therefore the number of tasks she could be allocated at this price without exceeding the total budget j<i  wj} as used in the procedure.
is  wi = min{wi,(cid:6) B The desirable property of this threshold price is that it sets a single price which on the one hand is low enough so that it e ciently exhausts the budget and on the other hand is high enough so that enough workers accept.
We formalize this in the following lemma.
The proof in similar to [32].
p Lemma 3.1.
For a given sample of bids, let L be the maximal number of tasks that can be allocated under a given budget.
Then, at least L/2 tasks can be allocated under budget at the price computed by the GetThreshold procedure.
Proof.
Let ai = (bi, wi) and {a1, a2, .
.
.
, ak} be the set of bids allocated by the procedure.
First, observe that  wi = wi (cid:7)   (cid:2)i 1 for all i < k, as otherwise given an i < k s.t.
wi >  wi = (cid:6) B j=1  wj we have: i(cid:3) i 1(cid:3) (cid:4) (cid:5) p (cid:7)   i 1(cid:3) j=1 (cid:6) B p  wj (cid:7) = (cid:6) B p  wj =  wj + j=1 j=1 and thus: bi+1   bi = p > B(cid:6)B/p(cid:7) + 1 = B(cid:2) j<i  wj + 1 which implies that bi+1 violates the condition in step 2 of the procedure, in contradiction to the assumption that it was allocated.
For the purpose of this analysis, without loss of generality we can assume that  wk = wk.
This is due to the fact that we can consider an input identical to ours except that ak is replaced with two bids ak(cid:2) = (bk,  wk) and ak(cid:2)(cid:2) = (bk, wk    wk).
The value over this input is identical to the value obtained over the original input, and ak(cid:2)(cid:2) is not allocated as it fails to meet condition in step 2 due to the same argument made above.
Given a  xed budget, to achieve the maximal number of tasks one must allocate to lowest bids until exhausting the budget, and thus {a1, .
.
.
, ak} are included in the optimal solution.
Let {a1 .
.
.
, ak, .
.
.
, ar} be the optimal solution.
As-i=1 wi.
sume for purpose of contraction that 2 i=k+1 wi.
By this assumption and the Then, de nition of bk+1 we have that: i=1 wi < i=1 wi < (cid:2)k (cid:2)k (cid:2)r (cid:2)r bk+1 > B(cid:2)k+1 i=1 wi + 1   B(cid:2)r i=k+1 wi which implies: B < bk+1   (cid:4) r(cid:3) (cid:5)   r(cid:3) wi wi   bi i=k+1 i=k+1 but since the optimal solution cannot exceeds the budget, we have a contradiction.
The above lemma suggests that if we accept all assignments that have cost that is smaller than the threshold price, we will be able to complete at least half of the assignments we would have been able to complete if we paid each worker their cost.
As one might imagine, once the sample size is large enough, the threshold prices obtained on the sample will be a good estimate to the real threshold price as if they were computed o ine.
We formalize this argument and give a rigorous proof in the following section.
Given a distribution on the arrival of workers, we can easily compute every 2i quantile (i.e. the time step t s.t.
the  i).
We there-probability that a worker arrives before t is 2 fore assume the mechanism is given {q1, .
.
.
, q(cid:2)} where q(cid:2) is the  rst time step t = 1.
We give a formal description of the mechanism below followed by a brief explanation in plain English.
We use S(t) to denote the set of all bids that arrived at and before time step t.
MaximizeTasks input: Budget B, quantiles {q1, .
.
.
, q(cid:2)}  ((cid:2)+1)B, t = 1;

 = 2 a. at time step t = qj set: (cid:3) (cid:3) assignments by the time the worker arrives.
In case ci   p, bidding below p wouldn t make a di erence in the worker s allocation and payment and her utility for each assignment would be p   ci   0.
Declaring a cost above p would deny the worker from being allocated, and her utility would be
 the worker unallocated with utility 0.
If the worker declares a cost lower than p she will be allocated though her utility will be negative.
In the realization where all workers with costs smaller than p are allocated until exhausting the budget, declaring a lower or higher number of tasks does not bene t the worker since her utility is linear.
In the realization where we select the  rst worker with bid smaller than p who will complete at least w tasks, if the worker i   min{w,(cid:6)B/p(cid:7)} > wi the worker will be allo-(cid:3) declares w cated more tasks than she can perform and will be paid 0, and will therefore not bene t.
  = min{max{a S(t)|ci p} wi,(cid:6) 2B(cid:2) p = GetThreshold(S(t), 2B w


   (cid:3) ), p (cid:3) (cid:7)}, Lemma 3.3.
The mechanism is budget feasible, i.e.
the sum of the payments that the mechanism makes to all workers never exceeds the given budget.
b. with probability 1/3 do: while qj < t  qj+1: for all ai who arrive at time t s.t.
bi   p do: allocate  wi = min{wi,(cid:6) B(cid:2) r A  wr} at p, set A = A   {i}; (cid:7)   (cid:2) p with probability 2/3 do: for  rst ai arriving by qj+1 s.t.
wi   w allocate  wi = min{wi,(cid:6) B(cid:2) (cid:7)} at p; p   : The mechanism iterates over q1, .
.
.
, q(cid:2) and at every time step qi it uses a budget of B/2i to allocate tasks, and decides whether to accept a bid using a threshold price which it computes on bids of all workers that arrived by time step qi.
At every time interval qi the mechanism randomly selects the procedure it will use on all workers that arrive until qi+1.
The  rst procedure pays each worker the threshold price per task, as long as her cost is below the threshold and the budget has not been exhausted.
The second procedure allocates its entire budget to the worker that bid below the threshold price and is willing to perform at least as many tasks as w .
The randomization between the two procedures handles extreme cases in which only a single worker can complete a large fraction of the tasks at the threshold price.
The way which we  nd the worker that can complete the maximal number of tasks is an incentive compatible variant of Dynkin s celebrated algorithm to the problem of hiring the best secretary [20], tailored to our setting.
  Lemma 3.2.
The mechanism is incentive compatible, i.e.
it is in every worker s best interest to bid her true cost for performing an assignment, and the number of assignments she wishes to perform.
Proof.
Consider a worker ai with cost of ci that arrives at some stage for which the threshold price was set to p.
If by the time the worker arrives there are no remaining assignments, then the worker s bid will not a ect the allocation of the mechanism and thus she cannot bene t by reporting a false cost.
Otherwise, assume there are remaining (cid:3) Proof.
At each stage i = 1, .
.
.
, (cid:4) the mechanism uses a = B/2i and threshold price p computed from budget of B the bids of the previous round, and allocates no more than (cid:6)B /p(cid:7) tasks.
Therefore every iteration is budget feasible (cid:2)(cid:2) and in total i=1 B/2i < B.
(cid:3) Lemma 3.4.
The MaximizeTasks mechanism is 360 competitive and 120-competitive when using its entire budget in the median time step.
Proof.
We will analyze the iteration of the mechanism, when the sample consisted of all workers who arrive by the median time step and the budget used for allocation was B/2.
We will compare the expected number of tasks allocated with the number of tasks that would have been possible to allocate with the same budget on the entire set of workers at a single threshold price.
(cid:3) (cid:2)
 Let S(T ) be the set of all workers who arrive by time step T , and consider running the GetThreshold procedure on S(T ) using a budget of B let A be the set of all workers who were allocated by this procedure, k =
 i A  wi, p be the threshold price obtained by running the procedure, and OP T be the maximal number of tasks that can be performed under budget B on S(T ).
Note that the maximal number of tasks that can be performed under budget B/2 is at least OP T /3, and therefore from Lemma 3.1 we have that W   OP T /6.
Assume  rst that maxi wi   W/10.
Consider the median time step t and all workers bids sampled until this time step, S(t).
Let A1 = S(t)   A, A2 = A \ A1, and i A2  wi.
Since each worker ai   A
 arrives before the median with probability 1/2 we can associate a random variable Xi that takes a value of  wi with probability 1/2 and 0 otherwise.
To evaluate the expected value of W1 and W2, we can use the following version of the Cherno  bound: i A1  wi, W2 = (cid:2) (cid:2) Theorem 3.5.
(Cherno  Bound) Let X1, .
.
.
, Xk be a set of k independent random variables that take values in [0, wi] 1160(cid:2)k i=1 Xi].
Then, for any     [0, 1] we have that: and   = E[ (cid:6) k(cid:3) P r P r i=1 (cid:6) k(cid:3) i=1 (cid:7) Xi > (1 +  )  (cid:7) Xi < (1    )  (cid:4)   e  (1 +  )(1+ ) (cid:5)   maxi wi   e  2   2 maxi wi This above bound implies that: (cid:6)
 P r
 (cid:6)
 (cid:6)




 P r
 (cid:7) (cid:7) = P r   21/100 (cid:7)


   59/100 Therefore, by union bound with probability at least 1/5 both W1, W2   W/4.
Now, let p(T ), p(t) be the thresholds computed using the GetThreshold procedure over S(T ) and the sample S(t), respectively.
Since p(t)   p(T ) as it is computed over a smaller subset, for each worker ai   A2 it follows that ci   p(t) and they will be allocated if the If all workers in A2 budget has not yet been exhausted.
were allocated by the mechanism this implies that at least W/4 tasks were performed since W2   W/4.
If the budget was exhausted before all workers in A2 arrived, a total of (cid:6)B (cid:3) /p(t)(cid:7) were performed.
Since (cid:3) p(t)   B
 (cid:3)

 this implies that at least W/4 tasks were performed with probability at least 1/5.
Therefore, since this procedure is realized with probability 1/3 at least W/60 were completed in expectation when maxi A wi   W/10.
In case maxi A  wi > W/10, let a = argmaxi S(T )wi, and b = argmaxi S(T )\{a}wi.
With probability 1/4, b appears at or before the median time step and a arrives after the median time step, we therefore have at least one worker ai at stage t > q1 s.t.
wi   w  wi = min{max (cid:3)   4B/W .
Since this procedure is realized with   .
In this case we have that: wi,(cid:6)B/p(t)(cid:7)}   W/10 since p i probability 2/3 we have obtain at least:

   1
   2
 =

 tasks in expectation, as in the previous case.
Since W   OP T /6 the mechanism is 360-competitive.
If the entire budget is used in step q1, W   OP T /2 and the mechanism is 120-competitive.
Theorem 3.6.
The MaximizeTasks mechanism is incentive compatible, budget feasible and O(1)-competitive.
While the constants may seem large, we emphasize that our goal is to show that the mechanisms are indeed O(1)-competitive, and thus that their guarantee is independent of the parameters of the problem that can be large (e.g.
number of workers, their cost, the number of tasks they are willing to perform, etc.).
We will later show that these mechanisms perform well in practice, implying that bounded competitive ratio serves as a good guide for designing such mechanisms.
The idea behind the mechanism for minimizing payments is based on the following observation.
Given a  xed number of tasks L, if we knew the minimal cost for perform-(cid:3) ing L = 2L tasks, we could use a procedure similar to GetThreshold with this minimal cost as its budget.
From Lemma 3.1 we know that the GetThreshold procedure (cid:3)  nds a price s.t.
at least L /2 = L tasks could be performed.
Therefore, such a procedure would be a 2-approximation to the minimal cost in our case.
The MinimizePayments mechanism is based on this idea: we compute the minimal cost for performing a constant blowup of the number of tasks required, and  nd an appropriate threshold price.
We describe the mechanism formally below, followed by a brief description.
MinimizePayments input: number of tasks L, budget  , price  , q1, T
 r A  wr} at  , allocate  wi = min{wi, (cid:6)   set A = A   {i}; (cid:7)   (cid:2)  
 set B = FindMinCost(S(t), 2L), set p = GetThreshold(S(t), B), = min{max{a S(t)|ci p} wi,(cid:6) B w   p (cid:7)} 3. with probability 1/4 do: for all ai who arrive at time t > q1 s.t.
bi   p do: r A  wr} at p, allocate  wi = min{wi, (cid:6) B (cid:7)   (cid:2) p with probability 3/4 do: for  rst ai arriving by T s.t.
wi   w allocate  wi = min{wi,(cid:6) B (cid:7)} at p; p   : The above mechanism has two iterations.
The  rst iteration samples the bids, and uses a given budget of   and price   speci ed by the requester to allocate to workers in the sam-ple.3 After the median time step, the FindMinCost(S(t), 2L) procedure  nds the minimal cost for performing 2L tasks (which can be done by a simple greedy algorithm which sorts workers according to their costs and allocates tasks until reaching the number of tasks required).
The threshold price is then computed using the minimal cost as its budget, together with an estimate of the maximal number of tasks a worker can perform.
Similarly to the MaximizeTasks mechanism, the mechanism then randomizes between a procedure which allocates tasks to workers with price smaller than p and a procedure that allocates all tasks to a single worker.
The properties of the mechanism can be proven using similar ideas as in the proofs in the previous section.
We state the theorem below, and leave the proof to the full version of the paper.
half of the budget for the sampling phase.
Here, since we do not a priori know what workers  costs are, we leave   and   as a design choice to the requester.
imizePayments mechanism is incentive compatible, allocates L tasks in expectation and is O(1)-competitive.
To evaluate the performance of the mechanisms in practice and explore bidding behavior in crowdsourcing markets, we created the Mechanical Perk platform which enables us to conduct experiments with workers from Mechanical Turk and to collect real bidding data and observe their behavior.
To enable implementing various mechanisms on Mechanical Turk (MTurk) we created the Mechanical Perk (MPerk) platform.
The platform provides a service for requesters who wish to post Human Intelligence Tasks (HITs) with various automated pricing and incentive mechanisms.
The platform receives the HIT from the requester as input and their choice for the mechanism they wish to use, along with additional information like the number of HITs to be posted, budget, expiration date, and other parameters for posting the HIT on MTurk.
The platform then posts a HIT on MTurk that serves as a wrapper for the requester s original HITs.
The HIT posted by MPerk enables workers to place bids and run a mechanism in the background.
In the Human Intelligence Task (HIT) workers are explained that a mechanism will decide how many assignments, if any, will be allocated to them based on their bid.
We explain to workers they would be paid through the Mechanical Turk bonus payment system, which allows a requester to pay workers beyond the  xed price associated with the HIT.
To encourage high quality work, we explain to workers they would not be paid if their work will be found unsatisfactory.
We also include a screenshot from an example assignment so that workers could assess their cost for performing the assignment prior to bidding.
Following the set of instructions, workers need to indicate their cost for performing an assignment and how many assignments they wish to perform.
Their bids are collected by a mechanism which decides on their allocation.
A worker that was allocated received assignments to work on, and based on the pricing decided by the mechanism was paid within a few days via the bonus payment system.
Each worker that placed a bid was paid for participating in the HIT, independent of the payments made according to the mechanism s decision.
An important fact is that MPerk can use various pricing mechanisms that allow e cient allocation of tasks.
In our experiments we used MaximizeTasks as well as other simple pricing schemes to gain insight to bidding behavior in crowdsourcing markets.
We describe these in detail in Section 4.5.
We conducted two main sets of experiments on MPerk.
The primary goals were to evaluate the performance of the online mechanism on real bids as well as to test workers  responses to di erent bidding mechanisms.
We implemented the bidding mechanisms through MPerk and for Human Computation tasks, we used a batch of automatically generated assignments.
Performance.
We conducted an experiment where we ran the MaximizeTasks on MechanicalPerk with a modest Figure 1: Screenshot of example assignment used in the experiments.
budget.
We primarily used this process to collect bids so we can run simulations with di erent budgets to observe the performance of the mechanism.
In general, we found that the mechanism performs very well on real inputs and the threshold prices converge quickly.
Bidding behavior.
The main goal in these experiments was to examine workers  responses to various features in the mechanism which could serve as guidelines for future design of mechanisms in crowdsourcing markets.
An important guideline in our design is incentive compatibility as we assume workers behave strategically in crowdsourcing platforms.
To examine this we observed workers  response to di erent pricing schemes which suggest that they indeed strategize their bids.
Another design principle is to avoid rejecting workers automatically.
The main reasoning is that we believe that although rejecting workers automatically to obtain a sample will not hurt the mechanism during its iteration, rejecting bids automatically is not likely to be sustainable in crowdsourcing markets, as workers will avoid tasks that use such mechanisms for pricing.
In our experiments we show evidence of this as well.
In the  rst experiment we ran the MaximizeTasks mechanism described in Section 3 primarily to collect bids that we used to test its performance, and allowed workers to bid only once.
In the second experiment we tested workers  responses to four di erent pricing mechanisms.
We allowed workers to bid up to 15 times to observe their responses to various pricing schemes.
We limited the experiment to workers with approval rate on Mechanical Turk higher than 90%.
We recorded workers  IP addresses and treated each IP session as a new worker.
While this does not guarantee the worker is a di erent person, we used this as a reasonable proxy.
In total we collected 1674 bids, from 764 di erent workers, allocated 3883 assign ments and collected 23298 answers.
1162s r e k r o
 f o #














 # of Tasks Requested








 s r e k r o w f o #










 bids requested Figure 2: (a) Histogram of price per assignment (0-40 cents) requested by bidders (b) Histogram of number of assignments requested by bidders The Human Computation Tasks.
We used assignments that required workers to estimate area sizes in pie charts.
Each assignment included six pie charts, where each pie chart consisted of three colors, one of which was red.
In each assignment, the workers were required to estimate the percentage of red color in each one of the six pie charts and these area sizes were randomly generated.
The reason for choosing this assignment is that it simulates a human computation task and allows to quantify a worker s performance objectively.
We also gave workers an option to send us feedback about their experience.
An example assignment is displayed in Figure 1.
To collect data for simulations we ran an experiment on Mechanical Perk where MaximizeTasks was used to allocate tasks.
To collect a representative data set for the simulations, we allowed workers to bid once in this experiment.
The maximal allowed bid was $0.40 and the limit on the number of assignments was set to 25.
We collected bids from 391 workers, each providing a single bid which indicates their cost and the number of assignments they wish to perform.
The mean bid was 16.33 cents and the mean number of assignments was 16.
We plot the distribution of bids and number of assignments in Figure 2.
To test the performance of the MaximizeTasks mechanism, we used the bids collected and compared our mechanism against several benchmarks.
Note that in order to show how many assignments can be allocated given a spec-i ed budget, we only require the workers  bids, which is a much larger set than the subset of workers that were actually allocated and submitted their answers to the assignments.
To simulate a task we use a random permutation of the bids we collected to model the random arrival of workers, and run our mechanism with a speci ed budget over this ordering.
We compared MaximizeTasks against two benchmarks.
The  rst benchmark is the optimal o ine algorithm which has full knowledge about workers costs.
The second benchmark is the GetThreshold procedure applied o ine.
This procedure is guaranteed to be within a factor of two of the  rst benchmark by Lemma 3.1 and is also the optimal incentive compatible solution due to a matching lower bound [32].
This mechanism does not have knowledge about workers  true costs, but it is an o ine mechanism, i.e., all workers submit their bids to the mechanism and wait for the mechanism to collect all the bids and decide on an allocation.
These benchmarks operate in simpler settings, where all the costs are known a priori and will therefore always outperform our mechanism.
Ideally, we would be able to compare against other pricing methods such as those used by commercial platforms, though this data is di cult to obtain.
We showed that the mechanism is guaranteed to be, in expectation over the arrival order of the workers, within a constant factor from the optimal o ine solution.
Our goal in this experiment was to examine this ratio on descriptive inputs.
Using the bids provided by workers, we simulated the di erent algorithms on budgets ranging from $50 to $1000 in increments of $50.
In Figure 3 we plot the resulting comparison between our mechanism and the benchmarks.
On the simulated data, the mechanism performs quite well.
Analytically, we guarantee that the mechanism has a constant competitive factor in comparison to the optimal o ine solution, and the experiments show that this ratio is almost as small as 2.
In comparison to the best incentive compatible mechanism, this ratio is substantially smaller, and there is almost no di erence in the performance of the two mechanisms.
The simulations suggest that the Maxi-mizeTasks has near optimal performance in practice.
To examine the change in the threshold prices as the number of workers increases in the sample, we simulated tasks with various budgets and ran the mechanism.
We observed that in all simulations, the threshold prices converged quickly, and typically after running 16 and 32 bids varied by
 old price as a function of the stage of the mechanism (the number of workers that submitted their bids) on a logarithmic scale, during a simulation that used a budget of $100.
As one can see, the threshold price quickly stabilizes and remains almost constant throughout the run.
To observe workers  responses to various features in pricing mechanisms we experimented with four simple mechanisms and allowed workers  to bid multiple times in order to observe how features of the mechanisms a ect their bidding.
All mechanisms required workers to bid how much they wish to be rewarded for each assignment they complete and the number of assignments for this bid.
We allowed only 5 assignments to be performed per HIT (where the bid is per assignment), allowed bids no higher than $0.50, and allowed workers to perform 15 HITs.4 For each HIT performed the workers received a $0.03  xed reward for participating, even when their bid was rejected.
All information was clearly indicated in the instructions.
The workers were not noti ed what the pricing scheme was, and to avoid giving them information about the mechanism, if their bid was accepted, the payment they received was their bid.
We used the following pricing schemes:   Always Win Mechanism: This mechanism accepted workers  bids as long as the total payment to the worker did not exceed $3.00.
The fact that there is a budget or that it was exceeded was not revealed to the workers.
Once meeting their budget workers were allowed to continue bidding but their bids we rejected.
  Always Lose Mechanism: This mechanism implements a  xed price mechanism with threshold price of $0, i.e. workers were rejected regardless of their bids.
scribed below where the limit on the bid was $0.99 and we set a spending budget on each worker.
s t n e c ( i d










 Bids data3







 s t n e m n g s s
 i Best Possible Best Incentive Compatible Our Mechanism

 Bidder Number





 Budget


 x 104 Figure 3: (a) Variation of threshold price (pink line) over time (b) Comparative performance of algorithms   Fixed Price Mechanism: This mechanism uses a  xed threshold price that is not revealed to the workers.
Each bid at or below the threshold price was accepted and otherwise rejected.
In our experiment we set this threshold price to be $0.04.
  Random Price Mechanism: This mechanism randomizes over di erent threshold prices each time a worker bids.
If the bid is below the price the worker was allocated and otherwise their bid was rejected.
We also ran a control mechanism which presented workers with a  xed price of $0.03 per task and required workers to reveal how many assignments they wish to perform.
As in the above pricing schemes the limit was set to 5 assignments per HIT, and workers were allowed to perform 15 HITs.
There were 1033 bids (including 138 in the control) from 378 di erent workers in total (including 66 in control) in this experiment.
Each bid consisted of the maximal number of assignments the worker is willing to perform in the bidding round and their cost for performing an assignment.
In total, there were 952 valid bids, including 131 in the control (there were 81 invalid bids we discarded where the bid cost or number of assignments violated our instructions).
Recall that in this experiment a worker was given a chance to bid 15 times (rounds), and if their bid was accepted they worked on the tasks and received their bid as payment.
Figure 4(a) plots the average bid at a given round for each pricing scheme.
This  gure is complemented by Figure 5 which gives a histogram of the number of workers that remained in each round.
In the AlwaysLose pricing scheme, for example, there were only 2 workers after the eighth round, and the average price shown in Figure 4(a) is an average of these two bids.
Up until the forth round there were 10 and 11 bidders in AlwaysLose and RandomPrice, respectively, and
 that the majority of the information in Figure 4(a) is the  rst 5 bids.
Evidence of strategic bidding.
To examine whether workers include strategic considerations in their bidding, one can observe the obvious di erence between the plots of the di erent responses to the pricing schemes as shown in Figure 4(a).
Bidders in the AlwaysWin scheme, increased their bids as they got accepted (the following drop o  is due to the budget constraint we enforced, for methodological reasons).
In the AlwaysLose or RandomPrice schemes where workers bids were rejected, bids were lowered.
We see this as clear evidence that when given an opportunity workers  will declare false costs if they believe this will increase their pro t.
We see this as strong support for insisting on incentive compatible mechanisms.
Interestingly, although a budget constraint was implemented in the AlwaysWin scheme and workers were automatically rejected, their bids were still signi cantly higher than those of other pricing schemes.
E ects of rejection.
To observe this e ect on workers, in Figure 4(b) we plot the mean of the success rate of workers (the number of bids that were accepted) vs. the number of bidding rounds they participated in for the RandomPrice and the FixedPrice mechanisms.
5 Although there seems to be a negative correlation between success and number of rounds, one must remember that there is very little data (in the sixth round there were 12 and 8 workers in the Fixed-Price and RandomPrice mechanisms, respectively).
Better evidence for whether rejection a ects workers can be seen in Figure 5, where we plot the number workers in each bidding round for each pricing scheme.
There are evident drop o s in the RandomPrice and AlwaysLose mechanisms, where almost no bidders stayed beyond 6 rounds.
Note that this is despite the $0.03 they received simply for placing a bid.
This strengthens the claim that workers will avoid a HIT if they know they will be automatically rejected, even if they are paid to bid.
Even when there is no monetary loss, there is a price associated with sampling in crowdsourcing platforms which can result in slower completion times for a batch of HITs.
Quality of work.
Although the main measure of performance we consider in this paper is the number of assignments that can be performed under the budget, we examined the quality of the work performed as well.
Showing that workers perform well on their allocated assignments helps exclude concerns regarding negative e ects the bidding method may have.
To examine the performance of workers we chose the percentage estimation assignment since it allows us to objectively quantify workers  performance by considering their errors from the true answer.
In total, our mechanism allocated to 161 workers who, in aggregate, submitted 10870 answers (we count the number of answers submitted for each pie chart).
no successes in the AlwaysLose mechanism and also in the AlwaysWin mechanism once workers exceeded their budget.
s t n e c ( i d
 n a e













 Random Price Fixed Price Always Win Always Lose Fixed Price Random Price





 e t a
 s s e c c u
 n a e




 Bidding Rounds












 Max Number of Bids Submitted Figure 4: (a) Bids as a function of number of rounds (b) Success rates of workers in Fixed Price and Random mechanisms.
All workers who left before the sixth round had all their bids rejected and their success rate of zero is not shown.
d n u o
 n i s r e k r o
 f o r e b m u
 d n u o
 n i s r e k r o
 f o r e b m u





















 Always Win

 Bidding Rounds
 Fixed Price

 Bidding Rounds
 d n u o
 n i s r e k r o
 f o r e b m u
 d n u o
 n i s r e k r o
 f o r e b m u




























 Always Lose

 Bidding Rounds
 Random Price

 Bidding Rounds





 Figure 5: Number of workers in each bidding round.
The error distribution is presented in Figure 6.
The error (vertical axis) is the di erence between the workers guess and the actual marked percentage of red in the pie chart task.
In general, workers performed well on the assignments.
The worker mean error was 2.57, and almost all workers who were allocated assignments completed them.
This was consistent with the control group of 66 workers who received a  xed price reward, where the mean error was 2.59.
This implies that performance is not negatively a ected by bidding.
A subject of ongoing debate in the crowdsourcing community is the relationship between performance and monetary incentives.
To examine this in our context we compared a worker s mean error on the assignments performed against their bid.
The mean error re ects on the quality of work, and the bid indicates the reward the worker expects to receive.
We plot the worker s bid against their mean error in Figure 6(b).
In our examination we found no signi cant correlation.
We note that the data for this comparison involves 271 workers, since this is the total number of workers who were allocated assignments by the mechanism.
guarantees.
The mechanisms we presented are easy to implement, have strong theoretical guarantees, and perform well in practice.
From the experimentation on the platform, it seems there is evidence for strategic behavior and negative e ects when automatically rejecting workers.
We believe this evidence strengthens our model and assumptions and should be taken into account when designing pricing schemes in crowdsourcing markets.
We believe the model provides a good basis for designing pricing mechanisms for crowdsourcing markets, and that it can be further extended.
A natural extension of this model could incorporate veri cation schemes and automatic quality control that could integrate with the pricing mechanisms presented here.
We would like to thank Bj orn Hartmann for valuable discussions and advice.
Part of this work was done while the  rst author was at UC Berkeley and supported by the Microsoft Research fellowship and the Facebook fellowship.
We would also like to thank our anonymous reviewers for their thoughtful comments and suggestions.
