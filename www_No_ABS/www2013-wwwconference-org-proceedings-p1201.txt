With the prevalence of smart phones and tablet PCs in the past few years, we have all witnessed an evolution in the search engine industry where the user search activities have shifted from desktop to mobile devices at an incredibly fast pace.
According to a recent report [1], the year-on-year growth of mobile search volumes have more than doubled Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
University of Illinois at Urbana-Champaign Urbana IL, 61801 USA from 2011 to 2012, while the search volumes on all platforms have merely increased by 11% over the year.
Therefore, it is quite evident that understanding user search behavior on mobile devices has become more and more crucial to the IR community as well as the success of search engine companies.
Particularly, what do users search?
How do users formulate/reformulate queries?
More importantly, what are the di erences between desktop search and mobile search?
Google and Yahoo!
released their statistics regarding mobile search usages [24, 14], in 2008 and 2009, respectively.
The report from Google revealed that iPhone users bear lots of similarities with desktop users in terms of query length and query type distribution, while other mobile phones have shown di erent search patterns.
On the other hand, Yahoo! s  ndings disclosed that the US mobile query categorization is noticeably di erent from those of international mobile queries, where US users often issue longer and more complicated queries.
At a high level, both reports have shown that users exhibit di erent search intent on mobile than desktop.
For example, personal entertainment is the most popular category on mobile.
While much has been revealed from the aforementioned reports, we still believe that it is valuable to revisit this problem with the latest data.
As suggested in the Yahoo!
report [24], the authors discovered that mobile search pattern is still evolving, in terms of query distribution they studied.
One reason to support that is the query length: Yahoo!
reported an average of 3.05 query length on mobile in 2008, which turns out to be 2.93 (for iPhone) and 2.44 (for mobile) in Google s 2009 report, indicating that users continue to change their search and reformulation behavior.
Moreover, at the time the above reports were written, tablet PCs were not so popular.
However, with the debut of Apple s iPad in 2010, the usage of tablets for search has soared drastically in the past three years.
It is reported that as of March 2012, over 30% of US users have one or more tablets [2].
Therefore, in this paper, we also analyze the behavior of tablet searchers in order to update the mobile search picture with this important missing piece.
Speci cally, we make the following contributions:   We collect search engine logs with over 1 million users for a period of three months (August 2012 to October
 we restrict our study to be English queries in the en-US search market.
The mobile search logs include cellphone users while desktop logs contain both desktop and tablet users.  We perform a series of thorough and rigorous analysis 1201on mobile search behavior, in terms of time distribution, search locality, query categories, click patterns, browse patterns and so on, where we also compare with previous studies.  Based on the results which revealed noticeable di er-ences in mobile and tablet search patterns, we propose a novel knowledge transfer framework to train new rankers for mobile and tablet search results by leveraging a set of novel device-speci c features, as well as incorporating the training labels from desktop data to improve the search relevance.
In this section, we review the literature of mobile search behavior in recent years.
Although there has been many related work on mobile search [17, 5, 9, 19, 10, 11, 20, 18], we decided to primarily focus on the study after 2007 since the screen of smart phones have drastically changed after the appearance of iPhone in January 2007, which is one of the primary devices we analyze in this paper.
Kamvar and Baluja were among one of the earliest to report a large-scale study of mobile search statistics using over
 in early 2007 [13].
They explicitly compared the search behavior in 2007 to those of 2005, and found out that users tended to enter a query faster in 2007 due to the availability of high-end devices.
They discovered that query length increased from 2.3 in 2005 to 2.6 in 2007.
They also found out that, surprisingly, the click through rate, i.e., percentage of queries that had one or more clicks, had dramatically increased from less than 10% to over 50%.
Their study also revealed more tail queries issued during 2007, as well as a larger portion of adult queries in mobile devices.
In 2008, Church et al.
[8] reported a study on European mobile search logs, using 6 million queries from 260,000 search engine users over a period of 7 days.
In their study, query length appeared to be similar for both desktop and mobile searchers, who also exhibited similar click patterns via focusing on top-ranked results.
The query categories, or topics, however, appeared to be quite di erent, where adult queries consisted of over 60% of all queries, followed by email-related queries and personal entertainment queries.
Their data also demonstrated a larger portion of navigational and transactional queries in mobile logs, consisted of 60.4% and 29.4%, respectively.
In [24], the authors studied Yahoo!
mobile search logs during a period of 2 months in the second half of 2007, containing both US searchers and international searchers.
They explicitly studied three user interfaces of mobile applications: Yahoo!
oneSearch XHTML/WAP interface, Yahoo!
Go for Mobile, and Yahoo!
SMS Search.
The authors have observed that personal entertainment was the most popular query category, after  ltering out adult and spam queries.
Comparatively, US searchers often issue longer queries with more words than international searchers, resulting more tail queries in the US search logs.
At the same time, international searchers have larger diversity in terms of search intent, as indicated by the query topical distributions.
Finally, the authors concluded that mobile search was still evolving, based on the inconsistency observed from a variety of studies.
The study in [14] by Kamvar, Kellar, Patel and Xu was among the  rst to make explicit comparison between iPhone users and other mobile users.
They used the data from Google mobile logs with a period of 35 days in the summer of 2008, which contains over 100,000 queries issued by over 10,000 searchers.
Their study revealed several interesting aspects.
First, queries issued by iPhone users had similar length with desktop searchers (2.93), but signi cant-ly shorter for other mobile searchers (2.44).
Second, iPhone searchers also exhibited similar query categories with desktop users, both of which were more diverse than mobile searchers.
In terms of session statistics, the authors discovered that desktop users have the most queries per session, followed by iPhone then mobile users, indicating a possibility that the information needs were more diverse on desktop and iPhone, whereas mobile users were more likely to issue simple navigational queries with a more focused intent.
The authors in [15] studied user behavior in terms of abandoned queries in both mobile and PC search.
The objective of the study was to approximate the prevalence of good abandonment, i.e., queries that lead to satis ed user information need without clicks.
In three locales, US, JP and CN, the authors discovered that on mobile the portion of good abandonment is signi cantly higher than PC search.
In particular, for the US search market, the highest rate of good abandoned queries were primarily from local, answer and stock search, which was di erent from the other two markets.
The study suggested that query abandonment should not be treated as negative signal uniformly, instead both the locale and modality should be considered when categorizing abandoned queries into good and bad.
More recently, Teevan et.al [23] addressed the issue of local search on mobile devices, in which they conducted a survey on 929 mobile searchers.
The authors claimed that the mobile local search experience di ered a lot from desktop due to the limitation of the device, and therefore a ected user search behavior for local-related information.
Particularly, location (geographic features) and time (temporal aspects) played an important contextual rule in user s search behavior.
Users are more likely to search for locations that are close to their relative locations.
Besides, users are more likely to initiate a local search during a speci c time period.
Nevertheless, among all the aforementioned work we covered, none of them had studied the search behavior on tablet PCs such as iPad.
In what follows, we will analyze the d-i erences between mobile phone users and tablet users in a variety of aspects, as well as comparing to desktop searchers.
Due to the availability of di erent types of mobile devices in terms of screen size and network capacity, it becomes di cult to study user behavior across di erent platforms.
Thus we are unable to draw conclusions that are applicable to all types of devices.
Consequently, we restrict our study by focusing on two devices that are most popular on the market, i.e., iPhone and iPad.
The other reason is due to the consistent screen size of iPhone, which has not changed since its  rst generation1, neither has iPad.
Therefore, it is much more convenient and reliable to study the user search experience on these two devices.
In this paper, the data we use is from two di erent sources of Bing search logs.
iPhone users who visit Bing are presented with Bing mobile search interface (http://m.bing.com), while iPad users are experiencing the same search interface
 as other iPhone users but two more extrac lines.
Total Queries Total Users



 Desktop

 Mobile Tablet Desktop Number of words Number of characters





 Table 1: The data sets used in this paper.The  rst row represents total query volume and the second row are number of unique users.
as desktop searchers (http://www.bing.com) but on a smaller screen.
We distinguish iPhone and iPad users from other mobile users by  ltering based on the request agent and the platform.
We extract a sample of three months search logs in the United States search market, during August 2012 to October 2012 for iPhone, iPad and desktop users, respectively.
We then sub-sample roughly 1,000,000 users for each of the logs based on their unique user id string.
Finally, we  lter out non-English queries and consider only sessions that start from the Web vertical.
Table 1 presents the overall statistics.
Note that from now on,  mobile  explicitly means  mobile phones  in our description, as to be distinguished from  tablet .
We will also be using terms  mobile  and  iPhone ,  tablet  and  iPad , interchangeably.
In this section, we analyze the di erences between mobile and tablet searchers in a variety of aspects, including search time, location, query categorization, clicks and etc.
In this section, we provide query statistics including query length, query categorization and etc for the three platforms.
Table 2 summarizes the query length in terms of words and characters for mobile, tablet and desktop, respectively.
We observe that mobile users in general issue longer queries than tablet and desktop users.
The average number of words for mobile is 3.05, which is identical to Yahoo! s report [24] and slightly larger than Google s report (2.93) [14].
The length of tablet queries is shorter than mobile but longer than desktop, an average of 2.88 words and 18.02 characters per query.
The inconsistency of query length among various reports can be attributed to many reasons.
e.g., the evolving of user typing behavior, the diversity of query intent on different platforms and so on.
Among all these aspects, we believe that query autosuggestion plays an important role.
We discover that in our data set, query reformulation rate is almost identical on three platforms.
However, mobile users are more likely to rely on autosuggestion for query reformulation, perhaps due to the di culty of typing [14], where often longer queries are suggested by the Bing search engine.
On the other hand, since tablet users experience the same search interface as desktop users, it is hence not surprising to see the query length to be similar on these two platforms.
To analyze the query categorization for each device, we use a multiple-class classi er which categorizes a query into over 80 di erent categories.
Our classi er works slightly Table 2: Average Query Length.
Category Mobile Tablet Desktop Adult Autos Celebrity Commerce Finance Health Image Local Maps Movie Music Name Sports Navigational









































 Table 3: Query Categorization Distribution.
di erent than previous ones [24, 14] in the sense that we use a di erent taxonomy than others, e.g., we do not have an  Entertainment  category but similar ones like  Celebrity  or  Game .
Additionally, our classi er also allows one query to be classi ed into more than one categories, e.g., query  Michael Jackson  is categorized into both  Celebrity  and  Name  categories.
Table 3 lists the top-14 categories and their corresponding query distributions.
Di erent from previous reports [14], we see a signi cant di erence in iPhone categories than desktop categories, while the di erence between iPad and desktop is much less comparatively.
In general, mobile users are much more likely (23.5%) to issue adult-content queries than tablet and desktop users, which aligns with the fact found previously.
Mobile users are also over two times more likely to search for celebrity.
Also, 42% of the queries on mobile contain image-intent, where those queries trigger image answers on search result pages.
Noticeably, desktop only consists of 19.9% image-intent queries.
The most surprising  nding for mobile queries, however, is the percentage of navigational queries, i.e., 15.4%, which is much less than iPad (32.6%) and desktop (36.9%).
This discovery is interesting and also truth-telling.
Our hypothesis is that, after digging into the data, for the iPhone platform which has a quite mature app market, developers have already released corresponding apps for those navigational queries such as the facebook app for query  facebook  and amazon app  amazon .
As a consequence, with those free and powerful apps in hand, iPhone users are more likely to directly use the apps for their tasks, instead of resorting to search engines to  nd the corresponding sites.
Since navigational queries are in general shorter than informational queries, this further explains why iPhone users have longer query length as shown in Table 2.
On the other hand, iPad users exhibit di erent query category distributions than desktop in primarily two classes: local and commerce.
First, our data con rms the  ndings
 Mobile Tablet Desktop (cid:2)


 (cid:2)


 (cid:2) Table 4: Top-100 Query Overlap on three platforms.
in [14] that for local-intent queries, only 1.2% more local queries were issued from iPhone users than desktop computers (1.7% in previous report).
Surprisingly, iPad users have the largest percentage of local queries (11.5%) among all three, which could attribute to the fact that they use the map application more often than iPad and computer users [14].
Secondly, we observe that iPad users exhibit noticeably stronger shopping intent by searching for more commerce-related topics (11.6%) than the other two platforms.
Quantitatively, by normalizing the overall probability distribution to sum up to 1 and calculating the Kullback Leibler (KL) divergence, we observe that desktop and mobile have the largest divergence in query distribution with a score of 0.31, followed by the score between tablet and mobile of
 query distributions with a KL score of 0.07.
Additionally, we list in Table 4 the number of overlapped queries for the three platforms, by comparing the top-100 most frequent queries from each of them.
Comparatively, mobile and desktop share only 47 common queries, the least among all three pairs, while tablet and desktop have overlapping on 63 top queries.
These statistics are consistent with the KL divergence score listed above.
So what time of the day do users use their mobile phones and tablets for search?
Figure 1 illustrates the query volumes as a distribution of the time of the day.
Not surprisingly, the majority of desktop search occurs during normal working hours from 8AM to 5PM.
Meanwhile, as we can see, mobile search volume continues to rise starting from 5AM till 10PM of the day, and then declines during midnights.
On the other hand, tablet usage shows a fairly di erent pattern.
The search volume of iPad maintains relatively low during normal business hours, i.e., from 7AM to 5PM.
It then rises sharply from 6PM to 10PM, which peaks at 7PM with over
 increase of iPad search activities during the night compensates for the steep decline of the desktop search volume at the same time period.
Furthermore, we are interested in the distribution of query categorization at di erent time of the day.
To do so, we separate one day into four groups with each group of 6-hour duration.
Table 5 lists the top-3 query categories for each platform during the day.
We omitted the night group (1AM to 6 AM) here since the search volume is not signi cant to draw a distribution.
Overall, mobile searchers exhibit much diversi ed information need at di erent time of the day.
While showing image and navigational-related intent during the mornings, mobile searchers are more likely to issue local queries in the afternoons, adult and music queries during the evenings.
For tablet users, local queries have been observed as one of the top-3 categories throughout the day, whereas commerce-related queries start to emerge in the afternoon and evening groups.
In contrast, desktop searchers n o i t i u b i r t s
 y r e u
 Mobile Tablet Desktop

 .
.
.
.
.
.
Hour of the Day











 Figure 1: The time distribution of usage in terms of search volumes for three platforms.
Morning (7-12) Mobile Image Navigational Name Afternoon Local (13-18) Celebrities Image/Name Adult Music Sports Evening (19-24) Desktop Tablet Navigational Navigational Image Local Local Commerce Image Navigational Navigational Commerce Local Image Local Navigational Local Name Local Commerce Table 5: Break down of top query categorization by time of the day.
are always more interested in issuing navigational queries than the other two platform users, and exhibit a fairly stable information need throughout the day.
Next, we analyze the location where search activities happen for mobile and tablets.
As previous reports indicated, tablets are mostly used on couches and beds [3].
Therefore, we hypothesize that in terms of mobility, iPad users do not move as frequently as iPhone users, who can basically perform search at any location they go to during the day.
To validate our assumption, we sub-select a sample of
 mobile and tablet logs, we were able to extract the longitude and latitude of the location where the user query was issued.
Each of the geo-location is mapped to its nearest city as well, e.g.,  Redmond, WA ,  Bellevue, WA  etc.
Table 6 lists some statistics regarding location changes during the three-month logs we collected.
Overall, mobile users searched from 4.52 di erent cities on average, while table users only traveled to an average of 1.79 cities.
Note, it does not mean that on average a mobile user only appeared at 4.52 di erent places when performing search, since multiple geo-locations can be mapped to the same city name.
Instead, the second row of the table shows how much distance (in miles) users traveled: on average mobile searchers were recorded to travel over 118 miles while tablet users only traveled for less than 50 miles.
The percentage of users who never traveled, i.e., all search requests came from the same city, also shows signi cant di erence between two types of devices.
For mobile, less than 10% of users stay in one city, while the number is 3.7 times more for tablet users.
Since
 Cities Visited Distance Traveled (mi) % users never traveled % queries issued at home







 Table 6: Statistics on location changes.
Mobile Tablet Desktop Number of queries in Session Session Duration(min) (Filtered) Session Duration(min) Daily Number of Sessions











 Table 7: Session statistics.
Filter sessions are sessions without abandoned queries.
SERP DwellTime (sec) Avg Click Position Answer CTR Algo CTR Click Entropy Mobile Tablet +87.35 -20.54 + 0.54 -0.04 +0.02 +0.07 -0.08 -0.20 +0.14 -0.05 Table 8: Click statistics are shown as relative numbers comparing to the desktop search numbers.
in general the least number of sessions in a day (1.42).
This could be explained by the fact, as also shown in Figure 1, that most iPad search happened during night time between
 are more likely to be in the same sessions.
Comparatively, desktop users have more search sessions in a day than mobile and tablet which is expected since desktop still has the largest search volume.
Next, we examine the di erences in search result clicks.
Note that our data is collected from Web vertical only.
However, since Web vertical sometimes also shows image, video or local results, we group these non-algorithmic clicks to be answer clicks in our analysis.
In Table 8, we illustrate basic click statistics.
Due to the sensitivity of the click data, we decide to report relative numbers here which use the number from desktop search as baseline comparison.
Our  rst observation comes from SER-P dwell time, where mobile searchers spent 20 less seconds examining the result page than desktop searchers.
Meanwhile, tablet users exhibited much stronger interests in examining the SERP, 87 seconds more than desktop searchers.
These numbers further con rm our results in Table 7 regarding the overall session duration time.
In terms of click position, we observe that mobile searchers are more likely to click results that rank lower, an average increased position of 0.54.
Tablet users, however, show no signi cant di erence than desktop with similar click position (-0.04).
The click through rate (CTR) is an important metric used to measure search relevance.
We can see from Table 8 that both mobile and tablet searchers are more lean to click on answers rather than algorithmic results (i.e., the ten blue-links).
Speci cally, mobile users have demonstrated 7% more CTR on answers but 20% less CTR on algo results, whereas tablet searchers are 2% more likely to click answers but 8% less willing to click algo results.
Finally, the click entropy scores [22] indicate that on mobile, the clicks are more spread out with a higher entropy score, whereas on tablets clicks are often concentrated on top results, similar to that of desktop clicks.
Figure 2: The locations where users performed search.
Blue eclipses correspond to mobile searchers while red eclipses mean tablet searchers.
we do not know exactly where the  home  of a user locates, we simply treat the city that most queries were issued for that user as his/her home.
By doing that, we observe that mobile users indeed issued queries at many more di erent locations than tablet users: 43% of queries issued at home versus 79%, which nearly doubled.
Figure 2 illustrates an example of three mobile users and three tablet users, respectively.
We use eclipses to approximate the perimeters that cover the cities in which users traveled, where blue corresponds to mobile and red to tablet users.
Clearly, the eclipses for mobile users cover much larger diameters than tablet users, who merely traveled to very nearby locations around their home.
In this section we analyze user session and click characteristics.
We use the conventional de nition of sessions in our analysis: a session contains user activities including query, query reformulation, URL clicks and so on.
Sessions are grouped based on user IDs which are unique for each user.
A session ends if there is no user activity for 30 minutes or longer.
Each session is also assigned with a unique ID.
The duration of a session is de ned as the di erence of times-tamps between the last and  rst activity.
Table 7 shows several basic statistics of sessions.
Since a large portion of the sessions contain only abandoned queries, i.e., sessions with only one query but no clicks [15], we want to distinguish these sessions from others so we report numbers with and without these sessions.
We  rst observe that mobile sessions contain the least number of queries (1.48), and therefore the shortest session duration of 7.62 minutes (8.25  ltered) among all three.
iPad users, however, spent longer time than desktop and iPhone users with an overall 1.94 queries per session and 9.32 minutes (12.78  ltered) in each session.
Despite longer session time, iPad users have
 youtube.com en.wikipedia.org answers.yahoo.com amazon.com ehow.com imdb.com amazon.com wiki.answers.com chacha.com facebook.com myspace.com msn.com ebay.com imdb.com ehow.com facebook.com craigslist.org itunes.apple.com craigslist.org Tablet youtube.com en.wikipedia.org Desktop facebook.com yahoo.com en.wikipedia.org youtube.com walmart.com ebay.com amazon.com mail.google.com aol.com Table 9: Top-10 click domains.
The previous results include user clicks on both answers and algorithmic results.
Since we see a signi cant di erence on algo result clicks, we will focus on click patterns only on algorithmic results in this section.
We  rst list the top-10 most clicked URL domains in Table
 on desktop (i.e., facebook and yahoo), have both been replaced by youtube and wikipedia sites on mobile and tablet.
The results again re ect our discovery in Table 3, but from the perspective of clicks, that mobile and tablet both have lower navigational-type queries and therefore users are less likely to click on those sites where iPhone and iPad apps are already available.
Next, we can also clearly observe that on tablet, shopping-related sites such as amazon, ebay and craigslist are ranked much higher than desktop and mobile.
This is also consistent with Table 3 where we show that tablet users in general have higher percentage of commerce-related queries.
Perhaps the most interesting discovery in Table 9 is those highly-ranked knowledge base sites on tablet and mobile, especially on mobile.
In particular, we observe that among the 10 most popular sites on mobile, 6 of them (Wikipedia, Yahoo!
answer, ehow, etc.)
are knowledge base sites.
Consequently, the click distributions of traditional navigational queries have changed accordingly on mobile and tablet platforms.
For example, for the query  louis vuitton , desktop searchers clicked on the  rst result and went to its o cial site with over 0.7 CTR.
However, this number dropped to
 and iPhone users clicked more frequently on the Wikipedia page of Louis Vuitton, with CTR of 1.5% and 4.3%, respectively, even that the page is ranked at the bottom of the search results.
Even for the query  facebook  that has the strongest navigation intent among all queries, mobile and tablet still possess 3.5% and 2.7% CTR on the Wikipedia page, respectively.
Table 10 lists some example queries and their CTRs on knowledge base sites for mobile, tablet and desktop respectively.
We have noticed that our  ndings concur with some recent studies as well [21, 7].
From the analysis we have performed in the previous section, we have clearly observed di erent user search behavior on both mobile (iPhone) and tablet (iPad) than desktop Query hotmail microsoft usa facebook louis vuitton CTR for Knowledge Base Sites on Desktop Mobile









 Tablet




 Table 10: CTR for knowledge base sites such as Wikipedia, Yahoo!
answers and etc.
Mobile and tablet users are much more likely to click on those sites than desktop searchers.
users, in terms of query categorization, click intent, time and location of search and etc.
Consequently, due to the diversity of search intent, mobile and tablet users have incurred a signi cantly lower click through rate (CTR) on algorithmic results as shown in Table 8, due to the factor of using a uni ed ranker on all three platforms.
To further improve the search relevance for mobile and tablet users, we propose to optimize the (algorithmic) search results by (1) incorporating new features that consider a variety of search aspects including time, location, intent and so on, (2) adopting the existing relevance labels from desktop search to train new rankers, as inspired by [4, 6].
Inspired by the analysis in previous section, two sets of features are derived in our framework.
Speci cally, query attributes features measure the characteristics of the query itself, across three devices and at di erent time of the day.
On the other hand, URL relevance features estimate the importance of URLs given a particular query.
Table 11 lists the features and the equations to calculate them.
q-prob(Query|d) and q-prob(Query|t) measure the query frequency on a particular device d at time t of the day, where d is the one of the three devices we considered here: mobile, tablet and desktop.
t is a numerical value indicating the time windows of the day, which is split into four groups (morning, afternoon, evening and night), similar as shown in Table 5.
q-prob-cross(Query|d) and q-prob-cross(Query|t), on the other hand, measure the cross-device and cross-time probability of a query.
Comparatively, q-prob(Query|d) estimates how important the query is comparing to all other queries issued on the same device, whereas q-prob-cross(Query|d) judges how likely this query is issued on that particular device rather than other two devices.
Likewise for t. These four features together demonstrate the overall importance of a query in the entire data set.
CTR(Query|t, d) signals the search intent of users when they are interested in the particular query.
The higher the CTR is, the more likely users are clicking on related URLs.
The CTR is estimated by averaging over CTRs on all return URLs for the query during time t on device d in the data set.
Entropy(Query|t, d) calculates the entropy of a given query.
This is also a signal to measure the popularity of the query during di erent time of the day, given a particular device d.
KL(Class(Query), Class(U)) measures the topical closeness between the query and the URL.
As we mentioned before, our classi er assigns each query (as well as a URL) into one or more of the 80 categories, which is essentially a probability distribution over all topics.
The smaller the KL score is, the more likely the query and URL are related.
click-prob(U|Query, t, d, loc) considers the probability a URL gets clicked when a user issues the query at time t on device d and at a speci c location loc.
We specify the location parameter at two levels: city and state.
To be concrete, we assign each city a unique ID and calculate click-prob(U|Query, t, d, city).
Likewise for the state level.
At these two di erent granularities, we measure the locality e ect of the URL clicks.
Comparatively, loc-prob(U|loc) is a query-independent metric that calculates the overall locality e ect of a URL.
Similarly, Entropy(U|loc) also measures how likely the URL gets clicked at location loc.
These two features are also pa-rameterized with two di erent location levels: city and state.
Likewise, we also include query-independent features for time and device, which have similar equations and therefore omitted from Table 11.
Since we have discovered that mobile and tablet users are more likely to click on knowledge base sites, we propose a feature to take that into consideration.
Speci cally, wiki-prob(U) calculates the probability of a sites to be knowledge base, according to the frequency the site is clicked.
We maintain a list of over 30 knowledge base sites including Wikipedia, Freebase, Yahoo!
answers, eHow and etc.
fer from Desktop Search With the new features derived in the previous section, we are ready to train a ranking model to improve the relevance on mobile and tablet.
One way to achieve this goal is to leverage the learning-to-rank framework [16] by collecting judgement labels for individual query-URL pairs to form a training set, and use the domain-speci c features in Table
 mal due to the expensive cost of acquiring human labels.
In particular, it is very labor-intensive and cost-ine ective to gather labels on mobile and tablet platforms, especially in our scenario where each query-URL pair can generate multiple labels according to di erent time, location and device.
Consequently, the cost could go exponential to the number of query-URL pairs and make this approach unable to scale.
On the other hand, human judgement labels for desktop search are available in abundance on many benchmark data sets, e.g., TREC, LETOR [16] and etc.
These data sets, along with a rich set of textual features such like BM25, term frequency and etc, facilitate the work of training rankers for desktop search using di erent machine learning methods.
Therefore, our objective is to leverage the labels and content-features from desktop search, combining with a few labels from mobile and tablet as well as their domain-speci c features, to train new rankers for the two new domains.
Our framework is greatly inspired by the work in [4] and [6].
The general idea is to simultaneously use the source (desktop) and target (mobile and tablet) domain training data during the learning process, where the training data from the target domain is di cult to acquire while the data from source domain is available in abundance.
These two domains, however, share certain common features, and therefore we can learn a low-dimensional representation so that the features from both domains can be projected to.
In [4], the authors have shown that this problem can be formulated as a 1-norm regularization problem that provides a sparse representation for multiple domains.
Furthermore, in [6], the authors have proven that for learning-to-rank, the same problem can be transformed into an optimization framework which can be solved by using Ranking SVM [12], after certain transformation of the training data.
Formally, we are given three sets of training data Dd, Dm and Dt, which correspond to desktop, mobile and tablet respectively.
These data sets share the same feature space w   Rd, which can be broken down into two parts.
The  rst part contains k features [w1, ...wk] that are common features available on all domains (e.g., BM25, document length and etc).
The second half of features [wk+1, ..., wd] are domain-speci c features which are only available on mobile and tablet domains.
The learning objective is to minimize the pairwise loss on all three domains.
Speci cally, for each domain, given a set of training data D = {xi}m 1 , we form a set of pairwise preferences S = {(xi1, xi2)}n 1 , where each pair indicates a preference relationship of xi (cid:3) xj, which can be determined, for example, using human labels.
Using Ranking SVM as a learning framework, we can assume the learning function to be linear, e.g., f (x) = (cid:4)w, s(cid:5), where si = xi1 xi2 is a new training sample by subtracting the feature values of xi2 from xi1.
The label yi of si is 1 if xi (cid:3) xj and 1 otherwise.
This way we form a set of new training data S(cid:2)
 = {si, yi}n Algorithm 1 sketches the learning process of how to minimize the pairwise loss for three domains.
Our algorithm is similar to the CLRank algorithm in [6].
The major difference is that we apply the learning to three domains instead of two which was the case in [6].
The general idea is to  nd a lower-dimensional representation of the three feature vectors, by performing SVD on D, which represents the covariance matrix of the model weights, or how many common features these three domains share..
The training instances are then transformed into this low dimension and trained using Ranking SVM.
After training, the original feature weights are updated by transforming back the weights into its original dimension.
The matrix D is also updated with the new W .
This algorithm runs in iterations and stops when some criteria are met, e.g., the covariance matrix D is no longer showing signi cant change.
In this section, we conduct rigorous experiments to assess the performance of ranking mobile and tablet algorithmic results using domain-speci c features and the CLRank algorithm.
The data sets used for ranking is the same as the ones we perform user behavioral analysis, as shown in Table
 sessors to manually label each query-URL pair with 5-point Likert scale: Perfect (5), Excellent (4), Good (3), Fair (2) and Bad (1).
Each query-URL pair is given to three human assessors and we apply majority vote to get its  nal label.
Overall, we randomly select 3,500 query-URL pairs from the 1 million queries used in our study for judgement.
On the other hand, as mentioned above, for mobile and tablet, it is di cult to collect human labels because each query-URL pair can have di erent ratings depending on the
 Feature Query Attributes q-prob(Query|d) = cnt(Query|d)+ qd (cid:2) q cnt(Query|d)+ d cnt(Query|d)+ qdc (cid:2) d cnt(Query|d)+ cd q-prob-cross(Query|d) = q-prob(Query|t)= q-prob-cross(Query|t)= CTR(Query|t, d) = avg [CTR(U|Query, t, d)] Entropy(Query|t, d) = -q-prob(Query|t, d) * log q-prob(Query|t, d) cnt(Query|t)+ qtc (cid:2) t cnt(Query|t)+ ct cnt(Query|t)+ qt (cid:2) q cnt(Query|t)+ t (cid:3) (cid:4) (cid:2) URL Relevance KL(Class(Query), Class(U)) = click-prob(U|Query, t, d, loc) = loc-prob(U|loc) = cnt(U|loc) l(cid:2) cnt(U|loc) Entropy(U|loc) = -loc-prob(U|loc) * logloc-prob(U|loc) wiki-prob(U) = Pc(Query) Pc(U ) cnt(U|Query,t,d,loc+ qtdl d(cid:2) (cid:2) c log q(cid:2) (cid:2) Pc(Query) cnt(U ) I(IsWiki(U )) (cid:2) u List(wiki) cnt(u) t(cid:2) (cid:2) (cid:2) (cid:2) l(cid:2) cnt(U|Query,t,d,loc)+ qtdl(cid:2) Table 11: List of query-attribute and URL-relevance features used for ranking.
All the  s are smoothing parameters that are estimated from the data set.
Algorithm 1 The CLRank algorithm
 i }Nm i}Nt
 features W =
 i , ym Parameter   for Ranking SVM
 , St = {st Sd = {sd i }Nd i , yd i, yt
 [wd, wm, wt] I d d
 4: while not converge do


 d ; get three new training data sets Sd(cid:2) , Sm(cid:2) , and St(cid:2)





 2 ut; Set D =
 11: end while

 trace(W W T )1/2 ud, um and ut;
 Transform wd = P T  


 2 um, wt = time, location, device and etc.
As a result, we resort to user clicks as pseudo labels for mobile and tablet.
We count all clicks for each query-URL pair which is parameterized by time, location and device.
For each query, we assign the same 5-point labels to URLs according to the descending order of the click counts.
Overall, we collect 5,000 such query-URL pairs for mobile and tablet, respectively   a total of 10,000 training examples.
For each training instance, we generate 400 content-based features such like BM25, document length [16], along with the 20 domain-speci c features proposed in Table 11.
To be more convincing, we compare with several baseline methods in order to show the superior of our proposal.
Baseline 1: the default ranking model of mobile and tablet   the same ranker as the desktop search.
Here we do not modify anything but just report the default score.
Baseline 2: content-based features plus new domain-speci c features.
i.e., we train Ranking SVM models for mobile and tablet respectively, using the new 5,000 training instances described in the previous section.
Baseline 3: knowledge transfer without new features.
Train CLRank algorithm on three domains with the original
 ranker except that it does not leverage the domain-speci c features proposed in Table 11.
Note that for both baseline 1 and baseline 2, two separate rankers need to be trained respectively for mobile and tablet.
While for baseline 3, as well as our  nal ranker, only one optimized model will be outputted, i.e., the W matrix on line 2 of Algorithm 1, which contains feature weights for all three domains.
For evaluation, we employ two classic metrics: MAP@K and NDCG@K. MAP calculates the mean of average precision scores on all queries in the test set.
NDCG score, on the other hand, takes both the 5-scale relevance score and the position of the relevant documents into consideration.
In our experiments, we report results for both K = 1 and 3.
To report experimental results that are statistically meaningful, we randomly separate the 10,000 labeled data of mobile and tablet into two parts for training and test at 1:1 ratio, where the test set is withheld only for evaluation.
We repeat this process 20 times and report the average performance.
To determine the optimal value of the only parameter   in the CLRank algorithm, we perform a 5-fold cross validation on the training set, and  nd out   = 0.15 to be the optimal value.
Table 12 compares the overall performance of the four methods on mobile and tablet, in terms of MAP and NDCG scores respectively.
In general, we see that both baseline 2 and baseline 3 make noticeable improvement over the default baseline 1.
In comparison, baseline 3, which applies the knowledge transfer framework (CLRank) without new features, slightly outperforms baseline 2 which only uses the new features to train new rankers.
As mentioned previously, baseline 3 leverages the CLRank model that jointly optimizes the rankers for all three domains, instead of training separate rankers for each domain as used by baseline 2.
This comparative result indicates the potential superiority of using existing labels from other domains to enhance the current ranking system of the target domain.
Tablet Baseline 1 Baseline 2 Baseline 3 Our Method
































 Table 12: Overall performance of three baseline methods and our framework in MAP and NDCG.
Our method outperforms all baseline methods, where * indicates p-value < 0.05 and ** means p-value < 0.01.
On the other hand, when combining the domain-speci c features with the labels from desktop training data, we have observed signi cant performance improvement of our method comparing with all the baselines, with statistical signi cance level at p-value < 0.01 for all the metrics.
Overall, our framework improves around 5% for both MAP and NDCG on mobile ranking baseline 1, whereas for tablet, the improvement is less (3%) but still quite signi cant comparing to other baselines.
In previous experiments, we limit the use of desktop training data to be 3,500.
It would be helpful to analyze the performance change when that part of data becomes more/less.
Consequently, we run a series of experiments by using only a certain portion of the desktop data for knowledge transfer, ranging from 500 to 3,500 instances.
Figure 3 illustrates the MAP and NDCG changes in terms of the training data size.
Note that neither baseline 1 nor baseline 2 leverages training data from desktop, their performance is therefore not a ected as demonstrated by the horizontal lines.
Overall, we observe that for both baseline 3 and our method, more desktop data indeed helps improving the performance.
More speci cally, for the mobile domain, we see a dramatic increase of MAP and NDCG scores when the data increases from 500 to 1,000.
The performance is then stabilized after 2,000 instances and only minor improvement can be observed beyond that.
Comparatively, we notice that for the tablet domain, the performance increase is almost linear to the number of desktop training data, where it shows no sign of stopping even when all 3,500 training data has been utilized.
Therefore, the tablet domain may potentially bene t more if we can provide more than 3,500 labeled desktop data in this scenario.
Next, we illustrate the performance improvement within In Figure 4, we show the im-di erent query categories.
provement in terms of the MAP@3 scores by our method over baseline 1.
Among all 14 categories, our algorithm improves mostly on navigational, local and map queries.
As we discussed before, for navigational queries, mobile and tablet users tend to click more on the knowledge base sites.
Therefore, by leveraging the domain-speci c features to improve the ranking of these sites, we successfully increase the MAP score for those navigational queries.
Comparatively, we also observe that tablet has less MAP@3 improvement.
In particular, queries in adult, movie and name categories bene t little from our algorithm.
It could be the reason that these queries are more informational, where user intents are more diversi ed and therefore more di cult to optimize.
Finally, we also break down the metrics based on the time of the day as discussed in Section 3, as shown in Table 13, which indicates the relative improvement of our CLRank algorithm over the default baseline method.
We see that in Mobile Tablet
















 Morning Afternoon Evening Night Table 13: MAP and NDCG improvement based on the time of day.
The numbers mean the absolute di erence between our algorithm and baseline 1.
general, mobile gains the most improvement during afternoons and evenings, whereas tablet has the biggest jump in evenings.
Since the majority of the search tra c is from afternoons (for mobile) and evenings (for mobile and tablet) as illustrated in Figure 1, we can clearly see the bene t of using our algorithm over the existing systems.
The objective of our query analysis study was to reveal the user behavior di erence between mobile, tablet and desktop searchers by using the latest log data from a commercial search engine.
More importantly, our research aimed at  lling the gap between mobile and desktop search by taking tablet user behavior into consideration, which we found missing in the previous studies of mobile search [24, 14].
We provided quantitative statistics on a variety of aspects for mobile and tablet search which were later used to guide the improvement of search result ranking on them.
Our study on the three-month Bing mobile and desktop logs disclosed various points that were di erent from previous studies.
The following is a few key discoveries from our study:   The query length on mobile continues to change.
Our study showed an average of 3.05 words per query for mobile and 2.88 for tablet, both of which are longer than desktop.
Comparing to the numbers from Yahoo!
(3.05) and Google (2.93), this number seems to change all the time.
Therefore, we think that the usage patterns are still evolving.
  The distribution of query categories was di erent between mobile, tablet and desktop in certain categories.
Specifically, tablet users were more likely to issue commerce and local queries, while mobile users issued more adult, celebrity and image queries.
One important  nding in our study was that both mobile and tablet users issued signi cantly less navigational queries than desktop users, due to the wide availability of mobile apps on these two platforms.
  The distribution of usage time was also di erent on three platforms.
While desktop users performed search mostly during working hours (8AM to 5PM), mobile and tablet
 Mobile NDCG@1 Tablet MAP@1 Tablet NDCG@1 Baseline 1 Baseline 2 Baseline 3 Our Method
 @ p a m


 .
.
.
.
Baseline 1 Baseline 2 Baseline 3 Our Method
 @ g c d n


 .
.
.
.
.
.
.
Baseline 1 Baseline 2 Baseline 3 Our Method


 .
.
.
.
.
.
.
@ p a m Baseline 1 Baseline 2 Baseline 3 Our Method


 .
.
.
.
.
@ g c d n

 Desktop Training Data Size



 Desktop Training Data Size



 Desktop Training Data Size



 Desktop Training Data Size

 Figure 3: MAP@1 and NDCG@1 change in terms of the desktop training data size, which is used in the knowledge transfer algorithm (CLRank) to boost the performance of mobile and tablet.
t n e m e v o r p m

 @




 .
.
.
.
.
.
Mobile Tablet Query Categories Adult Autos Celebrity Commerce Finance Health Image Local Maps Movie Music Name Navigational Sports Figure 4: MAP@3 improvement by our algorithm over baseline 1 in 14 query categories.
usages peaked during evenings (6PM to 10PM).
We also revealed that during di erent time of the day, mobile and tablet users had more diverse search intent than desktop, where the latter seldom changed throughout the day.
  The location of usage was quite di erent between mobile and tablet.
Overall, mobile users tended to travel a lot more than tablet users and issued queries at a variety of locations.
For tablet users, 79% of queries were issued at home, whereas only 43% mobile users issued queries at their home locations.
  Interestingly, mobile and tablet users tended to click more on knowledge base sites like Wikipedia, even for very top navigational queries such as  facebook  and  hotmail .
This discovery can help us better understand user click intent which may eventually lead us to retrain the query clas-si er for mobile and tablet.
  Merely using traditional content-based features to rank search results on mobile and tablet can lead to subopti-mal performance.
We observed a signi cantly lower CTR on these two platforms while using the default ranker from desktop.
We therefore proposed a set of domain-speci c features to address the limitation of features from desktop.
By leveraging a knowledge transfer algorithm (CLRank) that used training data from all three domains simultaneously, we eventually saw a signi cant performance improvement on mobile and tablet.
This study revealed that (1) domain-speci c features were important for mobile and tablet relevance.
Even with only 20 new features, we have witnessed a 5% and 3% relevance improvement for mobile and tablet, re spectively, and (2) human labels from desktop can be leveraged to improve the rankers for other domains as well.
A joint optimization on three rankers work better than optimizing rankers for di erent domains individually, especially when these domains share some common features.
Overall, we have observed that tablet users have distinguished themselves from desktop and mobile users with quite di erent user behavior and intent.
It is therefore suggested that when performing user behavior analysis or designing relevance algorithms, tablet should be treated as a separate device rather than merge it with either desktop or mobile.
There is still a lot left to be done in the future.
Our study covered two most widely used mobile and tablet devices (iPhone and iPad) on the market.
However, with the choice of smart devices becomes more and diversi ed nowadays, it is important to take user behavior on all di erent devices into consideration to get a complete picture to draw unbiased conclusions.
In the future, we plan to extend our work to cover more smart devices such as Android tablets, Microsoft Surface, iPad Mini and so on.
A comparison between these tablet users may yield di erent outcomes than what have been observed in this paper.
On the other hand, since we have observed that more and more users tend to click on answer results besides the algorithmic results, we will also be interested in extending our knowledge transfer algorithms beyond algorithmic results, by also adopting answers and ads into the framework.
