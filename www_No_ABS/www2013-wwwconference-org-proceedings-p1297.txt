An important problem in network analysis is partitioning nodes, also sometimes known as  nding communities.
This requires  nding clusters of nodes that are inherently well-connected within themselves with sparser connections between clusters; when the clusters do not overlap, they de- ne a partitioning of the graph.
This problem is also closely related to  nding network bottlenecks; if a graph has bottlenecks, then a good partition is often found by dividing the graph at its bottlenecks.
Many real-world networks are truly vast, encompassing hundreds of thousands to billions of nodes and edges; for example, communication, social and biological networks.
This scale produces serious computational challenges for detection of bottlenecks and communities: the large majority of algorithms are computationally too intensive to use at this scale on such graphs.
Instead, one can study smaller sub-graphs of these networks; for example, the portion of a social network corresponding to one university or the portion of a communication network corresponding to one Internet service provider, and hope to derive properties of the larger graph from those of the smaller sub-graphs.
In this paper, we show how to de ne key properties of a graph, its Dirichlet spectral gap and eigenvectors which aid in clustering, and show how these closely relate to the 1297spectral gaps and eigenvectors of its sub-graphs thus making identi cation of bottlenecks and points of congestion both more e ective and scalable.
More precisely, spectral graph theory [3], the study of eigenvalues and eigenvectors of graph-theoretic matrices, is often used to analyze various graph properties.
One might hope that the properties of a large sub-graph of a network will be representative of the properties of the entire network.
Unfortunately, the properties of an expander graph depend on the conditions imposed at its (large) boundary.
For example, the spectral gap of the graph Laplacian on a  nite truncation of an in nite regular tree approaches zero as the size of the truncation is increased, even though the spectral gap of its in nite counterpart is nonzero.
In this paper we show that, by contrast, if the spectral gap is calculated with Dirichlet boundary conditions, it approaches the in nite graph limit as the size of truncation is increased.
Computation of a better spectral gap makes it possible to do spectral clustering more e ectively thus making identi cation of bottlenecks and points of congestion more e ective.
Motivated by this result, we compute the Dirichlet spectral gap for ten IP-layer communication networks as measured and documented by previous researchers in the Rock-etfuel database [18].
We  nd that the Dirichlet spectral gap is much larger than the traditional spectral gap for these graphs.
(Traditional spectral clustering uses the normalized Laplacian matrix L or some similar matrix; we use the matrix LD: the Laplacian restricted to the rows and columns corresponding to non-boundary nodes.)
Moreover, unlike the traditional spectral gap, it does not trend downwards for larger networks.
This indicates that the spectral gap for these networks viewed as sub-graphs of a much larger graph is away from zero.
There are precedents for treating networks essentially as subsets of an an overarching (in nite) graph; many network generation models [2, 6, 21] exhibit unique convergence properties (to power-law degree distributions or otherwise) as the size of the network grows to in nity.
We also note that Dirichlet boundary conditions have been shown to be successful at mitigating other boundary-related issues in graph vertex ranking [5].
There is a direct connection between the spectral gap and clustering in networks, through the Cheeger inequality.
Spectral graph theory has led to many e ective algorithms for  nding cuts that result in a small Cheeger ratio, including spectral clustering [15, 17, 16, 20] and local graph partitioning algorithms [1].
These algorithms have been well-studied, both empirically [15, 17] and theoretically [15, 20].
Unfortunately, these algorithms can also exhibit some undesirable behavior.
It has been shown empirically [12] that the  best  partitionings of many networks, as measured by the Cheeger ratio, result in cutting o  nodes or subtrees near the boundary of the network.
The resulting  clusters  near the boundary actually consist of several disjoint fragments.
Especially when viewed as subsets of larger networks, this kind of clustering is not particularly meaningful.
In this paper, we use Dirichlet spectral clustering to identify good cuts in the networks in the Rocketfuel database.
We use the top two eigenvectors of LD, the normalized graph Laplacian with Dirichlet boundary conditions, to cut the network into two sections.
We demonstrate that, compared to traditional spectral clustering, there is a substantial reduction in the average number of components resulting from the cut, without a signi cant increase in the Cheeger ratio.
Instead of  nding cuts near the boundaries of the networks, Dirichlet spectral clustering obtains cuts in the network core.
The Cheeger ratio of a cut is a well known indicator of the congestion across the cut; small Cheeger ratios are likely to be associated with bottlenecks.
The emphasis on identifying core bottlenecks becomes more critical in the light of the recent observation that many real-world graphs exhibit large-scale curvature [10, 14].
It has been shown [10, 14] that such global network curvature leads to core bottlenecks with load (or betweenness) asymptotically much worse than  at networks, where  load  means the the maximum total  ow through a node assuming unit tra c between every node-pair along shortest paths [14].
As such, it is important to  nd and characterize bottlenecks at the core rather than the fringes, where they do not matter as much.
Our observations, suggest that Dirichlet spectral clustering may be more useful in this regard.
The rest of this paper is structured as follows: in Section 2, we give the theoretical justi cation for using Dirichlet eigenvalues [4] instead of the traditional spectrum for analyzing and clustering  nite portions of much larger graphs.
In Section 3, we then compare the spectral gap using Dirichlet eigenvalues to the traditional spectral gap on real, publicly-determined network topologies [18] that represent smaller portions of the wider telecommunications grid.
In Section
 graph partitions that are more indicative of bottlenecks in the network core rather than the fringes.
Throughout this paper, we analyze general undirected connected graphs G by using the normalized graph Lapla-cian L, de ned as in [3].
For two vertices x and y, the corresponding matrix entry is:  
   1   
 Lxy = dxdy if x = y, if x and y are adjacent, and otherwise, where dx and dy are the degrees of x and y.
We denote by   the spectral gap, which is simply the smallest nonzero eigenvalue of L.
For any graph G and  nite subgraph S   G, the Cheeger ratio h(S) is a measure of the cut induced by S: h(S) = e(S,  S) min(vol(S), vol(  S)) .
We use e(S,  S) to denote the number of edges crossing from S to its complement, and the volume vol(S) is simply the sum of the degrees of all nodes in S. The Cheeger constant h is the minimum h(S) over all subsets S. The Cheeger constant and spectral gap are related by the following Cheeger inequality [3]: 2h       h2
 .
Both   and h are often used to characterize expansion or bottlenecks in graphs.
This inequality shows that they are 1298both good candidates and gives the ability to estimate one based on the other.
For the in nite d-regular tree, the spectral gap and Cheeger constant have both been analytically determined [7, 13].
Using L, the spectral gap is   = 1   2 d   d   1, (1) and the Cheeger constant is h = d  2 [9].
Both of these values are nonzero, indicating good expansion.
However, the Cheeger ratio for truncated d-regular trees (T dT)   those with all branches of the in nite tree cut o  beyond some radius r from the center   approaches zero as the tree gets deeper.
By cutting o  any one subtree S from the root, there is only one edge connecting S to  S, and as the tree gets deeper, this ratio gets arbitrarily small.
Using the Cheeger inequality, it follows that the  T dT   0 as r    .
Thus, the standard spectral properties of  nite trees do not approach the in nite case as they get larger; in fact, they suggest the opposite.
This is problematic when making qualitative observations about networks and their expansion, necessitating another tool for spectral analysis of networks.
The main reason why the traditional spectral gap does not capture expansion well in large,  nite trees is the existence of a boundary.
This is also problematic in network partitioning algorithms; often times the  best  partition is a bag of whiskers or combination of several smaller cuts near the boundary [12].
In this paper, we will use Dirichlet eigenvalues to eliminate this problem.
Dirichlet eigenvalues are the eigenvalues of a truncated matrix, eliminating the rows and columns that are associated with nodes on the graph boundary.
We will use a truncated normalized graph Laplacian, LD, a submatrix of L.
This is di erent from simply taking the normalized Lapla-cian of an induced subgraph, as the edges leading to the boundary nodes are still taken into account; it is only the boundary nodes themselves that are ignored.
We de ne the Dirichlet spectral gap to be the smallest eigenvalue of LD.
This version of the graph Laplacian was  rst introduced in [4] to analyze local cuts on graphs.
Using Dirichlet eigenvalues, it is also possible to obtain a local Cheeger inequality [4] for the sub-graph S. First, the local Cheeger ratio is de ned [4] for a set of nodes T   S as
 e(T,  T ) vol(T ) ; because the boundary nodes are excluded from S in the de -nition of [4], the set T cannot contain any boundary nodes of S. The local Cheeger ratio H(T ) is the appropriate quantity when S is a sub-graph of a larger graph.
Note that there is no min in the denominator; this is because the local Cheeger ratio is speci c to a subgraph, and it does not make sense to take into account the rest of the graph beyond the boundary of that subgraph.
The local Cheeger constant hS for S is then de ned as the minimum of H(T ) for all T   S \  (S).
The local Cheeger inequality obtained in [4] is hS    S   h2

 , where  S is the Dirichlet eigenvalue of the normalized Lapla-cian restricted to the rows and columns corresponding to nodes in S. This inequality indicates a relationship between local expansion and bottlenecks.
Figure 1: Dirichlet spectral gap for successively larger 3-regular trees, showing convergence to a nonzero value.
The limit, as estimated by the Cheeger inequality, is     0.057.
The use of Dirichlet eigenvalues requires that the boundary of the graph S be de ned.
If S is a tree, the leaf nodes are a natural choice.
When S is actually a  nite truncation of a larger graph, the boundary can be de ned as the set of nodes that connect directly to other nodes outside the truncation; for the Rocketfuel data [18], we will use the nodes with degree 1 which presumably connect outside of the subnetwork.
We  rst use Dirichlet eigenvalues on d-regular trees as prototypical evidence for their e ectiveness in capturing true spectral properties on real-world networks.
There is empirical evidence in Figure 1, showing that the Dirichlet spectral gap for 3-regular trees indeed converges to a nonzero value as tree depth increases, contrasting with the traditional spectral gap which converges to zero.
This is made rigorous in the following theorem: Theorem 1.
For  nite d-regular trees of depth L, the Dirichlet spectral gap converges to the true spectral gap (1) of the in nite tree as L approaches in nity.
Proof.
To derive the Dirichlet spectral gap for  nite trees using the leaves as the boundary, we will solve a recurrence that arises from the tree structure and the standard eigenvalue equation Ld(cid:4)x =  (cid:4)x.
(2) Let T be a d-regular tree of depth L + 1; the (L + 1)st level is the boundary.
We  rst consider eigenvectors (cid:4)x which have the same value at every node at the same depth within T ; these eigenvectors are azimuthally symmetric.
We can represent each such eigenvector (cid:4)x as a sequence of values (x0, x1, .
.
.
, xL), where xi is the uniform value at all nodes at depth i, similar to the analysis of the in nite-tree spectral gap appearing in [7].
Using this eigenvector form for (cid:4)x in (2) leads to the recurrence: xi   1 d xi 1   d   1 d xi+1 =  xi, 2   i   L.
(3)
 condition: xL+1 = 0, (4) and at the root of the tree we have the boundary condition x0   x1 =  x0.
We can solve (3) using the characteristic equation: d   1 d r2   (1    )r +
 d = 0, whose roots can be written as
 d   1 r1,2 = e i  with   = 1   2 d   d   1 cos  .
(5) (6) (7) Since   has to be real, either the real or imaginary part of   must be zero.
Substituting the  rst boundary condition (4) yields a solution to (3) with the form Figure 2: Comparison of traditional and Dirichlet spectral gaps in Rocketfuel data as well as the 2-dimensional Euclidean grid.
xn = A(rn L 1
   rn L 1
 ).
(8)
 for some constant A and r1,2 given in (6).
Using (5), the condition for eigenvalues is tan   tan(L + 1)  =   d   2 d 0 <   <  .
(9) Since tanh x/ tanh(L + 1)x >0 for all real x, there are no imaginary solutions to Eq.(9).
Therefore all the L + 1 solutions are real.
From Eq.
(7), the corresponding L + 1 eigenvalues are all outside the in nite-tree spectral gap.
We now consider eigenvectors which are zero at all nodes up to the k th level with L > k   0.
The eigenvector is nonzero at two daughters of some k th level node and the descendants thereof.
We assume azimuthal symmetry inside both these two sectors.
The eigenvalue condition for the parent node at the k th level forces the eigenvector to be opposite in the two sectors.
Inside each sector, (3), (6), (7) (4) and (8) are still valid.
However, (5) is replaced by the condition xk = 0, from which sin(L + 1  k)  = 0.
There are L   k real solutions to this equation, corresponding to eigenvalues that lie outside the in nite-tree spectral gap, each with degeneracy dk(d   1).
The total number of eigenvalues we have found so far is
 L 1(cid:6) k=0 (d   1)(L   k) = dk dL+1   1 d   1 (10) i.e. we have found all the eigenvalues.
As L gets larger, the smallest   approaches 0, showing that the Dirichlet spectral gap converges to the spectral gap of the in nite tree (1) as the depth approaches in nity.
This derivation shows that Dirichlet eigenvalues capture the expansion properties of trees much better than the traditional spectral gap which has been shown to approach zero for large  nite trees.
This behavior on trees suggests that Dirichlet eigenvalues are a good candidate for use in analyzing real-world networks.
Such analysis appears in Section
 Our work is motivated by derivation of scalable methods for clustering of large graphs.
As an example, we study clustering of a series of datasets representing portions of network topologies using Rocketfuel [18].
Rocketfuel datasets are publicly-available, created using traceroute and other networking tools to determine portions of network topology corresponding to individual Internet service providers.
Even though like most measured datasets, the Rocketfuel networks are not free of errors (see for example [19]), they provide valuable connectivity information at the IP-layer of service provider networks across the globe.
Because the datasets were created in this manner, they represent only subsets of the much larger Internet; it becomes impossible to determine network topology at certain points.
For example, corporate intranets, home networks, other ISP s, and network-address translation cannot be explored.
The networks used range in size from 121 to 10,152 nodes.
Because of the method of data collection, the Rocketfuel datasets contain many degree-1 nodes that appear at the edge of the topology.
In actuality, the network extends beyond this point, but the datasets are limited to one ISP at a time.
As such, for these networks, it makes sense to view these degree-1 nodes as the boundary of a  nite subset of a much larger network.
Using this boundary de nition, we compute the Dirichlet spectral gaps of these graphs and compare with their standard counterparts, as shown in Table 1 and Figure 2.
It is apparent that the Dirichlet spectral gaps are much larger than the traditional spectral gaps for all the networks, implying a much higher degree of expansion than one would traditionally obtain.
The spectral gaps for a two-dimensional square Euclidean grid are also shown; the grid is known to be a poor expander, and accordingly even the Dirichlet spectral gap is very small.
Figure 3 shows the same data, plotted as a function of the number of nodes N in each network.
We see that the traditional spectral gap keeps decreasing as N is increased, whereas the Dirichlet spectral gap does not.
Since Figure 3 compares di erent networks, possibly with di erent properties, we con rm the result by computing the
 Dataset ID Nodes Edges Traditional spectral gap Dirichlet spectral gap









 Grid











































 Figure 3: Comparison of traditional and Dirichlet spectral gaps across Rocketfuel networks.
Figure 4: Comparison of traditional and Dirichlet spectral gaps in successively larger subgraphs, grown from the center of mass of dataset 7018.
spectral gap for subgraphs of di erent sizes drawn from a single network.
All the nodes that are within a distance r of the center of mass of a network are included in a sub-graph, with r varying between 1 and the maximum possible value for the network.
In Fig. 4 shows the results for the largest of the Rocketfuel networks, dataset 7018 containing over 10,000 nodes.
For a subgraph of radius r, the boundary is de ned as all the nodes which i) have edges connecting them to nodes in the graph that are outside the subgraph or ii) connect to the outside world, i.e. that have degree 1 in the full dataset.
As in Fig. 3, in Fig. 4, the traditional spectral gap keeps decreasing as r is increased, but the Dirichlet spectral gap does not.
One important application of the eigendecomposition of a graph is spectral clustering or partitioning [15, 17].
The problem is to group the nodes into partitions, clusters, or communities that are inherently well-connected within themselves, with sparser connections between clusters.
This is closely related to  nding bottlenecks; if a graph has a bottleneck, then a good partition is often found by dividing the graph at the bottleneck.
See [16] for a general survey of graph clustering.
It is often desirable for a network partition to be balanced, and  nding bottlenecks near the core or center of mass of a network is often more useful than simply clipping small subsets of nodes near the boundary.
But according to [12], using the Cheeger ratio as a metric on real-world data, the  best  cuts larger than a certain critical size are actually  bags of whiskers  or combinations of numerous smaller cuts.
Because many graph clustering algorithms, including spectral clustering, try to optimize for this metric, the resulting partitions often slice numerous smaller cuts o  the graph, which is not always useful.
For our Rocketfuel data, we know that the boundary of the network is imposed by the method of data collection.
Thus, by eliminating the boundary from graph clustering, we can more easily  nd partitions that are more evenly balanced, and bottlenecks that are closer to the core of the network.
To do this, we use standard spectral clustering techniques from [15], but instead of using the normalized graph Lapla-cian L, we use the truncated Dirichlet version LD.
The eigenvectors used for clustering will therefore not include components for the degree-1 boundary nodes, but we can assign them to the same side of the partition as their non-boundary neighbor nodes.
Speci cally, we compute the  rst two eigenvectors of LD and cluster the nodes based on their components in these eigenvectors using k-means.
For each node, we compute the distance to both centers and sort the nodes based on the di erence.
For a partition of size k, we take the top k nodes.
We follow the experiments of Leskovec et al. in [12] by using both traditional spectral clustering and Dirichlet spectral clustering to  nd cuts of di erent sizes.
Speci cally, we  nd Dirichlet cuts of all possible sizes, and then we  nd cuts 1301using traditional spectral clustering for those same sizes af- ter adding boundary nodes back in.
Thus, for each network of N nodes, we calculate N   B cuts, where B is the number of boundary nodes.
For each cut, we measure the Cheeger ratio h and the number of components c. Ideally, a logical cut would split the network into exactly c = 2 components, but as Leskovec et al. demonstrated, as cut size increases, spectral clustering and other algorithms that optimize for h yield cuts with many components.
This is precisely the problem we are trying to avoid using Dirichlet clustering, and our results show that Dirichlet clustering is e ective in  nding cuts with fewer components.
Furthermore, even though our algorithm is not speci cally optimizing for h, it does not  nd cuts that have signi cantly worse values for h while  nding cuts with far fewer components.
We outline some aggregate data in Table 2.
For several datasets, we count the number of cuts in four di erent categories, comparing the Dirichlet Cheeger ratio and number of components (hD and cD) with traditional spectral clustering (hT and cT ).
It is evident that Dirichlet clustering  nds cuts with fewer components than traditional spectral clustering (cD   cT ) for most cut sizes, indicating that while spectral clustering optimizes for Cheeger ratio, it often  cheats  by collecting whiskers as one cut.
In addition, despite the use of Cheeger ratio optimization, Dirichlet clustering sometimes  nds cuts with better Cheeger ratio as well.
In the last two columns for each dataset, we give the di erence in h and c averaged out over all cut sizes.
It turns out that the Cheeger ratios, on average, are not drastically di erent between the two methods, and Dirichlet clustering gives cuts with far fewer components.
Along with our aggregate data, we illustrate each individual cut for several of our Rocketfuel datasets in Fig. 5.
(A few of the datasets were too large for accurate numerical computation and are therefore not shown.)
For each cut size, we plot a point corresponding to the di erence in Cheeger ratio h and the number of components c between Dirichlet and traditional spectral clustering.
It should be clear that for the majority of cut sizes, Dirichlet clustering  nds cuts with far fewer components, but there is generally little change in Cheeger ratio.
This can be seen in the large variation on the c-axis with much smaller discrepancies on the h-axis.
In other words, Dirichlet clustering avoids  nding  bags of whiskers  while still maintaining good separation in terms of h, despite not explicitly optimizing for h.
We further visualize Dirichlet spectral clustering for two Rocketfuel data sets, shown in Figures 6 and 7.
In both cases, one side of the partition is colored blue, and the other side is colored red.
Notice that for these graphs, Dirichlet spectral clustering separates red and blue nodes much better than traditional spectral clustering as expected.
It is clear that using Dirichlet eigenvalues improves the partition by ignoring the boundary, alleviating the tendency to  nd  bags of whiskers  without drastically changing the Cheeger ratio.
Although traditional spectral clustering does not always fail, there is clear evidence that Dirichlet spectral properties are an important tool in the analysis of real-world networks.
Our results show evidence that eigenvalues of the normalized graph Laplacian can provide rich information about real-world networks when Dirichlet boundary conditions are applied.
We  nd that the Dirichlet spectral gap computed for several IP-layer networks is much larger than the traditional spectral gap, and is likely to go to a  nite limit as the size of the network is increased.
Rigorous analysis for in nite d-regular trees suggests that this may be the same as the spectral gap of a communication network that is a smaller section of something much larger.
Spectral clustering using Dirichlet eigenvalues yields much better clustering than traditional methods.
The spectral decomposition using Dirichlet eigenvalues also suggests a connection to large-scale negative curvature [10, 11, 14] in the Rocketfuel data.
Traditional negatively curved graphs such as trees and hyperbolic grids generally exhibit poor connectivity and core congestion.
Standard clustering often yields combinations of smaller cuts near the periphery of the graph, but using Dirichlet clustering, we can see that there tend to be bad larger-scale cuts as well in the Rocketfuel datasets, in the graph interior.
The presence of these larger-scale cuts is a hallmark of negative curvature or hyperbolicity [8], suggesting that Dirichlet spectral clustering may yield di erent behavior for hyperbolic and  at networks.
The hyperbolic grids themselves are also suitable for further analysis, building from our study of regular trees.
Many properties such as the spectral gap remain open questions.
With some evidence of a connection between global negative curvature, the spectral gap, and expansion, it would be interesting to empirically compare the hyperbolicity  , the Cheeger constant h, and the traditional and Dirichlet spectral gaps of Rocketfuel and other real-world networks as well as well-known network models.
From this, it could be possible to classify various networks based on these properties.
This work was performed during an internship of A.T.
at Bell Labs, Murray Hill, New Jersey and was supported by AFOSR Grants Nos.
FA9550-08-1-0064 (for O.N., as a consultant with Bell Labs) and FA9550-11-1-0278.
