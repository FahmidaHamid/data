In recent years, we have been witnessing the rapid emergence of social networks and an increasing amount of user generated data.
Meanwhile, it became apparent that the relevance of search results can be improved by personalization, i.e., by taking into account additional information about the user, such as interests, demographic and psychological traits, social background, or the context of the search.
As a consequence, search engines have been evolving into social-aware platforms, Google s social layer (Google+), and Bing s social pane being perhaps the two most noteworthy examples.
While leveraging the background information about the users in ranking models has shown signi cant promise in enhancing users  search experience both in academic [5] and industrial1 studies, obtaining such features for all users can be di cult.
For instance, a recent study suggests that only about 22% of Bing users are logged into Facebook account while searching2, and even them may have not given the search engine access to their pro le information.
It would therefore be useful to be able to infer characteristics of users relevant to their search experience from information more readily available in the context of a search engine, such as the search query histories.
This paper addresses the question of how demographic traits and users  views can be inferred based on the query histories.
The main challenge, however, lies in the fact that only a very limited amount of data is available to allow training models for predicting such traits based on the search


 cess to political views and religion, and only a small amount of data related to demographic traits.
How, then, can we build a machine learning system that predicts user demographics from query histories?
What comes to the rescue is a substantial publicly available dataset called myPersonality3, o ering psychometric test results and contents of the Facebook pro les for millions of anonymous Facebook users who volunteered to donate their data for research purposes.
In particular, myPersonality database allows matching users  demographic pro les with their Facebook Likes, i.e., those online entities and Facebook Pages4 with which users have associated themselves using the Facebook Like button.
Here we show how Facebook Likes can be used to build a model predicting users  individual traits that can be later applied to search query data.
There are two issues that need to be addressed to apply the model built on Facebook Likes to query histories.
First, Facebook Likes need to be matched against queries.
We achieve that by developing a common representation for Facebook Likes and search queries within the Open Directory Project (ODP)5 categories.
Second, the distribution of users  traits di ers between Facebook and search samples.
Traditional machine learning algorithms commonly assume that the training and test samples are randomly drawn from the same distribution.
To address this issue, we design a novel learner which is able to adapt the model learned from social data to search queries with a di erent distribution.
This learner does not require unlabelled search queries to be seen at training time, which relaxes the condition of traditional transfer learning.
Experimental results show that the new learner can give high prediction accuracy as well as some interesting demographic results.
Hence, the paper makes four important contributions:   We show how to predict users  traits based on the search query logs by applying models developed on the Facebook Likes data.
  We show how to use ODP categories to match Facebook Likes with search queries.
  We demonstrate how to mitigate the problem of differing distributions of the traits.
  We provide experimental results that show the validity of the approach by comparing the predictions with ground truth data and with aggregate data at the US state level.
The structure of the paper is as follows: we begin by discussing related work in Section 2.
We then describe our ODP representation scheme, learning algorithms and approach to dataset shift mitigation in Section 3.
The datasets are described in Section 4, while details of the evaluation methodology are provided in Section 5.
Finally, we describe our experimental results in Sections 6 and 7, and conclude in Section 8.
Our work is related to a wide spectrum of previous studies ranging from inferring the demographics of individual users, to the application of user demographics in predicting global trends or individual behaviour.
The impact of demographics & personality.
Bachrach et al.
[2] investigated the correlation between users  personality and the properties of their social network pro les.
They showed that some personality traits such as Extroversion and Neuroticism can be accurately predicted based on the user s pro le.
A similar study was conducted by Quercia et al. [22] on Twitter users.
Kosinski et al. [18] demonstrated that there is a psychologically meaningful relationship between the users personality pro les obtained using a questionnaire, and their choice of websites extracted from Facebook Likes.
Weber and Castillo [24] used the Yahoo!
query logs and pro le information to compare the queries submitted by users with di erent demographics.
They further analysed the queries submitted from each US ZIP code separately and mapped them against the US-census information for those area codes.
Their results suggested that users with similar demographics are more likely to search for similar things.
Weber and Jaimes [28] examined the queries submitted from di erent ZIP codes augmented by US-census data to highlight the di erences in user behaviour and search patterns of various demographic groups.
We take this line of previous work to the next level by showing that the demographics of users can be automatically predicted based on their past queries.
Lorigo et al.
[19] discovered that male and female users have di erent search behaviour; for instance, females on average submit longer queries.
Jansen and Solomon [14] found that males and females interact di erently with sponsored search results.
Kharitonov and Serdyukov [16] demonstrated how rerank-ing the search results based on users  genders may enhance their experience in particular for ambiguous queries.
Bennett et al. [3] inferred a compact density representation of locations of users that access di erent websites and showed that those features can be used for personalizing and reranking the search results.
Inferring user demographics.
Torres and Weber [23] reported that the reading levels of clicked pages are correlated with the demographic characteristics of the clicking users.
Weber et al. [25, 26] relied on user clicks on political blogs annotated with leaning to assign a leaning score (left versus right) to queries.
Pennacchiotti and Popescu [21] used the linguistic content of user tweets, along with their other social features to predict the political orientation, ethnicity and the favourite business brands of Twitter users.
They found the user-centric features such as linguistic content to be more e ective than social graph features in their classi cation task.
Ying et al. [29] showed that the users demographics can be predicted according to their mobile usage behaviour, such as the number of text messages sent or received.
Otterbacher [20] inferred the author gender of IMDB reviews based on stylistic and content features.
demographics based on their queries but mostly focused on the privacy angle.
They leveraged bag-of-word classi ers based on queries to train their models.
Perhaps in the most similar work to ours Hu et al.
[13], predicted the users  ages and genders based on their browsing model.
For each website in their corpus they used the Microsoft Live ID information of users that accessed them to build a demographic model.
They then used these models to predict the ages and genders of other users that access the same website.
In our approach we bring the social and query data into the same space but mapping them against the ODP categories.
As a result, we have a much denser feature space that allows us to have high generalisably and cover several other interesting aspects such as religion and political views in the inference.
From query trends to global statistics.
Weber and Jaimes [27] monitored the Yahoo!
query logs to determine if the same queries were submitted by di erent demographic groups at di erent times.
Their analysis revealed that certain queries (e.g.
movies) are searched by distinct demographics at different times, suggesting an information  ow pattern between di erent groups of users.
Goel et al.
[12] used query volume to predict the opening weekend box-o ce revenue of  lms,  rst-month sales of video games and the ranks of songs on the Billboard Hot 100 chart.
In each of these cases, the authors found that there was a signi cant correlation between the query volume and future outcomes.
Ettredge et al.
[9] performed a similar study but focused on predicting the unemployment rate.
Ginsberg et al. [11] accurately detected the in uenza epidemics by only using the frequency and volume of certain queries in Google logs.
Later on, Kong et al.
[17] utilized click-through for the same purpose, and Culotta [6] repeated a similar analysis on Twitter data.
Domain adaptation and transfer learning.
Our work is also related to domain adaptation and transfer learning techniques.
In domain adaptation [8] typically the same feature space is shared by the source and target domains.
We also deal with two distinct source (social data) and target (queries) spaces in our experiments, and bridge them by mapping them to a single common space (based on ODP categories).
Transfer learning techniques can be used to resolve the problem of the di erent distributions between source and target spaces.
It is worth noting that in contrast to typical transfer learning models [30, 10, 7, 1], our approach requires neither any data sharing between the source and target domains, nor any target data to be seen at training time.
As mentioned in the Introduction, we are addressing the problem of inferring users  traits from search queries based on the models trained on an independent set of Facebook Likes and pro les.
We thus face two challenges   How can we  nd a common representation for search queries and Facebook Likes?
  How can we address the problem that the users  traits are distributed di erently in those two datasets?
We address the  rst problem by mapping both search queries and Facebook Likes into a common representation given by the Open Directory categories, which form a mini-ontology of entities on the Web and can be thought of as a coarse grained representation of both search queries and Facebook Likes.
Figure 1 illustrates the common representation based on the DMOZ Open Directory Project (ODP) categories.
For Facebook Likes we turn the title of each liked entity into a query and submit it to a search engine (for example for the lady gaga Facebook Like, we submit the query lady gaga).
We classify each of the top ten results returned by the search engine (Bing was used in this study) into one of the top two-levels of the DMOZ/ODP categories, assigning a maximum of three categories to each result.
In total, there are 219 topical categories such as Arts/Movies, Business/Jobs and Computers/Internet.
For learning the category classi ers we follow the approach described by [4] and apply logistic regression with L2 regularization on a 2008 crawl of the documents linked with the ODP index.
Using the output of these classi ers we then represent each Facebook Like in the myPersonality dataset by a 219-dimensional vector.
Each element of this vector denotes the number of times that a particular ODP category has been assigned to the search results returned for that Like.
We then repeat the same process on search (Bing) users.
To generate the topical feature vector for each user, we collect the queries from their search history and classify them in the same way as the we did for the Facebook Likes.
Each user is represented again by a 219-dimensional vector, in which each element denotes the number of times the corresponding ODP category has been assigned to top-ranked documents returned for user queries.
The feature values are normalized into probabilities so that they all sum to one for each user.
The second problem arises because of the di ering users  traits distribution between users in the Facebook and search queries samples.
Traditional machine learning algorithms commonly assume that the training data consist of samples randomly drawn from the same distribution as the test samples about which the learned model is expected to make predictions.
This assumption is violated in our scenario where the model trained on Facebook data is applied to a query log to predict users  demographic characteristics in the search engine.
One of the examples is that there are relatively more female user in the Facebook (myPersonality) dataset, compared to search (Bing) users.
Naively training on one dataset and testing on the other can signi cantly decrease the predictive accuracy of a traditional learning algorithm.
This is because a learning algorithm aims to learn an optimal model for the query log by minimizing the expected risk:   = arg min   (q,y) Dq P (Dq)(cid:96)(q, y,  ) (1) where Dq is query log data, q is a query, y is an ODP category, and (cid:96)(q, y,  ) is a loss function with parameter  .
However, since we have to assume that no labelled data is available from the query log, we have to learn a model from the Facebook data instead by minimizing the empirical risk: (cid:88) (cid:88) (l,y) Df   = arg min   P (Df )(cid:96)(l, y,  ) (2)
 the left, the Facebook Likes of a small group of users are mapped to their corresponding ODP categories by issuing them as queries and classifying the top search results.
On the right, the search users are represented similarly by the set of ODP categories associated with the top-ranked results returned for their queries.
Note that here we have likes (l) instead of queries (q).
If P (Dq) = P (Df ), the two optimization problems are approximately equivalent.
However, as we can observe from the comparison of the Facebook and search data (see Table
 To predict demographic characteristics of the users in a query log, we essentially seek to obtain the conditional probability distribution P (Y |Q, Dq), where Y denotes the demographic characteristic of a user who issued queries Q, and Dq denotes the query log.
Note that P (Y |Q, Dq) (cid:54)= P (Y |L, Df ) as discussed earlier.
Since we choose to represent each user by a probability distribution over ODP categories, P (Y |Q, Dq) can be marginalized across ODP categories C: (cid:88) P (Y |Q, Dq) = P (Y |C, Dq)P (C|Q, Dq) (3) By Bayes  rule, P (Y |C, Dq) is given by:
 P (Y |C, Dq) = P (Y |Dq)P (C|Y, Dq) P (C|Dq) , (4) where P (Y |Dq) is the probability of class Y in the query log, which captures our prior knowledge about the relative frequencies of users of di erent demographics in a search engine.
These quantities can be obtained from the search engine internal statistics, or publicly available statistics about the search users.
On the other hand, P (C|Dq) captures the relative frequencies of queries of category C. This quantity could be estimated from search logs, but can also be approximated from the ODP/DMOZ statistics assuming that the ODP corpus is statistically similar to the set of results returned by the search engine.
P (C|Y, Dq) is the probability that a user with demographics Y is interested in category C when issuing a query.
The key insight here is that we can assume that whether a user is interested in some category C or not depends on their demographics Y , independent of whether he or she is using Facebook or doing search.
Therefore, it is reasonable to make the conditional independence assumption that P (C|Y, Dq) = P (C|Y ) = P (C|Y, Df ) , (5) which means that P (C|Y, Dq) can be estimated from Facebook data Df .
Let  Y denote the probability distribution P (C|Y ).
In order to avoid problems of estimation due to sparsity of the data we estimate the parameter vector  Y using Bayesian Maximum A Posteriori (MAP) estimation.
In particular, we estimate  Y by:  Y = arg max  Y P ( Y |Df ) = arg max  Y P (Df| Y )P ( Y ) .
(6) rameterized by pseudo-counts { k}, (  =(cid:80) This is a standard Bayesian estimation problem with a multi-nomial likelihood and a conjugate Dirichlet prior P ( Y ) pa-k  k).
If there is prior knowledge available, this can be taken into account, otherwise one can initialize the pseudo counts { k} uniformly.
The resulting MAP solution is given by:  Y k = k +  k   1
 N Y +     K , (7) where N Y k is the number of times the webpages, which are returned for the Likes of users of class Y , fall into the kth category, K is the total number of ODP categories, and N Y is the total number of categories for webpages returned for the Likes of users of class Y .
Note that we estimate the probability P (C|Q, Dq) in Equation (3) in a similar way.
In summary, the methodology outlined above allows us to train a demographics classi er on users characterized by their collection of Facebook Likes, yet evaluate it on users characterized by their search query history.
We believe that the two key ideas of a) creating a common representation in terms of ODP, and b) of mitigating the data shift problem by breaking up the problem into separate estimation tasks for demographics given category and category given query history will be more generally applicable to problems in which labels are available, but are not directly linked with the representation of interest through suitable training data.
Dataset Teenage Youngster Young Mid-Age Elder Male Female (10-18) (35-49) (50+) (19-24) (25-34) Social Dataset Search Dataset















 myPersonality Dataset (Facebook).
The myPersonality dataset was collected through the myPersonality Facebook application, which allowed its users to take real psychometric tests and receive feedback on their scores.
In addition to the results of the tests, respondents could opt in to record their Facebook pro le data to be used for the research purposes.
The dataset contains detailed psycho-demographic pro les of more than 6 million unique users from diverse age groups, backgrounds, and cultures.
Respondents were motivated to answer honestly, as the only grati cation they received for their participation was feedback on their results.
We used a subset of myPersonality users from US described by their age, gender, political views, religion, and lists of their Facebook Likes.
We  lter out all Facebook Likes associated with less than ten users.
The resulting dataset contains over 457,000 users, 122,000 unique Likes, and over 11 million associations between the users and Facebook Likes.
Users  religion and political views were stored as free text.
Although the great majority of users simply have the typical religion/party/philosophy names in those  elds (e.g.
Christian, Liberal), sometimes we had to use regular expression matching to extract the relevant information.
For instance,  Christian - Baptist  was recoded as  Christian  and  I dont go to church because i wanna leave room in the pews for the sinners that need it -mr. magee  was ignored after mismatching all of our regular expressions.
Bing Query Logs (Search).
We apply the models trained on the myPersonality dataset to infer the traits of users characterized by search queries.
Search query logs were obtained from Bing and were collected between October 14, 2012 and October 28, 2012.
We have selected queries submitted by the US users that were signed in with their Microsoft Live account while issuing their queries.
In total, we have collected
 was also described by age and gender as reported in their Microsoft Live pro les.6 Differing distributions (Data Shift).
Table 1 shows that the distributions of user demographics signi cantly di er between myPersonality and search query logs datasets.
For instance, on average there are more young and female users in our Facebook data, which considering the nature of myPer-sonality test may not be surprising.7
 IDs were all annoymized such that the actual usernames could not be identi ed.
for our search and social datasets necessarily cannot be regarded as representative statistics for Bing and Facebook.
The distributions in the datasets, particularly for the myPer-sonality data, are signi cantly a ected by how the data is

 For each user trait used here, we  rst train a model on
 on the remaining 34%.
We then apply the same model on search queries and repeat the classi cation for search users.
Evaluation on myPersoanality Sample.
In the binary clas-si cation tasks such as predicting gender or political view (liberal vs. conservative) we use the area under the ROC curve (AUC) measure for evaluating accuracy.
The ROC curves are created by plotting the ratio of true positive rate versus false positive rate at various threshold settings.
We turn each of the multiclass classi cation tasks such as predicting religion (among Christian, Buddhist, Jew, Agnostic) into multiple binary classi cation problems (e.g.
Buddhist or not Buddhist) and report the average values at the end.
Evaluation on Bing sample.
The age and gender information of the Bing users was obtained from their Microsoft Live pro les.
Hence, we can repeat the same type of AUC evaluation, but this time with the labels coming from Microsoft Live accounts.
Religion and political views are not available in the Microsoft Live pro les, hence we do no have the ground-truth information on the individual user level.
Therefore, we evaluate the accuracy of the trained classi ers on how well their output matches the o cially reported state-level statistics.
We  rst classify the religion and political views of individual users (e.g.
religion = Christian) and aggregate those results on the state level (e.g.
74% Christians in California) by using users  location acquired from the IP address.
We then look up the corresponding reported values for each state from publicly available o cial statistics (e.g., what percentage of Californians are Christian).
Next, for each given class (e.g., Christianity) and each state, we calculate the percentage of search users that are classi ed in that category with respect to (1) our predictions and (2) o cial statistics.
Finally, we compute the Pearson correlation value ( ) between (1) and (2) and consider it as a proxy for the accuracy of the prediction.
Using the compact ODP representation described earlier, we managed to model all users in both Facebook and search queries datasets.
In comparison, an exact-match approach that compares the text of queries and Likes  nds only 5.3% overlap by which only 36% of search users can be modelled and even for those there are often only few nonzero features.
Table 2 displays the evaluation results of the classi ers built on Facebook sample for inferring di erent demograph-collected.
The unique characteristics of the myPersonality test is likely to attract certain types of audience more than others.
Readers are encouraged to refer to other sources (such as alexa.com) for more representative statistics.
Religious Landscape Survey.
(Top-Right) The distribution of Christians in the US as predicted based on user queries.
The Pearson correlation ( ) is 0.39.
(Bottom-Left) The distribution of Buddhism in the Contiguous United States according to the U.S.
Religious Landscape Survey.
(Bottom-Right) The distribution of Buddhism in the US as predicted based on user queries.
The Pearson correlation ( ) is 0.53.
The spectrum bar at the left corner of each map speci es the scale and the corresponding color codes.
Table 2: The Area under the ROC Curve (AUC) for di erent demographic prediction models.
The numbers in the middle column show the AUC of a model trained on Facebook data for predicting the demographics of Facebook users.
In the right column, the models trained based on Facebook data are tested on search query sample.
The missing values  -  are used where the per-user ground-truth information is not available for AUC evaluation.
AUC Facebook-Facebook Facebook-Search Gender Age Religion Political view





 --ics.
The middle column (Facebook-Facebook) shows the AUC values when we trained and tested on the Facebook dataset.
The right column (Facebook-Search) shows the Facebook model AUC on classifying Search users.
For gender classi cation, we train two separate classi ers; one for male, and one for female, each computing the probability of given gender based on the user pro le (ODP features).
Each user is compared against both of these classi- ers, and the one producing the highest probability is used to set the class of gender.
The results in Table 2 show that the classi cation reaches 83% and 80% AUC respectively when tested on Facebook and Search samples.
Not surprisingly, the AUC of a model trained based on the Facebook sample, is higher when it is tested on other users from the same dataset.
However, the relative loss is not substantial, particularly considering the signi cant di erences in the demographic distributions of these two sets (37% Male in Facebook dataset, compared to 53% among Search users).
For age classi cation, we grouped the users in each dataset into  ve separate age groups as listed in Table 1.
For each age group we compute a model based on the training subset of users in our Facebook dataset.
At testing, each user   in Facebook and Search datasets   is compared against these models, and the one producing the highest probability is used for classi cation.
On the testing subset of Facebook users, the trained classi er achieves 77% AUC, while this number is slightly lower (73.5%) when applying the model on the Search sample.
To classify users  religion, we  rst apply a set of regular expressions as described in Section 4 to assign the social users into four groups: {Christian, Jewish, Buddhist, and Agnos-
Religious Landscape Survey.
(Top-Right) The distribution of Agnostics in the US as predicted based on user queries.
The Pearson correlation ( ) is 0.27.
(Bottom-Left) The distribution of Judaism in the Contiguous United States according to the U.S.
Religious Landscape Survey.
(Bottom-Right) The distribution of Judaism in the US as predicted based on user queries.
The Pearson correlation ( ) is 0.54.
The spectrum bar at the left corner of each map speci es the scale and the corresponding color codes.
tic/Una liated }.
These are also the four major religions in the United States according to U.S.
Religious Landscape Survey,8 accounting respectivelly for of 78.4%, 1.7%, 0.7%, and 16.1% of the entire US population.
We use these nationwide statistics as the prior when classifying the users in the Search dataset.
The AUC while classifying users religion in the testing subset of Facebook dataset is 76%.
Importantly, as there is no information about the religion on the Search user level, the accuracy of the classi cation was evaluated in terms of how well it predicted the state-level distributions as described in Section 4.
Figure 2 depicts the state distributions of Christians (top) and Buddhists (bottom) according to the U.S.
Religious Landscape Survey on the left, and according to our predictions for the search engine users on the right.9 The models predict that depending on the state, 65.8%-95.3% of search users are Christians.
These values are comparable to 69.1%-
identify the Mississippi state as the one with the highest ratio of Christians, and the states on the east coast with 8http://religions.pewforum.org
 scape Survey and hence are dropped from the analysis.
the lowest density (  = 0.39).
Similarly the models predict
 pending on the state, which is not far from the 0.5%-2.1% range reported in the Landscape Survey.
The models predict Vermont, Oregon, California, and New Mexico to have the largest population of Buddhists, and apart from the former   that accounts for 0.001% of our dataset and hence is somewhat prone to noise   the remaining three are also listed as the top three Buddhist states in the Landscape Survey (Overall,   = 0.53).
Figure 3 demonstrates the spread of Agnostic (top) and Jewish (Bottom) people in the United States.
The models predict 4.1%%-27.6% of the search users in our dataset to be agnostic or una liated with any particular religion.
The o cial numbers from the Landscape Survey for this category lie closely between 6.1% and 28.3%.
Consistent with the Landscape Survey, our models predict higher density of agnostics in North East and West, with the state of New Hampshire appearing on top of both   survey and predicted   lists (  = 0.27).
According to our predictions based on search engine users, Jews account for 0.3%-5.0% of the US population depending on the state.
These numbers are fairly consistent with the 0.5%-6.5% reported on the Landscape Survey (  = 0.54).
We also correctly identify the states in
 est density of Jewish people.
This is yet again aligned with the Landscape Survey and historical documents about the Jewish settlements in the United States.10 We matched a set of regular expressions against the Political view  eld of users in the Facebook dataset to group them into liberal (34%) and conservative (66%) categories.
We ignored users that did not match any of the regular expressions in building our models.
The distribution of liberal versus conservative in the social dataset is remarkably close to those reported by independent sources such as Gallup survey which reported 20.6% liberals versus 40% conservatives nationwide   the remainder of people in the poll were assigned to moderate and other groups.11 As in previous experiments, we build the classi ers based on the ODP features of the users in the training subset (64%) of Facebook dataset.
Applying the model on the remaining (34%) of users in that dataset produces the AUC of 0.74.
We then apply the same model on the Search sample; the middle and bottom maps in Figure 6 illustrate the distribution of liberals and conservatives in the US.
The middle map is generated based on the per-state statistics reported by the Gallup survey.
The bottom map is generated by applying the classi er trained on the Facebook sample to Search users.
The predicted class for each individual user contributes to generate the overall distribution for each of the states.
To enhance the visualization, the plots were produced with respect to the nationwide average so that the di er-ences between states become more prominent.
For instance, -0.10 would mean 10% more liberal, while 0.05 would suggest 5% more conservative than the nationwide average.
The middle and bottom maps in Figure 6 reveal very similar distributions ( = 0.72).
As expected, both maps look more blue on the East-West coasts, and more red in the so-called Bible Belt states.
Oregon with an o cially reported 13.8% swing towards liberals is the most noticeable mispredicted state; this was a ected to some extend by the ambiguity of the queries related to the civil war, a college football rivalry in Oregon, which was particularly trendy during our sampling period.
It is commonly known that liberals are more likely to vote for the Democratic Party and conservatives are more likely to vote for Republicans.12 Thus, perhaps it is not entirely surprising to  nd similarities in how the states were split between Democrats and Republicans in the recent 2012 US presidential election (top map in Figure 6).
In this section we show the importance of each category in predicting a given type according to its information gain computed in a leave-one-out fashion.
That is, for each ODP category C (e.g.
Arts/Movies), and a given demographic type Y (e.g.
Gender), we  rst calculate the prior values according to all other 219 categories in our data, and then calculate the change in information entropy when C is considered as,
 (8)


 Report, http://bit.ly/L85SmV Figure 4: (Top) The outcome of 2012 the US presidential election according to The Hu ngton Post.
The blue states were won by Democrats and the red states by Republicans.
(Middle) The distribution of conservatives versus liberals according to an independent poll   Gallup.a (Bottom) Liberal vs. conservative predictions on Bing users based on the models learned according to Facebook data.
The Pearson correlation ( ) between the Gallup data and our per-state predictions is 0.72.
The spectrum bar at the left corner of each map spec-i es the scale and the corresponding color codes.
aSource: Gallup, http://bit.ly/zC0PHk
 Gender Sports/Basketball Age Arts/Movies Religion Political view Religion and Spirituality/Christianity Politics/Liberalism Games Computers/Data Communications Religion and Spirituality/Religious Studies Politics/Conservatism Games Religion and Spirituality/Scientology Society/History News/Media Society/History Arts/Movies Science/Social Sciences Sports/Soccer Shopping/Gifts Shopping/Toys and Games Shopping/Jewellery Computers/Software Here, H(Y ) represents the prior entropy for the demographic type Y across all users, and H(Y |C) is the same value conditioned on observing category C in the user s pro le.
Table 3 shows the categories with the highest information gains for classifying each of the demographics.
For classifying gender, sport and shopping related categories are most e ec-tive.
Art/Movies, Games, Shopping/Toys and Games and computer-related categories are best in discriminating between di erent age groups.
For religion, subcategories of Religion and Spirtuality are the most important features, and for politics   not surprisingly   Politics/Liberalism and Politics/Conservatism have the highest information gain values for distinguishing between liberals and conservatives.
We also calculate the in uence ( ) of each category c   C (e.g.
Arts/Movies) in classifying a given demographic type to a particular class y   Y (e.g.
Gender = Male) by, P (C = c|Y = y)       = (9) where   represents the average probability of class c   C, for all values of y   Y .
That is,   (cid:80) y Y P (c|y)
   = (10) from their search queries.
We trained our predictive models on a sample of Facebook users that had agreed to provide their Likes and other pro le information for research purposes.
To the best of our knowledge, this is the  rst study that infers the demographics of search users based on the models trained on the independent social datasets.
We demonstrated that both Facebook Likes and search queries can be translated into a common representation via mapping to ODP categories.
In addition, we addressed the data-shift problem by breaking up the problem into separate estimation tasks for demographics given category, and category given query history.
Our experimental results on a large scale query log of a commercial search engine con rms that the demographics of search users can be accurately predicted based on models trained on an independent social data.
The trained classi- ers achieved 80% and 74% AUC respectively for classifying gender and age.
For various religious and political views the models consistently ranked the US states close to their rankings reported in the o cial statistics (Pearson   > 0.72 in all our experiments).
For future work, we are interested in expanding the models to capture other types of user traits, such as personality, intelligence, happiness, or interests and measuring the applications of those inferred traits in personalization, reranking and monetization of the search results.
Disclaimer.
The presented work on inferring the demographics of search users has been only done as a scienti c study and has not been implemented at Bing.
