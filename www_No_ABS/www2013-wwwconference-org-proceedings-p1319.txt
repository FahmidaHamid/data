How long will it take until a next comment arrive on your Youtube1 video given the past history of comments timestamps?
Does the be-1www.youtube.com Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
havior of commenting on Youtube videos di er from the behavior of commenting on web blogs or online news websites?
And how di erent these activities are from writing and receiving emails?
The current availability of large datasets containing digitalized information about human communication dynamics has made it possible to propose a question that many thought was already answered: what is the timing of human communications[2, 20]?
Thus, the focus of this work is to  nd patterns in inter-event times between real and modern communication activities of humans.
All the aforementioned communication activities are  point processes , and the simplest way to model them is by the Poisson Process (PP) [17].
Unfortunately, this simple and elegant model has proved unsuitable [34, 12, 15, 40], since analysis of real data have shown that humans have very long periods of inactivity and bursts of intense activity [2, 20], in contrast to the PP, where activities occur at a fairly constant rate.
Although researches agree that the PP is not suitable, there is no consensus about the right model between two major schools of thought.
The  rst viewpoint [2, 34] states that a power law [13] is an appropriate  t for the Probability Density Function (PDF) of the inter-event time distribution (IED), where bursts and heavy-tails in human activities are a consequence of a decision-based queuing process, when tasks are executed according to some perceived priority.
The second viewpoint is that the IED is well explained by variations of the PP [25, 32, 31, 30,
 events exhibits a Poissonian behavior [5, 21] and suggest a piece-wise Poisson process: the  rst interval has a constant rate  ; for the next, change the rate, and continue.
Given these two approaches, we ask: which is the correct one?
Can we reconcile them all?
We show here that, surprisingly, both approaches are correct, being corner cases of the proposed Self-Feeding Process (SFP).
The SFP generates a power-law-tail distribution for the inter-event time marginal, like [2], and it behaves as a PP in the short term, like [32].
Moreover, the SFP is also extremely parsimonious, requiring at most two parameters.
Additionally, unlike previous studies, we analyze the temporal correlations between inter-event times, illustrating the  i.i.d.
fallacy  that has been routinely ignored until recently [22].
We show that, unlike the PP that generates independent and identically distributed (i.i.d.)
inter-event times, individual sequences of communications tend to show a high dependence between consecutive inter-arrival times.
This is the basis of the SFP model, which uses a Markovian approach to determine that the next inter-event time
 eight diverse and large datasets from real and modern communication data, that can be divided into two groups.
The  rst group contains  ve datasets extracted from Web applications in which several users comment on a given topic.
The second group contains three datasets in which individuals perform and receive communication events.
In summary, the main contributions of the SFP are as follows:   Unifying power.
It reconciles existing and contrasting theories in human communication dynamics[2, 32];   Temporal correlation.
It shows positive correlation between consecutive inter-event times [22];   Parsimony.
It requires usually one and at most two parameters.
Moreover, we would like to point out that our  ndings open a new perspective in understanding human communication dynamics both at the network ( rst group) and individual (second group) level.
By knowing the typical human behavior, one can leverage a varied number of applications in di erent areas, such as popularity prediction of videos and news, identi cation of spammers and other anomalous behavior, resource allocation, among others.
The rest of the paper is organized as follows.
In Section 2, we provide a brief survey of the related work that analyzed inter-event times between communications.
In Section 3, we describe the eight datasets used in this work.
In Section 4, we analyze the IED of individuals from these datasets and we show that the Odds Ratio function of their IEDs is well modeled by a power law.
Later, in Section 5, we show that the typical behavior of inter-event sequences shows a positive correlation between consecutive inter-event times.
In Section 6, we describe our proposed SFP model that provides an intuitive and simple explanation for the observed data.
Then, in Section 7 we show that the SFP model also uni es existing theories on communication dynamics.
Finally, we show the conclusions and future research directions in Section 8.
The accurate understanding of the human dynamics on the Web can bene t a large number of applications, such as query suggestions, crawling policies, advertising, result ranking, recommender systems, anomaly detection, among others.
However, the dynamics of humans on the Web is very rich and varied, given the large number of activities one can perform online.
For example, in [36] the authors developed methods for modeling the dynamics of the query and click behaviors seen in a large population of Web searchers.
In [37], the authors analyzed the tendency a person has to comment on stories in the Web in order to connect users with stories they are likely to comment on.
Moreover, [19] analyzed and modeled the temporal behavior of users in social rating networks, what may leverage the prediction of future links, ratings or community structures.
Finally, in [27], the authors used stochastic models of user behavior on online news websites to predict the popularity of a given news based on early user reactions to new content.
Our work tackle a more general human behavior, i.e., her/his communications activities, which may occur on news websites, online social networks, video channels, directly via email and in many other ways.
The study of the time interval in which events occur in human activity is not new in the literature.
The most primitive model is the classic Poisson process [17].
Although the most recent approaches have among themselves signi cant di erences, they all agree that the timing of individuals systematically deviates from this classical approach.
The Poisson process predicts that the time interval  t between two consecutive events by the same individual follows an exponential distribution with expected value   and rate   = 1/ , where  t =     ln(U(0, 1)), (1) where U(0, 1) is a uniformly random distributed number between [0, 1].
While in a Poisson process consecutive events follow each other at a relatively regular time, real data shows that humans have very long periods of inactivity and also bursts of intense activity [2].
Recently, Barab si et.
al. [2, 34] proposed that a power law [13] is an appropriate  t for the Probability Density Function (PDF) of the inter-event time distribution (IED).
They propose that bursts and heavy-tails in human activities are a consequence of a decision-based queuing process, when tasks are executed according to some perceived priority.
In this way, most of the tasks would be executed rapidly while some of them may take a very long time.
The queuing models generate power law distributions p(X = x)   x  with slopes     1 or     1.5.
The second modern approach claims that the IED is well explained by variations of the PP, such as the Interrupted Poisson [25] (IPP), Non-Homogeneous Poisson Process [32, 31, 30] and Klein-berg s burst model [23].
All these studies are based on the fact that short-term communication events exhibits a Poissonian behavior [5, 21] and suggest a piece-wise Poisson process: the  rst interval has a constant rate  ; for the next, change the rate (say, to zero, for the IPP, or to double-or-half for Kleinberg s model), and continue.
Malmgreen et al. [32, 31, 30] proposes a non-homogeneous Poisson process, where the rate  (t) varies with time, in a periodic fashion (e.g., people answer emails in the morning; then go to lunch; then answer more emails, etc).
This model explains the data at the cost of requiring several parameters and careful data analysis, being impractical for synthetic data generators, for instance.
Later, the authors adapted this model to a more parsimonious version [30], but it still has 9 parameters.
In this work we analyze eight datasets that can be divided into two groups.
The  rst group contains  ve datasets extracted from Web applications in which several users comment on a given topic.
The datasets are extracted from  ve popular websites: Youtube, MetaFilter, MetaTalk, Ask MetaFilter and Digg.
The second group contains three datasets in which individuals perform and receive communication events.
In this group we have a Short Message Service (SMS), a mobile phone-call and a public email dataset.
For simplicity, we use the term  individual  to refer both to topics of the  rst group and users of the second group.
In the  rst group, we analyze a public online news dataset, containing a set of stories and comments over each story.
More specifically, the data is from the popular social media site Digg and has 1,485 stories and over 7 million comments [11].
The Digg dataset is public for research interests and can be downloaded at http://www.infochimps.com/datasets/diggcom-data-set.
We also analyze three publicly available datasets from the Meta lter Info-dump Project2, extracted from three discussion forums: MetaFil-ter3 (Me ), MetaTalk4 (Meta) and Ask MetaFilter 5 (Askme).
After September on 2downloaded http://stu .meta lter.com/infodump/ 3http://www.meta lter.com/ 4http://metatalk.meta lter.com/ 5http://ask.meta lter.com/ 22nd from
 dataset has 8,384 topics and 1,471,153 comments, the Meta dataset has 2,484 topics and 503,644 comments and the Askme dataset has 498 topics and 65,950 comments.
Our  nal dataset from the  rst group was collected from the Youtube website using the Google s Youtube API6.
We collected all the comments posted on the videos classi ed as trending by the API7 from 22/Aug/2012 to 25/Sep/2012.
We collected a total of 1,221,390 comments on 989 videos, but we use in our dataset only those videos with more than 30 comments and which the comments span for more than one week, a total of 610 videos and 1,008,511 comments.
The full dataset can be downloaded at www.dcc.ufmg.br/ olmo/youtube.zip.
In the second group, the mobile phone calls dataset contains more than 3.1 million customers of a large mobile operator of a large city, with more than 263.6 million phone call records registered during one month.
From this same operator, we also have a SMS dataset of 300,000 users spanning six months of data, for a total of 8,784,101 records.
These datasets from the mobile operator is under Non-Disclosure Agreement (NDA) and belong to the iLab Research at the Heinz College at CMU, but was already used in several papers [38, 39, 1].
We also analyze the public Enron email dataset, consisting of 200,399 messages belonging to 158 users with an average of 757 messages per user [24].
The data is public and can be downloaded at http://www.cs.cmu.edu/ enron/.
In this work, we are  rst interested on the inter-event time distribution IED of the random variable  k representing the time  k between the k   th and the (k   1)   th communication events on a given topic ( rst group) or of an user (second group).
For simplicity, we use the term  individual  to refer both to topics of the  rst group and users of the second group.
bution Function In Figure 1, we show the distribution of the time intervals  k between communication events for a typical active user of the SMS dataset, with 44,785 SMS messages sent or received.
The histogram is shown in Figure 1-a and, as we observe, this user had a signi cantly high number of events separated by small periods of time and also long periods of inactivity.
Moreover, both the power law  tting, which in the best  t has an exponent of   2, and the exponential  tting, which is generated by a PP, deviates from the real data.
The method we use to  t the power law is based on the Maximum likelihood estimation (MLE) described in [7].
In empirical data that spans for several orders of magnitude, which is the case of the IEDs, it is very di cult to identify statistical patterns in the histograms, since the distribution is considerably noisy at its tail [2, 32].
A possible option is to move away from the histogram and analyze the cumulative distributions, i.e., cumulative density function (CDF) and complementary cumulative density function (CCDF), which veil the data sparsity.
However, by using the CDF, as we observe in Figure 1-b,we lose information in the tail of the distribution and, on the other hand, by using the CCDF, as we observe in Figure 1-c, we lose information in the head of the distribution.
In order to escape from these drawbacks, we propose the use of the Odds Ratio (OR) function combined with the CDF as it allows for a clean visualization of the distribution behavior both in the 6https://developers.google.com/youtube/ 7https://gdata.youtube.com/feeds/api/standardfeeds/on_the_web head and in the tail.
This OR(k) function is commonly used in the survival analysis [3, 29] and measures the ratio between the number of individuals who have not survived by time t and the ones that have survived.
Its formula is given by: Odds Ratio(t) = OR(t) = CDF(t)
 .
(2) In this paper, for a set of n inter-event times { 1,  2, ...,  n}, we calculate the odds ratio for each percentile P1, P2, ..., P100 of the data.
This avoids that minor deviations in the data harms the goodness of  t test we perform, which we explain in Section 4.2.
Thus, in Figure 1-d, we plot the OR for the selected user.
From the OR plot, we can clearly see the cumulative behavior in the head and in the tail of the distribution.
Also, observe again that both the exponential and the power law signi cantly deviate from the real data.
Moreover, we can also observe that the OR of the inter-event times seems to entirely follow a linear behavior in logarithmic scales, having, then, a power law behavior with OR slope     1.
Again, in Figure 2, we plot the OR of a typical individual of each dataset.
The OR plots clearly show the cumulative behavior in the head and in the tail of the distribution.
Also, we can observe that the OR of the inter-event times seems to follow entirely the same linear behavior in logarithmic scales, having, then, an OR power law behavior.
This implies that the marginal distribution of the IEDs is approximately equal to a log-logistic distribution [14], since this distribution shows a OR power law behavior.
In this section, we check whether the OR of the IEDs of all individuals of our datasets can be explained by a power law.
We perform a linear regression using least squares  tting on the OR of the IEDs of all individuals.
Since we consider every percentile and the OR is a cumulative distribution, the linear regression is accurate.
We performed a Kolmogorov-Smirnov goodness of  t test, but because of digitalization errors and other deviations, this test presented biased results.
For instance, it rejects all  ttings on distributions where the data is rounded up from seconds to minute values (e.g.
45 seconds to 60 seconds).
Figure 3 shows the histogram of the determination coe cient R2 of the performed linear regressions.
The determination coef- cient R2 is a statistical measure of how well the regression line approximates to the real data points.
A R2 = 1.0 indicates that the regression line perfectly  ts the data.
We observe that for the vast majority of individuals of our eight datasets, the R2 is close to 1.0.
More speci cally, for the  rst group, the R2 averages 0.97 for the Youtube, Askme and Digg datasets and 0.98 for the Me  and Meta datasets.
For the second group, the R2 averages 0.99 for the phone dataset, 0.96 for the SMS dataset and 0.97 for the email dataset.
This allows us to state that for the vast majority of individuals, the OR of their IEDs is well  tted by a power law.
Since the IED of the majority of individuals is well modeled by an odds ratio power law, then we are able to characterize their behaviors by two values: the slope   and the median   of the  tted OR power law.
Observe in Figure 4 the PDF of the slopes  i measured for every individual i of our eight datasets.
Except the SMS dataset, the typical  i for the majority of individuals is approximately 1.
Moreover, observe in Figure 5 the PDF of the medians  i measured for every individual i of our eight datasets.
Observe that, while the typical  i is around 1 hour for the second group, for the  rst group it varies from 3 to 8 minutes.
(b) CDF (c) CCDF slope    

 o i t a
 s d d
 data power law ex ponential inter-event time   (s) (d) Odds Ratio Figure 1: The inter-event times distribution of the most active individual of our eight datasets, with 44,785 SMS messages sent and received.
We observe that both the power law  tting (PL  tting) with exponent 2 and the exponential  tting, generated by a PP, deviate from the real data.
We also observe that the OR is very well  tted by a straight line with slope   1.
gZ

  =  =
  =
  =
 o i t a
 s d d


 n= o i t a
 s d d


 n= o i t a
 s d d


 n=

 n= inter-event time   (s) inter-event time   (s) inter-event time   (s) inter-event time   (s) (a) Youtube (b) MetaFilter (c) MetaTalk (d) Ask MetaFilter  =
 data  =
 data  =
 s beck  =
 o i t a
 s d d


 n= o i t a
 s d d


 n= o i t a
 s d d


 n=

 n= o i t a
 s d d
 o i t a
 s d d
 inter-event time   (s) inter-event time   (s) inter-event time   (s) inter-event time   (s) (e) Digg (f) SMS (g) Phone (h) Email Figure 2: The Odds Ratio plot for one typical active individual of each dataset.
Observe that an odds ratio power law, represented by a straight line with slope   in a log-log scale, is an appropriate  t for all individuals.
Although most previous analysis focus solely on the marginal IED, a subtle point is the correlation between successive inter-event times ( k 1 and  k).
What we illustrate here is that the independence between  k and  k 1 does not hold for the eight datasets we analyzed in this work.
In Figure 6, we plot, for the same typical users of Figure 2, all the pairs of consecutive inter-event times ( k 1,  k).
We also show the regression of the data points using the LOWESS smoother [8].
While the PP, as for any other i.i.d.
process, the regression is a  at line with slope 0, for the eight typical users  k tends to grow with  k 1.
This means that if I called you  ve years ago, my next phone call will be in about  ve years later.
In short, there is a strong, positive dependency between the current inter-event time ( k) and the previous one ( k 1), clearly contradicting the independence assumption.
We formally investigate if two consecutive inter-event times are correlated analyzing the autocorrelation [4] of all the time series involving the inter-event times  k of the individuals of our datasets.
Autocorrelation refers to the correlation of a time series with its own past and future values.
A positive autocorrelation, which is suggested by Figure 6, might be considered a speci c form of  persistence , i.e., a tendency for a system to remain in the same state from one observation to the next.
We test if all the  k time series of every individual of our datasets are random or autocorrelated.
For this, we de ne the hypothesis test H0 that a series S = { 0,  1, ...,  n} of inter-event times is random.
If S is random, then its autocorrelation coe cient ACl   0 for all lags l > 0, where a lag l is used to compare, in this case, values of  k and  k l.
More formally, if ACl is between the 95% con dence interval for S to be random, then we accept H0 that S is random.
As we show in Figure 7, we reject the null hypothesis H0 that the inter-event times of the individual of Figure 1 is random, since all ACl, 1 < l   10 are outside the con dence interval.
Since we are interested only in the case where the lag l = 1, we propose an alternative hypothesis test H1 that the  rst-order auto-correlation coe cient AC1 is greater than 0.
If AC1 is greater than the con dence interval for randomness, then we accept H1 that the series is not random, i.e., there is a dependence between  k and  k 1.
In Figure 8, we show the empirical probability P(H1) of accepting H1 for individuals with a given number of events n of a given dataset.
As we observe, as the number of communication events n grows and becomes signi cant, the probability of accepting H1 increases rapidly.
This strongly suggests that, on the con-










-






k
 =











 -






k

 =















 -






k





 =



















 k





 =















 -






k




 =


















 k







 =














 -






k
 =













 -






k
 =















 -






k
 =




 data regression data regression s  k e m i t t n e v e r e n t i s  k e m i t t n e v e r e n t i (a) Youtube (b) MetaFilter (c) MetaTalk (d) Ask MetaFilter inter-event time   (s) inter-event time   (s) data regression data regression s  k e m i t t n e v e r e n t i s  k e m i t t n e v e r e n t i inter-event time   (s) inter-event time   (s) (e) Digg (f) SMS (g) Phone (h) Email Figure 6: I.i.d.
fallacy: dependence between  k and  k 1.
Each point represents a pair of consecutive inter-event times ( k 1,  k) registered for a typical active individual of each dataset.
The red line is a regression of the data points using the LOWESS smoother.
t n u o c t n u o c
   (ideal) Youtube Mefi Meta Digg Askme



 (a) First group Phone
 -mail
   (ideal)



 (b) Second group

 mean     for all datasets



 y t i s n e d Youtube Mefi Meta Digg Askme y t i s n e d



 mean     datasets for Phone
 mail odds ratio slope   (a) First group odds ratio slope   (b) Second group Figure 4: The PDF of the slopes  i measured for every user ui of our eight datasets.
Except the SMS dataset, the typical  i for the majority of individuals is approximately 1.
y t i s n e d Youtube Mefi Meta Digg Askme Phone
 -mail



 y t i s n e d median   (s) (a) First group median   (s) (b) Second group Figure 5: The PDF of the medians  i measured for every user of our eight datasets.
Observe that the typical  i is around 3 and 8 minutes for the  rst group and around 1 hour for the second group.
Figure 3: The goodness of  t of our proposed model.
We show the histograms of the R2s measured for every user in the eight datasets.
These histograms consider bins of size 0.05.
Thus, observe that the R2 value for the great majority of individuals is located in the last bin, from 0.95 to 1.
trary of what happens with the i.i.d.
inter-event times distribution generated by the Poisson Process or simply sampling from a log-logistic distribution, in real data there is a dependence between  k and  k 1.
This also agrees with a recent work [35], which reports that daily series of calls made by a customer exhibits long memory.
k 
( )

















 k 
( )

















 k 
( )

















 k 
( )







 ( % )








 ( % )







































 n o i t l a e r r o c o t u
 data
 Poisson not i.i.d.
correlation between   and   i.i.d.
Lag Figure 7: The sample autocorrelation for the same individual of Figure 1 and for synthetic data generated by the SFP and a PP with the same number of communication events and median.
Thus, in summary we can state that E( k| k 1) = f ( k 1) (3) where f is a function that describes the dependency between  k and  k 1.
n r




 Phone
 mail n r




 Youtube Mefi Meta Digg Askme of events n (a) First group of events n (b) Second group Figure 8: The empirical probability of an individual s inter-event times to be autocorrelated given his/her number of events.
Note that as the number of events grows, the probability of having an autocorrelated series increases rapidly for the eight datasets.
the long term, generates OR power law marginals and is extremely parsimonious: just one parameter, the median   of the IED.
We call this model the Self-Feeding Process (SFP).
We propose the generator as follows Model 1.
Self-Feeding Process SFP ( ).
//  is the desired median of the marginal PDF  1      k   Exponential (mean   =  k 1 +  /e) where   is the only parameter of the model, being the desired median of the IED.
The part  /e has to be higher than 0 to avoid  k to converge to 0 and has to be divided by the Euler s number e to make the median of the generated IED around the target median   (more details in the Appendix A).
This type of model is not new in the literature [41, 10] but they have not been extensively studied, perhaps due to the lack of empirical data  tting the implied distribution.
In Figures 9-a and 9-b we compare, respectively, the histogram and the OR of the inter-event times generated by the SFP model, all values rounded up, with the inter-event times of the individual of Figure 1.
Notice that the distributions are very similar and both are well  tted by a log-logistic distribution, which looks like a hyperbola, thus addressing both the power-law tail, as well as the  top-concavity  that real data exhibits.
For a generalized SFP model, that generates IEDs with di erent OR slopes, and more details about the log-logistic distribution, please see the Appendix B.
Moreover, for an analysis over the temporal correlations generated by the SFP, see the Appendix A.4.
OR slope     o i t a
 s d d
 data
 fitting (a) Histogram inter-event time   (s) (b) Odds ratio

 Given all the above evidence (OR power law; i.i.d.
fallacy) and all the previous evidence (power law tails by Barab si; short-term regular behavior as the PP), the question is whether can we design a generator which will match all these properties?
Our requirements for the ideal generator are the following: R1: Realism   marginals The model should generate OR power law marginal IED; R2: Realism   locally-Poisson: The model should behave as a Poisson Process within a short window of time; R3: Avoid the i.i.d.
fallacy Two consecutive inter-event times should be correlated; R4: Parsimony It should need only few parameters, and ideally, just one or two.
At a high level, our proposal is that the next inter-arrival time will be an exponential random variable, with rate that depends on the previous inter-arrival time.
It is subtle, but in this way our generator behaves like Poisson in the short term, gives power-law tails in Figure 9: Comparison of the marginal distribution of the inter-event times generated by the SFP model with the inter-event times of the user of Figure 1.
Observe that both the histogram (a) and the OR (b) are almost identical.
The SFP model naturally generates an odds ratio power law with slope   = 1, which is the slope that characterizes the majority of the users of our datasets (see Figure 4).
To the best of our knowledge, this is the  rst work that studies the IED of human communications using such a varied, modern and large collection of data.
Despite the fact that the means of communications are intrinsically di er-ent, having their own idiosyncrasies, we have observed that the IED of individuals of these systems have the same characteristics, i.e., they follow an odds ratio power law behavior.
Moreover, when the OR slope   = 1, the power law exponent of the PDF is   =  2 (see the Appendix B.5 for details).
This is the same IED slope   reported in [18, 40] as a result of  uctuations in the execution rate and in particular periodic changes.
It has been argued that seasonality can only robustly give rise to heavy-tailed IEDs when the exponent   = 2.
However, we again point out that the proposed (Generalized) SFP model (see the Appendix A) can generate IEDs









 -


-


-





% k k -











# (

 | )










 # (

 | )











 -






k
 with power-law slopes   varying in the range ( ,  1), agreeing with all the empirical studies we are aware of.
Moreover, we point out that the typical values of the parameters   and   can be easily extracted from the distributions shown in Figures 5 and 4.
Finally, we would like to emphasize the unifying power of the SFP.
Several works [21, 32, 31, 30, 25, 23] claim that in the short term, real data behave as regular as a PP.
Our model also captures that, since successive inter-event times are exponentially distributed, with similar (but not identical) rates.
Thus, one of the major contributions of this work is the uni cation of the two seemingly-con icting viewpoints we mentioned earlier.
The proposed SFP model uni es both theories by generating Poisson-like tra c in the short term, with smoothly varying rate, like the second viewpoint, and also generates a power-law tail distribution (see the Appendix B.5), even matching the top-concavity that power laws cannot match, like the  rst modern approach of Barab si [2].
In Figure 10, we explicitly show the SFP s unifying power.
We compare synthetic data generated by the SFP model using the same odds ratio slope  , median   and number of events of the user of Figure 1 with the real data from this user.
Notice the bursts of activity and also the long periods of inactivity, in the  rst two columns of Figure 10.
Also notice that both synthetic and real tra c significantly deviate from Poisson (sloping lines in Figures 10-b and 10-f) but are similar between themselves.
However, in the short term, both real and synthetic data behave like Poisson, being practically on top of the black dashed lines of Figures 10-d and 10-h.
In this paper, we propose the SFP model, which reconciles previous approaches for human communication dynamics.
The SFP is a parsimonious generator that requires at most two intuitive parameters, and yet it has several desirable properties:   Realism: it matches very well the properties of the IEDs of eight large, diverse and real systems, such as online forums, Youtube comments, online news, emails, SMSs and phone calls;   Uni cation Power: it reconciles seemingly-contradicting theories on human communication dynamics.
Our model exhibits power law tail behavior and burstiness in the long term, as well as Poisson-like behavior in the short term; fallacy :   It avoids the  i.i.d.
inter-event times are not independent, i.e., the time needed for the next event to arrive depends on the time the previous event took to arrive.
Our model is the  rst to capture this very subtle point.
Moreover, there are two additional contributions: (i) the proposal to use the so-called  Odds Ratio  function using the cumulative distribution function   most of our real data seems to obey a power-law in their Odds-Ratio function, even when their PDF deviates from a power-law; (ii) the proposal to use the log-logistic distribution, which has power-law tail, but also exhibits the so-called  top-concavity , that real data seem to have.
