We study how an advertiser changes his/her bid prices in sponsored search, by modeling his/her rationality.
Predicting the bid changes of advertisers with respect to their campaign performances is a key capability of search engines, since it can be used to improve the of ine evaluation of new advertising technologies and the forecast of future revenue of the search engine.
Previous work on advertiser behavior modeling heavily relies on the assumption of perfect advertiser rationality; however, in most cases, this assumption does not hold in practice.
Advertisers may be unwilling, incapable, and/or constrained to achieve their best response.
In this paper, we explicitly model these limitations in the rationality of advertisers, and build a probabilistic advertiser behavior model from the perspective of a search engine.
We then use the expected payoff to de ne the objective function for an advertiser to optimize given his/her limited rationality.
By solving the optimization problem with Monte Carlo, we get a prediction of mixed bid strategy for each advertiser in the next period of time.
We examine the effectiveness of our model both directly using real historical bids and indirectly using revenue prediction and click number prediction.
Our experimental results based on the sponsored search logs from a commercial search engine show that the proposed model can provide a more accurate prediction of advertiser bid behaviors than several baseline methods.
Categories and Subject Descriptors H.3.5 [Information Systems]: Information Storage and Retrieval - Online Information Services Keywords Advertiser modeling, rationality, sponsored search, bid prediction.
 This work was performed when the  rst and the third authors were interns at Microsoft Research Asia.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
Sponsored search has become a major means of Internet moneti-zation, and has been the driving power of many commercial search engines.
In a sponsored search system, an advertiser creates a number of ads and bids on a set of keywords (with certain bid prices) for each ad.
When a user submits a query to the search engine, and if the bid keyword can be matched to the query, the corresponding ad will be selected into an auction process.
Currently, the Generalized Second Price (GSP) auction [10] is the most commonly used auction mechanism which ranks the ads according to the product of bid price and ad click probability1 and charges an advertisers if his/her ad wins the auction (i.e., his/her ad is shown in the search result page) and is clicked by users [13].
Generally, an advertiser has his/her goal when creating the ad campaign.
For instance, the goal might be to receive 500 clicks on the ad during one week.
However, the way of achieving this goal might not be smooth.
For example, it is possible that after one day, the ad has only received 10 clicks.
In this case, in order to improve the campaign performance, the advertiser may have to increase the bid price in order to increase the opportunity for his/her ad to win future auctions, and thus to increase the chance for the ad to be presented to users and to be clicked.2 Predicting how the advertisers change their bid prices is a key capability of a search engine, since it can be used to deal with the so-called second order effect in online advertising [13] when evaluating novel advertising technologies and forecasting future revenue of search engines.
For instance, suppose the search engine wants to test a novel algorithm for bid keyword suggestion3 [7].
Given that the online experiments are costly (e.g., unsuccessful online experiments will lead to revenue loss of the search engine), the algorithm will usually be tested based on the historical logs  rst to see its ef-
than the reserve score are shown.
tion, bid extra keywords, and so on.
However, among these actions, changing the bid price is the simplest and the most commonly used method by advertisers.
Please also note that since GSP is not incentive compatible, advertisers might not bid their true values and changing bid prices is their common behaviors.
like traf c estimation, ad click prediction, and auction mechanism.
even if the algorithm works quite well in of ine experiment, it may perform badly after being deployed online.
One of the reasons is that the advertisers might change their bid prices in response to the changes of their campaign performances caused by the deployed new algorithm.
Therefore, the experiments based on the historical bid prices will be different from those on online traf c.
To tackle this problem, one needs a powerful advertiser behavior model to predict the bid price changes.
In the literature, there have been a number of researches [4] [5] [22] [19] [2] [17] [3] that model how advertisers determine their bid prices, and how their bid strategies in uence the equilibrium of the sponsored search system.
For example, Varian [19] assumes that the advertisers bid the amount at which their value per click equals the incremental cost per click to maximize their utilities.
The authors of [2] and [17] study how to estimate value per click, by assuming advertisers are on the locally envy-free equilibrium, and assuming the distributions of all the advertisers  bids are independent and identically distributed.
Most of the above researches rely highly on the assumptions of perfect advertiser rationality and full information access4, i.e., advertisers have good knowledge about their utilities and are capable of effectively optimizing the utilities (i.e., take the best response).
However, as we argue in this paper, this is usually not true in practice.
In our opinion, real-world advertisers have limitations in accessing the information about their competitors, and have different levels of rationality.
In particular, an advertiser may be unwilling, incapable, or constrained to achieve his/her  best response.  As a result, some advertisers frequently adjust the bid prices according to their recent campaign performances, while some other advertisers always keep the bid unchanged regardless of the campaign performances; some advertisers have good sense of choosing the appropriate bid prices (possibly with the help of campaign analysis tools [14] or third-party ad agencies), while some other advertisers choose bid prices at random.
To better describe the above intuition, we explicitly model the rationality of advertisers from the following three aspects:   Willingness represents the propensity an advertiser has to optimize his/her utility.
Advertisers who care little about their ad campaigns and advertisers who are very serious about the campaign performance will have different levels of willingness.
  Capability describes the ability of an advertiser to estimate the bid strategies of his/her competitors and take the best-response action on that basis.
An experienced advertiser is usually more capable than an inexperienced advertiser; an advertiser who hires professional ad agency is usually more capable than an advertiser who adjusts bid prices by his-self/herself.
  Constraint refers to the constraints that prevent an advertiser from adopting a bid price even if he/she knows that this bid price is the best response for him/her.
The constraint usually (although not only) comes from the lack of remaining budget.
With the above notions, we propose the following model to describe how advertisers change their bid prices, from the perspective
 however, they still assume that the priors of the value distributions are publicly known.
of the search engine.5 First, an advertiser has a certain probability to optimize his/her utility or not, which is modeled by the willingness function.
Second, if the advertiser is willing to make changes, he/she will estimate the bid strategies of his/her competitors.
Based on the estimation, he/she can compute the expected payoff (or utility) and use it as an objective function to determine his/her next bid price.
This process is modeled by the capability function.
By simultaneously considering the optimization processes of all the advertisers, we can effectively compute the best bid prices for every advertiser.
Third, given the optimal bid price, an advertiser will check whether he/she is able to adopt it according to some constraints.
This is modeled by the constraint function.
Please note that the willingness, capability, and constraint functions are all parametric.
By  tting the output of our proposed model to the real bid change logs (obtained from commercial search engines), we will be able to learn these parameters, and then use the learned model to predict the bid behavior change in the future.
We have tested the effectiveness of the proposed model using real data.
The experimental results show that the proposed model can predict the bid changes of advertisers in a more accurate manner than several baseline methods.
To sum up, the contributions of our work are listed as below.
First, to the best of our knowledge, this is the  rst advertiser behavior model in the literature that considers different levels of rationality of advertisers.
Second, we model advertiser behaviors using a parametric model, and apply machine learning techniques to learn the parameters in the model.
This is a good example of leveraging machine learning in game theory to avoid its unreasonable assumptions.
Third, our proposed model leads to very accurate bid prediction.
In contrast, as far as we know, most of previous research focuses on estimating value per click, but not predicting bid prices.
Therefore, our work has more direct value to search engine, given that bid prediction is a desired ability of search engine as aforementioned.
The rest of the paper is organized as the following.
In Section 2, we introduce the notations and describe the willingness, capabil- ity, and constraint functions.
We present the framework of the bid strategy prediction model in Section 3.
In Section 4, we introduce the ef cient numerical algorithm of the model.
In Section 5, we present the experimental results on real data.
We summarize the related work in Section 6, and in the end we conclude the paper and present some insights about future work in Section 7.
As mentioned in the introduction, how an advertiser adjusts his/her bid is related to his/her rationality.
In our opinion, there are three aspects to be considered when modeling the rationality of an advertiser: willingness, capability, and constraint.
In this section, we introduce some notations for sponsored search auctions, and then describe the models for these rationality aspects.
We consider the keyword auction in sponsored search.
For simplicity, we will not consider connections between different ad campaigns and we assume each advertiser only has one ad and bids on just one keyword for it.
That is, the auction participants are the keyword-ad pairs.
Advertisers are assumed to be risk-neutral.6
 vertisers  behavior, but not by the advertisers to guide their bidding strategies.
tions for all the advertisers.
However, our result can be naturally
 advertiser l as the default advertiser of our interest.
Suppose in one auction the advertisers compete for J ad slots.
In practice, the search engine usually introduces a reserve score to optimize its revenue.
Only those ads whose rank scores are above this reserve score will be shown to users.
To ease our discussion, we regard the reserve score r as a virtual advertiser in the auction.
We use ai,j to denote the click-through rate (CTR) of advertiser i s ad when it is placed at position j.
Similar to the setting in [2][17], we assume ai,j to be separable.
That is , ai,j =  i j, where  i is the ad effect and  j is the position effect.
We let  j = 0 when j > J.
The sponsored search system will predict the click probability [11] of an ad and use it as a factor to rank the ads in the auction.
We use si to denote the predicted click probability of advertiser i s ad if it is placed in the  rst ad slot.
Note that both ai,j and si are random variables [2], since they may be in uenced by many dynamic factors such as the attributes of the query and the user who issues the query.
We assume all the advertisers share the same bid strategy space   which consists of B different discrete bid prices denoted by bi, i =
  l = ( l,1,       ,  l,B), which is a mixed strategy.
It means that l will use bid strategy bi with a probability of  l,i, i = 1,       , B.
We assume advertiser l will estimate both the con guration of his/her competitors and their strategies in order to  nd his/her own best response.
We use S (including l) to indicate the set of advertisers who are regarded by advertiser l as the participates of the auction and use S l (excluding l) to indicate the set of competitors of l. We denote  (l) as l s estimated bid strategy for a competitor i (i 6= l), and denote l s own best-response strategy as  (l) .
i Note that both S and  (l) i (i 6= l) are random: (i) S is a random set due to the uncertainty in the auction process: a) the participants of the auction is dynamic [17]; b) in practice l never knows exactly who are competing with him/her since such information is not publicly available.
(ii)  (l) i (i 6= l) is a random vector due to l s incomplete information and our uncertainty on l s estimation.
More intuitions about  (l) i will be explained in the modeling of the capability function (see Section 2.3).
l To ease our discussion, we now transform the uncertainty of S to the uncertainty in bid prices, as shown below.
That is, we regard all the other advertisers as the competitors of l and add the zero bid price (denoted by b0) to extend the bid strategy space.
The extended bid strategy space is represented by   =  S{b0}.
If an advertiser is not a real competitor of l, we regard his/her bid price to be zero.
According to the above discussion, S will be the whole advertiser set with the set size I.
Thus, we will only consider the uncertainty of bid prices in the rest of the paper.
Willingness represents the propensity an advertiser is willing to optimize his/her utility, which is modeled as a possibility.
We model willingness as a logistic regression function Wl(x(l) t ).
Here the input x(l) t,H) is a feature vector (H is the number of features) extracted for advertiser l at period t, and the output is a real number in [0, 1] representing the probability that l will optimize his/her utility.7 That is, advertiser l with feature vector x(l) t,1,       , x(l) t = (x(l) t extended to the case where advertisers  different risk preferences are considered.
bid.
Probably, an advertiser attempts to optimize his/her utility, but  nally  nds that his/her previous bid is already the best choice.
In will have a probability of Wl(x(l) a probability of 1   Wl(x(l) t ) to take no action.
t ) to optimize his/her utility, and t t = 1; otherwise, y(l) In order to extract the feature vector x(l) t , we split the historical auction logs into T periods (e.g., T days).
For each period t   T , y(l) indicates whether the bid was changed in period t + 1.
If the bid was changed, y(l) t = 0.
With this data, the following features are extracted: (i) The number of bid changes before t. The intuition is that an advertiser who changes bid more frequently in the past will also have a higher possibility to make changes in the next period.
(ii) The number of periods that an advertiser has kept the bid unchanged until t. Intuitively, an advertiser who has kept the bid unchanged for a long time may have a higher possibility to continue keeping the bid unchanged.
(iii) The number of different bid values used before t. The intuition is that an advertiser who has tried more bid values in the past may be regarded as a more active bidder, and we may expect him/her to try more new bid values in the future.
(iv) A Boolean value indicating whether there are clicks in t. The intuition is that if there is no click, the advertiser will feel unsatis ed and thus have a higher probability to make changes.
With the above features, we write the willingness function as, Wl(x(l) t ) =
 1 + e{  (l)

 n=1   (l) n x (l) t,n} , (t = 1,       , T ).
Here  (l) = ( (l) 0 ,       ,  (l) H ) is the parameter vector for l.
the  rst-order errorPT To learn the parameter vector  (l), we minimize the sum of t )| on the historical data using the classical Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) [15].
Then we apply the learned parameter  (l) to predict l s willingness of change in the future.
t   Wl(x(l) t=1 |y(l)
 Capability describes the ability of an advertiser to estimate the bid strategies of his/her competitors and take the best-response action on that basis.
A more experienced advertiser may have better capability in at least three aspects: information collection, utility function de nition, and utility optimization.
Usually, in GSP auctions, a standard utility function is used and the optimal solution is not hard to obtain.
Hence, we mainly consider the capability in information collection, i.e., the ability in estimating competitors  bid strategies.
Recalling that l does not have any exact information on his/her competitors  bids, it is a little dif cult to model how advertiser l estimates his/her competitors  strategies, because different l has different estimation techniques.
Before introducing the detailed model for the capability function, we would like to brie y describe our intuition.
It s reasonable to assume that l s estimation on i is based on i s market performance, denoted by P erfi.
Then we can write l s estimation as Estl(P erfi), which means l applies some speci c estimation technique Estl on P erfi.
The market performance P erfi is decided by all the advertisers  bid pro les due to the auction property.
That is, P erfi = P erf  i is  i i s historical bid histogram.
Note that we use    i because we believe the observed market performance P erfi is based on the auctions during a previous period, while not just one previous auction.
However, we are mostly interested in pro table keywords, the auctions of which usually have so many advertisers involved that    i can be regarded as a constant environment factor for any i.
Therefore, P erfi only depends on   i ).
i , i.e., P erfi = P erf (  i ), here   i and   (  this case, he will keep the bid unchanged but we still regard it as  willing to optimize. 
 i )).
Till now, the problem becomes much easier: l is blind to   i , but the search engine has all the information of   i .
To know Estl(P erfi), the search engine only needs to model the function Estl(P erf ( )) given that   i is known.
Speci cally, we denote the above Estl(P erf (  i ).
As described in Section 2.1, Al(  i )) as our capability function Al(  i ) is denoted by  (l) .
The reason that Al is named as capability function is clear: Estl, the techniques l uses for estimation, re ects his/her capability.
The reason that  (l) i ) is modeled to be random is also clear: the search engine does not know what Estl, and thus aspired by the concept of  type  in Bayesian Game [12] which is a description of incomplete game setting, we regard Estl as a  type  of l and model its distribution.
For the same   i , different advertisers may have different estimations according to their various capabilities.
i = Al(  i i To simplify our model, we give the following assumption on .
We assume that l s estimations on other advertisers  bid strate-is a random Boolean vector  (l) gies are all pure strategies.
That is,  (l) with just one element equal to 1.8 i Given a bid bn with possibility   i,n from the historical bid histogram   i , we assume l s estimation has a  uctuation around bn.
The  uctuation can be modeled by a certain probability distribution such as Binomial distribution or Poisson distribution.
The parameters of the distribution can be used to indicate l s capability.
Here we use Binomial distribution to model the  uctuation due to the following reasons: (i) Theoretically, Binomial distribution can conveniently describe the discrete bids due to its own discrete nature.
Furthermore, the two parameters in Binomial distribution can well re ect the capability levels: the trail times N can control the  uctuation range (N = 0 means a perfect estimation) and the success possibility     (0, 1) can control the bias of the estimations.
Speci cally, if   > 0.5, it means the estimation is on average larger than the true distribution and vice versa.
(ii) Experimentally, we have compared Binomial distribution with some other well-known distributions such as Gaussian, Poisson, Beta, and Gamma distributions, and the experiment results show that Binomial distribution performs the best in our model.
For sake of simplicity, we let the  uctuation range be an integer 2Nl    , and the success possibility be  l   (0, 1).
Then (Nl,  l) are l s capability parameters.
The  uctuation on bn in   i is modeled by P r(Al(bn)=bn+m) =   i,n 2Nl Nl + m! (Nl+m) l (1    l)(Nl m), (m =  Nl, ..., Nl).
In the above formula, i 6= l; the symbol  =  means the equivalence of strategy;(cid:0) 2Nl Nl +m(cid:1) is the number of (Nl + m)-combinations in a set with 2Nl integers.
Therefore, by considering all the bid values in   i , we have, P r( (l) i =bn) = P r(Al(  i )=bn) = Nl Xm= Nl   i,n m 2Nl Nl + m! (Nl+m) l (1    l)(Nl m).
Constraint refers to the factor that prevents an advertiser from adopting a bid price even if he/she knows that this bid price is the
 with a bit more complicated notations and computing algorithms.
best response for him/her.
In practice, many factors (such as lack of remaining budget and the aggressive/conservative character of the advertiser) may impact advertiser s eventual choices.
For example, an advertiser who lacks budget or has conservative character may prefer to bid a lower price than the best response.
l t at period t, then Cl( (l) We model constraint using a function Cl, which translates the best response (which may be a mixed strategy) to the  nal strategy with step (a.k.a., difference) c(l) .
That is, if the best bid strategy is  (l) t with probability  (l) l,n.
Similar to the proposal in the willingness function, we model the step c(l) t using a regression model.
The difference is that this time we use linear regression since c(l) is in nature a translation distance but not a probability.
Here we use the remaining budget as the feature x(l) and build the following function form: l ) will be bn + c(l) t t c(l) t =  1,l +  2,l x(l) t   x(l) x(l) , where x(l) = Pt T x(l)
 t .
t t t x(l) .
Here b(l) l,n)   b(l) n=1 bn (l) as the label for c(l) use (PB In the above formula, T is the set of periods for training and is l s remaining budget in period t. In the training data, we is l s real bid at period t;  1,l and  2,l are the parameters for the linear regression.
Note that  1,l is only related to l himself/herself.
This parameter reveals l s internal character on whether he/she is aggressive or not.
One can intuitively imagine that for aggressive advertisers,  1,l will be positive because such advertisers are radical and they would like to overbid.
Moreover, we normalize the budget in the formula because the amounts of budget vary largely across different advertisers.
The normalization will help to build a uniform model for all advertisers.
t

 After explaining the advertiser rationality in terms of willingness, capability, and constraint, we introduce a new advertiser behavior model.
Suppose advertiser l has a utility function Ul.
The inputs of Ul are l s estimations on his/her competitors  bid strategies, which are given by the capability function Al.
The goal of advertiser l is to  nd a mixed strategy  (l) to maximize this utility, i.e., l arg max   (l) l Ul(Al(  i ), i = 1,       , I) = arg max Ul( (l) i   (l) l , i = 1,       , I, i 6= l).
If we further consider the changing possibility Wl, the constraint function Cl, and the randomness of Al, we can get the general advertiser behavior model that explains how advertiser l may determine his/her bid strategy for the next period of time:  l = WlEAl (Cl(arg max   (l) l Ul( (l) i , i = 1,       , I, i 6= l))) + (1   Wl)(0, ..0, 1, 0...0)T ,   l.
(1) Here (0, ..0, 1, 0...0) is the unchanged B-dimension bid strategy where the index of the one (and the only one) equals n if the bid in the previous period is bn.
 arg max  outputs a B-dimension mixed strategy of l; EAl means the expectation on the randomness of Al(  j ); Wl is the possibility that l decides to optimize his/her utility.
We want to emphasis that equation (1) is a general expression under our rationality assumptions.
Though we have provided the details of the model in Section 2 about Wl, Al, Cl and we will
 tainly propose any other forms of the model for all these functions.
To make the above model concrete, we need to de ne and calcu-i Recall our assumption that  (l) late the utility function Ul for every advertiser.
i = Al(  i ) is a pure strategy; that is, only one element in  (l) is one and all the other elements are zeros.
Suppose the bid value that corresponds to the  one  in  (l) is oi (oi     and i 6= l).
In this case, the bid con guration is o = (o1,       , oI ), in which all the advertisers  bids are  xed.
Please note that the representations in terms of oi and the original representations in term of  (l) are actually equivalent to each other, since they encode exactly the same information and its randomness in the bid strategies of advertiers.
i i Then we introduce the form of Ul.
Based on the bid prices in o and ad quality scores si (i = 1,       , I), we can determine the ranked list in the auction according to the commonly used ranking rules (i.e., the product of bid price and ad quality score [13]) in sponsored search.
Suppose l is ranked in position j and  l is ranked in position j + 1.
According to the pricing rule in the generalized second price auction (GSP) [10], l should pay o ls l/sl for each click.
As de ned in Section 2.1, the possibility for a user to click l s ad in position j is alj =  l j .
Suppose the true value of advertiser l for a click is vl (which can be estimated using many techniques, e.g., [9]), then we have, Ul = E l , j ,s l ,sl {( l j (vl   o ls l sl ))} =  l j (vl   o ls l sl ).
As explained in Section 2,  l,  j , s l, sl are all random variables.
Here  l,  j, s l, and sl are their means.
Since Ul is linear and the above four random variables are independent of each other, the outside expectation can be moved inside and substituted by the corresponding means.
With all the above discussions, we are now ready to give the  nal form of the advertiser model.
By denoting o l = (o1,       , ol 1, ol+1,       , oI ) as the bid con guration without l s bid, we get the following expression for l = 1,       , I:  l = WlEo l {Cl[arg max ( l j (vl   o ls l/sl))]}   (l) l +(1   Wl)(0, ..0, 1, 0...0)T .
Here the randomness of Al is speci cally expressed by the randomness of o l.
Note that  l is a constant for l and it will not affect the result of  arg max .
Therefore we can remove it from the above expression to further simplify the  nal model:  l = WlEo l {Cl[arg max ( j (vl   o ls l/sl))]}   (l) l +(1   Wl)(0, ..0, 1, 0...0)T ,  l.
(2)

 In this section we introduce an ef cient algorithm to solve the advertiser model proposed in the previous sections.
To ease our discussion, we assume that the statistics  j, s l, and sl are all known (with suf cient data and knowledge about the market).
Furthermore, we assume that the search engine can effectively estimate the true value vl in (2).
Considering the setting of our problem, we choose to use the model in [9] for this purpose.
Table 1: O-simulator initialize o = (o1,       , oI ) = (0, 0,       , 0) for i = 1, ..., I, f =random(); // random() uniformly outputs a random  oat number in [0,1].
sum = 0; n = 0; while(sum < f ) sum = sum + P (Oi = bn); n = n + 1; end; oi = bn; end; output o; Our discussions in this section will be focused on the computational challenge to obtain the best response for all the cases of bid con gurations o (corresponding to o l in (2)).
This is a typical combinatorial explosion problem with a complexity of BI , which will increase exponentially with the number of advertisers.
Therefore, it is hard to solve the problem directly.
Our proposal is to adopt a numerical approximation instead of giving an accurate solution to the problem.
We can prove that the approximation algorithm can converge to the accurate solution with a small accuracy loss and much less running time.
Our approximation algorithm requires the use of a O-simulator, which is de ned as follows.
DEFINITION 1.
(O-simulator) Suppose there is a random vector O = (O1,       , OI )   P (o), i.e., P (o) is the distribution of O.
Given  o     and P (o), an algorithm is called an O-simulator if the algorithm randomly outputs a vector o with the probability P (o).
As described above, O-simulator actually simulates the random vector O and randomly output its samples.
In general, it is dif cult to simulate a random vector; however, in our case, all the Oi are independent of each other and they have discrete distributions.
Therefore, the simulation becomes feasible.
In Table 1 we give a description of O-simulator.
Here we assume O = (O1,       , OI ) and Oi   Pi(oi), oi    .
Furthermore,   = {b0, b1,       , bB} is a discrete space shared by all i (like the bid space in our model) and all Oi are independent of each other.
Note that f is a uniformly random number from [0, 1], therefore the possibility that oi equals bn is exactly P (Oi = bn).
Thus, the possibility to output o = (o1,       , oI ) is  I i=1P (Oi = oi), which is exactly what we want.
  (l) l i and q(l) i,n, and thus q(l) We then give the Monte Carlo Algorithm as shown in Table 2 to ( j (vl o ls l/sl))} for a certain l. For i = bn) as q(l) calculate Eo l {arg max simplicity, we denote P r( (l) i,0 is the possibility that i is not in the auction.
In this algorithm, the historical bid histogram   i,0 are calculated from the auction logs by Maximum Likelihood Estimation.
Given rationality parameter  l, Nl, and q(l) i,n by the capability function.
Then with o l generated by O-simulator, we can calculate which ranked list is optimal for l by solving arg max ( j (vl o ls l/sl)).
Note that it is possible that different bids may lead to the same optimal ranked list (with the same utility).
In this case, the inverse function   will output a bid set Bo l including all the equally  arg max optimal bids.
By assuming that advertiser l will take any bid in Bo l with uniform probability, we allocate each bid in Bo l with i,0, we initialize q(l) (l) l   (l) l  
 for i = 0, ..., I, initialize   i , q(l) i,0, si; end; for j = 0, ..., J , initialize  j ; end; initialize ,  l, Nl, 1/sl;  (l) l,n = 0; for i = 1,       , I(l 6= i) and n = 1,       , B q(l) i,n = (1   q(l) i,0)  m= Nl PNl   i,n m(cid:0) 2Nl Nl+m(cid:1) (Nl+m) l end; Build an O-simulator with P (O = o l) =  I for t = 1,       , N , (1    l)(Nl m); i=1,i6=lq(l) i,oi ,  o l; O-simulator outputs a sample o l; Solve arg max for all bi   Bo l , (l) l    (l) l,i =  (l) l,i + 1/|Bo l |; ( j (vl   o ls l/sl)) to get Bo l ; end; end; for n = 1,       , B,  (l) l,n= (l) l,n/N .
end; output  (l) l,n; a weight
 |Bo l | averagely.
Finally, we use the simulation times N to normalize the distribution and output it.
For the Monte Carlo Algorithm, we can prove its convergence to the accurate solution, which is shown in the following theorem.
THEOREM 1.
Given   i and q(l) Algorithm converges to Eo l {arg max the times of simulation N grows.
(l) l i,0, the output of the Monte Carlo ( j(vl   o ls l/sl))} as   PROOF.
We assume that the accurate solution is  0 l and thus we need to prove  n (n = 1,       , B),  (l) l,n    0 l,n as N    .
For a certain player l, we construct the following map: M : o l   Bo l =(cid:8)all of l s best bids in case o l(cid:9) ,  o l.
According to the de nition, we know that  0 l,n equals to the nth element of Eo l {arg max   (l) l ( j(vl   o ls l/sl))}, and then  0 l,n =
 containing bn P (o l) |Bo l | .
all Bo l Here P (o l) is the probability of o l.
In the Monte Carlo algorithm, we initialize  (l) l,n increases by  t in each step of the loop  for t = 1,       , N  .
Therefore, the value of  (l) t=1  t)/N .
However, in each step t, for a sample o l, the expectation of  t is, l,n = 0, and suppose that  (l) l,n will  nally be (PN
 E( t) = all Bo l P (o l) |Bo l | .
containing bn Besides the above theorem, we can also prove some properties of the proposed model.
We describe the properties in the appendix for the readers who are interested in them.
In this section, we report the experimental results about the pre-In particular, we  rst diction accuracy of our proposed model.
describe the data sets and the experimental setting.
Then we investigate the training accuracy for the willingness, capability, and constraint functions, to show the step-wise results of the proposed method.
After that, we test the performance of our model in bid prediction, which is the direct output of the advertiser behavior model.
At last, we test the performance of our model in click number prediction and revenue prediction, which are important applications of the advertiser behavior model.
In our experiments, we used the advertiser bid history data sampled from the sponsored search log of a commercial search engine.
We randomly chose 160 queries from the most pro table 10,000 queries and extracted the related advertisers from the data.
We sampled one auction per 30 minutes from the auction log within

 (4 on mainline and 10 on sidebar) ads displayed.
We  ltered out the advertisers whose ads have never been displayed during these 4,320 auctions, and eventually kept 5,543 effective advertisers in the experiments.
For the experimental setting, we used the  rst 3,360 auctions (70 days) for model training, and the last 960 auctions (20 days) as test data for evaluation.
In the training period, we used the  rst 2,400 auctions (50 days) to obtain the historical bid histogram   i (i =
 (20 days) to learn the parameters for the advertiser rationality.
For clarity, we list the usage of the data in Table 3.
Note that the three periods in the table are abbreviated as P1, P2, and P3.
ity
 t First, we study the logistic regression model for willingness.
We train the willingness function using the auctions in P2 according to the description in Section 2.2, and test its performance on actions in P3.
In particular, for any auction t in P3, we get the value of y(l) according to whether the bid was changed in the time interval [t   1, t], and use it as the ground truth.
For the same time period, we apply the regression model to calculate the predicted value  y(l) t   [0, 1] of y(l) is correspondingly converted to 0 or 1.
Then we can calculate the prediction accuracy compared with the ground truth.
Figure 1 shows the distribution of different prediction accuracies among advertisers when the threshold is set to 0.15.
According to the  gure, we can see that the willingness function gets a prediction accuracy of 100% for 39% (2,170 of 5,543) advertisers, and a prediction accu- racy over 80% for 68% (3,773 of 5,543) advertisers.
In this regard we say the proposed willingness model performs well on predicting whether the advertisers are willing to change their bids.
.
We  nd a threshold in [0, 1] such that  y(l) t t Hence, referring to the Law of Large Number, (PN t=1  t)/N will converge to the expectation of  t, which exactly equals  0 l,n as N grows.
This  nishes our proof of Theorem 1.
with the seasonal or holiday effects, we can choose seasonal or holiday data from different years instead of the data in continuous time.
We only consider the general cases in our experiments.
Period #auctions Usage Information required Table 3: Data usage in the experiments Training Test P1: Day 1 to Day 50 P2: Day 51 to Day 70 P3: Day 71 to Day 90


 (i) Get historical bid histogram (ii) Learn true value bid price ad quality score ad position Learn rationality parameters Test model bid price ad quality score click number budget bid price ad quality score click number budget pay per click




 s r e s i t r e v d
 f o r e b m u







 Prediction Accuracy on Willingness Figure 1: Distribution of the prediction accuracy.
Second, we investigate the capability function.
For this purpose, we set Cl as an identify function, and only consider Wl and Al.
In the capability function Al, we discretely pick the parameter pair ( l, Nl) from the set {0, 0.1,       , 0.9, 1.0} {0, 1,       , 9, 10} and judge which parameter pair is the best using the data in P2 as described in Section 2.3.
We call the advertiser model with the learned willingness and capability functions (without considering the constraint function) Rationality-based Advertiser Behavior model with Willingness and Capability (or RAB-WC for short).
Its performance will be reported and discussed in Section 5.3.
Third, the constraint function is implemented with a linear regression model trained on P2, using the remaining budget as the feature, according to the discussions in Section 2.4.
By applying the constraint function, we get the complete version of the proposed model.
We call it Rationality-based Advertiser Behavior model with Willingness, Capability, and Constraint (or RAB-WCC for short).
Its performance will be given in Section 5.3.
In this subsection, we compare our proposed advertiser model with six baselines in the task of bid prediction.
The predicted bid prices are the direct outputs of the advertiser behavior models.
The baselines are listed as follows:   Random Bid Model (RBM) refers to the random method of bid prediction.
That is, we will randomly select a bid in the bid strategy space as the prediction.
  Most Frequent Model (MFM) refers to an intuitive method for bid prediction, which works as follows.
First, we get the historical bid histogram from the bid values in the training period, and then always output the historically most frequently-used bid value for the test period.
If there are several bid prices that are equally frequently used, we will randomly select one from them.
  Best Response Model (BRM) [5] refers to the model that predicts the bid strategy to be the best response by assuming the advertisers know all the competitors  bids in the previous auction.
  Regression Model (RM) [8] refers to the model that predicts the bid strategy using a linear regression function.
In our experiments, we used the following 5 features as the input of this function: the average bid change in history, the bid change in the previous time period, click number, remaining budget, and revenue in the previous period.
  RAB-WC refers to the model as described in the previous subsection.
  RAB-WCC-D refers to the degenerated version of RAB-WCC.
That is, we select the bid with the maximum probability in the mixed bid strategy output by RAB-WCC.
We adopt two metrics to evaluate the performances of these advertiser models.
First, we use the likelihood of the test data as the evaluation metric [9].
Speci cally, we denote a probabilistic prediction model as M,10 which outputs a mixed strategy of advertiser l in period t as  [t] l = ( [t] l,B) in the bid strategy space  0.
Suppose the index of the real bid strategy of l in period t is  [t] .
Considering a period set T and an advertiser set I, we de ne the following likelihood: l,0,       ,  [t] l PT ,I (M) =  t T ,l I( [t] l,  ).
[t] l PT ,I(M) re ects the probability that model M produces the real data  [t] for all t   T and all l   I.
To make the metric normalized and positive, we adopt the geometric average and a negative logarithmic function.
As a result, we get l DT ,I(M) =   ln( |T ||I|pPT ,I(M)) =   ln PT ,I(M)
 .
We call it negative logarithmic likelihood (NLL).
It can be seen that with the same T and I, the smaller NLL is, the better prediction M gives.
istic models.
We can still compute the likelihood for them because deterministic models are special cases of probabilistic models.
strategy and the real bid as the evaluation metric.
Speci cally, we de ne the metric as the aggregated expected error (AEE) on a period set T and an advertiser set I, i.e.,
 Xt T Xl I Xi=0 | [t] l,i(bi   b )|.
  [t] l (3) The average NLL and AEE on all the 160 queries of the above algorithms are shown in Table 4.
We have the following observations from the table.
  Our proposed RAB-WCC achieves the best performance compared with all the baseline methods.
  RAB-WCC-D performs the second best among these methods, indicating that the bid with the maximum probability in RAB-WCC has been a very good prediction compared with most of the baselines.
  RAB-WC performs the third best among these methods, showing that: a) the proposed rationality-based advertiser model can outperform the commonly used algorithms in bid prediction; b) the introduction of the constraint function to the rationality-based advertiser model can further improve its prediction accuracy.
  RBM performs almost the worst, which is not surprising due to its uniform randomness.
  BRM also performs very bad.
Our explanation is as the following.
In BRM, we assume the advertisers know all the competitors  bids before selecting the bids for the next auction.
However, the real situation is far from this assumption.
So the  best response  will not be the real response for most cases.
  MFM model performs better than BRM.
This is not dif cult to interpret.
MFM is a data driven model, without too much unrealistic assumptions.
Therefore, it will  t the data better than BRM.
  RM performs better than MFM but worse than RAB-WC, RAB-WCC-D, and RAB-WCC.
RM is a machine learning model which leverages several features related to the advertiser behaviors, therefore it can outperform MFM which is simply based on counting.
However, RM does not consider the rationality levels in its formulation, and therefore it cannot  t the data as well as our proposed model.
This indicates the importance of modeling advertiser rationality when predicting their bid strategy changes.
In addition to the average results, we give some example queries and their corresponding NLL and AEE on the 960th auction in P3 in Table 5 and Table 6.
The best scores are blackened in the table.
At  rst glance, we see that RAB-WCC achieves the  rst positions in most of the example queries, while RAB-WCC-D and RAB-WC achieve the  rst positions for the rest example queries.
In most cases, RBM performs the worst, and RM performs moderately.
To sum up, we can conclude that the proposed RAB-WCC method can predict the advertisers  bid strategies with the best accuracy among all the models under investigation.
To further test the performance of our model, we apply it to the tasks of click number prediction and revenue prediction.11 We compare our model with two state-of-the-art models on these tasks.
The  rst baseline model is the Structural Model in Sponsored Search [2], abbreviated as SMSS-1.
The second baseline model is the Stochastic Model in Sponsored Search [17], abbreviated as SMSS-
pected expenditure for each advertiser by considering some uncertainty assumptions on sponsored search marketplace.
SMSS-2 assumes that all the advertisers  bids are independent and identically distributed and they learn the distribution by mixing all the advertisers  historical bids.
We use the relative error and absolute error as compared to the real click numbers and revenue in the test period as the evaluation metrics.
Speci cally, suppose the value output by the model and the ground truth value are   and   respectively, then the absolute error and the relative error are calculated as |     | and |     |/  respectively.
The performance of all the models under investigation are listed in Table 7.
According to the table, we can clearly see that RAB-WCC performs better than both SMSS-1 and SMSS-2.
The absolute errors on click number and revenue made by SMSS-1 are very large as compared to the other methods.
The relative errors made by SMSS-1 are larger than 50% for both click number and revenue prediction, which are not good enough for practical use.
The relative error made by SMSS-2 for revenue prediction is even larger than 80%.
In contrast, our proposed RAB-WCC method generates relative errors of no more than 20% for both click and revenue prediction (and the absolute errors are also small).
Although the results might need further improvements, a 20% prediction error has already provided quite good references for the search engine to make decision.
Besides the randomized bid strategy and the strategy of selecting the most frequently used bid, there are a number of works on advertiser modeling in the literature.
Early work studies some simple cases in sponsored search such as auctions with only two advertisers and auctions in which the advertisers adjust their bids in an alternating manner [1] [21] [18].
Later on, greedy methods were used to model advertiser behaviors.
For example, in the random greedy bid strategy [4], an advertiser chooses a bid for the next round of auction that maximizes his/her utility, by assuming that the bids of all the other advertisers in the next round will remain the same as in the previous round.
In the locally-envy free bid strategy [10] [16], each advertiser selects the optimal bid price that leads to a certain equilibrium called locally-envy free equilibrium.
In [6], the advertiser bid strategies are modeled using the knapsack problem.
Competitor-busting greedy bid strategy [22] assumes that an advertiser will bid as high as possible while retaining his/her desired ad slot in order to make the competitors pay as much as possible and thus exhaust their advertising resources.
Other similar work includes low-dimensional bid strategy [20], restricted balanced greedy bid strategy [4], and altruistic greedy bid strategy [4].
In [5], a model that predicts the bid strategy to be the best response is proposed by assuming the advertisers know all the competitors  bids in the previous auction.
In [8], a linear regression model is used base on a group of advertiser behavior features.
In addition, a bid strategy based on incremental cost per click is discussed in [19]
 cess based on those bids and made estimation on the revenue and clicks according to the simulation results.
Model


















 Table 5: Prediction performance on some example queries (NLL) Model car insurance disney ipad jcpenney medicare stock market










































 [2], which proves that an advertiser s utility is maximized when he/she bids the amount at which his/her value per click equals the incremental cost per click.12 However, please note that most of the above works assume that the advertisers have the same rationality and intelligence in choosing the best response to optimize their utilities.
Therefore they have signi cant difference from our work.
Actually, to the best of our knowledge, there is no work on advertiser behavior modeling that considers different aspects of advertiser rationality.
and thank Pingguang Yuan for his help on the data preparation for the experiments.
In the appendix, we discuss some properties of the proposed model.
Firstly, we give a theorem on the relationship of true value and bid.
Secondly, we give a theorem related to the estimation accuracy of the true value.
In this work, we have proposed a novel advertiser model which explicitly considers different levels of rationality of an advertiser.
We have applied the model to the real data from a commercial search engine and obtained better accuracy than the baseline methods, in bid prediction, click number prediction, and revenue prediction.
As for future work, we plan to work on the following aspects.
  First, in Section 2.1, we have assumed that the auctions for different keywords are independent of each other.
However, in practice, an advertiser will bid multiple keywords simultaneously and his/her strategies for these keywords may be dependent.
We will study this complex setting in the future.
  Second, we will study the equilibrium in the auction given the new advertiser model.
Most previous work on equilibrium analysis is based on the assumption of advertiser rationality.
When we change this foundation, the equilibrium needs to be re-investigated.
  Third, we will apply the advertiser model in the function modules in sponsored search, such as bid keyword suggestion, ad selection, and click prediction, to make these modules more robust against the second-order effect caused by the advertiser behavior changes.
  Fourth, we will consider the application of the advertiser model in the auction mechanism design.
That is, given the advertiser model, we may learn an optimal auction mechanism using a machine learning approach.
We thank Wei Chen, Tao Qin, Di He, Wenkui Ding, and Xinxin Yang for their valuable suggestions and comments on this work,
 cost of additional clicks received at a better ad slot.
We discuss about the relationship between true value vl and our predicted bid strategy.
Note that we will mainly focus on the results from the capability function because both willingness and compromise functions are not effected by the true value vl according to their de nitions.
For this purpose, by setting Wl = 1 and Cl as the identity function in  l, we de ne: F (vl) = E o l {arg max   (l) l ( j (vl   o ls l/sl))} E(vl) = (b1, b2, ..., bB)(F (vl))T Here F (vl) is a B-dimension strategy vector and E(vl) is the average bid of the strategy F (vl).
Under a very common assumption that ad position effect  j decreases with the slot index j, Theorem 2 shows that an advertiser with a higher true value will generally set a higher bid to optimize the utility, which is consistent to the intuition.
This conclusion shows the consistency of our model in the capability part.
THEOREM 2.
Assume  j decreases in j, then E(vl) is monotone nondecreasing in vl.
PROOF.
To prove E(vl) is monotone nondecreasing, we only need to prove that  o l and   > 0, (b1, b2, ..., bB)(arg max   (l) l   (b1, b2, ..., bB)(arg max   (l) l ( j (vl(1 +  )   o ls l/sl))) ( j (vl   o ls l/sl))), (4) and then the   will keep unchanged in the expectation of o l.
We denote j  and j0 as the best rank of l for the cases that true values are vl(1 +  ) and vl respectively.
Here o l is  xed and  best rank  means the rank that leads to the optimal utility.
Model car insurance disney ipad jcpenney medicare stock market













































 We denote  l  and  l0 as the advertisers who rank at (j  + 1) and (j0 + 1) respectively.
Note that for a  xed o l, j  and j0 can be different due to different true value of l. If we are able to prove j    j0, then the inequality (4) will be valid since a nondecreasing best ranking yields a nondecreasing best bid strategy.
As j0 is the best rank for the true value vl, we have,  j0 (vl   o  l0 s  l0 /sl)    j  (vl   o l  s l  /sl).
Assuming j  < j0, we have,  j0 vl  >  j  vl .
By adding (3) and (4), we got, (5) (6) s l0 /sl) >  j  (vl(1 +  )   o l   j0 (vl(1 +  )   o l0 This equation reveals that j0 is a better rank than j  and j  should not be the best rank for the true value vl(1 +  ), which is contradictive to the de nition of j .
Therefore, the assumption j  < j0 is not valid, which also  nishes our proof of this theorem.
/sl).
s l  Table 7: Prediction performance in applications Model Relative Error (Click) Absolute Error (Click) Relative Error (Revenue) Absolute Error (Revenue)













 PROOF.
Since a change of  argmax   is equivalent to a change (l)   l of  BRo l  , we consider the critical point that the increase of   makes the best rank transfer exactly from j0 to j  (j0 6= j ).
Thus we have:  j (j  6= j0), s.t.
j , j0 maximizes  j (vl(1+  )   o ls l/sl) simultaneously, and then we can get,  j  (vl(1+ ) o l  From equation (7) we have, s l  /sl) =  j0 (vl(1+ ) o  l0 s  l0 /sl).
(7)  j0 (vl   o l0   = s l0 /sl)    j  (vl   o l  vl( j     j0 ) s l  /sl) .
(8)
 Assume there is a  0 such that As discussed in Section 4, we choose the model in [9] for the true value prediction.
Usually, the estimation is not perfect and there might be some errors.
Fortunately, we can prove a theorem which guarantees that the solution of this model will keep accurate if the estimation errors are not very large.
This holds true because the payment rule of GSP is discrete and it allows the small-scale vibration of true value.
Before introducing the theorem, we give some notations  rst.
For a  xed o l and true value vl, l s best rank is denoted as BRo l (Best Rank), the optimal utility is denoted as BUo l , and the ranking score of  l (the one ranked next to l) in the optimal case is denoted BSo l .
To describe the theorem, we also denote the second optimal utility as SUo l (Second Utility), which is the largest utility less than BUo l in the  xed o l.
THEOREM 3.
We assume that  j decreases in j and set   = SUo l BUo l ),   = maxo l(BRo l ), and   = maxo l (BSo l ), maxo l ( (vl    /sl > 0).
Let vl increase by  vl (    R), then F (vl) will keep unchanged if | |     ), where  1 is the  1 CTR at the  rst position.
(1    )(1     slvl  j  (vl   o l  s l  /sl) =  0 j0 (vl   o l0 s l0 /sl).
(9) Then equation (8) is transformed as,   = (1    0) j0 (vl   o l0 vl( j     j0 ) s l0 /sl) = (1    0)  j0  j     j0 (1   o l0 s l0 vl /sl ).
(10) Considering j0 is the best rank, from equation (9) we have,  0 =  j  (vl   o l   j0 (vl   o l0 s l  s l0 /sl) /sl)   SUo BUo  l  l     < 1.
(11) In addition, there holds j0   maxo l (BRo l ) =  ,    1  j     j0  j0  j0    , | | > , o l0 s l0 = BSo l   maxo l(BSo l ) =  .
(12) (13) In order to prove the bound of   keeps F (vl) unchanging, we According to (10) and (11),(12),(13), we  nally have, prove the following lemma instead.
The proof of Theorem 3 will be  nished at once after we sum up This ends our proof of Lemma 1.
all the cases of o l in Lemma 1.
LEMMA 1.
If   satis es | |      1 (1 )(1    slvl  j(vl(1+ ) o ls l/sl) = argmax  j (vl  (l)   l we have, argmax o ls l/sl).
(l)   l ), then  o l | | = |1    0|  j0 | j     j0 | (1   o l0 s l0 slvl ) >    1 (1    )(1     slvl ).
As   is the critical point, for any  xed o l, if | |      1 (1   " will keep unchanged.
