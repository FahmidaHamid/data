Short texts are prevalent on the Web, no matter in traditional Web sites, e.g.
Web page titles, text advertisements and image captions, or in emerging social media, e.g.
tweets, status messages, and questions in Q&A websites.
Uncovering the topics of such short texts is crucial for a wide range of content analysis tasks, such as content characterizing [26, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
35, 14], user interest pro ling [32], emerging topic detect ing [20] and so on.
However, unlike the traditional normal documents (e.g.
news articles and academic papers), the lack of rich context in short texts makes the topic modeling a challenging problem.
Conventional topic models, like PLSA [16] and LDA [3], are widely used for uncovering the hidden topics from text corpus.
In general, documents are modeled as mixtures of topics, where a topic is a probability distribution over words.
Statistical techniques are then utilized to learn the topic components and mixture coe cients of each document.
In essence, the conventional topic models reveal the latent topics within the text corpus by implicitly capturing the document-level word co-occurrence patterns [5, 30].
Therefore, directly applying these models on short texts will su er from the severe data sparsity problem (i.e. the sparse word co-occurrence patterns in each short document) [17].
More speci cally, 1) the occurrences of words in short document play less discriminative role compared to lengthy documents where the model has enough word counts to know how words are related [17] ; 2) The limited contexts make it more dif- cult for topic models to identify the senses of ambiguous words in short documents.
One simple but popular way to alleviate the sparsity problem is to aggregate short texts into lengthy pseudo-documents before training a standard topic model.
For example, Weng et al. [32] aggregated the tweets published by individual user into one document before training LDA.
Besides the user-based aggregation, Hong et al. [17] also aggregated the tweets containing the same word, and shown that topic models trained on these aggregated messages work better than the regular LDA.
However, such heuristic data aggregation methods are highly data-dependent.
For example, the user information is not always available in some datasets, like the collection of Web page titles or advertisements.
Even if the user information is available, e.g.
in tweets data, most users only have few tweets which makes the aggregation less e ective.
Another way to deal with the problem is to make stronger assumptions on the data.
A typical way is to assume that a short document only covers a single topic.
For example, Zhao et al. [35] modeled each tweet in the way of mixture of unigrams [23].
Similar approach can be found in [12], where words in each sentence are assumed to be drawn from the same topic.
Compared to LDA and PLSA, the simpli- ed data generation process may help alleviate the sparsity problem in short texts.
However, it loses the  exibility to capture di erent topic ingredients in one document, and suf-1445fers from over tting issues due to the peaked posteriors of topics P(z|d) [3].
Unlike these approaches, in this paper, we propose a novel topic model for short texts to tackle the sparsity problem.
The main idea comes from the answers of the following two questions.
1) Since topics are basically groups of correlated words and the correlation is revealed by word co-occurrence patterns in documents, why not explicitly model the word co-occurrence for topic learning?
2) Since topic models on short texts su er from the problem of severe sparse patterns in short documents, why not use the rich global word co-occurrence patterns for better revealing topics?
Speci cally, we propose a generative biterm topic model (BTM), which learns topics over short texts by directly modeling the generation of biterms in the whole corpus.
Here, a biterm is an unordered word-pair co-occurred in a short context.
The data generation process under BTM is that the corpus consist of a mixture of topics, and each biterm is drawn from a speci c topic.
Compared with conventional topic models, the major di erences and advantages of BTM lie in that 1) BTM explicitly models the word co-occurrence patterns (i.e. biterms), rather than documents, to enhance the topic learning; and 2) BTM uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse patterns at document-level.
By learning BTM, we can obtain the topic components and a global topic distribution of the corpus, except the topic distribution of each individual document as it does not model the document generation process.
However, we show that the topic distribution of each document can be naturally derived based on the learned model.
We conduct extensive experiments on two real-world short text collections, i.e. the datasets from Twitter and a Q&A website.
Experimental results show that BTM can discover more prominent and coherent topics than the baseline methods.
Quantitative evaluations con rm the superiority of BTM on several evaluation metrics.
Additionally, we also test our approach on a normal text collection, i.e. 20News-group.
It is surprising for us to  nd that BTM can outperform LDA even on normal texts, showing the potential generality and wider usage of the new topic model.
The rest of the paper is organized as follows: in Section
 duces our model for short text topic modeling, and discuss its implementation in Section 4.
Experimental results are presented in Section 5.
Finally, conclusions are made in the last section.
In this section, we brie y summarize the related work from the following two perspectives: topic models on normal texts, and that on short ones.
Topic models have been proposed to uncover the latent semantic structure from text corpus.
The e ort of mining semantic structure in a text collection can be dated from latent semantic analysis (LSA) [9], which utilizes the singular value decomposition of the document-term matrix to reveal the major associative words patterns.
Probabilistic latent semantic analysis (PLSA) [16] improves LSA with a sounder probabilistic model based on a mixture decomposition derived from a latent class model.
In PLSA, a document is presented as a mixture of topics, while a topic is a probability distribution over words.
Extending PLSA, Latent Dirichlet Allocation (LDA) [3] adds Dirichlet priors on topic distributions, resulting in a more complete generative model.
Due to its nice generalization ability and extensibil-ity, LDA achieves huge success in text mining domain.
In the last decade, topic models have been extensively studied.
Many more complicated variants and extensions of LDA and PLSA have been proposed, such as the author-topic model [27], Bayesian nonparametric topic model [29], and supervised topic model [2].
Among them two works close to us are the recently proposed regularized topic model [22] and the generalized P olya model [21], which also employ word co-occurrence statistics to enhance topic learning.
However, both of them utilize word co-occurrences as structure priors for topic-word distribution, rather than directly modeling their generation process.
Above all, almost all the models mentioned above deal with normal text without considering the speci city of short texts.
Early studies mainly focused on exploiting external knowledge to enrich the representation of short texts.
For example, Sahami et al.[28] suggested a search-snippet-based similarity measure for short texts.
Phan et al.[24] learned hidden topics from large external resources to enrich the representation of short texts.
Jin et al.[19] learned topics on short texts via transfer learning from auxiliary long text data.
These ways may be helpful in some speci c domains, but not general since favorable external dataset might not be always available.
Additionally, these approaches and ours are complementary rather than competitive.
With the emergence of social media in recent years, topic models have been utilized for social media content analysis in various tasks, such as content characterizing [26, 35], event tracking [20], content recommendation [25, 8], and in uen-tial users prediction [32].
However, due to the lack of spe-ci c topic models for short texts, some researchers directly applied conventional (or slightly modi ed) topic models for analysis [26, 31].
Some others tried to aggregate short texts into lengthy pseudo-documents based on some additional information, and then train conventional topic models [32,
 of topic modeling in Twitter, and suggested that new topic models for short texts are in demand.
In our previous works, we developed methods based on non-negative matrix factorization for short text clustering [34] and topic learning [33] by exploiting global word co-occurrence information.
This work extends them by proposing a more principle approach to model topics over short texts.
To the best of our knowledge, the proposed topic model is the  rst one focusing on general-domain short texts, which does not exploit any external knowledge.
Conventional topic models learn topics based on document-level word co-occurrence patterns, whose e ectiveness will be highly in uenced in short text scenario where the word co-occurrence patterns become very sparse in each document.
To tackle this problem, here we propose a novel biterm topic model, which learns topics over short texts by directly modeling the generation of all the biterms (i.e. word co-occurrence patterns) in the whole corpus.
and mixture of unigrams, BTM models the generation procedure of biterms in a collection, rather than documents.
For clarity, the  xed hyperparameters  ,   are not presented.
Without loss of generality, topics are represented as groups of correlated words in topic models, while the correlation is revealed by word co-occurrence patterns in documents.
For example, if the words  apple ,  iphone ,  ipad  and  app  frequently co-occur with each other in the same contexts, we can identify that they belong to a same topic (i.e. apple company and its products).
Conventional topic models implicitly capture such word co-occurrence patterns by modeling word generation from the document level.
Di erent from those approaches, our BTM directly models the word co-occurrence patterns based on biterms.
A biterm denotes an unordered word-pair co-occurring in a short context (i.e. an instance of word co-occurrence pattern).
Here the short context refers to a proper text window containing meaningful word co-occurrences.
In short texts, since documents are usually short and speci c, we just take each document as an individual context unit.
We extract any two distinct words in a short text document as a biterm.
For example, in the short text document  I visit apple store. , if we ignoring the stop word  I , there are three biterms, i.e.  visit apple ,  visit store ,  apple store .
The biterms extracted from all the documents in the collection compose the training data of BTM.
The key idea of BTM is to learn topics over short texts based on the aggregated biterms in the whole corpus to tackle the sparsity problem in single document.
Speci cally, we consider that the whole corpus as a mixture of topics, where each biterm is drawn from a speci c topic indepen-dently1.
The probability that a biterm drawn from a speci c topic is further captured by the chances that both words in the biterm are drawn from the topic.
Suppose   and   are the Dirichlet priors.
The speci c generative process of the corpus in BTM can be described as follows:
 (a) draw a topic-speci c word distribution  z   Dir( )
 collection
 same word occurrence are not independent.
This simpli ed assumption facilitate the computation by considering BTM as a model built upon a biterm set.
(a) draw a topic assignment z   Multi( ) (b) draw two words: wi, wj   Mulit( z) Following the above procedure, the joint probability of a biterm b = (wi, wj) can be written as: P (b) = = P (z)P (wi|z)P (wj|z).
 z i|z j|z (cid:2) (cid:2) z z (1) (2) Thus the likelihood of the whole corpus is: (cid:3) (cid:2)
  z i|z j|z (i,j) z We can see that, here we directly model the word co-occurrence pattern, rather than a single word, as an unit conveying semantics of topics.
No doubt the co-occurrence of a pair of words can much better reveal the topics than the occurrence of a single word, and then enhance the learning of topics.
Moreover, all the biterms from the whole corpus, rather than from a single document, are aggregated together for the topic learning.
Therefore, we can fully leverage the rich global word co-occurrence patterns to better reveal the latent topics.
For better understanding the uniqueness of BTM from conventional topic models, here we make a comparison between BTM and two typical models for topic learning, i.e. LDA and mixture of unigrams.
Figure 1 illustrates the graphical representation of the three models.
We can see, in LDA each document is generated by  rst drawing a document-level topic distribution  d, and then iteratively sampling a topic assignment z for each word w in the document.
LDA implicitly captures the document-level word co-occurrence patterns since the topic assignment variable z of each word depends on other words in the same document through sharing the same document-level topic distribution  d.
Hence, when documents are short, LDA will su er from the sparsity problem due to its excessive reliance on local observations for the inference of word topic assignment z, which in turn hurts the learning of topics  .
Di erent from LDA, mixture of unigrams draws the topic assignment z for each document from a corpus-level topic distribution  . Leveraging the information of the whole corpus, it alleviates the sparsity problem in topic inference, 1447which in turn helps the learning the topic components  .
However, mixture of unigrams assumes that all the words in a document are sampled from the same topic.
This assumption is so strong that it prevents the model from modeling  ne topics in documents.
As we can see, even in short texts, there might be multiple topics in one document.
BTM, shown in Figure 1(c), overcomes the data sparsity problem of LDA by drawing topic assignment z from the corpus-level topic distribution   as mixture of unigrams does.
Meanwhile, it also surmounts the disadvantage of mixture of unigrams by breaking documents into biterms.
In this way, BTM not only can keep the correlation between words, but also can capture multiple topic gradients in a document, since the topic assignments of di erent biterms in a document are independent.
A major di erence between BTM and conventional topic models is that BTM does not model the document generation process.
Therefore, we cannot directly obtain the topic proportions of documents during the topic learning process.
To infer the topics in a document, we assume that the topic proportions of a document equals to the expectation of the topic proportions of biterms generated from the document: P (z|d) = P (z|b)P (b|d).
(3) In Eq.
(3), P (z|b) can be calculated via Bayes  formula based on the parameters estimated in BTM: b (cid:2) P (z|b) = P (z)P (wi|z)P (wj|z) (cid:4) z P (z)P (wi|z)P (wj|z) , where P (z) =  z, and P (wi|z) =  i|z.
Then the remaining problem is how to obtain P (b|d).
Here we simply take the empirical distribution of biterms in the document as the estimation P (b|d) = nd(b)(cid:4) b nd(b) , where nd(b) is the frequency of the biterm b in the document d. In short texts, P (b|d) is nearly an uniform distribution over all biterms in the document d. Despite of its simplicity, we  nd this estimation always obtains good results in practice.
More sophisticated ways may be studied in the future work.
In this section, we describe the algorithm to infer the parameters { ,  } in BTM, and compare its complexity with

 Similar as LDA, inference cannot be done exactly in BTM.
Hence, we adopt Gibbs sampling to perform approximate inference.
Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo algorithm.
Compared to other inference methods for latent variable models, like vari-ational inference and maximum posterior estimation, Gibbs sampling has two advantages.
First, it is in principal more accurate since it asymptotically approaches the correct distribution.
Second, it is more memory-e cient since it only requires to maintain the counters and state variables, mak-Algorithm 1: Gibbs sampling algorithm for BTM Input: the number of topics K, hyperparameters  ,  , biterm set B Output: multinomial parameter   and   initialize topic assignments randomly for all the biterms for iter = 1 to Niter do for b   B do draw zb from P (z|z b, B,  ,  ) update nz, nwi|z, and nwj|z compute the parameters   in Eq.
(5) and   in Eq.
(6) ing it preferred for large-scale dataset.
More detailed comparison of these methods can be found in [1].
The basic idea of Gibbs sampling is to estimate the parameters alternatively, by replacing the value of one variable by a value drawn from the distribution of that variable conditioned on the values of the remaining variables.
In BTM, we need to sample all the three types of latent variables z,   and  .
However, with the technique of collapsed Gibbs sampling [10],   and   can be integrated out due to the conjugate priors   and  . Consequently, we only have to sample the topic assignment for each biterm from its conditional distribution given the remaining variables.
To perform Gibbs sampling, we  rst choose initial states for the Markov chain randomly.
Then we calculate the conditional distribution P (z|z b, B,  ,  ) for each biterm b = (wi, wj), where z b denotes the topic assignments for all biterms except b, B is the global biterm set.
By applying the chain rule on the joint probability of the whole data, we can obtain the conditional probability conveniently: (nwi|z +  )(nwj|z +  ) P (z|z b, B,  ,  )   (nz +  ) (cid:4) (4) , ( w nw|z + M  )2 where nz is the number of times of the biterm b assigned to the topic z, and nw|z is the number of times of the word w assigned to the topic z.
Following the conventions of LDA, here we use symmetric Dirichlet priors   and  .
Note that once a biterm b is assigned to the topic z, the two words wi and wj in it will be assigned to the topic simultaneously.
Finally, with the counters of the topic assignments of biterm and word occurrences, we can easily estimate the topic-word distributions   and global topic distribution   as:  w|z =  z = (cid:4) nw|z +   w nw|z + M   nz +   |B| + K  , where |B| is the total number of biterms.
, (5) (6) An overview of the Gibbs sampling procedure we use is shown in Algorithm 1.
Due to space limitation, we omit the detailed derivation of it.
The major time consuming part in the Gibbs sampling procedure of BTM is evaluating the conditional probability in Eq.
(4) for all the biterms, with time complexity O(K|B|).
During the entire process, we need to keep the counters nz, nw|z, and the topic assignment z for each biterm, in total of (K + M K +|B|) variables in memory.
Note that in LDA, we
 ables need to be maintained in Gibbs sampling implementation of LDA, mixture of unigrams, and
 method time complexity O(K|D| l)
 #variables |D|K + M K + |D|l


 Table 2: Time cost (seconds) per iteration of BTM and LDA on Tweets2011 collection.
38.07s BTM 128.64s 74.38s 250.07s 108.13s 362.27s 143.47s 476.19 s 178.66s 591.24s
 (cid:4) i li/|D| need to draw topic assignment for every word occurrence in documents, which costs time O(K|D| l), where  l = is the average length of documents in the collection.
For memory cost, LDA has to maintain the counters nz|b, nw|z, and the topic assignment z for each word occurrences[15], in total of (|D|K + M K + |D|l) variables.
Table 1 lists the time complexity and variables required to be maintained in the Gibbs sampling procedure of LDA, and BTM.
LDA, we approximately rewrite |B| as2: |B|   |D| l( l   1) To compare the time and memory cost between BTM and .
We can see the time complexity of BTM is about ( l   1)/2 times of LDA.
In short texts, the average length of documents are very small, e.g.
 l = 5.21 in the Tweets2011 collection, thus the run-time of BTM is still comparable with LDA.
However, for very large dataset and a large topic number K, LDA is susceptible to memory problems owing to a huge value of |D|K.
Table 2 shows the average run-time (per iteration) of BTM and LDA in our experiments on the Tweets2011 collection.
We  nd the run-time of BTM is always about 3 times of LDA for di erent topic number K. Table 3 shows the overall memory cost of BTM and LDA in the same collection.
We  nd that memory required by LDA rapidly increases as the topic number K grows, which costs more than 10 times of memory than BTM when K is larger than 200.
As opposed to LDA, memory required by BTM grows very slowly.
With further investigation, we  nd the major part of memory in BTM is used to store the biterms in training dataset.
Therefore, BTM is a better choice for large dataset and a large topic number K, when the memory cost is a bottleneck.
In this section, we conduct experiments on real-world short text collections to demonstrate the e ectiveness of our proposed approach.
We take two typical topic models as our baseline methods, namely LDA and mixture of unigrams.
All the experiments were carried on a Linux server with Intel Xeon 2.33 GHz CPU and 16G memory.
Both BTM
 biterms.
Here we simply take all the documents as with the same length, since the variance of the length of short documents is not large.
Table 3: Memory cost (m) per iteration of BTM and LDA on Tweets2011 collection.
LDA 3177m 5524m 7890m 10218m 12561m BTM 927m 946m 964m 1002m 984m and mixture of unigrams were implemented via C++ code3.
For LDA, we used the open-source implementation Gibb-sLDA++4.
Parameters were tuned via grid search: for LDA,   = 0.05 and on short text collections, and   = 50/K on the normal text collection,   = 0.01; for BTM and mixture of unigrams,   = 50/K and   = 0.01.
In all the methods, Gibbs sampling was run for 1,000 iterations.
The results reported are the average over 10 runs.
One typical way for topic model evaluation is to compare the perplexity or marginal likelihood on a held-out test set [3, 11, 12].
However, since BTM not models the generation process of documents, these measures are not available for us.
Moreover, these measures do not re ect the topic quality rightly [6].
Therefore, we evaluate the performance of BTM on topic modeling on some other task-dependent metrics.
To verify the e ectiveness of BTM on short texts, we carried experiments on a standard short text collection, namely Tweets20115.
It was published in TREC 2011 microblog track, which provides approximately 16 million tweets sampled between January 23rd and February 8th, 2011.
Besides the complete content of tweets, it also includes an user id, and a timestamp for each tweet.
To reduce low-quality tweets, we processed the raw content via the following normalization steps: (a) removing non-Latin characters and stop words; (b) converting letters into lower case; (c) removing words with document frequency less than 10; (d)  ltering out tweets with length less than 2; (e) removing duplicate tweets.
At last, we left 4,230,578 valid tweets, 98,857 distinct words, and 2,039,877 users.
The average document length is 5.21.
We compared BTM with three topic modeling methods on this short texts collection: (a) the standard LDA, which takes each tweet as a document; (b) LDA-U, which aggregates all the tweets from a user to a big psudo-document before training LDA; (c) mixture of unigrams (denoted as Mix), which assumes each tweet only exhibits a single topic.
In this collection, we set the number of topics K = 50 for all the methods.
To investigate the quality of topics discovered by all the test methods, we  rst sample some topics for visualization.
Following [7], we randomly drew two topics shared by the topic sets discovered by the four methods.
The selection process is described as follows.
Firstly, we collected the top 5 words in each topic into a topical word set for each method individually.
Then we randomly chose two terms (i.e., job and snow) from the intersection of the four topical word sets.
For each topic, besides the top 20 words, which are
 4http://gibbslda.sourceforge.net/ 5http://trec.nist.gov/data/tweets/ 1449most representative for a topic, we also listed 20 non-top words (i.e. ranked from 1001 to 1020) ordered by P (w|z).
Ideally, a high quality topic should be coherent as much as possible.
Hence, it is expected that the non-top words should be relevant to the top words in the same topic.
Table 4 presents the top words ( rst row) and non-top words (second row) of the topic selected by the word  job .
We  nd the two words  job  and  jobs  are ranked highest by all the four methods.
However, in LDA, some other words, like  web ,  website , and  google , are more related to a topic about website, rather than job.
The results in LDA-U and mixture of unigrams seem a little better than LDA, but still include a few of less relevant words like  website  and  www .
While in BTM, the top 20 words are more prominent and precise about  job .
In the non-top words, we  nd LDA includes the least words about  job , which is hard to connect them to the top words.
On the contrary, BTM includes more relevant words about  job  than others, suggesting this topic discovered by BTM is more coherent.
Table 5 presents the top words ( rst row) and non-top words (second row) of the topic selected by another word  snow .
In the  rst row, again we can see that the top words in LDA are mixed with words about two di erent subjects  weather  and  car .
The results in LDA-U is similar to LDA, but more about  weather .
In contrast, the top words in mixture of unigrams and BTM clearly describe weather.
In the second row, both LDA and LDA-U list words almost have no connection to  snow , while some of them are related to  car .
For mixture of unigrams, it is hard to explain the topic based on these non-top words.
In BTM, there are still many words about  weather , like  temperature  and  cyclone .
Besides the two topics presented here, we also  nd similar phenomenon in remaining topics, which suggests that the topics discovered by BTM are is more prominent and coherent than the three baselines.
In order to perform more comprehensive analysis, we utilize an automated metric, namely coherence score, proposed by Mimno et al [21] for topic quality evaluation.
Given a topic z and its top T words V (z) = (v(z) T ) ordered by P (w|z), the coherence score is de ned as: 1 , ..., v(z) C(z; V (z) ) = T(cid:2) t(cid:2) t=2 l=1 log D(v(z) l m , v(z) D(v(z) l ) ) + 1 , where D(v) is the document frequency of word v, D(v, v(cid:2) ) is the number of documents words v and v(cid:2) co-occurred.
The coherence score is based on the idea that words belonging to a single concept will tend to co-occur within the same documents.
It is empirically demonstrated that the coherence score is highly correlated with human-judged topic coherence.
It must be stressed that the coherence score only is appropriate for measuring frequent words in a topic.
Because the frequency of rare words is less reliable.
To evaluate the overall quality of a topic set, we calculated k C(zk; V (zk)), for the average coherence score, namely 1
 each method.
The result is listed in Table 6, where the number of top words T ranges from 5 to 20.
We  nd the result is in agreement with previous qualitative analysis.
BTM receives the highest coherence score in all the settings, and the superiority is statistically signi cant (P-value < 0.01 by T-test).
Both LDA-U and mixture of unigrams outperform LDA slightly, but the di erences are not signi cant.
(cid:4) Table 6: Average coherence score on the top T words (ordered by P (w|z)) in topics discovered by LDA, LDA-U, mixture of unigrams, and BTM.
A larger coherence score means the topics are more coherent.
It suggests that BTM outperforms others sig-ni cantly (P-value < 0.01 by t-test).
 236.4   2.0  1015.7   5.9  55.0   0.4

  234.8   1.1  1009.4   4.4  233.0   1.4  1007.6   6.7  53.8   0.1 Mix


 Table 7: Hashtags used for evaluation, not including the pre x  # .
jan25 superbowl sotu wheniwaslittle mobsterworld jobs agoodboyfriend bieberfact glee lfc rhoa itunes thegame celebrity tcyasi americanidol cancer socialmedia jerseyshore photography jp6foot7remix factsaboutboys meatschool libra android sagittarius thissummer tn sherman sagawards ausopen bears weather jaejoongday skins bfgw fashion pandora realestate teamautism travel nba football marketing design oscars food dating kindle snow obama
 In the Tweets2011 collection, there is no category information for tweets.
Manual labeling might be di cult due to the incomplete and informal content of tweets.
Fortunately, some tweets are labeled by their authors with hashtags in the form of  #keyword .
By investigating the data, we  nd there are mainly three types of usage of hashtags: (a) marking events or topics; (b) de ning the types of content, like  #ijustsayin ,  #quote ; (c) realizing some speci ed functions, like  #fb  means importing the tweet to Facebook in the meanwhile.
In our case, only the  rst type of hashtags are useful.
Therefore, we manually chose 50 frequent hashtags in type (a), listed in Table 7.
Since each hashtag in Table 7 denotes a speci c topic labeled by its author, we organized documents with the same hashtag into a cluster.
The following evaluation is based on the fact that these clusters should have low intra-cluster distances and high inter-cluster distances.
Considering topic models as a type of dimension reduction methods, each document can be represented by a vector of posterior distribution of topics: di = [p(z1|di), ..., p(zk|di)].
(7) Then we can measure the distance of two documents by the Jensen Shannon divergence: dis(di, dj) = DKL(di||m) + DKL(dj||m),



 (cid:4)
 where m = 1 is the Kullback Leibler divergence.
Given a set of clusters C = {C1, ..., CK}, we introduce two distance scores i pi ln pi qi Average Intra-Cluster Distance: IntraDis(C) = K(cid:2) k=1

     (cid:2) di,dj Ck i(cid:4)=j     2dis(di, dj) |Ck||Ck   1|
 while the second row lists non-top words ranked from 1001 to 1020 based on P (w|z).
job jobs business web website google design online marketing site blog project manager search www company service sales services post nonpro t gallery announced presence published converting select reps requirement mgr territory recruiters power involved announce poster larry dynamics feeds bristol
 job jobs design manager project web website site business service company hiring www support sales services london blog senior engineer expertise unemployed med iii host educational fort tags apps assignments labor introduction leads github assurance avon manchester starting automotive table Mixture of unigrams jobs job business marketing social media online web design website manager blog project seo internet sales tips company site hiring understand rep industrial sustainability rankings scholarships stay single campus extra cheap 101 vp relationships beginners colorado compliance face winning mechanical
 jobs job manager business sales hiring service services project company senior engineer management marketing nurse o ce assistant center customer development spring eld mlm recruit oil req unemployment processing overview awards recruiters ict  nish entrepreneur comp assist 1000 alliance locations patent auditor Table 5: Topics selected by the word  snow  on the Tweets collection.
The  rst row lists the top 20 words, while the second row lists non-top words ranked from 1001 to 1020 based on P (w|z).
snow car weather cold drive storm winter ice road bus driving rain ride tra c cars safe closed due warm train western dmv covering a4 push pulling milwaukee remains pace idiots 95 commuter buick owner cta transmission cyclist  urries camping tyre
 snow weather cold winter ice storm rain stay warm due car closed coming spring drive tra c safe sun blizzard city locations sunset drizzle mississippi interstate residents portland students  replace letting yuck ton counties signal counting blankets pushed 3pm spring eld venture Mixture of unigrams snow weather cold storm winter ice rain warm degrees stay sun spring safe blizzard coming wind cyclone chicago freezing inches australian thankful station stops groundhogday possibly cleveland traveling sidewalk covering predicting ten grass meant double a ect zoo schedule blew causing
 snow cold weather early stay ready ice winter storm hour hours weekend warm late coming spring rain tired sun hot temperature cyclone warmth issued colder mood couch snows pre traveling polar outages umbrella  lled yawn outage  urries online gloves speed Average Inter-Cluster Distance:     (cid:2) di Ck
 Ck,Ck(cid:2) C InterDis(C) =
 (cid:2) k(cid:4)=k(cid:2)     (cid:2) dj Ck(cid:2) dis(di, dj) |Ck||Ck(cid:2)| The intuition is that if the average inter-cluster distance is small compared to the average intra-cluster distance, the topical representation of documents agrees well with human labeled clusters (via hashtag).
Therefore, we calculate the following ratio to evaluate the quality of one topical representation of documents as [4, 13]:
 IntraDis(C) InterDis(C) .
Given a set of di erent topical representations of documents, the best one is which minimizes the H score.
Table 8 shows the H score for all the test methods.
From the results, we can see that BTM preforms signi cantly better than other three methods (P-value < 0.001).
LDA-U outperforms LDA slightly, implying that aggregating tweets for individual users brings moderate bene t.
Although LDA dominates mixture of unigrams on normal texts, it is somehow surprising that the performance of mixture of unigrams outperforms LDA and LDA-U substantially in this short text collection.
It suggests that the data sparsity problem seriously a ects LDA and LDA-U, while less in uences mixture of unigrams and BTM.
However, the H score of mixture of ungirams is still much worse than BTM.
With some further analysis, we  nd the average intra-cluster distance of mixture of unigrams is extremely large, owing to its peaked posterior distribution of P (z|d).
In other words, mixture of Table 8: H score for di erent methods on the Tweets2011 collection, smaller value is better.
The signi cant levels(P-value by t-test) are denoted as
 Method



 Mix BTM 0.474   0.005 >Mix***>LDA-U***>LDA***
 Signi cant di erences H score
 unigrams fails to recognize the resemblance of many documents.
From the above results, we  nd the improvement of LDA-U over LDA is not so much as shown in [17].
An explanation for this di erence is that there are less tweets posted by an user in average in our dataset than theirs.
Figure 2 shows the proportions of users who posted certain number of tweets in the Tweets2011 collection, we  nd 63.3% of users posted one tweet, and only 2.1% of users posted more than 9 tweets.
Thus it is not strange that aggregating tweets for individual users has limited a ects.
In order to demonstrate the e ectiveness of our approach is domain-independent, we evaluated it on another short text collection, called Question collection.
This collection includes 648,514 questions crawled from a popular Chinese Q&A website6.
Each question has a category label assigned by its questioner, making it convenient for automatic evalu-6http://zhidao.baidu.com 1451s r e s u f o n o i t r o p o r









 Number of tweets posted




 Figure 2: Proportions of users who posted certain number of tweets in the Tweets2011 collection.
Figure 3: Classi cation performance of BTM, mixture of unigrams, and LDA on the Question collection.
ation.
For pre-process, we removed stop words and low frequency words (i.e. document frequency is less than 3).
The  nal collection contains 189,080 documents, 26,565 distinct words, and 35 categories.
The average length of documents is 3.94.
Note that in this collection, our baselines do not include LDA-U, since there is few users whole submitted more than one question.
We performed the evaluation based on document classi cation.
Considering topic model as a way for dimensionality reduction, which reduces a document to a  xed set of topical features P (z|d), we would like to see how accurate and dis-criminative of the topical representation of documents for classi cation.
We randomly split documents into training and test subsets with the ratio 4 : 1, and classi ed them by the linear SVM classi er LIBLINEAR7.
We reported the accuracy on 5-fold cross validation in Figure 3.
From the results, we can see that BTM always dominates the two baselines.
Moreover, the advantage of BTM becomes more notable as the topic number K grows.
That is because when the number of topics is small, topics discovered are usually very general.
In such case, a short document is more likely to belong to a single topic, thus the performance of BTM is close to mixture of unigrams.
In contrast, with the increase of the topic number K, we will learn more speci c topics.
However, mixture of unigrams is unable to capture the multiple topics exhibited in a document.
Thus the di erence between BTM and mixture of unigrams becomes larger.
At the same time, a large topic number will aggravate the data sparsity problem of LDA by introducing more parameters, thus the gap between BTM and LDA also increases.
Another important  nding is that mixture of un-igrams outperforms LDA all the time.
It suggests that LDA is not a good choice for short texts due to the data sparsity problem.
One may wonder the impact of training data size on these methods.
We randomly sampled di erent proportion of documents, from 0.2 to 1, to train and test these methods separately.
The results are shown in Figure 4.
We can see when the size of the training data grows, all the methods work better.
However, both BTM and mixture of unigrams achieve more improvement than LDA.
LDA only get close to mixture of unigrams on small training data.
It suggests that increasing the training data will not overcome the data sparsity problem in LDA, since the documents are still short.
Figure 4: Classi cation performance comparison with di erent data proportions on the Questions collection (K=40).
Comparing mixture of unigrams with BTM, we  nd BTM has stable superiority over mixture of unigrams no matter of the size of the training data.
In previous experiments, we have demonstrated the e ec-tiveness of BTM on short texts.
Although we propose BTM for the short text scenario, there is no limitation for our model to be applied on normal text collections.
Therefore, it is also interesting to see how e ective is BTM on normal text.
For this purpose, we compared BTM with LDA, one of most popular topic models, on a normal text collection.
The experiments were carried out on the 20Newsgroup collection8, a standard corpora including 18,828 messages harvested from 20 di erent Usenet newsgroups.
Each newsgroup corresponding to a di erent topic.
Table 9 lists the names of these newsgroups.
For pre-process, we removed stop words and words with frequency less than 3, but without stemming.
Finally, 42697 words are left.
We directly trained LDA on the original documents without any other processing.
Note that in BTM, we need to extract biterms from the collection.
This process is a little di erent from that in short texts.
Recall that a biterm is de ned as a word-pair co-occurred in a short context.
It is not appropriate to view a lengthy document as a single short context, since it may involve a wide range of topics.
In or-7http://www.csie.ntu.edu.tw/ cjlin/liblinear/

 Newsgroups collection (K = 20).
Table 9: The newsgroup names in the 20 Newsgroups collection No.
Newsgroup Name









 alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball No.
Newsgroup Name









 rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc der to reduce meaningless and noise biterms, the biterm set is constructed by extracting any two words co-occur within a context window with range no larger than a prede ned threshold r in each document.
For quantitative evaluation, we compare the clustering performance of BTM and LDA.
Document clustering evaluation is a direct way to measure the e ectiveness of a topic model without depending on any extrinsic methods.
For document clustering, we take each topic as a cluster, and assign each document d to the topic z with highest value of conditional probability P (z|d).
Note that we do not know the optimal context range threshold r ahead, therefore, we tested di erent values of it, and report their results together.
We adopt three standard metrics in clustering evaluation as follows.
Let   = { 1,  ,  K} be the set of output clusters, and C = {c1,  , cP} be P labeled classes of the documents.
  Purity.
Suppose documents in each cluster should take the dominant class in the cluster.
Purity is the accuracy of this assignment measured by counting the number of correctly assigned documents and divides by the total number of test documents.
Formally: purity( , C) =
 n K(cid:2) max j i=1 | i   cj|.
Note that when all the documents in each cluster are with the same class, purity is highest with value of 1.
Conversely, it is close to 0 for bad clustering.
  Normalized Mutual Information(NMI).
Let I( ; C) denotes the mutual information between the two partitions   and C, NMI penalized I( ; C) by their entropy H( ) and H(C) to avoid the value biasing to large number of clusters.
Formally:
 (cid:4)
 | i cj|
 (cid:4) = ( i,j | i| n log i n | i| n + | i||cj| n| i cj| |cj| n log log (cid:4) j |cj| n )/2 Note that NMI is 1 for perfect match between   and C, while 0 if the clustering is random with respect to class membership.
  Adjusted Rand Index(ARI)[18].
Consider documents clustering as a series of pairwise decisions.
If two documents both in the same class and the same cluster, or both in di erent classes and di erent clusters, the decision is considered to be correct, else false.
Rand index measures the percentage of decisions that are correct.
Adjusted Rand index is the corrected-for-chance version of Rand index, whose expected value is 0, while the maximum value is also 1 for exactly match.
(cid:11)|cj| (cid:12) (cid:11) (cid:12) ]/ (cid:12) (cid:11)|cj| (cid:12)(cid:4) (cid:11)| i cj| (cid:12) (cid:4) (cid:12)(cid:4) (cid:11)| i| (cid:4) (cid:11)| i|
 (cid:4) n
 i,j (cid:12)   [ (cid:4) (cid:12) (cid:11)|cj| ]   [ (cid:11)| i| (cid:4)
 i
 i i
 j
 j



 + j
 (cid:11) (cid:12) n
 ]/ The results are shown in Figure 5.
On the whole, it is clear that BTM outperforms LDA signi cantly when the context range threshold r is between 30 and 60, suggesting that BTM also performs very well on normal texts.
In particular, we  nd when r = 10, LDA works better than BTM, implying that the context information utilized by BTM is not enough.
As the context range threshold r increases, more word co-occurrence patterns are included, which improves the performance of BTM substantially.
However, the improvement slows down when the context range threshold r increases from 30 to 60.
An explanation for this behavior is that when the distance between two words increasing, they might be less relevant.
At this point, the assumption that the two words in a biterm have the same topic will be less credible.
Moreover, a larger context range threshold r will generate much more biterms, which increases the training cost.
Therefore, for both e ectiveness and e ciency consideration, the context range threshold r should not be too small or too large for normal texts in practice.
column denotes the cosine similarity of the two topics in a row.
ax max g9v b8f a86 1d9 pl 145 3t giz god jesus christ church bible people lord christian key encryption chip clipper keys government system window server display widget set application xterm  le space earth launch mission orbit shuttle system solar writes article don ca david uk wrote cs org ax 0d cx 145 ah 34u w7 mv scx uw people don fbi  re children koresh gun batf people don god writes make good point question people government president don make time american disease medical people patients don time writes good drive scsi mac bit card apple system monitor problem image jpeg  le graphics images  les color data format












 14 mail university information fax internet list email


 18 windows dos  le system  les run don os pc program

 car don writes cars good ve engine time 00 year team 10 game 55 play players games 20 1993 health men number 10 hiv april study homosexual armenian armenians people war muslims turkish  le entry output program build line printf char info
 ax max b8f g9v a86 145 1d9 pl 0t 3t god jesus bible christian church christ christians paul key encryption chip clipper government keys public window server set application sun display problem manager space earth nasa gov time system mission launch writes article university uk ca cs michael mail brian 0d cx ah w7 mv sp 17 uw scx air people writes gun fbi  re children article koresh people writes true don religion evidence question god president government people state states rights american medical health disease drug study drugs men cancer windows drive dos card mac system apple scsi disk  le image program  les bit jpeg color output line graphics ftp software data mail pub computer car cars armenian armenians engine muslims turkish 000 writes year play game good ca insurance scott team games
 don people ve time good ll make things thing doesn information group list book post questions read subject writes price buy sale problem cost power good interested sim




















 Here we study the quality of topics discovered by the two topic models.
In practice, a topic model which  nds topics with good readability and accurately re ecting the topical structure of data is preferred.
Table 10 presents all the topics learned by BTM and LDA, when the number of topics is set to 20.
These topics from the two methods are matched based cosine similarity using greedy algorithm.
For each topic we list its top words ordered by P (w|z).
We can see that the topics 1-16 in BTM and LDA are very similar.
Comparison Table 9 and Table 10, we  nd it is easy to identify the corresponding newsgroup of a topic in topics 1-16, except topic 1 and topic 7.
For example, topic 2 is with respect to the newsgroup  soc.religion.christian .
It suggests that both BTM and LDA uncover the inherent topical structure of the collection closely.
We also note that topics 17-20 in Table 10 are very di er-ent in BTM and LDA.
In BTM, we can still identify that topics 17-20 relate to the newsgroups  sci.med ,  comp.os.ms-windows.misc ,  talk.politics.mideast ,  comp.os.ms-window-s.misc  respectively.
But in LDA, topic 17 is about numeral, topic 18 is a set of common words, while topics 19 and 20 are with poor interpretability.
In our view, the di er-ences between the results of the two models are caused by the following reasons.
BTM explicitly model the word co-occurrences in local context, it well captures the short-range dependencies between words.
Conversely, LDA captures the long-range dependencies in documents [11], which are less speci c than short-range ones, resulting in the last four topics more common but less readable.
Topic modeling for short texts is an increasingly important task due to the prevalence of short texts on the Web.
Compared to normal documents, short texts lack of word frequency and context information, causing severe sparsity problems for conventional topic models.
In this paper, we propose a novel probabilistic topic model for short texts, namely biterm topic model (BTM).
BTM can well capture the topics within short texts as it explicitly models the word co-occurrence patterns and uses the aggregated patterns in the whole corpus.
We carried on experiments on two real-world short text collections and one normal text collection.
The results demonstrated that BTM not only can learn higher quality topics, but also more accurately capture the topics of documents than previous methods.
Besides, BTM is simple and easy to implement, and also scales up well.
All these bene ts makes BTM a practicable choice for content analysis on short texts in a wide range of applications.
To the best of our knowledge, we are the  rst to propose a topic model for general short texts.
However, there is still room to improve our work in the future.
For example, we would like to  nd more sophisticated way to estimate the distribution P (b|d), which is uniform in the current work for simplicity.
Moreover, it is also interesting to explore the usage of our model in various real-world applications, like content recommendation, event tracking, and short texts retrieval, etc.
This work is funded by the National Natural Science Foundation of China under Grant No.
61202213, 61203298, No.
China under Grants No.
2012CB316303.
We would like to thank the anonymous reviewers for their helpful comments.
