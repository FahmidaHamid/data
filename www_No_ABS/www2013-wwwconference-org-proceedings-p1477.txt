Online social networks such as Facebook, Twitter and LinkedIn have been gaining increasing popularity in recent years.
People usually form links to indicate friend or follow relationships.
But in some other social networks people can form both positive and negative links.
Positive links express trust, like or approval attitudes, whereas negative links indicate distrust, dislike or disapproval attitudes.
For a given directed link from user u to v in a social network, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
we de ne its sign to be positive (or negative) if it expresses a positive (or negative) attitude from u to v. We call such networks with both positive and negative links signed social networks.
Examples include Epinions1 whose users can express trust or distrust of others [18], Slashdot2 whose participants can declare others to be either  friends  or  foes  [11], and Wikipedia3 whose users can vote for or against the promotion of others to administrator status [2].
In some signed social networks, the attitude of a link can be easily determined based on user rating score, i.e., positive or negative.
But in some other cases such as in online forums or BBS, the existence of interactions (i.e., links) between two users can be easily observed while the speci c semantic attitudes of these links are not explicitly labeled, since they are usually expressed implicitly by user reviews or comments.
As a more general case, hyperlinks between webpages can also indicate agreement or disagreement with the target of the link, but the lack of explicit labels makes it very di cult to determine the attitude over these hyperlinks [20].
Manually analyzing and labeling the signs of links will be very expensive and ine cient.
A promising solution is to train a classi er for link sign prediction in the signed networks.
However for some signed social networks, especially the newly formed ones, the paucity of available signs makes it di cult to train a good classi er to predict unknown link signs.
How to reliably and e ciently predict the signs of links in signed social networks is an important and challenging problem.
Previous research [15] has shown that, the structural information is a powerful and reliable source for the purpose of link prediction in unsigned networks, when applied in a traditional machine learning framework.
Some examples of such structural information include the number of common neighbors, or other local neighborhood statistics.
As for the signed social networks, a recent work by Leskovec et al. [14] studies the edge sign prediction problem using the signed triad features and a logistic regression model.
While its major contribution is the connections to theories of balance and status in social psychology, the prediction model makes a very strong assumption on the input network: the signs of all links except the one to be predicted are known in advance.
This is not very practical in reality as it is very expensive to obtain the signs of all links except one in a large network, especially for newly formed networks.
Thus, in this work we study the edge sign prediction prob-1www.epinions.com 2slashdot.org 3www.wikipedia.org
 signed network, we are interested in predicting the signs of edges whose signs are unknown.
We call this network a target network.
We assume there is a very small amount of edge sign information in the target network as the training data, but the quantity is inadequate to train a good clas-si er.
This assumption holds for many newly formed and fast evolving networks.
Thus we consider to leverage another more mature signed social network, called a source network, which has abundant edge sign information.
The source network may have a di erent joint distribution of the edge instances and the class labels from the target network, perhaps because the source network is outdated or is from a di erent application.
But the source network is not completely useless.
There still exists a certain degree of similarity, e.g., similar degree distributions and diameters, or common properties, e.g., structural balance and social status [14], between the source and target networks.
For example, according to the structural balance theory, many signed networks follow a common principle that  the friend of my friend is my friend  and  the enemy of my friend is my enemy .
Thus our task is to leverage the sign information in both the source and target networks to train a good classi er.
This approach is known as transfer learning [19,
 While most existing transfer learning works focus on trans-actional data [1], image [21] and text [22], in which the data instances are represented in a prede ned d-dimensional feature space, a unique challenge in our transfer learning problem across two signed social networks is that there is no pre-de ned feature vector for the edge instances in the networks.
Therefore, the  rst step is to investigate how to construct generalizable features that can transfer knowledge from the source network to the target for edge sign prediction.
Specifically, we propose two types of features, i.e., explicit topological features which express the manifest properties of edge instances such as degree and triads, and latent topological features which capture the common patterns between the source and target networks for knowledge transfer.
With the extracted features, a straightforward solution is to simply combine the source and target training instances and treat them equally to learn a model.
However, due to the distributional di erence between the two networks, some training instances in the source network are very different from the target network, thus may cause test edges in the target network to be wrongly predicted and degrade the performance.
Therefore, we adopt a transfer learning algorithm with instance weighting similar to [3].
This algorithm borrows the AdaBoost learning idea which assigns and iteratively adjusts the weight of each training instance in the source and target networks.
This instance weighting mechanism can e ectively distinguish the more useful edge instances from the less useful ones in the source network and attach more importance to the former.
Our main contributions are summarized as follows.
  We formulate the problem of edge sign prediction in a signed social network which may have only a very small amount of labeled training instances.
We consider to exploit another network, called source network, which has abundant labeled instances.
The source and target networks may have di erent yet related joint distributions.
Our task is to leverage the source network instances for feature construction and model learning.
To the best of our knowledge, this is the  rst work on transfer learning across large signed social networks.
  We design the latent topological features which can capture the common structural patterns between the source and target networks, thus are generalizable features across domains.
The latent features are obtained by nonnegative matrix tri-factorization [4].
  We adopt an AdaBoost-like transfer learning algorithm with instance weighting to distinguish the more useful training instances from the less useful ones in the source network.
  We conducted extensive experiments on three real large signed networks and demonstrated that our transfer learning algorithm can improve the prediction accuracy by 40% over baseline schemes.
The rest of our paper is organized as follows.
We introduce related work in Section 2 and give the problem de nition in Section 3.
We describe our proposed features for the edge sign prediction problem in Section 4.
In Section 5 we propose an AdaBoost-like learning algorithm with instance weighting in the transfer learning framework.
Experimental results are presented in Section 6 to show the e ectiveness of our features and transfer learning algorithm.
Finally, we conclude our work in Section 7.
In this section we  rst introduce related studies on signed social networks, and then the state-of-the-art transfer learning research.
Signed social networks have attracted more and more attention since Guha et al. proposed their leading work on trust propagation in signed social networks [6].
Kunegis et al. did spectral analysis on signed networks [11, 12].
They revealed fundamental characteristics of signed networks by evaluating various measures in [11].
They also studied signed spectral clustering methods, signed graph kernels and network visualization methods in signed graphs [12].
For the edge sign prediction problem in signed graphs, existing studies can be categorized into two major approaches: a matrix kernel approach [11] and a machine learning approach [14].
Kunegis et al. exploited the property of multiplicative transitivity in signed graphs to realize edge sign prediction and their method utilized the node adjacency information only.
Leskovec et al. used signed triads as features and constructed a logistic regression model for prediction [14].
While its major contribution is the connections to theories of balance and status in social psychology, the prediction model makes a very strong assumption on the input network: the signs of all links except the one to be predicted are known in advance, which is not very practical in reality.
Transfer learning [19] has been an important research topic and a useful technique in practice.
Transfer learning can effectively transfer the information from the source domain to facilitate a di erent target domain s learning task, where the labeled data in the target domain is very limited [1,
 actional data [1], image [21] and text [22], in which data instances are represented in a prede ned d-dimensional feature space.
They typically map data instances from di erent origins into the same latent domain [21] with sparse coding
 fer learning methods also use bipartite or tripartite graph as tools to facilitate knowledge transfer [7, 8] by mapping the instances and features to bipartite or tripartite graph nodes.
To the best of our knowledge, our work is the  rst one on transferring topological knowledge across large signed social networks.
Speci cally, we transfer the knowledge of a source network to the target network for both feature construction and model learning, which have been shown to be very effective.
t, Eu Let a directed graph Gt = (Vt, El t , S) be the target graph for edge sign prediction.
Here Vt denotes the set of vertices, El t denotes the set of directed edges with edge sign labels, S : El t 7  { 1, +1} is the edge sign mapping function that maps an edge e   El t to a positive label (+1) or a negative label ( 1), and Eu t denotes the set of directed edges whose signs are unknown and need to be predicted.
We treat the labeled edges El t as the training data which is an independent and identically distributed (i.i.d.)
sample drawn from the target graph.
Thus El t has the same distribution as the test edge set Eu t .
However, in many scenarios the quantity of the training edges is inadequate to train a good classi er.
Assume that we have another directed graph Gs = (Vs, Es, S), called the source graph, where Vs denotes the set of vertices, Es denotes the set of directed edges, and S : Es 7  { 1, +1} is the edge sign mapping function that maps an edge e   Es to a positive or negative label.
The labeled edges Es are assumed to be abundant, but the distribution of Es may di er from that of the test edge set in the target graph Gt, perhaps because Gs is outdated, or is from a di erent domain.
When a classi er trained on Es is applied to the test edge set Eu t from Gt, the performance of the classi er may substantially degrade.
However, the labeled edges Es from the source graph Gs is not entirely useless, because there may still exist a certain degree of similarity or common properties between the source graph Gs and the target graph Gt.
Considering the inadequate labeled edges El t from Gt, it is important and bene cial to leverage the labeled edges Es from Gs to help train a classi er to predict the edge signs of Eu t in Gt.
Formally, let T = Ts   Tt denote the training edge set.
Ts = {(es, S(es)},  es   Es, and Tt = {(et, S(et)},  et   El t.
We denote |Ts| = n and |Tt| = m. Eu t is the unlabeled test edge set.
The objective is to learn a classi er P : Es   El t 7  { 1, +1} that minimizes the prediction error on the test edge set Eu t .
In the following, we will  rst study feature construction to create useful topological features that are generalizable from the source graph Gs to the target Gt.
Then we will study model learning that uses an AdaBoost-like method to weigh the training edges from the source graph and the target graph di erently, for the transfer learning purpose.
In this section, we study how to construct useful features for edge sign prediction.
Di erent from many traditional machine learning or transfer learning problems in which instances are represented in a prede ned feature vector, there is no prede ned feature vector for the edge instances in a signed social network.
Therefore the problem is how to construct topological features for the edge instances that are generalizable from the source graph Gs to the target graph Gt.
In this work, we propose to create a collection of features from two categories: (1) explicit topological features which express manifest properties of the edge instances in the source or target graph; and (2) latent topological features which are hidden but express the common patterns between the source graph and the target graph.
Such latent topological features are generalizable across domains in principle.
For a directed edge e = (u, v), we begin by de ning a number of explicit topological features including node degree, betweenness centrality, triad count and edge embeddedness.
These features express the connectivity pattern of the edge e and its two end nodes u and v. It is noteworthy that we do not include any edge sign information in the features, because we only have a very small amount of edge signs in the target graph Gt.
When predicting the sign of an edge e   Eu t , it is practically infeasible to use the signs of edges in the local neighborhood of e as features, as those signs can be unknown.
Node Degree.
For a directed edge e = (u, v), we use degout(u) and degin(v) to denote the number of outgoing edges from u and incoming edges to v, respectively.
The node degree measures the aggregate connection strength of a node to the rest of a graph.
Betweenness Centrality.
Betweenness centrality measures a node s centrality in a graph.
For a node v   V , the betweenness centrality fbc(v) is de ned as fbc(v) = Xi6=v6=j  i,j(v)  i,j (1) where  i,j is the number of shortest paths from node i to j and  i,j(v) is the number of shortest paths from node i to j through node v. Here all possible node pairs i, j   V such that i 6= v 6= j are considered.
For a directed edge e = (u, v), we use fbc(u) and fbc(v) as two betweenness centrality features.
Triad Count.
Following [14], we also use the triad counts as features.
For a directed edge e = (u, v), we consider each triad involving the edge (u, v), consisting of a node w such that w has an edge either to or from u and also an edge either to or from v. Considering that the edge between u (or v) and w can be in either direction, there are 2   2 = 4 types of triads.
For an edge between u and w, we call it a forward edge (F) if it points from u to w, or a backward edge (B) otherwise.
Similarly, for an edge between v and w, we call it a forward edge if it points from w to v, or a backward edge otherwise.
Figure 1 shows the four types of triads involving (u, v).
We use four features fF F , fF B, fBF and fBB to record the number of triads of each type that the edge (u, v) is involved in.
Edge Embeddedness.
For a directed edge e = (u, v), the edge embeddedness [14] feb(e) is de ned as the number of common neighbors of nodes u and v. feb(e) is also used as one topological feature.
The above explicit topological features, though very intuitive, may not be generalizable from the source graph Gs to the target graph Gt, especially when Gs and Gt have dif-1479w fFF w fBF v v u u w fFB w fBB v v u u Figure 1: Four Types of Triads ferent distributions.
To better leverage the labeled edges in the source graph, we propose to construct latent topological features which can capture the common patterns between Gs and Gt, and thus are generalizable features.
Again, it is worth noting that only a very small amount of edge signs are available in the target graph Gt, so it is not practical to include the edge signs in the latent topological features.
Therefore, we only use the connectivity information of both graphs without edge signs to construct the latent features.
Speci cally, denote the source graph without edge signs as Gs = (Vs, Es) with |Vs| = M .
Let As   {0, 1}M  M denote the M   M adjacency matrix of Gs.
For a pair of vertices u, v, As(u,v) = 1 i  (u, v)   Es, and As(u,v) = 0 otherwise.
As is asymmetric since Gs is a directed graph.
Similarly, let At   {0, 1}N N denote the N   N adjacency matrix of the unsigned target graph Gt where |Vt| = N .
Given As and At, we propose to use Nonnegative Matrix Tri-Factorization (NMTF) [4], which originates from Nonnegative Matrix Factorization (NMF) [13] and has been widely applied and extended in machine learning [4, 17], to construct latent topological features through factorizing As and At under the same space.
The latent feature space can capture the principal common factors of the original link structures in both Gs and Gt.
We formulate the problem of  nding the latent feature space as follows: min J = kAs   Us kV T s k2 F + kAt   Ut kV T t k2
 +  k kk2
 k k s.t.
k Us( j) = 1, Xj=1 Xj=1 Us, Vs   RM  k Ut( j) = 1, + k Vs( j) = 1, Xj=1 Xj=1 , Ut, Vt   RN k Vt( j) = 1, + (2) + In this formulation, where k   kF is the Frobenius norm.
we consider the nonnegative matrix tri-factor decomposi-t .
Matrix  k   Rk k s and At   Ut kV T tion As   Us kV T is the common latent space for both graphs, which ensures that the extracted topological features of both graphs are expressed in the same space.
Us, Vs, Ut, and Vt are four latent topological feature matrices.
The ith row of matrix Us represents the outgoing linkage features of a source graph node i (i   Vs) in the latent space, while the ith row of matrix Vs represents the incoming linkage features of node i in the latent space.
Similar interpretations can be derived for matrices Ut and Vt in the target graph Gt.
  is the trade-o  regularization parameter to weigh the term k kk2 F .
Since all of our variables are nonnegative, excessively large values in  k will make many entries in Us, Ut, Vs and Vt approach 0.
As a result, this will cause each node to have nearly indistinguishable latent topological features.
Thus it is necessary to constraint the values in  k through a regularization term.
In addition, we enforce the constraints that each row in the topological feature matrices is positive and normalized.
By solving the joint NMTF problem in Eq.
2, we obtain four feature matrices Us, Vs, Ut and Vt for expressing the latent topological (both outgoing and incoming) features for nodes in Gs and Gt in terms of the same latent space  k.
We develop an iterative update algorithm to minimize J in Eq.
2.
Speci cally, we optimize the objective function in Eq.
2 by updating one variable while  xing the other variables.
We can rewrite the objective function in Eq.
2 as follows.
J = tr(AT + tr(AT +  tr( T s As   2AT t At   2AT k  k) s Us kV T t Ut kV T s + Vs T t + Vt T k U T k U T s Us kV T s ) t Ut kV T t ) (3) Update Rule of Us: Since we have the constraint Us   0, following the standard constrained optimization theory, we introduce the Lagrangian multiplier LUs   RM  k and minimize the Lagrangian function L(Us) = J   tr(LUsUs) (4) = 0.
Then considering the KKT condition  Us We set  L(Us) LUs(i,j)Us(i,j) = 0, we can get k + 2Us kV T ( 2AsVs T s Vs T k )(i,j)Us(i,j) = 0 (5) Based on Eq.
5, following a similar approach as in [13], we have the following multiplicative update rule Us(i,j)   Us(i,j)s (AsVs T (Us kV T k )(i,j) s Vs T k )(i,j) (6) Update Rules of Other Matrices: Similar to the update rule of Us, the update rules of Vs, Ut, Vt, and  k are (Vs T Vs(i,j)   Vs(i,j)s (AT Ut(i,j)   Ut(i,j)s (AtVt T Vt(i,j)   Vt(i,j)s (AT (Ut kV T (Vt T s Us k)(i,j) k U T s Us k)(i,j) k )(i,j) t Vt T k )(i,j) t Ut k)(i,j) k U T t Ut k)(i,j) (7) (8) (9)  k(i,j)    k(i,j)s
 s Us kV T s AsVs + U T s Vs + U T
 t AtVt)(i,j) t Ut kV T t Vt +  k)(i,j) (10) Algorithm 1 is the iterative update algorithm that uses the above multiplicative rules for updating each variable matrix to optimize Eq.
2.
The convergence criterion is, the gap between any two consecutive objective function values of Eq.
2 is less than a certain threshold.
We now study the convergence property of Algorithm 1.
First, we give the following two lemmas from [13, 4].
Data: [Adjacency Matrices As, At; Regularizer  ] Result: [Latent feature matrices Us, Vs, Ut, Vt; Latent space  k] begin Initialize Us, Vs, Ut, Vt,  k following [23] while beyond convergence do





 end Lemma 1.
[13] Aux(h, h ) is an auxiliary function for F (h) if the conditions Aux(h, h )   F (h), Aux(h, h) = F (h) are satis ed.
If Aux is an auxiliary function for F , then F is non-increasing under the update ht+1 = arg min h Aux(h, ht) where ht is the value of variable h in the tth iteration.
Lemma 2.
[4] For any matrices P   RN N , Q   Rk k + , , and P and Q are symmetric, the + S   RN k , S    RN k following inequality holds + + (P S Q)i,jS2 i,j
 i,j   tr(ST P SQ) Xi,j Theorem 1.
De ne J (Us) according to Eq.
2 as s Us kV T J (Us) = tr( 2AT s ) s + Vs T s Us kV T k U T then the following function Aux(Us, U   (AsVs T k )(i,j)U   s(i,j)(1 + ln s) =  2Xi,j + Xi,j
 s kV T s Vs T k )(i,j)

 s(i,j) s(i,j) (11) (12) Us(i,j)
 s(i,j) ) (13) is an auxiliary function for J (Us).
Furthermore, Aux(Us, U   s) is convex for Us and its global minimum can be achieved at Us(i,j) = U   s(i,j)s (AsVs T s kV T
 k )(i,j) s Vs T k )(i,j) (14) Proof.
First, in Lemma 2, let P = I, Q =  kV T s Vs T k , S = Us and S  = U   s. Then we have tr(ST P SQ) = tr(U T = tr(U T s   I   Us    kV T s Vs T k ) s Us kV T s   Vs T k ) s Vs T k )(i,j) s kV T


 s(i,j) s(i,j)   Xi,j Based on the property of trace, we have tr(U T s Us kV T s   Vs T k ) = tr(Vs T k   U T s Us kV T s ) Therefore, we can derive tr(Vs T k U T s Us kV T s )  Xi,j
 s kV T s Vs T k )(i,j) Second, since  z > 0, z   1 + ln z, we have

 s(i,j) s(i,j) tr(AT s Us kV T s ) =Xi,j (AsVs T k )(i,j)Us(i,j) (AsVs T k )(i,j)U   s(i,j)(1 + ln  Xi,j Us(i,j)
 s(i,j) ) If we multiply both sides of Eq.
16 with  2, we get tr( 2AT s Us kV T k )(i,j)U   (AsVs T s(i,j)(1+ln s )    2Xi,j (15) (16) ) Us(i,j)
 s(i,j) (17) When adding Eqs.
15 and 17 on both sides, we have s Us kV T s ) Us(i,j)
 s + Vs T s Us kV T s(i,j)(1 + ln k )(i,j)U   (AsVs T k U T ) s(i,j) J (Us) = tr( 2AT    2Xi,j +Xi,j
 = Aux(Us, U   s) s kV T s Vs T k )(i,j)

 s(i,j) s(i,j) In addition, it is easy to verify that Aux(Us, Us) = J (Us).
Therefore, we prove Aux(Us, U   s) de ned in Eq.
13 is the auxiliary function for J (Us).
Us(i,j), we get s and minimize Aux(Us, U   Next, if we  x U   s) w.r.t.
each  Aux(Us, U   s)  Us(i,j) k )(i,j)
 s(i,j) Us(i,j) =  2(AsVs T s kV T
 s Vs T k )(i,j) Us(i,j)
 s(i,j) (18) Moreover, the second partial derivative (Hessian matrix) is  2Aux(Us, U   s)  Us(i,j) Us(k,l) =  ik jl(2(AsVs T k )(i,j)

 s(i,j) s(i,j) + s Vs T k )(i,j)

 s kV T
 s(i,j) ) (19) We should notice that Hessian of Aux(Us, U   s) w.r.t.
Us is a diagonal matrix with all positive diagonal elements.
Thus, Aux(Us, U   s) is convex over Us and we can achieve its global minimum through setting Eq.
18 to be 0, which leads to the result in Eq.
14.
Let U   s(i,j) = U t s(i,j), then according to the update ht+1 = arg minh Aux(h, ht) in Lemma 1, Us(i,j) de ned in Eq.
14 which minimizes Aux(Us, U t s(i,j).
This is essentially our update rule for Us in Eq.
6.
This leads to the following lemma.
s(i,j)) is exactly U t+1 Lemma 3.
Using the update rule in Eq.
6 to update Us, J (Us) in Eq.
12 is monotonically decreasing.
Proof.
By Lemma 1 and Theorem 1, we have
 s )   J (U 1 s )   Aux(U 1 s ) = Aux(U 0 s , U 0 s , U 0 s )    
 where U t Therefore, J (Us) is monotonically decreasing.
Theorem 2.
Using Algorithm 1 to update Us, Vs, Ut, Vt and  k, the value of the objective function J will monotonically decrease.
The proof of Theorem 2 can be similarly achieved through Lemma 3.
Since the objective function value J in Eq.
2 is lower bounded by 0, Algorithm 1 can guarantee convergence by Theorem 2.
Theoretically, the computational complexity of factorizing Eq.
2 is at most O(k max{|Es|,|Et|}2) where k is the length of the latent topological feature vector, and Et = El t   Eu t .
In practice, due to the sparsity of adjacency matrices As and At, the exact computational cost can be much lower than the theoretical result.
So far we have constructed both explicit and latent topological features for edge sign prediction.
For an edge instance e = (i, j)   El t with label S(e), we have latent feature vectors Ut(i ) and Vt(j ) to represent node i s outgoing linkage pattern and node j s incoming linkage pattern.
We also have 9 explicit features, including node degrees degout(i) and degin(j), betweenness centrality fbc(i) and fbc(j), triad counts fF F (e), fF B(e), fBF (e), fBB (e), and edge embed-dedness feb(e).
We can similarly de ne features for edge instances in Es.
The features are used together, denoted as F (e), for learning an edge sign prediction model.
With the explicit and latent topological features, it is natural to learn a model from the training instances in the target graph, i.e., Tt = {(et, S(et))},  et   El t. However, using only the small amount of labeled edge instances for training does not give a classi er with good prediction performance on the target graph.
Fortunately, we still have the full knowledge of the edge signs in the source graph Gs.
Thus our task is to learn an edge sign prediction model by leveraging the labeled instances in both the source and target graphs.
With our extracted features, the source and target graph edge instances can be represented in the same feature space.
A straightforward approach is to simply combine the source and target edge instances and treat them equally to learn a model.
However, since discrepancy always exists between the distribution of source and target graph edges, a classi er learned from this simple combination may not necessarily achieve better performance than the model learned from the target graph edges only.
Sometimes the noise in the source graph instances may cause the model to predict wrongly on the test edges from the target graph, thus degrade the performance substantially.
Thus we need an e ective mechanism to distinguish the more useful edge instances from the less useful ones in the source graph.
ing To address the distributional di erence issue, we borrow the AdaBoost idea from Freund and Schapire [5] and Dai et al. [3] for instance weighting in our transfer learning framework.
That is, we treat the edge instances in Ts = {(es, S(es))},  es   Es and Tt = {(et, S(et))},  et   El t di erently, by assigning and adjusting the weight of each training instance during model learning.
For those edge instances in Ts that are more similar to the target edge instances in Tt, we should give them larger weights to attach more importance to them; conversely, for those edge instances in Ts that are less similar to the target edge instances in Tt, we should give smaller weights to weaken their impacts.
|Pt (et ) S(et )| Algorithm 2 shows an iterative algorithm which updates instance weights according to the basic classi er Pt s performance in each round.
It is similar to the traditional Ad-aBoost method where the accuracy of a learner is boosted by carefully adjusting instance weights.
We use w1, .
.
.
, wn to denote the weights of edges in Ts, and wn+1, .
.
.
, wn+m to denote the weights of edges in Tt.
It is worth noting the following special instance weighting policy in our transfer learning framework.
For an edge e, Pt(e)   [ 1, 1] is the predicted edge sign for e and S(e) is the true edge sign.
For any target graph edge et   El t, its weight will always get     [1, + ), and the increased by a factor of   t weight increment of a wrongly predicted edge is larger than that of a correctly predicted one.
In contrast, for any source graph edge es   Es, its weight will always get decreased by   (0, 1], and the weight decrement a factor of   of a wrongly predicted edge is larger than that of a correctly predicted one, because the wrongly predicted source graph edge may be very dissimilar to the target graph.
Therefore, the weights of source graph edges would never increase and are always less than those of target graph edges, which means that the source graph edges will never have a larger in uence than the target graph edges in model learning.
After K iterations, those source graph edges which are more similar to the target graph edges will have larger weights than the less similar ones to contribute to model learning.
|Pt (es ) S(es )|


 We analyze the training loss from both source and target graphs, based on the analysis in Freund and Schapire [5] and Dai et al. [3].
Consider the tth iteration training loss on source graph instances where each instance s normalized loss is de ned as lt(ei) = |Pt(ei)   S(ei)|/2, and its overall training loss through K iterations is Li = PK t=1 lt(ei), 1   i   n. Thus all source instances  training loss su ered by Algorithm 2 is
 n
 Xt=1 dt ilt(ei) Xi=1 where dt conclusion.
i = wt j=1 wt i /(Pn j).
We  rst present the following Theorem 3.
In Algorithm 2, we have
 K   min 1 i n Li
 +r 2 ln n
 + ln n
 (20) Theorem 3 and its proof can be found in [5].
It can rigidly bound the average training loss of source graph instances through K iterations, which cannot exceed the minimum average training loss of a single instance by more than K + ln n
 q 2 ln n Similar to [3], we have the following theorem.
Data: source edge instances Ts, labeled target edge instances Tt, and the iteration number K Result: edge sign classi er P begin Let n   |Ts|, m   |Tt| Initialize the weight vector w1 = (w1 n, w1 for t = 1, .
.
.
, K do 1, .
.
.
, w1 n+1, .
.
.
, w1 n+m) i=1 wt i )

 learn a model Pt : F (e)   Pt(e)   [ 1, 1]
 i=n+1 qt  t = Pn+m
 i   |Pt(ei) S(ei)| Pn+m i=n+1 qt i


 1 t ,   =
 wt+1 i =8< : P (e) =8>>< >>: end wt i   |Pt (ei ) S(ei )|
 , |Pt (ei ) S(ei )|
 wt   i   t 1   i   n , n + 1   i   n + m
 if log
  t   Pt(e)   0
 Xt=1  1, otherwise Theorem 4.
In Algorithm 2, qt training instance ei, which is de ned as qt = wt/(Pn+m Then, i denotes the weight of the i=1 wt i ).
lim
 t=1Pn i=1 qt
 i lt(ei) = 0 (21) Theorem 4 shows that the weighted average training loss in the source graph edge instances gradually converges to zero.
Next, according to Step 4 in Algorithm 2, we have the constraint  t   1/2    , for some   > 0.
Then we have the following bound on the prediction error of the  nal classi er on the labeled target edge instances.
Theorem 5.
Let I = {i : P (ei) 6= S(ei), ei   Tt}.
De ne the error of the  nal classi er P by Algorithm 2 as   = Pre Tt [P (e) 6= S(e)] = |I|/|Tt| and it is bounded as     exp{ 2   K 2}.
(22) Theorem 5 and its proof can be found in [5].
From Theorem 5, we can observe that the  nal classi er P will reduce the error on target graph labeled instances, when the maximum number of iterations K increases.
Therefore Algorithm 2 minimizes both the error on the target graph training in stances and the weighted average loss on the source graph training instances simultaneously.
In this section, we  rst describe how we prepare data for training and testing.
Then we present experimental results to show the e ectiveness of our proposed features and the transfer learning algorithm.
Studies on the convergence properties of both Algorithms 1 and 2 are also provided.
We use three large online social networks Epinions, Slashdot and Wikipedia where each link is explicitly labeled as positive or negative.
All three networks are downloaded from Stanford Large Network Dataset Collection4.
Since the original graphs are too large and sparse, we select 20,000 nodes from Epinions, 16,000 nodes from Slashdot and 7,000 nodes from Wikipedia with the highest degrees, as well as the edges between the selected nodes.
There are 13 nodes in Epinions, 1 node in Slashdot and 2 nodes in Wikipedia that are disconnected from the remaining selected nodes.
These isolated nodes are removed from the respective network and the remaining ones form a connected component.
Table 1 shows the statistics of the three extracted networks.
Table 1: Statistics of Extracted Graphs Number of Nodes Number of Edges Average Degree Positive Edges Average Distance Epinions




 Slashdot Wikipedia









 We can observe that Epinions has the largest number of nodes, edges, average degree and the percentage of positive edges among the three networks, while Wikipedia has the smallest number of nodes, edges, average degree and the percentage of positive edges.
The statistics demonstrate that there indeed exists discrepancy in data distribution among the three networks.
As the edge signs in all these networks are overwhelmingly positive, we follow the methodology adopted by both Guha et al. [6] and Leskovec et al. [14], to create a balanced dataset from each signed social network.
For every negative edge, we sample a random positive edge to ensure that the number of positive and negative edges is equal.
We consider each pair of networks out of the three and use one network as the source and the other as the target for transfer learning, and then reverse the source and the target networks.
There are totally `3
 we use 4-fold cross validation for performance evaluation   in each target graph we partition the edge instances into four parts evenly, each having a balanced class distribution.
We use one part as the test edge set Eu t , and randomly sample a small percentage of edge instances in the remaining three parts to form the labeled edge set El t and all edges Es in the source graph form the training set.
We run our experiment on the four test edge sets in turn and report the average classi cation accuracy.
t. This small El
 stance Weighting Comparison with Other Approaches: We compare the prediction accuracy of the following methods.
4http://snap.stanford.edu/data/index.html 1483y c a r u c c
 n o i i t c d e r






 y c a r u c c
 n o i i t c d e r






 Katz Src Target Src+Target



 Percentage (%) of Labeled Target Data (a) Slashdot vs. Epinions

 Katz Src Target Src+Target



 Percentage (%) of Labeled Target Data (d) Wikipedia vs. Slashdot







 y c a r u c c
 n o i i t c d e r


 y c a r u c c
 n o
 i i t c d e r
 Katz Src Target Src+Target



 Percentage (%) of Labeled Target Data (b) Epinions vs. Slashdot

 Katz Src Target Src+Target


 y c a r u c c
 n o
 i i t c d e r
 Katz Src Target Src+Target




 Percentage (%) of Labeled Target Data (c) Slashdot vs. Wikipedia


 y c a r u c c
 n o
 i i t c d e r

 Katz Src Target Src+Target




 Percentage (%) of Labeled Target Data (e) Epinions vs. Wikipedia




 Percentage (%) of Labeled Target Data (f) Wikipedia vs. Epinions

 Figure 2: Prediction Accuracy by Varying the Percentage of Labeled Target Edge Instances   Katz : Katz kernel [10] is a matrix kernel approach used in [11, 9] for edge sign prediction.
It uses the adjacency matrix of labeled edges in the target graph to predict the sign of the test edges.
  Src: using all edge instances in the source graph only for training.
  Target: using labeled edge instances in the target graph only for training.
  Src+Target: using both source graph edges and labeled target graph edges equally for training without instance weighting.
  IW : using both source graph edges and labeled target graph edges for training with instance weighting (our Algorithm 2).
For all the above schemes we use SVM as the classi cation model and use the same test edge set for prediction.
All schemes except Katz use our proposed explicit and latent topological features.
We set k = 30 for matrix  k representing the latent feature space.
In the  rst group of experiment, we use Slashdot as the source and Epinions as the target.
We vary the percentage of labeled edge instances in the target graph from 2% to
 results are shown in Figure 2(a).
We can observe that when the percentage of labeled target edge instances increases, the accuracy of all methods except Src increases and gradually saturates.
The accuracy of Target improves greatly when the percentage increases from 2% to 20% and it outperforms Src when the percentage exceeds 20%.
The performance of Src+Target without instance weighting lies between that of Src and Target.
When the amount of labeled target edge instances is very small, Src+Target can improve the accuracy over Target by leveraging source graph edge instances; but when the labeled target edge instances are abundant, the source edge instances become less useful, and the noise in the source edge instances may become more obvious, causing Target to be better than Src+Target.
Katz performs the worst among all methods.
Our proposed method IW (Algorithm 2) achieves the highest accuracy in all cases, demonstrating the e ectiveness of instance weighting in the transfer learning framework.
IW consistently outperforms Target even if we have 50% labeled target edge instances for training.
This result proves that the knowledge transferred from the source graph is bene cial to improve the model s performance, under proper instance weighting.
We can observe similar trends in Figure 2(b) in the second group of experiment when Epinions is the source and Slashdot is the target.
But due to the larger number of edges in Epinions, the source edge instances have a larger in uence in model learning.
This e ect causes Src+Target to have a very close performance to Src.
Similar conclusions can be drawn in Figures 2(c) 2(f) for the other four groups of experiments.
Another important observation is that, when the distributional di erences between the source and target networks become larger, the transfer learning performance becomes worse.
For example, the di erences between Wikipedia and Epinions as shown in Table 1 are larger than the di erences between Slashdot and Epinions.
Consider the case when Epinions is the target network.
The prediction accuracy when using Wikipedia as the source (Figure 2(f)) is lower than the accuracy when using Slashdot as the source (Figure 2(a)), due to the larger di erences between Wikipedia and Epinions.
Learning Convergence Analysis: It is important to assess the convergence of Algorithm 2 as an iterative algorithm.
Besides the theoretical analysis of convergence in Section 5, we also test the learning convergence in all our experiments.
Figure 3 shows the prediction accuracy in each iteration on all the `3
 E-0.02  means using Slashdot as the source and Epinions as the target with 2% labeled target edges for training.
We can see that the accuracy gradually increases with more iterations and converges after 30 35 iterations.
This result con rms the theoretical analysis on learning convergence.
In this experiment, we evaluate the e ectiveness of each type of features we propose, including degree, betweenness centrality (BC), triad counts, edge embeddedness (Embed) and latent features constructed by NMTF.
We set k = 30 in matrix  k for the latent feature space.
We use source graph edges and a certain percentage of labeled target graph edges for training; then use our instance weighting algorithm for



 y c a r u c c
 n o i i t c d e r





 Number of Iterations S vs E-0.02 S vs E-0.2 E vs S-0.02 E vs S-0.2

 y c a r u c c
 n o i i t c d e r












 y c a r u c c
 n o i i t c d e r



 S vs W-0.02 S vs W-0.2 W vs S-0.02 W vs S-0.2



 Number of Iterations E vs W-0.02 E vs W-0.2 W vs E-0.02 W vs E-0.2



 Number of Iterations (a) Slashdot vs. Epinions (b) Slashdot vs. Wikipedia (c) Epinions vs. Wikipedia Figure 3: Prediction Accuracy by Iteration of Algorithm 2 Slashdot vs Epinions Epinions vs Slashdot





 y c a r u c c
 n o i i t c d e r
 Slashdot vs Wiki Wiki vs Slashdot Epinions vs Wiki Wiki vs Epinions y c a r u c c
 n o i i t c d e r












 y c a r u c c
 n o i i t c d e r

 Degree BC Triad Embed Latent All
 Degree BC Triad Embed Latent All
 Degree BC Triad Embed Latent All (a) Slashdot vs. Epinions (b) Slashdot vs. Wikipedia (c) Epinions vs. Wikipedia Figure 4: Feature E ectiveness Comparison with 2% Target Training Instances model learning.
Only one type of feature is used in each learned model.
We also use all these features (All) to learn a model to show their overall performance.
We  rst use 2% labeled edge instances in the target graph plus all source edge instances for training.
Figures 4(a) 4(c) show the prediction accuracy of each feature type on the three network pairs respectively.
Among the  ve types of features, we can see that our latent features always achieve the highest accuracy, followed by Degree, Triad, BC and Embed.
This is because the latent features can capture the common structural patterns between the source and target graphs, despite the di erent distributions between them.
Thus the latent features can generalize well from the source graph to the target.
In contrast, the other four types of features are not generalizable from the source graph to the target.
As the statistics in Table 1 show that the three social networks have di erent distributions in degree and other dimensions, it is not di cult to understand that a model learned from the source edge instances based on these explicit features cannot generalize well to predict the sign of test edges in the target graph.
Finally, the model using all these features achieves the best performance.
When we increase the percentage of labeled target edge instances to 30% for training, the prediction accuracy is shown in Figures 5(a) 5(c).
Among the  ve types of features, Latent still achieves the highest accuracy in most cases, followed by Degree, Triad, BC and Embed.
But the performance di erence between Latent and Degree/Triad is not as signi cant.
This is because when the amount of labeled target edge instances is much larger, the labeled target instances provide reliable feature values on Degree and Triad.
The source graph edges which have a di erent distribution will have a smaller in uence in model learning through instance weighting.
Finally, the model using all these features still achieves the best performance.
Tri-Factorization Sensitivity Test: We perform sensitivity test on the dimension of the latent feature space, i.e., k in the k k matrix  k computed by matrix tri-factorization.
We vary the k value of the latent feature space and report the prediction accuracy in Figures 6(a) 6(c) on the three network pairs respectively.
30% labeled target edge instances plus all source graph edges are used for training.
Here legend Src+Target means using both source graph edges and labeled target graph edges without instance weighting, and IW means our instance weighting method.
We can observe that the prediction accuracy increases  rst when k increases and then becomes stable or even slightly decreases when k > 30 for all three groups of experiments.
When k > 30, we  nd many entries in the latent feature vector become almost 0, thus contribute little to prediction, or even degrade the performance.
Tri-Factorization Convergence Analysis: We prove the convergence of Algorithm 1 in Section 4.2.2.
We report the objective values (in log scale, i.e., lnJ ) over 20 iterations under three latent feature dimensions k = 10, 30, 60 in Figure
 Slashdot is the source and Epinions is the target.
We can observe that the objective values which measure the di er-ence between the original matrix and the decomposed ones converge very quickly (after 4 iterations) regardless of the feature dimension k. This result con rms our theoretical analysis on the convergence of Algorithm 1.
) n l ( l e u a
 e v i t c e b
 j






 k=10 k=30 k=60

 Number of Iterations

 Figure 7: NMTF Convergence

 We studied the edge sign prediction problem in signed social networks, which have both positive and negative links.
We assume the edge sign information is very scarce in the target network which is very common for newly formed networks.
This problem is important because knowing the edge
 Slashdot vs Epinions Epinions vs Slashdot y c a r u c c
 n o i i t c d e r





 y c a r u c c
 n o i i t c d e r




 Slashdot vs Wiki Wiki vs Slashdot Epinions vs Wiki Wiki vs Epinions y c a r u c c
 n o i i t c d e r





 Degree BC Triad Embed Latent All Degree BC Triad Embed Latent All Degree BC Triad Embed Latent All (a) Slashdot vs. Epinions (b) Slashdot vs. Wikipedia (c) Epinions vs. Wikipedia Figure 5: Feature E ectiveness Comparison with 30% Target Training Instances y c a r u c c
 n o i i t c d e r





 E vs. S-Src+Target E vs. S-IW S vs. E-Src+Target S vs. E-IW



 Latent Feature Dimension y c a r u c c
 n o i i t c d e r






 W vs. S-Src+Target W vs. S-IW S vs. W-Src+Target S vs. W-IW


 Latent Feature Dimension
 y c a r u c c
 n o i i t c d e r






 W vs. E-Src+Target W vs. E-IW E vs. W-Src+Target E vs. W-IW



 Latent Feature Dimension (a) Slashdot vs. Epinions (b) Slashdot vs. Wikipedia (c) Epinions vs. Wikipedia Figure 6: Sensitivity Test of Latent Feature Dimension signs will provide us with better understanding of user opinions in a social network.
It is challenging due to the inadequate edge signs in the target network and the prohibitive cost of manual labeling.
We adopt the transfer learning approach by leveraging a source network with abundant edge sign information but possibly under a di erent yet related distribution.
We propose to construct generalizable latent features through NMTF by considering both the source and target networks, and then adopt an AdaBoost-like algorithm with instance weighting to train a good classi er.
Extensive experiments on three real signed networks Epinions, Slashdot and Wikipedia prove the e ectiveness of our extracted features and transfer learning algorithm.
The work described in this paper was partially supported by two China National 973 projects (grant No.
2012CB315904 and 2013CB336700), several grants from the University Grants Committee of the Hong Kong Special Administrative Region, China (Area of Excellence Project No.
AoE/E-02/08 and General Research Fund Project 411211, 411310, 411010 and 411011), and two gift grants from Microsoft and Cisco.
