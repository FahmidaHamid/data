Latent factor model has been one of the most powerful approaches for collaborative  ltering.
Some of the most successful realizations of latent factor models are based on Matrix Factorization (MF) techniques [17].
The fundamental idea of these approaches is that user preferences can be determined by a relatively small number of latent factors.
A variety of matrix factorization methods have been proposed Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
and applied to various collaborative  ltering tasks successfully, such as Singular Value Decomposition (SVD) [17, 33], Non-negative Matrix Factorization (NMF) [18, 19], Max-Margin Matrix Factorization (MMMF) [34, 23] and Probabilistic Matrix Factorization (PMF) [26, 25].
However, MF approaches have also encountered a number of problems in real-world recommender systems, such as data sparsity, frequent model retraining and system scal-ability.
As the number of ratings given by most users is relatively small compared with the total number of items in a typical system, data sparsity usually decreases prediction accuracy and may even lead to over tting problems.
In addition, new ratings are usually made by users continuously in real-world recommender systems, leading to the need for refactoring rating matrices periodically, which is time consuming for systems with millions or even billions of ratings, and further restricts the scalability of MF approaches.
In this study, we propose a novel MF framework named Localized Matrix Factorization (LMF), which is general and intrinsically compatible with many widely-adopted MF algorithms.
Before problem formalization, we would like to use an intuitional example to brie y introduce the matrix structures used in LMF.
Figure 1(a) is a sparse matrix where each row/column/cross represents a user/item/rating.
By permuting Row4, Row9 and Column7 to  borders , the remaining part is partitioned into two  diagonal blocks , which results in a Bordered Block Diagonal Form (BBDF) [4] matrix in Figure 1(b).
By  recursively  permuting the  rst diagonal block, we obtain a Recursive Bordered Block Diagonal Form (RBBDF) matrix in Figure 1(c).
BBDF and RBBDF structures are generalizations of Block Diagonal Form (BDF) structure which has no  border .
RBBDF structure is intuitionally interpretable in collaborative  ltering tasks.
Consider movie recommendation as an example.
Di erent users may have di erent preferences on movie genres, which form di erent communities, corresponding to the diagonal blocks in the BBDF structure.
However, there does exist  super users  whose interests are relatively broad and thus fall into di erent communities.
This type of user is represented by row borders in the BBDF structure.
There are also some classical or hot movies widely known and enjoyed by users from di erent communities, which are  super items  making up column borders.
The structure may recurse at multiple  ner-grained levels in a community, resulting in the generation of RBBDF structures.
As di erent communities may have di erent rating patterns, it would be better to factorize them independently.
The LMF framework transforms a sparse matrix into RBBDF 1511structure and further extracts denser submatrices to con struct a BDF matrix.
Factorization of the BDF matrix is used to approximate the original sparse matrix.
The framework brings several attractive bene ts to recommender systems: 1) Factorizing extracted dense submatrices instead of the whole sparse matrix improves the prediction accuracy of matrix factorization algorithms.
2) The locality property of LMF makes it possible to refactorize only the recently-updated submatrices rather than the whole matrix.
3) The framework is suitable for parallelization, which further contributes to the scalability of recommender systems.
In summary, the main contributions of this work are:   The RBBDF structure of rating matrices is investigated, which is intuitionally interpretable in CF tasks.
  A density-based algorithm is designed to transform a sparse matrix into RBBDF structure.
  The LMF framework is proposed and its rationality is shown through theoretical analyses.
  Through a comprehensive experimental study on four benchmark datasets, both the e ciency and e ective-ness of the LMF framework is veri ed.
The remainder of this paper will be organized as follows: Section 2 reviews some related work, and Section 3 presents some preliminaries.
In Section 4, the LMF framework is introduced and investigated.
Experimental results are shown in Section 5.
Some discussions will be made in Section 6, and the work is concluded in Section 7.
Collaborative Filtering (CF) [35] techniques have been known to have several attractive advantages over other recommendation strategies, such as Content-based Filtering [22] in Personalized Recommender Systems [21].
Early CF algorithms mainly focus on memory-based approaches such as User-based [24] and Item-based [29] methods, which calculate the similarities of users or items to make rating predictions [21].
To gain better prediction accuracies and to overcome the shortcomings of memory-based algorithms, model-based approaches have been investigated extensively, which estimate or learn a model on user-item rating matrices to make rating predictions [35, 21].
Latent Factor Models (LFM) based on Matrix Factorization (MF) [36] techniques have been an important research direction in model-based CF methods.
Recently, MF approaches have gained great popularity as they usually outperform traditional methods [35, 12] and have achieved state-of-the-art performance, especially on large-scale recommendation tasks [17].
A variety of MF algorithms have been proposed and investigated in di erent CF settings, such as Principle Component Analysis (PCA) [1], Singular Value Decomposition (SVD) [16, 17, 33], Non-negative Matrix Factorization (NMF) [18, 19], Max-Margin Matrix Factorization (MMMF) [34, 23], and Probabilistic Matrix Factorization (PMF) [26, 25].
They aim at learning latent factors from a matrix, with which to make rating predictions.
According to the uni ed view of MF in [32], MF algorithms are optimization problems over given loss functions and regularization terms.
Di erent choices of loss functions and regularization terms lead to di erent MF methods.
However, MF approaches also su er from a number of problems in real-world recommender systems, such as data (a) Original matrix (b) BBDF matrix (c) RBBDF matrix Figure 1: An example of (R)BBDF structure sparsity, frequent model retraining and system scalability.
To overcome the problem of data sparsity, earlier systems rely on imputation to  ll in missing ratings and to make the rating matrix dense [28].
However, imputation can be very expensive as it signi cantly increases the amount of ratings, and inaccurate imputation may distort the data considerably [17].
The problem of frequent model retraining and scalability results from the fact that the total number of users and items is usually very large in practical systems, and new ratings are usually made by users continuously.
Most e orts to improve system scalability focus on matrix clustering techniques [38, 31, 10, 9, 37] or designing incremental and distributed versions of existing MF algorithms [30, 20, 11].
Usually, they can achieve only approximated results compared with factorizing the whole matrix directly, and many of them restrict themselves to one speci c MF algorithm.
In contrast with these approaches, we demonstrate that the LMF framework on a BDF matrix is theoretically equal to factorizing the whole matrix directly, and that it is compatible with any existing decomposable MF algorithm.
Another related research  eld is graph partitioning, as permuting a sparse matrix into BBDF structure is equivalent to conducting Graph Partitioning by Vertex Separator (GPVS) on its corresponding bipartite graph [4].
Graph partitioning is known to be NP-hard [7], but this problem has been investigated extensively, and many e cient and high-quality heuristic-based methods have been proposed [15], such as multilevel methods [14, 6], spectral partitioning [3] and kernel-based methods [2].
It is veri ed both theoretically and experimentally that multilevel approaches can give both fast execution time and very high quality partitions [27, 4, 14, 6, 15], which guides us to choosing the multilevel graph partitioning approach in this work.
We take the uni ed view of MF proposed in [32], which is su cient to include most of the existing MF algorithms.
Let X   Rm n be a sparse matrix, and let U   Rm r, V   Rn r be its factorization.
An MF algorithm P = (f,DW ,C,R) can be de ned by the following choices:

 , which if used must + be an argument of the loss function.
sure of the error when approximating X with f (U V T ).
(cid:104)DW (X, f (U V T )) + R(U, V ) (cid:105) For an MF model X   f (U V T ) (cid:44) X , we solve: .
argmin
 (1) (cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)!"""#"""$"""%""""&""" """("""")"""*""!+""!!(cid:1)!"""""#""$""%""&"" "("")"(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)!"""#""""$"""%""&&""" """("""&""&)"""*"""+(cid:1)!"""""#""$""%""&" ""(")"(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)!"""#"""$"""%"""&&"" """("""&"""&)"""*"""+(cid:1)!
"""""#""$""%""&" ""(")"1512The loss D( , ) is typically convex in its second argument, and often decomposes into a (weighted) sum over elements of X [32].
For example, the loss function of WSVD [33] is: DW (X, f (U V T )) = (cid:107)W (cid:12) (X   U V T )(cid:107)2 F ro where (cid:12) denotes the element-wise product of matrices.
In this paper, we refer to X = U V T as an Accurate Matrix Factorization of X, and refer to X   f (U V T ) (cid:44) X  as an Approximate Matrix Factorization of X.
(2) We consider permuting the rows and/or columns of a sparse matrix to reform its structure.
X is a Block Diagonal Form (BDF) matrix if:  


 .
.
.
Dk   (cid:44) diag(Di) It is not always the case that a sparse matrix can be permuted into BDF, but usually it can be permuted into Bordered Block Diagonal Form (BBDF) [4] shown in (4).
Each Di(1   i   k) is a  diagonal block .
Rb (cid:44) [R1   RkB] and Cb (cid:44) [C T k BT ]T are row and column  borders :

 (cid:34)
 (cid:35)  

 =  

 .
.
.
.
.
.
Dk Ck       Rk B (3) (4) Any of the k diagonal blocks Di in (4) may be permuted into BBDF structure recursively, resulting in Recursive Bordered Block Diagonal Form (RBBDF).
To avoid notational clutter, we present the following example, where I  and J  denote the row and column index sets:





   =



















  
 In (5), D1 is permuted into BBDF recursively.
Note that permuting rows and columns related to D1 a ects R1 and C1, but it only changes the order of the non-zeros therein.
Diagonal blocks D11, D12 and D2 may be further permuted depending on certain stopping rules.
This will be introduced in our algorithm for constructing RBBDF structures.
A sparse rating matrix can be equally represented by a bipartite graph.
Consider Figure 1(a) and Figure 2(a) as examples.
Each row or column of the matrix corresponds to an R-node or a C-node in the bipartite graph.
GPVS partitions a graph into disconnected components by removing a set of vertices (vertex separator) and their incident edges.
As demonstrated by [4], permuting a sparse matrix into BBDF structure is equivalent to conducting GPVS on its bipartite graph.
For example, removing nodes R4, R9 and C7 in Figure 2(b) corresponds to permuting Row4, Row9 and Column7 to borders in Figure 1(b), and the two resulting disconnected components correspond to two diagonal blocks.
GPVS is conducted recursively on the left component, and the RBBDF matrix in Figure 1(c) is constructed.
(a) Bipartite graph (b) GPVS on the bipartite graph Figure 2: The bipartite graph for a sparse matrix and graph partitioning by vertex separator on it.
We present some de nitions, propositions and theorems in this section, which will be the basis of the LMF framework.
A matrix in BDF or (R)BBDF structure has some important properties in terms of accurate matrix factorization.
Proposition 1.
For a BDF matrix X = diag(Di) in (3), for each diagonal block Di; we then i ) as a factorization for X.
(cid:3) if we have Di = UiV T have X = diag(Ui)   diag(V T i This proposition shows the independence of diagonal blocks from each other in a BDF matrix in terms of accurate matrix factorization.
As stated above, it is not guaranteed that a sparse matrix can be permuted into BDF structure.
However, we have the following proposition for a BBDF matrix.
Proposition 2.
For a BBDF matrix X in (4), let: (cid:20) Di Ci Ri B (cid:21)  Xi (cid:44) (cid:20) Ui1 Ui2 (cid:21)(cid:2) V T i1 = UiV T i =
 i2 (7) (5) be a factorization of  Xi; thus, we have: Di = Ui1V T i1 Ri = Ui2V T i1 Ci = Ui1V T i2 B = Ui2V T i2 and let:

 .
.
.
.
.
.
Uk1 U12 U22       Uk2 Vk1 V12 V22       Vk2 (cid:3)  
   We then have:




 .
.
.
k1 Uk1V T k2 Uk1V T
 k1 Ui2V T i2   =
 .
.
.
.
.
.
Dk Ck R1       Rk kB   The only di erence between U V T and X in (4) is that the border cross B in matrix X is multiplied by the number of diagonal blocks k. (cid:3) Proposition 2 is in fact factorizing a block diagonal form (1   i   k), as matrix  X = diag(  Xi) = diag denoted in (7).
According to Proposition 1, if  Xi = UiV T i , then we have  X = diag(Ui)   diag(V T i ).
By averaging the Ri B



 .
.
.
k(cid:80) i=1 (cid:16)(cid:104) Di Ci (cid:105)(cid:17) !"(cid:1)!#(cid:1)!$(cid:1)!%(cid:1)!&(cid:1)! (cid:1)!((cid:1)!)(cid:1)!*(cid:1)+"(cid:1)+#(cid:1)+$(cid:1)+%(cid:1)+&(cid:1)+ (cid:1)+((cid:1)+)(cid:1)+*(cid:1)+",(cid:1)+""(cid:1)!"(cid:1)!#(cid:1)!$(cid:1)!%(cid:1)!&(cid:1)! (cid:1)!((cid:1)!)(cid:1)!
*(cid:1)+"(cid:1)+#(cid:1)+$(cid:1)+%(cid:1)+&(cid:1)+ (cid:1)+((cid:1)+)(cid:1)+*(cid:1)+",(cid:1)+""(cid:1)1513 


     f(cid:0)U V T(cid:1) = f      (cid:2) V T


 .
.
.
Uk .
.
.
Xk   = f (cid:3)    


 k     (6) k

 .
.
.
      UkV T .
.
.
k k







 .
.
.
.
.
.
UkV T

 duplicated submatrices, for example, submatrix B in (4), the original matrix X is reconstructed with the factorizations of  Xi = UiV T i , where 1   i   k.
This property can be generalized to an RBBDF matrix.
To avoid notational clutter, the example in (5) is again used here.
To transform X into BDF, diagonal block D1 is transformed into BDF  rst, resulting in an intermediate matrix:  Xint.
=  























 (8)     This is a BBDF matrix with 3 diagonal blocks.
By conducting the same procedure on  Xint., it is transformed into a BDF matrix (  Xij = 0 for i (cid:54)= j):  




























 (9) (cid:44) diag(  X1,  X2,  X3) Similarly,  X1,  X2 and  X3 can be factorized independently, and duplicated submatrices are averaged to reconstruct the original matrix X in (5).
In fact, (9) can be derived from (5) directly without constructing intermediate matrices.
Each diagonal block  Xi in  X corresponds to a diagonal block Di in X.
By piecing together Di with the parts of borders on the right side, down side and right bottom side,  Xi can be constructed directly.
Additionally, permuting any Di into BBDF structure recursively in (5) would not a ect other block diagonals in  X.
In practical applications, approximate matrix factorization algorithms formalized by (1) are used.
Consider the BDF matrix X = diag(Xi)(1   i   k) in terms of the approximate matrix factorization denoted by (6).
For no-tational clarity, the superscript  tilde  of  X is removed in this section.
Decomposable properties will be investigated in di erent aspects in detail in this section, as they are of core importance with respect to what types of matrix factorization algorithms the framework can handle.
In the following de nitions and theorems, Xij = 0 (i(cid:54)=j) is used to denote submatrices of X in (6), and Wij denotes the weight matrix of Xij.
f (U V T )ij denotes the sub-matrix in f (U V T ) that approximates Xij, namely, Xij   f (U V T )ij.
Speci cally, f (U V T )i is used for f (U V T )ii, and Wi is used for Wii when i = j.
(cid:110) Xi (i=j) Definition 1.
Decomposable prediction link.
A prediction link f : Rm n   Rm n is decomposable if: f (U V T )ij = f (UiV T j ) (1   i, j   k) (10) A large class of MF algorithms use element-wise prediction links for each pair of element in Y = f (X), namely, yij = f (xij).
For example, the prediction link is f (x) = x in SVD, and in NMF, f (x) = log(x).
Element-wise link functions lead to the decomposable property above naturally.
Definition 2.
Decomposable loss function.
A loss function DW (X, f (U V T )) is decomposable if: DW (X, f (U V T )) = DWi (Xi, f (U V T )i) (11) k(cid:88) i=1 This property can be viewed in two aspects here.
First, a substantial number of MF algorithms restrict D to be expressed as the sum of losses over elements, e.g., SVD[17, 33], NMF[19], MMMF[34] and PMF[26].
Various decomposable regular Bregman divergences are the most commonly used loss functions that satisfy this property [5].
The per-element e ect gives the following property: DW (X, f (U V T )) = DWij (Xij, f (U V T )ij) (11.1) (cid:88) i,j Second, rating matrices in CF tasks are usually incomplete and very sparse in practical recommender systems.
A  zero  means only that the user did not make a rating on the corresponding item, rather than rating it zero.
As a result, many MF algorithms optimize loss functions on observed ratings.
Speci cally, Wij = 0 (i (cid:54)= j) in a BDF matrix, and: (11.2) DWij (Xij, f (U V T )ij) = 0 (i (cid:54)= j) (11.1) and (11.2) gives the decomposable property of loss functions in De nition 2.
Definition 3.
Decomposable hard constraint.
A hard constraint C is decomposable if: (U, V )   C i .
(Ui, Vi)   C (1   i   k) (12) Many MF algorithms do not apply hard constraints to target factorizations, but there are MF methods that require (U, V ) to meet some special requirements.
Some commonly used hard constraints are non-negativity (the elements of U ,V are non-negative), orthogonality (the columns of U ,V are orthogonal), stochasticity (each row of U ,V sums to one), sparsity (the row vectors of U ,V meet a desired sparseness constraint) and cardinality (the number of non-zeros in each row of U ,V satis es a given constraint).
In this sense, non-negativity, stochasticity, sparsity and cardinality constraints are decomposable hard constraints.
For example, each row of (U, V ) sums to one if and only if the same property holds for any (Ui, Vi) (1   i   k).
However, orthogonality is not decomposable: the orthogonality in (U, V ) does not ensure the orthogonality in each (Ui, Vi).
Our primary focus is on decomposable hard constraints.
1514k(cid:88) Definition 4.
Decomposable regularization penalty.
A regularization penalty R(U, V ) is decomposable if:
 R(Ui, Vi) (13) The most commonly used regularization penalty is the i=1 (cid:96)p-norm regularizer, which is decomposable: R(U, V ) =  U(cid:107)U(cid:107)p p +  V (cid:107)V (cid:107)p p k(cid:88) (cid:0) U(cid:107)Ui(cid:107)p = k(cid:88) (cid:1) = p +  V (cid:107)Vi(cid:107)p p R(Ui, Vi) i=1 i=1 The Frobenius norm is (cid:96)p-norm where p = 2.
The basic MMMF algorithm takes the trace-norm (cid:107)X(cid:107)  (the sum of singular values of X) [34], which is unfortunately not a de-composable regularizer.
However, a fast MMMF algorithm F + (cid:107)V (cid:107)2 based on the equivalence (cid:107)X(cid:107)  = min
 is proposed in [23], which also takes (cid:96)p-norm regularizers.
Definition 5.
Decomposable matrix factorization.
A matrix factoirzation algorithm P = (f,DW ,C,R) is decom-posable if f,DW ,C,R are decomposable.
Namely, properties (10) (13) hold.
(U, V ) = P(X, r) denotes the factorization of X by P using r factors.
It is necessary to point out that many commonly used MF algorithms are decomposable, including some of the state-of-the-art techniques, although they are required to satisfy all these four decomposable properties, which seems to be somewhat too strict.
Some typical examples are SVD, NMF, PMF, MMMF and their variations, which will be primarily considered and investigated in this work.
Theorem 1.
Suppose X is a BDF matrix in (6), and P = (f,DW ,C,R) is decomposable.
Let (U, V ) = P(X, r) and (Ui, Vi) = P(Xi, r)(1   i   k).
We have:

 i. U = [U T ii.
Xij   f (UiV T Proof.
i.
Consider the optimization problem de ned in (1) with decomposable properties of prediction link f , loss function DW , hard constraint C, and regularizer R; we have: k ]T , V = [V T j ) (1   i, j   k)





 k ]T (U, V ) = P(X, r) i=1 (cid:104)DW (X, f (U V T )) + R(U, V ) (cid:105) (cid:104)DWi (Xi, f (U V T )i) + R(Ui, Vi) (cid:105) k(cid:88) (cid:104)DWi (Xi, f (UiV T (cid:105) k(cid:88) i )) + R(Ui, Vi) (cid:104)DWi (Xi, f (UiV T i )) + R(Ui, Vi) (cid:111) k(cid:94) (cid:110) (Ui, Vi) i=1 (cid:105)(cid:41) = argmin
 = argmin
 = argmin
 k(cid:94) k(cid:94) i=1 i=1 i=1 argmin (Ui,Vi) C (cid:40) (cid:110)P(Xi, r) (cid:111)



 = = = thus, U = [U T k ]T and V = [V T



 k ]T .
ii.
This can be derived directly from the decomposable property of prediction link f in (10): Xij   f (U V T )ij = f (UiV T j ) and it holds for any 1   i, j   k, including zero submatri-ces where i (cid:54)= j.
According to Theorem 1, each diagonal block can be factorized independently, and the results can be used directly to approximate not only the nonzero diagonal blocks but also the zero o diagonal blocks.
Consider predicting the missing values of an incomplete sparse rating matrix through the LMF framework.
A sparse rating matrix is permuted into an RBBDF matrix (5)  rst and further transformed into a BDF matrix (9).
LMF is then performed on the resulting BDF matrix to make rating predictions for the original matrix.
Suppose an RBBDF matrix X is transformed into a BDF matrix  X = diag(  Xi)(1   i   k).
XI J  and  XI J  are used to denote the submatrices in X and  X correspondingly.
For example, R12 = XIB1  J12 in (5), and it is duplicated twice by  XIB1  J12 in (9).
The LMF framework approximates the original matrix X through the approximations of  X with three steps: i.
For a decomposable matrix factorization algorithm P = (f,DW ,C,R), obtain the factorization (Ui, Vi) = P(  Xi, r) of each diagonal block  Xi.
Then: i ) (cid:44)  X  Xi   f (UiV T   i (cid:44)  X (14)   ii where  X  i denotes the approximation of  Xi.
ii.
Predict zero blocks  Xij(i (cid:54)= j) in  X using factorizations of  Xi and  Xj: Now  X  (cid:44)(cid:8)  X   Xij   f (UiV T ij|1   i, j   k(cid:9) approximates  X.
j ) (cid:44)  X   ij (15) iii.
Average duplicated submatrices in  X  to approximate the corresponding submatrix in X.
Suppose that XI J  is duplicated k times in  X, and the tth duplication is in block  Xitjt , whose approxima- (itjt) tion is  X I J  .
Then the approximation of XI J  is: k(cid:88) t=1  


 k
  (itjt)
 (16) To make it easier to understand, take R12 = XIB1  J12 in  J12 is duplicated twice in (9): one (5) as an example.
XIB1 in  X12 and the other in  X22.
As a result:  


 Approximation X 
  (12)




  (22)

 ) I J  is constructed for each submatrix XI J  in X.
By piecing them together, approximation

 I J } is  nally achieved for X.
As shown in Section 3.3, permuting a matrix into (R)BBDF structure is equivalent to performing GPVS (recursively) on its bipartite graph.
In this work, both the performance and e ciency of graph partitioning algorithms are concerned, as the datasets to experiment on are huge1.
As a result, multilevel graph partitioning approach is chosen.
Perhaps the
 DCUP 2011, containing approximately 1m users and 0.6m items, which is the largest in present-day datasets.
1515most widely known and used package for graph partitioning is Metis [13] by Karypis, which is based on multilevel approach.
The core routine for GPVS in Metis is Node-based Bisection, which partitions a graph into two disconnected components by a vertex separator.
A density-based algorithm to permute sparse matrices into RBBDF structure is designed, as dense submatrices or sub-graphs are usually interpreted as communities, which is widely used in community detection and graph clustering problems [8].
The density of a matrix X is de ned as  (X) = n(X) s(X) , where n(X) is the number of non-zeros in X, and s(X) is the area of X.
The average density of k matrices X1X2   Xk is de ned as  (X1X2   Xk) = .
Note that the density of a matrix is equal to the density of its corresponding bipartite graph [8].
For an RBBDF matrix X with k diagonal blocks D1D2   Dk i=1 n(Xi) i=1 s(Xi) (cid:80)k (cid:80)k (e.g., the matrix in (5) has 3 diagonal blocks: D11D12 and D2, and the original rating matrix is viewed as a single diagonal block),  X = diag(  X1  X2    Xk) is used to denote its corresponding BDF matrix (e.g., the matrix in (9)).
Algorithm 1 shows the procedure, followed by more detailed explanations and analyses.
RBBDF(X,  , 1) is called to start the procedure.
Algorithm 1 RBBDF(X,  , k) Require: User-item rating matrix: X Average density requirement:   Current number of diagonal blocks in X: k Ensure: Matrix X be permuted into RBBDF structure BDF matrix  X which is constructed from X
 2: if       then
 4: else
 return  X  Density requirement has been reached [Ds1 Ds2   Dsk ]   Sort(cid:0)[D1D2   Dk](cid:1)  Sort diagonal blocks by size in decreasing order for i   1 to k do si D2 si ]   MetisNodeBisection(Dsi )  Partition
 Dsi into 2 diagonals using core routine of Metis if  (  Xs1    Xsi 1  Xsi+1    Xsk ) >   then X(cid:48)   Permute Dsi into [D1 si D2 RBBDF(X(cid:48),  , k + 1)  Recurse break  No need to check the next diagonal si ] in X
 si
 si end if end for return  X  No diagonal improves average density








 15: end if Algorithm 1 requires a  density requirement    as input, which is the expected average density of submatrices  X1    Xk in the  nal BDF matrix  X.
In each recursion, the algorithm checks each diagonal block Di of X in decreasing order of matrix areas.
If the average density of extracted submatri-ces can be improved by partitioning a diagonal block, then the algorithm takes the partitioning and recurses, until   is reached or none of the diagonal blocks can improve the average density any more.
Note that, to gain a high e ciency, a fundamental heuristic is used in this algorithm, which is the area of diagonal blocks, and we explain its rationality here.
(a)  Xi for Di (b)  X 1 i and  X 2 i for D1 i and D2 i Figure 3: Partition a diagonal block, rearrange its corresponding borders, and extract new submatri-ces.
The shaded areas represent nonzero blocks.
i and D2 i and  X 2 Figure 3(a) shows a diagonal block Di along with its corresponding borders.
Note that this  gure is, in fact, the submatrix  Xi in  X corresponding to Di.
Two new subma-trices  X 1 i are constructed when Di is partitioned into D1 i , which are boxed by dotted lines in Figure
 ure 3(b) constitutes the original submatrix  Xi.
As a result, transforming  Xi to  X 1 is essentially removing the two zero blocks and replacing them with some duplicated nonzero blocks.
t=1 s(  Xt) and n = (cid:80)k t=1 n(  Xt); let  s1 be the total area of removed zero blocks,  s2 be the total area of duplicated nonzero blocks, and  n be the number of nonzeros in  s2.
The increment of average density after partitioning Di is: (cid:48)     = Let s = (cid:80)k i and  X 2 n +  n   =   (17) = i s    s1 +  s2   n s s n + n s s(s    s) where   and  (cid:48) are the average densities of diagonal blocks in  X before and after partitioning Di, and  s (cid:44)  s1    s2.
Because s    s > 0, we have the following:   > 0   s n + n s = s n + n( s1    s2) > 0 If  s > 0, then (18) holds naturally.
Otherwise, the fol-(18) lowing is required: n s <  n  s2    s1 (19) Although not guaranteed, (19) can usually be satis ed as the following property usually holds: n s <  n  s2 <  n  s2    s1 (20) Intuitionally, (20) means that the density of duplicated nonzero blocks after partitioning is usually greater than the original average density of k submatrices  X1  X2    Xk, as the latter contains many zero blocks.
Additionally, a large  s tends to yield a large density increment   according to (17), which leads to adopting areas of diagonal blocks as heuristics.
It will be veri ed experimentally that this heuristic improves the average density at the  rst attempt nearly all the time.
The time complexity of Node-based bisection in Metis is O(n), where n is the number of non-zeros in a matrix [14].
Suppose that matrix X is permuted into an RBBDF structure which has k diagonal blocks (k (cid:28) n) in the end, and the algorithm chooses the largest diagonal block for partitioning in each recursion; then, the height of the recursion tree will be O(lg k) and the total computational cost in each level of the tree is O(n).
As a result, the time complexity of Algorithm 1 is O(n lg k).
Di(cid:1)Di(cid:1)D(cid:1)2(cid:1)i(cid:1)1(cid:1)!"#(cid:1)$!
"%(cid:1)Di(cid:1)Di(cid:1)!& (cid:1)1(cid:1)2(cid:1)15165.
EXPERIMENTS
 A series of experiments were conducted on four real-world datasets: MovieLens-100K, MovieLens-1M, DianPing and Yahoo!Music.
The MovieLens dataset is from GroupLens Research.
We also collected a year s data from a famous restaurant rating web site DianPing 2 from Jan. 1st to Dec.
31st 2011 and selected those users with 20 or more ratings.
The ratings also range from 0 to 5.
The Yahoo!Music dataset is from KDD Cup 2011, and its ratings range from 0 to 100.
Statistics of these datasets are presented in Table 1.
Table 1: Statistics of four datasets density of  X1  X2    Xk is  k, and the average density goes to  k+1 after partitioning a diagonal block.
Then, the relative density increment is  / 1 = ( k+1    k)/ 1, where the constant  1 is the density of the whole original matrix.
Experimental results show that the relative density increment becomes lower and lower as the number of diagonal blocks increases.
As a result, it is relatively easy to improve the average density at the beginning, as partitioning a large diagonal block Di gains a large density increment  .
However, this process tends to be more and more di cult when diagonal blocks become small.
The experimental result is in accordance with the analysis in (17) (20).
These results partially verify the heuristic used in Algorithm 1.
ML-100K ML-1M DianPing Yahoo!Music





 #users #items #ratings #ratings/user #ratings/item average density These datasets are chosen as they have di erent sizes and densities.
Besides, two of them have more users than items, and the others are the opposite.
We expect to verify how our framework works on datasets of di erent sizes and densities.
Four popular and state-of-the-art matrix factorization algorithms are the subject of experimentation in this work: SVD: The Alternating Least Squares (ALS) algorithm in [17] is used for SVD learning.
NMF: The NMF algorithm based on divergence cost in [19] is used.
We also use F-norm regularizer, similar to SVD.
PMF: The Bayesian PMF by Markov Chain Monte Carlo method in [25] is used.
MMMF: The fast version of MMMF in [23] is used.
For easier comparison with previous proposed methods in the literature, we use Root Mean Square Error (RMSE) to measure the prediction accuracy in this work.
For N rating-prediction pairs (cid:104)ri,  ri(cid:105): (cid:115)(cid:80)N
 i=1(ri    ri)2
 Five-fold cross validation is conducted to calculate the average RMSE for MovieLens and DianPing.
The Yahoo!
Music dataset itself is partitioned into training and validation sets, which are used for training and evaluation, respectively.
The only parameter to be tuned in the RBBDF algorithm is the average density requirement  .
Intuitionally, a low density requirement gives less and larger diagonal blocks in  X, and vice versa.
The relationship between the number of diagonal blocks k and the density requirement   on four datasets is shown by red solid lines in Figure 4.
We see that the number of diagonal blocks increases faster and faster with an increasing density requirement.
To investigate the underlying reason, a more straightforward view is given by the relative density increment in Figure 5.
Suppose that the current number of diagonal blocks is k; the average 2http://www.dianping.com (a) ML-100K & ML-1M (b) DianPing & Yahoo!Music Figure 5: Relationship between the Relative Density Increment  / 1 and the Current number of diagonal blocks k on four datasets.
The First Choice Hit Rate (FCHR) is used to verify the heuristic used in the RBBDF algorithm, as we expect the average density to be improved by partitioning the  rst diagonal block Ds1 in the sorted list [Ds1 Ds2   Dsk ], in which case there is no need to check the remaining diagonal blocks.
# recursions where Ds1 is chosen # recursions in total One can see that FCHR = 1 means that average density can always be improved by partitioning the largest diagonal block directly.
The relationships between FCHR and density requirement on four datasets are shown by blue dotted lines in Figure 4.
On all of the four datasets, FCHR = 1 at the beginning and begins to drop when density requirement is high enough, which is also in accordance with the analysis in Section 4.3.
As a result, by taking the areas of diagonal blocks as a heuristic, only one diagonal block is partitioned in each recursion, and there is no redundant computation when an appropriate density requirement is given.
When the density requirement is high, we have FCHR < 1, and redundant computation will be introduced: we might partition a diagonal block without improving the average density.
However, we would like to note that it does not matter very much in practice.
First, when considering the O(n) complexity of the Node-based Bisection, it will be faster and faster to partition diagonal blocks as they become smaller.
Second, there is no need to split a matrix into hundreds or even thousands of diagonal blocks in practice.
According to our experiments in the following sections, it is su cient to gain both high prediction accuracy and computational e ciency by partitioning a matrix into a small number of diagonal blocks, in which case we have FCHR = 1.
Experiments were conducted on an 8-core 3.1GHz linux server with 64G RAM.
We tuned the density requirement  
 (b) ML-1M (c) DianPing (d) Yahoo!
Music Figure 4: Number of Diagonal Blocks (#DB, solid lines) and First Choice Hit Rate (FCHR, dotted lines) under di erent density requirements  .
The tuning steps of   are 0.01, 0.01, 0.0008 and 0.0004, respectively.
to achieve the expected number of diagonal blocks k. The run time of the RBBDF algorithm is shown in Table 2.
In the experiments, we see that the run time increases along with the number of diagonal blocks, and it takes less time to partition a submatrix as they become smaller.
Moreover, the time used by the RBBDF algorithm is much less than that used for training an MF model on matrix X.
We will show the results on model training time in Section 5.5.
Table 2: Computational time of the RBBDF algorithm with di erent numbers of diagonal blocks k.
k ML-100K / ms ML-1M / s DianPing / s Yahoo!
/ min
























 The number of latent factors r plays an important part in MF algorithms.
It would be insu cient for approximating a matrix if r is too low, and would be computationally expensive if r is too high.
As the diagonal blocks in  X and the original matrix X are of di erent sizes, it s important to investigate how to choose a proper r in practical applications.
We use MovieLens-1M to test the impact of r in the LMF framework.
The density requirement is   = 0.055, and matrix X is permuted into an RBBDF structure with 4 diagonal blocks; then,  X = diag(  X1,  X2,  X3,  X4) is constructed.
Some statistical information about  X is shown in Table 3.
Table 3: Statistics of the four diagonal blocks




 #users #items
 #ratings 118,479 259,665 462,586 192,267 density









 We tuned r from 5 to 100, with a tuning step of 5.
It s necessary to note that r is required to be comparable with min(m, n) in MMMF, where m and n are the numbers of users and items in X.
However, it would be time consuming to train a model using thousands or even millions of factors.
Fortunately, according to [23], it s su cient to use much smaller values of r to achieve satisfactory performance in practice (r = 100 is used in [23] for ML-1M).
As a result, the tuning range of 5   100 is also used for MMMF.
For each of the four MF algorithms, two sets of experiments were conducted.
First, we approximate the original matrix X using r factors directly, and record the RMSE.
Second, predictions are made by the LMF framework in Section
 Cross-validation is performed on X to  nd the best regularization parameters for each MF method.
In SVD and NMF,   is set to 0.065; in PMF,  U and  V are both 0.002; and in MMMF, the regularization constant C is set to 1.5.
RMSE v.s.
the number of latent factors r is shown in Figure 7.
Experimental results show that better performance in terms of RMSE can be achieved in the LMF framework.
Furthermore, the improvement tends to be more obvious when the number of latent factors r is relatively small.
This result could arise because, in such cases, r is not su cient to approximate the original matrix X, while it is su cient to approximate relatively small submatrices in  X.
We view this as an advantage of the LMF framework, as better performance can be achieved with fewer factors, which bene ts the model complexity and training time.
The  nal number of diagonal blocks k in  X is di erent under di erent density requirements  .
We experimented RMSE with di erent density requirements.
The number of latent factors r is set to 60, as we  nd it su cient to smooth the performance improvement on the datasets.
The regularization coe cients are the same:   = 0.065 for SVD and NMF,  U =  V = 0.002 for PMF, and C = 1.5 for MMMF.
The RMSE versus di erent choices of   on all of the four datasets are plotted in Figure 6.
In each sub gure, the four curves correspond to the four MF methods used, which are SVD, NMF, PMF and MMMF.
The density requirement on the  rst point of each curve is the average density of the corresponding dataset; as a result, RMSE on this point is the baseline performance for the matrix factorization algorithm.
Thus, points below the beginning point of a curve indicate an improvement on prediction accuracy, and vice versa.
Experimental results show that our LMF framework helps decomposable MF algorithms to gain better prediction ac-curacies if appropriate density requirements are given, but might bring negative e ects if   is not appropriately set.
(a) SVD & NMF (b) PMF & MMMF Figure 7: RMSE v.s.
di erent numbers of latent factors.
Solid/dotted lines are results of approximating X directly/through the LMF framework.
(b) MovieLens-1M (c) DianPing (d) Yahoo!
Music Figure 6: RMSE on four datasets using the LMF framework under di erent density requirements.
Dotted lines in each sub gure represent baseline performance of the corresponding matrix factorization algorithm.
Here, by  appropriate , we mean that   is not too high.
According to the experimental results on four datasets, better prediction accuracies are achieved along with an increasing   (and also the number of diagonal blocks k) at the beginning, but the performance tends to drop when   is set too high.
In our view, this is not surprising because many small scattered diagonal blocks are extracted when a high density requirement is set, which would bring negative e ects to MF algorithms.
Table 4 presents the average number of users and items in the diagonal blocks of  X under di erent   on MovieLens-1M.
We see that the average number of users goes to only a hundred or less when     0.1.
Table 4: Average number of users and items in diagonal blocks of  X under di erent   on MovieLens-1M.
 
 k
 Avg #users Avg #items





















 However, better performance is achieved given appropriate density requirements.
By combining this observation with the experimental results in Section 5.3, it is neither reasonable nor necessary to use high density requirements that result in hundreds or even thousands of diagonal blocks.
Table 5 shows the best RMSE achieved on each dataset for each MF method.
To calculate the average RMSE on each dataset,  ve-fold cross-validation was conducted on Movie-Lens and DianPing, and experiments were conducted  ve times on Yahoo!
Music.
The standard deviations were  
 hoo!
Music.
We see that, in the best cases, MF algorithms bene t from the LMF framework in terms of RMSE on all of the four datasets.
Speci cally, the sparser a matrix is, the higher RMSE increment LMF tends to gain.
An important advantage of LMF is that, once the BDF matrix  X = diag(  Xi) is constructed, diagonal blocks  Xi can be trained in parallel.
According to the decomposable property in Theorem 1, sub-problems of learning di erent (Ui, Vi) are not coupled; as a result, there is no need to implement rather complicated parallel computing algorithms.
In fact, simple multi-threading technique is adequate for the task, which contributes to the scalability of recommender systems while, at the same time, keeps system simplicity.
The experiment comprises three stages.
In the  rst stage, X is permuted into an RBBDF structure, and a BDF matrix  X = diag(  Xi)(1   i   k) is constructed.
As we have 8 cores, the density requirement is tuned on each dataset to construct  X with 8 diagonal blocks.
In the second stage, each diagonal block is factorized independently with a thread, and (Ui, Vi) is achieved.
In the last stage, (Ui, Vi) from all of the diagonal blocks are used to approximate the original matrix X by LMF.
The computational time consumed in each stage is recorded (in the second stage, the time recorded is the longest among all of the diagonal blocks).
Finally, the total time of the three stages is adopted to evaluate the overall e ciency.
The number of factors and the regularization co-e cients are the same as those in Section 5.4.2.
The results are shown in Table 6, where  Base  represents the computational time of factorizing X directly,  LMF  is the time used by the LMF framework, and  Speedup  is  Base/LMF .
Table 6: Computational time and speedup by multi-threading with 8 diagonal blocks.
MovieLens-100K MovieLens-1M Method Base 23.9s 8.7s 43.8s


 MMMF 19.6min 4.71min
 7.7s 3.9s 11.6s Speedup Base 184.9s 86.6s 265.1s 1.73h 21.5min
 43.4s 22.1s 60.1s



 Speedup



 DianPing Yahoo!Music Method Base 143.7s 64.4s 190.5s


 MMMF 48.5min 10.2min

 16.6s 44.1s Speedup Base 6.22h 4.87h 7.91h 38.8h




 1.21h 1.05h 1.48h 6.22h Speedup



 Experimental results show that the LMF framework helps to save a substantial amount of model training time through very simple multithreading parallelization techniques.
This is especially helpful when learning large magnitude datasets, which is important in real-world recommender systems.
Unlike benchmark datasets, rating matrices in real-world recommender systems usually change dynamically as new ratings are made by users continuously.
A typical way to settle this problem in practice is to retrain MF models periodically or when a prede ned prediction accuracy threshold (say RMSE) is reached.
However, it would be time consuming to refactorize large rating matrices as a whole and to do so frequently.
In the LMF framework, however, it is possible to only refactorize those submatrices whose prediction accuracies have reached a prede ned threshold, rather than refactorize the whole matrix, which further bene ts system scalability.
This potential advantage that LMF might bring about will be investigated both by simulation and in practical real-world recommender systems in future work.
In this paper, we explored the BDF, BBDF and RBBDF structures of sparse matrices and their properties in terms of matrix factorization.
The LMF framework is proposed, and to explicitly indicate the scope of matrix factorizations
 blocks k. Bold numbers indicate improvements that are   0.01 on MovieLens and DianPing or   0.2 on Yahoo!Music.
The standard deviations are   0.002 on MovieLens and DianPing and   0.01 on Yahoo!Music.
Method



 MovieLens-100K MovieLens-1M DianPing Yahoo!Music   k RMSE baseline baseline






   k RMSE baseline   k RMSE baseline k RMSE








   that the framework can handle, decomposable properties of matrix factorization algorithms were investigated in detail.
Based on graph partitioning theories, we designed a density-based algorithm to permute sparse matrices into RBBDF structures, and studied its algorithmic properties both formally and experimentally.
Experimental results show that LMF helps the matrix factorization algorithms we studied to gain better performance and, at the same time, contributes to system scalability by simple parallelization techniques.
The authors thank Jun Zhu for the fruitful discussions and the reviewers for their constructive suggestions.
This work was supported by Natural Science Foundation (60903107,
 velopment (863) Program (2011AA01A205) of China.
