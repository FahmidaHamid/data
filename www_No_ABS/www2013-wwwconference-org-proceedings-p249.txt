Classic approaches to document indexing, clustering, clas-si cation and retrieval are based on the bag-of-words paradigm.
The limitations of this paradigm are well-known to the IR community and in recent years a good deal of work has attempted to move beyond by  grounding  the processed texts with respect to an adequate semantic representation, by designing so-called entity annotators.
The key idea is to identify, in the input text, short-and-meaningful sequences of terms (also called mentions) and annotate them with unambiguous identi ers (also called entities) drawn from a catalog.
Most recent work adopts anchor texts occurring in Wikipedia as entity mentions and the respective Wikipedia pages as the mentioned entity, because Wikipedia o ers today the best trade-o  between catalogs with a rigorous struc-
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
ture but low coverage (such as WordNet, CYC, TAP), and a large text collection with wide coverage but unstructured and noisy content (like the whole Web).
The process of entity annotation involves three main steps: (1) parsing of the input text, which is the task to detect candidate entity mentions and link each of them to all possible entities they could mention; (2) disambiguation of mentions, which is the task of selecting the most pertinent Wikipedia page (i.e., entity) that best describes each mention; (3) pruning of a mention, which discards a detected mention and its annotated entity if they are considered not interesting or pertinent to the semantic interpretation of the input text.
The focus around entity annotators has increased significantly in the last few years, with several interesting and e ective algorithmic approaches to solve the mention-entity match problem, possibly using other knowledge bases such as DBpedia, Freebase or Yago (see e.g.
[2, 3, 7, 9, 13, 16,
 tigated some speci c tasks using nonuniform terminology, non-comparable evaluation metrics, and limited datasets and systems.
As a consequence, we only have a partial picture of the e ciency and e ectiveness of known annotators which makes it di cult to compare them in a fair and complete way.
This is a particularly important issue because those systems are being used as black-boxes by more and more IR tools, built on top of them, such as [1, 8, 21, 22].
Motivated by these considerations, [20] recently attempted to compare entity-annotation systems mainly coming from the commercial realm2.
However, as the authors state in the concluding section of their paper, their evaluation is limited to strict metrics which account for  exact  matches over mentions and entities (and, rather observe that  a NE type might be not wrong but not precise enough. ), it does not consider datasets fully annotated by humans (i.e. mentions are only the ones derived by few parsers), and misses to consider the best performing tools which have been published recently in the scienti c literature.
This last issue is a crucial limitation because, as [12] showed recently, the DBpedia Spotlight system (the best according to [20]) achieves much worse performance than some of the systems tested in this paper.
Given this scenario, we aim with this paper at de ning and implementing a framework for comparing in a complete, fair and meaningful way the most e cient, e ective and publicly available entity-annotation systems: namely, AIDA [7], Illinois Wiki er [19], TagMe [3], Wikipedia-miner [16], and
 dia, OpenCalais, saplo, Wikimeta, Yahoo!
Content Analysis and Zemanta.
Disambiguate to Wikipedia (D2W) Annotate to Wikipedia (A2W) Input Text, Set of mentions Text Output Set of tionsw Set of tions relevant annota-relevant annota-Scored-annotate to Wikipedia (Sa2W) Text Set of relevant and scored annotations Concepts to Wikipedia (C2W) Text Set of relevant tags Scored concepts to Wikipedia (Sc2W) Text Ranked-concepts to Wikipedia (Rc2W) Text Set of relevant and scored tags.
Ranked list of tags relevant Description Assign to each input mention its pertinent entity (possibly null).
This problem has been introduced in [2].
Identify the relevant mentions in the input text and assign to each of them the pertinent entities.
This problem has been introduced in [16].
As A2W, but here each annotation is assigned a score representing the likelihood that the annotation is correct.
This problem has been introduced in [16].
Tags are taken as the set of relevant entities that are mentioned in the input text.
This problem has been de ned in [13].
As C2W, but here each tag is assigned a score representing the likelihood that the annotation is correct.
Identify the entities mentioned in a text and rank them in terms of their relevance for the topics dealt with in the input text.
This problem has been de ned in [13].
Table 1: A set of entity-annotation problems.
DBpedia Spotlight; which currently de ne the state-of-the-art for the entity-annotation task.
In order to achieve this goal, we will introduce (1) a hierarchy of entity-annotation problems, that cover the wide spectrum of annotation goals such systems could address; (2) a set of novel measures to  nely evaluate the e ectiveness of these systems; (3) all public datasets available for the entity-annotation task, allowing us to explore the e ciency and e ectiveness performance of the annotation process according to the introduced measures and hierarchy of problems.
We have made this framework publicly available (http: //acube.di.unipi.it/), with all datasets and software used in this paper, to make our experiments reproducible and provide a common ground for developing new and better solutions for this challenging problem.
which is uniquely identi ed by its page-ID.
We use the following terminology:   An entity (or concept, topic) is a Wikipedia article   A mention is the occurrence of a sequence of terms located in a text.
It is encoded by the integer pair (cid:2)p, l(cid:3), where p is the position of the occurrence and l is the length of the mention.
  An annotation is the linking of a mention to an entity.
It is encoded by the pair (cid:2)m, e(cid:3) where m = (cid:2)p, l(cid:3) is the mention and e is the page-ID of the mentioned entity.
  A tag is the annotation of a text with an entity which captures a topic (explicitly mentioned) in the input text.
(Thus, a tag comes with no mention.)
  A score is a real value s   [0, 1] that is assigned to an annotation or a tag to indicate its correctness.
As an example, consider the following text fragment:  Obama issues Iran ultimatum .
 Obama ,  Iran  and  ultimatum  are mentions which occur as anchors in Wikipedia and can thus be linked with the entities represented by the Wikipedia pages of Barack Obama, the nation of Iran and the threat to declare war, respectively.
Figure 1 and Table 1 exemplify and formalize, respectively, a set of problems which cover the wide spectrum of goals the entity-annotation systems could be asked to solve.
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13) (cid:14)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:6)(cid:4)(cid:5)(cid:3)(cid:4)(cid:5) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:1)(cid:6)(cid:7)(cid:7) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:35)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:24)(cid:31)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:34)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:24)(cid:36)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:32)(cid:36)(cid:15)(cid:16) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26) (cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26) (cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19) (cid:17)(cid:18)(cid:12)(cid:19)(cid:2)(cid:9)(cid:20)(cid:12)(cid:11)(cid:19)(cid:10)(cid:12)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:24)(cid:18)(cid:22)(cid:8)(cid:12)(cid:25)(cid:19)(cid:26)(cid:18)(cid:12)(cid:8)(cid:12)(cid:19)(cid:5)(cid:18)(cid:12) (cid:27)(cid:9)(cid:10)(cid:10)(cid:22)(cid:5)(cid:19)(cid:28)(cid:8)(cid:9)(cid:29)(cid:9)(cid:19)(cid:30)(cid:31)(cid:21)(cid:21)(cid:22)(cid:2)(cid:23)(cid:19)(cid:22)(cid:2)(cid:18)(cid:12)(cid:8)(cid:22)(cid:5)(cid:23)(cid:19)(cid:5)(cid:18)(cid:12)(cid:19)(cid:32)(cid:22)(cid:2)(cid:21) (cid:33)(cid:8)(cid:9)(cid:13)(cid:19)(cid:30)(cid:22)(cid:11)(cid:10)(cid:9) (cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26) (cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26) (cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19) (cid:37)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:25)(cid:19)(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)(cid:25)(cid:19)(cid:20)(cid:2)(cid:21)(cid:21)(cid:10)(cid:18)(cid:25) (cid:12)(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:25)(cid:19)(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)(cid:25)(cid:19)(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:38) (cid:37)(cid:39)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:25)(cid:19)(cid:40)(cid:41)(cid:42)(cid:43)(cid:25)(cid:19)(cid:39)(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19)(cid:25)(cid:19)(cid:40)(cid:41)(cid:44)(cid:43)(cid:25) (cid:39)(cid:20)(cid:2)(cid:21)(cid:21)(cid:10)(cid:18)(cid:25)(cid:19)(cid:45)(cid:41)(cid:40)(cid:43)(cid:25)(cid:19)(cid:39)(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:25)(cid:19)(cid:45)(cid:41)(cid:40)(cid:43)(cid:25) (cid:39)(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24)(cid:25)(cid:19)(cid:40)(cid:41)(cid:44)(cid:43)(cid:25)(cid:19)(cid:39)(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26)(cid:25)(cid:19)(cid:40)(cid:41)(cid:42)(cid:43)(cid:38) (cid:29)(cid:30)(cid:12)(cid:20)(cid:2)(cid:21)(cid:21)(cid:10)(cid:18) (cid:31)(cid:30)(cid:12)(cid:22)(cid:11)(cid:2)(cid:15)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26) (cid:32)(cid:30)(cid:12)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:33)(cid:30)(cid:12)(cid:23)(cid:10)(cid:5)(cid:21)(cid:2)(cid:12)(cid:23)(cid:17)(cid:24)(cid:24)(cid:10)(cid:25)(cid:26) (cid:34)(cid:30)(cid:12)(cid:27)(cid:25)(cid:4)(cid:12)(cid:28)(cid:10)(cid:25)(cid:24) (cid:35)(cid:30)(cid:12)(cid:8)(cid:9)(cid:10)(cid:11)(cid:4)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:15)(cid:5)(cid:4)(cid:16)(cid:4)(cid:17)(cid:11)(cid:18)(cid:9)(cid:19) Figure 1: Some instances of annotation tasks.
Numbers in problems Sa2W and Sc2W denote the correctness likelihood of each annotation/tag.
Some of these problems have been introduced in the literature, others are introduced in this paper as reasonable variations which complete the picture.
This will allow us to contextualize and compare the known systems, and assess their relations and limitations.
These problems can be casted in two main classes: the  rst consists of three problems which address the identi cation of (possibly scored) annotations, and thus the identi cation of mention-entity pairs; the second consists of three further problems that involve  nding tags (possibly scored or ranked), thus accounting only for the entities.
Given that these annotation problems are strictly related to each other, it is possible to introduce a few simple reductions that induce a hierarchy among them.
Such a hierarchy, in the form of a DAG, will ease the de nition of our framework and the setup of the experiments for comparing the annotators.
We adopt the notation A   B to indicate that 250251length, but is quite slow because of the quadratic programming approach.
Currently, this system can not be accessed or downloaded.5 links.
Wikipedia-Miner has been designed to deal with English documents of arbitrary length, it o ers a publicly available API (as of September 2012).9 Illinois Wiki er searches the input text for mentions extracted from Wikipedia anchors and titles, using the Illinois NER system [18].
Disambiguation is formulated as an optimization problem which aims at global coherence among all mentions.
It uses a novel relatedness measure between Wikipedia pages based on NGD (Normalized Google similarity distance) and pointwise mutual information.
Wiki er has been designed to deal with English documents of arbitrary length, its software can be downloaded (as of August 2012).6 DBpedia Spotlight searches the input text for mentions extracted from Wikipedia anchors, titles and redirects; the parser is the LingPipe Exact Dictionary-Based Ch-unker.
It then associates a set of candidate entities to each mention using the DBpedia Lexicalization dataset.
Given a spotted mention and a set of candidate entities, both the context of the mention and all contexts of each entity are cast to a Vector-Space Model (using a BOW approach) and the candidate whose context has the highest cosine similarity is chosen.
Note that no semantic coherence is estimated among the chosen entities.
Spotlight has been designed to deal with English documents of arbitrary length, it o ers a publicly available API (as of November 2012).7 TagMe 2 searches the input text for mentions de ned by the set of Wikipedia page titles, anchors and redirects.
Each mention is associated with a set of candidate entities.
Disambiguation exploits the structure of the Wikipedia graph, according to the relatedness measure introduced in [17] which takes into account the amount of common incoming links between two pages.
TagMe s disambiguation is enriched with a voting scheme in which all possible bindings between mentions and entities are scored and then they express a vote for each other binding.
A proper mix of heuristics is eventually adopted to select the best annotation for each mention.
TagMe has been designed to deal with short texts, it o ers a publicly available API.8 Wikipedia Miner is an implementation of the Wiki cation algorithm presented in [16, 11], one of the  rst approaches proposed to solve the entity-annotation problem.
This system is based on a machine-learning approach that is trained with links and contents taken from Wikipedia pages.
Three features are then used to train a classi er that selects valid annotations discarding irrelevant ones: (i) the prior probability that a mention refers to a speci c entity, (ii) the relatedness with the context from which the entity is extracted, given by the non-ambiguous spotted mentions, and (iii) the context quality which takes into account the number of terms involved, the extent they relate to each other, and how often they are used as Wikipedia
 able, we used their datasets to compare the other annotators.
http://www.mpi-inf.mpg.de/yago-naga/aida/.
http://spotlight.dbpedia.org/rest/annotate.
http://acube.di.unipi.it/tagme.
been provided in August 2012.
Its release 2.0 has Dataset



 Meij Gold Stan-dard




 Num Docs Avg length Num Anns Avg Anns/Doc



















 Table 3: Gold standard indicates the type of the dataset.
Num Docs is the number of documents in the dataset, Avg length is their average length, expressed in characters.
Num Anns is the total number of annotations or tags.
Avg Anns/Doc is the average number of annotations or tags per document.
For our experiments we collected all publicly available datasets that have been evaluated in the literature and we used them to test all reviewed entity-annotation systems.
Details about these datasets are given below and in Table 3.
We observe that they range from news to tweets, up to Web pages, providing a wide spectrum of text sources.
AIDA/CoNLL builds on the CoNLL 2003 entity-recogni-tion task.
Documents are news taken from Reuters Corpus V1.
A large subset of mentions referring to named entities are annotated, but the common names are not.
Entities are annotated at each occurrence of a mention.
The dataset was introduced in [7].
This dataset is divided into three chunks: Training, Test A and Test B.
Since the AIDA system has been tuned over the  rst two chunks, we will use only Test B, made up of 231 documents, for our experiments.
AQUAINT contains a subset of the original AQUAINT corpus, consisting of English newswire texts.
Not all occurrences of the mentions are annotated: only the  rst mention of each entity, and only the most important entities are retained.
This re ects the Wikipedia-style linking.
The dataset was introduced in [16].
IITB contains over a hundred of manually annotated texts drawn from popular Web pages about sport, entertainment, science and technology, and health [9].
This is the most detailed dataset since almost all mentions, including those whose related concepts are not highly relevant, are annotated.
Meij contains tweets annotated with all occurring entities.
Tweets are poorly composed and very short, less than
 MSNBC contains newswire text from MSNBC news network.
It annotates only important entities and their referring mentions.
The dataset was introduced in [2].
Three datasets are from news but they have di erent characteristics that will be useful to provide a full comparison among the tested systems.
Some of those datasets include
 http://wikipedia-miner.cms.waikato.ac.nz/.
current version of Wikipedia.
This may happen whenever a page has been deleted or renamed.
In our experiments we discarded annotations to no-longer existing entities.
The de nition of a correct match between two annotations is challenging because it involves two possible dimensions: the  syntactic  one of the mentions, and the  semantic  one of the entities.
A match in each dimension can be more or less correct depending on the nature of the match itself.
As an example, let us consider again the text  President Barack Obama issues Iran ultimatum .
A system might detect the mention  President Barack Obama  whereas the ground-truth identi es as the true mention  Barack Obama .
Also, a system could predict for the corresponding entity the personal page of Barack Obama whereas the annotators could have preferred the page of the President of the United States   which also contains information about Barack Obama   or vice versa.
It seems intuitive that simply counting these as errors is a sub-optimal choice as it transfers the human-annotators bias into the evaluation.
Furthermore, factoring out such biases from the evaluation metrics could also alleviate the dangers of over tting on small data samples that only o er a glimpse of the complexity of the total entity space.
Such issues arise more frequently than one would expect given that, often, the human-labeled instances in the available datasets o er a wide spectrum of entity/mention possibilities, all of them pertinent to some extent.
To overcome these problems we introduce novel fuzzy measures to evaluate entity/mention matching which account for slight, but non-signi cant, misalignments in the detected mentions, as well as for di erent, but yet pertinent, entities.
In practice, we propose to generalize standard evaluation measures such as true/false positives, true/false negatives, precision and recall by de ning them on top of a binary relation M which speci es the notion of correct match between either two tags or two annotations.
Given an input text t, let g be the correct items (being them mentions or entities or full annotations) speci ed in the ground truth for t and let s be the solution found by the tested system.
We can introduce the following de nitions: , x)} (cid:2)   g : M(x tp(s, g, M) ={ x   s |  x (cid:2) (cid:2)   g : M(x , x)} (cid:7)  x f p(s, g, M) = {x   s | (cid:2) , x)} (cid:2)   g : M(x (cid:7)  x tn(s, g, M) = {x (cid:7)  s | (cid:2) (cid:2)   s : M(x , x)} f n(s, g, M) = {x   g | (cid:7)  x (cid:2) (1) Based on these we can generalize the de nitions of precision, recall and F1 [10] and their (micro and macro) dataset-wise versions.
The macro measures are the average of the corresponding measure over each document in the dataset D, while the micro measures take into account all annotations together thus giving more importance to documents with more annotations.
Here follow the micro and macro measures, where we uses t and gt to denote, respectively, the solution found by an annotator and the gold standard for a document t   D.
|tp(s,g,M)| |tp(s,g,M)| P (s, g, M) = |tp(s,g,M)|+|f p(s,g,M)| R(s, g, M) = |tp(s,g,M)|+|f n(s,g,M)| F1(s, g, M) = 2 P (s,g,M) R(s,g,M) P (s,g,M)+R(s,g,M) (2) Pmic(S, G, M) = (cid:2) (cid:2) t D |tp(st,gt,M)| t D(|tp(st,gt,M)|+|f p(st,gt,M)|) (cid:2) t D |tp(st,gt,M)| (cid:2) t D(|tp(st,gt,M)|+|f n(st,gt,M)|) Rmic(S, G, M) = F 1mic(S, G, M) = 2 Pmic(S,G,M) Rmic(S,G,M) Pmic(S,G,M)+Rmic(S,G,M) t D P (st,gt,M) Pmac(S, G, M) = (cid:2)
 (3) (cid:2) t D R(st,gt,M) Rmac(S, G, M) = F 1mac(S, G, M) = 2 Pmac(S,G,M) Rmac(S,G,M) Pmac(S,G,M)+Rmac(S,G,M)

 Here we consider appropriate matching relations M which deal with both mentions and annotations, thus addressing the case in which the entity-annotation problem accounts for both of them.
Me for the C2W problem.
It is quite straightforward to devise a proper M for C2W, given that it considers only the match between entities.
We start from the observation that pages of Wikipedia can be divided into two distinct subsets: R be the set of redirect pages, nR be the set of non-redirect pages (thus R   nR =  ).
For example  Obama  and  Barrack Hussein Obama  are redirects to  Barack Obama .
Redirects are meant to ease the  nding of pages by the Wikipedia users.
Redirects can be seen as many-to-one bindings from all synonyms (pages in R) to the most common form of the same entity (page in nR).
Since some annotators  output, as well as some datasets, may contain annotations to redirect pages, we have to consider the de-referenced concept.
Let d : R   nR (cid:11)  nR be a deref-erence function that implements the many-to-one binding into nR.
We say that there exists a strong entity match Me between two entities e1 and e2 i  d(e1) = d(e2).
Me is re exive, symmetric, and transitive.
Ma for the D2W problem.
The output for D2W consists of a set of annotations, some of them involving a null entity.
Thus the match relation must deal with mention-entity pairs.
Say we have to compute Ma(a1, a2) where a1 = (cid:2)(cid:2)p1, l1(cid:3), e1(cid:3) and a2 = (cid:2)(cid:2)p2, l2(cid:3), e2(cid:3).
We de ne the strong annotation match Ma as a binary relation that is veri ed i  p1 = p2 and l1 = l2 and d(e1) = d(e2), where d is the de-reference function speci ed before.
Note that, though in D2W the mentions to be annotated are given in the input, there can be false-negative annotations, namely those that are assigned to a null concept.
Mw for the A2W problem.
Like D2W, the output of the A2W problem is a set of annotations; unlike D2W, A2W requires the systems to discover the corresponding mentions.
So the strong annotation match proposed for D2W would be too strict, imposing a perfect text-alignment between the mentions speci ed in the ground-truth and the mentions outputted by the tested system.
Here it is crucial to be fuzzy, yet meaningful, in the counted matches so we introduce the notion of weak annotation match Mw(a1, a2) which is true i  the mentions (textually) overlap10 over the input text and d(e1) = d(e2).
Mw is re exive and symmetric, but it is not transitive nor anti-symmetric.
index of the two mentions.
Two mentions overlap i  p1   p2   e1   p1   e2   e1   p2   p1   e2   p2   e1   e2.
the output of Rc2W is a ranking of entities, common metrics like P1, R-prec, Recall, MRR and MAP [10] should be used only after Me is computed.
M for the Sc2W and Sa2W problems.
These problems ask annotation systems to produce a likelihood score for each annotation.
In practice, it does not make much sense to compare the likelihood scores (reals) between the ground truth and the system s annotation, and indeed the literature does not o er such datasets.
Nonetheless we have systems that solve Sa2W, so in order to evaluate them, we adapt their output to some easier problems in the DAG-hierarchy by introducing a threshold on the annotation scores.
After that, it is enough to set M = Me for Sc2W and M   {Mw, Ma} for Sa2W, and then compute the generalized metrics discussed at the beginning of this section and detailed in Section 6.
The measures presented above for Sa2W and A2W quantify the ability of a system to  nd the correct annotation, which includes  nding both the correct mention and the correct entity.
Here we consider the two measures Me and the new Mm to dissect annotators by testing in depth two main features: their ability to recognize the mentions or their ability to disambiguate them.
In particular, we will address two questions.
correct mention recognition?
To answer this question, we introduce a mention annotation match relation Mm that only checks the overlap of mentions, ignoring its associated entity.
This will be useful to evaluate the precision of the parsers used by the systems to detect the mentions.
ciation to an entity?
That s exactly what is evaluated by the metric Me introduced for problem C2W.
This evaluation takes into account only the entities, and not their mentions, thus it is crucial in all applications interested in retrieving the concepts (tag/entity) a text is about.
Finally, although the entity-matching relation considers the weak dereference function d, we will complete our experimental evaluations by considering a  ner measure which will account for the  closeness  of entities in Wikipedia based on the Milne-Witten relatedness function [16] (see Figure 5).
The similarity between systems can be measured document-wise, by comparing annotations over single documents, or dataset-wise, by comparing annotations over all documents in a dataset.
This comparison can help asses in which phase of the annotation process systems behave di erently, providing insights that could help the design of new annotation approaches.
In the former case, given two sets of annota-tions/tags At and Bt for an input text t and a matching relation M, we de ne the similarity between the systems that produced them as: Sim(At, Bt, M) = |{x   At |  y   Bt : M(x, y)}|+ |{x   Bt |  y   At : M(x, y)}| |At| + |Bt| (4) This measure is inspired by the Jaccard coe cient [10], but takes into account the fuzziness of the matching relation M. Unlike the Jaccard measure, triangle inequality does not hold.
Note that all matching relations introduced above are re exive and symmetric, thus Sim is symmetric too and Sim(x, x, M) = 1.
Sim ranges in [0, 1].
In order to measure the similarity of two annotators over a dataset (call it D), we apply iteratively Sim over all documents and then  average  the values.
The average can either give the same importance to all annotations/documents regardless of their size (Smacro) or can be computed as the overall  intersection  divided by the overall size, giving more importance to bigger sets (Smicro).
Formally: Smacro(D, M) = 1|D|   (cid:2) t D Sim(At, Bt, M) (|{x   At |  y   Bt : M(x, y)}|+ |{x   Bt |  y   At : M(x, y)}|) (cid:2) t D Smicro(D, M) = (cid:2) t D(|At|+|Bt|) (5) The actual interpretation of this measures depends only on the chosen relation M. For the tag-based problems (C2W, Rc2W, Sc2W), the only available matching function is Me, thus Sim measures the fraction of commonly annotated entities.
For all other problems (Sa2W, A2W, D2W) the use of Ma gives the fraction of common annotations (having same concept and same mention); using Mw gives the fraction of common overlapping annotations (having same concept and overlapping mention); using Mm gives the fraction of common overlapping mentions.
We ran a robustness test for the  ve entity-annotation systems listed in Table 2 over the  ve (publicly available) datasets described in Section 4.
The annotators were tested as they are, without any  ne-tuning or training over the datasets, using the most up-to-date APIs or software made available by the authors of the tested systems, as explained in Section 3.
The following sections present our  ndings in terms of e ectiveness (precision, recall and F1) and e ciency (speed) of the annotation process by means of the evaluation measures introduced above.
We observe that the datasets AIDA-CoNNL and AQUAINT annotate only a subset of the mentions (See Section 4), therefore entity annotators are penalized, as they found many reasonable annotations that are not contained in the (limited) gold standard, and this explains the not much large F1  gures achieved in the experiments.
(cid:4)(cid:5)(cid:2)(cid:3) (cid:1)(cid:2)(cid:3) (cid:6)(cid:2)(cid:3) (cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:4)(cid:5)(cid:7)(cid:3)(cid:6)(cid:8)(cid:6)(cid:4)(cid:5)(cid:9)(cid:6) (cid:10)(cid:5)(cid:11)(cid:6)(cid:3)(cid:1)(cid:10)(cid:7)(cid:3)(cid:11)(cid:5)(cid:12)(cid:3)(cid:7)(cid:1)(cid:4)(cid:1)(cid:6)(cid:5)(cid:4)(cid:6) (cid:4)(cid:11)(cid:13)(cid:4)(cid:4)(cid:5)(cid:14)(cid:3)(cid:7)(cid:1)(cid:4)(cid:1)(cid:6)(cid:5)(cid:4) Figure 3: DAG of problems involved in our experiments.
Figure 3 shows the hierarchy of problems restricted to those involved in our experiments: we notice that all tested systems solve Sa2W natively, the three news datasets and the Webpage dataset provide a gold standard for A2W, whereas the Twitter dataset provides a gold standard for C2W.
Thus, when evaluating over news and Webpages, we
 Type News Datasets Problem Match-





relations Ma Mw Mm Me Mt Ma Mw Mm Me Experiment 2 Experiment 3 Web pages Tweets Meij
 Table 4: Experimental setup.
will adapt the systems output to solve A2W, by discarding annotations using a threshold; whereas when evaluating for tweets, we will adapt systems to solve C2W, by applying a threshold on the annotations score and by discarding all mentions.
Table 4 summarizes the setup.
In this experiment the evaluation focuses on the A2W problem, the hardest for which datasets are available, according to the reductions of Figure 2.
To gain a deeper understanding of systems performance, we used all three news datasets, because of their di erences (see Table 3).
Since the reduction from Sa2W (the problem all annotators can solve) to A2W needs a threshold on the likelihood score, our experiments will take into account all possible values for that threshold, ranging from 0 to 1.
We run three types of experiments to test the e ectiveness of the systems in detecting correct (full) annotations, or just the mentions (discarding the entities), or just the entities (discarding the mentions).
We computed all evaluation measures presented in Section 5.
For lack of space, we show in Figure 4 the plot of micro-F1 using the weak-annotation match Mw over the AIDA/CONLL dataset (the largest with 231 news and 4485 annotations).
The interesting data is given by the maximum value reached by F 1micro, varying t   [0, 1].
Each annotator gives a di erent meaning to the annotation-scores, thus it does not make sense to compare the performance of two annotators for the same value of t. Hence that maximum can be seen as the best performance achievable by each system (according to micro-F1).
Overall TagMe reaches maximum F1, followed by Illinois Wiki er, Wikipedia miner, AIDA in its three variants and the poorest performance is achieved by DBpedia Spotlight.
Detailed results for all news datasets are reported in Table 5.11 AQUAINT and MSNBC have longer documents.
Surprisingly TagMe gives the best results, even if it was designed to annotate short texts [3, 4].
For AQUAINT, Wikipedia Miner is the second best followed by Illinois Wiki er, DBPe-dia Spotlight and then AIDA.
For MSNBC, the dataset with the longest documents, the second best annotator is AIDA with the CocktailParty and Local algorithms, followed by Wikipedia Miner, Illinois Wiki er and DBPedia Spotlight.
We also compute the threshold that maximizes F 1micro for each system and for each dataset using the strong annotation match Ma.
Table 6 reports results only for AIDA/CONLL for lack of space, the other two news datasets generate consistent results.
Measurements based on Ma give slightly worse results
 a web appendix with the  nal version of paper.
o r c
 i






 F1 aidaconlltestb - weakannotationmatch AIDA - PriorityOnly AIDA - Local AIDA - CocktailParty Wikipedia Miner Illinois Wikifier TagMe 2 DBPedia Spotlight




 Threshold Figure 4: Micro-F1 measure for Sa2W systems on the dataset AIDA/CONLL, varying t   [0, 1].
than those based on Mw, especially for annotators such as TagMe, Illinois Wiki er and Wikipedia Miner.
This shows that these systems detect mentions which do not coincide with those in the gold standard, even though they overlap with the human-labeled ones.
Overall, TagMe is the best annotator in terms of micro-F1, followed by Illinois Wiki er and Wikipedia Miner.
This suggests which algorithm employed by these annotators is the most performing: TagMe s voting scheme, based on the relatedness measure introduced in [16].
Searching for the best global coherence as done by Illinois Wiki er also seems a good approach.
Next comes Wikipedia Miner s algorithm, using machine-learning on Wikipedia s links.
Among the three versions of AIDA, the best is CocktailParty, followed by Local and then PriorityOnly.
AIDA seems to consistently trade Recall for Precision (see also Section 6.1.2).
Moreover, its performance is a ected by the poor mention recognition, as inspected further.
Of course it is hard to judge an algorithm by the performance of the system implementing it, since many variables contribute: implementation (bugs, optimizations), con gu-ration, the Wikipedia version it is based upon, etc.
Consequently, our results show which system (according to its con guration available to the public) performs best, and so they give only a clue of the pro/cons of its underlying algorithm.
Let s answer another question.
If we consider the annotated entities that di er from the ground truth, how close are they to the correct ones?
In order to make our analysis as complete as possible, we estimated, for each system, the distribution of the Milne-Witten s relatedness [16] between the false-positive entities found by the systems and the corresponding (namely, those whose mentions overlap) entities in the ground-truth.
The closer to 1 is the relatedness, the better is the correspondence between the entity annotated by the system and the one annotated by humans.
Notice that graph-based measures could be used directly to de ne weak entity annotation relation (Me).
We leave this for future work.
Figure 5 gives, for r   [0, 1], the cumulative percentage of the number of false-output vs ground-truth entities whose


 (avg-len 1039 chars)
 (avg-len 1415 chars)
 (avg-len 3316 chars) Annotator AIDA-Local
 CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner AIDA-Local
 CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner AIDA-Local
 CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Best t F 1mic Pmic Rmic



















































































 s n o i t t a o n n







 FP relatedness aidaconll AIDA - PriorityOnly AIDA - Local AIDA - CocktailParty Wikipedia Miner Illinois Wikifier TagMe 2 DBPedia Spotlight




 MW relatedness Figure 5: Distribution of Milne-Witten s relatedness between gold/false-output entities whose mentions overlap.
Table 5: Results based on the weak-annotation match among all annotators over news datasets.
Column F 1mic indicates the maximum micro-F1 .
Columns computed for the best score-threshold t indicate the micro-precision and Pmic and Rmic   .
Figures are given as a per-micro-recall for that t centage.
  Dataset

 (avg-len 1039 chars) Annotator AIDA-Local
 CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Best t F 1mic Pmic Rmic



























 Dataset

 (avg-len 1039 chars) Annotator AIDA-Local
 CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Best t F 1mic Pmic Rmic



























 Table 6: Results based on the strong-annotation match over the AIDA/CONLL dataset.
A comment is in order at this point.
relatedness is above r. The highest a line is, the most entities close to the correct ones are found.
For r = 0.5, TagMe  nds entities more closely related to the correct ones, while Illinois Wiki er is the worst, Wikipedia Miner and DBpe-dia Spotlight are about the same, as are AIDA-PriorityOnly and AIDA-CocktailParty.
For r = 0.8, AIDA-CocktailParty and Aida-Local achieve the best position showing their effectiveness in  nding entities closer to the correct matches.
In [7], AIDA has been evaluated considering its ability to solve the D2W problem, namely passing the mentions contained in the dataset to the system and restricting its evaluation to only those mentions.
This way, mentions found by the tested system but not contained in the dataset were discarded, and thus, not counted as errors.
This is clearly a more restricted annotation challenge than the one investigated in this paper.
Moreover, in that paper, AIDA s parameters were tuned over the CONLL dataset, thus achieving the interesting  gures of 0.816 macro-precision@1 and 0.825 micro-precision@1.
These results are clearly not comparable with those showed in Table 5.
We also tried to tune TagMe over this speci c dataset and achieved comparable perfor-Table 7: Results based on the weak-mention match over the AIDA/CONLL dataset.
mance.12 This shows that there is ample room for improvement for those systems whenever the annotation problem is restricted, dataset is  xed and tuning is possible.
Precise results on TagMe s performance in this speci c scenario will be posted on its website.
For the sake of space we report in Table 7 the results only for F 1micro and over the AIDA/CONLL dataset, by using the (weak) mention-annotation match Mm.
This basically shows the performance of the NER algorithms used by the various annotation systems.
TagMe o ers the best parsing thanks to its very high precision.
The second best is Wikipedia Miner, Illinois Wiki er is very close, then AIDA s variants which all use the Stanford NER tagger, and  nally DBpedia Spotlight.
AIDA s NER algorithm has a poor F1 because of a very low recall.
This ranking holds also for the other news datasets, i.e. AQUAINT and MSNBC.
The good result obtained by TagMe and Wikipedia Miner is probably due to the choice of the mention corpus: their use of Wikipedia anchors and titles from the one hand does not let those systems recognize all the mentions contained in a text, but on the other hand it selects better candidates, given the well-de ned mention corpus (drawn from Wikipedia), and this leads to a better overall performance.
since they do not provide an interface for training.
(avg-len 1039 chars) Annotator AIDA-Local
 CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Best t F 1mic Pmic Rmic



























 Table 8: Results based on entity-match for the AIDA/CONLL dataset.
The performance for this task can be computed via the relation Me.
Table 8 reports the results for the AIDA/CONLL dataset.
TagMe outperforms the other annotators in terms of F1 and recall, while the highest precision is achieved by AIDA-CocktailParty, at the cost of very low recall.
The CocktailParty variant seems to be the best choice for AIDA s variants, resulting slightly better than Local and signi -cantly better than PriorityOnly.
By comparing entities match against weak-mentions match, we notice that the smallest drop in performance is for TagMe (9% in micro-F1), the largest is for Wikipedia Miner (about 19%), Illinois Wiki er (about 12%) and then DBpedia Spotlight (about 15%).
AIDA s drop depends on the variant (from about 3% to
 TagMe s one, together with AIDA-CocktailParty s; however, the better NER parser makes TagMe the absolute best in the whole annotation process.
This ranking among the systems holds also for the other news datasets, namely AQUAINT and MSNBC.
Similarity between systems
 Table 9 reports the values of the similarity measure Simmacro using as match relation both the (weak) mention-annotation match Mm and the entity-annotation match Me.
Both similarities are computed on the output produced by the systems when the score-threshold t is set to its value maximizing the annotator performance on either the mention-detection or the entity-match (see Tables 7 and 8).
As expected, the highest similarities are between the three variants of AIDA.
A similarity around 65% is shown among Illinois Wiki er, Wikipedia Miner and TagMe, which are rather dissimilar to the AIDA system.
We have seen the similarity on the whole output of two systems, but how good are the dissimilar annotations?
To answer this question, we have inspected these annotations by counting the amount of true and false positives.
Most correct dissimilarities fall in the range of 30-40%, meaning that the annotations found by one system and not by the other are mostly wrong.
But there are some exceptions, the most notable being that the dissimilarity of both AIDA-Local and AIDA-CocktailParty against all other systems has a true-positive rate of 53-62%.
In particular, the dissimilarity against Wikipedia Miner is 61% correct, against TagMe it is 62% correct and against Illinois Wiki er it is 53% correct.
This demonstrates that AIDA-Local and AIDA-CocktailParty  nd a set of annotations that are not found by the other annotators but still are mostly correct.
This suggests that an improved annotator could be designed with a NER algorithm and a disambiguation algorithm combining those of, e.g., TagMe and AIDA-CocktailParty.
DBPed.
Spotl.
Prior-Cocktail-Only Party


 Illinois Wik.
TagMe
 Wikip.
Miner















 AIDA-Local
 CocktailParty
 PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Table 9: Simmacro similarity among systems over the AIDA/CONLL dataset, computed both for recognition of mentions/entities.
Dataset Meij (avg-len 80 chars) Annotator AIDA-Local AIDA-CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Best t. F 1mic Pmic Rmic



























 Table 10: Results based on strong tag match for the Meij dataset.
This experiment aims at testing systems over very-short and poorly composed texts, as tweets.
This makes it hard for NER algorithms to recognize the mentions and  nd pertinent entities.
As expected results on tweets are lower than those on news stories.
A similar study on Twitter s posts was conducted by E. Meij and was published on his private blog13, but focused on di erent measures and was conducted on a few/di erent set of annotators.
Since Meij s dataset is for C2W, the output of known systems must be adapted by taking only the annotations whose score is higher than a threshold.
Since only TagMe is speci cally designed to address the annotation of short texts (as CMNS), this experiment is aimed also at testing the  exibility of all other systems.
Wikipedia Miner proves the best in F1 (because of its very good recall), followed by TagMe.
Illinois Wiki er achieves worse results, especially in terms of recall, while DBpe-dia Spotlight loses signi cantly in precision.
All variants of AIDA have a poor performance, although with highest precision; this is likely due to the poor performance of the Stanford parser on this type of text.
This experiment aims at testing systems over long texts drawn from Web pages.
This dataset is challenging for TagMe which was designed for very-short texts only.
The key di culty is that attempting to optimize global coherence of the annotations, as many systems do, may be dangerous because of concept drifts in the long input text.
Moreover, the text provided by the IITB dataset includes, without providing any structure, accessory textual elements taken from
 http://edgar.meij.pro/comparison-semantic-labeling-algorithms-twitter-data/

 (avg-len 3879 chars) Annotator AIDA-Local AIDA-CocktailParty AIDA-PriorityOnly DBPedia Spotlight Illinois Wiki er TagMe 2 Wikipedia Miner Best t F 1mic Pmic Rmic



























 Table 11: Results based on the weak-annotation match over the IITB dataset.
System Meij AIDA-Local
 AIDA-CocktailParty 865
 AIDA-PriorityOnly
 DBPedia Spotlight
 Illinois Wiki er
 TagMe 2
 Wikipedia Miner

































 Table 12: Avg.
annotation time per document (ms).
the web page (like menus, footers, etc.)
that are generally not semantically linked to the main body.
Table 11 shows that Wikipedia Miner is the best annotator, closely followed by DBpedia Spotlight, the third being Illinois Wiki er and then TagMe.
However, unlike the news datasets with long texts (i.e. AQUAINT, MBSC), AIDA is worse because of the low recall of the Stanford NER Tagger.
Time e ciency is a key feature of entity-annotation systems because it impacts onto their scalability.
Of course, network latency and the HW/SW features of the remote/-local servers may a ect time measures.
Nevertheless, we adopted some care in order to make these evaluations unbiased (see Table 12 for a summary).
For Illinois Wiki er, which run locally, the runtime has been recorded as the difference between the system time after and before the library call.
For AIDA, the time has been recorded as the one returned by the method getOverallRuntime() of the RMI Service, and thus it does not include the network latency.
Similarly, as TagMe s runtime, it has been considered that included in the response returned by the RESTful API.
For Wikipedia Miner and DBpedia Spotlight, accessed through a Web service, the time has been recorded as the di erence between the system time after and before the query, and the timing has been calibrated subtracting the time needed to process an empty query, which should account for the network latency.
TagMe is de nitely the fastest annotator, being about
 one.
DBpedia Spotlight is also fast, except for a peak of
 about three times slower.
AIDA-PriorityOnly is the fastest one among all AIDA variants, due to the basic disambiguation process, AIDA-Local and CocktailParty are slower, being about 300 times slower than TagMe.
For all systems, the runtime looks linear in the size of the text   which is good.
A special case is the dataset of Meij, for which texts are very short; TagMe is extremely fast, due to the engineering tuned on short texts, then comes Wikipedia Miner,











 Runtime - Best F1
 a g
 e

 i k i p e d i a
 i n


 e r
 -
Illi n o i s
 i k ifi e r ri o rit y
 n l y







 --

o o c c k t a l a il
 a r t y


 e d i a
 p o tli g h t



 Runtime (msec) Figure 6: Average runtime (in log-scale) for dataset AIDA/CONLL and best achieved F1 measures (metrics based on weak annotation match).
AIDA-PriorityOnly and Illinois Wiki er, the others being way slower.
We conclude by reporting in Figure 6 a 2d plot of the runtime and the best F1 of each tested annotator over the AIDA/CONLL dataset.
For all other datasets the plotting is similar.
The net result is that TagMe dominates all other systems in terms of runtime and F1, AIDA-priority is faster but at the prize of lower e ectiveness.
Despite the di cul-ties indicated before concerning the evaluation of systems  e ciency, our  gures di er so much that they provide a reasonable picture of systems  speed.
We designed, implemented and tested a benchmarking framework to fairly and fully compare entity-annotation systems.
Our goal was to help improving the understanding of this challenging new problem.
Our framework is easily extensible with new annotation systems, new datasets, new matching relations and new evaluation measures.
It is written in Java and it has been released to the public as open source code.
Using the framework we provided the  rst complete comparison among all publicly available entity annotators over all available datasets for the entity-annotation task, showing that some of these annotators are very e ective and ready to scale over large datasets and real applications.
We believe that our framework will spur new research on this interesting topic, providing a common ground upon which researchers will test the e ciency and e ectiveness of their proposals.
We thank all our colleagues who provided prompt support for their software, datasets and web services: Johannes Ho art, Edwin Lewis-Kelham, Edgar Meij, Lev Ratinov, Mark Sammons, Ugo Scaiella, Daniele Vitale, Gerhard We-ikum, Ian Witten.
We also thank the Wikimedia Foundation and the Wikipedia contributors, whose knowledge-sharing philosophy made this work possible.
This research was supported by the PRIN ARS-technomedia grant and by a Google research award 2013.
