Micro-blogging services such as Twitter, Tumblr and Weibo have become important tools for online users to share breaking news, interesting stories, and rich media content.
Moreover, they are widely used for disseminating information in emergencies, such as the Tsunami in Japan or Hurricane Sandy in New York.
Moreover, Twitter was used extensively in elections and played an instrumental role in facilitating the Arab Spring.
In addition to its use as a content sharing platform, micro-blogging services like Twitter, along with other location sharing services such as Foursquare, Gowalla, Facebook Places or Google+ are nowadays supporting location services.
That is, users are able to specify their location in messages, either explicitly, by letting users choose their place, or implicitly, by enabling geo-tagging functionality.
Such pervasive geotagging is facilitated by the availability of cheap GPS sensors and improved location based services, e.g.
via the use of WiFi  ngerprinting in mobile devices.
This wealth of data presents an exciting opportunity for statistical modeling.
For instance we may analyze the relationship between content creation and sharing and conversely, this allows us to infer geographic location purely based on user-generated content.
These issues have attracted signi cant attention [23, 28, 17, 13, 12], however, incorporating geographical information, language models and user preferences simultaneously is nontrivial.
Many models only cover some aspects of the problem described above while ignoring the remainder.
For instance, [30] partition the earth into equally sized grids and learn language models per area.
This approach has two important shortcomings: the size or the number of grids is pre-de ned and cannot be adapted e ciently according to the underlying distribution.
Secondly, the model loses the ablity to uncover global topics shared across areas.
On the other hand, [24] use uneven grids.
However their work ignores shared interests among users.
Moreover, the hidden structure inherent in the distribution over regions is ignored.
Another line of research [17, 16, 21] takes regional language variations and global topics into account by bridging  nite mixture Gaussian models and topic models.
Unlike pre-de ned grids or regions, these models usually employ a  at clustering model of locations.
This  at structure is unnatural in terms of the language model: while it is reasonable to assume that New Yorkers and San Franciscans might di er in terms of the content of the tweets, it is also
 more similar to each other, than to tweets from Egypt, China or Germany.
As a side e ect, location prediction is not always satisfactory.
For instance the languge model of some big regions can degenerate to stopwords [17] while smaller regions might correspond to airports [21].
In this paper we describe a joint hierarchical model of location and content, both personalized to the individual preferences of a user.
That is, we build a generative hierarchical model for topics occurring in tweets.
That is, we assume that topics occurring at lower levels of the tree are more speci c versions of the topics found closer to the root level.
We perform a similar approach when dealing with global topics.
There their distribution is assumed to follow a corresponding hierarchical structure.
Moreover, we assume that these topics are location speci c, that is we adorn this tree with locations at which such topics could be observed.
Finally, we represent users by modeling distributions over leaves within the overall tree.
That is, we encode location speci c generative models for each user.
Bayesian nonparametrics is rich in structured and hierarchical models.
Nonetheless, we found that no previously proposed structure was a good  t for our needs, hence the need the model proposed in this paper   the nested Chinese Restaurant Franchise (nCRF).
It addresses the following key problem: we want to model each user s tweets in a hierarchical treelike structure akin to the one described by nested Chinese Restaurant Process or the hierarchical Pachinko Allocation Model.
At the same time we want to ensure that we have sharing of statistical strength between di erent users  activities by sharing the hierarchical structure.
Our model allows us to discover a hidden tree structure with unbounded width and depth while allowing users to have di erent distributions over this structure.
The number of regions and their hierarchical structure are learned from data, and language models as well as topic distributions are cascaded over the tree structure, resulting in more natural clustering of tweets and locations.
We develop an e cient algorithm to perform posterior inference and apply the framework to organize tweets into a hierarchical structure and show that this tree structure can be used to better predict locations of unlabeled tweets, resulting in signi cant improvements to state-of-the-art approaches.
In addition, we show some interesting hidden patterns that are revealed through this hierarchical modeling.
These experiments provide theoretical validation of the model.
This model is experimentally validated by demonstrating signi cantly better accuracy (39% and 45% respectively on two benchmark datasets relative to the best prior work [21]) in inferring the location of a tweet without having access to its geographical location.
Secondly, we are able to annotate locations with topics without su ering from any of the degeneracies inherent in [17, 21].
These topics form a human-understandable hierarchy, generated automatically from data without the need for manual intervention.
This can be used, e.g.
for improved content targeting and representation.
It also provides improved information regarding the locations and topical preferences of individual users.
Given the considerable amount of prior work both on Bayesian Nonparametrics and on the analysis of user generated content it is worth while to discuss these issues in further detail before introducing the nested Chinese Restaurant Franchise and its application to Tweets.
We begin with a summary of key aspects when modeling microblogs.
There is a substantial body of research on geographical language modeling.
We review some key threads: [23] propose a model based on Probabilistic Latent Semantic Indexing (PLSA) [20].
It assumes that each word is either drawn from a universal background topic or from a location and time dependent language model.
However, the mixture coe cients between the background topic and other spatio-temporal topics ones is tuned manually.
Since the model uses PLSA, no prior distribution is (or could be) assumed.
Evaluation is performed via anecdotal results.
[28] introduce a fully Bayesian generative model.
Rather than working with actual locations, they  xed a number of region labels and assume that each term is associated with a location label.
For each word in a document, a topic assignment is  rst generated according to a multinomial distribution.
Then the term and the location are generated dependent on this topic assignment.
Again the evaluation is limited to anecdotal results.
[25] propose a similar model.
For evaluation they measure Deviation Information Criteria (a model complexity criterion similar to BIC), as well as classi cation accuracy using manually labeled data.
One of the drawbacks of the work is that they only use data from Flickr restricted to the greater London area.
[19] extend [28] by introducing the notion of global and local topics.
Inference uses Gibbs Sampling.
[17] propose a model utilizing the correlations between global and local topics.
In their model, each author is assigned a latent region variable and an observed GPS location.
Terms and the actual GPS location are both conditioned on the latent region variable.
The topics to generate terms are local topics, which are derived from global topics.
For inference [17] use Variational EM.
For evaluation purposes the accuracy of predicted location is used.
[31] propose a model similar in spirit to [17].
The terms and the location of a particular document are generated by a latent region.
The location is generated from a region by a normal distribution and the region is sampled from a multi-nomial distribution.
However, inference is performed using MAP-style EM rather than a fully Bayesian approach.
[30] use an even simpler approach where documents are assigned to geodesic grids and thus a supervised learning method is utilized, essentially via a na ve Bayes classi er on the grid.
Finally [13] studied human mobility in location sharing services.
They found that users tend to appear in a very limited number of places (e.g., o ce and home).
They demonstrated that it might be e ective enough to use a two component Gaussian mixture model to estimate users  locations.
We now proceed to reviewing some key components of the statistical toolkit we employ for content modeling.
One of the prototypical ingredients for nonparametric modeling is the Dirichlet Process DP (H,  ) [18, 8, 10].
It allows for a discrete distribution of observations drawn from an arbitrary base measure H. We write G0   DP (H,  ) to denote a draw from a DP where   controls the variance of the draws around
 number of components.
We then draw parameters  i   G0.
Placing this prior on top of a mixture model we get the Drichlet process mixture model (DPM).
In DPM we extend the aforementioned generative process to  nally draw the observed data points xi from  i: xi   f ( i).
A useful view of the Dirichlet Process is the Chinese Restaurant metaphor.
In it each data point is considered as a customer in a restaurant with an in nite number of tables.
Initially all tables are empty.
Customers pick existing tables in proportion to their popularity.
In it the probability for customer i to pick table j is (cid:40) Nj(cid:80)  (cid:80) k Nk+  Pr{zi = j} = for an existing table (1) for a new table.
k Nk+  current number of customers sitting at table j and (cid:80) Here zi encodes the choice of customer i. Nj denotes the k Nk is the total number of customers so far.
This makes the Chinese Restaurant Process a single parameter distribution over partitions of the integers.
The  dish  xj   H chosen at table j is drawn iid from H, the base measure.
i
 A key ingredient for building hierarchical models is the Hierarchical Dirichlet Process (HDP).
It is obtained by coupling draws from a Dirichlet process by having the reference measure itself arise from a Dirichlet process [26, ].
In other words, rather than (2) G   DP (H,  ) we now have Gi   DP (G0,   (cid:48) ) and G0   DP (H,  ) (3) Here   and  (cid:48) are appropriate concentration parameters.
This means that we  rst draw atoms from H to obtain G0.
This is then, in turn, used as reference measure to obtain the measures Gi.
They are discrete and share atoms via G0.
The Hierarchical Dirichlet Process is widely used in applications where di erent groups of data points would share the same settings of partitions, such as [26, 9].
In the context of document modeling the HDP is used to model each document as a DP while sharing the set of atoms (mixtures or topics) across all documents.
This is precisely what we also want when assessing distributions over trees   we want to ensure that the (partial) trees attached to each user share attributes among all users.
Integrating out all random measures, we arrive at the Chinese Restaurant Franchise (CRF).
In it each restaurant maintains its set of tables but shares the same set of mixtures.
A customer at restaurant k can sit at an existing table with a probability proportional to the number of customers sitting on this table, or start a new table with probability   and chose its dish from a global distribution.
In this global distribution, a dish (mixture) is chosen proportional to its use across restaurants, however, a new global dish can be chosen with probability proportional to  .
CRPs and CRFs allow objects, such as documents, to be generated from a single mixture (topic).
However, they do not provide a relationship between topics.
One option to address this issue is to introduce a tree-wise dependency.
This was proposed in the nested Chinese Restaurant Process (nCRP) by [11].
It de nes an in nite hierarchy, both in terms of width and depth.
In the nCRP, a set of topics (mixtures) are arranged over a treelike structure whose semantic is such that parent topics are more general than the topics represented by their children.
A document in this process is de ned as a path over the tree, and it is generated from the topics along that path using an LDA-like model.
In particular, each node in the tree de nes a Chinese Restaurant Process over its children.
Thus a path is de ned by the set of decisions taken at each node.
While this provides more expressive modeling tool, it is still only allows each document to have a single path over the tree   a limitation that our model in Section 3 will remedy.
We are now in a position to introduce the Nested Chinese Restaurant Franchise (nCRF).
As its name suggests, it borrows both from the Chinese Restaurant Franchise, thus allowing us to share strength between groups, and the Nested Chinese Restaurant Process, thus allowing us to obtain a hierarchical distribution over observations.
Although the Nested Chinese Restaurant Process, introduced in [11], provides a convenient way to impose a distribution over treelike structures, it is di cult to apply it directly in our settings due to the reason that the nCRP-induced distribution over the hierarchy is a global distribution shared across all data partitions, such as documents.
Instead, in our case, we wish to have a personalized distributions over the same hierarchy for each user.
Subsequently, we adorn each vertex in the tree with a generative model to represent language and topic cascades.
In the context of spatial modeling of user generated content, for instance, each node in the tree represents a geographical region.
In the context of document modeling, we will associate each document with an nCRP and tie together documents using the franchise.
Details of the nCRF, as applied to these problems are given in later sections.
For now we focus on the generative process itself.
Our goal is to design a non-parametric model over trees, where each user has its own tree, but the set of nodes in the trees, and their structure, such as parent-child relationships, are shared across all users.
This process is illustrated in Figure 1.
In a nutshell, we achieve this by associating an nCRP process with each user.
In Figure 1 each node in all processes (global and user processes) de nes a distribution over its children.
This distribution is represented by the histograms attached to the vertices A, A1, A2 and B, B1, B2 respectively.
A user  rst selects a node.
Subsequently the generative model for the data associated with this particular vertex is invoked.
For instance, user 1  rst selects a sibling of node A1 based on the local distribution or with probability proportional to   he creates a new child.
In the latter case the child is sampled according to the global distribution associated with node A.
Then user A continues the process until a path is fully created.
For instance, if the selected node is B1 then the process continues similarly.
Thus Nodes A,A1 and A2 constitute a CRF process.
In general, isomorphic nodes in the global and user processes are linked via a CRF process.
Since the user selects a path by descending the tree, we call this process the nested CRF process.
An equivalent representation is the nHDP process, where the base measure at nodes A1
 base measure at node A is in turn sampled from a global base measure over the integers.
Note that the global tree is just a superset of all user trees, and each user only places some probability mass in selected regions he visits.
Once the trees are generated, we de ne a cascading process of node attributes over the tree.
Consider the case where we want to generate a path for a tweet written by user u.
We  rst start at the root node in the process of user u.
This root node de nes a CRP process over its children.
Thus we can select an existing child (in this user s tree) or create a new child.
In the later case, the global CRP associated with the root node is consulted.
A child in the global tree is selected with probability proportional to its global usage across all users.
Alternatively a new child node is created (and thus made accessible to all other users).
All selection probabilities are governed using the standard CRF s rich-gets-rich mechanism.
Once a child node is selected, the process recurses with that node until a full path is de ned.
and ni :=(cid:80) We need some notation regarding the structure of the trees over di erent topics.
We denote by i a vertex in the tree.
level(i) denotes the level of node i, C(i) denotes the children and  (i) its parent.
Moreover, nij is the number of times child j is selected at node i when generating data j nij is the number of (non-unique) children of i.
Moreover, we use the superscript u to index the above quantities for each user.
That is, nu ij, C u(i) are matching user-speci c quantities.
Clearly  u(i) =  (i) since the user trees are mapped to the common tree.
Also note that usually C u(i)   C(i), i.e. we only select a subset of nodes in the user tree.
i , nu This allows us to specify the collapsed generative probabilities at vertex i.
As per the Chinese Restaurant metaphor the probability of selecting an existing child node is Pr{child j at i} = if j   C u(i) is from the user tree otherwise (4)   nu nu i +    nu i +  ij (cid:40) nij ni+    ni+    Whenever we choose a child not arising from the user tree we fall back to the distribution over the common tree.
That is, we sample as follows: Pr{child j at i} = if j   C(i) if this is a new child (5) Combining (4) and (5) we have the full probability as Pr{child j at i} = nu nu i +  +   ij nu i +    nu i +    nu i +  nij ni+  ni+    nij ni+  if j   C u(i) if j   C(i)\C u(i) if j (cid:54)  C(i) Note here that we used a direct-assignment representation for the CRF at each node to avoid overloading notations with maintaing di erent tables for the same child at each node.
This is correct due to the coagulation / fragmentation equivalence in Dirichlet Processes [22].
In other words, in all CRPs, each child node is represented by a single table, hence Figure 1: An illustration of the nested Chinese Restaurant Franchise involving a common tree over components (left) and two subtrees representing processes for two separate subgroups (e.g.
users on Twitter).
Each user samples from his own distribution over topics, smoothed by the global process.
Thus each user process represents a nested Chinese Restaurant Process.
All of them are combined into a common franchise, hence the name nCRF.
table and child become synonymous and we omit the notion of tables.
During inference, an axillary variable method is used to link the local nu ij and global counts nij using the Anotoniak distribution as described by [26].
Once a child node is selected, the process is repeated until a full path is de ned.
To ensure that the path terminates we need to add the notion of an  exit child .
We use sequence notation to index a vertex in the tree (e.g.
2:5:3:1 means choosing a path which uses the second,  fth, third and then  rst vertex in the path respectively) we de ne child 1 to be the terminator.
In other words, whenever we select child 1 anywhere on the path, sampling terminates.
At this point the observation is generated using the global parameters associated with node i.
Two important notes are in order here: We keep a global index of the children under each node across all users and track which of them are materialized under each user s tree using the list C u(.).
Hence giving a special semantic to child
 are only unique under each parent node, that is, we need to specify the entire path when implementing the inference routines.
However, when referring to a generic node in the tree we simply refer to node i, ignoring the fact that i is in fact, a slightly more complex data structure tracing the path from root to a particular vertex.
Note that by construction this sequence must end in a 1, i.e. the exit child.
Also note that we could use a di erent smoothing probability for the  rst vertex as was done in [1].
Microblogs are a rather unique form of expression.
They are concise, often very situation and context speci c, and they exhibit characteristics unique to to their authors.
We want to capture this in a generative model.
More to the point, we are given collections of tweets with timestamps, location information, and information regarding the author of the tweets.
We want to use this to form a joint generative model of both location and content.
We use tweets and documents interchangeably to mean the same object.
A natural assumption is that locations come with their own location speci c topics, such as airport names, local sports teams, politicians, companies, festivals, language idiosyncrasies, foreign languages, etc.
; That said, it is reasonable to assume that some topics are globally popular.
Nonetheless, their relative degree of popularity is likely to be location speci c.
One would assume that these preferences correlate hierarchically.
That is, quite likely tweets in California are more similar to those in Oregon than, say, in
 BGlobal process

 User
 process

 User
 Process
 content and location preferences in a tree.
The above reasoning leads to a tree for arranging locations, local topics, and general topic preferences in a structured fashion.
In statistical terms this means that we will model locations in a hierarchical tree-wise fashion.
That is, we will assume that locations drawn from the leaves of a vertex are more similar between each other than on another vertex of the tree.
Likewise, we assume hierarchical dependence of the language model, both in terms of content of the region speci c language models and also in terms of prevalence of global topics.
While these steps are useful in their own right, they do not yet resolve the issue of user speci c preferences.
That is, users commonly only frequent a small number of places.
Moreover, they only tend to write about a relatively small subset of topics.
Hence, a global tree-based hierarchical model is unlikely to be a good  t for the tweets generated by individuals.
A better approach is to assume that users only select a subtree of the global topic and location distribution and generate news based on this.
By intertwining location and topical distributions into a joint model we are able to dynamically trade o  between improved spatial accuracy and a better content description.
This leads to an improved model of both content and location.
In fact, we  nd that our model is signi cantly beter than any previous location estimator using Tweets.
We now map the above intuitive description of the process to a concrete model of data generation.
Tree distribution: This was discussed in the context of the nested Chinese Restaurant Franchise above.
Recall that at each vertex i we assume that the distribution over children follows a CRP process.
The  rst component corresponds to the probability mass of observations remaining at the present vertex whereas all other components correspond to proper children of i.
Note that this means that di erent vertices may have di erent numbers of children.
It also means that with probability 1 any path in the tree terminates after a  nite number of steps.
This greatly simpli es inference since we never need to instantiate an in nite hierarchy.
Hierarchical location model: In analogy to [1] we consider a hierarchical multivariate Gaussian model.
The main distinction is that we need not instantiate a shrinkage step towards the origin at each iteration (this would be ill de ned on the surface of the earth).
Instead, we simply assume an additive Gaussian model.
We are able to achieve this simply since we assume decreasing variance when traversing the hierarchy, whereas [1] did not impose such a constraint.
This yields the following model:  r   N(cid:0) (r),  (r) (cid:1) and  r =
 level(r)
 (6) Here  0 is the covariance matrix of the root node, and  r,  r are the mean vector and covariance matrix of region r. and In other words, we obtain a tree structured Gaussian Markov Random Field.
This is desirable since inference in it is fully tractable in linear time by means of message passing.
Generic topics: In our model we assume that we have a  nite number of topics T (that this could easily be alleviated but we found that this was unnecessary in our experiments and it made it more di cult to predict well bounded memory footprint).
We denote each global topic by  i.
It is drawn from a Dirichlet distribution over V words:  i   Dir( ).
(7) Note that one could add hierarchical language models over topics (or longer range n-grams as discussed e.g.
by [29]).
This is likely to improve estimation quality for longer and structured texts.
Location speci c language model: Using the intuition that geographical proximity is a good prior for similarity in a location speci c language model we use a hierarchical Dirichlet Process to capture such correlations.
In other words, we draw the root-level language model from  0   Dir( ).
(8) At lower levels the language model is drawn using the parent language model as a prior.
That is, we use  r   Dir(cid:0) (r) (cid:1) (9) In doing so, we will obtain more speci c topics at lower levels whereas at higher levels less characteristic tokens are more prevalent.
Location speci c mix of topics: A similar construction can be used for hierarchically modeling distributions over topics hierarchically.
This acts as a mechanism for mixing larger sets of words e ciently rather than just reweighting individual words.
 r is constructed in complete analogy to the location speci c language model.
That is, we assume the hierarchical model  0   Dir( )  r   Dir(cid:0) (r) (cid:1) (10) (11) User speci c tree distribution: Again, this is as described in Section 3.
We use the distribution over the common tree as a prior and draw a user speci c tree distribution over regions and associated topics.
In summary, the generative process is as follows: for each tweet d by user u, we  rstly use nCRF to choose a node (latent region) r. Once this node is chosen, both the content of the document (words) and the geographical location, i.e. latitude and longitude, are generated from corresponding distributions of this node.
For the geographical location, the generative process is straightforward as it is drawn from the regional dependent multivariate normal distribution.
For each word w in the document d, we  rstly choose a topic assignment z from the regional dependent topic distribution.
Then, a word w is generated from the corresponding language model.
We have two sets of language models: global topics encoded in a global matrix   and regional language models  i, one per latent region.
Following the de nition of the model one would need to introduce a Bernoulli random variable to determine which distribution might be the source for the
 bookkeeping and sampling.
Instead, we use a compact representation exploiting the fact that a the mixture between multinomials is multinomial.
The idea is to augment the global topic matrix with an additional topic (i.e. topic T +1) and reserve this topic for the the regional language model based on id.
Every time this special index is sampled from the topic distribution, we just refer to the Tweet s regional language model.
More speci cally, we use   to denote a (T + 1)-row matrix, which combines the regional language model and the original global topic matrix.
Using this trick we can eliminate the Bernoulli switch variable.
This yields the following generative process:
 (a) Sample a node rd   nCRF( ,  , u).
(b) If node rd is a globally new node then i.  rd   N(cid:0) (rd),  (rd) ii.
 rd   Dir(cid:0) (rd) (cid:1) iii.
 rd   Dir(cid:0) (rd) (cid:1) (cid:1) (c) Sample a location ld   N( rd ,  rd ).
(d) For each word w(d,i): i.
Sample a topic index z(d,i)   Multi( rd ).
ii.
Sample word w(d,i)   Multi( z(d,i) ).
We repeat this generative process for all users in the corpus.
In this section, we describe a collapsed direct-assignment Gibbs sampling algorithm to infer the latent structures imposed by the generative process introduced above.
The observed variables needed by the Gibbs inference algorithm are: w(d,i), the i-th word in document d and ld, the geographical location of document d. In addition to these input data, we also observe the author, as an index, of each documents.
We collapse all multinomial variables and alternate sampling a region assignment rd for each document/tweet d from each user, a topic assignment z(d,i) for each word in the document.
Furthermore, we need to sample the parameters (a mean vector and a covariance matrix) for multivariate normal distributions to be able to calculate the probabilities for geographical locations, and the mean-geographical parameters for latent regions.
We employ an auxiliary variable method similar to [26] to deal with the cascading behavior of topic mixing vectors   and  .
Each of the following Sections detail each step.
Instead of sampling a path rd, as a block as in the nCRP [11], we use a level-wise strategy to sample a latent region for each tweet1.
Where rd is a speci cation of the path from the root to the selected latent region.
Lets say starting from the root node, a document d with author u reached node i on the tree, then it can descend the tree further as follows: 1. stay on the current node i   i.e. pick child 0, and set rd,i = 0.
method that samples a path as a block, while being much faster.
A detailed comparison is left to the full version of this paper.
Algorithm 1: The sketch of one iteration of the inference algorithm.
D is the total number of documents Sampling region and topic assignments: for d = 1 to D do Sample region assignments rd, Eqn.
(12) Sample topic assignments zd, Eqn.
(15) Tree structure Kalman  lter: From the bottom to top: Perform Eqn.
(16, 17, 18) From the top to the bottom: Perform Eqn.
(19,20) Sampling topic proportions: From the bottom to the top: Sample  nr from C(r) r From the top to the bottom: Sample  r, Eqn.
(13) Sampling regional language models: From the bottom to the top: Compute  mr from C(r) r From the top to the bottom: Sample  r, Eqn.
(14)
 rd,i = j 3. create a new child node of node i and move to it, and set rd,i accordingly.
The probability of each choice shares a similar form: P (rd,i = node| r d, rest)P (wd, ld, zd | r, rest) (12) where the second term in the right hand side is essentially the probability of the data given a choice of the node, which can be decomposed into three componenets: the probability of the document (terms) P (wd | rd, rest) , the probability of the location P (ld | rd, rest), and the probability of the topic indicators P (zd | rd, rest).
For P (ld | r, rest), it is indeed P (ld |  rd ,  rd ), evaluating the multivariate normal distribution associated to the corresponding node at ld (This is because we explicitly represent these normal parameters, which is discussed in Section (5.5)).
The component corresponding to P (wd | r, rest) and P (zd | r, rest) are just standard Dirichlet-multinomial integrals which reduces to the ration of two log-partition functions.
Finally, the component P (rd = node| r d, rest), is computed according to the nCRF process de ned in Section (3.2).
Once a decision is made over node i and a child (other than 0) is selected, we descend with that child and repeat the process.
The key idea here is that each node among the children of i acts as proxy for the subtree rooted under it due to the cascading nature of how the parameters are de ned, and as such allows us to make an informed decision at each node separately without sampling the path as a block.
Since topic proportions for di erent regions are linked through the cascading process de ned in Equation (11), we use an auxiliary variable method similar to [26] that we detail below.
We sample  r based on three parts: 1) actual counts nr associated with node r, 2) pseudo counts  nr, propagated from all children nodes of r and 3) topic proportion  (r) from the parent node of r. Thus, topic proportions for 30 (r) r  (r) .
.
.
 npC(r) .
.
.
.
.
.
pC(r)  np1 p1 Figure 2: This is a demonstration of sampling  r, the distribution over topics for node r. The sampling is drawn from a Dirichlet distribution with parameters consisting of count statistics nr from node r, pseudo counts  nr gathering from its children nodes and topic proportions  (r) from its parent node.
node r are in uenced by its children nodes and its parent node, enforcing topic proportion cascading on the tree.
To sample  nr, we start from all children node of r. Let  sp,k be the number of counts that node p   C(r) will propagate to its parent node r and np,k is the actual number of times topic k appears at node p. We sample  sp,k by the following procedure.
We  rstly set it to 0, then for j = 1,  , np,k + , and increment  sp,k if  np,k,  ip a coin with bias the coin turns head.
The  nal value of  sp,k is a sample from the Antoniak distribution.
Thus, for node r,  nr,k = p C(r)  sp,k.
This sampling procedure is done from the bottom to the top.
Note that  sp,k has the meaning as the number of times the parent node was visited when sampling topic k at node p.
j 1+ r,k (cid:80)  r,k After smoothing over the tree from bottom to the top, we will have pseudo counts on each node.
Thus, new topic proportions for each node can be e ectively sampled by:  r   Dir(cid:0)nr +  nr +  (r) (13) (cid:1) where nr is the actual count vector for node r and  nr is the pseudo count vector.
We do this process from the top to the bottom of the tree.
As we discussed before, regional language models are cascaded through the tree structure.
Thus, we need to sample them explicitly in the inference algorithm.
The sampling process is also a top-down procedure where we start from the root node.
For the root node, we always sample it from a uniform Dirichlet distribution  root   Dir(0.1/V,  , 0.1/V ).
For all other nodes, we sample  r from:  r   Dir(cid:0)mr +  mr +  (r) (14) (cid:1) where mr is the count vector for node r,  mr is a smoothed count vector for node r and   is a parameter.
Here, m(r,v) is the number of times term v appearing in node r. For  mr, it is a smoothed vector of counts from sub-trees of node r.
It can be sampled through a draw from the corresponding Antoniak distribution, similar to Section (5.2).
However, since the element in  r is much larger than topic proportions, it is not e cient.
Here, we adopt two approximations [15,

 In this case each node p   C(r) pushed a value of 1 to its parent, if mp,v > 0.
mp,v vector to its parent node.
The sum of the values propagated from all p   C(r) to r de nes  mr.
Although the sampling process de ned here is reasonable in theory, it might be extremely ine cient to store   values for all nodes.
Considering a modest vocabulary of 100k distinct terms, it is di cult to keep a vector for each region.
To address this we use the sparsity of regional language models and adopt a space e cient way to store these vectors.
Given the current region assignment, we need to sample the topic allocation variable z(d,i) for word w(d,i) in document d: P (z(d,i) = k | w, z (d,i), r, l,  ,  )   P (z(d,i) = k | z (d,i), r,  ,  )P (w(d,i) | z, w (d,i),  ) Since all   are integrated out, this is essentially similar to the Gibbs sampling in LDA where document-level topic proportions in LDA becomes region-level topic proportions.
Thus, we can utilize a similar equation to sample topic assignments.
Note, as we discussed in the last section, we have a (T + 1) matrix   where the  rst dimension is a special row for regional language models that are distinct for each region.
The sampling rule is as follows:   (cid:16) (cid:0) n  i  n r,k + n  i r,0 + n  i r,k +  (r),k  i r,0 +  (r),0 (cid:105)  i k,v +   i k,w +V   (cid:17)(cid:104) m (cid:80) (cid:1)(cid:104) m i (cid:80) w m r,v +  mr,w + (r),v  i r,w +  mr,w +  w m (cid:105) k (cid:54)= 0 k = 0 (15) where v   w(d,i), nr,k is the number of times topic k appearing in region r and mk,v is the number of times term v assigned to k. Here, nr,0 and mr,v serve the purpose for the special index for the regional language model.
Note, n i  and m i  mean that the count should exclude the current token.
For all latent regions, we sample their mean vectors as a block using the multi-scale Kalman  lter algorithm [14].
The algorithm proceeds in two stages: upward  ltering phase and downward-smoothing phase over the tree.
Once the smoothed posterior probability of each node is computed, we sample its mean from this posterior.
We de ne the following two quantities,  n to be the prior covariance of node n, the sum of the covari-ances along the path form the root to node n, and Fn =  level(n) 1[ level(n)] 1, which are used to ease the computations below.
i.e.
We  rst begin the upward  ltering phase by computing the conditional posterior for a given node n based on each of its children m   C(n).
Recall that each child 0 of every node specify the set of documents sampled directly from this node.
Thus we have two di erent update equations as follows: (cid:104) (cid:105) 1  n,0 =  n (n)  1  (n)  n,0 =  n,0   (n) + |C(n)| n (cid:104) (cid:88) (cid:105) Id d C(n,0) (16)
  n,m = Fm mF T m + Fm n (17) where m   C(n).
Once these quantities are calculated for all children nodes for n, we update the  ltered mean and covariance of node n, ( n,  n) based on its downward tree as follows: (cid:105) 1 (cid:104)  n =
  n =  n (cid:88) n,m      1  1 n ]
  1 n + (cid:104) (cid:88) m C(n) (cid:105) m C(n)  1 n,m n,m
 (18) Once we reach the root node, we start the second downward smoothing phase and compute the smoothed posterior for each node ( (cid:48) n), as follows: n,  (cid:48) (cid:48)   root =  root   (cid:48) n =  n + Jn (cid:48) n =  n + Jn  
 (cid:104) (cid:104) (cid:48) root =  root  (n)    (n),n (cid:48)  (n)    (n),n (cid:48)
 (cid:105) (cid:105)   (19)
 n (20)  1 where Jn =  nF T  (n).
Here,  .,.
and  .,.
are from upward n phase.
After upward and downward updates, we sample the mean  n of each node n from N( (cid:48) n,  (cid:48) n).
We demonstrate the e cacy of our model on two datasets obtained from Twitter streams.
Two types of location information are attached to tweets: 1) geographical locations and 2) Twitter Places, a set of pre-de ned places of interest.
For geographical locations, each tweet contains a real-valued latitude and longitude vector.
For Twitter Places, we convert them into real-valued latitudes and longitudes.
We ignore all tweets without location information.
Moreover, we remove all non-English tweets.
This is achieved by a simple dictionary based method.
We randomly sample
 set of tweets between January 2011 and May 2011, resulting
 larger than the ones used in some similar studies (e.g, [17,
 split the tweets into disjoint training and test subsets such that users in the training set do not appear in the test set (we use 80%-20% split).
In other words, users in the test set are like new users.
This is the most adversarial setting.
In order to compare with other location prediction methods, we also apply our model a dataset available at http://www.ark.cs.cmu.edu/GeoText/, denoted as DS2, using the same split as in [17].
The priors over topics and topics mixing vectors were set to .1 and  ,   to .1 favouring sparser representation at lower levels.
The remaining hyper-parameters are tunded using cross-validation.
We ran the model until the training likelihood asymptotes.
Our model sheds some light on interesting patterns that are not easily obtained in other models: Figure 3 provides a small subtree of the hierarchy discovered on DS1 with the number of topics  xed to 10.
Each box represents a region where the root node is the leftmost node.
The bar charts demonstrate overall topic proportions.
The words attached Table 1: Top ranked terms for some global topics.
Entertainment video gaga tonight album music playing artist video itunes apple produced bieber #bieber lol new songs Sports winner yankees kobe nba austin weekend giants horse #nba college victory win Politics tsunami election #egypt middle eu japan egypt tunisia obama afghanistan russian Technology iphone wi  apple google ipad mobile app online  ash android apps phone data Table 2: Location accuracy on DS1 and DS2.
Results on DS1 Yin 2011 [31] Hong 2012 [21] Full Results on DS2 Eisenstein 2010 [17] Wing 2011 [30] Eisenstein 2011 [16] Hong 2012 [21] Full Avg.
Error Regions





 Avg.
Error Regions ---






to each box are the top ranked terms in regional language models (they are all in English since we removed all other content).
Because of cascading patterns de ned in the model, it is clear that topic proportions become increasingly sparse as the level of nodes increases.
This is desirable as we can see that nodes in higher level represent broader regions.
For instance, the three regions shown on the  rst level roughly correspond to Indonesia, the USA and the UK.
This is consistent with our observation that users from these countries are active in generating geotagged Tweets.
Also, by investigating the top ranked terms, we found that regional language models can capture the area dependent variations of languages, thus providing more discriminative features to location prediction.
Some examples of global topics are shown in Table 1.
Compared to regional language models, it is also clear that these shared topics capture higher level of interests among users across di erent regions2.
Compared to similar approaches such as [21] and [16], our hierarchical structure plus global topics has more expressive power is arguably more intuitive.
As discussed in Section 1, users  mobility patterns can be inferred from content.
We test the accuracy by estimating locations for Tweets.
Di ering from [17] who aim to estimate a single location for each user (note that they use the location of the  rst tweet as a reference, which may not be ideal), our goal is to infer the location of each new tweet, based on its content and the author s other tweets.
Based on our statistics, only 1%   2% of tweets have
 Tunisia as these were globably popular keywords during the time the data was collected   Jan-May 2011
 Table 3: Accuracy of di erent approximations and sampling methods for computing  r.
Method Minimal Paths Maximal Paths Antoniak







 Table 4: Ablation study of our model Results on DS1 Avg.
Error Regions Hong 2012 [21] Flat Topic Global Full









 Results on DS2 Avg.
Error Regions Hong 2012 [21] Flat Topic Global Full









 either geographical locations (including Twitter Places) explicitly attached, meaning that we cannot easily locate a majority of tweets.
However, geographical locations can be used to predict users  behaviors and uncover users  interests [13, 12] and therefore it is potentially invaluable for many perspectives, such as behavioral targeting and online advertisements.
For each new tweet (from a new user not seen during training), we predict its location as  ld.
We calculate the Euclidean distance between predicted value and the true location and average them over the whole test set

 tal number of tweets in the test set.
The average error is calculated in kilometres.
We denote our full model as Full.
For DS1 we compare Full with the following approaches: (cid:80) l( ld, ld) where l(a, b) is the distance and N is the to-no regional language models in the model.
Besides, no user level preferences are learned in the model.
Hong 2012 [21] Their method utilizes a sparse additive generative model to incorporate a background language models, regional language models and global topics.
The model also considers users  preferences over topics and regions as well.
For all these models, the prediction is done by two steps: 1) choosing the region index that can maximize the test tweet likelihood, and 2) use the mean location of the region as the predicted location.
For Yin 2011 and Hong 2012, the regions are the optimal region which achieves the best performance.
For our method, the regions are calculated as the average of number of regions from several iterations after the inference algorithm converges.
The results are shown in the top part of Figure 2.
The  rst observation is that Full model outperforms Yin

 ber of regions as well as the number of topics, which requires a signi cant amount of computational e orts, while for Full, the number of regions grows naturally with the data.
Also, we notice that the optimal number of regions inferred by Full is larger than its counterparts Yin 2011 and Hong 2012.
We conjecture that this is because the model organizes regions in a treelike structure and therefore more regions are needed to represent the  ne scale of locations.
For the comparison on the DS2 dataset, we compare with: Eisenstein 2010 [17] The model is to learn a shared topic matrix and a di erent topic matrix as the regional variation for each latent region.
No user level preferences are learned in the model.
The best reported results are used in the experiments.
Eisenstein 2011 [16] The original SAGE paper.
The best reported results are used in the experiments.
Wing 2011 [30] Their method is essentially to learn re-Yin 2011 [31] Their method is essentially to have a global set of topics shared across all latent regions.
There is gional language models per explicit regions.
Hong 2012 [21] This was the previous state of the art.
and, #followfriday , the, weekend, all, play, google , # , has, did jakarta, #iphonesia, news, happy, #jakarta, facebook, #indonesia, day, #tv #ladygaga, #ff, the, bieber, sunday, home, #<, youtube, holiday, twi=er london, park, and, #uk, home, all, not, share, #followfriday #california, earth, air google, #sfo, terminal, #<, google , # , #ca nyc, manha=an, #show, #ladaygaga, share, york, #<, # , city, new new, manha=an, , brooklyn, jus>n, nyc, google , #nyc, bronx, york #j@, land, co ee, kennedy, port, air, star, delay,  ight, terminal #occupy, #nyc , park, #<, new, street, #free, #show, yankees,
 For [21], the optimal region is reported from the paper.
The best reported results are used in the experiments.
For our method, the regions are calculated as the same fashion as above.
The results are shown in the second part of Figure It is obvious that our full model performs the best on
 this public dataset.
Indeed, we have approximately 40% improvement over the best known algorithm [21] (note that area accuracy is quadratic in the distance).
Recall that all prior methods used a  at clustering approach to locations.
Thus, it is possible that the learned hierarchical structure helps the model to perform better on the prediction task.
In Section 5.3, we discussed how to sample regional language models.
In Table 3 we compare the two approximation methods with directly sampling from Antoniak distributions.
We can see that all three methods achieve comparable results although sampling Antoniak distributions can have slightly better predictive results.
However, it takes substantially more time to draw from the Antoniak distribution, compared to Minimal Paths and Maximal Paths.
In Table

 In this section, we investigate the e ectiveness of di erent components of the model and reveal which parts really help with the performance, in terms of location prediction.
For both DS1 and DS2, we compare the following versions: Flat: We do not have a hierarchical structure of regions while the number of regions is still in nite.
Regional language models and a set of global topics are utilized.
Topic: No regional language model version of our proposed model: In this model, we still have the hierarchical structure over regions but no only having a global set of topics without regional language models.
Global: No personal distribution over the tree structure, we assume that all tweets are generated by a  ctitious user and no personal preferences are incorporated.
Full: Our full model.
The results are shown in Table 4.
The  rst observation is that Topic, Global and Full, which utilize hierarchical structures of regions are better than other methods.
This validates our assumption that hierarchies of regions can control the scope of regions and therefore smaller regions can be discovered from the data.
This is also clearly observable from the optimal number of regions these methods have discovered.
For Topic, it is only slightly better than Hong as it does not incorporate regional language models into account.
We can see the e ect of regional language models by focusing on Global where no personal distributions over the tree is introduced.
In summary, Full demonstrated that personalized tree structures can further boost the performance.
In order to understand how our model performs in terms of prediction we conduct a qualitative error analysis on our model as well on the the state-of-the-art model [21] on all users in the USA on DS1.
The results are given in Figure 4.
Each circle in the map represents 1000 tweets.
The magnitude of the circle represents the magnitude of average error made for these 1000 tweets.
Note that the circles are re-scaled such as to be visible on the map (i.e. radii do not correspond to absolute location error).
Figure 4: Error analysis for the state-of-the-art model [21] (blue) and our model (orange) on DS1.
We observe that in the industrialized coastal regions both models perform signi cantly better than in the Midwest.
This is because that we have more users in those areas and therefore we can, in general, learn better distributions over those regions.
At the same time, users in those areas might have much more discriminative mobility patterns relative to users in the Midwest.
The second observation is our method consistently outperforms [21].
This is particularly salient in the Midwest.
Nonparametric Bayesian models have been demonstrated as an e ective tool to discover hierarchies in many applications, however, existing methods usually exhibit a global distribution over the tree structure, not allowing this distribution to vary for di erent users.
The latter can be an impediment in applications such as behavioral targeting and online user pro ling.
In addition, the cascading of parameter spaces over hierarchies is usually an artifact of the stick-breaking process, not necessarily directly controlled by the design of models.
This leads to in exibility when handling multiple types of parameters which should be cascaded through the discovered tree structure.
In this paper, we propose a uni ed framework, the nested Chinese Restaurant Franchise, to discover a uni ed hidden tree structure with unbounded width and depth while allowing users to have di erent distributions over this structure.
Furthermore, the patterns of parameters cascading over the tree are explicitly speci ed.
An e cient algorithm is developed to perform the posterior inference.
We apply the framework to organize Twitter messages into a hierarchical structure and show that this tree structure can be used to predict locations of unlabeled messages, resulting in signi cant improvements to state-of-the-art approaches, as well as revealing interesting hidden patterns.
For future work, we plan to exploit distributed sampling techniques and data layout as in [2, 6] in addition to hash-based sampling [5] to scale the infer ence algorithm to the full twitter dataset.
In addition, to model temporal variation for extra location signals, we plan to extend the current work with ideas from [3, 4, 7].
Acknowledgement We thank the anonymouse reviewers for their helpful comments and Twitter for permitting us to publish results based on dataset

 [18] T. S. Ferguson.
A bayesian analysis of some nonparametric problems.
The Annals of Statistics,
 [19] Q. Hao, R. Cai, C. Wang, R. Xiao, J.-M. Yang, Y. Pang, and L. Zhang.
Equip tourists with knowledge mined from travelogues.
In Proceedings of WWW, pages 401 410, New York, NY, USA, 2010.
ACM.
[20] T. Hofmann.
Unsupervised learning by probabilistic latent semantic analysis.
Machine Learning,
 [21] L. Hong, A. Ahmed, S. Gurumurthy, A. Smola, and K. Tsioutsiouliklis.
Discovering geographical topics in the twitter stream.
In World Wide Web, 2012.
[22] L. F. James.
Coag-frag duality for a class of stable poisson-kingman mixtures, 2010.
[23] Q. Mei, C. Liu, H. Su, and C. Zhai.
A probabilistic approach to spatiotemporal theme pattern mining on weblogs.
In Proceedings of WWW, pages 533 542, New York, NY, USA, 2006.
ACM.
[24] S. Roller, M. Speriosu, S. Rallapalli, B.
Wing, and J. Baldridge.
Supervised text-based geolocation using language models on an adaptive grid.
In Proceedings of EMNLP, pages 1500 1510, Stroudsburg, PA, USA,
 [25] S. Sizov.
Geofolk: latent spatial semantics in web 2.0 social media.
In Proceedings of WSDM, pages 281 290, New York, NY, USA, 2010.
ACM.
[26] Y. Teh, M. Jordan, M. Beal, and D. Blei.
Hierarchical dirichlet processes.
Journal of the American Statistical Association, 101(576):1566 1581, 2006.
[27] H. Wallach.
Structured Topic Models for Language.
PhD thesis, University of Cambridge, 2008.
[28] C. Wang, J. Wang, X. Xing, and W. Ma.
Mining geographic knowledge using location aware topic model.
In Proceedings of the 4th ACM workshop on Geographical Information Retrieval, pages 65 70, New York, NY, USA, 2007.
ACM.
[29] X. Wang, A. McCallum, and X. Wei.
Topical N-grams: Phrase and topic discovery, with an application to information retrieval.
In International Conference on Data Mining ICDM, pages 697 702.
IEEE Computer Society, 2007.
[30] B.
Wing and J. Baldridge.
Simple supervised document geolocation with geodesic grids.
In Proceedings of ACL, pages 955 964, Stroudsburg, PA, USA, 2011.
Association for Computational Linguistics.
[31] Z. Yin, L. Cao, J. Han, C. Zhai, and T. Huang.
