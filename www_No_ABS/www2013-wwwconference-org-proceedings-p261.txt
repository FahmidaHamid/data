Although many research e orts concentrated on the development of methods and tools to generate web wrappers, large-scale data extraction is still a challenging issue.
Early proposals to infer web wrappers for data intensive websites were based on supervised approaches.
Wrappers Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
were generated starting from a set of training data, typically provided as labeled values, i.e., annotated pages.
To overcome the need of human intervention in the production of training data, unsupervised approaches have been investigated.
They exploit the local regularities of script-generated web pages to infer a wrapper.
Unsupervised approaches adopt sophisticated algorithms to generate the wrappers, and represent an attempt to \scale-up" the wrapper generation process.
Unfortunately, although they eliminate the costs of training data, they have a limited applicability because of the low precision of the produced wrappers.
The recent advent of crowd sourcing platforms (such as, for example, Amazon Mechanical Turk) can open new opportunities for supervised approaches.
These platforms provide support for managing and assigning mini-tasks to people.
In the wrapper production process, crowd sourcing platforms can be used to produce massive training data for supervised wrapper inference systems.
As they facilitate the involvement of a large number of persons to produce the training data, we may say that they represent a solution to \scale-out" the wrapper generation process.
However, to obtain an e cient and e ective process, two main issues need to be addressed.
First, since mini-tasks are performed by non-expert people, they should be extremely simple.
Second, since the costs of producing wrappers become proportional to the number of mini-tasks, the number of training data produced by the crowd to infer a wrapper should be minimized.
We are developing a system that relies on crowd sourcing platforms to create accurate web wrappers.
Our system adopts a supervised approach to infer wrappers with training data generated by means of a crowd computing platform.
The mini-tasks submitted to the platform consist of membership queries (MQ), which are the simplest form of queries, since they admit only a yes/no answer (e.g., \Ob-serve this page: is the string  Dean Martin  a correct value to extract?")
[1].
To address the costs issue, our system is able to select the queries that more quickly bring to infer an accurate wrapper, thus minimizing the number of mini-tasks assigned to the crowd platform.
The traditional approach to build wrappers for large websites is to provide training data for the attributes of interest on a set of pages, and then to apply an inference algorithm to learn a wrapper.
We observe that there are two hidden assumptions behind the approach: 1) the formalism used by the learning algorithm to specify the wrapper is su ciently expressive, 2) the wrappers inferred from the sample set, hopefully work also on the whole set of pages.
sumptions can largely a ect the learning costs and the quality of the solution.
Suppose we are interested in extracting data about actors from the pages of a large movie website.
A wrapper corresponds to a set of extraction rules, where each rule extracts the value of an attribute (e.g.
the actor name).
For the sake of simplicity, we concentrate on a single rule, but the discussion can be trivially extended to the more general case where a wrapper is composed of a set of rules.
To illustrate the example without digging into the technical details of the formalisms used to express the extraction rules, we represent pages as tables, where data is organized in rows and columns, as shown in Figure 1(a).
In such a simpli(cid:12)ed abstraction, an extraction rule speci(cid:12)es the cell containing the relevant data, and it can be expressed by absolute coordinates (e.g., (cid:12)rst row, second column), denoted abs(row,col), or by relative coordinates, that is, with respect to another cell (e.g.
the (cid:12)rst cell located right of/left of/above/under the cell containing  Home ), denoted right-of( x ).
For example, according to Hopkins s page, candidate extraction rules for Name are abs(1,1) and above( Height ).
Similarly, rules for Home are abs(6,2) and right-of( Home ).
More complex relative rules can be speci(cid:12)ed with a richer (more expressive) class, which admits longer paths from a pivot; e.g., below-right-of( Born ), to extract data from the cell containing the  Latest Film .
Later in the paper, we shall refer to a concrete representation (the same used in the implementation of the system), where pages are modeled with their DOM trees, and rules are XPath expressions.
The Expressiveness Problem.
As the following example illustrates, the number of training data to correctly infer a wrapper depends on the expressiveness of the language used to specify the rules.
Let us concentrate on the extraction of actors  Name.
Suppose that the learning algorithm adopts only absolute extraction rules: a correct rule for Name is abs(1,1).
Note that only one training data would su ce to infer this rule.
Suppose now to adopt a more expressive language, which also includes relative rules.
Using just one labeled value, say  Antony Hopkins  on page ph, several rules can be generated to extract the attribute Name: abs(1,1), above( Height ), above( 1.74m ).
To determine the correct rule it is necessary to carefully choose at least another annotation: only with the help of the labeled value  Laura Wolf  on page pw we have evidence that above( Height ) does not work as an extraction rule for Name.
This simple example illustrates a well known learning theory results: the more expressive is the model, the larger is the number of labeled examples that are necessary to infer the correct rule [1, 6] (intuitively, the space of hypotheses is larger and thus more examples are needed to discard the incorrect ones).
Supervised wrapper inference approaches that ignore the costs of training data tend to work with an overly expressive class of rules to have enough expressiveness for all the attributes.
However, as our example emphasizes, this implies more expensive training data.
The Sampling Problem.
Suppose that Home is an attribute to extract.
If the sample set is composed only of awarded actors (such as Hopkins), the inferred rule could not work for the broader set of all actors, including those without any award (such as Smith and Wolf).
For example, the rule abs(6,2) for Home might work for the awarded actors, but it does not extract the required information for others.
The usual approach to address this issue is to require a large set of labeled values that hopefully covers all the possible types of target pages.
However, if the labeled values are generated by submitting a task to a crowd sourcing platform, a bigger set implies higher costs; also, the training data should be representative for the whole collection of target pages and its choice is not trivial.
For some domains a set of labeled examples can be obtained automatically.
For instance, if we have a small database of popular actors, we can annotate the pages when they o er data matching with those stored in the database [8].
However, the database could be biased, e.g.
it contains only famous (awarded) actors, leading to the generation of wrong extraction rules, as discussed in the example.
We propose a logical framework based on original solutions for exploiting crowd platforms to infer wrappers around large web sources.
Since we aim at demanding to a crowd platform the burden of generating labeled examples, our approach considers a cost takes into account the number of membership queries submitted to a crowd platform.
Our framework includes a supervised active learning algorithm that aims at minimizing the number of membership queries to infer a wrapper.
Since the costs of learning a wrapper depends on the expressiveness of the class of rules, unlike traditional approaches we do not work with a statically de(cid:12)ned class, but we propose an original approach inspired to a statistical learning technique [14, 15], called structural risk minimization (SRM), in which the expressiveness of the language is determined at runtime.
To this end, we organize the class of candidate rules into a hierarchy of classes of increasing expressiveness: initially the correct rule is searched only within the less expressive class.
The class of rules is lazily expanded only if it is actually needed.
A fundamental decision is whether and when expanding the set of candidate rules: we introduce a probabilistic model that evaluates the quality of the wrapper by computing the probability that the rules in the current class of candidate rules are correct.
Our learning algorithm exploits the probabilistic model in order to trigger the expansion only if there is enough evidence that the correct rule is not amongst the current set of candidates.
The algorithm adopts active learning techniques [13] to ask for additional labeled values: it selects a value and poses a membership query to obtain a con(cid:12)rmation about the correctness of the extracted value.
By accurately choosing this query, the user interaction is minimized.
Our experiments prove that our learning algorithm can infer high quality wrappers with a fraction of the queries required by a traditional approach.
Our algorithm infers the wrapper on a set of labeled values, and the quality model evaluates the wrapper on a larger set, ideally on the whole set of target pages.
However, in many practical cases the evaluation on the whole set is unrealistic because of its size.
To overcome this issue, our frame-





Laura Wolf
 Latest Film One week (2007)





 Ranking laura.w.me
 pw v+ 0 = john.s.me (d) initial ann.
for Home






 Ranking Latest Film Anthony Hopkins 1.74m

 Noah (2014) Oscar anthony.h.me
 ph r1 r2 r3 r4 r5 r6 abs(5,2) right-of( Home ) above( 1002 ) below( Waco (1966) ) below-right-of( Latest Film ) above-right-of( Ranking ) (b) extraction rules John Smith





 Ranking Latest Film 1.88m

 Waco (1966) john.s.me
 ps (a) set of pages r1 Oscar john.s.me
 r2, r6 anthony.h.me john.s.me laura.w.me r3, r4 nil john.s.me nil r5 Oscar john.s.me laura.w.me ph ps pw (c) extracted values Figure 1: Running Example work also includes an algorithm to compute a small set of representative pages: the extraction rules inferred and evaluated against our representative set also work on the larger set of target pages.
Our experiments show that our algorithm is able to select a representative set several orders of magnitude smaller than the whole set of target pages, and that wrappers inferred from our representative sample outperforms (in term of precision and recall) wrappers generated from much larger randomly selected sets.
In summary, we make the following contributions: (cid:15) a framework that exploits crowd platforms to infer wrappers around large web sources; (cid:15) a cost model that takes into account both the processing costs and the human intervention costs needed to feed the crowd platform; (cid:15) a probabilistic quality model for computing correctness of a wrapper over the whole set of pages even if it is inferred on a smaller set of pages chosen by a sampling algorithm; (cid:15) an active learning algorithm for generating high quality wrappers in a cost-e ective manner; (cid:15) a sampling algorithm for selecting small yet representative sets of pages; (cid:15) the results of an experimental evaluation of our framework on real life websites.
The paper is organized as follows: Section 2 formalizes our setting; Section 3 develops our probabilistic model to characterize the correctness of extraction rules; based on the model, Section 4 presents the active learning algorithm to infer extraction rules; Section 5 introduces the sampling algorithm; Section 6 discusses experiments with a set of sources from the Web; (cid:12)nally, Section 7 discusses related work and Section 8 concludes the paper.
Let U = fp1; p2 : : : png be a set of pages.
Every page publishes several attributes of interest (e.g., in our running example, actors  Ranking, Height, etc.).
For simplicity, we develop the discussion concentrating on one attribute, and we assume that its values are either a textual leaf of the DOM tree representation of the pages, or a distinguished nil value.
We write v 2 p to denote that v is a value of the page p, and pv to denote the page in which the value v is located.
We refer to a generic extraction rule (or simply rule) r over the set of pages U as a concrete tool to build a vector of values indexed by the pages in U such that r(p) 2 p[fnilg.
Every rule extracts one vector of values from U denoted r(U ).
Figure 1(c) shows the vectors extracted by the rules r1, r2, r3, r4, r5, r6 in Figure 1(b).
We denote by R(U ) the set of vectors obtained by applying a set of rules R over U , and blur the distinction between a rule and the vector it extracts from U .
Note that jR(U )j (cid:20) jRj, with the strict inequality holding whenever a vector is extracted by di erent rules.
We introduce the concept of labeled sample value (or simply labeled value) vl where v 2 pv is a value from a page pv, and l 2 f+;(cid:0)g is either a positive or a negative label.
In the following v+ and v denote a positively labeled value (or annotation) and a negative labeled value, respectively, i.e., the two possible answers to a MQ.
(cid:0) A rule r is admissible wrt a set of labelled values L (denoted L(r)) i : L(r) , 8vl 2 L; l = + !
r(pv) = v l = (cid:0) !
r(pv)  = v that is, it is compliant with the labels in the set.
The concept can be trivially extended to a set of rules R.
We denote by RL = fr 2 R : L(r)g the subset of admissible L (U ) all the values they extract rules in R wrt L, and by bV R from U : bV R L (U ) = fv : v = r(p); r 2 RL; p 2 Ug.
Example 1.
Let ph, ps and pw be the pages in Figure 1(a) and let U = fph; ps; pwg.
The attribute Home is extracted by the rule r2 =right-of( Home ): two positive annotations 2630 ; v+ 0 = john.s.me  and v+ are v+ 1 = laura.w.me ; a negative la (cid:0)
 belled value is v L = fv+ g. Now consider another rule r1=abs(5,2) (cid:0) 1 ; v and the set of rules R = fr1; r2g.
Then r1 is not admissible
 wrt L since r1(pw) = 5500  which is the negatively labelled L (U ) = fv0; v1; v3g value v where v3 = anthony.h.me .
(cid:0) In the following, given a set of rules R, we will only consider special ordered sets of labeled values, called training sequences, which are formed by an initial set of positive annotations, and then by adding only new values which are still admissible with respect to those already seen.
Intuitively, a training sequence lists the answers to the MQ posed to learn a extraction rule.
A Training Sequence (t.s.)
L wrt a set of rules R and a set of pages U is speci(cid:12)ed by a sequence of labeled values that de(cid:12)nes a sequence of (observed) sets Lk with Lk+1 = Lk[fvkg = fv+ a(cid:0)1; va : : : ; vkg such that: (i) it begins with sequence of a (cid:21) 1 annotations v+  = nil with positive labels, and (ii) 8k (cid:21) a; vk 2 V R Lk (U ) n Lk.
Lk (U ) = bV R 0 ; : : : ; v+ 0 ; : : : ; v+ a(cid:0)1 The constraint (i) on the (cid:12)rst annotations of the sequence is useful to generate a (cid:12)nite set of admissable rules RLa , whereas the constraint (ii) on the remaining values entails that the new value vk that forms Lk+1 from Lk leads to smaller and smaller admissible sets: RLk+1 (cid:18) RLk .
It is worth noting that RLk+1 plays the role of what the learning communities call the version-space [13], i.e. the set of hypotheses still plausible after having considered an input set of labeled values.
L2 (U ) = bV R Example 2.
Consider again the above Example 1 and is g and RL2 = fr2; r6; r5g.
Possible candidate L2 (U )nL2 = f anthony.h.me ;  Oscar g.
our running example in Figure 1.
Then a possible t.s.
L2 = fv+ 0 ; v+
 values are V R A new MQ can be formed by choosing a new value v2 to query from the elements in V R L2 (U ).
E.g., \ is  anthony.h.me  a correct value?
".
In the following we will uniformly refer to both L and one of its observed subsets Lk blurring the di erences between the two concepts whenever the context clari(cid:12)es which one is actually involved.
It can always be decided whether a rule extracting the desired vector exists.
However, since it is not known in advance whether that rule was in the set of all candidate rules, the only certain way to be sure of its presence is by checking every single page [1].
We now introduce a probabilistic model for evaluating the quality of a wrapper expressed as an extraction rule r taken from a class of candidate extraction rules R.
Our model computes: (i) the probability P (rjLk+1) that a rule r is correct, observed a t.s.
Lk+1; (ii) the probability P (RjLk+1) that the correct rule is not in R, observed Lk+1, i.e., that a correct rule has not been generated at all.
Table 1 summarizes the notations used for the main events covered by our analysis, and their probabilities.
By applying Bayes  Table 1: The main events of the bayesian analysis Probability Notation
 jr; Lk) jR; Lk) P (vl k P (vl k P (Lk+1) = P (vlk k ; Lk) Event prior probability that a/none rule in R is correct likelihood of vl observed Lk likelihood of vl is not in RLk , observed Lk probability of a t.s.
Lk+1 k if the correct rule k if r is correct, theorem: P (rjLk+1) = P (vl k jLk) jr; Lk)P (rjLk) P (vl k jR; Lk)P (RjLk) P (vl k (1) P (vl k P (RjLk+1) = (2) jLk) is a normalization factor that can be ex-jLk) where P (vl k pressed as:  kjri; Lk)P (rijLk) + P (vl kjR; Lk)P (RjLk) P (vl Lk r2R For any k, P (rjLk+1) and P (RjLk+1) can be de(cid:12)ned iter-jR; Lk), P (RjLk) and atively by means of P (vl jR; Lk) can P (rjLk).
The probabilities P (vl k be de(cid:12)ned by abstracting the actual process that leads to the observed t.s.
into a simple generative model.
This is essentially equivalent to de(cid:12)ne a p.d.f.
over every t.s.
jr; Lk) and P (vl jr; Lk), P (vl k k k By repeatedly applying the bayesian updating rules expressed by equations 1 and 2, the model allows the computation of P (RjLk+1) and P (rjLk+1) for any k, starting from prior-probabilities P (RjLa) = P(R) of having generated a correct rule in R, and the probability P (rjR; La) of r being a correct rule once the t.s.
La has been observed.
The iteration continues until admissible rules exist, i.e., until Lk  =  .
For bootstrapping our probabilistic model, the following probabilities are needed: (i) the prior probability P(R) that a correct rule has been generated in the class of rules R, and; (ii) the probability P (rjR; La) that the extraction rule r 2 R does extract the correct vector of values from the input set of pages U once its has been observed the initial set of annotations La.
For the former prior P(R), we follow a standard approach, and estimate it by measuring the frequency of the involved events on a su ciently large set of attributes: P(R) has been (cid:12)xed to nh N where nh is the number of attributes extracted by a rule in R and N is the number of attributes sampled (we considered N = 290 attributes).
As regards the latter p.d.f.
P (rjR; La), if jRLa (U )j > 0 we redistribute P(R) according to a uniform p.d.f.
over all the vectors extracted by admissible rules r 2 RLa , as follows: P (rjR; La) = where: n = jfr n jRLa (U )j (cid:1) P(R);   2 RLa (U ) : r ; r     (U ) = r(U )gj:
 We now describe the generative model for the training it is used to derive the posterior probability sequences: 264(cid:0)
 P (vl (Lk; r) = V kjr; Lk) =

 jV Lk (U )j
 P (rjLk) that r(U ) is the correct vector to extract once a t.s.
Lk has been observed.
This vector is not known in advance, but the values forming the t.s.
Lk will be labeled as either positive or negative according to it.
Let P(R) be the prior probability that the correct vector can be extracted by a rule belonging to R. We suppose that the acquisition of a new labeled value vk to form Lk+1 from Lk follows a uniform p.d.f.
amongst all values still queryable, Lk (U ) n Lk.
Similarly, given a
 i.e., the values in V Lk (U )\ r(U ) denote the set
 correct rule r, let V +(Lk; r) = V of all and only the values that can form new positive values, Lk (U ) n V +(Lk; r) the set of values that
 and V { can form negative values.
It follows: Lk (U ) = bV ; i  vk 2 V l(Lk; r) ; otherwise jR; Lk) following an approach based on a uniform p.d.f.
over all possible values.
These are essentially all the values in V Lk (U ) but only the Lk (U ) can be labeled either positive or values in V negative (and we assume with the same probability) while Lk (U ) will surely be labeled nega-the values in V tive.
Therefore, it follows that P (vl k jR; Lk) = Lk (U ) \ R Lk (U )j ; i  vk 2 V
 Lk (U ) n R ; i  vk 2 V
 Lk (U )j ; i  vk  2 V
 Lk (U ) Note that the exact computation of the set R expensive, since given a value v 2 V out whether v 2 R very large number of vectors in R Lk (U ) can be
 Lk (U ), in order to (cid:12)gure Lk (U ), we should enumerate a potentially
 Similarly, we can compute P (vl k
 Lk (U )\R

 jV Lk (U )nR jR; Lk)= jR; Lk)= Lk (U )\R
 Lk (U ) n R
 k (cid:0) P (v k


 Lk (U ).
Lk (U ) Lk (U ) { We adopt an approximate and e cient solution based on the assumption that the equivalences holding for k = 0:1
 Lk (U ) =  , also hold for any k > 0.
Hence, it can be rewritten as:
 Lk (U ) and V Lk (U ) n R
 Lk (U ) = V kjR; Lk)   P (vl

 Lk (U )j

 ; i  vk 2 V ; i  vk  2 V
 Lk (U )
 Lk (U ) (3) Actually, this is an oversimpli(cid:12)cation when k gets bigger
 Lk (U ) and V Lk (U ) gets smaller Lk (U )  =  .
Since the algorithm and approaches jUj: both R Lk (U ) n R
 and smaller and V looks for the correct rule while minimizing k, in our setting this sempli(cid:12)cation does not signi(cid:12)cantly a ect the results.
The probabilistic model developed in the previous section aims at computing, observed a t.s.
Lk+1, the probability P (rjLk+1) that a given extraction rule r within a set of candidate rules R is correct, and the probability P (R Lk+1 ) that the correct rule is not inside R.
We now present an algorithm, called alf, that exploits the model to infer a wrapper.
alf aims at inferring extraction rules with a high probability of correctness, while minimizing the length of the t.s., i.e., the number of membership queries to be posed to the crowd.
As we discussed in Section 1.1, the length of the t.s.
depends on the expressiveness of the class of rules.
Rather then relying on a statically designed (and possibly oversize) class of extraction rules, alf organizes the class of candidate rules R into a hierarchy of classes fRhg0(cid:20)h(cid:20)m of increasing expressiveness.
The algorithm starts by looking for a rule within the class R0 of the lowest expressiveness and computes the probability of its correctness.
If such a probability is not satisfactory, the algorithm expands the class to the larger class R1, and consequently poses more membership queries, thus enlarging the t.s.
In order to choose the appropriate membership queries, alf uses an active approach by selecting the best queries to minimize its total number as further discussed in Section 4.1.
The process is repeated until either it (cid:12)nds a rule of satisfactory probability, or it concludes that it is unlikely that this rule exists within the considered hierarchy.
The approach is independent of the details of the formalism used to express the extraction rules.
In our implementation, we make use of XPath expressions; namely, we use absolute and relative XPath expressions.
The former specify paths from the root to the leaf node that contains the value to be extracted; the latter are paths starting from a generic pivoting node.2 Relative XPath expressions are classi(cid:12)ed based on the path length.3 Our hierarchy fRhg organizes absolute and relative XPath expressions as follows: R0 is the class of absolute XPath expressions, Rh, for 0 < h (cid:20) m, is obtained by adding to Rh(cid:0)1 the class of relative XPath rules with path length h.
g a(cid:0)1 Lk+1 ); o ; : : : ; v+ Lk+1 , P (Rh Listing 1 alf: An active learning algorithms for extraction rules Input: a set of pages U = fp1; : : : ; pjUjg Input: a set of initial annotations La = fv+ Parameter: a hierarchy of rules fRhg over U Output: P (rjLk+1) over r 2 Rh 1: let h 0;
 La ;
 vk chooseQuestion(R; Lk);
 l oracle(vk);
 Lk+1 Lk [ fvl

 compute P (rjLk+1); 8r 2 R according to eq.
1;
 compute P (RhjLk+1) according to eq.
2;
 h h + expandRuleSet(R; Lk+1);
 k k + 1;
 12: end while
 return Rh
 15: end if 16: return ?
; Lk+1 , P (rjLk+1) and P (RhjLk+1); Lk+1 ; g; k
 candidate pivot.
sured according to the number of edges crossed in the DOM representation of the HTML pages but considering contiguous siblings at distance 1.
as input it takes a t.s.
L built by actively [13] asking to an oracle (here modeled by means of the subprogram oracle()) the label of a value chosen by the subprogram choose-Question(); as output, it returns a p.d.f.
describing the probability of correctness over the rules still admissible, and the possibility that a correct rules does not exist at all.
Initially, R0 is taken as initial set of candidate rules, and the set of rules admissible wrt the initial annotations R0 La is computed (lines 1-2).
In every iteration, the oracle is asked to label a new value vk (lines 4-5) and the t.s.
is updated to obtain Lk+1 (line 6).
Then, the set of admissible rules is updated (line 7) (recall that RLk+1 (cid:18) RLk ), and the probabilities P (rjLk+1) and P (R Lk+1 ) are consequently updated (lines 8-9).
expandRuleSet() has to decide whether the set of candidate rule should be expanded (line 10).
alf can be instantiated by appropriately choosing the semantics of three subprograms: chooseQuestion(), which composes the next membership query, i.e., it selects the next value to be labeled by the user; halt(), which establishes an exit criterion before the t.s.
naturally expires (i.e., R becomes empty); expandRuleSet(), which decides at run-time whether Rh should be expanded with new candidate rules by incrementing h. The latter decision is based on the probability that the current class of rules cannot contain a correct rule (P (Rh Lk+1 )): the higher its value, the more likely new candidate rules are needed, and thus the class of rules needs to be expanded.
We now describes several strategies to instantiate the three subprograms.
The chooseQuestion() procedure chooses the next membership query: it decides the next value to be labeled.
We propose three alternative strategies: Entropy, Greedy, and Lucky, plus a baseline algorithm Random.
Random: It chooses a random admissible value: chooseQuestion(R,L) f return a random v 2 V R and it serves as a baseline against other strategies.
Entropy: It bases the choice on the p.d.f.
of the extracted values: a simple strategy is to choose the value on which rules most disagree, appropriately weighted according to their probability.
This is equivalent to compute the vote L (U ); g entropy [13] for each v 2 RLk (U ): H(v) = (cid:0)[P (v+jLk) log P (v+jLk) + P (v     P (v+jLk) = r2fr2R where: (cid:0)jLk) = P (v and: r2fr2R (cid:0)jLk) log P (v (cid:0)jLk)] (4) Lk :r(pv )=vg P (rjLk); Lk :r(pv ) =vg P (rjLk): are the probabilities that v is either a value to extract or an incorrect value, respectively.
The next value is that maximizing the vote entropy: chooseQuestion(R,L) f return argmaxv2V R L (U ) H(v); g This choice essentially removes the most uncertain value.
Greedy: The construction of the whole version-space is in-e cient, since it requires to enumerate all possible t.s.. However, the version-space can be exploited to (cid:12)nd the quickest t.s.
con(cid:12)rming that a given rule is a solution.
Let us call such a kind of sequences con(cid:12)rming t.s.
: they aim more at deciding as quickly as possible that a given rule is a solution, rather than at (cid:12)nding which is the solution.
In every search step, Greedy \elects" the most likely rule to play the role of the solution, and then it greedily builds a con(cid:12)rming t.s.
wrt that conjecture.
If, after a few labeled values, that rule is confuted and removed from the version-space, the whole process is repeated by formulating another conjecture around the most likely rule in the remaining version-space.
In this setting, the query is selected by greedily taking the value extracted by the supposedly \correct" rule from the page on which most other rules behaves di erently: if that value is labeled positive as expected, the largest number of rules is removed from the version-space.
(cid:3) chooseQuestion(R,L) f return r where: (cid:3) = argmaxr2RL(U ) P (rjL); = argmaxp2U ) g jfr(p) : r(p)  = r (cid:3) (p)gj: (cid:3) r (cid:3) p (p As the cost of this approach depends on the size of the version-space, it can be relevant in the early stages of the searching.
The next variant delays its construction until the best rule emerges as signi(cid:12)cantly more likely than other candidates.
Lucky: It is a hybrid of the former two approaches, and it works in two phases: (cid:12)rst, it accumulates enough evidence of the correctness of a rule by using Entropy; then, it switches to Greedy modality to con(cid:12)rm it.
The switch is triggered by a (cid:12)xed threshold (cid:21)r(cid:3) on the probability of the most likely rule r This approach can be seen as a generalization of Greedy: at the beginning it waits to observe enough evidence before allocating all its trust on the most likely rule.
(cid:3) .
Example 3.
Reconsider the running example in Figure 1, and the t.s.
L1 = fjohn.s.me+g.
Suppose that P(R) = 0:96 and that the probability is equally distributed among the set of candidate rules.
P (v+
 jL1) and P (v (cid:0)
 jL1) can be computed as follow: v1 anthony.h.me laura.w.me Oscar
 : : : P (v+

 0:24 (cid:1) 2 0:24 (cid:1) 2
 : : : jL1) P (v (cid:0)
 jL1) 0:24 (cid:1) 3 0:24 (cid:1) 2 0:24 (cid:1) 2 0:24 (cid:1) 3 : : : From P (vl obtained as follows:
 v1 anthony.h.me (cid:0)0:24 (cid:1) log(0:24) (cid:0) 0:72 (cid:1) log(0:72) = 0:58 (cid:0)0:48 (cid:1) log(0:48) (cid:0) 0:48 (cid:1) log(0:48) = 0:70 laura.w.me (cid:0)0:48 (cid:1) log(0:48) (cid:0) 0:48 (cid:1) log(0:48) = 0:70 (cid:0)0:24 (cid:1) log(0:24) (cid:0) 0:72 (cid:1) log(0:72) = 0:58 H(v1) Oscar
 : : : : : : Hence, Entropy can chooses v1 = laura.w.me or v1 = Oscar as the next value to query to get L2 = L1 [fv1g.
Note that P (rijL1) = 0:24, i = 1; : : : ; 6 (rules extracting the same vector are indistinguishable) and the set of admissible rules after L1 are equally probable.
In this case, Greedy would end up with a random selection of the most likely rule.
expandRuleSet() is in charge of deciding whether and when expanding the set of candidate rules used from a hierarchy of classes Rh.
We refer to this technique as SRM, since it is inspired by the Structural Risk Minimization principle originally proposed by the statistical learning community [15, 14] as a tool for dealing with the problem of over-(cid:12)tting.
Lk ) that the correct rule is not present in the current set of candidate rules Rh after observing as input a given t.s.
Lk.
We use a simple implementation expandRuleSet() based on a prede(cid:12)ned (cid:12)xed threshold (cid:21)R over P (R expandRuleSet(R, L) f if (R = Rm) return 0; // max expansion reached if (P (RL) > (cid:21)R ) return +1; else return 0; Lk ): g The set of rules is therefore enlarged lazily, i.e., only whenever according to P (RL) there is evidence that a correct rule is not amongst the currently available candidates.
The implementation of halt() depends on the overall goal of the search strategy.
A simple approach considers a minimum threshold (cid:21)r on the probability of the best rule: halt(R, L) f return (argmaxr2RL P (rjL) > (cid:21)r); g This function looks for the best rule r that suits the t.s.
and terminates as soon as it (cid:12)nds a rule with probability higher then (cid:21)r. It is an appropriate solution in many practical settings.
So far we considered feasible the application of our algorithm alf to the whole set of input page.
However, in many practical cases this assumption is unrealistic because of the number of pages (e.g., consider www.imdb.com, which provides more than 6 (cid:1) 106 pages about actors).
Finding a sampling set that is \cheaper" to work on, and yet it represents a larger population, is a traditional statistic problem.
In this section we contextualize this issue in our setting and move to the related problem of sampling the input pages into a much smaller set of sample pages.
The extraction rules can be evaluated on the sample set much more e ciently than on the whole set of pages; at the same time, a representative sample set must preserve the power of di erentiating the rules by showing all their di erences.
However, the sample pages need to be carefully selected to be representative while, at the same time, minimizing their number.
These aspects are often neglected in the literature.
Typically, sample pages are selected randomly, or they are collected following straightforward crawling strategies.
While random samples could end up not representing the whole set of pages, crawling strategies can lead to the composition of biased samples.
As an example, www.imdb.com exposes its content mainly in the form of top-lists, such as top-list movies, top-list actors and so on.
A crawler following the links in these lists will inherently collect biased samples concentrated around \famous" instances.
We formulate the problem of (cid:12)nding a set I (cid:26) U such that jIj   jUj yet I is representative (with respect to a given class of extraction rules R) of all the pages in U .
The representativeness of a set of pages I (cid:26) U wrt a set of rules R can be formalized by introducing the disagreement set of two extraction rules.
Given a set of pages P , and a set of rules R, the disagreement set, denoted as DP (ri; rj), between two rules ri; rj 2 R, is the set of pages in P making observable their di erences: DP (ri; rj) = fp 2 P : ri(p)  = rj(p)g, i.e., the subset of pages in P on which ri and rj extract di erent values.
Two rules ri, rj extract from P the same vector of values, and hence are indistinguishable for our purposes, if and only if DP (ri; rj) =  .
We say that a subset I (cid:18) U is representative of U wrt a set of rules R if and only if:
 In other terms, I is representative of U wrt to R if all the di erences amongst the rules in R are also observable on I.
Example 4.
Consider again our running example in Figure 1 and suppose that I = fps; pwg, while U = fph; ps; pwg.
I does not represent U since DU (r2; r5) = fphg whereas DI (r2; r5) =  .
Given the set of input pages U , and the class of rules R, there exist many representative subsets, including U itself.
As discussed above, our goal is to (cid:12)nd a small sample set.
Finding the smallest one is an instance of the well-known Set Cover problem: a page di erentiates the set of rules that extract distinct values from it.4 Set covering is an NP-complete problem but actually we do not need to compute the optimal sample set: it su ces to estimate it by considering a small but not necessary minimal set of pages.
Listing 2 proposes PageSampler, a greedy sampling algorithm to extract a representative set of pages I wrt a class of rules R from a large set of input pages U in O(jUj(cid:1)jRLaj) time and O(jRLaj) space.
Listing 2 PageSampler: A greedy sampling strategy Input: a set of pages U ; Input: a class of rules R; Input: a set of initial annotations La; Output: a set I (cid:18) U that is representative of U ; wrt R
 2: let n = 0;




 end if 8: end for
 if (jRLa (I [ fpg)j > n) then I I [ fpg; n jRLa (I)j; PageSampler processes the whole set of pages U (lines 3-It maintains a set of pages I, initially empty, that is

 such that the union of the sets of rules di erentiated from them equals the set of rules di erentiated directly by U .
Site www.imdb.com Actor www.imdb.com Movie www.allmusic.com Band www.allmusic.com Album www.nasdaq.com Stock quote jUj 5 (cid:1) 105 5 (cid:1) 105 5 (cid:1) 105 5 (cid:1) 105 7 (cid:1) 103 jICj




 Strategy Random Greedy Lucky Entropy

 SRM o  SRM on







 %MQ Precision Saved SRM on







 Recall SRM on



 Table 2: Dataset 1 Table 3: Total number of MQ for Dataset 1 representative wrt the subset of pages already processed.
It selects as representative only those pages that increase the number of di erent vectors extracted by the set of admissible rules RLa (line 4).
The pages selected according to this criterion make observable new di erences between at least two rules that were otherwise indistinguishable in the subset of pages processed until the previous iteration.
Example 5.
Consider the running example and suppose that PageSampler has already processed ps and pw, producing I = fps; pwg.
Let RL1 = fr2; r5; r6g be the set of
 admissible rules wrt to L1 = v+ in I do not di erentiate r5 from r2 and r6: r2(I) = r6(I) = r5(I).
However, when processing the next page ph, Page-Sampler detects the di erent behavior of r5 wrt other rules: r2(ph) = r6(ph)  = r5(ph), and then adds it to I.
To clarify how PageSampler is related to the disagreement sets, consider that if jRLa (I [ fpg)j > jRLa (I)j it follows that there exist at least two rules ri; rj 2 RLa such that ri(p)  = rj(p) and DI[fpg (ri; rj) n DI (ri; rj) = fpg.
Conversely, if jRLa (I [ fpg)j = jRLa (I)j then it follows that (ri; rj) n DI (ri; rj) =  ;8ri; rj 2 RLa .
Therefore, DI[fpg PageSampler maintains the representativeness of I for the subset of U already processed by adding a page p to I if and only if p changes the disagreement sets of the rules.
In this section we describe the experiments conducted to evaluate our approach.
Section 6.1 presents the results of the learning algorithm alf.
Our experiments mainly concentrate on the impact of the SRM technique, which dynamically expands the class of the extraction rules: the results show that, thanks to SRM, alf always reduces the number of membership queries, without penalizing precision and recall of the generated rules.
Section 6.2 illustrates the results of experiments to evaluate the e ectiveness of the sampling strategy implemented by the PageSampler algorithm.
Our experimental results show that a few dozens of pages selected by PageSampler are su cient to represent large collections of 105 pages from real-life websites.
We considered two distinct datasets to evaluate the learning algorithm alf.
The (cid:12)rst dataset has been obtained by downloading pages from large websites related to speci(cid:12)c domain entities, as shown in Table 2.
We wrote ad-hoc crawling programs, and let them collect around 5 (cid:1) 105 pages for each entity from www.imdb.com and www.allmusic.com, and all the available pages about stock quotes from www.nasdaq.com (around 7 (cid:1)
 total of 40 attributes.
We manually crafted a golden XPath jrg (U )\r(U )j
 jrg (U )\r(U )j jrg (U )j rule for every attribute to extract its values.
The (non-null) values extracted by the golden rules over the whole sets of pages were then used to compute and evaluate the precision and recall of the best rule inferred by our learning algorithm alf.
For each rule r generated by our algorithm wrt a golden rule rg, we used the standard metrics of precision (P ), and jr(U )j recall (R), as follows: P = .
We run the PageSampler algorithm over these sample sets of pages to derive a representative sample for every domain (the sizes of the input sets, jUj, and of representative samples, jICj, are shown in Table 2).
Over these sets we run the alf algorithm to infer the extraction rules or the target attributes.
In this experiment we set the probability threshold that governs the halt condition to 0:9, and the maximum expressiveness to R5.
We were mainly interested to evaluate the impact of the SRM technique used by alf and the di erent strategies to choose the next membership query.
Table 3 summarizes the results of the experiment.
We report the number of membership queries (#MQ) with and without the SRM technique for all the chooseQuestion() strategies.
Observe that SRM almost halves #MQ.
The most e cient strategy is Entropy, which signi(cid:12)cantly outperforms the baseline, represented by Random, and Greedy.
However, there is a small price to pay: without SRM, a perfect rule is found (precision and recall equal 1.0 not shown in the Table 3), whereas SRM introduces a loss lower than 1%.
This is due to early decisions made by the SRM technique: sometimes it decides to bet on the current, and imprecise, set of candidate rules rather than expanding the class and searching inside larger classes.
Another experiment aimed at considering the behavior of alf by using di erent chooseQuestion() strategies, wrt the size of the hypothesis space, which in our context corresponds to the number of admissible vectors after the initial annotations, i.e., jRLa (I)j.
Intuitively, the size of the hypothesis space is a measure of the cost that any learning algorithm needs to pay to infer a rule.5 The plots in Figure 2 show the average number of membership queries vs size of the hypothesis space.
Observe that the SRM technique (plot on the bottom) always reduces the number of queries needed by Entropy wrt the case in which it uses a (cid:12)xed expressiveness.
Also, note that when jRLa (I)j is low, the di erences in terms of #MQ are not apparent.
On the contrary, when jRLa (I)j   5, Random performs worse than other strategies.
Entropy and Lucky outperform the other approaches and, as expected from an active learning algorithm [13], #MQ follows a logarithmic trend with respect to the size of the hypothesis space.
used by the machine learning community, as the amount of training data to learn a concept [3].
Movies Actors Stocks Albums Bands Sampling Crawler IB Random IR Representative IC Crawler IB Random IR Representative IC Crawler IB Random IR Representative IC Crawler IB Random IR Representative IC Crawler IB Random IR Representative IC jIj














































 Table 5: Precision and recall with di erent sampling strategies
 We now discuss the experiments to evaluate the sampling algorithm PageSampler.
For this evaluation we used the pages of the (cid:12)rst dataset (Table 2).
We collected three sample sets I according to di erent strategies, as follows: (cid:15) IB represents a \biased sample": many large websites propose navigation paths to facilitate the browsing towards lists of relevant objects (e.g.
famous actors, top-stocks, etc.).
In our experiments, for each entity we downloaded the pages from the (cid:12)rst list proposed by the sites.
Therefore the size jIBj corresponds to the dimension of the proposed list.
(cid:15) IR is a set of pages randomly selected from the whole set U of pages with jIRj equals jIBj.
(cid:15) IC is the representative sample set as computed by our sampling algorithm starting from the pages collected in our data set.
jICj is determined by the algorithm.
The (cid:12)rst strategy does not pick up pages from the whole set of input pages U , while the second one chooses the sample pages in an uninformed way.
These sampling strategies are used by many wrapper inference approaches more focused on the inference phase rather than on the sampling.
To evaluate the role of the three sampling strategies, Table 5 reports the average precision and recall computed over the attributes of all the entities of each domain.
We inferred the extraction rules by running alf (without SRM) on the three samples obtained.
We obtained perfect rules when the inference was performed on the representative sample IC ; conversely, both the random set IR and the biased set IB loose precision and recall for a majority of cases, with a signi(cid:12)cant lost of recall with IB for bands and movies.
Table 5 also reports the size of the samples: it is worth observing that the representative sample set IC is always much smaller than the random sample set IR.
This is an important point as it a ects the running times of the learning algorithm, which performs better when working on small samples.
Figure 3 illustrates this issue: the graphic plots the average wrapper learning times (in logarithmic scale) vs the size of the random sample jIRj (number of pages).
The curve associated to IR describes the learning times to compute the wrapper using a random sample of increasing size.
As an example, for a random sample of 50 pages, it runs in about 15 Figure 2: #MQ vs size of the hypothesis space with SRM disabled (top) and enabled (bottom) Strategy Random Greedy Lucky Entropy

 SRM o  SRM on








 Saved



 Table 4: Total number of MQ for Dataset 2 It is interesting to observe that Greedy with SRM exhibits very good performances with attributes with a large hypothesis space, while it performs like Random with SRM disabled.
The explanation is that Greedy concentrates the queries on the most likely hypothesis by building the whole version-space in order to (cid:12)nd the shortest con(cid:12)rming t.s.
However, our probabilistic model equally redistributes the uniform prior p.d.f.
among all the admissible rules, and Greedy ends up choosing, randomly, among a set of rules of the same probability.
SRM solves this issue, as it does with Random, by concentrating the random queries around the most likely class of rules.
To evaluate SRM in a di erent setting we used another dataset composed by a large number of attributes (250) from 100 websites, including popular ones, such as amazon.com, youtube.com, and ebay.com.
Also for the attributes of this dataset, we manually wrote the golden rules.
However, for each website we downloaded a small number of pages (a few dozens).
As a consequence, we obtained a dataset with a large number of attributes, but with a limited hypothesis space.
Even in this dataset, the application of SRM produces signi(cid:12)cant improvements, as reported in Table 4 (we do not report precision and recall, since we did not register any loss in this experiment).
Once set, the set of candidate rules cannot be changed without seriously revisiting the inference algorithm.
Therefore they usually oversize the expressiveness of the formal language used to specify the extraction rules and additional samples are required only to compensate with the excess of expressiveness.
In this paper we concentrate on training data provided by means of a human intervention.
A di erent solution to exploit supervised approaches without any human intervention consists of relying on existing repositories to automatically annotate web pages [8].
Unfortunately, in many domains suitable data does not exist at all (consider pages that publish subjective values, such as customer ratings, or real time data, such as stock quote prices).
Also, the existing repositories might be biased over speci(cid:12)c instances (typically, the most popular): such a biased information will annotate just a subset of the target pages, possibly preventing the generation of a valid wrapper.
Active learning approaches for wrapper induction have been proposed in [9, 12].
However, also in these works the expressiveness is statically de(cid:12)ned.
The latter approach requires complex user interaction, since the user has to choose the correct wrapper within a set of ranked proposals.
A few recent proposals try to scale the wrapper inference to the web scale [8, 10].
In [8] the authors leverage an available dataset, but they ignore the presence of biased samples (as suggested by its running example based on popular objects itself), while in [10] it is needed domain knowledge that only an human expert can provide.
Our work is mainly motivated by the success of crowd sourcing platforms, which can be used to scale wrapper generation.
We propose a framework that allows supervised inference with simple mermbership queries suitable for the non-expert workers of a crowd platform.
An original algorithm, alf, applies active learning techniques to infer a wrapper, while minimizing the number of queries.
alf dynamically sets the expressiveness of the wrapper formalism, leading to a signi(cid:12)cant reduction of the number of queries needed to infer a wrapper.
It does not depend on the speci(cid:12)c classes of rules presented in the paper, and can be instantiated with other formalisms.
We developed a complimentary sampling algorithm, PageSampler, to select for the learning phase a small yet representative set of sample pages from a much larger set of pages to wrap.
Experimental results prove the e ectiveness of the approach.
The dynamic expansion of the expressiveness of the wrapper formalism reduces the number of queries to learn a wrapper, with tangible cost savings.
The sampling strategy leads to the selection of a small number of samples that e ectively represents a much larger set of pages.
The quality model and the cost model proposed in our framework are the basis for future developments.
For instance, the cost in term of total dollars spent for inferring a wrapper of the desidered quality can take into account both the cost of a PageSampler execution over large collection of pages on a cloud platform (such as EC2) and the number of MQ required by alf on a crowd platform.
We are studying strategies to further optimize the interactions with the crowd.
Namely, we are studying extensions of our bayesian model in order to manage worker s mistakes.
Figure 3: Wrapper learning times vs sample size secs; for a random sample of 450 pages, it runs in 100 secs.
The curve associated to the representative sample IC reports the learning times to infer the wrapper over a representative sample IC whose pages have been selected from a random sample IR with that number of pages.
For example, from a random sample of jIRj = 450 pages, PageSampler selected a representative sample composed of jICj = 25 pages, and on this sample alf inferred the wrapper in about 7 secs.
Computing the representative sample has its own costs.
However, as we can observe from the curves on Figure 3, even counting these costs the overall computation cost (sampling IR to compute IC + learning on IC ) is lower than the time required by learning without sampling (learning on IR).
In machine learning, the number of labeled samples needed by a supervised learning algorithm to infer a good hypothesis is called sample complexity [3], and has been studied from several perspectives.
For instance, similarly to our setting, [1] discusses the problem of exactly inferring a concept, i.e., a set of elements, by means of membership queries, i.e., question of the type \is this an element of the target concept?".
However, the main idea underlying our approach has been proposed by the statistical learning community [15], in which a loss function is given in order to characterize the quality of the produced hypothesis.
The structural risk minimization (SRM) [14], i.e., the decomposition of the set of hypotheses into a hierarchy of sub-classes, aims at avoiding the over(cid:12)tting problem: since the class of hypotheses studied by this community might be so expressive to be able to arbitrarily reduce the loss, a trade-o  with other quality criteria is needed to avoid that the learning algorithm selects the hypothesis perfectly describing the training data, rather than their underlying patterns.
Many researchers have proposed several variations of the learning paradigm to make it practically feasible in di erent applicative contexts: the learning approaches in which the inference algorithm is free to choose which sample to label next are usually de(cid:12)ned active [13].
These have recently gained interest, since, as clari(cid:12)ed in [3], they might produce exponential improvements over the number of samples wrt traditional supervised approaches.
