Ever since Amazon launched its Mechanical Turk in 2005, crowd-sourcing and human computing have become part and parcel of the World-Wide Web experience (en.wikipedia.org/wiki/ Crowdsourcing).
The topic frequently hits popular media, ranging from plaudits1 to all-round skepticism2.
Crowdsourcing has also attracted the attention of the research community at large, as evinced by the number of workshops and tutorials in many recent conferences dedicated to this topic: WWW3, WSDM4, SIGIR5,
 1sfgate.com/business/prweb/article/ Crowdsourced-mobile-fraud-intervention -solution-4009930.php 2www.technologyreview.com/view/416966/ how-mechanical-turk-is-broken/ 3crowdsearch.como.polimi.it/ 4ir.ischool.utexas.edu/csdm2011/ 5ir.ischool.utexas.edu/cse2010/ 6crowdresearch.org/chi2011-workshop/ 7www.humancomputation.com Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
As its name suggests, crowdsourcing taps into the wisdom of crowds.
In its most basic version, it involves posing a presumably hard question to a set of users and aggregating their individual responses in order to deduce the answer to the question.
This simple paradigm is useful in two scenarios where human labeling offers some version of the ground truth.
First, it can be used to generate large quantities of labeled examples for algorithms that are based on machine learning.
Second, it can be used for large-scale human evaluation and comparison of different algorithms for a problem.
Even this simplest version of crowdsourcing already poses an interesting research challenge: how to aggregate the responses of the users in order to obtain the true answer to each question?
Meticulous users can be more accurate than the others in answering the questions, whereas unreliable/lazy (or spammy) users can provide random (or even adversarial) answers.
To further complicate the problem, in many such systems, the reliability of a user may not be known a priori; indeed, a large fraction of the users may even be new recruits.
These issues entail a holistic approach to the problem: rather than aggregate the answers for each question in isolation, it becomes necessary to look at the global matrix of user provided answers to all the questions in order to simultaneously elicit both the user reliabilities and the true answers.
There have been several approaches [5, 10, 19, 2, 14, 3, 15, 11] to formalizing this problem.
These approaches posit a set of items with binary qualities, and a set of users indicating the qualities of items.
Not all users necessarily rate all items.
A bipartite graph G between items and users captures the set of items rated by each user.
Typically, a simple model is assumed for users: each user is associated with a reliability measure, which is used to independently  corrupt  her perception of the true quality of the item.
Given a set of user ratings, the problem is to collectively determine the reliability of each user and the true quality of each item.
These approaches fall into two broad categories: machine-learning based and linear-algebraic based.
The machine-learning approaches are based on variants of EM, which work on any graph G, but offer no guarantees as to how well they perform (see Section 2).
Algebraic approaches, on the other hand, can provide theoretical guarantees on the error in estimating item qualities, but so far have been limited to either complete assignment graphs (when each user rates all items) or to random graphs (when the assignment of users to items is random).
One of the  rst algebraic approaches was proposed by Ghosh et al. [5], who present an algorithm with the following guarantee: for a random user item assignment graph with n users, where in expectation each user rates D items and each item receives   ratings, the fraction of incorrectly estimated items D3 ).
This bound is vacuous for sparse graphs where each user rates o(n1/3) items.
Karger et al. [10] show that bounded by O((cid:112) n 285for random graphs, in the limit when number of items is going to in nity, the error in item qualities can be asymptotically bounded by e O( ), where   is again the expected number of users rating an item.
Thus their bound is stronger than [5] and holds for sparse random graphs as well, but only asymptotically.
Our work is motivated by the fact that the user item graph G is in practice neither random nor regular.
Often, users determine both the number as well as the set of items they want to rate.
The former is a function of their motivation level while the latter is determined by their expertise and familiarity with the items.
Under such circumstances, it is not obvious how the techniques developed in [5] and [10] generalize e.g., [5] depends crucially on the fact that the  expected  item item agreement matrix is low-rank and hence recoverable under random perturbations, which the assumed generative mechanism posits as the model for user mistakes.
Similarly, the performance of [10] depends crucially on whether belief propagation converges in arbitrary graphs.
Thus it remains an open question to develop a strategy for aggregating user ratings when we do not have too much control in deciding which users choose what set of items to rate whether there are characteristics of the user item rating graph that make it amenable to good aggregation.
Main results.
Our main contribution is an eigenvector-based technique to estimate both the user reliabilities and the item qualities that works for arbitrary user item assignment graphs G. We bound the error rate as a function of the expansion gap, i.e., the gap between the  rst and second eigenvalues of the graph GtG.
The essence of our technique is to look at the user user agreement matrices measuring agreement between pairs of users that are normalized by the number of items they decided to co-rate, and to then extract its topmost eigenvector.
A key element of our approach is to show a concentration result for structured random matrices, using the matrix version of McDiarmid s inequality.
We then present two algorithms that are based on matrix completion; for each of the algorithms, we prove that the estimated user reliabilities are close to the truth if the graph GtG has some expansion properties.
If the assignment graph is random, our estimate for user reliabilities translates into an approximation for the item qualities as well.
In particular, for a (D,  )regular graph with a large eigengap, our bounds translate into a user reliability estimation error of  O( 1    ).
On the other hand, even if we knew the true answer to each
 of the D questions that a user responds to, the estimated user reliability would still have a variance of 1/D, resulting in an estimation error of  (1/D) in the user reliabilities.
Our error bound of O(D 1/2) is not too far off from this lower bound.
For (D,  )random assignment graph our algorithm makes mistakes on only e O( ) fraction of the items.
Our bound generalizes both the results of [5] and [10], since this result holds for sparse graphs (unlike [5]) without requiring the asymptotic argument of the number of items going to in nity (unlike [10]).
Finally we also demonstrate our algorithms on real world datasets and show how they improve upon the state of the art in terms of accuracy of estimates in both item qualities and user reliabilities.
Crowdsourcing, using the global marketplace to perform micro-tasks in a scalable way, is a topic that has generated much excitement [8, 12] labeling and rating items consists of a large fraction of such tasks.
A key problem in here is to decide how to aggregate the labels from multiple labelers of varying reliabilities such that the effect of the underlying noise is mitigated.
The extensive empirical work by Sheng et al. [15] shows that getting more noisy labels per item and then aggregating them is more accurate than getting more expensive, and hence allegedly more  accurate", labels; their work uses only majority voting to aggregate labels from multiple users, and is primarily concerned with identifying the items that will bene t from more labels.
Dekel et al. [4] show that such aggregation can be improved if the bad raters are pruned.
A more general analysis of the user reliabilities was done by Dawid et al. [3], who are the  rst to model the obfuscation of labels by judges, and use the EM algorithm in order to derive the true labels.
Unfortunately, the EM technique suffers from lack of theoretical guarantees and has issues regarding convergence and initialization.
Since then, there has been a host of followup work modifying this approach using a Bayesian technique [2, 11], studying it in the context of learning a speci c classi er [14], and modifying it by  nding out spammers, i.e., labelers deliberately giving incorrect responses [13].
Other related results in applying machine learning techniques to cleaning user labels include [19, 16, 14, 18].
Much of the above work does not come with theoretical guarantees on the inferred user reliabilities or the item labels.
Both Ghosh et al. [5] and Karger et al. [10] study this problem independently in the same generative setting, where each user rates a random set of items, and has an inherent probability of identifying the correct label, or  ipping it.
Our model is essentially a generalization of their setting to arbitrary user item assignment graphs.
Ghosh et al. [5] present a spectral algorithm that provably learns the true item qualities, with bounded error.
However, as pointed out, these bounds are useful only when each user performs a large number of ratings.
Karger et al. [10] uses belief propagation8 to derive both a set of user reliabilities and an estimate for item qualities for a sparse random graph.
Their convergence analysis uses techniques from density evolution and hence critically depends on the fact the graph is both sparse and random.
Liu et al. [11] extend the BP algorithm of [10] via a Bayesian approach by choosing a suitable prior for item qualities and user reliabilities, and uses clever techniques to make the message passing more ef cient.
An orthogonal question to ours, and one that has received much attention, is how to design incentives such that each user performs to the best of his abilities and provides truthful ratings [6, 9].
+

 Let m be the number of items and n the number of users.
Let qi   { 1, 1} denote the quality of the ith item.
Let q denote the column vector of length m with qi as the ith entry.
Each user rates a subset of items.
Let G   {0, 1}m n denote the item user assignment matrix, i.e., Gij = 1 if item i is rated by user j.
The ratings given by n users on m items is represented by a stochastic matrix U generated by the following random process (similar to [5]).
Each user j is associated with a probability pj   [0, 1] that captures how correct is her rating.
Independently, for each item i she rates (as dictated by G), she tosses a coin with bias pj: with probability pj, she rates item i (correctly) as qi and with probability 1   pj, she rates item i (incorrectly) as  qi.
Thus, the random matrix U   { 1, 0, 1}m n can be described as  qi  qi
 Uij = if Gij = 1, w.p.
pj, if Gij = 1, w.p.
1   pj, if Gij = 0.
(1)
 and like any standard BP algorithm, excludes the message from the node when computing the outgoing message to that node if this message is included, then the algorithm reduces to that of [5].
are in the range [ 1, 1], where a reliability of 1 indicates a user who always answers correctly, a reliability of  1 indicates one who always answers incorrectly, and a reliability of 0 indicates one who answers uniformly at random.
Let w   (cid:60)n denote the vector of user reliabilities.
The algorithm is given as input a realization of the stochastic rating matrix U, assumed to be generated from the set of latent parameters q and w, which are unknown.
The aim is to estimate both the user reliabilities and the item qualities simultaneously, i.e., an estimate  w   [ 1, 1]n for the user reliabilities and an estimate  q   { 1, 1}m for the item qualities.
The performance of the algorithm is measured by the distance to the underlying reliability vector and the quality vector.
The errors for the estimates  w and  q provided by the algorithm will thus be de ned 2) and by the following quantities: error(  w) = 1 m 1[ qi (cid:54)= qi].
E(|| q   q||2 n error( q) = 1 m

 E(||  w   w||2
 We review some de nitions from linear algebra before presenting our algorithms.
Throughout the paper, we represent (column) vectors using lowercase letters (a, b, w, .
.
.)
and matrices by uppercase letters (M, N, .
.
.
).
Let x   y denote the innerproduct of x and y and let xt denote the transpose of x.
For a matrix M, the spectral and Frobenius norms are denoted by (cid:107)M(cid:107) = (cid:107)M(cid:107)2 = max(cid:107)x(cid:107)2=1 (cid:107)M x(cid:107)2 and ij)1/2 respectively.
For two matrices M and N of matching dimensions, we de ne the following Hadamard products: (cid:107)M(cid:107)F = ((cid:80) ij M 2 (cid:40) (M   N )ij = MijNij; (M (cid:11) N )ij = Mij/Nij
 if Nij (cid:54)= 0, otherwise.
For any matrix M   (cid:60)m n, we de ne the indicator matrix I(M )   {0, 1}m n such that I(M )ij = 1[Mij (cid:54)= 0], i.e., I(M )ij is 1 if and only if the corresponding entry of M is nonzero.
We will also denote the (scaled) top eigenvector of a matrix M as v1(M ) = arg minx (cid:107)M   xxt(cid:107)2, x(1)   0.
We use the convention that indices i always denote items and indices j, k, .
.
.
denote users.
Let  i denote the number of ratings that item i gets, i.e., the number of nonzero entries in row i in G.
Similarly, let dj denote the number of ratings that user j supplies.
De ne D = maxj dj and   = maxi  i.
Recall that we are only presented with a realization of the ratings matrix U (and hence its indicator, the assignment matrix G as well) and we need to estimate both the item qualities and the user relia-bilities.
Before describing the algorithms, we present the intuition behind them.
The main idea is to work with the two user user matrices U tU and GtG.
The entry (GtG)jk is the number of common items rated by users j and k. The entry (U tU )jk is the difference between the number of agreements and disagreements of the users j and k. Let E denote the matrix which contains itemwise expected values of the random matrix U, i.e., Eij = E[Uij] = (pjqi + (1   pj)( qi))Gij = qiGijwj.
j Uij  wj).
Algorithm 1: Ratio of eigenvectors.
It is easy to see that EtE = (GtG)   (wwt), EtE (cid:11) GtG = I(GtG)   wwt.
(2) Suppose we knew the expected matrix E. We could then estimate the user reliabilities by solving the following problem (cid:107)A   B   (wwt)(cid:107)F , F (A, B) = arg min w s.t.
 j, w2 j   1.
(3) with (A, B) as either (EtE, GtG) or (EtE (cid:11) GtG, I(GtG)).
It is easy to see why this approach works if the graph G is complete: the expected matrix E = qwt and solving (3) would give us back the user reliabilities w exactly.
This approach, however, has a few problems when the graph G is arbitrary.
First, the above program is computationally intractable for arbitrary G (e.g., [7]).
But more importantly, we show that for arbitrary assignment graphs the matrix EtE might not be informative, as shown in the following example.
Suppose there were two disjoint user groups A and B, and a user x (cid:54)  A   B.
All users in A have reliability 1, those in B have reliability  1, and user x has reliability 0.
The items have two disjoint groups S and T of size m/2.
All users in A rate all items in S, all users in B rate all in T , and user x rates all items in S   T .
It is clear that by looking only at the matrix EtE, it is not possible to distinguish the highly reliable users from the non-reliable ones.
It is easy to extend this construction to k + 2 user partitions such that we cannot distinguish the high and low pro ciency users even if we are explicitly given, in addition to EtE, the names of k users who answer all the questions.
Thus, we want to characterize the class of graphs G that allows us to recover w with small errors 9.
One of our main contributions is to identify the expansion of the graph G as a suf cient property that enables us to estimate w both ef ciently and with low errors  the resulting algorithms are presented in Algorithm 1 and 2.
Since the matrix E is not observable, we instead work with the matrix U. Algorithm 1 is inspired by the observation that when GtG has rank one, (3) has an exact solution  w where  w   v1(GtG) = v1(EtE) and hence  w = v1(EtE) (cid:11) v1(GtG).
We will show that when the graph G has suf ciently high expansion, this solution, even when using U tU in place of EtE, is a reasonable approximation.
Algorithm 2 is inspired by (2) and uses the same intuition that (3) is approx-imable when I(GtG) is close to a rank one matrix.
Hence, in this case, we  rst compute the rank one approximation v1(I(GtG)) and then use it to compute the  nal estimate  w.
We next show an error bound on the estimate  w for user reliability obtained from Algorithm 1.
(Similar bounds can be shown for Algorithm 2, which we defer to the full version.)
Our error bound holds for arbitrary graphs having expansion properties.
However,
 for EEt) [5]; by augmenting the above construction it is possible to show that such approaches will also incur a constant fraction error for arbitrary assignment graphs.
j Uij  wj).
Algorithm 2: Eigenvectors of ratio.
in order to illustrate our bounds, we state the results for (D,  )regular graphs.
The more general result is stated and proved in denote the aver-Section 5 (See Theorem 5.12).
Let  w = age reliability of users.
i w2 i n (cid:113)(cid:80)

 (USER ERROR BOUND).
Let ,   < 1 be a  xed positive constants.
If G is a (D,  )regular graph such that   >
 and the second eigenvalue of GtG, denoted by  2, satis es the condition  2 <   w2D  , then with probability 1    , Algorithm 1 returns an estimate  w, such that 2  w2
 (cid:18) 1   (cid:19) .
+

 error(  w) =  O When the item user assignment graph is random, this error bound translates into a bound for error in item estimates.
The question of whether such a bound holds for  xed graphs, under some assumptions, remains open.
THEOREM 4.2.
Let G be a random (D,  )regular graph.
With high probability, Algorithm 1 returns estimates  q, such that (cid:32) (cid:32) (cid:18) error( q)   exp
    w2   1     1 
 (cid:19)2(cid:33)(cid:33) .
When the average reliability  w is some constant bounded away from 0 (i.e., users are good on average), then error( q) scales as exp( O( )).
This matches the bound in [10].
However, the bound in [10] requires that the limit of number of items goes to in nity, an assumption we no longer require.
So far we have considered the case when G has a large expansion gap, i.e., when the second eigenvalue is much smaller than the  rst.
We propose a heuristic, without any theoretical guarantees, that improves the performance of both Algorithms 1 and 2 for low expansion graphs.
This heuristic is based on the standard alternating projections technique [1] for solving the weighted low rank approximation problem.
Recall that we are trying to  nd a user reliability vector w as a solution to the problem F (A, B) = arg minw (cid:107)A B (wwt)(cid:107)F .
When B and I(B) satisfy expansion properties, Algorithm 1 and 2 both give good approximations to this problem.
Consider a slight generalization of this problem that instead  nds two vectors u and v to minimize arg minu,v (cid:107)A   B   (uvt)(cid:107)F .
When one of the vectors, say u, is known, the other can be computed by solving a simple least squares problem.
Thus, this gives an EM-style alternating projections algorithm to iteratively compute u and v. On convergence, we are guaranteed to achieve a local optimum, which for symmetric matrices A and B implies that u = v. This common converged value can thus be used instead of w.
One problem with this approach is that since the original problem is not convex, the convergence can happen at a local minima.
Thus, both the rate of convergence and the quality of converged solution depends on the initialization for u and v. In practice we observed that when u = v =  w, where  w is the estimate obtained by either Algorithm 1 or 2, then both rate of convergence and quality of converged solution is good.
Intuitively, this is because Algorithm 1 and 2 already try to minimize the objective function (at least in the case of graphs with good expansions) and hence provide a very good seed for the alternating projections heuristic.
In this section we prove guarantees on the performance of our algorithms both in terms of the error incurred in estimating user reliabilities as well as for item qualities.
The underlying intuition behind the proof is as follows.
First we show that the response matrix U tU is close to the expectation matrix EtE.
In order to prove this concentration bound, we need to use machinery aimed towards giving Chernoff-like tail bounds for sums of random matrices.
We then use the expansion (and corresponding eigenvalue gap) of the user user co-rating graph GtG to show that the gap between the  rst and second eigenvalues of GtG translates to a corresponding gap between the  rst and second eigenvalue of EtE as well.
Using this, we then characterize the top eigenvector of EtE in terms of the top eigenvector of GtG and the reliability vector w; the error in this characterization depends, among other quantities, on the ratio between the top two eigenvalues of the graph GtG.
This enables us to use the eigenvalues of GtG and U tU to create  w, an estimate of w. After creating an estimate  w of the user reliabilities, we can then use it to create an estimate of the item qualities  q the error in  q will depend on the error in  w.
We start with a statement of the matrix McDiarmid inequality that we will use as a tool.
The underlying intuition behind this concentration result from [17] is that a random matrix is close to its expectation in terms of the spectral norm, if it can be expressed as the output of a function having bounded sensitivity over its input variables.
Note that A (cid:22) B denotes the usual semide nite ordering, i.e., B   A is semide nite.
(MATRIX BOUNDED DIFFERENCE [17]).
Let {Zk}n k=1 be an independent family of random variables, and let H be a function that maps n variables to a self-adjoint matrix of dimension d. Consider a sequence {Ak} of  xed self-adjoint matrices that satis es (H(z1, .
.
.
, zk, .
.
.
, zn)   H(z1, .
.
.
, z i. Compute the variance parameter   = (cid:107)(cid:80) where zi and z(cid:48) random vector z = (Z1, .
.
.
, Zn).
Then, for all t   0,  t2/8 2 i range over all possible values of Zi for each index
 k, .
.
.
, zn))2 (cid:22) Ak (cid:48) Pr[(cid:107)H(z)   E[H(z)](cid:107) > t]   d   e k Ak
 .
i=1 Gij 2 a user j, denote  j = (cid:80)m We will use Theorem 5.1 to show that the user user agreement matrix U tU is close to its expectation EtE in the following sense.
For i , i.e.,  j is the sum of squared degrees of items that j responds to, and denote   = maxn j=1  j.
We  rst de ne the function H( ).
Lemma 5.2 then characterizes the structure of the difference matrices when any of the random variables is perturbed.
Using this structural characterization Lemma 5.3 shows that function H( ) satis es the sensitivity conditions of Theorem 5.1, and Lemma 5.4 shows the  nal bound that we get using the sensitivity condition derived in Lemma 5.3.
We abuse notation and de ne the sequence of random variables U = {U11, .
.
.
, U1n, U21, .
.
.
, U2n, .
.
.
, Um1, .
.
.
, Umn}.
The function H( ) is then de ned as H(U ) = U tU, which is a self-adjoint matrix in (cid:60)n n.
We also de ne the sequence of self-adjoint matrices {Aij   (cid:60)n n, i   [m], j   [n]} where each Aij is a all k   [n].
Lastly, we de ne column vectors ej and rij of length n as following: ej is the unit vector with 1 as the jth element, and rij[k] =  2UijUik if k (cid:54)= j, and 0 otherwise.
The following Lemma shows the structure of the sensitivity matrices.
LEMMA 5.2.
For any response matrix U, denote  ij = H(U )  H(U(cid:48)), where U(cid:48) is the response matrix identical to U in all entries ij =  Uij and U(cid:48) except with the (i, j)th entry switched, i.e., U(cid:48) kl = ij + 4( i   1)Gijejet Ukl for (k, l) (cid:54)= (i, j).
Then  2 j.
ij = rijrt PROOF.
Recall that H(U ) = U tU is an n   n matrix with the (j, j)th diagonal entry dj, where dj is the number of items rated by user j.
Also H(U )kl = akl   bkl where (bkl) akl denotes the number of (dis) agreements between users k and l in rating the items that they have in common.
Now since  ij = H(U )   H(U(cid:48)), where U(cid:48) differs from U only in the (i, j)th entry,  ij is again an n   n matrix with all but the jth row and column as 0.
To see why, consider (k, l)th entry of  ij such that k (cid:54)= j and l (cid:54)= j.
Both users k and l have same responses in both U and U(cid:48).
Thus the number of agreements and disagreements between k and l is same in U and U(cid:48).
Hence the (k, l)th entry of  ij is zero.
Since H(U ) = U tU and H(U(cid:48)) = U(cid:48)tU(cid:48) are symmetric matrices, so is their difference  ij.
Thus, the jth row and column for  ij are identical.
We will show that the column is precisely the ij).
Consider the kth element of this vector rij (and hence row is rt row.
If user k has rated item i, and k and j agree according to U, then they will disagree according to U(cid:48).
Similarly, if they disagree according to U, then they will agree according to U(cid:48).
Thus, kth element of rij, which is the difference in agreements and disagreements of users k and j will change by either 2 or  2.
These cases can be summarized succinctly as  2UijUik = rij[k], by de ni-tion.
Only exception is rij[j], which is always 0, since no user disagrees with himself on the same item i.
Thus the jth column of  ij is precisely rij.
The fact that  ij is the matrix with jth row and column equal to rij and rest elements as zero can be written as  ij = rijet j + ejrt ij, which yields that  2 ij = (rijet +(ejrt = rij(et +ej(rt j)(rijet ij)(rijet jrij)et ijrij)et = 0 + (1)rijrt j) + (rijet j) + (ejrt j)(ejrt ij) ij)(ejrt ij) jej)rt ij ijej)rt ij j + rij(et j + ej(rt ij + 0 + 4( i   1)Gijejet j.
jej is 1, and rt ijrij is 4Gij( i   1) (since(cid:80) Here, the last equation follows from using the following values of the four innerproducts highlighted in penultimate equation: rt ijej = jrij is 0 (since ej has only jth entry as nonzero, which is zero et in rij), et k rij[k]2 = k(cid:54)=j 4GijGik = 4( i   1)Gij).
The k(cid:54)=j( 2UikUij)2 = (cid:80) (cid:80) entry equals(cid:112)8GikGij( i   1).
Using the above lemma, we can Let Aij   (cid:60)n n be de ned as a diagonal matrix where the kth proof follows.
show that  2 ij is bounded by the matrix Aij.
ij (cid:22) A2 ij.
PROOF.
From Lemma 5.2,  2 ij (cid:22) A2 ij + 4( i   1)Gijejet j.
ij = rijrt Now if we show that rijrt ij/2, then the proof of lemma is complete, since trivially, 4( i   1)Gijejet j (cid:22) A2 ij (cid:22) A2 ij.
If k, l (cid:54)= j, then we have ij/2, consider the (k, l)th element, denoted To show rijrt by Rkl, of rijrt ij/2.
Rkl = ( 2UijUik)( 2UijUil) = 4GijUikUil, and hence |Rkl| = 4Gij|Uik||Uij| = 4GijGikGil.
If either k = j or l = j, then the (k, l)th element is 0.
Hence for the kth row, the sum of the absolute values of (k, l)th entries is |Rkl| =
 (cid:88) l (cid:88) l (cid:80)m since each user l (cid:54)= j who rated item i contributes exactly 1 to the sum.
Thus the diagonal matrix with 4GijGik( i   1) as the kth diag-ij/2 by de nition ij (cid:22) A2 onal entry, diagonally dominates rijrt is precisely such a diagonal matrix.
Hence rijrt ij.
Now A2 ij/2.
The next statement shows that U tU is close to the expectation matrix EtE.
Recall that   = maxj i .
i=1 Gij 2 LEMMA 5.4.
Suppose the matrix U is generated by the rating generation process described above.
Then, for every     (0, 1), (cid:104)(cid:107)U tU   E[U tU ](cid:107)2   8(cid:112)  log (n/ ) (cid:105)   1    .
Pr PROOF.
Using the statement of Lemma 5.2, we get that the sensitivity of H( ) with respect to each variable Uij is bounded by A2 ij.
Thus, from the statement of Theorem 5.1, the variance parameter   is given by (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88) i=1 n(cid:88) j=1  2 =
 ij (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) .
m(cid:88) n(cid:88) n(cid:88) m(cid:88) m(cid:88) i=1 ij is diagonal, so is this sum.
The kth diagonal entry ij is 8GikGij( i   1) and hence the kth diagonal entry of the Since each A2 of A2 sum is given by

 Gij j=1
 i=1 j=1 Gik 2 i = 8 k.
= m(cid:88) i=1 i=1 Hence the spectral norm, which is the largest diagonal entry for a diagonal matrix, is simply 8 maxk  k = 8  and hence  2 = 8 .
Using this value for  , setting d = n, and t2 = 8 2 log(n/ ) =
 Finally, this implies the following result.
LEMMA 5.5.
For a matrix U generated by the random rating generation process, with probability 1 , and E = E[U ], (cid:107)U tU  EtE(cid:107)   8(cid:112)  log (n/ ) + D, where D is the maximum number of ratings done by a person.
PROOF.
Assuming the result of Lemma 5.4 holds, we only need to bound the norm of EtE   E[U tU ].
This is a diagonal matrix, with the jth diagonal entry to be dj(1   w2 j ).
Hence, (cid:107)EtE   E[U tU ](cid:107)   maxj d2 j = D.
In this section we show that the estimators for user reliabilities and item qualities have a small error.
For notational simplicity, we assume that the event in Lemma 5.4 holds, i.e., the matrix U tU is close to its expectation.
We  rst show the proof for Algorithm 1, which takes the ratio of the top eigenvectors of U tU and GtG.
The proof strategy is to  rst show that under a suitable set of assumptions for G, the matrix EtE has a large gap between the  rst and second eigenvalues, and hence can be represented accurately using only the topmost eigenvector  this will ensure that the eigenvector-based Algorithm 1 has small error.
Let the  rst and second eigenvalues of GtG be denoted by  1 and  2 respectively, and the top two eigenvalues of EtE be denoted by  1 and  2.
Let g denote the  rst eigenvector of GtG, and e be that of EtE.
Let gmin denote the minimum entry of g; by Perron  j .
De ne Frobenius theorem, gmin > 0.
Recall  w2 = 1 n   = (cid:107)U tU   EtE(cid:107)2 and W   n   n to be the diagonal matrix with wj for the jth diagonal entry.
(cid:80) j w2 LEMMA 5.6.  1    1(cid:107)W g(cid:107)2    2.
PROOF.
Recall that EtE = (GtG)   (wwt).
Since  1 and g are the  rst eigenvalue and vector of GtG, we have that GtG =  1ggt +A, where A is the matrix de ned as the difference between GtG and  1ggt.
Thus, (cid:107)A(cid:107) =  2.
EtE = (GtG)   (wwt) = ( 1ggt + A)   (wwt) =  1(W g)(W g)t + A   (wwt).
Hence we can write using the triangle inequality: (4) (cid:107)EtE(cid:107)   (cid:107) 1(W g)(W g)t(cid:107)   (cid:107)A   (wwt)(cid:107)    1(cid:107)W g(cid:107)2   (cid:107)A(cid:107) =  1(cid:107)W g(cid:107)2    2, where we use (cid:107)A   (wwt)(cid:107) = (cid:107)W AW(cid:107)   (cid:107)W(cid:107)2(cid:107)A(cid:107)   (cid:107)A(cid:107).
This completes the proof.
LEMMA 5.7.
(etW g)2   (cid:107)W g(cid:107)2   2 2 PROOF.
From (4), we know that EtE =  1(W g)(W g)t + A  (wwt), where (cid:107)A(cid:107)    2.
Also eEtEet =  1 and  1 .
etEtEe = et( 1(W g)t(W g) + A   (wwt))e =  1(etW g)2 + et(A   (wwt))e    1(etW g)2 +  2, where the last inequality again follows from (cid:107)A   (wwt)(cid:107)   (cid:107)A(cid:107)(cid:107)W(cid:107)2    2 maxj w2 j    2.
Thus, we have  1 = etEtEe    1(etW g)2 +  2.
LEMMA 5.8.  2   3 2.
PROOF.
Let x be the second eigenvector of EtE.
Then xEtExt =  2.
Also x is perpendicular to the largest eigenvector e of EtE.
So, we know (etW g)2 + (xtW g)2   (cid:107)W g(cid:107)2.
From Lemma 5.7, we know (etW g)2   (cid:107)W g(cid:107)2   2 2/ 1.
Hence, (xtW g)2  
  2 = xtEtEx = xt 1(W g)(W g)t + A   (wwt) =  1(xtW g)2 + x(A   (wwt))x    1   2 2  1 +  2 = 3 2.
LEMMA 5.9.
Let   = (cid:107)U tU   EtE(cid:107)2 and   = (cid:107)W g(cid:107).
If  2+3   1 < 1 and 2 2  1  2 < 1, then (cid:13)(cid:13)(cid:13)(cid:13)u   W g   (cid:13)(cid:13)(cid:13)(cid:13)   (cid:114)
  2 + 3   1 + (cid:114) 4 2  1  2 .
PROOF.
Since u and e are the top eigenvector of U tU and EtE respectively, and   = (cid:107)U tU   EtE(cid:107), by applying a standard matrix perturbation bound [5, Lemma 3.2], (e   u)2   1    2 + 3  .
 1     )2   We write the bound derived in Lemma 5.7 as follows: (et W g  1  2 .
From the condition stated in the Lemma, since 2 2   1   2 2    
  1  2, and 1   2 2     4 2  1  2   1  2 2 and thus (cid:107)e  u(cid:107)2   (cid:113) Similarly, etu  (cid:113)  1  2 .
Hence (cid:107)e  W g 1    2+3    (cid:107)2 = 2 2 etW g   1   2+3   1  2  1  1 2  2+3   1 .
The proof follows from the triangle inequality.
LEMMA 5.10.
Denote   = (cid:107)W g(cid:107) and let  w be the vector with the ith element   ui/gi.
If  2+3  < 1 and 2 2 (cid:18)  1  2 < 1, then  2 + 3  4 2  1  2  1 +
     2 ng2 min  1 (cid:114) (cid:107)  w   w(cid:107)2 n (cid:13)(cid:13)(cid:13)(cid:13)u   W g   (cid:13)(cid:13)(cid:13)(cid:13)  
  2 + 3   1 + (cid:114) 4 2  1  2 .
PROOF.
From Lemma 5.9, we know that error(  w) = Hence (cid:107)  w   w(cid:107)2 = (cid:107)(  u   W g) (cid:11) g(cid:107)2     2(cid:107)u   W g/ (cid:107)2   Since (   y)2   2(x + y), we have g2 min x + (cid:107)  w   w(cid:107)2     2 g2 min
  2 + 3   1 + 4 2  1  2 (cid:19) , and hence the proof.
(cid:19) .
.
(cid:18) (cid:113)(cid:80)      w/r.
LEMMA 5.11.
Let  w = users; let   = (cid:107)W g(cid:107) and r = gmax/gmin.
Then, i w2 i n be the average reliability of PROOF.
This follows from considering the weighted graph cor-(cid:88) responding to GtG.
Then i   n  w2g2 i g2 w2 min    w2n(g2 max/r2)    w2/r2, i which completes the proof.
Combining the above lemmas, we get the  nal theorem about the error bounds.
THEOREM 5.12.
For a  xed assignment graph G and a rating matrix U that is generated by the random rating generating process, if the graph G satis es  1  w2  2 < (5) then with probability 1    , Algorithm 1 returns estimates  w, such that 4r   6(cid:112)  log (n/ )   D (cid:16)  2 + D + 5(cid:112)  log (n/ ) (cid:17) .
error(  w) <
  1ng2 min
 (cid:107)U tU   EtE(cid:107)2   8(cid:112)  log (n/ ) + D.
Assume that the above event holds.
Also, for Lemma 5.10, we need the following bounds:  2 + 3   1 < 1, 2 2  1  2 < 1.
(6) Using the bounds on  1,  2 and  , the above bounds are satis ed if   6(cid:112)  log (n/ )   D   6(cid:112)  log (n/ )   D.
 2 < <  1  2
  1  w2 4r (7) (cid:19) , (cid:18) Conditioned on this and Lemma 5.10, we have that error(  w) = (cid:107)  w   w(cid:107)2 n     2 ng2 min
  2 + 3   1 + 4 2  1  2 min where   = (cid:107)U tU  EtE(cid:107)2.
Plugging in this value, and the bounds on  2 and  1 from Lemma 5.6 and Lemma 5.8, we have that error(  w)     2 ng2 (cid:32)
 (cid:32)
 (cid:16)  2 + D + 5(cid:112)  log (n/ ) We simplify this by using  1  2    2    1  2/2 to give error(  w)     2 ng2   10  2 ng2 (cid:33) (cid:33)  1  2    2 4 2  1  2 4 2  1  2 (cid:17)  1  2  1  2 min + +
 .
.
min In order to illustrate our bounds better, we also state a corollary for (D,  )regular graphs.
This is also a restatement of Theorem 4.1 and thus completes its proof.
graph such that   > 1 eigenvalue  2 satis es the condition (THEOREM 4.1).
If G is a (D,  )regular and the second
 2  w2 The proof of this theorem is based on the following lemma.
LEMMA 5.15.
Denote   =   n (w    w).
Then (i)          (  w2   )/2 and (ii) if  w2 > , the probability that the ith item is wrong is at most e 2/16    e (  w2 )2/64.
PROOF.
For (i), note that  = error(w,  w) = (cid:107)w    w(cid:107)2/n = .
Thus w    w/n = (|w|2 + |  w|2   n)/2n   |w|2+|  w|2 2w   w (  w2   )/2, which yields the result.
For (ii), de ne zi =(cid:80) n j Uij  wj.
Then E[zi] = qiE[Gij]wj  wj = qi( /n)w    w = qi .
(cid:88) j Then from (i) and assuming  w2 > , we get   > 0.
Thus, sgn(E[zi]) is same as qi.
Thus sgn(zi) (cid:54)= qi implies that |zi   E[zi]| > E[zi].
Thus the probability that sgn(zi) (cid:54)= qi is at most Pr[|zi   E[zi]| > E[zi]].
ity.
De ne yij = Uij  wj.
Then zi = (cid:80) For computing this probability, we will use Bernstein s inequal-j yij.
Also E[yij] = qi( /n)wj  wj.
Denote xij = yij   E[yij].
Now we will apply Bernstein s inequality over xij for a  xed i but j from 1 to n. Note that  1   |E[yij]|   xij   1 + E[yij].
Thus, it is safe to say that  2   xij   2.
Also ij] = E[y2 ij]   E[yij]2 = ( /n)  w2 j (1   ( /n)w2 ij]   ( /n)  w2 j   ( /n)2w2 j )    /n.
Applying E[x2 Thus, E[x2 j  w2 j .
Pr xij (cid:35)   e Bernstein s inequality for t =  /2, we get (cid:80) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    /2 j xij =(cid:80) Now(cid:80) j yij E[(cid:80) Thus |(cid:80) j yij|   |qi | |(cid:80) j yij] =(cid:80)   e qi  j j e 2/16 , which yields the result.
 2 /8 ]+2( /2)(1/3) E[x2 ij  2/8  + /3   e  2/16 .
j yij E[zi] =(cid:80) j yij  j xij|      /2 with probability  2 <   w2D  ,
 then with probability 1    , Algorithm 1 returns estimate  w, such that (cid:32) (cid:112)log(n/ )   (cid:33)
 error(  w) = O  2
 +
   +


 Analysis for Algorithm 2.
The proof for Algorithm 2 follows a similar route.
We  rst show a similar matrix concentration inequality and then use it to follow the the proof outline in Section 5.
We postpone the details to the  nal version.
The proof is straightforward, after noting that gmin = 1  nD = m , and using a bound on  1   m 2 gree in GtG.
n and n , the average de-Asymptotically, this gives error(  w) =  O( 1 ).
Finally, we show that estimating the set of user reliabilities accurately enables us to estimate the quality of each item with small error.
We show that for a random (D,  )regular graph the total error in estimating item quality falls exponentially with the maximum item-degree, as well as with the average reliability.
This is also a restatement of Theorem 4.2 and thus completes its proof.
  + 1 

 (THEOREM 4.2).
Let G be a random (D,  )-.
Let  w be an estimate with regular graph.
Let  w = error(w,  w)   .
Then, error( q)   e (  w2 )2/64.
In particular, for  q obtained by Algorithm 1, error( q)   e  O( (  w2  1     1  i w2 i n
 )2).
(cid:113)(cid:80) In this section we experimentally analyze the accuracy of the proposed algorithms in estimating both item ratings and user relia-bilities.
We implemented both Algorithm 1 and 2, which we denote by ALGORITHM 1 and ALGORITHM 2 respectively.
We compare them with the following algorithms: the simple majority voting algorithm denoted by MAJORITY, the iterative EM algorithm denoted by EM, the spectral algorithm from Ghosh et al. [5] denoted by GKM, and the belief propagation algorithm from Karger et al. [10] denoted by KOS.
We also implement LOWERBOUND which uses ground truth to compute the user reliabilities, and then uses the re-liabilities to infer item ratings.
Since it uses ground truth, it is not a true algorithm, but provides a benchmark to compare the performance of other algorithms.
Our implementation of ALGORITHM 1 and ALGORITHM 2 include the alternating projections heuristic described in Section 4.3.
Datasets.
To illustrate the properties of our algorithms we use both synthetic and real datasets as described below.
(b) NLP Items (c) TREC Users (d) NLP Users Figure 1: Error analysis on real datasets: (a) and (b) measure error in item ratings estimates as % of incorrect items, and (c) and (d) measure error in user reliabilities using correlation coef cient.
Lower % means better item estimates, while higher correlation coef cient means better user estimates.
In all cases, ALGORITHM 2 is either best or second best.
In terms of aggregate error, ALGORITHM 2 is best in both the item rating estimates and one user reliability estimate.
Name TREC.stage2 TREC.task1 TREC.task2 NLP.rte NLP.temp NLP.emotions m





 labels





 n





 responses





 Table 1: Statistics for the real datasets used in our experiments.
(1) TREC10: this dataset is a collection of topic-document pairs labeled as relevant or non-relevant by mechanical turks.
Several of the labels have ground truth assigned as well.
There are three distinct datasets corresponding to different competitions of the workshop: namely, TREC.stage2, TREC.task1, and TREC.task2.
The number of items, labeled items, users, and user responses for these datasets have been summarized in Table 1.
(2) NLP: this dataset [16] is a collection of three human judged (3) Synth: datasets, all having ground truth labels, as summarized in Table 1.
this is a synthetically generated dataset to help us analyze various algorithms in a controlled setting as a function of the numbers of responses by users and user reliabilities.
We compare the different algorithms over the TREC and NLP datasets.
We evaluate both item rating estimates and user reliability estimates.
Error in item ratings is measured in terms of % of incorrect item rating.
Thus lower the value, better is the estimate.
Figure 1(a) shows the error for the three TREC datasets.
We also show the overall aggregate error, which is the % of total items incorrectly predicted over the three datasets.
For the  rst two datasets, the best algorithms are ALGORITHM 2 and EM, with MAJORITY much worst than the rest.
This is perhaps because as we will see in synthetic datasets, MAJORITY is very sensitive to presence of spammers.
In the third dataset, MAJORITY is in fact the best, along with ALGORITHM 2.
Thus overall, ALGORITHM 2 is the most robust algorithm and has lowest aggregate error for the TREC dataset.11 10sites.google.com/site/treccrowd/home

 Correlation, Low Max Degree (b) Equal Spammers, Positive Correlation, High Max Degree (c) Equal Spammers, Negative Correlation, Low Max Degree (d) Equal Spammers, Negative Correlation, High Max Degree (e) No Spammers, Positive Correlation, Low Max Degree (f) No Spammers, Positive Correlation, High Max Degree (g) No Spammers, Negative Correlation, Low Max Degree (h) No Spammers, Negative Correlation, High Max Degree Figure 2: Item errors on synthetic data.
User degrees are drawn according to a power law.
First row considers equal number of spammers, hammers, and random users, while second considers only hammers and random users.
First two columns consider user reliabilities positively correlated with their degrees, while third and fourth considers negative correlation.
We break each scenario into two graphs to better visualize the differences.
In all graphs as max degree increases, so does the degree skew, and then ALGORITHM 2 performs consistently better than GKM and KOS.
In presence of spammers ( rst row), MAJORITY and EM deteriorate.
ALGORITHM 2 performs well across the spectrum.
Figure 1(b) shows the item errors for the three NLP datasets.
Again we see a similar story here, ALGORITHM 2 is best in two out of the three datasets.
For the third, MAJORITY is best, with ALGORITHM 2 not far behind.
Overall, ALGORITHM 2 has the lowest aggregate error in NLP.
Next we analyze the error in user reliability estimates.
Since some of the algorithms like KOS give user reliabilities only up to a constant normalization factor, we cannot directly measure user reliability estimates by comparing them to the ground truth (as they could be off by a constant factor).
Thus we use Pearson s correlation coef cient to measure the accuracy of user reliability estimates, which is always a number between  1 and 1, and measures the correlation between two vector quantities.
A value of 1 means complete positive correlation (up to some af ne transformation), 0 means the two quantities are independent of each other, and  1 means they are negatively correlated.
Larger the value, more the positive correlation, and therefore lower the error.
Figure 1(c) and 1(d) show that ALGORITHM 2 is either the best or close to the best in estimating user reliabilities for all the datasets, while other algorithms signi cantly underperform in at least one of the datasets.
To better understand the performance of the algorithms with respect to the different parameters, we perform experiments over synthetic datasets.
We generate synthetic datasets using the following steps.
The number of items and the number of users is  xed to
 generated as i.i.d.
Bernoulli variables with p = 1/2.
algorithms.
This is because many of the users are close to random, as evident in high % errors for this dataset.
Thus having a true estimate for these random users is not helpful, and LOWERBOUND is in fact worse than some of the other algorithms.
For generating the bipartite graph between the items and users, we use powerlaw sequences for user degrees, where the number of items rated by users follow a powerlaw distribution with an exponent of 2.5.
In each case, we generate a random graph satisfying the given degree sequence.
We study the accuracy of different algorithms as a function of the maximum degree.
We de ne three types of users: hammers, which have reliability 0.8, spammers, who have reliability  0.8, and random, who have reliability of 0.
We study the performance of algorithms as a function of the fraction of spammers, hammers and random users in the dataset.
We consider two con gurations: equal spammers, consisting of equal number of hammers, spammers and random users, and no spammers, consisting of equal number of hammers and random users.
To model real-life scenarios we consider cases when the user re-liabilities are correlated with degrees.
For e.g., reliable users could be more expensive, and hence offer less number of labels.
Thus we consider the case of negative correlation where reliabilities are negatively correlated to the user degrees.
For the sake of completeness, we also consider the case of positive correlation where reliabilities are positively correlated to user degrees.
This gives us four combinations: equal vs. no spammers and positive vs. negative correlations.
Figure 2 shows the performance of all the algorithms for the four combinations.
We explain the results below.
Figures 2(a) and 2(b) contains the results of the dataset with equal spammers and positive correlations.
We break the graph into two parts to focus on the low and high degree parts separately.
Because of a large number of spammers, MAJORITY has an error rate close to 50%, which is so large that it does not even appear in the plot.
EM also has a very large error for low max degree, but becomes competitive for high max degree.
As the maximum user degrees become larger, the skew in degrees also becomes larger, and
 spectral methods of GKM and KOS for high maximum degree.
This difference, although slight in synthetic data, manifests as a large one in real datasets, where the degree sequences are even more nonuniform.
We see a very similar trend in Figures 2(c) and 2(d).
For Figures 2(e) and 2(f), which have no spammers and positive correlation, MAJORITY and EM do better than before.
In fact, EM does slightly better than the spectral algorithms.
Among the spectral algorithms, ALGORITHM 2 outperforms everyone else because of the nonuniform degree sequence.
Figures 2(g) and 2(h) show as a similar trend for negative correlations as in the case of positive correlation, but the effect is less pronounced with all the algorithms bunched together more closely.
In summary, KOS and GKM perform well when the degrees are uniform (maximum degree is small and close to the minimum), but deteriorate when there is a skew in the degrees.
EM performs well when there are no spammers, but deteriorates with the introduction of spammers.
ALGORITHM 2 works well across the spectrum, and is robust to spammers and nonuniform degree sequences.
This helps ALGORITHM 2 perform well on most synthetic and real datasets.
We studied the problem of aggregating user ratings when the user item rating graph is arbitrary.
We formulated a matrix completion problem and presented two eigenvector-based algorithms that have guaranteed error bounds when the resulting user user co-rating graph satis es expansion properties.
It would be interesting to see if one can say anything directly about the alternate-projection based technique under a similar set of assumptions.
In practice not all items need similar effort to rate; incorporating this dif culty is also an interesting open direction.
