Versioned content is abundant on the Web.
Wikipedia, and wikis, constitute the most prominent example, and they account for a large portion of total page-views.
Blogs with multiple authors, and pages served by content-management systems, are another example in which the versioning is  The authors are listed in alphabetical order.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
Michael Shavlovsky Computer Science Dept.
University of California mshavlov@ucsc.edu Santa Cruz, CA 95064, USA present, but not directly exposed to the viewer.
Code is another prominent example of revisioned content, and one that is becoming common on the web, thanks to the success of sites like GitHub, where users can share their code repositories.
We study in this paper the problem of attributing revi-sioned content to its author, and more generally, to the revision where it was originally introduced.
This problem is insteresting for several reasons.
The Wikipedia Reuse Pol-icy1 requires people reusing Wikipedia material to either provide a link to the original article and revision history, or to cite the most prominent authors of the content.
Furthermore, in the Wikipedia community it is felt that proper content attribution is an important way to acknowledge and reward contributors, and to foster participation and contributions from communities where authorship has been traditionally recognized and rewarded, such as the academic community [7, 12].
Tracking the authorship of Wikipedia content is also an important tool in assisting editors, and viewers, in determining the origin of assertions, and analyzing page evolution.
In code, as in wikis, authorship tracking is useful to properly reward contributors.
Furthermore, authorship tracking can be useful in determining the reason behind implementation choices.
Several revisioning systems implement  blame  methods, which attribute every line to an author/revision, but this attribution is extremely crude and imprecise, as it cannot cope with blocks of code that are transposed from one location to another, or from one  le to another   changes that are common when code is polished or refactored.
At  rst glance, the attribution problem for revisioned content seems trivial: surely we can simply compare each revision with the previous one, detect any new content, and attribute it to the revision s author.
Unfortunately, things are not quite so simple.
Content in revisioned systems is often deleted, only to be later introduced, and it is important to be able to trace the authorship to the  rst original introduction.
In Wikipedia, the content of pages is frequently removed by vandals, and reinstated in subsequent revisions: this is illustrated in Figure 9, where the periodic dips in page size correspond to content deletions.
One way to guard against such attacks is to check whether the most recent revision happens to coincide with one of the previous revisions, in which case, authorship is carried over from the previous revision.
However, this ad-hoc remedy cannot cope with broader attacks.
For instance, attackers could  rst use
 Wikipedia_content 343a fake identity to remove the page contents, then use their main identity to restore the page to its previous contents, except for some small, imperceptible changes that foil the revision equality check: the whole page content would then be attributed to them.
The goal of this work is to present algorithms that can be used on Wikipedia, with the resulting authorship information available to visitors.
Once authorship information is prominently displayed, attacks that aim at in ating the size of one s authorship are likely, prompting our quest for general, robust algorithms.
A more general solution also bene ts code attribution, since blocks of code are commonly moved from one branch to another, or deleted and later reinserted.
We propose to attribute authorship of revisioned content by comparing the content of the most recent revision, with the entire content of all previous revisions.
For every symbol (word, or character, or token) in the most recent revision, we compute all statistically signi cant matches with previous content: these are the matches whose sequence of symbols is rare enough that the match is likely to be due to a shared origin, rather than serendipitous reinvention.
The symbol is then assigned the earliest possible origin that is compatible with all the matches.
We call this approach the earliest plausible attribution approach.
We show that earliest plausible attribution yields a more natural content attribution than other approaches, including approaches based on longest matches with previous content, or approaches inspired by the edit-analysis work of Tichy [14].
By comparing new revisions with the full set of previous revisions, the earliest plausible attribution approach achieves resistance to page deletions and vandalism.
As Figure 8 (A0 vs. A1) later in the paper indicates, the resulting attribution di ers by over 75% from the attribution computed via comparisons to the most recent revision only, the di erence being due chie y to deletion-reinsertion attacks and other vandalism.
We introduce e cient algorithms for earliest plausible attribution.
If fed all revisions at once, the algorithms can compute the content origin in time proportional to the size of the revision history, which is clearly optimal.
More commonly, though, revisions are created and must be analyzed one and a time.
A practical implementation must maintain a summary of all past revisions, and process a new revision on the basis of such a summary.
We show that the algorithm we propose uses a summary of size proportional to all the past change in the previous revisions   and this change size is typically much smaller than the total revision history size, since a new revision is usually identical to the preceding one except for a few small changes.
The algorithm runs in time proportional to the sum of the size of the previous summary, and the size of the most recent revision.
Again, since both summary and most recent revision must be read, this is optimal.
Related work.
The WikiTrust tool computes a value of reputation for Wikipedia authors and content, as well as the revision where each word was inserted [1, 2, 3].
The attribution algorithm achieves resistance to vandalism by comparing the most recent revision not only with the preceding one, but also with a set of  reference  revisions, consisting of recent revisions that either have high content reputation, or that were created by a high-reputation author.
The approach is fairly e ective in practice, but the attribution depends on the reputation computation: there is no independent characterization of the attribution that is computed, and the process is computationally involved.
Furthermore, it is not clear how to extend the approach beyond Wikipedia.
In [3], several text matching algorithms are evaluated for their ability to explain the editing process in Wikipedia.
Tichy-inspired algorithms [14] were found to be highly ef- cient, and as precise as any alternative, for the problem of comparing two revisions.
In contrast, in this work we show that for the problem of comparing a revision with all the preceding ones, the earliest plausible attribution yields more e cient algorithms, and arguably more natural results.
The problem of attributing Wikipedia content has also been studied in [6], where an algorithm based on hierarchical content matching is proposed.
When a revision is compared with the preceding revision, matches of large sections of text (sections, paragraphs, sentences) are evaluated  rst, and a  ner-grained algorithm based on the Python di ib library is used to attribute any remaining content.
The resulting attribution is reported to compare favorably with the one computed by WikiTrust.
String matching is a very well studied problem; see e.g.
[9] for an in-depth overview.
The algorithms presented in this paper make use of several results on string matching, including tries and su x trees.
Sophisticated string matching algorithms developed for genetic applications involve a two-step process: a coarse alignment is computed between the strings, followed by a  ner-grained analysis of string differences (see [9] again).
This  genetic  approach is resistant to the transcription errors that occur in gene sequencing.
The algorithms developed in this paper are based instead on exact matching of short sequences.
It is an interesting open question whether the algorithms for attribution of re-visioned content could bene t from the genetic approach.
The attribution problem considered in this paper is a special case of information provenance problem.
For an overview of information provenance, see e.g.
[4, 5] for provenance in databases, and [13, 11, 8] for an overview in a broader context.
Paper organization.
After introducing some notation and concepts, we compare in Section 3 conceptual methods of de ning attribution, providing justi cations for our choice of earliest plausible attribution.
In 4 we describe an e cient algorithm for earliest plausible attribution, and we prove that the algorithm is optimal.
We present empirical results obtained in the analysis of the English Wikipedia in Section 5, and we conclude with some discussion of the results and possible future work in Section 6.
All the code and data for the algorithms can be found at https://sites.google.com/a/ucsc.edu/ luca/the-wikipedia-authorship-project.
Revisions.
We model revisioned content as a sequence of revisions   =  0,  1,  2, .
.
.. Each revision   consists in a sequence of tokens t0, t1, .
.
.
, tm 1, taken from a set T of tokens, where len( ) = m is the length of  .
We assume that len( 0) = 0, so that  0 represents the initial, empty revision that exists before any subsequent revision is created.
For   = t0, t1, .
.
.
, tm 1, we indicate with  [i] the token ti, and we write  [i : j] for ti, ti+1, .
.
.
, tj 1.
Depending on the application, the tokens can be individual unicode characters, or they can be words in a text, tokens of a programming language, and so forth.
Given a sequence   of revisions, a 344global position is a pair (n, k) with n   0 and 0   k < len( k).
Matches.
A match M = (n, i, j, n(cid:48), i(cid:48), j(cid:48)) between positions [i..j   1] of revision  n and positions [i(cid:48)..j(cid:48)   1] of revision  n(cid:48) , denoted informally (and more intuitively) as M = ( n[i : j] =  n(cid:48) [i(cid:48) : j(cid:48)]), consists of two revisions  n,  n(cid:48) , and indices 0   i < j   len( n), 0   i(cid:48) < j(cid:48)   len( n(cid:48) ), such that:   j   i = j(cid:48)   i(cid:48) > 0, so that the matched portions have equal length and are nonempty;   for all 0   k < j   i, we have  n[i + k] =  n(cid:48) [i(cid:48) + k], so that tokens at corresponding positions of  n and  n(cid:48) match.
Given a match M = ( n[i : j] =  n(cid:48) [i(cid:48) : j(cid:48)]), we denote by len(M ) = j i its length.
We say that a position k is matched by M if i   k < j.
For a position k matched by M , we let M (n, k) = (n(cid:48), k i+i(cid:48)): thus, we think of matches as partial functions between global positions that relate positions  lled by equal tokens.
We denote by M( n,  m) the set of all matches between revisions  n and  m.
We say that a match M = ( n[i : j] =  n(cid:48) [i(cid:48) : j(cid:48)]) is a sub-match of M(cid:48) = ( n[  :  j] =  n(cid:48) [ (cid:48) :  j(cid:48)]) if     i,  j   j, and     i =  (cid:48)   i(cid:48); we say that the sub-match is proper if at least one of the two inequalities is strict.
Interesting matches.
Our interest in matches is due to the fact that a match between a later revision and earlier one may indicate that the content of the later revision originated in the earlier one.
Not all matches correspond to a common origin of the content, however.
For instance, in English, the two-word sequence  such that  is very common, and it would be unreasonable to assume that they have been copied from an earlier revision whenever they appear in a later one.
In order to use matches to study authorship, we need to distinguish fortuitous matches from those that indicate shared origin.
An in-depth approach would likely require a probabilistic model of content structure, and of how content propagates from one revision to the next.
Such a model could then be used to compute, for each revision token, a probability distribution over the places where the content might have originated.
We follow a simpler, discrete approach, where content is attributed deterministically to a revision of origin.
Deterministic attribution leads to e cient algorithms that can scale to very large bodies of content, such as Wikipedia.
We also remark that users of authorship information generally expect a deterministic attribution: Wikipedia visitors and editors want to know who wrote what, and copyright is based on deterministic, not probabilistic attribution.
Probabilistic attribution algorithms, and the question of their e cient implementation at scale and possible accuracy advantages, remain a topic for future work.
Consider a match M = ( n[i : j] =  k[i(cid:48) : j(cid:48)]) between two revisions  n and  k, with k < n. To decide whether to attribute the sequence   =  [i],  [i + 1], .
.
.
,  [j   1] to  n or  k, we use a rarity function   : T   (cid:55)  IR+: intuitively, the larger  ( ) is, the more likely it is that the sequence   in  n and  k shares the same origin.
We require that a rarity function   satis es the following two conditions:    ( ) = 0: the rarity of the empty sequence is zero.
  For all     T   and all t   T , we have  ( ) <  ( t): longer sequence are strictly rarer than shorter ones.
pt of each token t, we can take  (t0, t1, .
.
.
, tm) =(cid:81)m A simple choice is  ( ) = len( ): the rarity of a sequence is equal to its length.
More sophisticated rarity functions can be used: for instance, if we know the occurrence probability .
Rarity functions based on the occurrence frequency of multi-token sequences could also be used.
Given a match M = ( n[i : j] =  k[i(cid:48) : j(cid:48)]), we de ne its interest  (M ) =  ( n[i],  n[i + 1], .
.
.
,  n[j   1]) to be equal to the rarity of the matched sequence of tokens.
We de ne the interesting matches between revisions  n and  m, according to the rarity function   and threshold  , as the set of matches of rarity at least  : i=0
 pti M( n,  m |      ) = {M   M( n,  m) |  (M )    } .
We note that if we choose   = len, the set M( n,  m | len   l) will consist of all matches between  n and  m that have length at least l. Given a position 0   k < len( n) of revision  n, we denote by M[k]( n,  m |      ) the interesting matches between  n and  m that have interest at least   as measured by  , and that match position k of  n.
Origin labeling.
An origin labeling associates with each token the revision where the token originated.
Precisely, an origin labeling   for a ( nite or in nite) sequence   =  0,  1,  2, .
.
.
of revisions is a labeling that associates with each global position (n, k) of   its origin  (n, k)   IN, with  (n, k)   n. If  (n, k) = n, we say that the token  n[k] is new in  n.
In some instances of revisioned content, such as Google Docs, full information about the edit actions by each individual user are available.
In this case, the authorship can be computed by observing directly the typing, cutting, pasting, etc, performed by each editor.
In many other instances, however, we can observe only the outcome of the editing process, namely, the sequence of revisions produced by the various users.
This is the case for Wikipedia, and for code repositories, since the environments where users edit the code are independent from the repositories.
In these cases, we must infer authorship after the fact, by comparing the result of the editing with previous content.
There is no a-priori correct way to infer authorship, as we cannot reconstruct the mental process of the editors to tell whether they are copying or reinventing.
One of the contributions of this paper is to introduce the notion of earliest plausible attribution for revisioned content, showing that it leads to plausible attribution in practice.
We remark that, even when the actions of users are observable during editing, as in Google Docs, we can never be sure whether editors are retyping a passage, copying it from paper, or reinventing it anew: earliest plausible attribution can thus be a useful notion even when edit actions are observable in detail.
In this section we de ne earliest plausible attribution and we compare it with other attribution methods.
The question of e cient implementation will be the subject of the next section.
Algorithm A0 computes the origin of tokens in a revision  n by comparing the revision with the previous one in the sequence.
Given a sequence   =  0,  1,  2, .
.
.
of revisions,
 putes an origin labeling   for   proceeding inductively on the revisions.
The  rst revision  0, being empty, has a null labeling.
For each subsequent revision  n, n > 0, algorithm A0 computes all interesting matches with the preceding revision  n 1.
Every unmatched token in  n is assigned an origin label of n. Each matched token is assigned the origin label of the matching position in the previous revision; if the token had multiple matches to di erent positions, the token is assigned the minimum of the origin labels of the corresponding positions.
Algorithm A0 Matches with previous revision.
Input: A sequence   =  0,  1,  2, .
.
.
,  m of revisions, with  0 =  , along with a rarity function   and a threshold  .
Output: An origin labeling   for  .
for all positions 0   k < len( n) of  n do Let (cid:99)M := M[k]( n,  n 1 |      ).
if (cid:99)M =   then  (n, k) := minM (cid:99)M  (M (n, k)).
1: for revisions n = 1, 2, 3, .
.
.
do







 10: end for end if end for  (n, k) := n else One may conceive a variant algorithm, termed Algorithm A0M, where only the most interesting match(es) for each token are considered: the idea being that the longer the match, the more likely it is to correspond to origin.
Algorithms A0 and A0M may yield di erent labelings, as illustrated in Figure 1.
In the  gure, we use sequence length as the rarity function, together with a threshold of 3, so that matches that are 3 or more tokens are considered interesting.
In labeling symbols b c in  3, Algorithm A0 considers two interesting matches: ( 3[0 : 3] =  2[0 : 43), involving a b c, and ( 3[1 : 6] =  2[4 : 9]), involving b c z z z.
The  rst match yields origin 1 1 for b c, the second 2 2.
The origin assigned by A0 is the least of these two, namely, 1
 second match, as it is longer, and assigns to b c origin 2 2.
This example highlights why we prefer to consider all interesting matches, rather than just the longest ones: even though a b c in  3 matches a b c in  1, it is assigned origin
 match that is interesting (with a matched sequence of tokens that is su ciently rare) denotes a common origin of the content.
If there is more than one interesting match for a token position, we look at all such interesting matches as possible explanations for the origin of the content, and we err on the side of the oldest possible attribution, yielding the min in line 7 of Algorithm A0.
Algorithm A0 (and A0M) relies on comparisons with the immediately preceding revision only.
In many relevant examples of versioned content, content can be deleted from one revision only to reappear several revisions later.
For instance, the content of Wikipedia pages is frequently deleted by vandals.
If authorship is determined via a comparison with the immediately preceding revision only, then an edi-Algorithm A0M Origin via most interesting matches with previous revision.
Input: A sequence   =  0,  1,  2, .
.
.
,  m of revisions, with  0 =  , along with a rarity function   and a threshold  .
Output: An origin labeling   for  .
for all positions 0   k < len( n) of  n do  (n, k) = n Let (cid:99)M := M[k]( n,  n 1 |      ).
if (cid:99)M :=   then Let (cid:102)M = arg maxM (cid:99)M  (M ).
 (n, k) := minM (cid:102)M  (M (n, k)).
1: for revisions n = 1, 2, 3, .
.
.
do








 11: end for end if end for else  3:  2:  1:  3:  2:  1: a1 b1 c1 z2 z2 z2 a1 b1 c1 x2 b2 c2 z2 z2 z2 a1 b1 c1 (a) A0 a1 b2 c2 z2 z2 z2 a1 b1 c1 x2 b2 c2 z2 z2 z2 a1 b1 c1 (b) A0M Figure 1: A sequence of revisions, with origin labeled according to algorithms A0 and A0M.
We represent each revision by its list of tokens, using letters to denote tokens.
The origin labels are computed for a rarity function equal to sequence length, and threshold of 3.
We write above every token the origin that the algorithm assigns to it.
tor who restores the contents of a Wikipedia page after it is deleted would be attributed the authorship of all the restored content.
As these periodic acts of vandalism that destroy most of a page s content are common on Wikipedia, authorship algorithms that are based only on comparisons with the immediately preceding revision will grossly mis-attribute content, as we will show experimentally in Section 5.
Our preferred algorithm for attribution of revisioned content, Algorithm A1, compares the latest revision with all the previous revisions, looking for matches with any prior content, rather than just content in the immediately preceding revision.
We call this process earliest plausible attribution, since the attribution it produces is the earliest that is compatible with an explanation by interesting matches.
Figures 2 and 3 provides a comparison of algorithm A0 and A1 in presence of a delete-and-restore attack, as common on Wikipedia, and of a more complex attack involving content that is deleted, then gradually reinstated.
One of the better-known algorithms for generating edit di erences between revisions is due to Tichy [14].
Since the Tichy algorithm performs well in explaining the edit his-
ceding revisions.
Input: A sequence   =  0,  1,  2, .
.
.
,  m of revisions, with  0 =  , along with a rarity function   and a threshold  .
Output: An origin labeling   for  .
for all positions 0   k < len( n) of  n do
 Let (cid:99)M :=(cid:83) if (cid:99)M =   then  (n, k) := minM (cid:99)M  (M (n, k)).
1: for revisions n = 1, 2, 3, .
.
.
do







 10: end for end if end for  (n, k) := n else  4: a4 b4 c4 x4 f4 g4 h4  3: p3 q3  2: a1 b1 c1 x2 f1 g1 h1  1: a1 b1 c1 f1 g1 h1 (a) A0  4: a1 b1 c1 x2 f1 g1 h1  3: p3 q3  2: a1 b1 c1 x2 f1 g1 h1  1: a1 b1 c1 f1 g1 h1 (b) A1 Figure 2: A sequence of revisions, with origin labeled according to algorithms A0 and A1, with rarity equal to length and threshold 3.
This sequence illustrates a delete-and-restore event, common on Wikipedia.
tory of Wikipedia [3], it is of interest to adapt it to origin computation and compare it to A1.
Given a revision  n = t0, t1, .
.
.
, tm 1, the Tichy-based Algorithm A2 searches revisions  0, .
.
.
,  n 1 for the longest pre x of t0, t1, .
.
.
, tm 1.
If this longest pre x is, say, t0, t1, .
.
.
, tk, with k   m   1 and  (t0, t1, .
.
.
, tk) >   for the chosen rarity function   and threshold  , then the algorithm  xes the origin of t0, t1, .
.
.
, tk in  n according to the origin of the matching tokens (taking the minimum, in case the longest pre x appears multiple times).
The algorithm then proceeds searching for the longest pre x of the remaining unlabeled portion tk+1, tk+2, .
.
.
, tm 1.
If no longest pre x can be found, or if the longest pre x from t0 has rarity below the threshold, then t0 is labeled as having origin n, or  (n, 0) := n, and the search continues from the remaining unlabeled portion t1, t2, .
.
.
, tm 1.
The process continues until the whole of  n has been labeled according to its origin.
Figure 4 compares the origin labelings computed by Algorithms A1 and A2.
We see that Algorithm A2 attributes to the tokens c d a m in  4 origins 2 2 4 4, even though these tokens constituted the  rst revision  1.
The attribution 1 1

 Given a sequence of revisions  0,  1,  2, .
.
.
and two origin labelings  ,  (cid:48), we write      (cid:48) if  (n, k)    (cid:48)(n, k) at all positions n, k of the sequence; we write   <  (cid:48) if      (cid:48), and if there is at least a position (n, k) where  (n, k) <  (cid:48)(n, k).
The following property establishes that, among A0, A0M, and A1, Algorithm A1 computes the earliest attribution and A0M the latest.
 6:  5:  4:  3:  2:  1:  6:  5:  4:  3:  2:  1: a4 b4 c4 d4 e6 x6 w6 g6 h6 l6 q3 r3 a4 b4 c4 d4 f5 g5 p3 q3 r3 a4 b4 c4 d4 p3 q3 r3 a1 b1 c1 d1 e1 x2 f1 g1 h1 l1 a1 b1 c1 d1 e1 f1 g1 h1 l1 m1 (a) A0 a1 b1 c1 d1 e1 x2 w6 g1 h1 l1 q3 r3 a1 b1 c1 d1 f5 g5 p3 q3 r3 a1 b1 c1 d1 p3 q3 r3 a1 b1 c1 d1 e1 x2 f1 g1 h1 l1 a1 b1 c1 d1 e1 f1 g1 h1 l1 m1 (b) A1 Figure 3: A sequence of revisions, with origin labeled according to algorithms A0 and A1, with rarity equal to length and threshold 3.
In this sequence, content is  rst deleted and replaced with spam, then almost entirely restored.
 4:  3:  2:  1: a3 b3 c1 d1 a1 m1 a3 b3 c2 d2 g2 h2 c2 d2 g2 h2 c1 d1 a1 m1 (a) A1  4:  3:  2:  1: a3 b3 c2 d2 a4 m4 a3 b3 c2 d2 g2 h2 c2 d2 g2 h2 c1 d1 a1 m1 (b) A2 Figure 4: A sequence of revisions, as labeled by Algorithms A1 and A2 with rarity equal to length and threshold 3.
Property 1.
Let  A0,  A0M , and  A1 be origin label-ings computed by Algorithms A0, A0M, and A1 respectively for a sequence of revisions.
Then,  A1    A0    A0M .
Moreover, there are sequences of revisions for which each of two above inequalities is strict.
Proof.
The weak inequalities follow from the fact that, in deriving the label of a token, the matches considered by A0M are a subset of those considered by A0, which are in turn a subset of those considered by A1.
The fact that the inequalities can be strict is witnessed by Figure 1 and 2.
If a portion of content with rarity above the threshold occurs twice in the revision history, Algorithm A1 will assign to the later occurrence an origin that is no later than that assigned to the earliest occurrence, and that can in fact be strictly smaller.
a Property 2.
sequence Consider revisions  0, .
.
.
,  i, .
.
.
,  k, and a match M = ( i[l : m] =  k[l(cid:48) : m(cid:48)]) that is su ciently interesting.
Let   be the origin labeling computed by A1.
Then,  (k, m + j)    (i, m(cid:48) + j) for all 0   j < m   l, and there are cases where the inequality can be strict.
of
 ing revisions.
Input: A sequence   =  0,  1,  2, .
.
.
,  m of revisions, with  0 =  , along with a rarity function   and a threshold  .
Output: An origin labeling   for  .
k := 0 1: for revisions n = 1, 2, 3, .
.
.
do
 3: while k < len( n) do
 tk, tk+1, .
.
.
, tlen( n) 1.
Search in  0, .
.
.
,  n for the longest matching pre- xes of Let tk, .
.
.
, tm be the longest matched pre x, and let A = {(n1, k1), .
.
.
, (np, kp)} be the (possibly empty) set of pairs where the longest matches occur.
if A (cid:54)=      (tk, .
.
.
, tm)     then for i   {0, 1, .
.
.
, m   k} do  (n, k + i) := min1 j p  (nj, kj + i)









 15: end for else end if end while end for k := m + 1  (n, k) := n k := k + 1  5:  4:  3:  2:  1: a3 b3 c1 d1 g2 h2 a3 b3 c1 d1 a1 m1 a3 b3 c2 d2 g2 h2 c2 d2 g2 h2 c1 d1 a1 m1 Figure 5: A sequence of revisions, as labeled by Algorithm A1 with rarity equal to length and threshold
 tokens in  5 are smaller than the corresponding ones in  3.
Proof.
The result follows from the fact that the matches for  k[l(cid:48) : m(cid:48)] include  i[l : m].
The fact that the inequality can be strict is illustrated in Figure 5.
This property formalizes the robustness to attacks of Algorithm A1.
Consider a good revision  i of a page.
If vandals produce revisions  i+1,  i+2, .
.
.
,  k 1, and the page is then restored to its good state  k =  i, the property ensures that no content in  i =  k becomes attributed to an author of  i+1,  i+2, .
.
.
,  k 1,  k.2

 In the previous section, we presented various conceptual algorithms for attributing origin to versioned content.
The algorithms presented there are extremely ine cient, and have conceptual value only.
In this section, we examine the question of e cient implementation for these conceptual algorithms.
teresting match with the content of  i.
This can be ensured by adding start and end markers to the text of revisions, and by considering interesting any match that contains such markers.
algoirthms is | | =(cid:80)n Input size and change size.
Given a sequence of revisions   =  0,  1, .
.
.
,  n, the input size for our attribution i=0 len( i) (assuming that tokens can be represented in constant space).
In revisioned content, it is often the case that only a small portion of the content is modi ed at each revision, so that consecutive revisions differ only in a few tokens.
It is thus insightful to study the performance of the algorithms not only as a function of the size of the input, but also as a function of the size of the change that occurred.
To this end, given two consecutive i=1 | i|, where  1, .
.
.
,  k and  1, .
.
.
,  m are the shortest sequences so that we can write: revisions  ,  (cid:48), we de ne  ( ,  (cid:48)) = (cid:80)m i=1 | i| +(cid:80)m   =  0 1 1 2 2    n n =  0 1 1 2 2    n n (cid:48)   In other words, we write   and  (cid:48) as composed of maximal sequences of unchanged portions of text  0, .
.
.
,  m, and of portions  1, .
.
.
,  m in   that will be replaced by sequences  1, .
.
.
,  m in  (cid:48).
We then de ne the change size change( ) of  0,  1, .
.
.
,  n as change( ) =(cid:80)n 1 i=0  ( i,  i+1).
Summary size and one-revision update.
Revisioned content is produced, as the name implies, one revision at a time.
When computing the origin of the tokens in the newest revision  n, it would be impractical to read and reprocess all previous revisions  0, .
.
.
,  n 1.
Practical algorithms rely on a summary Sn 1 of  0, .
.
.
,  n 1, containing all the information that the algorithm needs to know about the preceding revisions to attribute later revisions.
The algorithms compute the origin labeling for  n on the basis of Sn 1 and  n, producing as output both Sn and the origin labeling for  n.
We refer to this computation as the one-revision update.
We will thus study how the summary size, and the running time for the one-revision update depend on the input size and change size.
Consider a  xed a rarity function   and a rarity threshold  .
We say that a sequence of tokens t1, t2, .
.
.
, tn is minimally interesting if  (t1, t2, .
.
.
, tn)    , and at least one of  (t2, .
.
.
, tn) <   or  (t1, .
.
.
, tn 1) <   holds.
When the rarity function is simply the number of tokens, and the rarity threshold   is an integer, then the minimally interesting sequences are the sequences consisting of   tokens.
We say that a match M = ( n[i : j] =  n(cid:48) [i(cid:48) : j(cid:48)]) is minimally interesting if  n[i],  n[i+1], .
.
.
,  n[j 1] is minimally interesting.
To obtain an e cient implementation of Algorithm A1, we start from the observation that in Step 3 of Algorithm A1, we need to consider only minimally interesting matches.
If in Step 3 of Algorithm A1 the set (cid:99)M is Lemma 1.
limited only to minimally interesting matches, the labeling computed by the algorithm is unchanged.
Proof.
For a token tk of  n, let M be a match realizing the minimum in Step 7 of Algorithm A1, and let ti, .
.
.
, tj be the matched sequence, with i   k   j.
If M is minimally interesting, the result holds.
If M is not minimally interesting, then both sub-matches for ti, .
.
.
, tj 1 and ti+1, .
.
.
, tj are interesting, and tk belongs to one of them.
Continuing in this fashion, we can  nd a submatch M(cid:48) of M that contains tk and that is minimally interesting.
Since tk would be assigned the same origin under M or M(cid:48), the result holds.
Input: A sequence   =  0,  1,  2, .
.
.
,  m of revisions, with  0 =  , along with a rarity function   and a threshold  .
Output: An origin labeling   for  .
2: for revisions n = 1, 2, 3, .
.
.
do



 end for for all minimally interesting sequences tk, .
.
.
, tm of  n do for all positions 0   k < len( n) of  n do  (n, k) := n if tk, .
.
.
, tm   T then







 else






 22: end for end if end for ik, .
.
.
, im := (cid:96)(tk, .
.
.
, tm) for all j   [k, .
.
.
, m] do  (n, j) := min{ (n, j), ij} end for end if end for for all minimally interesting sequences tk, .
.
.
, tm of  n do if tk, .
.
.
, tm   T then (cid:96)(tk, .
.
.
, tm) :=  (n, k), .
.
.
,  (n, m) T := Ins(T ; tk, .
.
.
, tm) (cid:96)(tk, .
.
.
, tm) :=  (n, k), .
.
.
,  (n, m) minimally interesting match for which the minimum in Line 7 is realized (this exists, due to Lemma 1).
By induction hypothesis, the trie T will contain the sequence  n(cid:48) [i(cid:48) : j(cid:48)] with its minimal labeling, in which the token  n[k] is labeled with origin p. Thus, Algorithm A3 in Steps 3 13 will assign to  n[k] an origin no larger than p.
  Conversely, assume that Algorithm A3 assigns origin p to token  n[k].
Then, T must have contained a minimally interesting sequence  n[j : l] = tj, .
.
.
, tl 1, for j   k < l, where tk is labeled by p. By inducton hypothesis, p is the minimal label of tk in all occurrences of tj, .
.
.
, tl 1 in  0, .
.
.
,  n 1, indicating that Algorithm A1 also labels  n[k] with label no greater than p.
Second, we show that once  n is processed by A3, the induction hypothesis holds also for  0, .
.
.
,  n.
Consider a minimally interesting sequence   occurring in  n (the situation of minimally interesting sequences not occurring in  n is unchanged).
The arguments in the  rst part of this proof ensure that once Steps 3 13 have terminated, the sequence   in  n is labeled according to its minimal labeling.
Steps 14 
 and is labeled in it according to its minimal labeling.
This completes the induction step.
The following theorem characterizes the time and space requirements for Algorithm A3.
Theorem 2.
If there is an integer M such that all token sequences of length at least M are interesting, then given a Figure 6: Trie resulting after processing revisions  1,  2,  3 as in Figure 4.
This result suggests implementing Algorithm A1 in terms of a trie.
A trie T is a tree whose edges are labeled with tokens, and such that the edges outgoing from a node are labeled by distinct tokens.
We say that a sequence of tokens t1, t2, .
.
.
, tm belongs to the trie T , written t1, t2, .
.
.
, tm   T , if there is a path from the root labeled with the sequence, and we use the sequence to refer to the node where the path ends.
If the sequence t1, t2, .
.
.
, tm is minimally interesting, we say that the corresponding node is minimally interesting.
If   = len and   is an integer, the minimally interesting nodes are those at depth   in the trie.
We denote by   the empty trie consisting only of a root node, and we denote by Ins(T ; t1, .
.
.
, tm) the result of creating a path labeled by t1, .
.
.
, tm in T in the trie.
In the implementation of A1, we use tries to represent all the minimally interesting sequences of tokens that have occurred in past revisions.
Each minimally interesting node t1, .
.
.
, tm of the trie is labeled with the origin k1, .
.
.
, km = (cid:96)(t1, .
.
.
, tm) of the sequence of tokens t1, .
.
.
, tm.
This yields Algorithm A3.
Figure 6 illustrates the trie resulting after processing revisions  1,  2,  3 as in Figure 4, for a rarity function equal to length, and threshold 3.
The leaf nodes are the minimally interesting nodes.
To save space in the trie, we omit the non-interesting nodes that have a single child, concatenating the labels of the edges leading into and out of such nodes.
The following theorem shows that Algorithms A3 and A1 compute the same origin labels.
Theorem 1.
Algorithm A3 computes the same origin labels as Algorithm A1.
To state the proof of this theorem, consider a sequence   = t1, .
.
.
, tk occurring at least once in a set of revisions  0, .
.
.
,  m that has been labeled according to its origin by Algorithm A1.
For 1   j   k, let pj be the minimum label that token tj is assigned in any of these occurrences.
We say that p1, .
.
.
, pk is the minimal labeling of   in  0, .
.
.
,  m.
Proof.
The proof proceeds by induction, using the after processing revisons inductive hypothesys  0, .
.
.
,  n, the trie T contains exactly all the minimally interesting sequences occurring in  0, .
.
.
,  n, each labeled with its minimal labeling in  0, .
.
.
,  n.
that, Assume that Algorithm A3 has processed  0, .
.
.
,  n 1 already, and is processing  n.
First, we show that this inductive hypothesis implies that algorithms A1 and A3 produce the same labeling.
There are two directions to the argument.
  Assume that Algorithm A1 assigns origin label p to token  n[k].
Let M = ( n[i : j] =  n(cid:48) [i(cid:48) : j(cid:48)]) be the rootc d d 3 3 2a b c 3 2 2b c d 1 1 11 1 1a 2 2 2 g a m 2 2 2 g h 349sequence  0,  1,  2, .
.
.
of revisions, Algorithm A3 can perform a one-revision update for revision  n using a summary of size O(change( 0, .
.
.
,  n 1)), and in time O(len( n)).
Proof.
Algorithm A3 uses as summary for  0, .
.
.
,  n 1 the trie Tn 1 resulting from the processing of these revisions.
To prove the space requirement, we can prove by induction over n that |Tn|   K   change( 0, .
.
.
,  n), for some  xed K   0.
Note that M is a bound for the length of any minimally interesting sequence: in fact, any interesting sequence   longer than M has its leftmost M tokens, and rightmost M tokens, also form interesting sequences, contradicting the minimality of  .
Let K = M (M + 1)/2 be the maximum number of sequences of length at most M that contain a given position.
Note that a single insertion or deletion going from  n 1 to  n a ects at most K minimally interesting sequences in  n.
Therefore, at most K    ( n 1,  n) new minimally interesting sequences are going to be inserted in Tn 1 in order to obtain Tn.
This leads to the space bound for the summary.
To prove the time bound for the processing of  n, it suf- ces to note that there are at most len( n) minimally interesting matches involving  n, and that processing each one of them (including accessing the trie for retrieving the minimal labeling of any match) takes constant time (the trie has depth at most K).
Note that the theorem implies that the processing of a sequence  0, .
.
.
,  n of revisions can be done in time O(|(| 0, .
.
.
,  n)).
If the rarity of a sequence of tokens is taken to be its length, then trivially all sequences longer than the rarity threshold are rare.
Another case when the length of minimally interesting sequences of tokens is bounded is when the rarity of a sequence of tokens t1, .
.
.
, tk is computed for some token probabilities 0   pki   1, and if there is an upper bound c < 1 for the probability of any token.
as  (t1, .
.
.
, tk) = (cid:81)k i=1
 pki These results suggest that Algorithm A3 is optimal: it is not possible to label a sequence of revisions in time less than the input size, and it is not possible to label a new revision storing less information about the past than all change that has occurred (except if compression techniques are used; such techniques can also be applied to the representation of our trie summaries).
In large-scale implementations of origin analysis, the summary of a revision sequence cannot be stored permanently in-memory: rather, it must be read from persistent storage (such as a database) before the algorithm analyzes a new revision, and written back to persistent storage once the analysis is done.
If the time to read and write the summary is included, then the time required for analyzing revision  n of sequence  0,  1,  2, .
.
.
is in O(len( n) + change( 0, .
.
.
,  n)).
The Tichy-based Algorithm A2 is de ned in terms of longest common matches.
We can obtain an e cient implementation in terms of su x trees, which provide the most time e cient implementation of the longest common sub-string problem [9].
A su x tree is a treelike data structure that can represent all the su xes ai, ai+1, ai+2, .
.
.
, am, 0   i < m, of a given string a0, a1, .
.
.
, am; they can be con structed in time linear in the length of the string [16, 10, 15].
The construction of su x trees can be adapted so that Sn 1 is a su x tree representing all the su xes of  0,  1, .
.
.
,  n 1; see [9] for similar adaptations.
The origin information can be associated with the su x tree in similar fashion to what was done for the trie; we omit the details to conserve space.
The drawback of this algorithm, compared to A3, is that the size required by the summary is proportional to the size of all previous revisions, rather than to the change size.
This because a change involving a token in the middle of a revision of length m gives rise to m/2 new su xes on average, each of which corresponds to at least one new su x tree node.
Theorem 3.
Let M = | 0, .
.
.
,  n| and D = change( 0, .
.
.
,  n).
The su x-tree-based implementation of Algorithm A2 produces the origin labels for revision  n in time O(M ); the time for labeling the complete sequence  0, .
.
.
,  n is O(M 2).
There are some examples of input for which the running time for  n exceeds K   D, for any K   0, so that the running time is not O(D).
The size of Sn is O(M ), and is not in O(D).
Proof.
The space and time results are a consequence of the results on the construction of su x trees [16, 10, 15,
 is proportional to the entire input size, rather than to the change size, follows from the fact that changing a single token in a revision of length m leads to the creation of a number of new su xes that is proportional to m (on average, equal to m/2).
These new su xes must be represented in the su x tree, so that Sn is in O(| 0, .
.
.
,  n|) but not in O(change( 0, .
.
.
,  n)).
We have produced a robust, scalable implementation of Algorithm A3 that can be applied to very large wikis, including the English Wikipedia.
Each revision is parsed in a sequence of tokens, which consists of white-space separated sequences of non-whitespace characters: tokens thus loosely correspond to words.
This tokenization step could be improved by considering the structure of the MediaWiki markup language.
We do not use individual (unicode) characters as our unit of tokenization, for two reasons.
First, using words as attribution units tends to produce more natural results, since contributors typically create or rearrange content in word units; word-level attribution is also easier to display via coloring or other visual cues.
Second, using individual characters as tokens would lead to a larger size for the trie summary, as the trie would grow deeper.
The algorithm uses as rarity function the length of a token sequence, and a con gurable threshold.
The algorithm does not use a rarity function that depends on token (word) frequency, chie y to save space by avoiding the need to store the frequency of a large number of words; we may revisit this decision at a later time.
For each wiki page P, the algorithm stores in persistent storage the pair (n,Tn), consisting of the index n of the last revision of P that has been processed, along with the labeled trie Tn representing the summary.
When a new revision  m for P is produced, with m > n, the algorithm processes all revisions  n+1, .
.
.
,  m: there can be multiple revision to analyze, since the algorithm may have been inactive at times (due to system maintenance), or indeed, it may not have run yet on the page.
Each of  n+1, .
.
.
,  m is fed to the algorithm; the algorithm
  N = 100,  T = 90 days attribution di erence

 trie size

 Figure 7: Attribution di erence, and trie size, for various aging thresholds, as compared to no content aging.
computes and stores the origin of these revisions, and  nally stores (m,Tm) associated with page P.
of this The code, and a demo implementation is available at https://sites.google.com/a/ucsc.edu/ luca/the-wikipedia-authorship-project, along with all the data used for the experiments reported here.
We provide experimental data computed on two revison datasets:   Dataset A: articles with more than 200 revisions in  les wiki-00000066.xml.gz and wiki-00000193.xml.gz.
The dataset consists of 78k revisions in 75 articles.
  Dataset B: articles with at revisions occurring in  les wiki-00000066.xml.gz, wiki-00000193.xml.gz, wiki-00000134.xml.gz and wiki-
 least
 Figure 8: Di erence between attribution by A0, A1 and A2 for length rarity function with various threshold.
The above *.xml.gz  les were chosen at random among the  rst 1000  les obtained by splitting in 100-page portions a 2010 dump of the English Wikipedia.
Unless otherwise noted, we provide results for a rarity function equal to length, and threshold 4.
Content aging.
In the editing of Wikipedia revisions, it occasionally happens that vandals introduce vast amounts of spurious content.
This content is almost immediately removed by editors or non-vandal users.
Yet, since our algorithms store a representation of the entire history of each page, that spurious content would persist inde nitely in our trie summary.
This would o er an avenue to vandals for severely impacting our performance.
To limit this e ects of vandalism, our implementation discards content that has not appeared in any recent revision: this is acceptable in practice, since content that has been long removed from a page is unlikely to be reinserted.
To this end, we label every node of the trie T used in Algorithm A3 with the node age, consisting of a pair (N, T ).
The integer N and the times-tamp T represents, respectively, the most recent revision index and the most recent time when the node was traversed by the algorithm.
Once Algorithm A3 has processed a revision n produced at time Tn, and before writing back the trie to persistent storage, we prune from the trie all the nodes that have both n   N >  N and Tn   T >  T , where the thresholds  N and  T are con gurable.
Table 7 compares the di erence in attribution and size arising from di erent aging thresholds.
The table was obtained from dataset A.
Attribution comparison among A0, A1, A2.
Figure 8 plots the di erence in the attributions computed by Algorithms A0, A1, and A2, for a rarity function equal to token sequence length, and various values of rarity threshold.
These comparisons have been done without using any age-driven pruning of trie nodes in Algorithm A1, to make the comparison fair across algorithms.
The  gure gives the tokens with di erent attribution, as percentage of the total tokens, for dataset A.
As we can see, Algorithm A1 computes an attribution that is over 75% di erent from the one Figure 9: Length of revisions (in number of characters) of the article  Dance Dance Revolution  compare to length of json string with the trie summary.
Dips in the revision size indicate content deletions due to vandalism.
computed by Algorithm A0.
This is due to the fact that Algorithm A0 considers new any content that is reinserted after a deletion.
As an example, Figure 9 plots the size of the revisions, and summary trie, for the Wikipedia article on  Dance Dance revolution ; the frequent dips in revision size correspond to content deletions by vandals.
From Figure 8 we see also that the attribution di erence between algorithms A1 and A2 is of only a few percentage points, when the length of minimally interesting matches is 3 or more.
The advantage of Algorithm A1 over A2 lies in its e cient implementation.
Size of trie and su x tree summaries.
Figure 10 plots the ratio between the size of the trie serialized in Json, and the average size of the last 10 revisions, for aging values  N = 100 and  T = 90 days and dataset B.
We use the average size of the last 10 revisions, rather than the size of the last revision, to avoid very large spikes in the ratio when the content of a revision is deleted by vandals.
The average ratio is approximately 10; the ratio can be reduced to about 3 by compressing the trie serializations with gzip.
This is a very practical amount of storage, which is dwarfed in the English Wikipedia by the amount of storage required to store all revisions of every page.
the average size of the last 10 revisions.
Figure 11: Size comparison between trie summaries for A3 and su x tree summaries for A2.
In Figure 11 we compare the size of the trie summaries used by Algorithm A3, with the size of the su x tree summaries used in implementing Algorithm A2.
Dataset B was used, and no content aging was applied, to make the comparison fair.
The trie sizes are tied to the change between revisions, and since we discard text that has been dead for long, they tend to be a constant multiple of the revision size.
On the other hand, the su x tree sizes are proportional to the total size of past revisions.
Time performance.
Figure 12 summarizes the time performance of Algorithm A3, as a function of revision size.
In the  gure, the algorithm time is the time required by steps
 is the time required for serializing and deserializing the trie into json.
As we see, these two times are of the same order of magnitude, indicating that there is limited scope for improvement by optimizing the implementation of steps 3 21.
The  gure was obtained using dataset A.
We have considered so far revisioned content that consists in a single revisioned entity.
Most revisioned content, however, consists of multiple entities: a national Wikipedia consists of a set of pages, each of which is versioned, and a code repository similarly consists of multiple  les, each revisioned.
Furthermore, in modern revisioning systems such as git (http://git-scm.com), the various revisions are organized in branches.
Since code is commonly copied across  les, and to a lesser extent, content is moved across Wikipedia pages, an origin analysis that spans a whole repository is often desirable.
We can perform such repository-wide analysis with the algorithms we discussed in this paper, by considering the stream of all revisions  0,  1,  2, .
.
.
in the order they are created, regardless of the entity (page, or  le, or branch) to which they belong.
The content of each new revision will be compared with all previous content, assigning origin via Figure 12: Time performance of algorithm A3.
Each point in the plot represents an article, with the average revision size on the X axis.
The times required by attribution computation, and trie serialization and deserialization, are reported on the Y axis.
matching with corresponding occurrences.
The algorithms could be improved by considering as more likely the matches that relate di erent versions of the same entity, as compared to matches that relate di erent entities.
For software repositories, which are of moderate size, and where revisions are typically generated at low speed (even large industrial code bases have intervals between revisions of several seconds), such a global origin analysis would be feasible.
In the English Wikipedia, however, several revisions per second may be created.
From our experimental data, the size of a global summary would about ten times the cumulative size of the most recent revisions of all pages, leading to a size of approximately one terabyte.
This size exceeds the RAM memory easily available in a single, low-cost host.
The design of a system capable of comparing, in real time, every revision of Wikipedia with the whole of its past history would be challenging, and the result expensive to operate.
For this reason, in our implementation we have opted to compare new revisions only with the previous content of the page to which the revisions belong.
If required, we will address content moved across pages via specialized tools.
Compared to the algorithm of [6], the one presented here o ers a simple mathematical de nition of authorship, is applicable to any revisioned content, comes with compexity bounds and robustness characterization, and is well-suited to an implementation in which the authorship information needs to be computed online, as revisions are made.
Unfortunately, we became aware of the work of [6] too late to be able to present here a quantitative comparison of how well the two algorithms match a human perception of authorship on the Wikipedia.
This work was supported in part by the NIH award
 content is solely the responsibility of the authors and does not necessarily represent the o cial views of the NIH.
