Crowdsourcing techniques have recently gained in popularity as they allow to build hybrid human-machine in-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
formation systems that combine scalable algorithms with the power of human intelligence to complete tasks that are di cult to tackle for current machines.
Examples of such systems include hybrid database systems [12] that use the crowd to answer SQL queries with subjective ranking criteria such as picture attractiveness, hybrid linking approaches to relate unstructured text to structured entities [10], or data integration systems [21].
Crowdsourcing approaches must provide incentive schemes to motivate the crowd to perform a given Human Intelligence Task (HIT).
While crowdsourc-ing games (also known as games with a purpose) exploit the fun incentive (see, for example, the ESP Game [25]), the most widely adopted incentive is  nancial, that is, to grant a small economic reward to each human worker that completes a task on the crowdsourcing platform.
Paid crowdsourcing is commonly run on top of platforms such as Amazon Mechanical Turk1 (AMT), which provides programmatic APIs as well as a Web interface for requesters to design and deploy online tasks.
Such platforms adopt a pull methodology where tasks published by requesters are available on specialized Web sites where workers can pick their preferred tasks on a  rst-come- rst-served basis.
This approach has several advantages including simplicity and minimization of task completion times, since any available worker from the crowd can pick and perform any HIT.
However, this mechanism does not guarantee that the worker who performs the task is the best  t: More knowledgeable workers may be available in the crowd but they might be unable to pick the HIT if they were not quick enough.
In this paper, we propose and extensively evaluate Pick-A-Crowd, a software architecture to crowdsource micro-tasks based on pushing tasks to speci c workers.
Our system constructs user models for each worker in the crowd in order to assign HITs to the most suitable available worker.
We build such worker pro les based on information available on social networks using, for instance, information about the worker personal interests.
The underlying assumption is that if a potential worker is interested in several speci c categories (e.g., movies), he/she will be more competent at tackling HITs related to that category (e.g., movie genre classi cation).
In our system, workers and HITs are matched based on an underlying taxonomy that is de ned on categories extracted both from the tasks at hand and from the work-1http://www.mturk.com
 are linked to the Linked Open Data (LOD) cloud2, where they are then matched to related tasks that are available on the crowdsourcing platform.
We experimentally evaluate our pull methodology and compare it against traditional crowdsourcing approaches using tasks of varying types and complexity.
Results show that the quality of the answers is signi cantly higher when using a push methodology.
In summary, the contributions of this paper are:   a novel Crowdsourcing framework that focuses on pushing HITs to the crowd;   a software architecture that implements the newly proposed push crowdsourcing methodology;   category-based, text-based, and graph-based approaches to assign HITs to workers based on links in the LOD cloud;   an empirical evaluation of our method in a real deployment over di erent crowds showing that our Pick-A-Crowd system is on average 29% more e ective than traditional pull crowdsourcing platforms over a variety of HITs.
The rest of this paper is structured as follows: We review the state of the art in Crowdsourcing, Recommender Systems, and Expert Finding in Section 2.
Section 3 gives an overview of the architecture of our system, including its HIT publishing interface, its crowd pro ling engine, and its HIT assignment and reward estimation components.
We introduce our formal model to match human workers to HITs using category-based, text-based, and graph-based approaches in Section 4.
We describe our evaluation methodology and discuss results we obtained from a real deployment of our system in Section 5, before concluding in Section 6.
Crowdsourcing.
Crowdsourcing has recently gained much attention from di erent research communities.
In the database community, hybrid human-machine database systems have been proposed [12, 23].
In those systems, crowdsourcing is for example used to explain null values in returned results, or to de ne subjective  ORDER BY  operators that allow to express queries such as  I want pictures for motivational slides .
In the Information Retrieval community, crowdsourcing has been mainly used for e ectiveness evaluation purposes.
Relevance judgements used to evaluate the results of a search engine can now be created by asking the crowd about the relevance of a document or, more generally, of a retrieved resource.
Crowdsourcing can be used to produce relevance judgements for documents [2], books [16, 17], or entities [5].
In the Semantic Web community, crowdsourcing has also been recently considered, for instance to link [10] or map [21] entities.
In both cases, the use of crowdsourcing can significantly improve the quality of generated links or mappings as compared to purely automatic approaches.
In the context of Natural Language Processing, games to crowdsource the Word Sense Disambiguation task[22] have recently been proposed.
2http://linkeddata.org/ An incentive that if often leveraged to get input from the crowd is fun.
Games with a purpose have studied how to design entertaining applications that can generate useful data to be processed by further algorithms.
An example of a successful game that at the same time generates meaningful data is the ESP game [25] where two human players have to agree on the words to use to tag a picture.
An extension of this game is Peekaboom: a game that asks the player to detect and annotate speci c objects within an image [26].
A successful crowdsourcing application is Recaptcha [27], which generates captcha codes that human users have to type on their machines and which contain scanned words (from books) that would be otherwise complex to identify by means of automated OCR approaches.
Thus, by entering valid captcha codes, human users help to digitalize a large amount of textual content only available on paper.
Designing such games makes high-quality data available at no cost.
Another incentive that is often simpler to put in place is the  nancial incentive: A small monetary reward is granted to workers who perform a simple task online.
While in this way it gets easier to recruit a larger crowd, this raises questions about the quality of the results.
Indeed, quality control is a common issue of paid crowdsourcing platforms, given the presence of malicious workers whose intent is to game the system to obtain the monetary reward without properly completing the task.
Approaches to control quality by identifying low-quality workers and, thus, stopping them from performing further work have already been proposed [16, 10].
Such approaches use either majority agreement or a set of known answers to check for errors and to identify workers who make many mistakes.
However, such approaches require workers to complete several tasks in order to be evaluated.
Instead, our system is able to model workers before they complete any HIT on the platform and thus assign tasks exclusively to those who are known to show interest in the task.
A  rst attempt to crowdsource micro-tasks on top of social networks has been proposed by [11], where authors describe a framework to post questions as tweets that users can solve by tweeting back an answer.
As compared to this early approach, we propose a more controlled environment where workers are known and pro led in order to push tasks to selected social network users.
Crowdsourcing over social networks is also used by Crowd-Searcher [6, 7, 8], which improves automatic search systems by means of asking questions to personal contacts.
The crowdsourcing architecture proposed in [9] considers the problem of assigning tasks to selected workers.
However, authors do not evaluate automatic assignment approaches but only let the requesters manually select individual workers, which they want to push the task to.
In this paper instead, we assess the feasibility and e ectiveness of automatically mapping HITs to workers based on their social network pro les.
Also related to our system is the study of trust in social networks.
Golbeck [13], for instance, proposes di erent models to rank social network users based on trust and applies them to recommender systems as well as other end-user applications.
Recommender Systems.
Assigning HITs to workers is similar to the task performed by recommender systems (e.g., recommending movies to buy
 taken as input by the system, which creates HITs, estimates their di culty and suggests a fair reward based on the skills of the crowd.
HITs are then pushed to selected workers and results get collected, aggregated, and  nally returned back to the requester.
to potential customers).
We can categorize recommender systems into content-based and collaborative  ltering approaches.
The former approaches exploit the resources contents and match them to user interests.
The latter ones only use similarity between user pro les constructed out of their interests (see [19] for a survey).
Recommended resources are those already consumed by similar users.
Our systems adopts techniques from the  eld of recommender systems as it aims at matching HITs (i.e., tasks) to human workers (i.e., users) by constructing pro les that describe worker interests and skills.
Such pro les are then matched to HIT descriptions that are either provided by the task requester or by analyzing the questions and potential answers included in the task itself (see Section 4).
Recommender systems built on top of social networks already exits.
For example, in [1], authors propose a news recommendation system for social network groups based on community descriptions.
Expert Finding.
In order to push tasks to the right worker in the crowd, our system aims at identifying the most suitable person for a given task.
To do so, our Worker Pro le Selector component generates a ranking of candidate workers who can be contacted for the HIT.
This is highly related to the task of Expert Finding studied in Information Retrieval.
The Enterprise track at the TREC evaluation initiative3 has constructed evaluation collections for the task of expert  nding within an organizational setting [4].
The studied task is that of ranking candidate experts (i.e., employees of a company) given a keyword query describing the required expertise.
Many approaches have been proposed for such tasks 3http://trec.nist.gov (see [3] for a comprehensive survey).
We can classify most of them as either document-based, when document ranking is performed before identifying the experts, or as candidate-based, when expert pro les are  rst constructed before being ranked given a query.
Our system follows the former approach by ranking online social network pages and using them to assign work to the best matching person.
In this section, we describe the Pick-A-Crowd framework and provide details on each of its components.
Figure 1 gives a simpli ed overview of our system.
Pick-a-Crowd receives as input tasks that need to be completed by the crowd.
The tasks are composed of a textual description, which can be used to automatically select the right crowd for the task, actual data on which to run the task (e.g., a Web form and set of images with candidate labels), as well as a monetary budget to be spent to get the task completed.
The system then creates the HITs, and predicts the di culty of each micro-task based on the crowd pro les and on the task description.
The monetary budget is split among the generated micro-tasks according to their expected di culty (i.e., a more di cult task will be given a higher reward).
The HITs are then assigned to selected workers from the crowd and published on the social network application.
Finally, answers are processed as a stream from the crowd, aggregated and sent back to the requester.
We detail the functionalities provided by each component of the system in the following.
Reward Estimation The  rst pipeline in the system is responsible for generating the HITs given some input data provided by the requester.
HITs can for instance be generated from i) a Web template to classify images in pre-de ned categories, together with ii) a set of images and iii) a list of pre-de ned categories.
The HIT Generator component dynamically creates as many tasks as required (e.g., one task per image to categorize) by combining those three pieces of information.
Next, the HIT Di culty Assessor takes each HIT and determines a complexity score for it.
This score is computed based on both the speci c HIT (i.e., description, keywords, candidate answers, etc.)
and on the worker pro les (see Section 4 for more detail on how such pro les are constructed).
Di erent algorithms can be implemented to assess the dif- culty of the tasks in our framework.
For example, a text-based approach can compare the textual description of the task with the skill description of each worker and compute a score based on how many workers in the crowd could perform well on such HITs.
An alternative a more advanced prediction method can exploit entities involved in the task and known by the crowd.
Entities are extracted from the textual descriptions of the tasks and disambiguated to LOD entities.
The same can be performed on the worker pro les: each Facebook page that is liked by the workers can be linked to its respective LOD entities.
Then the set of entities representing the HITs and the set of entities representing the interests of the crowd can be directly compared.
The task is classi ed as di cult when the entities involved in the task heavily di er from the entities liked by the crowd.
A third example of task di culty prediction method is based on Machine Learning.
A classi er assessing the task di culty is trained by means of previously completed tasks, their description and their result accuracy.
Then, the description of a new task is given as a test vector to the classi- er, which returns the predicted di culty for the new task.
Finally, the Reward Estimation component takes as input a monetary budget B and the results of the HIT assessment to determine a reward value for each HIT hi.
A simple way to redistribute the available monetary budget is simply by rewarding the same amount of money for each task of the same type.
An second example of reward estimation function is: reward(hi) = (1) (cid:80) B   d(hi) j d(hj) which takes into account the di culty d() of the HIT hi as compared to the others and assigns a higher reward to more di cult tasks.
A third approach computes a reward based on both the speci c HIT as well as the worker who performs it.
In order to do this, we can exploit the HIT assignment models adopted by our system.
These models generate a ranking of workers by means of computing a function match(wj, hi) for each worker wj and HIT hi (see Section 4).
Given such a function, we can assign a higher reward to better suited workers by reward(hi, wj) = (2) (cid:80) B   match(wj, hi) k,l match(wk, hl) example, in [15], authors propose game theoretic based approaches to compute the optimal reward for paid crowd-sourcing incentives in the presence of workers who collude in order to game the system.
Exploring and evaluating di erent di culty prediction and reward estimation approaches is not the focus of this paper and is left as future work.
The task of the Crowd Pro ler component is to collect information about each available worker in the crowd.
Pick-A-Crowd uses contents available on the social network platform as well as previously completed HITs to construct the workers  pro les.
Those pro les contain information about the skills and interests of the workers and are used to match HITs with available workers in the crowd.
In detail, this module generates a set of worker pro les C = {w1, .., wn} where wi = {P, T}, P is the set of worker interests (e.g., when applied on top of the Facebook platform pi   P are the Facebook pages the worker likes) and Ti = {t1..tn} is the set of tasks previously completed by wi.
Each Facebook page pi belongs to a category in the Facebook Open Graph4.
This component is responsible for linking each Facebook page liked by some worker to the corresponding entity in the LOD cloud.
Given the page name and, possibly, a textual description of the page, the task is de ned as identifying the correct URI among all the ones present in the LOD graph using, for example, a similarity measure based on adjacent nodes in the graph.
This is a well studied problem where both automatic [14] or crowdsourcing-based techniques [10] can be used.
HITs and workers are matched based on the pro les described above.
Intuitively, a worker who only likes many music bands will not be assigned a task that asks him/her to identify who is the movie actor depicted in the displayed picture.
The similarity measure used for matching workers to tasks takes into account the entities included in the workers  pro les but is also based on the Facebook categories their liked pages belong to.
For example, it is possible to use the corresponding DBPedia entities and their YAGO type.
The YAGO knowledge-base provides a  ne-grained high-accuracy entity type categorization which has been constructed by combining Wikipedia category assignments with WordNet synset information.
The YAGO type hierarchy can help the system better understand which type of entity correlates with the skills required to e ectively complete a HIT (see also Section 4 for a formal de nition of such methods).
For instance, our graph-based approach concludes that for our music related task, the top Facebook pages that indicate expertise on the topic are  MTV  and  Music & top artists .
A generic similarity measure to match workers and task More advanced reward schemes can be applied as well.
For 4https://developers.facebook.com/docs/opengraph/ 370is in equation 3 sim(wj = {P, T}, hi = {t, d, A, Cat}) = (cid:80) k,l sim(pk, al)
  pk   P, al   A (3) where A is the set of candidate answers for task hi and sim() measures the similarity between the worker pro le and the task description.
The HIT Assigner component takes as input the  nal HITs with the de ned reward and publishes them onto the Facebook App.
We developed a dedicated, native Facebook App called OpenTurk5 to implement this  nal component of the Pick-A-Crowd platform.
Figure 2 shows a few screenshots of OpenTurk.
As any other application on the Facebook platform, it has access to several pieces of information about the users that accept to use it.
We follow a non-intrusive approach; In our case, the liked pages for each user are stored in an external database that is used to create a worker pro le containing his/her interests.
The application we developed also adopts crowdsourcing incentive schemes di erent than the pure  nancial one.
For example, we use the fan incentive where a competition involving several workers competing on trivia questions on their favorite topic can be organized.
The app also allows to directly challenge other social network contacts by sharing the task, which is also helpful to enlarge the application user base.
While from the worker point of view this represents a friendly challenge, from a platform point of view this means that the HIT will be pushed to another expert worker, following the assumption that a worker would challenge someone who is also knowledgeable about the topic addressed by the task.
The  nal pipeline is composed of stream processing modules, where the Facebook App answers are being streamed from the crowd to the answer creation pipeline.
The  rst component collects the answers from the crowd and is responsible for a  rst quality check based on potentially available gold answers for a small set of training questions.
Then, answers that are considered to be valid (based on available ground-truth data) are forwarded to the HIT Result Aggre-gator component, which collects and aggregates them in the  nal answer for the HIT.
When a given number of answers has been collected (e.g.,  ve answers), then the component outputs the partial aggregated answer (e.g., based on majority vote) back to the requester.
As more answers reach the aggregation component, the aggregated answer presented to the requester gets updated.
Additionally, as answers are collected, the workers  pro les get updated and the reward gets granted to the workers who performed the task through the Facebook App.
In this section, we de ne the HIT assignment tasks and describe several approaches for assigning workers to such tasks.
We focus on HIT assignment rather than on other 5http://apps.facebook.com/openturk/ Figure 2: Screenshots of the OpenTurk Facebook App.
Above, the dashboard displaying HITs assigned to a speci c worker.
Below, a HIT about actor identi cation assigned to a worker who likes several actors.
system components as the ability to assign tasks automatically is the most original feature of our system as compared to other crowdsourcing platforms.
Given a HIT hi = {ti, di, Ai, Cati} from the requester, the task of assigning it to some workers is de ned as ranking all available workers C = {w1, .., wn} on the platform and selecting the top-n ranked workers.
A HIT consists of a textual description ti (e.g., the task instruction which is being provided to the workers)6, a data  eld di that is used to provide the context for the task to the worker (e.g., the container for an image to be labelled), and, optionally, the set of candidate answers Ai = {a1, .., an} for the multiple-choices tasks (e.g, a list of music genres used to categorize a singer) and a list of target Facebook categories Cati = {c1, ..cn}.
A worker pro le wj = {P, T} is assigned a score based on which it is ranked for the task hi.
This score is determined based on the likelihood of matching wj to hi.
Thus, the goal is to de ne a scoring function match(wj, hi) based on the worker pro le, the task description and, possibly, external resources such as the LOD datasets or a taxonomy.
The  rst approach we de ne to assign HITs to workers is based on the same idea that Facebook uses to target adver-
be de ned as the data context of the HIT.
For example, in crowdsourced databases ti can be the name of the column, table, etc.
the HIT is about.
worker as he likes the most pages related to the query.
tisements to its users.
A requester has to select the target community of users who should perform the task by means of selecting one or more Facebook pages or page categories (in the same way as someone who wants to place an ad).
Such categories are de ned in a 2 levels structure with 6 top levels (e.g.,  Entertainment ,  Company ), each of them having several subcategories (e.g.,  Movie ,  Book ,  Song , etc.
are subcategories of  Entertainment ).
Once some second-level categories are selected by the requester, the platform can generate a ranking of users based on the pages they like.
More formally, given a set of target categories Cat = {c1, ..cn} from the requester, we de ne P (ci) = {p1, .., pn} as the set of pages belonging to category ci.
Then, for each worker wj   C we take the set of pages he/she likes P (wj) and measure its intersection with the pages belonging to any category selected by the requester RelP =  iP (ci).
Thus, we can assign a score to the worker based on the overlap between the likes and the target category |P (wj)   RelP| and rank all wj   C based on such scores.
A second approach we propose to rank workers given a HIT hi is to follow an expert  nding approach.
Speci cally, we de ne a scoring function based on the Voting Model for expert  nding [18].
For the HIT we want to assign, we take the set of its candidate answers Ai, when available.
Then, we de ne a disjunctive keyword query based on all the terms composing the answers q =  iai.
In case Ai is not available, for example because the task is asking an open-ended question, then q can be extracted out of ti by mining entities mentioned in its content.
The query q is then used to rank Facebook pages using an inverted index built over the collection of documents  iPi  wj   C. We consider each ranked page as a vote for the workers who like them on Facebook and rank workers accordingly.
That is, if RetrP is the set of pages retrieved with q, we can de ne a worker ranking function as |P (wj)   RetrP|.
More interestingly, we can take into account the ranking generated by q and give a higher score to workers liking pages that were ranked higher.
An example of how to rank workers following the voting model is depicted in Figure 3.
The third approach we propose is based on third-party information.
Speci cally, we  rst link candidate answers and pages to an external knowledge base (e.g., DBPedia) and exploit its structure to better assign HITs to workers.
For a given HIT hi, the  rst step is to identify the entity corresponding to each aj   Ai (if Ai is not available, entities in ti can be used instead).
This task is related to entity linking [10] and ad-hoc object retrieval [20, 24] where the goal is to  nd the correct URI for a description of the entity using keywords.
In this paper, we take advantage of state-of-the-art techniques for this task but do not focus on improving over such techniques.
Then, we identify the entity that represents each page liked by the crowd whenever it exists in the knowledge base.
Once both answers and pages are linked to their corresponding entity in the knowledge base, we exploit the underlying graph structure to determine the extent to which entities that describe the HIT and entities that describe the interests of the worker are similar.
Speci cally, we de ne two scoring methods based on the graph.
The  rst scoring method takes into account the vicinity of the entities in the entity graph.
We measure how many worker entities are directly connected to HIT entities using SPARQL queries over the knowledge base as follows: SELECT ?x WHERE { <uri(a_i)> ?x <uri(p_i)> }.
This follows the assumption that a worker who likes a page is able to answer questions about related entities.
For example, if a worker likes the page  FC Barcelona , then he/she might be a good candidate worker to answer a question about  Lionel Messi  who is a player of the soccer team liked by the worker.
Our second scoring function is based on the type of entities.
We measure how many worker entities have the same type as the HIT entity using SPARQL queries over the knowledge base as follows:
 WHERE { <uri(a_i)> <rdf:type> ?x .
<uri(p_i)> <rdf:type> ?x }.
The underlying assumption in that case is that a worker who likes a page is able to answer questions about entities of the same type.
For example, if a worker likes the pages  Tom Hanks  and  Julia Roberts , then he/she might be a good candidate worker to answer a question about  Meg Ryan  as it is another entity of the same type (i.e., actor).
Given that the main innovation of Pick-A-Crowd as compared to classic crowdsourcing platforms such as AMT is the ability to push HITs to workers instead of letting the workers select the HITs they wish to work on, we focus in the following on the evaluation and comparison of di erent HIT assignment techniques and compare them in terms of work quality against a classic crowdsourcing platform.
The Facebook app OpenTurk we have implemented within the Pick-A-Crowd framework currently counts more than
 taining popular or less popular entities and to answer open-ended questions.
Overall, more than 12K distinct Facebook pages liked by the workers have been crawled over the Facebook Open Graph.
OpenTurk is implemented using cloud-based storage and processing back-end to ensure scalability with an increasing number of workers and requesters.
Open-Turk workers have been recruited via AMT, thus making a direct experimental comparison to standard AMT techniques more meaningful.
The type of task categories we evaluate our approaches on are: actors, soccer players, anime characters, movie actors, movie scenes, music bands, and questions related to cricket.
Our experiments cover both multiple answer questions as well as open-ended questions: Each task category includes 50 images for which the worker either has to select the right answer among 5 candidate answers or to answer 20 open-ended questions related to the topic.
Each type of question can be skipped by the worker in case he/she has no idea about that particular topic.
In order to analyze the performance of workers in the crowd, we measure Precision, Recall (as the worker is allowed to skip questions when he/she does not know the answer), and Accuracy of their answers for each HIT obtained via majority vote over 3 and 5 workers7.
As we can see from Figure 4, the HITs that asks questions about cricket clearly show how workers can perform di er-ently in terms of accuracy.
There are 13 workers out of 35 who were not able to provide any correct answer while the others spread over the Precision/Recall spectrum, with the best worker performing at 0.9 Precision and 0.9 Recall.
This example motivates the need to selectively assign the HIT to the most appropriate worker and not following a  rst-come- rst-served approach as proposed, for example, by AMT.
periments are available for comparative studies online at: http://exascale.info/PickACrowd Figure 4: Crowd performance on the cricket task.
Square points indicate the 5 workers selected by our graph-based model that exploits entity type information.
Figure 5: Crowd performance on the movie scene recognition task as compared to movie popularity.
Thus, the goal of Pick-A-Crowd is to adopt HIT assignment models that are able to identify the workers in the top-right area of Figure 4, based solely on their social network pro le.
As an anecdotal observation, a worker from AMT provided as feedback to the cricket task in the available comment  eld the following comment  I had no idea what to answer to most questions...  which clearly demonstrates that for the tasks requiring background knowledge, not all workers are a good  t.
An interesting observation is the impact of the popularity of a question.
Figure 5 shows the correlation between task accuracy on the movie scene recognition task and the popularity of the movie based on the overall number of Facebook likes on the IMDB movie page.
We can observe that when a movie is popular, then workers easily recognize it.
On the other hand, when a movie is not so popular it becomes more di cult to  nd knowledgeable workers for the task.
Figure 6 shows some statistics about the user base of OpenTurk.
The majority of workers are in the age interval
 Figure 8: OpenTurk Crowd Accuracy as compared to the number of relevant Pages a worker likes.
observation can be made about the Facebook Noti cation click rate.
Once the Pick-A-Crowd system selects a worker for a HIT, the Facebook app OpenTurk sends a noti cation to the worker with information about the newly available task and its reward.
Figure 7 shows a snapshot of the noti- cations clicked by workers as compared to the noti cation sent by OpenTurk over a few days.
We observe an average rate of 57% clicks per noti cation sent.
Figure 7: OpenTurk Noti cation click rate.
A third analysis looks on how the relevant likes of a worker correlates with his/her accuracy for the task.
Figure 8 shows a distribution of worker accuracy over the relevant pages liked using the category-based HIT assignment model to de- ne the relevance of pages.
In a  rst look, we do not see a perfect correlation between the number of likes and the worker accuracy for any task.
On the other hand, we observe that when many relevant pages are in the worker pro le (e.g., >30), then accuracy tends to be high (i.e., the bottom-right part of the plot is empty).
However, when only a few relevant pages belong to the worker pro le, then it becomes di cult to predict his/her accuracy.
Note that not-liking relevant pages is not an indication of being unsuitable for a task: Having an incomplete pro le just does not allow to model the worker and to assign him/her the right tasks (i.e., the top-left part of the plot contains high-accuracy workers with incomplete pro les).
Having worker pro les containing several relevant pages is not problematic when the crowd is large enough (as it is on Facebook).
Table 1: A comparison of the task accuracy for the AMT HIT assignment model assigning each HIT to the  rst 3 and 5 workers and to Amazon MTurk Masters.
Task Soccer Actors Music Book Authors Movies Anime Cricket AMT 3 AMT 5 AMT Masters 3





















 In the literature, common crowdsourcing tasks usually adopt 3 or 5 assignments of the same HIT in order to aggregate the answers from the crowd, for example by majority vote.
In the following, we compare di erent assignment models evaluating both the cases where 3 and 5 assignments are considered for a given HIT.
As a baseline, we compare against the AMT model that assigns the HIT to the  rst n workers performing the task.
We also compare against AMT Masters who are workers being awarded a special status by Amazon based on their past performances8.
Our proposed models  rst rank workers in the crowd based on their estimated accuracy and then assign the task to the top-3 or top-5 workers.
Table 1 presents an overview of the performances of the assignment model used by AMT.
We observe that while on average there is not a signi cant di erence between using
 AMT crowd on some tasks but do not outperform the crowd on average (0.54 versus 0.66 Accuracy).
A per-task analysis shows that some tasks are easier than others: While tasks about identifying pictures of popular actors obtain high accuracy for all three experiments, topic-speci c tasks such as cricket questions may lead to a very low accuracy.
Table 2 gives the results we obtained by assigning tasks based on the Facebook Open Graph categories manually selected by the requester.
We observe that the Soccer and
 tasks we had to reward $1.00 per task as compared to $0.25 granted to standard workers.
to 3 and 5 workers with manually selected categories.
Task Soccer Actors Music Book Authors Movies Anime Cricket Requester Selected Categories Sport,Athlete,Public  gure Tv show, Comedian, Movie, Artist, Actor/director Musician/band,Music Author,Writer,Book Movie,Movie general,Movies/music Games/toys,Entertainment Sport,Athlete,Public  gure Category-based 3 Category-based 5













 Table 3: E ectiveness for di erent HIT assignments based on the Voting Model assigning each HIT to 3 and
 q = Ai respectively.
Task Soccer Actors Music Book Authors Movies Anime Cricket VotingModel q = ti 3 VotingModel q = ti 5 VotingModel q = Ai 3 VotingModel q = Ai 5



























 Table 4: E ectiveness for di erent HIT assignments based on the entity graph in the DBPedia knowledge base assigning each HIT to 3 and 5 workers.
Table 5: Average Accuracy for di erent HIT assignment models assigning each HIT to 3 and 5 workers.
Task Soccer Actors Music Book Authors Movies Anime Cricket En.
type 3 En.
type 5 1-step 3 1-step 5



























 Cricket tasks have been assigned to the same Facebook category which does not distinguish between di erent types of sports.
Anyhow, we can see that for the cricket task the category-based method does not perform well, as the pages contained into the categories cover many di erent sports and, according to our crowd at least, soccer-related tasks are simpler than cricket-related tasks.
Table 3 presents the results when assigning HITs following the Voting Model for expert  nding.
We observe that in the majority of cases, assigning each task to 5 di erent workers selected using the Facebook Page indexing the task description as query leads to the best results.
Table 4 shows the results of our graph-based approaches.
We observe that in the majority of these cases, the graph-based approach that follows the entity type ( En.
type ) edges and selects workers who like Pages of the same type as the entities involved in the HIT outperforms the approach Assignment Method Average Accuracy

 AMT Masters 3 Category-based 3 Category-based 5 Voting Model ti 3 Voting Model ti 5 Voting Model Ai 3 Voting Model Ai 5 En.
type 3 En.
type 5 1-step 3 1-step 5












 that considers the directly-related entities within one step in the graph ( 1-step ).
Table 5 presents the average Accuracy obtained over all the HITs in our experiments (which makes a total of 320 questions) by each HIT assignment model.
As we can see, our proposed HIT assignment models outperform the standard  rst-come- rst-served model adopted by classic crowd-sourcing platforms such as Amazon MTurk.
On average over the evaluated tasks, the best performing model is the one based on the Voting Model de ned for the expert  nd-ing problem where pages relevant to the task are seen as votes for the expertise of the workers.
Such an approach ob-375tains on average a 29% relative improvement over the best accuracy obtained by the AMT model.
Crowdsourcing is a popular new means of performing large-scale, high-quality Human Intelligence tasks.
The long-term vision of current crowdsourcing research is to create hybrid human-machine systems capable of chimeric functionalities, which were not conceivable just few years ago.
One of the key impediments towards that vision is to obtain high-quality answers from the crowd.
This is currently an open-issue, given the way crowdsourcing tasks are commonly advertised and run today: Tasks are posted on simple platforms where the  rst worker who is available will perform the task.
This simplistic task allocation procedure represents a clear threat towards more qualitative results, especially when we consider paid crowdsourcing tasks where the monetary reward granted to workers who complete tasks attracts malicious people willing to earn money without spending too much time and e orts on the actual task.
For all these reasons, we proposed in this paper Pick-A-Crowd, a system exploiting a novel crowdsourcing scheme focusing on pushing tasks to the right worker rather than letting the workers pull the tasks they wished to work on.
We proposed a novel crowdsourcing architecture that builds worker pro les based on their online social network activities and tries to understand the skills and interests of each worker.
Thanks to such pro les, Pick-A-Crowd is able to assign each task to the right worker dynamically.
To demonstrate and evaluate our proposed architecture, we have developed an deployed OpenTurk, a native Facebook application that pushes crowdsourced tasks to selected workers and collects the resulting answers.
We additionally proposed and extensively evaluated HIT assignment models based on 1) Facebook categories manually selected by the task requester, 2) methods adapted from an expert  nding scenario in an enterprise setting, and 3) methods based on graph structures borrowed from external knowledge bases.
Experimental results over the OpenTurk user-base show that all of the proposed models outperform the classic  rst-come- rst-served approach used by standard crowdsourcing platforms such as Amazon Mechanical Turk.
Our best approach provides on average 29% better results than the Amazon MTurk model.
A potential limitation of our approach is that it may lead to longer task completion times: While on pull crowdsourc-ing platforms the tasks gets completed quickly (since any available worker can perform the task), following a push methodology may lead to delays in the completion of the tasks.
We anyway believe that this would be an acceptable tradeo , as crowdsourcing focuses on obtaining high-quality answers rather than real-time data from the crowd.
As future steps, we would like to test various HIT assignment models based on Machine Learning techniques where, given a few tasks with known answers used as training data, speci c worker features (e.g., education level) could be learnt and leveraged to match tasks to workers with a high accuracy.
Acknowledgments.
We thank the anonymous reviewers for their helpful comments.
This work was supported by the Swiss National Science Foundation under grant number PP00P2 128459.
