In recent years, Collaborative Filtering (CF) algorithms have become the hallmark of web-based recommender systems.
These techniques compute personalised recommendations for users by learning from the ratings or interactions that the users create as they engage with the (movie, music, news) content on the system [12, 28].
While traditional research on recommender systems has focused on  nding Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
relevant items for single users, there is an increasing occurrence of individual web pro les and accounts being shared amongst a group of people.
For example, a household of users may share a single movie-rental and recommendation account, or users of mobile recommender systems may be seeking locations (e.g., restaurants) for a group of friends to go to together.
Both of these types of scenarios have led to recent research on group recommendation [16].
Group recommendation scenarios tend to di er in terms of how preference data about the group is collected.
For example, in the case of shared household web accounts, the ratings that the system represents as a single user may actually re ect a number of people s preferences; in the case of a restaurant-recommender system, multiple pro les may need to be considered simultaneously in order to suitably person-alise the system s recommendations.
Group recommendation systems have been used in various forms across the web to recommend news pages [22], holidays [19], music [8], and both TV programs and movies [1, 21, 29].
The main challenge behind these scenarios has been that of computing recommendations from a potentially diverse set of group members  ratings.
Past approaches have tackled this in one of two ways.
On the one hand, all group members  ratings can be folded into a single pro le, which can then be treated as a unique user that recommendations should be computed for.
Naturally, this approach may bias its output away from those group members that have the sparsest pro les.
Alternatively, personalised recommendations can be computed for each group member  rst, and the resulting set of ranked recommendations can be merged into a single list for the group using pre-de ned heuristics [3,
 dation comes from the strategies used to merge the ranked lists, without modelling the group as a whole or considering how individuals  preferences may di er when they become part of a group.
In this paper, we tackle the problem of group recommendation for households of users sharing a movie-rental account.
Households may have a varied number of members and a nonuniform distribution of ratings amongst any members.
We present a probabilistic model for group recommendation; this model combines the relevance of items to di er-ent group members with the relevance of items to the group as a whole.
In doing so, we show that higher quality recommendations can be computed by shifting the focus toward better modelling of group members, rather than merging individual s ranked recommendations.
In summary, we make the following contributions:
 to CF (Section 2) and describe the information matching model that can seamlessly compute the probability that an item will be relevant to a given user (Section 3.1) by reasoning on the user s preference and the item s appeal.
Furthermore, the information matching approach allows for users to be described both in terms of their ratings as well as any other contextual feature that is available to the system (e.g., their role in the household).
  We then use this model to derive a framework for group recommendation (Section 3.2) that, unlike previous work which focuses on merging recommendations computed for individual users uses the principles of information matching in order to compute the probabilities of items  relevance to a group, while taking the entirety of the group into consideration.
  We evaluate (Section 4) the probabilistic model alongside state-of-the-art CF approaches, including popularity based, neighbourhood, and latent factor models using household rating data from MoviePilot1.
We  nd that not only do state-of-the-art methods perform very poorly in this domain, but the probabilistic model performs demonstrably better over a range of di er-ent evaluation metrics.
We further provide non-group, baseline results using the MovieLens dataset to allow our work to be compared to the research literature.
We believe these results will be useful to practitioners includ-whose systems may have shared groups of users, ing: movie rental, e-commerce, and web gaming portals.
We close in Sections 5 and 6 by discussing the key di erences between information matching and CF alongside our future work.
In this section, we introduce the problem space that we address, as well as describing a number of algorithms that we will compare our proposed model to.
Collaborative  ltering (CF) [12, 28] is the mainstream approach to building web recommender systems: it is based on the assumption that, as users give ratings for items on a web site, their discovery of new items can be aided by learning from the ratings that other users have given.
Broadly speaking, CF uses a matrix of user-item ratings in order to compute predictions for those items that users have not rated; items can then be ranked, in descending order, by these predictions.
In the following sections, we describe a baseline and two state-of-the-art CF algorithms that we will compare against when we evaluate our approach.
First, we use a non-person-alised, popularity-based, baseline.
Next, we compare with two techniques that pervade the literature in the  eld: neigh-bourhood approaches (Section 2.2) and latent factor models (Section 2.3).
Broadly speaking, neighbourhood approaches are based on the similarity between users or items.
The latent factor model, instead, represents the user and item in the same latent space with a prede ned number of hidden dimensions.
This is generally a lower dimensional space and
 Recommendation Challenge http://2011.camrachallenge.com/2011 part as of the Context-Aware Movie
 at ACM RecSys the rating between the user-item pair is predicted by the proximity of the relevant latent factors.
Finally, we discuss how these algorithms have been used in conjunction with other heuristics in order to generate recommendations for groups (Section 2.4), and the methods and challenges related to evaluating group recommendations (Section 2.5).
The baseline that we test all methods against is a simple, non-personalised approach based on popularity.
The popularity of an item is de ned as being proportional to the number of ratings that it has received by any user; by counting ratings, we can rank items in descending order of popularity.
Although this model is very simple, in the experiments that follow we  nd that it is in-fact very e ective in terms of performance.
In the following, we will refer to this baseline as Pop-item.
The k-Nearest Neighbourhood algorithm has been used in recommender systems research since its inception [10].
There are two di erent  avours of the algorithm: user and item-based.
The user-based approach represents users as a sparse, high-dimensional vector of item ratings.
The item-based alternative, instead, represents items as sparse, high dimensional vectors of user ratings.
Both methods operate by measuring any similarity between the users or items in order to predict a score for unrated items.
In this work, we use the Apache Mahout library2 implementation of the user and the item-based methods, which uses the Pearson correlation to measure similarity.
Speci cally, the similarity between a user u and v is measured as: sim(u, v) = (cid:2)  i Iuv (ru,i    ru)(rv,i    rv)  i Iuv (ru,i    ru)
  i Iuv (rv,i    rv) (1)
 (cid:2) where Iuv is the set of items rated by both users u, v and  ru,  rv are the average rating of co-rated items of u and v respectively.
Once similarities have been computed, the predicted rating for a user-item pair is computed as the similarity-weighted average of the ratings that (in the item-based approach) the user has given to the k most similar items or (in the user-based approach) the ratings that the k most similar users have given to the item.
More formally, the predicted rating  r(u, i) for user u and item i, in the user-based approach, is computed with:  r(u, i) =  ru +  k j=1 sim(nj, u)(rnj ,i    rnj )  k j=1sim(nj, u) (2) where  ru is the mean rating of user u, and nj is the jth neighbour of u.
In the following experiments, we set the neighbourhood size k to 50 for both approaches.
Many recent recommender algorithms have been based on latent factor models [17].
The idea behind these models is to factorize the rating matrix into two lower rank matri-2http://mahout.apache.org/ (cid:3) (cid:4) 496ces, one capturing user factors (pu), and one for item fac- tors (qi).
Each user and item is represented over a  xed f dimensional feature space, where f is a low-dimensional parameter to the model.
A predicted rating for a user-item pair (u, i) is then computed with the inner product between the related factor vectors:  r(u, i) = puqT i (3) Majority of recent models learn the factor vectors using any available ratings and an objective function; in this work, we follow Koren et al.
[17] and learn the vectors  factors via a gradient descent objective function.
To be consistent with the literature, we will refer to this model as PureSVD.
It uses f = 150 features which are optimised by running over 60 iterations.
We use the exact implementation used in [4].
We also reproduced the results reported in [4] using the ranking evaluation strategies to verify the model before we conducted our experimentation.
All the above methods can be used to generate recommendations for an individual user.
In this section, we describe the strategies that have been historically implemented in order to produce group recommendations.
Broadly speaking, there are two main strategies used for group recommendation: (a) creating a group pro le by combining the individual members  preferences into a single pro le and (b) generating and then aggregating, into a single set of recommendations, lists for each individual member of the group [16, 3].
Merging Pro les.
If we have two users, who have respectively rated {i1, i2} and {i3, i4}, then their group pro le is simply {i1, i2, i3, i4}.
If, instead, there is an overlap in the group member s ratings, such as if they have rated {i1, i2} and {i1, i3}, then the merged pro le is {mean(i1), i2, i3}.
The resulting pro le can then be directly used in any of the CF algorithms as if it were a single user.
Merging Recommendations.
A variety of aggregation techniques have been proposed in the literature [18, 11, 16,
 approach seeks to minimise the probability that any one member of the group will strongly dislike the recommendations.
More formally, once we have generated a set of recommendation lists for members of a group G, we set the relevance score of any item i as the smallest relevance score from amongst the group s individual s relevance scores.
This means that the relevance of an item to a group is the least satis ed member s score.
Recent work has compared the two strategies: in [5], the authors found that the  rst approach marginally outperforms the second when using recipe ratings collected from a variety of families.
In [3], instead, concludes that the LM strategy outperforms a range of other techniques.
In this work, we use and test both strategies.
All of the above approaches tend to assume that peoples  preferences do not change across being alone or being in a group.
In this work, we will revisit this assumption by incorporating group membership into a probabilistic model: [2] also addressed this aspect of groups by explicitly integrating a group disagreement score into the groups  relevance rating score.
Empirically measuring the quality of recommendations has, in the past, fallen into two camps.
In the majority of cases, researchers have measured the accuracy of learning algorithms  predictions for hidden user ratings [17].
However, since those predictions will be used to rank items, ranking-based evaluations of CF have also been used [7]: in this work, we will focus on the latter, and our proposed model will be tailored towards ranking rather than predicting user ratings.
Evaluating the e ectiveness of group recommendation algorithms has followed similar approaches to broader CF evaluations, but has su ered from the lack of available data on group preferences.
Many studies have been conducted by synthetically creating di erent groups of various members based on user-similarities.
For example, [3] studied the performance of di erent rank aggregation strategies by creating groups of di erent users based on similarities and generating recommendation lists for individual members and aggregating the lists.
We overcome this by using a dataset that contains individual user preferences and their group membership.
This dataset, from the German movie-rental site MoviePilot, was released as part of the Context-Aware Movie Recommendation Challenge at ACM RecSys 20113 and has been recently used to, for example, identify active members in a household [6].
As we noted in the previous section, a key aspect of group recommendation that is missing is the notion of matching items to users as a group, rather than as a set of individuals.
In this section, we introduce a probabilistic model that seeks to address this problem.
We describe the model via two steps:  rst (Section 3.1), we summarise the concept of information matching, and how it can be used for personalised recommendations to individuals.
Then (Section 3.2), we augment the information matching model to include group relevance.
In the following section, we then proceed to evaluate the e ectiveness of this model against all of the algorithms that have been described in Section 2.
Information Matching Much like traditional CF, the information matching model reasons upon users and items [13].
However, it does not represent these entities as sparse vectors of the numerical ratings they have input or received.
Instead, each user and item is described with a set of binary features.
More formally, we de ne a vector E = { 1,  2,  ,  f} of user features, and a vector F = { 1,  2,  ,  e} of item features.
A user is represented as u   {0, 1}f , or an f dimensional binary feature vector over E, where  k = 1 if u is described by the kth feature in E. Similarly, each item i is represented with an e-dimensional binary vector over F .
As before, each  l = 1 if i is described using lth feature in F .
The model makes no initial assumptions about what kinds of features exist: in practice, E and F may di er.
In our case, individual users are described by item-features (i.e., E is the item space), and we treat users as features for the items (F is the set of users).
We assume that, as implicitly de ned within any domain of recommendation, there exist directional relevance relationships between the users and items.
User features have 3http://2011.camrachallenge.com/call-for-papers/ 497a preference toward item features: for example, a user de- scribed with a  child  feature may prefer items that have the  cartoon  feature.
Similarly, item features have an appeal towards particular user features: as before, an item described with the  cartoon  feature may appeal to those users with the  child  feature.
We formally de ne the binary preference matrix as:             .
.
.
      .
.
.
           e
 ...
 l
 ...
Similarly, the binary appeal matrix is de ned as:
  1 ...
 f  1
 ...
 2
 ...
 1 ...
 e  1
 ...
 2
 ...
We note that M does not necessarily equal N T .
For example, a user described with feature  student  may have a preference for a car described with the  luxury item  feature, but the car described with the  luxury item  may not have appeal to users with the  student  feature4.
In practice, the model assumes that varying degrees of uncertainty exist on features that describe u and i.
Thus, we say that there is a probability that a user is described with a feature  l, and represent this with P ( l = 1|u).
In our case, we assume that an individual s rating for an item is a stochastic function that combines the user-item preference with the item-user appeal.
In other words, given a user u s rating r for item l, the feature  l is: P ( l = 1|u)   P ( l = 1|rl) (4) We assume that every individual user (and item) feature has a given Poisson rating distribution over the items (ref.
users) that it describes, and another Poisson rating distribution on the items it does not describe.
We opt for the Poisson distribution since it has successfully been used for term frequencies in information retrieval ranking [27] and since ratings tend to be small integers.
We further assume that this user s ratings are the sole result of this feature description of items.
Similarly, to estimate the item feature distribution over the kind of users, we assume that the observed ratings on this item are the result only of this item description of the user.
These assumptions are clearly over-simpli cations; we leave more sophisticated models to future work.
Using these conditions, we compute the probability that the feature  l describes user u as: P ( l = 1|r) = (cid:11) P (r| l = 1)P ( l = 1)  l {0,1} P (r| l)P ( l) (5) where r denotes user u s rating for the lth item.
We similarly compute P ( m = 1|i)   P ( m = 1|r).
The Poisson distributions have parameters  1,  0, and a mixture probability p1; we estimate these mixture parameters using the Expectation Maximisation algorithm [9].
In learning the mixture parameters values, we assign a  0  rating to all the items that the user never rated, which implies that the items were not relevant.
While this may not be the case, this is common practice in CF when rating prediction is not involved [7].
By substituting the estimated mixture parameter values, we can compute: P ( l = 1|u)   P ( l = 1|r) (cid:12) = pl1 e  l1  r  l1 p1e  l1  r + (1  p1)e  l1  l0  r  l0 (cid:13) (6) where pl1 = P ( l = 1),  l1 is the average value of  l if l describes u, and  l0 is the average value if the feature  l does not describe u.
We similarly compute the probability for P (em = 1|i).
Finally, we calculate the relevance probability of a user-item pair (u, i), or P (R = 1|u, i) as derived in [13]: P (R = 1|u, i)  R P ( l = 1|u) (cid:18) (cid:15) P ( l = 1) (cid:16)(cid:17) P ( m = 1|i) (cid:18) (cid:15) P ( m = 1) (cid:16)(cid:17) (7) < l, m> (cid:14) part 1 part 2 Which captures the user description of feature l (Part 1) and the item description for feature m (Part 2).
Once we compute the relevance score for each item for a given user using Equation 7, we rank all the items based on their probability of relevance.
In summary, information matching o ers a di erent means of computing the relevance between a user and an item, by reasoning on the preference that the user may have for the item, and the appeal that the item may have for that user.
In the following section, we build on this model so that it can further include the notion of groups of users.
We now describe how the model above can be augmented in order to provide group recommendations.
We  rst de ne a group, describe di erent aspects of group recommendation, our notation, and provide a de nition for item relevance to a group.
Then, we describe a framework that uses information matching, sets of users, and group ranking strategies to compute item relevance to groups.
A group is a set of individuals who are set in a shared context (e.g., members of the same household, friends, etc.)
and who may have common interests.
Each individual will have both personal and group preferences: interests that they are likely to pursue as individuals, which may be unique and independent from others, and preferences that arise from being a member of the group, which will thus be shared with other group members.
The relevance of an item to any given group will vary with the type of group that seeks recommendations.
There are many di erent types of such groups that could be de ned.
For example:
 friends who want to do something together (e.g., go to the cinema), then the recommended items should be relevant to all individuals, or, at least, not disliked by any one member in those instances where no consensus is available.
likely to be able to a ord luxury items, and the car manufacturer is probably not targeting his product to students.
seek items to be enjoyed together, but with conditions that are more relaxed than consensus preference 498groups: recommended items should be relevant to all members, or at least not disliked by the majority when no consensus is available.
If a group consists of a household who wants recommendations as a unit, but may  consume  the items at di erent times (e.g., members of a family watching di erent television programs), then the relevance of an item will be determined by whether there exists at least one user who likes the item.
In the following, we use these notions of group relevance to extend the information matching probabilistic framework so that it caters for recommending items to groups.
We de ne the following hypothesis: The relevance between a group and an item i is only dependent on the relevance of i to individual members of the group.
Using this hypothesis, we derive a probabilistic group recommendation framework that not only includes the preferences of individual users but also integrates the users preferences when they are in a group while recommending a set of items.
First, we de ne the following notation:
 of users (cid:5) = {u1, u2   , uh}.
item pair.
Rg is 1 if the item is relevant to the group, and 0 otherwise.
3.
(cid:6) is a vector of values containing the relevance of each user uj   (cid:5) to a given item i.
In order to recommend a set of items to a group G, we therefore need to calculate P (Rg = 1|G, i) for each item, or the probability of relevance between the group G and each individual item i in collection.
We derive this probability as follows.
First, the probability of an item s relevance to a group is de ned as: P (Rg = 1|G, i) = P (Rg = 1,(cid:5),(cid:6)|G, i) (cid:19) (cid:3) (8) (9) (cid:19) (cid:3) The Bayesian transformation allows to rewrite this as: (cid:18) (cid:15) P ((cid:5),(cid:6)|G, i) P (Rg = 1|G, i) = (cid:18) (cid:15) P (Rg = 1|(cid:5),(cid:6), i, G) (cid:16)(cid:17) (cid:16)(cid:17) Part 1 Part 2 We know that the Rg is dependent only on (cid:5),(cid:6), i, so we can ignore G in part 1, since group relevance of an item is dependent only on the relevance of each individual member of the group to that item (as per our hypothesis).
Similarly, we can ignore G in part 2 as individual user s relevance given an item is independent of the group/groups s/he belongs to: (cid:19) P (Rg = 1|G, i) = P (Rg = 1) P ((cid:6), u1,  , uh, i|Rg = 1)P ((cid:6)|u1,  , uh, i) (10) (cid:3) Assuming that the relevance Rj   (cid:6) is dependent only on uj, i and eliminating the constant P (Rg = 1), we can rewrite h(cid:14) the above equation as: h(cid:14) P (Rg = 1|G, i)  Rg (cid:19) (cid:16)(cid:17) (cid:21) P (Rj, uj, i|Rg = 1) (cid:18) (cid:20) (cid:15) j=1 (cid:3) Part 1 (cid:20) (cid:15) h(cid:14) j=1 (cid:21) (cid:18) P (Rj|uj, i) (cid:16)(cid:17) Part 2 (11) Using Equation 11, we obtain a probabilistic model for group recommendation that accounts for both group (Part 1) and user (Part 2) relevance.
The Group part of the Equation 11 captures the i relevance to uj when she/he is in a group where as the Individual captures the relevance of i to uj as an individual.
Assuming that the group recommendation scenario is to  nd items that all members like (i.e., the context is a consensus preference groups), we know that for Rg = 1 then Rj = 1 for each (uj, i) where uj   G. By turning this implication around, we set P (Rj = 1, uj, i|Rg = 1) = 1 and P (Rj = 0, uj, i|Rg = 1) = 0.
Finally, by substituting these into Equation 11 we get: P (Rg = 1|G, i)  Rg P (Rj = 1|uj, i) (12) j=1 We note that, by de ning the problem space probabilis-tically, any mechanism to estimate the relevance between a user-item can be substituted into the framework.
For example, if the context contains split preference groups, items could be ranked with: P (Rg = 1|G, i)  Rg min{P (R1 = 1|u1, i),  , P (Rh = 1|uh, i)} (13) Which captures the concept of Least Misery between the group members, and is the approach that we adopt in the following evaluation.
A crucial aspect of the computation of relevance in Equation 13 is that the probabilities of relevance between all the users in the group to all the items in the collection must be comparable, i.e. the probability space in which the probability of relevance between all user-item pairs is computed must be the same [24, 23].
This is uniquely achievable by using the principles of information matching [13]: most of the traditional relevance ranking functions used in information retrieval are not capable of estimating this relevance because they function on probability event spaces that are either conditioned on users (queries) [27] or items (documents) [30].
In summary, we outline the group recommendation algorithm that we have proposed here with the pseudocode in Algorithms 1 and 2.
Algorithm 1 focuses on computing the user and item description vectors, and facilitates parallelis-ing the algorithm for large-scale data.
Algorithm 2, instead, computes the relevance of items to groups based on the least misery relevance of items to each member.
In this section, we evaluate how the information matching model for group recommendation, introduced in the previous section, compares to the standard set of CF algorithms from Section 2: popularity, neighbourhood, and latent factor models.
MovieLens (100K) MovieLens (1 Million) MoviePilot (Training) MoviePilot (Evaluation) Users Movies







 Ratings



 Rating scale Label [1-5] [1-5] [0-100] [0-100]
 ML1m MPtr MPEval Table 1: Dataset Descriptions.
The number of users, items, and ratings as well as the two di erent rating scales used in each dataset.
The  label  column denotes how the algorithms are referenced in the text and other tables.
Algorithm 1 Separately Calculate User and Item Vectors Compute each user description vector for u   U sers U do for l = 1 f do Eu[l] = P ( l=1|u) P ( l=1) end for end for Compute each item description vector for i   Items I do for k = 1  e do Fi[k] = P ( k=1|i) P ( k=1) end for end for Algorithm 2 Given group G, Calculate the relevance  r for each item for the group by Least Misery t = relevance threshold X = (u, i)   U ser   Items where r(u, i)   t for u   G do for unrated i   Items I do  r(u, i) = 1  r(G, i) =   for (m, n)   X do  r(u, i) =  r(u, i)   (Eu[l]   Fi[k]) end for if  r(G, i)    r(u, i) then  r(G, i) =  r(u, i) end if end for end for Section 4.1 introduces the datasets that we used: a MoviePi-lot dataset of households  ratings for movies for group recommendation, and the widely used MovieLens datasets, that allow us to provide results that are comparable with the literature.
Section 4.2 describes the methodology and metrics that we use;  nally, Section 4.3 details all the results we obtained.
We use three di erent movie-rating datasets: the two publicly available MovieLens datasets (which di er in size) [20], and a MoviePilot dataset that was released as part of the Context-Aware Movie Recommendation 2011 Challenge at ACM RecSys [1], or CAMRa.
We note that the main task of the  rst track of the challenge was to address the same problem5 that we describe here; that is, recommending a given (mp) set of items to a household of users.
5http://2011.camrachallenge.com/call-for-papers/ The characteristics of the datasets are given in Table 1: there is a wide range in both the number of users and movies, and the MoviePilot data uses a 0 to 100 rating scale (as opposed to MovieLens  1-5 star ratings).
Furthermore, the MoviePilot data contains 290 unique households with between two to four members: majority of the user-ids have been assigned to a household.
We note that, although previous research has used the MovieLens data to examine group recommendation scenarios, these datasets do not include any explicit group membership data.
In the past, researchers have overcome this by forming implicit groups in the data [3].
However, since the MoviePilot data does contain group membership features, we solely use the MovieLens data to provide results for recommendations to individuals that allows our model to be compared with the CF literature.
Given the datasets above, we now describe how we tested and measured the e cacy of the recommendation algorithms described in Sections 2 and 3.
We tested the algorithms in two settings: how well they can produce recommendations for individuals, and how well they produce recommendations for groups.
We then compare these two settings to examine how much performance is lost between the two scenarios.
Recommendations to Individuals.
First, we veri- ed how our model performs when used to compute recommendations for individuals, compared to the other approaches.
The main purpose for doing so is twofold: (a) in previous sections we posited that better group recommendations could be obtained by more accurate models of group members, and (b) we demonstrate that we compare against strong baselines by reproducing results from [4].
In our evaluation, we rank all the items that each target user has not rated in the training set: for ML100K, ML1M and MoviePilot we ranked more than 1500, 3800,
 that the user rated higher than a given threshold (e.g., 4 stars or higher on the 1-5 scale) as relevant and all other items, both those rated below the threshold or not rated at all, as not relevant.
We note that, in doing so, we will tend to observe very low performance scores.
We rank all the items that were not rated by the user for two reasons: (1) it is more closer to the task that practical systems must do, and (2) it produced results with a more accurate distinction between di erent models, in terms of performance.
To conduct our experiments with the MovieLens data, we have randomly divided the datasets into training (60% of the data) and test sets, making sure that ratings from any given user are in both training and testing.
We repeat this process  ve times to compute 5-fold cross validated results.
We also used the MoviePilot data, by disregarding the group memberships.
performance with the following ranking metrics.
First, for every user we compute the precision at rank position N de- ned as:
 relN
 (14) where N is the length of the ranked list and relN is the number of items with user rating greater than the relevance threshold rating in the ranked list.
The precision metric evaluates how well a model performs in putting relevant items in a top-N recommendation list, regardless of its rank.
Second, we measure the Normalised Discounted Cumulative Gain (nDCG).
This metric measures the goodness of ranked list by considering the ratings for the lists  items with the discounted cumulative gain (DCG).
We denote the rating that user u gives item i as r(u, i).
The discounted cumulative gain for user u at rank N , or DCGu N , is computed as: DCGu N = r(u, i1) + r(u, ij) log2(j) (15) N(cid:19) j=2 |Lj|(cid:19) |U|(cid:19) j=1 We denote IDCGu N as the maximum gain value for the user that is obtained with the optimal reordering of the N items.
We use this value to produce the normalised discounted cumulative gain, nDCGu
 DCGu
 IDCGu
 (16) nDCGu
 In computing the nDCG score for each user, we assign a  0  rating to all the items that were not rated by the user, so that if the model recommends any item that was not rated by the user it will be penalised.
To evaluate the global performance, we compute the average value of each metric over all the users.
We denote the overall precision and nDCG at N with P @N , N DCG@N .
Finally, we also compute the system oriented performance metric Mean Average Precision [14], or MAP, which is de- ned as:



 P recision(RLjk) (17) k=1 where U is the set of users, Lj is the set of relevant items for the user j and RLjk is the ranked list of items until kth relevant item for the user j. P recision(RLjk) for user j is computed using precision@|RLjk|.
Recommendations to Groups.
Next, we experiment with the extent that the algorithms can produce quality recommendations for groups, using the MoviePilot data.
The methodology that we adopted sought to align itself to the structure of the CAMRa challenge.
The MoviePilot data contains a set of households H came already divided into training and evaluation sets: the task of the challenge was to recommend a speci ed number of items to each household; data splitting and the speci cation of number of items to recommend to each household was de ned by the organ-isers; the range of number of items recommended to each household is between 1 and 82.
We thus use the training set (MPtr) to learn the model and rank all the non-rated items for each user and compare to the evaluation set (MPEval) for both the recommendation tasks.
Figure 1: Recommending to Individuals.
Pre-cision@5 variance in performance, when using PureSVD and the IMM on the MovieLens 100k dataset, as the threshold of relevance is changed.
|Lj|(cid:19) |H|(cid:19) j=1 Group Recommendation Metrics.
To evaluate the quality of group recommendation we use group precision at rank position mp, grP@mp, for the group recommendation list and the Mean Average Precision (MAP) for the group, where mp is the number of items recommended to each household as de ned in the CAMRa data set.
Group MAP is computed as: gM AP =


 P recision(RLjk) (18) k=1 where H is a set of households, Lj is the set of relevant set of items for the household j and RLjk is the ranked list of items until the kth relevant item for Hj.
P recision(RLjk) is computed as the average of the precisions of individual members of the household Hj.6
 Since we will be providing both reference (individual recommendation) and group recommendation results, in the following we describe each evaluation strategy separately.
In the  nal subsection, we compare these two sets of experiments to examine how group and individual recommendation contexts di er.
Table 2 summarises the performance of each model on di erent collections, with respect to each evaluation metric, when an item is considered to be relevant if the rating given by the user is 5 for MovieLens and greater than 70 for the MoviePilot data.
Most notably, we have only reported MAP scores for the MoviePilot data.
The reason for this is that the performance of the neighbourhood and latent factor models was close to 07.
Furthermore, the simple popularity based model (Pop-item) often produces better results than 6http://2011.camrachallenge.com/evaluation/
 reported on the web page: final-evaluation/ result was competition http://2011.camrachallenge.com/2011/11/

 ML1m MPEval Metric






 Algorithm k-Item k-User













 PureSVD Pop-item IMM




















 Table 2: Recommending to Individuals.
Performance results, in terms of Precision@5, Non-Discounted Cumulative Gain@5 and MAP, when using the popularity baseline, neighborhood approaches (item and user based), PureSVD, and the Information Matching Model (IMM) in order to generate recommendations for individual users.
the personalised alternatives.
However, we clearly observe that the IMM performs better than the all the remaining models on all the metrics.
We also note that the IMM performance on P@10, P@20, NDCG@10, NDCG@20 is also higher when compared with the other models.
We have also tested the extent that performance on individual recommendation varies when we change relevance threshold from 3 to 5 (MovieLens) or 70 to 100 (MoviePilot).
Figure 1 shows the performance of both PureSVD and IMM across these thresholds: the precision increases as the relevance threshold rating is reduced, but the IMM continues to show better performance than the PureSVD approach.
In conclusion, the individual recommendation results show that the two best performing personalised recommender models are the PureSVD and IMM.
In the following section, we show how similar results emerge in the group recommendation scenario.
We evaluate group recommendation with our models when using both of the previously described strategies: (a) by creating pro le for each group based on merging all members  ratings, and (b) by merging members  ranked list of items using Least Misery strategy.
As before, we have also repeated the experiments for various relevance rating thresholds.
Table 3 summarises the performance of each model; we exclude the results from both the kNN user and item-based approaches as they were all zeroes.
Once again, it is clear that the group recommendation model based on the IMM outperforms the other two methods.
We note that the MoviePilot data does not contain the group information for all the users in the training data.
So, when we merge the group pro les the items considered in training were the items rated by at-least one member who has a group identi er.
As a result, we lack the rating information of the items that were not rated by any member of any group, and the total number of items ranked for each user is less than the total number potential test item-scompared to the other two methods.
From the Table 3, we can observe that the performance of the pro le aggregation model is higher than the performance obtained for the merging ranked lists model.
However, the group grP@mp score is higher for the merging ranked lists model than the aggregated user pro le.
and Group Recommendation Finally, we measured how much recommendation quality degrades between the individual and group scenarios.
Figure 2 puts the relative performance between individual and group recommendation of the PureSVD and IMM (in terms of P@5), as di erent relevance rating thresholds were tested.
It is clear that, throughout all approaches, a loss is incurred when transitioning from individual to group recommendations.
This captures the di culty of the group recommendation problem, and may be explained by the number of di erent assumptions that each algorithm must make.
However, not only does the IMM obtain better results compared to the PureSVD: it also degrades less when used for group recommendation.
In fact, precision wanes by up to 26% when IMM is used for group recommendation; the PureSVD approach, instead, loses between 50   95% of the precision it had for individuals when applied to groups.
In the previous section, we observed how the IMM outperforms the latent factor approach, when computing ranked recommendations for both individuals and groups.
In this section, we further discuss the di erences between these algorithms, and how these may a ect their performance.
The IMM di erentiates itself from latent factor and neigh-bourhood based collaborative  ltering in a number of ways.
First, the model does not require a common feature space.
It allows for the users and items to be modelled independently of one another (if necessary), and can incorporate any available information that is solely speci c to users or items.
Unlike dimensionality reduction methods, such as matrix factorisation, Latent Factor Models [17] and topic models [15], we do not need to set any speci c number of hidden dimensions in which both the users and items will be represented.
In other words, it does not involve a lower dimensional representation of features.
This approach to relevance seamlessly captures similarities between users and between items.
For a given user-item pair, the formulation incorporates information about other users who have the same features (i.e.
items) as this user, and other items which have the same features (i.e. users) as this item.
This means that the ranking function implicitly uses information from similar users to this one and from similar items to the target, through a set of relevant user-item pairs.
So, there is no need to explicitly compute the
 Metric PureSVD grP@mp gMAP Metric PureSVD grP@mp gMAP
 Metric grP@mp gMAP Merging Individuals  Pro les > 70 >80 >90

 Rank Aggregation By Least Misery



 >80 >90 > 70

 > 70





 IMM based Least Misery >90

 >80

 Table 3: Group Recommendation.
Performance results on the MoviePilot data for varied relevance rating thresholds.
We include group precision (grP@mp) and Group Mean Average Precision (gMAP) for (a) PureSVD on group pro les that are merged ratings of group members, (b) individual PureSVD results that were merged with Least Misery, and (c) the group information matching model.
(a) PureSVD (b) IMM Figure 2: Performance Loss Between Individual and Group Recommendations.
We plot, for PureSVD and IMM, the performance change as the relevance threshold is varied.
Note that the rating scales for each plot are di erent, and that the PureSVD consistently has a higher loss in performance when used for group recommendation (compared to when it is used for individual recommendation).
similarities between the users or items, which is the basis of the user-based approaches [26] and the item-based approaches [25].
The model is also capable of incorporating other kinds of features; for example, features of the users other than the items they like.
In principle, the preference and appeal matrices can map between any sets of features; we did not pursue this aspect in this work due to the lack of data.
Finally, we note that the two components of the ranking function 7 (parts 1 and 2 in Equation 4) can be treated independently of each other.
This also facilitates parallelising the algorithm to run on large scale data sets.
Web recommender systems are increasingly being applied to group recommendation scenarios, both in instances where users may share online accounts or may seek recommendations for shared activities.
Recent research in this domain, however, has addressed this scenario by computing recommendations for individuals and later merging those recommendations together; in doing so, these approaches exclude the e ect of group dynamics on users  preferences.
Therefore, we have introduced a probabilistic relevance framework for group recommendation that can integrate any group recommendation strategy while taking into account the preferences of the group as a whole.
The model can also include any group-speci c features (e.g., group-type, such as family or friends) that may not be captured in individual s pro les: in future work, we would like to investigate this further.
In particular, future work should seek to understand what additional features matter for group recommendation, and to discover what contextual features may in uence a group s preferences.
We evaluated our approach, particularly focused on the Least Misery strategy, using available data that contains the ratings that users (who belong to given households) have given to movies.
Our future work includes extending this evaluation to examine other techniques as well.
We found 503that our information-matching approach outperformed a pop ularity based baseline, neighbourhood models, and a latent factor model across a range of ranking metrics when recommending to groups: we therefore also tested the model on the MovieLens dataset for individual recommendations in order to ensure that our results are aligned with others reported in the literature.
