Auto-completion is an important feature for modern search engines, social networking sites, mobile text entry, and many web and database applications [35, 23, 16].
Speci cally, as the user enters a phrase one character at a time, the system presents the top-k completion suggestions to speed up text entry, correct spelling mistakes, and help users formulate their intent.
As shown in Figure 1, a search engine may suggest query completions of search pre xes, a browser may complete partial URLs, and a soft keyboard may predict word completions.
Typically, the completion suggestions are drawn from a set of strings, each associated with a score.
We call such a set a scored string set.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
(a) Search engine (b) Browser (c) Soft keyboard Figure 1: Usage scenarios of top-k completion.
Definition 1.1 (Scored string set).
A scored string set S, |S| = n, is a set of n pairs (s, r) where s     is a string drawn from an alphabet   and r is an integer score.
Given a pre x string, the goal is to return the k strings matching the pre x with the highest scores.
Formally, we de ne the problem of top-k completion as follows.
Problem 1.2 (Top-k completion).
Given a string p     and an integer k, a top-k completion query in the scored string set S returns the k highest scored pairs in Sp = {(s, r)   S | p is a pre x of s} (or the whole set if |Sp| < k).
To be e ective, an auto-completion system needs to be responsive, since users expect instantaneous suggestions as they type.
As each keystroke triggers a request, the system needs to scale to handle the high volume.
To host a large number of unique suggestions, the data should be compressed to avoid the latency costs associated with external memory access or distributed data structures.
If the data needs to be hosted on mobile clients, the compression should further scale across dataset sizes.
A simple solution is to store all the strings in a trie or compacted trie [21], and associate each leaf node with its corresponding score.
Although such a data structure is compact and allows us to quickly enumerate all the strings matching a given pre x, we need to explicitly sort the matches by their scores in order to return the top-k completions.
For large string sets where short pre xes may potentially match millions of strings, this approach is prohibitive in terms of speed.
Although we can precompute and store the top-k completions for the short pre xes [24], this requires a priori knowledge of k and the space scales poorly with k.
Many of the top-k completion application scenarios exhibit special properties which we can take advantage of to improve the space and time e ciency of the system.
First, the scores associated with the strings often exhibit a skewed power law distribution, as demonstrated by the histogram of search
 counts associated with the AOL queries [1] in Figure 2.
Most of the queries have low counts as scores that require only a few bits to encode.
Second, the distribution of the target strings that users enter one character at a time often approximates the distribution of the scores, after ignoring the pre xes not matching any of the strings in the set.
Speci cally, in practical usages of top-k completion systems, pre xes of entries with higher scores tend to be queried more than those associated with lower scored entries.
In fact, a common folklore optimization in practical trie implementations is to sort the children of each node by decreasing score.
Third, as a large number of strings share common pre xes, they are highly compressible.
In this work, we present three data structures that exploit the aforementioned properties to support e cient top-k completion queries with di erent space/time/complexity trade-o s.
  Completion Trie: A compact data structure based on compressed compacted trie, where the children of each node are ordered by the highest score among their respective descendants.
By storing the max score at each node, we can e ciently enumerate the completions of a string pre x in score order.
This data structure uses standard compression techniques, such as variable-length encoding, to reduce space occupancy.
  RMQ Trie: A generic scheme that can be applied to any data structure that bijectively maps a set of strings to consecutive integers in lexicographic order, by using a Range Minimum Query (RMQ) data structure [13] on the sequence of scores to retrieve the top-k completions.
In our experiments, we apply the scheme to the lexicographic path-decomposed trie of [17].
  Score-Decomposed Trie: A compressed data structure derived from the path-decomposed trie of [17], where we use a path decomposition based on the maximum descendant score.
This path decomposition enables e cient top-k completion queries.
There is a vast literature on ranked retrieval, both in the classical and succinct settings.
We report here the results closest to our work.
Using classical data structures, various studies have examined the task of word/phrase completion [7, 26, 24, 25, 30, 36], though most do not consider datasets of more than a million strings or explore e cient algorithms on compressed data structures.
In [24], Li et al. precompute and materialize the top-k completions of each possible word pre x and store them with each internal node of a trie.
This requires a predetermined k and is space ine cient.
Church et al. employ a kd-tree style su x array that alternates the sorting order of   nodes between lexicographic and score order at each level [7].
However, the lookup time is in the worst case O( n) and has empirical performance in milliseconds.
Recently, Matani [26] describes an index similar in principle to the proposed RMQ Trie structure in Section 5, but uses a suboptimal data structure to perform RMQ.
Although the system achieves sub-millisecond performance, both this and the previous work require storing the original string set in addition to the index.
From a theoretical point of view, Bialynicka-Birula and Grossi [4] introduce the notion of rank-sensitive data structures, and present a generic framework to support ranked retrieval in range-reporting data structures, such as su x trees and tries.
However, the space overhead is superlinear, which makes it impractical for our purposes.
As the strings are often highly compressible, we would like data structures that approach the theoretic lower bound in terms of space.
Succinct data structures use space that is the information-theoretically optimal number of bits required to encode the input plus second-order terms, while supporting operations in time equal or close to that of the best known classical data structures [20, 28, 3, 33].
Recent advances have yielded many implementations of string dictionaries based on succinct data structure primitives [17, 6], without scores.
Hon et al. [19] use a combination of compressed su x arrays [18, 12] and RMQ data structures to answer top-k document retrieval queries, which ask for the k highest-scored documents that contain the queried pattern as a substring, in compressed space.
While this is strictly more powerful than top-k completion, as shown in [6], string dictionaries based on compressed su x arrays are signi cantly slower than pre x-based data structures such as front-coding, which in turn is about as fast as compressed tries [17].
The RMQ Trie of Section 5 uses a similar approach as [19], but is based on a trie instead of a su x array.
As we will discuss in Section 8.4, speed is crucial when implementing more sophisticated algorithms, such as fuzzy completion, on top of the core top-k completion data structures.
Large scale evaluations on search queries, web URLs, and English words demonstrate the e ectiveness of the proposed approaches.
For example, on the AOL query log with 10M unique queries [1], the Completion Trie achieves a size of 120 bits/query (including the scores) while requiring an average of only 3.7 s to compute the top-10 completion on a simulated workload.
In comparison, the Score-Decomposed Trie increases the completion time to 8.0 s, but further reduces the size to 62 bits/query.
In fact, this is less than 30% of the uncompressed data size and within 11% of the gzip ed size.
The RMQ Trie obtains a similar space occupancy at 65 bits/query, but is signi cantly slower at 33.9 s.
In this section, we brie y describe some of the data structures and primitives used in this paper.
For additional details on the design and implementation of these primitives, please refer to the cited references.
String Dictionaries.
A string dictionary is a data structure that maps a pre x-free set S     of strings drawn from an alphabet   bijectively into [0,|S|), where pre x-free means that no string in the set is a pre x of another string in the set; this can be guaranteed, for example, by appending a special terminating null character to every string.
We call
 inverse function Access, i.e. Access(Lookup(s)) = s for all s   S. Lookup(s) returns   if s is not in S. A popular way of implementing a string dictionary is by using a trie data structure [14], possibly compacted, where each chain of edges without branches is collapsed into a single edge.
Priority queues.
A priority queue Q maintains a set under operations Push(Q, v), which adds the element v to Q; and Pop(Q), which returns the minimum element in Q according to a given total ordering on the values, and removes it from the set.
To implement priority queues, we use a classical binary heap [21].
While alternative solutions, such as Fibonacci heaps and pairing heaps, have O(1) amortized insertion cost, they are often slower than binary heaps in practice.
Bitvectors with Rank and Select.
Given a bitvector X with n bits, we can de ne the following operations: Rankb(i) returns the number of occurrences of bit value b   {0, 1} in X in the range [0, i).
Selectb(i) returns the position of the i-th occurrence of bit value b in X.
Note that Rankb(Selectb(i)) = i.
These operations can be supported in constant time by adding o(n) bits of redundancy to the bitvector [8, 20].
In our implementations we use the rank9 data structure [37] and a variation of the darray [31] when only Select is needed.
Balanced parentheses (BP).
In a sequence of n balanced parentheses, each open parenthesis ( is paired with its mate close parenthesis ).
Operations FindClose and FindOpen  nd the position of the mate of an open and close parenthesis, respectively.
The sequence can be encoded as a bitvector, where
 number of open and close parentheses in the range [0, i) is called the excess at i.
Note that Excess(i) = 2 Rank((i)   i.
It is possible to support the above operations in constant time with a data structure that takes o(n) bits [20, 28, 2,
 a variation of the Range-Min-Max tree [2, 33].
DFUDS representation.
The DFUDS (depth rst unary degree sequence) representation [3] maps a tree with t nodes to a BP sequence of 2t bits; several traversal operations can be implemented with a combination of Rank, Select, FindClose, and FindOpen operations.
Range Minimum Queries (RMQ).
Given an array A of n values, the operation RMQ(i, j) returns the position of the minimum value of A in the range [i, j], according to a given total ordering of the values (in case of ties, the leftmost value is chosen).
RMQ can be supported in constant time by pre-computing the Cartesian tree of A, which can be encoded using BP into 2n + o(n) bits [13].
In our implementation we use this data structure with a slight variation in the RMQ algorithm, described in more detail in Appendix A.
In implementing the succinct Implementation details.
data structures described above, we are mostly concerned with the actual speed and space of the data structures we consider, rather than theoretical optimality.
For this reason, although constant-time implementations of many succinct primitives are available, we often prefer logarithmic-time versions.
As shown in several papers [31, 37, 2, 17], such implementations are actually faster and smaller than their constant-time counterparts.
For this reason, when reporting time complexities, we will ignore the logarithmic factors introduced by succinct operations, treating them as constant-time; in this case we will use the  O notation to avoid ambiguity.
Our implementations of these structures are freely available as part of the Succinct C++ library [34].
A trie, or pre x tree, is a tree data structure that encodes a set of strings, represented by concatenating the characters of the edges along the path from the root node to each corresponding leaf.
We collapse common pre xes such that each string pre x corresponds to a unique path.
Whereas each edge represents a single character in the simple trie, a compacted trie, also known as a Patricia trie or radix tree, allows a sequence of characters to be associated with each edge such that no node can have a single child (except for the root node in degenerate cases).
To encode the score associated with each string, we assign to each leaf node the score of the string it represents.
To support e cient top-k completion, we further assign to each intermediate node the maximum score among its descendant leaf nodes.
Note that by construction, the score of each non-leaf node is simply the maximum score among its children, as exempli ed in Figure 3.
As each node score now represents the largest score among all strings starting with the pre x corresponding to the node, we can apply it as an exact heuristic function in a variation of the A* search algorithm [32] to  nd the best completion path from a node representing the pre x.
Speci cally, we  rst  nd the locus node, the highest node in the trie that matches or extends the pre x string, and insert it into a priority queue, if found.
Iteratively, pop the node with the largest score.
If it is a leaf node, add the string and score corresponding to the node to the list of completions.
Otherwise, iteratively insert its children to the queue until k completions have been found or the priority queue is empty.
ab b bba caca caccc cbac cbba







 ab

 b

 

 c

 ac

 b

 

 ba

 ba

 ac

 cc

 a
 Figure 3: Compacted trie with max scores in each node.
The worst-case time complexity of this algorithm is O(| ||p|+ | |kl log | |kl), where   is the alphabet from which the strings are composed, p is the input string pre x, k is the number of requested completions, and l is the average length of the completions returned excluding the common pre x p.
Speci cally, we need to examine up to |p| nodes with up to | | children each to  nd the locus node.
We may encounter and expand kl nodes on the way to the leaf nodes corresponding to top-k completions.
As the algorithm inserts all children of each expanded node to the priority queue, we add up to | |kl nodes to the binary heap, contributing an additional O(| |kl log | |kl) term.
Instead of inserting all children of each expanded node to the priority queue, if we were to sort the children by order of decreasing score, we only need to add the  rst child and the next sibling, if any, of each expanded node.
Conceptually, we can view this as adding a sorted iterator to the priority queue.
Whenever we remove an iterator from the queue, we return the  rst element and insert the remainder of the iterator back into the queue.
With this change, the time complexity
 as we insert a maximum of 2 nodes for each node expanded during the search algorithm.
In practice, sorting the children by decreasing score also reduces the number of comparisons needed to  nd the locus node.
A summary of the top-k completion algorithm on the Completion Trie data structure is presented in Algorithm 1.
Algorithm 1 Top-k completion with Completion Trie.
Input: Completion Trie T , pre x p, and k   0 Output: List c of top-k completions of p


 4 if n is not null then Push(Q, (Score(n), n))

 r, n   Pop(Q)
 if n is a leaf node then



 s   String corresponding to n Append (s, r) to result list c if |c| = k then return c fn, nn   First child of n, next sibling of n Push(Q, (Score(fn), fn)) if nn is not null then Push(Q, (Score(nn), nn)) else



 16 return c
 In addition to improving the theoretical time complexity, improving the locality of memory access also plays a signi cant role in improving the practical running time, as accessing random data from RAM and hard drive can be 100 and 10M times slower than from the CPU cache, respectively, easily trumping any improvements in time complexity.
For example, to improve memory locality when  nding the locus node, we store each group of child nodes consecutively such that accessing the next sibling is less likely to incur a cache miss.
However, instead of writing each group of sibling nodes in level order, we write the encodings of each group of trie node in depth rst search (DFS) order.
As each internal node is assigned the maximum score of its children and the children are sorted by decreasing score, iteratively following the  rst child is guaranteed to reach a leaf node matching the score of an internal node.
Thus, by writing the nodes in depth rst order, we typically incur only one cache miss per completion, resulting in signi cant speedup over other arrangements.
For each node, we encode the character sequence associated with its incoming edge, its score, whether it is the last sibling, and an o set pointer to its  rst child, if any.
Note that if the node has a next sibling, it is simply the next node.
Furthermore, we can use a special value of 0 as the  rst child o set for leaf nodes.
Assuming 4-byte scores and pointers, a na ve encoding would require (l + 1) + 4 + 1 + 4 = l + 10 bytes, where l is the length of the character sequence.
One way to reduce the size of each node is to apply a variable-byte encoding to scores and o sets.
However, as each group of child nodes are sorted by decreasing order and we traverse the children sequentially, we can  rst perform delta encoding by storing only the score di erence between the current node and its previous sibling.
As the  rst child shares the same score as its parent and is always traversed from its parent node, we can simply store a di erential score of 0.
Similarly, we observe that the  rst child o set for siblings can only increase.
Thus, we can apply the same delta encoding techniques to derive the  rst child o set of a node from its previous siblings.
To  nd the  rst child o set for the  rst sibling, we can traverse all the remaining siblings and return the next node, as each set of sibling nodes are stored in depth rst order.
However, as the number of siblings may be large, we simply store the di erence in o set between the  rst child and the  rst sibling node.
Note that we still encode leaf nodes with a  rst child o set of 0.
With delta encoding, we signi cantly reduce the values of the node scores and  rst child o sets.
While many variable-byte encoding schemes exist, we choose to apply an approach where we encode the size of each value in a header block.
As smaller values, including 0, are much more frequent than larger values due to the power law distribution of scores and the depth rst ordering of the nodes, we choose to allocate two bits in the header to represent values encoded with 0, 1,
 to indicate if the node is the last sibling.
Finally, if we limit the maximum number of characters that we can store with each node to 7 by adjusting how the trie is compacted, we can store the length of the character sequence in the remaining 3 bits of a 1 byte header.
Figure 4 shows the binary Completion Trie encoding of the example from Figure 3.


 ab

 c

 b

 ac

 b

 a

 cc

 ac

 ba

 

 ba
 Is last sibling First child o set size Delta Score Is last sibling First child o set size Delta Score .
.
.
[ 001 1 01 01 ] [b] [1] [20] .
.
.
[ 010 0 00 00 ] [a][c] [] [] .
.
.
Char size Score size Chars Delta  rst child o set Char size Score size Chars Delta  rst child o set Figure 4: Binary Completion Trie encoding.
As the trie nodes are stored in DFS order, it is possible to reconstruct the string corresponding to a completion leaf node by starting from the root node and iterative  nding the child whose subtrie node o set range includes the target leaf node.
However, this is an expensive O(| |d) operation, where d is the depth of the leaf node.
Instead, we can signi cantly reduce the cost of returning the top-k completion strings through additional bookkeeping in the search algorithm.
Speci cally, we store the nodes to be inserted into the priority queue in an array, along with the index of its parent node in the array.
By modifying the priority queue to access nodes through their corresponding array indices, we can retrieve the path from each completion node to the locus node by following the parent indices.
Thus, we can e ciently construct the completion string in time O(d) by concatenating the original pre x string with the character sequences encountered along the reverse path.
To further improve the running time of the algorithm, we employ a few bit manipulation techniques that take ad-
be represented in 4 bytes, we can simply change 4 to the number of bytes required to represent the largest value.
variable-byte encoding [38], we need to read multiple bytes to determine the size and decode the value.
But by storing the size of the variable-byte value in a 2-bit code, we can determine the size (cid:96) by looking up the code c in a small array: (cid:96)   sizeFromCode[c].
Furthermore, we can decode the value v by reading a 64-bit integer from the starting position p and applying a mask indexed by the size code c to zero out the extra bytes: v   ReadInt64(p) & codeMask[c].2 With a direct implementation, a signi cant amount of time is spent on matching strings in the pre x and constructing the completion string.
In the compressed encoding of the Completion Trie, each trie node represents at most 7 characters.
Thus, we can apply a similar masking technique to compare the  rst (cid:96) characters of two strings p and q: isMatch   (ReadInt64(p) & strMask[(cid:96)]) = (ReadInt64(q) & strMask[(cid:96)]).
When constructing the completion string, by over-allocating the string bu er that stores the completion string, we can copy 8 bytes from the node character sequence to the insertion position in one instruction and advance the insertion point by the desired length.
By replacing several unpredictable conditional branching instructions with a few simple bit operations, these optimizations signi cantly improve the performance of the runtime algorithm.
In this section, we describe a simple scheme to augment any sorted string dictionary data structure with an RMQ data structure, in order to support top-k completion.
As shown in Figure 5, if the string set S is represented with a trie, the set Sp of strings pre xed by p is a subtrie.
Hence, if the scores are arranged in DFS order within an array R, the scores of Sp are those in an interval R[a, b].
This is true in general for any string dictionary data structure that maps the strings in S to [0,|S|) in lexicographic order.
We call Pre xRange(p) the operation that, given p, returns the pair (a, b), or   if no string matches the pre x.
p v
 a b Figure 5: The scores of the strings pre xed by p correspond to the interval [a, b] in the scores vector R.
To enumerate the completions of p in ranked order, we employ a standard recursive technique, used for example in [29,
 inverted ordering, i.e. the minimum is the highest score.
The
 additional shift operation is required for big-endian systems.
Furthermore, we need to pad the end of the binary trie encoding with 7 bu er bytes to avoid reading past the end of the data.
On processors without support for unaligned access, such as some ARM processors, ReadInt64 is less e cient.
index of the  rst completion is then i = RMQ(a, b).
Now the index of the second completion is the one with highest score among RMQ(a, i   1) and RMQ(i + 1, b), which splits again either [a, i   1] or [i + 1, b] into two subintervals.
In general, the index of the next completion is the highest scored RMQ among all the intervals obtained with this recursive splitting.
By maintaining the intervals in a priority queue ordered by score, it is hence possible to  nd the top-k completion indices in  O(k log k).
We can then perform k Access operations on the dictionary to retrieve the strings.
The pseudo-code is shown in Algorithm 2.
Algorithm 2 Top-k completion with RMQ Trie.
Input: Trie T , scores vector R, pre x p, and k   0 Output: List c of top-k completions of p


 4 if found then






 i   RMQR(a, b) Push(Q, (R[i], i, a, b)) while Q is not empty do r, i, a, b   Pop(Q) s   AccessT (i) Append (s, r) to result list c if |c| = k then return c if i > a then j   RMQR(a, i   1) Push(Q, (R[j], j, a, i   1)) j   RMQR(i + 1, b) Push(Q, (R[j], j, i + 1, b)) if i < b then





 18 return c The space overhead of this data structure, beyond the space needed to store the trie and the scores, is just the space needed for the RMQ data structure, which is 2n + o(n) bits, where n = |S|.
If the trie can answer Pre xRange in time TP and Access in time TA, the total time to retrieve the top-k completions is  O(TP + k(TA + log k)).
The advantages of this scheme are its simplicity and modu-larity, since it is possible to reuse an existing dictionary data structure without any signi cant modi cation.
In fact, in our experiments we use the lexicographic compressed trie of [17].
The only change we needed to make was to implement the operation Pre xRange.
On the other hand, as we will see in Section 8, this comes at the cost of signi cantly worse performance than the two other data structures, which are speci cally designed for the task of top-k completion.
In this section, we introduce a compressed trie data structure speci cally tailored to solve the top-k completion problem.
The structure is based on the succinct path-decomposed tries described in [17], but with a di erent path decomposition that takes into account the scores.
Path decompositions.
Let T be the trie built on the strings of the scored string set S. A path decomposition of T is a tree T c whose nodes correspond to node-to-leaf paths in T .
The tree is built by  rst choosing a root-to-leaf path   in T and associating it with the root node u  of T c; the children of u  are de ned recursively as the path decompositions of 587b4 v4 r4  1 b6 b5 v6 r6 v5 r5 c1  2 c2  3 b2 c3 b3 v2 r2  4 c4 v3 r3  6 b1 v1 r1 u  b1, r1 v1 b2, r2 v2 b3, r3 v3 b4, r4 v4 b5, r5 v5 b6, r6 v6 Lu  BPu  Bu  Ru   1 2 c1 2 1 c2 3 2 c3 4 1 c4 5 (( b6b5 r6r5 ( b4 r4 (( b3b2 r3r2 ( b1 r1 ) Figure 6: On the left, trie T with the decomposition path   highlighted.
On the right, root node u  in T c and its encoding (spaces are for clarity only).
In this example v6 is arranged after v5 because r5 > r6.
the subtries hanging o  the path  , and their edges are labeled with the labels of the edges from the path   to the subtries.
See Figure 6 for an example.
Note that while each string s in S corresponds to a root-to-leaf path in T , in T c it corresponds to a root-to-node path.
Speci cally, each leaf (cid:96) in T is chosen at some point in the construction as the decomposition path of a subtrie, which becomes a node u in T c; the path from u  to u in T c corresponds to the root-to-leaf path of (cid:96) in T .
For the sake of simplicity we will say that s corresponds to the node u.
Max-score path decomposition.
A path decomposition is completely de ned by the strategy used to choose the path   and order the subtries hanging o    as children of the root u .
Since each string corresponds to a leaf in T , we can associate its score with the corresponding leaf.
We de ne the max-score path decomposition as follows.
We choose path   as the one to the leaf with the highest score (ties are broken arbitrarily).
The subtries are ordered bottom-to-top, while subtries at the same level are arranged in decreasing order of score (the score of a subtrie is de ned as the highest score in the subtrie).
To enable scored queries, we need to augment the data structure to store the scores.
Following the notation of Figure 6, let u  be the root node of T c and v1, .
.
.
, vd be the nodes hanging o  the path  .
We call ri the highest score in the subtrie rooted at vi (if vi is a leaf, ri is just its corresponding score).
We add ri to the label of the edge leading to the corresponding child, such that the label becomes the pair (bi, ri).
Succinct tree representation.
To represent the Score-Decomposed Trie, we use the same encoding described in [17], which we brie y summarize here.
For each node u in T c we build three sequences Lu, BPu, and Bu.
Figure 6 shows the encoding of the root node u ; the other nodes are encoded recursively.
Lu  contains the concatenation of the node and edge labels along the path  , interleaved with special characters 1, 2, .
.
.
that indicate how many subtries branch o  that point in the path  .
We call the positions of these special characters branching points.
BPu  contains one open parenthesis for each child of u , followed by a single close parenthesis.
Bu  contains the sequence of the characters bi branching o  the path   in reverse order.
The sequences for each node are concatenated in DFS order into the three ab b bba caca caccc cbac cbba







 c, 3 1ac1a c, 1 c
 2ab b, 2 1ac b, 1 a b, 2
 a b, 1 2ab 1ac1a  c 1ac  a  1  a 


 bcbcbb
 Figure 7: Score-Decomposed Trie example and its encoding.
sequences L, BP, and B.
In particular, after prepending an open parenthesis, BP is the DFUDS representation of the topology of the path-decomposed tree.
Note that the branching characters bi are in one-to-one correspondence with the open parentheses in BP, which in turn correspond to the nodes of T c. In addition, we need to store the scores in the edges along with the branching characters.
We follow the same strategy used for the branching characters: concatenate the ri s in reverse order into a sequence Ru  , and then concatenate the sequences Ru for each node u into a sequence R in DFS order.
Finally, append the root score to R.
The advantage of storing BP, B, L, and R separately is that they can be compressed independently with specialized data structures, provided that they support the operations needed by the traversal algorithms.
Speci cally, BP and B are stored explicitly as a balanced parentheses structure and character array, respectively.
We compress the sequence of labels L using a variant of RePair [22] that supports scanning each label in constant-time per character [17].
The sequence R is compressed using the data structure described in Section 7.
Top-k completions enumeration.
The operations Lookup and Access [17] do not need any modi cation, as they do not depend on the particular path decomposition strategy used.
We now describe how to support top-k completion queries.
Because of the max-score decomposition strategy, the highest score in each subtrie is exactly the score of the decompo-
of the subtrie rooted in vi, and ui is the node in T c corresponding to that subtrie, then ri is the score of the string corresponding to ui.
This implies that for each (s, r) in S, if u is the node corresponding to s, then r is stored in the incoming edge of u, except when u is the root u , whose score is stored separately.
Another immediate consequence of the decomposition is that the tree has the heap property: the score of each node is less or equal to the score of its parent.
We exploit this property to retrieve the top-k completions.
First, we follow the algorithm of the Lookup operation until the pre x p is exhausted, leading to the locus node u, the highest node whose corresponding string contains p. This takes time  O(| ||p|).
By construction, this is also the highest scored completion of p, so we can immediately report it.
To  nd the next completions, we note that the pre x p ends at some position i in the label Lu.
Thus, all the other completions must be in the subtrees whose roots are the children of u branching after position i.
We call the set of such children the seed set, and add them into a priority queue.
To enumerate the completions in sorted order, we extract the highest scored node from the priority queue, report the string corresponding to it, and add all its children to the priority queue.
For the algorithm to be correct, we need to prove that, at each point in the enumeration, the node corresponding to the next completion is in the priority queue.
This follows from the fact that every node u corresponding to a completion must be reached at some point, because it is a descendant of the seed set.
Suppose that u is reported after a lower-scored node u(cid:48).
This means that u was not in the priority queue when u(cid:48) was reported, implying that u is a descendant of u(cid:48).
But this would violate the heap property.
The previous algorithm still has a dependency on the number of children in each node, since all of them must be placed in the priority queue.
With a slight modi cation in the algorithm, this dependency can be avoided.
Note that in the construction, we sort the children branching o  the same branching point in decreasing score order.
Thus, we can delay the insertion of a node into the priority queue until after all other higher-scored nodes from the same branching point have already been expanded.
For each node u, the number of branching points in Lu is at most |Lu|.
Hence, we add at most |Lu| + 1 nodes to the priority queue: 1 for each branching point and the next sibling, if any, of node u.
Thus, the time to return k completions is  O(lk log lk) where l is the average length of the completions returned minus the pre x length |p|.
Comparison with Completion Trie.
The algorithm described above is very similar to Algorithm 1 for the Completion Trie.
In fact, the Score-Decomposed Trie can be seen as a path decomposition of the Completion Trie, and the previous algorithm as a simulation of Algorithm 1 on the transformed tree.
However there are two signi cant di erences.
First, the scores in the Completion Trie along the max-score path are, by construction, all the same.
Thus, they can be written just once.
Hence, while the Completion Trie stores at least 2n   1 scores for n strings, the Score-Decomposed Trie only stores n.
Second, after the locus node is found, only k   1 nodes need to be visited in order to return k completions.
In contrast, Completion Trie may require visiting up to  (kl) nodes.
This property makes the Score-Decomposed Trie very suitable for succinct representations, whose traversal operations are signi cantly slower than pointer-based data structures.
For both data structures described in Section 5 and Section 6, it is necessary to store the array R of scores, and perform random access quickly.
Further, it is crucial to e ec-tively compress the scores: if stored directly as 64 bit integers, they would take more than half of the overall space.
As noted in Section 1, many scoring functions (number of clicks/impressions, occurrence probability, .
.
. )
exhibit a power law distribution.
Under this assumption, encoding the scores with  codes [11] (or in general  codes [5]) would give nearly optimal compression.
However it would not be possible to support e cient random access to such arrays.
Speci cally, we experimented with a random-access version of  codes: concatenate the binary representations of the values of R (without the leading 1) into a bitvector and use a second bitvector to delimit their endpoints, which can be retrieved using Select1.
While this obtained very good compression, it came at the cost of a signi cant slowdown in retrieval.
We use instead a data structure inspired by Frame of Reference compression [15], which we call packed-blocks array.
The scores array of length n is divided into blocks of length l; within each block j the scores are encoded with bj bits each, where bj is the minimum number of bits su cient to encode each value in the block.
The block encodings are then concatenated in a bitvector B.
To retrieve the endpoints of the blocks inside B we employ a two-level directory structure: the blocks are grouped into super-blocks of size L, and the endpoint of each block is stored relative to the beginning of the superblock using O(log(Lw)) bits, where w is the size in bits of the largest representable value.
The endpoint of each superblock is encoded using O(log(nw)) bits.
To retrieve a value, the endpoints of its block are retrieved using the directory structure; then bj is found by dividing the size of the block by l. The overall time complexity is constant.
In our implementation, we use l = 16, L = 512, 16-bit integers for the block endpoints, and 64-bit integers for the super-block endpoints.
In our experiments, the slowdown caused by the packed-blocks array instead of a plain 64-bits array was basically negligible.
On the other hand, as we show in Section 8 in more detail, we obtain very good compression on the scores, down to a few bits per integer.
We attribute the good compression to the fact that each group of sibling scores are arranged in DFS order.
As the decomposed trie exhibits the heap property, the score of each node upper bounds the scores of its descendants.
This increases the likelihood that adjacent sibling groups have scores with the same order of magnitude.
Hence, the waste induced by using the same number of bits for l consecutive values is relatively small.
To evaluate the e ectiveness of the proposed top-k completion techniques, Completion Trie (CT), Score-Decomposed Trie (SDT), and baseline RMQ Trie (RT), we will compare their e ectiveness on the following datasets from di erent application scenarios on an Intel i7-2640M 2.8GHz processor with 128/512/4096KB of L1/2/3 cache and 8GB of RAM, compiled with Visual C++ 2012 running on Windows 8.
  QueriesA: 10,154,742 un ltered search queries and their associated counts from the AOL query log [1].
This dataset is representative of the style and frequency of 589queries users may enter into the search box of a search engine or large website.
  QueriesB: More than 400M  ltered search queries and their click counts from a commercial search engine for scalability evaluation.
  URLs: 18M URL domains and click counts derived from the query click log of a commercial search engine, representing the scenario of URL completion suggestions in web browser address bars.
As users generally skip the initial URL protocol (eg. http) and often the www domain pre x, for each URL, we arti cially inject additional entries with the same count to accommodate such behavior, for a total of 42M unique scored strings.
Unlike queries, URLs share a small set of extension su xes, which makes the data more compressible.
  Unigrams: The top 1M words and their probabilities from the Microsoft Web N-gram Service (bing-body:apr10) [27].
We quantize the probabilities to (cid:98)1000 ln(p)(cid:99).
This dataset is representative of the lexicons used by mobile devices with soft keyboards, which need a large lexicon for each language to support predictive text entry and spelling correction, but the tight memory resources require a space-e cient storage.
In each dataset we subtracted from the scores their minimum, so that the smallest score is 0, without a ecting the ordering.
The minimum is then added back at query time.
We evaluate the compactness of the data structures by reporting in Table 1 the average number of bits per string (including score).
For comparison, we also report the size of the original uncompressed text  le (Raw) and the gzip compressed binary (GZip).
Across the 4 datasets, the three presented techniques achieve an average compression ratio of between 29% and 51%, with SDT consistently having the smallest size.
In fact, its size is only 3% larger than that achieved by gzip compression on average, and is actually
 Dataset Raw GZip

 QueriesA 209.8 QueriesB 235.6 URLs

 Unigrams















 Table 1: Data structure sizes in bits per string.
To better understand how the space is used, we present in Figure 8 the storage breakdown of each of the techniques on QueriesA.
For CT, 70% of the space is used to store the uncompressed character sequences.
Compressing the node character sequences with RePair [22] can further reduce the size, but will incur some sacri ce in speed.
With delta encoding, storing the scores, including the 2 bit header, takes only 4.0 and 9.6 bits per node and string, respectively.
In comparison, standard variable-byte encoding with a single continuation bit [38] requires at least 8 bits per node.
Similarly, we utilize an average of only 16.4 bits per string in the dataset to encode the tree structure.
As reference, it would have required 24 bits just to encode the index of a string.
For SDT, nearly 90% of the space is dedicated to storing the compressed labels and branching characters.
On average, each score takes 4.1 bits, less than half of CT; while maintaining the tree structure via BP requires only 2.7 bits per string.
RT behaves similarly except each score takes 4.9 bits as the child nodes are sorted lexicographically rather than by score.
In addition, it requires a Cartesian tree to perform Range Minimum Queries, which takes a further 2.7 bits per string.
(a) CT (b) SDT (c) RT Figure 8: Data structure size breakdowns.
To evaluate the runtime performance of the proposed data structures, we synthesize a sequence of completion requests to simulate an actual server workload.
Speci cally, we  rst sample 1M queries in random order from the dataset according to the normalized scores.
Assuming that user queries arrive according to a Poisson process, we can model the inter-arrival time of each query using an exponential distribution.
We can control the average queries per second (QPS) by adjusting the   parameter of the exponential distribution.
For simplicity, we assume that each subsequent keystroke arrives 0.3 seconds apart, corresponding to an average typing speed of 40 word per minutes.
Users will continue to enter additional keys until the target query appears as the top suggestion, or until the query has been fully entered.
Note that with higher QPS, requests from di erent queries are more likely to overlap, leading to more cache misses.
In Table 2, we present the mean time to compute the top-10 completions, averaged over 10 runs.
Overall, CT achieves the best performance, about twice as fast as SDT.
While much of the di erences can be attributed to SDT s use of succinct operations for trie traversal and RePair decoding of label sequence L, CT s better memory locality, where all node information are stored together, still plays an important part.
For instance, we see that when the nodes are not arranged for locality, as is the case for RT, the performance is extremely poor.
Similarly, as the requests corresponding to higher QPS exhibit less overlap in memory access, the performance degrades by an average of 10% for CT and 21% for SDT.
As the pre xes used by the two workloads di er only in order, the performance gap is due entirely to the e ect of CPU cache, where CT shines.
To simulate a moderate workload, we use 1K QPS in the remaining analyses.
Dataset







 QueriesA 3.30 QueriesB 4.45 URLs

 Unigrams



















 Table 2: Average time per top-10 completion query in  s.
the techniques, we break down the total time to compute the top-10 completions on QueriesA into the time spent  nding the locus node and computing each successive completion.
As shown in Figure 9, CT using pointer arithmetic is signi -cantly faster than data structures using balanced parentheses for traversal, especially in  nding the initial locus node.
Theoretically, the cost of retrieving each additional completion increases logarithmically.
But in practice, the incremental cost for both CT and SDT remains mostly constant (not shown), as it is dominated by memory access time, with decreasing probability of cache miss for each additional completion.
In fact, for RT, it actually takes less time to compute each additional completion.
Furthermore, although we are also returning the completion string, each completion in SDT is about twice as fast as a random Access operation.
CT has an even larger ratio due to its less e cient Access operation.
Thus, by integrating string construction into the completion algorithm, we sigi cantly reduce the overall time required to enumerate the top-k completions.
Figure 10: Data structure size vs. dataset size.
the we have fewer completions to distribute the cost of Find-Locus over.
As SDT accesses more lines of CPU cache per completion, it performs worse than CT, with increasing time ratio.
RT further su ers from lack of memory locality among the top completions which magni es the e ect of cache miss.
Figure 11: Average time per completion vs. dataset size.
Figure 9: Completion time breakdowns.
In terms of build time, CT, SDT, and RT with unoptimized code currently take an average of 1.8, 7.8, and 7.7  s per string in QueriesA, respectively, with RePair compression taking up 73% of the time for the two succinct tries.
All algorithms use memory linear to the size of the binary output.
To assess the scalability of the data structures, we compare their performance on di erent size subsets of the QueriesB dataset.
Speci cally, to mimic practical scenarios where we have a limited memory budget and can only a ord to serve the most popular queries, we will generate these subsets by taking the top-N distinct queries in decreasing score order.
Figure 10 plots the change in average bytes per query as we increase the number of queries.
Overall, we see that lower count tail queries are longer and require more space across all techniques, likely due to the di erent characteristics exhibited by queries with only a few counts.
While SDT requires more space than CT below 100 queries due to its large sublinear overhead, its size continues to fall with increasing number of queries and actually becomes smaller than GZip on a wide range of dataset sizes.
We present in Figure 11 the e ect the number of queries has on the average time per completion for top-10 completion requests.
We use the synthesized workload based on the full QueriesB dataset to best approximate real world usage scenarios where users enter pre xes without knowing what queries the system can complete.
Thus, both the average number of completions and average completion length increase with the dataset size.
As shown, the average time per completion for CT increases very slowly, due to increasing completion length and more cache misses.
It is higher for smaller datasets as In practical scenarios, auto-completion needs to support not only exact pre x matches, but also inexact matches due to di erences in casing, accents, or spelling.
One way to support case and accent insensitive match is to normalize both the dataset strings and the input pre x into lowercase unaccented characters before computing the completions.
However, this removes the original casing and accents from the completions, which may be important for certain languages and scenarios.
An alternative technique is to apply a fuzzy completion algorithm, such as the one described by Duan and Hsu [10].
In short, after adding the root node to a priority queue, iteratively add the children of the best path to the queue, applying a penalty as determined by a weighted transformation function if the character sequence of the child node does not match the input pre x.
Once a path reaches a leaf node in the trie and has explained all characters in the input pre x, return the completion.
This fuzzy completion algorithm only requires basic trie traversal operations and access to the best descendant score of each node, which are supported by all of the proposed trie data structures.
As this algorithm essentially merges the top completions of various spell corrected pre xes, the ability to retrieve additional completions e ciently and on-demand is critical to meeting target performances on web-scale server loads.
Another common scenario is the need to associate additional data with each string entry.
For example, to map the injected partial URLs from the URLs dataset to their canonical forms, as shown in Figure 1b, we can create an array that maps the index of each string in the dataset to the index of full URL, or a special value if no alteration mapping is required.
These auxiliary arrays are often sparse and can be compressed e ciently using various succinct and compressed
 to a node o set, we can create a small bitvector with Rank and Select capabilities to convert between the o sets and indices.
Furthermore, some applications need to retrieve the top-k completions according to a dynamic score that depends on the pre x and completion.
As the static score is usually a prominent component of the dynamic score, an approximate solution can be obtained by taking the top-k(cid:48) completions with k(cid:48) > k according to the static score and re-ranking the completion list.
To truly scale to large datasets, we need to build the proposed trie structures e ciently.
Although we have not discussed the build process in detail due to the lack of space, we have implemented e cient algorithms that scale linearly with the size of the dataset.
For CT, we have further developed e cient techniques to merge tries with additive scores, enabling distributed trie building across machines.
In this paper, we have presented three data structures to address the problem of top-k completion, each with di erent space/time/complexity trade-o s.
Experiments on large-scale datasets showed that Completion Trie, based on classical data structures, requires roughly double the size of Score-Decomposed Trie, based on succinct primitives.
However, it is about twice as fast.
As it turns out, organizing the data in a locality-sensitive ordering is crucial to the performance gains of these two structures over the simpler RMQ Trie.
For scenarios where memory is scarce, Score-Decomposed Trie can achieve sizes that are competitive with gzip.
When throughput dominates the cost, Completion Trie can reduce the time for each completion to under a microsecond.
For most applications, the di erence of a few microseconds between Completion Trie and Score-Decomposed Trie should be negligible.
However, for algorithms that require numerous trie traversals, such as fuzzy completion where we consider a large number of locus nodes, the speedup from Completion Trie may become signi cant.
As handling big data becomes ever more important, succinct data structures have the potential to signi cantly reduce the storage requirement of such data while enabling e cient operations over it.
Although their theoretical performance matches their classical counterparts, there is still a noticeable gap in practice.
It is an interesting open question whether such gap can be closed, thus obtaining the best of both worlds.
