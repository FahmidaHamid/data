ACM 978-1-4503-2035-1 /13/05.The huge and ever fast increasing amount of information on the Internet has penetrated every corner of our life.
However, we become more easily overwhelmed by so ________________________ *Jian Cao is the corresponding author Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media.
much information and unable to find what we really desire.
When we follow events on Facebook, buy books on Amazon or add apps into a smartphone, systems may record our feedbacks, e.g., a rating assigned to a book.
Based on such observed feedbacks (or ratings) collected from like-minded users, collaborative filtering (CF) in recommender systems can predict personalized preferences to unconsumed items.
In general, CF methods can be subdivided into neighborhood-based and model-based approaches [5; 22; 26].
Therein, latent factor model based on matrix factorization (MF) [6; 9] has gained the dominance in recent years.
The essence to success in CF is highly dependent on the feedback data.
However, users are not always willing to provide feedbacks due to various personal reasons.
Even some applications possess the data sparsity problem in nature, for instance, users who has bought a new car recently may not have a new car purchase plan in next five years.
Thus most CF methods, including MF, suffer from the data sparsity [26] and cold-start [9; 23] issues.
The lack of reliable feedback data has become a major barrier for CF methods.
To deal with the sparsity issue, Cross-Domain Collaborative Filtering (CDCF), which leverages the information from multiple related domains, is an emerging research topic in recent years.
Some CDCF algorithms have been proposed in literatures [11], where the basic idea is based on the assumption of the existence of multiple related domains and the user preference learned from one dense domain, e.g.
movies, can be reused to make prediction in a sparse domain, e.g.
books (i.e. cross domain learning) [12; 18].
An early neighborhood based CDCF (N-CDCF) was mentioned in [1], but it can only provide a very local optimum solution as done by neighborhood based CF models (further analysis provided in the next section).
Recently, some cross-domain matrix factorization (CDMF) models [18; 24] have been proposed to overcome the local optimum problem of N-CDCF.
The underlying idea of CDMF can be illustrated using Figure 1 (b), where user factor matrix U serves as the bridge to transfer knowledge from auxiliary domain (A) to target domain (T).
Most CDMF models assume the auxiliary data is relative dense for all users or items [18].
However, we argue that this assumption is not always true in real world.
In general, our argument is based on the well-known power law, as illustrated in Figure 1(a), only the minority of users are rating frequently while the majority of users are quite inactive in providing feedback.
This observation might impact the hypothesis of traditional CDCF approaches, therefore resulting in the deterioration of recommendation performance.
phenomenon are ubiquitous in CDCF; (b) The demonstration of unacquainted-world issue in CDMF; (c) More accurate triadic factor analysis over CDCF.
Furthermore, due to the diversity of user interests a user is usually active in a few domains that she/he is really interested in, but silent in other domains hardly involved.
Given a set of domains, we call those user s uninvolved domains as unacquainted world.
Since each user has different domains of interest, the unacquainted-world phenomenon is common in CDCF problem, shown in Figure 1 (a).
Moreover, the ubiquitous unacquainted-world phenomenon may negatively impact the recommendation performance of CDMF models in heterogeneous domain settings.
Consider the example depicted in Figure 1 (b), CDMF aims to improve recommendation in the target domain T by utilizing the transferred knowledge (i.e.
the user factor matrix U) from the auxiliary domain A.
More specifically, this transferred user factor matrix U should be updated by taking into account users  feedbacks from T before serving as the user factor matrix U for target domain T [18; 24].
Now the problem occurs: no feedback is available for the last two users in T (i.e. the unacquainted world, marked with dashed red box) to adjust the transferred user-factor vector u, but the prediction of preference to an item in T is purely determined by the operation of uTw, which may yield an inaccurate result due to the unadjusted u and the heterogeneity of item factors between the heterogeneous domains A and T (e.g.
the heterogeneity between v and w).
Therefore, it results in unreliable prediction completely based on the preference learned from a heterogeneous domain.
Thus this raises a demand to devise a new cross-domain learning model by jointly leveraging the complementary data from multiple domains rather than simply relying on some dense feedback domain.
The major reason caused the above concerns is that CDMF deals with a set of user-item data over multiple domains in a flat manner but it does not consider the attribute of domain factor.
The absence of domain-specific information in factorization process leads CDMF to suffering from the unacquainted-world issue.
We argue that domain factors is an essential element in cross  domain  problem, so cross domain learning should take into consideration the full triadic relation user-item-domain to reveal the user preference on items within various domains in depth, rather than the dyadic relation user-item modeled by CDMF.
To learn such triadic factors from data, intuitively a tensor factorization (TF) could be introduced.
However, a standard TF model requires that the slice of each domain should be the same size.
Obviously, overlying all domain slices (differ in the number of items) in Figure 1 (c) cannot form a cubical tensor.
Inspired by PARAFAC2 [7], a special TF model, we propose Cross-Domain Triadic Factorization (CDTF) to relax the constraint that the same item-factor matrix is employed for all domains.
As illustrated in Figure 1 (c), CDTF allows an exclusive item-factor matrix for each domain to express heterogeneities.
In addition, user-factor matrix U in CDTF is used to model the general users concerns over all domains and the domain-factor matrix D carries the information to express the traits of each domain.
Hence each observation can be viewed as the result of triadic interaction among user, item and domain factors.
Further, we can interpret that the domain-specific user factors is generated by the interaction between domain factors and general user factors as shown in Figure 1 (c).
Obviously, such triadic factor model avoids the unacquainted-world issue in CDMF.
In real-world scenarios, another difficulty is that the user explicit feedback data (e.g., ratings) are sometimes hardly available.
How to alleviate this kind of problems becomes a new research direction in CF and more and more studies attempt to make use of implicit feedbacks (here implicit feedback means the intention conveyed by user activities, such as purchase history or browsing behavior) [6;
 implicit feedbacks, namely CDTF-IF, which can effectively deal with the one-class implicit feedback data that CDTF cannot handle.
Moreover, in cross domain learning problems, tuning the trade-off parameters over domains is an essential step to achieve better performance [16; 24].
Therefore, in this work we also investigate an automated and robust trade-off parameters determination approach for our models based on genetic algorithm (GA).
The contributions of this paper can be summarized as follows:   We address the CDCF problem by formulating a generalized triadic relation user-item-domain.
  We devise a cross-domain triadic factorization (CDTF) model to learn the triadic factors for user, item and domain, where the item dimensionality varies with domains.
  To alleviate the absence of explicit feedbacks, we extend our proposed CDTF model to be able to utilize the implicit feedbacks that CDTF cannot handle.
  We study an automated optimal weight parameter estimation algorithm based on genetic algorithm.
  We perform experiments on two real world datasets to evaluate our models and make comparisons with other state-of-the-art models.
Neighborhood and MF based methods are two kinds of dominant approaches in CF.
Although such classical CF methods can be applied to CDCF, they have disadvantages in nature.
Notations:   = { 1,  2,   ,  } denotes all the domains for modeling,   = { 1,  2,   ,  } denotes the users in   and   = { 1  } denotes items belonging to the domain  .
 ,  2  ,   ,   N-CDCF: Neighborhood based CF compute similarity between users or items, which can be subdivided into two types: user-based nearest neighbor and item-based nearest neighbor [26].
For a user-based CDCF algorithm, we first calculate the similarity,  , , between the users   and   who have co-rated the same set of items.
The similarity can be measured by the Pearson correlation: (c)(b)(a)5?4?5?4?1?3?2???5?1???2?53?41?A2??5?2??3?4?4??????
?TuUwvWVTransferitem-factor vectoru'w?=w unacquainted world phenomenonuuser-factorvectorwWVuUTAw?=Userduu=u'w?= f(du)domain-specific user factors:*3235345232521513522452BookMusicUserPower Law596 ,  =    ,  ( ,     )( ,     ) (1)    ,  ( ,     )
    ,  ( ,     )
 for different domains may quite heterogeneous so MF-CDCF fails to express them.
Furthermore, such model absolutely loses the information to model domain factors for CDCF problem.
      ,   =   where  ,  =       (   =   ) denotes the items over all domains   co-rated by   and  ;  ,  and  ,  are the ratings on item   given by users   and   respectively;   is the average rating of user   for all the items rated.
Then, the predicted rating of an item   for user   can be calculated by a weighted average strategy [22]:        ,  =   +      ,     ,  ( ,     )    ,  | , | (2)   denotes the set of top   users (  neighbors) that are where  ,  most similar to user   who rated item  .
Similar to user-based algorithm, the item-based CDCF needs to compute the similarity,  , , between item pair   and  .
Given co-rated cases  ,  over   and  , i.e. each case is that a user rated both   and  , the Pearson correlation is given by:  ,  =    ,  ( ,     )( ,     ) (3)    ,  ( ,     )
    ,  ( ,     )
 Then, the predicted value,  , , is taken as a weighted average of   .
the ratings for neighboring   items rated by  , denoted  ,   ,  =   +     :  ,     ,  ( ,     )    ,  | , | (4) MF-CDCF: The method to perform MF on a CDCF problem is straightforward.
We can construct a matrix   that takes all users   as the rows and all items   in domain   as the columns.
Thus, we easily obtain   matrices  1,  2,   ,   for   domains.
Then, an augmented matrix,   , can be built by horizontally concatenating all matrices as shown in Figure 2.
Figure 2: Horizontal concatenation of matrices for all domains With the matrix   in hand, we can exploit any classical MF algorithm, e.g.
the frequently used stochastic gradient descent (SGD) method [9], to construct user factor matrix and item factor matrix.
These factor matrices are used for prediction.
Disadvantage: Neighborhood based models are most effective at detecting much localized relationships and unable to capture the totality of weak signals encompassed in all of a user s ratings.
For example,  1 rated items { 1,  2} ,  2 rated items { 3,  4} and  3 rated items { 2,  3}.
The direct correlation between  1 and  2 is zero.
In fact,  1 is correlated with  3 by  2 and  2 is correlated with  3 by  3, so  1 is transitively correlated with  2 instead of zeros.
It proves that N-CDCF cannot obtain a global optimal solution, especially when the data is very sparse.
MF-CDCF accommodates items from all domains into a single matrix so as to employ single-domain MF.
However, single domain model assumes the homogeneity of items.
Obviously, item factors


 Before clarifying our model, we firstly introduce some basic notations, operations and algorithms for TF models.
There are different TF models in literatures, such as Tucker model, CP model (canonical decomposition/parallel factor analysis (PARAFAC)) [8;
 extension of PARAFAC2 which needs to cope with CP.
The order of a tensor is the number of dimensions, also known as ways or modes.
In this paper, tensors are denoted by boldface script letter, e.g.
 . Matrices are denoted by boldface capital letters, e.g.
 . Vectors are denoted by boldface lowercase letters, e.g.
 .
Entries are denoted lowercase letters with subscripts e.g.
 , , .
In addition, we denote the   row of a matrix   as  , , the   column as  ,  and  ,  for the entry ( ,  ).
The   mode matricizing operation maps a tensor into a matrix,
 e.g.,  (2) represents the mapping      (2) [8; 15].
  denotes the Kronecker product and   denotes the Khatri-Rao product, e.g.
      = [ ,1    ,1  ,2    ,2    ,     , ].
  is the element-wise product while   is the element-wise division.
denotes the inner product and the norm of a tensor is,   =      .
 , , , ,   , ,  Figure 3: The CP factorization of a three-order tensor
 CP model decomposes a tensor into a sum of rank-one components as illustrated in Figure 3.
For instance, given a three-order tensor
  , the factorization can be writtern as   =  ,  ,   =    ,     =1  ,     ,  , where  ,  ,   are R-component factor matrices and   denotes the outer product, i.e. the entries are computed  , ,  =
   .
Let   be a three-order tensor with the size      =1      .
We can formulate the problem of fitting   as a least squares optimization problem [8]:  , , ,  min  ( ,  ,  ) =




  
  


  


 where regularization terms are added to avoid overfitting,  is the Frobenius norm and  ,  ,   are regularization parameters.
It is easy to prove that the partial derivative of the objective function (5) w.r.t.
  is given by:  
 = ( (1)    (1))(     ) +   where   =  ,  ,  .
Setting the above equation equal to zero and the property of pseudo-inverse of Khatri-Rao product [27] yields:   =  (1)(     )( T     T  +  )  (6)
   =  (2)(     )( T     T  +  )  (7)   =  (3)(     )( T     T  +  )  (8) Above derivation corresponds to a regularized alternative least square algorithm, CP-ALS-R, given by Algorithm 1.
The complexity for this algorithm is proportional to   + (  +   +  ) 2, per iteration.
Since we normally have     (  +   +  ), the computational complexity is O( ).
Algorithm 1: [ ,  ,  ] = CP-ALS-R( ,  ,  ,  ) Input:   the tensor for factorization,  ,  ,   the regularization paramters Initialize  ,  ,   Output:  ,  ,   the factor matrices Begin:





 End
 We have discussed the weaknesses of traditional CDCF approaches in the previous section, where items from all domains are mixed together, so the item latent factors cannot be well learned due to the heterogeneity between domains.
In addition, all those approaches discard the most important domain-specific information.
A straightforward method to capture the 3-way interaction between user-item-domain is to model this triadic relation by a cube, i.e. a three order tensor, where each frontal slice in this cube corresponds to a rating matrix for each domain.
Unfortunately, the inconsistent number of items for each domain, as illustrated in the left part of Figure 4, cannot form a standard tensor.
PARAFAC2 [8; 15] relaxes CP s constraints that apply the same factors across a parallel set of matrices.
Inspired by this idea, we propose the cross-domain triadic factorization (CDTF) model, which can be applied to a collection of rating matrices for domains that are equivalent in the User dimension but vary in the Item dimensions over domains.
Figure 4: Slices of domain-specific matrices with heterogeneous items are transformed into a cubical tensor containing virtual items with identical length.
The standard CP model presented previously can be written as the factorization form w.r.t.
each slices,  , for k=1 to K   =  T +   (9) where  ,   are the factor matrices as given in previous section,   = diag( , ) is an       diagonal matrix of weights for the slice  , and   denotes the residuals [11; 27].
Therefore, we can rewrite the objective function (5) w.r.t.
the slices   for k=1 to K min  ( ,  ,  ) =

        T 
 +
  =1  
  




  


 Let us denote   = { 1,  2,   ,  } to be the rating matrices for domains, where each matrix,   , has the size       ,   is the number of users and   is the number of items in the kth domain.
We apply a PARAFAC2-like modeling strategy to the collection of rating matrices,  , with varying sizes in Item mode (see Figure 4).
Analogous to Eq.
(9) for CP model, we can write the similar form of factorization w.r.t.
the rating matrix of each domain.
  =   T +   (11) where   denotes the       factor matrix (it refers to user factors in our model),   is the       factor matrix (it refers to item factors in our model) for the slice   and   is an       diagonal matrix (it refers to domain factor in our model) for slice  .
Then, we easily obtain the objective function with the same form as Eq.
(12).
However, such PARAFAC2-like factorization is not unique without additional constraints [11; 27].
To improve the uniqueness property, Harshman [4] imposed a constraint that the cross product T  for k=1,   , K.
  Thus, Eq.
(11) can be written as: T  is a invariant matrix over k, i.e.,   =     =  ( )T +   (12) where  ,   are defined as usual,   is a column-wise orthonormal T  =  ) of size       and   is an       matrix matrix (i.e.   that does not vary by slice.
The cross-product constraint is enforced implicitly since   T  = ( )T( ) =  T  =   Then, the objective function can be given according to Eq.
(12) min  ( ,  ,  ,  ) =


        ( )T  (13)
  =1 Weights over Slices: In our model, the user factor matrix   is shared across all domains (see Eq.
(13)), i.e. learning   is affected by the loss on each slice.
Some domain may have a lot of items and feedbacks (heavy slice) while other domain may only have a few of items and a few feedbacks (light slice).
If the loss from a heavy slice overwhelms the loss from light slices,   is fully determined by the heavy slice.
On the other hand, the scale of ratings on each slice may be different, e.g.
the ratings on some slices are in the range of 1-5 and others may be 1-100, so the larger-scaled rating slice tends to account for more loss.
More importantly, sometimes we deliberately require that the learning of   is mainly determined by feedbacks from target domain so as to perform better prediction.
Consequently, we add the weight parameter,  , to the objective function (13) to adjust the penalty of loss on each slice as given by Eq.
(14).
If we assign a large weight to some domain slice, then factor matrix   is mainly learned from the factorization over this slice.
Note that the change of   will update other factor matrices  ,  ,   in turn during the process of factorization.
Therefore, we can control the learning result of all factor matrices by tuning the weight assigned to each slice.
min  ( ,  ,  ,  ) =


    (     ( )T)  (14)
  =1 X3X2BookMusicMovieVideoX1Y1UserVirtual Item598Minimizing Eq.
(14) is obviously equivalent to minimizing following objective function [7].
min  ( ,  ,  ,  ) =


        T  (15)
  =1 Let   =   ,   =   =  diag( , ), it is easy to see Eq.
(15) corresponds to a           cubical tensor as illustrated in the right part of Figure 4, where each slice   has the identical size       (N users and R virtual items).
Finally, we can obtain the full objective function for CDTF by appending the regularization terms as given by Eq.
(16).
min  ( ,  ,  ,  ) = +
  =1

        ( ) T 
  



  
  




 We need to reconstruct all missing values for prediction but the standard fitting algorithm for PARAFAC2 is based on the complete data [7].
Therefore, we need to design a new fitting algorithm which allows dealing with missing data.
Thus, we apply an Expectation Maximization (EM) [25; 27] sub-procedure into the fitting algorithm to handle the incomplete data by iteratively imputation after each full cycle of updates.
   
 ( +1) =       ( ) + (     )    T (17) (0) =   can be pre-computed and   is an indicator where   matrix whose entry ( ,  ) is one if  ( ,  ) has been rated (for observed values) and zero otherwise (for missing values),   is an all ones matrix that has the same size as  .
  So far we have described the detail of CDTF model and the EM algorithm for missing data handling.
In summary, Algorithm 2 gives the whole factorization scheme for CDTF extending from the direct fitting algorithm for PARAFAC2 [2; 7].
In this algorithm, the computational complexity is mainly dependent on the internal sub-procedure CP.
Here, we use the CP-ALS-R so the complexity is O( ) .
Since the   (the number of domains) and the   (dimensionality of factor) are small, the generated tensor   can be decomposed very efficiently.
Given the estimated factor matrices  ,  ,  ,   , the prediction of user  s rating of item   of target domain   is given by:  ( ,  )    T  where  T =  ,     ,  is the domain-specific user factors of   and  T =  ( , )  is the item factors of  .
As a whole, the reconstructed rating matrix of target domain   is given by      ( )T. Let   denote the set of  s all unrated items, then we can obtain the personalized recommendation ranking over   by sorting  ( ,  ) in a descending order.
In real applications, the explicit feedbacks are not always available but implicit feedbacks are easily gained from user behavior history.
For example, a user may not give ratings (explicit feedbacks) to the books she/he has bought but his purchase history can be considered as an implicit feedback.
Consequently, some single-domain CF methods have been proposed to exploit the more abundant implicit feedbacks [6; 20; 21].
However, even implicit feedback based CF models still suffer from data sparsity and cold-start issues.
Algorithm 2: [ ,  ,  ,  ] = CDTF( ,  ,  ,  ,  ,  ) Input:   the rating matrices for each domain   the weights for each domain   the indicator matrices for each slice  ,  ,   the regularization parameters Output:   the factor matrix for users   the factor matrix for domains  ,   the factor matrices for items Begin: Initialization:


 Initialize   principal eigenvectors    =1 by SVD T   
 EM Steps:
 T T k=1, ,K    
   k=1, ,K

   T ) [ ,  ,  ] =CP-ALS-R( ,  ,  ,  )


 Post Steps:  ,  , i.e. rescale back   k=1, ,K

 End     In particular, one-class implicit feedback is dominant in real world.
For example, a one-class purchase record matrix marks entries with 1 to indicate the buy and the rest of data on this matrix are unknown.
Since such one-class data is purely indiscriminate, most explicit feedback based MF/TF methods, including CDTF, cannot work well.
Hence, we devised an implicit feedback enhanced CDTF (CDTF-IF) model to deal with one-class feedbacks via confidence modeling.
In fact, implicit feedbacks can indirectly reflect opinions through user behavior because users may deliberately choose to access which items [17].
Given an observation matrix   of some domain, let us introduce a binary matrix  , where its element,  , , indicates whether the entry  ,  has an observed value.
Note that the matrix   can be rating based like above, or simply all ones to indicate observed entries for one-class implicit feedbacks.
 ,  = {

  ,  = 1 can be interpreted that   shows some explicit like to item   whereas  ,  = 0 indicates   never consumed  , which implies  , to some extent, implicitly dislikes  .
However, such implicit dislike can stem from many other reasons beyond real dislike.
For example, the user might be unaware of the existence of the item, or unable to consume it due to its price [6].
Therefore, we can use varying confidence levels to represent the degree of users  like or dislike over each item.
The confidence level of user preference is proportional to the value of given rating.
That is, the higher the rating is given by a user, the more the confidence indicates that the user indeed likes the item.
For example, if a user rates an item with 5 (the highest score), it indicates she/he likes this item very much so we can assign a high
 an item with 1 (the lowest score), it implies that she/he has much dissatisfaction with this item so a low confidence level is assigned to identify the dislike.
On the other hand, to model the confidences of users  dislike over unrated items (missing values in  ), a very low confidence level is associated with these entries since we have no evidence to prove the users  explicit dislike.
According to the above analysis, we can construct a confidence matrix   to indicate the level of users  like/dislike over each item:  ,  = {
  ,  = 0  ,   ,  = 1 (18) where     1 is a constant to scale confidence according to the rating of items.
For missing value ( ,  ), a small constant 1 is assigned to denote the minimal confidence of dislike.
Then, we can add the confidence matrix   into Eq.
14 and replace the rating matrix   with indicative matrix   for each domain.
Immediately, we obtain the following objective function.
min  ( ,  ,  ) =


  =1    [    (     

 (19) where   is the weight as discussed in CDTF and   is the confidence matrix over each domain.
Similar to the derivation of CDTF, it is possible to transform Eq.
(19) into a cube based TF model by variables substitution as follows.
Let   = ( )     be the observation matrix and   = T) be the approximate matrix, and then we ( )   (   substitute the variable   with the factorization form  ( )T.
So Eq.
(19) can be rewritten in the form of Eq.
(20) with the regularization terms appended.
min  ( ,  ,  ,  ) = +

  
  =1



        ( )T   


  


 where   satisfies the column-wise orthonormal constraint as previously.
Accordingly, the whole factorization algorithm w.r.t.
factor matrices  ,  ,  ,   is given by Algorithm 3.
According to Eq.
(19), the ranking score matrix for target domain   is computed by   =   T .
So the recommendation ranking over user  s unrated items   can be generated by descendingly sorting the  ( ,  ) .
Here, the ranking score matrix   can be computed by the back-substitution:   =   T = [ ( )T]   ( ).
However, such back-substitution is not necessary because   and  ( , ) = 1 are constant for all unrated items.
Therefore, we can directly sort each row of   =  ( )T in a descending order to rank the items for each user.
Weight parameters are also quite valuable to be discussed because they play an important role in CF models [16; 24] to control the amount of impact from auxiliary data.
It has been reported in many literatures that imposing both too much and too little influence will degenerate the performance [16; 24].
So finding well-tuned weight parameters is an inevitable step to achieve better performance.
Algorithm 3: [ ,  ,  ,  ] = CDTF-IF( ,  ,  ,  ,  ,  ) Input:   is the indicative matrices for each domain   is the weight for each domain   is the confidence matrix for each slice  ,  ,   are the regularization parameters Output:   is the factor matrix for users   is the factor matrix for domains  ,   is the factor matrices for items Begin: Initialization:

 by SVD Initialize   principal eigenvectors  
  =1 Iteration:
 T T k=1, ,K   T   
   k=1, ,K

   T ) [ ,  ,  ] =CP-ALS-R( ,  ,  ,  )
 Repeat 3  7 until convergence
 End The same holds true for CDTF and CDTF-IF, where the weights assigned on each domain exactly act as such trade-off parameters.
Too large weights assigned to auxiliary domains may overwhelm the information from target domain while too small weights may fail to transfer enough knowledge to target domain.
Most of current CF methods [16; 24] usually only involve one or two auxiliary relations so a common way to find an optimal model is to select the best setting from a group of manually given values via cross validation.
However, the CDCF problem often involve several domains and the number of possible weights assignments grows with the number of domains exponentially.
For example, if we have four domains and the weight on each domain has five possible values {0.01, 0.1, 1, 10, 100}, then the possible number of combinations of weights is 54.
Obviously, it will be a painful process to find the optimal weights assignment in semi-manual way by cross validation.
Moreover, such heuristically pre-given values do not guarantee to cover real optimal values.
Hence, we need a more robust and automated method to find the best weights assignment.
Figure 5: Searching optimal weights assignment by crossover and mutation operators using GA Here, we employ the genetic algorithm (GA) [3] to find such optimal weights assignment.
We fix the weight on target domain to be 1 so various weights assignments on auxiliary domains act as the individuals in the population.
In GA the crossover operator combines a part of elements in each parent to form children for the next generation, so it enables the automatic search for the best combination of weights as depicted in Figure 5(a).
And the w1w2w3w4w1w2w3w4w1w2w3w4w1w2w3w4w1w2w3w4w1w2w3w4w ~ N(w, 2)(a) Crossover(b) MutationCurrent GenerationNext Generation600mutation operator applies random changes to a single individual in the current generation to create a child.
As illustrated in Figure 5(b), the weights in each generation are automatically adjusted by mutation.
Since the range of weight is large, a uniformly randomized initial population will take too long time to converge.
So we take the following strategy to initialize the individuals with exponential growth, where     (0,1] is a constant to scale weight,   and   are integers to control the range of weight,   is an all-one vector with the length equal to the number of auxiliary domains.
  = (    10 )     for   =  ,   ,     Taking an example, we initialize the population as   = { 0.5,  1}, with   =  2 and   = 2, and then we obtain 10 weight vectors (i.e.
individuals) ranging from 0.005 to 100.
Accordingly, the change caused by mutation should match with the order of weight instead of fluctuating in the entire range, so we set up the following mutation rule to generate the child weight  ( +1), where   specifies the maximum number of generations and  ( ) denotes a normal distribution with the mean  ( ) and the standard deviation   shrinking as generations go by.
 ( +1) ~   ( ( )|( ( ) ( +1))
 ) ,  .  .  ( +1) > 0  ( +1) = 1   (  + 1)
 Following the above initial strategy and mutation rule, we can run the GA given any fitness function, e.g.
the MAE (mean absolute error) over the testing dataset using CDTF.
The experiments are conducted on two real world datasets, namely the ratings of Amazon products and the follow records of a social networking site.
In the following experiments, we evaluated the performance of the rating and ranking prediction by a set of metrics and compared our models with other state-of-the-art approaches.
Amazon 1 is the most famous e-business website to sell diverse products, such as books, DVDs, shoes, etc.
The dataset [10] was crawled from Amazon website and it contains 1,555,170 users and 1-5 scaled ratings over 548,552 different products covering four domains: 393,558 books, 103,144 music CDs, 19,828 DVDs and
 dependent across these domains, so it is very suitable to test CDCF algorithms over this dataset.
Data Preparation: In this experiment, we selected Book and Music CD as the target domain to evaluate respectively.
We filtered out users who have rated at least 50 books or 30 music CDs so that there are enough observations to be split in various proportions of training and testing data for our evaluation.
Finally, 2,505 users were selected, and in addition we retrieved all items rated by these users in these four domains and set aside top K rated items for each domain respectively.
Table 1 shows the statistics of the data for evaluation.
Then, we constructed rating matrices over filtered out data for each domain.
  Sparse Data Case: To simulate the sparse data problem, we constructed two sparse training sets, TR20 and TR75, by 1 http://www.amazon.com/ respectively holding out 80% and 25% data from the target domain Book, i.e. the remaining data of target domain for training is 20% and 75%.
The holdout data servers as ground truth for testing.
Likewise, we also construct two other training sets TR20 and TR75 when choosing Music as the target domain.
  Unacquainted World Case: We randomly select half users and hold out all their data from target domain to simulate the unacquainted world phenominon.
The training set used for this case is denoted as TRuw.
Table 1.
Statistics of amazon data for evaluation Domain Items Book Music





 Avg.
# ratings for each item Avg.
# ratings for each user







 Density



 Methods: In all following experiments, a group of state-of-the-art methods are evaluated for comparison, including our models.
When running the evaluation using each compared method, we set the dimensionality of factors and other hyper-parameters by cross validation.
  MF-SGD: It is a single domain based MF model to minimize the squared error by stochastic gradient descent [9].
It directly takes the rating matrix of the target domain as input and cannot handle the cold-start problem.
  N-CDCF-U: A user-based neighborhood CDCF model uses Eq.
(1) for prediction.
In this experiment, we use k=10 closest users.
  N-CDCF-I: An item-based neighborhood CDCF model uses Eq.
(3) for prediction.
In this experiment, we use k=10 closest items.
  MF-CDCF: A MF model, described in Section 2, takes the long concatenated rating matrix as input.
  CMF: Collective matrix factorization [24] is a CDMF which couples rating matrices for all domains on the User dimension so as to transfer knowledge through the common user-factor matrix   CDTF: Our model, which is described in Section 3.2, takes one of the above domains as target domain to perform prediction and others as auxiliary domains to borrow knowledge.
  PF2-CDCF: The main difference of PARAFAC2 [4] from our CDTF is that it does not have the mechanism to adjust the amount of knowledge contributed by each domain.
Metrics: we used the most widely used evaluation metric for CF problem, namely Mean Absolute Error (MAE) [26], to measure the rating prediction quality.
 ( ,     , )
  ,  where  ,  denotes the true rating user   gave to item  ,  ,  is the predicted rating, and   denotes the number of ratings in testing set.
Comparison: We evaluated the prediction performance using three differently sparse training sets, namely TR75, TR20 and TRuw constructed above.
Table 2 reports the evaluation results with setting Book and Music CD as target domains respectively.
with different training sets ---

Models TR20 TRuw









 Target Domain: Book Target Domain: Music TRuw
 ----















From Table 2, we can find that most CDCF models achieve much better performance than single-domain CF model.
Therein, our model, CDTF, significantly outperforms all other comparative CDCF methods over all testing sets.
Especially, more than 35% improvement is achieved in the case of TR75 training sets, which illustrates that CDTF can better capture personalized factors for each domain when users  feedbacks are relatively sufficient.
N-CDCF-U also achieves a not bad performance when the data is relative dense, i.e. TR75, but the performance decreases very fast when the data becomes sparser.
As analyzed in Section 2, such neighborhood based method usually fails to find global similarity among users when the data is sparse.
The typical CDMF model, CMF, overall outperforms MF-CDCF.
It is because that CMF provides a more effective way to transfer knowledge between domains instead of ignoring the heterogeneity between domains and integrating all data into a single matrix as MF-CDCF.
Not surprisingly, CDTF achieves the best performance again in the unacquainted-world cases, i.e. using the training sets TRuw, which can be mainly attributed to the triadic relation modeling over user-item-domain so CDTF can better recover the domain-specific user preferences than other models.
In comparison, CMF lags much behind CDTF.
The reason is that CMF only models a couple of dyadic relations over users and items like traditional MF models so, for the unacquainted-world cases, it cannot learn the domain-specific user factors due to the absence of domain factors.
Specially, PF2-CDCF cannot achieve a good performance though it is also a TF model based on triadic relation.
The main reason is that it does not provide a mechanism to trade off the amount of influence contributed from each domain.
Next, we will demonstrate impact of weights assignment on the prediction results.
Impact of Weights: CDTF offers a flexible mechanism to balance the amount of influence among auxiliary domains by tuning the weight assigned to each domain.
For example, we selected Book as the target domain and varied the weight on all other auxiliary domains from 0.01 to 100 exponentially and report the MAEs over TR20 in Figure 6 (a).
We can find that the performance is quite different with different weights assignments.
To find the optimal weights assignment, we ran the GA with initial population   = { 0.33,  0.66,  1} and   =  2 ,   = 2 , i.e. there are totally 15 initial individuals with different scale.
The rightmost bar in Figure
 much better than those results with heuristically setting weights.
Figure 6 (b) depicts the MAEs changing with iterations, and we can find that it converges very fast and reaches the optimal result within 10 generations.
2 http://www.kddcup2012.org/c/kddcup2012-track1 Figure 6: (a) Comparison of weights assignment optimized by GA to the weights settings varying from 0.01 to 100; (b) The converging status over generations for searching the optimal weights assignment.
Table 3 reports the results of optimal weights assignments learned through GA.
These results prove an obvious truth that the target domain needs only a little information transferred from auxiliary domains (relative small weights on auxiliary domains) if there are sufficient data on it, but it needs to leverage more and more information from auxiliary domains (relative large weights on auxiliary domains) when the data become sparser.
Table 3.
Optimal weights assignments found through GA over six different training sets Weight wBook wMusic wDVD wVHS Target Domain: Book Target Domain: Music TRuw





 TR20 TRuw













 Social networking sites (SNS) may be the most successful product in the age of Web 2.0.
People share videos, blogs, games, movies reviews and all kinds of things on SNS but most people only focus their interests on very a few domains.
Finding possible attractive items in people s unacquainted domains cannot only improve user experience but bring more profits.
In a SNS, we often only know what items users explicitly like, e.g.
the applications added or the groups followed, but no information about what they explicitly dislike.
Therefore, it is suggested to perform the implicit feedback CDCF algorithm.
In this experiment, we use the SNS dataset provided by KDD Cup2, where the dataset provides the profile for each user about the items she/he has followed.
These items are categorized into different domains with anonymous names which may be interpreted as game, sports, entertainment, etc.
Hence, we ran CDCF methods over this dataset to evaluate the performance of item ranking.
dataset, namely A, B, C and D, are selected for evaluation.
We filtered out 7,000 users who have followed at least 10 items in domain A and B.
Then, we obtain a dataset that contains about one million follow records over above four selected domains.
These follow records are representing one-class valued matrix for each domain, that is, the entry with value 1 denotes a user has followed this item.
Table 4 illustrates the statistics of this dataset.
Table 4.
Statistics of SNS data for evaluation Domain Items Avg.
# users Avg.
# items following an item a user Density following



















 In this experiment, domain A and B are chosen as the target domains for evaluation respectively.
For each target domain, we apply the same strategy to construct two training sets TR25 and TRuw as done in the first experiment to evaluate the sparse-data and unacquainted-world case respectively.
All the holdout data are used as ground truths for testing.
Methods: Some models in the first experiment do not support implicit feedbacks, so they cannot handle this dataset consisting of one-class values.
We provide some other methods instead.
  Most-Pop: It is a most widely used strategy to rank item by its popularity (measured by the number of users following).
  N-CDCF-U: Given a user, the similarity with other users is computed by cosine similarity over all items (using binary ratings).
Then, the ranking score is computed as a weighted average rating over all other users.
In this experiment, we use k=10 closest neighbors.
  MF-CDCF-IF: The concatenated matrix based CDCF model described in Section 2, taking advantage of implicit feedback [6].
  CDTF-IF: Our cross-domain factorization model using implicit feedback, described in Section 3.3.
  PF2-IF: We extend the original PARAFAC2 [4] to support implicit feedbacks as applying to CDTF-IF.
The main difference from CDTF-IF is that PF2-IF does not have weight parameters to trade off the influence from each domain.
Metrics: We use two frequently used metrics, recall and AUC [21;
 i.e. the positively followed items for each user in testing set.
  recall@N considers the positively followed items within the top N, a high recall with lower N will be a better system:  @  = # @  | |   AUC (Area under the ROC curve) measures the probability that a system ranks a positive instance higher than a negative one.
   
    ( ( ) <  ( ))  \  | |   | \ | where   denotes all items in the target domain,  ( ) retrieve the rank of item   created by some model and  ( ( ) <  ( )) is the delta function returning 1 if  ( ) <  ( ) and 0 otherwise.
Below we report the results using the average recall and AUC from all testing users.
Comparison: The performance of prediction is evaluated using two training sets TR25, TRuw with setting domain A and B as the target domain respectively.
For our model CDTF-IF, the scaling constant in Eq.
18 is set as   = 10 and the weight parameters are automatically optimized by GA. Table 5 reports the results of all comparative methods using the metric AUC.
Table 5.
AUC (the larger the better) of comparative models over different training sets Model Most-Pop



 Target Domain: A Target Domain: B





 TRuw










 TRuw




 A little surprisingly, the Most-Pop method performs better than all other models except CDTF-IF.
Through a further consideration, we concluded it reveals a general fact that the hot events, music, movies, tweets, etc.
are usually listed on the home pages of SNS so users will actively or passively keep their eyes on these popular items and share them with their friends.
Such  rich get richer  phenomenon over items is ubiquitous on SNS so it leads to a high AUC.
The neighborhood based method, N-CDCF-U, does not perform very well due to the inherent weakness of finding the closest users over the sparse data.
CDTF-IF surpassing Most-Pop proves that CDTF-IF not only concerns popular items but also better captures personalized preferences.
The metric recall@k can effectively check if a recommender system can successfully retrieve the items that user has shown positive preference by comparing the top k recommended items from its returned list.
Hence we evaluate recall@5~100 over all comparative methods.
Figure 7 reports the results over four different training sets.
Most-Pop does not achieve a high recall@k when k is relative small, which illustrates that apart from some most popular items people tend to follow much more personalized favorite items.
Similar to Most-Pop, N-CDCF-U also depends on other users  preferences so its performance is close to Most-Pop.
In particular, PF2-IF lags behind CDTF-IF due to the lack of adjusting the appropriate amount of influence among target domain and auxiliary domains.
Obviously, as illustrated in all four figures, the plots of CDTF-IF are consistently above those of all other models, so we can conclude that CDTF-IF can better capture the domain-specific personalized preference than other models.
Most state-of-the-art CDCF models are extended from single-domain MF models, where knowledge from auxiliary domains are transferred into target matrix by some shared factor matrices.
Codebook Transfer [12] assumes some cluster-level rating patterns, which are represented by a codebook, can be found between the rating matrices in two related domains.
Rating-Matrix Generative Model [13] extends this idea with a probabilistic model to solve collective transfer learning problems.
In reality, there are many cold-start users for most domains.
Therefore, it is always out of the question to find common patterns when the user data is absent in some domain, i.e. the unacquainted world case.
Dual Transfer Learning (DTL) [14] exploits the duality between by matrix tri-factorization, which mainly aims to solve the clustering or
 target domains, the marginal distribution corresponds to common latent features learned from the data over all domains, and the conditional distributions corresponds to domain-specific latent features.
However, the explicit features are commonly not available in collaborative filtering so DTL is not applicable to the CDCF problems as studied in this paper.
Since the user preference is not exclusive in a single domain, a more straightforward way is to transfer knowledge through the user-factor matrix.
Collective matrix factorization (CMF) [24] couples target matrix and all auxiliary matrices on User dimension to share the user factor matrix across all domains.
Similar to our model, CMF assigns a weight to the loss of fitting each matrix so that it can control the amount of influence from each domain on the user factor matrix.
However, CMF does not provide a mechanism to find an optimal weights assignment.
Coordinate System Transfer (CST) [19] learns the user-factor matrix  A from an auxiliary rating matrix in the first step, and then generates the user-factor matrix  T for the target domain based on  A , with the regularization of penalizing the divergence between  A and  T.
As pointed at the beginning, CST cannot be applied to the multiple domains (more than two) scenarios as studied in this paper.
Furthermore, all above models inevitably suffer from the unacquainted-world issue, which may lead to very poor recommendations.
Hu et al. [6] presented an implicit feedback based MF model, where they used a similar strategy with us to assign a small confidence to unrated items to represent implicit dislike.
Bayesian personalized ranking (BPR) [20] treats explicit rated item as positive class and unrated item as negative class and it uses those classified items to represent the preference ordering between each pair of items.
However, it degenerates into a one-class feedbacks problem (all negative items) for cold-start users.
Hence, no single-domain model can work.
It will lead to a typical unacquainted-world problem if we resolve such cold-start implicit feedback problem using CDMF models.
To the best our knowledge, we are the first to introduce implicit feedbacks into the CDCF problem.
In summary, current CDMF methods generally cannot address the unacquainted-world issue because they only model the user and item factors but absolutely ignore the domain factors, whereas our CDTF models introduce the triadic interaction among user, item and domain factors so as to overcome the unacquainted-world issue.
In this paper, we have discussed the requirements of CDCF in current web era and limitations of current CDCF methods in an unacquainted world.
Our triadic relational CDCF solution, namely CDTF and CDTF-IF, is proposed.
The experiments have evaluated the performance of rating and ranking prediction in terms of various metrics using our models and other comparative methods.
The evidence from all results has shown that our cross-domain factorization models significantly outperform all other state-of-the-art methods, especially for cold-start cases.
It is because our tensor based models can better capture the triadic relation between users, items and domains than only the dyadic relation between users and items modeled by other methods, which lose the domain-specific information.
The experiments also proved the efficiency of our GA algorithm to find an optimal weights assignment, which can achieve a much better prediction than PARAFAC2.
In the future, we may extend our model to be a time-varying CF model, since the number of items is always changing over time and users can give feedbacks to the same item multiple times.
Therefore, we can create a feedback matrix for each time stamp so as to construct a TF model like CDTF.
Such model can better capture the temporal factors and the shift of item factors over time.
This work is partially supported by China National Science Foundation (Granted Number 61073021 and 61272438), Research Funds of Science and Technology Commission of Shanghai Municipality (Granted Number 11511500102 and 12511502704) and Cross Research Fund of Biomedical Engineering of Shanghai Jiaotong University (YG2011MS38).
Figure 7: Comparison of CDTF-IF to Most-Pop, N-CDCF-U, MF-CDCF-IF and PF2-IF using the metrics recall@5~100 : (a) T25 w.r.t Target Domain A; (b) Tuw w.r.t Target Domain A; (c) T25 w.r.t Target Domain B; (d) Tuw w.r.t Target Domain B.
