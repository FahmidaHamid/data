A well established problem in the traditional Information Retrieval (IR) paradigm of a user issuing a query to a search system, is that a user is quite often unable to precisely articulate their information need in a suitable query.
As such, over the course of their search session they will often resort to engaging in exploratory search, which usually involves a  The  rst two authors have equal contribution to this work.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
short, probing query that loosely encapsulates their need, followed by a more rigorous exploration of the search documents and  nally the extraction of information [18].
Such short queries are often ambiguous and highly dependent on context; a common example being the query  jaguar , which can be interpreted as an animal, a car manufacturer or a type of guitar.
Recent research has addressed this problem by introducing the idea of diversity into search ranking, where the objective is to maximise the satisfaction of all users of a search system by displaying search rankings that re ect the di erent interpretations of a query [2].
An early ranking algorithm employing diversity is the Maximal Marginal Relevance technique [8], whereby existing search results would be re-ranked using a similarity criterion that sought to repel lexically similar documents away from one another.
A more modern approach is Radlinski s [21] application of multi-armed bandit algorithms to the online diversi cation of search results.
A common problem found in diversity research is in the evaluation of diversifying algorithms, this is due to the fact that the Probability Ranking Principle (PRP) [25] no longer applies in such cases due to the lack of independence between documents, and as such, common IR metrics such as MAP and nDCG must be interpreted carefully [11].
More recently, the PRP has been generalised to the diverse case by applying portfolio theory from economics in order to diversify documents that are dependent on one another [37].
Consequently, there exists a balance between novelty and relevance [8] which can vary between queries and users.
One way to resolve this is to introduce elements of relevance feedback into the search process so as to infer the context for the ambiguous query.
In relevance feedback, the search system extracts information from a user in order to improve its results, this can be: explicit feedback such as ratings and judgements given directly by the user, for example the well known Rocchio algorithm [26]; implicit feedback such as clickthroughs or other user actions; or pseudo relevance feedback, where terms from the top n search results are used to further re ne a query [7].
A problem with relevance feedback methods is that historically, users are reluctant to respond to them [19].
An analysis of search log data found that typically less than 10% of users made use of an available relevance feedback option during searches, even when results were often over 60% better as a result [33].
In addition, modern applications of the technique such as pseudo relevance feedback have come under criticism [5].
Nonetheless, it has proven e ective in areas such as image and video retrieval [27, 42].
Page 2 contains a re ned, personalised re-ranking of the next set of remaining documents, triggered by the  Next  button and depending on the documents viewed on Page 1 (the left Page 2 ranking contains documents about animals, the right ranking documents about cars).
The same search process can continue in the remaining results pages until the user leaves.
Relevance Feedback can be generalised into the  eld of Interactive Retrieval, whereby a user interacts with a search system throughout a search session, and the system responds to the user s actions by updating search results and improving the search experience.
Interaction can be split into two categories, system driven and user driven.
In system driven interaction, the system directly involves the user in the interactive process; for instance, a user can narrow down an e-commerce search by  ltering based on item attributes the system has intentionally speci ed [44].
Somewhat related is the  eld of adaptive  ltering, where a user re nes an information stream over time by giving feedback [24].
User driven interactions can allow the user to opt-in, or remain unaware of the interaction due to the use of implicit feedback, which can come from a number of sources, including scrolling behaviour, time spent on search page and clickthroughs, the latter two of which correspond well with explicit user satisfaction [13].
Due to their abundance and the low cost of recording them, clickthroughs have emerged as a common measure of implicit feedback, with much research into using search click logs for learning ranking algorithms [16].
When implicit feedback is used in interactive retrieval, the research can be split into long term and short term objectives.
Long term interactive retrieval methods are usually concerned with search personalisation or learning to rank over time, for instance, incorporating user behavior from search logs into a learning to rank algorithm [1], re-ranking search results for speci c users by accumulating user data over time [35] and using clickthroughs to learn diverse rankings dynamically over time [21].
Such techniques are useful for improving a search system over time for popular queries, but are less able to deal with ad hoc web search and tail queries.
On the other hand, short term methods aim to improve the immediate search session by incorporating user context such as clickthroughs and search history [29], but may have issues with user privacy concerning using user data.
In this paper, we propose a technical schema for short term, interactive ranking that is able to use user feedback from the top-ranked documents in order to generate contextual re-rankings for the remaining documents in ad hoc retrieval.
The feedback could be collected either implicitly, such as from clickthroughs and search page navigation, or explicitly, such as from user ratings.
The proposed feedback scheme naturally makes use of the fact that in many common retrieval scenarios, the search results are split into multiple pages that the user traverses across by clicking a  next page  button.
A typical use case would be a user examining a Search Engine Results Page (SERP) by going through the list, clicking on documents (e.g., to explore them) and returning to the SERP in the same session.
Then, when the user indicates that they d like to view more results (i.e.
the  Next Page  button), the feedback elicited thus far is used in a Bayesian model update to unobtrusively re-rank the remaining documents (which are not seen as yet), on the client-side, into a list that is more likely to satisfy the information need of the user, which are shown in the next SERP, as illustrated in Figure 1.
We mathematically formulate the problem by considering a Bayesian sequential model; we specify the user s overall satisfaction over the Interactive Exploratory Search process and optimise it as a whole.
The expected relevance of documents is sequentially updated taking into account the user feedback thus far and the expected document dependency.
A dynamic programming approach is used to optimise the balance between exploration (learning the remaining documents  relevancy) and exploitation (presenting the most relevant documents thus far).
Due to the optimisation calculation being intractable, the solution is approximated using Monte Carlo sampling and a sequential ranking decision rule.
Our formulation requires no changes to the search User Interface (UI) and the update stage can be performed on the client side in order to respect user privacy and deal with computational e ciency.
We test the method on TREC datasets, and the results show that the method outperforms other strong baselines, indicating that the proposed interactive scheme has signi cant potentials for exploratory search tasks.
In Section 2 we continue our discussion about the related work; in Section 3 we present the proposed dynamical model.
It s insights and it s approximate solutions are presented in Sections 4 and 5 respectively.
The experiments are given in Section 6 and conclusions are presented in Section 7.
A similar approach was explored by Shen [30], where an interactive re-ranking occurred when a user returned to the SERP after clicking on a document in a single session.
Aside from the UI di erences, they also did not perform active learning (exploration) on the initial ranking, and as such, did not take into account issues such as document dependence and diversity, and they immediately make use of implicit feedback rather than  rst collecting di erent examples of it.
An alternative approach displayed dynamic drop-down menus containing re-rankings to users who clicked on individual search results [6], allowing them to navigate SERPs in a tree like structure (or constrained to two levels [23]).
Again, this method only takes into account feedback on one document at a time, arguably, it could be considered intrusive to the user in some cases and it does not protect user privacy by working on the client side, but it does also perform active learning in the initial ranking in order to diversify results.
A common feature in the above-mentioned literature is the use of alternative UIs for SERPs.
Over the years many such interfaces have been researched, such as grouping techniques [43] and 3D visualisation [14], often with mixed user success.
Likewise, attempting to engage the user in directly interacting with the search interface (such as drop down menus) or altering their expectations (by changing the results ranking) may prove similarly detrimental.
In early relevance feedback research, it was found that interactively manipulating the search results was less satisfactory to users than o ering them alternative query suggestions [17], and likewise with automatic query expansion [28].
Users appeared to be less happy with systems that automatically adjusted rankings for them and more satis ed when given control over how and when the system performed relevance feedback [39].
Our technique requires no change to the existing, familiar UI and is completely invisible to the user, but it is still responsive to their actions and only occurs when a user requests more information; for the user who is interested in only the  rst page or is precision-minded, no signi cant changes occur.
A similar area of work is in the  eld of active learning, although there remain some fundamental di erences.
Whilst Shen [31] and Xu [41] researched methods that could be used to generate inquisitive, diverse rankings in an iterative way in order to learn better rankings, their aims were in long term, server-side learning over time that also lacked the ability to adjust the degree of learning for di erent types of query or user.
[15, 32] also investigated the balance between exploration and exploitation when learning search rankings in an online fashion using implicit feedback, although their goal was to optimise rankings over a period of time for a population of users.
We also consider the fact that the same search techniques are not employed uniformly by users across queries; for instance users exhibit di erent behaviour performing navigational queries than they do informational queries [3].
It is not only non-intrusive to use a Next Page button to trigger the personalised re-rank, but most importantly, the user deciding to access the next page sends a strong signal that there might be more relevant documents for the query and that the user is interested in exploring them.
In addition, our formulation contains a tunable parameter that can adjust the level of active learning occurring in the initial stage, including none at all.
This parameter is a re ection of how the user is likely to interact with the SERP, with di er-ent settings for those queries where users will follow the exploratory search pattern [18].
That our technique is able to handle search tasks of varying complexity using implicit feedback, and occurs during the middle, exploratory stage of a search session (when the user is changing SERPs) makes it an ideal candidate for implicit relevance feedback [40].
We see parallels with this work and other areas of online learning.
Our model and sequential search problem can be considered as a Partially Observable Markov Decision Process (POMDP) [9], where the unknown state of the POMDP is the probability of relevance for each document, the belief state is given by the multivariate Gaussian distribution, the observation is the user feedback, the reward is the IR metric being measured and curiously there is no transition function as we assume that the document relevancy doesn t change during interaction (however, the belief states of relevance keep updating).
Considering the problem in this manner could foster further research into approximative solution techniques or hint at a more general formulation.
In addition, the exploration and exploitation of the documents in our formulation (and other active learning methods) bears similarities with multi-armed bandit (MAB) theory [20, 21].
Our formulation concerns the exploratory, multi page interactive search problem; where a user performs an initial search which returns a set of ranked results that take into account the covariance of the documents, allowing for di-versi cation and exploration of the set of documents so as to better learn a ranking for the next pages.
The user then engages in usual search behaviour such as rating or checking the snippets and clicking on documents and returning to the SERP to view the remaining ranked documents.
Upon clicking on the next page button to indicate that they would like to view more results, the feedback obtained so far is combined with the document covariance to re-rank the search results on the next pages.
The Interactive Exploratory Search process continues until the user leaves the system.
In traditional IR ranking, we would order documents according to some IR score or probability of relevance in decreasing order, calculated using some prior knowledge such as the match between query and document features [25].
This score is an estimate of the underlying relevance score, which is  xed but unknown.
For our model, we assume that this true relevance is Normally distributed around the IR score estimate, which is equivalent to assuming that the estimate is correct subject to some Gaussian noise.
This is de ned for all documents using the multivariate Normal distribution Rt   [Rt N ] (cid:118) N ( t,  t) where N is the number of documents, Rt

 (1) i the random variable for the relevance score of document i on page t,  t   [ t N ] the estimates of the relevance score for the documents and  t the covariance matrix.
1, .
.
.
,  t Because we initially are not given any feedback information,   and   can be calculated using an existing IR model, for example,   could be derived from the BM25 [34] or Vector Space Model score and   could be approximated using a document similarity metric, or by topic clustering [36, 38].
page, we can then update our model as a conditional distribution.
1, .
.
.
, st M ] where st For a given query, the search system displays M documents in each result page and ranks them according to an objective function.
This rank action is denoted as vector st = [st j is the index of the document retrieved at rank j at the tth page i.e., s1 is the page 1 (initial) ranking and s2 is the page 2 (modi ed) ranking.
We let s represent all rank actions s1 .
.
.
sT .
We denote rt = [rt K ] as the vector of feedback information obtained from the user whilst using page t, where K is the number of documents given feedback with 0   K   M , and rt i is the relevance feedback for document i, either by measuring a direct rating from the user or by observing click-throughs.
1, .
.
.
, rt When considering the objective from page t until page T , we use a weighted sum of the expected DCG@M scores of the rankings of the remaining result pages, denoted here by (note that Rt sj   Rt T(cid:88) st j )  t U t s = tM(cid:88) t j=1+(t 1)M log2(j + 1) E(Rt sj ) (2)  
 sj ) =  t where U t s represents the user s overall satisfaction for rank action s, where E(Rt sj is the expected relevance of a document at rank j in result page t. We have chosen the objective function as it is simple and both rewards  nd-ing the most relevant documents and also ranking them in the correct order, although other IR metrics can be adopted log2(j+1) is used to give greater similarly.
The rank weight weight to ranking the most relevant documents in higher positions.
The tunable parameter  t   0 is used to adjust the importance of result pages and thus the level of exploration in the initial page(s).
When  1 is assigned to be zero, the initial ranking is chosen so as to maximise the next page(s) ranking and thus the priority is purely on exploration [41].
It could also be a decay function re ecting the fact that less users are likely to reach the later result pages.
An extreme case is when  t = 1  t, meaning there is no exploration at all and the documents are ranked according to the PRP.
In this paper, we  x  t a priori, while leaving the study of adaptive  t according to learned information intents [3, 22] for future work.
T indicates how far ahead we consider when ranking documents in the current page.
Our experiments using TREC data indicate that setting T = 2 (only considering the immediate next page) gives the best results.
To simplify our derivations, we consider T = 2 for the remaining derivations while the resulting algorithms can be easily generalised to the case where T > 2.
Thus, our overall objective is to  nd the optimal rank action s  such that U is maximised   s = argmax   s j=1 log2(j + 1)  1 sj
 (cid:27)  2 sj + (1    ) j=1+M log2(j + 1) (3) where we set   to balance the importance of pages 1 and 2 when T = 2.
This statistical process can be represented by an In uence Diagram as shown in Figure 2.
(cid:26) M(cid:88) Figure 2: The interactive search process as illustrated by an in uence diagram over two stages.
In the diagram, the circular nodes represent random variables; the square nodes represent the rank action at each result page.
The rhombus node is the instant utility.
We start by considering the problem in reverse; given that we ve already displayed a ranking to the user, how can we use their feedback to update our ranking in the second result page.
After receiving some observation of feedback r on page 1, and with the user requesting more documents, we can set the conditional distribution of the probability of relevance of the remaining documents given the feedback as where we have reordered the vectors R1 and  1 as R2 = P (R\s(cid:48)|Rs(cid:48) = r) (cid:18)R\s(cid:48) (cid:19) Rs(cid:48) (cid:18) \s(cid:48) (cid:19)  s(cid:48)
   = (4) (5) where s(cid:48) is the vector of documents in rank action s1 that received feedback in r, and \s(cid:48) the remaining N   K documents.
We also reorder  1 to give (cid:18)  \s(cid:48)  \s(cid:48)s(cid:48) (cid:19)
 (6) where  \s(cid:48) are the variances and covariances of the set s(cid:48), and  \s(cid:48)s(cid:48) the covariances between \s(cid:48) and s(cid:48).
Then, we can specify the conditional distribution as  s(cid:48)\s(cid:48)  s(cid:48) R2   N ( 2,  2) s(cid:48) (r    s(cid:48) )  1  2 =  \s(cid:48) +  \s(cid:48)s(cid:48)    2 =  \s(cid:48)    \s(cid:48)s(cid:48)    1 s(cid:48)  s(cid:48)\s(cid:48) (7) (8) (9) These formulas can then be used to generate new, posterior relevance score estimates for the documents, on the client-side, based on the feedback received in result page 1.
These score updates can be used to re-rank the documents in result page 2 and display them to the user.
Before presenting a ranked list at result page 1 and directly observing r, it remains an unknown variable and can
 exploration (active learning), we will need to be able to predict how a user will respond to each potential ranking.
Using the model above, we can predict the user s judgements r based on a particular rank action s1.
There are generally two simple models we can use when interpreting user feedback: 1) The user gives feedback on all of the top M documents i.e. K = M 2) The number of judgements is variable depending on how many documents the user looks through.
The top ranked documents are the most likely to gain feedback [12].
For the  rst case, r follows the multivariate normal distribution, we only need to display one of them at a time (M = 1), this allows us to assume that for some h  2 1 >  2 2   rs > h This indicates that when the observation rs is bigger than some h, the updated expected relevance score of document 1 will be larger than document 2, and vice versa.
This allows us to integrate over regions of possible values for rs given the optimal ranking decision for s2, simplifying Eq.
(15) into two integrals with exact solution r (cid:118) N ( 1 s ,  1 s) (10) (1    )W2  2
 V ( 1,  1, 1) = max s1  1 s W1+ (cid:19)(cid:27) (16)  2
 (cid:90)   h (cid:26) (cid:18)(cid:90) h   (15) Alternatively, if we rank document 2  rst (s1 = 2), we have  2 1 =  1
  1,2 ( 2)2 (r2    1
  2 2 = r2  2 1 <  2 2   r2 < h = 0.8947 s = [ 1 s1 , .
.
.
,  1 where  1 sM ].
For the second case, the probability of observing a particular feedback vector r depends on the rank bias weight 1/(log2(j + 1)), which can be easily derived as: (cid:27) P (r) = (2 )K/2| s(cid:48)|1/2 exp
 s(cid:48) (r    s(cid:48) )  1 (r    s(cid:48) )T     1
 (log2(j + 1)   1)1 j (11) log2(j + 1) (cid:26)   M(cid:89) j=1 where j = 1 if the document at rank j has received a user judgement, otherwise 0. s(cid:48) refers to the documents in rank action s1 that have feedback in r. In this paper, we consider case 2.
We are now ready to discuss how to maximise the objective function using the Bellman Equation [4] and the feedback distribution in Eq.
(11), and we present two toy examples to demonstrate how the model works and its properties.
Let V ( t,  t, t) = maxs U t s be the value function for max-imising the user satisfaction of the ranking up until step T.
Setting t = 1, we can derive the Bellman Equation [4] for this problem, giving us (cid:26) T(cid:88)  t tM(cid:88) (cid:18) T(cid:88) V ( 1,  1, 1) = max s1 t=1 j=1+(t 1)M = max s1 = max s1  1 1 s   W1 + max
  t t  1 1 s   W1 + E t=2 V ( 2,  2, 2) s2 (cid:18) E(Rt s) log2(j + 1) (cid:19)(cid:27) (cid:12)(cid:12)(cid:12)(cid:12)r s   Wt (cid:19)(cid:27) (cid:12)(cid:12)(cid:12)(cid:12)r
 (12) (13) (14) (cid:90)
 log2(1+(t 1)M ) , .
.
.
, where Wt = [ log2(M +(t 1)M ) ] is the DCG weight vector.
Eq.
(14) is the general solution to this problem, by setting T = 2 and   accordingly we can  nd the solution to our scenario = max s1 s   W1 + max  1 s2 (1    ) s   W2P (r)dr  2 r
 We illustrate the properties of the resulting document ranking using some toy examples.
To simplify matters, we observe that if there are only two documents (N = 2) and (cid:27) (cid:27) (cid:26) (cid:26) (cid:26)
 In this example we wish to illustrate how our model is able to use a document s score variance to explore rankings during the  rst result page; in this case we suppose that there are only two documents and that the search system must choose one at each result page.
Our prior knowledge about the score and its covariance matrix is (cid:18) 1 (cid:19)
 (cid:18)0.62
 (cid:19)

  1 =
 i.e. the documents are similarly scored (and have identical covariance), but have di erent variances.
We  rst consider ranking document 1  rst (s1 = 1) and observe feedback r1, allowing us to update our expected relevance scores  2 1 = r1  2 2 =  1
 = 0.9 +
  1,2 ( 1)2 (r1    1 0.62 (r1   1) = 0.344 + 0.556r1
  2 1 >  2 2   r1 > h = 0.775 We nominally set   = 0.5, then using Eq.
(16) we can calculate the value of ranking document 1  rst as = 1
  2
  2
 (cid:19) (cid:18)(cid:90) h   (cid:18)(cid:90) 0.775 (cid:90)     (1   0.556) (cid:18) (cid:19) P (r1)r1dr1
 =0.5 + 0.315

 =0.5 + 0.315
 0.444( 1 1    2
 =0.77169 (cid:90)   h (cid:19)
 when we rank document 2 at result page 1.
This is di er-ent from the myopic policy or the PRP which would simply rank the document with the highest score  rst, in this case document 1.
Here, because the variance of the score of document 2 is lower, it is optimal to display it  rst, a side e ect of which is that we may observe feedback from a user which subsequently reduces its variance and the variance of the other documents, giving more accurate rankings in a later step.
Now, suppose that there are three documents and the search system must choose two documents at each result page.
We assume that documents have a graded relevance from 1 to 5 and that the user will give explicit feedback on the top two documents when receiving a ranked list at result page 1.
In this example, we wish to demonstrate how the covariance of the documents can a ect our optimal ranking, so we  x the variance for all three documents to 1 and set our prior knowledge about their mean and covariance to  2.99


   1

  1 =  





 To simplify the problem, we also ignore the weights W and  .
For the  rst result page, the myopic ranking would be to display documents 3 and 2  rst because they have the highest prior scores i.e. s1
 feedback r3 and r2, we can update our expected relevance scores 1 = 3, s1  1{2,3}(r{2,3}    {2,3}) (cid:19) 1(cid:20)(cid:18)r2 (cid:19) (cid:18) 1 (cid:19)(cid:21) (cid:18) 1
  1
    2 1 =  1

 1 +(cid:0)0.1 0(cid:1)  

 =  1 r3 = 2.99 + 1.026(r2   3)   0.974(r3   5)  2 2 = r2  2 3 = r3
 Thus, the value for this particular ranking becomes (cid:90) (cid:90) (cid:90) =  1 3 +  1
 + + where P (r2, r3)   (r2 + r3)dr2dr3 P (r2, r3)   ( 2 1 + r3)dr2dr3 P (r2, r3)   ( 2 1 + r2)dr2dr3 = 16.380


 A = {r2, r3| 2 B = {r2, r3|r2 < min( 2 C = {r2, r3|r3 < min( 2 1 < min(r2, r3)} 1, r3)} 1, r2)} Alternatively, if we choose the ranking s1 we can calculate the value of the ranking as 16.528.
1 = 3, s1 2 = 1, then We see that the myopic policy (e.g.
the PRP) is once again not the optimal policy, in this case it is more optimal to rank s1
 documents 2 and 3 are highly correlated, unlike documents 1 and 3, and so we can learn more information from rank ing documents 1 and 3 together.
The toy example reveals 1 = 3, s1 that an optimal policy tends to rank higher the documents that are either negatively or positively correlated with the remaining documents.
Encouraging the negative correlation would guarantee a diversi ed rank list [37] whilst the positive correlation could result in the documents in the center of a document cluster being driven to the higher ranks.
In the previous section we outlined and demonstrated the formulae that can be used to calculate the optimal solution to the interactive, multi page exploratory search problem, making use of a Bayesian update and the Bellman equation.
Unfortunately, as is often the case with dynamic programming solutions the optimal solution does not reasonably scale, and for more than three documents, the integral in Eq.
(15) cannot be directly solved.
Instead, we can  nd an approximate solution by making use of Monte Carlo sampling.
The optimal value function V ( 1,  1, 1) can be approximated by (1    ) s   W1 + max s   W2P ( r)  2 (cid:88)   max (cid:27) (cid:26)  1 s1 s2

  r O (17) where O is the sample set of possible feedback vectors r and S is the sample size.
We can use Eq.
(17) to determine an estimate of the optimal ranking decision for the  rst page at t = 1.
Eq.
(17) does not completely solve the problem of intractability as there are still an insurmountable number of possible ranking actions possible, even if we restrict our ranking to only select documents at the top M ranking positions.
To counter this, we use a sequential ranking decision that lowers the computational complexity of generating rankings.
We  rst consider selecting only one document to display at rank position 1, which has only N possible actions.
We use Eq.
(17) to  nd the optimal rank action, then repeat the procedure for the remaining ranking positions and documents, sequentially selecting a document from rank position
 global optimal solution, it does provide an excellent balance between accuracy and e ciency.
Algorithm 1 illustrates our approximated approach.
Here, parameter K is assigned a priori, which depends on our assumptions about the behavior of user judgements.
If we believe a user only provides judgements on the top M documents, it is reasonable to assign M = K, as only the selection for the top M rank positions will in uence the updated model at result page 2.
When we  nd the optimal ranking from ranks 1 to K, the rest of the documents in page 1 can be ranked following the traditional IR scores.
Because T = 2, the maximum utility for the second page is obtained when we rank in decreasing order of  2\s i.e. the updated probabilities of relevance for the remaining documents given  r.
We consider the scenario where a user is presented with a list of M documents on the  rst page, where we set M = 10
 gorithm function IES( ,  , K) s = array(M ) loop j   1 to K value = array(N ) loop i   1 to N (cid:46) For each rank position (cid:46) For each document (cid:46) Ignore if document already ranked if i   s then continue end if s(j) = i O = array(S) O = createSample( ,  , s) sampleAvg = 0 for all  r   O do (cid:46) For each sample s ( r    1  2\s =  1\s +  \ss 1 s) s2 = sort( 2\s, descend)[1   M ] s2   W2   P ( r) sampleAvg+ =  2 end for sampleAvg\ = S value(i) =  1 (cid:46) Average sample value s   W1 + (1    )   sampleAvg end loop s(j) = index(max(value)) end loop s(K + 1   M ) = sort( 1\s, descend)[1   M   K + 1] return s end function as it provided reasonable and meaningful results within the scope of our test data set and is re ective of actual search systems.
The user then provides explicit or implicit judge-ments and then clicks the  Next Page  button, whereby they are presented with a second (and possibly third) page of M documents on which they also provide their judgements.
We test our Interactive Exploratory Search (IES) technique which uses dynamic programming to select a ranking for the  rst page, then using the judgements from the  rst page generates a ranking for the remaining pages using the conditional model update from Section 3.1.
We compare the IES algorithm with a number of methods representing the di erent facets of our technique: a baseline which is also the underlying primary ranking function, the BM25 ranking algorithm[34]; a variant of the BM25 that uses our conditional model update for the second page ranking which we denote BM25-U; the Rocchio algorithm[26], which uses the baseline ranking for the  rst page and performs relevance feedback to generate a new ranking for the second page; and the Maximal Marginal Relevance (MMR) method[8] and variant MMR-U which diversi es the  rst page using the baseline ranking and our covariance matrix, and ranks the second page according to the baseline or the conditional model update respectively.
We tested our algorithm on three TREC collections; we used the WT10G collection because it uses graded relevance judgements which are suited to the multivariate normal distribution we use, allowing us to test the explicit feedback scenario.
To test our approach on binary judgements representing implicit clickthroughs, we used the TREC8 data set.
Additionally, we used the 2004 Robust track which focused on poorly performing topics, which allowed us to test our algorithm on di cult queries which are likely to require recall-oriented, exploratory search behaviour.
The documents were ranked according to BM25 scores for each topic, and the top 200 used for further re-ranking using the IES algorithm and baselines.
Each dataset contained 50 topics, and relevance judgements for those topics were used to evaluate the performance of each algorithm.
The judge-ments were also used to simulate the feedback of users after viewing the  rst page, allowing for the conditional model update to generate a ranking for the second page.
We used four metrics to measure performance; precision@M, recall@M, nDCG@M and MRR, and measured the metrics at both M (for  rst page performance) and 2M (for overall performance over the 1st and 2nd page).
Precision, recall and nDCG are commonly used IR metrics that allow us to compare the e ectiveness of di erent rankings.
Because our investigation is focused on the exploratory behaviour of our approach, we did not explicitly use any subtopic related diversity measure in our experiments.
However, we used the MRR as a risk-averse measure, where a diverse ranking should typically yield higher scores [37, 10].
We applied the widely used BM25 [34] model to de ne the prior mean score vector  1 for result page 1.
Eq.
18 below was used to scale the BM25 scores into the range of relevance judgements required for our framework x   min(x) max(x)   min(x)   b   = (18) where x is the vector of BM25 scores and b is the range of score values i.e. b = 1 for binary clickthrough feedback, b > 1 when explicit ratings are given by the user.
The covariance matrix  1 represents the uncertainty and correlation associated with the estimations of the document scores.
It is reasonable to assume that the covariance between two relevant scores can be approximated by the document similarities.
We investigated two popular similarity measures, Jaccard Similarity and Cosine Similarity, and our experiments showed that the latter had a much better performance and is used in the remainder of our experiments.
The variance of each document s relevance score is set to be a constant in this experiment as we wish to demonstrate the e ect of document dependence on search results, and it is more di cult to model score variance than covariance.
In this experiment, we measured the recall@M, precision@M, nDCG@M and MRR for M = 10 and M = 20 i.e. the scores for the  rst page, and for the  rst and second page combined respectively.
We wished to explore how the parameter   affected the performance of the IES algorithm, in particular, the average gains we could expect to achieve.
To calculate the overall average gain, we calculated the average loss (metric(IES) metric(baseline))/M at M = 10 and M = 20, and added the two values.
This value tells us the di erence in a metric on average we d expect to gain/lose by using our technique over multiple pages.
In Figure 3, we see how the average gain varies for different values of   across the di erent data sets against the BM25 baseline.
We note that the setting of   has a de nite e ect on performance; we observe that for smaller values (  < 0.5), where the algorithm places more emphasis on displaying an exploratory, diverse ranking on the  rst page, we see a marked drop in performance across all metrics.
As
 con dence intervals.
Each bar represents the average gain in a particular metric for a given value of  , and each chart gives results for a di erent TREC data set.
Positive values indicate gains made over the baseline algorithm, and negative values losses.
Recall@

 Precision@




 #









 Rocchio



 Rocchio
 #










 #









 Rocchio
























































 nDCG@













































































 Robust
 Table 1: Table of metrics at M = 10 ( rst page) and M = 20 ( rst and re-ranked second page) for each algorithm and for each data set.
A superscript number refers to a metric value signi cantly above the value of the correspondingly numbered baseline in the table (using the Wilcoxon Signed Rank Test with p = 0.05).
  increases, performance improves until we start to see gains for the higher values.
This highlights that too much exploration can lead to detrimental performance in the  rst page that isn t recovered by the ranking in the second page, indicating the importance of tuning the parameter correctly.
On the other hand, we do see that the optimal setting for   is typically < 1, indicating that some exploration is bene cial and leads to improved performance across all metrics.
We also observe that the di erent datasets display di er-ent characteristics with regard to the e ect that   has on performance.
For instance, for the di cult to rank Robust data set the optimal setting is   = 0.8, indicating that exploration is not so bene cial in this case possibly due to the lack of relevant documents in the data set for each topic.
Likewise, we  nd that a setting of   = 0.9 is optimal for the TREC8 data set, although there is greater variation possibly owing to the easier to rank data.
Finally, the WT10G data set also showed variation, which could be a result of the explicit, graded feedback available during re-ranking.
For this data set we chose   = 0.7.
The di ering data sets represent di erent types of query and search behaviour, and illustrate how   can be tuned to improve performance over di erent types of search.
After setting the value of   for each data set according to the optimal values discovered in the previous section (and also setting for the MMR variants   = 0.8 for WT10G,   = 0.8 for Robust and   = 0.9 for TREC8 after some initial investigation), we sought to investigate how the algorithm compared with several baseline algorithms, each of which shares a feature with the IES algorithm; the Rocchio algorithm also performs relevance feedback; the MMR diver-si es results; and also some variants that use our conditional



 Robust





 Precision@







 nDCG@
















 Table 2: Table of metrics at M = 5 ( rst page) and M = 10 for the IES algorithm on each data set.
The superscript numbers refer to signi cantly improved values over the baselines, as indicated in Table 1.
model update.
We repeated the experiment as before over two pages with M = 10, and measured the recall, precision, nDCG and MRR in order to evaluate the overall search experience of the user who engaged with multiple pages (note that we  nd di erent values for MRR@10 and MRR@20 owing to the occasions where a relevant document wasn t located in the  rst M documents).
The results can be seen in Table 1.
We  rst observe that the scores for the  rst page ranking are generally lower than that of the baselines, which is to be expected as we sacri ce immediate payo  by choosing to explore and diversify our initial ranking.
We still  nd that the IES algorithm generally outperforms the MMR variants on the  rst step, particularly for the MRR metric, indicating improved diversi cation.
In the second page ranking we  nd signi cant gains over the non-Bayesian update baselines, and some of the baselines using the conditional model update.
It is worth noting that the BM25-U variant is simply the case of the IES algorithm with   = 1.
This demonstrates that our formulation is able to correctly respond to user feedback and generate a superior ranking on the second page that is tuned to the user s information need.
We refer back to Figure 3 to show that the second page gains outweigh the  rst page losses for the values of   that we have chosen.
Finally, in Table 2 we see a summary of results for the same experiment where we set M = 5, so as to demonstrate the IES algorithm s ability to accommodate di erent page sizes.
We observe similar behaviour to before, with the IES algorithm signi cantly outperforming the baselines across data sets, indicating that even with less scope to optimise the  rst page, and less feedback to improve the second, the algorithm can perform well.
Throughout this paper we have simpli ed our formulation by setting the number of pages to T = 2, so for this experiment, we observe the e ect of setting T = 3.
We performed the experiment as before where we display M = 5 documents on each page to the user and receive feedback, which is used to inform the next page s ranking.
For the T = 3 variant, dynamic programming is used to generate the rankings for both pages 1 and 2 (with   nominally set to 0.5), and the document s relevancy scores are updated after receiving feedback from pages 1 and 2.
We compare against the IES algorithm with T = 2, where after page 1 we create a ranking of 2M documents, split between pages
 of 3M documents.
The results can be seen in Table 3.
We can see from the results that whilst the T = 3 variant still o ers improved results over the baseline (except in the case of MRR), the performance is also worse than the T = 2 case.
This is an example of too much exploration negatively Algorithm Recall@15 Prec@15







 Baseline nDCG@15 MRR





 Table 3: Table showing metric scores at rank 15 for the baseline and two variants of the IES algorithm, where T is set to 3 and 2.
a ecting the results; when T is set to 3, the algorithm creates exploratory rankings in both the  rst and second pages, before it settles on an optimal ranking for the user on the third page.
Except for on particular queries, a user engaging in exploratory search should be able to provide su cient feedback on the  rst page in order to optimise the remaining pages, and so setting T = 2 is ideal in this situation.
In this paper we have considered how to optimally solve the problem of utilising relevance feedback to improve a search ranking over two or more result pages.
Our solution di ers from other research in that we consider document similarity and document dependence when optimally choosing rankings.
By doing this, we are able to choose more e ective, exploratory rankings in the  rst results page that explore dissimilar documents, maximizing what can be learned from relevance feedback.
This feedback can then be exploited to provide a much improved secondary ranking, in a way that is unobtrusive and intuitive to the user.
We have demonstrated how this works in some toy examples, and formulated a tractable approximation that is able to practically rank documents.
Using appropriate text collections, we have shown demonstrable improvements over a number of similar baselines.
In addition, we have shown that exploration of documents does occur during the  rst results page and that this leads to improved performance in the second results page over a number of IR metrics.
Some issues that arise in the use of this algorithm include trying to determine the optimal setting for  , which may be nontrivial, although it could be learned from search click log data by classifying the query intent [3] and associating intent with values for  .
Also, in our experiments we used BM25 scores as our underlying ranking mechanism and cosine similarity to generate the covariance matrix, it is for future work to investigate combining IES with di erent ranking algorithms and similarity metrics.
We would also like to further investigate the e ect that M plays on the optimal ranking and setting for  ; larger values of M should discourage exploration as  rst page rankings contain more relevant documents.
Also, whilst our experiments were able to handle the binary case, our multivariate distribution assumption is a better model for graded relevance, and our 663approximation methods and the sequential ranking decision can generate non optimal rankings that may cause detrimental performance.
In the future, we intend to continue developing this technique, including attempting to implement it into a live system or in the form of a browser extension.
We also note that multi-page interactive retrieval is a sub-problem in the larger issue of exploratory search, in particular, designing unobtrusive user interfaces to search systems that can adapt to user s information needs.
Furthermore, as previously stated we can consider our formulation in the context of POMDP and MAB research [9, 20], which may lead to a general solution that is applicable to other applications, or instead may grant access to di erent solution and approximation techniques.
