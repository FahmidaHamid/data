Personalization is being used by most online service platforms (OSPs) such as search, advertising, shopping etc.
Their Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
goal is to lure users by o ering a better service experience customized to their individual interests.
A popular trend is to employ pro le based personalization, where OSPs build extensive pro les for the user (based on her past interactions such as search queries, browsing history, links shared, etc.)
and personalize content using this pro le.
Several popular search1, movie services employ such personalization, e.g.
recommendations2, etc.
While OSPs track rich user data in their histories alone, they can infer much more information about them by mining this raw data.
Informally speaking, OSPs can determine a user s interests and biases on di erent categories, which can later be used (along with the history) for personalizing content for her.
For example (see [20] for details), Google is shown to have inferred users political a liations (republican or democratic) and use it to re-rank search results.
For a user, this raises a signi cant privacy concern she does not know what was tracked in her history, what has been inferred, and more importantly, is currently being used to personalize her content.
Moreover, as both the personal-ization techniques and the data they operate on are the key di erentiators for the OSPs (their secret sauce), they do not reveal either of them, making it even harder for an user to understand how personalization is being done (for her).
In this paper, we aim at extracting a user s pro le from the OSP.
We model this pro le as a weighted personalization vector over topics, where the weight on a topic indicates her interest in it (higher means more interested).
Informally, a topic is any concept or phenomenon that the user could be interested in, e.g.
a speci c sport, preference over cuisines, favorite author, movie genre, etc.3 Our goal is not to reverse-engineer the OSP s inference algorithms.
In fact, we treat the OSP as a black-box and assume that we only have access to its output, i.e. the (personalized) content served by the OSP to a user on di erent queries4.
The central idea in our approach is to get both personalized content (served for the user) and vanilla content (served for a new/not logged in user) for the same query and analyze their di erences.
Through careful inspection of
 http://support.google.com/websearch/bin/answer.py?answer=1710607.
of words, a common de nition in the topic modeling literature (see Section 3).
other information on a movie, or dynamically generated, e.g.
search results.
873these two types of content, we identify the hidden user pro  le (summarized through a weighted set of topics) used by the OSP to serve personalized content.
There is very little concrete information available on the techniques the OSPs use to personalize content.
Our paper provides a novel approach to tackle this problem, giving insights into the hidden user pro les without the knowledge of the speci c inference techniques used by the OSP or the history of the user.
We believe that this idea of comparing the di erences in output to extract the hidden personalized topics is a unique aspect of our paper and opens a new direction in privacy research that can be aimed at commercial OSPs.
As an example, consider the case of a search engine.
For any search query, we can get the personalized and vanilla results by making the query from a browser with and without logging in, respectively.
These results are basically two ranked lists with some urls in the latter moved up or down in the former (i.e. re-ranked ), based on the user s pro le.
We study these re-rankings over multiple queries and determine the topics of interest for the user that can best explain these di erences.
For the rest of the paper, we focus our attention on search engine personalization.
However, our techniques can easily be extended to other services where a) we can observe both vanilla and personalized content and b) the content is o ered as ranked lists of items (e.g.
urls, movies, products, etc.
).
For example, we can apply it to movie recommendations by Net ix based on the personalized (and vanilla) ranked list of related movies presented when a user visits a particular movie page.
Although the exact details of personalization for search engines are not publically available, recent works in the web-search community have thrown some light on them [8, 23,
 to  rst populate the vanilla result using the semantics of the query string and then personalize it by rearranging the items in this list, using the pro le information.
Therefore, conceptually, the vanilla and personalized contents are reordering of the same set of items.
We take advantage of this re-ranking of results to determine the topics present in the user s pro le with the OSP.
It is important to observe that the assumption of vanilla and personalized results being re-rankings of same set of urls, does not preclude the generality of our approach.
This assumption can easily be lifted by simply adding the extra urls in one list to the end of the other list5.
The important point is that personalization, by de nition, will a ect the ranks of the results shown, which is what we use in this paper.
Note that the topics that we learn may not be explicitly maintained by the search engine (or an OSP in general); in fact their pro le data could consist of parameters completely unrelated to, and may not even map to, our de nition of topics.
Our paper hinges on the intuition that a user pro le can be succinctly captured by a set of topics that re ects her interests.
Any search engine (or OSP) that personalizes results
 alized results contain any extra result compared to vanilla, and even these contain on average only 14% extra urls (or, 1.4 urls for an avg.
result size of 10).
based on her interests must give higher preference to the results matching these topics.
Thus our approach of  nding topic-level personalization is fairly generic it can work with OSPs who do not necessarily maintain topic-based pro les of users and without the knowledge of the inference algorithms they use.
An alternate approach to recreate the user pro le could be via mining the inputs to the search engine (i.e. the user s search query logs, results clicked, browsing history etc.
)[8,
 compared to us.
First of all, it is very hard to catch up to the commercial techniques used by OSPs that are usually more advanced and rapidly evolving.
Secondly, due to the proprietary nature of OSPs, it is not clear what algorithm or even what part of the history is being used by them.
Finally, in many cases the history information may not be available publicly (i.e. while a Google user s search history is available, past ads served are not), limiting the e ectiveness of these approaches.
In contrast, our approach is agnostic to OSP s personalization scheme and can work even when the history is not public.
The main contributions of the paper are as follows.
  We propose a new direction in privacy research that gives users a glimpse of their pro le information being used by commercial OSPs to serve personalized content.
We formally capture this information as a personalization vector over topics that provides a concise and accurate summary of the user pro le.
  We propose a novel way to compute this personalization vector based on the personalized and vanilla content served by an OSP.
This formulation treats the OSP as a black-box and hence can work with a variety of online services.
We believe that this is a unique aspect of our work and can open a new direction for privacy research by enabling access to (so far hidden) pro le information in OSPs.
  We present a probabilistic model (named Latent Topic Personalization, or LTP) that captures the intuition behind our approach.
LTP is both expressive and leads to computationally e cient inference algorithms (LTP-INF and LTP-EM) that  nd the personalization vector on real data-sets.
  Our experiments with synthetic data-sets generated by a state-of-the-art personalization engine show that LTP can learn the personalization parameters very accurately, achieving on average 84% precision in learning personalized topics.
  We perform experiments on a novel real-life data-set containing the personalized and vanilla query results collected from 10 Google users.
Our qualitative results demonstrate that the personalized topics determined by LTP for a user correspond well to his ad-categories determined by Google.
Search personalization: A large body of work exists on personalizing search results using user-pro les [8, 23, 18], that collectively give overwhelming evidence of its bene- ts.
More recently, researchers have also explored creating 874pro les using topic models [22] and other textual informa- tion [25].
These works are not competitors of our paper, but rather serve as a motivation for us, as they highlight the existence and importance of pro les in the state-of-the-art in personalization.
Another body of work explores short-term and session based personalization [1, 8], that personalize based on user s current intention, based on his recent history or session.
While such approaches are not aligned with our idea, there are two important points to note a) they do no imply pro le-based personalization does not happen, rather, they are typically used in conjunction with each other [1, 13], and b) since they are applicable only during a session, it is easy to remove their a ect by making sure no coherent session is tracked during our data collection (by doing the queries in a random order or adding su cient delays).
Researchers have also found that personalization is not always bene cial and have proposed various approaches, such as click-entropy [26, 27], dynamic user interests [13] and query di culty [31], to  lter queries that should not be personalized (irrespective of user s pro le).
Such  ltering is very hard replicate in our approach since the output may not contain any information to model them.
We therefore allow for existence of this hidden process in our model via a latent variable deciding (randomly) if personalization happens on a query (see Section 4.1 for details).
In a paper contemporary to ours, Hannak et al. [9], study the parameters (location, demographics, etc.)
that e ect personalization in Google search.
While these parameters give insights on what inputs in uence personalization in practice, it is very di erent from the topic-based approach we take to capture the hidden user pro les.
Topics Models: Although topic models are clearly a popular tool for processing textual information and have also been used in personalization, there is no work to our knowledge that models the di erences in two documents (or two ranked set of documents) as us.
A recent work by Bischof et al.
[2] comes close they  nd exclusive topics (that are su ciently di erent from each other) so that the documents can be classi ed into a non-overlapping hierarchy.
While this also involves  nding topics which are present in some documents and not in others, it is still very di erent from our approach of  nding a consistent (may not be exclusive) set of personalized topics that can di erentiate personalized and vanilla content.
Privacy: Finally, our problem stems from the general area of privacy of user data.
Various studies have highlighted problems of privacy in information leaks from OSPs[11, 17,
 point individual users in Facebook, Mao et al. [17] analyzed tweets to  nd vacation plans and medical conditions for real users, etc.
However, these studies are focused on  nding instances of privacy leaks from the entire OSP network and do not help users understand leaks in their own account.
Other approaches of privacy preserving personalization aim at building systems from scratch that ensure certain norms are preserved in the personalized output, e.g.
grouping user pro les [29, 30] to preserve k-anonymity or making a di er-entially private recommender system[14].
Recently, Chen et al. [6] presented a more user centric approach that gives user control over  ne grained categories (represented as a  xed hierarchical taxonomy) which they want personalization on.
These techniques, however, require the users to switch to these new systems which is not practical.
In this section, we introduce our notations and de ne the technical problem that we consider in this paper.
Let I = {i1, i2,       } be the universe of all the items being present at the personalization server, where, an item might represent a url (for search engines like Google, Bing, etc.
), a product webpage (for e-commerce sites like Amazon, Net-Flix, etc.)
or an advertisement (for ad servers).
For a query q, let  q and  q denote the personalized and vanilla results.
In the following discussion, we will often drop the subscript q, when the query is understood from the context.
As mentioned earlier, both   and   are treated as per-I     I. Technically, mutations over a  nite set of items a ranking/permutation6 is a bijection from a set to itself.
For any permutation  ,  (i) denotes the item assigned at rank (position) i, hence   = ( (1),  (2),       ).
The notation  1(d) denotes the rank i of an item d   I in   such that  (i) = d. For any two permutations   and  , we use the notation  1( (i)) to denote the rank of the item  (i) in  .
Observe that  1( (i)) = i.
We use Sn to denote the set of all permutation over n items.
We assume that there are T topics { 1,  2,       ,  T } in our system where each topic  k is de ned as a multino-mial distribution over a  xed vocabulary V .
For each word w   V , we have a parameter  k,w = Pr(w |  k) such that Pw V  k,w = 1.
Each item7 i   I is represented by its topic-map  i which is a multinomial distribution over the set of topics.
By inspecting the components of  i, one can infer how related the item is to a particular topic.
We now describe our representation of topic-level user pro le information.
For each user u and topic  k    , we associate a variable  u,k   R. It captures the importance of  k (more relevant topics have higher values) for serving personalized content to u.
The complete pro le information (we name it as latent personalization vector) is denoted by  u = ( u,1,  u,2,       ,  u,T ).
We often drop the subscript u and refer to it simply as   whenever the user is understood from the context.
Our strategy to learn the personalization vector   is to repeatedly frame queries to the search engine and observe the di erence between its vanilla and personalized results.
For a given user u, we  rst sign-in to her account and submit a query.
This gives the search engine an opportunity to personalize the result by using u s pro le information and through this process, we obtain the personalized result  .
Next, we submit the same query in an anonymized form, by removing all cookies from the http request, thus removing all account details (but keeping all other parameters same such as IP address, User-Agent, etc.).
This time the server sends back the vanilla result  .
We expect that as this process is repeated many times, the cumulative di erence between these two kinds of results will become statistically signi -

item.
875    z         g  

 f      
  


  
 the items and a personalization block to model the personalized responses (i.e.  1,  2,       ,  m).
Topic Block The topic block follows the description of standard topic models (c.f.
LDA [3]) and we present it here for the sake of completeness.
The generative process for the topic block is as follows   For each topic  k, k = 1, 2       , T
 Personalization Block Topic Block   For each item i   I Figure 1: Graphical model representation of LTP.
cant and contain substantial evidence for  .
In this paper, we study the following problem: Given pairs of query results ( 1,  1), ( 2,  2)       ( m,  m), how do we learn the latent per-sonalization vector  , for a given user?
Non-pro le factors Although personalization normally yields its bene ts by presenting more relevant results to the users, it is also known to be less e ective and even detrimental in many cases.
For example, while personalizing results are known to work well for short and ambiguous queries [24] where user searching same query may be looking for completely di erent things, for common and speci c queries two users with very di erent pro les are normally looking for the same information and are satis ed with the same (ordering of) results.
In such cases, even though user s pro le implies re-ranking, the server may decide not to personalize.
This creates a problem for our approach as a search engine s decision whether to personalize the result of a search query or not, is in uenced not only by the topical content of the query result, but also through other  ltering processes that are hidden from us.
We take care of this in our model by introducing a latent parameter that, during training phase,  lters out such inexplicable events and reduces the noise in the personaliza-tion vector.
In our experiments with the Google data-set, we found several instances of queries with results at higher ranks having higher  scores  (see Section 4 for de nition of scores) the ones at lower ranks, that were not personalized, while another query with similar scores was personalized.
Without this latent parameter, these instances would have reduced the e ectiveness of learning  .
The goal of topic-based personalization learning is to capture the following information: topics on which personaliza-tion takes place and a weight vector corresponding to the degree of personalization on these topics.
In addition, the approach has to scale with large number of queries.
To meet these objectives, we  rst propose Latent Topical Personal-ization model (LTP) to study the problem from a Bayesian perspective.
Following that, we develop e cient variational inference and estimation techniques for learning the parameters of this model.
We now formally describe the proposed LTP model.
LTP models (Figure 1) both topics and personalization.
It involves a topic block to model the topical content creation of

 (a) Sample topic Ki,j with Pr(Ki,j = k)   e i,k .
(b) Sample word Wi,j   Multinomial( Ki,j ).
The joint distribution for the topic-block can be written as p( , K, W,   |  ,  ) = Yi I ni p( i |  )   p( k |  )
 Yk=1 p(Ki,j |  i)   p(Wi,j | Ki,j ,  1 T ) (1) Yi I Yj=1 Personalization Block Our design of the personaliza-tion block is little more involved.
The main di culty stems from the non-pro le based factors, which may lead to no re-ranking of results even when the user pro le (i.e.  ) indicates personalization should happen.
In LTP, we achieve it by introducing a latent switch variable z (refer to Figure 1).
Independently, for each query, we sample z, governed by a prior parameter   and based on its value decide whether to allow topical personalization or not.
The parameter   is user-speci c and controls the rate at which topical person-alization takes place (for that user).
Based on the value of z, we pick a probability distribution over permutations and sample   from it.
Probabilistic models on permutations have recently been applied to solve various problems related to ranking [21].
Probability distributions de ned over permutations can be broadly categorized into two types distance based and score based.
In a distance based model [16], the probability of a permutation is de ned according to its distance from a central permutation.
They have rich expressive power as they can incorporate a wide variety of distance functions over permutations but are, in general, computationally ine cient.
Score based models [12], on the other hand, are very ef- cient as they divide permutation construction into stages and assign scores on each stage such that the  nal probability is a combination (multiplication) of stage-wise scores.
However, being de ned as a speci c function over scores, they have limited expressive power e.g.
they can not take into account any central permutation in the generative process.
For LTP, we have a central permutation (vanilla list  ) and want to model   as being generated from it.
Further, as explained later, we de ne scores on items as a function  .
Therefore, we need a model which combines the notion of distance with scores and is computationally e cient.
The probability distribution f (Figure 1) is a process for generating the personalized response  , and is decomposed 876   






 e 1
 e 2 e 1 Items



 e0 e1



  


 f (  |  ) = 0.03  1,  2  ,     z
     Personalization Block
 Figure 2: An example illustrating the steps of f .
We have assumed   = 1.
At each stage, the actual outcome is marked in blue and the most likely outcome is marked in red.
Figure 3: Variational distribution used for inferring personalization in LTP.
learned from the data.
The overall probability of sampling   is given by into sequential stages.
Observed that (see Figure 1) this process is activated only if z = 0, thereby, implying no topical personalization should happen.
In the  rst stage, we pick the item  (1) with probability exp( (1 1 (1))) Pj 1 exp( (1 1  (j))) .
Note that this probability is maximum when the two permutations agree with the  rst position i.e.  (1) =  (1).
However, if we happen to pick some other item i.e.  (1) 6=  (1), then for the second stage, the most likely outcome is to bring back the item  (1) and put it at the second position of   i.e.  (2) =  (1).
In general, in the kth stage, the probability of selecting exp( (k 1 (k)))  (k) is Pj k exp( (k 1 (k))) .
Intuitively, at each stage k, the model determines the items among  (1),  (2),       ,  (k   1) which are not yet sampled by f and assigns higher proba- bility on picking them.
In Figure 2 gives an example of this sampling process.
Considering all the stages, we obtain the overall probability of sampling   which is given by the following expression g(  |  ;  ,  ,  ) = Yi exp( T  (i) + (1    )(i    1 (i))) Pj i exp( T  (j) + (1    )(i    1 (j)))!
It can be veri ed that g is also a valid probability distribution.
The generative process for the personalization block can be described as   For each user u

   For each query qi, i = 1, 2,       , m
 allow topical personalization.
        The joint distribution for the personalization block can be written as (2) p( ,z,  ,   |  ;  ,  ,  ,  ,  ) = p(  |  )   p(  |  ) f (  |  ,  ) = Yi exp( (i    1 (i))) exp( (i    1 (j))) Xj i It can be shown that f is a valid probability distribution i.e.
f (  |  ,  )   0 for all     Sn and P  f (  |  ,  ) = 1.
The parameter   controls the spread of the distribution i.e.
if     0 then f converges to the uniform distribution over Sn; otherwise, for   > 0 the distribution is concentrated around  .
We assume     1.
We now describe our next permutation model g that captures the topic-level personalization which is invoked only if z = 1.
Model g is also decomposed into sequential stages and at each stage uses both the central permutation   and a set of scores, to determine  .
Each item d   I is assigned a score  T  d.
In the ith stage, g selects the item  (i) with probability exp( T  (i) + (1    )(i    1 (i))) Pj i exp( T  (j) + (1    )(i    1 (j))) The working principle for g is similar to f , except that it now allows for deviations from   only if it is explained by the scores.
Parameter   is tuned to adjust the relative importance of the scores and the central permutation  .
For example, if   = 0 then the scores are ignored and if   = 1 then the central permutation does not play any role.
We treat 0       1 as a free parameter whose value needs to be m Yi=1 p(zi |   )   g( i |  i,  ,  ,  )zi f ( i |  i,  )1 zi (3) Finally, the full joint distribution for LTP can be obtained by multiplying Equations 1 and 3.
We treat the parameters  ,  ,  ,   as constant and do not consider learning them.
However, the parameters   and   that controls the permutation models need to be learned.
We have assumed a Gaussian prior on  .
The role of this prior is to set   to zero when we do not observe any signi cant di erence between   and   i.e  i    i.
We  rst assume that   and   are prede ned constants and describe the inference (LTP-INF) of the personalization vector   based on these values in Section 4.2.
We will then use LTP-INF to also estimate these parameters in Section

 The key inferential problem that we study in this work is to obtain the posterior distribution on the latent variables i.e. to determine p( , K,  , z,  ,   |  ;  ,  ).
As with simpler topic models, the exact inference is intractable and therefore, we resort to approximate inference techniques.
Given the non-conjugacy of   and  , sampling based techniques
 are unlikely to be e cient.
variational approximation scheme.
In a variational inference, one de nes a family of simpler distribution over the latent variables to approximate the true posterior distribution.
This family of distribution is indexed by additional parameters (called variational parameters) which are tuned so as to minimize the KL divergence with the true posterior.
We  rst simplify the inference by breaking it into two parts.
For the  rst part, we ignore the dependency between the topic and the personalization block.
Therefore, our strategy is to  rst infer the topics and use the inferred topics and the topic-maps of the items to carry out inference for the personalization block.
This will simplify the exposition greatly and the ideas that we develop here will carry over naturally to the general case of inferring the blocks jointly.
We revisit the inference for the complete model in Section 4.4.
Inference for the topic block follows standard techniques (see e.g.
[3]) and therefore, we omit the details here.
For the rest of this subsection we assume that the topics have been inferred and develop an inference scheme for the personalization block.
For the personalization block, the key inferential problem is to obtain the posterior distribution p(z,  ,   |  ;  ,  ).
This posterior is approximated with the help of a variational distribution r. Figure 3 illustrates its graphical model representation.
The personalization vector   is assumed to be Gaussian with the following density r(  |  ) = (2 2)  T 2 exp(cid:18) 
 2 2 (     )    (     )(cid:19) Here, the variational parameter   represents the mean of the gaussian and its variance is  2I.
For query qi we assume that zi is sampled from a Bernoulli distribution with parameter  i   (0, 1).
Finally, for user u, we assume that   is sampled from a beta distribution having the following density function r(  |  1,  2) =  ( 1 +  2)  ( 1) ( 2)    1 1(1     ) 2 1 where the parameters  1,  2 > 0 and  (x) is the Gamma function.
We use the notation  (x) for the digamma function which is de ned as d dx ln  (x).
The next step in our variational analysis is to learn the particular value of the parameters ( ,  1,  2,  ) that minimizes the KL divergence between r and the true posterior p.
It can be shown8 that minimizing the KL divergence has the same e ect as maximizing the following objective function,  ( ,  1,  2,  ) = Er [ln p] + H(r) (4) where H(r) is the entropy and Er denotes expectation w.r.t the distribution r.
We use block coordinate-wise ascent to maximize the expression in Equation 4.
Intuitively, we perform  xed point iterations by updating one block of parameters at a time, keeping all other parameters  xed to their most recent value.
The update rule for parameters  1,2,  ,m,  1,  2 are obtained by setting the partial derivatives of   to zero.
Due to our choice of r, the update rules for  ,  1,  2 are particularly simple and have closed-form expressions.
Algorithm 1 LTP-INF: Variational Inference Algorithm for LTP
 ( ,  )1,2,  ,m; values for
 1 ,  (0)
 1 m,  (0)
 |  j ,  )    ,  ,  ,  ; 1,   1 m,   1 ,  (0) 2 ,  (0));

 1 ,  (0)  (0)) such that 1 >  (0) 1...m > 0 and  (0)
 1 m,  (0) 5: while   has not converged do

 j=1(1    (i 1) j=1  (i 1)

 i   i + 1;  (i)  (i) for j = 1...m do  j    ( (i) Er[ln g( j |  ;  j ,  ,  )];  (i) j   1/(1 + e j ); /* Update  j */
 1 ) + ln f ( j ); j ; j





 end for  (i)   argmax    ( (i) 1 m,  (i) 1 ,  (i)
 gate gradient to optimize this block */
 15: end while 16: return ( (i) 1 m,  (i) 1 ,  (i) 2 ,  (i)) 1 m,  (i) 1 ,  (i) 2 ,  (i)); To maximize   with respect to  , we use the conjugate gradient algorithm9.
The objective function for   can be written as L( ) =  

 (1    i)   Er [ln g( i |  i,  ,  )] It can be proved that L is concave (with respect to  ) and therefore, using simple optimizers like conjugate gradient, we will be able to obtain the global maximum [4].
Algorithm 1 summarizes the inference procedure.
Due to page limits, we omit the derivations and refer to the full version of the paper [15] for details.
We now focus our attention at learning   and  .
We use Maximum Likelihood Estimators (MLE) for this, where one  nds the value of the parameters that maximizes the (log) likelihood of the observed data i.e. the following expression ln p(  |  ,  ;  ) = m Xi=1 ln p( i |  ,  ;  i) (5) However, to calculate the likelihood function, we have to marginalize over the latent variables which is di cult in our model for both real variables ( ,   ), as it leads to integrals that are analytically intractable, and discrete variables (z1 m), it involves computationally expensive sum over exponential (i.e. 2m) number of terms.
We use the variational Expectation Maximization (EM) algorithm to circumvent this di culty.
In the E-step, Algorithm 1 approximates the true posterior distribution over the latent variables, using the current estimates of the parameters.
The variational parameters learned in this step 9http://en.wikipedia.org /wiki/Nonlinear conjugate gradient method



  (0)   1 and  (0) > 0.
4: while ( ,  ) have not converged do
 /* The variational inference step */ (  1 m,   1,  
      (i)( ,  )   r( , 
 1 ,  2,  ) [ln p];
 returned by a search engine.
During the training phase, we present these queries to LTP and let it learn the personaliza-tion vector  .
Once   is learned, the next step is to validate it, by measuring how well it corresponds to the ground truth.
However, in practice, such validation schemes are often di cult to design as the search engines do not reveal the actual user pro le10.
We therefore perform our experiments on both real-world data-set comprised of Google search history of a few users, and a large scale synthetic data-set.
We collected search history data from 10 real Google users in Nov 2012.
We fetched their past search queries using Google API that returns a sample of about 1000 web queries from her history (it also contains other queries like map, image, etc.
that we ignored).
We retrieved on average 850 distinct queries for each user.
We issued each distinct query in his history (in a randomized order) to Google both with and without their login credentials to retrieve the search results.
For a given query, both the results were fetched at the same time (within a few seconds of each other) and using the identical connection parameters such as user-agent (UA), IP-address, http headers (except cookies), etc.
This process removes non-pro le based personalizations such as those based on context of the current session (randomized order breaks any coherent context in user s history), IP address or location, time-of-the-day (the whole data collection for a user took only a few minutes), browser or OS type, etc.
Hence the di erences in results should be only due to user s pro le.
We then parsed the result pages and extracted the ranked results.
We ignored the paid links (at top and bottom of the page) and any map, image, or other embedded group of results that some queries return.
We then used the Mallet [19] toolkit to extract topics from the urls11 separately for each user.
Due to privacy considerations, we then annonymized the entire data set by mapping each url, query and topic to (randomly generated) IDs.
Our algorithms were run on this annonymized data.
We found ample evidence of pro le based personalization on Google roughly 30% queries received personalized results, i.e. had di erences in the ranks of urls in personalized and vanilla results.
We also found that the personalization is much more subtle compared to the impression we get from search personalization literature (and our experiments with AlterEgo server) most queries (  70%) were not personalized and while there were some queries with fair amount of personalization, on an average, we observed very little di erence between the results12.
We use an open source search personalization engine called AlterEgo [18] to generate the synthetic dataset.
AlterEgo contains implementation of various popular pro ling and
 to serve personalized ads.
Unfortunately, this data is not quite helpful as the categories are very high level and do not convey rich enough information.
the search results to obtain text for the urls.
the EMD of moving a with personalization was 5.9 (e.g.
single url at rank 5 to rank 1 is 4)
   ( (i+1),  (i+1))   argmax  (i)( ,  )  >0 1 0
 i   i + 1 8: end while 9: return ( (i),  (i)) are used in the subsequent M-step to maximize the likelihood function (over the true parameters   and  ).
Algorithm 2 summarizes the steps of the variational EM.
It can be shown [15] that the constraint maximization problem in step 6 is a concave program and therefore, can be solved optimally and e ciently [4].
For inference in the topic block (Figure 1), we augment our variational distribution with additional parameters in the following way.
Topic distribution  k is sampled from a Dirichlet prior with parameters {  k,w | w   V }.
The topic assignments Ki,j are sampled from a multinomial distribution with parameters  i,j,1 T and  i is sampled from a normal distribution with mean  i and variance  2I.
Using the same recipe as in Section 4.2 (c.f.
Equation 4), we arrive at the following simple update rule for learning the topic distributions  k,w =   + Xi,j Wi,j =w  i,j,k The topic assignments  i,j also has a closed form update rule as given by  i,j,k   exp(Er[ln  i] + Er[ln  k,wi,j ]) The main di culty in learning topic-maps (i.e.  i s) stems from the coupling between the personalization and the topic blocks through  .
While determining Er[ln g(  |  ,  ;  ,  )] (step 8 of Algorithm 1), we now have to take expectation over  , in addition to  .
However this calculation is analytically tractable due to our assumption of independence and gaussian priors on   and  .
We use gradient descent on   to solve it.
The rest of the calculation remains unchanged.
In this section, we describe a comprehensive set of experiments designed to evaluate the accuracy and e ectiveness of our techniques.
The input to our algorithm consists of a set of queries and the personalized and vanilla results (i.e.  ,   pairs) for them, 879personalization techniques; we used their  unique matching  technique for our experiments13.
In our simulation, we used AlterEgo as a surrogate personalization engine i.e. we obtain the vanilla result from Google and use AlterEgo to personalize it.
The bene t of this approach is that we can train AlterEgo on topics of our choice and use this information to validate the model output  .
The work ow and details of the data generation steps are presented below.
Generating Topics We extracted a set of 500 topics by running Mallet on approximately 420k urls obtained from the Delicious dataset[28].
We manually select 50 topics and label them into 10 categories (examples are health, cooking, science,  nance, etc.
); these topics serve as a ground-truth for us.
The selection of these topic categories and urls (used in the next step) is intended to simulate a typical user behavior, where, a user in interested in   10 categories of topics.
Training AlterEgo For each topic, we inspect the topic-maps of the urls and identify the ones which have signi cant (> 0.2) weight (on this topic).
These urls are used to train AlterEgo pro le.
We generated 10 pro les trained on a subset of 1 to 10 topics (i.e. 10 pro le for 1 topic, 10 pro le on 2 randomly selected topics, and so on), generating a total of 50 pro les.
Queries We generated 500 queries for each topic by randomly combining the top 10 relevant words from them.
This gives us a total of 5k queries (over 10 categories).
For each query, we retrieved the vanilla results from Google.
Note that, if a query is related to a topic used for training the pro le, only then AlterEgo will be able to personalize it.
Otherwise, the vanilla and personalized results will be more or less identical.
We use JOptimizer 14 - a java based open source optimization package for solving the convex program in Algorithm 2 (step 6) All our experiments are carried out on a Intel Pentium IV machine with 3.0GHz processor and 4GB of RAM.
We use the following values of the hyper-parameters :   =
 for inference in the topic-block (see Figure 1) and do not use the inference process described in Section 4.4.
In this section, we summarize the result of our experiments with the AlterEgo data-set.
Our  rst set of experiments are designed to evaluate the accuracy of LTP in correctly learning the personalized topics.
On each AlterEgo pro le, we train LTP and learn the personalization vector  .
Next we compare it with the actual list of topics that were used to train this pro le (by AlterEgo).
Let Tact be the true set of personalized topics and Tinf be the one inferred by LTP.
For this experiment, we measure the precision and recall values, where precision is de ned as |Tact Tinf | i.e. the fraction of reported topics that are actually personalized and recall by |Tact Tinf | |Tinf | |Tact| i.e.
nique, and got very similar results which are omitted due to lack of space.
14http://www.joptimizer.com/ the fraction of the original personalized topics that we are able to identify.
R-pre P@+1 P@+3 MAP



 Table 1: Performance (in %) of LTP in  nding personalized topics (with AlterEgo data-set).
We reorder the topics based on the (decreasing) value of   computed by LTP.
For each k, we declare the top-k topics (with maximum   values) as personalized and calculate the precision and recall value for this decision.
Table 1 summarizes the precision scores obtained by LTP.
Speci cally, we evaluate its performance in terms of Precision@1(P@1), P@3, P@5, R-precision (R-pre) and mean average precision (MAP) [5, 7].
Note that the size of actual topics was quite di erent for di erent runs (varies from 1-10).
Hence, along with the top-k topics, we also study the precision at |Tact+k| (denoted as P@+k).
.
.
.
.
.
.
l l a c e





 Figure 4: Precision-Recall results for LTP in retrieving the personalized topics (with AlterEgo data-set).
Precision In Figure 4, we illustrate the recall performance of our algorithm.
At the expense of low precision (< 0.4), LTP is able to retrieve all the personalized topics (recall   0.93) and its recall performance is relatively insensitive to precision; however, if we require high precision ( > 0.8), the recall drops to   0.5.
As evident from the  gure, a typical operating characteristic of LTP is precision   0.7 and recall   0.7, which is achieved when we return top-3 topics.
In this section, we develop two classi cation tests to evaluate LTP s predictive power.
For both these experiments, we randomly split the  ,   list into data-sets D1 (80%), used for training LTP, and D2 (20%), used for testing.
We repeat this split with 10 random seeds and report the average number in all the data presented below.
Query Disambiguation In this experiment, while testing on D2, we hide which result is personalized and which one is vanilla and the task of the model is to determine the correct labels.
We proceed with the classi cation task in the following way.
Let   be the parameter learned by LTP during the training.
For input lists l1 and l2, LTP calculates the likelihood values p(l1 | l2,  ) and p(l2 | l1,  ) and whichever likelihood is higher is assigned to the personalized result i.e.
if p(l1 | l2,  ) > p(l2 | l1,  ) then l1 is declared to be the 880#topics Accuracy (     ) Time (secs)
 .74   .09 .72   .06 .70   .05 .69   .04 .69   .05 .67   .04 .65   .04 .63   .04 .63   .05 .62   .02 .72   .09 .70   .09 .68   .06 .67   .05 .67   .05 .65   .05 .65   .05 .63   .04 .62   .05 .62   .02





























 Table 2: Summary of results with the AlterEgo dataset personalized result and vice versa.
We name this test as P-V disambiguation for a given pro le.
Over all the test points in D2, the fraction of queries that were labeled correctly is referred to as disambiguation accuracy.
Table 2 summarizes the result of this experiment.
In summary, we achieve disambiguation accuracy in the range of
 the 10 di erent runs and report its mean and standard deviation (     ).
Observe that our accuracy decreases slightly as the AlterEgo pro le is trained with more and more topics.
Table 2 also reports the training time of LTP-EM .
For pro les trained with many topics, LTP-EM takes more time to converge.
We repeat the experiment with LTP-INF with the parameter values  xed to   = 0.9 and   = 10.0.
As the results show, LTP-INF is up to 5 times faster to train but achieves slightly lower accuracy.
The accuracy however, improves slightly (< 3%) if we increase the amount of training data (D1) from 80% to 90% (not shown in the table).
1 topic 3 topics 5 topics ) % ( y c a r u c c



















 Number of users in a group Figure 5: Performance of LTP in user classi cation (with AlterEgo data-set).
User Classi cation For this experiment, we consider groups of users (i.e. pro les) and develop a classi cation test within the group members.
We vary the size of the group from 2 to 10 and for each group size, randomly pick
 LTP but do not reveal the user it belongs to.
The task of the model is to correctly predict the user.
We again use the likelihood test for this task.
Speci cally, u) for each user u   G and input ( ,  ), we calculate p(  |  ,   Google Category Topic in LTP Comics & Animation online read manga Anime & Manga Autos & Vehicles Vehicle Shopping Computers Software Utilities World Localities -kyojin shingeki chapter car india chrysler price jaguar sport bmw class import common org public implement seoul citi hotel South Asia location shop mall coex  



 Table 3: Correlation between personalized topics in LTP and Google categories.
u learned during training) and output the user for which (  the likelihood attains its maximum value.
In Figure 5, we summarize the result of this experiment.
There are two parameters in this experiment - the size of the group and the number of topics used to train AlterEgo for each pro le in the group.
For simplicity, we present here results for the homogeneous case, where we combine pro le which are trained on the same number of topics 15.
Observe that the accuracy reported by LTP is signi cantly higher than a random guess (which is 1/g, g being the group size).
The accuracy decreases slightly if pro les are trained with many topics.
We believe this reduction in accuracy is also an artifact of our data generation pro les trained on multiple topics can (and do) have topics in common, that will make it hard to distinguish personalized response on two pro le trained on the same topic.
In summary, these results, together with the precision-recall values from last section highlight that our model  ts the data well and learns the correct set of personalized topics on synthetic data.
In this section we describe the results with the Google data-set.
Note that since we do not know the actual person-alization on di erent topics (ground truth) for a real Google user, we cannot perform the precision-recall experiments as with AlterEgo dataset, and resort to only query disambiguation and user classi cation test described above.
However, we also perform some qualitative tests that give ample indication that we have found a good personalization vector.
We now present our analysis on  nding qualitative correctness of   using evidences of personalization.
An evidence is an instance of  ,   where results were re-ranked such that the ones with   were moved up.
Note that while such evidence have no statistical signi cance, they are much more helpful for a user s understanding of his pro le compared to the personalization vector.
Such evidences are a core feature of the privacy toolkit we are building (see Section 6).
Figure 6 shows an example evidence of personalization happening on a user s account.
The result for query Q ( how to decide mixing of markov chain ) and theta values for two relevant topics T1 (about  Algorithms  de ned by words algorithm, design, complexity) and T2 (about  Probability  de ned by words probability, distribution) are shown.
For
 by grouping pro les trained on 3 topics with 5 topics).
The results are similar and not repeated here.
.40   .90 .30
 .10 .15

 .01 .25 Figure 6: An example to illustrate the di erence between personalized (left) and vanilla (right) search results (for a real user) returned by Google.
User Id









 .74 .05 .68 .05 .67 .13 .54 .08 .54 .11 .85 .07 .73 .04 .66 .03 .52 .04
 .70 .05 .70 .04 .72 .14 .51 .06 .47 .09 .78 .05 .70 .05 .62 .03 .52 .03
 .70 .06 .70 .04 .67 .13 .55 .06 .49 .11 .84 .04 .71 .06 .61 .04 .50 .04
 .73 .04 .65 .03 .73 .11 .59 .07 .43 .09 .81 .07 .73 .06 .64 .03 .54 .04 Table 4: Accuracy of LTP over 9 Google users.
this user,   value for T1 is very high compared to T2.
Observe that the wiki link U1 (in the box), although less relevant to the query, is placed higher in the personalized results.
As our analysis shows, U2 is has a high weight on topic T1 compared to U2, which leads to this personalization.
The user can therefore see not just his inferred interests (more in  Algorithms  compared to  Probability ), but also how it a ects his results.
We next move to another qualitative analysis of   by comparing it directly with the categories Google itself associates with a user16.
We try to match topics with high   (top-k such topics) with the broad categories in Google.
Table 3 shows the result of such matching for 3 users.
Take for example, the  Anime and Manga  category, that was also assigned a very high   = .6 (compared to an average value of .004) by LTP.
Such anecdotes show that our techniques have, in fact, learned the personalization vector correctly.
Query Disambiguation Table 4 summarizes the result of query disambiguation on the Google dataset.
We  rst study the e ects of number of topics (T ) chosen for the user.
We notice that only a few topics 15-50 are enough to get good accuracy for any user.
Our accuracy results di er signi cantly for di erent users, varying from as low
 in Google ads preference manager https://www.google.com/settings/ads/onweb/ Group Size Number of Topics





 .59   .06 .48   .04 .61   .06 .53   .05 .65   .04 .60   .05 .58   .05 .50   .06 Table 5: User classi cation accuracy on Google data.
as 54% to 85%.
We believe this is because the amount of personalization is di erent for various users, and this a ects the learning accuracy of our techniques.
User Classi cation Table 5 show that even with 3 users, we are able to get an accuracy of up to 60%.
For this experiments, we extracted   values over a common set of topics for each user.
These   values learned were also very di erent for di erent users (data not shown).
This shows that   is in fact learned tailored to the personalization of each user.
In this paper we have presented a novel approach to extract user pro le information in the form of personalization vector over topics from commercial OSPs (such as Google search).
Our approach treats OSPs as black-boxes, i.e. assumes no knowledge of the personalization algorithms and history of users maintained by them, and works by comparing the personalized and vanilla content served by them.
To the best of our knowledge, this is the  rst work that tries to extract information based solely on mining the output of OSPs.
This approach is unique in not just enabling access to (so far hidden) pro les in OSPs, but also in providing a novel and practical approach for retrieving information from OSPs by mining di erences in their outputs.
Our approach also has direct bene ts for end users, as for the  rst time, it enables them to access their (so far hidden) pro le information tracked by an OSP.
While being an informational tool by itself, this has wider implications to the outlook of user privacy research it can be used to infer the personalization happening on sensitive topics (e.g.
 nancial, medical history, etc.
), which a user may not be comfortable with.
We believe that this can be used to build an end-user privacy preserving tool and are currently working on a prototype for the same.
