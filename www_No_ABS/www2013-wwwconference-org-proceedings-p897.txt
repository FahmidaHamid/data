 Even when experts all agree, they may well be mistaken    Bertrand Russell In order to predict how a user will respond to a product, we must understand the tastes of the user and the properties of the product.
We must also understand how these properties change and evolve over time.
As an example, consider the Harry Potter  lm series: adults who enjoy the  lms for their special effects may no longer enjoy them in ten years, once their special effects are obsolete; children who enjoy the  lms today may simply outgrow them in ten years; future children who watch the  lms in ten years may not enjoy them, once Harry Potter has been supplanted by another wizard.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
This example highlights three different mechanisms that cause perceptions of products to change.
Firstly, such change may be tied to the age of the product.
Secondly, it may be tied to the age (or development) of the user.
Thirdly, it may be tied to the state (or zeitgeist) of the community the user belongs to.
These mechanisms motivate different models for temporal dynamics in product recommendation systems.
Our goal in this paper is to propose models for such mechanisms, in order to assess which of them best captures the temporal dynamics present in real product rating data.
A variety of existing works have studied the evolution of products and online review communities.
The emergence of new products may cause users to change their focus [19]; older movies may be treated more favorably once they are considered  classics  [20]; and users may be in uenced by general trends in the community, or by members of their social networks [26].
However, few works have studied the personal development of users, that is, how users  tastes change and evolve as they gain knowledge, maturity, and experience.
A user may have to be exposed to many  lms before they can fully appreciate (by awarding it a high rating) Citizen Kane; a user may not appreciate a Roman e-Conti (the  Citizen Kane  of red wine) until they have been exposed to many inferior reds; a user may  nd a strong cheese, a smokey whiskey, or a bitter ale unpalatable until they have developed a tolerance to such  avors.
The very act of consuming products will cause users  tastes to change and evolve.
Developing new models that take into account this novel viewpoint of user evolution is one of our main contributions.
We model such  personal development  through the lens of user experience, or expertise.
Starting with a simple de nition, experience is some quality that users gain over time, as they consume, rate, and review additional products.
The underlying hypothesis that we aim to model is that users with similar levels of experience will rate products in similar ways, even if their ratings are temporally far apart.
In other words, each user evolves on their own  personal clock ; this differs from other models of temporal dynamics, which model the evolution of user and product parameters on a single timescale [20, 40, 41].
Naturally, some users may already be experienced at the time of their  rst review, while others may enter many reviews while failing to ever become experienced.
By individually learning for each user the rate at which their experience progresses, we are able to account for both types of behavior.
Speci cally, we model each user s level of experience using a series of latent parameters that are constrained to be monotonically non-decreasing as a function of time, so that each user becomes more experienced (or stays at least as experienced) as they rate additional products.
We learn latent-factor recommender systems for
 (a) Individual user evolution at uniform intervals: (b) Community evolution at learned intervals: (c) Individual user evolution at learned intervals: (d) Figure 2: Visualization of the models we consider.
Horizontal bars represent user review timelines; colors within each bar represent evolution parameters for each user.
beginners or by experts that is, we can discover which products are acquired tastes.
The rest of this paper is organized as follows: We describe our models for user evolution in Section 2, and we describe how to train these models in Section 3.
In Section 4 we describe our novel rating datasets and evaluate our models.
In Section 5 we examine the role of our latent experience variables in detail, before reviewing related work in Section 6 and concluding in Section 7.
We design models to evaluate our hypothesis that  experience  is a critical underlying factor which causes users  ratings to evolve.
We do so by considering alternate paradigms of user and community evolution, and determining which of them best  ts our data.
Figure 2 visualizes each of the four models we consider.
Each horizontal bar represents a single user s review timeline, from their  rst to their last review; the color within each bar represents the evolution of that user or their community.
At each of these stages of evolution, a different recommender system is used to estimate their ratings.
Recommender systems for adjacent stages are regularized to be similar, so that transitions between successive stages are smooth.
(a) Community evolution at uniform intervals: First we consider a model where  stages of evolution  appear at uniform time intervals throughout the history of the community.
The model of Figure 2 (a) is in some sense the most similar to existing works [20, 40, 41] that model evolution of users and products using a single global  clock .
The intuition behind this model is that communities evolve over time, and prefer different products at different time periods.
(b) User evolution at uniform intervals: We extend the idea of community evolution and apply it directly to individual users (Fig. 2 (b)).
This model captures the intuition that users go through different life-stages or experience levels and their preferences then depend on their current life stage.
(c) Community evolution at learned intervals: This model extends (a) by learning the rates at which communities change over time (Fig. 2 (c)).
The model is based on the intuition that a community may not evolve at a uniform rate over time and that it is worth modeling different stages of community evolution.
(d) Individual user evolution at learned intervals: Last, we consider a model where each user individually progresses between experience levels at their own personal rate (Fig. 2 (d)).
This model is the most expressive of all four and is able to capture interesting phenomena.
For example, some users may become experts very quickly while others may never reach the highest level of experience; others may behave like experts from the time they join (e.g.
the bottom right user of Fig. 2 (d)).
This model is able capture such types of behavior.
Models (a) and (b) are designed to assess whether user evolution is guided by changes at the level of individual users, or by changes in the community at large.
We  nd that, given enough data, both models lead to modest improvements over traditional latent-factor recommender systems, though there is no clear winner between the two.
Once we learn the stages at which users and communities evolve, as in (c) and (d), we signi cantly outperform traditional rec-ommender systems, though the bene t of learning is much higher when we model evolution at the level of each individual user, i.e., when we treat  evolution  as analogous to  becoming an expert .
Put simply, we  t recommender systems for different stages of user evolution, and the models differ only in terms of how users progress between stages.
Thus the actual recommender systems used by each model have the same number of parameters, though the models of Figure 2 (c) and (d) have additional parameters that control when users evolve.
The model of Figure 2 (d) is the most expressive, in the sense that it has enough  exibility to represent each of the other models.
For example, if no evolution took place at the level of individual users, the model of Figure 2 (d) could learn latent parameters similar to those of Figure 2 (c).
In practice, we  nd that this is not the case; rather we learn dynamics of individual users that are quite different to those at the level of communities.
Model Speci cation We shall  rst describe our most general model, namely that of Figure 2 (d).
The other models in Figure 2 can later be treated as special cases.
We start with the  standard  latent-factor recommender system [33], which predicts ratings for user/item pairs (u, i) according to rec(u, i) =   +  u +  i + (cid:2) u,  i(cid:3).
Here,   is a global offset,  u and  i are user and item biases (respectively), and  u and  i are latent user and item features.
Although this simple model can capture rich interactions between users and products, it ignores temporal information completely, i.e., users  review histories are treated as unordered sets.
Even so, such models yield excellent performance in practice [22].
the proxy of user experience.
We do so by designing separate rec-ommender systems for users who have different experience levels.
Naturally, a user s experience is not  xed, but rather it evolves over time, as the user consumes (and rates) more and more products.
For each of a user s ratings rui, let tui denote the time at which that review was entered (for simplicity we assume that each product is reviewed only once).
For each of a user s ratings, we will  t a latent variable, eui, that represents the  experience level  of the user u, at the time tui.
Each user s experience level evolves over time.
As a model assumption, we constrain each user s experience level to be a non-decreasing function of time.
That is, a user never becomes less experienced as they review additional products.
We encode this using a simple monotonicity constraint on time and experience:  u, i, j tui   tuj   eui   euj.
(1) What this constraint means from a modeling perspective is that different users evolve in similar ways to each other, regardless of the speci c time they arrive in the community.
In practice, we model experience as a categorical variable that takes E values, i.e., eui   {1 .
.
.
E}.
Note that it is not required that a user achieves all experience levels: some users may already be experienced at the time of their  rst review, while others may fail to become experienced even after many reviews.
Assuming for the moment that each experience parameter eui is observed, we proceed by  tting E separate recommender systems to reviews written at different experience levels.
That is, we train rec1(u, i) .
.
.
recE(u, i) so that each rating rui is predicted using receui (u, i).
As we show in Section 3, we regularize the parameters of each of these recommender systems so that user and product parameters evolve  smoothly  between experience levels.
In short, each of the parameters of a standard recommender system is replaced by a parameter that is a function of experience: rec(u, i) = receui (u, i) =  (eui) +  u(eui) +  i(eui) + (cid:2) u(eui),  i(eui)(cid:3).
(2) Such a model is quite general: rather than assuming that our model parameters evolve gradually over time, as in [20], we assume that they evolve gradually as a function of experience, which is itself a function of time, but is learned per user.
Thus we are capable of learning whether users   experience  parameters simply mimic the evolution of the entire community, as in Figure 2 (c), or whether there are patterns of user evolution that occur independently of when they arrive in the community, as in Figure 2 (d).
Because of this generality, all of the models from Figure 2 can be seen as special cases of (eq.
2).
Firstly, to model evolution of communities (rather than of individual users), we change the mono-tonicity constraint of (eq.
1) so that it constrains all reviews (rather than all reviews per user):  u, v, i, j tui   tvj   eui   evj.
(3) Secondly, the  learned evolution  models (c, d) and the non-learned models (a, b) differ in terms of how we  t the experience parameters eui.
For the non-learned models, experience parameters are set using a  xed schedule: either they are placed at uniformly spaced time points throughout the entire corpus, as in Figure 2 (a), or they are placed at uniformly spaced time points for each individual user s history, as in Figure 2 (b).
In the next section, we describe how to learn these parameters, i.e., to model the points at which a community or an individual user changes.
We wish to optimize our model parameters and latent experience variables so as to minimize the mean-squared-error of predictions on some set of training ratings rui   T .
Suppose each of our E recommender systems has parameters  e = ( (e);  u(e);  i(e);  u(e);  i(e)), and that the set of all experience parameters eui is denoted as E.
Then we wish to choose the optimal (  ,  E) according to the objective (  ,  E) = argmin

 rui T

 +  ( ) s.t.
tui   tuj   eui   euj.
(4) This equation has three parts: the  rst is the mean-squared-error of predictions, which is the standard objective used to train recom-mender systems.
The second part,  ( ), is a regularizer, which penalizes  complex  models  .
Assuming that there are U users, I items, E experience levels, and K latent factors, then our model has (1 + U + I + U   K + I   K)   E parameters, which will lead to over tting if we are not careful.
In practice, similar experience levels should have similar parameters, so we de ne  ( ) using the smoothness function

 e=1 (cid:8) e    e+1(cid:8)2
 (5) where ||   ||2
 between successive experience levels.
  is a regularization hyper-parameter, which  trades-off  the importance of regularization versus prediction accuracy at training time.
We select     10

 by withholding a fraction of our training data for validation, and choosing the value of   that minimizes the validation error.
The third and  nal part of (eq.
4) is the constraint of (eq.
1), which ensures that our latent experience parameters are monotonically non-decreasing for each user.
Simultaneously optimizing all of the parameters in (eq.
4) is a dif cult problem, in particular it is certainly not convex [22].
We settle for a local optimum, and optimize the parameters   and E using coordinate ascent [27].
That is, we alternately optimize (eq.
4) for   given E, and for E given  .
Optimizing (eq.
4) for   given E, while itself still a non-convex problem, can be approached using standard techniques, since it essentially reduces to optimizing E separate recommender systems.
In practice we optimize model parameters during each iteration using L-BFGS [31], a quasi-Newton method for nonlinear optimization of problems with many variables.
Alternately, optimizing (eq.
4) for E given   means assigning each of a user s reviews to a particular recommender system, corresponding to that review s experience level.
The best assignment is the one that minimizes the mean-squared-error of the predictions, subject to the monotonicity constraint.
Optimizing a sequence of discrete variables subject to a mono-tonicity constraint can be solved ef ciently using dynamic programming: it is related to the Longest Common Subsequence problem [4], which admits a solution whose running time (per user) is bilinear in E and the number of ratings in their history.
This procedure is visualized in Figure 3, for E = 5 experience levels, and a user with 8 ratings.
Rows represent each of the  ve experience levels, while columns represent each of a particular user s ratings, ordered by time.
The optimal non-decreasing set of experience levels is the shortest path from the  start  to the  end  of this 900dataset Beer (beeradvocate) Beer (ratebeer) Fine Foods (amazon) Movies (amazon) Wine (cellartracker)
 #users





 #items





 #ratings





 Table 1: Dataset statistics.
of experience levels, and the number of latent product and item dimensions to E = 5 and K = 5; larger values did not signi cantly improve performance in our experience.
Since it is unlikely to be fruitful to model the evolution of users who have rated only a few products, we compare our models on users with at least 50 ratings.
Users with fewer than 50 ratings are not discarded, but rather their ratings are combined so that they are treated using a single  background  model; we then model the evolution of this entire group as though it were a single user.
We use two schemes to build our test sets: our  rst scheme consists of selecting a random sample of reviews from each user.
This is the standard way of selecting test data for  at  models that do not model temporal dynamics.
The second scheme we use to build our test set is to consider the  nal reviews for each user.
The latter setting represents how such a system would be used in practice, in the sense that our goal is to predict how users would respond to a product now, rather than to make post hoc predictions about how they would have responded in the past.
However, sampling reviews in this way biases our test set towards reviews written by more experienced users; this is no longer the case when we sample reviews randomly.
Of course, we do not  t latent experience parameters for the ratings in our test set.
Thus for each rating used for testing, we assign it the experience level of its chronologically nearest training rating.
Datasets Our choice of rating data re ects a variety of settings where users are likely to have  acquired tastes .
The datasets we consider are summarized in Table 1.
Each of our datasets were obtained from public sources on the web using a crawler, and are made available for others to use.1 We consider the beer review websites BeerAdvo-cate and RateBeer, the wine review website CellarTracker, as well as reviews from the Fine Foods and Movies categories from Amazon.
In total we obtain over 15 million ratings from these sources.
In principle we obtain the complete set of reviews from each of these sources; data in each of our corpora spans at least 10 years.
We previously considered BeerAdvocate and RateBeer data in [28], though not in the context of recommendation.
Recommendation on (different) Amazon data has been discussed in [15] and [25].
Since each of these rating datasets has a different scale (e.g.
beers on RateBeer are rated out of 20, wines on CellarTracker are rated out of 100, etc.
), before computing the MSE we  rst normalize all ratings to be on the scale (0, 5].
Evaluation Results in terms of the Mean Squared Error (MSE) are shown in Tables 2 and 3.
Table 2 shows the MSE on a test set consisting Figure 3: Experience  tting as a dynamic programming problem.
Rows represent experience levels, columns represent ratings, ordered by time.
graph, where the cost of visiting a node with rating rui at experience level k is the prediction error (reck(u, i)   rui)
 These two steps are repeated until convergence, that is, until E does not change between successive iterations.
On our largest datasets, all parameters could be optimized in a few hours on a standard desktop machine.
Again, the above procedure refers to the most general version of our model, in which we learn monotonic evolution parameters per user, as depicted in Figure 2 (d).
Training the community version of our model (Fig. 2 (c)) simply means replacing the monotonicity constraint of (eq.
1) with that of (eq.
3).
Our goal in this section is to evaluate the models described in Figure 2.
We compare the following models: lf: a: b: c: d: A standard latent-factor recommender system [21].
A model whose parameters evolve for the entire community as a function of time (Fig. 2 (a)).
A model whose parameters evolve independently for each user (Fig. 2 (b)).
A model whose parameters evolve for the entire community as a function of time, where the  stages  of evolution are learned (Fig. 2 (c)).
A model whose parameters evolve independently for each user, where the stages of evolution are learned (Fig. 2 (d)).
The models of Figure 2 (a) and (c) are most similar to existing models for temporal evolution, e.g.
[22]: item parameters are shared by ratings made at the same time.
We aim to compare this to models where parameters are shared by users at the same experience level, regardless of the speci c time they arrive in the community (as in Fig. 2 (d)).
Experimental Setup To evaluate each method, we report the Mean Squared Error (MSE) on a fraction of our data withheld for testing, that is, for our test set U we report


 (receui (u, i)   rui)
 (6)
 rui U We also use a validation set of the same size to choose the hy-perparameter  .
Throughout our experiments we set the number 1http://snap.stanford.edu/data/ 901(lf) latent-factor model (a) community at uniform rate (b) user at uniform rate (c) community at learned rate (d) user at learned rate bene t of (d) over (lf) bene t of (d) over (c) BeerAdv.
(overall)






 BeerAdv.
(taste)






 BeerAdv.
(look)






 RateBeer (overall)






 Amazon Fine Foods Amazon Movies CellarTracker




















 Table 2: Results on users  most recent reviews.
MSE and standard error.
(lf) latent-factor model (a) community at uniform rate (b) user at uniform rate (c) community at learned rate (d) user at learned rate bene t of (d) over (lf) bene t of (d) over (c) BeerAdv.
(overall)






 BeerAdv.
(taste)






 BeerAdv.
(look)






 RateBeer (overall)






 Amazon Fine Foods Amazon Movies CellarTracker




















 Table 3: Results on randomly sampled reviews.
MSE and standard error.
of the most recent reviews for each user, while Table 3 shows the MSE on a random subset of users  reviews.
We also note that while we obtain signi cant bene ts on Amazon data, the mean-squared-errors for this dataset are by far the highest.
One reason is that Amazon users use a full spectrum of ratings from
 on a smaller spectrum (after normalizing the ratings, most wines have scores above 4.25); this naturally leads to higher MSEs.
Another reason is that our Amazon data has many products and users with only a few reviews, so that we cannot do much better than simply modeling their bias terms.
As we see in Section 5, bias terms differ signi cantly between beginners and experts, so that modeling expertise proves extremely bene cial on such data.
So far, we have used our models of user expertise to predict users  ratings, by  tting latent  experience  parameters to all ratings in our corpora.
Now we move on to examine the role of these latent variables in more detail.
Throughout this section we use the term  expert  to refer to those reviewers (and ratings) that are assigned the highest experience level by our model (i.e., eui = E).
We use the term  beginner  to refer to reviewers and ratings assigned the lowest level (i.e., eui = 1).
Again, we acknowledge that  expertise  is an interpretation of our model s latent parameters, and other interpretations may also be valid.
However, in this section, we demonstrate that our latent parameters do indeed behave in a way that is consistent with our intuitive notion of expertise.
We begin by examining how a user s experience level impacts our ability to predict their rating behavior.
Table 4 compares the prediction accuracy of our model on reviews written at different experience levels.
We  nd in all but one case that users at the highest experience level have the lowest MSE (for Amazon Movies they have the second lowest by a small margin).
In other words their rating behavior can be most accurately predicted by our model.
This is not to say that experts agree with each other (which we discuss later); rather, it says that individual experts are easier to model than other categories of user.
Indeed, one can argue that some notion of Table 2 shows that our model signi cantly outperforms alternatives on all datasets.
On average, it achieves a 14% reduction in MSE compared to a standard latent factor recommender system, and a 10% reduction compared to its nearest competitor, which models user evolution as a process that takes place at the level of entire communities.
Note that due to the large size of our datasets, all reported improvements are signi cant at the 1% level or better.
By considering only users  most recent reviews, our evaluation may be biased towards reviews written at a high level of experience.
To address this possibility, in Table 3 we perform the same evaluation on a random subset of reviews for each user.
Again, our model signi cantly outperforms all baselines.
Here we reduce the MSE of a standard recommender system by 17%, and the nearest competitor by 13% on average.
Reviews from BeerAdvocate and RateBeer have multiple dimensions, or  aspects  to users  evaluations.
Speci cally, users evaluate beers in terms of their  taste ,  smell ,  look , and  feel  in addition to their overall rating [28].
Tables 2 and 3 show results for two such aspects from BeerAdvocate, showing that we obtain similar bene ts by modeling users  evolution with respect to such aspects.
Similar results from RateBeer are omitted.
It is perhaps surprising that we gain the most signi cant bene- ts on movie data, and the least signi cant bene ts on beer data.
However, we should not conclude that movies require more expertise than beer: a more likely explanation is that our movie data has a larger spectrum of expertise levels, whereas users who decide to participate on a beer-rating website are likely to already be somewhat  expert .
We gain the most signi cant bene ts when considering reviews written at all experience levels (as in Table 3) rather than considering users  most recent reviews (as in Table 2).
However we should not conclude from this that experts are unpredictable (indeed in Section 5 we con rm that experts are the most predictable).
Rather, since inexperienced users are less predictable, we gain the most bene t by explicitly modeling them.
RateBeer Amazon Fine Foods Amazon Movies CellarTracker e = 1 e = 2 e = 3 e = 4 e = 5




 Table 4: MSE per experience level e  predictability  is a necessary condition for such users to be considered  experts  [12].
While we  nd that beginners and intermediate users have lower prediction accuracy, it is surprisingly the  almost experts  (eui = E   1) who are the least predictable; from Table 4 we see that such users have the highest MSE in three out of  ve cases.
From this we might argue that users do not become experts via a smooth progression, but rather their evolution consists of several distinct stages.
Experience Progression Next, we study how users progress through experience levels as a function of time.
Figure 4 shows the (cumulative) time taken to progress between experience levels (the  nal bar represents the entire lifetime of the user, since there is no further level to progress to).
The dark blue bars show the progression for those users who progress through all levels of experience, i.e., it ignores those users who arrive to the site already experienced as well as those who never obtain the highest experience levels.
The yellow bars show users who reach all but the highest experience level.
How much time is spent at each experience level?
First, we observe that on most datasets, the  nal experience level is the  longest , i.e., it covers the longest time period, and includes the largest number of reviews.
This makes sense from the modeling perspective, when taken together with our previous  nding that experts  ratings are easier to predict: the model is  ner-grained  during the stages of user evolution that are most dif cult to  t accurately.
Fewer distinct experience levels are required later on, once users  rating behavior has  converged .
Do users who become experts differ from those who don t?
Secondly, Figure 4 compares users who progress through all levels of experience to users who do not.
Yellow bars show the progression of users who reach all but the  nal experience level.
Surprisingly, while such users enter roughly the same number of ratings per level (Fig. 4, bottom) as those users who eventually become experts, they do so much slower (Fig. 4, top).
Thus it appears as though the rate at which users write reviews, and not just the number of reviews they write, is tied to their progression.
Do experts agree with each other?
Thirdly, Figure 5 shows the extent to which users agree with each other as they become more experienced.
 Agreement  has been argued to be another necessary condition to de ne users as experts [12].
To study this, we consider ratings of the same product, written at the same experience level.
Speci cally, for each item i and experience level k, we  nd the set of users who rated that item at that experience level, i.e., we  nd all u such that eui = k. We then compute the variance of such ratings for every item and experience level.
Our goal is to assess how this quantity changes as a function of users  experience.
We do so for all products that were reviewed at least 5 times at the same experience level.
Since this limits the amount of data we have to work with, we  rst linearly interpolate each user s experience function over time (so that their experience function is a piecewise linear function, rather than a step function), and compute this quantity across a sliding window.
Indeed, in Figure 5 we  nd that users do tend to agree with each other more as they become more experienced, i.e., their ratings have lower variance when they review the same products.
This is consistent with our  nding that experts  ratings are easier to predict than those of beginners.
User Retention Next we consider how experience relates to user retention.
We want to study how users who leave the community (de ned as users who have not entered a review for a period of six months) differ from those who remain in the community.
Figure 6 visualizes the experience progression of these two groups.
Here we consider the  rst 10 ratings for all users who have entered at least 10 ratings (Fig. 6, top), and the  rst 100 ratings for all users who have entered at least 100 ratings (Fig. 6, bottom); this scheme ensures that every datapoint is drawn from the same sample population.
We  nd that both classes of users enter the community at roughly the same level (at the time of their  rst review, both groups have roughly the same experience on average).
However, as the number of reviews increases, users who go on to leave the community have lower experience compared to those who stay.
In other words, they gain experience more slowly.
This discrepancy is apparent even after their  rst few reviews.
This failure to become experienced may be a factor which causes users to abandon the site, and could be used as a feature in  churn prediction  problems [11, 18].
We mention the parallel work of [9], which also studies BeerAdvocate and RateBeer data: there, a user s failure to adopt the linguistic norms of a community is considered as a factor that may in uence whether they will abandon that community.
Acquired Tastes In Figure 1, we hinted at the idea that our model could be used to detect acquired tastes.
More precisely, it can help us to identify products that are preferred by experts over beginners (and vice versa).
To do so, we compare the difference in product bias terms between the most expert (experience level 5) and the least expert (experience level 1) users.
That is, we compute for each item i the quantity di =  i(5)    i(1).
Thus a positive value of di indicates that a product is preferred by experts over beginners, while a negative value indicates that a product is preferred by beginners over experts.
How do expert and beginner biases differ?
In Figure 7 we compare the average rating of each product to di (for products with at least 50 ratings).
Our main  nding in this  gure is that there exists a positive relationship between products that are highly rated and products that are preferred by experts.
In other words, products with high average ratings are rated more highly by experts; products with low average ratings are rated more highly by beginners.
Recall that in Figure 1 we examined the same relationship on RateBeer data in more detail.
One explanation is that the  best  products tend to be ones that require expertise to enjoy, while novice users may be unable to appreciate them fully.
This phenomenon is the most pronounced on
 s k e e w ( s s e r g o r p o t e m i t ) s g n i t a r ( s s e r g o r p o t e m i t

















 BeerAdvocate




 experience level BeerAdvocate ) s k e e w ( s s e r g o r p o t e m i t ) s g n i t a r ( s s e r g o r p o t e m i t


 experience level













 users who never become experts users who become experts RateBeer


 experience level

 RateBeer Amazon Fine Foods


 experience level

 Amazon Fine Foods ) s k e e w ( s s e r g o r p o t e m i t






 ) s g n i t a r (






 ) s k e e w ( s s e r g o r p o t e m i t







 ) s g n i t a r (





 Amazon Movies




 experience level Amazon Movies ) s k e e w ( s s e r g o r p o t e m i t ) s g n i t a r ( s s e r g o r p o t e m i t


 experience level












 CellarTracker




 experience level CellarTracker


 experience level

 s s e r g o r p o t e m i t


 experience level

 s s e r g o r p o t e m i t


 experience level

 Figure 4: Users who never become  experts  tend to progress slower than users who do.
Cumulative time (top), and number of ratings (bottom), taken to progress between experience levels.
e c n a i r a v w e v e r i

 BeerAdvocate


 experience level e c n a i r a v w e v e r i








 RateBeer


 experience level e c n a i r a v w e v e r i







 Amazon Fine Foods




 experience level e c n a i r a v w e v e r i







 Amazon Movies


 experience level e c n a i r a v w e v e r i .018 .017 .016 .015 .014 .013 .012

 CellarTracker


 experience level
 Figure 5: Experienced users agree more about their ratings than beginners.
Experience versus rating variance (when rating the same product).
our Movies and RateBeer data, and exists to a lesser extent on our BeerAdvocate and Fine Foods data; the phenomenon is absent altogether on our CellarTracker data.
Again, we should not conclude from this that movies require more  expertise  than wine, but rather that our Movies data has a larger separation between beginners and experts.
Perhaps more surprising is the lack of products that appear in the top left or bottom right quadrants of Figure 7, i.e., products with below average ratings, but positive values of di, or products with above average ratings but negative values of di.
In other words, there are neither products that are disliked by beginners but liked by experts, nor are there products that are liked by beginners but disliked by experts.
It is worth trying to rule out other, more prosaic explanations for this phenomenon: for instance, it could be that beginners give mediocre reviews to all products, while experts have a larger range.
We mention two negative results that discount such possibilities:  rstly, we found no signi cant difference between the average ratings given by beginners or experts.
Secondly, we did not observe any signi cant difference in the variance (that is, the variance across all of a user s reviews, not when reviewing the same product as in Figure 5).
Which genres are preferred by experts or beginners?
In Figure 1 we showed that there are entire genres of products that tend to be preferred by experts or by beginners.
Speci cally, we showed that almost all strong ales have positive values of di (preferred by experts), while almost all lagers have negative values of di (preferred by beginners).
Of course, it is not surprising (to a beer drinker) that experts dislike lagers while preferring India Pale Ales (IPAs), though it is more surprising that beginners also have the same polarity with respect to these products the experts are simply more extreme in their opinions.
Table 5 shows which genres have the lowest and highest values of di on average, i.e., which products are most preferred by beginners and experts (respectively).
We focus on BeerAdvocate, Rate-Beer, and CellarTracker, which have the most meaningful genre information.
The results are highly consistent across BeerAdvo-cate and RateBeer, in spite of the differing product categorizations used by the two sites (Kvass is a form of low-alcohol beer, Kristall-weizen is a form of wheat beer, IPA is a form of strong ale, and Gueuze is a type of lambic).
Again, there is a clear relationship between products  overall popularity and the extent to which experts prefer them; nonalcoholic beer is naturally not highly rated on a beer rating website, while lambics and IPAs are more in favor.
904users who stay in the community users who leave the community l e v e l e c n e i r e p x e l e v e l e c n e i r e p x e














 BeerAdvocate l e v e l e c n e i r e p x e
 review number BeerAdvocate l e v e l e c n e i r e p x e

 review number




















 RateBeer l e v e l e c n e i r e p x e
 review number RateBeer l e v e l e c n e i r e p x e

 review number
















 Amazon Fine Foods l e v e l e c n e i r e p x e
 review number Amazon Fine Foods l e v e l e c n e i r e p x e

 review number
















 Amazon Movies l e v e l e c n e i r e p x e
 review number Amazon Movies l e v e l e c n e i r e p x e

 review number
















 CellarTracker
 review number CellarTracker

 review number

 Figure 6: Users whose experience progresses slowly are more likely to abandon the community.
First 10 ratings of all users who have at least 10 ratings (top), and  rst 100 ratings of all users who have at least 100 ratings (bottom).
Traditional recommender systems treat each user s review history as a series of unordered events, which are simply used to build a model for that user, such as a latent factor model [22].
In spite of the excellent performance of such models in practice, they naturally fail to account for the temporal dynamics involved in recommendation tasks.
Some early works that deal with temporal dynamics do so in terms of concept drift [23, 37, 39].
Such models are able to account for short-term temporal effects ( noise ), and long-term changes in user behavior ( drift ), for example due to the presence of new products within a community.
Sophisticated models of such temporal dynamics proved critical in obtaining state-of-the-art performance on the Net ix challenge [3], most famously in [20].
As discussed in [20], few previous works had dealt with temporal dynamics, other than a few notable exceptions [2, 10, 35].
Around the same time,  adaptive neighborhood  models were proposed [24], that address the problem of iteratively training recommender systems whose parameters ought to change over time.
Better performance may be obtained by modeling large-scale global changes at the level of entire communities [41], or by developing separate models for short term changes (e.g.
due to external events), and long-term trends [40].
Other works that study temporal dynamics at the level of products and communities include [29], where the authors studied how existing ratings within communities may in uence new users, and how community dynamics evolve; and [13], who studied how users are in uenced by previous ratings of the same product.
Expertise has been studied in domains other than recommender systems, for example in the literature on education and psychology [5, 34].
One area where  expertise  has received signi cant attention is web search.
The role of expertise with respect to search behavior is a rich and historied topic, whose study predates the emergence of modern search engines [14].
Of particular interest is [38], since the authors study how users evolve (with respect to the level of technical content in their queries) as they gain expertise.
We also brie y mention the topic of expertise identi cation [1, 6, 16, 32].
This line of work is orthogonal to ours, in that it deals with discovering experts, rather than recommending products based on expertise; however, such works offer valuable insights, in the sense that like our own work, they attempt to model the behavior of expert users.
An interesting  nding of our work is that beginners and experts have the same polarity in their opinions, but that experts give more  extreme  ratings: they rate the top products more highly, and the bottom products more harshly.
Thus naively, we might conclude that we should simply recommend both groups of users the same products: nobody likes adjunct lagers, so what does it matter if beginners dislike them less?
The counter to this argument is that in order to fully appreciate a product (by giving it the highest rating), a user must  rst become an expert.
Thus perhaps we should focus on making a user an expert, rather than simply recommending what they will like today.
