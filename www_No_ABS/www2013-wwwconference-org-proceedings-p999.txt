This paper highlights a solution to a very speci c problem, the prediction of a  like  or  association  signal from one-class data.
One-class or  implicit  data surfaces in many of Xbox s verticals, for example when users watch movies through Xbox Live.
In this vertical, we recommend media items to users, drawing on the correlations of their viewing patterns with those of other users.
We assume that users don t watch movies that they dislike; therefore the negative class is absent.
The problem is equivalent to predicting new connections in a network: given a disjoint user and an item vertex, what is the chance that they should be linked?
We introduce a Bayesian generative process for connecting users and items.
It models the  like  probability by interpreting the missing signal as a two-stage process:  rstly, by modelling the odds of a user considering an item, and secondly, by eliciting a probability that that item will be viewed or liked.
This forms a core component of the Xbox Live architecture, serving recommendations to more than Copyright is held by the International World Wide Web Conference Committee (IW3C2).
IW3C2 reserves the right to provide a hyperlink to the author s site if the Material is used in electronic media.
50 million users worldwide, and replaces an earlier version of our system [10].
The two-stage delineation of popularity and personalization allows systems like ours to trade them o  in optimizing user-facing utility functions.
The model is simple to interpret, allows us to estimate parameter uncertainty, and most importantly, easily lends itself to large-scale inference procedures.
Interaction patterns on live systems typically follow a power-law distribution, where some users or items are exponentially more active or popular than others.
We base our inference on a simple assumption, that the missing signal should have the same power-law degree distribution as the observed user-item graph.
Under this assumption, we learn latent parametric descriptions for users and items by computing statistical averages over all plausible  negative graphs .
The challenge for one-class collaborative  ltering is to treat the absent signal without incurring a prohibitive algorithmic cost.
Unlike its fully binary cousin, which observes  dislike  signals for a selection of user-item pairs, each unobserved user-item pair or edge has a possible negative explanation.
For M users and N items, this means that inference algorithms have to consider O(M N ) possible negative observations.
In problems considered by Xbox, this amounts to modelling O(1012) latent explanations.
The magnitude of real world problems therefore casts a shadow on models that treat each absent observation individually [17].
Thus far, solutions to large-scale one-class problems have been based on one of two main lines of thought.
One line formulates the problem as an objective function over all observed and missing data, in which the contribution by the  missing data  drops out in the optimization scheme [7].
It relies on the careful assignment of con dence weights to all edges, but there is no methodical procedure for choosing these con dence weights except an expensive exhaustive search via cross-validation.
If a parametric de nition of con dence weights is given, a low rank approximation of the weighting scheme can also be included in an objective function [18].
The work presented here di ers from these approaches by formulating a probabilistic model rather than an optimization problem, and quanti es our uncertainty about the parameters and predictions.
A second approach is to randomly synthesize negative examples.
Our work falls in this camp, for which there already exists a small body of work.
The foremost of these is arguably Bayesian Personalized Ranking (BPR), which converts the one-class problem into a ranking problem [21].
In it, it is assumed that the user likes everything that she has seen more than the items that she hasn t seen.
This 999assumption implies a constrained ordering of many unob- served variables, one arising from each item.
This user-wise ranking of items facilitates the inference of latent features for each user and item vertex.
By design, there is no distinction between missing items in BPR; however, popularity sampling of the unobserved items was employed to give more signi cance to popular missing items [5].
This approach was e ectively utilized by many of the leading solutions in the KDD-Cup 11 competition [4].
An alternative, more expensive approach is to construct an ensemble of solutions, each of which is learned using a di erent sample of synthesized  negative  edges [19].
We motivate our approach by discussing properties of typical bipartite real world graphs in Section 2.
A generative model for collaborative  ltering when such graphs are observed is given in Section 3.
A component of the model is the hidden graph of edges items that a user considered, but didn t like.
Section 4 addresses the hidden graph as a random graph.
Section 5 combines variational inference and stochastic gradient descent to present methods for large scale parallel inference for this probabilistic model.
In Section 6, we show state of the art results on two practical problems, a sample of movies viewed by a few million users on Xbox consoles, and a binarized version of the Net ix competition data set.
The frequency of real-world interactions typically follows some form of power-law.
In Xbox Live, we observe a bipartite graph G of M users and N items, with two (possibly vastly) di erent degree distributions for the two kinds of vertices.
Figure 1 (top) illustrates the degree distribution of a sample of M = 6.2   106 users that watched N = 1.2   104 di erent movies on their Xbox consoles, where an edge appears if a user viewed a movie.
Throughout the paper the edges in the observed graph G will be denoted with the binary variable gmn   {0, 1} for vertices m and n, with a zero value indicating the absence of an edge.
We denote the observed degree distributions as puser(d) and pitem(d).
If a user viewed on average   items, and an item was viewed on average   times, then Epuser [d] =   and Epitem [d] =  , and the constraint (1)  
 =  
 should hold [15].
In Figure 1 (top), the empirical distributions satisfy   = 7.1 and   = 3780, validating the mean constraint  /N =  /M = 0.0006.
We overlay a power law degree distribution to items pitem(d)   d  0.77.
The user distribution exhibits an marked exponential cut-o , with puser(d)   d  d/70, and shares its form with many sci-enti c collaboration networks [16].
The degree distribution of the publicly available Net ix data set is shown in Fig-In it, we have M = 4.8   105 users and ure 1 (bottom).
N = 1.8   104 items.
We took a positive edge to be present if a user rated an item with four or  ve stars.
 1.4 e Given puser(d) and pitem(d), one can sample i.i.d.
graphs with the given degree distribution.
Firstly, generate vertex degrees for each user and item at random, and calculate their sum.
If the sums are unequal, randomly choose one user and item, discard their degrees, and replace them with new degrees of the relevant distributions.
This process is repeated until the total user and item degrees are equal, after which vertex pairs are randomly joined up [15].
y c n e u q e r f y c n e u q e r f










 Xbox movies items users



 degree

 Netflix (4 and 5 stars) items users



 degree

 Figure 1: Degree distributions for two bipartite graphs between users and movies: a sample of
 the 5.6 107 four and  ve starred edges in the Net ix prize data set (bottom).
Our collaborative  ltering model rests on a basic assumption, that if an edge gmn = 1 appears G, user m liked item n. However, a user must have considered additional items that she didn t like, even though the dislike or  negative  signals are not observed.
This hidden graph with edges hmn   {0, 1} is denoted by H. We say that a user considered an item if and only if hmn = 1, and the rule gmn = 1   hmn = 1 holds; namely, a user must have considered all the items that she  liked  in G. The latent signal is necessary in order to avoid trivial solutions, where the interpretation inferred from data tells us that everyone likes everything or that every edge should be present.
It strongly depends on our prior beliefs about H, like its degree distribution or power-law characteristics.
G is observed as a subgraph of H, while the rest of the edges of the hidden graph H form the unobserved  negative  signals.
On knowing the hidden graph, we de ne a bilinear or  matrix factorization  collaborative  ltering model.
We asso-1000 ,    bu  u algorithm  v  bv hmn um gmn vn bm
 bn
 Figure 2: The graphical model for observing graph G connecting M user with N item vertices.
The prior on the hidden graph H is algorithmically determined to resemble the type of the observed graph.
ciate a latent feature um   RK with each user vertex m, and vn   RK with each item vertex n. Additionally, we add biases bm   R and bn   R to each user and item vertex.
The odds of a user liking or disliking an item under consideration (h = 1) is modelled with p(g | u, v, b, h = 1) =  (cid:4) 1  uT v +b uT v +b (cid:3)(cid:5) , (2) 1 g (cid:2) (cid:3) (cid:2) g def with the logistic or sigmoid function being  (a) = 1/(1 +  a), with a = uT v + b. Subscripts m and n are dropped e in (2) as they are clear from the context; b denotes the sum of the biases bm + bn.
The likelihood of g for any h is given by the expression p(g | a, h) = (3) As g = 1   h = 1 by construction, the last factor can be ignored in (3).
If the binary  considered  variable h is marginalized out in (3), we  nd that  (a)g(1    (a))1 g h   (1   g)1 h .
(cid:7) (cid:6) p(g = 1 | a) = p(h = 1)  (a) , p(g = 0 | a) = p(h = 1)(1    (a)) + (1   p(h = 1)) .
(4) In other words, the odds of encountering an edge in G is the product of two probabilities, separating popularity from personalization: p(h = 1), the user considering an item, and  (a), the user then liking that item.
(cid:8) def
 m=1 N (um ; 0,   The probability of G depends on the prior distributions of the vertices  hidden features.
We choose them to be Gaus- 1 sian: p(U) = u I) for the users, where = {um}M
 m=1, with similar Gaussian priors on the parameters governing the item vertices.
These are shown in the graphical model in Figure 2.
To infer the various scale parameters   , we place a conjugate Gamma hyperprior on M(cid:9) N(cid:9) m=1 n=1 (cid:10)  (cid:6) (cid:5)mn (cid:11)(cid:12) mvn + bmn)gmn    
  (uT mvn + bmn) (cid:7) N (um; 0,   u I) N (bm; 0,    1 bm )   G( u;  ,  )  1 p(G,  ) =   M(cid:9)   N(cid:9) m=1 each, for example G( u;  ,  ) =  / ( )      1  u .
e u The only prior beliefs in Figure 2 that do not take an explicit form is that of H.
It could be parameterized with a particular degree distribution, be it Poisson, exponential, or a power law with an exponential cut-o .
However, we would like this to (approximately) be in the same family as the observed data, and determine an algorithm which can generate such graphs.
Section 4 elaborates on this, including the closure of the graph family under random sampling of subnetworks.
We collectively denote our parameters by   def = {H, U, V, b,  }, def with bmn = bm+bn as shorthand notation.
The joint density of all the random variables, given the hyperprior parameters   and  , is (cid:13) hmn(1 gmn ) N (vm; 0,   v I) N (bn; 0,    1 bn )   G( v;  ,  )  1 n=1   G( bm ;  ,  )   G( bn ;  ,  )   p(H) .
(5) The sigmoid product is denoted with (cid:7)mn, and will later appear in a variational bound in (11).
Obtaining a posterior approximation to (5) would follow known literature [20, 25], were it not for the unknown occurrence of edges hmn in H.
Sections 4 and 5 are devoted to treating H.
One might also consider placing Normal-Wishart hyper-prior on the means and variances of um and vn [20, 23].
In practice, we bene t from additionally using meta-data features in the hyperpriors.
They allow us to learn how shared features connect the prior distributions of various items, but is beyond the scope of this paper.
It is analytically intractable to compute the Bayesian averages necessary for marginalization in (5).
This hurdle is commonly addressed in one of two ways: Samples from the posterior can be drawn by simulating a Markov chain with the posterior as its stationary distribution, and these samples used for prediction [14].
Alternatively, one might substitute the integration problems required for Bayesian marginalization with an optimization problem, that of  nding the best deterministic approximation to the posterior density [9].
We approximate the posterior from (5), rather than sample from it, as it allows a compact representation to be serialized to disk.
The posterior from (5) is approximated with the fully factorized distribution q, p( |G)   q( ) q(umk)   N(cid:9) M(cid:9) K(cid:9) K(cid:9) q(bm) q(vnk) def = q(bn)   q( u) q( v) q( bu ) q( bv ) q(H) .
m=1 n=1 k=1 k=1 (6) The factors approximating each of the vertex features in U, V, and b are chosen to be a Gaussian, for example
  1 mk).
Similarly, the    s are approximated by Gamma factors in the conjugate exponential family, for example q( u) = G( u;  u,  u).
The remaining the question is, what to do with p(H), and the posterior marginal approximation q(H)?
Although an observation gmn = 1 implies that q(hmn =
 there are typically O(1012) or more of them.
As a recourse, we shall specify q as an algorithm that stochastically generates connections hmn = 1, so that p(H) produces (roughly) the same type of graphs as is observed in G.
The graphical model in Figure 2 speci es that every  considered  edge (m, n) in H contains a  like  probability  mn.
For each edge in H, a coin is  ipped, and revealed with probability  mn to give G.
If we assume that the coin is on average unbiased, half the edges will be revealed, and |H|   2|G|.
Alternatively, G is a subnet of H, containing half (or some rate of) its connections.
Working back, we sample graphs H at this rate, and the family of graphs H that can be generated this way constitutes our prior.
This places no guarantee that the two graphs will always be of the same type, as not all graph types are closed under random sampling.
For example, random subnets drawn from exact scale-free networks are not themselves scale-free [26].
However, the practical bene ts of this algorithmic simpli cation outweigh the cost of more exact procedures.
The factor q(H) is de ned stochastically, with the criteria that it should not be too expensive to draw random samples H. One approach would be to generate samples, similar to Section 2, by specifying a degree distribution conditioned on the number of degrees d that each user and item vertex in G has.
If the mean of each is 2d, one can show that a version of (1) will also hold for H. At the cost of many redraws, one can sample half-edges, as in Section 2, and connect them until all half-edges are paired.
We propose a simpler scheme here, which samples H from G in O(|G| log N ) time.
The scheme has the  avour of  sampling by popularity  [5].
We de ne a multinomial histogram M( ) on the N items, where  n   0 for n = 1, .
.
.
, N .
This mimics a pseudo degree distribution for missing degrees.
Let user m have degree dm, or have viewed dm items.
For user m, the subset of dm edges in H that corresponds to gmn = 1 is marked.
We then sample dm random  negative  edges from M( ) without replacement this  lls in the remaining values for row m in H, i.e. hmn for n = 1, .
.
.
, N .
For user m the sample without replacement can be drawn in O(log N ) time by doing bookkeeping with a weighed binary tree on the items.
There are many ways to de ne histogram  , one of which is to simply let  n = dn, the number of degrees (or views) of item n. This is e ectively a uniform prior: each item should have the same rate of negatives.
If we believe that there is some quality bar that drives popular items to be more generally liked, the histogram can be adjusted with  n = d   n (7) so that it obeys a version of the observed power law.
A free rate parameter r is introduced, so that the most popular



 o i t a r e v i t a g e n / e v i t i s o p

 Xbox movies
 Item degree (number of users per item)


 r = 1 r = 0.5
 Figure 3: The ratio of positive to negative edges per item, from a single sample from q(H).
(The ratio is skewed at the head: sampled edges to more popular items have higher odds to already exist in G. Discarding and resampling them leaves popular items underrepresented in the  negative  set.
This can be overcome with another adjustment of   in M( ).)
item with degree dmax = max{dn} has histogram weight  max = rdmax .
(8) As an example, r = 1 edges to H for that item.
A substitution gives a power 2 will add half as many unobserved   = 1   log dmax/ log r (9) with which the histogram is adjusted in (7).
Figure 3 shows two samples of the edges of H for two settings of r. For each item, it shows the ratio of  positive  to  negative  edges.
A side e ect is that at the head, the most popular items are underrepresented in the remainder of H. This is because the items (or edges) sampled from M( ) might already exist in G, and are discarded and another edge sampled.
(cid:14) q( ) log p(G,  ) d  + H[q( )] .
The approximation q( ) in (6) is found by maximizing a variational lower bound on the partition function of (5), with log p(G)   L[q] = (10) Here H[q] is the (continuous) entropy of our choice of q.
The expression in (10) is not analytically tractable due to the sigmoids in (cid:7)mn, which appear in p(G,  ) in (5), as they are not conjugate with respect to the q(umk) s or any of the other vertex factors.
We additionally lower-bound (cid:7)mn with the logistic or Jaakkola-Jordan bound [8], introducing an additional variational parameter  mn on each edge.
The logistic bound is (cid:7)   eg(uT v+b)
 g+h(1 g)  ( ) e 2  (cid:4) (cid:5)
 )   1 (11) where subscripts m and n that are clear from the context are suppressed.
The bound depends on a deterministic function , 1002def = 1 2  [ ( )   1  ( )
 in (11) to (cid:7)mn creates a p (G,  ) that leaves the bounded likelihood conjugate with respect to its prior.
The bound L , L[q]   L [q] = q( ) log p (G,  ) d  + H[q] , (12) (cid:14) is therefore explicitly maximized over both the (variational) distribution q and the additional variational parameters   = { mn}.
The variational updates for the user factors q(umk) are presented in this section.
As the model is bilinear, the gradients of L  with respect to the item factors can be set to zero following a similar pattern.
To minimize L  with respect to q(umk), one might take functional derivatives  L / q(umk) with respect to each q(umk), and sequentially equate them to zero.
This is slow, as each update will require a loop for the user, K loops over all over all the vertex s edges: the items will be required.
The vertex factor can alternatively be updated in bulk, by  rst equating the gradients of L  with respect to a full Gaussian (not factorized) approximation  q(um) to zero.
The fully factorized q(umk) can then be recovered from the intermediate approximation  q(um) as those that minimize the Kullback-Leibler divergence DKL( means of q(umk) match that of  q(um), while their precisions match the diagonal precision of  q(um).
(cid:8) k=1 q(umk)(cid:7) q(um)): this is achieved when the
 How do we  nd  q(um)?
The functional derivative  L /   q(um) is zero where  q(um) has as natural parameters a precision matrix of N(cid:15) (cid:6) (cid:7)   2 ( mn)   Eq (cid:6) (cid:7) vnvT n + Eq[ u]I (13) Pm = Eq hmn n=1 and mean-times-precision vector  mPm, which will be stated in (15).
Apart from having to average hmn over q(H), which we cannot do analytically, the update in (13) su ers from having a summation over all N item vertices.
The burden of having to determine a sum over a full item catalogue in (13) can be removed with a clever rearrangement of expectations.
As hmn is binary, N(cid:15) (cid:6) (cid:7) Eq hmn f (vn) = n=1 q(H) hmn f (vn) (cid:15) (cid:15)
 N(cid:15) (cid:15) n=1 = q(H) f (vn) .
(14)
 n:hmn=1 The sum over H in (14) runs over all 2M N possible instan-tiations of H. A rearrangement of (13) therefore allows the updates to appear as a stochastic average, (cid:16) (cid:15) (cid:16) (cid:15) n:hmn=1 (cid:17) (cid:6) (cid:7) vnvT n + Eq[ u]I
 (cid:2) gmn   1 (cid:6)
     (cid:7)(cid:3) (cid:6) (cid:7)(cid:17) n:hmn=1       2 ( mn)   Eq bm + bn Eq vn .
(15) Pm = Eq(H)  mPm = Eq(H) Inside the expectation over q(H), the mean  eld update in (15) is a quantity speci ed on the hidden graph H only, and not all N plausible edges for the user.
We are able to sample graphs from q(H) according to Section 4.
Retrospectively, this choice now bears fruit, as the update exists as an average amenable to stochastic gradient descent.
We remark, too, that the natural parameters in (15) de ne the natural gradients of the variational objective function [1, 24].
The full natural gradient is periodic in the number of vertices and the updates are component-wise, and convergence with such updates can also be achieved using a stochastic gradient algorithm [12].
There are additional variational parameters at play in (15).
For the required edges hmn = 1 that connect user m with items n, the values  mn that maximize L  or Eq[log p (G,  )] are each given by (cid:6)  2 mn = Eq (uT mvn + bm + bn)2 , (16) (cid:7) (cid:7) and they are computed and discarded when needed.
We take the positive root as  mn, and refer the reader to Bishop [2] for a deeper discussion.
Given Pm and  mPm from (15), we have su cient statistics for  q(um), and hence for updating each of the K q(umk) s in bulk.
Deriving su cient statistics for q(vnk), q(bm) and q(bn) is similar to that presented in (15), and the derivation will not be repeated.
Given these, optimization proceeds as follows: At time t, we sample a hidden graph H, over which the user and item vertex factors are updated.
Fo-cussing on user m, let P(t 1) be the (diagonal) precision
 k=1 q(umk).
We then matrix of the factorized distribution  nd Pm in (15), and now the precision matrix of  q(um) m = tPm + (1   t)P(t 1) will be P(t) m , where t   [0, 1].
The factors q(umk) are then recovered from (cid:18)  the bulk computation of  q(um).
The mean-times-precision (cid:18)  vector of  q(um) is given through a similar stochastic update.
The series {t}  t=1 t =   and t <  , guarding against premature convergence and m , found through P(t) t=1 should satisfy (cid:8) m t=1 2 in nite oscillation around the minimum [22].
Finally, the marginal approximations for the hyperparam-eters are updated by setting the functional derivatives, say  L / q( u), to zero.
For instance for q( u) = G( u;  u,  u) the shape  u and rate  u are  u =   + KM/2  u =   +

 (cid:15)M m=1 (cid:6) Eq uT mum .
(17) As q(umk) is dependent on H, the rate is also stochastically updated as described above.
The use of a bipartite graph ensures that variational updates are parallelizable.
For instance, by keeping all q(vnk), q(bn) and q(bm)  xed for the item and user vertices, the gradients  L /   q(um), and hence the stochastic updates resulting from (15), have no mutual dependence.
Consequently, the loop over user vertex updates m = 1 .
.
.
M is embarrassingly parallel; the same is true for other updates.
This will not hold for more general graphs like those of social networks, though, where more involved logic will be required.
Due to the fact that a variational lower bound is optimized for, optimization can also be distributed across multiple machines, as long as the bound holds.
For example, one might distribute the graph according to item vertices in blocks Bb, and iteratively optimize one block at a time, or optimize 1003blocks concurrently (with embarrassingly parallel optimiza- tion inside the blocks, as discussed earlier).
In this example the sparse user-item graph (matrix) G is distributed such that all observations for a set Ba of items are co-located on the same machine.
The natural gradients for the users then distribute across machines, and can be written so that the dependence on the data blocks on various machines separates.
When optimizing using the item-wise data block Ba on one machine, we write Pm in (15) as     (cid:15) (cid:15) n Ba n:hmn=1
 (cid:11)(cid:12) (cid:6)
 vnvT n (cid:6) vnvT n (cid:7)       + Eq[ u]I .
(cid:13) (cid:7) Pm = Eq(H) (cid:10) (cid:15) b(cid:4)=a + n:hmn=1 n Bb block b s natural gradient X(b) m ;  xed (18) Update (18) de nes a thin message interface between various machines, where each block has to communicate only its natural gradients X(b) m  and similar mean-times-precision gradients to other blocks.1 In block Ba we might iterate between updates (18) and full item updates for all n   Ba, whilst keeping the incoming messages X(b) m from other machines  xed.
After a few loops over users and items, one can move to the next block.
Similarly, di erent machines can optimize on all the blocks {Bb} in parallel, as long as the natural gradient messages are periodically communicated to other machines.
The scheme presented here generalizes to a further subdivision of user vertices into blocks.
Given G, a pivotal task of collaborative  ltering is that of accurately predicting the future presence of an edge.
This allows online systems to personalize towards a user s taste by recommending items that the user might like.
The collaborative  ltering model in Section 3 explicitly separated the probability of a user considering an item from  , the probability for the user liking the item.
The odds of liking an item depends on our inferred certainty of the user and item parameters,2 (cid:14) p(g = 1 | h = 1)      (uT v + b) q(u) q(v) q(b) du dv db  (a)N (a ;  a,  2 a) da     (cid:14)(cid:14)(cid:14) (cid:26)(cid:27) .
(19) 1 +  2 a/8 (cid:25) (cid:28)  a = uT v + b, with The random variable a was de ned as a its density approximated with its  rst two moments under = Eq[(uT v + b   a)2].
The q, i.e.  a  nal approximation of a logistic Gaussian integral follows from MacKay [13].
= Eq[uT v + b] and  2 a def def def
 size of M and N ; for N (cid:9) M a user-wise division gives a smaller message interface, as only natural gradients for the items  updates will be required.
for the diagonal Gaussian (cid:8)
 k=1 q(umk).
r o r r e n o i t a c i f i s s a c l







 Classification error on like probability Xbox movies Netflix (4 and 5 stars)


 User degree (number of items per user)
 Figure 4: The classi cation error on Gtest, given h = 1 (the ground truth is g = 1).
The full histograms of probabilities p(g = 1|h = 1) are presented in Figure

 We evaluated our model by removing a test set from the Xbox movies and Net ix (4 and 5 stars) data sets.
The degree distributions for these data sets are presented in Figure
 ing one edge (or item) for each user from G; the removed edges formed the test set.
A core challenge of real world collaborative  ltering algorithms is to  nd a balance between popular recommendations and personalized content in a structured form.
Based on our experience, a criteria of a good recommender is the ability to suggest nontrivial items that the user will like, and surface less popular items in the tail of the item cat-alogue.
In the evaluations we highlight this by grouping results according to item popularity in Figure 6, for example.
Two evaluations are discussed below.
Firstly, given that an item is presented to a user with hmn = 1, we are interested in the classifying gmn   {0, 1}.
This is one of the key contributions that our model brings to the table.
As far as we are aware, there are no other algorithms that isolate p(like) in this way.
To be able to draw a comparison with a known state-of-the-art algorithm, we consider various forms of a rank-based metric in a second evaluation.
In the tests below, K = 20 latent dimensions were used.
The user biases were clamped at zero, as q(H) was de ned to give balanced samples for each user.
The rate and shape parameters of the hyperprior were set to   =   = 0.01, giving a hyperprior on the    s with mean 1 and a  exible variance of 100.
The means of the hyperparameter posterior estimates were E[ bv ] = 0.4, E[ v] = 3.5, and E[ u] = 2.0.
When rounded to one decimal place, these values were similar on both the Net ix (4 and 5 stars) dataset and the Xbox Movies dataset.
The classi cation error on the held-out data converges to a stable value as users view between ten and twenty items.
Its plot is presented in Figure 4, and has a natural interpre-
d user d user = 1 = 40





 y t i s n e d l a c i r i p m e




 p(g = 1 | h = 1) .
Netflix (4 and 5 stars)
 d user d user = 1 = 40







 y t i s n e d l a c i r i p m e




 p(g = 1 | h = 1)

 Figure 5: The distribution of p(gmn = 1|hmn = 1) on the held out items in the evaluation, sliced incrementally according to users connected to duser = 1 to
 tation.
Conventional wisdom dictates that the error rates for explicit ratings-based recommendation systems are typically in the order of 20% of the ratings range.
For Net ix s  ve-star ratings datasets, this error is around one star [11], while an error of around 20 points in the 0-100 scale of the Yahoo!
Music dataset is usual [3].
The 16-19% classi cation error in Figure 4 is therefore in line with the signal to noise ratio in well known explicit ratings datasets.
When users viewed only one item, the bulk of the predictive probability mass p(g = 1|h = 1) is centered around 50%, slightly skewed to being less certain.
This is illustrated in Figure 5.
As users view more items, the bulk of the predictive probability skews towards being more certain3.
The probability p(g = 1|h = 1) is useful in presenting a user with interesting recommendations, as it is agnostic
 that it produces an exact callibration plot.
For example, we expect 10% of edges to be misclassi ed for the slice of edges that are predicted with p(g = 1|h = 1) = 10%.
The callibration plot requires a ground truth negative class g = 0, which is latent in our case.
Figures 4 and 5 aim to present an equivalent to a callibration plot.
to each item s popularity.
It is therefore possible to de- ne a utility function that trades this quantity o  with an item s popularity, e ectively giving a knob to emphasize exploration or exploitation.
Such a utility can be optimized through A/B tests in a  ighting framework, but is beyond the scope of this paper.
We turn to a ranking task to draw a comparison against known work, as we are unaware of other algorithms that isolate p(like).
On seeing Gtrain, the absent edges (where gmn = 0) are ranked for each user m. The ranking is based on various scores smn: like the odds of a user liking an item, namely smn = p(gmn = 1| hmn = 1) as approximated in (19); popularity smn =  n; popularity like the odds of a user considering and liking an item, namely smn =  n p(gmn = 1| hmn = 1).
(cid:5) Our metric is computed as follows: If item n We evaluated models for the two settings of r in (9); a sample from H for each was shown in Figure 3.
(cid:5) the rank score counts the position of n diction list was removed, in an ordered pre-(cid:5)(cid:29) (cid:15) (cid:15) (cid:4) (cid:5) Srank(m, n ) def =
 smn(cid:2) > smn
 n:gmn=0 n:gmn=0 Random guessing would give S = 0.5, while S = 1 places the held-out item at the head of the list.
As a benchmark, we use the Bayesian Personalized Ranking (BPR) model of Rendle et al. [6, 21].
It has shown state of the art performance on ranking metrics against methods ranging from singular value decompositions and nearest neighbours to weighed regularized matrix factorization [19].
BPR was also used as a key component in many of the leading solutions for the second track of the KDD-Cup 11 competition [4].
The competition was designed to capture the ability of models to personalize recommendations that  t  speci c users regardless of an item s popularity.
In that setting, BPR was trained with missing items sampled with probabilities proportional to their popularity as described in [5].
We therefore implemented and trained two BPR models: BPR-uniform with missing items sampled uniformly; BPR-popularity with missing items sampled proportional to their popularity.
These two models capture two di erent aspects of recom-mender systems.
BPR-uniform is optimized to learn a user-wise ranking of items, where the objective function speci es that items that are liked (i.e. gmn = 1) should be ranked above missing items (i.e. gmn = 0).
The metric in (20) follows [21].
Because BPR-uniform directly optimizes this metric, it should come as no surprise that it will perform better than methods that do not optimize it directly (see Figure 6).
However, meaningful insights can still be gleaned from the comparison.
BPR-popularity is aimed at ranking observed  liked  items above other popular items that are missing from the user s history.
While two BPR models are required to capture these two di erent aspects of recommendations, our generative model captures both of these aspects in a structured manner.
k n a r e g a r e v a






 k n a r e g a r e v a

 Xbox movies popularity like (r = 1) like (r = 0.5) pop * like (r = 1) pop * like (r = 0.5) BPR uniform BPR pop

 User degree (number of items per user) Netflix (4 and 5 stars)
 .
popularity like (r = 1) like (r = 0.5) pop * like (r = 1) pop * like (r = 0.5) BPR uniform BPR pop
 User degree (number of items per user)






 k n a r e g a r e v a






 k n a r e g a r e v a

 Xbox movies popularity like (r = 1) like (r = 0.5) pop * like (r = 1) pop * like (r = 0.5) BPR uniform BPR pop




 Item degree (number of users per item) Netflix (4 and 5 stars) popularity like (r = 1) like (r = 0.5) pop * like (r = 1) pop * like (r = 0.5) BPR uniform BPR pop



 Item degree (number of users per item) Figure 6: The rank Srank(m, n) in (20), averaged over users (left) and items (right), grouped logarithmically by their degrees.
The top evaluation is on the Xbox movies sample, while the bottom evaluations are on the Net ix set, as given in Figure 1.
Figure 6 illustrates the mean rank scores, grouped log-arithmically by user and item degrees.
In the plots that are grouped by user degrees, we see improved results for algorithms that prefer popularity, i.e. popularity like and BPR-uniform.
This is explained by the dominance of popularity biases in both datasets.
As expected, BPR-uniform show best results as it is optimizes the task at hand directly.
The estimates for users with an order of 103 to 104 degrees are noisy as the data is very sparse (see Figure 1).
However, when looking at the per item breakdown, we learn that BPR-uniform and the popularity like models perform poorly on less popular items and their superior results are based on recommendations from the short head of the popular items.
When it comes to recommending from the long tail of the less familiar items, the like models show best results, with BPR-popularity just behind.
These trends are consistent on both datasets.
The distribution of the ranks over all users (and items) is heavy-tailed, and whilst the average is often reported, the median is much higher than the average reported in Figure
 the rank scores for tests like and popularity like for r = 1
 The rank variance decreases as users view a few movies, but increases for heavy users which are harder to model.
When popularity is included in the ranking, the error bars get tighter for heavy users, which implies that these users  lists are mostly governed by popularity patterns.
Random graphs can be leveraged to predict the presence of edges in a collaborative  ltering model.
In this paper we showed how to incorporate such graphs in an inference procedure by rewriting a variational Bayes algorithm in terms of random graph samples.
As a result, we were able to explicitly extract a  like  probability that is largely agnostic to the popularity of items.
The use of a bipartite graph, central to this exposition, is not a hindrance, as user-user interactions in a general network can be similarity modelled with mum(cid:2) ).
While scalable parallel inference is not imme- (uT diately obvious, we believe this to be a worthwhile pursuit.
By employing the same machinery on general graphs, one should be able to model connections in social or other similar networks.
The use of a Bayesian graphical model makes it easy to adapt the model to incorporate richer feedback signals.
Similarly, both structured and unstructured meta-data can be




 k n a r






 k n a r

 Xbox movies: like (r = 0.5) 90th percentile 80th " 70th " 60th " 50th " 40th " 30th " 20th " 10th "

 User degree (number of items per user) Xbox movies: pop * like (r = 0.5) .
90th percentile 80th " 70th " 60th " 50th " 40th " 30th " 20th " 10th "

 User degree (number of items per user)

 Figure 7: Error bars on the rank tests.
The median is much higher than the average rank reported in Figure 6.
plugged into the graphical model.
The hidden graph H may also be partly observed, for example from system logs.
In that case some true negatives exist.
Alternatively, we may know a priori when a user could never have considered an item,  xing some h at zero.
In both these scenarios the process of drawing random hidden graphs H can be adjusted accordingly.
For the sake of clarity, none of these enhancements were included in this paper.
The authors are indebted to Nir Nice, Shahar Keren, and Shimon Shlevich for their invaluable input, management, and stellar engineering skills.
