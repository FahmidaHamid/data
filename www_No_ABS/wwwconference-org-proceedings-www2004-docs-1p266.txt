In 1999, van Harmelen and Fensel [16], argued: No matter how nice any Knowledge Representation language is as proposed by the AI community, [.
.
. ]
the order of precedence is the other way round: how well can AI concepts be  tted into the markup languages that are widely supported on the Web, [.
.
.
our emphasis] This question is answered with OWL, the Web Ontology Language.
The underlying AI concept is Description Logic [1], the Web markup language is RDF [12].
This paper discusses how well the triple oriented RDF abstract syntax can encode the more conventional tree structured syntax for description logics.
The OWL Semantics and Abstract Syntax Recommendation [14] normatively de nes OWL.
Two different semantics are given, one for the RDF triples, the other for the OWL abstract syntax trees (corresponding to a mainstream description logic syntax such as in [1]).
Mapping rules are speci ed linking the trees with the triples.
Copyright is held by the author/owner(s).
For this to work, for OWL to make sense, it has to be possible to switch between the triples and the trees.
A minimal task is that of being a syntax checker [7]   which involves classifying the tree, given the triples.
During the development of OWL, some doubt was expressed as to whether this was possible.
An implementor reading the documentation gets a shock when they realize they have to run the nondeterministic mapping backwards.
We describe two different implementations, one based around the trees, the other on the triples.
The Web Ontology Language (OWL) [9] de nes three classes of documents: Lite, DL and Full.
All RDF/XML documents are OWL Full documents.
Some OWL Full documents are also OWL DL documents, and some OWL DL documents are also OWL Lite documents.
The characterisation of OWL DL and OWL Lite is essentially syntactic in nature.
That is, the relevant rules de ne structural manipulation, rather than the semantic rules that give interpretation of structures.
The  rst structural rules are those de ned by RDF/XML syntax [3], which gives a set of rules for converting an RDF/XML document into an RDF graph [12].
This paper is concerned with the further rules, found in the OWL Semantics and Abstract Syntax [14] (S&AS), which then charac-terise the RDF graphs that are in OWL DL and OWL Lite.
Syntax checking can be seen to have a number of uses.
Imple-mentors may choose to target a particular OWL sublanguage.
For example, OWL DL has been chosen to yield a language for which inference is decidable.
An application targeting OWL DL will need to know whether ontologies are amenable to inference using the choice of DL semantics and reasoning techniques.
At a more prosaic level, anecdotal experience suggests that many ontologies are OWL Full not through explicit choice, but rather through errors (see Section 2.4)   for example missing type triples may point to typographical errors in the ontology source.
A syntax checker can prove useful in  nding such errors.
A parser takes an input document and returns an abstract syntax tree (which can then be classi ed).
A recognizer (or species recognizer) takes an input document and indicates if it belongs to OWL DL or OWL Lite.
A parser can easily be transformed into a recognizer, but not vice versa.
An OWL Syntax Checker, as de ned in the OWL Test Cases Recommendation [7], is a recognizer for OWL DL and OWL Lite.
This paper presents two different systems re ecting two different approaches to OWL syntax.
The more conventional, Wonder-Web parser, constructs an abstract syntax tree and checks its well-formedness.
This approach is also used by the other OWL Syntax checkers that reported during the OWL Candidate Recommendation, such as OWLP and Pellet.1 The other, the Jena checker, is a recognizer, which is strongly triple oriented, depending on a pre-transformation of the grammar and mapping rules to be a triple-centric grammar with no reference to abstract syntax trees.
A document is in OWL DL, if it is an RDF/XML document for which the corresponding graph conforms to the rules for OWL DL.
The rules for OWL DL are de ned constructively in S&AS.
An abstract syntax is de ned, that describes a set of parse trees.
Each of these parse trees can then be converted into one or more RDF graphs using nondeterministic mapping rules.
This is shown in table 1.
An OWL syntax checker, has to, at least implicitly, do this process backwards   i.e. take an RDF graph, invert the mapping rules, and hence  nd a corresponding abstract syntax tree.
If there is one, then the document is in OWL DL, otherwise it is in OWL Full.
If more than one graph corresponds to a parse tree, then these graphs have the same semantic interpretation.
Moreover, more than one parse tree may correspond to the same RDF graph, in which case the two trees have the same semantic interpretation.
The abstract syntax rules are described in section 2 of S&AS [14].
These are fairly conventional looking BNF [10] rules: hontologyi ::=  Ontology(  [ hontologyIDi ] { hdirectivei }  )  haxiomi ::=  Class(  hclassIDi hmodalityi .
.
.
 )  |  DatatypeProperty(  hdatavaluedPropertyIDi .
.
.
 )  hindividuali ::=  Individual(  [ hindividualIDi ] .
.
.
{  type(  htypei  )  } { hvaluei }  )  The principle novelty is that these rules describe abstract syntax trees, and not a document.
There is no intent that the terminal leaves of the tree be read off to form a text string.
Thus the abstract syntax is a set of trees, de ned by a BNF.
The trees de ned by these rules are not quite the parse trees according to the BNF, but structural trees de ned by the  (  and  )  in the terminals in the rules.
A simple rule like: hfacti is not made explicit in any corresponding abstract syntax tree.
::= hindividuali The mapping rules are described in section 4 of S&AS [14].
A typical mapping rule looks like: Individual( value(pID1v1) .
.
.value(pIDkvk) ) :x  :x T (pID1) T (v1).
:x T (pIDk) T (vk).
.
.
.
This shows that an abstract syntax tree matching the left hand side, can be transformed into triples as given on the right hand side.
The
 functor T ( ) is used to show recursive application of the mapping rules.
A node is returned from such a recursive application that is used within the triples on the right hand side of the rule.
We show the node to be returned (a  main node  in the terminology of S&AS), as a superscript above the arrow of the rule.
The mappings of the substructures are shown on the right hand side in the same order as the abstract syntax tree on the left.
This is important when there are many optional or repeated elements.
Some OWL DL documents are also in OWL Lite.
They are those for which there is an abstract syntax tree which uses only the grammar rules from the OWL Lite section of S&AS.
As discussed in the OWL Overview [13], OWL Lite and OWL DL can be partially differentiated by the vocabulary used, for example owl:unionOf does not occur in OWL Lite.
However, this approximation is wholly inadequate for writing a species recognizer.
There are a number of situations when different constructions in the abstract syntax could yield the same triples.
For example, axioms [A] and [B] in Table 2 both yield the same RDF triple shown in the table, where T(restriction( p cardinality(0) ) is the bn-ode created to represent the restriction.
Note however, that in this case, [A] may be part of an OWL Lite ontology while [B] may not as it involves a disallowed expression in an axiom.
This particular example illustrates that species recognition between DL and Lite is not simply a case of checking vocabulary   we must also examine how the vocabulary has been used.
An ontology is in OWL Lite if there is some abstract tree  tting the OWL Lite conditions that yields the given triples under the mapping rules.
There are, in general, two ways in which an RDF graph may fail to correspond to an OWL Lite or DL ontology.
  There is an OWL Lite or DL ontology in abstract syntax form which maps to a superset of the given triples but some of the triples have been forgotten and are not in the graph.
  The ontologies in abstract syntax form that map to the triples or any superset of the triples violate some of the restrictions for membership of the OWL Lite or DL subspecies.
(This includes the case where there are no such ontologies).
We might (loosely) describe the  rst as external errors, and the second as internal errors.
Examples of external errors include:   Using a URI reference in a particular context (e.g.
as the subject of an rdfs:subClassOf triple) without including an appropriate explicit type triple;   Malformed syntactic constructs, e.g.
a node typed as an owl:Restriction that is not the subject of an owl:onProp-erty triple;   Using the wrong vocabulary, e.g.
rdf:Property instead of the more speci c owl:ObjectProperty or owl:DatatypeProp-erty; Examples of internal errors include:   Violation of the rules concerning separation of classes, individuals and properties (in DL and Lite we require that these interpretations are disjoint);   The use of expressiveness outside the scope of the species   for example using an owl:unionOf in an OWL Lite docu-ment;
 Class( eg:cl ) DataProperty( eg:p ) Individual( type( eg:cl ) value( eg:p, "bar" ) ) ) :o rdf:type owl:Ontology .
  eg:cl rdf:type owl:Class .
eg:p rdf:type owl:DatatypeProperty .
  :i rdf:type eg:cl .
:i eg:p "bar" .
<rdf:RDF xmlns:owl="http://www.w3.org/2002/07/owl#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" xml:base="http://www.example.org/eg" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"> <owl:Ontology/> <owl:Class rdf:ID="cl"/> <owl:DatatypeProperty rdf:ID="p"/> <eg:cl> <eg:p>bar</eg:p> </eg:cl> </rdf:RDF> OWL Abstract Syntax [14] RDF Graph [12] RDF/XML Syntax [3] Mapping Rules Table 1: Overview of De nition of OWL Syntax

 Class ( a complete restriction( p cardinality(0) ) ) EquivalentClasses ( a restriction( p cardinality(0) ) ) Abstract Syntax RDF triples a owl:equivalentClass T(restriction( p cardinality(0) ) ) Table 2: Axioms and OWL Lite   Rede ning the reserved vocabulary (e.g.
those things in the   A directed cycle of blank nodes is usually an internal error OWL, RDF and RDF(S) namespaces).
(see section 2.6.2) Errors concerning structure sharing, the use of blank nodes, may fall into either category depending on the exact error, (see section 2.6.2).
It may often be the case that  missing  triples are simply due to an omission, rather than a desire to use expressiveness outside the scope of OWL DL.
For example a URI may only be used in a  class  context, but without an explicit triple.
In this case, the document will be OWL Full, but a  x  may be applied to the document (effectively adding the missing triple).
We must be careful when applying such  xes that the original intention of the document is not altered, but such a facility is likely to prove useful.
We return to this issue in Section 9.
A number of dif cult issues in parsing and recognition relate to imports.
Species validation must be done on the imports closure of the ontology   this effectively involves retrieving URIs that are the object of an owl:imports triple and adding any triples from an RDF graph found there to the current RDF graph.
Validation cannot be performed locally, i.e. without  rst calculating the imports closure2, as it may be the case that required type triples are actually present in the imports.
For example in test imports-0053, the required triple typing the imported URI as an ontology (required in OWL DL) is actually contained in the imported ontology.
Similar situations can arise with classes and properties.
It can also be that case that an ontology imports an RDF graph from a URI where the imported graph is in OWL Full, but the importing ontology still remains in OWL Lite.
This would be the case where, for example, the imported ontology asserts a rdf:type b without explicitly typing b as a class.
We return to this issue in Section 8.
A number of conditions regarding valid OWL syntax relate to blank nodes (or bnodes) in the RDF graph.
A blank node is a node
 3http://www.w3.org/2002/03owlt/imports/ Manifest005.rdf that is not a URI reference or a literal   it is a unique node that can be used in one or more RDF statements, and has no globally distinguishing identity.
A key point is that bnodes cannot be referred to from outside the document that we are processing.
Bnodes generated by the mapping correspond to:   Anonymous classes, e.g.
arbitrary class expressions such as   Restrictions, e.g.
existential quanti cations over properties.
  Anonymous individuals, e.g.
John s brother.
The mapping rules [14] state: Bnode identi ers here must be taken as local to each transformation, i.e., different identi ers should be used for each invocation of a transformation rule.
intersections, unions and enumerations.
Thus whenever an expression such as intersectionOf (Person Male) is used in an ontology, the mapping creates a new bnode corresponding to that expression.
In general, no sharing of bnodes is permitted   each bnode can participate as the object of at most one triple.
There are, however, two cases where a blank node corresponding to an expression can be used in more than one place   when the translation results from an EquivalentClasses or DisjointClasses axiom.
When anonymous class expressions are used in EquivalentClasses or DisjointClasses axioms, the mapping rules permit the reuse of the resulting bnodes produced.
However, this reuse is only allowed within the context of the triples produced by the mapping from that particular axiom.
For an equivalent classes axiom: EquivalentClasses( d1 ... dn ) the mapping requires the production of a set of owl:equivalent-Class triples that form an (undirected) connected graph over the nodes produced by translating each di.
For a disjoint classes axiom: DisjointClasses( d1 ... dn ) the mapping requires the production of a set of owl:disjointWith triples s.t.
each node produced by translating a di is related to every other dj for i 6= j as either subject or object in a owl:disjointWith triple (forming an owl:disjointWith clique).
translation from an anonymous class expression may participate in a number of triples.
They may not, however, participate in any triples that do not correspond to those generated by the mapping rule applied to the axiom.
In addition, the SubClassOf axiom, may introduce a blank node that is the subject of an rdfs:subClassOf triple, and such a blank node cannot be the object of any triple.
In summary, blank nodes must  t at most one of the following cases:
 Class triples
 triples (in which case a further check must be applied)


 Hence, a graph may have an internal error concerning a blank node which is in more than one of these categories, or is involved in two triples in cases 3, 4 or 5.
Or it may have an external error, concerning the blank nodes involved with owl:disjointWith triples which may not form a clique.
In addition blank nodes may not form directed cycles, except in cases 1 and 2.
In the following sections we provide overviews of the two systems discussed in this paper   these will be referred to as Wonder-Web and Jena.
WonderWeb4 is an EU IST FET project concerned with  Ontology Infrastructure for the Semantic Web .
As part of the work of WonderWeb, an API for OWL Ontologies has been developed, providing a collection of (Java) interfaces allowing the representation and manipulation of OWL ontologies.
A detailed description of the rationale behind the API is given in [2], but put brie y, the API insulates application developers from the vagaries of concrete syntax, and provides a higher level view of the objects (classes, properties, axioms etc) in an OWL Ontology.
The structure of the data model in the API is largely based on the OWL Abstract Syntax [14].
The WonderWeb API also aims to separate the functionality that one might require when working with OWL ontologies.
Aspects of functionality such as:   change (addition/removal)   serialization   parsing/deserialization   inference are all considered separately, allowing implementations to be clear about the services they provide.
The codebase of the API (including a species validator as described here) is available for download as an open source project5.
Of course, insulating applications from concrete syntax is all well and good, but it is clear that mechanisms for parsing and serializing from/to concrete representations are vital for real-world 4http://wonderweb.semanticweb.org 5http://sourceforge.net/projects/owlapi applications.
To this end, a parser for OWL ontologies represented in RDF/XML has been produced.
The parser takes an RDF/XML document and attempts to produce a corresponding abstract syntax tree.
The WonderWeb OWL API is particularly targeted at the OWL DL and Lite species (a research agenda of the project is the use of Description Logic reasoners with OWL).
The ability to recognize when a particular document is in a species (and is thus amenable to the appropriate reasoning techniques) is a key requirement, and the WonderWeb parser performs species recognition as part of its parsing process.
Jena6 [6] is an open source semantic web developers kit, principally developed at HP Labs.
It provides APIs for manipulating RDF graphs.
The Ontology API provided for OWL and DAML ontologies, while abstracting from the underlying RDF graph, does not attempt to totally hide or replace it.
Moreover, the OWL support is intended as OWL Full support with reasoning support for cases not included in the OWL DL syntactic subset.
The Ontology API within Jena has explicit OWL Full support handling the polymorphism that can occur between classes and individuals, datatype properties and object properties, etc.
Thus the syntax checker requirements are an add on to the ontology support rather than a prerequisite.
The typical user requirement which may be addressed is to verify that a graph is within OWL DL before saving it to be exported.
There is no requirement to produce abstract syntax trees.
In keeping with the RDF-centric philosophy of Jena, the syntax checker operates in a triple oriented fashion, and hence depends upon a different triple oriented expression of the grammar of OWL DL.
This is produced in a precompilation stage, akin to the well known compiler-compiler technique [11].
In this stage, which occurs while the system is being built, the abstract syntax rules and the mapping rules are analyzed in detail.
A set of syntactic categories for nodes in a graph and a table of triples linking these syntactic categories is produced.
This table of triples is used as the grammar at runtime.
The Jena syntax checker s principle task is to  nd a mapping from the nodes in the graph to the syntactic categories such that every triple appears in the transformed grammar.
The WonderWeb parser takes an RDF graph and attempts to build an abstract syntax tree.
The basic strategy employed is as follows.
In the course of identifying axioms, we may need to translate nodes corresponding to class expressions.
conversion of class expressions occurring as the subject of rdf:type triples.
During this process, we keep a note of those triples that have been  used , e.g.
those that are identi ed as being the result of the application of the mapping rules to a particular construct.
By doing this we can identify any triples that are  left  after we have constructed all classes, properties and the axioms concerning those 6http://jena.sourceforge.org
 individuals in the ontology.
Identifying named objects is simply a case of  nding all those (named) nodes that are the subject of an rdf:type triple where the object is owl:Class, owl:ObjectProperty or owl:DatatypeProp-erty7.
In these cases, optional triples may also be present   for example if we have a rdf:type owl:Class, the rules also allow the addition of a rdf:type rdfs:Class (even though the additional triple adds no extra semantic information).
Once those objects are identi ed, we can translate axioms concerning them.
For example, for any triples of the form: p owl:-inverse q we check that p and q are instances of owl:ObjectProp-erty and add q to the list of inverses held by p. If p and q are not instances of owl:ObjectProperty, and we are simply interested in recognizing OWL DL and OWL Lite ontologies, we can stop at this point as we now know that the RDF graph cannot correspond to a OWL DL or Lite ontology.
Other axiom types such as rdfs:subProperty are dealt with in a similar manner.
A more interesting translation task is when the axiom deals with a bnode representing a class expression as in Figure 1.
In this situation, we  rst identify the node which is the object <owl:ObjectProperty rdf:about="#p"> <rdf:domain> <owl:Class> <owl:complementOf rdf:resource="#A"/> </owl:Class> </rdf:domain> </owl:ObjectProperty> Figure 1: Complex Domain Expression of the triple.
This is then translated to a class expression by a case analysis on the triples in which the node appears as subject.
In general, there should be a single such triple, with its predicate determining the form of the expression produced.
The presence of more than one such triple indicates an OWL Full ontology.
Recursive translations may be necessary if the expression includes nested expressions.
Lists are used to represent the operands in expressions such as intersections.
Such lists are translated by collecting all the nodes that appear in the list (as the object of an rdf:first triple) and forming a new expression using the translations of those nodes.
The well-formedness of the list (each node in the list should be the subject of exactly one rdf:first and rdf:rest triple) is also checked during this process.
As discussed in Section 2.3, care needs to be taken when handling axioms concerning classes.
The strategy employed here is to translate any rdfs:subClassOf and owl:equivalentClass triples to  class de nitions  whenever possible.
For example, with a triple a rdfs:subClassOf x, where a is a named node (as opposed to a bn-ode), then we attempt to construct an object corresponding to: Class( a partial Tx ) where Tx is the translation of x to a class expression.
Similarly, if we encounter a owl:equivalentClass x then this is translated to Class( a complete Tx ) (but see later discussion on handling blank nodes).
This strategy ensures that OWL Lite ontologies are produced whenever possible.
owl:AnnotationProperty, but space precludes us from providing a detailed exposition of the parsing process here.
The use of owl:imports allows us to refer to RDF graphs held at separate locations.
Although the validation process is performed over the imports closure of the graph, it can be useful to try and process each chunk separately.
It is often the case when we have onto1 owl:imports onto2 that the statements at onto1 are intended to form a single Ontology, while those at onto2 form another8.
A formal de nition of this dif cult due to the inexpressiveness of RDF   we cannot represent the fact that particular assertions belong in a particular Ontology (see Section 8).
The parser attempts, wherever possible, to perform this  chunking  (based on the physical locations of the triples) and builds individual Ontology objects corresponding to each RDF graph retrieved from a particular URI.
This is done by recursively calling a new parse on an imported URI.
In order to do this successfully though, we need to pass information between the parsing processes, in particular recording whether URIs have been correctly typed.
This then allows us to deal with situations where a URI is used in a class context in onto1 and has the appropriate type triple in onto2 (or vice versa).
In adopting this approach to imports, we need to relax our handling of typing somewhat.
In the owl:inverse example above, the local information may not be enough to determine whether the properties are of the required types as the triple regarding the type of p may be in an imported ontology.
In this case, we make an assumption that the types are appropriate, and check at the end of the complete parsing process that any such assumptions made have been discharged (e.g.
we really encountered the appropriate typing triple).
Assumptions are also passed to any recursive parse along with type information.
If assumptions remain at the end of the parse, required type triples were missing, signifying an OWL Full ontology.
Note that the grouping of statements into separate ontologies has no effect on the semantics of importing ontology, in terms of the entailments that hold.
In addition to  agging  used  triples, the parsing process also  ags bnodes as they are translated to expressions or used in lists.
If a  agged bnode is encountered in a translation, this indicates that structure sharing has occurred, the document is OWL Full, and appropriate action can be taken.
Cases involving equivalence and disjointness axioms (see Section 2.6.1 require special handling.
In order to check whether equivalence axioms are well-formed, we do the following.
owl:equivalentClass;
 the same set iff there exists a triple a owl:equivalentClass b or b owl:equivalentClass a.
Each of these equivalence classes can now be translated.
If the size of the set is 2 and the triple that induced the set is of the form a owl:equivalentClass x where a is a named node, then we translate to a class de nition (see Section 4.1).
Otherwise we translate to an EquivalentClasses axiom.
The conditions regarding disjointness are more complicated.
The rules for DisjointClasses axioms tell us that an axiom: Disjoint-Classes( d1 d2 ...dk ) is translated to a collection of nodes, one for each expression in the equivalence, and a number of owl:dis-jointWith triples, such that every node in the collection is connected to every other node by at least one triple (in either direction).
This may lead to blank nodes being used in more than one

 strategy to owl:disjointWith triples.
owl:disjointWith;
 have not already dealt with, do the following: (a) Pick a bnode n that we haven t already dealt with.
(b) Gather together all the nodes n1, n2, .
.
.
nk that can be reached from n via a path that consists of owl:disjointWith triples, and which does not pass through a named class node   in other words the traversal stops when we reach a named node.
Include n in this collection.
(c) In order for the graph to be in OWL DL, the subgraph formed from these nodes considering owl:disjointWith edges must be fully connected: every node must have an edge to every other node.
If this is not the case, the graph is not in DL.
If it is the case, then we add a DisjointClasses axiom using translations of the nodes in the collection formed above.
The Jena recognizer uses a very different technique.
We introduce it with an example, followed by describing the key concept of node categorization, before launching into a detailed discussion.
Suppose we are given the following three triples in order: _:r owl:onProperty eg:p .
_:r owl:hasValue "a value" .
eg:p rdf:type owl:ObjectProperty .
When processing the  rst triple, we can conclude that it must have come from one of the mapping rules for restrictions, for example: restriction( ID allValuesFrom( range )) :x rdf:type owl:Restriction .
:x  :x owl:onProperty T (ID) .
:x owl:allValuesFrom T (range) .
Thus eg:p must be either a datavaluedPropertyID9 or an indivi-dualvaluedPropertyID, and :r is the node corresponding to some restriction.
When we process the second triple, we already know that :r is a restriction of some sort, and the additional triple tells us that it is a value( ) restriction.
Moreover, the literal object, tells us that this is a value restriction using the following mapping rule: restriction( ID value( value )) :x rdf:type owl:Restriction .
:x  :x owl:onProperty T (ID) .
:x owl:hasValue T (value) .
We note that for T (value) to be a literal, then value must be dataL-iteral and the following abstract syntax rule must have been used: hdataRestrictionComponenti ::=  value(  hdataLiterali  )  This rule can only  re if ID is a datavaluedPropertyID.
Thus, the second triple tells us that :r corresponds to a value restriction on a datavaluedPropertyID.
If we now return to the  rst triple, given the new information about :r we now know that eg:p is a datavaluedPropertyID.
Since the mapping rule only applies to abstract syntax constructs that come from OWL DL we know that the triples are not from an OWL Lite document.
There is nothing more that can be said about either the predicate or the object of either the  rst or second triples.
Thus neither
 this section!
triple will make any further difference to the rest of the processing, and both could be discarded in an incremental recognizer.
All that needs to be remembered is the classi cation of :r and eg:p.
When we come to the third triple, we  nd a datavaluedProper-tyID as the subject of an rdf:type triple, with an owl:ObjectProp-erty object.
The mapping rules do not produce such triples, and so this is an internal error (cf.
section 2.4).
If we processed the triples in the reverse order, we would have concluded that eg:p was an individualvaluedPropertyID, (from the third triple), and found the error while processing the  rst triple, because the grammar does not generate owl:onProperty triples linking value restrictions on datavalued properties with individualval-uedPropertyIDs.
The example depended upon an analysis of   Whether eg:p was an individualvaluedPropertyID or a data  What sort of restriction corresponded to :r valuedPropertyID.
We view this as a function from the nodes in the graph to a set of syntactic categories generated from the grammar.
Each uriref node may be a builtin uriref, with its own syntactic category (such as owl:onProperty), or a user de ned ID, such as a classID.
Each blank node is introduced by one of the mapping rules.
We hence have one or more10 syntactic categories for each mapping rule that creates a blank node.
The main goal of the algorithm is to determine which category each of the nodes is in.
To make the runtime algorithm simpler, the grammar (including the mapping rules) is preprocessed into a grammar table of triples of syntactic categories.
Two of the entries in this table, relevant to the example are: individualValuedProperty rdf:type owl:ObjectProperty .
literalValueRestriction owl:hasValue literal .
(DL) Some of the entries are annotated with actions, for example the second triple sets the not-Lite  ag.
Each step in the algorithm processes one triple.
The currently known possible categories for each of the three nodes are retrieved.
Each combination of these is tested to see if it is in the grammar table.
Such tests allows the elimination of some of the previous possible categories for each node.
If all the possible categories are eliminated, then the graph did not conform to the syntax.
The algorithm is speci ed in terms of the de nition of a function C that assigns a set of categories to each node in the graph.
blank categories.
ID categories (classID etc).
ping rules, is such that the same mapping rule is used for two different abstract syntax constructs.
The rule for the value restriction is one, which can be used for both literal values and object values.
In such cases, we clone the mapping rule and have one for each abstract syntax construct, giving rise to two syntactic categories for blank nodes.
C(n) to {liteInteger} (for use in cardinality restrictions).
ger set C(n) to {dlInteger}.
{uTypedLiteral}.
re ne is de ned as: (a) Set S = C(s), P = C(p), O = C(o), to be the set of categories currently associated with the subject, predicate and object of t respectively.
< s , p , o  >  Grammar} (b) Set S0 = {s    S| p    P, o    O with (c) If S0 is empty then fail.
(d) Set P 0 and O0 similarly (e) If S 6= S0 update C(s) := S0 and for each t0 involving (f) Similarly for P 0 and O0 (g) If every match from S0, P 0 O0 in the grammar table is s which has already been processed, re ne(t0) annotated with the same action, perform that action.
tion 2.4).
Since the values of C are strictly monotonic decreasing through the recursive steps 8e and 8f, the algorithm terminates.
The actions in step 8g and the  nal checks in step 9 are discussed in more detail below.
The compiler compiler transforms the OWL DL grammar from the form in S&AS [14] to a triple oriented form suitable for the Jena checker.
The input consists of:   A list of the names of the syntactic categories for urirefs (e.g.
  The abstract syntax (somewhat reformulated)   The mapping rules (somewhat reformulated).
classID).
The output is as follows:   A list of syntactic categories for nodes (148 categories: 45 for the keywords in OWL, such as rdf:type, 14 corresponding to the different uses of user de ned urirefs, 83 for different usages of blank nodes, 6 arti cial pseudocategories)   Various groupings of these categories (e.g.
all those cate-  A table of legal triples of syntactic categories, annotated with   A lookup functions that assign an initial set of syntactic cat-gories that are involved with owl:disjointWith).
actions and a DL  ag (2486 entries).
egories to a node The compiler compiler is written in Prolog, and the grammar and mapping rules have been written in a Prolog form.
A detailed discussion can be found in [5].
The rules concerning blank nodes corresponding to descriptions and restrictions are somewhat complicated.
There are two natural categories: descriptions being blank nodes with explicit type owl:-Class, and restriction being blank nodes with explicit type owl:Re-striction.
It is convenient to subdivide these categories into one category per mapping rule.
the relevant division in OWL DL syntax.
In particular, non-integer XSD literals are treated the same as plain literals.
We saw in section 2.6.2 that blank node usage fell into four cases: in the compiler compiler, this is expressed by converting each of the syntactic categories coming from a mapping rule for descriptions or restrictions into four subcategories, one for each of these cases.
Since there are 19 such mapping rules in the grammar used, this accounts for 76 of the 83 blank node categories.
The actions used by the grammar are: the DL actions, for triples which do not occur in Lite; an Object action when the object of this triple is a blank node which must not be the object of any other triple; and the actions FirstOfOne, FirstOfTwo and SecondOfTwo when the subject of this triple corresponds to a construct with one or two components each re ected by a triple in the graph.
This triple is the stated component (e.g.
owl:onProperty is the  rst of the two components of a restriction).
For each of these, the runtime processing remembers the triple as ful lling the speci ed role and it is an error if some other triple plays the same role.
The actions are only acted on when all remaining categorizations of the triple require it.
In particular, this ensures that the DL action is not invoked until it is not possible to match the triple with only the OWL Lite grammar.
Given the framework of category re nement, some of the other syntactic aspects of OWL can be expressed within it.
This is achieved by introducing additional syntactic categories, which are included in the initial assignment of categories to nodes.
The table of triples is extended with a further virtual table of pseudotriples using these virtual categories.
This table of pseudotriples is a short piece of code rather than actual entries in a table.
By an appropriate choice of which pseudotriples are in the virtual table, global properties can be propagated through the node categories.
The goal with the pseudocategories is that nodes with syntactic defects are marked as being in a pseudocategory.
When a triple is processed which addresses those defects then the node is no longer marked as in the pseudocategory.
The  nal stage of the algorithm searches for all marked nodes and takes appropriate action (such as rejecting the input).
As an example, most nodes in OWL Lite and OWL DL have to have an explicit type triple in OWL Lite and OWL DL.
In the Jena checker, all relevant nodes have initially category assignment including the category notype.
This pseudocategory appears in pseudotriples in all three places with arbitrary real categories in the other two places.
The key exception is when the predicate is rdf:type which typically provides the required type.
Such triples do not appear in the pseudotable.
Thus, if there is an appropriate type triple, the pseudocategory is removed from the category assignment for the node by the operation of the re nement algorithm.
These checks check external errors, i.e. where needed triples were missing, and some internal errors which were not fully covered elsewhere.
To continue the typing example, the  nal check is a simple inspection for nodes in the pseudocategory notype.
Every blank node in category such as restriction or description which require one or two structural triples, is inspected to verify that such triples have been found.
Most of the dif cult cases are handled using a combination of pseudocategories, pseudotriples and  nal checks.
This is particularly suited to the external errors, which cannot be detected by the re nement algorithm.
Some of these cases concern exceptions to the required type triple rule, such as an owl:Class being a permitted type for a restriction.
Others concern orphans, blank nodes that are not the object of any triple: for example, list nodes may not be orphans.
Most directed cycles of blank nodes are prohibited.
While these form internal errors, the re nement algorithm cannot detect them.
However, using three pseudocategories, it can detect many cases of provably non-cyclic nodes (e.g.
a blank node that is the object of a triple whose subject is a URIref or a non-cyclic blank node).
The  nal check then only examines those nodes not already proven to be non-cyclic.
The hardest part is checking the constraint on owl:disjointWith.
During the re nement algorithm each pair of nodes linked by owl:-disjointWith is saved in a classical undirected graph G. The  nal check then veri es the following transitivity constraint on G, which is suf cient for there to be appropriate owl:disjointWith cliques:  a, c   V (G)   blank b   V (G), {a, b}   E(G)   {b, c}   E(G)   {a, c}   E(G)

 Performance  gures are shown in Table 3.
The two systems were run on the OWL Test Cases (resulting in approx 480 single document recognition tasks).
In addition, the systems were given a large12 OWL Lite ontology (the NCI Cancer Ontology13) to recognize.
Test documents were cached locally to reduce delays due to network access.
These are not intended to be detailed test results, but show the rough performance of the systems.
Elapsed time (in seconds) and total memory required are given.
Figures are broken down by test size (in terms of triples in the models).
Tests were run on a PC running Windows 2000 with a Pentium III 866 MHz processor and
 As we can see from the  gures, the systems are roughly comparable in time and space required.
Jena s better performance on the working group tests can be explained in part by the fact that it does less work and does not produce abstract syntax trees.
Here we compare the two approaches and consider their advantages and disadvantages.
The Jena implementation has two main attractions: much of the code is generated from the grammar, and, in principle, the algorithm need only remember relatively small amounts concerning triples that have been processed.
A key defect is that the approach does not generate an abstract syntax tree.
In general, generating code from a grammar using a compiler compiler should make it easy to change.
Many changes can simply be copied into the source grammar, and the system is then recompiled.
However, the grammar for OWL, particularly the mapping 12over 500,000 triples.
13http://www.mindswap.org/2003/ CancerOntology/ rules, is augmented with English text, which provides additional dif culties that sabotage the simple recompile.
Moreover, the treatment in the Jena checker depends on a number of global features of the abstract syntax grammar, such as the assignment of a type to every node.
These features are partly there as a result of lobbying by the second author between the  rst draft of S&AS and the  nal recommendation.
Hence, the approach is fragile to change.
A further advantage of using a compiler compiler is that the core engine is very small, which has facilitated optimization, permitting some incremental processing, see [5].
For the external errors, the Jena code could give elegant error messages.
For the internal errors, the code currently computes a minimal subgraph exhibiting the error.
The error message then prints this subgraph.
This is not user friendly, and is a consequence of the design with a core table driven engine.
A way to improve the error messages would be to write additional code that examined the minimal subgraph produced looking for common problems.
This would tend to duplicate some of the WonderWeb code.
What would make the Jena code particularly attractive is if the overall design of OWL DL, with an abstract syntax and mapping rules, were duplicated for some other RDF extension.
A possible candidate may be any speci c ontology, for which the data  les would be OWL  les using mainly the fact directives, and most of the axiom directives would be disabled.
Moreover, additional restrictions may be applied to the fact directives.
This would allow an ontology creator to specify a syntactic conformance to that ontology.
If the structure of the OWL DL de nition were used, then the Jena checker could be recompiled using the new de nition.
To make this feasible, a signi cant clean up to the approach taken in S&AS would be needed.
In particular, it is not plausible to support ad hoc English annotations to the formal rules.
The WonderWeb approach results in the construction of an object representing the abstract syntax tree of the Ontology.
This has a number of advantages, in particular it can facilitate further manipulations or translations of the ontology, e.g.
to an alternative format amenable for processing.
This has allowed us to experiment with alternative reasoning strategies for OWL using  rst order reason-ers14 or logic programming.
In addition, the approach allows us to provide  user friendly  error messages, informing the user why their ontology fails to belong to a particular language species.
Similarly,  xing particular kinds of errors (such as missing type triples) would be relatively easy.
Actually building the abstract syntax objects does not come without a cost.
In  dif cult  RDF graphs, say where we have two owl:-Ontology objects in the graph, we have no way of deciding where to put anything   if we are solely interested in validation (e.g.
whether an appropriate ontology can exist), then this is not an issue, otherwise we can only apply heuristics to determine where information should be contained.
As with Jena, the WonderWeb codebase is largely unoptimised and the memory footprint is large (see Section 6).
The strategy employed requires the entire RDF graph (or at least an interface allowing query of the entire graph) to be available to the checker.
This could, of course, be done using some persistent storage, reducing the memory requirements.
Similarly the construction of the abstract syntax objects requires storage, which is currently held in main memory.
The main drawback of the approach, however, is that it is effectively a bespoke or hard-coded solution   the rules for validation 14http://wonderweb.man.ac.uk/owl/first-order.
shtml
 Time (ms) Memory WonderWeb (Kb) WonderWeb Jena Jena












































 Table 3: Performance Figures are encapsulated in the implementation, both of the parser and the post-parsing validation.
Small scale changes to the mapping rules could be accommodated, large scale changes would require a more extensive rewrite.
Changes to S&AS as discussed in Section 7.1 are unlikely to be of much bene t here.
Another disadvantage is that the original structure of the RDF is lost   thus the WonderWeb parser and API is not well suited to handling general RDF15.
It is clear that the handling of owl:imports is a crucial aspect to parsing and recognition.
AS&S says: an owl:imports annotation also imports the contents of another OWL ontology into the current ontology and requires that an interpretation of an ontology O satis es the ontology iff it also satis es all ontologies mentioned in an owl:imports directive.
In terms of RDF graphs, the interpretation of imports is that [an ontology] is imports closed iff for every triple in any element of K of the form x owl:imports u, then K contains a graph that is the result of the RDF processing of the RDF/XML document, if any, accessible at u into an RDF graph.
The imports closure of a collection of RDF graphs is the smallest import-closed collection of RDF graphs containing the graphs.
There is a tension here between these interpretations of owl:-imports   the interpretation in terms of RDF graphs does not necessarily coincide or respect the boundaries of the interpretation in terms of  abstract syntax  ontologies.
This tension is re ected in the two implementations discussed here.
The triple based approach used in Jena handles imports in a natural fashion.
The abstract syntax approach taken in WonderWeb requires careful handling of imports   in some situations heuristics have to be applied in order to determine exactly where assertions belong, and information must be passed around during parsing of imported ontologies.
A related issue here is that of containment or context.
The mapping rules translate an Ontology to a collection of triples.
This collection includes triples that relate to the ontology object itself.
For example the ontology: Ontology( U Class( a ) ) yields the following triples: U rdf:type owl:Ontology a rdf:type owl:Class (a) (b) The problem here is that there is no connection between triple (a) and (b), other than the fact that they occur in the same graph.
The fact that the typing occurs within the context of the Ontology is not represented explicitly.
If an owl:imports triple occurs, the imported RDF graph is simply added to the existing graph   again though the fact that there is no explicit representation of the origin of those statements means that the context has to be handled using heuristics.
The situation is compounded further by the fact that the mapping rules allow for an RDF graph to be the translation of a collection of OWL ontologies.
In this case, we cannot even apply heuris-tics16 to determine which assertions belong with which ontology.
Of course, this dif culty is a direct consequence of the desire to actually reverse the mapping rather than demonstrate that a reverse mapping is possible   which is all that is required for recognition.
The desire to construct a representation closer to the abstract syntax is, however, not unreasonable.
The link between the physical location of the RDF graph and the URI assigned to the ontology in the abstract is also unclear.
For example the ontology: Ontology( http://xyz/things ... ) could be mapped to an RDF graph G1 which is then made available at a URI http://abc/stuff.
A second ontology can now make use of this: Ontology( Annotation(owl:imports http://abc/stuff) ) ... ) If this ontology is mapped to an RDF graph G2, calculating the imports closure of G2 actually results in the addition of the G1 into the RDF graph, including the triples referring to http://xyz/things as an ontology.
This is at best confusing, and is likely to result in OWL Full ontologies due to the lack of appropriate owl:Ontology triples.
Carroll and Stickler [8] suggest that the naming of RDF graphs should be promoted to a  rst class operation, and not handled implicitly through document names.
They suggest that this improves the semantics of both the owl:Ontology class and the owl:imports predicate.
Both systems could be modi ed to  x errors to transform OWL Full documents into OWL DL documents.
Some changes would be semantically sound, others unsound.
The sound changes are those where the original document both entails and is entailed by the transformed document, according to the OWL Full semantics.
Unsound changes either lose information or add information to the original document (or both).
Example sound  xups are:   Adding missing type triples for classes and individual valued   Adding missing type triples with type owl:Thing for individ-properties.
uals.
together as one  ontology .
the object of two triples (soundness depends on a conjecture from Carroll [4]).
  Converting a named restriction into a named class, and an unnamed restriction, linked by an owl:equivalentClass triple.
Example unsound  xups that add information, i.e.
the transformed document entails the original:   Adding missing type triples for data valued properties.
  Using a skolemization to resolve bnode problems.
Example unsound  xups that lose information, i.e. the original document entails the transformed document: object of two triples.
straint on a property with both.
  Forgetting either a transitivity constraint or a cardinality con  Doing a deep copy of an unnamed individual which is the   Take a large DL subset of the RDFS closure of the input graph.
This is particularly useful when the OWL vocabulary has been extended.
This is easier in Jena than in WonderWeb, since Jena includes an RDFS reasoner.
A further unsound, but potentially useful,  xup is to clone a property that is used both as a data valued property and a individual valued property.
One version is used for data values, the other for individual values.
For the external errors both systems know what went wrong and the appropriate place for the  xup is clear.
For internal errors, the Jena recognizer faces the same problem as with error messages, in that it  nds bad subgraphs without a clear idea of why they are bad.
Hence the  xup code would need to be able to analyse such subgraphs to identify the problem.
Some errors defy  xup   for example cycles of bnodes in descriptions.
We answer the syntactic aspects of van Harmelen and Fensel s question [16] of  how well can AI concepts be  tted into the markup languages on the Web?  with a weak af rmative: well enough, (but it was not easy).
We have demonstrated that it is possible to build OWL parsers and recognisers.
Moreover, one of our implementations re ects the needs and interests of the AI community, the other those of the Web community.
This is a nontrivial exercise, but the information in the OWL document set is suf cient to allow implementors to build recognisers that behave appropriately on the OWL Test Cases [7].
The identi cation and discussion of issues and hard cases may also prove useful for those wishing to implement OWL-based systems.
Demonstration of implementation experience is a key prerequisite to the endorsement of a recommendation by the W3C.
The existence of both the WonderWeb and Jena checkers17 and the fact that the two implementations decribed here adopt very different strategies to the task can be taken as further evidence that the speci cation is implementable.
To answer the question raised in the title of the paper, both triple-and tree-based approaches are possible.
Each has its pros and cons   which to choose depends primarily on the perspective of the application.
//www.w3.org/2001/sw/WebOnt/impls for details

 Jeremy Carroll is a visiting researcher at ISTI, CNR in Pisa, and thanks his host Oreste Signore.
Sean Bechhofer is supported by the WonderWeb project (EU grant IST-2001-33052) and thanks Boris Motik and Raphael Volz for implementation assistance.
The authors would also like to thank Peter Patel-Schneider for useful discussions on tackling the harder cases.
