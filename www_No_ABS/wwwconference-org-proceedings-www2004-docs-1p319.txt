The World Wide Web is estimated to contain 3-5 billion web pages nowadays and is still growing at a rate of 10 million per day.
The content of web pages ranges from dish washer advertisement to the proceedings of the W3C conference.
With Copyright is held by the author/owner(s).
I This research work is done at Microsoft Research Asia.
such huge volume and great variation in contents, finding useful information effectively from the web becomes a very challenging job.
Traditional  keyword based  text search engines cannot provide satisfying results to web queries since: (1) Users tend to submit very short, sometime ambiguous queries and they are reluctant to provide feedback information [3].
(2) The quality of web pages varies greatly [6], and users usually prefer high quality pages over low quality pages in the result set returned by the search engine.
(3) A nontrivial number of web queries target at finding a  navigational starting point  [9] or  URL of a known-item  [8] on the web.
Thus, web pages containing textually  similar  content to the query may not be relevant at all.
Based on the observations above, researchers tried different approaches to improve the effectiveness of web search engines.
One of the representative solutions is re-ranking the top retrieved web pages by their importance [1, 11, 17], which is calculated by analyzing the hyperlinks among web pages.
Hyperlink analysis (such as [1, 3-7, 17-19]) has been shown to achieve much better performance than full text search, in production systems.
According to their types, links can be classified into two categories: intra-type links, which represent the relationship of data objects within a homogeneous data space, and inter-type links, which represent the relationship of data objects between heterogeneous data spaces.
Most current web link analysis research only analyzes the hyperlinks within web pages, which can be considered as a homogeneous data space.
But in the real world, the web pages will often interact with other types of objects, such as users and the queries.
In this paper we try to deal with these interrelationships by expanding the link analysis to combine both inter-type link analysis and intra-type link analysis, and thereby improve web search performance.
In Figure 1, we show an example of inter and intra type links by analyzing the relationship of three related data types in the web environment: user, web page, and query.
Users and the queries they submit, plus the web pages they browse, form three homogeneous data spaces.
They are correlated when a user submits queries, a user browses web pages, and a query references web pages.
The three operations: submit, browse, and reference, involve inter-type links across these data spaces.
The hyperlinks within web pages, content-based 319similarity of queries, and social structure of users are intra-type relationships within each space.
It is obvious that when analyzing the attributes of web pages, not only the hyperlinks between them, but also the users who browse them and the queries that reference them can play important roles.
Webpage Reference Browse Query Issue User Figure 1: An example of multi-type interrelated data spaces Most existing web related research fits into the web multi-space model we described in Figure 1.
For example, web search [4, 17] uses the web page space and hyperlinks within the space; collaborate filtering [14] uses the document (web page) space, the user space, and the browsing relationship in-between; web query clustering [22] uses the web page space, query space, and reference relationship in-between.
Unfortunately, most of these works only consider one type of link/relationship when analyzing the links/relationships of objects, and they can be classified into intra-type link analysis and inter-type link analysis regarding the type of links they use.
In intra-type link analysis, the attribute of a data object is directly reinforced by the same attribute of other data objects in the same data space.
For example, in Google s PageRank algorithm [4], the  popularity  attributes of web pages are reinforcing each other via the hyperlink structure within them.
In inter-type link analysis, the attribute of one type of data objects is reinforced by attributes of data objects from other data spaces.
(Examples of inter-type link analysis will be given in Section 3.)
Hyperlink analysis reflects the attributes of web pages from the editor s view.
The assumption of the hyperlink analysis is that users agree with the editor/author of the web pages in terms of the link structure.
It may not work well when a user s perception of a web page differs from that of the authors/editors.
Another example of inter-type link analysis, the DirectHit algorithm [11], well captures the web user s view of the web pages from their interactions with the Web.
DirectHit utilizes the inter-type links provide by end-users for web search assuming that the more frequently users visit a web page the more important the web page is.
It is natural to ask: Is it possible to combine the process of intra-type link analysis for the same data type and inter-type links across different data types together to improve the process of understanding the organizational relationship of data objects and finding the correct order of data objects regarding different attributes in multiple data types?
Intuitively, a simple way is to calculate the data object attributes using inter-type and intra-type link analysis individually, and then combine the results together.
However, this solution does not fully utilize the fact that inter-and intra- type links may reinforce the attribute of a data object at the same time.
Hence, a unified framework for link analysis is proposed in this paper.
The assumption is that the attribute of a data type is influenced not only by the intra-links of its own type but also influenced by the interlinks from other attributes of other different data types.
Furthermore, different attributes of different data types can reinforce each other.
The problem of leveraging link structures within and across different data types to gain more understanding of the organizational structure and attribute order of objects within each data type can be referred to as the  Link Fusion  problem.
This name is borrowed from the concept of  Data Fusion  in information retrieval where multiple sources of evidences are combined in order to improve the prediction of the relevanc of documents to a query.
Experiments on an instantiation of the framework that makes use of users and web pages from a proxy log show that by using our approach, the search precision is improved by 24.6% and 38.2% compared to the traditional HITS [17] and DirectHit [11] algorithms, respectively.
The rest of this paper is organized as follows.
In Section 2, we present related work on current state-of-the-art link structure analysis algorithms.
In Section 3, we present the proposed unified link analysis framework for multi-type interrelated data objects, which can support HITS and PageRank, as well as the DirectHit algorithm.
Then, we show the experimental results in Section 4.
Finally, we conclude in Section 5 relative  importance  of each employee within

 Research on analyzing link structures to better understand the informational organization within data spaces can be traced back to research on  Social Networks  [13].
A good example comes from the telephone bill graph.
By searching connected and isolated components, scientists can estimate the diameter of the whole graph and hunt for each complete sub-graph or  clique , to indicate contacts among people.
Another interesting example is the famous sociology phrase  six degree of separation , which means that any pair of people on the earth can get acquaint through no more than six intermediaries.
Although proving this is still far from complete, some sub-graphs of human society can be explored easily and thoroughly.
For instance, members of an enterprise can form an operation graph.
By recognizing the functional relationship of each employee, one can learn structural and the organization.
The problem of link structure of social networks can be reduced to a graph G = (V, E), where set V refers to people, and set E refers to the relationship among people.
Katz [16] tried to measure the  importance  of a node in a graph by calculating the in-degree (both direct and indirect) of that node.
Hubbell [15] tried to do the same thing by propagating the  importance  weights on the graph so that the weight of each node achieves  equilibrium .
Researchers from the bibliometrics area claimed that scientific citations could be regarded as a special social network, where journals and papers are the nodes and the citation relationships are edges in the graph.
Garfield s famous  impact factor  [12] calculates the importance of a journal by counting the citations the journal received (the in-link) within a fixed amount of time.
Pinski and Narin [20] claimed that the importance of a journal is recursively defined as the sum of the importance of all journals that cited it.
Based on this hypothesis, they designed the following measure of importance.
Consider matrix A is the link matrix in the journal space.
Aij denotes the fraction of the number of citations from journal i to journal j.
Suppose wj is the importance value of journal j, their calculation can be represented 320w j = w

 n as
 (   u = ij )   M w
 (+  
 )  +   (1 .
Adding By iteratively calculating the formula above, it wi Ai =   ij
 w , where w is the vector of important weights of leads to A w = journals.
It is easy to find out that w is the principle eigenvector of
 A .
Following the same rationale, Brin and Page [4] design the PageRank algorithm to calculate the importance of web pages in the Web.
In addition to Pinski and Narin s algorithm, PageRank simulates a web surfer s behavior on the web.
That is, with probability 1 , the surfer randomly picks one of the hyperlinks on the current page and jumps to the page it links to; with probability  , the user  resets  by jumping to a web page picked uniformly and at random from the collection.
This defines a Markov chain on the web pages, with the transition matrix , where U is the transition matrix of uniform
   for all i, j).
The vector of transition probabilities ( PageRank scores w is then defined to be the stationary the distribution satisfying random surfer model can prevent the  sink node problem  in the PageRank calculation.
Kleinberg [17] claimed that web pages and scientific documents are governed by different principles.
Journals have approximately the same purpose, and highly authoritative journals always refer to other authoritative journals.
The World Wide Web, however, is heterogeneous, with different pages serving different roles.
Authoritative web pages do not necessary to other authoritative pages, thus Pinski and Narin s hypothesis for scientific literature does not hold in the web.
Based on his observations, Kleinberg divides the notion of  importance  of web pages into two related attributes:  Hub  (measured by the  authority  score of other pages that a page links to), and  Authority  (measured by the  hub  score of the pages that link to the page).
Different from the PageRank algorithm which calculates the importance of web pages independently from the search query, Kleinberg presented his Hyperlinked-Induced Topic Search (HITS) algorithm as following: (1) Use an ordinary search engine to search the query and form the root set as the starting point; (2) Get the base set by adding pages pointing to or pointed at root pages; (3) Count the authority and hub weights of each page in the base set with an iterative algorithm: for each page, let a(p) and h(p) denote its authority attribute weight and hub attribute weight.
The two attributes can be calculated as: link   q   h q ( ) and h p ( ) = p   p   a q ( ) q a p ( ) = Let A denote the adjacency matrix of the base set: aij=1 if page i has a link to page j, and 0 otherwise.
Vectors a and h correspond to the authority and hub scores of all pages in the base set, hence, a=ATh and h=Aa.
It is easy to show that a and h are eigenvectors of matrices ATA and AAT.
The search system [1] developed using the HITS algorithm achieves comparable performance with  Yahoo! , which maintains a manual compilation of net resources.
Many researchers have extended the HITS algorithms to improve its efficiency.
Chakrabarti et al. [5, 6] used texts that surround hyperlinks in source web pages to help express the content of destination web pages.
They also reduce weight factors of hyperlinks from the same domain to avoid a single website dominating the results of HITS.
Lempel and Morgan [18] extend HITS by replacing Kleinberg s Mutual Reinforcement approach with a new stochastic approach (SALSA), which can be considered as a weighted link structure analysis of the web sub-graph.
In their work, they identify the Tightly Knit Community (TKC) Effect in the web communities that hampers the HITS algorithm to identify meaningful authorities, and they show that SALSA is less vulnerable to the TKC effect than the HITS algorithm.
Ng et al. [19] presented randomized HITS and subspace HITS algorithms to enhance the stability of the basic HITS.
The former imitates a random walk on web pages and defines the authority/hub weight as a chance of visiting that page in time step t (t is large enough).
The latter uses the first k eigenvectors instead of the entire matrix ATA to count the authority values.
Cohn et al. [7] introduced a probabilistic factor into HITS and applied the EM model.
All these show that the authority idea has great potential in web applications.
Inter-type links (links that connect different types of data objects) represent relationships of different domains.
Researchers also analyzed this kind of link to find out whether it can help improve the link analysis of the data objects within the same data type.
For example, DirectHit [11] harnesses the web pages visited by millions of daily Internet searchers to provide more relevant and better-organized search results.
Based on the assumption that the most relevant pages of a topic are those most visited, DirectHit's ranking algorithm is used by Lycos, Hotbot, MSN, Infospace, About.com, and roughly 20 other search engines.
Miller [18] proposed a modified HITS algorithm, which also utilizes the users  behavior on the web to improve the calculation of hub and authority scores.
In his algorithm, the adjacency matrix A is modified and the value of aij in A is increased whenever a user travels from page i to page j (information obtained by analyzing website access logs).
Although Miller uses links from two different spaces (user and web space), he only converted inter-type links (links between users and webpages) to intra-type links (links within webpages) to enhance the link analysis for web pages.
The users  importance is ignored in this algorithm.
Most recently, Davison [10] analyzed multiple term document relationships by expanding the traditional document-term matrix into a matrix with term-term and doc-doc sub-matrices in the diagonal direction and term-doc and doc-term sub-matrices in the anti-diagonal direction.
The term-term sub-matrix represents term relationships (e.g., term similarity), and the doc-doc sub-matrix represents document relationships (e.g., link matrix for web pages).
He proposed that the links of the search objects (webpage or terms) in the expanded matrix could be emphasized.
With enough emphasis, the principal eigenvector of the extended matrix will have the search object on top with the remaining objects ordered according to their relevance to the search object.
Considering that terms and documents each form a different data space, with the doc-term and term-doc matrices representing inter-type links, and the term-term and doc-doc matrices as intra-type links, Davison s proposed research fits our framework very well.
There are similarities among link analysis in social networks, scientific citations, and hyperlink analysis in the web.
The data objects in these examples form one or multiple data spaces of different types.
Each data space contains one specific attribute of data.
Researchers take advantage of the links/relationships either within each data space (intra-type links) or across different data spaces (inter-type links) to calculate the specific attribute of the objects in each of the data spaces.
In this Section, we generalize previous link analysis studies and propose a unified link analysis 321framework to calculate the attributes of data objects within multiple data spaces.
We call this unified link analysis framework  Link Fusion algorithm .
Suppose we have n different types of objects X1, X2  Xn.
Each type of data object Xi contains a specific attribute Fi.
Data objects within the same type are interrelated with intra-type relationships Ri Xi Xi.
Data objects from two different types are related with inter-type relationships Rij Xi Xj (i j).
Suppose attributes of different types of data objects are comparable (e.g., similar in nature).
We borrow and extend Pinski and Narin s recursive definition of importance [20] and define that the specific attribute of a data object in one data type equals the sum of the attributes of other data objects in the same data space that link to it, plus the sum of other related attributes of data objects in other data spaces and links to it, mathematically as: (1)
 i =
 i i +     j i
 j ji



 = , of x }m ,
 and types two y }n relationships of objects
 , For simplicity, we first explain the case that only contains two types of related objects as example to illustrate Eq.
(1).
We consider and x x , { , =


 and R .
The
 ,
 y y { ,


 adjacency matrices are used to represent the link information.
YL stand for the adjacency matrices of link structures within and YXL stand for the adjacency set X and Y, respectively.
matrix of links from objects in X to objects in Y and adjacency matrix of links from objects in Y to objects in X respectively.
ix to node jy , and
 is the attribute vector of
 is the attribute vector of objects in Y, Eq.
(1) objects in X, can be mathematically represented as: , if there is a link from node otherwise.
Suppose XYL and i j = ( , ) 1 i ( , ) yw xw j =
 w w y x         = = L w
 y L w
 x y x + + L w
 xy L w
 yx x y (2) and it can be easily extended into N interrelated data spaces, as shown in Eq.
(3) +   (3) L w

 L w

 w =

    
 There are two issues that need to be considered in Eq.
(3): First, as noted by Bharat and Henzinger [3], mutually reinforcing relationships between objects may give undue weight to objects.
Ideally, we would like all the objects to have the same influence on the other objects they connect to.
This can be solved by normalizing the binary adjacency matrix in such a way that if an object is connected to n other objects in one adjacency matrix, each object it connects to receives 1/n of its attribute value.
The random surfer model used in PageRank also can be introduced here to simulate random connection, and avoid sink nodes during the computation.
Second, it is too na ve to assume that attributes from different data spaces are equally important, when used to calculate the attribute of data objects.
This can be solved by changing Eq.
(2) into a weighted sum of attributes.
With the consideration of the two issues above, Eq.
(3) can be further improved into Eq.
(4):
 =   +  
 L w '
 w  
     where   +      
  
       =


 (1 ) ; 0 ' +      

  


 (1 ) '   +   =      


  

 =
    
 >
 >
 (4)  
    
 L w '

 < <   ;0
 < <  
 In Eq.
(4), U is the transition matrix of uniform transition probabilities (uij=1/n for all i, j; where n is the total number of objects in data space N).
  and   are smoothing factors used to used to simulate random relationships in matrices LM and LNM.
LM and LNM are normalized adjacency matrices.
As with the PageRank and HITS algorithms, the attribute value of objects in our framework can be obtained by iteratively calculating Eq.
(4) until the result converges.
With the definition of Eq.
(4), we actually created a unified square matrix A, as shown in Eq.
(5), where n is the total number of all involved objects in different data spaces.
The unified matrix A has L  M on the diagonal direction, and L  NM in other parts of the unified matrix as illustrated below.
' '    



 ' '    

 ...
...
  n
   n

 ' n

 ' n

 =

 '   n n




 ' '   n n n ...
  n
 (5)
 iterative approach Suppose w is the attribute vector of all the data objects in different data spaces.
The proposed is actually transforming the vector w using matrix A (e.g., w=ATw).
It is relatively easy to find out that when the calculation converges, w is the principle eigenvector of matrix A.
The formal mathematical proof of the convergence of the calculation can be found in the appendix.
Two problems need to be addressed in the construction of the unified matrix A.
Suppose M and N are two heterogeneous data spaces, when a data object in M has no linking relationship to any data objects in N, we set all the elements in the corresponding row of the sub-matrix
 T to 1/n, where n is the total number of objects in data space N. The reason we use random relationship to represent no relationship is to guarantee all the sub-matrix L  T to be nonzero and to prevent  sink nodes  that may eat up all the weights during the calculation (as suggested by the PageRank algorithm).
However, in practice, we can always ignore undesired intra/inter type relationships by setting the corresponding   or   to 0.
In the unified matrix, if  MN>0, then  NM>0.
This is a necessary condition for the recursive calculation to converge, (as explained in the appendix).
However, if the relationship of L  T is really undesirable for the link analysis, we can always assign a very small positive  NM to reduce the effect of L  By constructing a unified matrix using all the adjacency matrices, we actually construct a unified data space, which contains different types/attributes of data objects.
Previous inter-type links are now intra-type links in the unified space, and the  link fusion algorithm  is reduced to link analysis in a single data space.
The proposed framework can be easily used to explain previous works on link analysis.
which is the original definition of PageRank algorithm.
The PageRank algorithm can be considered as a special case of our unified link analysis framework.
In PageRank, there is only one attribute (popularity) of one kind of data object (web pages) being considered.
Having  =1 and  =0, (4) reduces to w L w= The HITS algorithm also can be considered as a special case of the unified link analysis.
In the HITS algorithm, two attributes (hub and authority) of the same type of data objects (web pages) are being considered.
Hub attributes and authority attributes of the same set of web pages each form a data space; the hyperlinks in-between web pages are now inter-type links that connect the Hub space and Authority space as illustrated in Figure 2.
Hub Authority Figure 2: Hub and Authority Spaces in HITS algorithm

 = = w h L w ah a , where Since there are not intra-links in each data space, we set  =0 and  =1 and derive the recursive updating equation from Eq.
(4): L w w aw is the authority value and h ha a ahL are adjacency hw haL is the hub value vector and vector, matrices.
Considering the normalization of the adjacency matrices and the introducing of smoothing factor  , this is by definition the Randomized HITS algorithm [19], which is more robust and stable than the traditional HITS algorithm.
We use 10 days log from a proxy server at Microsoft to evaluate the effectiveness of our proposed Link Fusion algorithm.
The raw proxy logs records user visit information, in which one record corresponds to one HTTP request for a web object from an IP address.
In other words, different users from the same IP address are considered as the same user in our experiments.
Some heuristic rules (e.g., the words within the hyperlinks, the extension of the filenames, etc.)
are applied to filter out the unrelated information, (e.g., ads, images, etc.).
Only text pages are reserved in the final dataset, which contains 2,998,821 visit records to 1,773,718 pages by 38,887 users.
Our goal is to improve the end-user s search effectiveness through re-ranking the search results by our proposed Link Fusion algorithm.
In order to fit into our framework, we extended the underlying assumption of the HITS algorithm to incorporate the notion of user s  popularity  attribute, and it is defined as below:   A popular user always look at good hub and good authority pages;   A good Hub page always points to good Authority pages and is always visited by popular users;   A good Authority page is always pointed at by good Hub pages and is always visited by popular users too.
User The Hub, or Authority attribute of web pages, and the Popularity attribute of users form three different data spaces.
These three data spaces are correlated via the hyper links between web pages and user access information from the web proxy log.
Their relationships are more clearly illustrated in Figure 3 below.
Authority Hub Figure 3: Hub, Authority and User Spaces We find that the three data spaces and the links in-between them fit our Link Fusion algorithm perfectly.
We apply the Link Fusion algorithm from Eq.
(5) into this case, and derive the unified adjacent matrix as Eq.
(6):
 =   u   hu   au
 ' u
 ' hu
 ' au   uh   h   ah
 ' uh
 ' h
 ' ah   ua   ha   a
 ' ua
 ' ha
 ' a (6) d where the subscripts a, h and u denote the Authority, Hub, and User space respectively.
Since in our case, each data space has no intra-links, we set  i = 0 (i = a, h, u), and we set all the   equal to
 is the total number of objects in the corresponding data space N.
Suppose w is the attribute value vector of all the data objects in the three spaces, their final attribute values in w can be obtained by recursively calculating wi+1=ATwi (where i is the iteration is smaller than a number) until converge (e.g., thresh-hold value) After generating the link matrix, we calculate the different attributes of web pages and users and use the  Authority  attribute of web pages to re-rank the search results.
The detailed approach is described as follows.
We choose 10 sample queries (shown in Figure 1.)
to evaluate the Link Fusion algorithm.
Detailed experiment steps for each of the sample queries are: Step 1: Creating the Hub space and Authority space.
The Hub space and Authority space are constructed in a way similar to the w+   i
 w i =

 search engine, and the top 200 matching web pages are retained as the root set.
Then, the root set is expanded to the base set by its neighborhoods, which are the web pages that either point to or are pointed at by pages in the root set.
In this experiment, we set the maximum in-degree of nodes as 50, which is commonly adopted by the previous works [3, 17].
The expanded set of web pages forms the data objects in Hub space and Authority space.
Hyperlinks between web pages not on the same web site form the directed links connecting the Hub and Authority space.
Step 2: Creating the Hub/Authority spaces, we compare the web pages in these spaces with the MSN proxy log data, and extract out the overlapping web pages.
The users who browsed these overlapping web pages form the User space, and their browsing activity forms the links from the User space to the Hub/Authority space.
In this experiment we tried to select a set of popular web search queries to test the effectiveness of our Link Fusion algorithm.
The queries we selected are shown in Table 1.
the User space.
After we created Table 1.
Queries used in Experiments










 Query search engine telephone service audi car baby care windows XP computer vision notebook computer online dictionary network security daily news
































 In Table 1, PN denotes the total number of pages in the formed Hub/Authority space.
LN is the number of pages in the Hub/Authority space that were linked by User space (or the number of links from User to Hub or Authority Space).
UN denotes the total number of different users in the User space.
Step 3: Calculation.
After creating all three data spaces, we assign an initial weight to each data object, as introduced in Section 4.1.
and start the recursive calculation on the different attribute in the data spaces according to Eq.
(6) until convergence.
Step 4: Evaluation.
Finally, we re-rank top returned the documents according to the Authority value we derived from recursive calculation of wi+1=ATwi.
Then we use precision at top 10 documents to compare our results with other algorithms.
In this section, we compare the performance of Link Fusion algorithm with that of the text-based retrieval algorithm, HITS algorithm and DirectHit algorithm.
DirectHit algorithm is achieved by re-ranking the top 200 text-based search result according to their number of visits from the user space.
For each of the queries listed in Table 1, the union set of top 10 documents returned from the 4 algorithms are pooled together and rated for relevance by 5 volunteers.
The final relevance judgment for each <query, document> pair is decided by majority votes (e.g., the pair is relevant only if more than 3 volunteers voted it as relevance).
We then computed precision at top 10 documents (p@10) for each of the four algorithms.
This measurement is p r= /10 defined as: @10 , where r is the number of relevant documents in the top 10 pages returned.
The comparison of precision for 4 algorithms is shown in Figure 4.
The label  avg  is the average p@10 across the 10 queries.
Text Retrieval
 DirectHit Link Fusion n o i s i c e r
















 avg Query ID Figure 4.
The Precision Comparison of 4 Algorithms We can see from Figure 4 that our proposed Link Fusion algorithm outperforms the basic HITS algorithm and DirectHit algorithm by 24.6% and 38.2% respectively.
We give a more detailed analysis of the results by looking at the top URLs returned by three algorithms for several queries.
First we show the results of query  audi car  in Table 2.
Shaded cells in the table indicate relevant pages.
We found that the Link Fusion algorithm had returned 7 out of 9 relevant pages returned by HITS algorithm and DirectHit algorithm combined together, while only keep 2 of the 8 non relevant pages returned by HITS and DirectHit algorithm.
Furthermore, Link Fusion algorithm had returned one more relevant page: http://www.s-cars.org/ that has not been found in the top 10 results from either HITS or DirectHit algorithm.
The above observations shows that the Link Fusion algorithm has the capability of keeping the correct results from different link analysis algorithms it combined, while filter out incorrect results returned from these algorithms.
Researchers had reported similar findings from data fusion experiments in information retrieval [21].
They claimed that the combined search engine could keep the relevant results returned by different single search algorithms, while filter out those non-relevant results returned by single search algorithms.
However, whether the prerequisite conditions for data fusion in information retrieval to be effective are also valid for Link Fusion problem is still left to be explored.
Table 2.
Top 10 results for query  audi car 
 http://www.audiworl d.com http://www.audiusa.c om/ http://www.audicana da.ca/ http://www.vindis-cambridge.audi.co.u k/ http://pages.ebay.co DirectHit http://www.audiusa.co m/ http://www.autotrader.
com/ http://www.nytimes.co m/pages/automobiles/i ndex.html http://pages.ebay.com/ ebaymotors/browse/ca rs.html http://www.thecarconn Link Fusion http://www.audius a.com/ http://www.audiwo rld.com http://www.uvas.c om/ http://www.s-cars.org/ http://communities 324m/ebaymotors/brows e/cars.html http://www.quattrocl ubusa.org http://www.karquattr o.com/ http://www.porsche.c om/ http://www.vwvortex .com http://www.nytimes.
com/pages/automobil es/index.html ection.com/ http://www.gearheadc afe.com/mags.html http://www.uvas.com/ http://communities.ms n.co.uk/AudiSCarsUK /pictures http://www.autotrader.
com/ http://www.a4.org/ .msn.co.uk/AudiS CarsUK/pictures http://www.autotra der.com/ http://www.quattro clubusa.org http://www.a4.org/ http://www.vwvort ex.com http://www.thecarc onnection.com/ We also found that the binary relevance judgment of a web page we applied in this experiment cannot always fully reflect the  value  of a web page.
Although the number of relevant pages returned within top 10 pages by the Link Fusion algorithm (8) is slightly better than that of the HITS algorithm (6), the relevant pages (e.g., http://www.a4.org, http://www.s-ars.org) are more authoritative than the relevant pages returned by HITS (e.g.
http://www.vindis-cambridge.audi.co.uk).
This problem is well represented by another case below.
the Link Fusion returned by algorithm Table 3.
Top 10 results for query  search engine  Link Fusion http://www.google.
com/
 http://www.google.co m/ DirectHit http://www.google.co m/ http://dailynews.yaho o.com/ fc/Tech/Internet_Port als_and_Search_Engi nes http://www.search.co m/ http://www.decideinte ractive.com/ http://www.usaweeen d.com/01_issues/0107 22/010722web.html http://www.galaxy.co m/ http://searchenginwat ch.com/awards/ http://www.bcentral.c om/products/si/default .asp http://ixquick.com/ http://www.excite.
com/ http://www.lycos.c om/ http://search.msn.c om/ http://www.megas pider.com/ http://www.arelanr ecords.com/ http://www.ubnmo vies.com/ http://www.novan w.com/ http://www.ixquic k.com/ http://www.ubnmovie s.com/ http://www.arelanreco rds.com/ http://www.novanw.c om/ http://www.megaspid er.com/ http://www.excite.co m/ http://www.asiaco.co m/ http://www.lycos.com / http://search.ietf.org/s earch/brokers/internet -drafts/query.html http://www.searcheng inewatch.com/ http://www.infospace.
com/ http://www.dogpil e.com/ Although almost all the pages retrieved by the three algorithms are correct web pages for query  search engine , it is easy to see that the Link Fusion algorithm apparently gives higher ranks to more popular search engines (e.g., http://www.excite.com, http://www.lycos.com) than the other two algorithms.
While in HITS and DirectHit algorithms, correct but not very popular search engine web pages (e.g., http://www.ubnmovies.com/, http://www.search.com/) are returned on top.
This is because that if a correct web page is returned on top by the Link Fusion algorithm it must be favored by both the web editors (represented by hyperlinks) and the web users (represented by user links) rather than just one of them (e.g., HITS or DirectHit).
Thus the Link Fusion algorithm returns more popular results on top than HITS and DirectHit algorithm and also more robust than the other two algorithms.
Below are the results of query  daily news .
We can find from this example that the Link Fusion algorithm had both keep the correct results from HITS and DirectHit algorithm and rank the popular correct pages (e.g.
http://www.nytimes.com) much higher than the other two algorithms.
Table 4.
Top 10 results of query  daily news  Link Fusion
 http://www.surfinfo.c om/html/visreport.htm l http://dailythong.dhs.
org/index.php3 http://www.sportspag es.com/regions/mw.ht m DirectHit http://www.msnbc.co m/m/hor/horoscope_fr ont.asp http://daily.webshots.
com/ http://www.nytime s.com/ http://sportsillustra ted.cnn.com/ http://www.thedaily.c om/bikini.html http://encarta.msn.
com/ http://www.gossipcent ral.com/ http://www.poems.co m/today.htm http://www.thedaily.c om/overlook.html http://www.webcomic s.com/daily.html http://www.guampdn.
com/classifieds/index.
html http://www.nytimes.c om/ http://www.thedaily.w ashington.edu/ http://www.alrai.com/ http://www.poems.co m/ http://cityguide.guam pdn.com/fe/index.asp http://www.thedaily.w ashington.edu/ http://www.smarterti mes.com/ http://dailythong.dhs.
org/index.php3 http://www.thedail y.com/overlook.ht ml http://abcnews.go.c om/ http://www.poems.
com/ http://www.gossip central.com/ http://www.thedail y.com/bikini.html http://abcnews.go.c om/sections/enterta inment/ http://www.thedaily.c om/overlook.html http://www.thedail y.washington.edu/

 In this paper, we first defined two kinds of links among data objects within different data types: intra-type links, which represent the relationship of data objects within a homogeneous data type, and inter-type links, which represent the relationship of data objects between different heterogonous data types.
Then, we proposed a unified link analysis framework, called  link fusion , to analyze inter and intra-type links and to bring order to data objects in different data spaces at the same time.
Next, we evaluated the effectiveness of our proposed link fusion algorithm by applying it into a real world scenario of three data spaces: Hub-page space, Authority-page space, and User space.
Experimental results on 10 real world sample queries show that the Link Fusion algorithm achieved 24.6% improvement over the HITS algorithm and 38.2% improvement over the DirectHit algorithm based on the measurement of precision at top 10
 Link Fusion algorithm has the capability of keeping the correct answers returned by each of the link analysis algorithm it combined and trend to return the most popular results on top of its return list.
These results support our assumption that the Link Fusion algorithm when used properly can help find the correct order of attributes of data objects within different data spaces.
Although the Link Fusion algorithm seems to be promising according to our preliminary experiments, there are still many issues that need to be explored.
For example, in our experiment, we assumed the links from different data spaces are equally important when calculating the attributes of objects across different data spaces.
However, this assumption is overly na ve, and it is almost never the case that the links from different data spaces are equally important.
It is natural to think: Is there any way to identify the relative importance of links from different spaces automatically?
We will explore this problem in our future research works.
We thank Dr. Weiguo (Patrick) Fan from Virginia Tech and Li Wang from University of Michigan for their kindly help and suggestions.
