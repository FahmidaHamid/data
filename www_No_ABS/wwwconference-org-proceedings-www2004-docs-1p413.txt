Typical search engines show identical results for a given query independent of the user or the situation in which the query is being issued [15].
This may not be suitable since: (i) users may have different information need, (ii) the information need of a single user may change through time and (iii) the relevance of each object retrieved is extremely dependent on the context in which the query is issued [16].
For instance, the relevance of the results for the query  jaguar  will certainly depend whether the user is currently seeking information about F-1 pilots or is interested on the animal from the jungles of Suriname.
Techniques for community identi cation have been extensively used to improve the quality perceived by users of search engines.
Communities have been incorporated as a source of information for Copyright is held by the author/owner(s).
ranking algorithms and new applications such as automatic directory creation.
Furthermore, community identi cation studies have proven to be of great value to researchers trying to increase their understanding of the information society [1, 11, 12].
This paper presents and evaluates a novel ranking technique that uses the combination of these evidential sources of relevance: the content of the objects being retrieved and the interest-based community of the user issuing the search.
The theory of Bayesian belief networks [20] is used as the unifying framework of our approach since it naturally adapts to the problem of combining two sources of evidence in a single Information Retrieval (IR) model.
A simple case study based on data collected from an actual service available on the Web is presented.
Interest-based communities are groupings of users or, more spe-ci cally, user interactions that share common interests.
They are created using clickthrough data recorded in the form of user sur ng behavior.
Our approach to community identi cation in the Web [2] is different from previous work since the majority of them usually rely on the explicit information provided by the authors of the services in terms of the hyperlinked structure connecting the Web pages provided [11, 12].
Since we base our analysis on the user s perspective, we are allowed to infer communities even for sources that do not explicitly show relationships between the pieces of information provided.
As the Web evolves and new kinds of services (e.g., streaming media, online gaming) are created there is an urgent need for algorithms designed to work based on evidences other than link information.
Our model is directly applicable to Web services providing search interfaces to the content they provide, but it can also be adapted and useful to searching the whole Web.
Although information about accesses distributed over the Web is not available, any search engine can certainly infer interest from information about the queries and subsequent accesses to documents returned to them.
This piece of information can be easily gathered through the use of off-the-shelf technologies [14].
Query contextualization is achieved by the juxtaposition of the current user interaction with a set of previous user interactions of all users in a way similar to collaborative  ltering [6,10,14,17].
Although we do not try to use user interactions other than the current one in order to characterize use interests when submitting a query, our technique can be easily extended to deal with sets of previous user interactions.
This paper is organized as follows: Section 2 introduces the method for identifying Interest-based communities.
Section 4 shows how to combine the summaries with a content-based ranking technique using textual information as an indicative of relevance.
Section 5 presents experimental results.
Section 6 reviews related work.
Section 7 discusses concluding remarks and future work.
Usually, the information used by community identi cation algorithms is provided by the hyperlinked structure connecting Web pages [11, 12].
By modeling the Web as a graph and performing several operations on it, the authors were able so separate the Web in sets of related items.
Link information is provided on the creation of the page and is in uenced by the author s view of the content provided and its relationship to pages.
The content and outgoing links of a page represent a unique view about a subject, provided by its creator.
The incoming links to a page also represent unique views of that particular page.
These views are not necessarily identical to the creator s, but still represent separated views of the same object.
This distributed and uncoordinated nature of link creation is one of the main reasons for the success of community identi cation over the Web.
In order to identify Interest-based communities for a service, similar entities for representing user accesses must provided.
Our approach is to model users and their interests in a graph and use the techniques already proposed for community identi cation as a means of identifying communities of interest.
The structures we use to model user interaction as a graph are the Session Interest-based Graphs [2].
Session Interest-based Graphs, or simply SIGs, are centered on user sessions, i.e. a subset formed by the accesses issued by a user during a single interaction with a service.
Since the scope of a session is restricted to a single user interaction, it is assumed that the objects contained in a session will be somewhat related and will mostly refer to a single interest.
Nodes in a SIG model the sessions of the service being analyzed into nodes of the graph.
The weight of the edge connecting any two nodes representing sessions p an q is interpreted as their relationship and is denoted hereafter as S[p, q].
SIGs are an interesting modeling approach for they present characteristics that make them amenable to community identi cation such as: high clustering coef cient, small diameter and the existence of a giant strongly connected component.
In order to build representative SIGs not all requests made by the users are considered.
Although we need to consider all user requests in order to correctly estimate user sessions, before an actual SIG is built, some of the requests are  ltered.
The speci c requests used in order to estimate session relationship are chosen according to business and institutional goals and will vary on an application basis.
This distinction is made since some requests are more effective on characterizing user interest than others.
For instance, in the case of an Online Radio the service provider can choose to use simply requests for songs coming from streaming media clients and ignores all other Web requests.
In the case of a content provider all pages except the home page that is accessed by all of them might considered representative.
From now on, we will refer to the remaining elements of the sessions simply as objects, independently of their type (i.e. page, streaming media video, etc).
Relationships between sessions are measured as cosine [5] between the vector representations of these sessions over the conceptual space formed by the objects requested by users.
Despite the symmetry of the cosine measure, the SIGs are modeled as directed graphs so that algorithms for community discovery could be directly applicable to them.
Several algorithms on the literature address the problem of community identi cation [1, 11, 12].
The HITS [12] is used, in this paper, mostly because it is a well-studied algorithm, derived from a well-known statistical technique, the Singular Value Decomposition (SVD).
SVD has been successfully used in wide range of application varying from image segmentation to VLSI design [4].
Moreover, this technique has been proven to produce appropriate results when applied to a dataset with characteristics similar to the ones existent in the SIGs [4, 19].
The HITS algorithm was initially proposed as a method for im-Its key idea is to proving the quality of searches over the Web.
identify hubs and authorities in a graph through a mutually reinforcing relationship existent between its nodes.
This relationship may be expressed as follows: a good hub is a node that points to good authorities and a good authority is a node that is pointed to by good hubs.
Thus, each node p, has associated with it an authority weight ap, and a hub weight hp.
These weights form a ranking of the nodes ranging from good hubs/authorities, with high hp/ap values, to bad ones, with low hp/ap.
Let S denote the adjacency matrix representing the SIG from which one wants to identify communities (i.e. rows and columns represent sessions and entries represent similarity between the sessions) and a,h arrays storing authority and hub information for all the nodes.
Therefore, authority and hub weights for the sessions can be iteratively computed as follows: a = ST h = ST Sa h = Sa = SST h (1) (2) It has already been proved that, the authority and hub arrays, a and h, converge to the principal eigenvectors of ST S and SST respectively.
Subsequent work on HITS [12] showed that it can be used for discovery of communities if also non-principal eigenvectors of ST S and SST are considered as community descriptors.
An implicit ranking of the communities can be derived by this method: the  rst non-principal eigenvectors identify the most important community over the nodes, the second non-principal eigenvectors explicit the second most important community over the nodes, etc.
The participation of node s in community c is given by the authority weight associated with each session in each community, denoted as,c.
Let
 (3) be the SVD [8] for matrix S. Therefore, U de nes the orthonormal-ized eigenvectors associated with the eigenvalues of SST (i.e. the hub weights), V de nes the orthonormalized eigenvectors associated with the eigenvalues of ST S (i.e. the authority weights) and   is a diagonal matrix which elements are the square roots of the respective eigenvalues in non-decreasing order.
Therefore, the authority values for each pair community/object can be retrieved from matrix V produced by the SVD of the SIG s adjacency matrix.
For ranking purposes, a relationship between each community and the objects of its interest must be provided (i.e. a summary for each community).
So far, we have only characterized relationships between user sessions and communities.
These plus information about the objects requested in each session will serve as a starting point for the summarization of communities.
The sessions are split into three disjoint sets with respect to each community, the set of members, the set of nonmembers and the
 sessions, with positive as,c values.
The nonmembers set is formed by the sessions occupying the lowest positions of the ranking, with negative as,c values.
The remaining sessions are included in a third set not considered through the rest of the summarization process.
The union between the set of objects requested in member sessions and the set of objects requested in the nonmember sessions may not be disjoint.
Therefore, for each community, we positively evaluate the objects requested in the member sessions and negatively evaluate the objects accessed in the nonmember sessions.
The weight associated with each pair object/session is calculated based on a tf-idf measure [5].
The interest of community c on object o is computed as follows: wo,c = Xs m(c) wo,s   Xs nm(c) wo,s (4) Where m(c) represents the set of members of community c, nm(c) the set of nonmembers of community c and wo,s the weight of object o in session s. The weights wo,c basically accounts for the requests made in the sessions existent in the members set minus the requests made in the sessions in the nonmembers set and can be used to evaluate the interest of each community on each object.
In order to be able to provide context for queries based on community information, we must also be able to effectively and ef -ciently assign a community for new sessions coming to the service.
Suppose there exists a set of previous sessions already classi ed in communities and that those sessions are representative enough to capture most of user variability.
The Folding-in method for SVD update can be used in order to compute authority weights for new sessions.
There exists some methods for SVD update in which the new included information contributes for the semantic latent structure identi ed, but there is a good trade-off between effectiveness and ef ciency is obtained in using the Folding-in method and periodically reevaluating a new set of representative sessions.
In this technique, the new information (i.e. new sessions) is seen as a projection into the already known semantic latent structure [8].
Let S   be an array representing the relationships between the sessions previously analyzed and the new one to be classi ed, denoted by s .
S   can be thought as a new column added to the original S matrix.
The relationships represented by S   are also computed using the cosine measure (Section 2.1) 1.
Then, the authority weight associated with session s  in each community (i.e. as ,c) can be computed as the projection of S   into the SVD space as follows: as ,c = S  T U  1 (5) where U and   are the matrices found in equation (3) for the set of sessions previously analyzed.
In this section we describe how to use community information to provide personalized searches for a content-based search engine relying on textual information.
The novel Information Retrieval (IR)
 partial information about a user session (e.g., in the case of a non nished session).
Query side k



 ...
iK ...
tK Root nodes ...
...
jO


 Object side Figure 1: Example of a belief network for query Q speci ed using the terms K1 and Ki model proposed in this paper for combining textual and community information uses Bayesian belief networks as a unifying framework.
Section 4.1 brie y introduces the Belief Network Model for Information Retrieval (IR) [5] and Section 4.3 shows how it can be used to combine community and content evidences in a single IR model.
A Bayesian Belief Network is a Directed Acyclic Graph (DAG) used to represent relationships among a set of random variables.
Nodes, in this graph, are used to represent random variables and the directed edges connecting them portray their relationships.
The amount of belief one has on the causal relationship between the random variables I and J, i   j, is be modeled by the conditional probability P (J = 1|I = 1) [20].
Bayesian Belief Networks are used in IR for they are a graphical formalism capable of representing independencies between variables of a joint probability distribution.
The idea is that the known independencies among random variables of a domain are explicitly declared and that a joint probability is synthesized from this set of declared independencies [23].
Therefore, the combination can be done in a modular way so that it does not require that the pieces of information be based on the same principles nor does it require any modi cations of them.
Although other works have proposed to merge together several pieces of information (e.g.
[6, 7, 10]) they are all binded to some sort of information modeling.
Figure 1 shows a belief network that can be used as a framework to model all of the classical models for textual retrieval of objects, more precisely, the Boolean, the Probabilistic and the Vector Space models [5].
In these models the objects being retrieved and the queries are represented by the use of index terms.
These are considered to be independent among themselves and are used as representations for elementary concepts of a conceptual space in which the documents are placed.
In the Belief Network Model, index terms are modeled by t nodes in the network, K1 to Kt, where t is the number of unique index terms being considered.
Vector k represents the conceptual space formed by them.
Objects and queries are also mapped to nodes (e.g.
Oj and Q).
The nodes enclosed in vector k are called root nodes.
They are given this special treatment since they are the ones responsible for triggering the evaluation process.
Given an instance of k, one can determine a similarity between the query and the objects.
network.
It should always be clear whether we are referring to the the object/query/term, the respective node in the network or the random variable associated with it.
Variable Oj is 1, denoted by oj, to indicate that Oj is active and Oj is 0, denoted by  oj, indicating that Oj is inactive.
Analogously, variable Q is 1, denoted by q, to indicate that Q is active and Q is 0, denoted by  q, indicating that Q is inactive.
Although this is not the only interpretation existent, a similarity function between object Oj and query Q can be computed by calculating P (Oj = 1 | Q = 1).
This is done as follows:
 In order to include community information in the ranking, the network of Figure 1 needs to be extended.
The extension we propose is considered to be modular since the inclusion of community information does not alter the way the original network was built.
This is because community information is used as a way to provide context [15] for the textual query speci ed by the user.
P (Oj = 1 | Q = 1) = P (oj | q) Original Network =  Xk P (oj | k) P (q | k) P (k) (6) Qt Qc where   is a normalizing constant [24].
In spite of each node in the network being associated with a binary random variable, the varying degrees of relevance of the classical methods are achieved by the correct assignment of the conditional probabilities in equation (6).
The  nal ranking is the summation of the similarities pointed out by each instance of the root nodes k.
It has been show that equation (6) can be used to represent the classical models for IR.
In our experiments (Section 5) we have used the Vector Space model [5] as an evidence of textual similarity between the objects and a query provided by the user.
Here, we review how to use the belief network framework presented in the previous Section in order to compute the Vector Space ranking, or simply vectorial ranking, for a certain user query.
To compute the vectorial ranking, using the belief network introduced, proper probabilities P (k), P (q | k) and P (oj | k) must be provided.
The probability P (k), also called the prior probability of k, associated with the root nodes is computed as follows: P (k) =   1 , if  i ti(Q) = gi(k) 0 , otherwise (7) where ti(Q) indicates whether the index term Ki was informed by the user in his query Q and gi(k) returns the value of the i-th random variable of k (i.e. Ki).
Equation (7) establishes that only the states of k where the query terms are active will be considered.
Analogously, P (q | k) is assigned to be: P (q | k) =   1 , if  i ti(Q) = gi(k) 0 , otherwise Finally, P (oj | k) is computed by: P (oj | k) = P wij   wiq qP w2 ij  qP w2 iq where the summations are evaluated over the whole set of terms and wij and wiq are tf-idf weights [5] relating the index term Ki with object Oj and query Q respectively.
By substituting equations (7), (8) and (9) in equation (6), we  nd that the similarity between object Oj and query Q, using the network of Figure 1, is evaluated as: k



 ...
iK ...
tK c



 ...
hC ...
cC ...
...
Ot j Ot 1 OtN ...
...
Ocj OcN Oc1 ...
O j Figure 2: Modular extension of the original belief network for query Q using the fact that the user was characterized as pertaining to community Ch Figure 2 shows the extended version of the original network.
As can be noted, new nodes had to be added to the original network to cope with the inclusion of community evidence: (i) node Qc is used to represent the community of the user that issued the query, (ii) a set of root nodes c that represents the set of communities being considered, (iii) a query node that is formed by the previous textual query and the context provided by the community, (iv) nodes to model the objects in terms of community relevance and (v) nodes to model the  nal results as a combination of the textual and community rankings.
We further associate binary random variables with each of the new created nodes analogously to what have been done in Section 4.1.
The ranking of the objects provided by each community summary, used to describe the main interests associated with each community (Section 3.1), will be considered as the evidence of similarity between the objects and the users  interest during this search.
The community weights of the objects were normalized within each community using decimal scaling in order to provide a value between 0 and 1 for this evidential source.
Moreover, only the positive fold of the community summary is used (i.e. objects with negative wo,c values will not have their content-based ranking values affected).
Since the new communities we identi ed are based on a probably noisy data source (clickthrough data) and are also based on incomplete sessions (on-the- y classi cation) we may expect a certain number of session misclassi cations.
The choice of considering solely the positive fold of community summaries is expected to reduce the loss in quality when a misclassi cation occurs since we only directly improve the content-based ranking of objects.
Analogously to Section 4.2, we can compute the  nal similarity of a contextualized query Q and object Oj by the value of P (Oj = (8) (9) (10) s(Q, Oj)   P wij   wiq qP w2 ij  qP w2 iq that is proportional to the cosine of the angle between the vectors representing query Q and object Oj over the conceptual space formed by the index terms.
P (Oj = 1 | Q = 1) = P (oj | q) =  Xk,c =  Xk,c =  Xk,c P (oj | k, c) P (q | k, c) P (k, c) P (oj | k, c) P (qt | k, c) P (qc | k, c) P (k) P (c) P (oj | k, c) P (qt | k) P (qc | c) P (k) P (c) (11) As noted before, this extension is modular, therefore, P (k), P (qt | k) are analogous to the probabilities assigned in Section 4.2.
We need, therefore, to de ne P (c), P (qc | c), P (oj | k, c).
The prior probabilities P (c) are computed as: P (c) =   1 , if  i ci(Qc) = gi(c) 0 , otherwise (12) where ci(Q) indicates whether community Ci is the one query Qc belongs to and gi(c) returns the value of the Ci random variable.
Moreover, we establish that only one community can be speci ed as context for a given query.
Although we de ne the properties of the network in a way that the context of a query is composed of only one community, this framework is general enough to allow extensions that consider more than one community as a context for a content-based search technique.
We set P (qc | c) analogously as: P (qc | c) =   1 , if  i ci(Qc) = gi(c) 0 , otherwise (13) At last, we assign P (oj | k, c) considering that it will use disjunctive or operation to provide the  nal ranking: P (oj | k, c) = 1   [(1   P (Otj | k, c))   (1   P (Ocj | k, c))] = 1   [(1   P (Otj | k))   (1   P (Ocj | c))] (14) where P (Otj | k) is analogous to P (oj | k) in Section 4.2 and P (Ocj | c) can be computed by: P (Ocj | c) =   swj,c
 , if wj,c > 0 , otherwise (15) where swj,c is the scaled weight associated with object Oj in community c.
By substituting equations (7), (8), (9), (12), (13), (13), (14), (15) in equation (11) we  nd that the similarity of object Oj and query Qt constrained to context Qc can be computed as follows: s(Q, Oj)     1   [(1   c(Qt, j))   (1   swj,c)] c(Qt, j)) , if wj,c > 0 , otherwise where c(Qt, j) is the cosine of the angle between the vectors describing textual query Qt and object Oj (equation (10)).
The dataset we used, in this paper, was collected from a real online bookstore service.
The high-level requests used to de ne user interests are the ones for information about books.
In each of these requests we were able to correctly identify the referred books.
Dealing with a search engine for books of an online bookstore is interesting for there are no explicit links or relationships among them.
Therefore, we are able to notice the real advantage of new algorithms designed to work with evidences other than link information for the Web.
The full dataset comprises two weeks of accesses to the service, collected from August 1st to August 14th of 1999.
This service is an e-commerce company, based in the US, specialized on Computer Science literature, operating only on the Internet.
In the period recorded, the bookstore received 3.5 million requests, 87,000 of which were requests for information about books, such as: authors, price, category and reviews.
This dataset was split into two others: a training and a test dataset.
The former comprises the  rst week of data collection and recorded 1.7 million requests with 26,000 sessions with at least one request for information about books identi ed.
This dataset was used in order to estimate communities of users accessing this service.
One of the bene ts of our approach allows us to decide the best number of communities based on the eigenvalues associated with the eigenvectors describing each community [2].
For this paper we arbitrarily identi ed the top 10 communities (i.e.
the ones with higher eigenvalues) so that the number of communities is small enough to be evaluated by humans.
The communities identi ed were named from C1 to C10.
The number of signi cant communities for this dataset lies somewhere between 25-30 communities.
The latter dataset comprised the second week of data collection, received 1.5 million requests and had 17,000 sessions with at least one request for information about books.
One of the main drawbacks associated with methods that use information from other users to characterize interest of other users based on similarity patterns is the problem of objects that have never been accessed before or have been accessed rarely.
A  rst analysis of the number of the sessions that made requests for books might lead to erroneous conclusion that the session in the test set received less requests for books than the training one.
What really happens is that we could only consider in the test set the books that had already been requested in the training set.
Since we use community information as contexts for queries, the only drawback of our approach is that these objects will never have their content-based ranking altered.
This follows a similar approach to the one taken in collaborative  ltering area [6, 17].
Each of these 17,000 sessions identi ed in the test dataset was classi ed into the top 10 communities identi ed from the training dataset.
The method we used is described in Section 3.2.
We found, on the period recorded, a total of 3,027 books for which requests were made.
Information such as the categories that each book belongs to and a summary of each were collected from the Amazon 2 online store.
The textual information considered in the content-based ranking technique is this summary describing the books.
In this section we will present the dataset used in this work and also the results for the combination of textual and community information in the  nal ranking.
Section 5.1 reviews the data collected from a service available on the Web that was used to test our approach.
Section 5.2 shows the analysis of the results produced by the novel ranking technique proposed.
To be of practical use, we must show that our technique can be used to effectively model new sessions based on a latent structure identi ed from sessions previously analyzed.
The  rst experiment we conducted was to: (i)  nd the best and worst communities for 2http://www.amazon.com
 % ( n o i t i s o
 k n a











 Worst Ranked Average Best Ranked Best Community Worst Community Figure 3: Normalized rank positions for the best ranked object, the worst and the average for all objects averaged over all queries considering the community rankings each session of the test dataset (i.e.
respectively the ones with highest and lowest as,c) and (ii) to analyze the positions in which the objects accessed in it appeared on these two community summaries.
Figure 3 shows the normalized ranking position for the best, the worst and the average object position on the best and worst community summaries averaged over the whole set of new sessions.
In the normalized ranking, the value 0 indicates that the object was found to be on the  rst position of the ranking while the 100 value indicates that the object was located at the last position of the ranking.
Suppose we were analyzing a dataset with ten objects and session s accessed objects o1, o2 and o3.
Moreover, suppose that the best and worst communities for s are cb and cw respectively.
If the summary of cb object o1 appeared in the third position, o2 appeared in the sixth position and o3 appeared in eighth position, the normalized ranking position of the best ranked object (Best Community s Best Ranked bar) would be 30%.
The normalized ranking position for the average object (Best Community s Average bar) would be 50% and the normalized ranking position for the worst ranked object would be 80%.
As can be noted the the majority of objects that co-occurred in these sessions really appeared on the  rst half of the ranking provided by the best community and on the second half of the ranking provided by the worst one.
Therefore, the values as,c can be used to characterize the participation of an object in a certain community and communities are truly able to characterize users with different interests since the two groups of bars diverge.
Information about the categories that each book belongs to was used to show that communities really separat the users in groups with different interests.
The weight of each book, computed as proposed in Section 3.1, was accumulated for each pair category community.
Table 1 shows the qualitative analysis of subjects covered by communities C5 and C10.
An in-depth look of all top 10 communities identi ed can be seen in [2].
As can be noted, these two communities represent sets of users with distinct interests.
While community C5 is speci cally interested in database technologies, community C10 aggregates users interested in low level questions basically related to Unix-like operating system s issues and the algorithm was able to identify these two distinct groups without the use of any a-priori information.
Best-ranked categories Speci c Databases; Database Management Systems; Database Design C10 Networking; Linux; Unix Operating Systems Worst-ranked categories Certi cation; Networking; Software Design Certi cation; Databases; Development Speci c Microsoft Table 1: Qualitative analysis for communities C5 and C10 Table 2 shows the top 10 books retrieved for a broad-topic query,  system administration , using the vectorial ranking alone and its combination with communities C5 and C10.
The vectorial ranking returns generic results with respect to the query covering several aspects of it, namely, operating systems, networking, database and ERP systems.
The results found by the combination of textual and community information show that community information is useful in focusing on a speci c subject.
From the results returned for community C5 seven of results are relevant for this community while only one could be found using the classical vectorial ranking.
For the combination with community C10 we found a similar relationship of 8 to 5.
Several other broad-topic queries showed similar results.
This experiment shows the utility of our methodology to cope with the problem of broad-topic queries in the Web [15].
These queries are usually composed of a small number of index terms that generally refer to several knowledge areas.
By providing context, in terms of the communities identi ed from user accesses, we are capable of producing results that exceed the quality of the ranking of typical content-based techniques.
We also performed another experiment to show that our algorithm is capable of providing better rankings given that community information is available.
A set of queries and relevant sets were synthetically created.
These queries were based on data from the logs recorded for the online bookstore.
The process of query creation was done as follows: for all s   T do if (|O(s)| > 1) then for all o   O(s) do create a new query: q; make q = title of book o; create a set of relevant objects for this query: r; make r = O(s) \ {o}; associate the best and worst communities for s with q; end for end if end for where T is the set of sessions identi ed in the test dataset and O(s) the set of objects accessed in session s. Using this procedure we were able to create 23,300 queries.
These queries were submitted to a database formed by the objects accessed in the test dataset.
For each query we considered the usual content-based ranking and two variations of our approach.
For the community-aware search engine experiment, we considered the communities in which the session that originated each query were ranked best and worst.
For instance, if query q was originated from session s, we would process three queries: (i) the pure textual query based solely on content, (ii) the content-based query in conjunction with community cb and (iii) the content-based query in conjunction with community cw.
Pocket Consultant Essential Windows NT System Administration UNIX System Administration Handbook (2nd Edition) Oracle8 Administration and Management AIX Version 4: System and Administration Guide Zero Administration for Windows HP-UX 11.x System Administration  How To  Book (2nd Edition) SAP R/3 System Administration : The Of cial SAP Guide Essential System Administration

 (Certi cation Series) (b) Vectorial & Community C5 Oracle Performance Tuning Tips and Techniques Oracle Certi ed Professional Application Developer Exam Guide Oracle8 Administration and Management Oracle Database Administration: The Essential Reference Sybase Dba Companion AIX Version 4: System and Administration Guide Oracle8 Advanced Tuning & Administration Oracle8: A Beginner s Guide HP-UX 11.x System Administration  How To  Book (2nd Edition)













 (c) Vectorial & Community C10 UNIX System Administration Handbook (2nd Edition) The Linux Kernel Book Operating System Concepts, 5th Edition HP-UX 11.x System Administration  How To  Book (2nd Edition) Linux: Companion for System Administrators

 Quick Reference UNIX System V: A Practical Guide (3rd Edition)



 Table 2: Results for the query  system administration  ) % ( n o i t i s o
 k n a









 Worst Ranked Average Best Ranked Best Community+ Vectorial Ranking Vectorial Ranking Worst Community+ Vectorial Ranking Figure 4: Normalized rank positions for the best ranked object, the worst and the average for all objects averaged over all queries considering the combination of the vectorial ranking with the community rankings Figure 4 shows the normalized ranking position for the relevant objects averaged over all queries using the content alone and the combination with both the best and worst communities.
On average, we get an improvement of 80% in terms of normalized ranking position when we combine textual information with the best community summary.
For the best ranked object the improvement is the highest, that means that the users are able to see the  rst relevant result for a given query in advance when we aggregate information about communities on the ranking technique.
The same happens for the worst ranked object and this suggests that we might  nd better precision values for 100% of recall.
Figure 5 shows precision  x recall plots for the vectorial ranking and its combination with the best community.
Although this synthetic query dataset is not appropriate for precision  x recall analysis we are able to see that the combined approach presents better precision values than the content-based technique alone.
The improvement reaches up to 48% in terms of average precision.
On average the number of relevant results for each query is too small since session sizes are small.
Therefore, the relevant sets are underestimated and little perturbations on the ranking position of a single object causes great change on the associated precision value.
Moreover, since queries are based on titles of books that generally contains many generic terms, the precision values we identify are quite low.
It is also interesting to note that the combination of textual information with the worst community (Figure 4) does not alter sig-ni cantly the results.
This is achieved since objects with negative wj,c values do not change the content-based ranking function.
This characteristic is important since, in most of the times, the information for computing the actual community of the user will be based on partial information and, therefore, will not be ideal.
There exists some related work in web log mining [3, 9, 18, 21].
Our work differs from those in the following ways: i) Our method makes use of high-level application-oriented information (e.g., requests for books or the songs), to detect common interests among users, instead of looking for patterns of user s accesses; ii) Some
 Vectorial Ranking

 ) % (
 i i n o s c e r









 Recall (%) Figure 5: PrecisionxRecall values for the combined ranking using the best community methods use simple object relationships [21], while other methods consider the order at which requests are made [3, 9] (e.g., using Markov models as a framework for modeling user interactions).
Those methods would not work for our high-level datasets since most of the sessions have small sizes, 1 or 2 requests at most.
iii) Our approach differs from classical clustering techniques, for we try to identify densely connected user sessions that may not account for much of the whole dataset while clustering techniques try to split the full dataset into a few subsets.
There has also been some work on the combination of rankings using a Bayesian approach.
In [24] the authors combined textual information of a Web page with the one induced by its hyperlink connections.
Our work differs from the previous in two ways: (i) we use implicit information provided by user behavior in order to create a graph structure capable of representing relationships between user interactions.
Although each user behaves independently from others we were able to  nd structures that are similar to other ones explicitly created (e.g.
small-worlds), (ii) the modeling effort on their work and ours is different since the hyperlink information used by then is somewhat related to the query terms while in our approach we model an evidential source that is independent from the textual information provided by the user in terms of his query.
A Bayesian model similar to the one used in [24] was also proposed in [25].
Although the modeling in these two works are slightly different it has already been shown that they exhibit similar expressive power.
In [10], the authors also propose a way to combine two information sources , the link-based information and the contents of the pages, in a single model.
The approach taken by the authors is different from ours and from the others that are based on Bayesian Networks on the sense that this work tries to unify both sources using a single principle to evaluate them.
They use variants of Latent Semantic Analyses (LSA) in order to evaluate the texts and links of the documents and then combine the results easily since the analyses were made based on the same principled manner.
Although the combination produces good results it is rigidly connected to LSA.
The previous personalized search techniques for the web, such as [13, 16, 22], have mostly focused on how to gather information from a single user in order to provide context for his searches.
Generally they are based on the creation of user pro les and make use of explicit actions from the users.
In our approach we are able to combine information about several users together since the analysis is made in the servers and we are able to transparently provide context for the queries.
Another difference from our approach to the others is that we only considered, so far, the actual interaction of the user with the service what means that we do not have to analyze changes on user interest but are subjected to less information for contextualization purposes.
In the area of recommendation algorithms several authors have shown the bene ts of combining several sources of information in a single result as a way to improve the quality of the systems [6, 7,
 means to learn a retrieval function that binary classi es movies with respect to a user (e.g.
like or dislike).
In [7] the authors use several textual information sources in order to recommend technical papers to be conference reviewers.
Although their analysis is based on the same several sources of the same type they have shown that the combination of several information sources produced results that were better than the ones produced by the single sources alone.
In [17] the authors have shown that one can use information from the objects (e.g., text) in order to enhance the quality of the recommendations made by traditional collaborative  ltering approaches based on previous user relevance judgments.
The technique introduced in [14] uses clickthrough data in order to improve the quality of searches on the Web.
The main difference between his work and ours is that he is only able to enhance the quality of the searches to a single user or groups of user that share common interests.
Besides, the author works with information coming from the answers of queries to search engines and builds upon relative preferences between the documents returned.
Our approach uses information from the whole sets of users to  rst characterize the user interest and then uses this information as context for subsequent queries of this user.
Personalized search services will play a major role on the Web in the near future.
In this type of application, a search for a certain textual query may return different results depending on context information (e.g.
the user issuing the query, his current interests).
In this paper we propose and evaluate a novel ranking technique that uses community information as a new evidential source for providing personalized ranking.
Our approach is to use communities as contextualization cues for the queries.
The framework proposed is general enough to allow the use of any of the classical models for content-based information retrieval and of most of community identi cation algorithms, as long as summaries for the communities can be produced.
In the experiments conducted in this paper we were able to provide an improvement of 48% in term of average precision, and even better results if we were to consider the position occupied in the ranking by the relevant objects for each query.
As future work on this area we envision: (i) adaptation of the framework for other alternative techniques for content-based ranking and community identi cation algorithms; (ii) experimentation with a real real (operational and interactive) system to demonstrate the effect of our method; (iii) the experimentation with partial session information as a way to characterize user interests; (iv) the expansion of the framework to work with more than one community as an evidential source for relevance.
In order to tackle problems that might occur while using partial session information.
The use of multiple communities may bene t from the fact that we have a value as,c describing the participation of s in each community c.
Although the synthetic query dataset used in this paper is useful in demonstrating the ability of the combination proposed to im-420prove the quality of the searches, more detailed studies could be conducted if a better query dataset were available.
The authors would like to thank the anonymous service owners and operators for enabling this research to proceed by providing us access to their logs.
This work has been partially supported by a number of grants from CNPq-Brazil.
