A taxonomy, or directory or catalog, is a division of a set of objects (documents, images, products, goods, services, etc.)
into a set of categories.
There are a tremendous number of taxonomies on the web, and we often need to integrate objects from a source taxonomy into a master taxonomy.
This problem is currently pervasive on the web, given that many websites are aggregators of information from various other websites [2].
A few examples will illustrate the scenario.
A web marketplace like Amazon 1 may want to combine goods from multiple vendors  catalogs into its own.
A web portal like NCSTRL 2 may want to combine documents from multiple libraries  directories into its own.
A company may want to merge its service taxonomy with its partners .
A researcher may want to merge his/her bookmark taxonomy with his/her peers .
Singapore-MIT Alliance 3, an innovative engineering education and research collaboration among MIT, NUS and NTU, has a need to integrate the academic resource (courses, seminars, reports, softwares, etc.)
taxonomies of these three universities.
This problem is also important to the emerging semantic web [4], where data has structures and ontologies describe the semantics of the data, thus better enabling computers and people to work in cooperation.
On the semantic web, data often come from many information processing across different ontologies, and ontologies the semantic mappings between taxonomies are central components of ontologies, ontology mapping necessarily involves finding the correspondences between two taxonomies, which is often based on integrating objects from one taxonomy into the other and vice versa [8, 15].
is not possible without knowing them.
Since If all taxonomy creators and users agreed on a universal standard, taxonomy integration would not be so difficult.
But the web has evolved without central editorship.
Hence the correspondences between two taxonomies are inevitably noisy and fuzzy.
For 1 http://www.amazon.com/ 2 http://www.ncstrl.org/ 3 http://web.mit.edu/sma/
 and Yahoo 5 : what is  Arts/ Music/ Styles/  in one may be  Entertainment/ Music/ Genres/  the other, category  Computers_and_Internet/ Software/ Freeware  and category  Computers/ Open_Source/ Software  have similar contents but show nontrivial differences, and so on.
It is unclear if a universal standard will appear outside specific domains, and even for those domains, there is a need to integrate objects from legacy taxonomy into the standard taxonomy.
in this process would be Manual taxonomy integration is tedious, error-prone, and clearly not possible at the web scale.
A straightforward approach to automating it as a classification problem which has being well-studied in machine learning area [18].
In this paper, we attempt to use a powerful classification method, Support Vector Machine (SVM) [7], to attack this problem.
to formulate Our key insight is that the availability of the source taxonomy data could be helpful to build better classifiers in this scenario, therefore it would be beneficial to do transductive learning rather than inductive learning, i.e., learning to optimize classification performance on a particular set of test examples.
Noticing that the categorizations of the master and source taxonomies often have some semantic overlap, we propose a method, Cluster Shrinkage (CS), the classification by exploiting such implicit knowledge.
Our experiments with real-world web data show substantial the performance of taxonomy integration.
to further enhance improvements in The rest of this paper is organized as follows.
In  2, we give the formal problem statement.
In  3, we describe a state-of-the-art solution.
In  4, we present our approach in detail.
In  5, we conduct experimental evaluations.
In  6, we review the related work.
In  7, we make concluding remarks.
Now we formally define the taxonomy integration problem that we are solving.
Given two taxonomies:   a master taxonomy M with a set of categories
 ,



 each containing a set of objects, and   a source taxonomy N with a set of categories

 ,


 each containing a set of objects, we need to find the category in M for each object in N.
To formulate taxonomy integration as a classification problem, we take C as classes, the objects in M as training
 ,


 examples, the objects in N as test examples, so that taxonomy integration can be automatically accomplished by predicting the class of each test example.
It is possible that an object in N belongs to multiple categories in M. Besides, some objects in N may not fit well in any existing category in M, so users may want to have the option to form a new category for them.
It is therefore instructive to create an ensemble of binary (yes/no) classifiers, one for each category 4 http://www.google.com/ 5 http://www.yahoo.com/ C in M. When training the classifier for C , an object in M is labeled as a positive example if it is contained by C or as a negative example otherwise.
All objects in N are unlabeled and wait to be classified.
This is called the  one-vs-rest  ensemble method.
Taxonomies are often organized as hierarchies.
In this paper, we focus on flat to hierarchical taxonomies is straightforward and will be discussed later.
taxonomies.
Generalizing our approach

 Agrawal and Srikant recently proposed an elegant approach to taxonomy integration by enhancing the Na ve Bayes algorithm [2].
is a well-known The Na ve Bayes (NB) algorithm text classification technique [18].
NB tries to fit a generative model for documents using training examples and apply this model to classify test examples.
The generative model of NB assumes that a document is generated by first choosing its class according to a prior distribution of classes, and then producing its words independently according to a (typically multinomial) distribution of terms conditioned on the chosen class [16].
Given a test document d , NB predicts its class to be arg max Pr[ C d .
The posterior probability Pr[ C d can be computed via Bayes s rule: | Pr[ , C d = C d Pr[ ] Pr[ ] d =   ( Pr[ ]
   w d n d w is the number of occurrences of w in d .
The where ( , ) ]C can be estimated by the proportion of training probability Pr[ documents in C .
The probability Pr[ ]w C can be estimated by ] ] Pr[
 Pr[ ] d ) ( ] n d w w C d C d C ]Pr[ Pr[ Pr[ Pr[   ,
 = ] ] ] ] | | | | | |
 ) , + ( ) n C w ( , , ( n C w i   + )   , where n C w is ( ) , the number of i )   w V   occurrences of w in training documents in C , V is the   is the Lidstone s smoothing vocabulary of terms, and 0 parameter [1].
Taking logs, we see that NB is actually a linear classifier: 1 < (   log Pr[ =     w d ]C d | ( ( , ) n d w
 log Pr[   log Pr[   ] | w C   w d ] ( ) Pr[ + ) ] n d w ( , ) w C | ) log Pr[
 ] .
The enhanced Na ve Bayes (ENB) algorithm [2] uses the categorization of the source taxonomy to get better probability estimations.
Given a test document d that is know to be in category S in N, ENB predicts its category in M to be C d S can arg max Pr[ C d S .
The posterior probability Pr[ ] ] , , | |
 be computed as Pr[ C d S ] , | = , , Pr[ Pr[ , d S C d S ] ] =
 Pr[ ]Pr[ Pr[ , , C d S d S ] | ] .
ENB invokes a simplification that assumes d | , ] Pr[ C d S   and S are independent given C , therefore Pr[ = d S C ) C d S ] Pr[ ) ( , ] n d w ]   ] w C

 = ( Pr[ Pr[ Pr[ .
] , , | | | | |   w d = Pr[
 | ] Pr[ d C | ]
 NB.
For the probability Pr[ w C can be estimated in the same way of C S , ENB estimates it by ] ] | |      
 i  
 ) (
 i
 i  
 , where C is the number of documents
     S  is the number of documents in S classified into in C , C 0  is a parameter reflecting the C by the NB classifier, and degree of semantic overlap between the categorizations of M and N. Taking logs, we see that ENB is still a linear classifier: ( log Pr[ =   , C d S | ] ( ( , n d w   w d   log Pr[   log Pr[ )
 |   ] ) + ]   w d ( Pr[ w C | ) ] n d w ( , ) ) w C | log Pr[
 | ] .
=   + i f ) ( b x ( ) w x ix is { iy     y f x where i , where The decision function of an SVM is  w x is the dot product between w (the normal vector to the hyperplane) and x (the feature vector representing an example).
}1,1 The margin for an input vector ix .
In the linear case, the margin is is the correct class label for geometrically the distance from the hyperplane to the nearest positive and negative examples.
Seeking the maximum margin can be expressed as an quadratic optimization problem: i  .
When minimizing positive and negative examples are linearly inseparable, soft-margin SVM tries to solve a modified optimization problem that allows but penalizes the examples falling on the wrong side of the hyperplane.
 w w subject to   ) 1 w x +   , b y ( i i Comparing the classification functions of NB and ENB, it is obvious that all ENB does is to shift the classification threshold of its base NB classifier, no more and no less.
Here we present our approach in detail.
In  4.1, we review Support Vector Machine (SVM).
In  4.2, we review transductive learning and explain why it is more suitable to our task.
In  4.3, we propose the Cluster Shrinkage (CS) method and analyze its effect.
In  4.4, we compare our approach with ENB.
Support Vector Machine (SVM) [7, 13] is a powerful classification method which has shown outstanding classification performance in practice.
It is based on a solid theoretical foundation   structural risk minimization [24].
In its simplest linear form, an SVM is a hyperplane that separates the positive and negative training examples with maximum margin, as shown in Figure 1.
Large margin between positive and negative examples has been proven to lead to good generalization [24].
A regular SVM tries to induce a general classifying function which has high accuracy on the whole distribution of examples.
However, this so-called inductive learning setting is often unnecessarily complex.
For in taxonomy integration situations, the set of test examples to be classified are already known to the learning algorithm.
In fact, we do not care about the general classifying function, but rather attempt to achieve good classification performance on that particular set of test examples.
This is exactly the goal of transductive learning [25].
the classification problem Transductive SVM (TSVM) introduced by Joachims [14] extends SVM to transductive learning setting.
A TSVM is essentially a hyperplane that separates the positive and negative training examples with maximum margin on both training and test examples, as shown in Figure 2.
Figure 1: An SVM is a hyperplane that separates the positive and negative training examples with maximum margin.
The examples closest to the hyperplane are called support vectors (marked with circles).
Figure 2: A TSVM is essentially a hyperplane that separates the positive and negative training examples with maximum margin on both training and test examples (cf.
Figure 1).
Why can TSVM be better than SVM?
There usually exists a clustering structure of training and test examples: the examples in same class tend to be close to each other in feature space.
As explained in [14], it is this clustering structure of examples that TSVM exploits as prior knowledge to boost classification performance.
This is especially beneficial when the number of training examples is small.
TSVM) assume that both the training and test examples come from the identical data distribution.
This assumption does not necessarily hold in the case of taxonomy integration.
Intuitively, TSVM seems to be more robust than SVM to the violation of this assumption, since TSVM takes the test examples into account for learning.
This interesting issue needs to be stressed in the future.
Applying TSVM, we can effectively use the objects in N (test examples) to boost classification performance.
However, thus far we have completely ignored the categorization of N.
identical, Although M and N are usually not their categorizations often have some semantic overlap.
Therefore the categorization of N contains valuable implicit knowledge about the categorization of M. For example, if two objects belong to the same category S in N, they are more likely to belong to the same category C in M rather than to be assigned into different categories.
We hereby propose a method, Cluster Shrinkage (CS), to further enhance the classification by exploiting such implicit knowledge.
for each category S { compute its center: c S x for each example =   replace it with x c '   ; 1  where 0 } }
 =  
 x { +  
 (1 x ;   ) x , Figure 3: The Cluster Shrinkage algorithm.
Figure 4: The Cluster Shrinkage process.
Since the success of TSVM relies on the clustering structure of examples, we intend to use the categorization information in the taxonomies to strengthen this clustering structure and thus help TSVM to find better classification.
This can be achieved by treating each category S (or C ) as a cluster and shrinking it.
Figure 3. presents our proposed Cluster Shrinkage (CS) algorithm, and Figure 4. depicts its process.
  =   c +     ) x (1 x is actually a linear interpolation The formula of the example x and its category s center c .
When an example x belongs to multiple categories whose centers respectively, the above formula should be are         amended to
 hg +     =   ) x .
  = ,..., ,...,       (1 x


 c c c c ( 2) ( 2) (1) (1) ( h ) ( g , ( g ) ,
 g ) Our approach to taxonomy integration is in three steps: first apply CS on all objects in M and N, then train TSVMs on these objects, finally use the learned TSVMs to classify the objects in N into the categories in M. We name this approach CS-TSVM.
We first study the effect of CS in inductive learning setting.
Denoting the Euclidean distance between two examples (vectors) with function , we can get the following theorem.
d     ( , ) , suppose the center of S S x  x , then x c .
( , ) (     = (1   x c ) ( , ) d THEOREM 1.
For any example is c , CS makes x become   x c , ) d d Proof:   = +       x , we get c x Since ) (     = +   =       c x x x c ) , ) d       =     = =   x c x c )( ) ) (1 1    , we get Since 0 1      , (1
 x c ) ( , ) d (1 c   (1 (1   d ( x c .
( , )   ) (1 c     ) ( , ) d x c .
From the above theorem, we see that CS is actually moving all examples in a category towards their center.
Hence, applying CS on the objects in M (training examples) would make SVM behave alike the Rocchio algorithm [3, 22], which is not going to provide much help because Rocchio is not as powerful as SVM..
Given a linear classifier following theorem.
f x ( ) =   w x + b , we can get the S x  x , then , suppose the center of S f ( x (1 c ( )   )   f +   THEOREM 2.
For any example is c , CS makes x become   = x .
) ( ) f Proof:   = +     c x Since (1       + = w x x ) ( b f   +       = (1 ) w c ) ( + +       = (1 b w c +   =     x .
c ( ) ( ) ) (1 f w   w x (   ) x , we get   ) =   f +       x c (1 ( ) )     +   +
 ( )b ) +   w x b + b From the above theorem, we see that applying CS on the objects in N (test examples) can push these objects to get classifying function outputs more similar to those of their category centers.
However, in inductive learning setting, the objects in N (test examples) are not involved in construction of the classifiers, i.e.,
 change the classifiers.
Therefore the benefits of CS to inductive learning algorithms for taxonomy integration would be limited.
This thought has been confirmed by our experiments of CS-SVM (the combination of CS and SVM).
We then study the effect of CS in transductive learning setting.
S x and 2x become THEOREM 3.
For any pair of examples suppose the center of S is c , CS makes and ( S x 1x and   ) ( d (1   .
d ) ( )

 x x
 ,
 ,  x


 ,     c   = )  x respectively, then
   x x x x , d

 Proof: +   x and x Since (1

 (             c x x x ) , d

           = x (1 )( ) (1   , we get 1  Since 0 1      , ) ( (1 d
   = =   ) =   x
 = x x x
 ( ) ,



   c   x ) x
 (1   = +     x
 x
 +     x , we get ) (1
 ) ) ( +       c (1   =   ) ( d   ) (1 x x

 x ) ,

   d ( x x
 ,
 ) .
) From the above theorem, we see that CS lets all examples in a category become closer to each other.
Because TSVM seeks the maximum margin hyperplane (the thickest slab) in both training and test examples, making the examples in category S closer to each other directs TSVM to avoid splitting S .
Consequently applying CS on the objects in N (test examples) guides TSVM to reserve the original categorization of N to some degree while doing classification, as shown in Figure 5.
On the other hand, applying CS on the objects in M (training examples) meanwhile can reduce TSVM s dependence on training examples and put more emphasis on taking advantage of the information in N.
Figure 5: A CS-TSVM attempts to reserve the original categorization of the source taxonomy to some degree while doing classification (cf.
Figure 2).
To sum up, the CS-TSVM approach can not only make effective use of the objects in N like TSVM, but also make effective use of the categorization of N.
controls the strength of the The CS parameter 0 clustering structure of examples.
Increasing   results in more   1  influence of the categorization information on classification.
When category in N as a whole into a specific category in M. When
 value of   is set appropriately, CS-TSVM should never be worse than TSVM because it includes TSVM as a special case.
The optimal value of   can be found using a tune set (a set of objects whose categories in both taxonomies are known).
The tune set can be made available via random sampling or active learning, as described in [2].
, 1  Another way to incorporate the categorization of N into TSVM S as binary is to treat the source category labels



  x by appending features, and expand each feature vector x to extra columns for these label features.
Similarly a parameter   can be used to decide the relative importance of
 category and ordinary features: category features are scaled by factor   and ordinary features are scaled by 1   .
This method looks simpler, but it does not leverage as much categorization information as CS.
For illustration, consider two different 2c respectively, categories 1 = , given two examples , while CS the above simpler method would get would provide a more reasonable dot product function =

 x
 1c and , let parameter     x

  x   x and =
  




    c
 c
 .
One salient property of SVM / TSVM is that the only operation it requires is the computation of dot products between pairs of examples.
One may therefore replace the dot product with a Mercer kernel [7], implicitly mapping feature vectors in   into a higher dimensional space  (cid:1) , and applying the original algorithm in this new space.
Using a nonlinear kernel (e.g., polynomial, rbf or sigmoid) enables SVM / TSVM to get nonlinear classification in   , thus greatly promoting the power of


 ) x =   (  x ) Suppose for SVM / TSVM we use a nonlinear kernel k   , where   is a nonlinear map from   to  (cid:1) .
( The idea of Cluster Shrinkage in  (cid:1) is to replace each feature , where vector

 is the center of S in  (cid:1) .
We are usually unable ( ) x in category S with =     x ) ( )     x ( ) +     x ( )   c (1 = c   (cid:1) (cid:1) x
 ( ) x in  (cid:1) , because the to explicitly express a feature vector dimension of  (cid:1) is extremely large or even infinite.
However, CS in  (cid:1) can still be achieved implicitly by replacing the kernel k with the following one.
          x x ( ) ( ( ) k
 ) (     =     x c x ) ( ) ) ( ) ( +         ) ( (1 )       +   =   ( ) x x ,
 (   c (1
 ( +     c ( (1
         c c (
 +   (1   c ( ) )   ) ( ) +  
 x
 (1 = )   x x x ( ) ) ( ) ) ) ) )










 (cid:1) (cid:1)

 +   c (
     ( ) x
 ) ) ) )   ( +         x ( (1 =     c c , ) ( k
 +   +     (1 )   c (
 +   (1 c x ( ,
 ) ( k )



 k x x ( ,

 c x , (

 ) ) ) Note that in the above formula, we have approximated the center .
k (cid:1) c

   x ( ) , with =   of category S in  (cid:1) ,   =  
     c ( )    
     k  x x Although it is possible to derive a strict formula of ) ( without this approximation, it would be computationally more expensive.
In this way, we are able to implement CS for nonlinear SVM / TSVM efficiently.
   
 x .
,   x x



 As mentioned before, taxonomies are often organized as hierarchies.
Although it is possible to flatten the hierarchy to a single level [2], past studies have shown that exploiting the hierarchical structure can lead to better classification results [5,
 can be easily extended then incorporate the hierarchical version of CS.
For instance, consider a two-level taxonomy H where jS , jc , for , one reasonable way to achieve hierarchical CS using a x with each is as follows: first compute parameter and x   +   c ) replace 1    .
to hierarchical TSVM and jkS is a subcategory of jkc and the center of x using a parameter 0 suppose the center of 1      )   =   c ik then
 +   jkS is jS is   c     (1 (1 = , x

 c     ik ik i i ik
 Although ENB [2] has been shown to work well for taxonomy integration, we think an approach based on SVM but not NB is still attractive.
In contrast to NB, SVM is a discriminative classification method, i.e., SVM does not posit a generative model but attempt to find the best classifying function directly.
It is generally believed that SVM is more promising than NB for text classification [10, 26], and SVM has been successfully applied to many other kinds of data such as images [7].
Both ENB and CS-TSVM exploit the categorization of N to enhance classification.
While all ENB does is to shift the classification threshold of its base NB classifier (see  3), CS-TSVM has the ability to adjust the direction of the classification hyperplane of its base TSVM classifier.
Moreover, CS-TSVM has the potential to be extended to achieve nonlinear and hierarchical classifications.
Although CS-TSVM looks more effective, ENB still has the advantage in efficiency.
We conduct experiments with to demonstrate the advantage of our proposed CS-TSVM approach to taxonomy integration.
real-world web data,
 We have collected 5 datasets from Google and Yahoo.
One dataset includes the slice of Google s taxonomy and the slice of Yahoo s taxonomy about websites on one specific topic, as shown in Table 1.
Table 1: The datasets.
Book Google / Top/ Shopping/ Publications/ Books/ Disease Movie / Top/ Health/ Conditions_and_Diseases/ / Top/ Arts/ Movies/ Genres/ Music / Top/ Arts/ Music/ Styles/ / Top/ News/ By_Subject/ Yahoo / Business_and_Economy/ Shopping_and_Services/ Books/ Bookstores/ / Health/ Diseases_and_Conditions/ / Entertainment/ Movies_and_Film/ Genres/ / Entertainment/ Music/ Genres/ / News_and_Media/ News In each slice of taxonomy, we take only the top level directories as categories, e.g., the  Movie  slice of Google s taxonomy has categories like  Action ,  Comedy ,  Horror , etc.
For each dataset, we show in Table 2 the number of categories occurred in Google and Yahoo respectively.
Figure 6: An object (listed item) corresponds a website on the world wide web, which is usually described by its URL, its title, and optionally a short annotation about its content.
For each dataset, we show in Table 3 the number of objects occurred in Google (G), Yahoo (Y), either of them (G Y), and both of them (G Y) respectively.
The set of objects in G Y covers only a small portion (usually less than 10%) of the set of objects in Google or Yahoo alone, which suggests the great benefit of automatically integrating them.
This observation is consistent with [2].
The number of categories per object in these datasets is 1.54 on average.
This observation confirms our previous statement in  2 that an object may belong to multiple categories, and justifies our Table 2: The number of categories.
Google Yahoo









 Book Disease Movie Music News In each category, we take all items listed on the corresponding directory page and its sub-directory pages as its objects.
An object (listed item) corresponds to a website on the world wide web, which is usually described by its URL, its title, and optionally a short annotation about its content, as illustrated in Figure 6.
477strategy to build a binary classifier for each category in the master taxonomy.
Table 3: The number of objects.
Google




 Yahoo
















 Book Disease Movie Music News The category distributions in all theses datasets are highly skewed.
For example, in Google s Book taxonomy, the most common category contains 21% objects, but 88% categories contain less than 3% objects and 49% categories contain less than 1% objects, as shown in Figure 7.
In fact, skewed category distributions have been commonly observed in real-world applications [26].
n o i t r o p o r p s t c e b o j






 category rank (from common to rare) Figure 7: The category distribution of Google s Book taxonomy.
For each dataset, we pose 2 symmetric taxonomy integration tasks: G Y (integrating objects from Yahoo into Google) and Y G (integrating objects from Google into Yahoo).
As described in  2, we formulate each task as a classification problem.
The objects in G Y can be used as test examples, because their categories in both taxonomies are known to us [2].
We hide the test examples  master categories but expose their source categories to the learning algorithm in training phase, and then compare their hidden master categories with the predictions of the learning algorithm in test phase.
Suppose the number of the test examples is n. For G Y tasks, we randomly sample n objects from the set G-Y as training examples.
For Y G tasks, we randomly sample n objects from the set Y-G as training examples.
This is to simulate the common situation that the sizes of M and N are roughly in same magnitude.
For each task, we do such random sampling 5 times, and report the classification performance averaged over these 5 random samplings.
For each object, we assume that the title and annotation of its corresponding website summarizes its content.
So each object can be considered as a text document composed of its title and annotation.
=x The most commonly used feature extraction technique for text data is to treat a document as a bag-of-words [13, 14].
For each document d in a collection of documents D , its bag-of-words is first pre-processed by removal of stop-words and stemming.
Then ix it is represented as a feature vector x x
 iw (the i-th distinct indicates the importance weight of term word occurred in D ).
Following the TF IDF weighting scheme, we set the value of ix to the product of the term frequency TF w d and the inverse document frequency IDF w , i.e., TF w d means the , iw in d .
The inverse document TF w d number of occurrences of .
The term frequency IDF w i , where ,..., )m ) ) ( ( )i , , i i ( , ( i ( ) x
 ( )   frequency is defined as IDF w i ( ) = log      
 ( DF w i , where D is the total number of documents in D , and of documents in which normalized to have unit length.
DF w is the number iw occur.
Finally all feature vectors are (     )   )i ) (
 = pr + p r
 As stated in  2, it is natural to accomplish a taxonomy integration task via an ensemble of binary classifiers, each for one category in M. To measure classification performance, we use the standard F-score (F1 measure) [3].
The F-score is defined as the harmonic average of precision (p) and recall (r), , where precision is the proportion of correctly
 predicted positive examples among all predicted positive examples, and recall is the proportion of correctly predicted positive examples among all true positive examples.
The F-scores can be computed for the binary decisions on each individual category first and then be averaged over categories.
Or they can be computed globally over all the M n  binary decisions where M is the number of categories in consideration (the number of categories in M) and n is the number of total test examples (the number of objects in N).
The former way is called macro-averaging and the latter way is called micro-averaging [26].
It is understood that the micro-averaged F-score (miF) tends to be dominated the classification performance on common categories, and that the macro-averaged F-score (maF) is more influenced by the classification performance on rare categories [26].
Since the category distributions are highly skewed (see  5.1), providing both kinds of scores is more informative than providing either alone.
We use our own implementation of NB and ENB.
The Lidstone s smoothing parameter   is set to an appropriate value 0.1 [1].
The performance of ENB would be greatly affected by its parameter  .
We run ENB with a series of exponentially increasing values of  : (0, 1, 3, 10, 30, 100, 300, 1000) [2] for 478each taxonomy integration task, and report the best experimental results.
We use SVMlight6 for the implementation of SVM / TSVM [13,
 parameters except  j  and  p .
The parameter  j  is set to the ratio of negative training examples over positive training examples, thus balance the cost of training errors on positive and negative examples.
The parameter  p  used in TSVM means the fraction of test examples to be classified into the positive class.
To estimate the value of  p , we first run SVM and get  p (the fraction of test examples predicted to be positive by SVM), then we set the value of  p  to a smoothed version of  p : , where   is set to 99% in our experiments.
    +    p (1     ) 0.5 The CS algorithm is simple to implement and executes quickly.
It only requires one sequential scan to compute the cluster centers and another sequential scan to reposition the examples.
In all our CS-SVM and CS-TSVM experiments, the CS parameter   is set to 0.5.
Fine-tuning   using tune sets would decisively generate better results than sticking with a prefixed value.
In other words, the performance superiority of applying CS technique is underestimated in our experiments.
Table 4: Experimental Results of NB and ENB.
Book Disease Movie Music News Book Disease Movie Music News maF









 miF









 maF









 miF









 Table 5: Experimental Results of SVM and TSVM.
Book Disease Movie Music News Book Disease Movie Music News maF









 miF









 maF









 miF









 6 http://svmlight.joachims.org/ The experimental results of SVM and TSVM are shown in Table
 The experimental results of NB and ENB are shown in Table 4.
We see that ENB really can achieve much better performance than NB for taxonomy integration.
Table 6: Experimental Results of CS-SVM & CS-TSVM



 Book Disease Movie Music News Book Disease Movie Music News maF









 miF











 maF









 miF



















 k o o
 e s a e s
 i i e v o
 i c s u
 s w e
 k o o
 e s a e s
 i i e v o
 i c s u
 s w e


 Figure 8: Comparing the macro-averaged F-scores of ENB and CS-TSVM.
k o o
 e s a e s
 i i e v o
 i c s u
 s w e
 k o o
 i e v o
 i c s u
 s w e
 e s a e s
 i

 Figure 9: Comparing the micro-averaged F-scores of ENB and CS-TSVM.
effective use of the objects in N to enhance classification.
The experimental results of CS-SVM and CS-TSVM are shown in Table 6.
Comparing the experimental results of CS-SVM and SVM, it turns out that in inductive learning setting the CS technique can not provide much help to taxonomy integration.
In contrast, CS-TSVM greatly improves TSVM in the performance of taxonomy integration.
This implies that the real power of CS-TSVM comes from the marriage of CS and TSVM but not either alone.
The experimental results of ENB and CS-TSVM are compared in Figure 8 and 9.
It is clear that CS-TSVM outperforms ENB consistently and significantly.
is to identify (typically one-to-one)

 Most of the recent research efforts related to taxonomy integration are in the context of ontology mapping on semantic web.
An ontology specifies a conceptualization of a domain in terms of concepts, attributes, and relations [11].
The concepts in an ontology are usually organized into a taxonomy: each concept is represented by a category and associated with a set of objects (called the extension of that concept).
The basic goal of ontology mapping semantic correspondences between the taxonomies of two given ontologies: for each concept (category) in one taxonomy, find the most similar concept (category) in the other taxonomy.
Many works in this field use a variety of heuristics to find mappings [6, 17, 19,
 to further automate the ontology mapping process [8, 12, 15, 20,
 them derive similarities between concepts (categories) based on their extensions (objects) [8, 12, 15], therefore they need to first integrate objects from one taxonomy into the other and vice versa (i.e., taxonomy integration).
So our work can be utilized as a basic component of an ontology mapping system.
As stated in  2, taxonomy integration can be formulated as a classification problem.
The Rocchio algorithm [3, 22] has been applied to this problem in [15]; and the Na ve Bayes (NB) algorithm [18] has been applied to this problem in [8], without exploiting taxonomy.
To our knowledge, the most advanced approach to taxonomy integration is the enhanced Na ve Bayes (ENB) algorithm proposed by Agrawal and Srikant [2], which we have reviewed and compared with our approach.
the source information in

 Our main contribution is to show that the implicit knowledge in the source taxonomy can be effectively exploited to boost taxonomy integration by marrying Cluster Shrinkage (CS) and Transductive Support Vector Machines (TSVM).
The future work may include: looking for methods to accelerate the proposed CS-TSVM approach, incorporating commonsense knowledge and domain constraints into the taxonomy integration process, extending to full-functional ontology mapping systems, and so forth.
We would like to thank the anonymous reviewers for their helpful comments and suggestions.
