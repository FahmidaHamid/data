Just a decade ago, large-scale  ows of information such as news-feeds were owned, monitored, and  ltered by organizations specializing in the provision of news.
The Web has brought the challenges and opportunities of managing and absorbing newsfeeds to all interested users.
We have pursued mathematical tools and user-interface  The research described herein was conducted while the  rst author was on a summer internship with Microsoft Research, during the summer of 2003.
Copyright is held by the author/owner(s).
designs that can assist people with extracting the most relevant information from news sources.
One approach to navigating a large corpus of news information is to identify differences and similarities between various decompositions of the collections.
Such dif-ferencing machinery could be useful for distinguishing what is being said for the  rst time from what has already been mentioned, as well as in revealing differences of opinion on the issues covered.
Identifying  important  information has been an essential aspect of studies on Web search and text summarization.
Search methods focus on identifying a set of documents that maximally satis es a user s acute information needs.
Summarization strives at compressing large quantities of text into a more concise formulation.
In the absence of automated methods for identifying the deep semantics associated with text, prior work in summarization has typically operated at the level of complete sentences, weaving together the most representative sentences to create a document summary.
Research on search and summarization has generally overlooked the dynamics of informational content arriving continuously over time.
We shall present methods for identifying information novelty and show how these methods can be applied to manage content that evolves over time.
We start by describing a general framework for comparing collections of documents.
We assume documents are organized into groups by their content or source, and analyze inter-group and intra-group differences and commonalities.
Juxtaposing two groups of documents devoted to the same topic but derived from two distinct sources (e.g., news coverage of an event in different parts of the world) can reveal interesting differences of opinions and overall interpretations of situations.
Moving from static collections to sets of articles generated over time, we can examine the evolution of content.
For example, we can seek to examine a stream of news articles evolving over time on a common story, with the goal of highlighting truly informative updates and  ltering out a large mass of articles that largely relay  more of the same.  In contrast to prior work in text summarization, we work at the level of individual words rather than entire sentences.
Working at this  ner resolution, we gather detailed statistics on word occurrence across sets of documents in order to characterize differences and similarities among these sets.
We further enhance the simple bag of words model by extracting named entities that denote names of people, organizations, and geographical locations.
In contrast to phrases and collocations whose discriminative semantic properties are usually outweighed by lack of suf cient statistics named entities identify relatively stable tokens that are used in a common manner by many writers on a given topic, and so their use contributes a considerable amount of information.
In fact, one type of analysis we describe below represents articles using only the named
 ysis was comparable or even superior in performance to methods that manipulate the full bag of words.
We will focus on the analysis of live streams of news.
Live news streams pose tantalizing challenges and opportunities for research.
Newsfeeds span enormous amounts of data, present a cornucopia of opinions and views, and include a wide spectrum of formats and content from short updates on breaking news, to major recaps of story developments, to mere reiterations of  the same old facts  reported over and over again.
We describe algorithms that identify signi cant updates on the stories being tracked, relieving the users from having to sift through long lists of similar articles arriving from different news sources.
The methods provide the basis for personalized news portal and news alerting services that promise to minimize the time and disruptions to users who wish to follow evolving news stories.
The contributions of this paper are threefold.
First, we present a framework for identifying differences in sets of documents by analyzing the distributions of words and recognized named entities.
This framework can be applied to compare individual documents, sets of documents, or a document and a set (for example, a new article vs.
the union of previously reviewed news articles on the topic).
Second, we present a collection of algorithms that operate on live news streams and provide users with a personalized news experience.
These algorithms are implemented in a system named Newsjunkie that presents users with maximally informative news updates.
Users can request updates per every user-de ned period or per each burst of reports about a story.
Users can also tune the desired degree of relevance of these updates to the core story, allowing delivery of offshoot articles that report on related or similar stories.
Finally, we describe an evaluation method which presents users with a single seed story and sets of articles ranked by different novelty-assessing metrics, and seeks to understand how participants perceive the novelty of these sets in the context of the seed story.
As we shall discuss, the results of this study highlight several interesting issues such as what is considered a story  on a given topic,  how people judge novelty, and how their judgments are in uenced by the relevance of the information being read to the topic de ned by the seed story.
The remainder of this paper is organized as follows.
In Section 2, we review related research in language modeling, text sum-marization and topic detection and tracking (TDT).
In Section 3, we present a framework for comparing text collections, and discuss its applications to  nding differences both between and within groups of documents.
We report the results of experiments with users in Section 4.
In Section 5, we describe two new types of document analyses we developed that allow users to personalize the frequency and content of the news updates that they receive.
Finally, we discuss future research directions in Section 6.
The AT&T Internet Difference Engine (AIDE) [9] was one of the earliest attempts to develop a tool for comparing the content of material drawn from the Internet.
The HtmlDiff system built in the course of this project looked for simple syntactic differences between Web pages similarly to what the Unix diff utility does; however, no attempt was made to capture semantic differences between the pages.
In an effort to characterize content differences beyond simple syntactic variations, several past studies focused on identifying words that are particularly characteristic of a given text.
Kilgar-riff [15] presents a good summary of these studies as well as their potential applications to characterizing text genres and differences in male and female speech.
Comparing distributional properties of individual words naturally evolved into comparing language models for entire text collections, which was found useful in two ways.
In corpus linguistics, researchers studied to what degree reasoning about one text collection can be based on the model computed for another collection [15].
In the realm of information retrieval, comparing the language models for documents returned by a query and the entire collection was found to be valuable for predicting query quality (i.e., how well the query is formulated and how focused the results are expected to be) [6, 7].
In this work we reason about the similarity of document sets by comparing their language models using several distance metrics.
Another way to compare document sets is to represent the documents as bags of words, and use established word similarity metrics that sum over pairwise distances between individual words.
Prior work on semantic word similarity developed in two major directions, either using purely statistical analysis such as Latent Semantic Indexing (LSI) [8] and Hyperspace Analog to Language (HAL) [2], or capitalizing on background knowledge resources such as WordNet [20].
The two approaches have also been combined in a single framework [11].
Lee [17] investigated measures of distributional word similarity by using language models similar to ours to improve probability estimations for unseen word cooccurrences.
We seek to order news by novelty in an iterative manner, attempting at each step to identify the article that carries the maximum amount of new information, in the context of a background model composed from content that has been already reviewed.
This approach is conceptually related to the notion of Maximum Marginal Relevance (MMR) [3], developed in the context of information retrieval.
When selecting the next document to be returned in response to a query, the MMR criterion prefers relevant documents that are maximally different from documents that have been selected before.
Research on Topic Detection and Tracking (TDT) has led to investigations of a variety of problems related to novelty detection.
Yang et al. [27] studied a speci c form of novelty detection, namely First Story Detection (FSD).
The authors used a Rocchio-style text classi er [26] to classify documents into prede ned broad topics, and then measured the novelty of new documents given the topics predicted for them.
Documents were represented as vectors of words and named entities weighted with a TF.IDF scheme [22].
Working in the domain of news, Swan and Jensen [24] automatically generated timelines from historic date-tagged news corpora (TDT).
They used a  2 test to identify days on which the number of occurrences of a given word or phrase exceed some (empirically determined) threshold, and then generated timelines by grouping together contiguous sequences of such days.
Kleinberg [16] used randomized in nite-state automata to model burstiness and hierarchical structure in text streams.
This work was not speci cally focused on news, but experimented with other kinds of time-stamped text corpora such as personal email archives and conference proceedings spanning a number of years.
Sample applications of this formal model include identi cation of increased activity bursts in email trails, as well as computing the most important words in conference paper titles over different time periods.
In contrast to [24], this approach zooms into the bursts to determine their hierarchical structure, identifying short, intense bursts within longer but weaker ones.
Assessing the novelty of a quantity of text is related to analyzing its most informative sentences.
The latter task has been the focus of research in extractive text summarization, which combines 483most important sentences of a collection of documents to produce a summary.
A study performed in the course of the Columbia Newsblaster project [23] identi ed key sentences by looking for importance-signaling words and high-content verbs obtained by analyzing large news corpora, as well as  nding dominant concepts by consulting WordNet [10].
Allan et al. [1] computed usefulness and novelty measures at the sentence level by developing probabilistic models for news topics and events.
Collins-Thompson et al.
[4] selected sentences by using a C4.5 [19] classi er with a set of surface and semantic sentence features; they also compared pairs of sentences to check if one of them is a statistical translation of the other.
In contrast to prior research, our work focuses on developing a system that manipulates live newsfeeds and offers users personalized updates that are maximally novel in the context of information the user has reviewed before.
Given two sets of textual content, how can we characterize the differences between them?
Answering this question is useful in a variety of applications, including automatic pro ling and comparison of text collections, automatic identi cation of different views, scopes and interests re ected in the texts, and automatic identi cation of novel information.
In general, several aspects of  difference  may be investigated:   Differences in content may re ect the different ways a particular person or event is described in two sets of documents.
For example, consider analyzing differences in prede ned partitions, e.g., comparing US vs. European reports on various political issues, or comparing the coverage of a recent blackout of the East Coast of the United States in the news coming from sources based in the East Coast and West Coast.
  Differences in structural organization may go well beyond text and also consider link structure of Web sites, e.g., comparing IBM Web site vs. Intel Web site.
  Differences in time (i.e., temporal aspects of content differences) can reveal interesting topical changes in series of documents.
This kind of analysis can be used to compare today s news vs. the news published a month or a year ago, to track changes in search engine query logs over time, or to identify temporal changes in topics in users  personal email.
Temporal differences are a particularly interesting case, and in this paper we focus on automatically assessing the novelty over time of news articles coming from live newsfeeds.
Speci cally, we formulated the following two research challenges:
 to order news articles so that each article adds maximum information to the (union of) previously read ones.
tify the importance and relevance of news updates, granting end users control over these parameters and offering them a personalized news experience.
In the remainder of this section, we outline a methodology for analyzing newsfeeds and describe an algorithm for ranking news articles by predicting the amount of novel information they carry.
The results of an empirical evaluation of this algorithm are reported in the next section.
Section 5 studies topic evolution over time and proposes a new approach to analyzing different types of articles.
We developed a software toolset named Newsjunkie that implements a collection of algorithms and a number of visualization options for comparing text collections.
Newsjunkie represents documents as bags of words augmented with named entities extracted from the text.
In-house extraction tools were used for this purpose, which identi ed names of people, organizations and geographical locations.
Document groups contain documents with some common property, and constitute the basic unit of comparison.
Examples of such common properties can be a particular topic or source of news (e.g., blackout stories coming from the East Coast news agencies).
We draw inferences about the differences between document groups by  rst building a language model for each group, and then comparing the models using some similarity metric (see Section 3.2).
To facilitate exploring a variety of language models, Newsjunkie represents documents either as smoothed probability distributions1 over all the features (words + named entities), or as vectors of TF.IDF weights [22] (in the same feature space).
Let us imagine the common situation where something interesting happens in the world, and the event is picked by the news media.
If the event is of suf cient public interest, the ensuing developments are tracked in the news as well.
Suppose you have read an initial report and, at some later time, are interested in catching up with the story.
In the presence of Internet sites that aggregate thousands of news sources such as Google News or Yahoo!
News2, your acute information-seeking goal can be satis ed in many ways and with many more updates than even the most avid news junkie has the time to review.
Automated tools for sifting through a large quantity of documents on a topic that work to identify nuggets of genuinely new information can provide great value.
Avoiding redundancy and overlap can help minimize the overhead associated with tracking news stories.
There is a great deal of redundancy in news stories.
For example, when new developments or investigation results are expected but no new information is yet available, news agencies often  ll in the void with recaps of earlier developments until new information is available.
The situation is further aggravated by the fact that many news agencies acquire part of their content from major multinational content providers such as Reuters or Associated Press.
Users of news sites do not want to read every piece of information over and over again.
Users are primarily interested in learning what s new.
Thus, ordering news articles by novelty promises to be useful.
We use a number of document similarity metrics to identify articles that are most different from the union of those read previously.3 respectively.
Then, We implemented the following distance metrics:   Kullback-Leibler (KL) divergence [5], a classical asymmetric information-theoretic measure.
Assume we need to compute the distance between a document d and a set of documents R. Let us denote the probabilistic distributions of words (and named entities if available) in d and R by pd and pR, =
 law of succession [21] or linear smoothing with word probabilities in the entire text collection [18]; the latter option was used throughout the experiments reported in this paper.
2http://news.google.com and http://dailynews.yahoo.com, respectively.
the fact that we are actually looking for documents that are most dissimilar from documents reviewed earlier.
distKL(pd, pR) 484(cid:80) w words({d} R) pd(w) log pd(w) pR(w) .
Note that the computation of log pd(w) pR(w) requires both distributions to be smoothed to eliminate zero values (corresponding to words that appear in d but not in R, or vice versa).
  Jensen-Shannon (JS) divergence [5], a symmetric variant of the KL divergence.
Using the de nitions of the previous item, distJS(pd, pR) = distKL(pd,q)+distKL(pR,q) , where q = pd+pR
 .
  Cosine of vectors of raw probabilities (computation does not require smoothed probabilities).
  Cosine of vectors of TF.IDF feature weights.
  A metric we formulated to measure the density of previously unseen named entities in an article (referred as NE).
The intuition for this metric is based on our conjecture that novel information is often conveyed through the introduction of new named entities, such as the names of people, organizations, and places.
Using the notation of Figure 1, the NE metric can be de ned as follows: Let N E(R) be a set of named entities found in a set of documents R. Let N Eu(R1, R2) be a set of unique named entities found in the set of documents R1 and not found in the set R2.
That is, N Eu(R1, R2) = {e|e   N E(R1)   e /  N E(R2)}.
Then, distN E(d, R) = N Eu({d}, R)/length(d).
Normalization by document length is essential, as, without normalization the NE score will tend to rise with length, because of the probabilistic in uence of length on seeing additional named entities; the longer the document is, the higher the chance it contains more named entities.
These distance metrics can be harnessed to identify novel content for presentation to users.
In the Newsjunkie application, we apply the novelty ranking algorithm iteratively to produce a small set of articles that a reader will be interested in.
We employ a greedy, incremental analysis.
The algorithm initially compares all the available updates to the seed story that the user has read, and selects the article least similar to it.
This article is then added to the seed story (forming a group of two documents), and the algorithm looks for the next update most dissimilar to these articles combined, and so on.
The pseudocode for the ranking algorithm is outlined in Figure 1.
Algorithm RANKNEWSBYNOVELTY(dist, seed, D, n) R   seed // initialization for i = 1 to min(n,|D|) do d   argmaxdi D{dist(di, R)} R   R   {d}; D   D \ {d} where dist is the distance metric, seed   seed story, D   a set of relevant updates, n   the desired number of updates to select, R - list of articles ordered by novelty.
Figure 1: Ranking news by novelty.
To validate the algorithm presented in the previous section, we conducted an experiment that asked subjects to evaluate sets of news articles ordered by a variety of distance metrics.
For the experiments described herein we used a live newsfeed supplied by Moreover Technologies4, who aggregates news articles from over 4000 Internet sources.
A simple clustering algorithm was used to group stories discussing the same events (called topics in the sequel).
We used 12 clusters that correspond to topics reported in the news in mid-September 2003.
The 12 topics covered news reports over a time span of 2 to 9 days, and represented between
 SARS in Singapore, the California governor recall, the Pope s visit to Slovenia, etc.
(Table 1 shows the full list of topics).
Judging novelty is a subjective task.
One way to obtain statistically meaningful results is to average the judgments of a set of users.
In order to compare different novelty-ranking metrics, we asked participants to read several sets of articles ordered by alternate metrics, and to decide which sets carried the most novel information.
Note that this scenario requires the evaluators to keep in mind all the article sets they read until they rate them.
Because it is dif cult to keep several sets of articles on an unfamiliar topic in memory, we limited our experiment to evaluating the following three metrics:
 tion-theoretic basis (KL).
tically motivated alternative (NE).
For each of the 12 topics, we selected the  rst story as the seed story, and used the three metrics described above to order the rest of the stories by novelty using the algorithm RANKNEWSBYNOV-ELTY (Figure 1).
The algorithm  rst selects the most novel article relative to the seed story.
This article is then added to the seed story to form a new model of what the user is familiar with, and the next most novel article selected.
Three articles were selected in this manner for each of the three metrics and each of the 12 topics.
For each topic, the subjects were  rst asked to read the seed story to get background about the topic.
They were then shown the three sets of articles (each set chosen by one of the metrics), and asked to rate the sets from most novel to least novel set.
They were instructed to think of the task as identifying the set of articles that they would choose for a friend who had reviewed the seed story, and now desired to learn what was new.
The presentation order of the sets generated by the three metrics was randomized across participants.
Originally, we had quite a few reservations about whether such an evaluation procedure would be feasible at all.
Not only had the users to bear in mind quite a lot of information before pronouncing their decision, but the situation was further complicated by the varying relevance of articles to the seed story (we discuss this issue in detail in Section 5.3).
The procedure described above was re ned through a series of calibration experiments, in which we tried several techniques for eliciting judgments, and then thoroughly debriefed the subjects after each experiment.
One notable alternative 4http://www.moreover.com.
Another possible option would have been to use standard Topic Detection and Tracking (TDT) datasets used in TREC experiments.
However, as TDT data dates back to 1998 99, we thought that current news stories from Moreover Technologies  feeds would be more engaging for the volunteers evaluating our system in action.
Topic description topic1 topic2 topic3 topic4 topic5 topic6 topic7 topic8 topic9 topic10 topic11 topic12 Pizza robbery RIAA sues MP3 users Sharon visits India Pope visits Slovakia Swedish FM killed Al-Qaeda CA governor recall MS bugs SARS in Singapore Iran develops A-bomb NASA investigation Hurricane Isabel #times most novel






































 Mean rank






































 Figure 2: Cumulative results.
would be to present the users with pairs of next articles, and ask them to give relative judgements as to which of the two articles carries more novel information given the seed story.
We chose the set-based approach because it was more similar to the scenario we wanted to support, namely, given one article, to  nd a set of new articles to read.
Furthermore, pairwise tests would also have involved more reading load for the participants.
Overall we obtained 111 user judgments on 12 topics, averaging
 each metric was rated the most, medium and least novel.
As can be readily seen from the graph, the sets generated by the KL and NE metrics were rated more novel than those produced by the baseline metric (ORG).
Table 1 presents per-topic results.
The three penultimate columns show the number of times each metric was rated the most novel for each topic.
The last three columns show mean ranks of the metrics, assuming the most novel is assigned the rank of 1, medium novel   2, and least novel   3.
We used Wilcoxon Signed Ranks Test5 [25] to assess the statistical signi cance of experimental results.
Comparing the mean ranks of metrics across all the topics (as summarized in Figure 2), both KL and NE were found superior to ORG at p < 0.001.
Considering individual per-topic results, the ORG metric never achieved the lowest (= best) rank of all three metrics.
In 6 cases (topics 2, 4, 5,
 scoring metric was statistically signi cant at p < 0.05, and in one additional case the signi cance was borderline at p = 0.068 (topic
 in favor of KL was statistically signi cant at p < 0.05 for topics
 difference in mean ranks in favor of NE was borderline signi cant for topics 2 and 3 (p = 0.096 and p = 0.057, respectively).
The experiment we conducted should be considered a preliminary one as it only involved 12 different topics.
Nevertheless, it allowed us to verify our evaluation procedure, as well as to observe the superiority of the two metrics tested (KL and NE) over the baseline (ORG).
Based on these results, we do not observe signi cant difference in performance between KL and NE.
However, we be-
paired t-test when the underlying distribution cannot be assumed to be normal.
Table 1: Results by topic.
lieve that these preliminary results warrant further investigation of the potential content-sensitivity of the value of using KL versus NE metrics.
In our future research we will attempt to characterize the properties of collections of articles on topics that could indicate where each metric performs the best for users.
Algorithm RANKNEWSBYNOVELTY presented and evaluated in the previous section works under the assumption that a user wants to catch up with latest story developments some time after initially reading about it.
In this case the algorithm orders the recent articles by their novelty compared to the seed story, and then the user can read a number of highest-scoring articles depending on how much spare time he or she can allocate for the reading.
But what if the user wants to be updated continuously as the new developments actually happen?
Some logistic support is needed to constantly keep track of the articles the user reads in order to estimate the novelty of the new articles streaming in the news-feed.
Based on user s personal preferences, that is, how often the user is interested in getting updates on the story, the server decides which articles to display.
Therefore, an online decision mechanism is needed that determines whether any article contains suf ciently new information to warrant its delivery to the user.
In a more general analysis of the bene ts versus the costs of alerting, there are opportunities to balance the informational value of particular articles or groups of articles with the cost of interrupting users, based on a consideration of their context [14].
In what follows, we examine two scenarios of updating users with current news.
The  rst scenario (discussed in Section 5.1) assumes the user is interested in getting updates once a day, while the second scenario (Section 5.2) updates the user continuously by monitoring incoming news for bursts of novel information.
Finally, Section 5.3 introduces a mechanism that allows users to control the degree of relevance of the articles they are about to read.
Let us  rst consider the simpler case when the user wants to see no more than a single daily update on the story.
One way to achieve this aim would be to use an algorithm similar to RANKNEWSBYNOVELTY, that is, accumulate the stories received on all the preceding days, and assess the novelty of each new story that arrived today by computing its distance from the accumulated set.
The main problem with this approach is that the more stories are pooled, the less signi cant becomes the distance from any new story to the pool.
After several days worth of articles have been accumulated, even a major update will be seen as barely new.
d   argmaxdi D{dist(di, Bg)} if dist(d, Bg) > thresh then display(d) Bg   D where dist is the distance metric, Bg   the background reference set (union of all the relevant articles received on the preceding day), D   a set of new articles received today, thresh   user-de ned sensitivity threshold.
Figure 3: Picking daily updates.
To avoid this pitfall, we modify our original algorithm as shown in Figure 3.
Given the user and her choice of the topic to track, algorithm PICKDAILYUPDATE compares the articles received today with the union of all the articles received the day before.
The algorithm picks the most informative update compared to what was known yesterday, and shows it to the user, provided that the update carries enough new information (i.e., its estimated novelty is above the user s personalized threshold).
Such conditioning endows the system with the ability to relay to the user truly informative updates and to  lter out articles that only recap previously known details.
The algorithm can be trivially generalized to identify n most informative updates per day.
It could be argued that by ignoring all the days before the immediately preceding one, algorithm PICKDAILYUPDATE might also consider novel those articles that recap what was said several days ago.
In practice this rarely happens, as most of the articles are written in the way that interleaves new information with some background on previous developments.
In our future work we plan to consider more elaborate distance metrics, that consider all previous articles relevant to the topic but decay their weight with age.
The algorithm presented in Section 5.1 is still largely an  of ine  procedure, as it updates users at prede ned time intervals.
Hardcore news junkies might  nd it frustrating to wait for daily scheduled news updates!
For some, a more responsive form of analysis may be desired.
Taking the previous idea to the extreme and comparing every article to the preceding one will not work well, as the system will predict nearly every article as novel.
Instead, we use a sliding window covering a number of preceding articles to estimate the novelty of the current one.
Observe that estimating distances between articles and a preceding window of  xed-length facilitates the comparison of scores.
We evaluated different window lengths of 20 60 articles.
We found that lengths of approximately 40 typically worked well in practice.
In contrast to the algorithm PICKDAILYUPDATE, the background reference set now becomes much shorter, namely, 40 articles instead of a full day s content.
This increases the likelihood that the window is not long enough to cover delayed reports and recaps that follow long after the story was initially reported.
In order to  lter out such repetitions, we  rst need to better understand the nature of news reports.
When an event or information update about an event of importance occurs, many news sources pick up the new development and report it within a fairly short time frame.
If we successively plot the distance between each article and the preceding window, such arrival of new information will result in peaks in the graph.
We call such peaks a burst of novelty.
At the beginning of each burst, additional articles tend to add new details causing the graph to rise.
As time passes, the sliding window covers more and more articles Algorithm IDENTIFYBREAKINGNEWS(dist, D, l, f w, thresh) i=1 di   D W indow  (cid:83)l for i = l + 1 to |D| do Scoresi   dist(di, W indow) W indow   (W indow \ di l)   di Scoresf ilt   M edianF ilter(Scores, f w) for j = 1 to |Scoresf ilt| do if Scoresf ilt j > thresh then display(dj+l) skip to the beginning of the next burst where dist is the distance metric, D   a sequence of relevant articles, l   sliding window length, f w   median  lter width, thresh   user-de ned sensitivity threshold.
Figure 4: Identifying breaking news.
conveying this recent development and the following articles do not have the same novelty; as a result, the computed novelty heads downward signifying the end of the burst.
Delayed reports of events as well as recaps on a story are less likely to be correlated in time between different sources.
Such reports may appear novel compared to the preceding window, but since they are usually isolated they only cause narrow spikes in the graph.
In order to discard such standalone spikes and not to admit them as genuine updates, we need to  lter the novelty signal appropriately.
The median  lter [12] provides exactly this functionality by reducing the amount of noise in the signal.
The  lter successively considers each data point in the signal and adapts it to better resemble its surroundings, effectively smoothing the original signal and removing outliers.
Speci cally, a median  lter of width w  rst sorts the w data points within the window centered on the current point, then replaces the latter with the median value of these points.
After computing the distance between every article and a sliding window covering the preceding ones, we pass the resultant signal through a median  lter.
We considered  lters of width 3 7; the  lter of width 5 appears to work well in the majority of cases.
We note that the use of a median  lter may delay the routing of novel articles to users, as we need to consider several following articles to reliably detect the beginning of a new burst.
However, we found that such delays are rather small (half the width of the median  lter used), and the utility of the  lter more than compensates for this inconvenience.
If users are willing to tolerate some additional delay, the algorithm can scan forward several dozens of articles from the moment a burst is detected, in order to select the most informative update instead of simply picking the one that starts the burst.
Combination approaches are also feasible such as the rendering of an early update on breaking news, and then waiting for a more informed burst analysis to send the best article on the development.
Figure 4 shows the pseudocode the algorithm IDENTIFY-BREAKINGNEWS that implements burst analysis for news alerting.
Figure 5 shows the application of the algorithm IDENTIFYBREAK-INGNEWS to a sample topic.
The topic in question is devoted to a bank robbery case in Erie, Pennsylvania, USA, where a group of criminals apparently seized a pizza delivery man, locked a bomb device to his neck and, according to statements made by the delivery man, forced him to rob a local bank.
The man was promptly apprehended by police, but soon afterwards the device detonated and killed him.
The bizarre initial story and ensuing investigation were tracked by many news sources for some two weeks in September 2003.
The x-axis of the  gure corresponds to the sequence of articles as they arrived in time, and the y-axis plots (raw and median ltered) distance values for each article given the pre-487sion, but the new information they add is suf ciently differ ent from that reported in the seed story to warrant a new trail.
mistakes.
Of these classes, relationship types 2 and 3 are probably what most users want to see.
But how can we identify them automatically, and how can we empower the users themselves to exercise control over this spectrum?
To achieve this aim we suggest a new type of document analysis that scrutinizes intra-document dynamics.
As opposed to previous kinds of analysis that compared entire documents to one another, the new technique  zooms into  documents estimating the relevance of their parts.
We start with building a language model for every document, and  x a distance metric to use, e.g., KL divergence.
Then, for each document, we slide a window over its words and plot the distance scores of each such window versus the seed story.
We construe the score of a window of words as a sum of pointwise scores of each word vs. the seed story, as stipulated by comparing the language model of the current document with that of the seed story using the selected metric.
Several different window lengths were considered, and the value of 20 was found to work well in practice.
An important property of this technique is that it goes beyond the proverbial bag of words, and considers the document words in their original context.
We opted for using sliding contextual windows rather than apparently more appealing paragraph units, since using a  xed-length window makes distance scores directly comparable.
Another obvious choice of the comparison unit would be individual sentences.
However, we believe that performing this analysis at the sentence level would consider too little information, and the range of possible scores would be too large to be useful.
A recent study in novelty detection [13] corroborates this reasoning   when the importance of individual sentences (deemed relevant) is considered, 93% of them are classi ed as carrying novel information.
Figure 6 shows sample results of intra-document analysis.
The seed story for this analysis was a report on a new case of SARS in Singapore.
Articles that mostly recap what has already been said typically have a very limited dynamic range and low absolute scores.
Elaboration articles usually have higher absolute scores that re ect the new information they carry.
One elaboration for this story reported that the patient s wife was being held under quarantine.
Further along this spectrum, articles that may qualify as offshoots but are still anchored to the events described in the seed story have a much wider dynamic range.
One offshoot was a story that focused on the impact of SARS on the Asian stock market, and another was on progress on a SARS vaccine.
Both offshoot articles used the recent case as a starting point, but were really about a related topic.
We believe that analyzing intra-document dynamics such as the dynamic range and patterns of novelty scores are useful in identifying different types of information that readers would like to follow.
The Web has been providing users with a rich set of news sources.
It is deceptively easy for Internet surfers to browse multitudes of sources in pursuit of news updates, yet sifting through large quantities of news can involve the reading of large quantities of redundant material.
We presented a collection of algorithms that analyze live news-feeds and identify articles that carry most novel information given a model of what the user has read before.
To this end, we extend the conventional bag of words representation with named entities Dotted line represents raw distance scores, solid line   median ltered scores.
Figure 5: Identifying breaking news   sample plot of raw and  ltered novelty signals.
ceding sliding window.
Raw distance scores are represented by a dotted line, and  ltered scores are plotted with a solid line.
The text boxes accompanying the  gure comment on the actual events that correspond to the identi ed novelty bursts, and show which potentially spurious peaks have been discarded by the  lter.
The smoothed novelty score, which incorporates the median  lter, does a good job of capturing the main developments in the story (interviews with friends, details about the weapon, FBI bulletin for two suspects, and a copycat case), while at the same time  ltering out spurious peaks of novelty.
When we debriefed several users who completed the experiments described in Section 4, several of them reported that it was dif cult to judge the novelty of articles because of their varying relevance to the seed story.
In some cases this had to do with errors in the tagging of the news stories by the newsfeed we relied upon, while in one or two extreme cases a failure of the Moreover parser caused grossly unrelated articles to be glued to the relevant ones.
In other cases the variance in relevance was due to the differences in writing styles and policies among different publications.
These comments led us to believe that the novelty scores we compute should not be relied upon as a sole selection criterion; some articles are identi ed as novel by virtue of changing the topic.
To further re ne the analysis of informational novelty, we have formulated a classi cation of types of novelty, based on different relationships between an article and a seed story.
These classes of relationships include:
 offer reviews of what has already been reported and carry little new information.
topic set forth by the seed article.
Friends and neighbors believe delivery man is innocent Found gun disguised as a cane FBI looking for 2 people Repetitions [filtered] Completely unrelated story (tagging failure by news provider) [filtered] Copycat case in Missouri Repetitions [filtered] Articles in time Distance scores 488topic-focused discussions.
Although technically possible, we believe that using these techniques for estimating the novelty of email messages would be less useful.
In personal email, information is usually transmitted in an extremely compact way, so that even the most novel piece of information may be conveyed in a single word or only a few words.
Therefore, much deeper language understanding is required to adequately re ect the importance of information, while more statistically-oriented approaches relying on a bag of words might overlook such distinctions.
We thank Uri Nodelman for ongoing discussions and constructive comments.
We are also grateful to Lucy Vanderwende, Mike Calcagno and Kevin Humphreys for the assistance with the use of tools for extracting named entities from text.
