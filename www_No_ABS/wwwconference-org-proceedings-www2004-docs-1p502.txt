Nowadays the Web poses itself as the largest data repository ever available in the history of humankind.
Major efforts have been made in order to provide ef(cid:2)cient access to relevant information within this huge repository.
At least two broad views of this problem have evolved recently.
The (cid:2)rst one, characterized by the unstructured view of data, has developed breakthrough technologies (such as Web search engines) based on information retrieval [3] methods, which have been used in many successful commercial products.
The second one, characterized by the structured or semi-structured view of data, borrows techniques from the database area to provide the means to effectively managing the data available on the Web [9].
Thus, several techniques have been adapted (or targeted speci(cid:2)cally) to the problem of extracting data from the Web Copyright is held by the author/owner(s).
for further processing (querying, integration, mediation, etc.)
[13].
However, these techniques are still not spread as the information retrieval based ones.
This happens mostly because of two problems with these techniques: (1) the need for high human intervention and (2) the low quality of the extraction results.
Thus, the motivation to develop new methods and tools to allow the effective deployment of a more structured view of the data available on the Web still remains.
Devising generic methods for extracting Web data is a complex (if not impossible) task, since the Web is very heterogeneous and there are no rigid guidelines on how to build HTML pages and how to declare the implicit structure of the Web pages.
Thus, in order to develop effective methods for extracting Web data in a precise and completely automatic manner, it is usually required to take into account speci(cid:2)c characteristics of the domain of interest.
One of such domains is that of online newspapers and news portals on the Web, which have become one of the most important sources of up-to-date information.
Indeed, there are thousands of sites that provide daily news in very distinct formats and there is a growing need for tools that will allow individuals to access and keep track of this information in a automatic manner.
In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites.
This approach is based on the concept of tree-edit distance [17, 20] and allows not only the extraction of relevant text passages from the pages of a given Web site, but also the fetching of the entire Web site content, the identi(cid:2)cation of the pages of interest (the pages that actually present the news) and the extraction of the relevant text passages discarding non-useful material such as banners, menus, and links.
To support this approach, we have developed a highly ef(cid:2)cient tree structure analysis algorithm that outperforms, for practical purposes, the best results on tree-edit distance calculation in the literature.
We have tested our approach with several important Brazilian online news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites.
The rest of this paper is organized as follows.
Section 2 gives an overview of the theory behind tree edit distance algorithms, the basis of our work.
Section 3 presents our improved tree structure analysis algorithm, while Section 4 shows the application of this algorithm in the various tasks that comprise our approach.
Experimental results demonstrating the effectiveness of our approach are in Section 5.
Section 6 discusses related work.
Finally, conclusions and directions for future work can be found in Section 7.
The approach we have developed for (cid:2)nding and extracting data of interest from Web pages is based on the analysis of the structure of these target pages.
More precisely, by evaluating the structural similarities between pages in a target site we are able to perform tasks such as grouping together pages with similar structure to form page clusters and (cid:2)nding a generic representation of the structure of the pages within a cluster.
Indeed, as we shall see, such tasks are key to our approach.
Since the structure of a Web page can be nicely described by a tree (e.g., a DOM tree), we have resorted to the concept of tree edit distance [17, 20] to evaluate the structural similarities between pages.
Intuitively, the edit distance between two trees TA and TB is the cost associated with the minimal set of operations needed to transform TA into TB.
In this section we review this important concept along with its related formalisms and describe how we use it to analyze the structure of Web pages.
Trees are one of the most common data structures used in computer science.
Formally, they are de(cid:2)ned as directed acyclic simple graphs.
Although most of the discussion in this section can be generalized to deal with different types of tree, we are interested only in one speci(cid:2)c type of tree, called labeled ordered rooted tree.
A rooted tree is a tree whose root vertex is (cid:2)xed.
Ordered rooted trees are rooted trees in which the relative order of the children is (cid:2)xed for each vertex.
Labeled ordered rooted trees have a label l attached to each of their vertices.
Figure 1 shows an example of such a tree.
From now on, we refer to labeled ordered rooted trees simply by trees, except when explicitly stated.
Figure 1: A labeled ordered rooted tree with root R In its traditional formulation, the tree edit distance problem considers three operations: (a) vertex removal, (b) vertex insertion, and (c) vertex replacement.
To each of of these operations, a cost is assigned.
The solution of this problem consists in determining the minimal set of operations (i.e., the one with the minimum cost) to transform one tree into another.
Another equivalent (and possibly more intuitive) formulation of this problem is to discover a mapping with minimum cost between the two trees.
The concept of mapping (introduced in [18]) is formally de(cid:2)ned next.
DEFINITION 1.
Let Tx be a tree and let Tx[i] be the i-ism vertex of tree Tx in a preorder walk of the tree.
A mapping between a tree T1 of size n1 and a tree T2 of size n2 is a set M of ordered pairs (i; j), satisfying the following conditions for all (i1; j1); (i2; j2) 2
 (cid:15) i1 = i2 iff j1 = j2; (cid:15) T1[i1] is on the left of T1[i2] iff T2[j1] is on the left of T2[j2]; (cid:15) T1[i1] is an ancestor of T1[i2] iff T2[j1] is an ancestor of T2[j2].
In De(cid:2)nition 1, the (cid:2)rst condition establishes that each vertex can appear no more than once in a mapping, the second enforces order preservation between sibling nodes and the third enforces the hierarchical relation between the nodes in the trees.
Figure 2 illustrates a mapping between two trees.
Intuitively, a mapping is a description of how a sequence of edit operations transform a tree into another, ignoring the order in which these operations are applied.
In Figure 2, a dotted line from a vertex of T1 to vertex of T2 indicates that the vertex of T1 should be changed if the vertices are different, remaining unchanged otherwise.
Vertices of T1 not touched by dotted lines should be deleted, and vertices of T2 not touched should be inserted.
Figure 2: A mapping example As we have already mentioned, estimating a tree edit distance is equivalent to (cid:2)nding the minimum cost mapping.
Let M be a mapping between tree T1 and tree T2, let S be a subset of pairs (i; j) 2 M with distinct labels, let D be the set of nodes in T1 that do not occur in any (i; j) 2 M and let I be the set of nodes in T2 that do not occur in any (i; j) 2 M. The mapping cost is given by c = Sp + Iq + Dr, where p, q and r are the costs assigned to the replacement, insertion, and removal operations, respectively.
It is common to associate a unit cost to all operations, however, speci(cid:2)c applications may require the assignment of distinct costs to each type of operation.1 The tree edit distance problem is a dif(cid:2)cult one, and several algorithms, with different tradeoffs, have been recently proposed, but all formulations have complexities above quadratic [6].
Further, it has been proved that, if the trees are not ordered, the problem is NP-complete [27].
The (cid:2)rst algorithm for the mapping problem was presented in [18], and its complexity is O(n1n2h1h2), where n1 and n2 are the sizes of the trees and h1 and h2 are their heights.
This is a dynamic programming algorithm that recursively calculates the edit distance between the strings formed by the sets of children vertices of each internal vertex in the tree.
In [21], a new algorithm was presented with cost O(d2n1n2min(h1; l1)min(h2; l2)), where d is the edit distance between the trees and l1 and l2 are the number of leaves in each tree.
Notice that this cost depends on the algorithm output.
The best known upper limit for this problem is due to an algorithm presented in [6] with complexity O(n1n2 + l2 1 + l2:5 1 l2).
Despite the inherent complexity of the mapping problem in its generic formulation, there are several practical applications that can be modelled using restricted formulations of it.
By imposing conditions to the basic operations corresponding to the original formulation in De(cid:2)nition 1 (i.e., replacement, insertion and removal), four classical restricted formulations are obtained: alignment, distance between isolated trees, top-down distance, and bottom-up distance, for which more convenient and fast algorithms have been proposed [19, 22].
Detailing each one of these formulations and algorithms is beyond the scope of this paper, but since our approach is based on a restricted version of the top-down mapping problem, we will brie(cid:3)y review and illustrate it.
Informally, a top-down mapping restricts the removal and insertion operations to take place only in the leaves of the trees.
Figure 3 illustrates a top-down mapping which is formally de(cid:2)ned as follows.
is said to be top-down only if for every pair (i1; i2) 2 M there is also a pair (parent(i1),parent(i2)) 2 M, where i1 and i2 are non-root nodes of T1 and T2 respectively.
Figure 3: A top-down mapping example The (cid:2)rst algorithm for the top-down edit distance problem was proposed by Selkow [17].
In [25], Yang presents a recursive dynamic programming algorithm with cost O(n1n2) for the problem, where n1 and n2 are the sizes of T1 and T2, respectively.
One of the most popular algorithms for the problem is presented in [5] also with cost O(n1n2).
This algorithm, however, is not recursive and the problem is solved within a single dynamic programming instance.
The paper also presents an external memory variation for this algorithm.
Top-down mappings have been successfully applied to several Web related applications such as document categorization.
For instance, Nierman and Jagadish [16] use a top-down distance algorithm to cluster XML documents.
In our case, we are interested in the problem of evaluating the similarity between Web pages.
Indeed, most Web pages are structured according to formats such as HTML and XML which, as mentioned before, can be seen as labeled ordered rooted trees [7].
Actually, the DOM paradigm, commonly used for manipulating Web pages, uses this tree representation.
In the next section, we present a new algorithm for determining a restricted form of top-down mapping between two trees that represent Web pages and, as a consequence, the tree edit distance between them.
In this section, we present an algorithm for determining a new type of mapping that we call Restricted Top-Down Mapping.
Intuitively, in the restricted top-down mapping, besides the insertion and removal operations, the replacement operation of different vertices is also restricted to the leaves of the trees.
More formally, we have the following de(cid:2)nition.
DEFINITION 3.
A top-down mapping M between a tree T1 and a tree T2 is said to be restricted top-down only if for every pair (i1; i2) 2 M, such that t1[i1] 6= t2[i2], there is no descendent of i1 or i2 in M, where i1 and i2 are non-root nodes of T1 and T2 respectively.
Figure 4 shows a restricted top-down mapping.
As done for the family of edit distances mentioned before, we can de(cid:2)ne the restricted top-down edit distance between two trees T1 and T2 as the cost of the restricted top-down mapping between the two trees.
The RTDM algorithm combines the ideas presented in [25] and [19].
To determine the restricted top-down mapping between two trees T1 and T2, the RTDM algorithm (cid:2)rst (cid:2)nds all identical sub-trees that occur at the same level of the input trees.
This step is performed in linear time using a graph of equivalence classes, in











 Figure 4: A restricted top-down mapping example a similar way to what is done in [19].
Our algorithm, however, is based on a post-order traversal of the trees.
We can use this much simpler approach because we only look for the identical sub-trees of the same level.
This (cid:2)rst step of the algorithm has linear cost, with respect to the number of vertices in the trees.
Once the vertices in the trees are grouped in equivalent classes, an adaptation of Yang s algorithm [25] is applied to obtain the minimal restricted top-down mapping between the trees.
This second step of the algorithm is shown in Figure 5.
This (cid:2)gure only shows the algorithm version for calculating the tree edit distance, but its modi(cid:2)cation for obtaining the mapping is straightforward.
As we have already mentioned, the traditional top-down edit distance algorithm by Chawathe [5] has a complexity of O(n1n2) for all cases, that is, the best, the expected and the worst cases.
The RTDM algorithm also has a worst case complexity of O(n1n2), but, in practice, it performs much better due to the fact that it only deals with restricted top-down mappings.
The worst case of the RTDM algorithm occurs when the two trees being compared are all identical, except for their leaves.
In all other cases, the cost is amortized by the shortcuts in lines 20(cid:0) 25, which we call the top-down shortcut, or in lines 17(cid:0)18, which we call the bottom-up shortcut.
Further, when all we want to know is whether the tree edit distance is under a given threshold, the shortcut in lines 15 (cid:0) 16 prevents the recursion to continue.
This is a very common situation when we need to cluster trees based on their structural similarities.
We also notice that we can trivially alter lines 20 (cid:0) 25 (the so-called top-down shortcut), to create an algorithm that determines the traditional (i.e., non-restricted) top-down edit distance.
Thus, we also have a new algorithm for the traditional formulation of the problem.
Another interesting aspect of the RTDM algorithm is its (cid:3)exi-bility with respect to the cost of the edit operations.
This property allows using the algorithm in more complex derivations of the problem.
For instance, it allows comparing a given tree instance with a tree pattern of variable size.
This problem is analogous to the problem of matching regular expressions with strings and has been addressed in the literature [26].
In the next section, we show how the RTDM algorithm can be applied to the problem of automatically (cid:2)nding news available on Web sites and extracting their components (e.g., titles, body, etc.)
for further processing.
In this section we discuss a Web news extraction approach that relies on the RTDM algorithm to identify relevant text passages containing news and their components, extract them and discard useless material such as banners, links, etc.
Our approach has basically two main tasks: (1) the crawling of news portals to fetch the pages of interest and (2) the extraction of the news from the
 2 begin

 let m be the number of children of T1 root let n be the number of children of T2 root

 for i = 1 to m for j = 1 to n

























 delete(t1[k]) insert(t2[k]) k k elsif Ci descendents(t1[i]) Cj descendents(t2[j]) d M [i (cid:0) 1; j] +Pt1[k]2Ci i M [i; j (cid:0) 1] +Pt2 [k]2Cj if M [i (cid:0) 1; j (cid:0) 1] > (cid:15) s 1 elsif t1[i] and t2[j] are identical sub-trees s 0 if t1[i] is a leaf s replace(t1[i]; t2[j]) elsif t2[j] is a leaf s replace(t1[i]; t2[j]) s s +Pt2[k]2Cj s s +Pt1[k]2Ci s RTDM(t1[i]; t2[j]; (cid:15)) else insert(t2[k]) k (cid:2) k delete(t1[k]) (cid:2) M [i; j] min(d; i; s); end end return M [m; n] end Figure 5: The RTDM Algorithm.
The functions replace, delete and insert give the costs of vertex replacement, vertex removal and vertex insertion, respectively HTML pages collected.
Since Web crawling techniques have been extensively discussed elsewhere [12], we focus our discussion on the extraction task, in which resides most of our contributions.
To extract the desired news, our approach recognizes and explores common characteristics that are usually present in news portals.
For instance, most news sites have the following organization: (a) a home page that presents some headlines, (b) several section pages (or channels) that provide the headlines divided in areas of interest (e.g., sports, technology, international, etc.
), (c) pages that actually present the news, containing the title, author, date and body of the news.
The goal of our approach is to correctly extract the news, disregarding the other pages.
Our approach relies on the basic assumption that the news site content can be divided in groups that share common format and layout characteristics.
This is rather a safe assumption, since nowadays most of the Web content is built using programs or scripts that read the content from a database, format it, and generate the output as an HTML page.
We call this set of common layout and format features a template.
Figure 6 presents two different templates available in the CNN site.
DEFINITION 4.
A template is the set of common layout and format features that appear in a set of HTML pages that is produced by a single program or script that dynamically generates the HTML page content.
In the case of news, templates are (cid:2)lled by journalists, usually through the use of speci(cid:2)c Web applications or some database interface.
Each (cid:2)eld of a template (e.g., a news title) we call a data-rich object.
Ideally, the extractors generated by our approach should be able to identify each one of these data-rich objects, and discover, among them, which ones correspond to the title and the body of the news.
According to our approach, the extraction task is performed in four distinct steps: (1) page clustering, (2) extraction pattern generation, (3) data matching and (4) data labeling.
Figure 7 illustrates these steps.
In the following sections, we detail each step that comprises the extraction task.
We notice that our approach is simple and orthogonal, once the core of the main steps (clustering, extraction and matching) is the RTDM algorithm, with variations on the cost model for the edit operations.
This (cid:2)rst step takes as input a previously crawled set of pages (a training set) and generates clusters of pages that share common formating/layout features, i.e., share the same template.
Each cluster is later generalized into an extraction structure for a template, in the extraction pattern generation step.
Notice that the cluster algorithm cannot simply group pages by their address (URL), because subtle changes in script or cgi parameters may result in a completely different HTML page.
To generate the clusters, we use traditional hierarchical clustering techniques [23] in which the distance measured is the output of our RTDM algorithm.
There are no pre-de(cid:2)ned number of clusters.
Instead, we adopt a constant threshold to determine if two given clusters should be merged.
In our implementation we used 80% of similarity as the threshold value.
The cost model for this step is the simplest one.
Every vertex insertion, removal or replacement has unit cost.
The replacement of equally labeled vertices has cost zero.
Other works [16] suggest a more sophisticated set of operations, but our experiments have shown that this simple model is effective for our purposes.
The output of this step is a set of page clusters that share the same template.
In this step, our approach generalizes a cluster of pages into what we call a node extraction pattern (ne-pattern).
Formally, an ne-pattern is a tree de(cid:2)ned as follows.
DEFINITION 5.
Let a pair of sibling sub-trees be a pair of sub-trees rooted at sibling vertices.
A node extraction pattern is a rooted ordered labeled tree that can contain special vertices called wild-cards.
Every wildcard must be a leaf in the tree, and each wildcard can be of one of the following types: (cid:15) SINGLE ((cid:1)) A wildcard that captures one sub-tree and must be consumed.
(cid:15) PLUS (+) A wildcard that captures sibling sub-trees and must be consumed.
(cid:15) OPTION (?)
A wildcard that captures one sub-tree and may be discarded.
(cid:15) KLEENE ((cid:3)) A wildcard that captures sibling sub-trees and may be discarded.
We can think of an ne-pattern as a kind of regular expression for trees.
We call a wildcard every vertex in the tree that can match any symbol (any label) with its associated type.
Our purpose in this step is to assure that each wildcard corresponds to a data-rich object in the template.
Single and plus wildcards should correspond to
 Pages + * + ?
ne patterns Figure 6: Some templates available in the CNN site + * + ?
+ Clustering Extractor Generation * + ?
ne patterns Crawled Pages + + * + + ?
<title> ... </title> <body> ... </body> <title> ... </title> <body> ... </body> <title> ... </title> <body> ... </body> <title> ... </title> <body> ... </body> Data Matching Data Labeling Figure 7: The main extraction steps required objects, such as the title of a news, and option and Kleene wildcards should correspond to optional objects, such as related news lists.
Further, we say that an ne-pattern accepts (or matches) a given tree if there is a mapping with no in(cid:2)nite cost between the ne-pattern and the target tree.
We de(cid:2)ne formally this concept and the cost model associated with this mapping in Section 4.3.
The goal of this step in the extraction task is, taking as input a page cluster, to generate an ne-pattern that accepts all the pages in this cluster.
Thus, the content differences between the pages in the cluster are modeled as wildcards in our ne-patterns.
To generate such ne-patterns, we rely on what we call a composition operation, de(cid:2)ned as follows.
DEFINITION 6.
Let T x the composition of T x that:




 3 such
 (cid:15) Let S1 be the set of trees accepted by T x
 (cid:15) Let S2 be the set of trees accepted by T x

 (cid:15) Let S3 be the set of trees accepted by T x (cid:15) Then S1 [ S2 (cid:18) S3.
The process of generating an ne-pattern consists of iterating all the trees that represent the pages in the cluster and incrementally composing one to each other in the cluster.
Notice that any tree can be seen as an ne-pattern without any wildcard.
At the end of the process, we have an ne-pattern that accepts all pages in that cluster.
Let us see how we can use the RTDM algorithm to implement the composition operation.
First, we say that vertices a and b of an ne-pattern are equal if and only if: (cid:15) a and b are wildcards and both are of the same type; (cid:15) a and b are not wildcards and the labels associated with a and b are equal.
This is the equality operator for the RTDM algorithm.
As a cost model, we give the same weight, 1, to any edit operation in the
 gorithm to obtain a mapping MT x create the composite ne-pattern T x rules:



 2 using the following

 (cid:15) if a is not in the mapping, then add a0 to T x 3 where a0 = f (a; ?
); (cid:15) if a maps to b then add a0 to T x (cid:15) and f (a; b) is de(cid:2)ned as: 3 where a0 = f (a; b); f ((cid:3); (cid:3)) = (cid:3) f ((cid:3); +) = (cid:3) f ((cid:3); ?)
= (cid:3) f ((cid:3); :) = (cid:3) f ((cid:3); n) = (cid:3) f (n1; n2) = n1 f (n1; n2) = : f (+; +) = + f (:; :) = : f (+; :) = + f (:; ?)
= ?
f (+; ?)
= (cid:3) f (:; n) = : f (+; n) = + f (?
; ?)
= ?
f (?
; n) = ?
if n1 and n2 have identical labels if n1 and n2 have different labels where n, n1, n2 are non-wildcard vertices and the parameter order is not relevant.
The motivation behind this set of operations is that optional vertices of the template that the ne-pattern is trying to model should be kept optional after composing the ne-pattern with a new tree, and higher quanti(cid:2)ers (i.e., Kleene and plus) should be kept in the (cid:2)nal ne-pattern.
Non-wildcard vertices in the ne-pattern that are mapped to different (as de(cid:2)ned by our equality operator) non-wildcard vertices in the tree being composed should result in new wildcards.
We notice that some data-rich objects in the pages might span through several sibling sub-trees, like a text of a news body that is composed of many adjacent paragraphs.
Capturing each of these objects as a single entity is the purpose of the plus and Kleene wild-cards.
the de(cid:2)nition of If we look carefully at the function f (a; b) above, we will see that there is no wildcard quanti(cid:2)er (cid:147)pro-motion(cid:148) policy, or, in other words, wildcards plus and Kleene will never be generated if there are no plus or Kleene wildcards in the input of the function.
These wildcards are created in a post-processing step whenever we compose two ne-patterns.
This post-processing is actually quite simple.
Every wildcard followed by a set of option wildcards should be converted into a wildcard for variable size objects, that is, Kleene or plus wildcards.
If the wildcard before the set of option wildcards is a single or a plus wildcard, then the set of option wildcards and the precedent wild-card are converted to a plus wildcard.
If the wildcard is an option or Kleene wildcard, then both this wildcard and the adjacent option wildcards are converted to a Kleene wildcard.
Figure 8 illustrates the whole ne-pattern generation task, including the (cid:147)promotion(cid:148) of a wildcard.
In our approach, even if wildcards are separated by a maximum of 3 non-wildcard vertices they can be merged (including the non-wildcard vertices) into a single variable size wildcard (plus or Kleene).
In this step, our approach matches the set of generated ne-patterns to the set of recently crawled pages.
To (cid:2)nd the most appropriate ne-pattern to a crawled HTML page, we again rely on our RTDM algorithm.
Before discussing the cost model for the matching step, we need to understand what the intuition behind the matching of the ne-patterns is.
In this context, we say that, in a given mapping, if one wildcard vertex in the ne-pattern maps to a vertex in the target HTML tree, then the wildcard consumes the vertex.
Now let us de(cid:2)ne the desired behavior for a mapping between the ne-pattern and the target tree, so that we can create an appropriate cost model.
DEFINITION 7.
We de(cid:2)ne a match between an ne-pattern and a target HTML tree as a mapping such that the following rules are satis(cid:2)ed in this order:
 identical vertex in the target tree.
wildcard vertex in the ne-pattern or be consumed by a wild-card.
tree.
target tree.
tree, if it is possible.
the target tree, if it is possible.
of the target tree as possible.
trees of the target tree as possible.
The satisfaction of Rules 1, 2, 3 and 4 is enough to guarantee that the ne-pattern accepts the target tree.
Rules 5 and 6 assure that the match is as tight as possible, or, if it is possible to use an optional wildcard without violating the acceptance condition, it must be used.
Rules 7 and 8 are always automatically satis(cid:2)ed, and are declared to help understanding the behavior of the ne-pattern.
The equality function for the RTDM algorithm is very simple.
Non-wildcard vertices with identical labels are equal and the equality comparison with a wildcard vertex always fails.
Let a be a vertex in the ne-pattern, and b a vertex in the target tree.
We de(cid:2)ne the cost model for the RTDM algorithm as follows: (cid:15) Vertex Replacement (A) a is a wildcard !
0 (B) else !
1 (cid:15) Vertex Insertion (C) There is an ancestor of b such that it is consumed by a wildcard !
0 (D) The left sibling of b is consumed by a (cid:3) !
0 (E) The left sibling of b is consumed by a + !
0 (F) else !
1 (cid:15) Vertex Removal (G) a =?
or a = (cid:3) !
1 (H) else !
1 The replacement cost (A) guarantees that only wildcards can be replaced by the sub-trees they consume.
The insertion cost (C) allows complete sub-trees to be consumed by the wildcards.
Costs (D) and (E) allow wildcards to consume lists of sibling sub-trees.
The vertex removal cost (G) assures that only optional wildcards can be deleted, and it associates a nonzero cost with the deletion of an optional wildcard, so they are preferably covered by cost (A).
Finally, costs (B), (F) and (H) together guarantee that the ne-pattern must accept the target page, or the mapping will have in(cid:2)nite cost.
...
Page Cluster Required wildcards If a vertex in the source tree maps to a vertex with different label in the target tree, we consider it as a required wildcard, since it is present in both trees.
Variable size objects The presence of optional wildcards following another wildcard is the evidence of a variable size object.
After creating each new pattern, we look for wildcards followed by a series of optional wildcards and create a new wildcard that can capture variable sized objects.
?
Optional wildcards When a vertex in a tree has no equivalent in a target tree, we consider the presence of the vertex optional in our extractor and generate an optional wildcard.
.
            ?
?
  +
 ...
?
Figure 8: How an ne-pattern is created from a cluster of similar pages Although costs (C), (D) and (E) seem quite complicated at a (cid:2)rst glance, they are trivially implemented in constant time.
To check the validity of any of them, we just need to check if the vertex in the target tree is being inserted in the position of (or immediately after) a wildcard in the ne-pattern.
This cost model guarantees that either the conditions in De(cid:2)nition 7 are satis(cid:2)ed or the mapping has in(cid:2)nite cost.
Once the ne-pattern has been selected, the extraction process is straightforward.
Both trees (the ne-pattern and the HTML page) are traversed in pre-order and for each wildcard found in the ne-pattern, the text passage in the vertices consumed by the wildcard is extracted from the HTML page.
Figure 9 illustrates the matching process.
The output of the data matching step is a set of ordered text passages, each one corresponding to a set of vertices consumed by a ne-pattern wildcard.
More formally, we can de(cid:2)ne the output of a match as a set T = (t1; p1); (t2; p2); :::; (tn; pn) where each ti is a text passage retrieved by a wildcard and pi is the vertex position of this wildcard if we perform a pre-order traversal of the ne-pattern.
The goal of the data labeling step is to select from T the passages ti and tj that correspond to the title and the body of the news being extracted from the Web page2.
To achieve this, we apply simple heuristics to T as discussed bellow.
Given a set of extracted passages T = (t1; p1); (t2; p2); :::; (tn; pn) we say that: (cid:15) length(ti) is the number of terms (words) in passage ti; (cid:15) j tk \ ti j is the number of terms that occur in passages tk and ti; (cid:15) ti is a news body iff length(ti) > length(tk) 8 1 < k < n; k 6= i and length(tk) > 100 (Body labeling heuristics); (cid:15) tj is a news title iff 1 (cid:20) length(tj) (cid:20) 20 and jtj\tij > pj(cid:0)pi jtk\tij pk(cid:0)pi 8 1 < k < i; k 6= j (Title labeling heuristics).
because we can trivially determine it from the date the news (cid:2)rst appeared on the Web site.
In other words, the passage elected to be the body of the news is the longest one with more than 100 words.
Further, the passage selected to be the title is one that has ranges from 1 to 20 words, has a maximum intersection with a body passage, and is the closest one to the body.
The intuition behind the title selection is that most of the times the title is placed near the body and its terms usually appear in the news body.
Despite using this simple heuristics, our labeling strategy is very effective, as shown next by our experiments.
Our experiments were run using 4088 HTML pages collected from 35 different Brazilian online news sites.
The sites chosen are the most popular vehicles from the Brazilian press, including countrywide newspapers, and main regional publications.
All the experiments were carried out using a 700MHz Pentium III processor with 128MB of RAM.
agencies, magazines news Considering that the RTDM algorithm is the basis of the news extraction approach described in this paper, we must assure that it runs fast and scales well.
To the best of our knowledge, there is no other restricted top-down mapping algorithm in the literature, so we decided to compare the RTDM algorithm with the competitive top-down edit distance algorithm presented by Chawathe in [5].
Adapting Chawathe s algorithm to the extraction pattern generation and data matching steps of our approach is not trivial, but the page clustering step can be easily adapted to use this algorithm.
Thus, we built two versions of the clustering step, one powered by the RTDM algorithm, and the other one powered by Chawathe s.
Comparing the executions times, the RTDM algorithm in general outperforms the alternative algorithm by 4 times, but sometimes it is more than 10 times faster.
The main disadvantage of Chawathe s algorithm is that it is always quadratic with respect to the number of vertices of the trees being compared.
Figure 10 shows how the algorithms perform when the average number of vertices of the trees being compared increases.
If we analyze the behavior of the
  * +




 ?
...
+


 ?
The extraction result Each wildcard of the ne pattern consumes a set of vertices of the target tree.
Each set of vertices results in one data rich object.
In the example below, two data rich objects (BC and F) were extracted.
=







 Matching the ne patterns Each HTML page is converted to tree, and a set of ne patterns is matched against the tree.
The first pattern matches with cost 1, because it discards its Kleene wildcard.
The second pattern matches with 0 cost and is the selected ne pattern.
The last pattern fails to match because there is no possible mapping for the G vertex.
Figure 9: How ne-patterns are matched with Web pages RTDM algorithm when the number of vertices grows, we see that it depends not only on the number of vertices in the trees, but also on the properties of the trees.
This is due to the several shortcuts that the algorithm uses to avoid recursively checking the complete trees and to the different properties of the restricted top-down mappings.
Each point in Figure 10 roughly corresponds to a cluster.
Figure 10: RTDM and Chawathe s algorithm - the bezier approximation of the curves shows that Chawathe s algorithm has quadratic growth
 The second part of our experiments consisted of analyzing the output of the complete extraction process.
We manually compared the extracted news with the original HTML pages, to check for their correction and completeness.
Table 1 presents the results for all 35 sites.
Our approach was able to extract correctly an average of 87.71% of the news, while 9.25% were erroneously extracted and 3.04% were not extracted.
During our experiments, we noticed that the use of restricted top-down mappings is really suitable for identifying data-rich portions of Web pages.
In the data labeling step, however, it is still dif(cid:2)cult to precisely identify the title of the news.
Most of the errors were due to subtitles and authors names that were misidenti(cid:2)ed as titles.
Despite this, we achieved very good results with a completely automatic approach and simple labeling heuristics.
Even though we have used simple labeling heuristics, the reason for this high level of effectiveness is that, after the extraction of the data-rich portions of the pages, the size of the set of candidates text passages for title and body is usually reduced from a range of hundreds to thousands to a range of two to (cid:2)ve candidates.
One of the reasons why the Web has achieved its current huge volume of data is the fact that a great and increasing number of data-rich Web sites have their pages automatically generated from databases.
Taking advantage of this, a number of approaches have been recently proposed to analyze the structure of the pages of these Web sites with the purpose of inferring a general data schema for them and ultimately generating wrappers to extract this data.
The (cid:2)rst solution for this problem was proposed by Grumbach and Mecca [11] assuming the existence of collections of data-rich pages bearing a similar structure or schema.
In [7], an algorithm is proposed to infer union-free regular expressions that represent page schemas.
For complex schemas with optional attributes, the algorithm execution can explode and thus it is considered as having exponential cost [7].
By using several heuristics, Arasu and Garcia-Molina [1] have recently proposed a polynomial time algorithm for the problem.
Since the approaches proposed in [1] and [7] require no human intervention, an important problem that they have left open is how to automatically label the extracted data.
This problem is addressed in [2] but the solution proposed is not general enough.
There are also several works in the literature that address the problem of schema extraction from collections of XML documents.
The XTRACT system [10] uses MDL, an information theory technique, to infer concise and accurate schemas from a collection of XML documents.
Min et al. presented a much faster system, with better results in [15].
Although we do not directly consider the schema extraction problem in this work, the ne-patterns we generate resemble schema de(cid:2)nitions, and we believe that the techniques proposed here can also be applied to the schema extraction problem.
Also, the ideas behind the XML schema extraction systems can be used to improve our work in situations in which the data
 A not (cid:17)cia Joenville AOL Brasil Ag(cid:136)encia Estado Correio Brazilense Correio da Bahia
 Di ario de Natal Di ario Grande ABC Di ario do Maranh(cid:152)ao Di ario Popular Di ario de Cuiaba Di ario do Com.
BH Estado de Minas Estado de S(cid:152)ao Paulo Folha de Pernam.
Folha de S(cid:152)ao Paulo Gazeta Digital Gazeta Mercantil Hoje em Dia IDG Now ITWeb InvestNews Jornal da Tarde SP O Dia RJ O Globo Tribuna Santos Tribuna da Bahia Tribuna da Imprensa
 Valor On Line Verdade On Line Vox News Yahoo Zero Hora Total p (cid:2)




















































 Not Extracted


































 # pages


































 Table 1: Results obtained for the news extraction process.
being extracted is ruled by more complex schemas than those of found in online news.
Actually, the problem of schema extraction for Web pages has been proven NP-Complete recently [24].
The automatic classi(cid:2)cation of Web pages based on their structure is addressed in [8].
However, this work differs from ours since in our case the classi(cid:2)cation is based on the structural properties of the pages and not on the results of the wrapping process.
The ChangeDetectorT M system [4] uses an algorithm very similar to ours in its entity-based change detection step.
The algorithm, however, works with hashes of the contents of the subtrees, falling back to the tree view when any hash comparisons fails.
This is equivalent to our bottom-up shortcut.
Furthermore, when aligning child vertices, it does not take into account the cost of the recursive operations.
Bing Liu et al. have developed an effective algorithm for mining data records from Web pages [14].
The algorithm has two steps.
In the (cid:2)rst step it identi(cid:2)es the data region of the Web page and in the second one it extracts the records themselves.
The algorithm works each time in a single page, so it does not compare the page trees.
Although achieving good results, the algorithm only works with multi-record pages and therefore cannot be applied to online news pages, that are almost exclusively single-record pages.
Compared to the recent work in the literature, the work in this paper offers an alternative and uniform solution for three important problems in automatic Web data extraction: structure-based page classi(cid:2)cation, extractor generation, and data labeling.
The fact that this solution is based on the well established concept of tree-edit distance brings the additional advantage of allowing the use of existing results for studying these problems from a new perspective.
In this paper we have introduced a new algorithm for calculating the edit distance between two given trees, which is based on a restricted form of top-down mapping.
This algorithm, which we call RTDM, improves existing results in the literature [5, 25] for the problem of automatically analyzing the structure of Web pages.
Furthermore, we show how this algorithm can be applied to solve three important problems in automatic Web data extraction, namely: structure-based page classi(cid:2)cation, extractor generation, and data labeling.
In particular, we have addressed the problem of automatically (cid:2)nding and fetching news available on Web sites, and extracting their components.
Through experimentation with 35 news Web sites, we have demonstrated that the RTDM algorithm is highly effective for these tasks.
Indeed, the results show an average of 87.71% correctly extracted news without any human intervention.
The approach provided by the RTDM algorithm is currently being used as the core of a fully operational Web news clipping sys-
most important Brazilian newspapers to over (cid:2)fty companies.
As future work, we plan to generalize the proposed approach to deal with different application domains, especially those in which the schema of the data on the pages is complex.
In fact, it is a challenge to provide a generic method for automatic Web data extraction [24].
Furthermore, we plan to use the RDTM algorithm to improve Web search engines by incorporating structural evidences derived from Web pages in addition to content evidences traditionally used by current search engines.
This work is partially supported by project GERINDO (grant MCT/CNPq/CT-Info 552087/02-5) and by the fourth author s individual CNPq grant 304890/02-2.
