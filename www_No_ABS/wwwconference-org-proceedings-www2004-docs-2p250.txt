Streaming media through overlay networks has received much attention recently [1]-[5].
In this paper, we present a novel multi-cast streaming framework, Gossip Based Streaming (GBS), in which an overlay is construct based on a random graph topology, and streaming contents are not come from a single upstream source, but delivered from several sources to a client, like in gossiping protocols [6,7].
This is quite different from many existing tree-based overlays, as shown in Figure 1.
The principle of distributed gossip communication is simple: in each time slot, a node in the network (randomly) selects another node as a communication partner, and exchanges information with each other.
Thus, over the time, contents will spread throughout the network in an epidemic fashion.
A key challenge for gossiping based streaming, however, is to minimize delay and date outage, as continuity is important in a stream playback.
GBS employs an unstructured overlay (random graph) with a bootstrapping node.
A system diagram of each GBS node is shown in Fig. 2.
When a new node joins the system, it first contacts the bootstrap node to obtain a candidate list of neighboring nodes.
The new node then contacts these candidates to explore other nodes, finally to set its neighbors in the overlay.
After join-Copyright is held by the author/owner(s).
ACM xxx.
Jiangchuan Liu Department of Computer Science and Engineering The Chinese University of Hong Kong Shatin, N.T., Hong Kong ljc@cse.cuhk.edu.hk ing the overlay, a node will continuously probe other nodes to maintain a set of best neighbors, in general, of size 3 to 4.
The procedure of streaming for this node begins from establishing connections to the neighbors.
We assume that a video content is divided into blocks of a fixed-size.
Once connected, a node will fetch a buffer map from each of its neighbors, which indicates the availability of each block in the neighbor.
(a) Fig. 1.
Overlays structures.
(b) (a) Application-layer multi-cast; (b) GBS.
Fig. 2.
System diagram for GBS.
After that, the node s scheduler continuously makes decisions on which block is to be fetched from which neighbor.
A simple scheduling algorithm is given as below, where the demanded blocks is transferred from the neighbors in a round-robin fashion.
This enables balanced data delivery among the neighbors.
Moreover, during streaming, a node should also notify all its neighbors about its buffer status changes, which ensures that a node is aware of the up-to-date buffer status of its neighbors.
Such updates can be piggybacked in data blocks to reduce network traffic.
Scheduling algorithm at a node If local buffer is not full:
 fetched from it);
 able at (cid:1) but not in the local buffer;


 In a dynamic environment, each node could fail or leave at will.
The failure (or leave) may cause data outage in some related nodes before new delivery paths are formed.
For streaming applications, the outage is highly undesired because it generally leads to discontinuity in playback.
In this section, we sketch our analytical results of the outage rate for overlay networks.
We first consider a tree-based application-layer multicast, which serves as a baseline in our performance evaluation.
In such an overlay, each node has up to (cid:3) children, but has only one parent from which it can receive streaming contents.
Assume each direct downstream node of the failure node has probability (cid:4)(cid:5) of not finding a new upstream node within (cid:6)(cid:1) time, it can be calculated that the average outage, i.e., the number of nodes suffer from data outage in (cid:1)(cid:6) time, is (see [6] for details): (cid:1)(cid:2) (cid:3)(cid:6)(cid:7) (cid:2) (cid:8) (cid:3) (cid:3) (cid:3) (cid:3) (cid:4) (cid:3) (cid:5) (cid:4) (cid:3) (cid:2) (cid:9) (cid:3) (cid:4) (cid:2)(cid:5) (cid:3) (cid:5) (cid:10) (cid:6) (cid:1) (cid:4) (cid:3) (cid:8) (cid:3) (cid:3) (cid:2) (cid:2)(cid:5) (cid:3) (cid:3) (cid:3) (cid:2)(cid:5) (cid:3) (cid:3) (cid:5) (cid:3)(cid:2)(cid:5) (cid:5) (cid:8) (cid:3)(cid:3) (cid:5) (cid:10) (cid:6) (cid:1) (cid:4) (cid:3) (cid:3) (cid:5) (cid:10) (cid:6) (cid:1) (cid:4) (cid:3) (cid:3) (cid:2)(cid:5) (cid:5) (cid:10) (cid:6) (cid:1) (cid:4) (cid:8) (cid:3) (cid:5) (cid:3) (cid:3) (cid:3) (cid:5) (cid:4) (cid:3) (cid:3) (cid:3) (cid:5) where (cid:8) is the height of the multicast tree, R is the average failure rate of node.
In GBS, however, each node has (cid:3) neighbors and all of them can provide streaming contents to the node, which potentially lead to much better robustness.
Based on a random graph model, we can calculate the data outage rate (cid:1)(cid:2) (cid:3)(cid:11)(cid:7) for GBS as (cid:3)(cid:3) (cid:3)(cid:3) (cid:2) (cid:3)(cid:3) (cid:3) (cid:3) (cid:2) (cid:5) (cid:2) (cid:2)(cid:2)(cid:5) (cid:3) (cid:2)(cid:5) (cid:3)(cid:2)(cid:5) (cid:2)(cid:2)(cid:5) (cid:1)(cid:2) (cid:7) (cid:11) (cid:9)(cid:5) (cid:4) (cid:10) (cid:6) (cid:1) (cid:5) (cid:10) (cid:6) (cid:1) (cid:3) (cid:4) (cid:4)(cid:5) and (cid:3) (cid:2)(cid:5) is the probability that a node itself can support stream-where ing to all its neighbors, i.e., serving as a parent node like in the (cid:2)(cid:5) depend on network delay and band-tree case.
Both width, and can be estimated given any specific configurations.
We have conducted several preliminary experiments to investigate the performance of GBS.
Fig. 3 shows the node data outage as a function of the overlay size.
For comparison, we also show the analytical results as well as the results for tree-based application-layer multicast, which operates as follows: when a new node joins the overlay, it first finds the oldest node that can accept a new child (less than (cid:3) children) and then retrieves the streaming content from the node.
It can be seen that the average node data outage of GBS is much lower than that of the tree-based application-layer multicast.
The difference is particularly remarkable for large overlays.
As an example, for an overlay of 5000 nodes, the data outage of GBS (2.5%) is nearly an order of magnitude lower than that of tree (20%).
This implies that GBS is more scalable in dynamic environment than a tree based structure.
This paper has presented the basic design of Gossip Based Streaming (GBS).
We have sketched some analytical results, particularly for the data outage rate, which matches well with the simulation results.
More importantly, both simulation and analytical results reveal that GBS performs much better than tree-based overlays in dynamic environments, where nodes could fail or leave at will.
We are currently improving the GBS framework along three directions:
 oping better scheduling algorithms for retrieving blocks from neighbors;
 tions are generally delay-sensitive; some preliminary results can be found in [9]
 and building a prototype based on the PlanetLab platform, in which we are currently participating.
m=4, Po=0.1, Ps=0.5, R=0.001, D t=10 theoretical tree theoretical GBS simulation tree simulation GBS




 e g a t u o a t a d e d o n e g a r e v







 Overlay Size



 Fig. 3.
Data outage as a function of overlay size.
The Y-axis is the average percentage of nodes that suffer from data outage in a period of (cid:1)(cid:6) seconds.
This work was supported by grants from the Research Grants Council of the Hong Kong Special Administrative Region, China, under Grant CUHK C001/2050312.
