Facing the complexity of dynamically generated corporate web sites, it becomes increasingly di(cid:14)cult to understand user navigation patterns without deep knowledge of the content and the semantical linkage of the pages.
Another challenge for web site owners as well as web strategists is to keep track of the information structures on the web site in the content management system.
Following recent research concerning integration of web structure, content and usage mining [1] [2], we intend to provide an insight into the relationship between the structure and the content.
We (cid:12)rst try to extract the key content from each page taking its hypertext markup into account.
Then, we reason about the role of the web structure[3] since we believe that the linkage created by the web designer conceals a semantic connection between web pages.
Copyright is held by the author/owner(s).
Before associating structure and content with each other, we will try to de(cid:12)ne them.
The structure of a web site is determined by the link structure between its web pages.
A link in this context is considered to be a link encoded as a HTML tag.
We do not consider the anchor text of a link as structural information since we believe it belongs to the content information.
Excluding self-references, we focus on hard coded inter-page links represented by a directed cyclic graph.
The content of a web site may consist of text, hypertext, meta information or multimedia content like pictures, (cid:12)gures, video or sound.
Here, we concentrate on text and html since they contain most of the information.
As more and more web sites use di(cid:11)erent media types it could be reasonable to include those in future work .
In order to combine structure and content, we have to map structural and content information to a common concept.
We achieve it by putting them in a common data structure, in order to make them comparable.
Next, we combine the textual content of a web page with linked contents, and thus we implicitly create a content-structure graph.
In this section we describe how to create a content - structure graph and compare it with the structure graph of a web site.
Thereafter, we present a way to measure the distance between both.
From each page we extract the links within a web site generating an implicit directed cyclic structure graph.
We consider site-internal links for each web site, respectively.
In order to describe the content, we extract all words and stem them.
Further more, stop words are (cid:12)ltered as we are only interested in content, and not in style or grammatical information.
Next, we calculate word frequencies per web page.
We assume that the most frequent words de(cid:12)ne the content of the web page.
The highest ranked words are said to be key words, where their number is proportional to the text length.
Not all key words contribute to the semantic: very frequent key words like the company s name, occurring virtually on all pages, are pruned.
Whereby the pruning threshold must be determined empirically.
Additionally to





 x i
 LCp i



 Figure 1: The Local Context LC the key words, phrases from HTML tags are taken into account since we believe that they re(cid:13)ects author s focus.
This could be for example, <h1> which typically describes the major topic.
For a single page, we combine its set of key words with the key words of its direct neighbourhood.
The latter is de(cid:12)ned as the pages, having a direct link to (inedges) or from (outedges) the page in focus (see (cid:12)gure 1).
We call it the local context (LC) of a page xi where: LC(xi) = fxj jxj 2 (inedges(xi) [ outedges(xi))g (1) We perform the combination of structure and content by generating a new set of key words.
The new set is initiated with the own key words of the processed web page.
Now, involving the site structure, we add the key words from the LC and accumulate the frequencies over the entire key word set.
Additionally, the anchor tags from the LC are included as key words.
To avoid overweighting of the LC, we assigned low constant frequencies to the key words originating from it.
Then, we arrange the set according to the accumulated frequency.
The number of words, which we keep, is again proportional to the text length.
We perform this process bottom-up with respect to the minimal click path from the entry point of the web site.
For already processed pages we use the new combined descriptions.
We interpret the results of our algorithm as an intersection between the contents of the LC and the particular page.
Moreover, we assume that salient words ascend towards the root page.
In the empirical study, we will discuss the relationship between the key word set which describes the page alone content and the new combined set.
We selected corporate web sites of Bayer, Motorola and Siemens in order to evaluate the performance of the above introduced algorithm.
For a random set of test pages, we compared the automatically generated key word sets with manually compiled abstracts.
Studying the results, we observed that the page key words (like those in the left column of table 1) cover nearly all topics mentioned in the abstracts which was desired.
Next, we analyzed the results of the mapping from section
 Comparing this combined set with the manual abstract as well as with page key words, we observed that some key words - topics fell out.
These key words carried page speci(cid:12)c information.
In contrast, the newly inserted words re(cid:13)ect topics present in the LC of the page in focus.
A manual review of the LC con(cid:12)rmed the relevance of the new words.
Page key words Network revenue Wireless Quality Operators subscribers Infrastructure Operability Capacity and Coverage LC key words




 Support







 Brochure








 Table 1: www.motorola.com/networkoperators Regarding the two di(cid:11)erent sets of key words, the di(cid:11)er-ences between them can be interpreted as follows.
In the case of a homogeneous LC with topics matching the content of the page in focus, we observed no or only slight changes in its key word set.
On the other hand, pages dealing with di(cid:11)erent topics than its homogeneous LC, showed signi(cid:12)-cant di(cid:11)erences.
Whereas a heterogeneous LC, regardless of the topic of the page it is unlikely to have any signi(cid:12)cant in(cid:13)uence on the resulting set.
While evaluating the mapping, we observed several e(cid:11)ects which led us to improvements: In contrast to the page-wise extracts, the HTML tag based key words were removed from mapping in order to rule out local information, speci(cid:12)c only to one page in the LC.
On the other hand, we kept the anchor tags linking to this page as they enhance the relation between structure and content.
Our experiments showed that by excluding inedges from the LC, the key words re(cid:13)ected the relationship of the page and the subsequent contents more suitably.
By  subsequent  we imply that, except for navigation links, the outgoing links lead to more detailed information on the parent topic.
We showed that our approach is able to make structure and content comparable.
In this way, our approach can indicate whether content of a page (cid:12)ts to content of the web pages in its neighborhood whereas the neighborhood is de(cid:12)ned by the link structure.
Additionally, we implicitly identify topics which span over several connected web pages.
Which in a way leads us to discovering of semanti-cal relationships.
With our algorithm we can support web designers and strategists by comparing their intentions with the actual structure and content of a web site.
A subsequent redesign of a web site incorporating our results can improve user perception and customer retention.
Our future research includes developing more di(cid:11)erenti-ating measurements for the structure and content analysis.
Furthermore, we (cid:12)nd it interesting to combine our approach with usage patterns to improve the topic identi(cid:12)cation capabilities of our algorithm.
