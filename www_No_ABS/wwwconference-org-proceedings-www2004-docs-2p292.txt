The design of ef cient crawlers is a key issue in the design of Web search engines [1].
Whereas the crawlers of large search engines can exploit dedicated network connections, departmental search engines usually share the network bandwidth with other services.
For this reason, bandwidth control is a key point of crawler design.
The design of the bandwidth control policy must achieve two contrasting goals.
First, the crawler should not hinder the users who share the connection; second, it should exploit completely the assigned network resources.
Recently, ef cient crawling strategies have been studied either based on the importance of the documents [2, 4] or on the topic of the pages [3, 5].
Focused crawlers aim at maximizing the quality of the retrieved resources but do not consider the instant bandwidth in use.
Instead, bandwidth control can be achieved by optimizing the number of parallel downloads [6].
In this paper, we propose a policy to control the bandwidth of a crawler, which is particularly suited for small focused search engines.
The crawler collects statistical data on the transfer rates of each server, which are then used to decide the optimal number of parallel downloads.
We assume that each URL is assigned a score, which measures Copyright is held by the author/owner(s).
the estimated importance of the related document.
The crawler selects the next URL to download by popping it from a priority queue sorted according to this score.
In the case of focused crawlers, the score may represent the probability that the document is on a desired topic or that it is on a path leading to relevant pages.
In our design, the crawler is composed of a set of fetchers and a fetcher manager.
Each module is run as a separate thread and the pool of fetcher threads are created upon startup.
A fetcher waits until the manager activates it for retrieving a given URL.
Then it performs an HTTP request to the appropriate Web server and, after having transferred the document, it returns to a wait state.
The manager implements the crawler control policy by feeding appropriately the fetchers.
The manager selects the URLs to be retrieved using their priorities and it activates the optimal number of fetchers to exploit the assigned bandwidth.
In order to select the next URL u, the manager estimates the bandwidth B(u) which a fetcher is expected to consume by retrieving the document u.
When the download is completed, the manager can measure the actual transfer rate M (u) and use this value to update its estimate of the server speed.
The manager can compute the expected total transfer rate of the crawler CB as u S B(u), where S is the set of the documents that are currently assigned to the fetchers.
(cid:1) Assuming that the crawler has already activated the optimal number of fetchers, when a fetcher completes its current download uc, a slot of bandwidth becomes available to start one or more downloads.
Thus, the crawler bandwidth is updated as CB = CB   B(uc) and the manager searches the URL queue for a document ud such that the estimated bandwidth will remain below the assigned threshold L while downloading it, i.e. CB + B(ud)   L. The search starts from the top of the queue and continues up to a given maximum depth maxd, in order to consider only the documents with higher priority.
If the search is successful, the manager activates a fetcher to download ud, otherwise it stops and waits for the completion of another download.
If the expected bandwidth of the new download does not consume completely the available slot, the queue is searched for further candidates.
The algorithm to activate the fetchers is the following.
byte/s






































 Figure 1: Measured bandwidth (continuous line) and predicted bandwidth (dotted line) in a forty seconds time window.
Figure 2: Measured bandwidth with maxd = 6 (continuous line) and with maxd = 2 (dotted line).
update CB = CB + B(ud)

 The manager periodically checks the queue to avoid the starva-those servers whose transfer rate tion of the fastest servers (e.g.
exceeds the threshold L).
The statistics on the server transfer rates are stored in a lookup table indexed by the server IP address.
The transfer rate is measured dividing total number of transferred bytes by the time interval from the issue of the HTTP request till the completion of the document download.
This measure considers both the network delivery latency and the server response speed.
Thus, the measured transfer rate depends both on the characteristics and state of the network links connecting the machine running the crawler to the server and on the speed and current load of the server machine itself.
Since the network state and the server load can vary during time, we decided to model this dependency by assuming a hourly distribution: we estimate the bandwidth B(IP, h, w) for each hour h = 0, .
.
.
, 23 both for working days (w = 1) and holidays (w = 0).
This yields a total of 48 values for each server.
We found experimentally that this assumption guarantees suf cient accuracy in the estimates.
We decided to use a discrete distribution to reduce the algorithm complexity.
The values B(IP, h, w) are updated after each download by using the measured bandwidth M (IP ).
If the measure is obtained at time t (in minutes) of the day, each estimate is updated according to B(IP, h, w) = (1 (t 60h))B(IP, h, w)+ (t 60h)M (IP ), where  (y) = key2/ 2 , being k and   design parameters, and w is chosen accordingly with the current date.
This formula was derived assuming that the bandwidth measured at time t affects the estimates for the parameters according to a Gaussian function  (t).
This approach to bandwidth prediction is feasible for small departmental and focused search engines, which are likely to repeatedly contact only a limited set of servers.
In order to evaluate the proposed algorithm, we decided to force the crawler to download the URLs contained in a prede ned queue.
This approach allows to compare the results since the same queue is used for all the tested settings.
In the  rst experiment, the bandwidth limit was set to 140KB/s and the actual bandwidth was measured every second.
The parameters used to update the estimates were k = 0.3 and   = 120, while the maximum search depth maxd was set to 6.
Figure 1 shows that

 s / e t y






























 Figure 3: Measured bandwidth for a crawler with simple control on the number of fetchers.
the predicted bandwidth (dotted line) is close to the actual bandwidth (continuous line) and it is always below the assigned threshold.
The choice of the maximum search depth maxd affects the actual use of the available bandwidth, since by using a higher search depth it is more likely to  nd a download which  ts the available slot.
Figure 2 compares two runs of the crawler with maxd =
 ment proves that a larger maxd improves the average bandwidth (71, 788 for maxd = 6 versus 64, 562 for maxd = 2).
However, when maxd increases, some URLs with a smaller score are downloaded before URLs with a lager score.
Finally, we compared our method with the one proposed in [6].
In this case, the bandwidth is controlled by stopping a fetcher when the actual bandwidth is above the limit and by starting a new one when it is below the limit.
Figure 3 shows that the control is less accurate and the bandwidth oscillates considerably.
In this paper we have presented a technique to control the bandwidth of a crawler.
The approach is particularly useful for small and focused search engines and the experiments, even if preliminary, have showed its effectiveness.
