A wealth of new applications is being spurred on the Web by the wish of contents providers to assume a more active role in contents delivery: query-free search, advertising services (e.g.
Google s AdWords and AdSense), editorial promotions, shopping advice (e.g.
Amazon), recommending systems, matchmaking applications in e-commerce.
An unusual feature of this class of applications is that they involve retrieval operations where the direction of search is reversed.
In a traditional IR system, the user selects with a query the information he is looking for.
In the mentioned applications, the (provider of) information selects among the users the most appropriate targets for specific contents.
We call this class of applications Best Bets systems, since the contents chosen by a provider for a specific user is in a sense his bet on the fact that the user will find it interesting and often providers also bet against each other for user attention.
Best Bets are similar to Information Filtering systems which involve matching contents against a collection of user profiles.
Retrieval systems can be characterized in terms of a general retrieval model and the retrieval functions they provide.
Copyright is held by the author/owner(s).
rank: Q   D   [0, 1] a ranking function
 A retrieval model provides an abstract description of the indexing process, the representations used for documents and queries, the matching process between them and the results  ranking criteria.
Definition 1.
A retrieval model consists of a tuple  D, Q, match, rank  where:



 Modern information retrieval systems, and in particular Web search engines, use a combination of the Boolean and vector space models: documents are selected according to Boolean combinations of term matching conditions (match) and the results are ordered according to a similarity measure as in the vector space model (rank).
Matching may involve other conditions: for instance proximity or phrase search conditions.
Each retrieval system or application provides a specific set of retrieval functions expressible in terms of the model.
Here are some typical examples.
In Information Retrieval (IR), given a document collection D, the task is to retrieve all or the top k best ranking documents satisfying a given query q, i.e.
search(q, D) = { d   D | match(q, d) } searchTop(k, q, D) = top k elements in sort(search(q, D), rank(q, .))
where sort(S, rank(q, .))
sorts the documents in S according to rank(q,.
), the ranking function partially applied to query q.
In Information Filtering (IF) the task is to match incoming documents against user profiles, expressed as queries.
The difference between IR and IF is that  in filtering, an incoming stream of objects is compared to many profiles at the same time, rather than a single query being compared to a large, relatively static database  [3].
In IF the roles of documents and queries are swapped (Figure 1) and the task can be described as query search.
Given a collection of queries Q, the goal is to find all queries q that match a given document d, i.e.
QuerySearch(d, Q) = { q   Q | match(q, d) }
 In the abstract retrieval model, the direction of search is not accounted for.
But the techniques devised for implementing a specific retrieval function are tailored to optimize one direction.
exploits inverted lists, compressed posting lists, signature files and a number of query optimization techniques.
These techniques are inappropriate for query search, where the task is to select queries that match a given document; in fact alternative techniques were proposed in [4] in the context of IF systems.
Document stream IF System Matching Profile Collection ?
?
Ranking in Best Bets models the competition among providers by deciding which results appear in the top k positions, where k is usually a small number, as user attention is valuable.
Hence ranking is crucial to determine if some contents will be delivered or not.
Rank criteria must be fair and transparent to producers.
They are typically based on parameters that vary dynamically and must be updated frequently, preserving an efficient computation.
Customer Best Bets System Customer Result Streams Best Bets Figure 1.
The IF model.
Collection query/document pairs ?
?
?
Matching and Ranking Figure 2.
Best Bets model.
We have developed [2] efficient implementation techniques for Best Bets applications and tested them in the deployment of an AdWords-like service [1].
The techniques are different from those described in [4] for IF systems as they aim to achieve: Efficient query search: search time on complex query collections as fast as search on document collections of comparable size.
Incremental updates: updates to the query collection have immediate effects, without performance degradation.
Dynamic ranking: support for a ranking model based on continuously updated ranking parameters (after each search).
A two-level caching system is used to support real time updates of queries and ranking parameters.
On a collection of one million queries running on a single PC, a steady performance of 180 searches per second was achieved stressing the system with 20 concurrent streams of queries and update requests.
The cache helped sustain a rate of 200,000 updates, with less than 4% performance degradation.
Dump of the updates from cache to disk takes about a minute, a reasonable time since a typical application is likely to take several hours to reach such volume of updates.
Tests [2] using different collection sizes, cache sizes, and queries formulation (i.e. size, term dictionary and term frequency) showed near-linear performance scalability on these parameters, thus indicating the affordability of a large scale Best Bets service.
KSolutions supported this research with a grant within the ClickWorld Project.
We thank Antonio Cisternino for useful brainstorming sessions in earlier stages of this work.
