There is a growing variety of mobile computing devices and an increased use of mobile data services.
Therefore, application developers and user interface designers are faced with a continuous challenge.
An increasing effort is required to make sure an application is accessible from all the different types of devices on the market.
This is especially true in the context of multimodal access, where traditional graphical user interfaces are complemented by voice interaction [7].
The objective of the
 (Mobile multimOdal Next-generation Applications) project [2] is the development of a generic platform for multimodal services for mobile devices.
Target environments for MONA services are WLAN and 2.5G/3G mobile phone networks.
MONA user interfaces are specified in a high-level XML-based format offering an integrated device-independent description for Copyright is held by the author/owner(s).
both graphical and voice modality.
The platform translates this generic description into a specific target format.
Currently implemented transformations are: WML, HTML, VoiceXML [6], SALT [3] and two proprietary multimodal formats combining HTML/VoiceXML and WML/VoiceXML, respectively.
Device Independence.
The user interface description format must provide a generic representation of a user interface.
It must be independent of any specific client device technology.
Mapping to the target device markup language and adaptation towards device capabilities is performed by the MONA platform.
Modality Independence.
Client device input and output restrictions, user preferences and currently active modalities are only known to the platform.
MONA applications are unaware of these parameters.
Customizability.
The user interface description format must provide mechanisms that offer a high degree of control over layout and graphical appearance.
This mechanism must not be restrictive in the context of device independence.
The structure of both graphical and voice user interfaces can be represented in the form of a tree.
For graphical user interfaces, the tree determines the geometrical grouping of the elements on the screen.
For voice interfaces, the tree reflects the temporal sequence of the user interface dialogue.
The MONA description format is an integrated XML tree-view of the user interface that is valid for both modalities.
By allowing  voice-only  as well as  graphics-only  branches in the tree, it can also provide different behavior of visual and voice interfaces if desired.
The User Interface Markup Language UIML [1], [4] is an abstract metalanguage for a canonical XML representation of any user interface.
UIML itself does not define any device or toolkit specific tags.
It introduces a basic set of tags for defining a user interface structure.
In order to use UIML, one must first define a vocabulary.
A number of such vocabularies have been defined.
However, many vocabularies are limited in the sense that they are specifically designed for a certain single target technology (e.g.
Java AWT/Swing, HTML, WML, VoiceXML [5]).
This contradicts the original concept of device independent authoring, since the author requires knowledge of both UIML and the target language.
Another limitation shared among the vocabularies we 434encountered is the fact that they assume a strict one-to-one mapping of abstract user interface elements to target markup language elements.
Practical experiences within the MONA project have shown that this is insufficient.
A third weak point of current UIML vocabularies (and other generic user interface markup language approaches) is poor control over layout and graphical appearance.
This requirement is in conflict with the requirement of device independence and is addressed by our vocabulary s  layout control elements .
The MONA UIML vocabulary defines a set of element classes that the author can use to build a user interface.
The vocabulary consists of three classes of user interface elements: non-interactive, interactive and layout control.
Table 1 lists the available MONA user interface elements.
Table 1.
MONA UIML vocabulary Non-interactive elements text, image, list, listItem Interactive elements command, choice1ofN, choiceMofN, choiceItem, input, table, headerRow, headerCell, tableRow, tableCell Layout control elements userInterface, paragraph, columnLayout Non-interactive elements are used for static presentation of information.
Interactive elements allow the user to communicate with the application business logic.
Each interactive element can generate events which can be handled using a defined set of behavior functions.
These behavior functions are translated into a suitable target scripting language by the platform.
Layout control elements are container elements that enclose other user interface elements and suggest a graphical layout for them.
The platform will consider these suggestions if possible, but may be forced to ignore them due to limitations on the target device.
Figure 1 shows two example graphical user interfaces obtained from engine implementation, each displayed on two different mobile device emulators.
the MONA platform s current rendering

 Enhanced multimodal user interfaces.
The MONA rendering engine can currently generate graphical as well as voice user interfaces.
Both can be combined to form multimodal interfaces.
However, a good multimodal user interface beneficially combines both modalities instead of simply offering both of them in parallel.
Future work will focus on improved rendering of true multimodal user interfaces in different input and output-modality-combinations.
Figure 1.
MONA login screen and example MONA voice messaging application on PDA and WAP device emulators.
User interface semantics.
More flexible layout control concepts will allow a semantics-based user interface description.
With the semantic relationships of user interface elements included in the description markup, the MONA platform will be able to achieve better layout adaptation (e.g.
for devices with a landscape screen aspect ratio rather than portrait) and to split a user interface into multiple screens for small-screen devices.
This work was supported within the Austrian competence center program Kplus and by the companies Kapsch CarrierCom, mobilkom Austria and Siemens Austria.
