Search engines typically combine analysis of Web page content and links to retrieve and rank hits in response to user queries.
While there is a large body of literature on both text and link analysis,1 it is not known how these should be combined to achieve optimal retrieval performance.
Here I present preliminary data on how similarity measures based on content and link analysis might be combined in order to best approximate a measure of semantic similarity induced by manual classi cation of pages into a hierarchical directory.
More generally, I explore what page content and links say about each other, and what they say about the meaning of pages.
The connection between Web lexical and link cues, and between either of these and semantic characterizations of pages, has been previously studied in the context of hypertext document classi cation [3], topic distillation [1], and navigation [4].
In the present approach the relationships between content, link, and semantic topology in the Web is studied empirically at a  ne level of resolution.
The idea is to measure the correlations between similarity measures driven by content, link, and semantic evidence.
 Funded in part by NSF CAREER Grant No.
IIS-0348940.
Copyright is held by the author/owner(s).
The  rst step is to sample a set of pages that are representative of the Web at large and for which independent semantic information is available along with content and link data locally accessible by crawling the pages.
The Open Directory (ODP) was used to sample
 resulting in a set of 150,000 URLs belonging to 47,174 topics.
The pages were crawled, preprocessed and stored locally.
The second step is a brute force approach: for each pair of pages p, q measure three similarities all de ned in [0, 1]: Content similarity  c(p, q) = (~p ~q)/(k~pk k~qk) where ~p, ~q are the representations of the pages in word vector space, after removing stop words and stemming.
This is actually the  cosine similarity  function, traditionally used in information retrieval.
Link similarity  l(p, q) = |Up   Uq|/|Up   Uq| where Up is the set containing the URLs of p s outlinks, inlinks, and of p itself.
The outlinks are obtained from the pages themselves, while a set of inlinks to each page in the sample is obtained from a search engine.
This Jaccard coef cient measures the degree of clustering between the two pages, with a high value indicating that the two pages belong to a clique.
Semantic similarity  s(p, q) = log Pr[t(p)]+log Pr[t(q)] where t(p) is the topic containing p in ODP, t0 is the lowest common ancestor of p and q in the ODP tree, and Pr[t] represents the prior probability that any page is classi ed under topic t. This information theoretic measure uses entropy to compare how much meaning is shared by two topic nodes compared to what distinguishes them.
It reduces to the familiar tree distance measure for a perfectly balanced tree.
This measure relies on the existence of a hierarchical organization such as ODP that classi es all of the pages being considered.
However, the ODP ontology is more complex than a simple tree; it has various types of cross-reference links between categories.
Here I sidestep this issue by reducing the directory to a single, prototypical tree.
A total of 3.8   109 page pairs yielded valid ( c,  l,  s) tuples.
These were divided into 106 bins (100 bins per similarity measure).
From this 3D histogram information a number of interesting statistics and visual maps can be derived.
The Pearson s correlation co-ef cients between pairs of similarity metrics are  ( c,  l) = 0.10,  ( c,  s) = 0.11, and  ( l,  s) = 0.08.
These are weak but very signi cant correlations when considering the number of pairs.
All three metrics appear to have a roughly exponential distribution.
Most pairs tend to have very small values for all similarity measures; given two random pages we do not expect them to be lexically similar, closely clustered, or semantically related.
The very small number of pairs with high similarity values is the main reason for the low correlation coef cients.
To visualize the relationship between different similarity measures, let us map  c and  l into  s.
For any given ( c,  l) coordinates, averaging  s is akin to precision while summing is akin to recall.
Let us de ne localized precision and recall as follows: P (sc, sl) = R(sc, sl) = p,q: c(p,q)=sc, l(p,q)=sl  s(p, q) |p, q :  c(p, q) = sc,  l(p, q) = sl|  s(p, q) p,q: c(p,q)=sc, l(p,q)=sl p,q  s(p, q)
 (3) (4) .
Figure 2 maps R and P as functions of content and link similarity coordinates.
The majority of semantically related pairs occur near the origin, as shown by the high R, because the distributions of content and link similarity are so heavily skewed.
However all this relevant mass is washed away in a sea of unrelated pairs, so P near the origin is negligible.
This underscores that while emphasis on precision is a very reasonable approach for a search engine, it costs dearly in terms of recall.
Focusing on the precision map, for very high  c there is signi cant noise making it hard to get a clear signal from link cues.
The many relevant pages in this region cannot be identi ed from link analysis.
These may be cases where authors do not know of related pages or do not want to point to the competition.
There are also many pairs in this region that are not semantically related, so very high  c is not a reliable signal.
For medium-high  c and low  l we observe a surprising basin of low precision.
Such an inversion likely corresponds to pairs of pages that are more semantically related than  s reveals, a symptom of the limitations implicit in reducing the ODP ontology to a particular tree.
In the same  c range, precision reaches many high peaks for maximum  l.
Here we note few pairs of highly related pages.
A normative strategy suggested by these maps is to mine through pages with medium-high  c, then use link analysis to distill the most relevant pages.
Understanding how semantic information can be mined from the content and links of Web pages is key for advancing search technology.
This paper reports on the  rst attempt to approximate semantic associations by mining content and link information from billions of pairs of Web pages.
The preliminary results presented highlight the importance of appropriately combining different sources of evidence for page meaning.
However, any simple combination of of  c and  l (linear or not) will result in both false positives and false negatives because of the many local optima.
The approach proposed in this paper should be validated and extended by considering alternative de nitions of content, link or semantic similarity, different hierarchical classi cations or more general ontologies, and different cues altogether.
