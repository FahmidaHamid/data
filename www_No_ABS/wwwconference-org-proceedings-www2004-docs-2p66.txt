Web services are self-describing software applications that can be advertised, located, and used across the Internet using a set of standards such as SOAP, WSDL, and UDDI [8].
Web services encapsulate application functionality and information resources, and make them available through standard programmatic interfaces.
Web services are viewed as   Yutu Liu is currently pursuing his graduate study at Department of Computer Science, Texas A&M.
This work was conducted while he was a master student at Texas State University Copyright is held by the author/owner(s).
one of the promising technologies that could help business entities to automate their operations on the web on a large scale by automatic discovery and consumption of services.
Business-to-Business(B2B) integration can be achieved on a demand basis by aggregating multiple services from di erent providers into a value-added composite service.
In the last two years, the process-based approach to web service composition has gained considerable momentum and standardization [1].
However, with the ever increasing number of functional similar web services being made available on the Internet, there is a need to be able to distinguish them using a set of well-de ned Quality of Service (QoS) criteria.
A service composition system that can leverage, aggregate and make use of individual component s QoS information to derive the optimal QoS of the composite service is still an ongoing research problem.
This is partly due to the lack of an extensible QoS model and a reliable mechanism to compute and police QoS that is fair and transparent to both service requesters and providers.
Currently, most approaches that deal with QoS of web services only address some generic dimensions such as price, execution duration, availability and reliability [12, 6].
In some domains, such generic criteria might not be su cient.
QoS model should also include domain speci c criteria and be extensible.
Moreover, most of the current approaches rely on service providers to advertise their QoS information or provide an interface to access the QoS values, which is subject to manipulation by the providers.
Obviously, service providers may not advertise their QoS information in a  neutral  manner, for example, execution duration, reliability, etc.
In approaches where QoS values are solely collected through active monitoring, there is a high overhead since QoS must be checked constantly for a large number of web services.
On the other hand, an approach that relies on a third party to rate or endorse a particular service provider is expensive and static in nature.
In our framework, the QoS model is extensible, and QoS information can either be provided by providers, computed based on execution monitoring by the users, or collected via requesters feedback, depending on the characteristics of each QoS criterion.
In a nutshell, we propose a framework that aims at advancing the current state of the art in QoS modeling, computation and policing.
There are three key aspects to our work:   Extensible QoS model.
In the presence of multiple web services with overlapping or identical functionality, service requesters need objective QoS criteria to
 it is not practical to come up with a standard QoS model that can be used for all web services in all domains.
This is because QoS is a broad concept that can encompass a number of context-dependent nonfunctional properties such as privacy, reputation and usability.
Moreover, when evaluating QoS of web services, we should also take into consideration domain speci c criteria.
For example, in the domain of phone service provisioning, the penalty rate for early termination of a contract and compensation for non-service, offered in the service level agreement are important QoS criteria in that domain.
Therefore, we propose an extensible QoS model that includes both the generic and domain speci c criteria.
In our approach, new domain speci c criteria can be added and used to evaluate the QoS of web services without changing the underlying computation model.
  Preference-oriented service ranking.
Di erent users may have di erent preferences or requirements on QoS.
It is important to be able to represent QoS from the perspective of service requesters  preference.
For example, service selection may be driven completely by price, regardless of the time it takes to execute the service.
A di erent requester may be very service sensitive.
This means that criteria such as penalty rate or the ability to return the goods after the purchase are viewed as more important than the price and the time.
Another service selection may be driven completely by time because of tight deadlines.
A QoS model should provide means for users to accurately express their preferences without resorting to complex coding of user pro les.
  Fair and open QoS computation.
Once a set of QoS criteria have been de ned for a particular domain, we must ensure that QoS information is collected in a fair manner.
In our framework, QoS information can be collected from service properties that are published by providers, execution monitoring, and requesters  feedback based on the characteristic of quality criterion.
For example, execution price can be provided by service providers, execution duration can be computed based on service invocation instances, while service reputation is based on service requesters  feedback.
We also build a policing mechanism that will prevent the manipulation of QoS value from a single service requester by requiring each requester to have a valid pair of user id and password to update the QoS registry.
Furthermore, this pair of id and password must be veri ed by the service providers at the consumption of the service to ensure that only the actual consumer of the service is allowed to give feedback.
On the other hand, to be completely fair, providers can review those criteria and can improve their QoS if they desire to.
Moreover, providers can update their QoS information (e.g., execution price, penalty rate) at any time.
Providers can also check the QoS registry to see how their QoS is ranked among other service providers.
We believe that an extensible, transparent, open and fair QoS model is necessary for the selection of web services.
Such a model can bene t all participants.
Although this model requires all requesters to update their usage experiences with a particular type of service in a registry, this overhead on the user is not large.
In return, QoS for a particular service is actively being policed by all requesters.
Each requester can search the registry to get the most-updated QoS of listed providers.
Service providers can view and change their services at any time.
With such an open and dynamic de nition of QoS, a provider can operate its web services to give its end-users the best user experience.
This paper is organized as follows.
In Section 2, we give the details of an extensible QoS model and its computation.
In Section 3, we describe the implementation of QoS registry and explain how QoS information can be collected based on active monitoring and active user feedback.
Section 4 discusses the experiments that we conducted to con rm the fairness and the validity of the various parameters used in QoS computation.
Finally, we discuss related work in Section 5 and conclude in Section 6.
Currently, in most SOC frameworks, there is a service broker which is responsible for the brokering of functional properties of web services.
In our framework, we extend capability of the service broker to support nonfunctional properties.
This is done by introducing an extensible and multiple dimensions QoS model in the service broker.
We aim to evaluate QoS of web services using an open, fair and transparent mechanism.
In the following subsections, we  rst introduce the extensible QoS model, and then we give the details on how to evaluate web service based on our model.
In this section, we propose an extensible QoS model, which includes generic and domain or business speci c criteria.
The generic criteria are applicable to all web services, for example, their pricing and execution duration.
Although the number of QoS criteria discussed in this paper is limited (for the sake of illustration), our model is extensible.
New criteria (either generic or domain speci c) can be added without fundamentally altering the underlying computation mechanism as shown in Section 2.2.
In particular, it is possible to extend the quality model to integrate nonfunctional service characteristics such as those proposed in [7], or to integrate service QoS metrics such as those proposed by [11].
We consider three generic quality criteria which can be measured objectively for elementary services: (1) execution price, (2)execution duration, and (3) reputation.
Criteria such as availability and reliability is not required in our model due to the use of active user feedback and execution monitoring.
  Execution price.
This is the amount of money which a service requester has to pay to the service provider to use a web service such as checking a credit, or the amount of money the service requester has to pay to the service provider to get a commodity like an entertainment ticket or a monthly phone service.
Web service providers either directly advertise the execution price of their services, or they provide means for potential requesters to inquire about it.
Let s be one 67web service, then qpr(s) is the execution price for using that service s.
  Execution duration.
The execution duration qdu(s) measures the expected delay in seconds between the moment when a request is sent and the moment when the service is rendered.
The execution duration is computed using the expression qdu(s) = Tprocess(s) + Ttrans(s), meaning that the execution duration is the sum of the processing time Tprocess(s) and the transmission time Ttrans(s).
Execution time is obtained via active monitoring.
  Reputation.
The reputation qrep(s) of a service s is a measure of its trustworthiness.
It mainly depends on end user s experiences of using the service s. Different end users may have di erent opinions on the same service.
The value of the reputation is de ned as the average ranking given to the service by end users, , where Ri is the end user s rank-i.e., qrep = ing on a service s reputation, n is the number of times the service has been graded.
Usually, end users are given a range to rank Web services.
For example, in Amazon.com, the range is [0, 5].
(cid:1) n i=1 Ri n
 The number of business related criteria can vary in different domains.
For example, in phone service provisioning domain, the penalty rate for the early termination and the  xed monthly charge are important factors for users to consider when selecting a particular service provider.
We use the generic term usability to group all business related criteria.
In our chosen application, we measure usability from three aspects, transaction, compensation rate and the penalty rate.
  Transaction support is used for maintaining data consistency.
In prior QoS models, no transactional criteria is being used in the computation of QoS value.
However, from the perspective of a requester, whether a web service provides an undo procedure to rollback the service execution in certain period without any charges is an important factor that will a ect his/her choice.
Transactional property can be evaluated by two dimensions: whether undo procedure is supported qtx(s) and what s the time constraints qcons(s) on undo procedure.
It should be noted that qtx(s) = 0/1, where 1 indicate the web service supports transaction and 0 otherwise; qcons(s) indicates the duration of undo procedure is allowed.
  Compensation rate qcomp(s) of a web service indicates percentage of the original execution price that will be refunded when the service provider can not honor the committed service or deliver the ordered commodity.
  Penalty rate qpen(s) of a web service indicates what percentage of the original price service requesters need to pay to the provider when he/she wants to cancel the committed service or ordered commodity after the time out period for transaction to roll back is expired.
The QoS registry is responsible for the computation of QoS value for each service provider.
Assuming that there is a set of web services that have the same functional properties, where S (S = {s1, s2, ..., sn}), web service computation algorithm determines which service si is selected based on end user s constraints.
Using m criteria to evaluate web service, we can obtain the following matrix Q.
Each row in Q represents a web service si, while each column represents one of the QoS criteria.
   
     q1,1 q2,1 ...
qn,1 q1,2 q2,2 ...
qn,2 .
.
.
.
.
.
...
.
.
.
q1,m q2,m ...
qn,m (1) In order to rank the web services, the matrix Q need to be normalized.
The purposes of normalization are: 1) to allow for a uniform measurement of service qualities independent of units.
2) to provide a uniform index to represent service qualities for each provider.
Provider can increase and decrease his/her quality index by entering a few parameters,
 number of normalizations performed depends on how the quality criteria are grouped.
In our example criteria given in Section 2, we need to apply two phases of normalization before we can compute the  nal QoS value.
The second normalization is used to provide uniform representation of a group of quality criteria (e.g.
usuability) and set threshold to a group of quality criteria.
  First Normalization Before normalizing matrix Q, we need to de ne two arrays.
The  rst array is N = {n1, n2, ..., nm} with
 is for the case where the increase of qi,j bene ts the service requester while nj = 0 is for the case where the decrease of qi,j bene ts the service requester.
The second array is C = {c1, c2, ..., cm}.
Here cj is a constant which sets the maximum normalized value.
Each element in matrix Q will be normalized using the following equation 2 and equation 3.
(cid:1) (cid:1) n qi,j i=1 qi,j if 1 n and n i=1 qi,j (cid:3)= 0 (cid:1) n qi,j i=1 qi,j < cj In the above equations, 1 n (cid:1) n i=1 qi,j is the average value        
 n cj
 n cj vi,j = vi,j = (cid:1) n i=1 qi,j qi,j (2) n i=1 qi,j = 0   cj qi,j i=1 qi,j
 n (cid:1) and nj = 1 if 1 n and nj = 1 (cid:1) n or if qi,j (cid:3)= 0 and nj = 0 (cid:1) n
 n
 n and if qi,j = 0 and nj = 0 (cid:1) n i=1 qi,j qi,j or
 n   cj i=1 qi,j qi,j < cj (3)
 For the value of each element in matrix D, di,j = 1 if is included in jth group the ith quality criterion in Q in G. By applying Matrix D to Q , we have matrix G. The equation is shown below: (cid:1) (cid:1) (4)
 (cid:1)   D (7) of quality criteria j in matrix Q.
Applying these two (cid:1) equations to Q, we get matrix Q which is shown below:     (cid:1)
 =     v1,1 v2,1 ...
vn,1 v1,2 v2,2 ...
vn,2 .
.
.
.
.
.
...
.
.
.
v1,m v2,m ...
vn,m Example 1.
Data in the following example is taken from our QoS registry implementation.
We assume that there are two web services in S and that their values of quality criteria are: Q=(q1,1, q1,2, q1,3, q1,4, q1,5, q1,6, q1,7,q2,1, q2,2, q2,3, q2,4, q2,5, q2,6, q2,7 ) = (25,1,60,0.5,0.5,100,2.0,40,1,200,0.8,0.1,40,2.5).
The quality criteria are in the order of Price, Transaction, Time Out, Compensation Rate, Penalty Rate, Execution Duration and Reputation.
For array N, since the increase of price, penalty rate and execution duration doesn t bene t the service requester, and the increase of the remaining quality criteria bene ts the service requester, the value for each element in array N is {0, 1, 1, 1, 0, 0, 1}.
For array C, each element in the array is set to 5 which is considered as a reasonable maximum normalized value in this implemented domain.
Using equation 2 and equation 3, we have the normalized matrix: Q =(v1,1, v1,2, v1,3, v1,4, v1,5, v1,6, v1,7,v2,1, v2,2, v2,3, v2,4, v2,5, v2,6, v2,7 ) = (1.3,

 (cid:1)   Second Normalization In our QoS model, quality criterion can also be represented as a group and manipulated as a group.
For example, the usability criteriion.
Each group can contain multiple criteria.
For example, both compensation and penalty rates belong to the usability group.
To compute the  nal QoS value for each web service, we introduce Matrix D and Matrix G. Matrix D is used to de ne the relationship between quality criteria and quality groups.
Each row in Matrix D represents a quality criterion, and each column in Matrix D represents one quality group value.
Matrix G represents QoS information based on the values of quality group of web services.
Each row in Matrix G represents a web service, and each column in Matrix G represents one quality group value.
Matrix D and Matrix G are shown below:        
 (5) d1,2 d1,1 d2,1 d2,2 ...
...
dm,1 dm,2 .
.
.
d1,l .
.
.
d2,l ...
...
.
.
.
dm,l    
     g1,1 g2,1 ...
gn,1 g1,2 g2,2 ...
gn,2 .
.
.
.
.
.
...
.
.
.
g1,l g2,l ...
gn,l To normalize matrix G, two arrays are needed.
In the  rst array T = {t1, t2, ..., tl}, tj is a constant which sets the maximum normalized value for the group j.
In the second array F = {f1, f2, ..., fl}, fj is a weight for group j such as price sensitivity and service sensitivity etc.
This is used to express users  preferences over jth group.
Each element in matrix G will be normalized using equation 8.
(cid:1) n gi,j i=1 gi,j
 n if 1 n and     hi,j = tj (cid:1)
 n (cid:1) n i=1 gi,j (cid:3)= 0 (cid:1) n gi,j i=1 gi,j < tj if 1 n or n i=1 gi,j = 0   tj.
(cid:1) n gi,j i=1 gi,j (cid:1)
 n (8) n i=1 gi,j is the average In the above equations, value of group criteria j in matrix G. Applying equa-(cid:1) tion 8 to G, we get matrix G which is shown below:
 n     (cid:1)
 =     h1,1 h1,2 h2,1 h2,2 ...
...
hn,1 hn,2 .
.
.
h1,l .
.
.
h2,l ...
...
.
.
.
hn,l (9) Finally, we can compute the QoS value for each web (cid:1) service by applying array F to matrix G .
The formula of QoS is shown below: l(cid:12) QoS(si) = j=1 (hi,j   fj) (10) Example 2.
This example continues the computation of QoS from Example 1.
For array T, each element in the array is set to 5 which is considered as a reasonable maximum normalized value in this implemented domain.
For array F, each weight is given as 1.
This means user gives the same preference to every group.
The de ned D, which determines the grouping of the quality criteria in this example, is shown below:        







 (6) The column in D is in the order of Price, Time, Usability and Reputation.
First, by applying D to 69(cid:1) matrix Q , we obtained G= (0.813, 1.75, 7.068, 1.111,
 (cid:1) to G to have a normalized G = (h1,1, h1,2, h1,3, h1,4, h2,1, h2,2, h2,3, h2,4) = (0.769, 1.429, 1.334, 1.111,
 we have QoS(s1)=4.643 and QoS(s2)=3.072


 To demonstrate our proposed QoS model, we implemented a QoS registry as shown in Figure 1 within a hypothetical phone service (UPS) provisioning market place.
The UPS market place is implemented using BEA Weblogic Workshop web service toolkit.
It consists of various service providers who can register to provide various type of phone services such as long distance, local phone service, wireless and broadband.
The market place also has a few credit checking agencies which o er credit checking service for a fee.
The UPS market place has web interfaces which allow a customer to login and search for phone services based on his/her preferences.
For example, the customer can specify whether the search for a particular type of service should be price or service sensitive.
A price sensitive search will return a list of service providers who o ers the lowest price.
A service sensitive search will return a list of service providers with the best rated services.
The market place also has web interfaces that allow service providers to register their web services with the QoS registry, update their existing web services in the registry or view their QoS ranking in the market place.
The registry s other interfaces are available for requesters/end-users to give feedback on the QoS of the web services which they just consumed.
Web Browser QoS Feedback Service Requester QoS Monitoring QoS Registry QoS Ranking Get Service QoS Computation Data base Publish
 Update services Web Service
 Service Provider message exchange
 Service Description
 Figure 1: Architecture Diagram of QoS Registry.
In our framework, we distinguish two types of criterion: deterministic and non-deterministic.
Here, deterministic indicates that the value of QoS quality criterion is known or certain when a service is invoked, for example, the execution price and the penalty rate.
The non-deterministic is for QoS quality criterion that is uncertain when web service is invoked, for example execution duration.
For deterministic criterion, we assume that service providers have mechanisms to advertise those values through means such as web service quality-based XML language as described in [2].
How all service providers come to an agreement of deterministic set of QoS criteria to advertise is beyond the scope of this paper.
In this section, we discuss how all QoS criteria in our model can be collected in a fair, open and objective manner via active monitoring and active user s feedback.
  Collecting quality information from active execution monitoring.
The actual service execution duration is collected by the service requester.
This requires interfaces used by all service requesters to implement some mechanisms to log the actual execution time.
Although this approach puts more burden on the service requester, it has the following advantages over approaches where the service broker or QoS registry is required to perform the monitoring: 1) it lowers the overhead of QoS registry and simpli es the implementation of QoS registry, 2) data is collected from actual consumption of the service which is up to date and objective, 3) it avoids the necessity to install expensive middleware to poll the large number of service providers constantly.
Through active execution monitoring, when a service requester gets a di erent set of values for those deterministic criteria advertised by the service provider, this di erence can be logged.
If this phenomenon is common in a particular domain, we can expand the criteria for Reputation to record the di erence between the actual deterministic quality criteria and the advertised deterministic quality criteria from the service provider.
The bigger the di erence, the lower the QoS will be for that service provider.
  Collecting quality information from users feedback.
Each end-user is required to update QoS of service he/she has just consumed.
This ensures that the QoS registry is fair to all end-users since QoS values can be computed based on real user experience with up to date runtime execution data.
To prevent the manipulation of QoS by a single party, for each feedback, the end-user is given a pair of keys.
This pair of keys must be authenticated by the service provider before the user is allowed to update the QoS value.
The update must take place in a limited time-frame to prevent the system from being overloaded with unconsumed service requests.
We assume that service requester can either download a plugin from QoS registry for providing feedback on QoS or access the services via a portal which must implement the active monitoring mechanism as outlined above for QoS computation.
We conducted a series of experiments to: 1) investigate the relationship between QoS value and the business criteria, 2) study the e ectiveness of price and the service sensitivity factors in our QoS computation.
The experiments are conducted on a Pentium computer with a 750Mhz CPU and 640MB RAM.
We  rst simulated 600 users searching for services, consuming the services and providing feedbacks to update the QoS registry in the UPS application.
This will




 yes yes out

 Rate

 Rate

 Duration



 Table 1: Comparing Service Quality Data of two providers generate a set of test data for those non-deterministic quality criteria such as reputation and execution duration for each service provider in the QoS registry.
The deterministic quality criteria (price, penalty rate) have been advertised and stored in the QoS registry for each service provider prior to the simulation.
Table 1 shows the values of various quality criteria from two phone service providers with respect to local phone service.
From this table, we can see that provider ABC has a better price but the various criteria that are related to services are not very good.
Its time out value is small, this means it is not so  exible for end-users.
Its compensation rate is lower than BTT, and its penalty rate is higher than BTT.
Its execution time is longer than BTT and reputation is lower than BTT as well.
Using our QoS computation algorithm, can we ensure that ABC will win for a price sensitive search or BTT will win for a service sensitive search?
Table 2 shows that BTT has a QoS value of 3.938 with a price sensitivity search and a QoS value of 5.437 with a service sensitivity search.
On the other hand, ABC has a QoS value of 4.281 with a price sensitivity search and a QoS value of 3.938 with a service sensitivity search.
ABC is winning for a price sensitive search (4.281 > 3.938), BTT is winning for a service sensitive search (5.437 > 4.281) which veri es our original hypothesis.
The following three  gures show the relationship between the QoS value and the price, the compensation rate, the penalty rate, and the sensitivities factors.
From Figure 2, we can see that the QoS increases exponentially as the price approaches zero.
As the price reaches 9x20 = 180, the QoS becomes very stable.
This gives us the con dence that price can be used very e ectively to increase QoS within certain range.
It also shows that in our QoS computation model, QoS cannot be dominated by price component inde nitely.
For the penalty, it shows that the QoS value decreases as the penalty rate increases.
For the compensation, it shows that the QoS value increases as the compensation rate increases.
This concludes that QoS computation cannot be dominated by these two components inde nitely.
Compensation and Penalty Rate
 o










 Compensation Rate Penalty Rate




 Rates Unit = 0.1 Figure 3: Relationship between QoS, Compensation and Penalty Rate.
Figure 4 indicates that QoS value increases almost four times faster for the service sensitivity than the price sensitivity.
This shows that using the the same sensitivity value for both price and service factors will not be e ective for the price sensitivity search in our QoS registry.
To get an e ective price and service sensitivity factors for a domain, we need to analyze the sensitivity factors after the QoS registry has been used for a while and readjust the values by performing experiment mentioned in Figure 4.
o










 Price Price and Service Sensitivity Price
 o





 Price Sensitivity Service Sensitivity









 Price Unit = 20





 Sensitivities Unit = 0.1 Initital Value = 1 Figure 2: Relationship between QoS and Price.
Figure 4: Relationship between QoS, Price and Service Sensitivity factors.
Figure 3 shows the relationship between QoS and services.
Sensitivity



 QoS Service Sensitivity Value







 Table 2: Results of Sensitivity Experiments

 Multiple web services may provide similar functionality, but with di erent nonfunctional properties.
In the selection of a web service, it is important to consider both functional and nonfunctional properties in order to satisfy the constraints or needs of users.
While it is common to have a QoS model for a particular domain, an extensible QoS model that allows addition of new quality criteria without a ecting the formula used for the overall computation of QoS values has not been proposed before.
Moreover, very little research has been done on how to ensure that service quality criteria can be collected in a fair, dynamic and open manner.
Most previous work is centered on the collection of networking level criteria such as execution time, reliability and availability etc [12].
No model gives details on how business criteria such as compensation and penalty rates can be included in QoS computation and enforced in an objective manner.
In a dynamic environment, service providers can appear and disappear around the clock.
Service providers can change their services at any time in order to remain competitive.
Previous work has not addressed the dynamic aspects of QoS computation.
For example, there is no guarantee that the QoS obtained at run time for a particular provider is indeed the most up to date value.
Our proposed QoS computation which is based on active monitoring and consumers  feedback ensures that QoS value of a particular provider is always up to date.
In [9], the author proposed a QoS model which has a QoS certi er to verify published QoS criteria.
It requires all web service providers to advertise their services with the QoS cer-ti er.
This approach lacks the ability to meet the dynamics of a market place where the need of both consumers and providers are constantly changing.
For example, it does not provide methods for providers to update their QoS dynamically, and does not give the most updated QoS to service consumers either.
There are no details on how to verify the QoS with the service providers in an automatic and cost e ective way.
In [10], the authors proposed a QoS middleware infrastructure which required a build-in tool to monitor metrics of QoS automatically.
If such a tool can be built, it needs to poll all web services to collect metrics of their QoS.
Such an approach requires the willingness of service providers to surrender some of their autonomy.
Moreover, if the polling interval is set too long, the QoS will not be up to date.
If the polling interval is set too short, it might incur a high performance overhead.
A similar approach which emphasizes on service reputation is proposed in [5, 4].
A web service agent proxy is set up to collect reputation rating from previous usage of web services.
The major problem with this approach is that, there is no mechanism in place to prevent false ratings being collected.
We have a mechanism to ensure that only the true user of the service is allowed to provide feedback.
In [3], the author discusses a model with service level agreement (SLA) which is used as a bridge between service providers and consumers.
The penalty concept is given in its de nition of SLA.
The main emphasis about this concept is what should be done if the service provider can not deliver the service under the de ned SLA, and the options for consumers to terminate the service under the SLA.
Penalty is not included as a service quality criteria which can be used in QoS computation.
In [2], a language-based QoS model is proposed.
Here, QoS of a web service is de ned using a Web Service QoS Extension Language.
This XML-based language can de ne components of QoS in a schema  le which can be used across di erent platforms and is human readable.
The quality-based selection of web services is an active research topic in the dynamic composition of services.
The main drawback of current work in dynamic web service selection is the inability to ensure that the QoS of published web services remain open, trustworthy and fair.
We have presented an extensible QoS model that is open, fair and dynamic for both service requesters and service providers.
We achieved the dynamic and fair computation of QoS values of web services through a secure active users  feedback and active monitoring.
Our QoS model concentrates on criteria that can be collected and enforced objectively.
In particular, we showed how business related criteria (compensation, penalty policies and transaction) can be measured and included in QoS computation.
Our QoS model is extensible and thus new domain speci c criteria can be added without changing the underlying computation model.
We provided an implementation of a QoS registry based on our extensible QoS model in the context of a hypothetical phone service provisioning market place.
We also conducted experiments which validate the formula we used in the computation of QoS values in the phone service provisioning domain.
The formula for computing the QoS values can be varied for di erent domains by using di erent sensitivity factors or di erent criteria with its associated groupings.
Additional factors, di erent groupings, or criteria might serve certain domains better.
We provided a mechanism for service providers to query their QoS computed by the registry, and update their published services to become more competitive at anytime.
The success of this model depends on the willingness of end-users to give feedback on the quality of the services that they consume.
Our future work includes ways to automate the collection of feedback data.
To ensure that all users will 72provide feedback, we need to make providing feedback very straight forward.
We also want to see that the implemented QoS registry becomes intelligent internally and externally.
Internal intelligence refers to the ability to infer the appropriate sensitivity factors to use to get the list of providers which meets end-users requirements.
External intelligence refers to the ability of QoS registry to be able to predict the trend of QoS from certain providers at runtime.
For example, we want to have a system which knows how much the QoS value decreases in the morning and increases in the evening or during di erent time of the year, and takes action to notify its users.
For the service provider, we want to have the ability to notify the provider when the QoS of its service is below a certain threshold.
