Recently, online monitoring of data streams has emerged as an important data management problem.
This research topic has its foundations and applications in many domains, including databases, data mining, algorithms, networking, theory, and statistics.
However, new challenges have emerged.
Due to their vast sizes, some stream types should be mined fast before being deleted forever.
In general, the alphabet is too large to keep exact information for all elements.
Conventional database, and mining techniques, though e ective with stored data, are deemed impractical in this setting.
  EIA 00-80134, NSF 02-09112, and CNF 04-23336.
  Part of this work was done while the  rst author was at ValueClick, Inc.
This work was supported in part by NSF under grants Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
There is a growing need to develop new techniques to cope with high-speed streams, and answer online queries.
Currently, data stream management systems are used for monitoring click streams [37], stock tickers [13, 58], sensor readings [10], telephone call records [19], network packet traces [21], auction bidding patterns [4], tra c management [5], network-aware clustering [16], and security against DoS [16].
Golab and Ozsu review the literature in [33].
This work is primarily motivated by the setting of Internet advertising commissioners, who represent the middle persons between Internet publishers, and Internet advertisers.
In a standard setting, an advertiser provides the publishers with its advertisements, and they agree on a commission for each user action, e.g., clicking an advertisement,  lling out a form, bidding on an item, or making a purchase.
The publisher, motivated by the commission paid by the advertiser, displays advertisements, text links, or product links on its page; and uses a form of tracking code for these advertisements on their sites to keep logs of the tra c it drives to each advertiser s site.
On the other end, the advertiser keeps track of the tra c that is generated by each of its publishers.
Inconsistencies between the size of the driven tra c as measured by the advertiser and the publisher are resolved by a third tracking entity, the advertising commissioner.
Whenever a customer uses a link, the customer is referred from the publisher s Web site to the servers of the advertising commissioner, who logs the click and clicks-through the customer to the Web site of the advertiser.
Since the publishers earn revenue on the tra c they drive to the advertisers  Web sites, there is an incentive for them to falsely increase the number of clicks their sites generate.
This process is referred to, in [3], as click in ation.
One of the advertising commissioner s roles is to detect any fraud taking place on either the publisher s side or the advertiser s side.
Thus, the advertising commissioner should be able to tell whether the clicks generated at the publisher s side are authentic, or are generated by a script running on some machines on the publisher s end, to claim more traf- c, and thus, more revenue.
In order to do that, the advertising commissioner should be able to track each click by the advertisement ID, and the customer ID.
The advertising commissioner tracks individual customers, by setting cookies.
Duplicate clicks within a short period of time, a day for example, raise suspicion on the commissioner s side.
Classically, advertising commissioners run queries towards the end of the month, when calculating the publishers  commissions, to capture duplicate clicks within a short period of time.
Due to the large size of click streams, such queries degrade the performance of their databases considerably.
Hence, due to monthly accounting constraints, this problem has to be solved on a real-time basis.
From the above example we are motivated to solve the problem of  nding duplicates occurring in a very fast data channel, the click stream.
To the best of our knowledge, this problem has not been addressed before.
A similar problem, the frequent elements [48], has been proposed to search for all elements which have occurred more than a user speci ed ratio of the stream.
However, the stream properties that are assumed in the context of  nding frequent elements are different from those that are assumed for duplicate detection.
In the context of duplicate detection, the majority of the elements occurring in the data stream are supposed to be distinct.
That is, the skew in the data stream is very weak.
On the other hand, in the context of  nding frequent elements, the underlying assumption is that a few elements, the frequent ones, are supposed to have much higher frequencies than the other elements.
Thus, the number of distinct elements observed in the data stream is much smaller than the size of the stream; and the data is supposed to be more skewed than in the case of streams queried for duplicates.
The theoretical space bound for  nding frequent elements is inversely proportional to the error, which is one tenth to one hundredth of the user required ratio [11].
Thus, using these algorithms for duplicate detection is costly and ine cient.
We develop an e cient solution based on Bloom Filters [9], and consider both types of stream windows, the sliding, and the landmark windows.
We analyze the space and time requirements, and run a set of experiments to evaluate the performance of our proposed solution.
The rest of the paper is organized as follows.
Section 2 highlights the related work.
In Section 3, we formalize the problem, followed by a discussion of the preliminary solutions in Section 4.
The more e cient, and approximate solution, is presented in Section 5.
We report the results of our experiments in Section 6, and conclude in Section 7.
This work touches on three main domains: management of data stream, approximate duplicate detection, and the security of web advertising schemes.
We brie y outline the literature in these three domains.
Recently, data streams management has emerged as an active research area.
There have been several works for analyzing data streams.
Problems studied in this context include approximate frequency moments [1], di erences [25], distinct values estimation [26, 39, 56], bit counting [20], approximate quantiles [34, 44, 49], histograms [36, 35], wavelet based aggregate queries [30, 50], correlated aggregate queries [28], elements classi cation [38], frequent elements [11, 17, 18, 21, 22, 24, 31, 41, 42, 48, 51], and top-k queries [7, 12,

 The problem of hit shaving has been studied in [54].
Hit shaving is another type of fraud performed by advertisers, who do not pay commission on some of the tra c received from publishers.
Reiter et al. [54] presented a new scheme to click-through the customers from the publisher to the advertiser, so that the publisher can keep a more accurate record of the tra c driven to the advertiser.
Anupam et al. [3] proposed a click in ation attack on the pay-per-click scheme described above.
The attack requires the cooperation of more than one publisher to fraud an advertiser.
The attack is relatively di cult to detect, though it leads to a relatively high click-through-rate 1 , which should still raise the suspicion of the advertising commissioners.
The problem of  nding approximate duplicate items has been researched before in both the contexts of data management, and Web applications.
The work done in [2, 8, 27, 40, 46, 52, 53, 55] focused on detecting fuzzy duplicate records in databases.
These works were primarily motivated by data cleaning in the process of loading data into a data warehouse.
On the other end of the spectrum, [14, 15, 43, 45, 47, 57] were interested in detecting duplicate documents and Web pages.
This is directly applicable to eliminating duplicate pages reported by search engines.
A work that is very close to ours was done in 1979 [6].
The work focused on using specialized hardware to implement projection and join operators in the context of relational databases.
Duplicate elimination, a crucial intermediate step in data processing, was implemented using a technique very similar to ours.
The problem of duplicate detection has two basic variations, depending on the way the stream is handled.
The Duplicate Detection on a Sliding Window asks for duplicate elements that have occurred in the last N elements.
An example of this query asks for the duplicate clicks that occurred in the last 1, 000, 000 clicks.
The Duplicate Detection on a Landmark Window asks for duplicate elements that have occurred since the occurrence of a speci c landmark.
For instance, a query could ask for the duplicate clicks that have occurred since the beginning of the current day or week.
Windows The model for sliding window processing in data streams was  rst introduced in [20].
The goal is to make the query answer relevant to the last observed part of the stream.
Applying the sliding window concept to our motivating application, N could be set large enough so that even if duplicates occur on intervals which are more than N clicks apart, they would have negligible e ect on the commission paid to the publisher.
In addition, it is still acceptable, from a practical perspective, that a user might click the same advertisement, a few number of times in the span of a long time period.
From a practical point of view, in order to slide the window, for every new entry arriving, an old entry has to be It is necessary to keep the latest N clicks in a evicted.
circular queue, in order to know which entry to evict [32].
In addition, the entire window has to be summarized in a
 who click advertisements on the publisher s Web site, as a ratio of all the visitors to the Web site.
data structure that supports fast search, incorporation, and deletion of entries.
Windows Processing a stream based on landmark windows requires handling disjoint portions of the streams, which are separated by landmarks.
Landmarks can be de ned either in terms of time, e.g., on daily or weekly basis, or in terms of the number of elements observed so far since the previous landmark, e.g., every 1,000,000 elements.
In this model, it is not necessary to keep the entire window, rather a summary may be su cient, since none of the elements will be deleted.
On the other hand, at the beginning of every landmark window, all the structures that hold the clicks have to be reinitialized.
In addition, the window size is not  xed, in contrast to the sliding window model, since the window grows as more clicks are observed.
The classical landmark window model facilitates the computation of duplicates at the expense of the e ectiveness of the query.
Although the landmark window is expected to require less space, since it does not store the entire window, but only a structure that can be queried for duplicate clicks, it could always happen that two duplicate clicks, that are very close in time, are missed, because they did not occur in the same window.
This can happen if one of them is towards the end of a window, and the second click is towards the beginning of the next window.
This is the reason landmark windows are usually set to grow in size slightly more than that of sliding windows.
Windows The jumping window model [58] is a compromise between the landmark and the sliding window models.
The basic idea is to slide the window in jumps rather than in individual elements.
The window is divided into smaller disjoint sub-windows of size n, and a summary of each sub-window is stored in a separate data structure.
When the data structure of the latest sub-window is populated, its results are combined with the results of the jumping window; and the data structure of the eldest sub-window is deleted, and its results are deleted from the jumping window.
The basic advantage of the jumping window model is that it guarantees the freshness of the results, since the queried portion of the stream is of  xed length N , and is at most (n  1) elements behind the most recent observed element in the stream.
In addition, this model does not require storing the whole window.
However, it entails using data structures whose results can be combined, and subtracted e ciently.
To  nd duplicates in a click stream, the obvious solution is to store all the clicks, and when observing a new click, just compare it to all the entries already stored.
If the new click is found to be a duplicate, then do not count it towards the corresponding publisher s commission.
The basic disadvantage of this solution is the need to scan the entire window to tell whether a newly observed click is a duplicate, or not.
Thus processing a stream/window of size N requires O(N 2) comparisons to detect duplicates.
Building an index on the elements, would reduce the search cost to O(log(N )) per element, but on the other hand, would increase the element insertion cost to O(log(N )), instead of just appending an element in constant time.
The straightforward solution described above solves the exact problem, but su ers from slow processing of the data stream.
The best estimate of the complexity is O(N log(N )), which will not be practical for large streams.
We propose another e cient technique, which has a constant amortized cost, does not need more space, detects all duplicates, and produces very few false positive errors.
By false positive errors, we refer to non-duplicate elements erroneously identi- ed by the algorithm as duplicates.
We will start by proposing simpler algorithms that will lead to the development of the e cient solution.
We will limit the discussion to handling landmark windows, and then will extend our solution to sliding and jumping windows in Section 5.2.
Bits 0 - 31 Bit Vector Figure 1: The bit vector solution.
The ID is 32 bits long, and the bit vector is of length 232 bits.
The  rst improvement on the index-based solution for detecting duplicates in a data stream is to use a bit vector.
Assume the elements of the data stream, the clicks, are coming from an alphabet A.
Then keeping a bit for every element in A is enough to keep track of which elements have been observed in the stream by  agging their corresponding bits to 1.
Figure 1, clari es the solution by giving an example for the case where the ID has 32 bits, which entails keeping a bit vector of length 232 bits.
A new element is a duplicate if its bit has been  agged 1, before.
The algorithm is simple, exact, and takes O(1) steps and space to insert a new element into the bit vector, or to check it for duplication.
However, this simple scheme cannot be implemented in our case.
The alphabet we are dealing with in our application is the domain of IDs of clicks.
Each click ID is represented by a pair of an advertisement ID, and a user cookie ID.
Click IDs are represented by 64 characters.
Thus, keeping a bit for every ID entails keeping 2512 bits   1.676 10153 bytes, which is infeasible.
The next modi cation is to keep partial information, rather than all the combinations of the alphabet.
Assuming an element is represented by b bits, where b is 512 in our application, we can keep less than 2b bits, and still get approximate results with a very low error rate.
The basic idea is to keep a bit vector of size 2p for all the combinations of the  rst p bits, another bit vector for the second p bits, and so forth, where 1   p   b.
Assuming the bits of an element are labeled 0 .
.
.
(b   1), the  rst bit vector has bits for all the combinations of bits
 binations of bits 1 .
.
.
p. Notice that the  rst and the second bit vectors correspond to bits that largely overlap in the element ID.
In general, the nth bit vector has bits for all the combinations of bits (n 1) .
.
.
(n+p 2).
There are b p+1 bit vectors utilized.
As the stream is observed, for each element, its  rst p bits will be checked if they have occurred before in the  rst bit
 Bits 0 - 15 Bits 1 - 16 Bits 15 - 30 Bits 16 - 31 Bit Vector 1 Bit Vector 2 Bit Vector 16 Bit Vector 17 Figure 2: The overlapping bit-substrings solution.
The ID is 32 bits long, and 17 bit vectors is of length 216 bits are kept.
vector, then, the bit corresponding to those  rst p bits will be  agged 1.
The focus will be shifted by 1 bit to the bits 1 .
.
.
p, and those bits will be checked against the second bit vector in the same manner, and so on.
An element which has at least one substring of p bits that never occurred before is guaranteed not to be a duplicate.
On the contrary, there is a high probability that an element is a duplicate if it has collided on all its substrings of length p bits.
If p = b, then this is the exact bit vector solution discussed above.
Otherwise, the space requirements will drop from 2b to 2p(b p+1) bits.
An example of the solution where b = 32, and p = 16 is given in Figure 2.
There are (b   p + 1) = (32   16 + 1) = 17 bit vectors.
The number of bits used in this solution, as a percentage of the number of bits used   100 = 2p(b p+1) in the exact solution is
   100 = 216 17
 2b There is a tradeo  here between the rate of false positive errors reported as duplicates and the space usage.
The larger the value of p, the smaller the rate of false positive errors, and the larger the space requirements.
Any two elements picked at random have a probability of  p that they will collide in any bit vector of length p, given
 that the bits are distributed uniformly.
The underlying idea of the algorithm is that when viewing the ith element, it that it, by chance, has a probability of collides with any of the previously observed i   1 elements on the  rst p bits.
It is even less probable that an element collides with others on the  rst and the second p bits.
The probability gets smaller as more substrings of length p bits are taken into consideration.
Thus, if the algorithm  nds out that an element has collided on all its bit-substrings of length p, for a reasonable length of p, then most probably, this element is a duplicate.
 p)i 1 (cid:1) (cid:2) Unfortunately, a probabilistic analysis of this scheme is intractable.
The main problem is that an element a that collides with another element b in a bit vector n is more likely to collides with the same element b than the rest of the elements in the bit vectors (n  p + 1) to (n + p  1).
For example, given that the two elements a, and b have the same values in bits (n 1) to (n+p 2), then there is a probability 2 that the values of the bits starting at n to (n+p 1) will of 1 be equal, using the basic conditional probability rule.
This  p given probability is much higher than the probability of 2 above, which is the general probability of collision between two elements in one bit vector.
We modify the algorithm once more to serve both purposes of achieving better results, and facilitating the probabilistic analysis.
We will use the same idea of shrinking the size of the bit vector to less than
 the IDs, we will use independent hash functions.
Utilizing independent hash functions eliminates the risk of correlated collisions discussed above, and thus yields better results, and facilitates the probabilistic analysis.
Interestingly, using independent hash functions makes our solution another development of Bloom Filters [9].
In this section we will describe the classical Bloom Filters and their motivating application of testing approximate membership of elements.
Then, we will discuss how to utilize Bloom Filters in our context.
A Bloom Filter [9] is a data structure that was proposed to detect approximate membership of elements.
Given two sets, X, and Y , the Bloom Filter algorithm would loop on every element in set X, to check if it belongs to set Y , too.
The algorithm is probabilistic, requires O(|X|) operations, and O(|Y |) space.
A Bloom Filter can assert that an element in X does not belong to Y , but cannot assert that an element in X belongs to Y .
That is, its errors are only false positive, and never false negative.
On the whole, the number of elements that are erroneously found to belong to Y , and that actually do not, is very small, and is inversely proportional to the hidden constant in the big-O of the space and time requirements.
An empty Bloom Filter is an array of M cells, with addresses 0 .
.
.
M   1, that are initially zeroed.
Each element, y, in Y is hashed using d independent hash functions to addresses y1, y2, .
.
.
, yd, which are set to 1, such that
 results, x1 to xd, are generated in the same manner, and checked against the Bloom Filter that represents the set Y .
If any of the cells x1 to xd is not set to 1, then it can be asserted that x /  Y .
However, if all the cells x1 to xd are set to 1, then there is a good probability that x   Y .
The intuition is very simple, and is similar to that discussed in Section 4.
The probability of a false positive is inversely proportional to d, the number of hash functions used, given that the space utilized grows proportionally with d. The more hash functions used, the less it is probable that a non-duplicate element will collide with other elements on all its d hash functions.
The interesting thing about Bloom Filters is that they do not store the elements of the set whose membership is tested.
This is very useful in cases were the IDs of the elements are huge, like in our case.
However, it is not possible to regenerate the original set from its Bloom Filter representation.
We summarize now the original analysis given in [9].
Assuming the d hash functions are distinct, and each almost uniformly distributes the hashed elements, then the probability that a certain cell has not been set by any of the hash functions after inserting one element in Y is (1   1 M )d. After inserting all the elements in Y , then the probability that the cell has never been set to 1 is (1   1 M )d |Y | .
Thus, the probability that it has been set to 1 is 1   (1   1 M )d |Y | When testing whether, or not, an element in X belongs to Y , the probability that it collides on all its d hash functions, )d   (1   without being a duplicate, is (1   (1   1 M )d |Y | .
e d |Y |/M )d. This probability clari es the tradeo .
The false positive errors increase as the size of Y increases, and decrease as the number of cells, M , increases.
Di erentiating this probability with respect to d, for a  xed array size, and a  xed size of Y , yields the number of hash functions to be used to minimize the probability, which is M|Y |   ln 2.
Substituting this value for d back in the probability of false positive errors yields (2ln 2)M/|Y |   (0.619)M/|Y | , which is a concise form for the probability of false positive errors.
In the next section we will discuss how Bloom Filters can be used to detect duplicates in data streams.
Detection in Data Streams Bloom Filters can be used to detect duplicates in a data stream, by assuming that both X and Y are the data stream.
We start by allocating M bits, where M is O(N ), and N is the estimated size of the processed window.
As illustrated in Figure 3, using d = 17 independent hash functions, we test every new element on the Bloom Filter structure of the previously observed elements, and then insert it into the Bloom Filter structure.
Before setting any of the d cells to 1, the cell is tested whether it has been set before to 1, or not.
The element is not counted as a duplicate if at least 1 bit was switched from 0 to 1, and is considered to be a duplicate otherwise.
Both M and d can be determined according do the required error rate, and the expected window size.
Hash Function 1 Hash Function 2 Hash Function 16 Hash Function 17 Bit Vector Figure 3: The classical Bloom Filters.
The ID is 32 bits long, and 17 hash functions are used.
Bloom Filters are very concise when compared to the basic solution using indexed storage.
It does not store the excessively long IDs of the clicks, and its false positive errors are almost negligible.
Bloom Filters  errors do not depend on the nature or distribution of the duplicated elements.
That is, the number of elements erroneously identi ed as duplicates is the same whether one element was duplicated a lot of times, or a lot of elements were duplicated a few times.
The probability of outputting false positive errors can be made very low using very limited storage.
For instance, based on the previous analysis, using 9.6 bits per element in the data stream reduces the probability of outputting false positive errors to 1%; and using 2 bytes per element reduces this probability to 0.046%, which is a great achievement from a practical point of view.
In addition, the IDs that are output as false positive errors can be stored in a separate cache to verify that they are duplicated frequently.
The original Bloom Filters use one hash space and hashes all the elements onto it.
However, when we developed this
 Hash Function 1 Hash Function 2 Hash Function 16 Hash Function 17 Bit Vector 1 Bit Vector 2 Bit Vector 16 Bit Vector 17 Figure 4: Another Variation of Bloom Filters.
The ID is 32 bits long, and 17 hash functions are used.
solution, independently of Bloom Filters, we used separate hash spaces for di erent hash functions, as sketched in Figure 4.
This scheme of using hash functions perfectly replaces the scheme of bit vectors substrings described in Section 4, and has a similar analysis to Bloom Filters.
Assuming d hash functions, the hash space for each hash function will be M d .
Assuming each hash function almost uniformly distributes the hashed elements, then the probability that a certain cell has not been set by any of the hash functions after inserting one element is (1  d M ).
When inserting the ith element, the probability that the cell has never been set to 1 is (1   d M )(i 1).
Thus, the probability that it has been set to 1 is 1   (1   d M )(i 1).
The probability that the ith element is identi ed by mistake as a duplicate, i.e., it collides on all its d hash functions, is (1  (1  d M )(i 1))d   (1  e d (i 1)/M )d. This is the same as the probability of the original Bloom Filter.
However, having di erent hashing spaces assigned for different functions results in increased server parallelism, since the memory activation latency is minimized.
The memory activation latency,   , is the idle time between two accesses to the same memory module.
Assuming the availability of D memory modules, when using separate hash spaces for di erent hash functions, the time wasted for memory access latency is (cid:6) d  (cid:7).
However, when using one shared hash space for all the hash functions, the time wasted for memory access latency is between (cid:6) d  (cid:7), and d     , depending on how the hashed-to  ags are distributed throughout the hash space.
When used for landmark windows, the Bloom Filter structure is reinitialized at the beginning of each landmark window.
Interestingly, Bloom Filters are more e ective when used for duplicate detection on landmark windows than when used for approximate membership testing.
The basic argument is that superimposing the data stream window, whose size in growing, on both the X and Y sets reduces the false positive errors dramatically.
For instance, when inserting the  rst element in the stream into the Bloom Filter, the probability that it is found to be a duplicate is 0.
When inserting the second element into the Bloom Filter, the probability that it is found to be a duplicate is (1   e d/M )d.
In general, when inserting the ith element into the Bloom Filter, the probability that it is found to be a duplicate is (1   e d (i 1)/M )d. Therefore, the probability that an element is found to be a duplicate is proportional to the number of elements previously inserted into the Bloom Filter.
This is because the more elements accommodated in the Bloom Filter, the higher the probability that a hash collision occurs, and the higher the chance that an element is identi ed as a duplicate.
It is easy to show that the expected number of dupli-(1   cates found in a landmark window of size N is e d (i 1)/M )d, which is less than N  (1 e d N/M )d, the expected error rate of Bloom Filters when testing approximate set membership.
N(cid:3) i=1 Although we have assumed that the hash functions distribute the elements uniformly, which is a di cult to realize assumption, we expect this will be balanced by the fact that the probability of false positive errors is lower in the early parts of the stream than the estimated bound.
The practical performance of the Bloom Filters will be compared to its expected theoretical bound in Section 6.
The main challenge when applying Bloom Filters to sliding window streams is that the structure does not store the IDs of the elements observed so far in the stream, and thus, it is di cult to delete elements which have been already inserted into the Bloom Filter, and is now expiring, i.e., sliding out of the sliding window.
We use a modi cation of Bloom Filters2.
The purpose of this modi cation is to enable Bloom Filters to implement the delete operation, without necessarily storing the IDs of the elements inserted into a Bloom Filter structure.
The underlying idea is to replace the array of bits with an array of counters, of the same size.
For every element that is inserted, increment the d counters to which the element hashes.
To delete an element, decrement the d counters to which the element hashes.
An integer in a cell represents the number of elements which hash to this cell.
A cell is decremented to 0, only if all the elements that hashed to this cell expire, i.e., slide out of the current sliding window, which is equivalent to deleting all those obsolete elements from the Bloom Filter structure.
Therefore, we will call this solution the counting Bloom Filters.
Thus, it is possible to update the Bloom Filter structure as new elements are added to the sliding window, and as aging elements are deleted.
Bloom Filters have the following nice property that makes them usable to detect duplicates in jumping window streams.
Two Bloom Filter structures are, in abstract, two bit arrays.
Thus, it is possible to perform some basic binary operations on them, such as OR, given that they use the same hash functions.
OR-ing two Bloom Filter structures results in a new one that accommodates all the elements that were inserted into the two original structures.
Similarly, the counting Bloom Filter structures proposed in Section 5.2.3 can be added and subtracted.
Adding two Bloom Filter structures, A and B, results in a new one, (A+ B), that accommodates all the elements originally inserted into A and B.
Since the hash space consists of counters, rather than bits, each counter in A (B) corresponds to all
 ters for implementing a scalable distributed cache sharing protocol.
the occurrences of elements that hash to this counter in A (B).
Similarly, each counter in the Bloom Filter structure (A + B) corresponds to all the occurrences of elements that hash to this counter in both A and B.
Notice that the operation A   B is meaningless, unless all the elements in B have also occurred in A.
Thus, it is possible to represent every sub-windows as a Bloom Filter structure.
When a new sub-window is populated, its Bloom Filter representation is combined with the main Bloom Filter structure that represents the entire jumping window, and the eldest window is subtracted from the main Bloom Filter structure.
Combining the two Bloom Filter structures is carried out by adding their counters; and deleting the Bloom Filter structure that correspond to the expiring sub-window is performed by subtracting its counters from the main Bloom Filter structure.
Thus, the Bloom Filter representation of the jumping window is always correct.
In order to evaluate how e ective the proposed solution is, we ran a comprehensive set of experiments.
The experiments used a real click stream data collected at Commission Junction, a ValueClick company.
In addition we ran a set of experiments on synthetic data.
The results are analyzed in this section.
In a preliminary phase of experiments, we used a combination of the user IP address and the advertisement ID, as the click ID.
Both components of the click ID were represented as 32-bit integers, and thus, the click ID was represented as 64-bit integers.
The preliminary thinking was that if some clicks on the same advertisement come from the same IP address more than once in a short period, then this reveals some kind of fraud taking place at the publisher s site.
However, since Network Address Translation (NAT) boxes can hide hundreds to thousands of computers under the same IP address, we found this scheme to be overly aggressive in terms of detecting fraud.
The other option was to use the cookie ID, which is set on the user Internet Browser to be able to track individual users.
Hence, individual clicks were identi ed using 64-byte identi ers.
We are interested in the accuracy of Bloom Filters when detecting duplicates.
To calculate the accuracy, we measure the error rate, the number of non-duplicate clicks identi- ed as duplicates, as a ratio of the number of non-duplicate clicks in the entire stream.
We evaluated the e ectiveness of Bloom Filters for solving the problem of duplicate detection on data streams with di erent window models.
We carried out the experiments on both landmark and jumping windows.
For landmark windows, measuring the error rate is straightforward.
However, for jumping windows, we measure the error rate after each jump, and calculate the mean error rate throughout the whole stream.
We did not test the Bloom Filters approach on sliding windows.
It is infeasible to measure the accuracy of the solution, since that would require exact identi cation of all duplicates for every window.
For instance, if the stream size is 5,000,000, and the window size is  xed to 1,000,000 clicks, then the number of windows that cover the given stream is 5,000,000 - 1,000,000 + 1 = 4,000,001 windows.
Thus, to check the error rate for such solution, the exact solution has to be run 4,000,001 times, and be compared to the results of the approximate solution.
Thus, we only run experiments on the landmark and jumping windows.
For jumping windows, using a small sub-window will result in an overwhelming number of windows.
To reduce this problem, we limit the experiments to only a small number of overlapping windows.
The underlying idea of using Bloom Filters is to exploit the tradeo  between space and time on one hand, and the error rate on the other hand.
The more hash functions used to test for duplicates, the larger the required space, and the smaller the probability of producing false positive errors.
However, since the space usage of Bloom Filters is a constant multiple of the number of hash functions used, throughout the experiments, we use the number of hash functions as the independent axis in our graphs, and will report the ratio between the number of hash functions and the size of the hash space used.
Hence, the reader can recognize the tradeo  between the space usage and the error rate.
In addition, the experiments validate our arguments in Section 5.2.2.
As we mentioned above, we expect that the practical error rate for duplicate detection, in landmark and jumping windows, is close to the theoretical error rate calculated in Section 5.1, if not better.
The experiments support this proposition.
the Number of Hash Functions and the Error Rate In this subsection we sketch and discuss the results of both the real and the synthetic data experiments.
We conducted experiments using both landmark and jumping windows on streams of synthetic data to illustrate how the theoretical and the practical error rates vary with the number of hash functions.
We use a synthetic stream of length 1,000,000 clicks for the landmark window experiments.
The clicks are distinct, i.e., there are no duplicates in the data.
Thus, all the duplicates output by the algorithm are erroneous.
For the jumping window experiments, each jumping window consists of 200,000 distinct clicks, and each sub-window is of size 50,000 clicks.
Therefore, the number of overlapping windows is 8.
The experimental results of the jumping window, and the landmark window are sketched in Figures 5 and 6, respectively.
For landmark windows, the ratio between the hash function to the number of  ags used is 1:1,442,695.
For jumping windows, the ratio between the hash function to the number of counters used is 1:288,539.
The hash space used is proportional to the size of the window handled.
As is clear from Figure 5, the error rate of Bloom Filters is 4 to 8 times lower than the theoretical error rate.
The error rate ratio increased as the number of hash functions increased.
However, the tradeo  relation still exists between the number of hash functions used and the error rate of the Bloom Filters.
The same results hold for the jumping window experiments.
From Figure 6, the error rate of Bloom Filters is also 4 to 8 times lower than the theoretical error rate.
A Comparison Between the Theoretical and Practical Error Rates on Landmark Windows Using Synthetic Data Practical Error Rate Theoretical Error Rate





































 e t a
 r o r r







 Number of Hash Functions Figure 5: Comparing the Theoretical and the Practical Error Rates of Bloom Filters for Duplicate Detection on Landmark Window Using a Synthetic Click Stream A Comparison Between the Theoretical and Practical Error Rates on Jumping Windows Using Synthetic Data Practical Error Rate Theoretical Error Rate e t a
 r o r r



















































 Number of Hash Functions Figure 6: Comparing the Theoretical and the Practical Error Rates of Bloom Filters for Duplicate Detection on Jumping Window Using a Synthetic Click Stream
 The experiments were run on a clicks stream that was collected on August 30, 2004.
The stream was of size 5,583,301 clicks.
Each click had a 64 character identi er, which is composed of an advertisement ID, and a user cookie ID.
The stream has 4,093,573 distinct elements, and 1,489,728 duplicates.
The duplicates were not uniformly distributed, but rather some elements were duplicated much more than others.
The most duplicated element occurred 10,781 times, and understandably, was identi ed as fraudulent.
The second most duplicated element had 4,487 occurrences, and was also  agged as being fraudulent.
To evaluate the tradeo  between the number of hash functions and the error rate, we conducted a set of experiments using both landmark and jumping windows.
For the landmark window experiments, we use the whole stream as the dataset.
For the jumping window experiments, each jumping window is of size 2,000,000 clicks, and each sub-window is of size 500,000 clicks, and we limit the number of windows to 4.
The experimental results of the landmark window are sketched in Figure 7, while those of the jumping window A Comparison Between the Theoretical and Practical Error Rates on Landmark Windows Using Real Data Practical Error Rate Theoretical Error Rate





























 Number of Hash Functions




 e t a
 r o r r
















 Figure 7: Comparing the Theoretical and the Practical Error Rates of Bloom Filters for Duplicate Detection on Landmark Window Using a Real Click Stream A Comparison Between the Theoretical and Practical Error Rates on Jumping Windows Using Real Data Practical Error Rate Theoretical Error Rate

























 e t a
 r o r r




















 Number of Hash Functions


 Figure 8: Comparing the Theoretical and the Practical Error Rates of Bloom Filters for Duplicate Detection on Jumping Window Using a Real Click Stream experiments are sketched in Figure 8.
For landmark windows, the ratio between the hash function to the number of  ags used is 1:5,905,777.
For jumping windows, the ratio between the hash function to the number of counters used is 1:2,177,154, on average, depending on the number of distinct elements in the stream window.
Notice that the ratio between the hash function to the size of the hash space is proportional to the number of distinct elements in the stream/window.
From Figure 7, it is clear that the error rate of Bloom Filters is much lower than the theoretical error rate, though the tradeo  relation still exists between the number of hash functions used and the error rate.
From Figure 8, it is noteworthy that the practical error rate of landmark window processing is lower than that for handling jumping windows.
Most probably this is due to the nature of the real click stream, since that was not the case for synthetic data.
In this paper, we have introduced the problem of  nding duplicates in data streams, to detect fraud in advertisement networks.
To the best of our knowledge, no similar work has been done before.
A solution based on Bloom Filters [9] was proposed, and was applied to the motivating application of detection fraudulent hit in ation in advertising networks.
Interestingly, the idea of independent hashing employed in Bloom Filters is more e ective when used for duplicate detection than when used for approximate membership testing, which is the motivating application for the original Bloom Filter.
We ran a set of experiments using both real and synthetic data.
The results demonstrated that applying the scheme of independent hashing to duplicate detection in click streams yields practical error rates that are lower than the theoretical error rates.
The reason is that the Bloom Filter structure is not fully populated throughout the landmark or jumping windows.
Hence, it can be deduced that using the theoretically optimum number of hash functions does not necessarily yield the least error rate.
Thus, in the cases of landmark or jumping windows, it could be advisable to use slightly more hash functions than the theoretical optimum.
It could be argued that  nding duplicates in click streams in the way discussed above is not e ective for detecting the hit in ation attack.
The fraudulent publisher could modify the script to delete the cookies every time it simulates a click on the advertisement.
Thus, every time the script simulates a click, the advertising commissioner will detect no cookie, and will assign a new cookie ID.
Thus the proposed duplicate detection technique will not detect this variation of attack, since the cookie ID is a part of the click ID.
The solution for this attack variation is very simple.
If the publisher runs such a modi ed script, then the advertising commissioner will notice a suspicious number of IDs being assigned to a speci c IP address at a fast rate, and for a long period of time.
This could not be a normal situation, since the number of computers behind any NAT box is  nite.
Therefore, the advertising commissioner needs to keep track of the IP addresses that are assigned extremely large numbers of cookie IDs.
This is a problem of detecting frequent elements in data streams, which is already a solved problem, as discussed in Section 2.
