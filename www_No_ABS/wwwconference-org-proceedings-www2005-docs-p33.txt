Edge service architectures have become the most widespread platform for distributing Web content over the Internet.
Commercial Content Delivery Networks (CDNs) like Akamai [4] and Speed-era [28] deploy edge servers around the Internet that locally cache Web pages and deliver them to the clients.
By serving Web content from edge servers located close to the clients, the response time for serving clients is reduced as each request need not travel across a wide-area network.
Typically, edge servers store static Web pages and a plethora of techniques and commercial systems exist for replicating static pages [26].
In practice, however, a large amount of Web content is generated dynamically.
These pages are generated upon request using Web applications that take, e.g., individual user pro les and request parameters, into account when producing the content.
These Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
applications tend to run on top of databases.
When a request arrives, the application examines the requests, issues the necessary read or update transactions to the database, retrieves the data, and composes the page that is sent back to the client.
To speed up the access, traditional CDNs use techniques such as fragment caching whereby the static fragments (and sometimes also certain dynamic parts) of a page are cached at the edge servers [13, 23, 9, 18, 11].
However, this solution assumes that temporal locality of requests is high and database updates are infrequent.
Applications that do not meet these assumptions require different solutions than fragment caching.
In this paper we explore a different approach to delivering dynamic content that offers more  exibility than fragment caching.
The idea is to replicate not the pages but the data residing in the underlying database.
This way, the dynamic pages can be generated by the edge servers without having to forward the request to a centralized server to guarantee consistency.
In particular, if the temporal locality and update rate are high, an application may ben-e t by moving (part of) the underlying database, along with the code that accesses the data, to the location where most updates are initiated.
If the temporal locality and the update rate is low, then replication of the database and access code may be a viable solution.
This allows the edge servers to generate documents locally.
In both cases, the amount of wide-area traf c can be reduced, leading to better client-perceived performance.
We are thus confronted with the problem of sustaining performance through distribution and replication of (parts of) an application, depending on access and usage patterns.
In general, we need to address the following three issues: (1) determine which parts of an application and its data need to be replicated, (2)  nd where these parts need to be placed in a wide-area network, and (3) decide how replicated data should be kept consistent in the presence of updates.
These issues are currently handled by human experts who manually partition the databases of a Web application, and subsequently distribute the data and code among various servers around the Internet.
Not only is this a dif cult and time-consuming process, it is also not very feasible when usage and access patterns change over time [24].
In addition, consistency in these systems tends to be ad-hoc and largely application-dependent.
In this paper we propose GlobeDB, a system for hosting Web applications, which handles distribution and partial replication of application data automatically and ef ciently.
Our system provides Web-based data-intensive applications the same advantages that content delivery networks offer to traditional Web sites: low latency and reduced network usage.
We substantiate these claims data requires addressing many issues such as identifying the granularity and constituents of the data segments,  nding the optimal placements for each data unit, and maintaining consistency of replicated data units.
In this section, we discuss these issues in detail.
The  rst and foremost issue is to decide the extent to which an application should be aware of data replication.
Replication can yield the best performance if it is completely tuned to the speci c application and its access patterns [15].
However, this requires sig-ni cant effort and expertise from an application developer.
As a consequence, optimal performance is often not reached in practice.
Furthermore, changes in access pattern may warrant changes in in the replication strategies, thereby making the design of an optimal strategy even more complex.
In our system, we chose a transparent replication model.
The application developer need not worry about replication issues and can just stick to the functional aspects of the application.
The system will automatically  nd a placement and replication strategy, and adapt it to changing access patterns when needed.
The underlying principle behind our system is to place each data unit only where it is accessed.
In our previous research on replication for static Web pages, we showed that the optimal replication performance in terms of both client-perceived latency and update bandwidth can be achieved if each Web page is replicated according to its individual access patterns [20].
A naive transposition of this result would lead to replicating each database record individually.
However, such  ne-grained replication can result in signi cant overhead as the system must maintain replication information for each record.
In our system, we employ an approach where the data units are initially de ned at a very  ne grain.
Data units having similar access patterns are then automatically clustered by the system.
The system subsequently handles replication at the cluster level, thereby making the problem of managing a cluster feasible without losing the bene ts of partial replication.
However, a caveat of this approach is that if the access patterns change, then the system must perform re-clustering to sustain good performance.
More information on the clustering algorithm used in our system is presented in Section 5.
One important issue in any replicated system is consistency.
Consistency management has two main aspects: update propagation and concurrency control.
In update propagation, the issue is to decide which strategy must be used to propagate updates to replicas.
Many strategies have been proposed to address this issue.
They can be widely classi ed into push-based and pull-based strategies.
Pull-based strategies are mostly suitable for avoiding unnecessary data update transfers when no data access occurs between two subsequent updates.
In our system, we decided to use a push strategy where all updates to a data unit are pushed to the replicas immediately.
Pushing data updates immediately ensures that replicas are kept consistent and that the servers hosting replicas can serve read requests immediately.
Yet, propagating updates is not suf cient to maintain data consistency.
The system must also handle concurrent updates to a data unit emerging from multiple servers.
Traditional non-replicated DBMSs perform concurrency control using locks or multiversion models [16].
For this, a database requires an explicit de nition of transaction, which contains a sequence of read/write operations to Figure 1: Example of bene ts of autonomic replication.
through extensive experimentation of a prototype implementation running the TPC-W benchmark over an emulated wide-area network.
Our major contribution is that we demonstrate that con guration of Web applications for data and code replication can be largely automated, and in such a way that it yields a substantial performance improvement in comparison to simple or non-replicated approaches.
As such, our work improves upon other research demonstrating that application-speci c replication improves performance as well (e.g., In particular, we argue that there is often no compelling reason to adapt the logical design of an application in order to support replication.
By automatically partitioning and replicating an application s associated database, we can achieve the same results without involving the application designer or requiring other human intervention.
[15]).
The rest of the paper is organized as follows.
Section 2 presents several design issues involved in building the system and motivates our design choices.
Section 3 presents GlobeDB s architecture and Section 4 describes the design of the data driver, the central component of GlobeDB.
Section 5 describes the replication and clustering algorithms adopted in our system.
Section 6 presents an overview of GlobeDB implementation and its internal performance.
Section 7 presents the relative performance of GlobeDB and different edge service architectures for the TPC-W benchmark.
Finally, Section 8 discusses the related work and Section 9 concludes the paper.
To illustrate the bene ts of autonomic replication, consider the scenarios presented in Figure 1 that show edge server(s) hosting a Web application.
As seen in the  gure, there is a fraction of data accessed only by clients of server 1, another fraction by clients of server 2 and the rest are accessed by clients of both servers.
Not replicating at all (centralized system) can result in poor client response time.
Replicating all data everywhere (full replication) can result in signi cant update traf c between servers for data they barely access.
In such scenarios, GlobeDB can be very useful as it places the data in only those servers that access them often.
This can result in a signi cant reduction in update traf c and improved client-perceived response time.
Building a system for autonomic replication of Web application    ReplicationFullServer 1 DataServer 1Server 2 DataServerAutonomic ReplicationSystemCentralizedUpdatesUpdates DataGlobeDBAutonomic ReplicationServer 2 Data Data Data Data !!!
""""####$$%%&&&''((()))**++,,--./011223333333344444444555566667777777788888888Clients accessingClients accessingClients accessing999999::::::;;;;<<<<==>>?
@ABCDDDDDDDDEEEEEEEEFFGGHHHHIIJJJJKKLLLLLLMMMMMMNNOOPPPPQQQQRRRSSa database.
For example, PostgreSQL uses a variant of multiver-sion model called snapshot isolation to handle concurrent transactions [3].
When querying a database each transaction sees a snapshot of consistent data (a database version), regardless of the current state of the underlying data.
This protects the transaction from viewing inconsistent data produced by concurrently running update transactions.
However, concurrency control at the database level can serialize updates only to a single database and does not handle concurrent updates to replicated data units at multiple edge servers.
Traditional solutions such as two-phase commit are rather expensive as they require global locking of all databases, thereby reducing the performance gains of replication.
To handle this scenario, the system must serialize concurrent updates from multiple locations to a single location.
GlobeDB does not guarantee full transaction semantics, but enforces consistency for each query individually.
In other words, it imposes that each transaction be composed of a single query/operation which at most modi es a single data unit.
The concurrency control adopted in our system is explained in Section 3.
Automatic data placement requires the system to  nd a set of edge servers to host the replicas of a data cluster according to certain performance criteria.
One can measure the system performance by metrics such as average read latency, average write latency, amount of update traf c, etc.
But a naive approach based on optimizing the system performance for one of these metrics alone can easily result in degrading the performance according to other metrics.
For example, a system can be optimized for minimizing read latency by replicating the data to all replica servers.
However, this can lead to huge update traf c if the number of updates is high.
In general, there is a clear tradeoff between the performance gain due to replication and the performance loss due to consistency enforcement.
However, there is no universal de nition of  best  tradeoff.
In fact, each system administrator should specify a particular tradeoff based on the system needs.
For example, the administrator of a CDN with (theoretically) unlimited bandwidth may choose to optimize on client response time alone.
The same administrator, when facing a bottleneck at the central server, may prefer to minimize update traf c.
In our system, the system administrator speci es relative performance tradeoffs as the weights of a cost function.
This function aggregates multiple performance metrics into a single abstract metric.
Optimizing the cost function is equivalent to optimizing the global system performance.
This function therefore acts as a measure of the desired system performance and aids the system in making its placement decisions.
The cost function and placement algorithms are presented in Section 5.
The application model of our system is shown in Figure 2.
An application is made of code and data.
The code is written using standard dynamic Web page technology such as PHP and is hosted by the Web server (or the Web application server).
It is executed each time the Web server receives an HTTP request from its clients, and issues read/write accesses to the relevant data in the database to generate a response.
Access to the data is done through a data driver which acts as the interface between code and data.1 The data driver preserves
 Figure 2: Application Model distribution transparency of the data and has the same PHP interface as regular PHP drivers.
It is responsible for  nding the data required by the code, either locally or from a remote server, and for maintaining data consistency.
We assume that the database is split into n data units, D1, D2,  , Dn, where a data unit is the smallest granule of replication.
Each unit is assumed to have a unique identi er, which is used by the data driver to track it.
An example of a data unit is a database record identi ed by its primary key.
GlobeDB enforces consistency among replicated data units using a simple master-slave protocol: each data cluster has one master server responsible for serializing concurrent updates emerging from different replicas.
GlobeDB assumes that each database transaction is composed of a single query which modi es at most a single data unit.
When a server receives an update request, it forwards the request to the master of the cluster, which processes the update request and propagates the result to the replicas.
Note that this model is sometimes called eventual consistency.
If stronger consistency is needed, then models such as session guarantees can be envisaged [29].
Figure 3: System Architecture - Edge servers serving clients close to them and interactions among edge servers goes through Wide-area network.
The architecture of GlobeDB is presented in Figure 3.
An application is hosted by m edge servers spread across the Internet.
Communication between edge servers is through wide-area networks just an interface driver but also responsible for functional aspects such as location of replicated data.
Requests/ResponsesClientDatabaseServerDataDataDriverAccessRead/WriteCodeApplicationServerWeb    Web ServerDataAccessDataDriverRead/WriteServerDatabaseWeb ServerDataAccessDataDriverRead/WriteServerDatabaseWeb ServerDataAccessDataDriverRead/WriteServerDatabaseConsistency UpdatesConsistency UpdatesConsistency UpdatesInternet...Edge Server 1Edge Server mOrigin ServerWebClients                         incurring wide-area latency.
Each client is assumed to be redirected to its closest edge server using enhanced DNS-based redirection [12, 14].
Furthermore, for each session, a client is assumed to be served by only one Web server.
When a client issues an HTTP request to a Web server, the request is forwarded to the application code (e.g., PHP) residing in the server.
The application code usually issues a number of read or write accesses to its data through the data driver.
The application data are partially replicated, so the local database hosts only a subset of all data clusters.
The data driver is responsible for  nding the relevant data either locally or from a remote edge server if the data are not present locally.
Additionally, when handling write data accesses, the driver is also responsible for ensuring consistency with other replicas.
To perform autonomic replication, the system must decide on the placement of replicas for each data cluster and choose its master according to its access pattern.
To this end, each application is assigned one origin server, which is responsible for making all application-wide decisions such as clustering data units and placing clusters on edge servers.
The origin server performs replica placement periodically to handle changes in data access patterns and the creation of new data units.
For reasons explained below, the origin server also has a full replica of the database.
Consistency is enforced using a simple master-slave protocol: each data cluster has one master server responsible for serializing concurrent updates emerging from different replicas.
When a server holding a replica of a data cluster receives a read request, it is answered locally.
When it receives an update request, it is forwarded to the master of the data cluster.
If the server does not have a replica, then requests are forwarded to the origin server.
More information on locating data units is presented in Section 4.
Note that eventual consistency does not guarantee that all replicas are identical at all times.
The master of a cluster delivers updates to all replicas in the same order without any global locking.
This can lead to transient situations where the latest updates have been applied only to a fraction of the replicas.
We assume that this can be tolerated by the application as each client session is handled by only one edge server in the network.
Note that even existing solutions over commercial database systems such as DBCache [6] and MTCache [17] face similar issues and guarantee the same level of consistency as GlobeDB.
The data driver is the central component of GlobeDB.
It is in charge of locating the data units required by the application code and maintaining consistency of the replicated data.
The primary functionality of the data driver is to locate the data units required by the application code.
Data access queries can be classi ed into write and read queries based on whether or not an update to the underlying database takes place.
Another way of classifying queries is based on the number of rows matched by the query selection criterion.
We refer to the queries based on primary keys of a table or that result in an exact match for only one record as simple queries.
An example of a simple query is  Find the customer record whose userid is  xyz .  Queries based on secondary keys and queries spanning multiple tables are referred to as complex queries.
An example of a complex query is  Find all customer records whose location is  Amsterdam .  As noted earlier, we assume that each data unit has a unique identi er.
For  ne-grained data units, such as database records, we use the primary key as the record s unique identi er.
This allows the data driver to map simple queries to required data units, which makes locating data units relatively straightforward.
For answering a complex query, the driver cannot restrict its search to its local database but needs to inspect the entire database.
Such a query can be answered by forwarding it to a subset of servers that jointly have the complete database table.
For the sake of simplicity, we stipulate that the origin server has a replica of all data units.
For this reason all complex queries are forwarded to the origin server which will incur a wide-area latency.
At the outset, being able to answer only simple queries locally might appear very limiting.
However, typical Web applications issue a majority of simple queries.
To get an idea of the percentage of simple queries used on real e-commerce applications, we examined the TPC-W benchmark which models a digital bookstore [27].
We collected the SQL traces generated by execution of the benchmark s ordering mix workload [1].
We analyzed the accesses to the book tables (which contains the records pertaining information about individual books) and found that more than 80% of the accesses to the table consist of simple queries or can be easily rewritten as simple queries.
Similarly, about 80% of accesses to the customer tables use simple queries.
Similar  gures are seen for other workload mixes of TPC-W.
In TPC-W, updates to a database are always made using simple query.
if complex query then Execute query at the origin server and return result; else if read then Execute query locally; if execution returns result then return result; else end execute on origin server and return result; else end Get cluster id of data unit from (local or origin server); Find master for cluster from cluster-property table; Execute query on master server and return result; end Algorithm 1: Pseudocode used by data driver for executing queries The data driver of each edge server maintains three tables.
The cluster-membership table stores the identi ers of data units contained in each cluster.
The cluster-property table contains the following information for each data cluster: the origin server, the master replica, and the list of servers that host a copy of this cluster.
These two tables are fully replicated at all edge servers.
Each driver also maintains an access table to keep track of the number of read and write accesses to each cluster.
To answer simple queries, the driver locates a data unit by identifying the cluster to which the data unit belongs, using the cluster-membership table.
Once the appropriate cluster is identi ed, the driver uses the cluster-property table to  nd details about the location of the cluster and its master.
Upon each read or write access, the driver updates the access table accordingly.
The data driver forwards all complex queries to the origin server.
A naive design of the cluster-membership table can be a scala-bility bottleneck.
In our initial implementation, we used bit arrays for numerical primary key IDs and bloom  lters for nonnumerical IDs [5].
A typical bit-array based cluster membership table will have a size of only 125 Kbits for each cluster to represent a database with a million data units.
Such a small size allows the table to reside in main memory thereby resulting in faster access.
However, these  lters have several disadvantages.
First, storing nonnumerical IDs using Bloom  lters can result in potential inaccuracies that result in redundant network traf c.
Second, the system needs to allocate enough memory for  lters to accommodate creation of new data units in the future, which poses a scalability problem.
To overcome these shortcomings, in our current implementation the cluster-membership table is stored along with the respective database records.
In this implementation, each database record has an extra attribute that indicates which cluster the record belongs to.
The pseudocode for executing queries by the data driver is shown in Algorithm 1.
As seen in the  gure, simple queries with read accesses are always  rst executed locally.
If a record is returned, the result is returned immediately.2 Otherwise, the query is forwarded to the origin server (which contains a full copy of the database).
For update queries, the driver  rst runs a query to  nd the cluster ID.
If the data record is replicated locally, its cluster ID will be returned and the information regarding who its master is can be obtained from the cluster-property table.
Then, the update query is forwarded to the master.
In case of update queries to a data unit not present locally, the query to  nd cluster information for the data unit will return no result and this query is then run on the origin server.
Subsequently, the update query will be sent to the appropriate master server.
The cluster-property table is simply implemented as a  le.
Replicating an application requires that we replicate its code and data.
For the sake of simplicity, in this paper we assume that the code is fully replicated at all replica servers.
In this section, we present the algorithms we use for clustering data units, placing the data cluster, and selecting their master.
For the placement of data, we use a cost function that allows the system administrator to tell GlobeDB his/her idea of optimal performance.
As we explain in detail in this section, GlobeDB uses this function to assess the goodness of its placement decisions.
As we mentioned earlier, in GlobeDB, data units with similar access patterns are clustered together.
A similar problem was addressed in [10] in the context of clustering static Web pages to reduce the overhead in handling replicas for each Web page.
The authors propose several spatial clustering algorithms to group pages into clusters and incremental clustering algorithms to handle the creation of new pages.
They show that these clustering algorithms perform well for real-world Web traces.
We use similar algorithms for clustering data units.
The origin server is responsible for clustering the data units during the initial stages of system.
The origin server collects access patterns of data units from all edge servers into its access table.
Each data unit Di s access pattern is modelled as a 2 m-dimensional vector, Ai =< ri,1,ri,2,  ,ri,m,wi,1,  ,wi,m >, where ri,j and wi,j are respectively the number of read and write accesses made by the edge server Rj to the data unit Di.
The origin server runs a spatial clustering algorithm that uses correlation-based similarity on the access vectors of the data units.
As a result, two data units Di and Dj are grouped into the same cluster if and only if Ai
 single data unit) based on non-primary queries.
and Aj are similar.
In correlation-based similarity, the similarity between two data units Di and Dj is given by: (cid:80)2 m (cid:113)(cid:80)2 m k=1 (ai,k    ai)(aj,k    aj) k=1 (ai,k    ai)2 (cid:80)2 m k=1 (aj,k    aj)2 Sim(i, j) = where < ai,1,ai,2,  ,ai,m > = < ri,1,ri,2,  ,ri,m > and < ai,m+1,ai,m+2,  ,ai,2 m > = < wi,1,  ,wi,m >.
Data units Di and Dj are clustered together if Sim(i, j)   x, for some threshold value x, where 0   x   1.
The origin server iterates through data units that are yet to be clustered.
If a data unit Di is suf ciently close to the access vector of an existing cluster, it is merged into it.
Otherwise, a new cluster with Di as the only member is created.
Once the data clusters are built, the origin server creates the appropriate cluster-membership table for each cluster.
Obviously, the value of x has an impact on the effectiveness of the replication strategy.
In all experiments from Section 6 we  x the threshold x to 95%.
We will study the process of determining the optimal threshold value in the near future.
Data clustering works well if data units once clustered do not change their access pattern radically.
However, if they do, then the clusters must be reevaluated.
The process of re-clustering requires mechanisms for identifying stale data units within a cluster and then re-clustering them.
For reasons of space, we do not address the re-clustering problem in this paper.
It is orthogonal to the replication strategy and it mainly involves determining when to re-cluster and how to re-cluster ef ciently.
Also note that re-clustering can be done by progressively invalidating and validating copies at the different edge servers as it is done for data caches.
The origin server must periodically select the best replication strategy for each cluster.
A replication strategy involves three aspects: replica placement, consistency mechanism, and, in our case, master selection.
As we use push strategy as our consistency mechanism, the selection of a replication strategy for a cluster boils down to deciding about replica placement and selecting the master.
As noted earlier, to select the best replication strategy, the system administrator must specify what  best  actually means.
In GlobeDB, we represent overall system performance into a single abstract  gure using a cost function.
A cost function aggregates several evaluation metrics into a single  gure.
By de nition, the best con guration is the one with the least cost.
An example of a cost function which measures the performance of a replication strategy s during a time period t is: cost(s, t) =     r(s, t) +     w(s, t) +     b(s, t) where r is the average read latency, w is the average write latency, and b is the amount of bandwidth used for consistency enforcement.
The values  ,   and   are weights associated to metrics r, w, and b respectively.
These weights must be set by the system administrator based on system constraints and application requirements.
A larger weight implies that its associated metric has more in uence in selecting the  best  strategy.
For example, if the administrator wants to optimize on client performance and is not concerned with the bandwidth consumption, then weights   and   can be increased.
Finding the  best  system con guration now boils down to evaluating the value of the cost function for every candidate strategy and selecting the con guration with the least cost.
In GlobeDB we use the cost function as a decision making tool to decide on the best server placements and master server for each cluster.
Ideally, the system should periodically evaluate all possible combinations of replica placement and master server con gurations for each cluster with the cluster s access pattern for the past access period.
The con guration with the least cost should be selected as the best strategy for the near future.
This relies on the assumption that the past access patterns are a good indicator for the near future.
This assumption has been shown to be true for static Web pages and we expect the dynamic content will exhibit similar behavior [20].
Ideally, the system should treat the master selection and replica placement as a single problem and select the combination of master-slave and replica placement con guration that yields the minimum cost.
However, such a solution would require an exhaustive evaluation of 2m   m con gurations for each data cluster, if m is the number of replica servers.
This makes this solution computationally infeasible.
In GlobeDB, we use heuristics to perform replica placement and master selection (discussed in the next subsections).
We propose a number of possible heuristics for placement and a method for optimal selection of master server.
This reduces the problem of choosing a replication strategy to evaluating which combination of master server and placement heuristics performs the best in any given situation.
After selecting the best replica placement and master for each data cluster, the origin server builds the cluster-property table and installs it in all edge servers.
Proper placement of data clusters is important to obtain good client latencies and reduced updated traf c.
We de ne a family of placement heuristics P x where an edge server hosts a replica of a data cluster if its server generates at least x% of data access requests.
Obviously, the value of x affects the performance of the system.
A high value of x will lead to creating no replica at all besides the origin server.
On the other hand, a low value of x may lead to a fully replicated con guration.
Expecting the system administrator to determine the appropriate value for x is not reasonable, as the number of parameters that affect system performance is high.
Instead, in GlobeDB, administrators are just expected to de ne their preferred performance trade-offs by choosing the weight parameters of the cost function.
The origin server will then evaluate the cost value for placement con gurations obtained for different values of x (where x=5,10,15), and select the one that yields the least cost as the best placement con guration.
Master selection is essential to optimize the write latency and the amount of bandwidth utilized to maintain consistency among replicas.
For example, if there is only one server that updates a data cluster, then that server should be selected as the cluster s master.
This will result in low write latency and less update traf c as all updates to a cluster are sent to its master and then propagated to the replicas.
We use a method for optimal selection of master server that results in the least average write-latency.
Let wi,j be the number of write access requests received by edge server Rj for cluster i and ljk be the latency between edge server j and k (we assume that latency measurements between servers are symmetric, i.e., lkj=ljk).
The average write latency for data cluster i whose master is k is given by: wlki = ( j=1 wi,j).
The origin server selects the server with lowest average write latency as the master for a data cluster .
(cid:80)m j=1 wi,j   ljk)/( (cid:80)m


 In this section, we discuss the salient components of our pro-Figure 4: Salient components of the system totype system and also present performance measurements on the overhead that the data driver introduces to a single edge server.
Implementation Overview The salient components of our prototype system are shown in Figure 4.
We implemented our data driver by modifying the existing Apache PHP driver for PostgreSQL, by adding new query interfaces to the existing PHP driver.
This driver can be added as a module to the Apache Web server.
Each edge server runs a logger service, which is responsible for collecting information regarding which cluster was accessed by the Web clients.
The logger is implemented as a standalone multi-threaded server that collects information from the driver and periodically updates the access database.
As seen in the  gure, the origin server runs two special services, clustering and replica placement services.
The clustering service performs the clustering of data units during the initial stages of system deployment.
It periodically collects access patterns from the edge servers and stores it in its database.
Subsequently, it performs clustering with the algorithm described in Section 5.
The origin server only acts a backend database so it does not need to have a Web server or logger service.
The replica placement service in the origin server  nds the  best  locations for hosting a replica and master for each cluster.
It does so by evaluating the cost obtained by different replica placement strategies and master selection for the cluster s access pattern in the previous period.
Note that once data units are clustered, the logger service in the edge servers starts collecting access patterns at the cluster level.
This information is then used by the replica placement service to place the replicas.
Upon deciding which servers would host which clusters, the placement service builds the cluster-property table for each cluster and copies it to all edge servers.
To perform periodic adaptation, the replica placement service is invoked periodically.
We note that the period of this adaptation must be set to a reasonable value to ensure good performance and stability.
We intend to study the need and support for continuous adaptation and effective mechanisms for them in the future.
The clustering service is also run periodically.
The bulk of its work is done during the initial stages when it has to cluster large numbers of data units.
Later, during every period, it performs incremental clustering of newly created data units.
The current prototype does not perform any kind of re-clustering.
Data Driver The data driver receives SQL queries issued by the application code and is responsible for locating the relevant data unit from the ApacheClusteringServiceAccess tablePlacementServiceStores/reads access tablefor clusteringOrigin ServerRead accesstable forplacementEdge ServerData DriverDatabase ServerDatabaseLogger ServerServicelocal server or from a remote server.
It does so by checking appropriate cluster-membership tables for each request.
Performance gains in terms of latency or update traf c occur when the clusters accessed by a given edge server can be found locally.
However, it is important to ensure that the gain obtained by replication is not annulled by the overhead due to checking the cluster-membership table for each data access.
To analyze this, we study the response time of our driver when executing a query on a local replicated data in comparison to the response time of the original PHP driver.
In our experimental setup, we ran the Apache Web server on a PIII-900MHz Linux machine with 1 GB main memory.
The Post-greSQL database also runs on an identical machine.
We created a book table with  elds such as book id, book name, author id, stock quantity and 5 other integer  elds.
The database was populated with 100, 000 random records.
We measured the execution latencies of read and write queries using the original PHP driver and the GlobeDB PHP driver for different throughput values.
In both cases, the requested data is available locally; the only difference is that the GlobeDB driver needs to check its cluster membership and cluster-property tables before each data access.
Read queries read a random database record using a simple query.
Write queries increment the stock  eld of a randomly chosen book record.
To make sure that each access is local, the server is assumed to be the master for all clusters.
We computed the execution latency as the time taken by the server to generate the response for a request.
We do not include the network latency between the client and server as the objective of this experiment is only to measure the overhead of our driver in processing the request.
The results of this experiment are given in Figure 5.
As seen in the  gure, even for high throughputs, the overhead introduced by our implementation is between 0 and 5 milliseconds for read accesses, and at most 10 milliseconds for writes access.
This is less than 4% of the database access latencies incurred by the original driver.
We conclude that the overhead introduced by our driver is very low and, as we shall see in the next section, negligible compared to the wide-area network latency incurred by traditional non-replicated systems.
The experiments presented in the previous section show that the overhead introduced by GlobeDB s driver in a single edge server is low.
However, it does not offer any insight into the performance gains of GlobeDB.
In this section, we study the performance gain that could be obtained using GlobeDB while hosting an e-commerce application.
We chose the TPC-W benchmark and evaluated the performance of GlobeDB in comparison to other existing systems for different throughput values over an emulated wide-area network.
As the experiment results presented in this section will show, GlobeDB can reduce the client access latencies for typical e-commerce applications with large mixture of reads and write operations without requiring manual con gurations or performance optimizations.
We deployed our prototype across 3 identical edge servers with Pentium III 900 Mhz CPU, 1-GB of memory and 120GB IDE hard disks.
The database servers for these edge servers were run on separate machines with the same con guration.
Each edge server uses Apache 2.0.49 Web servers with PHP 4.3.6.
We use PostgreSQL
 con guration as the edge servers except that it acts just as a back-end database and does not run a Web server.
We emulated a wide-area network (WAN) among servers by directing all the traf c to an intermediate router which uses the NIST Net network emulator [2].
This router delays packets sent between the different servers to simulate a realistic wide-area network.
In the remaining discussion, we refer to links via NISTNet with a bandwidth of 10M bps and a latency of 100ms as WAN links and 100M bps as LAN links.
We use two client machines to generate requests addressed to the three edge servers.
A similar setup to emulate a WAN was used in [15].
We deployed the TPC-W benchmark in the edge servers.
TPC-W models an online bookstore and de nes workloads that exercise different parts of the system such as the Web server, database server, etc.
The benchmark de nes activities such as multiple online browsing sessions using Remote Browser Emulators (RBEs), dynamic page-generation from a database, contention of database accesses and updates.
The benchmark de nes three different workload scenarios: browsing, shopping and ordering.
The browsing scenario has mostly browsing related interactions (95%).
The shopping scenario consists of 80% browsing related interactions and
 mixture of shopping and browsing interactions.
In our experiments, we evaluate GlobeDB for the ordering scenario, as it generates the highest number of updates.
The performance metrics of TPCW-benchmark are WIPS (Web Interactions Per Second), which denotes the throughput one could get out of a system and WIRT, which denotes the average end-to-end response time that would be experienced by a client.
The benchmark uses the following database tables: (i) tpcw item stores details about books and their stocks; (ii) tpcw customer stores information about the customers; (iii) tpcw author stores the author-related information; (iv) tpcw order and tpcw orderline store order-related information.
In our experiment, we study the effects of replication only for the tpcw customer and the tpcw item tables as these are the only tables that receive updates and read accesses simultaneously.
The tpcw author table is replicated everywhere as it is never updated by Web clients.
In our experiments, we want to compare the performance of GlobeDB with traditional centralized and fully replicated scenarios.
In principle, a fully replicated system should replicate all tables at all edge servers.
However, each record of ordering-related tables is mostly accessed by only a single customer.
In addition, the entire order database is used to generate the  best sellers  page.
In such a scenario, full replication of ordering-related tables would be an overkill as it would result in too much update traf c among edge servers.
Any reasonable database administrator would therefore store ordering-related database records only in servers that created them and maintain a copy at the origin server (which would be responsible for generating responses to  best sellers  queries).
So, for a fair comparison with GlobeDB, we implemented this optimization manually for the fully replicated system.
We use the open source PHP implementation of TPC-W benchmark [19].
We disabled the image downloading actions of RBEs as we want to only evaluate the response time of dynamic page generation of the edge server.
To account for the geographical diversity of the customers, we de ned 3 groups of clients which respectively issue their requests to a different edge server.
We believe this is a realistic scenario as customers typically do not move often and are usually redirected to the server closest to them.
The tpcw item table stores  elds such as its unique integer iden-ti er, author identi er, item name, price and stock.
In addition to these  elds, it also stores  ve integer  elds that have identi ers of books that are closely related to it and have a similar customer (a) Read Accesses (b) Write Accesses Figure 5: A comparative study of GlobeDB driver implementation with original PHP driver for reading and updating local data units base.
These  elds are read by the application code to generate promotional advertisements when a client reads about a particular book.
For example, related  elds of a Harry Potter book may contain identi ers of the other Harry Potter books.
In our experiment, we  lled these related identi ers as follows: the item records are classi ed into three groups and related entries of each item is assigned to an item in the same group.
In this way, the related  eld truly re ects books with similar customer base.
Furthermore, TPC-W stipulates that each Web session should start with a book for promotion.
In our experiments, client of edge server i receive the starting random book id only from item group i.
At the outset, this might look as if each client group request books from only one item group.
However, this is not the case as the RBE clients select the books to view also from other interactions, such as bestsellers and search result interactions, which have books spanning across multiple item groups.
As it can be seen, even though the assumptions we make about the clients access patterns are realistic, they do not capture all kinds of client access patterns.
To address this issue, in one of our earlier studies, we simulated our proposed replication techniques for different kinds of access patterns from uniform popularity (all clients are interested in all data) to very skewed popularity (only a small set of clients are interested in a particular piece of data) using statistical distributions [25].
We found that our techniques perform well in all cases compared to traditional fully replicated and centralized systems.
Hence, in this experiment we restrict our evaluations to only the workload described in this setup and study the relative performance of different system implementations.
In our experiments, for each run we ran the benchmark for 8 hours.
After this, the origin server collected the access patterns from the edge servers and performed clustering and replication.
Analysis of the tpcw customer table resulted in 3 clusters.
Each of these clusters were mostly accessed by only one edge server (different one in each case).
Analysis of tpcw item accesses led to 4 clusters.
3 out of the 4 clusters are characterized by accesses predominantly from only one edge server (a different one in each case).
However, the fourth cluster represents data units that are accessed by clients of all the three edge servers.
We evaluated the performance of four different systems (shown in Figure 6): (i) a traditional centralized architecture where the code and the database is run in a server across a WAN (Centralized), (ii) a simple edge server architecture where three edge servers run a Web server and the database is run in a server across the WAN (SES), (iii) our proposed system with 3 edge servers (GlobeDB) Figure 6: Architectures of different systems evaluated and (iv) a full replication system (Full) which is similar to the GlobeDB setup - the only difference being that the tpcw item and tpcw customer tables are fully replicated at all edge servers unlike GlobeDB.
As we noted earlier, replication decisions are made through evaluation of the cost function and its weights  ,   and   as described in Section 5.
In our experiments, we assumed the system administrator wants to optimize the system for improved response time and assigned higher weights to   and   compared to  .
We set  =2/rmax;  =2/wmax; and  =1/bmax, where rmax, wmax and bmax are maximum values of average read latency, write latency, and number of consistency updates, respectively.
These values effectively tell the system to consider client read and write latency to be twice as important in comparison to update bandwidth (on a normalized scale).
These weights result in the following placement con guration: For the 3 clusters of the customer table, 3 different placement con gurations are obtained where a different edge server is the master while hosting a replica of the cluster.
Similar placement con gurations are obtained for 3 out of the 4 clusters of the book table.
The fourth cluster (which contains data units that were accessed from all edge servers) was automatically placed at all the 3 edge servers as it represents book records that are popular among all clients.
In our experiments, we study the WIRT for different WIPS until the database server cannot handle more connections.
The results of our experiments are given in Figure 7.
As seen in the  gure, even for low throughputs GlobeDB performs better than the traditional Centralized and SES architectures and reduces response time by
  !!!
""""""""######$$$$$$$$$$$$%%%%%%%%%%%%&&&&&&''''''(((()))*************+++++++++++++,,,,,,,,,,,-----------...........///////////00000000000111111111112222222222233333333333444444444445555555555566666666666777777777778888888888899999999999:::::::::::tration.
We believe this is a promising start for realizing a truly scalable self-managing platform for large-scale Web applications.
Figure 7: Performance of different system architectures running TPC-W benchmark a factor of 4.
GlobeDB performs better than SES and centralized system as it is able to save on wide-area network latency for each data access as it is able to  nd many data records locally.
Moreover, this also shows that replicating application code is not suf cient to obtain good performance.
The difference in WIRT between the GlobeDB and Full setup varies from 100 to 400ms.
This is because the GlobeDB system is capable of performing local updates (the server that writes most to a database cluster is elected as its master) but the Full setup forwards all updates to the origin server.
These updates constitute 30% of the workload.
On the other hand, the Full setup gains in the fact it can handle some complex queries such as search result interactions locally, while GlobeDB forwards it to the origin server.
It is obvious that the fully replicated system produces more update traf c than GlobeDB as it propagates each update to all edge servers.
In this experiment, we found that GlobeDB reduces the update traf c by a factor of 6 compared to Full replication.
This is because GlobeDB prevents unnecessary update traf c by placing data only where they are accessed.
Reducing the update traf c potentially leads to signi cant cost reduction as CDNs are usually charged by the data centers for the amount of bandwidth consumed.
The centralized and SES systems pay the penalty for making connections over a WAN link.
Note that between these systems the Centralized setup yields lower response time as each client request travels only once over the WAN link.
In the SES setup, each Web request triggers multiple data accesses that need to travel across WAN links thereby suffering huge delays.
However, among these traditional systems, the SES architecture yields better throughput as it uses more hardware resources, i.e., the Web server and the database system are running in separate machines.
SES yields a throughput of 7.9 req/s, while the centralized architecture yields much less (1.9 req/s) as it runs both Web server and database on the same machine.
Replication also affects throughput.
GlobeDB attains a throughput of 16.9 req/sec and is 2 WIPS better than the Full setup and
 data handling workload is shared among many database servers.
While the Full setup has the same number of database servers as GlobeDB, the latter sustains higher throughput as a server does not receive updates to data that it accesses rarely.
This reduces the workload of individual database servers in addition to reducing overall consumed bandwidth.
In conclusion, the results show that the GlobeDB s automatic data placement can reduce client access latencies for typical e-commerce applications with a large mixture of read and write operations.
With these experiments we have shown that automatic placement of application data is possible with very little adminis-Figure 8: Relative performance of autonomic system architectures In our earlier experiments, we showed that autonomic placement of data can yield better performance than traditional strategies.
As we noted in Section 5, the underlying decision making tool used for autonomic placement is the cost function and its weights  ,  , and  .
The objective of the results presented in this section is not to show what are the right weights for a system.
Rather, this experiment is just a guide to show the utility of our cost function and the simplicity with which it can attain different objectives (latency optimization, bandwidth optimization or a mixture of it).
We evaluated the relative performance of autonomic systems that uses three different weight parameters, in effect three different criteria for placement.
The three systems evaluated are: (i) GlobeDB (0, 0, 1): a system with weights ( ,  ,  ) =(0,0,1), which implies the system wants to preserve only the bandwidth and does not care about latency, (ii) GlobeDB (1, 1, 0): a system whose weights are set such that the system cares only about the client latency and does not have any constraints on the amount of update bandwidth.
Effectively, this situation leads to creating more replicas.
(iii) GlobeDB ( 2, 2, 1): a system that prefers to optimize latency twice as much as the update bandwidth.
As can be seen, these systems are designed for different conditions.
For example, GlobeDB(0, 0, 1) is useful for a system that cannot afford to pay for the wide-area bandwidth consumed in its remote edge servers and GlobeDB(1, 1, 0) is useful for a CDN that values its client quality of service (QoS) more than its maintenance costs consumed for maintaining consistency among replicas.
The goal of this experiment is to analyze the impact of different cost function parameters on the client response time of the TPC-W benchmark and the results are given in Figure 8.
As seen in the  gure, GlobeDB(0, 0, 1) performs the worst in terms of WIRT, as it leads to a placement where the data are not replicated at all.
Furthermore, its throughput is saturated because all transactions are sent to a single database server.
The other two systems perform equally well and yield good throughput.
With respect to update traf c, GlobeDB(2, 2, 1) performs better than GlobeDB(1, 1,
 GlobeDB(1, 1, 0) and a fully replicated system have similar goals, the former yields better WIRT as it is able to perform local updates.
A number of systems have been developed to handle Web application replication [4, 7, 22].
These systems replicate code at the
 cache them at the replica servers.
Furthermore, all these systems also rely on manual administration and the administrator must decide which data must be cached and placed at which server.
This is the most important shortcoming of existing systems.
Commercial database caching systems such as DBCache [6] and MTCache [17] cache the results of selected queries and keep them consistent with the underlying database.
Such approaches offer performance gains provided the data accesses contain few unique read and/or write requests.
However, the success of these schemes depends on the ability of the database administrator to identify the right set of queries to cache.
This requires a careful manual analysis of the data access patterns to be done periodically.
In [15], the authors propose an application-speci c edge service architecture, where the application itself is supposed to take care of its own replication.
In such a system, access to the shared data is abstracted by object interfaces.
This system aims to achieve scalabil-ity by using weaker consistency models tailored to the application.
However, this requires the application developer to be aware of an application s consistency and distribution semantics and to handle replication accordingly.
This is in con ict with our primary design constraint of keeping the process of application development simple.
Moreover, we demonstrated that such an awareness need not be required, as distribution of data can be handled automatically.
Recently, database researchers have built middleware systems such as C-JDBC [8] and Ganymed [21] for scalable replication of a database in a cluster of servers.
However, the focus of these works is to improve the throughput of the underlying backend database within a cluster environment, while the focus of our work is to improve the client-perceived performance and reducing wide-area update traf c.
We also note that these works can be combined with GlobeDB to scale the database in a single edge server.
In this paper, we presented GlobeDB, a system for hosting Web applications that performs ef cient autonomic replication of application data.
We presented the design and implementation of our system and its performance.
The goal of GlobeDB is to provide data-intensive Web applications the same advantages CDNs offered to static Web pages: low latency and reduced update traf c.
We demonstrated this with experimental evaluations of our prototype implementation running the TPC-W benchmark over an emulated wide-area network.
In our evaluations, we found that GlobeDB signi cantly improves access latencies and reduces update traf c by a factor of 6 compared to a fully replicated system.
The major contribution of our work is to show that the process of application development can be largely automated and in such a way that it yields substantial improvement in performance.
We note that our current prototype assumes that the system is free of server and network failures.
However, such failures may create a consistency problem if the master for a data unit is unreachable.
We plan to address these issues in the near future.
The system presented in this paper is the  rst step in achieving the goal of complete autonomic system for replication.
There are several open issues that need to be addressed to realize an autonomic CDN we envisage.
These issues include effective re-clustering, continuous adaptation of placement con gurations and fault tolerance.
We plan to address them in our next steps.
We thank Christian Plattner (ETH) and Arno Bakker (VU) for their helpful discussions and helping us with the setup of the emulator test bed.
We also thank the anonymous reviewers for their comments that helped in improving the clarity of the paper.
