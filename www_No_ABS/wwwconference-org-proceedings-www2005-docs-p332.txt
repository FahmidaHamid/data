The current situation of the Semantic Web is one of a vicious cycle in which there is not much of a semantic web due to the lack of annotated web pages, and there is such a lack because annotating web pages currently does not provide much bene t.
Thus, the application of (semi) automatic techniques in order to support web page annotation is a key factor for the vision of the Semantic Web to become true.
Several supervised machine learning based techniques have been proposed to automate the information extraction as well as annotation process of documents [4, 9, 20, 31].
However, these techniques rely on assumptions that are not compatible with the vision of the Semantic Web.
First, machine learning approaches inducing extraction rules for each concept from training data such as in [4, 9,
 Web ontologies consist of.
Second, in order to annotate with respect to a few hundred concepts, a training set in the magnitude of thousands of examples needs to be provided1, an effort that probably not many people are willing to make.
Third, machine-learning based approaches rely on the assumption that documents have a similar structure as well as content, an assumption which seems quite unrealistic considering the heterogeneity of the current web.
Thus, several researchers have started to look at unsupervised approaches such as [15] as well as approaches performing a  rst unsupervised step and then using the results of this  rst step to induce new extraction rules in a bootstrapping manner [3, 10, 16].
Further, as a way out of the above mentioned vicious cycle, in [7] we presented our vision of a  Self-Annotating Web  in which globally available syntactic resources are considered to support meta-data creation.
The main idea herein is to approximate semantics by considering information about the statistical distribution of certain syntactic structures over the Web.
Our concrete instantiation of this paradigm is called PANKOW (Pattern-based ANnotation through Knowledge On the Web).
The core of PANKOW was a pattern generation mechanism which creates pattern strings out of a certain pattern schema conveying a speci c semantic relation, an instance to be annotated and all the concepts from a given ontology.
It counts the occurrences of these pattern strings on the Web using the Google API.
The ontological instance in question is then annotated semantically according to a principle of maximal evidence, i.e. with the concept having the largest number of hits.
Our approach is thus unsupervised as it relies on no hand labeled train-
least ten examples for each concept to be extracted are necessary.
  ing examples and does not assume that documents have a similar structure, thus avoiding two main problems with which supervised techniques are confronted.
Let s for example assume that the string  Niger  appears in a web page and we have no idea about how to annotate it.
Figure 1 shows the Google hits for the following four expressions: Niger is a country, Niger is a state, Niger is a river and Niger is a region.
Intuitively, given these  gures we would naturally tend to annotate Niger as a country as it seems to be its main meaning on the Web.
This illustrates the fact that formal (semantic) annotations can be approximated to a certain extent by considering the statistical distribution of certain syntactic structures over the web.
However, as Niger can be a country or a river depending on the context in which it appears, the example also shows that ambiguity is an important problem we need to deal with in such an approach.
This paper presents C-PANKOW (Context-driven and Pattern-based Annotation through Knowledge on the Web), which tackles the ambiguity problem by taking into account the context the entity to be annotated appears in.
The predecessor of C-PANKOW in fact suffered from a few shortcomings.
First, due to the restrictions of the pattern generation process, a lot of actual instances of the pattern schemes were not found.
In particular the approach exhibited problems generating the correct plural forms of concept labels as well as matching more complex linguistic structures such as noun phrases including determiners, noun modi ers etc.
We overcome this problem by actually downloading the pages, analyzing them linguistically, and matching the patterns instead of merely generating them and counting their Google hits.
The results of the pattern-matching are also linguistically normalized, i.e. words are mapped to their base forms thus completely solving the problem with the generation of plural forms.
At the same time we overcome the second problem in this way, i.e. the large number of queries sent to the Google Web API.
In fact, by downloading the pages and processing them locally we reduce network traf c.
PANKOW issued a number of Google queries proportional to the size of the ontology considered.
Thus PANKOW was dif cult to scale to large ontologies.
In C-PANKOW, we generate only a constant number of queries per instance that we annotate.
Thus, C-PANKOW is able to annotate using very large on-tologies.
Though PANKOW was already able to take into account more concepts than standard named entity recognition systems, C-PANKOW thus de nitely overcomes the scalability problem with which supervised techniques are faced.
Third and most important, we contextualize the pattern matching by distinguishing between relevant and non-relevant pages.
A pattern matched in a relevant web page counts more than one matched in a less relevant one.
Hereby, relevance assessment boils down to calculating the similarity of the involved pages.
We present an evaluation of our system analyzing the impact of our notion of contextual relevance as well as varying the number of pages downloaded.
Thereby, our experiments show that C-PANKOW outperforms its competitors.
The remainder of this paper is structured as follows: Section 2 describes the process of C-PANKOW.
Section 3 presents an evaluation of the approach.
Section 4 describes the implementation of the system as a freely accessible web service.
Before concluding, we discuss some related work in section 5.
The process of C-PANKOW is schematically described by the pseudocode in Figure 2 and is summarized in the following: Figure 1: Statistical distribution of  is a patterns for Niger
 stances.
This process is described in detail in Section 2.1.
discovered and for each clue/pattern-(described in Section 2.4), we is-pair in our pattern library sue an automatically generated exact query to GoogleTM and download the abstracts of the  rst hits (cf.
Section 2.2).
notated and each of the downloaded abstracts.
If the similarity is above a given threshold , the actual pattern found in the abstract reveals a phrase, which may possibly describe the concept that the instance may belong to.
Section 2.3 describes how the similarity is calculated.
are updated according to the similarity previously calculated, i.e. a pattern matched in an abstract that is very similar to the page to be annotated counts more than a pattern that is matched in a less similar abstract.
having the largest number as well as most contextually relevant hits.
is annotated with that concept In what follows, we describe in detail every important step of the algorithm.
The recognizeInstances() procedure is described in Section 2.1.
Section 2.2 describes the process of downloading Google-abstracts, whereas Section 2.3 describes how the similarity is computed.
Finally, Section 2.4 describes our pattern library and Section 2.5 discusses some complexity issues.
The whole process is illustrated with a running example.
In fact, we describe the result of each process step on the web page depicted in Figure 3.
In order to detect candidate instances in a web page we  rst eliminate the complete HTML markup and extract the text body of the page.
This step is necessary because we apply a part-of-speech tagger to assign word categories to every token of the extracted text and the tagger is not able to handle HTML markup.2 Then we split the text into sentences and interpret as an instance every string which matches the following pattern: w+ DT )?
([a-z]+ JJ )?
PRE (MID POST)?
PRE := POST := (([A-Z][a-z]*) NNS MID := the DT 
 in part-of-speech http://web.bham.ac.uk/O.Mason/software/tagger/.
QTag s part-of-speech tagset can be found at http://www.ling.ohio-state.edu/ ntyson/tagset/english-tagset.txt.
of IN
 tagger QTag



 use the                  C-PANKOW(document d) /* recognize all the instances in input document */ I = recognizeInstances(d); foreach    foreach (p,c)
  rst Google abstracts /* download the matching the exact query Abstracts = downloadGoogleAbstracts(c(i),n); foreach a in Abstracts  */   /* calculate the similarity between the document d and the Google abstract a */ sim = calculateSimilarity(a,d); if (sim t) if (p.matches(a))  c = p.getConcept(); Res[c] = Res[c]+sim; annotate(i,maxarg Res[c]); Figure 2: C-PANKOW s process in pseudocode (de la los las del) FW [a-z]+ NP


 These expressions are intended to be interpreted as standard regular expressions over words and their corresponding part-of-speech tags, which are indicated in curly brackets.
Paraphrasing, INSTANCE matches each optional sequence of arbitrary characters ( w+) tagged as a determiner (DT), followed optionally by a sequence of small letters ([a-z]+) tagged as an adjective (JJ), followed by an expression matching the regular expression denoted by PRE, which in turn can be optionally followed by an expression matching the concatenation of MID and POST.
Thereby, PRE and POST match a sequence of tokens in which the  rst character is capitalized and tagged either as a plural proper noun (NPS), a plural common noun (NNS), a common noun (NN), a proper noun (NP), an adjective (JJ) or an interjection UH3.
MID matches a sequence of determiners  the , prepositions  of , the possessive marker  , a hyphen  - , a foreign word FW such as  de,del,las,los,las  and lower case singular or plural proper and common nouns.
For example, the tagged sequence Pas NP de FW la  FW Casa NP would be recognized as an instance, whereby Pas would match the PRE, de la the MIDDLE and Casa the POST part of the above regular expression.
The instances discovered this way in our running example web page are given in Table 1 as a cross  X  in the S(system) column.
are tagged as  UH  by the part-of-speech tagger.
Figure 3: andorra/activities.htm http://www.lonelyplanet.com/destinations/europe/
   a function  and a clue   , the query where The patterns in our pattern library are actually tuples   is a regular expression de ned over part-of-speech tags as de-called the is sent       .
For example, given the clue scribed above, and clue.
Given an instance to the GoogleTM API and we download the abstracts of the  rst documents matching this query and then process the abstracts to  nd instances of pattern  "!
ab-%$ stracts matching the query f(Seville), i.e.  such as Seville .4 With the use of such clues, we thus download a number of pages in which a corresponding pattern will probably be matched thus restricting the linguistic analysis to a few promising pages.
and the instance Seville we would download '&() #+* As described in our pseudocode algorithm in Figure 2, we then calculate the similarity between each downloaded abstract and the web page in question.
For this purpose, we  rst remove stopwords from both documents and then adopt the bag-of-words model [30] to create vectors representing the count for each word in the document.
Then we use the cosine measure to calculate the similarity between the abstract and the document to be annotated.
Thus, we measure the similarity between the abstract and the document as the cosine of the angle between their vectors, i.e.
,'.
0/12 (45+!
We only consider those pages as relevant for which this similarity is over the threshold .
Further, we weight the contribution of the pattern matched in that page with this value thus  contextualizing  the pattern-matching process with the result that a pattern matched in a very similar page counts more than a pattern matched in a less
 strings.
denotes the concatenation operator de ned on two                               # 



 : ;

 ;
 ;
 : ;  * similar one.
Thus, a certain instance can be annotated with a different concept in different contexts, i.e. web pages.
In general, the intuition behind this is to yield more accurate annotations and to choose the contextually most appropriate sense or concept for a given instance in case it is ambiguous.
Additionally, by this we only linguistically analyze Google abstracts which seem relevant.
After a few initial experiments we decided to use  0.05  as threshold value.
In section 3 we also present further experiments with different threshold values.
In what follows we present the pattern library we use and brie y describe the intuition behind each pattern:
 These patterns have been applied by Marti Hearst ([23]) to discover sub-/superconcept relations.
As we have argued several times in [7] and [8] these patterns can however also be used to discover instance/concept relations.
The relations reused from Hearst are the following: ((and

 or) CC (INSTANCE ,?
)+ ((and HEARST1:= CONCEPT such DT as IN HEARST2:= CONCEPT ,?
especially RB or) CC HEARST3:= CONCEPT ,?
including RB or) CC HEARST4:= INSTANCE ,?
)+ and CC other JJ CONCEPT HEARST5:= INSTANCE ,?
)+ or CC other JJ CONCEPT (INSTANCE ,?
)+ ((and

 %$ clues are: clue clue clue clue clue where CONCEPT := [a-z]+ NN(S)? + and the corresponding                 An example for an expression matched by the HEARST1 patterns is hotels such as the Ritz, the Hilton and the Holiday Inn, one for HEARST3 is sights, including the Eiffel Tower, the Statue of Liberty or the St. Peter s Chapel, and one for HEARST4 is New York, Tokyo, Rio de Janeiro and other big cities.
+!
%& +!
95 +!
+!
* +!
(4  
 5& & 
 The next pattern involves a de nite, i.e. a noun phrase introduced by the de nite determiner  the .
Frequently, de nites actually refer to some entity previously mentioned in the text.
In this sense, a phrase like  the hotel  does not stand for itself, but it points as a so-called anaphora to a unique hotel occurring in the preceding text.
Nevertheless, it has also been shown that in common texts more than 50% of all de nite expressions are non-referring ([29]), i.e. they exhibit suf cient descriptive content to enable the reader to uniquely determine the entity referred to from the global context.
For example, the de nite description  the Hilton hotel  has suf cient descriptive power to uniquely pick-out the corresponding real-world entity for most readers.
One may deduce that  Hilton  is the name of the real-world entity of type Hotel to which the above expression refers.
whereby the corresponding clue is clue .
w+ JJ )?
INSTANCE CONCEPT, 5& 	  DEFINITE: the DT (
 The probably most explicit way of expressing that a certain entity is an instance of a certain concept is by the verb  to be  in a copula5 construction as for example in  The Excelsior is a nice hotel in the center of Nancy .
Here s the general pattern:
 w+ BE(D?
)(Z
 where is, are, was and were are tagged by the part-of-speech tagger as BEZ, BER, BEDZ and BEDR, respectively.
The corresponding clue is clue .
 "!$# %  !
 
 and The runtime complexity of C-PANKOW is (' is the total number of instances to be annotated, , where   (' the number of patterns we use, and the maximum number of pages down-are constant and a document of size con-loaded.
As tains at most instances, the overall complexity of C-PANKOW is thus linear in the size of the document, i.e.
.
As the GoogleTM API does not allow to retrieve more than 10 documents per query, the number of queries sent to the GoogleTM API is *' , so that  ,+.-0/ queries per instance independently of how big we issue the ontology in use is.
This is an important reduction of the number of queries compared to PANKOW in which we had to issue indicate the number of concepts the ontology had.
For a small set of concepts with this meant 590 queries per instance to be annotated.
As C-PANKOW is independent of the size of the ontology, we can thus consider even larger ontologies than PANKOW, which already provided annotation based on much larger ontologies than most other approaches.
per instance.
In our special settings,  (+.-3/ queries per instance, where 4 1!7698 as well as !
:-0/ !21

 In order to evaluate our system, we have reused the dataset de-In order to create this dataset, we asked two hu-scribed in [7].
man subjects to annotate 30 texts with destination descriptions from http://www.lonelyplanet.com/destinations.
They used a pruned version of the tourism ontology developed within the GETESS project ([32]).
The original ontology consisted of 1043 concepts, while the pruned one consisted of 682.
The latter ones have been considered also in our evaluation.
The subjects were told to annotate instances in the text with the appropriate concept from the ontology.
In what follows, we will refer to these subjects as A and B.
Subject A actually produced 436 annotations and subject B produced 392.
There were 277 instances that were annotated by both subjects.
For these 277 instances, they used 59 different concepts and the categorial agreement on these 277 instances as measured by the Kappa statistic was 63.50% (cf.
[5]), which allows to conclude that the annotation task is overall well de ned but that the agreement between humans is far from perfect.
In what follows,
 an adjective or a constituent denoting a property of the subject.
      *     ( *       $
   * * (         !
*    * &                  )   )  &   )      
    
 
   
  
  
    we present results for the detection of instances as well as for the actual automatic classi cation of these.
In order to detect potential instances in a web page to be annotated, we apply the method described in Section 2.1.
We apply this method to the dataset described above and compare the instances discovered by our system with the instances annotated by the annotators in terms of Precision, Recall and F-Measure.
More formally, let B in the document 3 by the system in the same document 3 F-Measure are de ned as follows:

 7 be the set of instances detected , then Precision, Recall and 7 and , and let  
 For these and all the other measures considered in this section, we average over all the documents and over both annotators, i.e.:  
 Instance Andorra Andorra Activities Andorra Atlantic Canillo
 Lonely Planet World Major Andorran Mediterranean Ordino Pas de la Casa Pyrenees Roig Soldeu Soldeu-El Tarter Traveler
















 Table 1: Results of the instance detection algorithm Precision, Recall and F-Measure are de ned as follows:

 !1


 #" ,  , a recall of  ,
 -

 ! ,  .
Thus, In our running example web page, the system for example detected 11 instances, while subject A found 9 and subject B 7 (compare Table 1).
The system coincided with subjects A and B in 6 and
 running example: , 
 !
 and thus  !

 ,  and /*6 !$*8 


 .
For the whole dataset, the system achieved a preci-

sion of and a F-Measure !
'#" 
 of  .
In order to get an upper limit on the task, we also !
(&  "*8 compared the precision, recall and F-measure of subject A against subject B and vice versa and yielded  as a human baseline on the task.
While we are still quite far away from the human performance on the task, the results are as desired in the sense that we have a higher recall at the cost of a lower precision.
This is useful as some of the spurious instances will be  ltered out by the instance classi cation step due to the fact that if no patterns are found, the instance will not be assigned to any concept.
Thus, the precision can be increased by the instance classi cation step, while the recall needs to be reasonably high as it can not be increased later.
)+*, /# :.-- The annotations by the subjects A,B as well as by the system for a document 3 are modeled by the functions spectively.
The actual classi cation of the detected instances is also evaluated in terms of Precision, Recall and F-measure with respect .
For this purpose we introduce a set 7 and 7 , re-
 C of instance/concept pairs as follows: to both reference sets %  5 and /
 0    0  +!
 It is important to mention that our recall corresponds to the accuracy used in other approaches (compare [2] or [22]), such that we can compare with these using this value.
As above these measures are averaged over both annotators and over the 30 documents.
In our running example for instance, the system produced 6 annotations, while subject A and B produced the above mentioned 9 and 7 annotations, respectively.
The system agrees with A and B in 2 annotations, respectively (compare Table 2).
This leads to the follow, ing precision, recall and F-Measure values: and , , .
Thus, ,  ,   &  , 
 & 
 /   !2"9/ 
 .
Furthermore, to assess how good the actual classi cation is, we also consider the Accuracy of the system which abstracts from the actual instance recognition task.
For this, we present two further ) considers the instances annotated by the system as well as by the human thus not penalizing the system for not giving answers for instances it didn t discover.
, considers the 277 common instances annotated by both subjects in order to compare our results with our earlier system presented in [7].
Here follow the formal de nitions: measures of accuracy.
The  rst one ( The second one, !"" !1"" !3"" ""# ""# ""#
 0 0
 0 0

 0 0 0
 0 0
 0
 0  5     Thus we get for our running example: .
!7*/# and !8 ""# "" Finally, as instances can be tagged at different levels of detail and there is certainly not only one correct assignment of a concept, 54 0  !8 !6           
 !
      
     
     
 !
      
     
     
 !

    

    
        
        
 !
 
   
  !
 
  )     
     !
    
 !
        
 !
    
 !
 !
!
  
 !
   
 !
 
 !

  
 !
   !
   !
      
  
 !
    
  ( 
  
     
 !

  
  
  
  
    
    
 !

  
  
  
  
    
 !

   

   
   
   
   
 !
    
 !
 !
   
 !
    
 !
     
 !
 !
!
   
 
 
 !
 
 !

          
 !

  
  
    
    
      
 !
     
     
  
 !
  
  
    
   
      
     
 !
 
 !
     
     
  
 !
  !
we also consider how close the assignment of the system is with respect to the assignment of the annotator by using the Learning Accuracy originally introduced by Hahn et al. [22].
However, we consider a slightly different formulation of the Learning Accuracy in line with the measures de ned in [26].
Both measures are in fact equivalent, the only difference is that we measure the distance between nodes in terms of edges   in contrast to nodes in Hahn s version   and we do not need any case distinction taking into account if the classi cation was correct or not.
First of all we need the notion of the least common superconcept of two concepts and   which is de ned in line with [26]:     Now the taxonomic similarity 	  ned as: 4 (  (  $ 5&4( %& % is minimal % between two concepts is de- (       where The Learning Accuracy is now de ned as follows: .
	  (  (     %  7 , 	 
 , 	
 0  0 0  
 0  5 0 Here we also average over both annotators and over the documents in the collection.
In a  rst series of experiments, we examined the effect of varying the threshold, but without taking into account the similarity for weighting the patterns.
As the results in Table 3 show, the best results were in fact achieved when using a threshold of 0.05.
When increasing the threshold, the precision of the annotations certainly increases, but the recall is drastically reduced.
In general, all values except for the precision decrease when increasing the threshold such that we conclude that the best threshold lies somewhere between 0 and 0.1.
Thus a threshold of 0.05 seems reasonable.
Table 4 shows the results for the baseline experiment with no threshold, i.e. considering all the pages returned by Google up to a maximum of 100 (labeled with  no threshold  in the table).
It also shows the results of the experiment using a threshold of 0.05 as well as one in which the patterns were weighted according to the corresponding similarity.
The results already show that taking into account the similarity and considering only those pages with a similarity over the threshold indeed yields better results.
Further, the version weighting the patterns yields a higher precision at the cost of a slightly lower recall, but increases the Learning Accuracy by more than 3 points.
Thus, we conclude that using the version of our system weighting the patterns according to the similarity of the involved pages indeed yields better results.
Finally, we also varied the maximum number of abstracts downloaded.
The results are given in Table 5.
Interestingly, using a lower or higher number of maximum pages also decreased the results independently if the contribution of each pattern was weighted according to the similarity between the page to be annotated and the corresponding Google abstract or not.
In general, we conclude that using maximally the  rst 100 hits returned by Google for the clue patterns is enough.
In order to apply our approach to a larger set of web pages, we selected a set of 307 news stories from http://news.kmi.open.ac.uk/rostra/.
These 307 news stories were automatically annotated using our C-PANKOW web service using the  rst 100 pages returned by Google, a threshold of 0.05 and taking into account the similarity of the abstract in which the pattern was matched.
In this way 1270 annotations were produced, i.e. 4.1 annotations on average per document.
One of our annotators manually analyzed the annotations a posteriori and evaluated each annotation by assigning a value from 0 (incorrect) to 3 (totally) correct.
We are thus only able to give results for the precision of our system on this dataset.
We measure three types of Precision: an answer as correct if it was assigned at least 3, 2 and 1 points, respectively.
On average, the annotations produced by the system received 1.81 points by the annotator.
The precisions were: .
In order to compare these results with the results on the Lonely Planet dataset, we also performed this a posteriori evaluation on that dataset yielding 2.1 points per annotation on average and the following precisions: .
These results corroborate the fact that the annotations produced by C-PANKOW are indeed very accurate.
  considering  and
 & 


 /&#
  &&# -.% and - - and  , , ,
 As a  nal experiment, instead of annotating with respect to a domain-speci c ontology, we used a general purpose ontology, i.e.
WordNet [18].
An additional complication here is that words can have different meanings or senses   as they are called in WordNet terminology   and thus the correct meaning needs to be chosen on the basis of contextual evidence.
For this purpose we implemented a simple word sense disambiguation algorithm in line with the one presented in [25].
In fact, in our approach we choose that sense whose gloss maximizes the overlap   in the number of words   with the web page in question.
In order to evaluate the annotations with respect to WordNet, we conducted again a posteriori evaluation on the 307 news stories as mentioned above.
The human subject was asked to evaluate the appropriateness of the annotation on the basis of the synset s gloss, i.e. a natural language description of its intention.
For this task, the results were actually poorer with precisions of .
Further we identi ed two main reasons why the results are lower with respect to using domain-speci c ontology.
First, in some cases the term with maximal evidence, i.e. with the largest number of Google hits, is not speci c to the domain in question, but as WordNet is so large, a corresponding concept or synset is almost always found and the named entity in question thus annotated with it.
Second, in other cases the term with maximal evidence is contextually appropriate, but our simple word sense disambiguation approach fails in selecting the correct sense.
We thus conclude that in order to accurately annotate with respect to WordNet we would need a mechanism to consider only relevant terms for the domain in question as well as to improve our word sense disambiguation algorithm.
"  ""  
 #" and
 -+ ,
 We have presented results showing that the use of the similarity as an indicator of the relevance of a certain Google abstract for the page in question indeed improves the quality of the annotations produced by the system.
Further, we have also examined two parameters: the similarity threshold and the maximum number of result pages considered.
The results show that a similarity threshold of 0.05 seems reasonable.
Furthermore, an interesting result is that increasing the number of pages considered does not improve  (   !
     ,  ,  !
           !
  
 !
   ,  
   
    
    
         !
  !
   !
  !
  !
   !
  !

   !
   !
Instance Andorra Atlantic Canillo
 Mediterranean Ordino Pas de la Casa Pyrenees Roig Soldeu Traveler
 country sea town walking trail sea region town mountain town
 country sea area sea area
 country  sh hotel valley mountain mountain family person Table 2: Annotations by subjects A, B and the system (S) Threshold no threshold





 =Acc Acc  Acc 





















 Table 3: Results of varying the threshold (no weighting,n=100) Threshold no threshold t=0.05 t=0.05 + sim 22.27% 29.29% 18.92%



 =Acc Acc  Acc 
 Table 4: Impact of using the similarity measure (n=100) No.
pages




 No.
pages




 No Weighting =Acc Acc  Acc 





 Weighting =Acc Acc  Acc 





 Table 5: Results of varying the number of pages (t=0.05)             the quality of the annotations.
This is a very important result from a practical point of view as it shows that we can get good results while maintaining ef ciency at the same time.
In order to assess the performance of our system we compare it to state-of-the-art systems performing the same task, i.e. assigning instances appearing in texts to their corresponding concept in a given ontology.
The approaches we directly compare with are the ones in [2] and [22] as they consider a similar scenario.
However, we also situate our approach in the context of other named entity recognition and classi cation approaches.
In particular, we discuss other systems with a special emphasis on the number of concepts considered, which renders a classi cation easier or more dif cult.
Table 6 shows the results of different systems ordered according to number of concepts considered for the assignment.
The systems are described in more detail in Section 5.
C-PANKOW has been implemented in Java and is accessible as a Web Service using Axis6, an open implementation of the Simple Object Access Protocol (SOAP)7, as well as a Servlet on top of Tomcat8 with a web frontend.
Thus, C-PANKOW can be either accessed in a programmatic way or alternatively through the web frontend at http://km.aifb.uni-karlsruhe.de/pankow/annotation/.
In C-PANKOW, KAON [14], an open source ontology management infrastructure developed at our institute is used to represent on-tologies and perform ontology validation.
The Google index is searched using the Google API, which is available from Google.
Our implementation uses the free-of-charge Google API, which only yields ten results per search.
For ef ciency reasons, this queries to Google are multi-threaded with up to ten simultaneous searches.
Furthremore, as the default timeout of the Google API is relatively high, time limits are imposed on each thread after which the thread will be restarted.
Because annotating one page can take up to 20 minutes, the server has a dedicated worker thread that does the actual annotation using the methods put forth in this paper.
Both the Servlet and the Axis interface enter the user s request into a queue which is then emptied by the worker thread.
The results are then dispatched to the user via email, although other methods of noti cation are possible.
Currently, C-PANKOW is integrated with the semantic web browser MagPie [13] with the purpose of providing additional named entities to be highlighted.
As many others, our work is based on the seminal work of Hearst on applying handcrafted patterns denoting a certain relation to  nd instances of these relations in a text corpus.
Hearst applied such relations to discover is-a relations in text.
Hearst s idea has been reapplied by different researchers with either slight variations in the patterns used [24], in very speci c domains [1], to acquire knowledge for anaphora resolution [28], or to discover other kinds of semantic relations such as part-of relations [6] or causation relations [21].
Instead of matching these patterns in a large text collection, some researchers have recently turned to the Web to match these patterns such as in [7], [15], [27].
Especially interesting in our context is the work in [15] which aim is to acquire instances for a given concept.
In particular, Etzioni et al. present results on the task of acquiring instances of cities, countries, US states,  lms and actors.
They make use of a Bayesian classi er in order to decide weather 6http://ws.apache.org/axis
 8http://jakarta.apache.org/tomcat/ an instance belongs to a certain concept or not.
Recently, they have also considered learning new patterns by a rule learning process [16].
Though their work is quite similar to ours, the aims of both approaches are quite orthogonal, i.e. while we are concerned with annotating the instances in a given document with the corresponding concept, Etzioni et al. aim at learning the complete extension of a certain concept in order to build a search engine  knowing it all .
Thus a crucial aspect of the approach presented in this paper is to account for the context in which a certain instance appears in order to annotate it with the contextually most appropriate concept.
This is an aspect totally neglected by the approach of Etzioni et al., such that it is doubtful if their approach could be used for an annotation task as we consider here.
Furthermore, as their aim is to learn the extension of certain concepts such as actors, cities etc.
and thus their task quite different to ours, we do not give any quantitative comparison.
Another interesting system is SemTag [12] which also automatically annotates web pages.
Though its results are certainly impressive, it is important to note that SemTag actually only performs the task of disambiguating entities appearing in a web page as it relies on the TAP lexicon to list all the possible concepts, senses or meanings of a given entity.
Our approach does not rely on such a handcrafted lexicon and in fact automatically discovers all the possible concepts for a given entity from the Web and then suggests the contextually most appropriate concept for it thus performing two tasks in one: induction of possible concepts and context-based disambiguation.
Thus, as the tasks are not comparable, we also do not provide any quantitative comparison of results.
Brin [3] presents a bootstrapping approach in which the system starts with a few patterns, and then tries to induce new patterns using the results of the application of the seed patterns as training dataset.
This is also the general idea underlying the Armadillo system [10], which exploits redundancy in the World Wide Web to induce such extraction rules.
The work of [11] is also concerned with inducing certain patterns to extract information, but does this by learning  soft patterns  and relying on a similarity criterion when matching these.
This is quite different from the  hard patterns  that are considered in the approaches such as the one presented in this paper as well as [9] or [15].
Concerning the task of learning the correct class or ontological concept for an unknown entity, there is some related work, especially in the computational linguistics community.
In the context of the Message Understanding Conferences (MUC), systems typically achieved accuracies of well above 90% in the task of tagging named entities with respect to three classes: organization, person and location.
However, this task is quite moderate compared to the task of using 682 concepts as considered in our approach.
Other systems have also considered more categories such as Hovy et al.
[19] which considered 7 or Evans [17] which considered from 2-
et al. [22] and Alfonseca et al. [2] who consider 325 and 1200 concepts, respectively.
Hahn and Schnattinger [22] create a hypothesis space when encountering an unknown word in a text for each concept that the word could belong to.
These initial hypothesis spaces are then iter-atively re ned on the basis of evidence extracted from the linguistic context the unknown word appears in.
In their approach, evidence is formalized in the form of quality labels attached to each hypothesis space.
At the end the hypothesis space with maximal evidence with regard to the quali cation calculus used is chosen as the correct ontological concept for the word in question.
The results of the different version of Hahn et al. s system (compare [22]) in terms of accuracy can be found in Table 6.
Their approach is very related System
 Evans Fleischman et al.
Hahn et al. (Baseline) Hahn et al. (TH) Hahn et al. (CB)
 Alfonseca et al. (Object) No.
Concepts








 Preprocessing various typology derivation (clustering) N-gram frequency extraction none perfect syntactic and semantic analysis required perfect syntactic and semantic analysis perfect syntactic and semantic analysis POS tagging & pattern-matching syntactic analysis Accuracy/Recall Learning Accuracy








 n.a n.a.
n.a.
Table 6: Comparison of results to ours and in fact they use similar patterns to identify instances from the text.
However, the approaches cannot be directly compared.
The reason is that they evaluate their approach under clean room conditions as they assume accurately identi ed syntactic and semantic relationships and an elaborate ontology structure, while our evaluation is based on very noisy real-world input   rendering our task harder than theirs.
Furthermore, while our evaluation was conducted with respect to a reference standard, it is not clear how they evaluated their system.
Alfonseca and Manandhar [2] have also addressed the problem of assigning the correct ontological class to unknown words.
Their system is based on the distributional hypothesis, i.e. that words are similar to the extent to which they share linguistic contexts.
In this line, they adopt a vector-space model and exploit certain syntactic dependencies as features of the vector representing a certain word.
The unknown word is then assigned to the category corresponding to the most similar vector.
The best result measured against a reference standard (strict evaluation mode as they call it) was achieved using only verb/object dependencies as features (compare Table 6).
Fleischmann and Hovy [19] address the classi cation of named entities into  ne-grained categories.
In particular, they categorize named entities denoting persons into the following 8 categories: athlete, politician/government, clergy, businessperson, entertainer/ artist, lawyer, doctor/scientist, police.
Given this categorization task, they present an experiment in which they examine 5 different Machine Learning algorithms: C4.5, a feed-forward neural network, k-nearest Neighbors, a Support Vector Machine and a Naive Bayes classi er.
As features for the classi ers they make use of the frequencies of certain N-grams preceding and following the instance in question as well as topic signature features which are complemented with synonymy and hyperonymy information from WordNet.
They report a best result of an accuracy of 70.4% when using the C4.5 decision tree classifer.
Fleischman and Hovy s results are certainly very high in comparison to ours   and also to the ones of Hahn et al. [22] and Alfonseca et al. [2]   but on the other hand though they address a harder task than the MUC Named Entity Task, they are still quite far away from the number of categories we consider here.
Evans [17] derives similar statistical  ngerprints as considered in our approach by querying GoogleTM and then clusters named entities on the basis of these  ngerprints as features in order to derive a class topology from the document in question.
He uses a bottom-up hierarchical clustering algorithm for this purpose.
His approach differs from the others discussed here in that it is totally unsupervised without even the set of categories being given.
Thus, the entities are classi ed with respect to different sets of categories depending on the document considered.
Overall, he reports 41.41% of correctly classi ed entities, considering from 2 to 8 classes.
We have presented an enhancement of our original PANKOW approach called C-PANKOW.
With C-PANKOW we have overcome several shortcomings of the earlier system.
In fact, C-PANKOW has outperformed PANKOW as well as some of its closest competitors.
First, by downloading a certain number of pages, linguistically analyzing and normalizing them, we overcome problems of our earlier pattern generation method and increase the recall of the pattern matching process by being able to consider structures with a more complex linguistic structure such as noun phrases containing determiners, adjectives or other noun modi ers.
At the same time we reduce the number of queries sent to the Google API which was originally proportional to the size of the ontology.
Instead of generating patterns for all the concepts in the ontology, we match the patterns in the downloaded Google abstracts and map the results to the ontology in question such that the approach is independent of the ontology s size, thus being scalable to arbitrarily large ontolo-gies.
In fact, the complexity of our approach is now linear in the number of instances and hence in the size of the document.
The number of queries sent to the Google API is now constant for each instance to be annotated.
Finally and most important, by considering the similarity between the page to be annotated and the Google abstract in which the pattern was matched we have contextualized our approach to provide provably more accurate annotations in the context of the document to be annotated and to choose the contextually most appropriate concept for a possibly ambiguous instance.
Our results have shown that considering such a similarity indeed improves the quality of the results of our system, thus being one step closer towards the  Self-Annotating Web .
Our work has been so far restricted to learning annotations for instances.
Further work will extend this to also learning to annotate conceptual relations between discovered instances.
A very interesting direction for further research would be to learn new patterns indicating a certain relation by a certain rule induction process such as in [3, 9, 11, 16].
The named entity recognition in Web sites could also be improved.
Acknowledgments.
We would like to thank Google for their support with their Web service API.
Thanks also to all our colleagues for their support, in particular to Siegfried Handschuh and Kai K uhn for integrating PANKOW with our in-house annotation tool OntoMat as well as to Martin Dzbor for helping to integrate C-PANKOW with MagPie.
Further, we would also like to acknowledge Ursula Sciesinski and Laura Goebes for annotating the Lonely Planet Web pages and the latter for performing the a posteriori evaluation with respect to the KMI stories and Lonely Planet datasets.
Thank also to Victoria Uren from KMI for providing this dataset.
Research for this paper has been funded by the IST-Dot.Kom project (grant IST-2001-34038).
The Dot.Kom project (http://www.dot-kom.org) is sponsored by the EC as part of the framework V.
