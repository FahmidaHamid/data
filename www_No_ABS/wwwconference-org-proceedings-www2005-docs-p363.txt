With the recent developments in communication media and a growing awareness of foreign cultures, people have become more exposed to foreign languages than at any other time in recent history.
More people are learning foreign languages, and language tools are increasingly important.
In this paper, we deal with the issue of how a person studying a foreign language can learn word usage in that language.
(cid:3) Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Hiroshi Nakagawa Information Technology Center University of Tokyo
 Tokyo, Japan nakagawa@dl.ic.u-tokyo.ac.jp For language learners stuck on problems of language usage, dictionaries based on (cid:12)ne distinctions and abstracted usages have been the main means of obtaining answers for hundreds of years.
Dictionary revisions, however, are few and far between, and users often cannot (cid:12)nd explanations concerning current usages or specialized terms in dictionaries.
Another way to search for information on usage is to use the KWIC (Key Word in Context) tool.
Recent advances in the full-text search algorithms originally proposed by Manber [11] now allow users to instantly consult text data encompassing several gigabytes.
The problem, however, is that large bodies of data are not yet readily available in languages used less commonly in international situations.
Given that the World Wide Web has become the largest electronic text body available in any language, one modern solution is to use a search engine.
For example, if a learner wants to (cid:12)nd out which verb is used with \jet lag" in English, he can simply type \jet lag" into a search engine and obtain usages such as \avoid jet lag" and \recover from jet lag" among various snippets of information.
Similarly, search engines can be used to (cid:12)nd out how to use articles correctly: for instance, in French, nonnatives will have problems with usages such as: \revenir du/de Paris" (here, it s de), compared with  revenir du/de Japon" (here, it s du).
Unfortunately, the problem here for users is that the search results are simply lists of individual usages.
Of course, the search engine ranks the usages, but they are not ranked in terms of linguistic relevance.
Also, using materials from the Web for language learning can be risky, since there are sites where language is not used in a standard style.
One way to verify such usages is to scan through several pages and statistically determine the most prominent usage.
In the case of the verb used with \jet lag", a user can see that \avoid" appears many times by scanning through multiple pages of search results.
To automate this task of scanning and summing, we created a tool, called Kiwi, that can read a large number of pages on behalf of the user and statistically accumulate usages from snippets of those pages.
The idea of creating such a tool to facilitate usage lookup is not new.
The earliest such existing tool is WebCorp [17].
Upon receiving a query, Webcorp sends it to a search engine and totals the results.
This tool is only available in English, however, and it is not readily applicable to languages without word segmentation.
Other interesting examples are Google Fight [6] and Google Duel [13], in which the user enters two di(cid:11)erent phrases and the tool suggests which is more commonly used according to the frequency reported by Google.
Although these two sites allow multilingual consultation, the range of (cid:13)exibility is limited to the \duel" of two given phrases.
Additionally, the e(cid:11)ectiveness of this idea of summing up search engine results in the usage lookup context has not been veri(cid:12)ed.
In contrast to these existing tools, Kiwi allows (cid:13)exible entry, making it language-independent.
This feature is implemented by using our language-free candidate extraction methods based on techniques originally proposed in the term extraction domain of natural language processing (NLP).
Interestingly, Kiwi can be applied not only for language usage consultation but also be used as a \pseudo" question-answering (QA) tool.
Of course, the input to our tool is a mere query, and it therefore cannot directly receive questions, as is done in QA.
We can obtain answers, however, by entering a query in the form of an answer, such as \* is the president of France", or \* is the capital of Tonga".
Then, the system will look for strings that can (cid:12)ll the * part, as if looking for usages.
Thus, Kiwi can be situated in between a search engine and a question-answering tool, having the feature of summing up search engine results.
This feature is also utilized in recent QA systems.
For example, several QA systems utilize search engine results to obtain answers by summing the results [10][1][3].
In such studies, the interest was in the quality of the answers, so the performance was evaluated in terms of the whole results of the QA procedure.
In contrast, the statistical behavior of the summing process has not been thoroughly discussed.
Thus, another contribution of this paper is to provide a fundamental analysis of the statistical behavior of summing up search engine results.
We (cid:12)rst discuss questions such as the e(cid:11)ectiveness of summing up results, the number of snippets needed to obtain the results, and the e(cid:11)ectiveness for a user trying to acquire a target usage by looking at summed results.
Another important question is the statistical behavior when di(cid:11)erent search engines are used.
The hypothesis given here is that even for search engines using di(cid:11)erent ranking methods, when a large quantity of search results is obtained and summed up, the engines  results will converge to the same results.
We thus discuss the commonness of usages acquired from di(cid:11)erent search engines.
In the next section, we describe the overall design of Kiwi.
Then, we explain our language-independent data analysis method, especially for candidate extraction.
In Section 4, we present example usages obtained using Kiwi.
Then, from section 5 on, we evaluate the usability of Kiwi from various points of view: through automatic experiments on collocations verifying the accuracy of usage extraction, through user evaluation of the tool s speed and the number of actions required, and (cid:12)nally, by comparing the results obtained by di(cid:11)erent search engines.
Figure 1: Overall design of Kiwi Figure 2: The Kiwi site

 engine results that match the query.
didates.
Kiwi is software situated between a user and a search engine (Figure 1).
Kiwi passes through the following stages in one cycle of usage consultation:

 The essential procedure of Kiwi is the totalization of the usages triggered by a query, which corresponds to stages 3-
municating directly with a search engine server.
Currently, however, it has been developed on a server available at [16].
Figure 2 shows a Kiwi site with a query on the Chinese expression of \how * is!"
(to look for adjectives in the position of *).
In the topmost part is a query box, where the user enters a query; in the next line are two pulldown menus for choosing a language and the number of snippets to be downloaded.
The lower half contains the lists of resulting usages.
Each line shows a possible usage acquired from the query and can be clicked to return to the original, individual snippet.
This is pasted on the right-hand side of Figure 2.
With this clicking function, the user can view how the usage actually appears in a longer context.
Compared with previous works, such as [17] and [6], Kiwi has two distinctive features: the query (cid:13)exibility, and its multilingual capability.
First, regarding (cid:13)exibility, every query is constructed by a user based on a pattern match.
Basically, there are three patterns: Wild card  *  can be used to replace a missing word or string.
For example, \human *", \* sans (cid:12)l", or \fed * with" can be entered into Kiwi.
Comparison  /  allows users to compare two or more phrases and prioritize them depending on which is the most relevant.
This can be expressed as (A/B/C...).
And search  +  can be used for narrowing the search domain.
For example, \* Bush +American president" or \chateau * +vin France" can be entered into Kiwi.
The cognitive background of this design is as follows.
When a user has a question regarding language usage, he often cannot clearly remember the complete expression but still has a few clues that can be utilized to access language data.
Such vague recollections typically follow two major patterns: in one, the user lacks the information needed to complete part of a phrase, while in the other, he is uncertain in choosing the correct word from among several candidates.
Second, as noted above, Kiwi is language independent.
In any language, there is the problem of updating conventional resources to include current language usage.
Conventional dictionary revisions are few and far between.
Such cases are emphasized for languages that are not widely used internationally.
The available dictionaries are usually older, the available corpora are too small for KWIC, and the cycle of revising the data is likely to be longer.
Therefore, students learning a foreign language other than English often (cid:12)nd that the available language resources are limited, and that they must rely more on search engine results.
This is the main reason why we designed Kiwi to be independent of language; that is, Kiwi uses neither language-dependent algorithms nor a language dictionary for analyzing search engine results.
In the next section, we describe the essential procedure inside Kiwi: candidate processing.
Figure 3: Concept behind the language-independent candidate extraction methodology


 When a wild card is used at the beginning or end of a query, Kiwi needs to cut out the relevant chunk1.
As the method of processing the beginning and end of a query is symmetric, candidate extraction to obtain the end part of the query is discussed here.
The easiest way is to cut out a string, or a group of words, of a (cid:12)xed length.
Fixed-length strings are commonly used in the KWIC system for non-segmented languages.
One drawback of using (cid:12)xed-length strings is that it hinders the aggregation of statistics obtained from data.
For example, although \with" is often found after \be acquainted", all occurrences of \with" cannot be put together if the length is (cid:12)xed at 10 characters, because there are a variety of strings that can come after \with."
Therefore, we want a candidate that has the correct border.
Intuitively, we would expect a candidate to have the following properties: A It occurs frequently.
B It has moderate length.
C It is succeeded by various kinds of characters.
A concerns the relevance of the candidate (as will be discussed in the next section).
On the other hand, C essentially expresses information about word boundaries.
B can be used for either objective.
Therefore, we have tried to implement candidate extraction by combining C and B.
C leads to the idea of looking at the branching degree at a language chunk border, which has been discussed in the context of automatic term-recognition methods [5][12].
In these works, the branching notion was used as part of the extraction evaluation function for detecting a collocation border.
These methods, however, are word based and presume the use of taggers for non-segmented languages.
The above observations on word sequence can be generalized to a character sequence as follows.
If the branching degree at each character inside a word is observed, it is seen to monotonically decrease according to the o(cid:11)set length.
This is a natural result, because the longer the preceding character n-gram, the longer the preceding context.
On the other
 straightforwardly determined.
Table 1: Live examples of Kiwi hand, the average branching degree at the point of a word border becomes greater, as the complexity increases, once it is out of context.
This suggests that the word border can be detected by focusing on the di(cid:11)erentials of the branching degree.
This concept is visualized in Figure 3.
Based on this concept, we devised a simple string-based method for cutting out candidates even from non-segmented language data.
Most importantly, this method is language independent.
With X as a string, let Xi be the head i characters of X, and let Ci be the number of kinds of characters at the (i + 1)th position of all strings with the pre(cid:12)x Xi.
We extract Xi as a candidate when Ci > Ci(cid:0)1: (1) Note that this method integrates the notion of the length (condition B) at the same time, because the decreased branching degree puts constraints on the length.
To facilitate this procedure, after obtaining snippets, Kiwi transforms the data into a tree by counting the frequencies and branching degrees.
Then, using the method described above, Kiwi cuts out candidates by traversing the tree.
Related to this approach in totalizing snippets, Grouper [18] transforms snippets into a su(cid:14)x tree in order to classify documents by grouping snippets semantically.
Their method will be applicable to our simple tree approach in the future, for the purpose of organizing the resulting usages further on from a contextual point of view.
Note that there are cases where this method will always fail to extract candidates: i.e., when a candidate occurs only once.
More generally, the extraction with our method tends to fail when a candidate seldom occurs.
For example, if there are only two occurrences of \international" and \intention", then \inte" is output as a word.
To cope with such cases, we can use a stop word, such as the space symbol, as a delimiter for words in segmented languages.
The treatment of stop words was actually considered for the actual system described in [16]2.
Nevertheless, we performed all of the experiments described in x5 without special processing of symbols.
After candidates are obtained, Kiwi ranks them.
Here, the most important information is the frequency (condition A).
The length is also of concern, though, because short strings obviously occur more frequently than longer ones (condition B).
Therefore, a bias towards longer candidates should be included in the evaluation function.
The question is how best to incorporate these two features into an evaluation function formula.
We decided to empirically choose the evaluation function.
With X as the candidate, jXj as its length, and f req(X) as its frequency, we used the following as our evaluation function: F (X) = f req(X)log(jXj + 1): (2)
 e(cid:11)ective, because it can output word chains as candidates.
Table 2: Top 10 AltaVista results for  wireless 



 5th 6th 7th 8th

 Point Wireless has AT&T Cingular Wireless U.S. wireless carrier the wireless plan We chose this function because it outperformed all other functions that we considered in the evaluation described from x5 on.
Before moving on to our veri(cid:12)cation on Kiwi s performance, we consider some actual examples of how Kiwi can be used (Table 1).
These examples are related to seven languages.
Some involve the translation of \wireless network", while others show VIP names, (cid:12)lm titles, food names, and so on.
Though we found some data bias towards Internet-related topics (e.g., wireless LAN cards being highly ranked because of advertisements), the examples directly re(cid:13)ect our times: Arnold Schwarzenegger watching Harry Potter and the Sorcerer s Stone and The Matrix, eating kimchi and Peking duck, and listening to Bach.
Kiwi here is functioning as both a simple question-answering tool and a usage consultation tool.
What we want to emphasize here is that such an overall view cannot be obtained through the direct use of a search engine by the user.
Table 2 shows the AltaVista search results for \wireless".
(Kiwi currently uses AltaVista as its mother search engine because of its indexing strategy).
These results are badly a(cid:11)ected by noise.
By comparison, Kiwi gives far clearer results: network, communications, and Internet.
We (cid:12)rst evaluated Kiwi s basic performance by using collocations.
Our evaluation was done in four stages:
 words in length.
These collocations were obtained from language-learning books and sites.
part with a wild card.
In the following, these are called queries, and the replaced words are called answers.
Sample queries include \is sick in *" (bed is the answer), \catch * with" (up), and \il vaut *" (mieux, in French, means \it had better").
The resulting queries amounted to 300 in English and 100 each in Japanese and French.
The results do not include queries with multiple answers, such as the fact that \in comparison *" can be (cid:12)lled with \to" or \with".
from AltaVista (only snippets).
The maximum data quantity was set at 1000 matches for each query.
Figure 4: Accuracies for Kiwi and the baseline when the top N candidates were veri(cid:12)ed
 N candidates.
For comparison, a baseline was calculated similarly by looking at the query matches in descending order of the search engine results.
The rates were counted with respect to an exact match for the Kiwi results, whereas the baseline results were counted as correct when the corresponding string included the required answer.
Figure 4 shows the Kiwi and baseline results for collocations.
The horizontal axis indicates di(cid:11)erent values of N , whereas the vertical axis indicates the accuracy.
Each line corresponds to a di(cid:11)erent language.
The three upper lines correspond to results obtained from Kiwi, whereas the three lower lines show results for the baseline.
Each plot corresponds to the results averaged for the head, tail, and middle (i.e., the averaged results of 900 queries for English and 300 queries for French and Japanese).
When N =1 (where only the top-ranking candidate is ver-i(cid:12)ed), the Kiwi performance was twice as good as that of the baseline.
Also, more than 90% of the Kiwi results included the required answers among the top ten, which was superior to the baseline results.
As this tool is intended for human use, this result of answers being included in the top ten is important.
We can conclude that the Kiwi process of totalization is e(cid:11)ective.
We can also see that Kiwi s performance depends on the availability of data.
Data is the most abundant in English,

 summer or in winter?
\The last shogun of the Tokugawa era was .
.
.
th shogun Tokugawa .
.
.
".
Figure 6: TREC-like question examples used in the user evaluation Table 3: User evaluation of Kiwi vs. AltaVista Required time (mins) Kiwi Search engine Average Deviation



 Number of clicks Kiwi Search engine Average Deviation



 Con(cid:12)dence (1 low | 5 high) Kiwi Search engine Average Deviation



 Accuracy (percentage) Kiwi Search engine Average Deviation



 have to wait around 30 seconds on average, and majority of the time is used for downloading.
As this is de(cid:12)nitely slower than just using a search engine, we are currently modifying the system so that the analysis results are immediately shown for a small number of snippets, then dynamically changed along with the increase in the downloaded quantity of data.
We next conducted an experimental user evaluation of Kiwi, which had two goals.
The (cid:12)rst was to see how effective Kiwi could be in actual use.
Second, we sought to observe how reasonably Kiwi could be used as a \pseudo" QA system.
As explained in x1, our tool can be used for QA tasks by entering answer phrases with the answers replaced by wild cards.
This feature is also di(cid:14)cult to evaluate au-tomatically3.
Consequently, we tested the performance of Kiwi through this user evaluation.
We collected 20 Japanese students from the computer sci-
that are downloadable from a search engine.
Otherwise, we would not have known whether we were measuring the Kiwi performance or the quality of the queries.
This condition could not be completely ful(cid:12)lled for many collocations, however, and this degraded Kiwi s performance.
Here, for QA, this di(cid:14)culty of obtaining suitable queries is even more strongly emphasized.
Figure 5: Ranking shift of answers for each downloaded data quantity then in Japanese, and lastly in French (see [14]).
The number of matches for a query was around 600 to 700 in English,
 performance when N=1 in each language exhibits such differences.
There were two cases where Kiwi could not provide the required answers from among the candidates.
First, if the answer did not occur frequently enough on the web, Kiwi could not extract it.
This defect is caused not by Kiwi itself, but rather by the query not being appropriate for the evaluation, or by the Web not having enough data concerning the query.
The error rate attributable to such cases equals (100 - plot of baseline when N =1000)%; this is because, as we collect at most 1000 matches, the plot for the baseline at N =1000 forms the upper bound of the accuracy.
The di(cid:11)erence between the Kiwi and baseline plots for the same language at N =1000 indicates the error caused by candidate extraction.
This di(cid:11)erence was only 1% in English and Japanese but 5% in French, where less data was available as compared with English.
This demonstrates the performance of our candidate extraction method using the branching factor.
We can say that it was successful in most cases.
On the other hand, the di(cid:11)erence between the plots at N =1 and at N =1000 for the same line of the Kiwi results indicates the error due to the Kiwi s ranking method of formula (2).
This amounts to 10 to 30%.
In our future work, we plan to revise the evaluation function used in the ranking process so that the rate of availability of required answers can be pushed higher.
Using the same queries, we observed the ranking changes of the answers according to the number of snippets downloaded from the search engine.
Figure 5 shows the results.
The horizontal axis indicates the number of snippets, while the vertical axis indicates the rankings of the answer.
The two lines correspond to the average rankings of the answers for the head and tail parts for English.
We can see that the more snippets are downloaded, the better (lower) the ranking becomes.
Finally, it converges to 1.0, meaning that the answer acquired the highest ranking among all other candidates.
This happens at around 900 snippets.
When small numbers of snippets are downloaded, the ranking still oscillates, and the behavior of Kiwi is unstable.
To download and process 900 snippets, the user would ence (cid:12)eld, all of whom have been using the Internet for more than 5 years.
Each of the students was requested to answer TREC-like questions in Japanese by using either AltaVista or Kiwi.
The 32 questions were constructed based on web site usability test guidelines [15].
Examples of the questions are shown in Figure 6.
The questions were divided into two groups of 16.
10 students answered the (cid:12)rst group by using Kiwi and the second with AltaVista, whereas the other 10 students answered the second group with Kiwi and the (cid:12)rst group with AltaVista.
Each student was asked to answer the 32 questions by using Kiwi and AltaVista in turn.
In using both AltaVista and Kiwi, the students were asked to create queries on their own to answer the questions.
They were allowed to re(cid:12)ne these queries while searching for the answers.
Also, they were allowed to use  / (comparison) and  + (And search) with Kiwi and the full functionality of query construction with AltaVista.
The results are shown in Table 3.
Each block indicates the time, number of clicks, con(cid:12)dence score, and accuracy for Kiwi and AltaVista, while the columns represent the averages and deviations of the values through 10 subjects.
Kiwi outperformed AltaVista in all categories: with Kiwi, users could get more accurate answers faster, with fewer clicks and more con(cid:12)dence.
Note that the method of formulating a query, which was totally up to the students in this experiment, remains an issue with Kiwi, as well as with Al-taVista.
Still, the students could get answers faster with less action with Kiwi.
Additionally, the deviations were smaller for Kiwi, meaning that many of the users could use Kiwi in a more stable manner.
We guess that this results from Kiwi s totalization of search engine results, so that it helps users think based on the arranged information.
The performance of Kiwi was reduced for questions of the following two types.
The (cid:12)rst type was a question whose answer was not available on the Web: in this case, the users could not obtain the correct answer.
Note that the subjects who used AltaVista had the same problem with this sort of question.
The second type was a question whose query form does not match with its form in Kiwi.
Currently, Kiwi only supports one wild card per query.
When the subjects faced a question like No.
4 in Fig. 6, however, they all faced the problem of not being able to enter several wild cards.
A user-friendly interface thus plays an important role, and we still have to consider this viewpoint in our future work.
So far, we have used AltaVista as the search engine for Kiwi.
In this section, we examine the di(cid:11)erences in Kiwi s results when di(cid:11)erent search engines are used.
First, we compared the performance di(cid:11)erence for collocation.
We chose AllTheWeb and Google as the next two engines for evaluation.
Note that they use di(cid:11)erent indexing and ranking strategies from AltaVista, although most of the details of these strategies are not open to the public for commercial reasons.
The most important di(cid:11)erence, above all, is that Google and AllTheWeb use word-based indexing [2]: for example, a query of \moon*" will not generate candidates such as \moonlight" or \moonshot".
This is the same Figure 7: Accuracy of Kiwi using AllTheWeb and Google for Japanese and Chinese: Google and AllTheWeb depend on chunkers.
The results are shown in Figure 7.
As in Figure 4, the horizontal axis indicates di(cid:11)erent values of N , whereas the vertical axis indicates the accuracy.
Each line corresponds to a language (English, Japanese, French) that was tested with one of the search engines (AllTheWeb or Google).
The AltaVista results were already shown in Figure 4.
Generally, not much performance di(cid:11)erence was observed from the point of view of accuracy, although the search engines adopt very di(cid:11)erent indexing and ranking strategies.
In English, where data is abundant, the performance was especially similar, whereas in Japanese and French, some di(cid:11)erences were seen.
Usually, AltaVista outperformed the other two, because of its indexing strategy.
This trend was emphasized in Japanese.
Next, we veri(cid:12)ed the commonness of the candidates proposed by Kiwi when using di(cid:11)erent search engines.
The results are shown in Figure 8.
The horizontal axis indicates di(cid:11)erent values of N that were veri(cid:12)ed, whereas the vertical axis indicates the percentage of common candidates.
The common candidates were veri(cid:12)ed between two di(cid:11)erent engines (i.e., AllTheWeb vs. Google, AltaVista vs.Google, AltaVista vs. AllTheWeb) before being averaged into one score.
Each line corresponds to a language.
The commonness rate was the highest in English.
Curiously, French then came before Japanese.
In English, 80 to 90% of the candidates were the same among any two search engines when the best ranking candidate was veri(cid:12)ed.
This was reduced to 50 to 70% when the top (cid:12)ve were examined, on the use of the WWW.
Among the most recent ones, [10] compared the performance of their MULDER QA system to that of AskJeeves on questions drawn from the TREC-
recall is more than a factor of three higher than that of AskJeeves.
This study supports the possibility of applying our usage tool for QA tasks.
In exploiting Kiwi s feature of totalization of search results, we might go further and utilize contexts for structuring the resulting usages.
Giving a richer context to organize usages might help as much as eliminating noise.
This can be done in one of two ways.
The (cid:12)rst is to adopt classi(cid:12)ca-tion of documents, in the manner shown by [7] or [9], or by structuring snippets using the technology proposed in [18].
The second way is to organize resulting usages directly by utilizing the user context.
The techniques described in [4], where search is performed using the context provided by a user document, may be applicable.
Kiwi is a web-based usage consultation tool employing search engines.
When a user enters a string of words to check their usage, the system sends a query to a search engine to obtain a data related to the string.
The result is then statistically analyzed, and the results are displayed.
Our system di(cid:11)ers from existing systems in two ways.
First, users can enter more (cid:13)exible queries than with other existing software for related purposes.
Second, Kiwi is language independent, so queries can be made in any language if the search engine supports that language.
This is achieved by string-based processing of the candidate extraction and ranking.
We have formalized the extraction through character branching, and Kiwi ranks candidates based on frequency and length.
According to our evaluation, the missing parts of collocations were provided by the highest-ranked candidate at a rate of 70% to 80% in English, Japanese, and French.
When the top ten candidates were considered, nearly 95% of the answers were found.
Also, a user evaluation was conducted by requesting subjects to solve TREC-like questions.
Kiwi outperformed AltaVista in terms of user performance.
Although search engines are designed for general use, rather than language-speci(cid:12)c use, we have found that they can be useful for usage consultation when the results are aggregated.
Moreover, Kiwi results with di(cid:11)erent search engines demonstrated similar distributions of usages in our investigation based on collocations.
Users should bear in mind, however, that the results are biased by the amount of available data related to certain topics.
Currently, Kiwi is available to the public at [16].
