The increase of WWW resources has fueled the demand for e ective and e cient information retrieval.
Millions of searches are conducted every day on search engines such as Yahoo!, Google and MSN, etc.
Despite the popularity, search engines have their de ciencies: given a query, they usually return a huge list of results and the pages ranked at top may not meet users  needs.
One reason for this problem is the keyword-based query interface, which is di cult for users to describe exactly what they need.
Besides, typical search engines often do not exploit user information.
Even two users submit the same query, their information need may be di erent [4, 16].
For example, if a query  jaguar  is issued to Google, 11,900,000 results are returned.
Regardless of who submits the query, both the pages returned and the rank orders are identical.
Since  jaguar  may refer to  jaguar car  or  jaguar cats , two users with di erent interests may want the search results ranked di erently: a car fan may expect car relevant pages ranked highly, however, these pages may be unnecessary to be displayed for a zoologist.
Thus the search results should be adapted according to the person who submits the query and which query he/she submits.
Personalized Web search is to carry out retrieval for each user incorporating his/her own information need.
As the competition in search market increases, some search engines have o ered the personalized search service.
For example, Google s Personalized Search allows users to specify the Web page categories of interest [1].
Some Web search systems use relevance feedback to re ne user needs or ask users to register their demographic information beforehand in order to provide better service[2, 8].
Since these systems require users to engage in additional activities beyond search to specify/modify their preferences manually, approaches that are able to implicitly capture users  information needs should be developed.
This paper focuses on utilizing clickthrough data to improve Web search.
Consider the typical search scenario: a user submits a query to a search engine, the search engine returns a list of ranked Web pages, then the user clicks on the pages of interest.
After a period of usage, the server side will accumulate a collection of clickthrough data, which records the search history of Web users.
The data objects contained in the clickthrough data are of di erent types: user, query and Web page, furthermore, relationships among these objects are complicated [25].
For example, users with similar information needs may visit pages of similar topic even they submit di erent queries; users with dissimilar needs may visit di erent pages even they submit the same query, as the  jaguar  example indicates.
It can be assumed that the clickthrough data may re ect Web users  interests and may contain patterns that users found their information [13, 14].
By performing analysis on the clickthrough data, we attempt to discover the latent factors that govern the associations among these multi-type objects.
Consequently, Web pages can be recommended according to the associations captured.
Here we clarify some characteristics speci c to personalized Web search based on clickthrough data analysis.
This task is related to recommender systems which have been extensively studied [3, 6, 11, 21].
While most recommendation algorithms like Collaborative Filtering (CF) are applied to two-way data containing user preferences over items, the clickthrough data analysis deals with three-way data.
As far as we know, previous literature on recommendation contains few studies on data of this kind.
The three-way clickthrough data imposes at least two challenges:
 complicated.
There exist intra-relations among objects of the same type, as well as interrelations among objects of different type [25].
For personalized Web search tasks, what we are concerned about are the 3-order relations among them.
That is, given a user and a query issued by the user, the purpose is to predict whether and how much the user is interested in a Web page.
Therefore, a uni ed framework is needed to model the multi-type objects and the multi-type relations among them.
most CF algorithms are susceptible to data sparsity [21,
 more serious because each user only submits a small number of queries, and only a very small set of Web pages are visited by each user.
Latent Semantic Indexing (LSI) [7] has been proved useful to address the data sparseness problem in two-way data recommender systems [20, 21], however, it is still an open problem for the three-way data case.
In order to address the problems mentioned above, we need an approach dealing with the clickthrough data which is three-way and highly sparse.
In this paper, we develop a uni ed framework to model the three types of objects: user, query and Web page.
The clickthrough data is represented by a 3-order tensor, on which 3-mode analysis is performed using the Higher-Order Singular Value Decomposition (HOSVD) technique [15].
Because our tensor representation is 3-dimensional and our approach is a multilin-ear extension of the matrix Singular Value Decomposition (SVD), we name it CubeSVD.
The remainder of this paper is organized as follows.
Section 2 provides related work.
Section 3 gives a brief introduction to SVD and HOSVD techniques.
Section 4 describes our proposed CubeSVD algorithm.
Section 5 presents the experimental results and Section 6 o ers some concluding remarks and directions for future research.
In this section we brie y present some of the research literature related to personalized Web search, recommender systems, SVD for recommendation, clickthrough data relevant mining technique and Higher-Order Singular Value Decomposition (HOSVD).
Some previous personalized search techniques, e.g., [2, 16,
  les are created by asking users to  ll out registration forms or to specify the Web page categories of their interests [1].
Users have to modify their preferences by themselves if their interests change.
There are also some works on automatic creation of user preferences.
In [23], user pro les were updated by accumulating their preferences re ected in the past browsing history.
In [16], the user pro le was represented by a hierarchical category tree and the corresponding keywords associated with each category.
The user pro le was automatically learned from the user s search history.
Many current Web search engines focus on hyperlink structures of the Web.
For example, Google calculated a universal PageRank vector which re ects the relative importance of each page.
Personalized PageRank, which is a modi cation of global PageRank, was  rst proposed for personalized Web search in [18].
In [10],  topic sensitive  PageRank was proposed to improve personalized Web search.
The authors proposed to compute a set of PageRank vectors which capture the page importance with respect to a particular topic.
Since no user s context information is used in this approach, it is di cult to evaluate whether the results achieved satisfy a user s information need.
Besides search engines, many recommender systems have been developed which recommend movies, music, Web pages, etc.
Most recommender systems analyze a matrix containing user preferences over items.
Among the algorithms used, Collaborative Filtering (CF) is a group of popular methods [6, 11].
The philosophy behind CF is to recommend items based on preferences of similar users.
That is, if a group of users share similar interests, the items preferred by one user can be recommended to others of the group.
Since neighborhood formation requires su cient amounts of training data, CF is sensitive to data sparsity [21, 3].
In order to address this issue, Latent Semantic Indexing (LSI) was applied to recommender systems and promising results were achieved [20, 21].
LSI was based on truncated singular valued decomposition and has also been successfully used in information retrieval (IR) community [7].
In [21], the authors use LSI for two recommendation tasks: to predict the likeliness of a product preferred by a customer; and to generate a list of top-N recommendations.
LSI was also studied in [22] for collaborative  ltering applications.
Web usage mining techniques have achieved great success in various application areas [13, 14, 17].
As far as we know, there was seldom works on incorporating three-way click-through data for personalized Web search.
An exception is [3], which extended Hofmann s aspect model to incorporate three-way co-occurrence data for recommendation problem.
Figure 1: Visualization of matrix SVD However, it was not used for Web search application.
The technique introduced in [14] uses clickthrough data in order to improve the quality of Web search.
The author uses the relative preferences between Web pages and learns the retrieval functions.
In [25], the authors also examine the interrelated data objects of clickthrough data and put forward a reinforcement clustering algorithm to cluster these multi-type objects.
The higher-order singular value decomposition technique was proposed in [15].
It is a generalization of singular value decomposition and has been successfully applied for computer vision problems in [24].
We propose to use the HOSVD technique for personalized Web search in this paper.
Since our CubeSVD approach is based on HOSVD technique, which is a generalization of matrix SVD, we  rst brie y review matrix SVD and then introduce tensor and the HOSVD technique.
In this paper, tensors are denoted by calligraphic uppercase letters (A,B   ), matrices by uppercase letters (A, B   ), scalars by lower case letters (a, b  ), vectors by bold lower case letters (a, b  ).
I1   I2 matrix F , it can be written as the product: The SVD of a matrix is visualized in Figure 1.
For a i
 1 u(1) 1 u(2) ) and U (2) = (u(2)
 2   u(1) ,1   i   I1 and u(2) (1) 2   u(2) where U (1) = (u(1)
 are the matrices of the left and right singular vectors.
The ,1   j   I2 are column vectors u(1) orthogonal.
S = diag( 1,  2,  ,  min(I1,I2)) is the diagonal matrix of singular values which satisfy  1    2        min(I1,I2)   0.
By setting the smallest (min{I1, I2}   k) singular values in S to zero, the matrix F is approximated with a rank-k matrix and this approximation is best measured in reconstruction error.
Theoretical details on matrix SVD can be found in [9].
j A tensor is a higher order generalization of a vector ( rst order tensor) and a matrix (second order tensor).
Higher-order tensors are also called multidimensional matrices or multi-way arrays.
The order of a tensor A   RI1 I2 IN is N .
Elements of A are denoted as ai1 in iN where 1   in   In.
In tensor terminology, matrix column vectors are referred to as mode-1 vectors and row vectors as mode-2 vectors.
The mode-n vectors of an N -th order tensor A are the In-dimensional vectors obtained from A by varying the index in and keeping the other indices  xed, that is the column vectors of n-mode matrix unfolding A(n)   RIn (I1I2 In 1In+1 IN ) of tensor A.
See [15] for details on matrix unfoldings of a tensor.
Figure 2: Visualization of a 3-order Singular Value Decomposition The n-mode product of a tensor A   RI1 I2 IN by a matrix M   RJn In is an I1   I2   In 1   Jn   In+1       IN tensor of which the entries are given by (A  n M )i1 in 1jnin+1 iN ai1 in 1inin+1 iN mjnin
 = in (2) Note that the n-mode product of a tensor and a matrix is a generalization of the product of two matrices.
It can be expressed in terms of matrix unfolding: B(n) = M A(n) (3) where B(n) is the n-mode unfolding of tensor B = A  n M .
In terms of n-mode products, the matrix SVD can be rewritten as F = S  1 V (1)  2 V (2).
By extension, HOSVD is a generalization of matrix SVD: every I1   I2       IN tensor A can be written as the n-mode product [15]:
 (4) as illustrated in Figure 2 for N = 3.
Vn contains the or-thonormal vectors (called n-mode singular vectors) spanning the column space of the matrix A(n) (n-mode matrix unfolding of tensor A).
S is called core tensor.
Instead of being pseudodiagonal (nonzero elements only occur when the indices satisfy i1 = i2 =   = iN ), S has the property of all-orthogonality.
That is, two subtensors Sin=  and Sin=  are orthogonal for all possible values of n ,   and   subject to   6=   .
At the same time, the Frobenius-norms i = kSin=ik are n-mode singular values of A and are in  n In   0.1 S is in general decreasing order:  n a full tensor and governs the interactions among Vn.
2        n 1    n

 ) When using a search engine to  nd information: a user(u) submits a query(q), the search engine returns a list of URLs and the corresponding descriptions of the target Web pages, then the user clicks on the pages(p) of interest.
After some time of usage, the search engine accumulates a collection of clickthrough data, which can be represented by a set of triplets hu, q, pi.
From the clickthrough data, we can construct a 3-order tensor A   RU Q P , where U ,Q,P are sets
 is de ned as hA,Bi= ai1i2 iN   bi1i2 iN .
Sin=i is the subtensor of S obtained by  xing the nth index of S to i.
More details are referred to [15].
phA,Ai.
And the scalar product hA,Bi of two tensors A,B


 iN i1 i2 (cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:3)(cid:1)(cid:2)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:1)(cid:3)(cid:9)(cid:6)(cid:7)(cid:3)(cid:8)A=V1SV3V2Table 1: Details of the Web Pages Used in the Toy Problem Title
 http://www.bmw.com http://www.audiusa.com BMW International Website Audiusa.com Home Page http://www.jaguarusa.com/us/en/home.htm Jaguar Cars http://dspace.dial.pipex.com/agarman/bco/ver4.htm Big Cats Online Home Page p1 p2 p3 p4
 pose the numbers of user, query and Web page are m, n, k respectively, then A   Rm n k.
Each tensor element measures the preference of a huser, queryi pair on a Web page.
tensor A.
Au is calculated by varying user index of tensor A while keeping query and page index  xed.
Aq and Ap are computed in a similar way.
Thus Au, Aq, Ap is a matrix of m   nk, n   mk, k   mn respectively.
to be the left matrix of the SVD respectively.
the rightmost m   m0, n   n0 and k   k0 columns from Vu, Vq and Vp , then denote the reduced left matrix by Wu, Wq and Wp respectively.
Calculate the core tensor as follows:
 p u  2 W T q  3 W T
  A = S  1 Vu  2 Vq  3 Vp (5) (6) Figure 3: Outline of the CubeSVD algorithm.
of users, queries and pages respectively.
Each element of tensor A measures the preference of hu, qi pair on page p.
In the simplest case, the co-occurrence frequency of u, q and p can be used.
In this paper, we also tried several other approaches to measure the preference.
After tensor A is constructed, the CubeSVD algorithm can be applied on it.
Our CubeSVD approach is to apply HOSVD on the 3-order tensor constructed from the clickthrough data.
In accordance with the HOSVD technique introduced in Section
 the input is the clickthrough data, the output is the reconstructed tensor  A.
 A measures the associations among the users, queries and Web pages.
The elements of  A can be represented by a quadruplet hu, q, p, wi, where w measures the likeliness that user u will visit page p when u submits query q.
Therefore, Web pages can be recommended to u according to their weights associated with hu, qi pair.
In this subsection, in order to illustrate how our approach works, we apply the CubeSVD algorithm to a toy problem.
As illustrated in Figure 4, 4 users issued 4 di erent queries ( bmw ,  audi ,  jaguar ,  big cat ) and clicked on 4 Web pages.
In Figure 4, the arrow line between a user and a query represents the user issued the corresponding query.
The line between a query and a page indicates the user clicked on Figure 4: Clickthrough data of the toy problem.
the page after he/she issued the query.
The numbers on the arrow line gives the correspondence between the three types of objects.
For example, user u1 issued query  bmw  and then clicked on page p1.
The users performed seven clicks on the 4 pages in this toy problem.
The URLs and titles of the pages visited are given in Table 1.
Query  jaguar  may refer to  jaguar car  or  jaguar cats .
From Table 1, we can  nd that p1, p2 and p3 are Web pages on  cars , page p4 is related to  cats .
From Figure 4, we can see that user u1 and u2 have common interests on cars, while user u3 and u4 are interested in big cat animals.
A 3-order tensor A (4 4 4) can be constructed from the clickthrough data.
For simplicity, we assume there are no duplicate page visits.
That is, if a user issues a query and then clicks on a Web page, the user only clicks on the page once.
We use the co-occurence frequency of user, query and page as the elements of tensor A, which are given in Table
 reconstructed tensor  A.
Table 3 gives the output of the CubeSVD algorithm, as illustrated in Figure 5.
In Table 3, the rows in italic font represents that this link relation does not exist in the original clickthrough data.
As given in Table 3 and Figure 5, the output of the Cube-SVD algorithm for this toy problem is interesting: new associations among these objects come out.
From the original clickthrough data (Figure 4), we can  nd that neither user u1 nor u4 issued query q3.
There is also no direct indication on which pages to recommend if either of the two users submits query q3, because query q3 is ambiguous.
According to the algorithm outputs given in Table 3, the element of  A associated with hu1, q3, p3i is 0.354 and elements associated with other pages are zero.
Thus if u1 issues query q3, then u1 is likely to visit page p3 (arrow line 9).
Similarly, if user u4 submits query q3, then u4 is likely to visit p4 (arrow line (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:1)(cid:7)(cid:8)(cid:1)(cid:9)(cid:10)(cid:1)(cid:11)(cid:12)(cid:8)(cid:13)(cid:1)(cid:8)(cid:14)(cid:1)(cid:15)(cid:3)(cid:10)(cid:13)(cid:16)(cid:17)(cid:8)(cid:18)(cid:6)(cid:15)(cid:6)(cid:11)(cid:6)(cid:7)(cid:1)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:4)(cid:5)(cid:5)(cid:6)(cid:6)(cid:7)(cid:7)(cid:2)(cid:8)(cid:8)(cid:9)(cid:1)(cid:9)(cid:3)(cid:9)(cid:4)(cid:9)(cid:5)Table 2: Tensor Constructed from the Clickthrough Data of the Toy Problem Arrow Line User Query Page Weight






 u1 u2 u2 u2 u3 u3 u4 q1 q1 q2 q3 q3 q4 q4 p1 p1 p2 p3 p4 p4 p4






 Table 3: Output of CubeSVD Algorithm on the Toy Problem Arrow Line User Query Page Weight









 u1 u2 u2 u2 u3 u3 u4 u1 u1 u4 q1 q1 q2 q3 q3 q4 q4 q2 q3 q3 p1 p1 p2 p3 p4 p4 p4 p2 p3 p4










 cars rather than big cat animals, while u4 is opposite.
Even the two users have not issued query q3, our algorithm can still recommend Web pages by analyzing the clickthrough data.
That is, the CubeSVD approach is able to capture the latent associations among the multi-type data objects: user, query and Web page.
The associations can then be used to improve the Web search accordingly.
The latent associations among the three types of objects captured by CubeSVD are stored in the reconstructed tensor  A.
From step 5 of the CubeSVD algorithm in Figure 3, we know tensor  A is constructed by the product of the core tensor S and the left matrix Vu, Vq and Vp and the dimensions of S are selected in step 4.
Since the core tensor S governs the interactions among user, query and Web page objects, the determination of core tensor dimensionality may play an important role in the result of the algorithm.
This is further veri ed by our experiments in Section 5.
Recall in the two-dimensional case [7], LSI computes a low rank approximation of the original term-by-document matrix to capture the semantic concepts of a document set.
The resulted matrix is calculated by truncated SVD as Figure 1 indicates.
Previous experiments indicate that the number of singular values kept in the diagonal matrix S is crucial for LSI s performance [12].
And how to determine the dimension is still an ongoing research problem.
For the CubeSVD approach, determination of the core tensor s dimensions seems more di cult than LSI.
Because for LSI, the term-by-document matrix is two dimensional, thus only one parameter (the number of nonzero singular values) needs to be decided.
For CubeSVD, there are three dimensional parameters to be determined.
According to the CubeSVD algorithm in Figure 3, the core tensor S is cal-

culated from the product of tensor A by W p.
q and W Therefore how many columns of Vu, Vq, and Vp are kept determines the dimensions of the core tensor (m0   n0   k0).
Since the left matrix Vu, Vq and Vp are calculated by solving SVD problems on the matrix unfolding Au, Aq and Ap respectively, in this paper we use an eigenvalue based method to determine the core tensor dimensions empirically.
u, W According to the tensor decomposition property [15]: nX kX mX kA    Ak   ( u iu )2 + ( q iq )2 + ( p ip )2 iu=m0+1 iq =n0+1 ip=k0+1 m,  q m0+1,  q n, p k0+1,  , p m0 , q n0 and  p n0+1,   , q k0 are much bigger than  u (7) By discarding the smallest n-mode singular values  u m0+1,   ,  u k to zero, we obtain an approximation  A of the original tensor A.
As discussed in [15], if  u n0+1,  k k0+1 respectively, the energy lost is not signi cant and is bounded as in Equation 7.
Based on this property, we use the eigenvalues in the three matrix unfolding SVD problems, i. e., the smallest eigenvalues are discarded, thus reducing the dimensionality of the core tensor to     (m   n   k).
In this paper,   is tuned empirically.
In our CubeSVD algorithm, the tensor value measures the preference of a huser, queryi pair on a Web page.
If the page click frequency is used as tensor value, the algorithm is inclined to biasing towards tensor elements with high frequency.
We also try three other weighting approaches: for each huser, queryi pair, if a page is clicked on, then the tensor value associated with the three objects is 1, otherwise 0.
use a method used in IR community.
For each clickthrough data triple hu, q, pi, the weight of the corresponding tensor value is a re-weighting of the page click frequency f :
 f = log2 (1 + f ) (8) Figure 5: Illustration of the CubeSVD algorithm output for the toy problem given in Figure 4.
The log function is used for scaling the page click frequency in order to reduce the impact of highly frequent visits.
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:1)(cid:7)(cid:8)(cid:1)(cid:9)(cid:10)(cid:1)(cid:11)(cid:12)(cid:8)(cid:13)(cid:1)(cid:8)(cid:14)(cid:1)(cid:15)(cid:3)(cid:10)(cid:13)(cid:16)(cid:17)(cid:8)(cid:18)(cid:6)(cid:15)(cid:6)(cid:11)(cid:6)(cid:7)(cid:1)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:4)(cid:5)(cid:5)(cid:6)(cid:6)(cid:7)(cid:7)(cid:2)(cid:8)(cid:8)(cid:9)(cid:1)(cid:10)(cid:1)(cid:10)(cid:9)(cid:11)(cid:1)(cid:11)(cid:3)(cid:11)(cid:4)(cid:11)(cid:5)(cid:12)(cid:12)3) The third approach is similar with the second one.
Here we take into account the Inverse Document Frequency (IDF) of a Web page (that is, frequency of a page visited by different users).
The intuition is that, if a Web page is visited by most users, then it is not representative for measuring users  interests:
 f = log2 (1 + f /f0) (9) In Equation 9, f0 denotes IDF of a Web page.
The above three weighting schemes (denoted by Weight-Boolean, Weight Log Freq, Weight Log Freq IDF respectively), as well as the scheme without weighting (denoted by Weight Freq), are all tested in our experiments in Section 5.
In the 2-dimensional case, LSI uses the co-occurrence of words and documents to capture the latent semantics of a document set: if two words co-occur frequently, they may be semantically related.
In the 3-dimensional case, our Cube-SVD algorithm is applied on the clickthrough data, which contains the co-occurrence of the three types of objects: user, query and Web page.
If the link relations among them are scarce, the latent associations may be di cult to capture.
Generally, when a user issues a query, she may only visit a very small set of pages of interest, which may lead to a highly sparse tensor.
In this work, we employ two smoothing methods to address the sparseness problem and the corresponding results are compared with the one without smoothing.
For pages that a user query pair hu, qi does not visit, the corresponding tensor value is zero.
An intuitive and straightforward smoothing method is to replace the zero tensor elements with a small constant c(0   c   1) .
That is, even a page p is not visited by hu, qi according to the click-through data, it is assumed that page p is in general visited by u with a small probability if u issues query q.
The second smoothing method is based on content similarities between Web pages.
For each user query pair hu, qi, a set of pages S1 are visited.
For each page p   S2 (S2 denotes pages not visited by hu, qi), an overall similarity between p and pages S1 can be calculated and used to replace the corresponding tensor elements: sim(p, S1) =
 a S1 s(p, a) , p   S2 (10) In Equation 10, s(p, a) measures the similarity between page p and a.
Here, each page is represented by a vector of word weight and the similarity between two pages is measured by cosine of the angle between the corresponding vectors: s(p, a) = j (11) where wpj denotes weight of term j in page p.
The two smoothing techniques, as well as no smoothing, are denoted by Smooth Constant, Smooth Content and Smooth None respectively.
 
   wpj   waj ||wp||   ||wa|| For the 2-dimensional case, when LSI is used for information retrieval, normalization scheme has a high impact on the retrieval precision [12].
Since the tensor A is of 3 dimensions, it can be normalized from any dimension and the experiment result may be di erent.
In this work, we compared all the three normalization methods.
For example, if the tensor is normalized from the user dimension, then for each user u, all the tensor values corresponding with u are devided by a constant and the tensor values sum to 1 after division, that is: X
 aiuiq ip = 1 (12) 1 iq n 1 ip k Normalization from query or Web page dimension is similar.
The three normalization methods are denoted by Normalize User, Normalize Query, Normalize Page respectively.
More is discussed in Section 5.4.2.
There is an ordering issue when the techniques discussed in Sections 4.3-4.6 are combined with the CubeSVD algorithm.
As discussed in Section 4.1, dimension selection is used in step 4 of the CubeSVD algorithm.
Since the weighting, smoothing and normalization techniuqes are used to construct a tensor from the clickthrouth data, they are applied in the  rst step of CubeSVD.
Similar with LSI applied in IR applications, the order of the three kinds of techniques used is: weighting, smoothing and normalization.
The weighting technique is  rst used to assign a value to the tensor elements associated with the hu, q, pi triples which occurred in the clickthrough data.
Next, the smoothing techniques are used to replace some empty elements of the tensor.
After smoothing is used, normalization is applied in order to regard objects of the same type with equal importance in the tensor construction.
For example, if the tensor is normalized from the user dimension, then each user is equally important for tensor construction, even though the number of queries each user issued or the number of pages each user visited may be di erent.
After the weighting, smoothing and normalization techniques are applied, the tensor construction (step 1 in Figure 3) is complete.
In this section, we introduce the experimental data set, our evaluation metrics, and the experiment results.
A set of MSN clickthrough was collected as our experimental data set.
This data set contains about 44.7 million records of 29 days from Dec 6 of 2003 to Jan 3 of 2004.
As we collected the clickthrough data, we crawled all Web pages of the ODP (http://dmoz.org/) directory (about 1.3 million).
The clickthrough data was split into two parts: a training and a test set.
The former comprises of the  rst two weeks of data collection.
The rest of the data is used for testing.
For the training data, unique items with same user, query and Web page are grouped into one entry and the frequency is summed up.
And we remove the Web pages which occurred in the clickthrough data but not crawled by our crawler.
After this processing step, the training data contains 19,644,518 entries having 3,676,296 users, 248,149 pages and 996,090 queries.
That is, among the 1.3 million ODP Web pages, 248,149 of them are clicked by Web users in the  rst 2 weeks.
Each user is identi ed by their IP address.
Figure 6: Performance of CubeSVD as the dimensions of the core tensor vary.
For the leftmost  gure, the user dimension is  xed at 115 and the other two dimensions change.
For the middle  gure, the query dimension is  xed at 144.
For the rightmost  gure, the page dimension is  xed at 112.
This is not appropriate sometimes when multi-users share one IP address or user accesses Web by dynamic IPs.
In other words, the Web search may be conducted by a group of users.
From the training dataset, we randomly select 500 users  clickthrough data and apply our CubeSVD algorithm on it.
The noise is reduced by removing the Web pages which were visited by no more than 3 times and users who visited no more than 3 pages.
Then we use these users  clickthrough data from the test set to evaluate the search performance.
In this work, we do not handle the new queries and new Web pages contained in the test set.
The SVDPACKC/las1 software package is used for SVD computation[5].
For comparison purpose, we also investigate whether the 3-order associations can be captured by the 2-dimensional SVD approaches.
We apply LSI on the huser, queryi-by-page matrix and use the reduced rank approximation of the original matrix for Web page prediction [22].
Besides, we also use the Collaborative Filtering algorithm in the experiments.
For CF, we apply the memory-based algorithm with the vector similarity measure to form neighbors (Refer to Equation (1) and (3) in [6]).
We evaluate the Web search accuracy of di erent algorithms using rank scoring metric [6].
The expected utility of a ranked list of items is de ned as Rs =  (s, j) 2(j 1)/( 1) (13)
 j where j is the rank of a Web page in the list recommended,  (s, j) is 1 if a huser, queryi pair s accessed page j in the test set and 0 otherwise, and   is set to 5 as the author did.
The  nal score re ects the utilities of all huser, queryi pairs in the test set:
 (14)
 s RsP s RM ax s s where RM ax is the maximum possible utility obtained when all pages that each huser, queryi pair has accessed appear at the top of the ranked list.
We implemented all the 4 weighting methods, 3 smoothing schemes and 3 normalization methods discussed in Section 4, which lead to 36 di erent settings.
In this work, we evaluated CubeSVD with all the settings.
We also compare CubeSVD with CF and LSI in our experiments.
In uence of the Core Tensor Dimensions We  rst conduct experiments to study the in uence of core tensor dimensions on the performance of our Cube-SVD algorithm.
When we apply CubeSVD to tensors constructed with di erent weighting, smoothing and normalization methods, all the results show the search accuracy has high dependency on dimensions of the core tensor.
For example, when we use Boolean weighting, normalization from query dimension without smoothing, we get a 500   168  
 matrix unfoldings are 235, 157 and 182 respectively after SVD is performed.
The CubeSVD algorithm achieves optimal accuracy (utility is 69.62) when the core tensor dimension is 115, 144 and 112 respectively.
If one dimension of the core tensor is  xed, we can  nd the search accuracy varies as the other two dimensions change, as illustrated in Figure 6: the vertical axis denotes the utility measure and the other two axes denote the corresponding dimensions.
For each  gure, one dimension is  xed and the other two dimensions are varied.
Each dimension increases in step (0.1   the corresponding highest dimension) and is measured with fraction.
We also employed our eigenvalue based method to determine dimensions of the core tensor.
The parameter   is varied from 0.1 to 1 in step 0.1.
For this experiment, when   = 0.9, we get a 211  141  163 dimension core tensor and the utility achieved is 68.6, which is approximate with the optimal result (utility 69.62).
In uence of Weighting, Smoothing and Normalization Methods According to our experiment results, we  nd normalization from query dimension is slightly better than normalization from user or page dimension.
Even when di erent weighting or smoothing techniques are used, this conclusion is consistent.
We give a group of experiment results in Figure 7, these results correspond with normalization from query dimension.
Di erent weighting and smoothing meth-
normalized from query dimension, associated with di erent weighting policies and smoothing schemes.
ods are used in this experiment.
We can  nd that the weighting policy may in uence the search results, especially when the log frequency weighting method is used.
The Boolean model performs worst compared with the other three weighting methods.
Out of our expectation, the Weight Log Freq-IDF weighting method is not so good as Weight Log Freq method, sometimes even worse than without weighting scheme (Weight Freq).
From Figure 7, we can also  nd that smoothing can improve the search accuracy.
Even the constant based smoothing method (c = 0.05 in this experiment) outperforms the one without smoothing.
The page similarity based smoothing approach is better than constant based smoothing.
We also conduct experiments to compare CubeSVD with LSI and CF.
In all the settings, CubeSVD outperforms both LSI and CF.
Figure 8 describes the results of the three algorithms with page similarity based smoothing and normalization from query dimension.
Results associated with the 4 weighting methods are plotted.
For LSI, the reduced dimension varies from 1 to the highest possible dimension (the matrix rank) and the best result is reported.
For CF, we vary the number of neighbors and report the best result.
According to the results, we can  nd CubeSVD outperforms either of the two baseline algorithms signi cantly.
From the experiments, we observe that CubeSVD achieves better search accuracy than CF and LSI.
The reason is CubeSVD can exploit the clickthrough data to capture the latent associations among the multi-type objects.
And this kind of high order associations can not be well captured by CF or LSI applied on the 2-dimensional matrix data.
We can also  nd that the core tensor dimensionality is crucial to the performance of CubeSVD.
Di erent weighting, smoothing and normalization methods also have impacts on the search accuracy.
According to the experimental results, the Weight Log Freq approach is the best weighting method.
When Inverse Document Frequency is used, the search result does not improve.
In our opinion, the reason is: there do not exist so many pages which are frequently visited by users with di erent interests.
Therefore, when IDF is used for weighting, the search accuracy even decreases.
Figure 8: Search Results of CF, LSI and CubeSVD.
Smoothing techniques can improve the search result.
Since the page content information is used, the page similarity based smoothing is better than constant based smoothing.
The e ect of similarity based smoothing for sparse data is also observed in [3].
By analyzing the CubeSVD algorithm illustrated in Figure 3, we can  nd that most time is consumed by steps 3-5.
In step 3, SVD is performed on the three unfolded matrices.
If the tensor scale is large, this step is quite time-consuming.
Especially if smoothing is used, the original sparse tensor becomes relatively dense and the scale of the SVD problem increases.
If no smoothing is used, there are many zero columns in the unfolded matrices which decrease the scale of the SVD problem.
Even though the large scale CubeSVD algorithm is quite time-consuming, the computation can be performed o ine beforehand.
After the CubeSVD analysis, the results can be used to help search Web pages in real time.
Because the preferences of each huser, queryi pair on Web pages have been computed in advance.
Thus the search results can be adapted to users according to the associations among Web pages, users and queries submitted.
Personalized Web search service will play an important role on the Web.
This paper focuses on utilizing click-through data to improve Web search.
A novel CubeSVD approach is proposed to deal with the clickthrough data which is three-way and highly sparse.
We used a real-world data set to evaluate the CubeSVD algorithm combined with a variety of techniques, examining the impact of di erent weighing, smoothing and normalization methods.
The experimental results indicate that CubeSVD approach can sig-ni cantly improve Web search performance.
There are also many areas for future research:
 whose clickthrough data was recorded.
And only queries issued and pages clicked on by these users are considered.
Therefore, it would be interesting to adapt our framework to newly emerged objects (new users, queries and Web pages).
One possible approach is by combining the CubeSVD technique with traditional content-based search model.
(cid:1)(cid:2)(cid:1)(cid:1)(cid:3)(cid:2)(cid:3)(cid:1)(cid:4)(cid:2)(cid:4)(cid:1)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:8)(cid:13)(cid:14)(cid:6)(cid:7)(cid:8)(cid:8)(cid:9)(cid:10)(cid:11)(cid:15)(cid:8)(cid:13)(cid:16)(cid:9)(cid:17)(cid:13)(cid:9)(cid:6)(cid:7)(cid:8)(cid:8)(cid:9)(cid:10)(cid:11)(cid:15)(cid:8)(cid:13)(cid:9)(cid:14)(cid:13)(cid:9)(cid:18)(cid:14)(cid:19)(cid:20)(cid:10)(cid:9)(cid:11)(cid:21)(cid:8)(cid:8)(cid:22)(cid:14)(cid:17)(cid:13)(cid:18)(cid:14)(cid:19)(cid:20)(cid:10)(cid:9)(cid:11)(cid:23)(cid:24)(cid:14)(cid:25)(cid:18)(cid:14)(cid:19)(cid:20)(cid:10)(cid:9)(cid:11)(cid:26)(cid:8)(cid:20)(cid:11)(cid:23)(cid:24)(cid:14)(cid:25)(cid:18)(cid:14)(cid:19)(cid:20)(cid:10)(cid:9)(cid:11)(cid:26)(cid:8)(cid:20)(cid:11)(cid:23)(cid:24)(cid:14)(cid:25)(cid:11)(cid:27)(cid:28)(cid:23)(cid:1)(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:2)(cid:6)(cid:2)(cid:7)(cid:2)(cid:8)(cid:2)(cid:9)(cid:2)(cid:10)(cid:11)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:13)(cid:18)(cid:19)(cid:11)(cid:20)(cid:21)(cid:16)(cid:17)(cid:13)(cid:18)(cid:19)(cid:11)(cid:20)(cid:21)(cid:16)(cid:17)(cid:13)(cid:18)(cid:21)(cid:22)(cid:23)(cid:16)(cid:24)(cid:16)(cid:19)(cid:25)(cid:22)(cid:24)(cid:26)(cid:27)(cid:13)(cid:25)(cid:28)(cid:23)(cid:1)2) The o ine computation of CubeSVD is quite time-consuming, especially when the clickthrough data contains a large number of objects.
With CubeSVD as a base approach, we will seek ways to improve its e ciency.
matically determine the optimal dimensionality of the core tensor.
not limited to Web search but is general enough and can be applied to other applications where three-way relations exist.
We thank Xue-Mei Jiang and Ya-Bin Kang for their help in preparing the data used in this work.
We also express thanks to Xuan-Hui Wang for his comments on this paper and helpful discussions.
