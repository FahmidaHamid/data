We face an era not only of an information explosion, but also a tremendous increase in the extent of our relations to other people.
We are constantly presented with new people names, chances to meet and communicate with people, and opportunities to add people to our social network in our work, from the media, and from our social and business use of the Internet.
It is now common that we do not actually meet (or even phone) our acquaintances; instead we communicate through email, chatrooms and discussion Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
forums.
We correspond with hundreds of people simultaneously.
Our social network is tens of times larger than that of our grandparents, and will likely grow more with time.
Even when we have trouble tracking all these connections, we (intentionally or unintentionally) add new ones.
We are in need of personalized tools that will help us manage our social network both to track people we know already, and also to tell us about new people we meet.
For example, when we receive email messages from people whose names we do not know, it would be useful to be able to rapidly search for any public facts about them.
This may help us know how to rate the importance of the message, or prioritize our e ort in making replies.
For example, a message from the head of an industrial research lab who works in your research area may warrant a higher priority than a corporate recruiter working for a company with little relation to you, even when the remainder of the body of the message is substantially similar.
A useful summary of public information about a person could often be gathered from the Web: news articles, corporate pages, university pages, discussion forums, etc.
contain a lot of information about people.
But how would the system identify whether certain Web pages are about the person in question or a di erent person with the same name?
Can we  nd not just a few pages, but a comprehensive set of pages?
For example, consider David Mulford,1 the US Ambassador to India.
When the query  David Mulford  is issued as a query to Google, most of the pages retrieved are actually related to the Ambassador; however, there are also two business managers, a musician, a student, a scientist, and a few others.
If we are looking for information about a particular person, we want to  lter out information about other namesakes, while also preserving the maximum amount of relevant information.
It is sometimes quite di cult to determine if a page is about a particular person or not.
In case of Ambassador David Mulford, much of the information that can be found at  rst may seem to be unrelated: one site states that in the late 1950s David attended Lawrence University and was a member of its athletic team; other sites mention his work at di erent positions in governmental departments and commercial structures, including Chairman International of Credit Suisse First Boston (CSFB) in London; a few sites (mostly in Spanish) relate his name to a  nancial scandal in Argentina.
It is a di cult challenge to automatically determine whether all of these sites discuss the same person.
scribed in Section 4.
In previous work [5] we addressed the problem of automatically populating a database of contact information of people in a user s social network.
Given a personal name extracted out of a user s mailbox, we queried Google in order to locate the person s homepage.
We then applied conditional random  elds [12] to extract institution, job title, address, phone, fax, email and other information from the homepage.
The main problem of our homepage  nding approach was that we used a simple heuristic for disambiguating person names, which sometimes failed.
So, in some cases we extracted the contact information of namesakes of people from the user s social network.
In this paper, we address the problem not simply of  nding homepages, but  nding all search engine hits corresponding to a person, and separating them from hits about namesakes.
We look beyond homepages because signi cant further information is often found elsewhere.
Moreover, the person s homepage may be old and abandoned, containing out-of-date information, and this may be discovered if we have a broader view on the person s Web appearances.
Rather than using simple heuristics, we present results with two statistical frameworks for addressing this problem: one based on link structure, and another based on the recently introduced multi-way distributional clustering method [3].
Furthermore, and crucially, rather than searching for people individually, we leverage an existing social network of people, or lists of people who are known to be somewhat connected, and use this extra information to aid the disambiguation.
We state the problem of Web presence identi cation as inferring a model that ultimately provides a function f answering whether or not Web page d refers to a particular person h, given a model M and background knowledge K.
Obviously, the perfect background knowledge K is in most cases unavailable, so the discrimination process must be made using some limited available information.
Note that given no background knowledge at all, the problem becomes ill-de ned: in order to automatically perform the task, the person h must have an electronic representation, which cannot be built without having any prior knowledge about the person.
The background knowledge K can be of various kinds.
For instance, K can include training data pages that are related or unrelated to the person.
In this case, the problem is reduced to a binary classi cation task that is widely addressed in the machine learning literature of the past decade (see, e.g., [11]).
However, in real-world situations, labeled examples are di cult and expensive to obtain.
Positive instances of a person s Web presence could possibly be obtained by making use of the person s email messages, but obtaining negative instances could be much more di cult.
In this paper, we employ unsupervised solutions.
The problem of disambiguating collections of Web appearances has been explored surprisingly little.
There has been much work on homepage  nding, starting from the early years of the Internet.
In 1997 Shakes et al. [17] launched AHOY!
 the  rst system for homepage  nding.
They primarily used heuristics and pattern matching for recognizing URLs of homepages.
Later on, standard IR techniques have been used for this task.
The TREC homepage  nding competition was held in 2002 (see, e.g., [1]).
The problem of person name disambiguation has been approached in the domain of research paper citations (see, e.g., [10]), with various supervised methods proposed for its solution.
There has been some research on person name disambiguation in the Web domain [2, 13, 7], within the general framework of entity coreference (see, e.g.
[16, 9]).
Ag-glomerative clustering has been applied in all three.
Bagga and Baldwin [2] use agglomerative clustering over traditional vector space models of text windows around a personal name mention.
Mann and Yarowsky [13] propose a richer document representation involving automatically extracted features.
Their clustering technique however can be basically used only for separating two people with the same name.
Recently, Fleischman and Hovy [7] construct a MaxEnt clas-si er to learn distances between documents that are then clustered.
This method needs to be provided with a large training set.
Note that these all use average-link clustering methods: the distance between data points and cluster centroids is considered, not the distance between individual data instances.
This lacks the bene ts of transitivity: if page d1 is related to the same person as page d2, while page d2 is related to the same person as page d3, then pages d1 and d3 are probably related to the same person, although the distance between them can be relatively large.
In this paper we propose two Web appearance disambiguation methods that also involve clustering, but are better adapted to our speci c task at hand.
The  rst method is based on the link structure of Web pages.
This method focuses on constructing only one cluster (of relevant pages), which nicely  ts into our binary framework.
The second technique employs Agglomerative/Conglomerative Double Clustering (A/CDC) an application of a new multi-way distributional clustering method [3], which does not directly compute distances between clusters.
The A/CDC objective can be also derived from the Multivariate Information Bottleneck (MIB) clustering principle [8].
In addition, we experiment with a hybrid approach combining the Link Structure and A/CDC methods.
All three of these methods outperform a baseline agglomerative clustering technique by more than 20% F-measure on a large real-world dataset.
In our attempts to use as little background knowledge as possible, we propose the following application scenario: given a group of people H = {h1, .
.
.
, hN} who are related to each other, we would like to identify the Web presence of all of them simultaneously.
Therefore, instead of solving one problem, we solve N interrelated problems: for each person hi in the group H we  nd Web pages that refer to hi.
Dealing with a group of people instead of dealing with an individual is not overly burdensome.
One can imagine many situations where a personal name is given within the context of people whom the person communicates with.
Examples include coauthors of a scienti c paper, participants in a newsgroup, or correspondents in a user s email.
Moreover, given a separate name without any additional information about the person, it is often fundamentally ambiguous to whom it refers.
But given a group of names of connected people, we can usually see to what group of people it refers, even if we do not know some of the names in the group.
For example, when searching for a person on the Web, one personal name is usually ambiguous.
Although Google  nds only one person named Ron Bekkerman, it  nds at least a dozen of unrelated people named Andrew McCallum.
However, if both names are provided to Google, pages that refer to only one of those Andrew McCallums will be retrieved.
Thus, as little background knowledge about the person as his or her membership in a group of people makes the Web appearance disambiguation problem feasible.
We will now describe our three proposed methods of solving the Web appearance disambiguation problem.
An important observation is that Web pages of a group of acquaintances are likely to be interconnected.
On the other hand, it is hard to imagine that pages of their namesakes would be interconnected.
Indeed, the namesakes probably have nothing in common, while the actual people of the group often tend to maintain homepages on the same domain (when they are colleagues), tend to refer to the same resources, and tend to be referred to from the same Web sites.
However, the existence of a direct hyperlink from one relevant page to another may be rare, so the term  intercon-nectedness  should be carefully de ned (see Section 3.1.1).
Meanwhile we de ne that two Web pages are linked to each other if their hyperlinks share something in common.
According to the problem statement in Section 2, we construct a function f that discriminates between relevant and irrelevant pages d for a person h with name th.
Our background knowledge K is a set of names TH = {th1 , .
.
.
, thN} in a group of N people in a user s social network.
Our set of Web pages D is constructed by providing a search engine with queries th1 , .
.
.
, thN and retrieving top K hits for each one of the query, so that N   K Web pages are retrieved overall.
Note that in this way every page d is already associated with a personal name thi : the name thi was in fact the query that retrieved page d. However, it is yet unknown whether the page d refers to the actual person h or to his/her namesake (or to neither).
We now construct our model M given the set of Web pages D.
Let graph GLS = (V, E) be the Link Structure Graph over a set of Web pages D if nodes of the graph are the Web pages (V   D) and there exists an edge between any pair of nodes di and dj i  di and dj are linked to each other.
In graph GLS linked Web pages compose connected components.
We naturally expect relevant pages to interconnect much more than irrelevant pages would interconnect.
Of special importance is that relevant pages that refer to di er-ent people are likely to interconnect, while irrelevant pages that refer to di erent people would probably not connect to each other.
We might decide that the Maximal Connected Component (MCC) of graph GLS consists of only relevant pages, so the MCC would be the  core  of our model.
However, there can be a case where the MCC consists only of Web pages retrieved in response to a single query this can happen when pages of one person h are heavily interconnected.
If this person h appears to be an irrelevant namesake of a relevant person, such MCC will be totally irrelevant.
Therefore, we come up with the following de nition: De nition 1.
Let us denote central cluster C0 as the largest connected component in GLS that consists of pages retrieved by more than one query.
Figure 1: Relevant and irrelevant Web pages according to the Link Structure model.
Relevant pages are within the  radius from the  central cluster .
White, gray and black colors indicate that the pages are retrieved by three di erent queries.
We denote other connected components in graph GLS as clusters C1, .
.
.
, CM , where M < N   K. We are now ready to de ne our link structure model: De nition 2.
The Link Structure Model MLS is a pair (C,  ), where C is the set of all connected components of the graph GLS (note that C0   C), and   is a distance threshold.
So, our discrimination function f is de ned as: (cid:40) f (d, h|M(K)) = if d   Ci : (cid:107)Ci   C0(cid:107) <  , i = 0..M otherwise

 (1) The intuition behind this de nition is that the pages of the central cluster and of a few clusters that are close to the central cluster are considered to be relevant, while others are irrelevant.
Figure 1 illustrates this intuition.
In the description of our Link Structure model we intentionally did not specify the following design choices:


 and Ci.
These are implementation details that can vary from system to system.
For example, two pages can be considered as linked if both contain a hyperlink to the same page, or both are hyperlinked from one page, or one page can be reached within three hyperlink hops from the other.
Different approaches can also be considered, for example, two pages are linked if both mention the same organization, e.g., University of Massachusetts at Amherst.
Similarly, the distance measure between two clusters can be di erent.
For example, it can be the cosine similarity or Kullback-Leibler divergence.
It can be learned using Max-Ent classi cation as proposed in [7, 15].
It can also be the distance not between clusters themselves, but between their closest elements.
In this case the discrimination function f would be rede ned as (cid:40) f (d, h) = if d   Ci :  di   Ci  dj   C0 (cid:107)di   dj(cid:107) <   otherwise.
(2) In our experimental setup we have made the following design choices:
 consider the hyperlink structure of the pages.
Since the full URLs of the hyperlinks seem to be too spe-ci c, while the URL domains seem to be too general, we de ne a function url(d) to output the domain of the d s URL with its  rst directory in case this directory exists.
For example.
given page d1 with URL http://www.cs.umass.edu/~ronb/timeline.html the function url(d1) will return www.cs.umass.edu/~ronb.
Given page d2 with URL http://www.cs.umass.edu/ the function url(d2) will return www.cs.umass.edu.
For the reminder of the section, by URL we mean the output of url(d).
We de ne the set P OP to be a set of URLs of extremely popular domains, such as www.amazon.com.
The popularity of a domain can be determined using operator :link of the Google command line.
We de ne the set T R(D) of trusted URLs as {url(di)} \ P OP .
We also de ne the function links(d) that given page d returns a set of URLs that occur in d.
De nition 3.
The link structure LS(d) of a page d is de ned as LS(d) = (links(d)   T R(D))   url(d).
So, the link structure of a page is its own URL and its hyperlinks given that they appear as URLs of other pages in the dataset.
By this we minimize undesirable hazards that can occur if a page contains too many hyperlinks, pretending to be a hub.
De nition 4.
Two pages d1 and d2 are linked to each other if their link structures intersect, that is LS(d1)  LS(d2) (cid:54)=  .
tance threshold  .
Instead, we set it so that one third of the pages in the dataset are within the threshold.
cosine similarity with a novel variation of the t df term weighting function: tf idf (w) = tf (w) log google df (w) , (3) where google df (w) is the estimated total results count of the term w if being provided as a query to Google.
This document frequency count seems to be the most adequate measurement of the commonness of the term.
The estimated total results counts of words in our dataset were obtained using Google API.2
 Double Clustering (A/CDC) Model The problem of Web appearance disambiguation can be addressed within the standard clustering framework: the set of Web pages D is split into M clusters, then one of the clusters is considered as containing only relevant pages while all the other clusters are irrelevant.
The decision about which one of the M clusters is the relevant one can be made based on either internal or external information.
An internal resource might be the measure of interconnectedness of the clusters, in the sense of the discussion in Section 3.1.
The most interconnected cluster is then chosen as relevant.
An external resource can be, e.g., email messages from all or some people in the group: the distance between the set of messages and each one of the clusters is computed, then the closest cluster is chosen.
Since we intend to minimize the background knowledge about the people, we adopt the former technique.
So, our Clustering Model MCL is a pair (C, L( )), where C is the set of clusters of documents in D, and L( )) is the interconnectedness measure of a cluster.
Then the discrimination function f is de ned as follows: if d   C  : C  = arg maxi=1..M L(Ci) otherwise f (d, h|M(K)) =

 (cid:40) (4) As our particular clustering method, we apply the A/CDC algorithm an instance of the new multi-way distributional clustering (MDC) method we propose in [3].
The main idea of A/CDC is to employ the fact that similar documents have similar distributions over words, while similar words are similarly distributed over documents.
Starting with one cluster containing all words and many clusters with one document each, we iteratively split word clusters and merge document clusters, while conditioning one clustering system on the other, until meaningful clusters are obtained.
This method has demonstrated high performance on various datasets including the benchmark 20 Newsgroups.
Multi-way distributional clustering stands in close correspondence with the Multivariate Information Bottleneck (MIB) method.
The A/CDC algorithm, while being the simplest MDC application, can also be derived from MIB, which will be shown in this section.
We  rst provide some background on related Information Bottleneck methods, then discuss motivation of the A/CDC approach and overview the A/CDC algorithm.
The Information Bottleneck (IB) method [21] is a convenient information-theoretic framework for solving various real-world problems, especially clustering.
It has been widely applied in Information Retrieval [20, 4, 18].
The main idea that lies behind the IB clustering is in constructing an assignment of data points X into clusters  X that will maximize information about entities Y that are interdependent with X.
The information about Y gained from  X is represented in terms of Mutual Information:
 P (  X, Y ) log

 .
(5) (cid:88)
 2http://www.google.com/apis/ A natural constraint is imposed on the Mutual Information between data instances X and their clusters  X: it penalizes Mutual Information I(  X; X) from being too large because otherwise the clustering will tend to be degenerative (each instance will form a cluster).
This constraint is referred to as the compression constraint.
Thus, the Information Bottleneck problem is stated as I(  X; Y )    I(  X; X), arg max
 (6) where   is a Lagrange multiplier.
Many applications and extensions of the original IB method have been proposed.
Some relevant results are listed below.
Slonim and Tishby [19] propose a greedy agglomerative algorithm for document clustering based on the Information Bottleneck method, where X stands for documents and Y stands for words in the documents.
This simple algorithm achieves surprisingly good results but is computationally expensive.
Slonim et al. [18] propose a greedy sequential IB clustering algorithm based on local optimization that demonstrates incredibly high performance and is computationally e cient in practice.
Slonim and Tishby [20] notice that the IB method is symmetric in X and Y .
They propose a double clustering technique in which words are  rst clustered with respect to documents and documents are then clustered with respect to clusters of words.
El-Yaniv and Souroujon [6] propose an incremental version of this method that signi cantly improves its performance.
Friedman et al. [8] propose the Multivariate Information Bottleneck (MIB) framework: they consider clustering instances of a set of variables X = (X1, .
.
.
, Xn) into a set of clustering systems  X = (  X1, .
.
.
,  Xn).
After generalizing the standard bivariate Mutual Information I(X; Y ) to an n-variate Multi-Information I(X), Friedman et al. reformulate the Information Bottleneck principle as computing I Gout    I Gin , arg max
 (7) where Gin and Gout are graphical models over (X,  X) that best describe the dependencies between X and  X in the input space and in the output space respectively.
The double clustering problem thus becomes a partial case of MIB and can be derived as (cid:179) (cid:180) Figure 2: A/CDC procedure.
At each iteration black clusters are split and then white clusters are merged.
least two frameworks are ready for this task: agglomerative (bottom-up) and conglomerative (top-down) clustering.
We basically have three possibilities for performing the double clustering: we can use a top-down clustering scheme for both, we can cluster both by a bottom-up scheme, or we can apply a top-down scheme to one of the two clustering systems, while applying a bottom-up scheme to another one.
Two top-down schemes are clearly a bad choice, because in the top-down scheme we start with one cluster that contains all the instances, and if both systems start with one cluster, then conditioning one on the other will lead to a completely random split.
Two bottom-up schemes are also a bad choice, because of the computational issues: at the initial stages the two clustering systems are so large that the calculation of the Mutual Information I(  X;  Y ) can be infeasible.
We are left with top-down clustering in one system and In this case, iterative bottom-up clustering in the other.
splits and merges (when one clustering system is conditioned in another) cause the e ect that the two clustering systems  bootstrap  each other.
Thus, the A/CDC method is the simultaneous clustering of X by a top-down scheme and Y by a bottom-up scheme, while applying the objective function from Equation 9.
Figure 2 visualizes the A/CDC procedure.
I(  X;  Y )     arg max

 , (8)
 where I(  X; X) and I(  Y ; Y ) are the compression constraints.
In the hard clustering variation of the IB method we set the Lagrange multiplier   to zero (see, e.g., [19]).
Since we cannot just omit the compression constraints this way, a decent substitute would be to  x the number of (hard) clusters.
The double clustering objective is then derived from Equation (8) as I(  X;  Y ), subject to |  X| = N  X ,|  Y | = N  Y , (9) arg max
 where |  X| and |  Y | are sizes of the clustering systems  X and  Y respectively.
Since determining the  good  number of clusters is a hard problem, we cannot a priori be satis ed with  xed sizes N  X and N  Y .
Our intention is to explore di erent possibilities while employing the hierarchical structure of the clusters.
At Following El-Yaniv and Souroujon [6], we break Equation (9) down to two parts: arg max
 I(  X;  Y ), arg max

 (10) At each iteration of our algorithm we attempt to  rst build the best clustering system  X and then build the best clustering system  Y .
We initiate the two clustering systems with one cluster  x that contains all data points x, and one data point yi per each cluster  yi.
We then calculate the initial Mutual Information I(  X;  Y ).
At each iteration of the algorithm, we perform four operations:
 dom to two equally sized parts.
gorithm proposed by Slonim et al. [18]: we pick each data point xj out of its cluster and place it sequentially 3210into each one of the other clusters, while attempting to maximize I(  X;  Y ).
We  nally place the data point xj into a cluster  xi such that I(  X;  Y ) is maximal.
We perform this procedure twice in order to closer approach the local maximum of our objective.
cluster  yi, and  nd its best mate while applying a criterion for minimizing Bayes classi cation error that was proposed in [19].
sequential pass as in Step 2 over all data points yj.
Following Slonim et al. [18], in order to get closer to the global maximum of our objective function, at each iteration we perform a number of random restarts of Steps 1-2 and then of Steps 3-4.
We also e ciently cache slices of the Mutual Information I(  X;  Y ) so that it should not be entirely recalculated during the sequential passes.
The computational complexity of our algorithm is O(NxNy log Ny), where Nx and Ny are sizes of X and Y respectively.
In the case of Web appearance disambiguation, we use the top-down scheme for clustering words and the bottom-up scheme for clustering documents.
We continue the process until we have three document clusters (one of which is then chosen to be the class of relevant pages).
Since in both solutions of the Web appearance disambiguation problem (Link Structure method and the A/CDC method) we build one group of relevant Web pages, we can attempt to overlap the groups built by the two methods.
At one of the iterations of the A/CDC clustering we choose the most interconnected cluster C  of the size that is roughly correspondent to the size of the central cluster C0.
Then we compose a new central cluster C  0 by uniting all the con nected components that overlap with C : (cid:91) Ci C (cid:54)= , i=0..M Ci (11) After that, our discrimination function f is very similar to the discrimination function from Equation (1): f (d, h|M(K)) = if d   Ci : (cid:107)Ci   C  otherwise


 (12) This method gives us a larger but still clean central cluster which leads to more accurate choice of clusters within the   radius from C 


 For evaluation of our methods, we have gathered and labeled a dataset of 1085 Web pages.
In this section we describe the dataset and provide some interesting insights into its structure.
In a collaborative e ort to create publicly available email datasets, participants in CALO project [14] are encouraged to collect and folder their correspondence on CALO-related topics.
From the Feb 2, 2004 snapshot of this data, we selected one folder from Melinda Gervasio s email directory and extracted 12 person names that appeared in headers of messages found in this folder.
The names are primarily of  

 (cid:40) Personal name Position Num Num of Num of relevant of categories SRI Manag CMU Prof SRI Eng SRI Manag MIT Prof Adam Cheyer William Cohen Steve Hardt David Israel Leslie Pack Kaelbling Bill Mark Andrew McCallum Tom Mitchell CMU Prof David Mulford Andrew Ng Fernando Pereira Lynn Voss Stanford Undergrad Stanf Prof UPenn Prof SRI Eng
 SRI Manag UMass Prof pages












 pages

























 Table 1: Statistics of our dataset.
Categories are di erent namesakes or other in case the page does not refer to any of the namesakes.
SRI employees and professors from di erent universities.
All of the individuals are likely to be present on the Web.
These 12 names (taken in quotation marks) were then issued as queries to Google and for each query the  rst 100 pages were retrieved.
We manually  ltered the pages, removing pages in non-textual formats, HTTPD error pages and empty pages.
We labeled the remaining pages by the occupation of the individuals whose name appeared in the query.
In 10 out of 12 cases, the names were heavily ambiguous, thus pages representing 187 di erent people were retrieved given the 12 names of people in Melinda s social network.
In some cases, it was di cult to decide to which of the namesakes the page refers.
To determine this, we often performed manual Web investigations.
Table 1 shows some statistics of the dataset.
Finally, all the pages were cleaned of their HTML markup and scripts.
All the URLs mentioned in the pages were extracted and placed at the end of each page, together with the URL of the page itself.
The dataset is publicly available at http://www.cs.umass.edu/ ronb.
The most ambiguous personal name among the twelve is Tom Mitchell.
Although the CMU Professor s pages are prevalent over all the others, 37 di erent Tom Mitchells can be distinguished in the 100  rst Google hits, including professors in di erent  elds, musicians, executive managers, an astrologist, a hacker and a rabbi.
Two personal names out of the 12, Adam Cheyer and Leslie Pack Kaelbling, seem to be unique in the Internet.
However, for either of them, one page was retrieved that did not contain any part of their names.
These two pages were put into respective categories other.
Two other people, David Mulford and Lynn Voss, seem to have very little Web presence.
Only one page out of the 100 was related to any of the two.
William Cohen s and David Mulford s namesakes are well known politicians: the former Secretary of Defense William S. Cohen and the current US Ambassador to India David C. Mulford.
Naturally, Method Agglomerative Link Structure

 Precision



 Recall



 F-measure



 Table 2: Web appearance disambiguation results.
A/CDC results are averaged over 4 random restarts.
the distributions of Cohen s and Mulford s pages are heavily biased toward the politicians who are well represented on the Web.
An interesting phenomenon is observed for the names David Israel and Bill Mark.
Many of pages that responded to these queries only accidently contain the two words adjacent to each other: Bill Mark s pages often refer to markups of certain bills, or just list people s  rst names (e.g.
 Thanks Bill, Mark! ), while some of David Israel s pages discuss Israeli history and King David.
None of these pages were removed from the dataset, despite the fact that they are clearly unrelated to a particular living person.
A real challenge for any Web presence  nding system is the pages of Bill Mark and Fernando Pereira.
Both researchers have namesakes who are also researchers in Computer Science: another Bill Mark is a UTexas Professor, while another Fernando Pereira is a Professor at Instituto Superior T ecnico in Portugal.
We term these pairs  doubles .
To separate them is an especially di cult task.
The opposite problem occurs with Steve Hardt: he appears on the Web not only as an SRI engineer, but also as a creator of an online game.
We ourselves are actually unsure whether this is one person or two di erent people, but most likely this is one person.
Given the class of relevant documents obtained by one of our models, our evaluation method computes precision and recall of the class with respect to the true labels in our dataset.
We then compute the F-measure by averaging precision and recall.
Our goal is to maximize the F-measure; however, we also consider separate precision and recall measures, because various real-world scenarios may prefer one over another.
As our baseline method, we implemented greedy agglom-erative clustering (as applied in the related work [2, 13, 7]), based on the cosine similarity measure between clusters and the augmented t df weighting function from Equation (3).
We did not measure interconnectedness of the clusters, we simply chose the cluster whose F-measure was the highest among all the clusters.
The motivation for this choice was that we would like to show that our methods overcome the best possible results of the baseline method.
The summary of the results is in Table 2.
As it can be seen from the table, the results of the three proposed methods are quite close to each other, while the hybrid method nicely improves the recall (and then the F-measure).
The relatively high deviation in precision and recall of the A/CDC method is caused by the fact that it never ends up with clusters of the exactly same size.
Interestingly, this almost does not a ect the F-measure: the precision trades o  quite well against the recall.
Name Found correct Not Found found wrong Adam Cheyer William Cohen Steve Hardt David Israel Leslie Pack Kaelbling Bill Mark Andrew McCallum Tom Mitchell David Mulford Andrew Ng Fernando Pereira Lynn Voss







































 Table 3: Results by person of the LS+A/CDC hybrid model.
Table 3 collates the results by person, as achieved by the hybrid model.
For quite a few people both precision and recall are amazingly high, e.g.
for David Israel, Leslie Pack Kaelbling, Andrew McCallum, Andrew Ng.
It is also noticeable that the only relevant page of David Mulford (the Stanford student) is found.
As could be anticipated, the worst precision is for Bill Mark and and Fernando Pereira, because both of them have  doubles .
However, only 9 of
 pear in the category of relevant pages.
The worst recall is for Steve Hardt and Adam Cheyer.
This can be easily explained for Steve: most of his pages refer to an online game he created relevance of these pages would be too di cult to determine.
As for Adam, the low result is a bit surprising, but it still makes sense: Adam s name often appears in an industrial context, while the language of most correctly-found pages is purely academic many of Adam s pages fall too far from the central cluster.
Unfortunately, the single relevant page about Lynn Voss was not found, probably for the same reason: it uses an industrial vocabulary.
The problem of disambiguating the  doubles the two Bill Marks and two Fernando Pereiras who all work in Computer Science can in fact be handled within the A/CDC framework.
At some intermediate stages during the course of the A/CDC algorithm the most interconnected cluster is relatively small but extremely clean.
Figure 3 shows the pre-cision/recall curve for one run of the A/CDC algorithm.
It can be seen in the graph that when the recall of the relevant cluster is around 45% (there are  ve clusters overall), the precision is very high (above 98%).3 This cluster contains two pages of Bill Mark the SRI Manager and none of the pages of Bill Mark the UTexas Professor; it also contains 15 pages of Fernando Pereira the UPenn Professor and only one page of Fernando Pereira the Professor of Instituto Superior T ecnico.
This result shows that when our algorithm is stopped with 5, 9 or 17 clusters, rather than with three clusters, its perfor- mance is still very reasonable, at least in terms of precision.
Constructing clustering systems with all possible granularity levels is an important feature of the A/CDC algorithm.
all), we obtain 100% precision.
model.
In Proceedings of COLING-ACL-17, pages
 [3] R. Bekkerman, R. El-Yaniv, and A. McCallum.
Multi-way distributional clustering via pairwise interactions.
Submitted.
[4] R. Bekkerman, R. El-Yaniv, N. Tishby, and Y.
Winter.
On feature distributional clustering for text categorization.
In Proceedings of SIGIR-24, pages
 [5] A. Culotta, R. Bekkerman, and A. McCallum.
Extracting social networks and contact information from email and the web.
In Proceedings of CEAS-1,
 [6] R. El-Yaniv and O. Souroujon.
Iterative double clustering for unsupervised and semi-supervised learning.
In Proceedings of NIPS-14, 2002.
[7] M. B. Fleischman and E. Hovy.
Multi-document person name resolution.
In Proceedings of ACL-42, Reference Resolution Workshop, 2004.
[8] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby.
Multivariate information bottleneck.
In Proceedings of
 [9] C. H. Gooi and J. Allan.
Cross-document coreference on a large scale corpus.
In Proceedings of
 [10] H. Han, L. Giles, H. Zha, C. Li, and K. Tsioutsiouliklis.
Two supervised learning approaches for name disambiguation in author citations.
In Proceedings of JCDL-4, 2004.
[11] T. Joachims.
Learning to Classify Text using Support Vector Machines.
Kluwer Academic Publishers, Dordrecht, NL, 2002.
[12] J. La erty, A. McCallum, and F. Pereira.
Conditional random  elds: Probabilistic models for segmenting and labeling sequence data.
In Proceedings of ICML-18, pages 282 289, 2001.
[13] G. S. Mann and D. Yarowsky.
Unsupervised personal name disambiguation.
In Proceedings of CoNLL-7, pages 33 40, 2003.
[14] W. Mark and R. Perrault.
CALO: a cognitive agent that learns and organizes.
https://www.calo.sri.com.
[15] A. McCallum and B. Wellner.
Conditional models of identity uncertainty with application to noun coreference.
In Proceedings of NIPS-17, 2005.
[16] V. Ng and C. Cardie.
Improving machine learning approaches to coreference resolution.
In Proceedings of
 [17] J. Shakes, M. Langheinrich, and O. Etzioni.
Dynamic reference sifting: a case study in the homepage domain.
In Proceedings of WWW-6, 1997.
[18] N. Slonim, N. Friedman, and N. Tishby.
Unsupervised document classi cation using sequential information maximization.
In Proceedings of SIGIR-25, 2002.
[19] N. Slonim and N. Tishby.
Agglomerative information bottleneck.
In Proceedings of NIPS-12, 2000.
[20] N. Slonim and N. Tishby.
Document clustering using word clusters via the information bottleneck method.
In Proceedings of SIGIR-23, pages 208 215, 2000.
[21] N. Tishby, F. Pereira, and W. Bialek.
The information bottleneck method, 1999.
Invited paper to the 37th annual Allerton Conference.
Figure 3: Precision/recall curve of the A/CDC algorithm.
Points correspond to consequent iterations of the algorithm (merges of Web page clusters).
Another interesting result is that our methods can also be applied to the problem of homepage  nding.
Among the 12 people in our dataset ten have homepages (David Mulford and Lynn Voss do not maintain homepages), and nine of the ten homepages are inside the class of relevant documents found by the LS+A/CDC hybrid method.
The only homepage the system does not  nd is the homepage of Steve Hardt.
This paper is the  rst attempt to approach the problem of  nding Web appearances of a group of people.
We have proposed two relatively straightforward statistical methods for solving this problem.
Both methods are purely unsupervised  they involve minimum of prior knowledge about the people.
Essentially, only the a liation of a person with the group is all the information required.
For evaluation purposes we built a large annotated dataset that is publicly available to the scienti c community.
Both methods demonstrate high performance on this dataset.
The methods are general enough to allow large variety of implementations and extensions.
We are now working on more sophisticated probabilistic models for solving this problem that would capture the relational structure of the class of relevant pages.
For example, pages that are retrieved by queries with pairs of names can signi cantly enrich the model.
The problem of Web appearance disambiguation is novel and poses a lot of exciting challenges.
We thank Ran El-Yaniv for many fruitful discussions.
This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract number NBCHD030010.
Any opinions,  ndings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily re ect those of the sponsor.
