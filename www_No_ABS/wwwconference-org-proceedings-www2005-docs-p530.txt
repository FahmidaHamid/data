There exists an increasing body of work on automated service selection, based on criteria such as quality of service (QoS), trust, cost, etc.
Some works focus on service matchmaking [26, 20, 2, 19, 28, 11, 3, 4, 21, 12, 13], that is, a process that given a service request returns the set of available services that can be used to ful(cid:12)ll that request (o(cid:11)ers may be ranked according to their similarity to the request).
Some other papers focus on modelling nonfunctional properties such as the above criteria, that induce preference orderings on the available services [16, 5, 17].
However, no paper tackles in depth the optimization problem that follows matchmaking and nonfunctional property (cid:3)Partially supported by the EU FP6 Network of Excellence
 Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Universit`a di Napoli FEDERICO II P. Festa Napoli, Italy paola.festa@unina.it evaluation: What is the best way of binding each service request to a matching service?
The problem may be nontrivial if the optimization involves multiple service requests at once.
Consider for example composite services; they can be modelled as work(cid:13)ows [6, 12], where each activity potentially corresponds to a di(cid:11)erent service.
In this framework, the decision problem consists in (cid:12)nding an optimal matching (w.r.t.
the adopted criteria) between the set of activities occurring in the work(cid:13)ow and the set of available services that can be used to carry out those activities.
In this paper we consider optimal service selection based on a given set of service requests (such as the activities occurring in a work(cid:13)ow), a set of service o(cid:11)ers (the available services), the result of the matchmaking process (that associates each request to the set of o(cid:11)ers that can satisfy it), and a numeric preference measure.
Numeric measures are well-suited to a number of preference criteria of practical interest, based on costs of various sorts, as well as bandwidth, trust [1, 30, 24, 29], and other QoS criteria.
Moreover, different criteria can often be merged into a single numerical value [5].
Preferences and costs may be associated to services, service invocations, or both, as illustrated by the following examples: (cid:15) Trust is often associated to services, not service invocations.
User preferences driven by privacy protection and security usually refer to services, independently of the speci(cid:12)c call.
(cid:15) However, an information service may be trusted on some queries and not on others; in this case trust is associated to individual invocations.
(cid:15) Some services have an activation cost or a registration cost, to be paid only the (cid:12)rst time the service is invoked, or before the (cid:12)rst use.
Such costs are associated to the services and do not depend on the number of calls nor on their nature.
(cid:15) Other services work on a pay-per-use basis (such as paper downloads from a digital library and other electronic purchases).
In this case, costs and preferences may depend on the speci(cid:12)c request and are associated to each service invocation.
Some services have both a peruse cost and an activation cost (such as telephone providers).
(cid:15) Also bandwidth and transmission speed may vary across di(cid:11)erent service calls.
For example a service may be faster at certain times of the day.
Another example is given by connection costs that depend on the duration of each particular call.
In this paper we shall contribute to the understanding of the service selection problem (SSP, for short) by formalizing and studying three classes of SSP problems where selection is based on costs and on two di(cid:11)erent QoS-like criteria, respectively.
For simplicity, in this paper we assume that costs and preferences are totally ordered and static (i.e., time independent); partially ordered and dynamic nonfunctional properties will be dealt with in a forthcoming paper.
We shall prove that in general|and despite the aforementioned simplifying assumptions|the optimal service selection problem is harder than NP (unless the polynomial hierarchy collapses).
More precisely, some SSPs are in FPNP, like many famous hard optimization problems, while checking whether the optimal cost equals a given constant K is DP-complete.
We shall identify practical cases where the problem can be solved in polynomial time.
In particular, we show that the high computational complexity of the service selection problem is caused by the onetime costs associated to service o(cid:11)ers (e.g., initialization and registration costs).
In the absence of onetime costs, the optimal selection problem can be solved in polynomial time by applying a greedy approach.
Finally, we shall illustrate the results of an experimental evaluation of both exact and heuristic algorithms over di(cid:11)erent classes of problem instances.
The paper is organized as follows.
In Section 2 we recall the de(cid:12)nition of the complexity classes needed in this paper.
In Section 3 the service selection problems are formalized.
Section 4 contains the complexity results and the algorithms for the (cid:12)rst class of SSPs (based on cost-like criteria), and reports the experimental results for these algorithms.
Section 5 illustrates the complexity results and the algorithms for the remaining two classes of SSPs (based on QoS-like criteria).
Section 6 concludes the paper with a discussion of the results and a list of interesting directions for future work.
We assume the reader to be familiar with the basics of computational complexity.
We refer to [22] for more details.
The class DP is a class of decision problems containing NP.
DP can be de(cid:12)ned as the class of all languages L such that L = L1 \L2, for some L1 in NP and some L2 in co-NP.
If L1 and L2 are complete for NP and co-NP, respectively, then L is complete for DP.
The class FPNP is the class of all function problems (i.e.
problems that compute a value, not only a yes-no answer) that can be solved in polynomial time by a deterministic Turing machine with an oracle for NP.
Many standard optimization problems are complete for FPNP.
For example, the Traveling Salesman Problem and Max-weight SAT are FPNP-complete [22, Chapter 17.1].
The instances of the service selection problems (SSP) addressed in this paper are tuples hR; O; M; c; ki where: (cid:15) R = f1; 2; : : : ; mg is a nonempty set of service requests; (cid:15) O = f1; 2; : : : ; ng is a nonempty set of service o(cid:11)ers; (cid:15) M (cid:18) R (cid:2) O is a matching between requests and o(cid:11)ers such that
 (1) (intuitively, if some request cannot be satis(cid:12)ed then the decision phase is never reached); (cid:15) c : O !
Q is a function that assigns a cost or quality measure cs to each o(cid:11)er s 2 O; (cid:15) k : M !
Q is a function that assigns a cost or quality measure krs 2 Q to each pair hr; si 2 M (that is, to each possible service call).
The goal is (cid:12)nding a binding between requests and o(cid:11)ers, compatible with the given matching and optimal w.r.t.
the preferences associated to services and invocations.
Formally, a binding for hR; O; M; c; ki is a total function b : R !
O such that b (cid:18) M .1 Condition (1) on M ensures that a binding always exists.
As anticipated in the introduction, in this paper the opti-mality of bindings will be evaluated against di(cid:11)erent objective functions.
The (cid:12)rst objective function, denoted by Cb, is appropriate for criteria based on totally ordered costs (money, time, etc.
)2 The overall cost of a binding is obtained by summing up the costs of all the calls speci(cid:12)ed in the binding, plus the onetime costs associated to the called services (e.g., initialization and registration costs).
More precisely, let b[R] = fs 2 O j 9r 2 R: b(r) = sg denote the range of b (informally speaking, b[R] is the set of services \used" by b); then the total cost of binding b is given by Cb = X kr b(r) + X cs : r2R s2b[R] (2) The second objective function, denoted by Qb, is appropriate for many QoS-like criteria.
Suppose that the aim of the optimization problem, in this case, is maximizing simultaneously the quality of each requested service.
Then the overall quality of a binding b can be modelled by summing up the qualities of each selected request-o(cid:11)er match: Qb = X f (kr b(r); cb(r)) : r2R (3) Here f : Q2 !
Q computes the quality of the solution provided by the selected service b(r) to request r by appropriately combining the measure kr b(r) associated to the service call and the measure cb(r) associated to the service.
We assume only that f can be computed in polynomial time (w.r.t.
the given instance), because di(cid:11)erent applications may require di(cid:11)erent functions f .
For example, suppose r is satis(cid:12)ed by invoking b(r) via a network connection.
Packet rate is in(cid:13)uenced both by the server s speed and by the bandwidth allowed by the intermediate routers; the lowest rate determines the overall rate for the connection.
Suppose the values kij measures the


 costs, and for multidimensional costs with a total preference over dimensions (e.g., money over time, and so on).
packet rate allowed by the connection between i and j, and the values cj measure the packet rate of the servers; then it is appropriate to set f = min.
For another example, suppose the values kij measure the quality of the connections between i and j, and the values cj measure the level of trust in the information released by service j.
Then a service b(r) may be preferred because (i) the quality of the connection is good, and at the same time (ii) the level of trust in b(r) is high.
In this case f = min does not seem adequate; it is not sensitive to any increment of the maximal argument, therefore it does not forces simultaneous improvement of the two values kij and cj .
A function more sensitive to both of its parameters seems more appropriate (e.g., one may adopt f = + or f = (cid:2)).
The third objective function, denoted by Q0 b, is appropriate for QoS-like criteria, too.
Sometimes, in a compound service, the quality of the worst component service a(cid:11)ects the quality of the entire service.
For example, the overall privacy preservation degree of a compound service issuing a set of requests R, is determined by the minimal privacy preservation degree of the service components (i.e. the individual invocations b(r)).
In this kind of scenario, the quality estimates f (kr b(r); cb(r)) are combined by taking their min-imum:
 b = minff (kr b(r); cb(r)) j r 2 Rg : The three objective functions Cb, Qb, and Q0 (4) b induce three classes of SSP: SSPC : Given a SSP instance I, (cid:12)nd a binding b for I that minimizes the cost function Cb.
SSPQ: Given a SSP instance I, (cid:12)nd a binding b for I that maximizes the quality function Qb.
Q: Given a SSP instance I, (cid:12)nd a binding b for I that maximizes the quality function Q0 b.
The last two problems, Qb and Q0 b, are not much di(cid:11)erent from each other, as stated by the following result: Theorem 3.1.
For each SSP instance I,
 I under SSP0

 Q is also a solution of I under SSPQ.
Intuitively, the reason is that SSP0 Q considers only the bottlenecks, while SSPQ tries to improve all services.
We prove that SSPC is NP-hard by reduction from the Uncapacitated Facility Location Problem (UFLP), which is de(cid:12)ned as follows.
We are given a bipartite graph (F; C) with set of n facilities F and m cities C. Let fj represent the cost of opening a facility at location j in F , and cij represent the cost of serving city i from an open facility j.
The goal is to (cid:12)nd a subset I of F along with an assignment function (cid:8) : C !
I to assign the cities such that the total cost is minimized.
There is a great variety of types of facility location problems depending on the features of the components that contribute in the model de(cid:12)nition.
Some basic classes of facility location problems are listed below.
a facility can serve, then the corresponding problem is classi(cid:12)ed as a Capacitated Facility Location Problem.
the problem is considered to be stochastic; otherwise it is referred to as a deterministic one.
location of the facilities to be open but also with \the moment" of their opening, then the corresponding problem is called a Dynamic Facility Location Problem; otherwise it is called a Static Facility Location Problem.
The uncapacitated facility location problem we need in this paper is static and deterministic and admits the following integer programming formulation.
(UFLP) min m n

 i=1 j=1 cij xij + n
 j=1 fj yj s:t: n
 xij = 1; 1 (cid:20) i (cid:20) m j=1 yj (cid:0) xij (cid:21) 0; 1 (cid:20) i (cid:20) m; 1 (cid:20) j (cid:20) n xij ; yj 2 f0; 1g 1 (cid:20) i (cid:20) m; 1 (cid:20) j (cid:20) n; (a) (b) where constraints (a) impose that each city is assigned to at least one facility, while constraints (b) restrict assignments to open facilities only.
Despite their simple formulation, most location problems are very di(cid:14)cult to solve.
Except for some special cases, their decision version (For a given K, is there a solution with cost (cid:20) K? )
have been shown to be NP-hard by reduction from the Vertex Cover Problem (membership in NP is straightforward).
An extensive survey of location problems, their complexities and applications can be found in the book edited by Mirchandani and Francis [18].
By setting R = C, O = F , cj = fj, and kij = cij (1 (cid:20) i (cid:20) m, 1 (cid:20) j (cid:20) n), UFLP can be reduced to SSPC, and viceversa.
Then, we can prove the following result: Proposition 4.1.
Deciding optimal cost of a given instance of SSPC is less than or equals a given rational K is NP-complete.
whether the With this result, we can express the optimality check as the conjunction of an NP-complete test and a co-NP-complete test, so we get the following theorem.
Theorem 4.2.
Deciding whether the optimal cost of a given instance of SSPC equals a given rational K is DP-complete.
The optimal cost can be computed through a binary search of K, based (by the above proposition) on an oracle for NP.
This procedure provides an upper bound to the complexity of the optimization problem.
Theorem 4.3.
Computing the optimal cost of a given instance of SSPC is in FPNP.
Note that by Theorem 4.2, the optimization problem is harder than NP, unless the polynomial hierarchy collapses.
The source of complexity lies in the onetime costs associated to services, as shown in the next two subsections.
The algorithms described in this section accept slightly modi(cid:12)ed instances of SSPC, where the function k is extended to all of R (cid:2) O by setting kij = +1 for all hi; ji 2 (R (cid:2) O) n M .
Algorithm 1 solves exactly the problem in the obvious way, by exhaustively trying all possible bindings.
The only optimization consists in aborting a tentative binding construction whenever the value of the current partial binding exceeds the best cost found so far.
Nevertheless, the intractable nature of the problem makes approximate solutions the natural choice for dealing with large instances.
The (cid:12)rst constant factor approximation algorithm for facility location problems due to Shmoys et al.
appeared in the literature in 1997 [23].
In 1999 Guha and Khuller [8] proved that it is impossible to get an approximation guarantee of 1.463 unless NP(cid:18)DTIME[nO(log log n)].
Since then, several scienti(cid:12)c papers have been published along this line of research [10, 9, 14, 25].
In our exploration of the approximate solutions we have implemented the best known approximation algorithm (Algorithm 2) proposed by Mahdian et al. [15].
This algorithm ensures that the ratio between the cost of the returned solution and the optimal cost is bounded by 1.52.
Algorithm 2 combines the greedy algorithm proposed by Jain et al. [9] (Algorithm 3) with the idea of cost scaling and can be implemented in quasi-linear time, as showed by the authors using a result of Thorup [27].
We have also investigated a simple heuristic approach requiring time O(mn2).
Algorithm 4 consists of two phases: a greedy adaptive construction phase (line 3, calling Algorithm 5) and a local search phase (lines 4{21).
These algorithms use the sets of requests Rs served by each service s, formally de(cid:12)ned by Rs = fr 2 R j b(r) = sg : Algorithm 4 and Algorithm 5 return inverse bindings, represented by pairs (y; fRsgs2O) where (i) y is a boolean vector such that ys = 1 i(cid:11) o(cid:11)er s is used in the binding, and (ii) the family fRsgs2O de(cid:12)nes for each o(cid:11)er s the requests satis(cid:12)ed by s.
Starting from an empty solution, the (cid:12)rst phase (Algorithm 5) iteratively constructs a feasible solution in a greedy and adaptive fashion with a greedy function de(cid:12)ned on both matching and service costs.
At each iteration, a new matching is determined between an unmatched request and the most convenient o(cid:11)er.
In future iterations, the cost of this o(cid:11)er will not be considered again while evaluating the greedy choice (a greedy adaptive schema).
The running time of Algorithm 5 that performs this phase is O(mn).
Starting from the feasible binding found by the construction phase, the local search phase tries (in time O(mn)) to (cid:12)nd a better binding by slightly perturbing it.
In particular, for each invoked o(cid:11)er s 2 f1; 2; : : : ; ng the algorithm looks for an alternative and more convenient o(cid:11)er l 6= s that can serve the requests currently matched to s (l may have been already associated to other requests, but not necessarily).
If such a service l is found, then all the requests served by s are redirected to l.
This strategy is expected to work especially well in the presence of multi-function services that make peruse discounts to users that register to many of the service s options.
In case of heavy use of these functionalities, the algorithm Algorithm 1 ExhaustiveSearch (U R; CO; P C; BC; c; k; b)
 P C:partial cost, BC:best cost, c:vector of costs associated to services, k:matrix of costs associated to invocations.
3: begin


 return BC fabort search; keep current best costg tion, as P C < BCg save the current binding b; return P C fnew best costg choose r 2 U R; for all s such that krs < +1 do

 9: else







 b(r) := s; fbind r to sg if s 2 CO then P Cs := P C + krs else P Cs := P C + krs + cs BC := ExhaustiveSearch (U R n frg; CO [ fsg; P Cs; BC; c; k) return BC
 19: end is likely to (cid:12)nd that the service is more convenient even if its onetime cost is higher than those of the competing services.
An experimental evaluation of Algorithms 2 and 4 is discussed in Section 4.4.
Let us suppose that the onetime costs associated to services are null, that is:
 (5) (the costs krs associated to service invocations may be greater than zero).
This special case of SSP is equivalent to a special transportation problem and can be polynomially solved by following a greedy approach with greedy function given by k : R (cid:2) O !
Q.
The optimal solution activates all services and simply matches a service request with the cheapest service.
It is easy to show that the optimal cost can be computed through GreedyAdapt (Algorithm 5) with null input cost vector c: Theorem 4.4.
The binding corresponding to the values y; fRsgs2O returned by Algorithm 5 is optimal if c is null.
Note that in this case GreedyAdapt is a pure greedy algorithm running in O(mn) time.
The next corollary follows immediately.
Corollary 4.5.
If (5) holds, then SSPC can be solved in time O(mn).
In the light of Section 4.2, it is interesting to investigate the complexity of SSPC when the invocation costs are null, that is:
 (6) Algorithm 2 1.52approx (m; n; c; k; (cid:14)) Algorithm 3 jain (m; n; c; k)
 for (cid:14) = 1:504, 1.52-approximate binding -
represented by y and fRsgs2O - and its cost C.
and fRsgs2O - and its cost C.
c(s) := c(s) (cid:3) (cid:14) c(s) := c(s) (cid:14) for s = 1 to n s.t.
ys = 0 do if (max< ( (cid:22)C (cid:0) ^C (cid:0) cs)=cs) then Q := Q [ frg ^C := ^C + krs (cid:22)C := (cid:22)C + krb(r) ^C := 0, (cid:22)C := 0
 for r = 1 to m s.t.
krs < +1 do 2: begin 3: for all s = 1 to n do

 6: for all s = 1 to n do
 8: bool:=true 9: while (bool) do 10: max:= 0























 35: end max:= ( (cid:22)C (cid:0) ^C (cid:0) cs)=cs v := s ^Cv := ^C Qv := Q j := b(r) dj := dj (cid:0) krj Rj := Rj n frg if (Rj = ;) then yv := 1, dv := ^Cv, Rv := Qv for r 2 Rv do yj := 0 b(r) := v if (max> 0) then bool:=false else if (b(r) = 0) then else if (toto(cid:11)er(cid:21) cs) then b(r) := 0, budget(r) := 0 ys := 0 for all s = 1 to n do if ys = 0 then budget(r) :=budget(r) + 1 if (krb(r) (cid:0) krs > 0) then if (budget(r) (cid:0) krs > 0) then for all r = 1 to m s.t.
b(r) = 0 do toto(cid:11)er:= 0, i := 0 for all r = 1 to m do toto(cid:11)er:=toto(cid:11)er+krb(r) (cid:0) krs i := i + 1, L(i) := r toto(cid:11)er:=toto(cid:11)er+budget(r) (cid:0) krs i := i + 1, L(i) := r 2: begin

 4: for all r = 1 to m do
 6: for all s = 1 to n do
 8: while (there exists r 2 f1; 2; : : : ; mg s.t.
b(r) = 0) do































 41: for all s = 1 to n do


 45: end Rj := Rj n fvg dj := dj (cid:0) kvj if (Rj = ;) then b(v) := s, Rs := Rs [ fvg ds := ds + kvs Rs := Rs [ frg b(r) := s, ds := ds + krs v := L(k) j := b(v) if (j 6= 0) then ys := 1 for k = 1 to i do if (budget(r) = krs) then for r = 1 to m do if (b(r) = 0) then if (ys = 1) then C := C + cs + ds yj = 0 else Algorithm 4 GreedyAdaptHeur (m; n; c; k)
 fRsgs2O { and its cost C.
2: begin
 if ys = 1 then fLocal search phaseg 4: for all s = 1 to n do





 if l 6= s then improved := false l := 1 while (not improved and l (cid:20) n) do q := X krl r2Rs gain := ( cs + ds ) (cid:0) [ cl(1 (cid:0) yl) + q ] if gain> 0 then Rl := Rl [ Rs dl := dl + q Rs := ; ds := 0 ys := 0 yl := 1 improved := true C = C(cid:0)gain












 24: end l := l + 1 fend whileg Algorithm 5 GreedyAdapt (m; n; c; k)
 fRsgs2O {, its cost C and the costs d (see below).
fInit structuresg Rs := ; ys := 0 fi.e.
s not usedg ds := 0 ftotal cost of all calls to sg 2: begin 3: for all s = 1 to n do





 9: for all r = 1 to m do 10: min := +1








 20: end min := cs(1 (cid:0) ys) + krs best := s C := C + min Rbest := Rbest [ frg dbest := dbest + kr best ybest := 1 for all s = 1 to n do if min > cs(1 (cid:0) ys) + krs then (the costs cj may be nonzero.)
In this case, SSPC remains di(cid:14)cult.
Its computational complexity remains high even if (6) holds and the costs cj are all identical (but nonzero), that is,
 (7) To prove this, we note that the hitting set problem [7] can be reduced to the decision version3 of SSPC satisfying (6) and (7).
The hitting set problem can be formulated as follows: Given a (cid:12)nite set S, a collection of sets Si (cid:18) S (1 (cid:20) i (cid:20) z), and a positive K, decide whether there exists S 0 (cid:18) S such that for all i = 1 : : : z, S0 \ Si 6= ; and jS0j < K.
The hitting set problem is known to be NP-complete.
The hitting set problem can be reduced to the decision version of SSPC under restrictions (6) and (7) by de(cid:12)ning: R = f1; : : : ; zg, O = S (we may assume w.l.o.g.
that S is a (cid:12)nite initial segment of N), M = fhi; ji j j 2 Sig, and c = f1gm.
Then, by analogy with the cost estimates for the general case, we can prove that for the class of SSPC instances satisfying (6) and (7): (cid:15) Checking whether the optimal cost equals a given rational K is DP-complete.
(cid:15) Computing the optimal cost is in FPNP.
From this result and the results of the previous section, we conclude that the costs cj associated to services are entirely responsible for the high computational complexity of SSPC.
This holds even if the service o(cid:11)ers all have the same cost.
Intuitively, in this case, it is hard to choose among services with the same activation cost that compete by o(cid:11)ering different, partially overlapping sets of free functionalities.
We performed some preliminary experiments with Algorithms 1, 2, and 4, using a C implementation running on a Pentium 4, 2.4GHz, 512Mb.
To compare the algorithms, we applied them to a set of 300 randomly generated instances, according to the follow ing criteria.
Recall that m is the number of requests and n is the number of o(cid:11)ers.
We have considered instances with 5 (cid:20) m (cid:20) 100 and 100 (cid:20) n (cid:20) 10000 (assuming that the set of o(cid:11)ers in practice will be signi(cid:12)cantly larger than the set of requests in a work(cid:13)ow).
We (cid:12)xed the range of the invocation costs k to [0; 100] and the range of the onetime costs c to [1; p (cid:1) 100] for p = 0:1; 1; 10, in order to check the in(cid:13)uence of the relative weight of k and c. For each triple (m; n; p) 10 instances have been randomly generated.
The runs longer than 1 hour have been killed.
Algorithm 1 (that computes an optimal solution) exhibited a satisfactory performance for all the instances with m (cid:20) 10 and n (cid:20) 100.
The maximal elapsed time was 0:35 seconds.
The performance started to decrease for (n; m) = (15; 150).
(cid:15) For (n; m) = (15; 150) the maximal elapsed time was

 and a cost K, decide whether there is a solution with cost (cid:20) K.
(cid:15) For (n; m) = (20; 200), 20% of the runs have been killed and the maximal elapsed time of the other runs was 210:0100.
(cid:15) For (n; m) = (20; 200), 73% of the runs have been killed and the maximal elapsed time of non-killed runs was over 59 minutes.
Algorithms 2 and 4 are much faster, of course.
The former has been killed only once (m = 100, n = 10000), the latter has never been killed.
The average time of Algorithm 2 was
 than 30 seconds), but more extensive experimentations are needed to con(cid:12)rm and explain this observation.
Some of the average execution times of Algorithm 2 are reported in Figure 1.
The (cid:12)gure illustrates both how execution time grows with the size of the problem instance, and the in(cid:13)uence of onetime costs on performance.
In particular, it appears that as onetime costs become negligible, Algorithm 2 becomes faster.
When the upper bound for onetime costs used by the random generator is one tenth of the upper bound for peruse costs, the average time drops down to 65.19 seconds.
We measured the quality of the approximate solutions returned by Algorithm 4 by evaluating the relative error of each solution; the relative error is A(cid:0)C C , where A is the approximate cost and C is the optimal cost (we computed the error only for those instances whose optimal cost was available, i.e. the exact algorithm was not killed).
The average of the errors is around 70%, which is not bad for a naive heuristics.
Also in this case, we need more experiments to validate this observation.
Unlike SSPC, SSPQ and SSP0
 Q are always easy.
These two problems can be solved almost in the same way.
Algorithm 6 solves the version of SSPQ with objective functions Qb.
Algorithm 6 GreedyAdapt-Q (m; n; c; k) for all s = 1 to n do if maxlev r < f (krs; cs) then
 2: begin

 4: for all r = 1 to m do 5: maxlev r := (cid:0)1






 13: end maxlev r = f (krs; cs) best := s L := L + maxlev r b(r) := best To solve the version based on Q0 b, only one change to Algorithm 6 is required: replace line 10 with
 L := minfmaxlev r; Lg : It is not hard to prove the correctness of the two versions Q; from this property of Algorithm 6 w.r.t.
SSPQ and SSP0 and a straightforward analysis of Algorithm 6, we conclude that: Theorem 5.1.
SSPQ and SSP0 Q can be solved in time O(mn).
This approach can be easily extended to any objective function similar to Q and Q0, based on polynomially com-putable, monotonic combination functions besides P and min.
The details will be given in an extended version of the paper.
Summarizing, we formalized three kinds of optimal service selection problems|based on cost minimization and on two di(cid:11)erent quality maximization criteria|and we proved that the cost minimization problem, SSPC, is generally hard, while the two quality maximization problems, SSPQ and
 Q, can be solved in polynomial time.
In particular, SSPC is in FPNP and harder than NP(unless the polynomial hierarchy collapses).
We proved that the reason of the high computational complexity of SSPC lies in the onetime costs associated to service o(cid:11)ers (such as initialization and registration costs).
When these costs are all null, SSPC is solvable in polynomial time (on the contrary, in the absence of peruse costs the problem does not become easier).
We designed and implemented algorithms for computing exact solutions for all these versions of SSP.
The exact algorithm for SSPC (Algorithm 1) has been evaluated experimentally.
According to the current results, instances with up to 10 requests and 100 o(cid:11)ers can be nicely handled by this algorithm; for larger instances, the performance quickly decreases, making the algorithm inapplicable.
We have also designed and implemented suboptimal solutions and evaluated them experimentally.
Currently, it seems that the algorithm with a guaranteed 0:52 bound on relative error is too slow for real-time service selection over large work(cid:13)ows and o(cid:11)er sets.
The heuristic algorithm (Algorithm 4) seems to be faster, but it has no guarantees on the quality of the solution.
We are planning to carry out more experiments to validate and re(cid:12)ne these preliminary observations.
Moreover, we are trying to sharpen the complexity bounds (we do not yet know whether SSPC is complete for FPNP).
Finally, we are generalizing the framework presented in this paper by considering optimization problems that involve simultaneous cost minimization and quality maximization, as well as multidimensional measures that induce partially ordered measures of nonfunctional properties (although in several cases multiple criteria can be reduced to single, totally ordered numeric measures [5]).
Another direction for generalization concerns time-dependent costs and preferences.
The authors are grateful to the anonymous referees for their insightful and constructive comments, and to Alessio Contino for his help in the implementation and experimentation phase.
This work has been partially supported by the Network of Excellence REWERSE, EU Sixth Framework Program, IST-2004-506779.
