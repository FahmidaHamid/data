PageRank [17] is one of the most important ranking techniques used in today s search engines.
Not only is PageRank a simple, robust and reliable way to measure the importance of web pages [3], but it is also computationally advantageous with respect to other  This work has been partially supported by a  Finanziamento per grandi e mega attrezzature scienti che  of the Universit  degli Studi di Milano and by the MIUR COFIN Project  Linguaggi for-mali e automi .
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
ranking techniques in that it is query independent, and content independent.
Otherwise said, it can be computed of ine using only the web graph1 structure and then used later, as users submit queries to the search engine, typically aggregated with other, query-dependent rankings [4, 12, 16].
One suggestive way to describe the idea behind PageRank is as follows: consider a random surfer that starts from a random page, and at every time chooses the next page by clicking on one of the links in the current page (selected uniformly at random among the links present in the page).
As a  rst approximation, we could de ne the rank of a page as the fraction of time that the surfer spent on that page on the average.
Clearly, important pages (i.e., pages that happen to be linked by many other pages, or by few important ones) will be visited more often, which justi es the de nition.
However, we also allow the surfer to restart with probability 1     from another node chosen randomly and uniformly, instead of following a link.
As remarked in [5], a signi cant part of the current knowledge about PageRank is scattered through the research laboratories of large search engines, and its analysis  has remained largely in the realm of trade secrets and economic competition .
As the authors of the aforementioned paper, however, we believe that a scienti c and detailed study of PageRank is essential to our understanding of the web, and we hope this paper can be a contribution in such program.
PageRank is de ned formally as the stationary distribution of a stochastic process whose states are the nodes of the web graph.
The process itself is obtained by combining the normalised adjacency matrix of the web graph (with some patches for nodes without out-links that will be discussed later) with a trivial uniform process that is needed to make the combination irreducible and aperiodic, so that the stationary distribution is well de ned.
The combination depends on a damping factor   2 T0; 1/, which will play a major r le in this paper.
When   is 0, the web-graph part of the process is annihilated, resulting in the trivial uniform process.
As   goes to 1, the web part becomes more and more important.
The problem of choosing   was curiously overlooked in the  rst papers about PageRank: yet, not only PageRank changes signi -cantly when   is modi ed [19, 18], but also the relative ordering of nodes determined by PageRank can be radically different [14].
The original value suggested by Brin and Page (  D 0:85) is the most common choice.
Intuitively, 1     is an amount of ranking that we agree to give uniformly at each page.
This amount will be then funneled through the outlinks of the node.
A common form of link spamming funnels carefully this amount towards a single page, giving it a preposterously great importance.
whose arcs correspond to hyperlinks.
It is natural to wonder what is the best value of the damping factor, if such a thing exists.
In a way, when   gets close to 1 the Markov process is closer to the  ideal  one, which would somehow suggest that   should be chosen as close to 1 as possible.
This observation is not new, but it has some naivety in it.
The  rst issue is of computational nature: PageRank is traditionally computed using variants of the Power Method.
The number of iterations required for this method to converge grows with  , and moreover more and more numerical precision is required as   gets closer to 1.
But there is an even more fundamental reason not to choose a value of   too close to 1: we shall prove in Section 3 that when   goes to 1 PageRank gets concentrated in the recurrent states, which correspond essentially to the nodes whose strongly connected components have no passage toward other components.
This phenomenon gives a null PageRank to all the pages in the core component, something that is dif cult to explain and that is contrary to common sense.
In other words, in real-word web graphs the rank of all important nodes (in particular, all nodes of the core component) goes to 0 as   goes to 1.
Thus, PageRank oscillates between a meaningless uniform distribution (  D 0) and a meaningless distribution concentrated mostly in irrelevant nodes (  D 1).
As a result, both for choosing the correct damping factor and for detecting link spamming, being able to describe the behaviour of PageRank when   changes is essential.
Recently, indeed, a sophisticated form of link-spam detection has been based on the study of the value of PageRank with respect to   [21].
To proceed further in this direction, it is essential that we have at our disposal analytical tools that describe this behaviour.
To this purpose, we shall provide closed-form formulae for the derivatives of any order of PageRank with respect to  , and an iterative algorithm (an extension of the power method) that approximates them.
The most surprising consequence, easily derived from our for-mulae, is that the vectors computed during the PageRank computation for any   2 .0; 1/ can be used to approximate PageRank for every other   2 .0; 1/.
This happens because the k-th coef cient of the Maclaurin series for PageRank can be easily computed during the k-th iteration of the Power Method.
This allows to study easily the behaviour of PageRank for any node storing a minimal amount of data.2

 Let G be the adjacency matrix of a directed graph of N nodes (identi ed hereafter with the numbers from 0 to N   1).
A node is terminal if it does not have outlinks, except possibly for loops (or, equivalently, if all arcs incident on the node are incoming).
If we want to be speci c about the presence of a loop, we shall use the terms looped and loopless3.
We note that usually G is preprocessed before building the corresponding Markov chain.
Common processing includes removal of all loops (as nodes should not give authoritativeness to themselves) and thresholding the number of links coming from pages of the same domain (to reduce the effect of link spamming).
all Java code the algorithms implementing
 described in this paper will be available for download at http://law.dsi.unimi.it/.
commonly known as dangling nodes; the same kind of node is often called a sink in graph-theoretic literature.
Our choice avoids the usage of ambiguous terms that have been given different meanings in different papers.
If no loopless terminal nodes are present (note that after the pre-processing sketched above they will be the only kind of terminal nodes), we can just normalise uniformly to 1 the row-sums of G by multiplying it by D 1, the inverse of the diagonal degree matrix.
However, D is not invertible if loopless terminal nodes are present.
The classical way to handle this situation consists in substituting them with nodes that have one outgoing arc toward every node (including the node itself.
In other words, in G rows of zeroes are substituted with rows of ones.
Let NG be the (adjacency matrix of the) resulting graph, and ND be the diagonal matrix of the outdegrees of NG (i.e., dii is the number of ones on the i-th row of NG).
Let also 1 be the vector4 of all 1 s, and v be any personalisation vector (a vector whose elements are all non-negative and sum to 1, which is used to bias PageRank w.r.t.
a selected set of trusted pages).
We are providing a toy example in the Appendix that will guide the reader through the paper.
In Table 5, the example graph G and its modi ed version NG are presented.
In the rest of the paper, we shall use the matrices de ned in Figure 1; some of them are functions of the damping factor   2 T0; 1/, and we will use a notation re ecting this fact.
Note that Q. / is well de ned for all   2 T0; 1/, as .I     P/ is known to be invert-ible [20].
A. / VD   P C .1    /1T C. / VD I     P Q. / VD PC. / 1 v Figure 1: Basic PageRank de nitions.
The PageRank vector r. / is de ned as the dominant eigenvec-tor of A. /; more precisely, as the only vector summing to 1 such that r. /A. / D r. /.
Noting that r. /1T D 1, we get r. /  P C .1    /1T  r. /P C .1    /v D r. / v  D r. / .1    /v D r. /.I     P/; which yields the following closed formula for PageRank: r. / D .1    /vC. / 1: (1) This is Lemma 3 of [8], albeit in the original statement of this lemma the factor 1     is missing, probably due to an oversight.
Note that (1) can be written as r. / D .1    /v .  P/t ;
 Xt D0 which makes the dependence of PageRank on incoming paths very explicit.
The reader can see the PageRank vector in Figure 7 (the preference vector v is the uniform vector).
PageRank is represented as a function of   in Figure 8.
In this section, we shall discuss the general behaviour of Page-Rank as a function of the damping factor  , considering in particular what happens when   gets close to 1.
Recall that P (the row-normalised adjacency matrix) is a Markov chain, but in general it is neither aperiodic nor irreducible.
Usually, though, in all practical cases P will be aperiodic, but reducible.
In this paper, we shall assume that P is indeed aperiodic.
Introducing the damping factor has the consequence of obtaining an aperiodic irreducible chain.
Indeed, for all   2 T0; 1U, A. / is a Markov chain; moreover, if   < 1, A. / is irreducible and aperiodic.
Hence, A. / admits a unique limit distribution r. /.
Clearly, r. / is a rational (vector) function of  : usually, though, one looks at r. / only for a speci c value of  .
All algorithms to compute PageRank actually compute (or, more precisely, provide an estimate of) r. / for some   that you plug in it, and it is by now an established use to choose   D 0:85.
This choice was indeed proposed by Brin and Page [17], and it is rumored that Google itself uses this value; it seems that the rankings obtained with this choice are very natural and satisfactory for the users.
Many authors had tried to devise a more thorough a posteriori justi cation for 0:85.
It is easy to get convinced that choosing a small value for   is not appropriate, because too much weight would be given to the  uniform  part of A. /: indeed, as we remarked in the introduction, A.0/ is the uniform matrix and r.0/ is the uniform distribution.
Conversely, as   !
1 , the matrix A. / tends to P: this fact seems to suggest that choosing   close to 1 should give a  truer  or  better  PageRank: this is a widely diffused opinion (as we shall see, most probably a misconception).
In any case, as we remarked in the introduction there are some computational obstacles to choosing a value of   too close to 1.
The Power Method converges more and more slowly [9] as   !
1 , a fact that also in uences the other methods used to compute PageRank (which are, after all, variants of the Power Method [17, 7, 6, 15, 11, 10]).
Indeed, the number of iterations required could in general be bounded using the separation between the  rst and the second eigenvalue, but unfortunately the separation can be abysmally small if   D 1, making this technique not applicable.
Moreover, if   is large the computation of PageRank may become numerically ill-conditioned (essentially for the same reason [8]).
Even disregarding the problems discussed above, we shall provide convincing reasons that make it inadvisable to use a value of   close to 1.
First observe that, since r. / is a rational (coordinatewise) bounded function de ned on T0; 1/, the limit r   D lim  !1  r. / exists (the reader can see the vector r   for our example in the caption of Figure 7).
It is easy to see that r   is actually one of the limit distributions of P (because lim !1  A. / D P).
There are some natural questions about r   that we want to address:   can we somehow characterise the properties of r  ?
  what makes r   different from the other (in nitely many, if P is reducible) limit distributions of P?
The  rst question is the most interesting, because it is about what happens to PageRank when   !
1 ; in a sense, fortunately, it is also the easiest to answer.
Before doing this, recall some basic de nitions and facts about Markov chains.
  Given two states x and y, we say that x leads to y iff there is some m > 0 such that there is a nonzero probability to go from x to y in m steps.
  A state x is transient iff there is a state y such that x leads to y but y does not lead to x.
A state is recurrent iff it is not transient.
  In every limit distribution p of an aperiodic Markov chain, if px > 0 then x is recurrent [20].
Let us now introduce some graph-theoretical notation.
Let G be a graph.
  Given a node x of G, we write Tx UG for the (strongly connected) component of G containing x.
  The component graph of G is a graph whose nodes are the components of G, with an arc from Tx UG to TyUG iff there are nodes x 0 2 Tx UG and y0 2 TyUG such that there is an arc from x 0 to y0 in G. The component graph is acyclic, apart for the possible presence of loops.
  If x, y are two nodes of G, we write x  G y iff there is a nonempty directed path from x to y in G (by nonempty we mean that the path should contain at least one arc).
Clearly, a node is recurrent in P iff Tx U NG is terminal; otherwise said, x is recurrent (in the Markov chain P) iff x   NG y implies y   NG x as well.
Note that nodes with just a loop are recurrent (and their component is looped, too).
We now turn to our characterisation theorem, which identi es recurrent states on the basis of G, rather than NG.
The essence of the theorem is that, for what concerns recurrent states, the difference between G and NG is not signi cant, unless there are no looped terminal nodes among the components of G. The latter case, however, is as pathological as periodicity in a large web graph.
THEOREM 1.
Let G and P be de ned as above.
Then:
 component graph), then a node is recurrent for P iff its component is looped and terminal; hence, given any limit distribution p for P, px > 0 implies that x is a node of G whose component is looped and terminal;
 nal, then every node is recurrent.
PROOF.
Note that x   NG y means that there is a nonempty path from x to y in NG.
Such a path can be decomposed into a sequence of (possibly empty) paths in G, from x D x0 to a loopless terminal node y0, from a node x1 to a loopless terminal node y1, .
.
.
, from a node xk to yk D y.
Moreover, either k > 0, or the only path (a path from x to y in G) contains at least one arc.
For case (1), let x be contained in a looped terminal component, and suppose that x   NG y.
By the observation above, this path in NG can be decomposed into a sequence of paths of G towards loopless terminal nodes, plus a  nal path to y: but from x you cannot reach a loopless terminal node of G (because x is contained in a looped terminal component), so the path is simply a nonempty path of G, i.e., x  G y.
But then y is in the same component as x, so y  G x as well, and we obtain the result.
For the converse, suppose that x is not in a looped terminal component: we will show that there is a y such that x   NG y but not y   NG x.
We distinguish two cases:   suppose that there is a looped terminal component that can be reached from Tx UG in the component graph of G; let y be any node in such component.
Clearly x  G y, and hence x   NG y, but y   NG x does not hold (from y you can only reach nodes of TyUG both in G and in NG);   otherwise, suppose that there is a loopless terminal y such that x  G y (or x D y if x itself is terminal); let z be any node in a looped terminal component G: now x   NG z (you  rst go from x to y and then you  jump  to z), but from z you cannot reach x (because x is not in the same component).
THEOREM 2.
The following identities hold:

 PROOF.
Multiplying (1) by C. / and differentiating member-wise: r. /C 0. / C r 0. /C. / D  v r 0. /C. / D  r. /C 0. /   v r 0. /C. / D r. /P   v: (2) (3) (4) For case (2), take any two nodes x and y of G. In the component graph of G there will be two terminal components Tx 0UG and Ty0UG that are reachable from Tx UG and TyUG, respectively.
Both are, by hypothesis, loopless.
In other words, there are two terminal nodes x 0 and y0 such that x  G x 0 (or x D x 0) and y  G y0 (or y D y0).
This means that x   NG y and vice versa, unless x D y (and both are terminal), in which case again both x   NG y and vice versa.
The statement of the previous theorem may seem a bit unfathomable.
The essence, however, could be stated as follows: except for strongly connected graphs, or graphs whose terminal components are all trivial and loopless, the recurrent nodes are exactly those whose component is looped and terminal.
These nodes are often called rank sinks, as they absorb all the rank circulating through the graph.
As we remarked, a real-world graph will certainly contain at least one looped terminal component, so the  rst statement of the theorem will hold.
This means that most nodes x will be such that r   x D 0.
In particular, this will be true of all the nodes in the core component [13]: this result is somehow surprising, because it means that many important Web pages (that are contained in the core component) will have rank 0 in the limit (see, for instance, node 0 in our example).
This is a rather convincing justi cation that, contradicting the common beliefs, choosing   too close to 1 does not provide any good PageRank.
Rather, PageRank becomes  sensible  somewhere in between 0 and 1.
As far as the second question is concerned, we provide a CONJECTURE 1. r   is the limit distribution of P when the starting distribution is uniform, that is, Since C. / is invertible: r 0. / D .r. /P   v/C. / 1: Moreover, differentiating once more (4), we obtain: r 0. /C 0. / C r 00. /C. / D r 0. /P r 00. /C. / D r 0. /P   r 0. /C 0. / r 00. /C. / D r 0. /P C r 0. /P hence r 00. / D 2r 0. /PC. / 1; which accounts for the base case (k D 1) of an induction for the second statement.
For the inductive step, again multiplying by C. / and differentiating memberwise: r .kC2/. /C. / C r .kC1/. /C 0. / D .k C 1/r .kC1/. /P and the thesis follows easily.
r .kC2/. /C. / D r .kC1/. / .k C 1/P   C 0. /  We can reformulate the statement concerning the  rst-order derivative as follows: COROLLARY 1.
The following identity holds: r 0. / D r. /(cid:181)Q. /  
 1    
 PROOF.
From Theorem 2, we obtain r 0. / D r. /PC. / 1   vC. / 1.
Using (1) we can rewrite this as r. /PC. / 1  1 hence the result.
1  r. /, lim  !1  r. / D lim n!1

 Pn : Moreover, we can explicitly write a closed formula for the generic derivative: Note that the conjecture is trivial when P is irreducible, because in that case P has but one stationary distribution.
The reader should by now be convinced that the behaviour of PageRank with respect to the damping factor is nonobvious: r. / should be considered a function of  , and studied as such.
The standard tool for understanding changes in a real function is the analysis of its derivatives.
Correspondingly, we are going to provide mathematical support for this analysis.
The main objective of this section is providing exact formulae for the derivatives of r. /.
De ne r 0. /, r 00. /, .
.
.
, r .k/. / as the  rst, second, .
.
.
, k-th derivative of r. / with respect to  .
We start by providing the basic relations between these vector functions: COROLLARY 2.
For every k > 0 r .k/. / D kW .r. /P   v/C. / 1 Q. /k 1 or, equivalently,
 1     I  Q. /k 1: r .k/. / D kW r. /(cid:181)Q. /   PROOF.
Just proceed from Theorem 2 by iterate substitution, and  nally apply Corollary 1.
The formulae obtained in Section 4.1 do not lead directly to an effective algorithm that computes derivatives: even assuming that the exact value of r. / is available, to obtain the derivatives one should invert C. / (see Theorem 2), a heavy (in fact, unfeasible) computational task.
However, in this section we shall provide a way to obtain simultaneous approximations for PageRank and its derivatives for a given value of  , and we will show how these approximations converge to the desired vectors.
norm between the k-th iterate and the exact value is O k .
The simplest and most important algorithm that computes PageR-ank [17] is an application of the Power Method; the algorithm computes a sequence of vectors v0, v1, : : : where v0 D v and vkC1 D vk A. /.
This sequence of vectors converges to r. /, and convergence speed depends on  ; more precisely, the difference in In practice, the algorithm provides good approximation quickly: in the original paper [17] the authors state that 40 to 50 iterations are enough on reasonable data sets; of course, more sophisticated approaches have been proposed in the literature to reduce the number of iterations and/or the amount of computation needed at each iteration [17, 7, 6, 15, 11, 10], but they are basically all variants of the Power Method.
The reader can see the  rst few iterates of the Power Method applied to our example in Figure 1.
t We are going to present a modi ed version of the basic algorithm that will compute PageRank and its derivatives up to (any desired) index K , and to do this it will use K C 1 vectors.
In other words, it will build K C 1 vector sequences: the sequence s.0/ 1 . /, : : : , s.0/ . /, : : : will be used to approximate r. / (and will be de ned exactly as in the classical PageRank algorithm); the sequence s.1/ . /, : : : will be used to approximate r 0. /; and so on.
Note that the sequence s.k/ 1 . /, : : : , s.k/ . /, : : : will not, in general, converge to r .k/. / per se; rather, t there will be an associated sequence q .k/ . /, : : : based on it, that will actually converge to the desired derivative.
1 . /, : : : , s.1/ t 1 . /, : : : , q.k/ t 0 . /, q.k/ 0 . /, s.1/ 0 . /, s.0/ 0 . /, s.k/ s.0/
 s.0/ t C1. / VD s.0/ t s.kC1/ . / VD q.k/ 0 . /
 . / VD  s.kC1/ s.kC1/ t C1 t . /A. / . /P C q.k/ t . /P q.0/ t q.1/ t q.k/ t . / VD s.0/ . / VD s.1/ t t . / . /   . / VD ks.k/ t . /
 s.0/ t . / 1     for all k   2: Figure 2: Basic de nitions for the approximation algorithm.
The vector sequences are de ned in Figure 2.
Note that only the . / (0   k   K ) need to be stored, whereas . / are only de ned for convenience, and can be implemented, K C 1 vectors s.k/ q.k/ t for example, as a function.
t Our  rst result is about convergence of the  rst-order derivative.5 THEOREM 3. limt!1 q.1/ . / D r 0. /, and the difference in t norm is O t  t , that is: q.1/ t (cid:176)(cid:176)(cid:176)
 with any p-norm.
We just note that kSk1 D 1 for any stochastic . /   r 0. /(cid:176)(cid:176)(cid:176) S (as we use row vectors), and so(cid:176)(cid:176) D O t  t  as t !
1.
Pt(cid:176)(cid:176)p is bounded by a constant that depends on P s size, but not on P s elements or on t.
. /   r. /(cid:176)(cid:176)(cid:176) XsD0 t  1 PROOF.
Recall that(cid:176)(cid:176)(cid:176) q.0/ t ond eigenvalue of A. / is at most   [9].
An easy proof by induction shows that for all t D O t , since the sec. / D v.  P/t C q.0/ t  s 1. /.  P/s P; s.1/ t so q.1/ t . / D v.  P/t C q.0/ t  s 1. /.  P/s P  
 1     q.0/ t . /: t  1 XsD0 Now, by Corollary 1, we have: r 0. / D r. /(cid:181)Q. /  
 1    
 Since Q. / D PC. / 1 D P.I     P/ 1 D we have
 r 0. / D XsD0 r. /.  P/s P  
 1     r. /: Hence, we can bound the convergence rate as follows: .  P/s P;
 XsD0
 t t



 t  1 (cid:176)(cid:176)(cid:176) XsDt r 0. /   q.1/ C(cid:176)(cid:176)(cid:176)  (cid:176)(cid:176)(cid:176) r. /   q.0/ C(cid:176)(cid:176)v.  P/t(cid:176)(cid:176) : X / 1 X n (whenever the  rst series converges).
Thus, We provide an upper bound for each of the summands above.
As t Dn X t D .I   . /(cid:176)(cid:176)(cid:176) XsDt XsD0 r. /   q.0/ 1     (cid:176)(cid:176)(cid:176) r. /.  P/s P(cid:176)(cid:176)(cid:176) t  s 1. /  .  P/s P(cid:176)(cid:176)(cid:176) . /(cid:176)(cid:176)(cid:176) far as the  rst summand is concerned, recall thatP1 (cid:176)(cid:176)(cid:176)  (cid:176)(cid:176).  P/t(cid:176)(cid:176)   kr. /Pk ; and the  rst summand is O t .
The second summand can be (cid:176)(cid:176)(cid:176) XsD0 r. /   q.0/ t  s 1. / .  P/s P(cid:176)(cid:176)(cid:176) D O t (cid:176)(cid:176)(cid:176) D O t  XsD0 PsC1(cid:176)(cid:176)(cid:176) O.1/ D O t  t : The third and the fourth summands are both O t .
All summands are thus O t  t hence the result.
.I     P/ 1(cid:176)(cid:176)(cid:176)  (cid:176)(cid:176)(cid:176) r. /.  P/s P(cid:176)(cid:176)(cid:176) As far as the other derivatives are concerned, we have: THEOREM 4.
For every k > 1, limt!1 q.k/ bounded as follows: . / D r .k/. /, XsD0 t  1 t  1 t  1
 and the difference in norm is O tk  t .
PROOF.
First of all, by induction on k and t, one can prove that t s.k/ t . / D .k   1/W       1 v.  P/t C q.k 1/ t  s 1. /.  P/s P; t  1 XsD0 procedure step() for i:=0, 1, : : : , N   1 do for k:=0, 1, : : : , K do s0Tk; i U VD 0; end for end for for i:=0, 1, : : : , N   1 do d:=outdegree of node i; for all successors j of i do . /(cid:176)(cid:176)(cid:176) s0T0; j U:=s0T0; j U C   C q(0, i)=d; for k:= 1, 2, : : : , K do s0Tk; j U:=s0Tk; j U C . sTk; i U C q(k   1, i)/=d;   end for end for end for procedure computePageRankAndDerivatives() init(); do step(); while not stopping condition; for all k > 1 and all t. The base case t D 0 is trivial (by an easy induction on all k > 1), whereas the case k D 2 can be obtained by induction on t using Theorem 3, noting that the rule for computing q.1/ . / is a special case.
The inductive step is then obtained using t the rule that de nes s.kC1/ . /.
t C1 Now, recalling that (from Theorem 2) r .k/. / D kr .k 1/. /PC. / 1 D k r .k 1/. /.  P/s P;
 XsD0 we have (cid:176)(cid:176)(cid:176)
 XsDt k(cid:176)(cid:176)(cid:176) r .k/. /   q.k/ t r .k 1/. /.  P/s P ks.k/ t
 k D(cid:176)(cid:176)(cid:176) . /(cid:176)(cid:176)(cid:176) XsD0 r .k 1/. /.  P/s P(cid:176)(cid:176)(cid:176) C k(cid:176)(cid:176)(cid:176) t  1 C kW  
 t  s 1. /  .  P/s P(cid:176)(cid:176)(cid:176) : XsD0 r .k 1/. /   q.k 1/ The result follows along the lines of the last part of the proof of Theorem 3.
We remark two important points that deserve further analysis.
First of all, the big-oh notation hides a number of constants independent of t. However, when k is large or   very close to 1 these constants may become important.
Second, we did not give a detailed evaluation of the numerical precision that is necessary to perform these computations.
The results of the previous section can be used to modify the classical PageRank algorithm, based on the Power Method, so to compute an approximation of the derivatives of PageRank up to a certain index.
The algorithm uses a vector sT ;  U where the  rst index represents the derivative index (from 0 to K , inclusive, where K is the highest derivative order to be computed) and the second index represents the node.
In other words, at step t the vector sTk;  U represents s.k/ . / is not itself represented as a vector, but rather it is implemented by the procedure q().
. /.
The vector q.k/ t t The procedure init() initialises the vector sT ;  U, whereas step() computes the vector for the next iteration (the new vector is indicated by s0T ;  U).
The stopping criterion can be decided in many ways: for example, at each step, the norms of the differences between each derivative and the derivative at the previous step are computed, and the iteration is stopped if all such norms are below a certain threshold.
procedure q(k, i) if k=0 then return sT0; i U; else if k=1 then return sT1; i U   sT0; i U=.1    /; else return k   sTk; i U; procedure init() for i:=0, 1, : : : , N   1 do sT0; i U:=vTi U; for k:=1, : : : , K do sTk; i U:=q(k   1, i); end for end for

 The rational function r. / can be expressed using its Maclaurin series (e.g., the Taylor series about 0); let us denote by tn . / the nth degree Maclaurin polynomial of r. / evaluated in  .
Clearly, Maclaurin polynomials offer an appealing way to study PageRank in relation to  .
To obtain an explicit formula for tn . /, just recall from Corollary 2 that r .k/.0/ D kW r.0/.Q.0/ I /Q.0/k 1.
Since r.0/ D v and Q.0/ D P, we have, for all k > 0, r .k/.0/ D kW v.P   I /Pk 1: Now, since tn . / DPn kD0.1=kW/ k r .k/.0/, we have tn . / D v(cid:181)I C n XkD1  k Pk   Pk 1 : Two important problems face us now:  rst of all, how to compute tn . /; second, how to choose n. Both problems will be solved by a surprisingly simple relationship between Maclaurin polynomials and the Power Method that will be proved in this section.
To obtain our main result, we will need the following: LEMMA 1.
Let C be a set of square matrices of the same size, and R 2 C such that for every M 2 C we have M R D R. Then for all M 2 C ,   2 R and for all n we have . M C .1    /R/n D  n Mn C .1    /  k R Mk : n 1 XkD0 or, equivalently, . M C.1 /R/n D  n Mn CR.I  n Mn 1/CR n 1 XkD1  k Mk  Mk 1 : PROOF.
By an easy induction.
The  rst statement is trivial for n D 0.
If we multiply both members by  M C .1    /R on the right we have . M C .1    /R/nC1 D  nC1 MnC1 C .1    /
 n 1 XkD0  k R D D  nC1 MnC1 C .1    / C .1    /2 1    n

  kC1 R MkC1 C  n .1    /RC n 1 XkD0  kC1 R MkC1 C  n .1    /RC n 1 XkD0 D  nC1 MnC1 C .1    /  k R Mk : n XkD0 The second statement can be then proved by expanding the summation and collecting monomials according to the powers of  .
Of course, the last result can be easily restated in any R-algebra.
We can now come to the main result of this section, which equates analytic approximation (the degree of the Maclaurin polynomial) with computational approximation (the number of iterations of the Power Method): THEOREM 5.
The nth approximation of PageRank computed by the Power Method with damping factor   coincides with the nth degree Maclaurin polynomial of PageRank evaluated in  .
In other words, v A. /n D tn . /.
PROOF.
Apply Lemma 1 to the case when M D P, R D 1T v and   D  .
We have: A. /n D  n Pn C1T v    n1T v Pn 1 C1T v n 1 XkD1  k Pk   Pk 1 ; hence q.0/ n . / D v A. /n D D  n v Pn C v    n v Pn 1 C v D v C v n 1 XkD1 n 1  k Pk   Pk 1  D XkD1  k Pk   Pk 1  D tn . /: As a consequence: COROLLARY 3.
The difference between the k-th and the .k  
 divided by  k, is the k-th coef cient6 of the Maclaurin series of PageRank.
can be used to compute immediately PageRank for any other  , obtaining the result of the Power Method after the same number of iterations.
Indeed, by saving the Maclaurin coef cients during the computation of PageRank with a speci c   it is possible to study the behaviour of PageRank when   varies.
Even more is true, of course: using standard series derivation techniques, one can approximate the k-th derivative (lowering of course by k the approximating polynomial).
Note, however, that the algorithm presented in Section 4.3 provides values of the derivatives for a speci c   with a precision guarantee.
The  rst few coef cients of the Maclaurin polynomial for our example are shown in Figure 2.
Figure 3 illustrates from an experimental viewpoint the convergence speed theorems of Section 4.2.
We computed PageRank and its derivatives up to index four (inclusive) and we plotted the difference, in L2-norm (we used for the computation a small, 325 557-nodes graph of the sites of the Italian CNR), between two successive iterates during the  rst 70 iterations; for every derivative we also show the upper bounds proved in Theorems 3 and 4.
Note that there is a transient irregular behaviour due to the constants hidden in the proofs.
Figure 4 shows the convergence of Maclaurin polynomials toward the actual PageRank behaviour for a chosen node.
Finally, in Figure 9 we display the approximation obtained with a 100-degree Maclaurin polynomial.
We choose four nodes with different behaviours (monotonic increasing/decreasing, unimodal con-cave/convex) to show that the approximation is excellent in all these cases.
For this experiment we used a 41 291 594-nodes snapshot of the Italian web gathered by UbiCrawler [1] and indexed by Web-Graph [2].
1e-05 1e-06 1e-07






 Figure 3: The convergence speed in the computation of derivatives up to order 4 (the label is the order of the derivative).
The previous corollary is apparently innocuous.
However, as a consequence the data obtained computing PageRank for a given  
 tor function.
We have presented a number of results which outline the  rst analytic study of PageRank when the damping factor changes.
While our results are mainly theoretical in nature, they provide ef cient 3.5e-08 3e-08 2.5e-08 2e-08 1.5e-08 1e-08





 5e-09
 Figure 4: Approximating r. / for a speci c node (cross-shaped points) using Maclaurin polynomials of different degrees (shown in the legend).
ways to study the global behaviour of PageRank, and dispel a few myths (in particular, about the signi cance of PageRank when   gets close to 1).
A last point that is worth being noted is that our algorithm to obtain the Maclaurin polynomials for PageRank may be used to determine new forms of ranking; for example, one may de ne the
 the PageRank curve of node x) is independent from  , and induces interesting rankings that will be studied in a forthcoming paper.
