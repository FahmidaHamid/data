Motivated largely by the success and scale of Google s PageRank ranking function, much research has emerged on e ciently computing the stationary distributions of web-scale Markov chains, the mathematical mechanism underlying PageRank.
The main challenge is that the web graph is so large that its edges typically only exist in external memory and an explicit representation of its stationary distribution just barely  ts in to main memory.
The time required to compute the stationary distribution is on the order of tens Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
of hours to days, and constant factor improvements in running times can save substantial time and money.
Even for the common researcher with interests in ranking research, computing and recomputing vectors of ranks is a time consuming processes that greatly limits research throughput.
As such, much work has been done on accelerating the performance of PowerIteration, the traditional approach to computing stationary distributions.
These optimizations cover a spectrum of techniques, ranging from transformations to the Markov chain that accelerate mixing to e cient heuristic updates that behave like PowerIteration to clever reuse of previously computed solutions.
Most of these techniques are developed and evaluated in isolation, and it is unclear to what degree they can be e ectively combined, both in terms of implementation and performance.
Throughout this note we will frequently refer to vectors, matrices, and the scalars they comprise.
For clarity, we consistently use lowercase letters (x) for vectors and capital letters (A) for matrices.
For each, subscripted quantities (xu and Auv) are used to reference the scalar values at the indexed coordinates.
PageRank [2] is a system of scoring nodes in a directed graph based on the stationary distribution of a random walk on the directed graph.
Conceptually, the score of a node corresponds to the frequency with which the node is visited as an individual strolls randomly through the graph.
For technical reasons, the random walk is also encouraged to occasionally reset to a prespeci ed distribution, overcoming issues of weakly connected components in which a random surfer might get stuck and accelerating the rate at which a random walk approaches the stationary distribution.
A random walk on n nodes can be described by a n   n matrix P , where entry Pvu is the probability that from node u the walk next arrives at node v. Starting from a distribution x over the nodes (x is a vector of n entries that are non-negative and sum to one), after one step the distribution becomes P x, and more generally after i steps the distribution becomes P ix.
We can decompose P into those transitions due to traversing a web link, and those transitions due to random reseting.
Let the sparse matrix A have entries Avu equal to the probability that from node u the walk traverses the link (u, v) to node v. Additionally, we de ne the vector r with each coordinate ru = 1   Pv Avu, equal to the probability that the that the walk chooses not to follow an arc from node u and instead resets randomly to a node v chosen with probability proportional to dv.
P can then be written as P = A + drT , capturing both of the types of transitions.
PowerIteration is the traditional manner of computing the stationary distribution of P , explicitly simulating the dissemination of probability mass by repeatedly applying P to a supplied initial distribution x.
Under modest assumptions, e. g. that all entries of d and r are positive, for any initial distribution x, P ix converges to a unique stationary distribution as i increases.
PowerIteration(P, x)
 (a) Set x = P x While P itself is a dense matrix, every node can reset to any other node, we can e ciently compute P x by viewing it as Ax+drT x.
We assume that A is stored on disk in a sparse format, perhaps as a list of (source, target, value) triples, though there are more compact representations.
Ax is then computed using sparse matrix-vector multiplication: since (Ax)v = Pv Avuxu, we can populate the result vector by scanning the edge  le, for each nonzero Avu adding Avuxu to coordinate v of the result.
We can produce the vector drT x by determining rT x in a pass over r and x, and scaling d appropriately before adding it to Ax.
For acceptable performance, we may only perform sequential access to the edge  le, which is too large to  t into main memory.
Generally speaking, the number of nonzero entries in A is the limiting performance factor, both because of our need to scan over the edge  le to read these entries, and also the random accesses to x each entry requires.
The other operations, vector addition, scaling, and inner product, can all be done using sequential access to main memory.
Oddly, we start our generalization of PowerIteration by restricting the problem we address.
There are many vectors satisfying x = P x; any solution x can be multiplied by an arbitrary scalar value and still satisfy the equality.
Typically we focus our attention on  nding the vector x with kxk1 =
 rT x = 1, and for which x = P x = Ax + drT x = Ax + d .
As we will see in Theorem 1, if x = Ax + d, then x = P x.
Normalized, this vector is the stationary distribution of P .
Consider an analog of PowerIteration in which we repeatedly set x = Ax + d. As with PowerIteration, this iterative process will converge, and it converges to a vector satisfying x = Ax + d. We can monitor convergence through the vector y = Ax   x + d: so long as y is nonzero, x has not yet converged.
But, y also tells us the direction to update x; we advance to the next iterate of x by adding y, yielding Ax+d.
This  rst role is crucial, we must bring y to zero, but we needn t be so rigid as to only ever add y to x.
We might instead add other vectors to x that yield forward progress, maintaining y both as a convergence criteria, but also for guidance in choosing updates to x.
Consider an algorithm that monitors y = Ax   x + d, but is free to choose an arbitrary update vector z at any step, advancing from x to x + z.
It is not hard to appropriately update y, as its new value satis es A(x + z)   (x + z) + d = y + Az   z .
For any update vector z, we can update y by passing z through the matrix A, adding the result to y, and subtracting z.
Intuitively, z is extracted from y and propagated across the links in A, informing nodes of changes in their parent s values and insisting that they now update in turn.
Operationally, this is exactly the algorithmic framework that we will consider.
However, it will be useful not to  x y in terms of a particular A, x, and d, but rather let it be speci ed as an input parameter, properly determined before the method is invoked.
UpdateIteration (A, x, y)
 (a) Choose an update vector z.
(b) Set x = x + z.
(c) Set y = y + (Az   z).
While this framework is presently little more than a system of bookkeeping, we will solidify how one might choose z to shrink kyk1, and which choices lead to e cient algorithms.
We now state two theorems regarding the limit and rate of convergence of UpdateIteration(A, x, y).
The proofs, while short, are rote and unilluminating, and are defered to Appendix A.
We  rst argue that choosing y = Ax   x + d leads to a stationary vector of P = A + drT , but also, in a rather oblique manner, describe where x ends up if we start UpdateIteration(A, x, y) with an arbitrary y.
Theorem 1.
For vectors x, y, d and substochastic matrix A, if y = Ax   x + d and d is a non-negative vector, then de ning the stochastic matrix P = A + drT /kdk1, kP x   xk1   2kyk1 and kxk1   kdk1   kyk1 .
To reiterate, Theorem 1 not only describes the correct initial value of y to arrive at a stationary vector of P = A + drT , but also says that for any A, x, y, if the vector d satisfying y = Ax   x + d is non-negative, then x arrives as the stationary distribution of a random walk on A that resets to a distribution proportional to d.
While the limit of x is well de ned, choosing z arbitrarily clearly need not result in rapid, or any, convergence to this limit.
Much as y leads x to its limit, vectors z whose coordinates agree with those of y also exhibit brisk convergence of kyk1 to zero.
Theorem 2.
If each zu lies between zero and yu, then ky + Az   zk1   kyk1  Xu ru|zu| .
When all ru are equal, the exponential convergence of PowerIteration is a special case of Theorem 2: processing z = y each round reduces kyk1 by a factor of 1   ru.
Moreover, when r is not uniform Theorem 2 gives a tighter characterization of progress than eigenvalue bounds, which are generally in terms of the smallest ru value.
Finally, and critically, Theorem 2 describes progress made when we process an update z 6= y, and informs us as to where in y the progress is being made.
In this section we consider several manners of choosing the vector z in UpdateIteration that give rise to various acceleration techniques.
Most have occurred in some form previously in the literature, and we will discuss the often signi cant di erences between their current and previous incarnations.
In each case we will  nd shortcomings of previous techniques that are resolved by casting them in our common framework.
Additionally, a signi cant advantage is the simple manner in which the techniques now compose, both from an algorithmic and performance perspective.
We also present experimental data detailing the performance of the acceleration techniques and compositions we discuss on a 34M page crawl from 2002 containing roughly
 hosts are sorted by crawl discovery order, and within each host by crawl discovery order.
Several bene ts of such an ordering are discussed in Kamvar et al. [6], who use a more thorough sorting within each host.
We choose to order pages by hosts, independent of the signi cant performance gains noted in [6], because one of our optimizations relies on it, and we require a consistent experimental framework.
For each approach, we plot the total error kP x xk1/kxk1 against computational e ort, measured in units of 800M edges processed, corresponding to the e ort required by a single pass of PowerIteration.
The normalization by kxk1 is required because our vector x need not remain at unit norm, and it would be unfair for a vector to achieve small kP x xk1 simply by virtue of a small x.
In reading the graphs, the acceleration can be seen by in the ratios of e ort along a  xed (horizontal) level of error.
In choosing an update vector z, each coordinate makes a commitment to the update zu it intends, at which point each update is applied to x and propagated through A in parallel.
However, in most implementations these updates will be processed serially, typically reading and propagating each zu in turn.
As the zu may be chosen arbitrarily, there is no need for a node to commit to a particular value until it is needed.
Rather, we can delay the choice of zu until it is needed, conceptually processing a long series of single coordinate updates of the form z = (0, .
.
.
0, zu, 0, .
.
.
0).
Sequential updating allows us to base zu on a value of yu that re ects all updates applied thus far, even those applied in the current iteration, allowing us to propagate the e ect of a single update multiple times in a single pass over the edge  le.
Even if the nodes are ordered randomly, roughly half of the edges will point forward in the node order.
Updates passed along these edges will be processed again before we complete a pass over the edge  le.
Well organized graphs can bene t even more, with updates pushed along entire acyclic subgraphs in one pass.
Sequential updating is based on a speci c ordering of nodes, and clearly some orderings are better than others.
The ordering we use is based on crawl order, which has the peculiar property that 80% of the edge point backwards; crawling very quickly discovers pages with many incoming links, and placing them early reverses the direction of the bulk of their links.
As any ordering can easily be run in both orders, forwards or backwards or both, we will also consider the sequential updating in reverse order.
Experimentally, alternating direction, forwards then backwards, performs more poorly than either unidirectional approach.
This is peculiar, and merits further investigation.
Figures 1 and 2 compare traditional PowerIteration (PI) against sequential updating (SU) and sequential updating applied in reverse order (R-SU).
It should be stressed that these techniques exhibit exactly the same data access patterns as traditional PowerIteration, passing linearly over the edge  le and probing u in main memory for each edge Auv.
The approaches di er only in what they do for each Auv (and the direction of scan, for R-SU).
Their running times are e ectively identical to PowerIteration.
r o r r
 l t a o
 1e+00 1e-01 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08








 Figure 1: Sequential Updates: Total Error We see acceleration of nearly 2x and 3x for sequential updates on the crawl ordered and reverse crawl ordered graphs, respectively, with the gap between SU and R-SU diminishing with time.
r o r r
 m u m x a
 i 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08 1e-09








 Figure 2: Sequential Updates: Maximum Error Here we see again a lead of reverse crawl ordering, but the lead is less initially, growing after several iterations.
Related Work: Sequential updating is similar to the Gauss-Seidel approach described by Arasu et al. [1], in which one sequentially sets xu = Pv Puvxv, using the most current values of xv rather than those of the previous iteration.
In contrast with UpdateIteration, the Gauss-Seidel approach requires the graph s edges to be grouped by destination, rather than source, which can substantially complicate data maintenance.
A large fraction of links in the web graph are intra-host, and as such it is common to group pages by host for locality bene ts, discussed in [6].
Given such a grouping, after processing the nodes associated with a host, a large fraction of the propagated update z will return to nodes on that host.
While this may seem frustrating at  rst, recall that Theorem 2 says that kyk1 decreases by at least ru|zu|, independent of where Az ends up.
Moreover, various caches will retain the data used to process this group, making reprocessing it very e cient.
Rather than process the next group, reading sequential edge data from disk and probing yv in main memory, we can reprocess the current group, reading sequential edge data from main memory and probing yv in the L3 cache.
The latter is substantially faster than the former, and represents a good payo  so long as substantial updates remain in the group.
As the intra-host edge density is high, we might perform several iterations on a group before its y updates dissipate to other groups.
Another popular grouping is by strongly connected component.
Ordered topologically, there is no reason to advance from a component until it has satisfactorily converged, as there are no edges along which updates from subsequent components may return.
This approach has the decided advantage that the working set of edges and nodes at any point in time is only as large as the associated strongly connected component, each of which is visited only once.
The main disadvantage is that computing strongly connected components is di cult in external memory, and an approximation should probably be used instead.
Notice that we do not actually require that the grouping have no back edges, but the fewer that exist, the fewer updates return upstream and the more e ective each pass is.
Strongly connected components ensure that one pass su ces, but groupings that simply have low reverse edge density are highly e ective as well.
There are other interesting groupings that one can imagine (we will discuss some more in Sections 3.3 and 3.4) and the question quickly emerges of which one should be used.
In fact, we can use several.
Our main constraint is that we access the edge  le sequentially, and therefore we must collocate edges from nodes in the same group.
If we have the disk space to maintain multiple edge  les, we can produce an edge  le for each grouping, and choose to use a particular edge  le based on our needs at the time.
In reading the edge  le, we process the collocated nodes in a group, and can easily make multiple passes over this data without tripping over the intervening edges in the original edge  le.
This approach does not give us locality of reference in the y vector, as we have not actually changed node indices.
Figures 3 and 4 examine the performance bene ts of grouping by host and processing each one, two, and three times.
We also examine 10x reiteration, though only to demonstrate its limit.
As we count operations instead of measure execution time, we will need to make some assumptions about the execution time of subsequent passes.
For presentation reasons, we will assume that subsequent iterations are free, which is clearly false.
However, the 2x and 3x reiterations result in acceleration of nearly 2x and 3x, re spectively, so acceleration clearly exists for more pessimistic assumptions.
Actual runtimes suggest that subsequent iterations are cheap, with 2x and 3x reiteration taking roughly 1.25 and 1.50 times as long, respectively, in a not especially well controlled environment.
Also, we only need to process the intra-group edges while reiterating, propagating updates along inter-group edges only once  nished.
r o r r
 l t a o
 1e+00 1e-01 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08










 Figure 3: Reiteration: Total Error The acceleration for total error is almost 2x and 3x over SU, suggesting that reiterated updates can be nearly as effective as multiple passes over the matrix.
Of course, there are diminishing returns, visible as 3x and 10x converge.
r o r r
 m u m x a
 i 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08 1e-09










 Figure 4: Reiteration: Maximum Error Additional iterations help maximum error as well, but the diminishing returns are even more pronounced.
Related Work: Arrangement by strongly connected components has appeared several times in various forms.
Eiron et al. [4] note that pages with no outlinks form a large fraction of the web and describe how to infer their ranks from the stationary probabilities of a modi ed graph with these nodes removed.
More generally, Arasu et al. [1] and Langville and Meyer [10] view the problem as a block upper triangular linear system, processing strongly connected components in turn.
Their techniques focus on decomposing the Markov chain, and require a strict topological order.
These approaches are captured by reiteration over the equivalent grouping.
Moreover, UpdateIteration can take advantage of groupings that are only mostly topological.
This  exibility addresses concerns of the substantial e ort needed for data organization and maintenance, and enables grouping by host/domain, which was not possible in the more rigid block triangular techniques.
While zu = yu is clearly one e ective choice, an alternate choice is zu = 0.
In e ect, we can choose not to update node u.
Clearly if yu = 0 we need not expend e ort to propagate yu through A, as we will simply be adding zero to several locations in y.
Even when yu is nonzero but small, we may want to defer the update until the gains are more in line with the typical entry.
With this in mind, there are various predicates we could use to decide if we should process yu.
We will speci cally consider: E ort : Set zu = yu i  |ruyu| degu   avg v   |rvyv| degv   , selecting entries with the highest anticipated progress |ruyu| per expended e ort degu.
There are other predicates that could be used, each resulting from a di erent view of which entries are important to process.
Examples include choosing those entries with largest relative error |(P x   x)u|/|xu| or those entries whose range of possible ordinal ranks is largest.
Selective updating has some interesting interaction with sequential and reiterated updates.
As we run a pass of sequential updates the average value of |rvyv|/degu will change, and while we could maintain the average exactly by carefully watching the changes in y, we can also do a more e cient approximation by assuming that kyk1 decreases by exactly ru|zu|.
Reiteration is similar, in that each reiteration lowers the weight in a group markedly, by a factor of at least 1 ru, but not the average value over all of y.
We could base our decisions on the group s average, shrinking with the values we consider, or on the entire average over y.
While the gains of selective updating in terms of computation and memory accesses are clear, savings in terms of disk accesses are less so.
It is not possible to skip entries on disk at no cost; data is read from disk in blocks, and the cost is amortized over all entries in the block.
Likewise, disk prefetching will prepare subsequent blocks cheaply, and it is unclear that we gain anything by ignoring edge data passed to us.
To address this somewhat, it is certainly possible to apply selective updating at a coarser scale than the node level.
One could skip entire groups of entries at a time, permitting a volume of edges to be passed over at once and resulting entire disk blocks skipped.
Alternately, Kamvar, Haveliwala, and Golub note in [5] that some pages converge more slowly than others, determining which these are at runtime by observing their relative change in ranks each round.
While they use converged values to cull edges associated with converged nodes, we might base a grouping scheme (a la Section 3.2) on convergence rate, determined in a similar manner at run time.
Emitting an appropriately grouped edge  le can be done e ciently in a single pass so long as the number of groups is not terribly large.
This organization of the edge  le allows e cient passes over pre xes of the edge  le, letting us e ciently process each group at an arbitrary rate.
Understanding and experimenting with coarse-grained selective updating and grouping is interesting future research.
Figures 5 and 6 examine the E ort predicate applied to previous schemes (denoted in the  gures labels by  E+ ).
The acceleration we see here is substantial, as selective updating takes advantage of the initially high variability of magnitudes in y.
We stress that actual acceleration will be less, although some may be recouped via clever grouping.
r o r r
 l t a o
 1e+00 1e-01 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08


 E + 3x SU





 Figure 5: Selective Updating: Total Error We see substantial acceleration in terms of edges processed, which is, admittedly, a somewhat suspect measure.
The message is that the work that needs to be done is less.
r o r r
 m u m x a
 i 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08 1e-09


 E + 3x SU





 Figure 6: Selective Updating: Maximum Error Initial acceleration is especially pronounced here, as the selective updates immediately leap two orders of magnitude.
Related Work: Selective updating can be seen in the work of Kamvar, Haveliwala, and Golub [5], who describe a power iteration process wherein entries xu that appear to have converged are frozen, saving them the recomputation of these entries every iteration.
Moreover, edges associated with the frozen nodes are trimmed from the edge  le, reducing disk IO required.
Freezing is analogous to the non-transmission of an update, and the edge trimming is clearly the basis of the grouping discussed previously, though more  nal.
The main di erence between [5] and UpdateIteration is that in the former the recipient decides whether an update will be propagated, and it is conceivable that signi cant updates may be ignored.
This is particularly evident during incremental changes to the web graph.
As the matrix A changes over time, previous stationary distributions prove good starting points for converging to the new stationary distribution.
But if only a few links change, entries of xu not incident to a changed edge will remain stationary after an iteration and will be frozen and never updated.
Over time the adjacency structure of the web changes, and we will want to compute the stationary distribution of a new chain that di ers from the old in a relatively small number of locations.
The stationary distribution of the old chain is generally viewed as a good  rst approximation, and indeed it is easy and intelligent to restart PowerIteration on a new chain using the old x.
Restarting UpdateIteration from a speci c x appears nontrivial, as we must compute y = Ax   x + d, involving a matrix multiplication.
In fact, the process is much simpler: Let A and B describe the old and new edge transition matrices, and consider the two associated update vectors yA and yB, yA = Ax   x + d and yB = Bx   x + d .
We can relate the update vector yB to its antecedent yA as yB = yA + (B   A)x .
This equivalence shows how to e ciently update yA to yB, allowing us to e ciently reinvoke UpdateIteration(B, x, yB).
The e ort required in this matrix-vector multiplication is proportional to the number of nonzero entries in B   A, corresponding to the number of changed edge weights.
Several approaches to personalization of PageRank are based on personalization of the reset distribution [8, 9], shifting in uence to those sites that the distribution favors, and the site linked by them.
Recall from Theorem 1 that x converges to the stationary distribution of the chain with reset distribution d = y   (Ax   x).
Personalization of the reset distribution is easily performed by incorporating any changes to d into y instead, changing x s limit appropriately.
It is worth stressing that x s limit is de ned by the distribution proportional to d, and we need not worry about renormalizing d if we only make a few changes.
Finally, much of research into Markov chain based ranking research is exploratory: the best setting of weights in A and vectors d and r are not known.
Uniform weights seem natural as defaults, but are clearly primitive choices.
Exploring link weighting schemes based on content analysis or resetting policies based on content quality require e cient recomputation of ranks.
Each of these explorative choices: updating Avu, ru, and du values, are easy in UpdateItera-tion, corresponding to simple updates to y.
In these three cases above, the changes to the Markov chain often result in sparse updates to y: most of the edges in the graph are stable between recrawls, and much of per-sonalization of reset distributions is localized (upweighting a few trusted/bookmarked pages, for example).
In this context, selective updating of Section 3.3 is well suited to e -ciently process just those substantial entries, and leave the converged regions of the graph untouched.
Of course to accommodate this properly, it makes sense to maintain an edge  le of those parts of the graph that experience frequent edge churn, so that we needn t pass over the entire graph.
The  ne granularity of sequential updates also allows a very smooth incremental update: we can decompose any update to the adjacency matrix into a set of small updates to the links of each node, which we apply as we visit each node.
We need not pause the system to compute (B   A)x, but can apply the implications of changes at each node in turn.
This becomes all the more relevant in a distributed setting where such pauses could destroy parallelism.
Figure 7 compares various techniques applied to a converged vector y that has had 1000 random positions updated randomly by  1/n, emulating either a change in the link structure or reset distribution.
For small initial kyk1, the scale of the updates does not a ect the shape of the curves, so the choice 1/n is arbitrary.
r o r r
 l t a o
 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08 1e-09 1e-10


 E + 3x SU






 Figure 7: Incremental Updating It is di cult to characterize the acceleration of the incremental updates by a multiplicative factor, as it is clearly a di erent shape than the standard curves.
Several orders of magnitude are gained immediately, with the slope arriving at the shape of Figure 5 as the initially concentrated y vector is distributed more uniformly.
r o r r
 m u m x a
 i 1e-06 1e-07 1e-08 1e-09 1e-10 1e-11



 E + 3x SU





 Figure 8: Incremental Updating Maximum error exhibits the same behavior as total error, dropping rapidly as the initially sparse vector is dispersed.
The initial hiccup again re ects the sensitive nature of the maximum error measure.
Related Work: Chien et al. [3] describe an approach to incremental updating that is based on the construction and analysis of a new a Markov chain on nodes within a modest neighborhood of the graph changes, and a supernode representing nodes outside this horizon.
Their approach is similar in spirit to ours, in that attention is restricted to the relatively small region where change may occur.
However, rather than  x a region and degree of accuracy, Up-dateIteration discovers where updates are needed as it goes, accommodating any degree of accuracy  uidly.
Haveliwala [8] and Jeh and Widom [9] have done work on e cient personalization, observing that the function mapping reset distributions to stationary distributions is linear.
This enables very e cient manners of synthesizing personalized PageRanks from a set of precomputed PageRanks based on various reset distributions.
For example, a page s du value can be increased by folding in the stationary distribution of a random walk that resets to only that page, exactly analogous to increasing and propagating yu.
Our ability to choose z arbitrarily has implications for  oating point error.
We have the  exibility to always choose zu to be a power of two, so that its addition to x will result in nominal  oating point loss.
This is harder to guarantee with y, as transmission along weighted edges will change zu from a power of two.
Understanding and improving  oating point behavior has positive implications for the introduction of strength reduction and low precision arithmetic, of particular interest in this setting where maintaining all of x or y in memory is challenging.
Additionally, UpdateIteration propagates and combines updates zu, which are typically of smaller magnitude than the xu values that PowerIteration operates with, and the precision maintained is thus higher.
If we remove the sequential behavior from sequential updates, we see that updates in UpdateIteration can actually be totally asynchronous.
Moreover, our choices for zu are made locally with only a modicum of global information.
This allows for a very smooth distributed implementation, in which the only coordination between compute nodes that is required is eventual communication of the updates applied.
We can delay and reorder inter-node z transmissions until the updates are signi cant, batching and trimming network overhead.
Clearly, the best update schedule is highly dependent on the system topology, and we refrain from giving explicit suggestions here.
In an extreme case of delay, a compute node may be unavailable for a long period of time or even crash.
Other compute nodes can continue in its absence, functioning under the belief that the PageRanks associated with that compute node simply have not changed.
If the node comes online again it simply reenters the computation, transmitting and receiving updates.
As noted for incremental updates, the granularity of sequential updating is very  ne, and the amount of work needed to roll forward from any checkpoint can be arbitrarily small.
The Markov chain we have studied simulates the propagation of probability mass through a directed communication network whose nodes happen to be computational agents.
The propagation of updates is easily performed within the communication network, as updates are only transmitted along links.
The initial values of x = 0 and y = d are easily chosen, as d needn t be normalized.
As the network changes, in the incremental fashion suggested by Section 3.4, the necessary updates to y are computable by the source of the edge that has arrived or departed.
Gracefully departing nodes removing their incoming edges using this update mechanism and apply the update zu = yu   du before departing.
We have examined an algorithmic reformulation of the traditional power iteration algorithm based on the propagation of updates rather than values.
UpdateIteration enables several algorithmic optimizations that result in more e cient convergence.
Moreover, the optimizations are well suited to the problems of incremental and personalized updates to the underlying Markov chain, and permit  exible operation in a distributed setting.
The optimizations presented here are likely just a sampling of what can be done to accelerate computation of PageRank.
These optimizations are intended to take advantage of particular features of computer systems, and it seems likely that other features may yet be exploited, both for performance and potentially quality of ranking.
Techniques such as Arnoldi Iteration and unsymmetric Lanc-zos are tempting targets, as is the power extrapolation approach of Kamvar et al. [7].
Additionally, there is work to be node exploring the new possibilities enabled through ef- cient PageRank computation.
The author would like to thank several people who contributed constructive ideas and observations.
Michael Is-ard, Steve Chien, Kevin McCurley, the participants of the Workshop on Search and Meta-Search, and the anonymous reviewers all gave valuable comments which have greatly improved the presentation.
