Textual ads are ubiquitous on the Web today, as they appear on a wide variety of Web pages ranging from obscure forums to major search engine result pages.
Online textual ads connect advertisers to prospective customers, and clicks on these ads by Web users account for a signi cant fraction Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
of the revenue for many online businesses: from news publishers to blogs to Web search engines.
As of 2008, textual advertising comprised approximately 40% of the $22 billion online advertising market, which is predicted to double in size over the next 5 years.1.
Textual ads are distributed via two primary channels, namely, sponsored search and content match.
In sponsored search, textual ads are shown alongside of the Web search results, while in content match, contextually relevant ads are displayed on third-party Web pages.
Historically, sponsored search evolved by allowing advertisers to explicitly bid on queries (bid terms2) that they wished to display their ads for.
In this paradigm, the burden of ad selection was placed primarily on the advertisers, since they needed to skillfully choose a comprehensive list of queries relevant for their ads.
This scenario is called exact match, since the query and the bid term must match exactly for an ad to be shown.
However, it quickly became apparent that it is virtually impossible to explicitly enumerate billions of less popular  tail  queries, hence it is impractical to cover them via exact match, even though such queries provide valuable advertising opportunities.
To unlock the revenue potential of these numerous yet individually infrequent queries, the advanced match method was introduced.
Here, the query and the bid term no longer need to match exactly, and ads are selected algorithmically by the search engine.
Recently, information retrieval techniques have been proposed for advanced match by indexing the ads as documents using the ad text visible to the user as well as the ad s bid terms [25, 4, 5].
Ads are then selected using an ad query that is generated from the user s query (sponsored search) or the Web page on which ads are to be displayed (content match).
The ad query is executed against the index of ads using standard IR matching and ranking techniques.
Most implementations of advanced match make the simplifying assumption that ads are atomic units that are independent of each other, even though ads from the same advertiser could be quite similar or nearly identical.
In practice, however, textual ads are de ned and organized as a structured database with several types of entities, as shown in Figure 1.
Each advertiser has one or more accounts.
Each account in turn contains several ad campaigns, which have di erent temporal or thematic goals (e.g., sale of home appliances during the holiday season).
Campaigns

 terms in a sponsored search setting.
Therefore, we use these concepts interchangeably throughout the paper.
consist of ad groups, which include multiple creatives (the visible text of the ad) and bid terms.
Bid terms correspond to di erent products or services o ered by the advertiser, while creatives represent di erent ways to advertise those products.
Any creative can be paired with any bid term within the same ad group to create an actual ad displayed to the user.
Search engines usually allow ad groups to contain a few dozen creatives and up to a thousand bid terms.
This type of ad schema has been designed with the advertisers  needs in mind, as it allows the advertisers to easily de ne a large number of ads for a variety of products and marketing messages.
In view of this hierarchical de nition, the ad retrieval problem can be formulated as the retrieval of (cid:104)creative, term(cid:105) pairs from a structured schema.
In this paper we study the key issues in postulating ad retrieval as a structured retrieval problem, where the unit of retrieval is de ned hierarchically.
We also discuss several crucial tradeo s that must be analyzed in this unique retrieval scenario.
Na vely indexing all possible retrieval units (i.e., all possible (cid:104)creative, term(cid:105) combinations) using standard IR indexing approaches would result in a signi cant amount of wasted storage, since each creative and bid term will be indexed multiple times due to the Cartesian product semantics.
Most of the inverted indexing algorithms used in modern search engines incur increased cost with larger index sizes, making the na ve approach infeasible in practice.
To avoid this problem, we explore several hierarchical indexing schemes that signi cantly reduce the amount of duplication.
In our study, we also quantify the impact that di erent indexing strategies have on ad retrieval e ectiveness.
We further develop a novel ranking algorithm that exploits the hierarchical structure and is both more e cient and more e ective than the na ve indexing approach.
This is done by employing a multi-phase retrieval approach, where we  rst retrieve an ad group, then select an optimal creative, and  nally choose a bid term that makes the resulting ad the most relevant for the given query.
The main contributions of this paper are threefold.
First, we propose novel, e cient ways of indexing sponsored search ads.
We  rst transform the ad corpus to a collection of hierarchically structured textual documents, and then adapt standard IR indexing techniques to construct a compact yet e ective ad index.
The approaches we propose can also be applied to other retrieval tasks where the retrieval units are structured hierarchically.
Second, we propose several ranking strategies that leverage the hierarchical structure of the ad corpus to achieve more accurate ad ranking.
Finally, we conduct a large-scale evaluation using sponsored search data from a large commercial search engine that helps quantify, in a real world setting, the usefulness of our proposed structured indexing and retrieval approaches.
The remainder of the paper proceeds as following.
In Section 2 we survey the related work.
In Sections 3 and 4 we explain our novel structured indexing and retrieval approaches, respectively.
Section 5 discusses our experimental evaluation carried out over real-life sponsored search data from a large commercial search engine.
Finally, Section 6 concludes the paper.
AdvertiserAccount 1Account 2Account 3Campaign 1Campaign 2Campaign 3Ad group 1Ad group 2Ad group 3CreativesBid phrasesAd.........Brand name appliancesFree delivery before Cristmaswww.appliances-r-us.com miele stovemiele microwavekitchen Aidcusinartwolf...Kitchen appliances...Buy appliances on Black FridayNew Year deals on lawn & garden toolsBrand name appliancesBuy one get second freewww.appliances-r-us.com Best deal on appliances!Buy now and get 20% offwww.appliances-r-us.com ...WWW 2010   Full PaperApril 26-30   Raleigh   NC   USA1022.
BACKGROUND AND RELATED WORK Textual advertising is a signi cant source of revenue for modern Web search engines.
It is therefore not surprising that there is a growing research interest in designing better algorithms for sponsored search.
Advanced match has been a focus of a few recent studies, most of which focus on the query rewriting paradigm [13, 23, 15].
However, recent work shows that employing a more  exible advanced match paradigm, which uses IR techniques to index ads as documents, leads to signi cant improvements in sponsored search performance [4, 25, 5].
Standard IR techniques such as tf-idf weighting [4], pseudo-relevance feedback [5], query reformulation [3, 25], clustering [29], document categorization [4, 1] and supervised learning of optimal ranking functions [16] have been shown to be highly bene cial for improving the quality of the retrieved ads.
Structured indexing and retrieval is a research  eld that has received increasing attention, especially due to the spreading adoption of the XML format for representing documents [10, 17].
Techniques that take into account document structure were shown to be bene cial in a variety of domains ranging from Web search [26] to book retrieval [14] to desktop search [9].
To the best of our knowledge, our work is the  rst to apply the principles of structured retrieval in sponsored search.
In many domains, a simple linear combination of scores from di erent document  elds is su cient for improving retrieval performance in the structured retrieval setting [2, 7,
 ward approach is not su cient.
As we show in Section 4.1, due to the heterogeneity inherent in the ad databases, more elaborate feature design and parameter estimation techniques are necessary to achieve performance gains over the baseline that does not employ any structural information.
Structure was also recently found to be useful for the text classi cation task [22].
Speci cally for sponsored search, Raghavan and Hillard [24] reported that a weighted combination of features based on creative and term  elds helps reduce the number of irrelevant ads shown to the user (a task called ad  ltration).
In this work, we show that adding features that exploit the hierarchical structure of ad groups helps to further improve the accuracy of ad  ltration.
As explained in Section 1, our target retrieval unit is a (cid:104)creative, term(cid:105) pair, which together comprise a displayable ad.
Thus, our ultimate goal is to produce a ranked list of relevant (cid:104)creative, term(cid:105) pairs.
Our index contains two types of  elds: creative  elds (c) and term  elds (t).
Each creative  eld consists of three sub elds: title, description and URL, as shown in Fig. 1.
Each ad group g consists of several creative and term  elds, grouped by an advertiser (see Fig. 1).
The set of all ad groups is denoted G, and the sets of all creatives and terms (for all the ads groups) are denoted C and T, respectively.
The set of creatives or terms associated with a speci c ad group g is denoted Cg and Tg, respectively.
Hence, the total number of unique  elds associated with a given ad corpus is: |C| + |T| = |G|(|Cg| + |Tg|), (1) where |Cg| and |Tg| denote the average number of creatives and terms per ad group, respectively.
Although our structured retrieval approach is general enough to incorporate any type of scoring functions, in this work we take the well known language modeling approach to information retrieval [8, 33].
In this approach, indexing units are scored by their probability of generating the query terms [21].
Formally, given a query q and an indexing unit u, we score each unit using a unigram language model (cid:89) wi q p(q|u) = p(wi|u), where p(wi|u) is estimated using Dirichlet smoothing [34], so that the  nal scoring function is as follows: scq(u) (cid:44) log tfwi,u +   (cid:80) |u| +   tfwi ,C wj C tfwj ,C , (2) (cid:88) wi q where tfw,k is the number of occurences of a term w, in either a particular indexing unit (k = u) or the entire collection (k = C), and   is a free parameter, which controls the amount of smoothing.
Indexing units are then ranked in descending order, based on their score.
As the user typically only sees a limited number of sponsored search results in response to her query, we assume that only a list of top K indexing units [u(1), .
.
.
, u(K)] is retrieved in practice.
Each unit u in the list is associated with an ad group identi er gIdu.
To promote diversity and advertiser coverage in the ranked list, we can require that [gIdu(1), .
.
.
, gIdu(K)] are unique, thereby limiting the number of ads retrieved from a single ad group to one.
We now describe three proposed indexing strategies for sponsored search.
These strategies take into account the ad structure outlined in Section 1.
The strategies presented in this paper investigate the indexing of the two lower levels of the hierarchal ad structure (Fig. 1), namely the ad group and the creative-bid term levels.
The underlying principles of these strategies, however, are general enough to easily extend them to the higher levels of ad hierarchy, which we intend to do as part of future work.
The strategies presented di er in their choice of the atomic indexing unit and their ranking algorithms.
Table 1 provides a formal de nition of indexing units and estimated index sizes for each strategy.
In what follows, we provide a detailed description of these indexing strategies and their applications to sponsored search.
In the  rst indexing scheme, we index units that are composed of (cid:104)creative, term(cid:105) pairs (cid:104)c, t(cid:105).
This is the most  ne-grained indexing unit, and this approach e ectively indexes the Cartesian product of the creatives and terms in each ad group.
Since we index all possible (cid:104)creative, term(cid:105) pairs, the retrieval with this index is a single-level process   no postprocessing is required, since indexing units correspond to displayable ads.
As we shall see below, other (more compact) indexes require some additional ranking after the initial retrieval is performed.
The CTInd index, on the other hand, only requires  ltering by gIdt, as we retrieve a single displayable ad per ad group (deduping).
Algorithm 1 shows the retrieval algorithm pseudocode.
CTInd CrtvInd AdGrpInd {(cid:104)Cg, Tg(cid:105) : g   G} Indexing unit {(cid:104)c, t(cid:105) : c   Cg, t   Tg, g   G} {(cid:104)c, Tg(cid:105) : c   Cg, g   G} # indexing units # indexed  elds |G||Tg||Cg| |G||Cg|

 |G||Cg|(1 + |Tg|)
 Table 1: Summary of the three index versions.
Algorithm 1 CTRank (CTInd index)

 3: return (cid:104)c, t(cid:105) In the CTInd index, the average number of indexing units per ad group is a product of cardinalities |Cg||Tg|, and each such indexing unit contains two  elds (i.e., a creative and term  eld).
The expected number of  elds indexed by the CTInd index is, therefore, 2|G||Cg||Tg|.
The indexing unit in this index is a single creative c coupled with all the bid terms associated with its ad group gIdc.
This is a much smaller index that CTInd, since it no longer computes a Cartesian product of every creative and every term.
Using this index, in order to retrieve a (cid:104)creative, term(cid:105) pair (cid:104)c, t(cid:105), we  rst retrieve a ranked list of creatives,  lter them by gIdc and then retrieve the term with the highest score associated with each creative in the index; the score is computed according to Eq.
2.
Algorithm 2 shows the retrieval algorithm pseudocode.
Algorithm 2 CrtvRank (CrtvInd index)


 4: return (cid:104)c, t(cid:105) scq(t) : c   c] In the CrtvInd index, for each creative we index, on average, 1 + |Tg| term  elds, and there are total of |G||Cg| creatives in the collection.
The expected number of indexed  elds in this index is, therefore, |G||Cg|(1 + |Tg|).
In this approach, the indexing unit is the ad group itself.
In order to retrieve a (cid:104)creative, term(cid:105) pair,  rst a ranked list of ad groups is retrieved.
Then, for each ad group, a creative and a term with the highest scores are retrieved.
Since the creative and the term scores are independent, and assuming that the scoring function is monotonic3, the retrieved (cid:104)c, t(cid:105) pair is the pair with the highest score in the ad group.
The algorithm pseudocode is presented in Algorithm 3.
Note that the AdGrpInd index is the only index type with no duplicated  elds.
The number of indexing units is the same as the number of ad groups, |G|, and for each ad group there are, on average, |Tg| + |Cg|  elds.
Therefore, the number of indexed  elds is |G|(|Tg| + |Cg|), which is also the number of unique  elds in the ad corpus, |C| + |T|, as shown in Eq.
1.
This is the most compact index of the three alternatives we explore.
Note also that this indexing scheme eliminates
 positive contribution to the overall score.
the need for  ltering the results by the ad group identi er gIdu, since by de nition each retrieved ad will be constructed from a di erent ad group.
Algorithm 3 AdGrpRank (AdGrpInd index)


 4: return (cid:104)g, c, t(cid:105) scq(c) : g   g] scq(t) : g   g]

 In the previous section, we discussed the basic ranking algorithms for each of the proposed indexing structures.
In this section, we show that the basic ranking is sometimes not su cient to produce the best retrieval results.
As the size of each indexing unit grows, which is the case with the CrtvInd and AdGrpInd indexes, we face challenges that hinder the performance of the retrieval models.
To address these challenges, which are described in Section 4.1, we propose a structured reranking strategy in Section 4.2.
In the indexing strategies described in Section 3.2, the indexed and retrieved units are hierarchically structured from atomic and composite  elds.
Previous work on structured document retrieval shows that a combination of  eld scores often yields better retrieval performance than matching each  eld independently [7, 20, 2].
To test this hypothesis, we design a simple experiment that combines the scores obtained for the ad group and the (cid:104)creative, term(cid:105) pair retrieved by AdGrpRank (Algorithm 3).
Formally, ads are reranked based on a mixture of scores: scq(g, c, t) =  scq(uc,t) + (1    )scq(g), (3) where uc,t is a (cid:104)creative, term(cid:105) pair (cid:104)c, t(cid:105) treated as a single indexing unit, and scq( ) is as de ned in Eq.
2.
Fig. 2 shows the retrieval performance of this rerank-ing (as measured by NDCG@10 [11]).
Setting   = 0 in Eq.
3 produces ranking that is equivalent to that of AdGr-pRank, while setting   = 1 produces ranking that ignores the ad group structure, and is, therefore, equivalent to that of CTRank.
It should be noted that in contrast to previous work, where mixture models usually yield improvements [7, 20, 2], combining the (cid:104)creative, term(cid:105) pair score with the ad group score is detrimental   setting   = 1, and ignoring the ad group structure, yields the best performance.
We postulate that this is a result of several key traits that di erentiate sponsored search from other information retrieval tasks.
First of all, sponsored search is characterized by the large variance in ad group lengths.
While some ad groups contain only a few bid terms, others can have up to 1,000 bid terms mixture model in Eq.
3.
Since this model does not succeed in outperforming the baseline method that uses no structural information (CTRank), we expand the set of features used by StructRank to address the issues speci c to sponsored search that were described in Section 4.1.
As we show in Section 5, using this expanded set of features results in substantial performance improvements on a number of tasks.
Algorithm 4 StructRank (AdGrpInd index)

 3: return (cid:104)g, c, t(cid:105)
 We now de ne the features used by StructRank, as well as its parameter optimization.
Recall that the features have the form f (g, c, t), and are therefore de ned over a (cid:104)c, t(cid:105) pair and an ad group associated with it.
The features used are as follows.
  crtvTermPairScore is the score of the given (cid:104)creative, term(cid:105) pair.
It is equivalent to the scq(uc,t) component in the mixture model in Eq.
3.
  adGrpScore is the score of the entire ad group, from which the (cid:104)creative, term(cid:105) pair was selected.
It is equivalent to the scq(g) component in the mixture model in Eq.
3.
  adGrpTermCount is the number of term  elds in a given ad group.
As previously mentioned, a number of bid terms associated with an ad group can vary signi cantly.
Since document length can a ect the document s prior probability of retrieval [27], we explicitly add the number of bid terms as a feature in our model.
  adGrpEntropy is the entropy of an ad group.
The entropy is computed over the individual words of the ad group as w g pg(w) log pg(w), where the probability of word wi is computed using a maximum likelihood estimate pg(wi) = .
Following previous work, where entropy was  (cid:80) (cid:80) tfwi,g wj g tfwj ,g found related to document heterogeneity [2], we use entropy as an estimate of ad group cohesiveness ad groups with smaller entropy will tend to be more cohesive.
  adGrpQueryCover produces a real number r   [0, 1], such that r is the ratio of query words  covered  by any  eld in the ad group.
It is common, especially for longer queries, that not all query terms will appear in the selected creative and term pair.
The adGrpQueryCover feature help di erentiate between the ad groups that achieve high relevance scores due to a disproportional repetition of a single query word (bid term spamming) and those that achieve high relevance scores due to a more comprehensive coverage of query words.
  adGrp[Field]Ratio is the fraction of  elds of type [Field] in an ad group that match at least one query word.
Intuitively, we expect that ad groups that have high  eld match ratios w.r.t.
query, will yield more relevant (cid:104)creative, term(cid:105) pairs, since these ad groups will tend to be more focused on the query topic.
[Field] denotes either one of the sub elds of the creative  eld, or the entire term  eld.
This produces three features based on the location of the match: adGrpURL-Ratio, adGrpTitleRatio and adGrpTermRatio.
Therefore, we have a total of 8 features.
Of course, adding other features is possible, but we limit ourselves to this small well-de ned set of features for the purpose of this study.
Figure 2: Mixture method retrieval performance with 0       1.
Dotted line indicates the performance attained by CTRank with no mixture.
associated with them.
This causes a strong length bias: the probability of shorter documents (ad groups with smaller number of terms) to be retrieved is higher than their probability of being relevant.
While length bias is a well known phenomenon in information retrieval [27], its e ects are more pronounced in sponsored search, since the latter has a much higher variance of document lengths.
Another issue that is unique for sponsored search is the cohesiveness of the ad groups.
In traditional retrieval, it is usually assumed that documents, even structured ones, are about a single topic; however, this assumption does not necessarily hold for ad groups.
A set of bid terms associated with an ad group can range from being focused and cohesive (associated with a single service or product) to being fragmented or even scattered (associated with several products or even with a broad set of generic bid terms), depending on the strategy of the advertiser.
There is also a possibility that some advertisers might misuse the bidding mechanisms by purposefully associating unrelated terms to their ad groups in an attempt to attract more customers (spamming).
Since existing structured retrieval techniques do not address these issues, we propose a novel structured rerank-ing approach that is speci cally tailored towards sponsored search.
Our approach relies on the ad structure and employs features that go beyond the simple mixture model in Eq.
3.
Our structured reranking method is a generalization of the mixture model presented in the previous section.
It takes into account a given (cid:104)creative, term(cid:105) pair and the associated ad group.
The method assumes we do an initial retrieval round using AdGrpRank.
Then, the method reranks the initially retrieved ads using a linear model n(cid:88) rScq(g, c, t) (cid:44)  ifi(g, c, t), (4) i=1 where fi( ) is a feature function,  i are weights assigned to each function, and n is the number of such functions used for the reranking.
We refer to this reranking procedure as StructRank, and the entire retrieval procedure is described in Algorithm 4.
Clearly, the choice of features used in StructRank plays a crucial role in the resulting algorithm performance.
Lim-Mixture MethodlnDCG@1000.10.30.50.70.910.650.700.750.80WWW 2010   Full PaperApril 26-30   Raleigh   NC   USA1054.2.2 Reranking Optimization In this section, we present our method for optimizing the free parameters  i in the ranking function in Eq.
4.
When the number of free parameters is small (as, for instance, is the case in Eq.
3), it is possible to optimize the parameters using an exhaustive search over the parameter space.
However, when more features are introduced, such an exhaustive search quickly becomes infeasible.
To address this problem, we rely on a large and growing body of literature on the learning to rank methods for information retrieval (see Liu [18] for a survey).
Learning to rank methods allow e ective parameter optimization for ranking functions with respect to various retrieval metrics, even when a number of free parameters is high.
In this work, we employ a simple yet e ective learning to rank approach that directly optimizes the retrieval metric of choice (e.g., NDCG@k).
It is easy to see that our ranking function is linear w.r.t.
 i.
Therefore, we make use of the coordinate ascent algorithm proposed by Metzler and Croft [19].
This algorithm iteratively optimizes a multivariate objective function (in our case, rScq(g, c, t)) by performing a series of one-dimensional line searches.
It repeatedly cycles through each parameter  i, holding all other parameters  xed while optimizing  i.
This process is performed iteratively over all parameters until the gain in the target metric is below a certain threshold.
Although we use the coordinate ascent algorithm primarily for its simplicity and e ciency, any other learning to rank approach that estimates the parameters for linear models can be used.
Other possible learning to rank algorithms include ranking SVMs [12], SV M M AP [32] or RankNet [6].
Due to the linearity of our ranking function, query dependent features (e.g., query length) cannot be readily incorporated into StructRank, since they will have the same contribution across all documents associated with a query.
While this can be addressed by using nonlinear rankers, such rankers typically require more training data and are more prone to over tting than linear models [31].
As a middle ground between linear and nonlinear approaches, we bin our queries, and train a speci c model for each bin.
Any query-dependent feature (or combination of thereof) can be used for query binning.
In our experiments we found that binning by query length is both conceptually simple and empirically e ective for retrieval optimization.
In this section we describe the experimental results of our work.
In Section 5.1 we describe the experimental setup.
In Section 5.2 we empirically demonstrate the di erences in index sizes and query run-times between the three ad index versions described in Section 3.
In Section 5.3 we compare the retrieval e ectiveness of these three index versions, as well as demonstrate the bene ts of the structured reranking approach proposed in Section 4.
Finally, in Section 5.4, we show the bene ts of employing the ad structure for ad  ltering.
All indexing and retrieval experiments are implemented using an open-source search engine Indri4.
Indri natively supports indexing and retrieval of structured documents [28], 4http://www.lemurproject.org/indri/ and provides a best-in-class ad hoc retrieval performance using a popular language modeling approach for information retrieval [21].
Indri allows us to compare the performance of all our retrieval methods using the same search engine.
We constructed our dataset as follows.
We sampled a set of queries from a Web search log using strati ed sampling.
We then randomly divided the queries into three disjoint sets: a development set of 1,570 queries used for debugging, feature selection and parameter tuning; a training set of 3,134 queries, and a held-out testing set of 773 queries with at least one relevant (non-Bad, see below) judgment associated with each query.
We then retrieved ads for these queries using a commercial ad retrieval system, and had them evaluated by human judges using graded relevance judgments.
This resulted in a set of judged query-ad pairs, and our experiments then consisted of reranking these pairs using each of the proposed methods.
Each judged ad consisted of a single (cid:104)creative, term(cid:105) pair, and one such pair per ad group was judged.
Overall, our collection consisted of 5,477 queries and 93,632 unique ad groups that contained a total of 239,011 creative  elds and 5,143,010 term  elds.
In all our retrieval experiments, all documents and queries were stemmed using Krovetz stemmer, and no stopword removal was performed.
The smoothing parameter   in Eq.
2 is set to 90, which was observed to be the optimal setting on the development set.
In the (re)ranking experiments, we measured the performance using the normalized discounted cumulative gain measure (NDCG), a standard retrieval metric for Web retrieval [11].
The relevance grades assigned by human judges were [Perfect, Excellent, Good, Fair, Bad], and we used the following DCG gains for these grades [10, 7, 3, 0.5, 0], respectively.
In the parameter optimization of our reranking model, we use normalized DCG at position 10 as the target metric for the coordinate ascent search (Section 4.2.2).
As described in Section 4.2.2, we found that performance of the StructRank algorithm can be improved by separating the training set into several query bins, and training a separate linear model for each bin.
Ideally, each bin should contain only queries of a certain type (for instance, only navigational or informational queries).
In the lack of explicit query type information, we simply bin queries by length.
We use three similar-sized bins, one for one word queries, one for two and three word queries and one for queries of four words or longer.
We found that this binning procedure signi cantly improves performance on the development set, and used it in the rest of our ranking experiments.
To test the e ciency of our proposed indexing strategies, we create 4 sub-samples of the increasing size from the entire corpus of 93,632 ad groups.
Sub-sample sizes ranged between 25% and 100% of the entire corpus.
Each of these sub-samples was separately indexed using one of the three indexing methods: CTInd, CrtvInd and AdGrpInd.
Overall, the combination of 4 sub-sample sizes and 3 index types yielded 12 experimental indexes.
Table 2 details the number of documents in each sub-sample, and the size of all the indexes constructed.
As can be seen from Table 2, index AdGrpInd is, as expected, the smallest of the three versions, both in terms of the number of documents, and the # Docs
 CTInd CrtvInd AdGrpInd

 Sample Size = 50% Index Size # Docs








 Index Size





 Sample Size = 75% # Docs Index Size Sample Size = 100% # Docs Index Size

















 Table 2: Details of index samples.
(a) (b) (c) Figure 3: Retrieval runtime for the three index types.
(a) No reranking; (b) with reranking of the top 10 results for CrtvInd and AdGrpInd using the crtvTermPairScore feature; (c) the di erence between reranking and no-reranking runtimes.
overall index size.
This is due to the fact that it is the only version of the index where no  eld duplications occur.
Due to a large number of terms that can be potentially associated with a single ad group (average number of terms per ad group is approximately 50, with maximum number of terms per ad group bounded by 1,000), the CTInd index is the most costly, both in terms of number of documents and the index size.
On the smallest 25% sub-sample, the size of index CTInd is 36 times larger than that of the index AdGrpInd.
Moreover, this di erence grows with the number of ad groups, as the ad group  elds in index CTInd are not uniquely indexed.
On the entire corpus, the size of index CTInd is already 41 times larger than that of index AdGrpInd.
Note that in our experiments, only a small number of ad groups (  100, 000) is used.
As the ad corpora used by commercial search engines typically contain millions of ad groups and are frequently updated, constructing the (cid:104)creative, term(cid:105) pairs index CTInd becomes practically in-feasible.
The same trend, though on a smaller scale, occurs when comparing the CrtvInd index and the AdGrpInd index.
Each ad group contains, on average, 2.5 creatives, and the CrtvInd strategy indexes each of the terms once for each of them.
As can be seen in Table 2, the same ratio of 2.5 approximately explains the di erence in index size between CrtvInd and AdGrpInd.
Based on the relative sizes of the indexes, we expected to see a signi cant reduction in query runtime when AdGr-pInd is used, especially compared to CTInd.
To test this, we sample 900 queries from our development set, and run them on all 12 indexes, with or without reranking, as shown in Fig. 3.
Each curve corresponds to an index type; each point on the curve corresponds to one of the sub-samples.
It is evident that without the reranking step the execution of the queries in CTInd is signi cantly slower than that of the other two methods (Fig. 3 (a)).
Moreover, the di erence in runtime increases with the index size.
Reranking the top ten results (Fig. 3 (b)) increases the runtime for the two indexes where reranking is performed5, however the runtime still remains well below of that of CTInd.
Finally, Fig. 3 (c), shows the di erence between the runtime with or without reranking for CrtvInd and AdGrpInd.
Note, that this di erence increases slowly with the size of the index.
This growth is much slower than that demonstrated by CTInd, indicating that even for larger index sizes, reranking with the CrtvInd or AdGrpInd indexes will remain a more e cient strategy than retrieval with the CTInd index.
In the previous section, we have shown that the AdGr-pInd index provides the smallest index size and the best retrieval e ciency, overall.
In this section, we demonstrate that when the proposed structured reranking algorithm Struc-tRank is applied, AdGrpInd can provide the best retrieval e ectiveness (in terms of nDCG), as well.
To this end, we consider the ad ranking task.
This task is similar to the standard ad hoc information retrieval setting.
Given a user query, ads are ranked by their relevance score and the top ones are presented to the user.
Note, that in contrast to standard document retrieval, only the selected (cid:104)creative, term(cid:105) pair is shown to the user, and not the entire ad group.
Our retrieval algorithms di er in the way the underlying index is structured and in the way the relevance score is computed (see Section 3).
We evaluate all the algorithms using a set of judged query-document pairs originally retrieved by a commercial ad retrieval system6.
All the methods are evaluated using a held-out set of 773 queries that have at least one

 duction system, since it takes into account many other features not accounted for in this study.
CrtvRank CTRank StructRank nDCG@1



   (+3.66%) nDCG@5



   (+2.34%) nDCG@10



   (+1.69%) Table 3: Retrieval e ectiveness of all the retrieval methods.
The   symbol denotes statistically sig-ni cant di erence between the results of CTRank and those of AdGrpRank and CrtvRank methods, while the   symbol denotes statistically signi cant di erence between StructRank and CTRank (using Wilcoxon sign test,   < 0.05).
The numbers in the parentheses indicate % improvement of StructRank over the CTRank baseline.
non-Bad judgment associated with them (that is, the ranking in response to this queries can be potentially improved by reranking).
We employ each of the retrieval strategies described in Section 3 as baselines.
In addition, we use the StructRank algorithm (Algorithm 4) to test whether structural information captured by this method bene ts the ranking.
Table 3 summarizes the retrieval results for all four retrieval algorithms.
First, we note that CTRank has the best performance among the baseline methods.
CTRank outperforms the other baselines to a statistically signi cant degree on all nDCG measures.
While using CTRank for ranking using a CTInd index is infeasible for large corpora (as was explained in Section 5.2), Table 3 shows that it can signi cantly improve the performance when applied as a reranking method on a smaller index.
The key result in Table 3 is the performance of the Struc-tRank algorithm.
As described in Section 4.2, StructRank ranking function encodes the hierarchical structure of an ad group as a vector of features, and linearly combines these features.
StructRank consistently and signi cantly improves the performance over the CTRank baseline at all NDCG positions.
The improvements at the top ranks are particularly high (3.7% improvement for nDCG@1); this is crucial for sponsored search since the user only sees a few top results (ads).
Fig. 4 plots the nDCG curves corresponding to the four retrieval algorithms from Table 3.
The performance improvements attained by StructRank over the baselines are consistent over ranks 1 through 10.
As mentioned in Section 5.1, StructRank learns a separate linear model for each query length bin.
Table 4 provides a breakdown of retrieval performance improvements attained for each of these bins.
Table 4 shows consistent improvements over the best baseline, CTRank, in all the bins.
These performance gains are the most striking for long (four words or more) queries, reaching more than 7% improvement for NDCG@1.
This indicates the importance of using the structural information of ad groups for longer queries, as the probability of an exact or even partial match of these queries to any single (cid:104)creative, term(cid:105) pair in the index is low.
Therefore, the ad group structure provides a much needed context to improve the coverage and the relevance of the ads displayed in response to these long queries.
Figure 4: NDCG Curves.
Finally, Table 5 shows the highest weighted features, as learned for each of the query length bins.
Note the dominantly high weight of a crtvTermPairScore feature, and its increase with the query length.
This indicates the high importance of a close match with a (cid:104)creative, term(cid:105) pair, if such exists, especially for long queries.
This high weight also explains the success of the CTRank algorithm, which sig-ni cantly outperforms the other two baseline methods (see Table 3).
In addition, we note the di erences in feature weights learned for each bin.
While for short, single-word queries, features traditionally associated with navigational queries (like title and URL match) receive high weights, for longer queries, query word coverage features (for instance, the feature adGrpTermQueryCover) are more important.
These di er-ences showcase both the bene t of our strategy of learning a separate model for each query bin, and an adequacy of query length as the binning criterion.
In this section we apply our algorithms to the ad  ltra-tion task [24].
In this task, the goal is to learn a binary classi er that is trained to distinguish between relevant and non-relevant ads.
This type of classi er is often applied to an initial ranked list of ads that has been returned by an information retrieval system such as the one described in the previous section.
The classi er is used to pass only the most relevant ads to the next stage of ranking, where they can be reranked using non-textual features such as click-through rates and bid amounts.
Filtration of non-relevant ads is important in sponsored search, where the display of an inappropriate ad can lead to a decrease of click-through rate on the entire list.
On the other hand, a failure to show a highly relevant ad can lead to a loss of potential revenue for the search engine.
Accordingly, we convert the original graded relevance judgments into a binary class label.
All the (cid:104)creative, term(cid:105) pairs rated as Bad w.r.t.
a query are assigned a negative label, while all the other pairs are assigned a positive label.
As is often the case with relevance judgments in information retrieval, the majority of our data is associated with the negative class.
Out of 27,763 rated pairs in our development set, only 6,219 pairs have a positive label.
nDCG CurvespositionsnDCG@123456789100.550.600.650.700.750.80AdGrpRetCrtvRetCTRetStructRankWWW 2010   Full PaperApril 26-30   Raleigh   NC   USA108Len1 (143 queries) Len2+3 (443 queries) Len4+ (187 queries) nDCG@1 nDCG@5 nDCG@10 nDCG@1 CTRank StructRank







 (+4.96%) (+0.95%) (+0.69%) (+2.14%) nDCG@5

 (+2.09%) nDCG@10

 (+1.59%) nDCG@1

 (+7.2%) nDCG@5

 (+4.57%) nDCG@10

 (+3.08%) Table 4: Retrieval e ectiveness of retrieval methods CTRank and StructRank, for queries of di erent length.
The   denotes statistically signi cant di erence with CTRank method (Wilcoxon sign test,   < 0.05).
The numbers in the parentheses indicate % improvement of StructRank over CTRank baseline.
Len1 crtvTermPairScore adGrpUrlMatchRatio adGrpEntropy adGrpTitleMatchRatio adGrpTermQueryCover




 Len2+3 crtvTermPairScore adGrpEntropy adGrpScore adGrpTermQueryCover adGrpUrlMatchRatio



 < 0.01 Len4+ crtvTermPairScore adGrpTermQueryCover adGrpTermCount adGrpEntropy adGrpUrlMatchRatio

 < 0.01 < 0.01 < 0.01 Table 5: Learned weights for di erent query groups.
AdGrpRank CrtvRank CTRank StructRank Precision



 Recall








 (+11.1%) (+17.9%) (+14.5%) Table 6: Summary of ad  ltration results.
Precision and recall are reported at the decision threshold that achieves maximum F1.
We use C4.5 decision tree implemented in Weka for all our classi cation experiments [30].
In the development phase of our experiments, we tested other linear and nonlinear clas-si ers implemented in Weka, and found that C4.5 decision tree performed better than other standalone classi ers, and its performance was comparable to that obtained by complex ensemble classi ers like AdaBoost, while being much faster at train time.
The class imbalance present in our data often leads to a poor recall of positive examples.
Since Weka allows to output a prediction con dence level for each instance, we tune the prediction threshold of our classi er (a con dence level below which the instance is classi ed as negative) to optimize the F1 measure on the development set.
Previous work [24] concentrated on identifying the features within the (cid:104)creative, term(cid:105) pair that improve the ad In this work, we are interested in  ltration performance.
the importance of features outside the (cid:104)creative, term(cid:105) pair, namely the structural features from the entire ad group, as discussed in Section 4.2.1.
To this end, we construct three baseline classi ers, each using as a single feature an output of the retrieval algorithms CTRank, CrtvRank and AdGrpRank ((cid:104)creative, term(cid:105) pair, creative and ad group scores, respectively).
We compare the performance of these baseline classi ers, to the performance of a classi er which uses all the structural features used by StructRank.
Table 6 shows the precision, recall and F1 measure attained by the three baseline classi ers and the structural classi er, denoted StructRank, for an ad  ltration task.
As Table 6 demonstrates, StructRank is superior to all the baselines.
It achieves more than 10% improvement over the best performing baseline, CTRank, on all three measures, and all the improvements are statistically signi cant according to the McNemar s test (  < 0.05).
Table 7 provides a more detailed analysis of the ad  l-tration experiments.
Table 7 (a) breaks down the performance of each classi er (in terms of F1 measure) by query length bins, similarly to the analysis shown in Table 4 for the NDCG measure.
The key observation in this table is the importance of structural features for long queries.
While the contribution of these features is signi cant for all query lengths, for long queries (having with four or more words) they provide an almost 90% improvement (sic!)
over the baselines that use no structural information.
Finally, Table 7 (b) details the  ltration rates for all  ve relevance grades.
It is clear that StructRank consistently outperforms the other baselines,  ltering out fewer relevant and more non-relevant ads, across the entire range of relevance grades.
In this paper we investigated the important yet often overlooked issue of ad indexing.
Existing advanced match algorithms index the ads using standard IR indexing techniques with  at document representation.
However, in practice ads are inherently hierarchically structured and organized into accounts, campaigns and ad groups.
We investigated three di erent strategies for indexing this structured data corpus, including creative-bid term coupling, creative coupling, and ad group coupling.
We showed that the creative-bid term coupling, which is the most straightforward implementation, results in an infeasibly large index, while the ad group coupling index was both signi cantly smaller and much more practically useful.
In addition to investigating di erent indexing strategies, we also explored how to e ectively retrieve ads using structured ad indexes.
We proposed a novel reranking strategy that exploits the ad corpus structure.
Our structured reranking approach makes use of a linear machine-learned ranking function that uses a variety of structural ad features.
As we show, the approach is not only useful for ranking ads, but also for  ltering (classifying) bad ads.
We conducted a comprehensive set of experiments on a sponsored search test collection from a large commercial search engine.
Our experimental results show that using the CrtvRank CTRank StructRank Len1 (2419)



 (+11.2%) (a) Len2+3 (16182)



 Len4+ (9162)



 (+16.3%) (+89.7%) Grade Perfect (54) Excellent (71) Good (980) Fair (5114) Overall Rel.
(6219) Bad (21544) AdGrpRank CrtvRank CTRank StructRank











 (b)











 Table 7: (a) F1 measures for the ad  ltration classi ers, binned by query length.
(b) Detailed  ltration rates (ratio of  ltered ads) for each relevance grade.
The numbers in parentheses indicate the number of (cid:104)creative, term(cid:105) pairs in each group.
proposed structured reranking strategy with the (small) ad group index yields statistically signi cant improvements in retrieval and  ltering e ectiveness over the simple approach that produces a combinatorially huge creative-bid term index.
To summarize, we show that a small, structured index can be used to retrieve highly relevant sponsored search ads.
All of the experiments reported in this paper were conducted on a sponsored search test collection.
However, we believe that our methodology can be directly applied to the content match scenario as well.
Although the construction of the ad query would be di erent in that case (the input would be a Web page rather than a search query), the server-side indexing and retrieval of ads should mostly stay the same as in sponsored search.
We intend to apply our methodology to content match advertising in future work.
