Ranking has become a central research problem for informational retrieval and Web search, since it directly in uences the relevance of the search results, the quality of a search system and users  search experience.
The task of ranking in the search process can be brie y described as follows.
Given a query, the deployed ranking function measures the relevance of each document to the query, sorts all documents based on their relevance scores, and presents a list of top-ranked ones to the user.
Thus, the essential problem of search technology is to design a ranking function that can best represent relevance.
In the past, many models have been proposed, including the Boolean model [2], vector space model [18], probabilistic model [17] and language modeling method [14].
Most recently, there are renewed interests in exploring the techniques from machine learning for building ranking functions, some popular algorithms including MCRank [15], RankNet [6], RankSVM [12], RankBoost [8], GBRank [23], ListNet [7],and IsoRank [24].
In most of the previous work, the signi cant di erence in queries is not adequately addressed in the context of ranking.
This is clearly not appropriate, particularly for Web search, since queries vary largely in multiple facets, such as semantics, users  search intention, popularity, length, number of relevant documents, etc.
For example, queries can be related to various semantical domains, such as product, travel, and autos, or be categorized as navigational, informational, and transactional [5] in terms of search intentions; queries can vary in terms of their popularity, i.e. the frequency of occurrence in the search log; queries can also be di erent in length; and at some time, queries are classi ed as those associated with many relevant documents and those having very few relevant documents.
These various types of query di erence make it di cult to build a single general ranking function for all kinds of queries, because the ranking function, while indicating good ranking relevance for a certain type of queries, may not be able to achieve similar performance for other types of queries.
For instance, many current search engines can achieve good ranking performance on general short queries with 2-3 query terms, but they usually can not achieve high relevance for long queries with more query terms.
And, for popular queries that are often searched by Web users, search engines can generally return good results, but for those that rarely occur or are new phrases on the Web, search engines may not be able to give sound ranking relevance.
Before exploring new ranking methods to resolve these di culties, we need to  rst investigate why query di erence can result in such ranking problems.
on ranking relevance with respect to di erent queries.
For instance, for homepage  nding (a kind of navigational) queries, the textual similarity between the query and the document title may be the best indicator of ranking relevance, whereas for topic distillation [21] (a kind of informational) queries, the whole-document TFIDF and BM25 features may be better for inferring relevance.
As another example, for popular queries, document popularity features, such as PageRank, may be important for ranking, whereas for rare queries, it is not necessary to use document popularity to measure the ranking relevance.
As a conclusion, a single ranking model cannot re ect di erent feature impacts for di erent queries, such that it would not be adequate to use one single ranking model for diverse types of queries.
Therefore, it is necessary to  nd some new approaches which can give better ranking relevance for all the di er-ent types of queries.
A straightforward method is to add query di erence in terms of additional features into learning the single ranking function, however, since this method requires high quality of both the new features and training data, it usually does not e ective in practice (as shown in experiment in Section 5.1.2).
In this paper, we propose a divide-and-conquer framework for improving the ranking relevance for all queries.
The basic idea is to  rst identify a set of ranking-sensitive query topics and divide the problem of learning one single ranking model for all queries into learning a set of sub-models for corresponding di erent query topics.
Then, we conquer these learning problems by introducing a global loss function and exploring a uni ed approach to co-optimize all sub-models.
At testing time, we select a set of ranking-sensitive query topics the new query most likely belongs to, and apply respective ranking models to ranking the documents, then we assemble these ranking results together to obtain the  nal ranking for the new query.
We name the whole framework as ranking specialization.
By applying ranking specialization, we can bene t ranking relevance in several ways: Firstly, ranking specialization can help to improve the overall search relevance, since the divide-and-conquer approach allows us to create and use topic-speci c features and training data for learning the dedicated ranking model for each ranking-sensitive query topic.
Secondly, we observed that, the current learning-to-rank algorithms, exploited to train a single ranking model, usually improve relevance on some queries, but hurt relevance on other queries compared with some baseline approach.
Ranking specialization allows us to take deep-dive analysis and improve relevance on a subset of queries, without hurting others, which is the key for continuously improving Web search relevance.
Furthermore, ranking specialization allows us to incrementally update the ranking models for search engines.
From deep dive analysis, after we identify a group of queries that share the same ranking problems, we can easily update the model for the existing ranking-sensitive query topics, or add the queries as a new query topic, and then build a dedicated model for them.
However, there are three major challenges for our framework.
The  rst one is how to identify the ranking-sensitive query topics.
Most of previous query classi cation focus on categorizing queries in terms of semantics [3, 19], but such query classi cation may not be best for the purpose of improving ranking.
Some of query taxonomy [5] is based on search intent [5], but they may not be  ne-grained enough for ranking purpose.
Furthermore, pre-de ned query categorization may not even be available at learning time.
In this paper, in addition to pre-de ned query categories, we propose to identify ranking-sensitive query topics, in the sense that, each ranking-sensitive query topic represents a group of queries having the similar set of important features for measuring ranking relevance, and di erent query topics re ect diverse feature impacts on ranking.
Due to the high correlation between the ranking-sensitive query topics and ranking features, we identify ranking-sensitive query topics by taking advantage of both ranking features of query s pseudo feedbacks and the prior knowledge of importance of ranking features.
The second major challenge is how to train the ranking model with respect to each ranking-sensitive query topics.
Previous works, especially on local ranking or query-dependent ranking [13, 9], have proposed to train the ranking model for each query category separately.
Although having achieved better ranking performance than single-model approaches, these methods do not consider dependency between di erent query categories, which may be bene cial to further improving ranking.
Moreover, since they use only a small part of the training data for learning each model, it may cause the declining accuracy due to the lack of enough training examples.
We instead propose a uni ed SVM-based method to learn all the models of all ranking-sensitive query topics simultaneously.
In particular, we de ne a new global loss function by combining the ranking risks of all the training examples (for all query topics) with di erent weights according to the training query s similarity to di erent query topics.
Intuitively, if one training query is highly correlated to a certain query topic, training examples with respect to this query will contribute more to learn the ranking model of this particular query topic.
We name this learning method as Topical RankSVM.
The last challenge is how to conduct ranking for new testing queries.
In our paper, we employ a testing method consistent with the training process.
We  rst select a set of ranking models based on the correlation between the test query and the corresponding ranking-sensitive query topics.
To obtain the  nal ranking result, we then aggregate the ranking scores computed by all selected models with weights based on the similarity between the test query and query topics.
The intuition is that the ranking relevance of the test query depends more on the ranking models for the query topics that the test query most likely belongs to.
To evaluate the e ectiveness of our proposed approaches for learning ranking functions, we conduct experiments on both Letor [16], a public benchmark dataset for learning ranking functions, and a large scale dataset from a commercial search engine.
Experimental results show that our proposed approaches can signi cantly improve the performance of ranking over the single ranking model approach on both benchmark dataset and commercial search engine dataset.
We also observe that our approach can outperform previous local ranking or query-dependent ranking approaches.
Furthermore, we provide analysis and conduct additional experiments to gain deeper understanding on the advantage of incorporating ranking-sensitive query categories.
The remaining parts of this paper are organized as follows.
Section 2 introduces related work.
Section 3 presents our divide-and-conquer ranking framework and its three major parts in details.
Experimental setup and results are demonstrated in Section 4 and 5, respectively.
We conclude the paper and point out future research directions in Section 6.
Some of recent works have realized the importance and necessity of incorporating query di erence into learning the ranking function.
Zha et al. [22] propose an aTVT algorithm which implicitly incorporates query di erence using monotonic transformations of the learned ranking functions.
This approach focuses on the boundary of each query without considering broader query grouping.
Kang et al. [13] classify queries into two categories (navigational and informational) and build two corresponding ranking models separately.
However, it requires the availability and high accuracy of query classi cation.
In a most recent work, Geng et al. [9] propose a K-Nearest Neighbor (KNN) method to employ di erent ranking models for di erent queries.
Specifically, each training query holds a ranking model which is learned using the query itself and its neighboring queries.
Given a test query, they  nd the most similar training query and use the corresponding model for ranking.
Training time of this method is quite large, since many models need to be trained separately.
And each model is trained using only a part of whole training set, which may cause the declining accuracy due to the lack of adequate training examples.
In our work, we employ a divide-and-conquer approach for ranking specialization to improve the ranking relevance.
We will discuss our approach in details in the following sections.
In this section, we will explore a new divide-and-conquer approach for ranking specialization for improving search relevance.
We will start with introducing the general divide-and-conquer framework, followed by concrete discussion on three major parts of the framework in sequence.
The divide-and-conquer framework consists of three major steps.
In the  rst step, we target at identifying a set of ranking-sensitive query topics, Cquery = {C1; C2;  ; Cn}, based on all of training queries, Qtrain = {q1; q2;  ; qN}.
The recognized query topics are considered ranking-sensitive in the sense that di erent queries of the same topic should have similar characteristics in terms of ranking, especially, the similar family of important features for ranking.
After this step, for each training query qi   Qtrain, we can obtain its distribution over all of extracted ranking-sensitive query topics, i.e. Topic(qi) =  P (C1|qi); P (C2|qi);  ; P (Cn|qi) , where P (Ci|q) is the probability that q belongs to Ci.
This step is like dividing the problem of learning one single ranking model for all training queries into a set of sub-problems of learning the ranking model for each ranking-sensitive query Figure 1: The ranking specialization framework for improving search relevance topic.
The number of query topics can be set either empirically to constants, or through gap statistic [20].
The second step is to develop a uni ed learning method for learning multiple ranking models Mk (k = 1; 2;  ; n), each exclusively corresponding to one ranking-sensitive query topic Ck   Cquery.
In our work, we propose a global loss function by combining risks of di erent ranking-sensitive query topics and introduce Topical RankSVM to train all the models M1; M2;  ; Mn, simultaneously, by minimizing the global loss function.
By applying this uni ed learning method, we have considered dependency between di erent query topics when building their respective ranking functions, which can be bene cial to further improve ranking.
Moreover, though treated unequally in learning each ranking function, all the training queries contribute to learn all ranking models of query topics, which avoids the lacking of training examples for learning the model of any single query topic.
This uni ed method is quite general as we can use di erent feature set for di erent query topics.
As incorporating information of query topics into the ranking algorithm, this step is like conquering the problem of learning the respective ranking model for each query topic.
The goal of the last step is to conduct ranking for new testing queries, Qtest = {q1; q2;  ; qt}.
For each testing query qj   Qtest, we apply an ensemble method, which try to minimize the risk consistent with the loss in training process.
We  rst select a certain number H of ranking models Mj1, Mj2,  , MjH , whose corresponding query topics hold H highest correlation with qj, and then aggregating the ranking results Sj1, Sj2,  , SjH obtained by Mj1, Mj2,  , MjH into a  nal ranking results S. After divide-and-conquer, this step aggregates ranking results from sub-models together into the improved  nal ranking results.
We summarize the general framework in Figure 1.
Such ranking specialization approach allows us to use di erent feature sets or data sets to learn respective ranking models for di erent query topics, so as to boost the relevance for each query groups; And, the global loss in the second step serves as a uni ed relevance target when training di erent models for di erent query topics, such that we can optimize the overall search relevance when we train di erent ranking models.
In the rest of this section, we will discuss the details of each step of the framework in sequence.
Topics
 To identify ranking-sensitive query topics, we  rst generate a set of features to represent queries by taking ad-of the query.
For each training query q   Qtrain, we retrieve a set of pseudo feedbacks, P F (q) = {d1; d2;  ; dT}, consisting of the top T documents ranked by a reference model (we use BM25 in this paper).
The ranking features of query-document pair of  q; di  are de ned as a feature vector xqdi =  xqdi   (D is the number of ranking features).
To represent q in a feature space, we aggregate the ranking features of top-T pseudo feedbacks of q into a new feature vector.
We take the mean and variance of the ranking feature values as two aggregation methods.
Thus, the feature vector of query q can be represented as ;  ; xqdi ; xqdi


  (cid:22)1(q); (cid:22)2(q);  ; (cid:22)D(q); (cid:27)2 1(q); (cid:27)2 2(q);  ; (cid:27)2 D(q)  where (cid:22)k(q) denotes the mean value of k-th feature over q s pseudo feedbacks, and (cid:27)2 k(q) denotes the variance value of k-th feature over q s pseudo feedbacks.
In this paper, we will employ linear SVM model as the ranking algorithm.
Thus, before generating query features, we have applied quantile normalization [4] on ranking features of query-document pairs in both training and testing dataset, such that the values of all ranking features are in the scale of [0; 1].
As a result, the values of extracted query features are also in the scale of [0; 1].
Distribution for Queries After generating query features, we employ the mixture model as the clustering method to obtain ranking-sensitive query topics.
We can either empirically set the number of cluster as a constant n, or determine it through gap statistic [20].
After learning the model, we can obtain a set of query clusters {C1; C2;  ; Cn}, where each cluster Ck can  , and Dq be represented as a vector xCk =  xk denotes the number of query features.
We consider each cluster as one query topic.
2 ;  ; xk 1 ; xk Dq Furthermore, we incorporate the prior knowledge of feature importance for ranking into identifying ranking-sensitive query topics.
We obtain feature importance scores by using the ranking weights learned by a general RankSVM on a sample of training data.
In our paper, we integrate the feature importance as weights into aggregated query features.
Assuming the feature importance scores are w =  , the weighted feature vector of query qi is  w1; w2;  ; wDq computed as   Dq Dq 1; xi 1; w2xi
 w   xi    w1xi 2;  ; xi (1)   is the original feature vector where xi =  xi of query qi.
Based on the new query features weighted by feature importance, we can employ the mixture model to obtain the ranking sensitive query topics with integrated feature importance.
Based on this representation of query topics, we are able to calculate the topic distribution Category(q) = {P (C1|q), P (C2|q),   ; P (Cn|q)} for query q as P (Ck|q) =   |xq   xCk|2 |xq   xCi|2 n i=1 (2) Since we take advantage of ranking features of top retrieved pseudo feedbacks of the query to generate query features as well as we compute topic distribution for queries by incorporating prior knowledge of feature importance for ranking, we consider the identi ed query topics and computed topic distribution for each query as ranking-sensitive.
Multiple Ranking Models
 For traditional ranking approach, the task is to  nd a function f in the form of y = f (X; !
); f   F where X denotes an M   D matrix representing D dimensional feature vectors of M documents; !
represents the unknown ranking parameters; y is a vector representing ranking scores of the M documents.
The goal of learning is to select a best function  f , such that  f minimizes the given loss function:
  f = arg min f2F L(f (Xi; !
); yi) (3) i=1 where N is the number of queries in the training set; Xi denotes the set of documents associated with the i-th query; yi is the vector of corresponding ranking scores; L denotes a de ned query-level loss function.
Clearly, traditional ranking approach learns single ranking function for all queries.
Inspired by the diverse ranking characteristics implied by di erent queries, to improve ranking relevance, we formalize a new problem of learning multiple ranking functions, f1; f2;  ; fn   F , given the identi ed ranking-sensitive query topics C1; C2;  ; Cn, where each ranking model fi(i = 1;  ; n) can represent the ranking characteristics of its cor responding query topic Ci.
In order to create a uni ed relevance target for all topic-speci c ranking models and let all training examples contribute to all ranking models, we propose a new global loss function by combining ranking risks of all training examples with di erent weights according to the training query s similarity to di erent query topics.
By optimizing this global loss function, we can learn multiple ranking functions, simultaneously.
The intuition is that if the query of one training example is highly correlated to a certain query topic, this training example will contribute more to learn the ranking function of this query topic.
Speci cally, the global function is de ned as    f1;  ;  fn  = arg min P (Cj|qi)fj(Xi; !j); yi)
 n 
 f1:::fn i=1 j=1 (4) where n is the number of identi ed ranking-sensitive query topics; P (Cj|qi) represents the probability that qi belongs to Cj; and !j denotes unknown parameters of the ranking function fj corresponding to the query topic Cj.
Intuitively, if query qi is highly correlated to a query topic Cj, i.e. with high value of P (Cj|qi), the loss of ranking under qi will be much associated with learning !j.
The learning task in Eq.
4 is speci ed when the form of the ranking function f and that of the loss function L are de ned, for example, we can use linear function as ranking function, i.e. f (X; !)
= !T X, and L2 norm as loss function, i.e. L(f (X; !
); y) =  f (X; !)
  y 2.
The ranking specialization framework is quite general and  exible in the sense that it can apply di erent ranking algorithms.
In this paper, we use RankSVM as an example to demonstrate its advantages.
For simplicity, we consider the linear function f (X; !)
= !T X.
We refer to our method as Topical RankSVM.
Note that the same idea can also be applied to by modifying the respective loss function according to Eq.
4.
  RankSVM: We  rst make a review of RankSVM [10, 12].
Its learning task is de ned as the following quadratic programming problem:   min !
;(cid:24)q;i;j s:t: !T X q i  X q i (cid:24)q;i;j  ! 2 + c

   !T X q   X q q;i;j j + 1   (cid:24)q;i;j; j ; (cid:24)q;i;j   0 Assuming that  f1;  f2;  ;  fn are ranking models learned with the method in 3.3 and correspond to query topics C1, C2,   , Cn respectively;  q is a testing query, and {  d1,  d2,   ,  dM ~q } is the set of documents to be ranked with respect to  q; and P (C1| q), P (C2| q),   , P (Cn| q) are the probabilities that the new testing query belongs to query topics.
Then, we compute the ranking score S( q;  di) for each document  di (i = 1;  ; M~q) as follows: n  (5) S( q;  di) = P (Ck| q)  fk(x~q ~di ; !k) (7)   X q where X q j implies that document i is ranked ahead i of j with respect to query q in the training dataset; (cid:24)q;i;j denotes slack variable; and  ! 2 represents structural loss.
  Topical RankSVM: We then describe Topical RankSVM.
Inspired by the fact that the ranking model of one speci c ranking-sensitive query topic depends more on the training query-document pairs, the query in which has higher correlation to the certain query topic, we modify Eq.
5 into the optimization problem of Topical RankSVM: n  k=1

 min !
;(cid:24)q;i;j n   !k 2 + c (cid:24)q;i;j     n  q;i;j s:t: k=1 P (Ck|q)!T   X q  X q i k X q i j ; (cid:24)q;i;j   0 k=1 P (Ck|q)!T j + 1   (cid:24)q;i;j; k X q (6) where !k denotes the parameters of ranking function with respect to query topic Ck.
Note that we can use di erent feature sets for di erent query topics by using this method, but for simplicity, we didn t try it in this work.
The optimization problem can be solved by employing existing optimization techniques, the computation details of which, though tedious, are rather standard and will not be presented here.
Note that, there are several advantages by using Topical RankSVM.
Firstly, we are able to embed ranking-sensitive query topics directly into the ranking algorithm and learn multiple ranking functions for di erent query topics, simultaneously.
Secondly, the global loss serves as a uni ed relevance target for all topic-speci c ranking models, which can boost the relevance than training models separately.
And, we employ all of the training queries to learn ranking models of each query category, avoiding the reduction of the training examples.
Moreover, compared to previous work [9, 13], our approach is not less e cient on training time, which has been proved in the experiments on a large scale dataset.
The same idea can also be applied on ranking algorithms other than RankSVM, in the similar way.
In the rest of this section, we will employ a testing method which is consistent with the training method, in the sense that both of them focus on optimizing the same risk of ranking.
After obtaining multiple ranking models corresponding to ranking-sensitive query topics, we employ an unsupervised ensemble method for improving ranking at query time.
The intuition is that: Similar queries in ranking-sensitive feature space are more likely to hold similar ranking characteristics, therefore, if one query topic has higher correlation to the testing query, the corresponding ranking models should contribute more to the  nal ranking of the testing query.
k=1 where x~q ~di is the ranking feature vector of the query-document pair of  q and  di; !k denotes parameters of  fk.
Then, we can obtain the  nal ranking of documents under  q according to the aggregated ranking scores computed by Eq.
7.
Note that this testing approach tries to minimize the ranking risk consistent with that in training process.
This section presents our evaluation setup.
First, we describe the datasets we used in the experiments (Section 4.1).
Then, we describe our evaluation metrics (Section 4.2) and the ranking methods to compare (Section 4.3) for the experimental results reported in Section 5.
In the experiment, we used three datasets, including both the publicly benchmark dataset and that obtained from a commercial search engine.
LETOR 3.0 [16] is a benchmark dataset for research on ranking [1].
We use TREC2003 and TREC2004 in LETOR 3.0 to evaluate the performance of exploring ranking-sensitive query topics for improving ranking.
TREC2003 contains
 query, there are about 1,000 associated documents.
Each query-document pair is given a binary judgment: relevant or irrelevant.
In total, there are 64 features for each query-document pair, which can be referred to [16] for the details.
Both of these two tracks classify all the queries into three pre-de ned categories, including topic distillation (TD), homepage  nding (HP) and named page  nding (NP), according to search intent.
The statistics of queries for three categories in LETOR 3.0 can be found in [16].
Note that, this pre-de ned hard categorization can also be used to improve ranking by applying the same method in Section 3.3.
In our experiment, we will compare the e ects on improving ranking by using ranking-sensitive query topics with that by using the pre-de ned categorization.
LETOR4.0 is a new release of benchmark dataset for research on ranking [1].
It uses the web page collection ( 50M pages) and two query sets from Million Query track of TREC
 There are about 1700 queries in MQ2007 with labeled documents and about 800 queries in MQ2008 with labeled documents.
Each query-document pair is represented by a 46-dimensional feature vector.
And we used the data with the setting of supervised ranking to evaluate the performance of our proposed ranking approach.
Commercial search engine dataset (SE-Dataset): We also conduct experiment on a dataset obtained from a major commercial search engine.
This dataset contains for training as well as 7,668 testing queries and 252,086 query-document pairs for testing.
All the training and test queries are randomly sampled from the real user tra c to the search engine.
Each query is associated with its retrieved documents, along with human judged labels that represent the degrees of relevance of those documents with respect to the queries.
There are  ve levels of relevance: perfect, excellent, good, fair, and bad.
Features for each query-document pair used in building the ranking functions can be roughly grouped into the following categories: text-matching features, link-based features, user-click features, query and page classi cation features.
We denote this dataset as SE-Dataset.
This dataset classi es all the queries into four semantic domains, including autos domain (Dauto), local domain (Dlocal), product domain (Dproduct), and travel domain (Dtravel).
For each query, SE-Dataset provides the pre-computed similarity score between the query and each query domain.
The value of the similarity score is in the range of [0; 1], 0 meaning the smallest value of similarity while 1 meaning the largest value of similarity.
There are some queries in SE-Data that have the 0 similarity scores with all of four pre-de ned domains.
In our experiment, we de ne a new soft query classi cation, which contains  ve classes, including Dauto, Dlocal, Dproduct, Dtravel, and general domain Dgeneral.
For each query q, we set the similarity score with respect to general domain class as 1, and after normalizing similarity scores with respect to all  ve classes, we can obtain a soft query classi cation.
We adapt the following information retrieval metrics to evaluate the performance of the ranking function.
 Normalized Discounted Cumulative Gain (NDCG): NDCG has been widely used to assess relevance in the context of search engines [11].
For a ranked list of documents, the NDCG score at position n is calculated as follows: n  j=1 2r(j)   1 log2(j + 1) NDCG(n)   Zn (8) where j is the position in the document list, r(j) is the rating of the j-th document in the list, and Zn is the normalization factor which is chosen so that the perfect list gets a NDCG score of 1.
 Mean Average of Precision(MAP): Average precision for each query is de ned as the mean of the precision at n values calculated after each relevant documents was retrieved.
The  nal MAP value is de ned as the mean of average precisions of all queries in the test set.
This metrics is the most commonly used single-value summary of a run over a set of queries.
Thus, MAP is calculated as:   n=1(P @n   rel(n))



 |Rq|   q2Qr where Q is a set of test queries, Rq is the set of relevant document for q, n is the rank position, N is the number of retrieved documents, rel() is a binary function on the relevance of a given rank.
To evaluate the performance of our approach employing ranking-sensitive query topics in learning the ranking function, we compare the performance of the following methods:
 (denoted as RSVM) and conduct training over all the training queries to learn one single ranking model.
At testing time, we use the single model to generate ranking results for all testing queries.
query categorization into our proposed SVM-based learning method, where each query category is viewed as one query topic.
In LETOR 3.0 dataset, each query can only belong to only one category.
Thus, this method equals to separating the training queries based on query categorization and training a specialized model for each category of queries.
In commercial search engine dataset, we can employ the pre-de ned soft query categorization into our proposed SVM-based learning method directly.
We call this method  Semantic-class based RankSVM , denoted as CRSVM.
At testing time, for hard query categorization, we choose the ranking model according to the category of each testing query; for soft query categorization, we apply the ensemble approach in Section 3.4 to generate ranking results.
in [9].
After identifying the ranking-sensitive query categories, we classify each training query into the closest query category.
Based on this hard partition of training queries, we train separate ranking model for each query category using its own fraction of training queries.
This method is usually called  Local Ranking  in previous work.
By using RankSVM, we denote this method as LRSVM.
At testing time, according to the correlation between the test query and query categories, we select the ranking model of the most correlated query category and use it to generate the ranking results.
sensitive query topics (Section 3.2), we employ the proposed  Topical RankSVM  (Section 3.3) with topic distribution of training queries to learn ranking models for those query topics.
At testing time, we use the proposed ensemble method (Section 3.4) to generate the ranking results for testing queries.
We evaluate the performance of the ranking methods on TREC2003 and TREC2004 in LETOR 3.0 as well as MQ2007 and MQ2008 in LETOR 4.0.
For each of these datasets, we conduct 5-fold cross-validation experiments, using the default partitions in LETOR.
For TREC2003 and TREC2004, since the data of each class (TD, NP, HP) is split into 5 folds, we combine respective i-th fold (i = 1;  ; 5) of each class together to form up the i-th fold of the whole dataset.
To identify ranking-sensitive query topics, we use BM25 as the reference model to rank documents and choose the top T = 50 ranked documents (if the total number of documents under one query is lower than T , all documents will be used) as pseudo feedback to create ranking-sensitive features.
The topic number n is crucial to the performance of TRSVM and LRSVM.
Since there are three pre-de ned classes in TREC2003 and TREC2004, we select the value as n = 3 to compare the performance of TRSVM with that of LRSVM and CRSVM for experiments on TREC2003 and TREC2004.
Since there is no pre-de ned class in MQ2007 and MQ2008, we will not test the performance of CRSVM on these two dataset.
And, we set n = 10 to compare the performance of TRSVM with that of LRSVM for experiments on MQ2007 (b) TREC2004 (c) MQ2007 (d) MQ2008 Figure 2: Ranking relevance in terms of NDCG@K of TRSVM compared with other methods on LETOR 3.0 and 4.0.
and MQ2008.
In practice, this parameter is tuned automatically based on a validation set.
In order to clearly illustrate the in uence of this parameter on the ranking performance, we will also present the results with respect to di erent values of the topic/category number.
For RSVM, we can make use of its results provided in LETOR.
For all the SVM models in the experiment, we employ the linear SVM.
This is because the LETOR data set o ers results of linear RankSVM.
The purpose of this experiment is to compare the average relevance of di erent ranking algorithms on di erent benchmark datasets.
Figure 2(a) and 2(b) illustrate the NDCG values of TRSVM compared with RSVM, CRSVM and LRSVM on TREC2003 and TREC2004, respectively.
From these two  gures, we observe that, by building different ranking models with respect to di erent query cate-gories/topics, TRSVM, CRSVM and LRSVM outperform the single model learned by RSVM on both dataset.
Furthermore, by extracting the ranking-sensitive query topics and applying the proposed uni ed learning method, TRSVM give better relevance than CRSVM and LRSVM.
We conduct t-test on the improvements in terms of NDCG@3, and the results indicate that for both TREC2003 and TREC2004, the improvements of TRSVM over other ranking methods are statistically signi cant (p-value< 0:05).
In Table 1, we report the MAP scores of TRSVM compared with RSVM, CRSVM, LRSVM on TREC2003 and TREC2004, respectively.
Table 1 indicates that TRSVM achieves much better relevance than RSVM, CRSVM and LRSVM.
In particular, TRSVM achieves a gain of about
 to RSVM on TREC2004; and TRSVM obtains more gains than CRSVM and LRSVM on both dataset.
Table 1: MAP value of RSVM, CRSVM, LRSVM, and TRSVM on TREC2003 and TREC2004 Ranking Method TREC2003 Gain TREC2004 Gain







 -+4% +5% +9%



 -+2% +4% +6% Moreover, Figure 2(c) and 2(d) illustrate the NDCG values of TRSVM compared with RSVM, and LRSVM on MQ2007 and MQ2008, respectively.
From these two  gures, we also observe that, on both dataset TRSVM and LRSVM outperform the single model learned by RSVM by employing di erent ranking models with respect to di erent query cat-egories/topics.
Furthermore, TRSVM, which extracts the ranking-sensitive query topics and applying the proposed uni ed learning method, can reach better ranking relevance than LRSVM based on local ranking.
(Note that the NDCG@10 of queries in MQ2008 is much lower because a portion of queries have less than 10 documents to rank in this dataset.)
We conduct t-test on the improvements in terms of NDCG@3, and  nd that the improvements of TRSVM over other ranking methods are statistically signi cant (p-value< 0:05).
Table 2: MAP value of RSVM, LRSVM, and TRSVM on MQ2003 and MQ2004 Ranking Method MQ2007 Gain MQ2008 Gain





 -+2% +4%


 -+1% +4% Table 2 demonstrates the MAP value of TRSVM compared with RSVM as well as LRSVM on MQ2007 and MQ2008, respectively.
From the table, we can observe that TRSVM achieves much better relevance than RSVM and LRSVM.
In particular, TRSVM achieves a gain of about 4% relative to RSVM on both MQ2007 and MQ2008; and TRSVM obtains more gain than LRSVM on both datasets.
In the following, we will investigate the reason that TRSVM can achieve better ranking relevance than the single model RSVM, the class-based ranking approach CRSVM, and the local ranking approach, LRSVM.
I.
Multiple ranking models v.s.
single model Table 3 and 4 demonstrates the 8 most important features for single model learned by RSVM and for separate models corresponding to query topics learned by CRSVM and TRSVM, respectively.
These results are based on the experiment on TREC2003.
The feature importance is measured by the absolute value of learned feature weight.
The detailed description of features can be found in [16].
From these tables, we can observe that, introducing query di erence in terms of query classi cation/clustering into ranking can help to build multiple ranking models with respect to various query classes (CRSVM) or query topics (TRSVM).
These ranking models can represent multiple  ne-grained ranking characteristics of various query classes/topics, while the single ranking model can only describe a coarse summa-rization of ranking characteristics over all various queries.
In particular, if we build one single ranking model using RSVM, the top 5 most important features of the model are weighted in-link, weighted out-link, TF-IDF of title, TF of title, and TF of body, as shown in Table 3.
To verify whether these  ve features are most important to ranking for most of queries, we conduct an experiment as follows: we randomly sample 20 queries from the whole dataset and build respective ranking models for each query based on the documents and labels associated with the certain query, then we compute the respective feature importance of each query.
We randomly sample 20 queries for 3 times, and there are only in average 6.3 queries (31:5%) whose top 5 most important features include at least three of top 5 most important features of the model learned by RSVM.
To gain an understanding of what is the better way to use

 weighted in-link weighted out-link TF-IDF of title TF of title TF of body TF-IDF of whole document length of URL topical PageRank


 sitemap based score propagation sitemap based term propagation DL of title length of URL HostRank BM25 of title DL of URL BM25 of anchor BM25 of whole document number of slash in URL LMIR.JM of whole document sitemap based score propagation sitemap based term propagation LMIR.JM of anchor BM25 of anchor HostRank HITS hub DL of URL length of URL LMIR.JM of title LMIR.DIR of whole document LMIR.ABS of whole document sitemap based score propagation sitemap based term propagation Table 4: Top 10 most important features for TRSVM on TREC2003 TRSVM (topic-3) TRSVM (topic-2) TRSVM (topic-1) sitemap based term propagation sitemap based score propagation length of URL number of slash in URL DL or URL weighted in-link number of child page BM25 of title number of slash in URL HostRank HITS sub sitemap based score propagation sitemap based term propagation length of URL outlink number sitemap based term propagation sitemap based score propagation number of slash in URL uniform out-link Outlink number LMIR.ABS of URL HITS sub DL or URL DL or title the information of ranking-sensitive query topics to improve ranking, we perform a study on the e ects of adding query topics information as features in learning.
Speci cally, for each query-document pair in both training and testing set, we use the query s topic distribution as additional features and apply RSVM to learn the single ranking model on expanded feature space.
The testing results show that this method does not increase the ranking performance signi -cantly, but even performs worse than the multiple ranking approach.
The similar experiment on SE-Dataset also reports the same observation.
II.
Ranking-sensitive query topics v.s.
pre-de(cid:12)ned semantic classes After considering query di erent in terms of query classi ca-tion(CRSVM)/clustering(TRSVM), we can build multiple ranking models to represent di erent  ne-grained ranking characteristics.
For example, by using CRSVM, we can learn three ranking models corresponding to three query classes: topic distillation (TD), namepage  nding (NP), and homepage  nding (HP).
Table 3 shows the top 8 most important features of each learned model.
From this table, we can  nd that each of the three models learned by CRSVM are quite di erent with that of RSVM.
In particular, some features, such as TF-IDF of title, weighted out-link etc., are important for RSVM, but are not essential for ranking of each spe-ci c query class.
Furthermore, the most important features of each query class are diverse, for example, sitemap based score/term propagation are most important to TD queries; NP queries depend mostly on language model based (LMIR) and probabilistic (BM25) features; and number of slash in URL is the most important one for HP queries but not included in top 8 for the other two classes.
Similarly, Table 4 shows that three ranking models learned by TRSVM are di erent with not only the single model of RSVM but also multiple models learned by CRSVM.
To verify whether TRSVM can obtain multiple ranking models that better represent ranking characteristics of respective query topics than other methods, we conduct the following experiment: for each identi ed query topic, we randomly sample 20 queries from those belonging to this query topic, and build respective ranking model for each of these 20 query based on their own associated documents and labels; then, we compute the respective feature importance of these queries and validate whether they include the top important features learned by TRSVM.
We conduct this experiment on 5 query topics, and randomly sample 20 queries under each topic for 3 times.
There are in average 14.8 queries (74%) whose top 5 most important features include at least three of those of the model learned by TRSVM.
For LRSVM and CRSVM, the results shows that there are in average 12.7 (63:5%) and 11.3(56:5%) queries whose top 5 most important features include at least three of those of the model learned by LRSVM and CRSVM, respectively.
III.
Why TRSVM performs better?
We hypothesize the reasons of why TRSVM can perform better than LRSVM and CRSVM as follows.
Firstly, instead of pre-de ned query classi cation, TRSVM and LRSVM represent each query by aggregating the ranking features of documents under such query and conduct unsupervised clustering method to identify ranking-sensitive query topics, which can be better to distinguish queries based on their various ranking characteristics.
Secondly, LRSVM and CRSVM employ hard query classi- cation/clustering and build multiple ranking models corresponding to each query class/cluster, however, many queries can  t into more than one classes/clusters, therefore, TRSVM can bene t ranking performance by taking advantage of soft query categorization into building multiple ranking models.
Another advantage of TRSVM is avoidance of the reduction in the number of training examples.
Speci cally, either LRSVM or CRSVM uses only a part of training dataset to learn the ranking model for each query class/cluster.
It may cause the declining accuracy due to the lack of enough training examples.
However, TRSVM can avoid the reduction of training dataset size since it uses all training examples, with di erent weights based on query soft clustering, in learning the ranking model of each query topic.
In this experiment, we explore the e ects of di erent settings of the parameter n, i.e. the number of query topics, on ranking performance by conducting comparison study with varying the value of n.
Figure 3 show the performance of TRSVM on Letor dataset with varying values of n in terms of MAP.
From the  gure, we can  nd that, as n increases, the performances  rst increase, but then as n becomes much larger, there is no sig-ni cant raising on the performance, and the performance is even deteriorated at some time.
More speci cally:  When setting only a small number of query topics, the performances of TRSVM are not so good since the identi ed query topics are a bit broad to re ect the query di erence.
 As setting the higher number of query topics, we can im-tures In this experiment, we test the performance of the proposed TRSVM method with using di erent information for aggregating query features.
By default, we compute the mean values of ranking features of top pseudo feedbacks as the feature vector of the query.
In this experiment, we explore the e ects of adding extra statistical quantities beyond means into the query feature vector.
We use variance as the extra statistical quantity in our experiment.
The ranking method using the expanded query features is denoted as TRSVMvar.
Moreover, we test the e ects of making use of the knowledge of ranking feature importance into the aggregation for query features.
In particular, we  rst learn the ranking function by using a sample of the training dataset.
After that, we can obtain the importance of each feature for learning the ranking function.
Then, we incorporate feature importance as weight into computing query features.
We denote the ranking method with incorporated feature importance as TRSVMimpt.
We also test the ranking method which both expands query features with aggregated variance values and incorporates feature importance in identifying query topics, which is denoted as TRSVMvar(cid:0)impt.
Figure 5 shows the performances of the proposed TRSVM method with applying various information into the aggregation for query features.
From this  gure, we observe that, utilizing feature importance into identifying query topics can increase the ranking performance.
After conducting t-test on the improvement in terms of NDCG@3, we  nd that TRSVMimpt outperform TRSVM with p-value less than 0:02.
Figure 5 also illustrates that expanding query features with aggregated variance value (TRSVMvar) does not improve ranking performance but even cause a decreased accuracy than TRSVM, the reason of which could be variance is not useful to identify ranking-sensitive query topics.
However, it does not indicate that other statistical quantities are not useful for query topic recognition either.
It would be an interesting future work to explore what kind of statistical quantities are essential to identify ranking-sensitive query topics.
Moreover, Figure 5 demonstrates that TRSVMvar(cid:0)impt can achieve better performance than TRSVMvar and TRSVM, but not TRSVMimpt, which also indicate that using feature importance can boost ranking performance while expanding query features with aggregated variance may not be useful for improve ranking.
In the following, we perform experiments to evaluate the robustness of our divide-and-conquer ranking approach to noisy ranking-sensitive query topics.
We manually add some noises into the ranking-sensitive query topics and test this e ects on the overall ranking relevance.
Speci cally, after identifying ranking-sensitive query topics, we randomly select a portion of training queries and change their topic distribution into random values.
Then, we employ TRSVM with these noisy ranking-sensitive query topics.
Figure 6 illustrates the NDCG@1,3,5 scores for the testing queries (on MQ2007 and MQ2008) against varying amount of queries with noisy topic distribution in the training data.
The  gures show that, although the relevance declines as the amount of queries with noisy topic distribution increases, TRSVM even outperform single ranking model approach RSVM and therefore is robust to the noisy ranking-sensitive query topics.
And we can  nd the similar results when we take the same experiments on LETOR 3.0 dataset.
(a) TREC2003 and TREC2004 (b) MQ2007 and MQ2008 Figure 3: Ranking performance (MAP) of TRSVM on the Letor datasets against varying topic number n prove the performances since identi ed query topics are more  ne-grained to re ect query di erence in ranking.
 In the ideal case, with increasing number of query topics, the proposed method can achieve much better ranking performance, since we can build ranking models, each of which focuses on more  ne-grained ranking characteristics.
However, the actual results indicates that the ranking performances do not increase signi cantly but even deteriorate at some time.
We hypothesize that, the extracted query features and identi ed ranking-sensitive query topics, though more e ective than other query categorization method for enhancing ranking, are still not optimal for recognizing the ranking-sensitive query topics; therefore, when each identi- ed query topic becomes more  ne-grained, the bias of query categorization will be enlarged so as to deteriorate the ranking performance.
We compare the performance of proposed ranking methods (TRSVM) with the baselines of the single model approach (RSVM), class-based approach (CRSVM) and the local ranking based approach (LRSVM) on the commercial search engine dataset (SE-Dataset).
The pre-de ned classes in CRSVM include auto, local, product, travel, general as described in Section 4.1.
To identify ranking-sensitive query topics, we use BM25 as the reference model to rank documents and choose the top T = 20 documents (if the total number of documents under one query is lower than T , all documents will be used) as pseudo feedback to create ranking-sensitive features.
We set the query topic/cluster number in TRSVM/LRSVM, the parameter n, as n = 20 in the experiment.
i.e.
In practice, this parameter is tuned automatically based on a validation set.
In order to clearly illustrate the in uence of this parameter on the ranking performance, we will also present the results with respect to di erent values of the topic/category number.
Figure 4 demonstrates the NDCG values of TRSVM compared with RSVM, CRSVM, and LRSVM on SE-Dataset.
From the  gures, we observe that, by building di erent ranking models with respect to di erent query categories/topics, TRSVM, LRSVM and CRSVM outperform the single model learned by RSVM.
Furthermore, by extracting the ranking-sensitive query topics and applying the proposed uni ed learning method, TRSVM give better performance than CRSVM and LRSVM.
We conduct t-tests on the improvements in terms of NDCG@3, the results of which indicate that the improvements of TRSVM over RSVM, CRSVM and LRSVM are statistically signi cant (p-value< 0:05).
with the other methods on SE-Dataset Figure 5: Relevance (ndcg@k) of TRSVM with di erent aggregation method for query features on SE-Dataset The robustness of our divide-and-conquer ranking approach can also be demonstrated on SE-Dataset, especially from Figure 5.
This  gure shows that di erent aggregation methods give rise to di erent ranking-sensitive query topics, and hence di erent ranking performance; but, in spite of such di erence, any of these di erent query categorizations can be used to trained the ranking models which outperform the single model approach (RSVM).
Therefore, the proposed divide-and-conquer framework is robust to the noisy ranking-sensitive query topics.
In this paper, we point out that, due to great variance of queries and di erent ranking characteristics of Web queries, single ranking model is not appropriate for diverse types of queries.
We employ a divide-and-conquer approach to learn multiple ranking functions according to diverse ranking characteristics of queries.
We  rst identify ranking-sensitive query topics based on query features from pseudo feedbacks and prior knowledge of feature importance.
Then, we propose a uni ed learning process to obtain ranking models corresponding to recognized query topics.
An ensemble approach is applied to compute ranking for test queries by making use of multiple topic-speci c ranking models.
Experimental results illustrate that the proposed approach can signi cantly improve the ranking relevance over the single model approach and a straightforward local ranking approach, and the identi ed ranking-sensitive topics are more useful for improving ranking than pre-de ned query categorization.
In future, we plan to explore deeply on what kind of features as well as which aggregation method are more essential to identify ranking-sensitive topics for queries.
We also plan to investigate how to recognize the ranking-sensitive query topics jointly with learning the ranking function.
We will also explore hierarchical query topics in the framework.
Moreover, we intend to explore how to extend the current framework in order to allow incremental updating on any of ranking models without hurting the relevance of other queries.
