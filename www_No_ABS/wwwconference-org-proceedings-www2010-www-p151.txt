The World Wide Web contains numerous real-world entities connected by numerous semantic relations.
Identifying the relations between entities is of paramount importance for numerous tasks on the Web such as information retrieval [29], information extraction [4], and social network extraction [23].
A semantic relation that exists between two given objects (e.g., concepts, words, or named-entities) can be de ned in two ways [12]: extensionally or intensionally.
An extensional de nition of a concept formulates its meaning by specifying every object that falls under the de ni-tion of the concept.
On the other hand, an intensional de nition of a concept formulates its meaning by specifying all the properties that are necessary to reach that de nition.
For example, consider the ACQUISITION relation between two companies.
An extensional de nition of the ACQUISITION relation enumerates all pairs of entities between which an ACQUISITION relation holds (e.g.
(You Tube,Google), (Powerset, Microsoft), etc.)
Alternatively, we can express the ACQUISITION relation intensionally by stating the different ways that we can express an acquisition between two companies X and Y such as X is acquired by Y, X is purchased by Y, or X is bought by Y.
As described in this paper, we refer to this dyadic representations of semantic relations as relational duality, and use it in an unsupervised co-clustering algorithm to extract numerous semantic relations from a given corpus.
In contrast to previously proposed supervised or semi-supervised approaches, which require some form of human intervention such as annotated training data, seeds of entity-pairs, or extraction patterns, the proposed method is fully unsupervised.
Extracting relations between entities has received much attention lately.
In object-level search engines such as Renlifang1 [38], it is particularly important to mine entity relations from the Web to build automatically, an entity relation graph to link all the extracted information together.
In contrast to document-level search engines, for which a user enters a keyword and retrieves a set of documents, in an object-level search engine, users search for a particular entity or a relation between entities.
In open information extraction (Open IE) systems such as TextRunner [4], the goal is to extract a large set of relational tuples without the need for any human input.
The extracted relations can then be used to answer natural language questions.
In the bio-medical domain, identifying the relations between proteins and diseases is helpful to discover potential side effects of various medicines.
Despite its numerous applications, extracting semantic relations among entities at Web scale is challenging for several reasons.
First, a single semantic relation can be expressed using multiple lexical patterns.
For example, aside from the pattern X acquired Y, an ac-1http://renlifang.msra.cn patterns such as X purchased Y, X completed its acquisition of Y, etc.
Second, there might exist more than one semantic relation between a pair of entities.
For example, before an ACQUISITION relation is established between two companies, those companies can have a COMPETITOR relation.
A relation extraction system must discover the different relations that hold between a pair of entities.
Third, the entities themselves might have variants.
For example, Microsoft Corp. is often designated as the Redmond software giant.
Manually specifying all different name variants of an entity is not feasible.
Moreover, the scale and the heterogeneity of Web text prohibit the use of time-consuming, domain-speci c approaches that require deep language processing techniques.
Supervised approaches to relation extraction that require manual annotation of all relations to be extracted are costly and impossible to execute on a Web scale because we do not know in advance the number or the types of relations that we must extract from the Web.
Semi-supervised approaches to relation extraction require some seed instances (i.e., a few pairs of entities between which the desired relation exist) or extraction patterns (either domain spe-ci c or independent) to be provided by a human.
Unfortunately, the quality of the extracted relations depends heavily on the initial seeds given to the system.
Moreover, it is not clear how many seeds are necessary to extract a particular relation correctly beforehand.
We propose an unsupervised approach that solves all the above-described problems in a principled manner.
Given a text corpus, the proposed method  rst extracts all mentions of entities and lexical-syntactic patterns that connect those entities.
We then represent pairs of entities and lexical-syntactic patterns in a matrix whose rows represent pairs of entities and columns represent lexical-syntactic patterns.
An ef cient sequential co-clustering algorithm is proposed to simultaneously identify different patterns that describe the same semantic relation, and entity-pairs between which the same semantic relation holds.
Moreover, we introduce a computationally ef cient heuristic to determine the row (entity-pair) and column (lexical-syntactic pattern) clustering thresholds in the algorithm.
To identify the relations represented by each pattern cluster, we use the resultant clusters to train a self-supervised multi-class logistic regression model with L1 regularization [24].
This approach produces a sparse representation of relations over lexical-syntactic patterns, thereby enabling us to identify the representative patterns in a cluster.
The contributions of this paper can be summarized as follows.
  We propose a dyadic representation of semantic relations that exist between a pair of entities using extensional and inten-sional representations.
Speci cally, a semantic relation R is expressed extensionally by extracting the set of entity pairs E(R), between which relation R holds.
Alternatively, the same relation R can be expressed intensionally by specifying the set of lexical-syntactic patterns P (R) used to express R.
To identify the potential entities in a corpus, we use a part-of-speech tagger and a noun-phrase chunker.
We employ a subsequence pattern-mining algorithm to extract lexical-syntactic patterns that express numerous semantic relations between a pair of entities.
  We propose a sequential co-clustering algorithm to simultaneously cluster different lexical-syntactic patterns that describe a particular semantic relation, and different entity pairs between which the same semantic relation holds.
The proposed sequential co-clustering algorithm avoids combinato-rial pairwise comparisons of data points and scales linearly with the number of data points to be clustered.
This desirable property of the proposed co-clustering algorithm enables us to use it with large real-world datasets containing numerous semantic relations.
Moreover, we propose an ef cient heuristic to determine the optimum values of clustering thresholds.
  To identify representative patterns that describe the semantic relation expressed by a cluster, we train a multi-class logistic regression model with L1 regularization.
The training is conducted in a self-supervised manner that requires no manually annotated training data, which is particularly important because we do not know the number or the type of relations that exist in a given corpus beforehand.
Moreover, L1 regularization yields sparse representations, consequently enabling us to  nd the most representative patterns that describe a particular semantic relation.
  We evaluate the proposed relation extraction algorithm in three tasks: measuring relational similarity between entity-pairs, open information extraction, and classi cation of relations in a social network system.
The proposed method outperforms previously proposed Open IE systems on a benchmark dataset.
Furthermore, the produced pattern clusters improve existing relational similarity measures.
Moreover, the proposed method is used to classify 53 different relations in a social network of 470, 671 nodes and 35, 652, 475 edges, thereby demonstrating its ef cacy in real-world large-scale applications.
The remainder of the paper is organized as follows.
In Section 2.1, we introduce the concept of relational duality.
We present a single-pass extraction algorithm in Section 2.2 to identify both entity pairs and lexical-syntactic patterns simultaneously in a given corpus.
A sequential co-clustering algorithm is proposed in Section 2.3 to identify the numerous semantic relations described by extensional and intensional representations of relations extracted in Section 2.2.
A self-supervised classi er is presented in Section 2.4 to identify the representative patterns that describe the relation captured by each cluster.
We evaluate the proposed relation extraction method using three tasks in Section 3.
We discuss related research efforts in Section 4 and conclude this paper.
We propose relational duality: a dyadic representation of the semantic relations that exist between two entities.
A semantic relation R is de nable by stating all entity pairs between which relation R holds.
We use the notation E(R) to denote the set of entity pairs in which the relation R holds between the two entities in each element (i.e. entity pair).
De ning a concept by enumerating all its instances is called an extensional de nition of the concept.
Alternatively, we can de ne R by stating the different properties that must be satis ed by two entities to realize a relation R between them.
We denote the set of properties of R as P (R).
De ning a concept by stating all the properties that are required to come to that de nition is called an intensional de nition of the concept.
Both E(R) and P (R) de ne the same semantic relation R. Therefore, a duality exists between the two de nitions for R.
For example, we can extensionally de ne the ACQUISITION relation by specifying all pairs of companies between which an acquisition has taken place, such as (Google,YouTube), (Microsoft, Powerset), (Yahoo, Inktomi), etc.
An intensional de nition of ACQUISITION speci es the different expressions that indicate an acquisition has taken place between two companies X and Y, such as for a human to enumerate all the different expressions that describe a particular semantic relation.
Consequently, in Section 2.2, we propose a single-pass extraction method that extracts both entity pairs and numerous lexical-syntactic patterns that describe different relations that exist in a given corpus.
The dual representation of semantic relations can be captured ef- ciently within a co-clustering framework.
We represent relational data in a matrix in which the rows correspond to entity pairs, and columns correspond to lexical-syntactic patterns.
The row vectors can be considered as de ning the distribution of a particular entity pair over the space spanned by lexical-syntactic patterns.
Similarly, each column vector represents the distribution of patterns over the space spanned by the entity pairs.
Distributional similarity [19, 21] is useful to identify the different patterns that describe the same semantic relation R (i.e.P (R)), and different entity pairs between which the same semantic relation R exists (i.e.E(R)).
In Section 2.3, we propose a sequential co-clustering algorithm to cluster a large data matrix ef ciently.
By framing relational duality as a co-clustering problem, we can identify the variants of a particular entity.
For example, if Redmond software giant is a variant of Microsoft, then we would expect both entity pairs (Mi-crosoft,Powerset) and (Redmond software giant,Powerset) to appear within the same cluster.
By merging the variants of an entity, we can reduce the sparsity in pattern vectors, thereby improving the accuracy of clustering.
Each cluster produced by the co-clustering algorithm expresses a particular semantic relation.
However, in this paper, we do not assume any prior knowledge related to the number or the type of relations that exist in a given corpus.
Therefore, it is extremely useful to identify the relation expressed by each cluster both for evaluation purposes, and for presentation purposes.
In Section 2.4, we propose a self-supervised relation detection algorithm that labels each entity pair (row) cluster with representative lexical patterns.
To extract entity pairs and lexical-syntactic patterns from a given text corpus, we propose a single-pass extraction method.
Because the proposed extraction method requires only a single traversal over the corpus, visiting each sentence once, it is scalable to large datasets.
We resort to shallow linguistic processing techniques such as sentence boundary detection, part-of-speech (POS) tagging, and noun phrase chunking, for which ef cient and accurate tools are available for many languages.
First, we split the given text corpus into sentences using a sentence boundary detection tool2.
We then run a part-of-speech (POS) tagger3 and annotate each sentence with POS tags.
To detect potential entities in sentences, we use a noun phrase chunking tool4 and extract noun phrase chunks containing at least one proper noun (NP).
It is noteworthy that we require no deep linguistic analysis, as do Open IE systems, which require dependency parsing [4] or co-reference resolution [31].
Moreover, we do not assume the availability of named entity recognition (NER) tools that can tag entities of different types in a text.
Instead, we use a name phrase chunking tool that can group multi-word entities such as Adobe Systems or Microsoft Corporation.
No additional information, such as the type of the entity (i.e., person, company, or location) is required in the subsequent processing.
2http://stp.ling.uu.se/~gustav/java/classes/ MXTERMINATOR.html 3http://nlp.stanford.edu/software/tagger.
shtml 4http://chasen.org/~taku/software/yamcha/ An example is presented in Table 1, from which we extract the entity pair (Adobe Systems,Macromedia).
Next, we replace the two entities respectively with two variables X and Y in a sentence.
The entity that occurs  rst in the sentence is replaced by X, whereas the entity that occurs second is replaced by Y.
The corresponding POS tags in the POS tag sequence is also replaced by X and Y, as presented in Table 1.
Lexical-syntactic patterns have been used successfully in various natural language processing tasks such as extracting hyper-nyms [20, 32], or meronyms [6], question answering [27], and paraphrase extraction [7].
Following those previous works, we present a shallow lexical pattern extraction algorithm to represent the semantic relations between two entities.
We generate subsequence patterns from both surface forms of the sentences and POS tag sequences that satisfy the following conditions.
(i).
A subsequence must contain exactly one occurrence of each X and Y (i.e., exactly one X and one Y must exist in a sub-sequence).
(ii).
The maximum length of a subsequence is L tokens.
(iii).
A subsequence is allowed to have gaps.
However, we do not allow gaps of more than g number of tokens.
Moreover, the total length of all gaps in a subsequence should not exceed G tokens.
(iv).
We expand all negation contractions in a sentence.
For example, didn t is expanded to did not.
We do not skip the word not when generating subsequences.
For example, this condition ensures that from X is not a Y, we do not produce the subsequence X is a Y.
We designate the subsequences of surface forms produced by the procedure described above as lexical patterns.
The corresponding POS tags of a lexical pattern is called a syntactic pattern.
The values of parameters L, G, and g are set experimentally, as explained later in Section 3.
The proposed lexical-syntactic pattern extraction algorithm considers all the words in a sentence, and is not limited to extracting patterns only from the mid x (i.e., the portion of text in a sentence that appears between a pair of entities).
Moreover, the consideration of gaps enables us to capture relations between entities located at a distance in a sentence.
We use pre xspan algorithm [26] to generate subsequences.
The constraints listed above are used to prune the search space, thereby reducing the number of subse-quences generated by pre xspan.
Some lexical-syntactic patterns extracted using the proposed method are shown in Table 1 (not all patterns are shown because of the limited availability of space).
We consider all entity pairs in a sentence, and extract lexical-syntactic patterns using the procedure described above.
We then aggregate entity pairs and lexical-syntactic patterns we extract from each sentence.
It is noteworthy that the proposed extraction method visits each sentence only once.
Once we have completed processing the entire corpus, we select the most frequently occurring entity pairs and lexical-syntactic patterns for the co-clustering algorithm described in Section 2.3.
Assuming that the set of all extracted entity pairs is E, and that the set of all extracted lexical-syntactic patterns is P , then we propose a sequential co-clustering algorithm to simultaneously identify the subset of lexical-syntactic patterns P (R) that describes a particular semantic relation R, and the subset of entity pairs, E(R), between which R holds.
First, we represent the dyadic relation between entity pairs and lexical-syntactic patterns as matrix A.
Each POS tagged Entity chunked Substitution surface form POS sequence lexical patterns syntactic patterns Table 1: Extracting entity pairs and lexical-syntactic patterns from text.
another example of a statutory merger is software maker Adobe Systems acquisition of Macromedia .
another example of a statutory merger is software maker [Adobe Systems] acquisition of [Macromedia].
Adobe Systems = X, Macromedia = Y another example of a statutory merger is software maker X acquisition of Y .
X acquisition of Y, software maker X acquisition of Y, X of Y, software X acqusition Y
 p   POP(P ) ASSIGN(p, Cp,  ) e   POP(E) ASSIGN(e, CE ,  ) Algorithm 1 Sequential co-clustering algorithm.
Input: sets E, P , matrix A, thresholds  ,   Output: row clusters CE, column clusters CP










 9: end while

 12: max     13: c    null
 sim   cosine(x, cj)

 if sim > max then


 end if 20: end for 21: if max >   then c    c    x
 23: else C   C   {x}
 25: end if max   sim c    cj extracted entity pair in Section 2.2 is represented as a row in this matrix, whereas each lexical-syntactic pattern is represented as a column.
The Aij element of the data matrix denotes the number of times the lexical-syntactic pattern pj was extracted for the entity pair ei.
Each normalized row vector ei in matrix A denotes the distribution of an entity pair ei over lexical-syntactic patterns.
Similarly, each normalized column vector pj in matrix A denotes the distribution of a lexical-syntactic pattern pj over entity pairs.
From distributional hypothesis [19], it follows that if two entity pairs are distributed similarly over a set of lexical-syntactic patterns, then those entity pairs must be relationally similar.
We use distributional similarity to cluster entity pairs and lexical-syntactic patterns simultaneously.
(cid:80) The pseudo-code of the proposed sequential co-clustering algorithm is presented in Algorithm 1.
The algorithm takes as its input E, P , A, and two clustering thresholds: row (entity pair) clustering threshold,  , and column (lexical-syntactic pattern) clustering threshold,  .
The output of the clustering algorithm is the set of row clusters, CE, and column clusters, CP .
First, in Line 1, we sort the set of entity pairs E in the descending order of total frequency, j Aij, of each entity pair ei with all lexical-syntactic patterns in P .
Similarly, in Line 2, we sort the set of lexical-syntactic patterns P in the descending order of total frequency, i Aij, of each pattern with all entity pairs in E. After sorting, the most common entity pairs and patterns in the corpus appear respectively at the (cid:80) beginning of E and P , whereas rare instances are shifted to the end.
In Line 3, we initialize both row and column cluster sets to the empty set.
The function, POP(P ) in Line 5, returns the  rst pattern p   P and removes p from P , thereby reducing the size of P by one.
Next, the function, ASSIGN, measures the similarity between the vector p that corresponds to pattern p and each column cluster cj in CP .
Here, cj denotes the centroid vector of the j-th column cluster.
Similarity between p and cj is measured using cosine similarity.
If the similarity between p and the most similar cluster c  is greater than the column clustering threshold  , then we merge p to c .
Here, the operator   denotes vector addition.
Otherwise, we form a new column cluster that contains p and append it to CP .
This procedure is repeated for entity pairs in Lines
 and P are empty.
It is noteworthy that the operation of merging rows or columns in Line 22 changes the distributions of patterns and entity pairs, thereby directly in uencing the subsequent similarity computations.
For example, if a pattern p is merged into a column cluster cj, then, in the next iteration, when we compute cosine similarity between entity pairs, all patterns in cluster cj will be considered as forming a single dimension.
Algorithm 1 can be considered as a co-clustering extension of the one-sided sequential clustering proposed by Bollegala et al. [8].
However, as we demonstrate experimentally in Section 3.1, the co-clustering version produces better results than its one-sided counterpart that does not cluster entity pairs.
The sorting operations in Algorithm 1 require respectively, O(|E|log|E|) and O(|P|log|P|) complexities for entity pairs and lexical-syntactic patterns, where |S|, denotes the cardinality of a set S. This sorting operation is required only once at the start.
The while-loop starting from Line 4 in Algorithm 1 terminates after max(|E|,|P|) iterations.
The greedy nature of the algorithm avoids combinatorial pairwise comparisons among all entity pairs or all lexical-syntactic patterns.
Most co-clustering algorithms [14, 17] require the number of row and column clusters to be given as inputs.
However, we do not know the exact number of relations in the given corpus from which we must extract relations.
Alternatively, Algorithm 1 has two parameters,   and  , which indirectly specify the number of clusters.
A popular approach to setting the number of clusters, cross-validation, tries different combinations of parameter values and evaluates some goodness criteria on a set of held out data.
However, an exhaustive search in parameter space is infeasible for large datasets.
Consequently, we propose an ef cient heuristic to determine the optimal values of the clustering thresholds.
First, we consider the distribution of similarity scores in a dataset of T points, portrayed in Figure 1.
There are T (T   1)/2 pairs in a dataset of T points, between which we compute the similarity scores.
Figure 1 plots the normalized frequency (i.e., dividing the frequency counts by T (T   1)/2) such that the total area of the blue bars equals one).
The solid green line connects the midpoints of the bars in the histogram and represents the distribution of similarity scores.
Figure 1 shows results obtained using the ENT dataset ing threshold   by considering the similarity distribution for entity pairs.
It is noteworthy that the optimum value of the threshold given as Formula 6 depends only on   and g( ) (dependence on a and k can be eliminated using Formulas 4 and 5) .
In practice, we set a small bin size (e.g.
  = 0.05) and compute g( ) as the fraction of data points that have similarity less than  . Ef cient algorithms have been proposed [30] that do not require pairwise comparisons of all data points to  nd the number of data points that have a similarity score that is less than a given threshold.
In unsupervised relation extraction, the relation types to be extracted are an unknown prior.
Co-clusters generated by Algorithm 1 capture numerous semantic relations that exist in a given corpus.
For evaluation and presentation purposes, it is useful to label clusters with representative lexical patterns in them, which is particularly challenging for two reasons.
First, we have no labeled data to train a supervised classi er to discriminate one cluster from another.
Second, from among numerous lexical patterns, we must select the representative ones.
We describe a self-supervised clas-si cation approach that overcomes both challenges.
The objective of Algorithm 1 is to produce a set of clusters where each cluster represents a different semantic relation between entity pairs.
We assign a unique cluster i.d., yk, to all entity-pairs ei in a cluster Ck.
We represent an entity pair ei as a feature vector ei, in which the j-th feature is set to Aij.
We then model the problem of identifying representative lexical patterns in a cluster as one of dis-criminative feature selection in multi-class classi cation.
Speci -cally, we use L1 regularized multi-class logistic regression, where the posterior probability of an entity pair ei is given as p(yk|ei) = exp(wT k ei) y Y exp(wT k ei) .
(7) Here, Y denotes the set of cluster i.d.s; wk is the weight vector associated with cluster Ck.
We maximize the following w.r.t.
wk, as log p(yi|ei)     ||wk||1.
(8) (cid:88) (cid:88) (ei,yi) k The  rst term in expression 8 is the log-likelihood of entity pairs; the second (regularization) term is the sum of L1 norms of weight vectors wk.
We use the notation, ||x||1 to denote the L1 norm of a vector x.
The effect of regularization towards overall training process is adjusted using the regularization coef cient  (> 0).
We use Orthant Wise Limited-memory Quasi-Newton (OWL-QN) method to solve the optimization problem de ned in the expression shown in 8.
Actually, L1 regularization is known to produce sparse weight vectors, where most of the weights are set to zero [24].
This feature is particularly useful for the current task because it enables us to identify the lexical patterns that discriminate one cluster from another.
After training, we select the highest nonzero weighted lexical patterns from a cluster as the representatives of the semantic relation described by that cluster.
We evaluate the proposed method in three tasks.
First, in Section 3.1, we use the pattern clusters produced using the proposed method to measure the relational similarity between entity pairs in the ENT benchmark dataset [8], and compare it to the previously proposed relational similarity measures.
We empirically study the behavior of Algorithm 1 over the complete parameter space.
More-(cid:80) Figure 1: Approximating the distribution of similarity scores between data points.
Bars show the actual histogram and the solid green line connects the midpoints of the bars.
Approximation g(x) is shown as a dotted red line.
(described later in Section 3.1), thereby showing the distribution of similarity in real data.
The similarity among most data points tends to be very small in Open IE because, generally, there exist numerous semantic relations in a corpus and only a few between a given pair of entities.
We approximate the actual distribution of similarity scores as a power-law curve and use this approximation to estimate the optimum values for clustering thresholds.
This is analogous to Zipf s law [22], which is observed with word counts in large text corpora.
We de ne the power-law approximation g(x) of the actual distribution of similarity scores x as g(x) = ax  k.
(1) Here, a and k (1 < k < 2) are real-valued constants.
Assuming the bin size in the histogram in Figure 1 to be  (< 1).
Then, by  xing g(x) at x =  , we have By considering the area under the curve for g, we have g( ) = a   k.
(cid:90) 1 g(x) dx = 1,   From Formulas 2 and 3, we obtain a = g( ) k, k =  g( ) + 1.
(2) (3) (4) (5) Algorithm 1 adds a new entity pair (or a pattern) to an existing cluster only if the similarity between the cluster and the entity pair (or the pattern) exceeds the corresponding row or column clustering threshold.
Therefore, with numerous data points (e.g., entity pairs or patterns), the average similarity between data points inside a cluster converges to the clustering threshold (proof omitted).
In the case of optimum clustering, the similarity between data points in different clusters will be zero.
Therefore, the average similarity among all data points (i.e., the mean Eg(x) of the distribution g(x)) will be equal to the average similarity between data points inside clusters (i.e., the clustering threshold).
For column clusters, the optimal threshold   is given as   = Eg(x) = xg(x) dx =   a(1    2 k) 2   k .
(6) (cid:90) 1
 supervised relation detection method.
Second, in Section 3.2, we compare the proposed method against three previously proposed Open IE systems on SENT500 benchmark dataset.
This dataset contains 500 manually annotated sentences that describe four semantic relations between named-entity pairs.
It has been used extensively in previous work on Open IE.
This experiment is intended to demonstrate the effectiveness of the proposed method in Open IE.
Previous work using the ENT dataset has not used syntactic patterns.
Consequently, to compare the proposed method against results of previous works that used the ENT dataset, we limit ourselves to lexical patterns.
We evaluate the ben-e t of using syntactic patterns with the SENT500 dataset.
Third, in Section 3.3, we employ the proposed method to identify 53 different relations in a social network system containing 35 million nodes, thereby demonstrating the accuracy and scalability of the proposed method in real-world relation extraction tasks.
In all experiments, we  x the values for the one-pass extraction to L = 5, g = 2, and G = 4.
Relational similarity between two pairs of words is de ned as the correspondence between semantic relations that exist between the two words in each word pair.
For example, the two pairs, (os-trich,bird) and (lion,cat) are considered relationally similar because the relation X is a large Y holds between the two words X and Y, in each of those word pairs.
Bollegala et al. [8] proposed a supervised method (RELSIM) to measure the relational similarity between two word pairs using a set of automatically extracted lexical pattern clusters.
We employ the lexical-syntactic pattern clusters extracted using the proposed unsupervised method to measure the relational similarity between two word pairs.
If the pattern clusters produced by the proposed method are useful to predict the relational similarity between given two word pairs, then it not only justi es the proposed method; it also demonstrates a useful application of it.
Following [8], we represent a word pair (a, b) as an n-dimensional vector f(a,b), in which the k-th element, f k (a,b), is set to the total frequency of all lexical-syntactic patterns in a pattern cluster Ck with word pair (a, b).
Under this representation, each pattern cluster Ck contributes a single feature to the feature vector for a word pair.
Considering the fact that each pattern cluster is expected to represent a unique semantic relation, this feature representation can be regarded as a projection of a word pair over the space de ned by semantic relations.
We then measure the relational similarity (alternatively the relational distance) between two word pairs (a, b) and (c, d) using Mahalanobis distance between the corresponding feature vectors f(a,b) and f(c,d), which is given as  1(f(a,b)   f(c,d)).
(9) Here,  1 denotes the inverse of the inter-cluster correlation matrix computed for pattern clusters.
The (i, j) element of   is computed as the inner product between the centroid vectors of pattern clusters i and j.
In contrast to the Euclidean distance, the Mahalanobis distance has shown to be more appropriate for the task of measuring relational similarity [8] because semantic relations are not independent.
(f(a,b)   f(c,d))T  We use the ENT dataset [8] as a gold standard of relational similarity.
The ENT dataset contains 100 entity pairs describing the  ve semantic relations: ACQUISITION (between two companies, where one company is acquired by the other, e.g.
(Google,YouTube)), HEADQUARTERS (between a company and the location of its headquarters, e.g.
(Microsoft,Redmond)), FIELD (between a person and his  eld of expertise, e.g.
(Albert Einstein,Physics)), CEO Table 2: Performance of the proposed method and previous work on relational similarity measures.
Relation




 Overall Average Precision































 (between a company and its current CEO, e.g.
(Steve Jobs,Apple)), and BIRTHPLACE (between a person and his place of birth, e.g.
(Charlie Chaplin,London)).
For each entity pair (a, b) of relation R in the ENT dataset, we measure the relational similarity between (a, b) and the remaining 99 entity pairs.
A good relational similarity measure must assign higher similarity scores to entity pairs with similar semantic relations.
Consequently, we evaluate the top k similar pairs to each entity pair in the dataset using average precision given as (cid:80)k r=1 Pre(r)   Rel(r) no.
of relevant entity pairs .
Average Precision = (10) Here, Rel(r) is a binary valued function that returns 1 if the entity pair at rank r has the same relation (i.e. R) as in (a, b).
Furthermore, Pre(r) is the precision at rank r, which is given as Pre(r) = no.
of entity pairs with relation R in top r pairs r .
(11) In fact, the ENT dataset contains 20 entity pairs for each relation.
Following previous work, we evaluate using the top 10 ranked results (i.e. k = 10).
The pattern extraction algorithm described in Section 2.2 extracts 142, 655 lexical patterns from the text snippets provided in the ENT dataset.
We then use Algorithm 1 to co-cluster both entity pairs and the extracted patterns.
Clustering thresholds   and   are estimated respectively as 0.67 and 0.83 using Formula 6.
This process produces 9 row (entity pair) clusters and 139 column (pattern) clusters.
Table 2 presents a comparison of the relational similarity measured using the pattern clusters produced using the proposed method (PROP) against four others: VSM (Vector Space Model-based approach [33]), LRA (Latent Relational Analysis [33]), EUC (Euclidean distance between feature vectors [8]), and RELSIM (Mahalanobis distance between feature vectors [8]).
Except for the proposed method, all other  gures in Table 2 are obtained from previously published results obtained using the ENT dataset.
Overall, PROP shows the highest average precision score (0.76) in Table 2.
Moreover, for three out of the  ve relations in the ENT dataset, PROP outperforms all existing relational similarity measures.
It is noteworthy that although RELSIM has a similar average precision score (0.74) to that of PROP, unlike PROP, which is unsupervised, RELSIM is a supervised method that requires labeled data for training.
Moreover, both EUC and RELSIM use one-sided sequential clustering in which only patterns are clustered.
In contrast, PROP clusters both patterns and entity pairs simultaneously using Algorithm 1, which exploits the dyadic structure in the data more effectively.
To study the behavior of the proposed sequential co-clustering algorithm empirically, we vary   and   in range [0, 1], and measure the average precision over entity pairs in the ENT dataset using  ve-fold cross-validation.
Average precision scores for various combinations of   and   values are shown in Figure 2.
From Figure 2 it is readily apparent that for  xed low values of  , average apparent that lexical patterns that describe the target relations are identi ed correctly using the proposed method.
We subjectively compare the patterns presented in Table 3 against a baseline pattern selection method, which simply selects the most frequent pattern in each cluster.
We asked three judges (excluding the authors of the paper) to grade the top 10 patterns extracted using the proposed method against the top 10 frequent patterns in each cluster using four grades: A (the pattern selected using the baseline method is better), B (the pattern selected using the proposed method is better), C (patterns selected using both methods are equally good), and D (patterns selected using both methods are bad).
We elicit human judgments for 50 pairs of patterns (top 10 patterns for each of the 5 relations in the ENT dataset).
Judges are not informed of the underlying nature of the baseline or the proposed method.
Moreover, the ordering between the two patterns in each example is set randomly to avoid any bias in judgment towards a particular system.
The inter-judge agreement measured using Fleiss s kappa is 0.4497 and is statistically signi cant (p < 0.05).
Results of the subjective evaluation are presented in Table 4.
Overall, patterns selected using the proposed method are preferred three times more than those selected using the baseline method.
Particularly for CEO and FIELD relations, the patterns selected using the proposed method are preferred more than four times more than those obtained using the baseline method.
Selecting frequent patterns from clusters usually produces ambiguous common patterns such as X and Y, X or Y, X is a Y, etc.
In contrast, the proposed method identi es discriminative patterns that uniquely identify a particular semantic relation.
We evaluate the proposed relation extraction method on Open IE using the SENT500 [5] published benchmark corpus.
This corpus contains 500 sentences; each sentence has one pair of entities.
In fact, the SENT500 dataset includes 26 unique entity pairs.
Some entities are represented by more than one name variant (e.g.
Adobe Systems Inc. vs. Adobe Systems), thereby producing 65 unique name variant pairs in the corpus.
Each entity pair in SENT500 dataset describes one of the four relation types: ACQUISITION (an acquisition relation between two companies), BIRTHPLACE (a person and that person s place of birth), INVENTOR (a person and that person s invention), and WONAWARD (a person and an award that person won).
Previous works on Open IE have measured micro-averaged precision, recall, and the F score of relation extraction using the SENT500 dataset.
Therefore, by evaluating using the SENT500 dataset, we can directly compare the proposed method to previous work on Open IE.
We run the one-pass extraction method (Section 2.2) on SENT500 dataset, and extract both lexical and syntactic patterns.
The extracted lexical and syntactic patterns are respectively, 947 and 384.
The estimated values of the clustering thresholds   and   are respectively, 0.0005 and 0.01153.
For those threshold values, Algorithm 1 produces 4 row clusters (corresponding to entity pairs) and 14 column clusters (corresponding to lexical-syntactic patterns).
For evaluation purposes, we label each row cluster with the relation that exists between most entity pairs in the cluster.
The proposed method (PROP) is compared against previous works on Open IE using the micro-average precision, recall and F scores, as shown in Table 5.
In Table 5, O-NB is the naive Bayes relation classi- er described in [4], O-CRF is the conditional random  eld-based Open IE system described in [5], and MLN is the Markov logic network-based Open IE system described in [38].
To study the contribution of lexical and syntactic information for Open IE, we Figure 2: Average precision as a function of theta and phi.
Table 4: Subjective evaluation of patterns.
Relation




 overall












 precision is insensitive w.r.t.
  because low   values produce large and noisy pattern clusters that contain patterns for more than one semantic relation.
However, when   increases, the average precision also increases and the highest average precision score (0.78) is obtained for   = 0.70 and   = 0.85.
When   is increased further, we obtain numerous pattern clusters with very few patterns in them, thereby yielding sparse feature vectors.
Consequently, the average precision drops for high   values.
The predicted values of   (0.67) and   (0.83) closely approximate the optimal parameter values obtained using cross-validation.
However, conducting cross-validation over the full parameter space to determine threshold values is time consuming and is not scalable to large datasets.
For example, on a computer with a Quad-Core processor (Intel Corp.) with 8GB of RAM, it takes more than 10 days to perform cross-validation over the parameter space as shown in Figure 2 for the ENT dataset.
In stark contrast, the estimation method described in Section 2.3 requires less than 30 min to estimate the threshold values using the same computer.
To select the representative lexical patterns that describe the semantic relations in ENT dataset, we run the self-supervised relation detection method described in Section 2.4 as follows.
First, the 142, 655 patterns extracted by the single-pass extraction method described in Section 2.2, are clustered into 139 pattern clusters and
 signed with a pseudo-class label that indicates its entity pair (row) cluster.
Next, multi-class logistic regression with L1 regularization is run on pseudo-labeled training instances.
The regularization coef cient   is set to its default value of 1.
The self-supervised relation detection method assigns nonzero weights to 263 patterns, thereby producing a small subset of representative patterns.
Table 3 shows the top 10 patterns with the highest weights in  ve clusters describing the relations in ENT dataset.
For explanatory purposes, on the  rst row of Table 3 we have indicated the relation that is assigned in the ENT dataset for most entity pairs in a row cluster.
The total number of patterns in a cluster is shown within brackets following the name of the relation.
Two entities are indicated as X and

 Table 3: Lexical patterns selected using the self-supervised classi er Y chairman and ceo X X , ceo of Y Y founder X Y chef X Y ceo X, at ceo X has Y on Y chairman X Y ceo X
 X has Y on the X acquires Y X to purchase Y X has purchased the Y X bids $ N.Nb for Y X (formerly Y) Y (now X when X bought Y X to acquire Y X buys Y X buys ad  rm Y X headquarters in Y X head-of ce in Y X laboratories Y Y based X X in Y X building in Y Y of ce of X past X of ces in Y Y calif.-based X X .com Y
 Y legend X X revolutionized Y Y great X Y champion X X retires from Y Y star X X and modern Y X referred to Y Y player X X pga tour Y
 X was born in Y Y born X X composer Y X s birthplace in Y X s childhood in Y Y native X X left Y to X walk, Y X of Y X wird in Y city Table 5: Evaluation of open IE on SENT500 dataset.
Precision Method


 PROP (lexical patterns) PROP (syntactic patterns) PROP (lexical+syntactic patterns)





 Recall











 Table 6: Classifying relations in a social network.
Relation colleagues alumni fan husband brother Micro














 Relation friends co-actors teacher wife sister


























 implement the proposed method in three  avors: using only lexical patterns (PROP (lexical patterns)), using only syntactic patterns (PROP (syntactic patterns)), and using both (PROP (lexi-cal+syntactic patterns)).
All  gures presented in Table 5, except for the proposed method, are obtained from original publications.
The highest precision among the different methods compared in Table 5 is reported using the proposed method using only lexical patterns.
However, the use of syntactic information improves recall and consequently the best F score is reported using the proposed method using only syntactic patterns.
Lexical patterns are useful to detect speci c relations between entities.
On the other hand, syntactic patterns can generalize the relations that exist between entities, thereby improving the recall.
Combining both lexical and syntactic patterns does not improve the performance beyond the mere use of syntactic patterns.
To evaluate the scalability and performance of the proposed method in large real-world systems, we use the proposed method to classify relations between entities in an online social network mining system, SPYSEE5.
This network contains 470, 671 nodes (people) and 35, 652, 475 edges describing numerous relations between nodes.
In fact, SPYSEE uses the automatic social network extraction method described by Matsuo et al.[23]; it is the largest of such systems in Japan.
To mine the social network of a person P ,  rst, SPYSEE uses the name of that person as the query to a Web search engine and downloads the top ranked search results.
Subsequently, for each person name Q that appears in the downloaded search results, SPYSEE determines whether a relation exists between P and Q using various co-occurrence statistics such as the Jaccard coef -cient and the overlap coef cient.
To apply the proposed method to identify relations between people in SPYSEE, we  rst run the single-pass extraction method described in Section 2.2 on the web pages that had been downloaded by SPYSEE for all personal names in the system.
We then perform sequential co-clustering (Algorithm 1) to identify the entity pairs between which the same relation holds.
Self-supervised relation detection method (Section 2.4) is used to label each entity pair cluster with representative lexical patterns that describe the relation between entities in the cluster.
Manually evaluating the extracted rela-5http://spysee.jp/ tions among all the entity pairs in SPYSEE is impossible because of the extremely large number of entity pairs.
Consequently, we randomly selected 50, 000 entity pairs (edges) from SPYSEE and evaluate on this subset.
The proposed pattern extraction algorithm (Section 2.2) extracts 38, 076 unique lexical-syntactic patterns, out of which 11, 193 appear for more than two entity pairs.
To avoid using noisy and rare patterns, which frequently contain misspellings and other irregularities, we consider only those 11, 193 patterns in the subsequent processing.
Clustering thresholds   and   are estimated respectively, as 0.007 and 0.0131 using Formula 6.
For those values of thresholds, Algorithm 1 produces 383 pattern clusters and 664 entity pair clusters.
We manually classi ed the entity pairs into 53 different relations (designated as the gold-standard), and evaluate the performance of the proposed method using micro and macro-averaged precision (P), recall (R) and F scores (F).
Because of the limited availability of space, we show the clustering performance for randomly selected 10 relation types in Table 6.
Two entities can share more than one relation.
For example, colleagues can also be friends.
Consequently, the gold standard assigns multiple relation types for such entity pairs.
Table 6 shows that the proposed method correctly identi es numerous relations existing among people in a social network.
In Figure 3, we present a comparison of the proposed co-clustering algorithm (Algorithm 1) against two other co-clustering algorithms: MinSQD (minimum sum-squared residue co-clustering) [11], and ITCC (information theoretic co-clustering) [14], using Co-cluster6, a publicly available co-clustering tool.
As shown in Figure 3, the proposed method is several orders of magnitude faster than MinSQD and ITCC over widely various dataset sizes.
The end-to-end processing time taken by the proposed method to extract relations, cluster, and classify for the SPY-SEE dataset is ca.
2 hr using a Quad-Core processor (Intel Corp.) with 8 GB of RAM.
The approach proposed in this paper is motivated by work in three  elds: relation extraction, relational similarity measurement, and co-clustering.
Next, we discuss the previous work in those  elds and compare it to the proposed method.
6http://www.cs.utexas.edu/users/dml/ Software/cocluster.html the proposed method in the sense that they both extract unknown relations from heterogeneous corpora, the proposed method differs from the Open IE methods in several respects.
First, unlike the proposed method, Open IE systems require human-selected features to learn a good extractor.
Second, Open IE systems use deep linguistic parsing techniques to label training examples.
In contrast, the proposed method uses cheaper and more robust linguistic processing and depends on an ef cient co-clustering algorithm to produce training data for relation classi cation.
Although the proposed method extracts unknown relations in a given text as done by Open IE systems, we go one step ahead and attempt to label the extracted relations.
This step of identifying the extracted relations with informative lexical patterns is particularly important in any relation extraction system that attempts to extract unknown relations.
It not only enables us to evaluate the accuracy of the extraction; it also provides a useful insight into which relations exist in a given corpus.
Relational similarity [8, 33, 34, 35] measures the correspondence between semantic relations existing between two pairs of words.
For example, the relation X is a large Y exists between the two words in each word pair (lion, cat) and (ostrich,bird).
Consequently, the two word pairs are considered to be relationally similar.
The row clusters produced by the proposed co-clustering algorithm group entity pairs with the same semantic relation.
Relational similarity measures  rst represent a pair of words as a vector of lexical patterns and then measure the distance between those vectors.
A fundamental difference between relational similarity measures and the proposed method is that in relational similarity measures the two word pairs between which relational similarity must be computed are given in advance; the proposed method has the additional challenge of  nding related word pairs.
As Section 3.1 shows, the pattern clusters produced by the proposed method are useful to improve existing relational similarity measures.
The goal in co-clustering [3] is to cluster the rows and the columns of a given matrix simultaneously, such that the difference between the original matrix and the clustered matrix is optimal with respect to some objective function.
Co-clustering algorithms [11, 14] that optimize different objective functions have been devel- oped and used in a wide array of applications such as simultaneously clustering words and documents in information retrieval [14], and clustering genes and expression data for biological data analysis [11].
Co-clustering is an NP-hard problem [2] for which approximate algorithms have been proposed.
However, most existing co-clustering algorithms require the number of row and column clusters as inputs, which is unknown in an open relation extraction setting.
Moreover, the high computational complexities of these algorithms prohibit their application in large-scale relation extraction tasks.
The sequential co-clustering algorithm presented in Section 2.3 does not require the number of clusters as input.
It can ef ciently cluster numerous data points, as described in Section 3.3.
We proposed a relation extraction method that exploits the dyadic nature in semantic relations within a co-clustering framework.
Semantic relations that exist between numerous entities were represented using lexical-syntactic patterns.
A one-pass extraction algorithm that can ef ciently extract numerous expressive patterns was introduced.
To cluster the extracted entity pairs and lexical-syntactic patterns simultaneously, we proposed an ef cient sequential co-clustering algorithm.
To identify the representative patterns that describe a particular semantic relation, we proposed a self-Figure 3: Comparison of processing times.
Traditionally, relation extraction is framed as a binary classi cation problem: Given a sentence S and a relation R, does S assert R between two entities in S?
Supervised classi cation methods such as support vector machines (SVMs) with language-oriented kernels have been used to learn binary classi ers [10, 16, 18, 36,
 in which they jointly learn to identify named entities and relations.
Culotta et al. [13] model the problem of relation extraction as a one of sequence labeling and used conditional random  elds to identify the relations in a given document.
Speci cally, they perform relation extraction on biographical text in which the topic of each document is known in advance.
Then, for each entity found in a document, their goal is to predict the relation between that entity and the topic of the document from a  nite set of pre-de ned relations.
In our setting however, we do not know the relations that must be extracted beforehand.
Moreover, the need for manually annotated training data by these supervised relation extraction systems makes it dif cult to apply them to large-scale heterogeneous relation extraction tasks such as relation extraction from the web.
Bootstrapping methods [1, 9, 15, 25, 38] to relation extraction are attractive because they require markedly fewer training instances than supervised approaches do.
Bootstrapping methods are initialized with a few instances (often referred to as seeds) of the target relation [1, 25, 38] or general extraction templates [15].
During subsequent iterations of the bootstrapping process, new extraction patterns are discovered and used to extract new instances.
The quality of the extracted relations depends heavily upon the initial seeds provided to the bootstrapping system.
If the extracted relations are of low quality, then we must restart with a different set of seeds and rerun the bootstrapping process.
It might not be readily apparent to a non-expert user to devise good seeds of the target relation.
Moreover, in a setting such as ours, in which we attempt to process a heterogeneous corpus such as Web text, it is not possible to know the target relations in advance, let alone provide seeds or extraction patterns for each relation.
Open Information Extraction (Open IE) [4, 5, 31] is a domain independent information extraction paradigm and has been studied in both the natural language document corpus [31], and the Web environment [4, 5] to extract relation tuples.
Open IE systems are initialized with a few manually provided domain independent extraction patterns.
To produce training data for the algorithm, dependency parsing is conducted on a text corpus; domain independent extraction patterns are used to identify correct extractions.
Using the created training data, a classi er is trained to identify the correct
 for three tasks: measuring relational similarity between entity pairs, Open IE, and classifying relations in an online social network.
The pattern clusters extracted using the proposed method improved previously proposed relational similarity measures on the ENT benchmark dataset.
Moreover, a subjective evaluation showed that the proposed self-supervised relation detection method can identify representative lexical patterns of a semantic relation.
Experiments investigating the ability of the proposed method to conduct Open IE revealed that the proposed method outperforms all existing Open IE systems on a benchmark dataset of 500 sentences.
The proposed method correctly classi ed 53 relation types in an online social network system with over 35 million edges, thereby proving its applicability in large real-world datasets.
