Web, in general, and electronic markets, in particular, are still evolving, periodically giving birth to new amazing market mechanisms like Amazon s product review system, Google s sponsored search engine [8], mashups [9] and cloud computing [4].
A recent, prominent and quite controversial example of such new mechanism is crowdsourcing.
The term  crowdsourcing  was  rst used by Jeff Howe in a Wired magazine article [14] and the  rst de nition appeared in his blog: Simply de ned, crowdsourcing represents the act of a company or institution taking a function once performed by employees and outsourcing it to an unde- ned (and generally large) network of people in the form of an open call.
This can take the form of peer-production (when the job is performed collaboratively), but is also often undertaken by sole individuals.
The crucial prerequisite is the use of the open call format and the large network of potential laborers.
An important distinguishing feature of crowdsourcing, in addition to open call format and large network of contributors, is that it blurs boundaries between consumption and production creating a new proactive consumer type: the  working consumer  [16].
Whether these individuals are writing a blog or a product review, answering questions [21] or solving research problems [17], there is a signi cant economic value created by their actions; crowd-sourcing platforms, that understand and successfully leverage this economic value, thrive, while others vanish.
Yet little we know about incentives of  working consumers , and even less we know on how to shape them.
This paper presents an empirical analysis of determinants of individual performance in crowdsourcing contests using a unique dataset for the world s largest competitive software development portal: TopCoder.com.
TopCoder is a crowdsourcing portal with strong market orientation 1 : users compete to design and develop software, which is later sold for pro t by the sponsoring  rm; monetary prizes are awarded to contestants with winning solutions.
Not amazingly, we  nd that the prize amount is a strong determinant of the individual performance in the contest after controlling for the project complexity and the competition level.
A very important distinguishing feature of our empirical study is that we analyze effects of the reputation system currently used by TopCoder.com on strategic behavior of contestants.
In particular, we  nd that strong
 nity of interest.
The social networking side of the platform is beyond the scope of this paper.
tition in the contest phase, yet they strategically use cheap talk to deter competitor entry in the contest in the project choice phase.
Our contributions are as follows:
 online reputation of individuals in crowdsourcing contests have signi cant economic value and individuals may act strategically in order to extract pro ts from their online reputations.
cheap talk in multiple simultaneous contests to enhance their equilibrium payoffs and providing an estimate of the associated increase in user surplus.
vidual performance in a contest is related to individual and project speci c characteristics.
For instance, we show that the length of the requirements document has no signi cant effect on the contest outcome, once we control for the number of distinct requirements.
Although crowdsourcing attracted signi cant attention in popular press, there are only few empirical studies of incentives and behavior of individuals in crowdsourcing environments.
Yang et al. [21] examine behavior of users of the web-knowledge sharing market Taskn.com.
They  nd signi cant variation in the expertise and productivity of the participating users: a very small core of successful users contributes nearly 20% of the winning solutions on the site.
They also provide evidence of strategic behavior of the core participants, in particular, picking tasks with lesser expected level of competition.
Huberman et al. [15] use data set from YouTube to conclude that the crowdsourcing productivity exhibits a strong positive dependence on attention as measured by the number of downloads.
Brabham [3] performed online survey of the crowdsourcing community at iStockphoto.
The survey results indicate that the desire to make money, develop individual skills, and to have fun were the strongest motivators for participation.
We emphasize that crowdsourcing is, by de nition, related to a single  rm extracting economic value from the enterprise.
In that respect, crowdsourcing differs from open source (OSS) where the product produced is a public good.
While some driving forces behind OSS phenomenon may have economic nature such as signaling of skills to prospective employees, it is currently acknowledged that intrinsic motivation is the crucial factor [2].
Another example is knowledge sharing in community forums such as  Yahoo!
Answers": Wasko and Faraj [20] show that individuals perceiving that participation will enhance their reputation within the community, tend to contribute more.
Although it is not clear whether value of such reputation is intrinsic or the individuals expect that the reputation can be  cashed  in the future, scale of such contributions 2 de nitely favors the  rst explanation.
As the example of  Yahoo!
Answers" shows, making a clear distinction between crowdsourc-ing and other forms of peer production on the Web is often hard when the mechanism produces both a public good (publicly available knowledge forums) and private bene ts (advertising pro ts).
A very important distinguishing feature of our empirical study is that we analyze effects of the reputation system currently used by TopCoder.com on strategic behavior of contestants.
In particular,
 lion users and 65 million answers.
we  nd that strong contestants (as measured by their reputations) face tougher competition in the contest phase, yet they strategically use cheap talk to deter competitor entry in the contest in the project choice phase.
These results are novel to the Information Systems literature due to the fact that the prior research on electronic markets predominantly concentrated on studying reputation effects for  rms rather than individuals [5].
To the best of our knowledge, most empirical studies con rmed signi cant monetary value of online  rm reputation.
For instance, there is evidence that sellers with good reputation on eBay enjoy higher sales [18] and even price premiums [19].
In another example, sellers on Amazon.com with better record of online reputation can successfully charge higher prices than competing sellers of identical products [11].
In our study, the reputation premium is of different nature: contestants with higher reputations enjoy more freedom in the project choice phase and can successfully deter entry of their opponents in the same contest.
TopCoder.com is a website managed by the namesake company.
The company hosts weekly online algorithm competitions as well as weekly competitions in software design and software development.
The work in design and development produces useful software, which is licensed for pro t by TopCoder.
As of July 23, 2008
 those registered have participated in at least one Algorithm competition, 0.3% in Design, 0.7% in Development 3.
We are particularly interested in Design and Development competitions as they have tangible payments to competitors.
The business model underlying software Design and Development competitions is brie y summarized below.
4 TopCoder produces software applications for major clients.
It interacts directly with the client company to establish application requirements, time-lines, budget etc.
Once the application requirements are de ned, the application goes to the Architecture phase, where it is split into a set of components.
Each component is supposed to have a relatively small scope and precise set of technical requirements de n-ing the expected component behavior and interface for interacting with other components.
For instance, an  Address Book  component can be required to implement certain address management functionality, moreover, it should be written in Java and provide a Web service interface.
The set of requirements to each component is summarized in a single document (Requirements Speci cation) and posted on the website as a single Design competition.
Any registered member of the website satisfying minimum legal requirements can submit a UML 5 design to any posted design competition.
Winning design submission goes as input into the Development competition, which has similar structure, only the competitors are required to submit actual code implementing the provided UML design.
Output from Development competitions is assembled together into a single application, which is later delivered to the customer.
Design and Development competitions are posted on TopCoder website on a weekly basis, Figure 1 shows a sample list of weekly Development competitions.
Each competition belongs to a certain catalog.
Four largest catalogs are Java (generic components

 we omit some of the recent changes to the business process such as introduction of the Architecture and Speci cation contests.
Custom Java (custom components for Java platform), Custom .Net (custom components for .Net platform), but there are other catalogs such as catalogs for major clients (AOL can be seen in Figure 1).
Every competition has two associated deadlines: the registration deadline and the submission deadline.
The registration deadline speci es the time by which all individuals willing to participate must register for the competition, it is usually two or three days after the competition posting date.
The submission deadline speci- es the time by which all solutions must be submitted, it is usually within  ve to seven day interval after the competition posting date.
Every competition has associated payment that is given to the competition winner and 50% of this amount is given to the  rst runner-up.
The registration information is public, so that competitors can see who else has registered for the component.
This is achieved by clicking on items in the  Registrants Rated/Unrated  tab.
The result may look like shown in Figure 2.
Moreover, the information is updated instantly: as soon as one competitor has registered for the contest, others can see that.
Important component of the Design and Development process is its scoring and review system.
Once the submission deadline has passed, all submissions enter the review phase.
Each submission is graded by three reviewers according to a prespeci ed scorecard on dimensions varying from technical submission correctness and clarity of documentation to  exibility and extendability of the solution.
After the review process is complete, submissions enter the appeals phase where the competitors get a chance to appeal the decisions made by the reviewers.
Once all appeals have been resolved, the placement is determined by the average score across all three reviewers.
A sample of results is shown in Figure 3.
TopCoder implements policy of maximum observability 6.
At  rst, competitors can always observe identities of their opponents, i.e., other members registered for the same contest (see Figure 2).
Moreover, for every member TopCoder tracks all prior competition history and summarizes it in a single rating number 7.
The rating is provided for members who have submitted at least one solution in some contest.
It is calculated via a relatively complex formula taking into account all prior submission history of the contestant and relative performance compared to other contestants 8.
Fortunately, the exact formula for calculating the rating value is not important, as, in fact, even more information is available for each rated competitor, including all prior competition history.
This information can be revealed by clicking on the member s handle; the sample is shown in Figure 4.
Thus, we will simply think of ratings as proxies for the coder s performance so far: the better the coder performed in the past, the higher the rating will be, with more recent performance having higher effect on the current rating.
Finally, there is also the reliability score, which is provided for members who have been registered for at least 15 contests and is equal to the fraction of the last 15 contests they registered for in which they delivered a solution satisfying the minimum quality requirement (received a score of at least 75 in the review phase).
Members with the reliability rating equal to or exceeding certain
 see scores given by the reviewers to their opponents until after the appeals phase is over.
vidual participating in both Design and Development competitions will have two different ratings - one for Design and one for Development.
http://www.topcoder.com/wiki/display/tc/ Component+Development+Ratings threshold will receive a bonus for every prize they receive.
Due to space limitations, we will not give any more attention to the reliability score, but only mention that we take it into account when calculating the expected coder s payment.
Dataset We obtained historical contest data from the TopCoder website.
The dataset included 1,966 software Design contests and 1,722 software Development contests ran by TopCoder from 09/02/2003 to 08/23/2009.
The total number of different TopCoder members, who participated in at least one of the contests in our dataset, was
 sign competitions only, 1,106 individuals participated in Development competitions only, and 253 participated in at least one Design and at least one Development competition.
Descriptive statistics of the data are given in Table 1.
The  rst two rows of this table summarize the design and development rating distributions extracted from the competition data; note that these distributions are different from the static snapshot of the rating distributions for all TopCoder members as they weigh active competitors more.
The next two rows of Table 1 summarize the distribution of review scores extracted from the contest data.
Due to the nature of the review scorecard, one can never get the score higher than 100.0.
75.0 is the of cial reservation score - the minimum score required for submission to be accepted and the prize (if any) to be paid to the competitor.
Out of 5,113 design submissions and 7,602 development submissions in our dataset, 4,247 and 6,046 submissions respectively received at least the score of 75.0.
The  fth and the sixth row of Table 1 summarize the distribution of the number of submissions per contest and the next two rows summarize the distribution of the number of contests per active individual 9 as of the last date in our dataset (08/23/2009).
Additionally, by crawling the website, we obtained project requirements data for 1,742 software Design projects from our sample.
10 For every Design contest, we obtained length in pages of the project s requirement speci cation 11 as well as the number of distinct project requirements and the target compilation platform.
The two most popular target languages were Java (994 projects) 12 and C# (605 projects), together they account for more than 90% of all projects.
The rest of the projects (Ruby, PHP, other languages, and projects for which we failed to extract the target platform from the Requirements Speci cation) were classi ed as  other .
Finally, we collected payment data for projects in our sample.
Table 1 lists the  rst place prize amounts.
Note that TopCoder also awards second place prizes equal to exactly half of the  rst place prize for the corresponding project.
In this Section, we perform simple statistical analysis of the dataset to determine what factors in uence individual performance in a software contest once the registration phase of the contest is closed.
The usual suspects are individual speci c characteristics and skills, project speci c characteristics, experience with TopCoder competition platform, level of competition in a particular contest, current

 i cation document.
gether with the design documentation produced during the design contest, goes as input to the corresponding Development contest.
Figure 2: Sample list of registrants for a Development competition from TopCoder.
reputation of the individual.
We start by providing evidence that individual s rating is a signi cant predictor of future performance.
Table 1 shows signi cant variation in the distribution of  nal project scores.
For instance, for Design contests the median review score (88.6) is only slightly more than a standard deviation (10.1) away from the perfect review score (100), as well as from the minimum passing score (75.0).
To further understand the factors behind the dispersion of scores, we  rst grouped all competitors in  ve different groups on the basis of their Design rating just before the contest 13.
The group boundaries were chosen in accordance with the color group assignments by TopCoder:  grey  (rating 0-899),  green  (rating 900-1199),  blue  (rating 1200-1499),  yel- low  (rating 1500-2199),  red  (rating 2200+).
Within each group, we estimated the distribution of the review scores.
Density and cumulative density functions for each group are shown in Figure 5.
The plot suggests the  rst order stochastic dominance ranking of score distributions for different groups.
We formally tested this statement by Mann-Whitney-Wilcoxon stochastic dominance test on pairs of adjacent groups; in all 4 cases ( red  vs.  yellow ,  yellow  vs.  blue  etc.
), the test rejected the null (no stochastic dominance) hypothesis at 1% signi cance level.
Similar results were obtained for the Development contests.
There might be several alternative explanations of why members with high rating today are expected to deliver higher scores in the future:   Hypothesis Ia: Higher rated members are those with more inherent skills and abilities and therefore they deliver better solutions.
  Hypothesis Ib: Higher rated members are those who inherently care more about their rating and therefore consistently
 the same person may be classi ed to two different categories at different moments of time.
Figure 5: Distribution of Software Design Review Scores Clustered By Coder Rating put more effort into the competition to keep the status high.
  Hypothesis II: Higher rated members are those with more accrued experience and therefore they deliver better solutions.
  Hypothesis III: The rating is  addictive , members that achieved high rating today tend to contribute more in the future to keep their status high (this is similar to Huberman et al. [15] statement that users that get more attention on YouTube tend to contribute more in the future).
  Hypothesis IV: Higher rated members experience less competition in the project choice phase, therefore they can afford to choose easier, better paying or less competitive projects and deliver higher scores.
Figure 4: Sample Development competition history for a TopCoder member.
  Hypothesis V: Higher rated members expect  ercer competition from opponents and therefore have to deliver better solutions in order to win.
Note that these hypotheses are not mutually exclusive.
In order to test the hypotheses, we estimate a set of econometric models.
The  rst set of econometric models, analyzed in this paper, relates contestant s performance on a particular project to the set of observable contestant and project characteristics.
Results of the  rst set of models for Design contests are given in Table 2 14.
To ensure no  cold-start  effect for contestants who have not been rated yet or are not familiar enough with the TopCoder Software methodology, we dropped the  rst  ve contests for every contestant from our sample.
The  rst column of Table 2 presents a simple OLS model which speci es that scoreij = constant +  1paymentij +  2ratingi +  4 max rating i +  5experiencei +  6opponentsj +  7 max spec.
lengthj +  8num.
req.j +  9is Javaj +  10is C#j +  ij, where
 omit them due to space limitations.
1. scoreij is the ( nal) submission score for the contestant i in the contest j on a scale of 0 to 100.
2. paymentij is the  rst prize payment for the contest j mea- sured in $1,000 15.
3. ratingi is the contestant i rating immediately before the con test j.
4. max rating i is the maximum opponent rating for the con testant i in the contest j.
5. experiencei is the contestant i experience right before the contest measured in the number of previous contests for this contestant.
6. opponentsj is the number of opponents in the contest j.
 document for the contest j measured in pages.
8. num.
req.j is the number of distinct requirement items in the Requirements Speci cation document for the contest j.
the payment to take into account individual speci c bonuses, such as the reliability bonus.
design rating development rating design submission score development submission score number of submission per design contest number of submission per development contest number of design contests per member number of development contests per member requirements speci cation length number of requirements per design contest design contest 1st place payment development contest 1st place payment Min Max Mean Median















































 S.Dev











 Table 1: Descriptive Statistics
 the target platform.
target platform.
The second column of Table 2 (GMM 1) presents the GMM (Generalized Method of Moments) [12] estimation results for Equation 1 controlling for potential endogeneity of the contest payment (paymentj), as well as the coder s rating (ratingi) and reliability (reliabilityi).
Endogeneity of the contest payment comes from the fact that there might be unobservable project characteristics that affect both the contest complexity as well the contest payment set by TopCoder.com.
For instance, it is plausible to assume that the contest sponsor will set higher prizes for more complex contests, however such contests will also have lower average solution quality due to the complexity of the underlying project.
OLS estimates will capture both the direct effect of the contest payment on the contestants  performance and the projection of the unobserved complexity variable, thus underestimating the causal effect of the contest prize.
This is a classic endogeneity problem in economics.
In order to account for endogeneity of the contest payment, we instrument this variable with the average payment in the contemporaneous contests 16.
This is essentially the Hausman et al. [13] approach of using prices from different markets as instruments for prices in the given market.
Furthermore, contestant s rating might be correlated with unobservable individual traits which affect project choice and therefore, indirectly, contestant s performance.
To account for potential endogeneity of the rating, we instrument it with 3 lags of differences.
The assumption here is that, although the rating might be correlated with the individual speci c characteristics, short term  uctuations of the rating are not; this is conceptually similar to the Anderson and Hsiao [1] estimator for the dynamic panel data.
The third column of Table 2 (GMM 2) presents the GMM estimation result for Equation 1 controlling additionally for potential endogeneity of the maximum opponent rating (max rating i) and the number of opponents in the contest (opponentsj): both can be correlated with unobservable project characteristics affecting the individual performance.
Again, we use the Hausman et al. [13] approach and instrument the maximum opponent rating with the average of the maximum contestant rating in the contemporaneous contests and the number of contestants with the average of the number of contestants in the contemporaneous contests.
The fourth column of Table 2 extends the model by including the coder speci c  xed effects in the previous model.
Finally, for 16more precisely, average payment across the contests ran in the previous two weeks (note it excludes the instrumented contest) comparison purposes, the last column of Table 2 presents results of the  xed effects model without the rating variable.
Payment is a strongly signi cant determinant of the individual contestant performance in all  ve models.
From the  rst column of Table 2, we can see that the OLS signi cantly underestimates the effect of payment, as compared to the GMM 1 model, suggesting the project payment is in fact correlated with unobservable project characteristics, beyond the speci cation length, the number of requirements and the target platform.
Durbin-Wu-Hausman endogeneity test using the average contemporaneous project payment as an instrument con rms that the OLS model is inconsistent (p < 0.001).
Furthermore, endogeneity test for the maximum opponent rating also rejects the null hypothesis (p < 0.001), therefore in the rest of the Section we will concentrate on analyzing the results of the last three models (GMM 2, IV FE 1 and IV FE 2).
The last three columns of Table 2 suggest that the marginal effect of a $1,000 payment on the  nal quality of submission in a TopCoder Software Design contest lies somewhere between 6 to
 nant of the  nal project score, however the number of requirements is: there is approximately 1.2 point loss in quality for every 10 additional project requirements 17.
Java and C# contests seem to have lower average quality score than  other  contests, although this effect is barely statistically sig-ni cant.
We acknowledge that these two variables are also potentially endogenous as they depend on the project choice by the contestant.
While we did not instrument for the target platform endogeneity, we performed a robustness check by dropping these regressors from the model and verifying that coef cient estimates for the rest of the variables are not signi cantly affected.
Experience of the contestant is not a signi cant predictor of the contestant s performance 18, therefore we suggest that the Hypothesis II does not hold in our sample.
While the number of opponents is a barely signi cant predictor of the contestant performance, the maximum rating of the opponent is strongly statistically and economically signi cant: the marginal effect of an extra 1,000 points of the opponent s rating is somewhere between 6 and 8 project points.
Note that this is comparable with an effect of an extra $1,000 of the project payment.
In par-
iar with TopCoder Design contests, we should emphasize that we count every bullet point in the section 1.2 of the Requirements Speci cation document as a separate requirement.
payment (in $1,000) rating (in 1,000 pts) max opponent rating (in 1,000 pts) experience (num contests) number of opponents speci cation length number of requirements Java
 constant (1)

 (3.81)
 (22.27) -0.0435 (-0.10)
 (3.01)
 (2.92) -0.0651 (-0.96) -0.0247 (-1.78) -1.895  (-2.07) -1.083 (-1.17)
 (57.84)

 t statistics in parentheses   p < 0.05,   p < 0.01,   p < 0.001 ticular, this result means that, conditional on facing the same set of opponents, a higher rated contestant will face  ercer competition from the opponents.
We performed a robustness check of this result by considering two separate cases: the highest rated opponent is rated lower than the coder (i.e., the coder is the favorite) and the highest rated opponent is rated higher than the coder (i.e., the coder is the underdog).
In both cases, there was a (similar in magnitude) positive effect of the opponent s rating on the performance of the coder.
The news is not all bad for high rated coders: as the next Section shows, higher rated coders face less competition in the project choice phase.
We conclude this Section with discussion of the effect of rating on the contestant performance.
As Figure 5 and the OLS results suggest, there is a lot of variation in performance between contestants of different ratings.
The result persists even if we instrument all variables except for the rating properly, but it significantly decreases and becomes statistically insigni cant when one puts contestant speci c dummies in the model and instruments the contestant s rating by its short term  uctuations.
This suggest that much if not all effect of ratings is due to inherent contestant speci c traits (Hypothesis Ia and Ib) and we do not see empirical evidence for Hypothesis III that the rating is  addictive .
We acknowledge that our current dataset does not allow us to test Hypothesis Ia and Ib separately, i.e, although we know that some individuals consistently perform better than others due to some individual characteristics, we cannot conclude whether they are simply more skilled or care more about their performance than other contestants.
(2)

 (2.08)
 (2.26) -0.0241 (-0.04)
 (0.42)
 (3.22) -0.135 (-1.39) -0.0397  (-2.24) -0.285 (-0.27)
 (0.71)
 (24.60)
 (3)

 (3.52)
 (1.84)
 (3.10) -0.000881 (-0.20)
 (0.81) -0.0944 (-0.65) -0.0813 (-1.56) -1.611 (-1.39) -1.568 (-1.30)
 (16.36)
 (4)

 (2.35)
 (1.56)
 (2.42)
 (0.83)
 (2.05) -0.0722 (-0.49) -0.109  (-2.66) -2.612  (-2.11) -2.997  (-2.23)
 (13.46)
 (5)

 (2.82)
 (3.16) -0.00134 (-0.13)
 (1.84) -0.151 (-0.89) -0.122  (-2.56) -2.778  (-1.99) -3.517  (-2.31)
 (12.79)



 Empirical results of the previous section show that higher rated contestants, ceteris paribus, face tougher competition from their opponents in the competition phase of the contest.
We have hypothesized (Hypothesis IV) that this effect might be compensated by a competitive advantage in the project choice phase: TopCoder competition system allows contestants to register for projects in advance, thus letting strong contestants to signal their project choices in order to deter entry of their opponents.
In this Section, we formally test this statement.
Our  rst test is based on a simple observation that, in order to deter entry of their opponents in the contest they like, higher rated contestants should register early in the contest, while lower rated contestants will prefer to wait until the higher rated opponents made their choices.
If this is true, one should empirically observe a correlation between the contestant s rating and the probability of being the  rst registrant in a contest.
As Figure 6 shows, the correlation is present in our dataset.
The effect is also visible (although weaker) if one plots contestant s rating against the probability of NOT being the last registrant in the contest (due to space limitations, we omit this Figure).
Furthermore, we formally tested for presence of the  early registrant  effect by performing the Bernoulli (binomial probability) test of the  rst registrant being the highest rated coder in the contest conditional on the number of contestants.
If the coder registration process is independent of the ratings, then the null hypothesis of tal number of coders.
The second column of Table 4 adds project speci c dummies to the regression.
The next column additionally includes the maximum rating of the coder who registered so far.
Finally, the last column includes the coder speci c dummies (for the last registered coder) in addition to the project dummies.
The simple OLS model ( rst column of Table 4) shows strong positive correlation between the rating of the last registered coder and the rating of the coder who will register next for the same project.
We believe that this correlation picks up the fact that certain project types (like projects requiring speci c set of skills) attract higher rated coders while other project types attract lower rated coders, therefore, as long as one does not control for the project speci c effects, there is a positive correlation between ratings of the coders registered for the same project.
Second column of Table 4 shows that, as soon as project speci c dummies are included to the regression, the correlation turns negative.
The next column explains that it is the maximum rating of the coder who registered so far, not the rating of the last registrant, that in uences future registration process.
Adding coder dummies in the last column does not make a signi cant difference.
We emphasize that registration for a contest in TopCoder is not legally binding as the registered contestants may choose not to participate; however, registering and not participating has a negative effect of reducing the contestant s reliability score, thus (potentially) decreasing the amount of future winnings.
In our sample, only small fraction of contestants (about 20%) had reliability score high enough (at least 80%) to qualify for a project related bonus.
We performed a robustness check by eliminating these contestants from the sample and performing estimation only for coders who had no signi cant cost of registering but not participating.
Our results were quantitatively similar, thus indicating that the cost of signal (registration) had no signi cant effect on its value.
We conjecture that the registration phase in TopCoder contests works largely as cheap talk [10] (communication between players which does not directly affect the payoffs of the game) rather than costly signaling.
We conclude this Section, by providing a back-of-the-envelope estimate of the effect of early registration of a high rated coder on the expected surplus in the contest.
In order to do this, we need several other estimates:
 coder affects the maximum opponent rating in the contest.
We can infer this estimate from Table 4, which shows that each rating point of the highest rated registrant in the contest (so far) decreases the rating of all future registrants by approximately 0.5 rating point.
tition phase decreases when the maximum opponent rating decreases.
While Table 2 reports this value, we take a conservative estimate and assume that coders do not reduce the quality of their submission when the opponent ratings drop.
This assumption ensures that our estimate will provide a robust lower bound on the coder surplus whether or not the coder behaves strategically in the competition phase.
ning the  rst prize in a contest depends on the maximum rating of the opponent.
Note that we assume that the highest rated coder in the contest is guaranteed to win at least the second place prize; this assumption is consistent with our dataset: we have only 9 observations where a member with rating more than 1, 800 did not win the  rst or the second Figure 7: For three member contests, a fraction of all contests in which the registration order of contestants represents a certain permutation of the rating order.
1 represents the highest rated coder, 3 represents the least rated coder.
Coders are order by the registration time.
the  fair  coin ip model holds and, for contests with exactly N members, the fraction of contests, where the  rst registrant was the N .
In our sample, highest rated member, should be approximately 1 out of 153 two member contests, in 92 contests the  rst member was the highest rated; out of 130 three member contests, in 67 contests the  rst member was the highest rated; out of 79 four member contests, in 37 contests the  rst member was the highest rated.
19 In all three cases, the null hypothesis of the fair coin  ip is rejected (p < 0.001).
In fact, we observe even stronger effect in our data.
Again, consider all contests with exactly m participants.
Encode every participant by a number from 1 to m in order of their ratings, with 1 being the highest rated coder and m being the lowest rated coder (assume no ties).
Any registration sequence can be represented by a permutation of numbers from 1 to m. For instance, in a three-person contest, sequence 132 would represent that the highest rated coder was the  rst one to register and the lowest rated coder was the second one to register.
For any particular registration sequence, one can count how many times did it occur in the data.
Amazingly, not only the  rst registrant is most often the highest rated one but also the ordering of registration sequences by contest shares is similar to the lexicographic ordering.
In case of m = 3 shown in Figure 7, it is in fact identical to the lexicographic ordering.
Next, we formally test whether an early entry of a high rated coder has a positive deterrence effect on entrance of other high rated coders in the contest.
If the effect exists, then, controlling for the project complexity, there must be a negative correlation between the ratings of the coders who registered for the project so far and the rating of the coder who is going to register next (assuming that there is such).
Table 4 presents results of a series of regressions, starting from a simple OLS model in the  rst column: next coder ratingj = constant +  1last coder ratingj +  2 the number of codersj +  j, where the left hand side of the equation represents the rating of the coder who is going to register next and the right hand side rep-
sideration.
member rating (excluding the  rst  ve contests).
Y axis represents the sample probability estimate.
The plot includes only members that participated in at least 15 Software Design contests.
Table 3: Regressions of the next registrant rating for Software Design contests (4) (2) (3) the last registratnt rating (1)

 (2.73) FE project -0.195  (-3.71) FE project -0.0759 (-1.04) the number of registrants so far -0.00284 (-0.30) -0.0101 (-0.70) the maximum registrant rating so far
 t statistics in parentheses   p < 0.05,   p < 0.01,   p < 0.001


 (1.88) -0.476  (-2.95)
 FE project coder
 (1.04)
 (2.58) -0.614  (-3.11)
 prize in a contest where he was the highest rated participant.
In order to estimate the probability of winning the prize, we estimate a series of logistic regressions; results are shown in Table 5.
In columns 2 to 4 of Table 5, we additionally control for unobservable project speci c characteristics by including the project level random effects 20.
Random effects and additional regressors do not affect value of the coef cient on the maximum opponent rating variable signi cantly, therefore we use more conservative estimate from the simple logit model.
The corresponding value of the marginal effect is  0.54.
Putting it all together, for a suf ciently high rated member, every additional rating point decreases the rating of the toughest opponent by half a point and increases the probability of winning a prize by 0.5   0.001   0.54 = 0.027%.
For a $1, 000 contest, the difference between the  rst and the second prize is $500 and therefore the change in the expected winnings per single rating point is
 ni cant.
For instance, the difference between the smallest rating in the  red  (the highest) rating category and the smallest rating in the  yellow  (the second highest) rating category is 700 points, what translates to the difference of $94.5 per a $1, 000 contest.
Overall, our empirical results support Hypothesis IV that higher rated members face less competition in the project choice phase and behave strategically to exploit this competitive advantage.
problem.
Crowdsourcing is a new Web phenomenon, in which a  rm takes a function once performed in-house and outsources it to a crowd, usually in the form of an open contest.
Designing ef cient crowd-sourcing mechanisms is not possible without deep understanding of incentives and strategic choices of all participants.
This paper presents an empirical analysis of determinants of individual performance in multiple simultaneous crowdsourcing contests using a unique dataset for the world s largest competitive software development portal (TopCoder.com).
Special attention is given to studying the effects of the reputation system currently used by Top-Coder.com on behavior of contestants.
We  nd that individual spe-ci c traits together with the project payment and the number of project requirements are signi cant predictors of the  nal project quality.
Furthermore, we  nd signi cant evidence of strategic behavior of contestants.
High rated contestants face tougher competition from their opponents in the competition phase of the contest.
In order to soften the competition, they move  rst in the registration phase of the contest by signing up early for particular projects.
Although registration in TopCoder contests is nonbinding, it deters entry of opponents in the same contest; our lower bound estimate shows that this strategy generates signi cant surplus gain to high rated contestants.
This study has a number of limitations.
At  rst, while we  nd that better performance of higher rated coders should be attributed to some individual speci c traits, we currently cannot say whether such differentiation is purely skill based or some coders care more about their reputation within community than others.
Next, al-max opponent rating (in 1,000 pts) (1) logit -2.371  (-14.89) (2) -2.372  (-14.89) logit + RE project logit + RE project logit + RE project (3) -3.007  (-15.67)
 (15.33) (4) -2.840  (-14.50)
 (15.15) -0.138  (-3.59) rating (in 1,000 pts) number of opponents constant
 t statistics in parentheses   p < 0.05,   p < 0.01,   p < 0.001
 (12.91)

 (12.92)
 -0.0344 (-0.10)

 (0.36)
 though we  nd that strategic behavior of contestants in the registration phase increases the surplus of high rated coders, it is not clear what effect does it have on the sponsor s surplus and the overall social ef ciency of the mechanism.
We hypothesize that ability of skilled contestants to signal their intention to perform a particular project should result in more ef cient allocation of the overall pool of contestants to particular contests, thus improving the overall social surplus.
Indeed, imagine a situation in which the reputation system is not in place and contestants choose competitions in a setting of incomplete information.
Theoretical analysis of such  choice game  for simultaneous crowdsourcing contests is provided in a recent paper of DiPalantino and Vojnovic [7] and also, in a setting of simultaneous online auctions by sellers of different reputation, by Dellarocas [6].
Both papers identify a unique symmetric equilibrium in which
 the zone l covering a certain interval range [vl+1, vl) of skills (values).
(auctions) with the top k highest rewards (seller reputation).
