User intent understanding has become a hot topic in Web search and data mining, and commercial search engines have realized its importance for providing better search experiences.
For example, Google, Yahoo!, Bing1, and Ask all provide query suggestion [1, 3, 13] as a prediction of user search intent.
While such a technique has achieved certain success, it also has obvious limitations, some of which are listed as below.
  The prediction was conducted in a passive manner, in the sense that it was performed only after users submitted their queries to search engines.
Note that Web users spend most of their time not on search, but instead on browsing, authoring, and so on.
Therefore in most cases, when users have latent search intents, search engines cannot make meaningful predictions.
In this regard, the impact of passive intent prediction on Web users will not be su ciently signi cant.
  The prediction was usually based on historical queries issued by the user, or similar queries issued by other users.
Search engines are not aware how the information need of a user was originally generated and what motivated him/her to issue the query.
As a result, the contextual information that search engines can obtain will be insu cient to produce high-quality and personalized search results.
In order to overcome the aforementioned limitations, it is desired to predict users  search intents in an active manner, based on their behaviors even beyond search (e.g., browsing, authoring, etc.
), and leverage information related to these behaviors to improve search quality.
This is exactly the motivation of our work.
For this purpose,  rst of all, we should understand the factors that may trigger users  information needs.
According to our study, in a signi cant proportion of cases, information needs are generated when users browse the Web.
Our analysis (see Section 2) on user browsing behavior data shows that about 19.3% browsing sessions contain  browse   search  patterns (i.e., the user searched something right after he/she browsed a page).
In 66% of such sessions, some search queries were almost certainly triggered by the contents of the pages that users browsed before their search actions.
That is, when browsing a page, the user might  nd something interesting or unclear in the page, or be reminded
 to conduct a search immediately after browsing the page, to learn more about all of this.
For example, Linda is browsing the news of Michael Jackson s death and she wants to listen to some songs of Jackson to honor the memory of him.
So she comes up with the search query Music of Michael Jackson.
For ease of reference, we call this scenario SearchTrig-ger.
More accurately, SearchTrigger refers to a  browse   search  pattern, in which the search query is triggered by the content of the browsed page.
We call the query a SearchTrig-ger query, and the pattern a SearchTrigger pattern.
Our further analysis shows that the SearchTrigger queries are very diverse.
For 92% pages, the corresponding queries fall into at least two dissimilar topical clusters.
A real example of SearchTrigger that we found during the study is given as follows.
Example 1.
(SearchTrigger.)
A user is a fan of Shakespeare.
He/she opened a webpage about Shakespeare s FAQ (http://absoluteshakespeare.com/trivia/faq/faq.htm).
There are tens of FAQs in the page about Shakespeare.
For instance, there is a question  Is it true nobody knows Shakespeare s birthday?
  at the top of the page.
The answer is:  It is true we don t know Shakespeare s date of birth.
We know it was in 1564 but our only record at this time was of his baptism at the Holy Trinity Church on April the 26th.
By convention and some guesswork, Shakespeare s birthday is by tradition celebrated three days earlier on April the 23rd.  After reading the notes, the user felt interested in the birthday of Shakespeare.
To satisfy his/her curiosity, the user raised a query  when was Shakespeare born  to a search engine for more answers.
Note that the SearchTrigger queries are those queries triggered by the browsed page, but usually NOT the key phrases of the page or the queries that have the page as its top search result.
For instance, in the above example,  when was Shakespeare born  does not appear in the page of Shakespeare s FAQ, and therefore is not one of the key phrases.
Also if we search  when was Shakespeare born  using major search engines, the page of Shakespeare s FAQ even does not appear in the top-100 results.
This is actually reasonable.
If the query is a key phrase of the page or the page is in the top search results for the query, it is very likely that the page has already contained the answer to the query and it is unnecessary for the user to issue the query to search engines.
The query is submitted usually because the user wants to obtain some novel information that is not covered by the triggering page.
If we can predict the latent search intent in the SearchTrig-ger scenario, and suggest meaningful queries to users when they are browsing, we will be able to help both search engines and their users.
On one hand, by providing search shortcuts corresponding to these suggested queries, we actually extend the search function to outside the search box, and provide more opportunities for users to use search engines.
On the other hand, by using the historical browsing behaviors (including the content of the pages browsed by the user) as contextual information, we can improve search accuracy for these suggested queries and provide much better user experiences.
This task is, however, nontrivial due to the following reasons: (i) not all  browse   search  patterns correspond to real SearchTrigger ; (ii) the search intents of users can be very diverse even if they read the same page, according to the statistics given in Section 2.
To tackle the challenging task, in this work, we propose a series of technologies.
searched right after reading the page from user browsing behavior data.
cording to their likelihoods of being SearchTrigger queries.
ranked list of queries obtained in the second step, and present the diversi ed ranked list of queries to users.
We have tested our proposed approach on large-scale user browsing behavior data, and develop a contextual retrieval algorithm to leverage the page that triggers a query to improve search accuracy for the query.
The experimental results have shown that the approach can improve user experience and enhance search accuracy.
To sum up, the contributions of this work are as below.
  We have proposed the concept of active prediction of users  search intents, which extends the functionality of search engines beyond their original boundaries.
  By mining user browsing behavior data, we have discovered a special pattern called SearchTrigger, in which the search intent is triggered by the page that a user visits right before his/her search action.
  We have found that SearchTrigger queries for the same page are usually very diverse.
Accordingly, we have proposed a method to suggest a diversi ed query list for a given page and demonstrated its e ectiveness through a contextual retrieval algorithm.
The rest of the paper is organized as follows.
In Section 2, we present the study on user browsing behavior data, and show that SearchTrigger is a popular pattern in such data.
The algorithms to e ectively predict SearchTrigger queries for a given page are introduced in Section 3.
Experimental results are discussed in Section 4.
Conclusions and future work are presented in the last section.
In order to better understand how what users browsed triggered their search intents, we have conducted an extensive study on user browsing behavior data, as reported in this section.
The primary source of data for this study was the anonymous logs of URLs visited by users who opted in to provide data through a widely-distributed toolbar on Internet browsers.
Each log entry is a tuple of {user ID, timestamp, URL}, meaning a user visited a URL at some time.
User ID was encrypted by an irreversible hash function.
Intranet and secure (e.g., https) URL visits were all removed from the source.
To minimize the in uence caused by linguistic and regional variations, we only kept the records generated in the United States.
As a result, the data consist of about 3 billion anonymous page views in 32 successive days during May and June in 2007.
We extracted  browse   search  patterns from all sessions in the user browsing behavior data.
Here we de ne a session as a logical unit of time-ordered user browsing activities.
For each user s data, we start a new session if there is more than 30 minutes of inactivity between the current page view event and its immediate preceding event [17].
For the page view events and sessions, we have the following de nitions.
De nition 1.
Search Portal Event / Search Event / Browse Event.
If a page view event contains the URL of a (general or vertical) search engine portal2 but does not contain any search query, we call the event a search portal event; if the page view event contains the URL of a search engine and a query as its argument, we call the event a search event; otherwise, we call the event a browse event.
For example, in the following session, the  rst URL corresponds to a browse event, the second a search portal event, and the third a search event.
Example 2.
(Three successive URLs in a session.)
http://www.apple.com/iphone/ http://www.google.com/ http://www.google.com/search?q=iphone+release+date De nition 2.
Search Session / Non-search Session.
If a session contains at least one search event, it is called a search session.
Otherwise, it is a non-search session.
De nition 3.
 Browse   Search  Pattern.
After excluding all search portal events from a search session, if there is a search event immediately after a browse event, we call the tuple {URL, query} a  browse   search  pattern where URL is the page visited in the browse event and query is extracted from the search event.
For instance, from Example 2, one can extract the following  browse   search  pattern, {http://www.apple.com/ iphone/, iphone release date}.
Note that there is only one case that a search session does not contain any  browse   search  pattern: all search events happened prior to the browse events in the session.
In this case, this is no evidence that the search events are triggered by the content of previously visited pages.
Therefore, they are not in the interested scope of our study.
De nition 4.
 Browse   Search  Session.
If a search session contains at least one  browse   search  pattern, it is called a  browse   search  session.
We processed the entire user browsing behavior data in our study, and obtained the statistics as shown in Table
 are  browse   search  sessions and the average number of  browse   search  patterns per such session is 5.6.
As will be seen in the next subsection, these  browse   search  patterns potentially correspond to SearchTrigger, but not necessarily all quali ed SearchTrigger.
from Google, Yahoo!, Windows Live Search, AOL, and Ask, because the majority of the search market share in the United States (around 90% according to [24, 25, 26]) comes from these  ve search engines.
Table 1: Statistics on sessions and  browse   search  patterns.
Entity Log entries Unique URLs Sessions Non-search sessions Search sessions  Browse   search  sessions  Browse   search  patterns Average entries per session Percentage of non-search sessions Percentage of search sessions Percentage of  browse   search  sessions Average  browse   search  patterns per  browse   search  session Quantity












 We randomly sampled 200  browse   search  sessions and asked experienced human analysts to perform further analysis on the data.
As a result, we found 849  browse   search  patterns, the queries in which can be classi ed into seven categories.
The query is a key phrase of the browsed page.
For example, after visiting a page about Shakespeare s FAQ (http: //absoluteshakespeare.com/trivia/faq/faq.htm), a user issued the query Shakespeare s play, which is a key phrase in the page due to its high relevance with the main topic of the page and its high frequency.
The query describes some interesting part of the browsed page, but it is not a key phrase.
The query given in Example
 page such as images and  ashes may also trigger queries belonging to this category.
We sent the queries in this category to major search engines, and found that their corresponding top-10 search results do not contain the browsed pages.
Many users have the experiences that they wanted to visit a famous website such as facebook, youtube, myspaces, but they forgot its exact URL.
They then issued a query like facebook to a search engine to get a shortcut to the portal of the website.
Such queries are usually not triggered by the page that a user previously browsed.
Sometimes, a user searched a query absolutely unrelated to the content of the previous page.
There might be various reasons for this situation.
For example, the user changed his/her interest to another totally di erent topic, or it is some information need from his/her daily life that triggered the query (e.g., the user felt hungry and searched for the phone number of a restaurant).
Sometimes a user  rst submitted a query to a search engine, and browsed a page in the search results.
How-Percentage Number of Queries Category Average Pattern Frequency SearchTrigger Non-SearchTrigger Cannot judge Key phrase of the page Information in the page but not key phrases Famous site Unrelated topic Repeated search Query re nement Cannot judge
























 ever, he/she was not satis ed with the page.
Then he/she clicked the back button in the browser to try another page in the search results, or changed to another search engine and searched the query again.
As a result, we can extract a sequence of events like query   URL1   query   URL2.
Here the second query is not triggered by the content of URL1 since it is simply a repeated search.
Sometimes a user  rst submitted a query to a search engine, and browsed a page in the search results.
However, he/she was not satis ed with the page and he/she re ned the query and resubmitted it to the search engine.
As a result, we can obverse the following sequence of events, query 1   URL1   query 2   URL2.
In this case, query 2 is not triggered by the content of URL1 either.
It was di cult for the analysts to categorize all the browse   search  patterns accurately.
In some cases, the analysts could not make sure which category a pattern should belong to.
In some other cases, the page in a pattern has been expired or needs login (e.g., forums) to view its content.
We regard all these cases as cannot judge.
The statistics of the above categories are summarized in Table 2.
In addition to the number of queries and percentage in the 849 patterns, we also count the frequency of a  browse   search  pattern in the entire user browsing behavior data (see the last column in Table 2), which can re ect whether a particular pattern is popular or not.
According to the de nition of SearchTrigger given in the introduction, the human analysts thought that both the  rst and the second categories in the study correspond to SearchTrigger, categories 3 to 6 are not SearchTrigger, and category 7 is not judgeable.
From this result, we can come to the following conclusions.
  About a quarter (23.8%) of  browse   search  patterns contain SearchTrigger queries.
Further analysis shows that 132 of the sampled 200 sessions (66%) contain at least one SearchTrigger query.
All these numbers show that SearchTrigger is a frequent pattern of user behaviors and it is worthy of further investigation.
  Among the SearchTrigger queries, the proportion of queries belonging to category 2 is signi cantly larger than that belonging to category 1.
This coincides with our discussions in the introduction: most SearchTrig-ger queries (in our study, 88.6%) are not key phrases of the browsed page.
Table 3: Statistics on diverse SearchTrigger queries.
Number of Search Intents Number of Pages   1

   10



   According to the average pattern frequency in each category, we can see that not all high-frequency patterns correspond to SearchTrigger (e.g., famous site and repeated search also have very high frequencies).
We randomly sampled 100 pages from the user browsing behavior data.
Then we collected the queries in all the  browse   search  patterns containing the page.
We asked human analysts to judge whether these queries are SearchTrigger queries.
For the queries that are judged as SearchTrigger queries, human analysts further grouped them into several clusters according to their corresponding search intents.
For example, after reading a page about rabbits (http://exoticpets.about.com/od/rabbits/Rabbits.htm), users issued 11 di erent queries, 7 of which were identi ed as SearchTrigger queries.
These queries were organized into three groups, i.e., {rabbits, pet rabbit, wild rabbits}, {rabbits pictures, pet pictures}, and {rabbit care guide, rabbits breeds}, indicating three di erent search intents.
The statistics of this study are summarized in Table 3.
From the table we can see that about 92% pages triggered at least two different search intents, showing that SearchTrigger queries are often diverse.
Our explanation to this observation is that a page may contain several di erent (but correlated) topics and each of them can motivate users to search something.
The statistics in Section 2 show that SearchTrigger is a frequent pattern of user behaviors.
Then if search engines can accurately predict users  intents ahead of time and suggest SearchTrigger queries while users are browsing a page, they can provide timely search function when users need it, and thus greatly improve user experiences.
This task is, however, nontrivial, as discussed in the introduction.
To tackle the challenges, we propose a set of techniques.
First, we extract the  browse   search  patterns from user browsing behavior data, and build a candidate query set for each page.
Then, given a page and its candidate queries, we extract various features and learn a ranking model to sort these queries according to their likelihoods of being SearchTrigger diverse the ranked list of the SearchTrigger queries.
This di-versi ed query list will be presented to users as suggestions.
Given a page, the task is to predict a ranked list of SearchTrig-ger queries that a random user may want to issue after reading the page, based on historical user browsing behavior data.
To this end, one can segment user browsing behavior data into sessions, and extract all  browse   search  patterns.
For each page p, a candidate query set can be generated by aggregating all its co-occurrence queries in the patterns.
Suppose the candidate query set is Sp = {q m }.
Then a straightforward solution to the task is to count the frequency of each query in Sp, and suggest the most frequently-asked queries to users.
However, this naive method would not work well, because 71.1% candidate queries are non-SearchTrigger, and many of them are of high-frequencies, according to the analysis in Section 2.
2 ,  , q (p) 1 , q (p) (p) To solve the aforementioned problem, we propose extracting multiple features for each candidate query and adopting machine learning technologies to rank these candidate queries according to their likelihoods of being SearchTrigger queries based on the features.
On one hand, the task of query ranking can be regarded as a dual problem of document ranking in search.
Therefore, it is straightforward to also extract query-document matching features for the task, which are widely used in the literature of document ranking.
For example, we extract the following features to describe the matching between a query and its preceding page: term frequency (TF)[2], inverse document frequency (IDF)[2], TF *IDF[2], LMIR with ABS smoothing (LMIR.ABS)[20], LMIR with DIR smoothing (LMIR.DIR)[20], and LMIR with JM smoothing (LMIR.
JM)[20].
If we consider that each page contains three parts, i.e., url, title, and body, we will have 18 query-document matching features in total.
In addition, we also extract the length of query, unique word count of following features: query, and maximum word length of query.
In many cases, we had better not suggest long queries (or query words) to users, because most of them are rarely asked by real users.
These features can help avoid such cases in the suggested queries.
On the other hand, however, there is also di erence between the task of query ranking and that of document ranking.
In the former case, each candidate query is represented by features while in the latter case each candidate document is represented by a set of features.
This di erence actually poses a challenge to us: queries are usually much shorter than documents, which makes the above content matching features not informative enough to describe queries.
For example, only from the query word, it is di cult to judge whether a query re ects the interesting part of a page that can attract users  attention.
Such information, which is important for identifying SearchTrigger queries, need to be extracted from other information sources, e.g., the bipartite graph as described below.
In our work, we extract all  browse   search  patterns from user browsing behavior data, and build a page-query bipartite graph.
In this bipartite graph, a page node is cre-Figure 1: An example of page-query bipartite graph.
ated for each unique page, while a query node is created for each unique query in the patterns.
An edge eij is generated between page pi and query qj if they co-occur in a  browse   search  pattern.
The weight wij of edge eij is the frequency of such patterns.
An example page-query bipartite graph is shown in Figure 1.
From the bipartite graph, we extract the following features, in hope to describe users  interests:   Query Visibility.
We call the number of edges linking to a query query visibility.
If a query has large query visibility, it means that users would ask the query after visiting many di erent pages.
  Query Popularity.
We call the sum of weights of all the edges linking to a query query popularity.
If a query has large query popularity, its total number of occurrences in the extracted patterns is large.
  Pattern Frequency.
We call the weight of the edge between a query and the given page pattern frequency.
This feature re ects whether the same query is issued by many di erent people after reading the page.
(p) Previous work [4, 9, 12] has shown the advantage of using a learning to rank approach over using heuristic rules, especially when there are multiple evidences of ranking to be considered.
Given the query features as described in Section 3.2, we also adopt a learning to rank technique to rank the candidate queries.
Given page p and its candidate query set Sp = {q m }, where m is the number of queries.
Let X   Rd be the q feature space of queries, where d is the number of features.
(p) Then x i with respect to p, i = 1, 2,    , m. Suppose Y = {l1, l2,  , lK} is the set of labels representing the likelihood that a query is a SearchTrigger query for the document.
Assume that there is a total order between the labels, i.e., l1 > l2 >   > lK.
In our study, K is set to 3, and l1, l2, and l3 represent the labels of SearchTrigger, cannot judge, and non-SearchTrigger respectively.
i   X denotes the feature vector of q (p) 1 , q (p) (p)
 In the training process, there is a set of n pages, their candidate queries, and the corresponding labels (given by human annotators), in which zp = {(x m )}.
Here (p) (p) 1 , y 2 , y i   Y is its label.
i   X is the feature vector of q x If we have y i.e., Z = {z1, z2,  , zp,    , zn}, (p) 1 ), (x 2 ),  , (x and y (p) , then we can say q i (p) i > y (p) m , y (p) i (p) j (p) (p) (p) (p) (p) higher than q j , denoted as the partial order q should be ranked (p) j i (cid:9) q (p) .
(p) (p) i ) > f (x i (cid:9) q j   f (x Suppose F is the set of ranking functions, then each instance of it f   F can rank the pair q (p) j ).
The training process aims to  nd the optimal f that can  t as many pairs of partial orders in the training set as possible.
Any pairwise3 learning to rank algorithms, such as Ranking SVM[12], RankBoost[9], and RankNet[4], can be adopted to learn the ranking function f , in the above setting.
For example, when using Ranking SVM, we assume f to be a linear combination of features f (x) =  T x (where   is the parameter vector representing the weights of the features), and use the following optimization problem to learn the parameter  , (cid:11) (cid:11)2

 min  , 
 (cid:2) (cid:2) (p) ij   p x (p) i (p) j (cid:2)x ij , x (p)
 (p) (p) (x i   x j ) > 1     ij   0.
(1) s.t.
  Here (cid:11)   (cid:11) is the L2 norm,   is a set of slack variables, and C is a trade-o  coe cient.
If the solution to (1) is  , then the ranking function can be written as i (cid:9) x (p) j ,   (p) (p) f (x) =  
 x.
(2) This ranking function will be used to rank the candidate queries for a new page in the user browsing behavior data.
Then the top-ranked queries can be presented to users as SearchTrigger suggestions.
As shown in Section 2.3, SearchTrigger queries can be very diverse even if they are triggered by the same page.
In order to minimize the dissatisfaction of a random user after seeing the suggested SearchTrigger queries, one needs to diversify the queries before presenting them to users4.
p   Sp), (cid:3) which contains SearchTrigger queries that are diverse in their topics.
We formulate this task as a set selection problem inspired by [10].5 In particular, we de ne an objective g( ), as a function of ranking model f ( ) (e.g., learned in the previous subsection) and a query dissimilarity measure  ( ,  ).
The goal is to select a set of queries, S p   Sp, such (cid:3) that the objective function g( ) can be maximized, i.e., To this end, our task is to select a subset S (cid:3) p (S p, f ( ),  ( , )).
(cid:3) (3) (cid:3) where m (cid:3)
 p = arg
 max p Sp,|S(cid:2) S(cid:2) p|=m(cid:2) g(S (cid:3) p.
p| is the size of S (cid:3) (cid:2) A simple yet reasonable objective function is given as follows (  > 0 is a trade-o  coe cient), (cid:2) p, f ( ),  ( ,  )) = (cid:3) g(S  (i, j) +   f (i).
(4) (p) q i (p) j  S(cid:2) p ,q (p) i  S(cid:2) p q It is clear that the maximization of this objective function will guarantee that the queries selected will have a large ranking score (since the sum of the ranking scores has been maximized), and each two queries will be di erent (since the average pairwise dissimilarity has been maximized).
choose pointwise [8] and listwise [5] learning to rank algorithms.
diversi cation.
like those discussed in [7, 15].
To solve the above optimization problem, one needs to address two technical challenges.
First, since queries are usually very short, it is nontrivial to de ne an e ective query dissimilarity measure.
Second, the problem is a typical NP-hard problem and thus the e cient optimization of it is nontrivial.
We will present our solutions to these two challenges in the following subsections.
To compute e ective query dissimilarity measure, we propose using the page-query bipartite graph built in Section 3.2, since it contains rich information of query relationships.
However, this graph is not fully reliable since many edges in the bipartite graph do not correspond to SearchTrigger patterns.
This might not be a big issue for ranking model learning since it is a supervised process and we can leverage other features to avoid the negative in uence of this graph.
However, it may become a problem when we use the graph for query diversi cation, since this is an unsupervised optimization process.
If the graph is unreliable, the optimization results will accordingly become unreliable.
To tackle the problem, we clean the graph before using it to compute query dissimilarity.
For each page in the graph, we extract features for its co-occurrence queries and compute the ranking scores of these queries using the model learned in Section 3.3.
After that, we normalize the scores to interval [0, 1] and use the normalized scores to re-weight the corresponding edges in the bipartite graph.
That is, for edge eij with original weight wij , if the normalized ranking score of query qj with regards to page pi is vij , we will change the edge weight of eij to wij vij .
In this way, the bipartite graph is cleaned because the weights of the SearchTrigger patterns are enlarged and those non-SearchTrigger patterns are reduced.
We then calculate query dissimilarity by Jensen-Shannon divergence (JSD) [23] on the cleaned graph.6 The basic idea is that if two queries share many pages with high weights, they should be similar to each other; otherwise dissimilar.
According to the bipartite graph, we can represent each (p) as a vector  i.
Each dimension of the vector query q i  ij = wijvij , where j is the index of a page in the graph.
(p) (p) Given two queries q k , their dissimilarity in terms i of JSD is calculated as below, and q  (i, k) = (D( i(cid:11) ) + D( k(cid:11) ))/2, (5) where D( (cid:11) ) is the Kullback-Leibler divergence [22] and   = ( i +  k)/2.
(cid:3) (p)  i = Let   be an indicator vector de ned as below, (i = 1, 2,  , m).
(cid:2) Then the objective function g( ) can be written as, p, f ( ),  ( , )) = (cid:3) i   S
 (cid:2) 1, q (p) 0, q i  (i, j) i j +   (cid:3) p (cid:3) p g(S (6) f (i) i.
(p) q i (p) j  Sp ,q (p) i  Sp q
 such as those based on cosine similarity and Pearson correlation [21].
Here the use of JSD is just an example.
(7) matrix, then we can get the following equivalent form of the original optimization problem, max


  
   +  f   ;   = { i},  i = 0, 1; ; f = {fi}.
  = m e = (1, 1,    , 1)
 (cid:3) s.t.
e (8) The above optimization problem is a typical 0-1 integer programming problem, which is NP-hard.
We propose relaxing the values of   to be continuous (i.e.,  i   [0, 1]), and converting (8) to the following quadratic optimization problem.
Note that the same trick has been widely used in semi-supervised learning and spectral clustering [6][16].
min
  

 L     f   ;   = { i}, 0 (cid:2)  i (cid:2) 1; (cid:3)
 s.t.
e   = m e = (1, 1,  , 1)   1   1
 ; f = {fi}.
(9) Here L = I D


 and D is a diagonal matrix with its diagonal elements equal to the sum of all the elements in the corresponding rows of  .
Suppose the solution to the above optimization problem is   .
Then we can select the queries corresponding to the largest m to form the suggested query set.
Actually, the above optimization problem has the following properties.
elements in       (cid:3)   It is not di cult to verify that L is a positive semi-de nite matrix and thus (9) is a convex optimization problem.
As a result,   is the global optimal solution to (9).
In contrast, in some previous work like [10], a greedy method was used to solve similar set selection problem, which is not guaranteed to result in an optimal solution.
    The problem can be solved in a time complexity of O(m3).
For each page, the number of candidate queries is usually not very large (e.g., less than 100).
Therefore, the computational complexity turns out to be affordable.
Note that when users go to a search engine with the suggested SearchTrigger queries, the page that they previously browsed can serve as an informative context for the search engine.
There is a rich literature of contextual information retrieval, which basically leverages various contextual information to improve search quality [18].
Many ideas in the previous work can be used directly or indirectly.
In Section 4.3, we tested a simple contextual retrieval algorithm and the experimental results clearly demonstrated the bene t of using the aforementioned contextual information to answer SearchTrigger queries.
In this section, we presented our experimental study on the proposed approach.
We used the user browsing behavior data as mentioned in Section 2 for our experiments.
After partitioning the data to






 i s n o s s e s f o r e b m u







 Number of pages in a session Figure 2: The distribution of page numbers in the sessions.
i s n o s s e s f o r e b m u



















 Number of "browse search" patterns in a session Figure 3: The distribution of the numbers of  browse   search  patterns in the sessions.
sessions, we extracted 167,570,019  browse   search  patterns from them.
The distribution of page numbers and the numbers of  browse   search  patterns in the sessions are shown in Figure 2 and Figure 3.
We  ltered out the patterns whose queries contain non-alphanumeric terms like Chinese or Arabic words.
Then we removed those pages whose number of occurrences in the data is less than 5, for most of them correspond to rare patterns.
After the cleaning, we obtained 56,929,950 patterns left and built a page-query bipartite graph from these patterns.
The graph contains
 degree distribution of queries in the graph is shown in Figure
 and compute query dissimilarities.
gestion We compared our proposed approach with some baseline methods, and investigated the bene t of diversifying suggested queries.
All the methods under comparison are listed as below.
Key Phrase Extraction (KPE).
KPE is a technique to extract important keywords or phrases from a given text document [11, 14, 19].
We use the method described in [19] to






 s e d o n y r e u q f o r e b m u








 Degree of query node Figure 4: The degree distribution of the queries in the page-query bipartite graph.
extract key phrases in a page and regard them as triggered queries.
Pattern Frequency (PF).
After extracting all  browse   search  patterns from user browsing behavior data, a straightforward solution is to count the frequencies of all queries that co-occur with a page, and suggest the most frequently-asked queries to users.
Query Ranking based on Query Features (QRQF).
This method uses the features introduced in Section 3.2, employs Ranking SVM to combine them for query ranking.
No di-versi cation is introduced, and the ranking result given by Ranking SVM is directly suggested to users.
The trade-o  coe cient C is empirically set to 5.
SearchTrigger Queries Diversi cation (SQD).
Based on the ranking result given by QRQF, the diversi cation method described in Section 3.4 is used to obtain a re ned query set.
The trade-o  coe cients C and   are empirically set to 5 and 2.
This algorithm is exactly our proposed approach.
As mentioned in Section 2, we have two labeled datasets.
The  rst dataset contains the labels of 849  browse   search  patterns, and the second one contains the labels of all the queries with regards to 100 pages.
We used the  rst dataset to train the models of QRQF and SQD, and then used the second one to test the performance of all the algorithms.
Table 4 shows some examples of the suggested query sets produced by di erent algorithms.
Pages No.
1, 2, and 3 correspond to http://movies.about.com/od/currentfilms/, http://nationalpriorities.org/index.php?option=com_ wrapper&Itemid=182, and http://pds.jpl.nasa.gov/planets/ welcome.htm, respectively.
Due to space restrictions, for each page, only the top-5 ranked queries are shown.
From the examples, we have the following observations:
 di erent from the queries issued by users.
Some key phrases like planetary exploration can be well understood by viewing the page content and thus users may not want to learn more about them; some key phrases like Hollywood are well-known words and users seldom issue them as search queries; some extracted key phrases like links within this site seem to be neither a good summary of the page nor a possible triggered query.
@ n o s c e r
 i i





 (1) Average precisions @5



 @ s t n e t n i h c r a e s f o r e b m u
 (2) Average search intent hitting numbers @5







 Figure 5: Average precisions and average search intent hitting numbers for di erent algorithms.
ample, rick maze was suggested for page No.2.
This query is the name of an editor in the magazine of Army Times.
We searched within the whole site of page No.2 and did not  nd any convincing evidence to support that this query was triggered by the page.
By further investigation on the browsing behavior data, we found the query corresponds to typical repeated search in several sessions.
than those suggested by KPE and PF.
However, many of the queries suggested by QRQF are very similar to each other, e.g., recent movies, recent movie releases, and new movies releases.
By using SQD, we obtained even better results which are both reasonable and diverse.
For example, The results produced by SQD for the planet page No.3 is a good example to demonstrate this.
To make a statistical comparison among these algorithms, for each page, we computed the precision [2] of the query set produced by each algorithm according to the ground-truth set labeled by human annotators; we also counted the number of search intents that the query set hit.
After that, we computed the average precision and average hitting number for each algorithm.
The results are shown in Figure 5, where the two measures are computed with respect to the suggested sets of the top 5 queries.
We can see that QRQF and SQD correspond to the largest average preci-sions and average search intent hitting numbers, which are signi cantly better than the other two algorithms.
Compared with QRQF, SQD performed signi cantly better in hitting more search intents, with only a small loss of precision.
Therefore, we say that SQD is able to satisfy more users  information needs.
To better understand the bene t of diversifying the sug-Instead gested query set, let us have a look at Figure 6.
of presenting the average results as in Figure 5, in Figure 6 we plot the distribution of pages with regards to di erent precisions and hitting numbers.
In particular, each page pi is represented by a two-dimensional vector ( i,  i), where  i is the precision and  i is the hitting number of the top-5 queries produced by an algorithm.
In each sub gure, X-axis corresponds to  i, Y-axis corresponds to  i, and Z-axis corresponds to the frequency of ( i,  i) (denoted as  ( i, i)).
The curved surface is  tted upon the tuples ( i,  i,  ( i, i)).
From the sub gures, we can clearly see the advantage of SQD over QRQF: its peak lies in the area with larger hitting number than that of QRQF, while their precisions are similar to each other.
No.
Cost of War Obama Afghanistan Billion iraq war rick maze positives of iraq war iraq war cost war in iraq deaths iraq war iraq war cost information on the war in iraq cost of iraq war war in iraq cost cost of iraq war chart iraq war coalition of the willing members war in iraq deaths current iraq war debt
 planetary planetary exploration planetary exploration program Welcome to the Planets links within this site planets pictures of the planets pictures of space shuttle photos of planets saturn photos planets 9 planets photos of planets planets solar system pictures of the planets planets nasa kids pictures saturn photos pictures of space shuttle ufos




 Movies Movie News Hollywood Movies Hollywood New on Video recent movies new movies releases new movies new movies released sports movies recent movies movies recent movie releases dancing movies new movies releases new movies releases recent movies dancing movies sports movies movies of 2006
 s e g a p l e p m a s f o r e b m u








 Number of SearchTrigger queries






 Number of search intents
 s e g a p l e p m a s f o r e b m u








 Number of SearchTrigger queries






 Number of search intents Figure 6: The performance comparison of QRQF and SQD.
To verify whether it is bene cial to use the triggering pages as contextual information for search, we tested a simple contextual retrieval algorithm in this subsection.
The basic idea is to extract some additional features to represent the content similarity between the triggering page and the documents to rank, and use these features to rerank the original search results.
Speci cally, suppose a user selects a suggested SearchTrigger query q for page p, then both q and p will be sent to the search engine.7 Given query q, the search engine can retrieve top-k relevant documents, i.e., D = {t1, t2,  , tk}, using its default retrieval function.
After that, the content similarities between page p and these documents are calculated.
According to the similarities, the similar documents to p will be promoted in the search result, as compared to those equally relevant but dissimilar documents.
This heuristic is designed by considering that query q is triggered by page p and thus the user might be willing to see documents sharing similar content or topic with the triggering page p. The re-ranking algorithm is described in Table 5, where sim( ) is the cosine similarity function and   is a parameter to set the weight of contextual information (in our experiments, we empirically set   = 0.4).
To test the above algorithm, we designed the following experiment.
We used the model learned by SQD to test unlabeled pages in the cleaned bipartite graph.
If a  browse   search  pattern is predicted as SearchTrigger and the user did click a URL in the search result given by a search engine SE for the query (which can be observed in user browsing behavior data), we will regard it as a  browse   search   click  pattern.
We sampled 500 such patterns from the  browse   search  sessions.
For each of these patterns, we submitted that query to search engine SE and got the top-50 pages in its search result.
We crawled the content
 search engine.
With the URL, one can quickly retrieve the content of the page from the index of the search engine.
Input: triggering page p, SearchTrigger query q, and weighting parameter  ; Output: re-ranked documents {t
 2,    , t (cid:3) k}; (cid:3) (cid:3) 1, t using search engine s default retrieval function.
The documents are sorted in the descending order of their relevance scores {r1, r2,  , rk};
 si = sim(p, ti); (cid:3) s i = ri +  si;
 (cid:3) 1, t 2,  , t (cid:3) k} in the (cid:3) descending order of s (cid:3) i; of these pages and used the algorithm in Table 5 to rerank these pages.
We regard the clicked page in the session as ground truth.
If the reranking algorithm really boosted this clicked page, we say there is gain for this pattern.
Among the 500 patterns, our experimental results show that there are 337 patterns (or 67.4%) with gains, 22 patterns (or 4.4%) without any position change, and 141 patterns (or 28.2%) with losses.
This demonstrates that the use of contextual information can improve search quality and user satisfaction.
In this paper we have proposed the concept of actively predicting users  search intents based on their browsing behaviors.
Our analysis on large scale user browsing behavior data indicates that many search intents are triggered by the pages that a user browses right before his/her search actions.
In order to suggest meaningful queries to satisfy such intents of users when they browse the Web, we have proposed a machine learning method and demonstrated its e ectiveness in the scenario of contextual retrieval.
Our experimental results have shown that the proposed approach can predict meaningful queries to users for a given page, and can sig-ni cantly improve the search quality with regards to these queries.
For future work, we would like to study the case that a query is triggered by a sequence of successively browsed pages, and the case that a page triggers several queries with di erent intents of a user in the same session.
We believe that the deep understanding of users  search intents when they are browsing can help extend the functionality of search engines beyond their current boundaries, and can also provide users with a much better experience on the Web.
