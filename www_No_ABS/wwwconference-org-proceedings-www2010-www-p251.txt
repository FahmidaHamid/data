Web search is the gateway to the Internet for billions of users daily.
When the user issues a query to the search engine, two separate searches are evaluated: the search over the corpus of pre-crawled web pages is called web search; the advertisements that are displayed at the top and the side of the web search results are retrieved by sponsored search.
Sponsored search provides revenue for the search engine and brings users to numerous advertiser sites.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Web search and sponsored search di er in a few key aspects.
Sponsored search is evaluated over a set of ads that promote products and services.
As it is customary in the advertising world, the textual content visible to the user (ad creative), is generated by the advertiser to maximize the response of the target audience.
In web search on the other hand, the snippet shown on the search result page is generated automatically by the summarization mechanism of the search engine.
Another important di erence is in the way the ads and the web results are selected.
While the web pages are selected based on their content, the ad selection depends heavily on the use of the ad bid phrase   a query that the advertiser has speci ed as suitable for the ad.
In the early days of the sponsored search marketplace, this mechanism allowed for simple ad selection where the whole burden (and control) is shifted to the advertiser.
However, with the development of the sponsored search market, it become quickly apparent that the advertisers cannot possibly  nd all the queries that could be relevant to their advertisements.
To alleviate this problem the search engines allow for advanced match where ad can be selected even if their bid phrase does not match the query.
The advanced match problem corresponds closer to the web search problem.
Recent advanced match approaches use search techniques for ad selection by evaluating the query over a corpus of documents that are created from the ads [7, 25].
One of the key di -culties in this ad retrieval approach is that the ads are much shorter than documents in most other search applications [25].
In this paper, we explore the use of the landing page in ad retrieval for sponsored search.
We contrast the use of the content of the ad creative with the use of the whole, or parts, of the landing page.
Our study was partly motivated by a preliminary examination of a set of textual ads and their landing pages, which indicated that over 30% of the landing pages are not at all, or very remotely, related to the ads.
Thus our intuition suggests that indiscriminate use the content of such landing pages in the ad selection would decrease the precision of the ad retrieval.
We explore two types of extractive summarization techniques to select useful regions from the landing pages: out-of-context and in-context methods.
Out-of-context methods select regions from the landing page by simply analyzing the landing page itself, without taking the ad context into account.
The ad context is composed of the creative, bid phrase, title and any other information about the ad that can be computed o ine (i.e., prior to query processing).
In-regions of the landing page should be used by the ad selection engine.
In addition, we introduce a simple yet e ective unsupervised algorithm motivated by compositional vector space models [21, 22, 14] based on compositional semantics [23, 21] in order to enrich the ad context and enhance the ad selection.
Experimental results demonstrate that selective use of landing pages can signi cantly improve the quality of ad selection.
We also  nd that our extractive summariza-tion techniques reduce the size of landing pages substantially, thereby reducing the amount of data that needs to be indexed, while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page.
The contributions of this paper are threefold:   We quantify the bene t of using the landing page for ad selection in sponsored search.
In particular, we examine a number of di erent extractive summarization techniques to make the best use of landing pages.
  We propose a simple yet e ective unsupervised algorithm using compositional vector space models to enrich the ad context.
We then present two di erent ways in which the enriched ad context can be utilized to enhance the ad selection.
  We report experimental results that show that our proposed methods for selecting landing page regions have up to 8.5% performance improvement in Discounted Cumulative Gain (DCG), measured over a production-level ad selection system.
A large part of the $30 billion Web advertising market consists of textual ads, the ubiquitous short text messages usually marked as  sponsored links .
There are two main channels for distributing such ads.
Sponsored search places ads on the result pages of a Web search engine, where ads are selected to be relevant to the search query (see [11] for a brief history of the subject).
All major Web search engines (Google, Microsoft, Yahoo!)
support sponsored ads and act simultaneously as a Web search engine and an ad search engine.
Content match (or contextual advertising) places ads on third-party Web pages.
Today, almost all of the for-pro t non-transactional Web sites (without direct sales) rely at least to some extent on contextual advertising revenue.
Content match supports sites that range from individual bloggers and small niche communities, to large publishers such as major newspapers.
In this paper we focus on sponsored search.
However, content match ads are identical to the sponsored search ads and we believe that using landing page content for ad selection would be applicable to content match as well.
Sponsored search is an interplay of three entities: The advertiser provides the supply of ads.
As in traditional advertising, the goal of the advertisers can be broadly de ned as promotion of products or services.
The search engine provides  real estate  for placing ads (i.e., allocates space on search results pages), and selects ads that are relevant to the user s query.
Users visit the Web pages and interact with the ads.
The prevalent pricing model for textual ads is that the advertisers pay for every click on the advertisement (pay-per-click or PPC).
The amount paid by the advertiser for each sponsored search click is usually determined by an auction process [9].
The advertisers place bids on a search phrase, and their position in the column of ads displayed on the search results page is determined by their bid.
Thus, each ad is annotated with one or more bid phrases.
In addition to the bid phrase, an ad also contains a title usually displayed in bold font, and a creative, which is the few lines of text, usually shorter than 120 characters, displayed on the page.
Naturally, each ad contains a URL to the advertised Web page, called the landing page.
A recent study, analyzed the types of landing pages [3] and we classi ed the landing pages into three main categories: homepage, which are top-level pages on the advertisers  Web site; search transfer, which are dynamically generated search result pages on the advertiser s site; and category browse, which are subsections of the advertiser s site, generally related to the user query.
In this work we explore the use of landing pages in the context of an ad retrieval system that is based on information retrieval principles, as reported in [7].
The input to our system is a search (or  Web ) query, and the output is a set of ads that are relevant to this query.
We represent the ads and the queries in a vector space model using their unigrams and phrases as features.
The query-ads similarity is computed using cosine between the angle between their vectors.
Assuming that the query vector and the ad vectors are pre-normalized using L2 norm, the scoring function is a simple dot product: score(query, ad) = X wf,adwf,query f  ad query where wf,ad and wf,query are the weights of the feature f in the ad and the query accordingly.
For weighting of the features we use a section-aware variant of tf-idf where each region of the ad is given a tf multiplier.
This weighting scheme can be naturally extended to incorporate new regions of the ad and the query, as the one we explore in this paper   the landing page.
The ads are processed and indexed in an inverted index of ads that at runtime is used to evaluate similarity queries by a document-at-the-time algorithm.
For more details we refer the reader to [7].
We explore a number of di erent ways to extract information from landing pages that can be used to augment ad indexing and eventually help with ad selection.
Since the extracted information should be a succinct representation of the most useful information in the landing page, it can be viewed as a  summary  of the given page.
A fundamental question we face here is what constitutes a good summary for our ad retrieval task?
We start by noting that a landing page can contain many di erent regions, each focusing on one type of information about the subject matter of the page.
For instance, in the example shown in Figure 1, the page is mainly about  Canon EOS Digital Camera .
It contains several regions that are directly related to the product: a product-description region, a customer-review region, and a transaction region with information for purchase.
It also contains a region with a list of related products   information that is tangentially related to the product being advertised.
In addition, there Bid phrase Short description Compare Prices in 101+ stores.
Machine Learning machine learning Find cheap book prices every time.
Figure 2: An example of an ad := TF-logIDF representation of an ad a CR := {[-5,+5] landing page text around any word in a}
 For each candidate region ri   CR, If cosine similarity(a, ri) >  , Then RR   RR   ri, Return RR as relevant regions for the given ad Figure 3: Extracting relevant regions
 We investigate the  rst hypothesis by introducing two variants of an algorithm that select relevant regions in the landing page with in the context of the ad intent.
Both use  seed words  representing the ad intent to help select relevant regions.
In the  rst (and simpler) variant, we use only words from the ad content to represent the ad intent; in the second variant, we use an extended set of words.
We start by describing the simpler variant.
Figure 3 shows the procedure to select terms from a landing page by extracting the relevant regions based on the content of the ad.
We use the tf-idf model to represent ads.
The textual information we utilize in an advertisement consists of three components: a title, a short description, and a bid phrase.
As we can see in Figure 2, terms that are repeated across di erent components (e.g.,  machine  and learning ) tend to be more important than words that are repeated inside only one component (e.g.,  prices ).
Thus, for the computation of term frequency, we count the number of components that term appears in.
Similarly, we treat each ad component as a separate document in an ad corpus for the computation of document frequency.
We refer to the resulting vector as ad vector.
Next, we locate candidate regions in a given landing page in the context of the ad vector.
For any word in the landing page that also appears in the ad vector, we consider the text span in [5, +5] window as a candidate region.
For each candidate region, we compute the cosine similarity between the candidate region and the ad vector.
We merge all candidate regions whose similarity scores are above a certain threshold  .
The resulting regions are relevant regions for a given advertisement.
One natural concern regarding this approach is that some good regions might not be selected as relevant due to the vocabulary mismatch between the ad and the corresponding landing page, because textual information given in an ad has to be very succinct.
In order to address such concern, we next introduce an algorithm that extends the ad vector into a richer context.
We start by building co-occurrence vectors of words appearing Figure 1: An example of a landing page.
are regions with navigational information that carry no information about the product at all.
Clearly, not all of these regions are equally important for ad retrieval.
Our goal is to investigate what is the best way to select information from the right regions that could help improve ad retrieval.
In this paper, we explore two di erent hypotheses.
The  rst hypothesis is that a good summary should be de ned in the context of the advertisement intent.
For instance, if the intent of a given ad is to provide customer reviews, then a good summary should likewise focus on the customer reviews in the page rather than, say, the transaction information.
In contrast, the second hypothesis assumes that a good summary for a landing page can be de ned solely based on information available in the landing page itself, without referring to the content of the ad creative for the advertisement intent.
In what follows, in-context term selection refers to sum-marization of landing pages based on the  rst hypothesis, and out-of-context term selection refers to approaches based on the second hypothesis.
Note that our end goal is not to produce a human readable summary of a web page.
Rather, we plan to extract those terms from the landing pages that can assist advertisement selection.
Co-occurring words(PMI) mattress futon(6.4), king(2.95), pillow(4.92) queen(5.64), shopping(2.2), brand(2.5) tempur-pedic(6.66), bunk(5.28), mite(5.79) serta(7.64), sealy(7.79), visco(7.75) platform(4.74), products(1.94), store(2.44) cover(4.1), outlet(3.46), directory(2.4) savings(1.37), topper(5.71), allergen(6.63) Figure 4: An example of a co-occurrence vector in an ad corpus (Section 3.1.2).
Using the co-occurrence vectors for all words in a given ad, we then compose a semantic vector that represent the collective semantic meaning of the advertisement intent (Section 3.1.3).
Finally, the resulting semantic vector, in conjunction with the original ad vector, is used to assist extracting relevant regions from the landing page (Section 3.1.4).
pus In order to overcome the vocabulary mismatch problem, we built co-occurrence vectors from an advertisement corpus that contained over half million ads.
Again, each of the three textual ad components (title, short description, and bid phrase) was treated as a separate pseudo-document d.
We de ne the co-occurrence count for a pair of words u and w as the number of pseudo-documents they co-appeared in: cooccnt(u, w) = |{d | u   d   w   d}| We discarded stop-words and infrequent words (those that appeared in the corpus fewer than 4 times).
We then formed the co-occurrence vector for each word u as coocvec(u) = {w | cooccnt(u, w) > 0} We kept only those with |coocvec(u)|   3.
For all w   coocvec(u) we computed its point-wise mutual information (PMI) to u.
The de nition of PMI is given as follows: PMI(u, w) = log cuw
   Pn j=1 cuj
 Pn i=1 ciw
 where cuw is the number of times u and w co-occured, n is the number of unique words, and N is the total word occurrences.
As an example, the co-occurrence vector for u =  mattress  is given in Figure 4.
The PMI scores were shown in the parentheses.
PMI scores re ect how informative a co-occurring word is for u.
That is, those with higher PMI scores (e.g.,  futon ,  tempur-pedic ,  serta ) are in general more informative than words with lower PMI scores (e.g.,  shopping ,  products ,  savings ).
For each u, we also computed the average PMI score for it as avgP M I(u) = Pw coocvec(u) PMI(u, w) |coocvec(u)| avgP M I (u) represented how  speci c  u was.
That is, if u co-occurred with many words with low PMI scores, then u was likely to have appeared in many di erent contexts and domains.
In other words, they tended to act like stop words in the advertisement corpus.
We added the 50 words with the lowest average PMI scores to our existing stop-word list.
Examples of such words included  nd ,  search ,  save ,  free , etc.
We then rebuilt the co-occurrence vectors using the extended stop-word list.
to Enrich Ad Context Having constructed co-occurrence vectors for each word u in a given ad, the next question was how to combine them into one vector that captured the ad intent.
Let {ui} be the bag-of-words representation of an ad, and V = {v1, ..., vn} be the set of PMI based co-occurrence vectors for this ad, such that vi = {vij |j   coocvec(ui)} and vij is set to the PMI value between the ad word ui and j.
We then investigated di erent ways to compose these vectors into one single vector, which we refer to as the compositional semantic vector (csv) for the given ad: csv = f (v1, ..., vn) (1) The need for vector composition arises often in information retrieval (IR) and national language processing (NLP).
However, it has rarely been the main focus of research until recently.
As such, the choice of composition function has been rather arbitrary.
The popular choices have been component-wise vector averaging or component-wise vector addition (e.g., [12, 19]).
Mitchell and Lapata [21] speci -cally addressed this issue by viewing the vector composition in light of compositional semantics [23] where the meaning of the whole is a function of the meaning of its parts.
This principle of compositionality [13] has been a fundamental presupposition in some of the branches of mathematics, linguistics and philosophy.
Researchers extended this insight by comparing various compositional operations in broader NLP applications (e.g., [22, 14]).
In this paper, we investigate this problem in the context of ad retrieval.
In particular, we explore di erent vector compositions in order to compose a semantic vector representation for a given ad.
One that has been used often is component-wise vector addition: csvj = X vij i (2) where, csvj and vij are jth components of vector csv and vi respectively.
Another compositional vector operation can be component-wise vector multiplication as shown below.
csvj = Y vij i (3) Mitchell and Lapata [21] argues that a component-wise vector multiplication is an operation that has been rarely used, but it is conceptually more desirable for meaning composition because multiplication picks out the content relevant to the combination by scaling each component more explicitly.
As noted in [21], it might be desirable that each word should contribute di erently to the overall meaning.
This is particularly the case in our task.
For instance, in Figure 2, the words  Compare  and  Find  are not as informative as the word  book  when distinguishing the given ad from others.
In fact, words such as  Compare  or  Find  type of object being advertised.
Therefore, such uninformative words should make relatively smaller contribution when composing the semantic meaning of the overall advertisement.
To address this issue, we weigh the contribution of each co-occurrence vector by its average PMI scores (as de- ned in Section 3.1.2).
Equation 2 and 3 are then modi ed as follows: csvj = X avgP M I(ui)vij i csvj = Y avgP M I (ui)vij i (4) (5) Note that the resulting vector csv of Equation 5 is equivalent to that of Equation 3 modulo normalization.
It is worth noting that the weighting scheme used by [21] is di erent from the one shown in this paper; in [21], weights are de ned based on the syntax and semantic role of each word in a given sentence.
However, such weighting scheme is not suitable for advertisement retrieval for two reasons.
First, languages used in advertisement are succinct and often are not complete or valid sentences.
Therefore, it can be hard to determine the semantic role of each word reliably in an advertisement.
Second, we often need to weight words in the same syntactic category di erently.
For instance, in Figure 2, both  prices  and  book  are nouns, and used as objects of verbs.
However, the word  prices  is not as informative as  book .
One aspect of composition that previous work (e.g., [21, 14]) did not discuss is the e ect of zeros in the multipli- cations.
This is less of a problem if composing only two vectors, as was the case in [21, 14].
However, when composing more than two vectors, if a word did not appear in all of the vectors, its value in the csv is zero.
To address this problem, we adopt a simple smoothing scheme: csvj = Y smoo avgP M I (ui)vij (6) where the operation Qi factor   whenever vij = 0.
i smoo replaces vij with a smoothing There are other compositional operations that have been explored in literature.
For simplicity, consider two vectors v1 and v2, where the length of each vector is given as m1 and m2 respectively.
One example is a tensor product [26], where the resulting vector is a matrix U with dimension-ality m1   m2, and the component Ui,j of the matrix is given as Ui,j = v1 i   v1 j. Tensor product is not practically useful for advertisement retrieval, as the dimensional-ity of the composed vector explodes exponentially.
Another compositional operation is circular convolution [27], where the resulting vector u is given as ui = Pj v1 j v2 i j.
In this case, the dimensionality of the resulting vector is manageable, but the computational cost is much heavier than component-wise operations such as Equation 2 6.
Also, [14] reports that the performance of convolution is not better than other simpler alternatives.
Therefore, we experiment only with component-wise operations.
:= TF-logIDF representation of an ad := compositional semantic vector of an ad := TF-logIDF representation of N best entries of csv a csv c CR+ := {[-5,+5] landing page text around any word in a or c}
 For each candidate region ri   CR+, If cosine similarity(a, ri) + cosine similarity(c, ri) >  +, Then RR+   RR+   ri, Return RR+ as relevant regions for the given ad Figure 5: Extracting relevant regions with composi-tional semantic vectors for ads
 Compositional Semantic Vectors Finally, Figure 5 shows the procedure to extract relevant regions with the enriched context.
First, we represent the content of the ad with the ad vector, as described in Section 3.1.1).
We then compute the compositional semantic vector (csv) of the given ad, as described in Section 3.1.3.
We keep the top N entries with highest scores in the csv in order to keep the size of csv similar to that of the ad vector.
This is to ensure that the extended vector will not be dominated by csv, which could potentially introduce topic shift.
Note that some of the compositional operations we consider involve component-wise multiplications among multiple vectors (Equation 5-6).
As a result, the distribution of scores across di erent entries can be undesirably skewed.
Thus, before combining the csv with the ad vector, we compute the tf-idf score for each of these N entries in csv in the same way tf-idf scores are computed for ad vectors.
Terms that do not appear in the ad receive a tf score of 1.
Once we compute the converted compositional semantic vector c, we determine the candidate regions in the landing page in a way similar to what we described in Section 3.1.1.
For any word in the landing page that also appears in either a or c, we consider the text span in [5, +5] window as a candidate region.
For each candidate region, we compute the cosine similarity between the candidate region and the ad vector, as well as the cosine similarity between the candidate region and the converted compositional semantic vector.
If the sum of the two cosine similarity scores is above a certain threshold  +, then the candidate region is selected as a relevant region.
As we can see, the overall procedure given in Figure 5 is similar to the one given in Figure 3, except the former incorporates the compositional semantic vector of the given ad in order to complement the succinct language of ad.
We next explore algorithms that extract a summary-like representation of a landing page without consulting the advertisement associated with the given landing page.
One popular strategy is taking the top portion of the landing page as a summary (e.g., [1]).
Albeit the simplicity, this method is known to be very e ective, and often a hard baseline to beat (e.g., [6, 10]).
We take up to N unique words that appear  rst in the given landing page.
Baseline Overlap
 First Best All n/a out out out in in in in RR csv(Qsmoo) in RR csv(P) RR csv(Q)

 *0.594 (  5.5%)
 *0.600 (  6.6%)

 *0.603 (  7.1%) +*0.604 (  7.3%)

 +*0.871 (  3.7%) *0.866 (  3.1%) +0.868 (  3.3%)
 *0.877 (  4.4%) +*0.877 (  4.4%) +*0.884 (  5.2%) +*0.611 (  8.5%) +*0.892 (  6.2%)








 *1.087 (  3.1%)

 +*0.534 (  3.7%) *0.531 (  3.1%) +0.532 (  3.3%)
 *0.537 (  4.3%) +*0.538 (  4.5%) +*0.542 (  5.2%)








 +*0.547 (  6.2%) +*0.510 (  3.0%) Table 1: Evaluation of di erent term selection strategies for landing pages.
Next strategy to consider is taking up to N words that are the most representative of the landing page.
We use TF-IDF weighting to extract such words.
where reli is the human graded relevant score (reli   {10.0,
 the ideal DCG at position k.
Summarization Finally, we also try with all words from the entire landing page as an extreme case.
This option is not practically as attractive however, for it does not reduce the amount of data that needs to be indexed.
Table 1 shows the performance of di erent term selection strategies for landing pages.
The table omits NDCG at 1 as it is the same as DCG at 1.
The brief description of each method in Table 1 is as follows:

 To validate and compare the proposed approaches, we use a data set sampled from the sponsored search tra c of a major search engine.
The query-ad pairs in this sample were evaluated for relevance by professional editorial sta .
We evaluate our methods by augmenting the existing ad selection mechanisms to use the landing page features and rerank the judged pairs.
In the following we  rst give more details about the data set, and then present the evaluation results.
The development data consists of about 3600 query-ad pairs, and the test data consists of about 22500 query-ad pairs.
In order to measure the e ect of landing pages on ad selection more directly, we evaluate on only those query-ad pairs that have valid landing pages.
We consider landing pages with less than 50 content words as invalid, as most of such landing pages were pages with error messages, such as a page that says the clicked link is no longer valid, or the searched item no longer exists.
We also exclude URL queries from the evaluation, as the relevance of an ad for a URL query has little to do with the content of landing pages.
For each query-ad pair, human editors judged the quality of ad into  ve di erent values: perfect (10.0), excellent (7.0), good (3.0), fair (0.5), bad (0.0).
In advertisement retrieval, the quality of top few results is the most important.
Therefore, we report the performance in terms of Discounted Cumulative Gain (DCG) and Normalized DCG (NDCG) at k   {1, 2, 3}.
The de nition of DCG and NDCG are given as follows: DCGk := rel1 + k
 i=2 reli log2i NDCGk := DCGk IDCGk   Baseline: This method corresponds to our ad retrieval system without utilizing landing pages.
Next three methods extract features from landing pages without considering the context of the ad.
  First: This method corresponds to the approach described in Section 3.2.1.
In particular, it utilizes  rst N = 100 unique words in landing pages.
  Best: This method corresponds to the approach described in Section 3.2.2.
In particular, it utilizes best N = 100 unique words in landing pages.
  All: This method corresponds to the approach described in Section 3.2.3.
In particular, it includes all words in the landing pages.
Next  ve methods extract features from landing pages based on the context of the ad.
  Overlap: This method includes only those words in the landing pages that also appeared in the given ad.
The purpose of this method is to indirectly contrast the relative performance gain resulted from words in landing pages that did not appear in the ad.
  RR: This method corresponds to the approach introduced in Section 3.1.1.
In particular, it determines Relevant Regions in landing pages based only on ad vectors.
In the procedure described in Figure 3, we set   = 0.03 to select relevant regions based on the cosine similarity.
We choose this value so that the size of resulting relevant regions is approximately one third of the size of all words in the landing page.
  RR csv(P): This method corresponds to the approach introduced in Section 3.1.4 in conjunction with the compositional operation speci ed by Equation 4.
ing page based on both the ad vector as well as the compositional semantic vector(csv) of the given ad, and csv is computed by applying weighted component-wise summation of co-occurrence vectors.
In the procedure described in Figure 5, we set  + = 0.05 to select relevant regions based on the cosine similarity scores.
We choose this value so that the size of resulting relevant regions by this method is close to the size of resulting relevant regions by RR method described above.
  RR csv(Q): This method corresponds to the approach introduced in Section 3.1.4 in conjunction with the compositional operation speci ed by Equation 5.
That is, csv is computed by applying weighted component-wise multiplications of co-occurrence vectors.
In the procedure described in Figure 5, we use the same  + = 0.05 value as above.
  RR csv(Qsmoo): This method corresponds to the approach introduced in Section 3.1.4 in conjunction with the compositional operation speci ed by Equation 6.
That is, csv is computed by applying smoothed weighted component-wise multiplications of co-occurrence vectors.
In the procedure described in Figure 5, we use the same  + = 0.05 value as above.
We set the smoothing factor   to 0.01.
In Table 1, the relative performance gain of each method with respect to the baseline is given in parentheses.
We perform two statistical signi cant tests: Wilcoxon signed-rank test and paired Student s t-test.
Numbers marked with  +  indicate the performance gain is statistically signi cant by Wilcoxon test, and  *  indicate the gain is statistically signi cant by paired t-test.
Numbers in bold indicate the best performing method for each evaluation metric.
We  rst discuss the performance of approaches that do not take into account the context of ad: First, Best, and All.
Among the three, First performs the best for most of the evaluation metrics, achieving 5.5% relative gain for DCG@1,
 gain with respect to the baseline is not always statistically signi cant however.
It is interesting that First performs slightly better than Best, albeit its simplicity.
This result con rms the observation from previous research that  rst N bytes of a web page often make a very strong baseline as a summary (e.g., [6, 10]).
All three approaches make substantial improvement over the baseline for DCG/NDCG at 1 and 2.
This result indicates that landing pages contain valuable information that can improve ad selection.
One of our initial goal was to reduce the amount of data we need to index from the landing pages.
Therefore, it is a good news that First performs at least as good as All for most metrics.
The performance di erence between First and All is not statistically signi cant.
Next we discuss the performance of approaches that extract features from landing pages based on the context of ad.
The best performing approach is RR csv(Qsmoo), achieving 8.5% relative gain for DCG@1, 6.2% for DCG@2, and
 In fact, RR csv(Qsmoo) icant for all evaluation metrics.
is the only approach that makes a statistically signi cant improvement for DCG/NDCG at 3.
We also make the following observations: Method Size of selected terms All First Best Overlap
 RR csv(P) RR csv(Q) RR csv(Qsmoo) csv(Qsmoo)

















 Table 3: Size of selected terms in landing pages.
(1) Among the approaches that select relevant regions using the compositional semantic vector(csv), those that are based on multiplicative vector composition perform better than the one based on additive vector composition.
This result echoes the empirical results reported by [21].
(2) Notice that all three approaches that utilizes compo-sitional semantic vector(csv) of the ad perform better than RR, which select relevant regions only based on the ad.
(3) In general, approaches that consider the context of the ad perform better than the approaches that do not.
(4) Finally, the worst performing approach is Overlap, which indicates that the performance gain of other approaches comes from utilizing terms in landing pages that did not appear in the ad.
In other words, the language used in advertisement is often too succinct, and it might not match any term used in web queries.
Utilizing landing pages enables to bridge the vocabulary gap between the advertisement and the web queries.
Vectors Having seen that RR csv(Qsmoo) is the best performing method in Table 1, and that it performs better than RR that does not utilize the compositional semantic vector(csv), we conduct an indirect evaluation of the quality of compo-sitional semantic vectors.
In particular, we extract 100 best entries in the compositional semantic vector in Figure 5, and use those words as features for our ad retrieval system, without extracting any relevant region from the landing pages.
We denote this approach as csv(Qsmoo).
As shown in Table 2, we  nd that RR-csv(Qsmoo), the method utilizing extractive summarization of landing pages achieves overall a better performance with the exception of DCG/NDCG at 3, where csv(Qsmoo) performs slightly better.
We draw following two conclusions from this experiment: First, utilizing the compositional semantic vectors without consulting landing pages brings out a performance gain that is close to the gain of a method that uses ex-tractive summarization of landing pages.
(The di erence between the two approaches is not statistically signi cant.)
This implies that compositional semantic vectors proposed in this paper have strong utility on their own.
Second, we conjecture that the performance gain from either method comes from reducing the vocabulary mismatch between the Baseline csv(Qsmoo) RR csv(Qsmoo)

 +*0.603 (  7.1%) +*0.611 (  8.5%) +*0.892 (  6.2%)



 +*0.882 (  5.0%) +*1.092 (  3.6%) *1.087 (  3.1%)



 +*0.541 (  5.0%) +*0.513 (  3.6%) +*0.510 (  3.0%) +*0.547 (  6.2%) Table 2: Evaluation of the quality of compositional semantic vectors for ads.
ad and the query by enriching the context of the ad.
The fact RR-csv(Qsmoo) achieves a better performance in general suggests that landing pages provide extra information that is not available in compositional semantic vectors.
Finally, it is worthwhile to highlight two potential utilities of extractive summarization of landing pages(RR-csv(Qsmoo)), that are not available if using only compositional semantic vectors without consulting landing pages(csv(Qsmoo)).
(1) Extractive summarization of landing pages can be used to detect landing pages that are either spam or broken.
That is, spam or broken pages might not contain any relevant region pertaining to the advertisement, and lack of relevant regions can signal bad landing pages.
(2) Unlike compositional semantic vectors, extractive sum-marization of landing pages can be potentially utilized as snippet of landing pages, when displaying the search advertising.
In this section, we examine the e ect of data reduction by di erent summarization strategies for landing pages.
As shown in Table 3, the size of selected terms when using all words in the landing pages(All) amounts to 30.1MB.
Most of summarization strategies reduces the data in the range of 32-43%.
One exception is Overlap, which drastically reduces the data down to 3%, but Overlap performs poorly as shown in Table 1.
The best performing method RR-csv(Qsmoo) reduces the data approximately to a third in comparison to All, saving the time and space required for indexing substantially.
Notice that there are small di erences in size of selected terms among di erent summarization strategies, even though we always set the equal upper bound (100) on the number of words selected for each landing page.
The small di er-ence between First and Best comes from the di erence in lengths among di erent words.
The di erence between First (or Best) and csv(Qsmoo) comes from the fact that landing pages might not always have as many words to hit the upper bound, while csv almost always have more than
 tween First (or Best) and some of the RR variants comes from the fact that selected regions are smaller than the original landing pages and might not have as many words to hit the upper bound.
We discuss the related work from three di erent aspects; sponsored search and content match (Section 5.1), web page summarization (Section 5.2), and applications of composi-tional vector space models (Section 5.3).
Sponsored search in general and advanced match in particular have been an area of active research in the last few years.
There are several approaches based on query rewriting techniques that are easy to implement on the top of exact match by mapping query to rewrites and then using the rewrites to fetch ads.
[2, 28, 16].
In those approaches, ad selection is performed using a single feature, in that it is essentially based on exact match between the rewrites and the bid phrases of ads.
In contrast, a few recent approaches employ search based techniques to overcome the limited bid phrases supplied by advertisers (advanced match).
For instance, Ribeiro-Neto et al.
[25] examine the use of vector space model and cosine similarity as a ranking function for content match ad retrieval.
To resolve the vocabulary mismatch, the triggering page (used as a query) is expanded by features from related pages.
The proposed method in [25] is particularly suitable for content match, but the application to sponsored search is not straightforward due to lack of the triggering pages in sponsored search.
Although most of previous research for ad retrieval did not utilize landing pages for ad-side expansion, some (e.g., [25, 24]) experimented with augmenting the ad with the entire landing page to improve the content match.
In contrast, we perform a more focused study of landing pages, contrasting a number of in-context and out-of-context extractive summa-rization techniques.
In addition, our study is  rst to explore the use of landing pages in the context of sponsored search, where the query is much shorter than that of content match studied in [25, 24].
The ad-side expansion presented in this paper is complementary to the query side expansion in an ad retrieval system for sponsored search in [7], where queries are expanded using web search results.
The sponsored search problem is then e ectively mapped to contextual advertising on the search result page.
The major di erence from our work is that it does not consider the use of landing pages or employ any ad-side expansion technique.
The ad-side expansion can be viewed as document-side expansion, which has been examined extensively in the general IR community.
An interesting study by Billerbeck and Zobel [5] demonstrates that document-side expansion is inferior to query-side expansion when the documents are long.
It is worthwhile to point out that this conclusion does not extend to advertisement retrieval, since the ads are signi -cantly shorter than the web documents used in [5].
There are several studies that show the bene t of document-side expansion by extracting features from similar documents based on the language models (e.g.
[17, 20]).
However, the particularity of the ad retrieval and the relationship between the landing page and the ads makes our problem signi cantly di erent than the setting explored there.
There are a good deal of previous work investigating web page summarization (e.g., [4, 15]), but most of them are geared toward producing a human readable summaries.
In contrast, our main thrust in this paper is to extract relevant regions from the landing pages in the context of sponsored search.
Our empirical results demonstrate that extractive summarization of landing pages can improve the ad retrieval while reducing the amount of data that needs to be indexed.
Another notable di erence from the work of [4] is that we only consider summarization techniques that are unsupervised algorithms.
In general, summarization techniques that require human annotated summaries are not easily applicable to our task, since such human annotations do not exist for advertisers  landing pages, and the typical web pages available for summarization tasks are much di erent from advertiser s landing pages.
Lam-Adesina and Jones [18] present a query expansion technique that share conceptual similarities to our work; they employ summarization techniques to make a better use of relevant documents (pseudo relevance feedback), and compare context-independent (out-of-context) summaries with query-biased (in-context) summaries.
However, the actual task of focus is very di erent in that [18] studies query-side expansion for ad hoc information retrieval, while our work explores ad-side expansion in the context of sponsored search.
Unlike [18], we investigate document summaries in the context of ads, not queries, because o line processing of landing pages is much desirable for e ciency reasons.
The need for vector composition arises often in information retrieval (IR) and national language processing (NLP), but it has rarely been the main focus of research until recently.
Researchers cast compositional vector operations in light of compositional semantics [23], and explored the utility of compositional vector space models in a number of NLP applications [14, 21, 22, 27].
Our work is the  rst to utilize compositional vector space models in the context of sponsored search.
Some might wonder the connection between Latent Semantic Analysis [8] and the compositional vector space models explored in this paper.
Both are based on vectorial representation of semantic meaning, but the object of representation is inherently di erent; that is, the former analyzes the relationships between documents and terms, while the latter captures the relationships among terms.
A well known problem of LSA is that it cannot represent polysemy, because each word represents a single point in the meaning space [8].
On the contrary, compositional vector space models naturally embody polysemous representation, as each co-occurrence vector may contain multiple implicit clusters of co-occurring words corresponding to di erent semantic meanings of the center word [21].
Therefore, a particular meaning of a polysemous word is chosen only as a result of compositional vector operations with other co-occurrence vectors.
In this paper, we explore a number of extractive summa-rization techniques for landing pages in order to enhance sponsored search ad retrieval.
We contrast two hypotheses   in-context and out-of-context summarization of landing pages with respect to the advertisement intent, and  nd that in-context summarization techniques are more e ec-tive for improving sponsored search.
Empirical results show that applying extractive summarization techniques to landing pages can reduce the amount of data that needs to be indexed signi cantly, while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page.
Our work is the  rst to utilize compositional vector space models in the context of ad retrieval.
We explore a range of compositional vector operations that combine co-occurrence vectors to enrich the succinct advertisement.
We then show two di erent ways in which the enriched ad context can be utilized.
First, it helps to extract more useful regions in the landing page with respect to the ad intent.
Second, the enriched ad context can be a useful resource on its own to reduce the vocabulary mismatch.
In the future, we plan to extend extractive summarization techniques presented in this paper in order to reliably detect the deceptive advertisements that link to spam or broken landing pages.
We thank Andrei Broder, Peter Ciccolo, Donald Metzler, and Je rey Yuan for helpful discussions and technical assistance.
We also thank the anonymous reviewers for their comments and suggestions.
