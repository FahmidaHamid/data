Users turn to search engines in order to satisfy their information needs [8].
The classical scenario is for them to express their needs as a free-text query and then scan through results in order to identify full or partial answers.
Their needs can then be:   Satis ed: this happens when users obtain the answer they were seeking immediately on the  rst page of results.
They can either directly read the answer on the results page itself, when generated for instance by services such as a Google s oneboxes [16] or a Yahoo shortcut [32], (e.g., calculator, weather and sports results), or reach it after clicking on one of the top ranked pages.
  Not satis ed: this happens when (1) users did not succeed in formulating their query, when (2) relevant content does not exist, or  nally when (3) relevant content does exist but the search engine can not identify it.
By default, most engines will focus at this stage on the  rst case (since there is not much they can do in real time with the two other cases) and will o er users query assistance tools, pointing them to related queries that might bring better results.
  Partially satis ed: this usually happens when users have complex information needs, for which there does not exist one single Web page that holds all the needed information.
In this scenario, the user will like some of results, disregard others and continue exploring, clicking on a few results, trying out related queries, while still gathering information.
Such sessions can last from a few minutes to several days.
Examples include travel needs (when a traveler veri es hotels, restaurants or transportation means around a certain location), education needs (when students work on an assignment), or medical needs (when a patient studies a speci c illness, its symptoms and treatments).
the users  needs are too complex or too heterogeneous to be answered in one shot.
We refer to these non-ephemeral information seeking tasks as  research missions .
This notion relates to the notion of information gathering on the Web as introduced by Kellar et al. in [20], and de ned as the collection of information from multiple sources.
In the user s study they conducted over a period of one week in 2005 with 21 university students, the authors reported that information gathering tasks accounted for 13.4% of overall Web usage and was the fourth most important activity on the Web after  transactions  (46.7%),  just browsing  (19.9%) and  fact  nding  (18.3%).
Research missions, as we de ne them, restrict and re ne the notion of information gathering as they can occur only during search sessions as opposed to overall Web usage.
By manually analyzing actual query sessions over a period of 3 days, we veri ed that on average 10% of search sessions are  research missions  but more interestingly, that about
 this data, we argue that research missions deserve special attention and treatment.
Our longer-term vision is that automatically identifying these research missions could potentially break the classical search paradigm, which maps one query to a list of results, into one where entire sessions are analyzed for intent, and results match session intent.
We believe that by considering queries in the context of a session of related expressions of a common need, rather than in isolation, search engines might be able to achieve a better understanding of the real users  intent.
Our belief is supported by some recent works showing that not only the previous query, but also the long-term interests of users are important for understanding their information needs [23, 27].
As a  rst step towards this long-term goal, we propose to demonstrate that research missions can indeed be discovered on the  y, while users are interacting with the search engine, with a good enough precision rate to make them valuable.
In order to substantiate our claim, we will use the recently deployed Yahoo!
Search Pad application, which integrates a research mission identi cation component that we have developed.
Search Pad is an application that has been designed precisely to help users undertake research missions.
Search Pad allows users to easily keep trace of results they have consulted.
They can arrange and annotate them for later personal usage or for sharing with others.
Search Pad s novelty comes from its being triggered only when the search engine believes that the user is undertaking a research mission rather than looking for quick, disposable results.
Visited pages are automatically added to the appropriate search pad (either the current one when the query belongs to the same research mission or a new one if necessary) without requiring the user to speci cally  mark  them.
There have been multiple attempts in the past to support similar functionality, such as Bharat s original Searchpad1 [4] research tool, or Google s notebook [14], whose development has now been discontinued [22].
A major di erence between these previous tools and Yahoo!
Search Pad was that they required users to proactively invoke the notes taking tool and to manually mark relevant pages.
In contrast, Search Pad s novelty
 word, while ours is called  Search Pad  in two words.
comes from its automatic identi cation of research missions and its being triggered only when the system decides, with a fair level of con dence, that the user is in the right context for gathering notes.
As such, Search Pad represents the ideal application for us to verify our claim that identifying and using search missions is valuable to users.
Search Pad is automatically triggered at query time when a search mission is identi ed.
The goal of this paper is thus to demonstrate, using actual data gathered by Search Pad since it has already been deployed in the US for a few months, that research missions can be automatically detected, at the scale of Web (meeting performance and scalability requirements) and with enough accuracy that they bring value to users.
The rest of this paper is organized as follows.
Section 2 describes the Search Pad application that we will use as basis for demonstrating the validity of our approach.
Section 3 provides the needed preliminaries and de nitions of the technical problem tackled in this paper: automatically identifying research missions.
Section 4 describes our solution, i.e., the internals of the triggering component of Search Pad, a machine-learning based module aimed at detecting research missions to trigger Search Pad.
Section 5 reports our evaluation using Search Pad and provides results inferred from actual usage data.
We conclude with directions for future work,  rst for enhancing Search Pad, and then for leveraging the bene ts of research missions in other domains such as core ranking.
In spite of recent e orts at personalizing search, and storing past interactions for logged-in users, Web search remains basically  stateless .
Search Pad is a feature of Yahoo!
Search that was launched in July 2009 and precisely addresses this issue.
It help users keep track of related searches and visited pages that relate to a same  research mission 2.
Figure 1: Take notes on Barcelona cheap  ights
 far, which is su cient for understanding the application and usage scenarios.
We will formalize the de nition of search missions in the following section.
nario with Search Pad.
A user is planning a trip to Barcelona and starts issuing a few related queries, such as Barcelona cheap flights, Barcelona airport and Barcelona airport transfer, clicking each time on a few results.
When issuing the query Barcelona airport transfer, Search Pad detects that the user has been researching a topic rather than looking for quick answers and asks whether she wants to take notes, as shown in Figure 1.
If she accepts, she will then be shown a  Search Pad document  , or  pad  for short, already populated with the links of the pages she has visited during the current research mission, together with their thumbnails, as shown in Figure 2.
At this stage, the user can delete some of the links, move them around, add some personal notes, and eventually save the pad.
Figure 2: An opened pad object The pad object is fully editable, supporting drag and drop and paste operations and can be reopened and reused at a later time, for sessions that span over a number of days or weeks.
It can also be shared on Facebook, Twitter, and Delicious or simply embedded via a persistent HTML link into any Web page, as illustrated in Figure 3.
Figure 3: Sharing a pad object Previous e orts at taking notes while searching or browsing the Web can be roughly classi ed into three categories: Link centric e orts were the  rst to appear as it was clear that users would not easily remember URLs of interesting pages while sur ng the Web or visiting search results.
Bookmarks were invented for this purpose and were an integral part of Web browsers since the early days of the Web (under the name  Hotlist  in the original Mosaic, or  Favorites  in Internet Explorer).
In spite of a few research attempts at automatically structuring them [21, 24], bookmarks have been shown to be complex to manage as soon as their number goes over a few dozens [1].
Some applications such as Zotero [33] or hosted bookmark services, as o ered by major Web search engines, do a slightly better job at managing and searching bookmarks.
Google launched in 2006 its Notebook under the form of a hosted service, and browser plugin, that allowed users to store links in a collection of  notebooks , and annotate them with comments and labels.
This tool saw its development discontinued in 2009 and stopped accepting new users.
There exist numerous alternatives for note taking, such as Evernote [11], yet no clear winner has emerged to make the unanimity in this market.
Finally bookmarks took an interesting turn with the social bookmarking3 phenomenon, in the Web2.0 model of sharing comments and opinions with the community.
Nevertheless, at the personal level, native browser bookmarks are still the most handy means of keeping track of interesting search results.
Page centric e orts work at a  ner level of granularity by allowing users not only to save interesting results but to directly annotate paragraphs or passages in the page.
An example of application providing this functionality is Di-igo [10], which allows users to attach sticky notes to highlighted part of the Web page in a persistent manner.
Hunter Gatherer [28] is a similar tool that allows to collect components from within Web Pages in just one single page.
The tool supports both the within-page collection making and the information management.
Another recent example is Google Sidewiki [15], a new feature of Google s toolbar that allows users to publish and share annotations on a given Web page.
Search centric e orts are slightly di erent as they are spe-ci c to search results, while the previous ones can also relate to arbitrary pages reached by other means.
The  rst piece of work in this space was SearchPad [4], which used a proxy to decorate results from a list of search engines (namely Al-taVista, Excite, Google and HotBot) with a  Mark  button.
Clicking on this button resulted in storing the selected results in SearchPad together with their associated queries.
Users could at any time visit SearchPad to retrieve all  bookmarked queries , and their associated  leads , i.e results manually marked by users.
A few years later, Ask o ered the related MyStu  service [2].
MyStu  also decorates each search result with a  save  link, which allows registered users to save interesting results for later usage.
Yahoo!
Search Pad di ers from these solutions on several aspects
 the scope this paper, which focuses on the personal bene ts of identifying research missions.
anism, which prompts the user for taking notes only when chances of its being useful are high.
In other words, Search Pad is made clearly visible to users during these research missions for which it should be most needed, and stays out of the way otherwise.
This feature is to our knowledge not available in any other existing tool and is supported by the live research mission detection mechanism, which is our key technical contribution in this paper.
tion between di erent sessions based not only on time considerations but also on topical coherences between related queries as more formally de ned later.
Thus, when a user undertakes a di erent research mission, Search Pad should segment between those and automatically start a new pad, if given su cient evidence.
pad in such a manner that when a pad is prompted to a user, it is prepopulated with content.
While not all links might be relevant to the same extent, this motivates the user to continue taking notes.
It is also easier to delete less relevant links, as opposed to breaking the search  ow by selectively adding them.
users to use a dedicated plugin, a speci c toolbar or separate application, but is builtin within the main search service of Yahoo!.
Users will be required to sign-in (but not to register to a speci c service) only when saving a pad.
This feature signi cantly reduces the adoption barrier.
We will focus in the rest of this paper on the  rst two aspects, which are the ones that required technical novelty, namely the automatic identi cation and segmentation of research missions.
Numerous studies [5, 6, 7, 17, 19, 25, 26, 29] have investigated various aspects of users  behavior on the Web.
As a result, it is now commonly agreed upon that the information inferred by mining users  search activities is extremely valuable to search engines.
Given that the extracted information is adequately anonymized and aggregated, it is a critical source of information for delivering quality results in any type of search engines service, such as ranking, spelling and query assistance, to name just a few.
We de ne below the basic elements and objects that are used in mining users  interactions.
Query log.
Search engines store information about user s interactions in  query logs .
Query logs di er in format between search engines, but at a bare minimum typically associate with each submitted query, a list of results, as well as whether or not users clicked on them.
More formally a query log L is a set of records hqi, ui, ti, Vi, Cii, where: qi is the submitted query, ui is an anonymized identi er for the user who submitted the query, ti is a timestamp, Vi is the set of documents returned as results to the query, and Ci is the set of documents clicked by the user (which can be empty).
Sessions.
We reuse here the de nition of query session (or session for short) that was given in [5].
A session is the sequence of queries issued by a single user within a speci c time limit.
If u   U is the user identi er and t  is a timeout threshold, a user query session S is de ned as a maximal ordered sequence S = (cid:10) hqi1 , ui1 , ti1 i, .
.
.
, hqik , uik , tik i (cid:11), where ui1 =       = uik = u   U , ti1           tik , and tij+1   tij   t , for all j = 1, 2, .
.
.
, k   1.
The activity of a same user is split in two (or more) sessions whenever the time interval between two sequential queries exceeds the timeout threshold.
The typical timeout is t  = 30 minutes [9, 25, 30].
Chains and Missions, Radlinski and Joachims [26] de- ne a query chain (or chain for short) as a sequence of reformulated queries that express a same information need.
Baeza-Yates et al. de ne in [3] the related concept of logical session and Jones et al.
in [19] de ne the concept of mission, as  a related set of information needs, resulting in one or more goals , where a goal is  an atomic information need, resulting in one or more queries .
To illustrate, Jones et al. give, as example of a sequence of queries that can be mapped into a same mission, the following set of consecutive queries: (brake pads, auto repair, auto body shop, batteries, car batteries, buy car battery online).
An interesting distinction between chains and missions is that chains deal with the actual sequence of queries, while missions refer to the underlying information needs.
Following this distinction, we de ne here the notions of research session and associated research mission.
A research session belongs to the same space as a chain and represents a set of queries (and associated information as provided by query logs) that ful lls certain constraints.
A research mission, in the same space as Jones  missions, is a set of related and complex information needs.
Such complexity is re ected by the user s engagement such as the time spent or the number of atomic tasks accomplished by the user in the attempt of ful lling these needs.
Thus, a research session is de ned as the set of all the user activities (queries and clicks) needed to ful ll a research mission.
We introduce these two de nitions to insist on the fact that research sessions do not follow some arbitrary timeout rules, as sessions typically do via the previously mentioned t  threshold.
Indeed, as research sessions re ect research missions, they can span over a number of hours if not days.
To illustrate, consider our previous example of a user who is planning a trip to Barcelona and searches for cheap plane tickets and accommodations.
She will typically pursue this task over a period of several days.
Consequently the queries composing a research session do not need to be consecutive.
Following the previous example, our user might search for plane tickets then search for reviews and show times of a few movies before going to the theater the same evening.
She will then return to her trip planning the next day, searching for a hotel in Barcelona.
Thus, a general session bounded by a given t  may contain queries re ecting several distinct and a research session may contain queries originating from several distinct sessions, e.g., one session on a day, and another on the following day.
As they are not limited in time, research sessions use other signals to identify a shared research mission, namely topical coherence and user s engagement.
To re ect topical coherence, we use the two functions f and s as de ned below.
First, given a query q   Q and a set of topics T , we use a function f : Q   T that maps each query q in the query log L to a topic p. In the extreme case, f can simply be the identity function, no topical association is provided, and we will not be able to identify that the queries auto repair and car batteries, as per our previous example, belong to the same research session.
We will verify in the evaluation section that an adequate choice of f does increase recall and thus recommend picking a function f that can be computed e ciently at the scale of Web tra c.
A number of options exists to de ne f , depending on the desired level of precision, one possible solution being to use Wikipedia as source of world knowledge as done in [13].
Most popular Web search engines do have at their disposal such functions that they use to improve recall for sponsored search [12] and this what we recommend using in our case.
In addition, we use a similarity measure among topics in T , that is a function s : T   T   [0, 1].
Again in the extreme case of f being the identity function, a simple similarity function can be the normalized ratio of common words or stems between two queries.
In that case, in our previous example of  batteries  and  car batteries , the measure of similarity will be 0.5.
More sophisticated techniques as proposed by [13] can be used.
To re ect user s engagement, we augment the basic hq, u, ti query/user/time session tuple with the click information stored in query logs, namely the set of clicked results C.
We use C or rather the size of C, as will be seen below, as a strong signal of the complexity of a research mission.
The larger |C|, the more probable it is that the user did not see her needs satis ed and is in the unsatis ed/partially satis- ed category and therefore continues exploring.
Intuitively, long sessions with a su cient number of related queries, for which the user exhibits a stronger engagement in terms of actions performed, that is, with a high |C|, are good candidates for research sessions.
A more sophisticated model that would allow to better distinguish between unsatis ed and partially satis ed needs would be to work at a  ner level of granularity and verify that some of the clicks are  good clicks  in the sense that the user spent enough time reading the selected results and came back to the session to continue exploring.
We reserve this  ner analysis for future work and limit ourselves  rst to the simple observation of the number of clicks.
Thus, we formally de ne a research session R as a maximal order sequence R = (cid:10) hqi1 , ui1 , ti1 , Ci1 i, .
.
.
, hqik , uik , tik , Cik i (cid:11), where, for given thresholds s , k  and c , we have:
 2.
 l, j   {i1, .
.
.
, ik} s(f (ql), f (qj))   s ;

 j=1 |Cij |   c .
The second condition re ects topical coherence, while the third and fourth ones re ect user s engagement, via the total number of issued queries and the total number of clicked results.
In the following section, we present our solution for automatically detecting research sessions and thus identifying research missions.
The Search Pad application previously introduced makes research missions persistent by storing in a pad relevant information pertaining to a given research session.
As such, it is the ideal application for us to verify that research missions can be e ciently and e ectively identi ed, either for direct usage as described in Section 2 or future research.
Our contribution to Search Pad was its Triggering System, whose goal is precisely to trigger Search Pad whenever a research mission is identi ed.
The rest of this section describes the architecture and internals of this system.
As discussed in the previous session, Search Pad must be triggered when the user is involved in a research mission, which is a long, complex sequence of searches that are topically coherent.
Therefore the Triggering System must focus on signals such as session length, user s engagement and topical coherence.
Moreover, some query topics are more likely to be involved in research missions, such as, for instance, travel, health, or job search.
Keeping in mind these consideration, we built the Triggering System of Search Pad as a two-level classi er, whose architecture in depicted in Figure 4.
Figure 4: The core of the Triggering System.
The system is built around three base models, which are each responsible for di erent tasks, and a meta-classi er, called hereafter the  mixer , that receives the signals arriving from the base classi ers and combines them in order to make the  nal decision of whether to trigger Search Pad or not.
Given the scale of deployment of a system like Search Pad, it was necessary to allow administrators to react fast and tune, boost or disable certain features in case of poor unexpected behavior.
Consequently we opted for a two-tier architecture that gives control to the Search Pad administrators without requiring deep understanding of the internal operations.
classi ers remain hidden in a black-box mode, and only the semantics of the signal they produce need to be understood.
In addition, the output of the mixer is a probability value that expresses a con dence level.
Search Pad is triggered if this probability is higher than a given threshold.
Consequently, depending on ongoing marketing or assessment studies, the administrators can decide to favor precision over recall or conversely, without requiring any retraining or deep understanding of the base, simply by changing this triggering threshold.
Finally, the mixer combines the signals of the various clas-si ers via a simple interpretable formula.
The in uence of each classi er can thus be changed by the system administrator, and additional rules can be manually added so as to change the triggering behavior.
An example of the rules that can be embedded in the decision mechanism are rules that promote or demote certain topics based on the  Black Lists  and the  Boost Lists .
These lists consist respectively of topic categories for which Search Pad must not be triggered, (because the topics might be o ensive to users for instance) or for which Search Pad can trigger even with weaker signals (because they are more likely to relate to research missions such as health topics for instance).
While it is not recommended to abuse the system and arbitrarily change the di erent parameters and thresholds, this  exibility is necessary as mentioned before in order to adapt quickly to market needs and changes.
The Mixer.
As discussed above, the mixer receives the signals from the three base classi ers.
In particular, for a sequence of two consecutive queries q1, q2 the mixer receives the following signals:   research(q1, q2).
This signal indicates whether the two queries are part of a complex research mission or not.
The mixer also receives as auxiliary signal the con -dence of such prediction.
  same mission(q1, q2).
This signal indicates whether the two queries are topically coherent and thus susceptible to be part of the same mission.
Also in this case, the mixer receives as auxiliary signal the con -dence of the prediction.
  f (q1) and f (q2) provide the topics of q1 and q2, and s(f (q1), f (q2)) estimates the similarity between these two topics, as per the notation introduced in Section 3.
The users  engagement in research missions is estimated by the total number of queries issued in the session (among other variables).
For this system, we  xed the number of queries (k@) to 3, as a rough heuristic for having a long enough session to do our analysis.
Hence, the signals produced by the base classi ers for the last 3 queries are kept in a  Session State  and used by the mixer to carry its decision.
Based on all these signals, a formula is learned by means of logistic regression [31].
The formula returns a probability p such that the higher p the more likely it is the current session be a research session.
The mixer produces its  nal recommendation, namely trigger/do not trigger, based on the value of T , the triggering threshold, where T   [0, 1], B a boosting factor, whose value belongs to the interval [1, 5].
The decision is made according to the following set of rules:   if the topics of the research session belong to the previously mentioned topics black list, do not trigger.
  else if the topics of the research session belong to the previously mentioned topics boost list and p   T /B (the larger the value of B, the easier the triggering for those topics becomes), then trigger   else if p   T , then trigger.
We next describe the three base classi ers and their signals.
Research Detector and Mission Detector.
The Research Detector component is in charge of the classi cation task behind the signal research(q1, q2) described above.
The Mission Detector component is in charge of the classi cation task behind the signal same mission(q1, q2) also described above.
Both the Research Detector and the Mission Detector components are boosted decision trees classi ers [31], trained on the same large set of data (approximately 40K pairs of consecutive queries) that we gathered as follows.
We sampled several day-sessions from a subset of Yahoo!
Search users during a week toward the end of 2008.
The sampling was strati ed over the days of the week, so as not to over-represent any particular day of the week.
The time period was long enough to capture extended search patterns for some users, exceeding typical 30-minute timeouts, and allowing for missions to extend over multiple days.
A group of annotators were instructed to exhaustively examine each session and to annotate each pair of consecutive queries q1, q2 and with the labels research(q1, q2) and same mission(q1, q2).
The annotators inspected the entire search results page for each query, including URLs, page titles, relevant snippets, and features such as spelling and other query suggestions.
They were also shown clicks to aid them in their judgments.
Both classi ers used a set of 30 features that we computed for each pair of consecutive queries q1, q2.
Many of these features were shown to be e ective for query segmentation [17, 18, 19] and can be summarized as follows:   Textual features.
We compute the textual similarity of queries q1 and q2 using various similarity measures, including cosine similarity, Jaccard coe cient, and size of intersection.
Those measures are computed on sets of stemmed words and on character-level 3-grams.
  Session features.
We compute the number of clicks and queries in the current session, the number of queries since last click, number of clicks since last queries, etc.
  Time-related features.
We compute the time difference between q1 and q2, the sum of reciprocals of time di erence for the pair q1, q2, total session time, etc.
The prediction task of the Mission Detector is quite easy, while the one of the Research Detector is hard.
Indeed, the Mission Detector achieves a very high accuracy, approximately close to the 95%, while the Research Detector exhibits an accuracy around 75%.
The most predictive features for the mission boundaries detection are textual features, among which size of the intersection on character-level words.
Also temporal features play an important role: the closer q1 and q2 are in time, the more likely is that they are part of the same mission.
For the research detection, the most relevant features are the session-based ones.
In particular, the number of clicks and number of queries since the beginning of the session.
Also the length of the queries is a predictive feature: intuitively, a longer query is likely to be a complex query.
Topic Classi er.
The last component, the Topic Classi- er, is responsible for the signals f (q1) and f (q2).
For this purpose, we reused an existing in-house Yahoo!
tool as a black box.
As mentioned before, most Web search engines have built a similar component for various usages.
For each query q it returns a topic category taken from taxonomy of 1026 categories hierarchically organized in a tree with maximum depth 7.
Therefore, the similarity among topics, s(f (q1), f (q2)), is de ned as a distance on the tree.
The boosting list described above was created observing the most likely topics returned by this classi er for queries belonging to research missions.
We limited the topics in the boosting list to the  rst two levels of the category tree.
For the training and the evaluation, we took advantage of the editorial services internally available at Yahoo!, and
 from a pool of 10,000, each session gathering all the queries issued by a same user over a period of 3 days.
The editors were instructed to decorate each query with one of the following labels (1) research, (2) maybe research (3) not research (4) adult and (5) can t tell.
In the case of a  research  label, since research sequences may not necessarily be contiguous, the editors were asked to qualify the label with a counter, namely  research 1 ,  research 2 , etc.
in order to make clear which queries belong to which research session.
Finally the editors were given qualitative guidelines to help them decide what a research mission is.
It was critical for us to verify that underlying research missions can be qualitatively identi ed so as for us to devise techniques to quantitatively identify the associated research sessions, which su ciently good approximation.
The guidelines were mostly given  by example  to insist on the qualitative aspects.
They included actual examples of research missions and some of their representative queries, some less intuitive than the usual academic, travel or health examples we previously gave, such as:   shopping research, e.g.,  I need to  nd the best deal on HDTVs .
  political research e.g.,  who should I vote for?  or  how do I  nally win that argument with my dad about global warming? .
  local research e.g.,  I m trying to choose a karate dojo for my kids  or  what are the best sushi bars in town? .
  how-to research e.g.,  I m learning how to play the guitar  or  I m collecting recipes for the big pie bake-o  next month .
Additional research/not research instances of variations of similar queries were also provided in order to help the editors distinguish between them at a  ner grain.
Such examples included:   Not research: a user looking for the correct spelling of  iambic pentameter , since a single fact is su cient to completely satisfy the need   Research: a user collecting spelling information on a variety of verse forms, e.g.,  trochaic hexameter .
  Not research: a user looking for the score of a particular football game, since this score is  nal and will not change   Research: a user collecting the latest stats on the players in their fantasy football team, since these stats will change, and the user will have to conduct similar research again In order to distinguish between these  ne cases, the editors had to go beyond the isolated query expressions and do their best at  guessing  the user s actual needs.
This huge editorial e ort allowed us to verify that research sessions were signi cant enough in Web search tra c to justify special attention.
The result of this study was that on average 10% of sessions qualify as research sessions as an underlying research mission could be identi ed for them by our editors.
We then computed the numbers of queries that occurred during these research sessions as compared to the overall number of queries in all sessions and found out that were responsible for more than 25% of query volume.
This result alone was in our view su cient to motivate further study of research missions.
The  rst type of experiments we had to conduct was to verify that Search Pad would trigger without signi cantly a ecting latency at the Web scale tra c.
We conducted in March and April 2009, two  bucket experiments , over 1% and 3% of Yahoo!
search tra c to this e ect.
We veri ed in both cases that the incurred average latency did not go over 12ms and was thus acceptable from a user s perspective.
We also measured during these experiments that the average number of queries in a research session amounted to 5 queries.
We did not rely on the triggering prompt as validation of search sessions as there is a discoverability issue with such artifacts.
Indeed we noticed that the Click-Through Rate (commonly referred to as CTR) remained constant independently of the coverage and this signaled to us that more e orts need to be invested to make the prompt more discoverable.
However, an encouraging  gure is that we see the number of users steadily increasing.
We also conducted a few live experiments in order to set the a priori value of some of the thresholds we previously introduced and that were added to make the system easily tunable to market needs.
We run the system o line varying the value of the threshold T in [0.1, 0.2, ..., 0.8, 0.9].
For each value of T we count the number of triggering events.
Following the standard de nitions in Information Retrieval, we then measured the precision and recall of the system (in Figures 5 and 6).
In particular the precision is the number of triggering events correspondent to research sessions divided by the total number of triggering events.
The recall is the number of triggering events correspondent to research sessions divided by the total number of research sessions.
April, 23th Mar, 26th


 Prompt Coverage

 Table 1: Coverage for di erent values of T quire complex and lengthy interaction with search engines.
Research session are excerpts of query log sessions that exhibit certain signals indicating that users are undertaking a research mission.
It has been empirically veri ed through the manual analysis of more than 7000 users  sessions, covering 3-day long activities each, that about 10% of all users  sessions are associated with an underlying research mission, and even more interestingly, that they were responsible for more than 25% of the query volume.
A distinctive characteristic of such research missions is that they are complex and that in many cases, users will return to them over a period of time collecting information that will help them research the topic at hand.
The current ephemeral nature of search sessions in Web search engines does not provide any direct support for this type of activities.
As this under-addressed need was identi ed, a novel application was developed, Search Pad, that automatically gathers notes about results visited during such sessions.
One unique feature of Search Pad is that it is visibly triggered at run time when the search engine detects that the user is undertaking a research mission.
We explained in this paper how we devised an approach for automatically detecting research sessions, and how we embodied it in the triggering component of the Search Pad application.
A few months after the initial launch of Search Pad, we have now su cient data to verify that detecting research sessions at run time at the scale of Web tra c is feasible and that the quality of our triggering is su cient for users to continue using and saving Search Pad documents.
We believe that identifying research sessions for the simple purpose of making them persistent is only a very  rst, yet critical step, in the exploitation of research missions.
We plan in the future to continue the studies on research missions in several directions.
First, we would like to continue improving Search Pad in various manners.
We would like to re ne our understanding of research missions, and increase our recall and precision measures, by  ner analysis of users  activities.
An immediate improvement, which was mentioned earlier, would be not to measure user s engagement by the simple number of results visited for each query, but by a compound score that would also take into account the time spent by users on these results (too short a time would indicate irrelevance, too long a time that would indicate abandonment of the task) in order to signal that the result contain relevant information (in an implicit relevant feedback spirit).
A better analysis of Search Pad session data would also help us understand the more common usage patterns and thus allow us not only to improve its user experience, but automate the tuning of certain parameters such as the boosting and triggering thresholds, as well as the content of the Boost and Black Lists.
In addition, we would like to study the exploitation of research missions in other contexts and other search activities.
Figure 5: Precision in function of threshold T Figure 6: Recall in function of threshold T These plots allowed us change the threshold in function of the desired recall and precision ratios in given markets.
More responsive and early-adopter markets, such as Taiwan for instance, will tolerate higher recall at the cost of lower precision.
In Table 1, we report the coverage, de ned as the total number of prompts over the total number of searches, for two di erent values of the threshold T .
We measured the coverage over two di erent datasets collected by sampling 3% and 1% of the Yahoo!
search tra c during the same March, and April 2009 bucket experiments.
In the  rst case, we set T = 0.5 and observed a coverage of 11.5%, while in the second case, we set T = 0.6 with a corresponding coverage of 6%.
A threshold set at 0.6% thus represents the most conservative choice: it guarantees a reasonable precision but a very low recall, given our ground truth estimation that 10% of sessions are research sessions.
Coverage is obviously increased when using a boosting list that favors speci c topics.
In-house user behavior studies identi ed 511 categories, i.e., at level 2 in the hierarchical tree classi cation generated by the topical classi er module.
These subcategories belong to the following 11 main classes: Automotive, Consumer Packaged Goods, Finance, Health Pharma, Issues and Causes, Life Stages, Miscellaneous, Retail, Small Business and B2B, Sport, Technology and Travel.
To evaluate the increase in coverage due to the boost list, we simulated o line the behavior of the triggering component over 6, 418 consecutive query triples and we counted how many of the triggered events are indeed determined by the boosting parameter B.
Our simulation showed that 393 prompt events over a total of 2, 764, that is, a relative increase of 14%, can be credited to the boosting list.
We have de ned in this paper the concepts of research missions and associated research sessions.
Research missions represent a certain type of information needs that re-improvement of search results for queries that are identi ed as part of a search mission.
These queries are typically  hard queries  in the sense that the search engine can verify that the user s information needs were not immediately satis ed.
We could learn from common patterns in research sessions over a large population of users.
By verifying that a query q4 has a very high probability to follow queries, q1, q2 and q3, as part of a same research mission, we could for instance, if given su cient evidence, start bringing good results from q4 already at the q3 stage even before the user had a chance to issue q4.
We believe that, in general, considering research sessions as an entity, as opposed to an atomic query, could provide tremendous value in many aspects of ranking.
We would like to thank for their valuable suggestions and help many colleagues at Yahoo!
(in alphabetical order): Rob Aseron, Ricado Baeza-Yates, Carlos Castillo, Marcus Chan, Vivian Li Dufour, Sarah Ellinger, Ashley Hall, Isabelle Peyrichoux, Flavian Vasile and Shen Hong Zhu.
In addition, we would like to acknowledge the tremendous e ort done by the entire Search Pad team at Yahoo!
  the application would not have launched without their collective e ort   and  nally, our users, who by their clicks and queries make our research possible.
