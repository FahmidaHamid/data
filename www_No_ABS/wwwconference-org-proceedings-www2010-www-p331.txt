An effective portal web search engine must satisfy a wide range of queries.
At a high level, these include, for example, navigational queries (e.g.
 yahoo ) and transactional queries (e.g.
 red shoes ) [5].
However, a portal s query population can be further segmented into more granular categories.
Each query class may require a different ranking strategy and an effective system will support a broad set of query classes.
Recency sensitive queries refer to queries where the user expects documents which are both topically relevant as well as fresh.
For Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
example, consider the occurrence of some natural disaster such as an earthquake.
A user interested in this topic probably wants to  nd documents which are both relevant (e.g.
discuss aspects of the earthquake) and timely (e.g.
discuss very recent information).
A web search engine must effectively handle time sensitive queries because failures can be more severe for this class than for other query classes.
This is case for two reasons.
First, time sensitive queries are more likely to suffer from the zero recall problem, the failure to index any relevant documents for a query.
This results from the fact that these queries often refer to events for which documents have been lightly published.
Zero recall queries are detrimental because no amount of effort through reformulation or scanning can expose the user to relevant content.
Second, even when relevant content exists in the index, a user s need for relevant content is immediate.
She may be less willing to invest the effort to search for relevant content through reformulation.
We hypothesize that information from a micro-blogging site can be exploited to improve search engine performance for recency sensitive queries.
Micro-blogging refers to a web publishing system where posts are severely constrained in size.
These constraints allow rapid publishing from a variety of interfaces (e.g.
laptops, SMS) and encourage low-cost, realtime updates and links on developing topics.
In our experiments, we use data from Twitter, a micro-blogging site where posts consist of no more than 140 characters.
These posts are generated frequently and include a rich linking structure.1 In this paper, we provide empirical evidence supporting the following claims,
 likely to be relevant to recency sensitive queries.
text representations.
ranking.
We test these claims using a real portal web search engine and evaluate performance using queries submitted to this engine.
nition of micro-blogging.
These include instant messaging status messages and social network status updates.
Most of the prior work on Twitter and ranking deals with ranking individual tweets.
Although Twitter maintains a specialized search engine (http://search.twitter.com/), there exist several vertical search engines which index content across realtime data (e.g.
blogs, collaborative bookmarking sites)2.
While these search engines are able to return very fresh documents, they can often suffer from coverage or ranking issues.
For example, consider a breaking-news query.
Ranked tweets consist of very brief comments on the news topic from random Twitter users.
While such content may satisfy some searchers, other searchers may desire a more sophisticated ranking algorithm incorporating authority or integrating content outside of Twitter.
As an alternative, a portal web search engine may decide to integrate Twitter content into general web search results.
However, for these queries, Twitter content might obscure more relevant documents.
Furthermore, this content also may hurt the user experience for those who are not familiar with Twitter; this is especially true for general web search users.
Besides ranking individual tweets, Bing Twitter Search 3 also provides search results for the URLs referred to by Twitter users.
The contents of this dedicated search website are all extracted from Twitter data.
Our approach is to surface the URLs posted to Twitter on a general search results page.
Even though we use the Twitter content in order to perform this blending, we never expose the user to this content.
In this way, we can be more con dent that the results are both high quality and comprehensive.
Twitter as a research topic has been investigated by researchers in social network anaslysis.
Java et al. [18] study the topological and geographical properties of Twitter social network, and  nd that people use Twitter to talk about their daily activities and to seek or share information.
More importantly, their analysis shows that users with similar intentions connect with each other.
Huberman et al. [14] use Twitter data to con rm that users  attentions limit the number of people with whom they interact in a social network.
Hughes et al. [15] examine Twitter usage as a result of an unexpected event.
Compared to general Twitter behavior, they  nd that Twitter messages sent during unexpected events contain more information broadcasting.
Jansen et al. [16] investigate Twitter as a form of sharing consumer opinions concerning brands, and found that discuss the implications for corporations using microblogging as part of their overall marketing strategy.
Krishnamurthy et al. [21] identify distinct classes of Twitter users and their behaviors, geographic growth patterns and current size of the network, and compare crawl results obtained under rate limiting constraints.
Shamma et al. [25] compare Twitter messages in the context of live media events.
They  nd that analysis of Twitter usage patterns around this media event can yield signi cant insights into the semantic structure and content of the media object.
Twitter has also been studied in the context of education [3, 10], communication [27] and collaboration [13].
We use Twitter in order to address recency sensitive queries.
Previous work has focused on detecting recency sensitive queries in the context of selectively displaying news articles [8, 20].
As we 2http://collecta.com/ http://www.oneriot.com/ http://www.yourversion.com/ 3http://bing.com/twitter will see in Section 3, this approach has several shortcomings which would be addressed by incorporating recency into the general web search results.
In this respect, our work builds on our prior results work in recency ranking [9].
We extend this work by using Twitter to quickly update our index and generate new features.
Many existing search engines address recency sensitive queries by selectively integrating content from a specialized news index [8, 20].
The integration process usually comes in the form of a small display containing a small number of documents from the news index.
Unfortunately, relying on this approach alone suffers from several problems,   For some recency sensitive queries, relevant documents are not news documents but web pages.
  A relevant document may be published on a low-priority news site.
  Even when relevant news content exists, the web search results may remain stale.
In order to provide for a consistently relevant experience for all users, we believe that addressing the recency ranking should focus on web results directly.
General web search algorithms incorporate a wide variety of signals when computing the rank of a document.
These include query match signals (e.g.
how often query words appear in different sections) as well as query-independent signals (e.g.
PageRank, popularity, aggregated clicks).
Unfortunately, many of these latter features may not be accurately represented in fresh documents.
Therefore, even when relevant documents have been indexed, several issues might exist which prevent effective retrieval for time sensitive queries.
For example,   Fresh documents may have very few if any in-links, affecting link-based authority metrics.
  Fresh documents may have very few if any clicks, affecting click-based authority metrics.
Coping with poor link information can be problematic because it would require a second tier of crawling to monitor links to fresh content.
Coping with poor click information requires the search to surface the fresh documents; this is precisely the problem we are addressing.
In the context of recency ranking, micro-blogging data can address challenges in recency ranking.
First, micro-blogging links include both news and non-news URLs.
This allows us to gather information about relevant non-news documents and improve the web results.
Second, micro-blogging links are posted according to users  diverse and dynamic browsing priorities, as opposed to a crawl policy attempting to predict that priority.
Third, the social network unique to our micro-blogging data provides a method for computing authority for fresh documents.
Furthermore, a micro-blogged URLs often include meta-data such as the messages containing this URL.
This meta-data can be used to compute ranking features likely to be correlated with relevance.
Our system naturally breaks down into two parts: crawling and ranking.
Crawling micro-blogged URLs requires addressing spam boosting from weak learners; GBrank [28] uses gradient boosting with decision tree; RankNet [6] uses gradient boosting with neural network.
A typical learning-to-rank framework must be trained using some editorially labeled data.
This is accomplished by sampling a set of query-url pairs for human judgment.
Each of these query-url pairs is given a grade based on the degree of relevance (e.g.
bad match, excellent match, etc).
Each query-url pair is then represented by a feature vector consisting of variables indicating query term matches, link-structure-based features of the document (e.g.
PageRank), and click-based features of the document.
Using the features of each document as well as the editorially labeled training data, a machine learned ranking system can predict effective rankings for queries unseen in the training data.
One of the most important aspects of a learning-to-rank system is the feature set.
We differentiate between the types of features we use in our work.
Content features refer to those features which are functions of the content of the document (e.g.
query term matches, proximity between query terms).
We expect these features to behave normally for fresh URLs.
Aggregate features refer to those features which represent a document s long term popularity and usage (e.g.
in-link statistics, PageRank, clicks).
We expect that these features will be poorly represented for fresh URLs.
Twitter features refer to those features which are related to tweets containing a URL pointing to the document (e.g.
tweet content, number of tweets containing the URL).
We expect that these features will be poorly represented and often nonexistent for those documents not crawled from the Twitter feed.
In the subsequent subsections, we will describe our Twitter features in detail.
We will also describe a special training method required for recency sensitive queries.
Content and aggregate features are thoroughly described in [1, 23, 28].
Twitter is comprised of two parts: a publishing system and a subscription system.
Data about the publishing system (i.e. what are users posting) allows us to detect new URLs.
The publishing system also provides textual information associated with these new URLs.
Data about the subscription system (i.e. which users are following which other users) allows us to measure the qualities of the new URLs.
This is only possible because Twitter subscriptions are exposed to the public.
A URL posted to twitter can be associated with the text surrounding it.
Figure 1 depicts a set of tweets from users about a common tiny URL.
The text in tweets accompanying the tiny URL can provide useful additional information.
Assume we have m tweets and w URLs.
Let M be the m w binary matrix represent the occurrence of a URL in a tweet.
Assume we have observed v words in all tweets.
We de ne the m   v matrix D so that Dij represents the number of times a tweet i contains a term j.
In practice, we remove stop words from our vocabulary.
We can construct a term vector for a URL, j, as,
 uT j = MijDi  (1) i where Di  represents row i of D. This represents a URL by the combination of tweet contents.
A query can also be represented as the v   1 vector, q, of term occurrences.
These representations allow us to use text similarity features in order to predict URL relevance.
For example, we can use the cosine similarity between Figure 1: A typical tweet message accompanying a tiny URL.
This speci c tiny URL is tweeted by  ve unique users with their respective tweet messages.
Such tweet messages are indicative of the content of the tiny URL.
The users  photos and names are mosaicked to protect privacy.
and non-recency URLs (Section 4).
Ranking micro-blogged URLs requires incorporating these documents into a larger web ranking system (Section 5).
In the subsequent sections, we will address each of these tasks in the context of Twitter data.
There are several disadvantages to na vely crawling all Twitter URLs.
The URLs posted by Twitter users include a significant amount of links to spam, adult and self-promotion pages, which probably should not be included in results for time sensitive queries.
Furthermore, realtime crawling and indexing of all Twitter URLs would require considerable overhead.
We employ simple heuristics to  lter out most of the undesired Twitter URLs.
In order to address spam and self-promotion URLs, we discard URLs referred to by the same Twitter user more than two times.
Furthermore, we discard URLs only referred to by one Twitter user.
This removes spurious URLs from our crawl.
We did an experiment to study the effect of these  ltering rules.
During the period of 15:00 20:00 (UTC) on September 9th, 2009, there are a total of 1,069,309 URLS that were posted in Twitter.
After applying the  rst rule, 713,178 URLs remained, 66.7% of the original URLs.
In other words, 33.3% of the URLs are  ltered out as they are very likely to be spam, adult or self-promotion pages.
After applying the second rule, 63,184 URLs remained, only 5.9% of the original URLs.
We also manually inspected a sample of discarded URLs and found that most of them were undesirable.
Machine-learned ranking, the underlying algorithm for many web search engines, refers to the automatic construction of a ranking function which optimizes retrieval performance metrics [6,7,11,19,
 and consistent gains over many manually tuned rankers for a variety of data sets.
Optimization usually is formulated as learning a ranking function from preference data in order to minimize a loss function (e.g., the number of incorrectly ordered documents pairs in the training data).
Different algorithms view the preference learning problem from different perspectives.
For example, RankSVM [19] similarity between a URL and a query.
For a URL, j, the cosine similarity feature is de ned as,  j cosine = uT j q (cid:107)uj(cid:107)2(cid:107)q(cid:107)2 (2) By design, tweets are very short pieces of text and therefore are susceptible to problems when applying classic text ranking methods [24].
For example, unmatched terms should be more severely penalized than they are in cosine similarity.
For this reason, we also inspect the term overlap as another textual feature.
Let  D be the binary version of D (i.e.  Dij = 1 if Dij > 0;  D = 0 otherwise).
De ne  q similarly.
The term overlap between a query and the text of a tweet can be represented as,  iq = (  Di )T q iq = (cid:107)  Di (cid:107)1    iq  iq = (cid:107)q(cid:107)1    iq overlapping terms extra terms missing terms were (cid:107)x(cid:107)1 is the (cid:96)1 norm of x.
For a candidate URL j, the unit match feature is de ned as,  j unit =
 (cid:107)q(cid:107)1   iq  iq iqMij (3) The parameters   and   control the importance of extra and missing terms.
In our experiments, parameters   and   are set to be 0.5 and 0.65 respectively, based on earlier experience with this parameter in a popular search engine.
Finally, we also include a simple exact match feature.
The feature counts the number of tweet messages in which all query tokens appear contiguously, and in the same order, mX i=1  other-1  other-2  other-3  other-4  other-5  other-6  other-7  other-8  other-9  other-10  other-11  other-12  other-13  other-14  other-15  other-16  other-17  other-18  other-19 average number of followers for the users who issued the tiny
 average post number for the users who issued the tiny URL average number of users who retweeted the tweets containing the tiny URL average number of users who replied those users that issued the tiny URL average number of followings for the users who issued the tiny URL average Twitter score of all the users who issued the tiny URL number of followers for the user who  rst issued the tiny URL number of posts by the user who  rst issued the tiny URL number of users who retweeted the user who  rst issued the tiny URL number of users who replied the user who  rst issued the tiny
 number of followings for the user who  rst issued the tiny
 Twitter score of the users who  rst issued the tiny URL number of followers for the user who issued the tiny URL with the highest Twitter score number of posts by the user who issued the tiny URL with the highest Twitter score number of users who retweeted the user who issued the tiny URL and has the highest Twitter score number of users who replied the user who issued the tiny URL and has the highest Twitter score number of followings for the user who has the highest Twitter score among the users that issued the tiny URL Twitter score of the users who issued the tiny URL and who is the highest Twitter score number of different users who sent the tiny URL.
Table 1: Twitter features.
 j exact =
 (cid:107)M j(cid:107)1 mX i=1 phraseMatch(q, i)Mij (4)  j unit  =
 (cid:107)q(cid:107)1   iq  iq iqMij i authority (7) authority-weighted unit match score as mX i=1 where M j returns column j of M and phraseMatch(q, i) return one if there exact phrase q occurs in tweet i.
follows user j.
In practice, we normalize W so thatP
 We adopt the convention of representing user data as a social network where vertices represent twitter users and edges represent the follower relationship between them.
Mathematically, we represent this graph as a u u adjacency matrix, W, where Wij = 1 if user i j Wij = 1.
Given this matrix and an eigensystem, W  =  , the eigenvec-tor,  , associated with the largest eigenvalue,  , provides a natural measure of the centrality of the user [2].
The analog in web search is the PageRank of a document [4].
This eigenvector,  , can be computed using power iteration,  t+1 = ( W + (1    )U) t (5) m .
The interpolation where U is a matrix whose entries are all 1 of W with U ensures that the stationary solution,  , exists.
The interpolation parameter,  , is set to 0.85.
In our experiments, we perform  fteen iterations (i.e.   =  15).
If we assume that a user i posted URL j, we de ne the authority feature of URL j as  j authority =  i (6) We can also use the authority of the user in the computation of our unit match score (Equation 3).
In particular, we de ne the
 In addition to the features described in the preceding sections, we can compute simple aggregate metrics of the URL over a period of time (the details on the period selection are in Section 6.1.1).
We present these features in Table 1.
Some of the features are designed to improve relevance ranking by incorporating Twitter speci c features: user s authority estimation from twitter rank (5), which we also call Twitter user score or Twitter score.
For example, the feature  other-6 is the average Twitter score of all the users who issued the tiny URL.
Over a period of time, there could be many users who issued, replied, or retweeted the tiny URL.
This feature is to calculate the average twitter score of all the users.
The feature  other-12 is the Twitter score of the users who  rst issued the tiny URL.
The features in Table 1 can be grouped into three set.
Features  other-1 other-6 are the average statistics of the users who issued the tiny URL.
Using average statistics can improve feature s robustness and discount any bias on a single user.
Features  other-7    other-12 are the features related to the user who is the  rst that issued the tiny URL.
We assume the authority of the  rst issuer may affect the URL importance.
Features  other-13 other-18 are the features related to the user who issued the tiny URL but has the highest Twitter score.
The user with the highest Twitter score means he/she is the one with the most authority among the users.
In each set, we consider the number of followers, tweets, users being retweeted and replied, and user s Twitter score.
Those features estimate the tiny URL s popularity from different aspects.
The last feature,  other-19, is the number of different users who issued the is.
Algorithm 1 Ranking functions used in the ranking system, including ranking functions for documents represented using content and aggregated features (Mregular), only content features (Mcontent), and twitter and content features (Mtwitter).
D represents data set including query-URL pairs with labeled relevance grades.
F represents feature set.
TRAIN-MLR(D, F) is the ranking function learning algorithm, which is based on the training data set D using feature set F. PREDICT(D,M) scores the data set, D using model M.
TRAIN-MODELS(Dregular, DTwitter) Dregular: training data set from regular data DTwitter: training data set from Twitter data






 The most straightforward method to train a ranking function for Twitter documents is to follow the standard procedure prescribed above: sample query-URL pairs (including both regular URLs and Twitter URLs) and label them, train a ranking function, and apply this function on future queries.
Unfortunately, there are far more regular URLs than Twitter URLs.
Twitter feature values will be missing from the majority amount of regular URLs.
As a result, the machine-learned ranking system will likely ignore these features.
We employ a divide-and-conquer strategy, which fully exploits the available ranking features for regular URLs and Twitter URLs respectively.
As shown in Algorithm 1, for regular URLs, we learn a regular ranking function Mregular based on content features and aggregate features; for Twitter URLs, we learn a Twitter ranking function Mtwitter based on content features and Twitter features.
In addition to these two ranking functions, we also learn a ranking function Mcontent only based on content features.
We train this model for comparison in our experiments.
We use the Gradient Boosted Decision Tree (GBDT) algorithm [12] to learn a ranking function for TRAIN-MLR in Algorithm 1.
GBDT is an additive regression algorithm consisting of an ensemble of trees,  tted to current residuals, gradients of the loss function, in a forward step-wise manner.
It iteratively  ts an additive model as ft(x) = Tt(x;  ) +    tTt(x;  t) t=1 such that certain loss function L(yi, fT (x+i)) is minimized, where Tt(x;  t) is a tree at iteration t, weighted by parameter  t, with a  nite number of parameters,  t and   is the learning rate.
At iteration t, tree Tt(x;  ) is induced to  t the negative gradient by
 Use Mregular on regular and Twitter (Mregular,Mregular) URLs.
Use Mcontent on regular and Twitter (Mcontent,Mcontent) URLs.
Use Mregular on regular URLs and (Mregular,Mcontent) Mcontent on Twitter URLs.
Use Mregular on regular URLs and (Mregular,Mtwitter) Mtwitter on Twitter URLs.
(Mregular,Mcomposite) Use Mregular on regular URLs and Mcomposite on Twitter URLs.
Table 2: Runs used in our experiments least squares.
That is   := argmin 
 i ( Git    tTt(xi);  )2 where Git is the gradient over current prediction function    L(yi, f (xi))   Git =  f (xi) f =ft 1 The optimal weights of trees  t are determined by
  t = argmin  L(yi, ft 1(xi) +  T (xi,  )) i
 To rank the URLs (both regular URLs and Twitter URLs) with the given query, we apply our relevance models to regular URLs and Twitter URLs.
We then rank the URLs by sorting their ranking scores.
Because we always model a relevance grade, the predicted grades are calibrated and comparable.
As a result, we can directly blend regular URLs and Twitter URLs according to their ranking scores.
We study the  ve ranking approaches listed in Table 2.
We adopt the convention of representing algorithms as model tuples where (Mx,My) means  apply Mx to regular URLs and My to Twitter URLs .
Our two baseline approaches apply Twitter-unaware models to all URLs being considered.
The (Mregular,Mregular) baseline can be interpreted as applying a general ranking algorithm to all URLs.
In the cases where URLs lack valid aggregate features, we set their aggregate feature values as default value zeros.
The (Mcontent,Mcontent) baseline indirectly promotes the Twitter URLs focusing the ranking on features shared between both regular and Twitter URLs.
We also consider approaches which apply different models to different URLs.
The (Mregular,Mcontent) run preserves the production ranking for regular URLs but applies a content-only model to Twitter URLs.
We expect this model to leverage the content features learned across the pooled data to rank Twitter URLs.
This behavior may not be present in a model which was trained using both content and aggregate features.
The (Mregular,Mtwitter) run explores the bene t of combining features speci c to Twitter with content features.
One drawback of the Mtwitter model is the relatively small training pool: we expect far fewer example documents with both Twitter and content features de ned compared to those  nal run, (Mregular,Mcomposite), which uses the content model score as a feature for Twitter URLs.
document class time insensitive example document wikipedia entry time sensitive very fresh somewhat fresh somewhat outdated totally outdated very recent news article a day-old news article old news article very old news article
 Our data set of queries and tweets was collected over a few different days.
We use a web index from a large-scale commercial search engine.
Our Twitter stream consists of all tweets from the Twitter Firehose.4 On each day of the study, we collected queries issued to the search engine between 23:00 23:59 UTC.
Recall that we are interested using Twitter only for time-sensitive queries, those queries which expect fresh URLs.
Therefore, we only consider queries which are classi ed as  time-sensitive  queries using an automatic classi er [9].
We constructed two sets of URLs for each day,   regular URLs: in the search engine index during 23:00 23:59,   Twitter URLs: posted by Twitter users during the 9-hour period before the query time (i.e., 14:00 22:59).
The 9-hour period is heuristically determined only for experimental purposes.
This period corresponds to the hours during which Twitter volume is highest.
For each query, we apply simple text-matching rules on Twitter URLs in order to remove non-relevant URLs.
For example, we remove URLs from consideration if there are no query term matches in the body or title.
For the regular URLs, we consider the top ten URLs as decided by the production ranking algorithm of the search engine.
We also consider all Twitter URLs.
Recall that we train the ranking function Mcontent in Section 5.2, which is only based on content ranking features (from document title and body).
For each query, we apply Mcontent to the Twitter URLs and heuristically determine a ranking score threshold: if a Twitter URL has higher ranking score than this threshold, we keep this Twitter URL for the query; otherwise, we discard this Twitter URL.
Therefore, we obtain Twitter URLs with reasonable relevance to the query.
Given these queries, we are interested in labeling the relevance of documents in both sets.
We ask human editors to label each tuple (cid:104)query, URL, tquery(cid:105) with a relevance grade.
We apply  ve judge-ment grades on query-URL pairs: perfect, excellent, good, fair and bad.
For editors to judge the tuple, we ask them to  rst grade it by non-temporal relevance, such as intent, usefulness, content, user interface design, and domain authority.
Because we are interested in time-sensitive queries, we categorize documents according to their temporal properties.
We present the classes we consider in Table 3.
We would like to promote  very fresh  documents and demote  outdated  documents.
Those documents which are  temporally insensitive  or  somewhat fresh  are unlikely to affect the recency of a ranking so we leave those documents in the original order.
We can combine these temporal categories with the relevance judgments using recency demotion [9],   shallow demotion (1-grade demotion): if the result is  somewhat outdated , it should be demoted by one grade (e.g., from excellent to good); 4http://apiwiki.twitter.com/ Table 3: Document classes for time-sensitive queries.
The  very fresh  documents are those which were published on the same day as the query.
  deep demotion (2-grade demotion): if the result is  totally outdated , it should be demoted by two grades (e.g., from excellent to bad).
We collect testing data from the search engine and Twitter stream on the day of October 14th, 2009.
The time-windows and procedures are described in Section 6.1.1.
The testing data consists of
 in which there are unique 392 queries.
For regular query-URL pairs, content features and aggregate features are extracted.
For Twitter query-URL pairs, content features and Twitter features are extracted.
In the experiment, there are 66 content features, 454 aggregate features and 23 Twitter features.
There are two training data sets.
One set is used to train ranking function for regular URLs, another set is to train ranking function for Twitter URLs.
For regular training data set, we collect a large amount of 206,249 query-URL pairs.
Content features and aggregate features are extracted from this training set.
For Twitter training data set, we collect the Twitter data from two days: October 12th, 2009 and October 19th, 2009.
The time-windows and procedures are described in Section 6.1.1.
The data from these two days are combined together, and there are totally
 To make it fair for our experiment evaluations, we remove the queries from this training set that are similar to or same as the queries in the testing set.
After removing these similar (same) queries, the Twitter training data set consists of 5006 query-URL pairs and there are 1800 associated unique queries.
Content features and Twitter features are extracted.
We desire an evaluation metric which supports graded judgments and penalizes errors near the beginning of the ranked list.
In this work, we use discounted cumulative gain (DCG) [17], DCGn = Gi log2(i + 1) i=1 where i is the position in the document list, Gi is the function of relevance grade.
Because the range of DCG values is not consistent across queries, we adopt the normalized discounted cumulative gain (NDCG) as our primary ranking metric, nX nX NDCGn = Zn Gi log2(i + 1) i=1 (8) (9) NDCG of ideal list be 1.
We use NDCG1 and NDCG5 to evaluate the ranking results.
Our recency demotion guidelines con ate relevance and recency.
In order to evaluate freshness in isolation, we also include a freshness metric based on DCG, discounted cumulative freshness (DCF), nX DCFn = (a) relevance grade (demoted) Regular Twitter Perfect Excellent Good




 Fair Bad (b) relevance grade (non-demoted) Perfect Excellent Good Fair




 Bad Fi log2(i + 1) i=1 (10) Regular Twitter where i is the position in the document list, Fi is the freshness label (1 or 0).
A query may have multiple very fresh documents, for example when multiple news sources simultaneously publish updates to some ongoing news story.
Note that DCF is a recency measurement that is independent of overall relevance.
Therefore, when we evaluate a ranking, we should  rst consider demoted NDCG which represents the overall relevance, then inspect the value of the DCF.
We de ne normalized discounted cumulative freshness (NDCF) as in Equation 9.
In our experiments, we use the following freshness criterion: if the main content of a document is created on the same day as query time, this document is labeled as a very fresh document.
Using this criterion, editors can easily and quickly evaluate documents.
For a very small portion of breaking-news queries, it is possible that a document becomes stale only a few hours after its creation because more related documents are created with signi cantly newer contents.
However, the current criterion, in general, appropriately re ects the fresh document distribution for most breaking-news queries.
Before presenting results demonstrating our ranking improvements, we offer some descriptive statistics of our collected data.
Recall that we used an automatic classi er to extract our candidate queries.
We were interested in validating the accuracy of this pool of queries.
For the queries in the testing set, we randomly selected 242 queries and asked editors to judge whether these queries were breaking news queries or not.
Our criterion for breaking-news query was stricter than those used to train the automatic classi- er [9] we use in Section 6.1.1.
Speci cally, we asked editors to label a query as a breaking news query only if there is at least one new document created within the last 24 hours that is relevant to the query.
Our editorial experiment con rmed that 212 (91.7%) of the queries were breaking new queries.
We can also measure the quality of Twitter URLs in aggregate by inspect the freshness and relevance grades of our Twitter and regular URLs.
We present the distribution of grades broken down by source in Table 5.
We observe that the quality of Twitter URLs is better than Regular URLs in sense of both relevance and recency.
Of the Twitter URLs, 53.8% are very fresh documents while, for Regular URLs, this fraction is only 19.4%.
Furthermore, the relevance grade distribution does not change after recency demotion, which means there are no stale documents in Twitter URLs.
This con rms our assumption that the URLs extracted from Twitter data are generally very fresh.
At same time, the overall relevance quality of Twitter URLs is also higher than of regular URLs.
The percentages of perfect and excellent Twitter URLs are higher than those of Regular URLs, while the percentages of fair and bad Twitter URLs are lower than those of regular URLs.
This means Twitter URLs are potentially useful to improve ranking for time sensitive queries.
Finally, while we use our user authority feature,   as a ranking (c) recency label Regular Twitter Non-fresh Fresh

 Table 4: Data distribution in sense of relevance grade and re-cency label.
feature, it is also worth noting that it can be used in isolation to qualitatively inspect a set of users.
We computed   for ten million users and present the top users associated with high values of  i in Table 5.
Though the top users are largely dominated by celebrities, many popular bloggers, and news sources are also surfaced as highly authoritative.
Therefore, we expect that this feature will be valuable when used in conjunction with our other features.
As shown in Table 6, our proposed approach which blends Twitter content into the standard ranked list signi cantly improves ranking in sense of both relevance and recency.
We notice this improvement across all of our metrics.
The baseline approach, (Mregular,Mregular), uses content and aggregate features for both regular and Twitter URLs.
This prevents Twitter URLs from being promoted because Twitter URLs suffer from feature impoverishment.
We expect this behavior given our discussion of the role aggregate features in Section 5.
The content-only approach, (Mcontent,Mcontent), underperforms the baseline approach, (Mregular,Mregular), because it does not use aggregate features.
Nevertheless, as a result, Twitter URLs have no disadvantage when they compete with regular URLs.
The NDCF values are improved which means more fresh documents (i.e., Twitter documents) are promoted to the top ranking results.
However, in sense of relevance represented by NDCG values, there is no improvement because the absence of aggregate features hurts the ranking of regular URLs.
When we consider models which leverage the representational strength of each URL class, performance improves across metrics.
For example, using the content and aggregate features for regular URLs and content features for Twitter URLs, (Mregular,Mcontent), improves both relevance and recency metrics.
If we enrich the representation of the Twitter URLs, (Mregular,Mtwitter), we get the best performance across all metrics.
This means that we were able to successfully incorporate realtime web content without hurting relevance.
In fact, we improve relevance.
Our experiments did not con rm that (Mregular,Mcomposite) leveraged the additional training data from regular URLs for content features.
Our results show that the performance of this algorithm is very similar to using Mtwitter, a model built with much less training data.
Table 7 qualitatively illustrates the behavior of our algorithms.
Compared with the baseline result, (Mregular,Mregular), our Twitter-top performance for that metric.
Top 1 Top 5 (Mregular, Mregular) (Mcontent, Mcontent) (Mregular, Mcontent) (Mregular, Mtwitter) (Mregular, Mcomposite) NDCGnodemote,1 NDCGdemote,1
  3.2%

 +1.8% +18.4% 0.708 +17.9% 0.702





  0.2% 0.513
 +1.2% +13.7% 0.717 +13.0% 0.747
 +7.5% +8.8% +33.8% +36.5% NDCGdemote,5 NDCGnodemote,5



 +9.9% 0.729

 +9.4% 0.723
 +0.3% 0.587 +1.3% 0.569 +6.5% 0.736 +5.8% 0.756
 +11.7% +8.9% +29.6% +31.4% Table 7: An example of ranking recency improvement.
The query is wwe captain lou albano, and the query issue time is during
 rank (rank in (b))




 rank (rank in (a))




 (a) Ranking result by baseline approach (Mregular,Mregular).
http://en.wikipedia.org/wiki/Captain_Lou_Albano http://www.wwe.com/superstars/halloffame/inductees/captainloualbano/ http://www.wrestlingmuseum.com/pages/bios/halloffame/albanobio.html http://www.wwe.com/superstars/halloffame/inductees/captainloualbano/photos/ http://wjz.com/entertainment/captain.lou.albano.2.1248290.html (b) Ranking result by new approach (Mregular,MTwitter).
http://edition.cnn.com/2009/SHOWBIZ/TV/10/14/obit.albano/ http://en.wikipedia.org/wiki/Captain_Lou_Albano http://www.wwe.com/superstars/halloffame/inductees/captainloualbano/ http://www.wwe.com/superstars/halloffame/inductees/captainloualbano/photos/ http://www.wrestling-edge.com/wwe-news/wwe-hall-of-famer-capt-lou-albano-passes-away.html grade good excellent good good excellent grade excellent good excellent good fair fresh no no no no yes fresh yes no no no yes from Twitter no no no yes no from Twitter yes no no yes yes User/Type Twitter Of cial Kim Kardashian Ashton Kutcher Denise Richards Demetria Lovato Katy Perry userID twitter kimkardashian aplusk denise_richards ddlovato katyperry khloekardashian Khloe Kardashian johncmayer astro_mike robdyrdek ...
nasa mcuban wired problogger chrispirillo cbsnews jkottke John Mayer Mike Massimino Rob Dyrdek ...
NASA Space Program Mark Cuban Wired Magazine Darren Rowse Chris Pirillo CBS News Jason Kottke Table 5: Result of Markov chain analysis on the twitter follower graph.
The top half shows the top ten users, most, if not all dominated by celebrities.
However, a select subset from the top hundred users features many news media sites, popular bloggers, and technology authorities.
Table 8: Twitter feature importance list.
The Twitter feature de nitions can be found in Table 1.
Twitter feature  unit  other-17  other-15  other-3  other-1 importance rank importance score









 based algorithm signi cantly promotes relevant and recent content to the top of the ranked list.
Note that in this example, none of the displayed URLs is stale; thus, the recency demotion grades and non-demotion grades are always equal.
We have demonstrated that Twitter features can signi cantly boost the performance of a recency sensitive ranker.
It is worth investigating which Twitter features in particular were highly valued by our model.
As presented in Algorithm 1, the Twitter ranking function, MTwitter, uses both content features and Twitter features.
We can compute the importance of each feature by the method proposed in [12].
We rank features by the descending order of the importance and show the top  ve Twitter features in Table 8.
The feature importance score is on a scale of [0, 100].
The most important Twitter feature is  unit, which is the unit match feature between query and tweet text as de ned in (3).
This means the text similarity between a query and a tweet in general highly correlates with the relevance between the query and the feature is highly complementary to the content ranking features (e.g., text-proximity features based on document title and body) and can be see as a proto- anchor text  for new URLs.
The other important Twitter features include  other-17,  other-15,  other-3 and  other-1, which are described in Table 1.
These features represent the authority and activity of the users that are related to the Twitter URLs from different aspects.
For example,  other-17 is the number of the followings for the user who has the highest twitter user score among the users that issued the Twitter URL.
Our experiments demonstrate that there is a clear advantage to using Twitter both as a source of URLs as well as a source of evidence for fresh URLs.
These results complement existing Twitter ranking results insofar as they demonstrate the ef cacy of a blending approach as opposed to a vertical selection approach [8].
On the other hand, our Twitter features can be further developed with a knowledge from social studies of Twitter (Section 2.2).
There is also an opportunity for more sophisticated spam detection in our work.
The Twitter URLs we used in our study have undergone multiple  ltering steps during both crawling (Section 4) and ranking (Section 6.1.1).
As shown in Table 5, the Twitter URLs in our experiments are both highly relevant and fresh.
We can potentially increase the size of candidate Twitter URLs by relaxing our  ltering rules.
However, as a result, the quality of the candidate Twitter URLs will almost certainly become degrade.
Our model, then, would need to incorporate spam  ltering.
For example, it would need to learn that certain features (e.g.
number of users posting the URL) are even more indicative of a low quality (or spam) document.
Fortunately, our training procedure supports such an approach.
As a search system begins to index Twitter URLs in near real time, spam detection will become increasingly important.
We have also assumed that the set of Twitter URLs is disjoint from the set of regular URLs.
This models a retrieval system as being composed of two parts: a long term index and a realtime index.
The long term index contains content whose freshness is limited by the effectiveness of the crawler  nding and incorporating new data.
The realtime index consists of very fresh content with impoverished representation.
In many cases an overlap will exist.
In our experiments, using a commercial search engine, roughly 10% of the Twitter URLs were already indexed.
To simplify experimentation, we treated these URLs as Twitter URLs.
However, as the long term index begins to accumulate fresher and fresher content for example, through more effective/adaptive crawl policies or superior indexing architectures, the overlap will increase.
As a result, developing models which support Twitter, content, and aggregate feature will be important.
Finally, we have only touched the surface of blended ranking.
In our experiments we combined scores based on relevance alone.
This suffers from a few problems inherent to ranking in general.
For example, this creates a problem for multiple intent queries (e.g.
 election results  could refer to one of several regional elections) or queries which deserve summarization (e.g.
 candidate speeches  may be satis ed by a summary including documents which together discuss several candidates  recent speeches ).
As a result, ranking diversity will be an important area of research.
Traditional diversi- cation approaches focus on content-based similarity of documents from the same index.
It is unclear how these approaches can be extended to rankings which combine content from several indexes.
We have presented preliminary evidence supporting the claim that micro-blogging data can be exploited to improve web ranking for recency sensitive queries.
Our approach is based on preserving the quality of data presented to the general web searcher by only using micro-blog data as evidence for discovering and ranking URLs.
For recency queries, we demonstrated that both relevance-based and freshness-based metrics can be improved with our approach.
More generally, our results demonstrate the power of leveraging widespread user behavior for recency sensitive queries.
Although other sources of user behavior information exist (e.g.
click logs, toolbar data), Twitter is one of the only sources which is both public and widely adopted.
This makes Twitter a valuable source of realtime user behavior for institutions lacking access to more sensitive log data.
In the future, we are interested in improving spam detection, enriching the features we extract from Twitter using regular URL information and results from Section 2.2, and incorporating diversity.
Furthermore, we are interested in synthesizing signals from Twitter streams with other sources of realtime evidence into a cohesive re-cency ranking module.
Finally, if demographic information about Twitter users can be extracted or predicted, then this resource can also be used for conducting personalization experiments.
We thank Alex Smola, Tamas Sarlos, Deepayan Chakrabarti, Ciya Liao and Jyh-Herng Chow for their helpful discussions.
