Travel, as an integral part of human history, has become more and more popular in people's everyday lives in recent years, partly owing to the increasing amount of travel-related information and services on the Web, which provide people with efficient ways to * This work was performed at Microsoft Research Asia.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
online plan and prepare for their trips.
Meanwhile, Web 2.0 technologies facilitate and encourage people to contribute rather than just obtain information, leading to a huge amount of user-generated content (UGC) on the Web.
In the tourism domain, more and more people have willingness to record and share their travel experiences on weblogs, forums or travel communities, in the form of textual travelogues and photos taken during the trips.
Since travel-related UGC not only underlies the communities and social network among travelers but also provides other web users with rich information related to travel, how to leverage it has attracted extensive attention in the literature.
For instance, a lot of work has been proposed to mine knowledge from user-contributed photos on Flickr [6] to support various applications such as landmark discovery and recognition [22], landmark image selection [11][18], location explorer [1][5], and image tag suggestion [15].
By contrast, fewer research efforts have been dedicated to knowledge mining from travelogues.
One related work is [8], in which the authors proposed to generate overviews for locations by mining representative tags from travelogues.
However, to the best of our knowledge, the complete framework of travelogue mining and its applications has not been specially investigated.
We claim that travelogues can serve as a promising resource of travel-related knowledge, which is complementary to user-generated photos because travelogues cover various travel-related aspects, including not only landmarks and natural things which correspond to specific visual descriptions in photos, but also abstract aspects (e.g., history, culture, genius loci) which are informative to tourists but difficult to visualize using photos.
With such rich information, travelogues could support more comprehensive descriptions of locations and comparisons between locations than user-generated photos, and thus could be leveraged to recommend locations according to various queries.
In addition, travelogues contain rich textual contexts of locations to meet various information needs.
For example, representative snippets can be extracted to describe a location s characteristics and linked to original travelogues as detailed context.
Furthermore, in spite of the access to a great deal of structured travel-related information (e.g., vacation packages, flights, hotels) offered by travel websites and travel agents, many people who are planning a trip prefer to learn experience and guidance from other travelers.
Travelogues supplement this structured information with unstructured but personal descriptions of tourist destinations and services.
Although the information in a single travelogue is possibly noisy or biased, numerous travelogues as a whole could reflect people s overall preference and understanding of travel resources, and thus can serve as a reliable knowledge source.
However, acquiring the knowledge in travelogues is a nontrivial task, especially for common users.
Actually, there is a gap between raw travelogues and the information needs of tourists due to the data s intrinsic limitations listed as follows: We went back to the hotel, packed our bags and took the shuttle back to the airport for our flight from Oahu to Kauai  Since we had a few hours to kill before our big sunset dinner cruise, we went and checked out Spouting Horn near Poipu Beach.
It's a pretty cool thing so see because as water rushes under a lava shelf, it spurts through a small opening at the surface.
A very unique sight!
 .
After a few photos, we continued on our drive.
What a scenic ride!
We drove along the eastern coastline and up to Kilauea, Princeville and Hanalei...just beautiful.
We stopped along the way for lots of pictures, one place had at least a dozen chickens in the parking lot...
Oahu, Kauai, Poipu Beach, Kilauea, Princeville   locations surface, water   lava...
coastline, sunset ...
local topics hotel, shuttle ...
airport, flight ...
picture, photo ...
global topics Figure 1.
Different topics in travelogues, where local topics are shown in italic and blue; global topics are shown in green; and locations are shown in underline and red.
x Noisy topics: As other UGC, most travelogues are unstructured and contain much noise.
For instance, the depictions of destinations and attractions, in which the common tourists are most interested, are usually intertwined with topics common in travelogues related to various locations.
x Multiple viewpoints: For each destination, there are usually various descriptions coming from many previous travelers.
When trying to comprehensively know about a destination, users usually confront a dilemma that the viewpoint in each single travelogue may be biased, while it is time-consuming to read and summarize a number of related travelogues to outline an overview of the destination s characteristics.
x Lack of destination recommendation: Although a large collection of travelogues can cover most of popular destinations in the world, the depictions in a single travelogue usually focus on only one or a few destinations.
Hence, for tourists who have particular travel intentions (e.g., go to a beach, go hiking) and need to determine where to go, there is no straightforward and effective way to obtain recommended destinations, except for surveying a lot of travelogues.
x Lack of destination comparison: In travelogues, besides the explicit comparison made by the authors, there is little information about the similarity between destinations, which is helpful for tourists who need suggestions about destinations similar (or dissimilar) to the ones that they are familiar with.
To overcome these limitations of raw travelogue data and bridge the gap to real information needs, several kinds of information processing techniques need to be leveraged.
(1) For the issue of noisy topics, we need to discover topics from travelogues and further distinguish location-related topics with other noisy ones.
(2) For the issue of multiple viewpoints, we need to find a representation of locations that summarizes all the useful descriptions of a location to capture its location-representative knowledge (i.e., local characteristics such as attractions, activities, styles).
(3) To provide destination recommendation, a metric of relevance is necessary to suggest locations most relevant to tourists  travel intentions.
(4) For destination comparison, a location similarity metric is necessary to compare locations from the perspective of travel.
We believe that the first two points should be given primary importance because the location-representative knowledge mined from location-related topics underlies the ranking and similarity measurement of locations.
In this paper, we consider the above issues and investigate the problem of mining location-representative knowledge 1 from a
 but not specific to locations is beyond our objective in this paper.
Location Extraction Travelogue Modeling Location-Representative Knowledge Destination Recommendation Destination Summarization Travelogue Enrichment User Travelogues Knowledge Mining Applications Figure 2.
The overview of the proposed framework, in which location-representative knowledge is first mined from a travelogue corpus, and then used to support three applications.
large amount of travelogues to facilitate tourists to fully utilize such knowledge.
In recent years, probabilistic topic models, such as latent Dirichlet allocation (LDA) [2], have been successfully applied to a variety of text mining tasks [3, 4, 13, 14, 16, 17, 19, 20].
This kind of models are suitable for the task of travelogue mining owing to its powerful capability of discovering latent topics from text and representing documents with such topics.
However, to the best of our knowledge, the existing models are not applicable for our objective because none of them can be used to address the limitations of travelogue data.
Specifically, although documents under these models are represented as mixtures of the discovered latent topics, the entities appearing in the documents (e.g., locations mentioned in travelogues) either lack of representation in the topic space, or are represented as mixtures of all the topics, rather than the topics appropriate to characterize these entities.
Considering the noisy topics in travelogues, the representation of locations using all the topics would be contaminated by the noise and thus is unreliable for further relevance and similarity metrics.
Therefore, we propose a probabilistic topic model, i.e., Location-Topic (LT) model, to discover topics from travelogues and simultaneously represent locations with appropriate topics.
Specifically, we define two different types of topics (as illustrated in Figure 1), i.e., local topics which characterize specific locations from the perspective of travel (e.g., lava, coastline), and global topics (e.g., hotel, airport) which do not characterize certain locations but rather extensively co-occur with various locations in travelogues.
Since each local topic corresponds to some specific locations and travel-related characteristics, a location s overall characteristics can be generally represented in the local topic space, as a mixture of (i.e., a multinomial distribution over) local topics.
Based on the LT model, the aforementioned limitations of travelogue data are handled to some extent because: x By decomposing travelogues into local and global topics, we can obtain location-representative knowledge from local topics, with other semantics captured by global topics filtered out.
x By representing each location using local topics mined from the entire travelogue collection, multiple viewpoints of each location can be naturally summarized.
x Based on the representation of locations in the local topic space, both the relevance of a location to a given travel intention and the similarity between locations can be measured.
As shown in Figure 2, given a collection of travelogues (in the implementation, either of two data sets: 100K English travelogues, or 94K Chinese ones), we first extract the locations mentioned in the text.
Then a LT model is trained on the collection to learn local and global topics, as well as the representation of locations Local Topic m r e
 =     m r e
   Document Global Topic + (1 )   m r e
     n o i t a c o
 i c p o
 Document l a b o
 l (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1856)(cid:4667)(cid:3)(cid:3404)(cid:3)(cid:2019)(cid:3400)(cid:3533) (cid:3533) (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667)(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1856)(cid:4667)
 (cid:1846)(cid:1859)(cid:1864)(cid:1878)(cid:1314)(cid:3404)(cid:883) (cid:3)(cid:3397)(cid:3)(cid:4666)(cid:883)(cid:3398)(cid:2019)(cid:4667)(cid:3400)(cid:3533) (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:1314)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:1314)(cid:513)(cid:1856)(cid:4667)
 i c Location p o
 l a c o
 (cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:1878)(cid:3404)(cid:883) (cid:1838)(cid:1864)(cid:3404)(cid:883) Figure 3.
An illustration of the travelogue decomposition with (I) Tloc local topics and (II) Tgl global topics.
It should be noted that this figure mainly serves as an illustrative interpretation of the ideas, but does not exactly accord with the model details.
in the local topic space.
Based on the learnt knowledge, we can fulfill different application tasks.
Specifically, we consider a scenario where a user learns online knowledge to plan a trip in three steps: 1) selecting a destination from some recommended ones, 2) browsing the characteristics of the selected destination to get an overview, and 3) browsing some travelogues to figure out detailed travel route.
To facilitate these three steps, the following three applications are implemented, respectively: x Destination Recommendation: We recommend destinations to users, in terms of either similarity to a given destination or relevance to a given travel intention.
x Destination Summarization: Each destination is presented as an overview by summarizing its representative aspects with textual tags.
Representative snippets are also offered as further descriptions to verify and interpret the relation between each tag and the destination.
x Travelogue Enrichment: To help a user better browse and understand travelogues, we identify the informative parts of a travelogue and highlight them with related images.
The paper is organized as follows.
In Section 2, we introduce the proposed Location-Topic model.
Then we describe three applications of the LT model in Section 3.
Experimental and evaluation results are shown in Section 4.
Section 5 presents the related work.
In Section 6, we give the conclusion and future work.
In this section, we present the Location-Topic (abbreviated as LT) model and its usage for further applications.
By modeling the generative process of travelogues, the model could discover topics from travelogues and represent locations with the learnt topics.
Following the existing work on probabilistic topic models, we treat each travelogue document as a mixture of topics, where each topic is a multinomial distribution over terms in vocabulary and corresponds to some specific semantics.
As discussed in Section 1, we further assume that travelogues are composed of local and global topics, and each location is represented by a multinomial distribution over local topics.
Thus, the proposed LT model aims at discovering local and global topics, as well as each location s distribution over local topics, from a travelogue collection.
We use Figure 3 to provide an illustrative and intuitive explanation how we decompose travelogue documents into local topics and global topics.
A travelogue collection can be represented by a Term-Document matrix where the (cid:1862)th column encodes the (cid:1862)th document s distribution over terms, as illustrated at the top left of Figure 3.
Based on this representation, our goal is equivalent to decomposing a given Term-Document matrix into multiple matrices, including Term-LocalTopic, Term-GlobalTopic, and Lo-calTopic-Location matrices.
In Figure 3, there are another two matrices that we should learn, i.e., GlobalTopic-Document matrix and Location-Document matrix.
The former is the same as that of common topic models, whereas the latter is specific and important to our objective, and thus need particular discussion.
For Location-Document matrix, we have some observed information, namely the user-provided location labels associated with each travelogue.
However, such document-level labels are not fit for our scenario because they are usually too coarse and incomplete to support knowledge mining for all the locations described in travelogues, and sometimes they are even labeled incorrectly.
Hence, we rely on the locations extracted from text instead of these labels.
There are several methods for location extraction, e.g., looking up a gazetteer, or applying a Web service like Yahoo Placemaker2.
As such pre-processing is not our focus, we employ a beforehand implemented location extractor based on a gazetteer and location disambiguation algorithms handling geographic hierarchy and context of locations, which can achieve high accuracy by considering all the candidate locations in a document simultaneously.
We will detail this location extractor elsewhere.
Intuitively, the extracted locations can serve as strong indications of locations described in travelogues.
However, these extracted locations are improper to be taken as the real Location-Document matrix, due to an observed gap between them and the locations actually described.
For instance, a series of locations may be mentioned only as a trip summary, but without (or with quite unequal) descriptions in the contextual text.
Besides, we also observe that in a typical travelogue, the author usually concentrates on depicting some locations in consecutive sentences.
That is, consecutive words tend to correspond to the same locations.
Considering these observations, we assume that all the words in a text segment (e.g., a document, paragraph, or sentence) share a multinomial distribution over locations, which is affected by a Dirichlet prior derived from the extracted locations in the segment.
In this way, the Loca-tion-Document matrix is kept variable to better model the data, while also benefiting from the extracted locations as priors.
As the decomposition of likelihood (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1856)(cid:4667) shown in Figure 3, each word in a document is assumed to be  written  by making a binary decision between two paths, i.e., (1) selecting a location, a local topic, and a term in sequence, and (2) selecting a global topic and a term in sequence.
Once decomposed as above, a travelogue collection preserves its location-representative knowledge in LocalTopic-Location matrix, and topics in Term-LocalTopic and Term-GlobalTopic matrices.
2 http://developer.yahoo.com/geo/placemaker/ (cid:142) (cid:154) (cid:15) (cid:153) (cid:17)(cid:134) (cid:7) (cid:156) (cid:588)(cid:134)(cid:481)(cid:149) (cid:590)(cid:134)(cid:481)(cid:149) (cid:22)(cid:134) Figure 4.
Graphical representation of the proposed LT model.
words, (2) a binomial distribution over global topics versus local We extend the extensively used bag-of-words assumption to treat nomial distribution over local topics, with symmetric Dirichlet a segment could be a sentence, a paragraph, or a sliding window
 (cid:23)(cid:3)(cid:142)(cid:145)(cid:133)(cid:596)(cid:142)(cid:145)(cid:133) (cid:596)(cid:137)(cid:142) (cid:23)(cid:3)(cid:137)(cid:142) (cid:598)(cid:134)(cid:481)(cid:149) (cid:580)(cid:142)(cid:145)(cid:133) (cid:576) (cid:580)(cid:137)(cid:142) (cid:573) In the LT model, each location (cid:1864) is represented by(cid:3)(cid:2032)(cid:1864) , a multi-prior(cid:3)(cid:2010); each document (cid:1856) is associated with(cid:3)(cid:2016)(cid:1856) , a multinomial distribution over global topics, with symmetric Dirichlet prior(cid:3)(cid:2009).
each document (cid:1856) as a set of (cid:1845)(cid:1856) non-overlapping segments, where in the document.
Each segment (cid:1871) is associated with (1) a bag-of-topics(cid:3)(cid:2024)(cid:1856)(cid:481)(cid:1871), with Beta prior(cid:3)(cid:2011)(cid:3404)(cid:4668)(cid:2011)(cid:1859)(cid:1864)(cid:481)(cid:2011)(cid:1864)(cid:1867)(cid:1855)(cid:4669), and (3) a multinomial distribution (cid:2022)(cid:1856)(cid:481)(cid:1871) over segment(cid:3)(cid:1871) s corresponding location set (cid:2278)(cid:1856)(cid:481)(cid:1871)(cid:1565)(cid:4668)(cid:1864)(cid:513)(cid:1864)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:149)(cid:135)(cid:137)(cid:143)(cid:135)(cid:144)(cid:150)(cid:3)(cid:1871)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4669), with Dirichlet prior parameterized by (cid:2031)(cid:1856)(cid:481)(cid:1871) defined as (cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:1565)(cid:3419)(cid:2012)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:1864)(cid:3404)(cid:2020)(cid:942)(cid:851)(cid:4666)(cid:1864)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:149)(cid:135)(cid:137)(cid:143)(cid:135)(cid:144)(cid:150)(cid:3)(cid:1871)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4667)(cid:3423)(cid:1864)(cid:1488)(cid:2278)(cid:1856)(cid:481)(cid:1871) , where  (cid:851)(cid:4666)(cid:942)(cid:4667)  is short for  the number of times  and coefficient (cid:2020) tion(cid:3)(cid:1829) , which consists of (cid:1830) documents covering (cid:1838) unique locations and (cid:1849) unique terms, is defined as follows: x For each local topic(cid:3)(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:4669), draw a multinomial distribution over terms, (cid:2030)(cid:1878)(cid:1864)(cid:1867)(cid:1855)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2015)(cid:1864)(cid:1867)(cid:1855)(cid:4667).
x For each global topic(cid:3)(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1859)(cid:1864)(cid:4669), draw a multinomial distribution over terms, (cid:2030)(cid:1878)(cid:1859)(cid:1864)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2015)(cid:1859)(cid:1864)(cid:4667).
x For each location(cid:3)(cid:1864)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1838)(cid:4669), draw a multinomial distribution over local topics, (cid:2032)(cid:1864)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2010)(cid:4667).
x For each document (cid:1856)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1830)(cid:4669): (cid:2016)(cid:1856)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2009)(cid:4667).
o For each segment (cid:1871) of document (cid:1856): cal topics, (cid:2024)(cid:1856)(cid:481)(cid:1871)(cid:817)(cid:1828)(cid:1857)(cid:1872)(cid:1853)(cid:4666)(cid:2011)(cid:4667);   draw a multinomial distribution over locations in (cid:1871) , (cid:2022)(cid:1856)(cid:481)(cid:1871)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:3435)(cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:3439).
o For each word (cid:1875)(cid:1856)(cid:481)(cid:1866) in segment (cid:1871) of document (cid:1856):   draw a binary switch (cid:1876)(cid:1856)(cid:481)(cid:1866)(cid:817)(cid:1828)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2024)(cid:1856)(cid:481)(cid:1871)(cid:3439);   if (cid:1876)(cid:1856)(cid:481)(cid:1866)(cid:3404)(cid:1864)(cid:1867)(cid:1855), draw a location (cid:1864)(cid:1856)(cid:481)(cid:1866)(cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2022)(cid:1856)(cid:481)(cid:1871)(cid:3439), and then draw a local topic (cid:1878)(cid:1856)(cid:481)(cid:1866)(cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2032)(cid:1864)(cid:1856)(cid:481)(cid:1866)(cid:3439);   if (cid:1876)(cid:1856)(cid:481)(cid:1866)(cid:3404)(cid:1859)(cid:1864), draw a global topic (cid:1878)(cid:1856)(cid:481)(cid:1866)(cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:4666)(cid:2016)(cid:1856)(cid:4667);   draw word (cid:1875)(cid:1856)(cid:481)(cid:1866)(cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:4672)(cid:2030)(cid:1878)(cid:1856)(cid:481)(cid:1866)(cid:1876)(cid:1856)(cid:481)(cid:1866)(cid:4673).
denotes the precision of the prior.
In the implementation, each paragraph in a travelogue is treated as a raw segment, with further merging to ensure that each segment contains at least one location.
The graphical representation of the LT model is shown in Figure
 travelogue   draw a binomial distribution over global topics versus loo Draw a multinomial distribution over global topics, the generative process of a lapsed Gibbs sampling [7] with the following updating formulas.
of global/local binary switches, locations, and topics for all the
 To estimate the parameters of the LT model, we need to estimate the latent variables conditioned on the observed variables, namely (cid:1868)(cid:4666)(cid:2206)(cid:481)(cid:2194)(cid:481)(cid:2208)(cid:513)(cid:2205)(cid:481)(cid:2238)(cid:481)(cid:2009)(cid:481)(cid:2010)(cid:481)(cid:2237)(cid:481)(cid:2015)(cid:4667), where (cid:2206)(cid:481)(cid:2194)(cid:481)(cid:2208) are vectors of assignments words in travelogue collection(cid:3)(cid:1829), respectively.
We use the col-For global topic(cid:3)(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1859)(cid:1864)(cid:4669), (cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:3404)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:3404)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:3404)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2208)(cid:819)(cid:1861)(cid:481)(cid:2205)(cid:819)(cid:1861)(cid:481)(cid:2009)(cid:481)(cid:2237)(cid:481)(cid:2015)(cid:1859)(cid:1864)(cid:3439) (cid:3397)(cid:1849)(cid:2015)(cid:1859)(cid:1864)(cid:942) (cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:3397)(cid:2009) (cid:1866)(cid:1875)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:3397)(cid:2015)(cid:1859)(cid:1864) (cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864) (cid:3397)(cid:1846)(cid:1859)(cid:1864)(cid:2009)(cid:942)(cid:4672)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:1859)(cid:1864) (cid:3397)(cid:2011)(cid:1859)(cid:1864)(cid:4673)(cid:484) (cid:1503) (cid:963) (cid:1866)(cid:1875)(cid:1314)(cid:481)(cid:819)(cid:1861) (cid:1859)(cid:1864)(cid:481)(cid:1878) (cid:1875)(cid:1314) For local topic(cid:3)(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:4669) and location(cid:3)(cid:1864)(cid:1488)(cid:2278)(cid:1856)(cid:481)(cid:1871), (cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:3404)(cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1864)(cid:1861)(cid:3404)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:3404)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:3404)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2194)(cid:819)(cid:1861)(cid:481)(cid:2208)(cid:819)(cid:1861)(cid:481)(cid:2205)(cid:819)(cid:1861)(cid:481)(cid:2010)(cid:481)(cid:2237)(cid:481)(cid:2015)(cid:1864)(cid:1867)(cid:1855)(cid:3439) (cid:1503) (cid:1866)(cid:1875)(cid:481)(cid:819)(cid:1861) (cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861)(cid:3397)(cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:2010)(cid:942)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:3397)(cid:1849)(cid:2015)(cid:1864)(cid:1867)(cid:1855)(cid:942) (cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861) (cid:1864)(cid:1867)(cid:1855) (cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:942)(cid:3435)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:1864)(cid:1867)(cid:1855) (cid:3397)(cid:2011)(cid:1864)(cid:1867)(cid:1855)(cid:3439), (cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878)(cid:3397)(cid:2015)(cid:1864)(cid:1867)(cid:1855) (cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878)(cid:3397)(cid:2010) (cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:1864) (cid:1864) (cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:963) (cid:1866)(cid:1875)(cid:1314)(cid:481)(cid:819)(cid:1861) (cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878) (cid:1875)(cid:1314) where (cid:1866)(cid:1875)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878) is the number of times term (cid:1875) is assigned to global (cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878) is that for local topic(cid:3)(cid:1878).
(cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878) is the topic (cid:1878), and similarly (cid:1866)(cid:1875)(cid:481)(cid:819)(cid:1861) number of times a word in document (cid:1856) is assigned to global topic is the number of times a word in document (cid:1856) is (cid:1878), while (cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864) assigned to a global topic.
(cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861) (cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878) is the number of times a word assigned to location (cid:1864) is assigned to local topic (cid:1878) , out of (cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861) words assigned to location (cid:1864) in total.
(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:1864) a word in segment (cid:1871) of document (cid:1856) is assigned to location (cid:1864), and and (cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) consequently to a local topic.
(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:1859)(cid:1864) (cid:1864)(cid:1867)(cid:1855) of times a word in segment (cid:1871) of document (cid:1856) is assigned to global and to local topics, respectively.
For all the counts, subscript (cid:819)(cid:1861) indicates that the(cid:3)(cid:1861)-th word is excluded from the computation.
(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1876) (cid:1503)(cid:1866)(cid:1875)(cid:1876)(cid:481)(cid:1878)(cid:3397)(cid:2015)(cid:1876)(cid:481)(cid:1876)(cid:1488)(cid:4668)(cid:1859)(cid:1864)(cid:481)(cid:1864)(cid:1867)(cid:1855)(cid:4669)(cid:481)(cid:1878)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1876) , (cid:2032)(cid:1864)(cid:481)(cid:1878)(cid:1503)(cid:1866)(cid:1864)(cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878)(cid:3397)(cid:2010)(cid:481)(cid:1878)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855) .
After such a Gibbs sampler reaches burn-in, we can harvest several samples and count the assignments to estimate the parameters:
 Once estimated, the parameters of the LT model can support several applications by providing the data representations and similarity metrics for both locations and terms.
is the number of times denote the number
 ing multinomial distribution over local topics.
For the latter, we derive a probability distribution over terms conditioned on loca-Each location (cid:1864) can be represented in either the (cid:1846)(cid:1864)(cid:1867)(cid:1855) dimensional local topic space or the (cid:1849)-dimensional term space.
For the former, location (cid:1864) is simply represented by(cid:3)(cid:2032)(cid:1864) namely its correspond-tion (cid:1864) directly from the raw Gibbs samples, by counting the words assigned to location(cid:3)(cid:1864), as (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1864)(cid:4667)(cid:1503)(cid:1866)(cid:1864)(cid:1875)(cid:481)(cid:3)(cid:3)(cid:3)(cid:1875)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849), where (cid:1866)(cid:1864)(cid:1875) is the number of times term (cid:1875) is assigned to location(cid:3)(cid:1864).
the symmetric similarity between two locations (cid:1864)(cid:883) and (cid:1864)(cid:884) is meas-distributions over local topics (cid:2032)(cid:1864)(cid:883) and(cid:3)(cid:2032)(cid:1864)(cid:884), as (cid:1838)(cid:1867)(cid:1855)(cid:1845)(cid:1861)(cid:1865)(cid:4666)(cid:1864)(cid:883)(cid:481)(cid:1864)(cid:884)(cid:4667)(cid:3404)(cid:1857)(cid:1876)(cid:1868)(cid:3419)(cid:3398)(cid:2028)(cid:1830)(cid:1836)(cid:1845)(cid:3435)(cid:2032)(cid:1864)(cid:883)(cid:3630)(cid:2032)(cid:1864)(cid:884)(cid:1)(cid:3439)(cid:3423), According to the location representation in the local topic space, ured by the distance between their corresponding multinomial fined as (cid:1830)(cid:1836)(cid:1845)(cid:4666)(cid:1868)(cid:1313)(cid:1869)(cid:1)(cid:4667)(cid:3404)(cid:883)(cid:884)(cid:1830)(cid:1837)(cid:1838)(cid:4672)(cid:1868)(cid:4699)(cid:1868)(cid:3397)(cid:1869)(cid:884)(cid:1)(cid:4673)(cid:3397)(cid:883)(cid:884)(cid:1830)(cid:1837)(cid:1838)(cid:4672)(cid:1869)(cid:4699)(cid:1868)(cid:3397)(cid:1869)(cid:884)(cid:1)(cid:4673), while (cid:1830)(cid:1837)(cid:1838)(cid:4666)(cid:1)(cid:942)(cid:513)(cid:513)(cid:942)(cid:1)(cid:4667) denotes the Kullback-Leibler (KL) divergence; coefficient (cid:2028)(cid:3408)(cid:882) is used to normalize different numbers of local topics.
In addition to that of locations, we also need a representation and corresponding similarity metric of terms, so as to measure the relevance of a location (or a snippet) to a given query term in the application of destination recommendation (or summarization).
Hence, we expand each term (cid:1875) in the vocabulary into a probability distribution over the learnt (cid:1846)(cid:1864)(cid:1867)(cid:1855) local topics, denoted by(cid:3)(cid:2012)(cid:1875) , as (cid:2012)(cid:1875)(cid:3404)(cid:4668)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1875)(cid:4667)(cid:4669)(cid:1878)(cid:3404)(cid:883)(cid:1846)(cid:1864)(cid:1867)(cid:1855) (cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1875)(cid:4667)(cid:1503)(cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:4667)(cid:1503)(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1864)(cid:1867)(cid:1855)(cid:1866)(cid:1878)(cid:1864)(cid:1867)(cid:1855)(cid:1) , (cid:4682)(cid:3)(cid:3) where (cid:1866)(cid:1878)(cid:1864)(cid:1867)(cid:1855) is the total number of words assigned to local topic (cid:1878).
Accordingly, the symmetric similarity between two terms (cid:1875)(cid:883) and (cid:1875)(cid:884) is measured based on their distributions over local topics, as (cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)(cid:4666)(cid:1875)(cid:883)(cid:481)(cid:1875)(cid:884)(cid:4667)(cid:3404)(cid:1857)(cid:1876)(cid:1868)(cid:3419)(cid:3398)(cid:2028)(cid:1830)(cid:1836)(cid:1845)(cid:3435)(cid:2012)(cid:1875)(cid:883)(cid:3630)(cid:2012)(cid:1875)(cid:884)(cid:1)(cid:3439)(cid:3423).
Given the estimated parameters(cid:3)(cid:563) , we can infer hidden variables unseen document (cid:1856) using the following updating formulas: (cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:3404)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:3404)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:3404)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2208)(cid:819)(cid:1861)(cid:482)(cid:563)(cid:3439) (cid:1503)(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1859)(cid:1864) (cid:942) (cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:3397)(cid:2009) (cid:1859)(cid:1864) (cid:3397)(cid:2011)(cid:1859)(cid:1864)(cid:4673)(cid:481)(cid:1878)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1859)(cid:1864) , (cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864) (cid:3397)(cid:1846)(cid:1859)(cid:1864)(cid:2009)(cid:942)(cid:4672)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:3404)(cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1864)(cid:1861)(cid:3404)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:3404)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:3404)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2194)(cid:819)(cid:1861)(cid:482)(cid:563)(cid:3439) (cid:1864)(cid:1867)(cid:1855) (cid:3397)(cid:2011)(cid:1864)(cid:1867)(cid:1855)(cid:3439)(cid:481)(cid:1878)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855) .
(cid:1864)(cid:1867)(cid:1855) (cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:942)(cid:3435)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:1503)(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1864)(cid:1867)(cid:1855)(cid:942)(cid:2032)(cid:1864)(cid:481)(cid:1878)(cid:942)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) (cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:1864) (cid:1864) (cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861) over locations for each term (cid:1875) appearing in document (cid:1856) by counting the number of times term (cid:1875) is assigned to each location (cid:1864) as (cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:3404)(cid:851)(cid:4666)(cid:1875)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)(cid:139)(cid:149)(cid:3)(cid:131)(cid:149)(cid:149)(cid:139)(cid:137)(cid:144)(cid:135)(cid:134)(cid:3)(cid:150)(cid:145)(cid:3)(cid:1864)(cid:4667) (cid:851)(cid:4666)(cid:1875)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4667) for unseen travelogues.
Specifically, a Gibbs sampler is run on the After collecting a number of samples, we can infer a distribution

 In this section, we introduce how to leverage the learnt LT model to enable three interesting applications: destination recommendation, destination summarization, and travelogue enrichment.
.
The first question raised by a tourist is: where should I go?
Meanwhile, a tourist has some preferences about the travel destinations, which are usually expressed in terms of two criteria: x Being similar to a given location   I quite enjoyed the trip to Honolulu last year.
Is there any other destination with similar style?  x Being relevant to a given travel intention   I plan to go hiking next month.
Could you recommend some destinations good for hiking? 
 Given a query location (cid:3)(cid:1864)(cid:1869) and a candidate destination set(cid:3)(cid:2278), each destination (cid:1864)(cid:1488)(cid:2278) has a similarity to (cid:1864)(cid:1869) in the local topic space, defined as (cid:1838)(cid:1867)(cid:1855)(cid:1845)(cid:1861)(cid:1865)(cid:3)in Section 2.4.1.
Besides, each destination has a query-independent popularity which should also be considered.
The ranking score for recommendation is computed as .
(cid:963) (cid:1864)(cid:1314)(cid:1488)(cid:2278)
 (cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:3404) (cid:851)(cid:4666)(cid:1864)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1829)(cid:4667) (cid:851)(cid:4666)(cid:1864)(cid:1314)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1829)(cid:4667) (cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:1864)(cid:1869)(cid:4666)(cid:1864)(cid:4667)(cid:3404)(cid:142)(cid:145)(cid:137)(cid:1838)(cid:1867)(cid:1855)(cid:1845)(cid:1861)(cid:1865)(cid:3435)(cid:1864)(cid:1869)(cid:481)(cid:1864)(cid:3439)(cid:3397)(cid:2020)(cid:142)(cid:145)(cid:137)(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:481) (cid:1864)(cid:1488)(cid:2278)(cid:481)(cid:2020)(cid:3410)(cid:882), where coefficient (cid:2020) controls the influence of the static popularity (cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:3)in ranking.
Here, (cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667) is simply defined as the occurrence frequency of location (cid:1864) in the travelogue collection(cid:3)(cid:1829), as Given a travel intention described by a term (cid:1875)(cid:1869) (e.g.,  hiking ), we rank destinations in terms of relevance to(cid:3)(cid:1875)(cid:1869) .
Since a travel single term, we expand (cid:1875)(cid:1869) in the local topic space as (cid:3)(cid:2012)(cid:1875)(cid:1869) (a dis-this way, the relevance of each location (cid:1864) to the (cid:1875)(cid:1869) can be meas-(cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:1875)(cid:1869)(cid:4666)(cid:1864)(cid:4667)(cid:3404)(cid:3398)(cid:1830)(cid:1837)(cid:1838)(cid:4672)(cid:2012)(cid:1875)(cid:1869)(cid:1313)(cid:2032)(cid:1864)(cid:1)(cid:4673)(cid:3397)(cid:2021)(cid:142)(cid:145)(cid:137)(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:481) (cid:1864)(cid:1488)(cid:2278)(cid:481)(cid:2021)(cid:3410)(cid:882), where (cid:2032)(cid:1864) is location (cid:1864) s distribution over the local topics.
Actually, with the above query expansion strategy, it is straightforward to support multi-word queries for more complex travel intentions.
intention usually contains more comprehensive semantics than a tribution over the local topics, as introduced in Section 2.4.2).
In ured using KL-divergence.
The ranking score is thus computed as
 Once a destination has been determined, a tourist would like to know more details of the destination, like   What are the most representative things in San Francisco?
Can you tell me with a few words or sentences?  described in Section 2.4.1, and simply select those terms with highest probabilities in this distribution as the representative tags.
To summarize the representative aspects of a destination, we first generate a few representative tags, and then identify related snippets for each tag to further describe and interpret the relation between the tag and the destination.
For a given location(cid:3)(cid:1864)(cid:1869), we can obtain its probability distribution over terms (cid:3419)(cid:1868)(cid:3435)(cid:1875)(cid:3627)(cid:1864)(cid:1869)(cid:3439)(cid:3423)(cid:1875)(cid:3404)(cid:883)(cid:483)(cid:1849) as Then, given a representative tag(cid:3)(cid:1875)(cid:1869) , we generate its corresponding snippets by ranking all the sentences (cid:4668)(cid:1871)(cid:4669) in the travelogue collection according to the query  (cid:1864)(cid:1869)(cid:3397)(cid:1875)(cid:1869)  .
Specifically, a sentence(cid:3)(cid:1871) consisting of a (mentioned) location set (cid:2278)(cid:1871) and a term set (cid:2289)(cid:1871) is rated in terms of the geographic relevance to location (cid:1864)(cid:1869) and the semantic relevance to tag(cid:3)(cid:1875)(cid:1869) , as (cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:1864)(cid:1869)(cid:481)(cid:1875)(cid:1869)(cid:4666)(cid:1871)(cid:4667)(cid:3404)(cid:1833)(cid:1857)(cid:1867)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1864)(cid:1869)(cid:4666)(cid:1871)(cid:4667)(cid:3400)(cid:1845)(cid:1857)(cid:1865)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1875)(cid:1869)(cid:4666)(cid:1871)(cid:4667), where (cid:1833)(cid:1857)(cid:1867)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1864)(cid:1869)(cid:4666)(cid:1871)(cid:4667)(cid:3404)(cid:851)(cid:3435)(cid:1864)(cid:1869)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:2278)(cid:1871)(cid:3439)(cid:513)(cid:2278)(cid:1871)(cid:513) (cid:932) (cid:1845)(cid:1857)(cid:1865)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1875)(cid:1869)(cid:4666)(cid:1871)(cid:4667)(cid:3404)(cid:963) (cid:142)(cid:145)(cid:137)(cid:4666)(cid:883)(cid:3397)(cid:513)(cid:2289)(cid:1871)(cid:513)(cid:4667) (cid:932) (cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)(cid:3435)(cid:1875)(cid:1869)(cid:481)(cid:1875)(cid:3439) (cid:3) (cid:1875)(cid:1488)(cid:2289)(cid:1871) where (cid:513)(cid:942)(cid:513) denotes the cardinality of a set, and (cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865) is the terms in sentence (cid:1871) contribute to the semantic relevance more or less, according to their similarities to the query tag.
Besides the brief summarization, a tourist would also like to browse through some travelogues written by other tourists.
Given a travelogue, a reader is usually interested in the places visited by the author and how these places look like.
pairwise term similarity defined in Section 2.4.2.
Note that all the , and ,   Where did Jack visit when he was in New York City?
And how do those places look like?  To facilitate browsing, we extract the highlights of a travelogue and enrich them with images to provide additional visual descriptions.
Given a travelogue (cid:1856) which refers to a set of locations(cid:3)(cid:2278)(cid:1856) , lights.
As described in Section 2.4.3, each term (cid:1875) in travelogue (cid:1856) has a probability (cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:3)to be assigned to location(cid:3)(cid:1864)(cid:1488)(cid:2278)(cid:1856) .
Hence, the highlight corresponding to location (cid:1864) is represented as a (cid:1849)-dimensional term-vector(cid:3)(cid:2203)(cid:1864)(cid:3404)(cid:3435)(cid:1873)(cid:1864)(cid:481)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1873)(cid:1864)(cid:481)(cid:1849)(cid:3439), where (cid:1873)(cid:1864)(cid:481)(cid:1875)(cid:3404)(cid:851)(cid:4666)(cid:1875)(cid:3)(cid:1853)(cid:1868)(cid:1868)(cid:1857)(cid:1853)(cid:1870)(cid:1871)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:481)(cid:1875)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849).
To visually enrich every identified highlight(cid:3)(cid:2203)(cid:1864), we select images from a candidate image set (cid:2284)(cid:1864) that is geographically relevant to location(cid:3)(cid:1864).
Each image (cid:1870)(cid:1488)(cid:2284)(cid:1864) is annotated with a set of tags(cid:3)(cid:2286)(cid:1870) , and is also represented as a (cid:1849) dimensional vector(cid:3)(cid:2204)(cid:1870)(cid:3404) (cid:3435)(cid:1874)(cid:1870)(cid:481)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1874)(cid:1870)(cid:481)(cid:1849)(cid:3439), where (cid:1874)(cid:1870)(cid:481)(cid:1875)(cid:3404)(cid:963) (cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)(cid:4666)(cid:1872)(cid:481)(cid:1875)(cid:4667)(cid:481)(cid:3)(cid:3)(cid:3)(cid:1875)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849) (cid:1872)(cid:1488)(cid:2286)(cid:1870) Then, the relevance of image (cid:1870) to highlight (cid:2203)(cid:1864)(cid:3)is computed as (cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:2203)(cid:1864)(cid:4666)(cid:1870)(cid:4667)(cid:3404)(cid:1731)(cid:2203)(cid:1864)(cid:481)(cid:2204)(cid:1870)(cid:1732)(cid:942) (cid:142)(cid:145)(cid:137)(cid:4666)(cid:883)(cid:3397)(cid:513)(cid:2286)(cid:1870)(cid:513)(cid:4667)(cid:481)(cid:1870)(cid:1488)(cid:2284)(cid:1864), (cid:883) where (cid:1731)(cid:942)(cid:481)(cid:942)(cid:1732) denotes inner product, and the second term is used to the kth image (cid:1870)(cid:1863) is chosen, we update (cid:2203)(cid:1864)(cid:4666)(cid:1863)(cid:4667)(cid:3404)(cid:4672)(cid:1873)(cid:1864)(cid:481)(cid:883)(cid:4666)(cid:1863)(cid:4667)(cid:481)(cid:485)(cid:481)(cid:1873)(cid:1864)(cid:481)(cid:1849)(cid:4666)(cid:1863)(cid:4667)(cid:4673) to (cid:1873)(cid:1864)(cid:481)(cid:1875)(cid:4666)(cid:1863)(cid:4667)(cid:3404)(cid:4682)(cid:1873)(cid:1864)(cid:481)(cid:1875)(cid:4666)(cid:1863)(cid:3398)(cid:883)(cid:4667)(cid:3400)(cid:1857)(cid:1876)(cid:1868)(cid:3435)(cid:3398)(cid:2028)(cid:942)(cid:1874)(cid:1870)(cid:1863)(cid:481)(cid:1875)(cid:3439)(cid:3)(cid:481)(cid:1863)(cid:3410)(cid:883) (cid:1873)(cid:1864)(cid:481)(cid:1875)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:481)(cid:1863)(cid:3404)(cid:882)(cid:1)(cid:3) , (cid:1875)(cid:3404)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849), where (cid:2028)(cid:3408)(cid:882) is a coefficient to control the strength of decay.
normalize images with different numbers of tags.
Moreover, to diversify the resulting images, we select images one by one.
Once

 In this section, we present experimental results of the LT model and its applications.
Both objective and subjective evaluation methods are used to evaluate the effectiveness of the framework.
.
decay the semantics already illustrated by the selected images, as
 There are many sources of travelogues on the Web, either from Weblogs such as Windows Live Spaces, or dedicated travel websites like TravelPod3, IgoUgo4, and TravelBlog5.
We collected approximately 100,000 travelogues written in English and related to tourist destinations in the United States, to form an English corpus.
A location extractor was applied to extract locations mentioned in these travelogues, yielding 18,000 unique locations.
As some subjective evaluations require participators  knowledge, we also built a Chinese corpus from Ctrip6, consisting of 94,000 Chinese travelogues related to around 32,000 locations in China.
After pre-processing including stemming and stop-word removal, we trained a LT model on each corpus to learn a number of local topics and global topics.
The numbers of local and global topics were set empirically to 100 and 50, respectively.
The training procedure for each corpus included 2,000 iterations of Gibbs sampling and lasted for approximately 40 hours on a server with an AMD Opteron quad-core 2.4GHz processor.
To illustrate the topics learnt by the LT model, we show the top terms (i.e., terms with the highest probabilities) of some topics in Table 1.
We can see that local topics characterize some tourism 3 http://www.travelpod.com/ 4 http://www.igougo.com/ 5 http://www.travelblog.org/ 6 http://www.ctrip.com/ Table 1.
Top terms of example local and global topics.
local #23 local #57 local #62 local #66 local #69 desert cactus canyon valley hot west heat spring museum art collect gallery exhibit paint work sculpture dive snorkel fish aquarium sea boat whale reef casino gamble play slot table machine game card mountain peak rocky snow high feet lake summit global #8 global #19 global #22 global #26 global #37 flight airport fly plane check bag air travel great best fun kid family old beautiful children enjoy wonderful love amaze fun love young age room hotel bed inn breakfast bathroom night door rain weather wind cold temperature storm sun warm (a) Local topic #57 (museum,  ) (b) Local topic #62 (dive,  ) Figure 5.
Geographic distributions of two local topics.
The darker a region is, the higher correlation with the topic it has.
styles and corresponding locations, including both natural styles like seaside (local #62) and cultural styles like museum (local #57); whereas global topics correspond to common themes such as accommodation (global #26) and opinion (global #19), which tend to appear in travelogues related to almost any destination.
ics (#57 museum and #62 seaside in Table 1) on the U.S. state To exemplify the relationships between local topics and locations, we utilize, following [13], the Many Eye visualization service7 to visualize the spatial distribution of some local topics.
Based on the LT model, the correlation between a local topic (cid:1878) and a location (cid:1864) is measured by the conditional probability(cid:3)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667), which is equal to(cid:3)(cid:2032)(cid:1864)(cid:481)(cid:1878) in the LT model.
In Figure 5, we plot two local top-map respectively, where a darker state indicates a higher (cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667) it has.
Both maps show uneven geographic distributions of local topics, indicating high dependence between local topics and locations.
From Figure 5 (a) we see that New York, Illinois, and Oklahoma are famous for {museum, art,  }; while in Figure 5 (b) Hawaii shows the highest correlation with {dive, snorkel,  }.
This demonstrates the learnt relationships between local topics and locations are reasonable and consistent with prior knowledge.
Since the effectiveness of similarity-oriented destination recommendation highly relies on the pairwise similarity metric of locations, we directly evaluate this metric s capability of discovering similar locations from a given set.
7 http://manyeyes.alphaworks.ibm.com/manyeyes/ (b) Baseline method Figure 6.
Location similarity graphs generated by the LT model and the baseline method, where different colors and shapes stand for different location categories.
We first collected the top destinations recommended by TripAdvi-sor8 for four travel intentions including Beaches & Sun, Casinos, History & Culture, and Skiing.
After filtering out locations not appearing in our corpus, we built a location set consisting of 36 locations, based on which pairwise location similarities were computed (as describe in Section 2.4.1) to form a location similarity graph.
To demonstrate how well the graph is consistent with the ground-truth similarity/dissimilarity between four categories of locations, we use the NetDraw9 software to visualize this graph where similar locations tend to be positioned close to each other, as shown in Figure 6 (a).
As a comparison, we implemented a baseline method which formed a pseudo document for each location by concatenating all the travelogues referring to it, and then measured the pairwise location similarity using the common TF-IDF-based cosine similarity.
Comparing the two graphs in Figure 6 (a) and (b), we can see that different categories of locations are roughly differentiated by our similarity metric, while under the baseline metric some of them are coupled together.
This is owing to one advantage of the LT model, namely preserving the information that characterizes and differentiates locations when projecting the travelogue data into a low-dimensional topic space.
To evaluate the relevance-oriented recommendation, we collected the top destinations recommended by TripAdvisor for five travel intentions, i.e., Beaches & Sun, Casinos, Family Fun, History & Culture and Skiing, as the ground-truth for five queries, respectively.
For the sake of uniformity, all the queries are truncated into unigrams.
Besides the LT model-based method presented in Section 3.1.2, we also set up a baseline method, which ranks locations for a query term in decreasing number of travelogues containing both a location and the query term.
The resulting location ranking lists of both methods are evaluated by the number of locations, within the top K ones, matching the ground-truth locations.
The evaluation results are shown in Table 2, while Table 3 lists some top destinations recommended by our approach.
From Table 2 we observe that the locations recommended by the LT model generally match more ground-truth ones than the baseline; whereas the baseline exceeds our approach at the top 5 and top 10 results for the query family.
This observation can be interpreted as the two sides of a coin.
On one hand, our method measures each location s relevance to the query term in the local topic space to naturally expand the query with similar terms, and thus enable partial match and improve the relevance measurement for queries well captured by local topics (e.g., beach, casino).
On the

 Table 2.
Comparison of the relevance-oriented destination recommendation results for five queries.
Query #Ground-truth Methods #Matches at top K
 beach casino family history skiing




 baseline LT model baseline LT model baseline LT model baseline LT model baseline LT model







































 Table 3.
Top destinations recommended by the LT model-based method, where those in the ground-truth shown in bold.
Query Top 10 recommended destinations beach Myrtle Beach, Maui, Miami, Santa Monica, Destin, Hilton Head Island, Virginia Beach, Daytona Beach, Key West, San Diego casino Las Vegas, Atlantic City, Lake Tahoe, Biloxi, Reno, Deadwood, New Orleans, Detroit, Tunica, New York City family Orlando, Las Vegas, New York City, Washington, D.C., New Orleans, Charleston, Myrtle Beach, Chicago, San Francisco, Walt Disney World history New Orleans, Charleston, Williamsburg, Washington, D.C., New York City, Chicago, Las Vegas, Philadelphia, San Francisco, San Antonio skiing Lake Tahoe, Park City, South Lake Tahoe, Jackson Hole, Vail, Breckenridge, Winter Park, Salt Lake City, Beaver Creek, Steamboat Springs other hand, for queries mainly captured by global topics (e.g., family, a top term of the global topic #22 shown in Table 1), this query expansion mechanism is less reliable, due to the low confidence of these terms  distributions over local topics.
To compare with the location-representative tag generation approach described in Section 3.2, we implemented three baseline methods.
The first one ( TF ) is to generate a pseudo document for each location by concatenating all the travelogue paragraphs referring to it, and then rank terms in decreasing frequency in the pseudo document.
The second one ( TF-IDF ) is to further multiply each term s frequency with the Inverse Document Frequency (IDF) to penalize common terms.
The third baseline is similar to the LT model-based approach but disable the global topics.
As there is no existing ground-truth of location-representative tags, we built one by borrowing people s knowledge.
For each location, we first formed a tag pool by merging the top tags generated by the proposed method and three baselines, and then asked 20 graduate students to select the top 10 most representative tags.
Finally, each tag was rated according to the number of times it was selected, to generate a ranking list of tags as the ground-truth.
Considering the participators  background knowledge, we used the Chinese corpus in this experiment and involved 20 popular tourist destinations in China to form the questionnaire.
Based on the ground-truth, the tag ranking list generated by each method is evaluated using the Normalized Discounted Cumulative @












 LT (only local topics) LT (local + global topics)





 Baseline LT-Model










 Figure 7.
NDCG@K results of location-representative tags generated by (a) TF, (b) TF-IDF, (c) LT model with only local topics, and (d) LT model with both local and global topics.
geographic relevance semantic relevance Figure 8.
A subjective evaluation of representative snippets generated by the LT model-based method and the baseline.
comprehensiveness overall satisfaction Table 4.
Representative tags generated by the LT model-based method for example destinations in the United States.
Destination Top 10 representative tags Anchorage Boston Chicago Las Vegas Los Angeles Maui New York City Orlando San Francisco Washington,
 bear, moose, alaskan, glacier, fish, cruise, salmon, wildlife, trail, mountain fenway, whale, historic, sox, cape, england, red, history, revere, church michigan, institute, field, lake, museum, cta, tower, loop, windy, cub strip, casino, show, hotel, bellagio, gamble, fountain, venetian, mgm, slot hollywood, star, studio, universal, movie, boulevard, theatre, china, getty, sunset island, beach, snorkel, whale, ocean, luau, volcano, dive, fish, surf subway, broadway, brooklyn, zero, avenue, island, yorker, manhattan, village, greenwich disney, park, universal, resort, world, theme, studio, kingdom, magic, epcot bay, cable, alcatraz, chinatown, wharf, bridge, prison, bart, fisherman, pier museum, memorial, monument, national, metro, capitol, war, smithsonian, lincoln, president Gain at top K (NDCG@K) [9], which is commonly used in the IR area to measure the accuracy of ranking results.
The results averaged over all the 20 locations are shown in Figure 7.
It can be seen that our method significantly outperforms the baselines consistently at top K ranking positions.
Out of the baselines, the TF-IDF method outperforms the TF method consistently, owing to the penalty to noisy tags commonly co-occurring with various locations.
However, this frequency-based penalty mechanism is too coarse to filter out all the noisy tags.
Our approach properly filters out these tags using global topics.
When global topics are disabled and all the information is modeled by local topics as in the third baseline, the performance is even worse than the TF-IDF method.
In addition to the above quantitative evaluation, we also generated representative tags for some U.S. destinations based on the English corpus.
As exemplified in Table 4, the generated tags include not only landmarks (e.g., bellagio, alcatraz) but also styles (e.g., historic, beach) and activities (e.g., gamble, dive).
As it is quite subjective to evaluate the extent to which a textual snippet is informative for something at somewhere, we resorted to user study to evaluate the generated representative snippets.
Based on the Chinese corpus, we prepared 20 groups of data, each consisting of a query in the form of  location + term , 5 snippets generated by the proposed method, and another 5 snippets from a baseline snippet ranking method based on the number of occurrences of the query in a snippet.
Twenty graduate students were asked to assess the two snippet sets (presented in random order) in each group using 1 to 5 ratings, from four aspects namely (1) geographic relevance (i.e., to what extent the snippets are describing the query location), (2) semantic relevance (i.e., describing the query term), (3) comprehensiveness (i.e., providing rich information about the query), and (4) overall satisfaction.
Using these aspects we want to demonstrate whether the proposed method can suggest snippets not only relevant to the query but also informative and comprehensive.
For each snippet set, we averaged all the users  evaluations as its ratings on the four aspects.
The two methods are compared using pairwise t-test on the 20 groups and exhibit significant differences (p<0.01) in all the four aspects.
As depicted in Figure 8, although the difference in the geographic relevance is relatively small due to the straightforward measurement in both methods, our method shows significant advantages in other three aspects due to the query term expansion mechanism.
Besides, some examples generated based on the English corpus are illustrated in Table 5, where words relevant to the query term (shown in bold and italic) provide informative and comprehensive descriptions for the queries.
For the evaluation of travelogue enrichment, we conducted a user study based on the Chinese corpus.
The materials presented to users consist of 10 travelogue segments, each referring to at least one location and related characteristics.
For each segment, there are two image sets (each with three images) generated by our method and a baseline method which simply uses the mentioned locations as queries to retrieve geo-tagged photos from Flickr.
We asked 20 graduate students to assess both image sets (presented in random order) of each segment from four aspects including (1) geographic relevance (i.e., to what extent the images are depicting the main locations in the segment), (2) semantic relevance (i.e., depicting the objects mentioned in the segment), (3) diversity (i.e., depicting different objects in the segment), and (4) overall satisfaction (well highlighting and enriching the text).
The results are depicted in Figure 9, and pairwise t-test on the 10 pairs of image sets exhibits significant differences (p<0.01) in all the four aspects.
It indicates that the images selected by our method are more favorable compared with the baseline.
Note that in the aspect of geographic relevance, the difference of two methods is small because the baseline is actually geo-based, while in other three aspects, our method exhibits larger advantages due to the learnt location-representative knowledge and query expansion enabled by the LT model.
Another two English examples are illustrated in Figure 10, where each travelogue segment is enriched by three images that depict are shown in underline and words relevant to the query term are shown in bold and italic.
Query Alaska + wildlife Las Vegas + casino Waikiki Beach + beach Top snippets
 This time, the bus stopped at the Alaska Wildlife Conservation Center, where we saw musk oxen, bison, brown bears, black bears, eagles, owls, foxes, rein-deers, and a few other animals.
The rest of my week in Alaska was filled with more wildlife sightings, including numerous brown bears and cubs, caribou, and a rare black wolf in Denali; and a moose with a calf and several wild trumpeter swans.
With the ringing of slot machines it finally hit me, I was rolling into a Las Vegas casino!
Sitting at the first row of slot machines we experienced one of the most amazing aspects of common casino etiquette.
Being more inclined to stick my hard earned cash in a money market fund than a slot machine, Las Vegas and casinos have never held much allure for me, but I have to admit that the Las Vegas Strip with all the big casinos seemed pretty glamorous and exciting.
We have played in the Casinos of Las Vegas!
They want you to stay in the Casinos so they have waitresses coming around giving you free drinks and all you have to do is tip her $1 or $2 dollars and she just keeps coming back.
The famous Waikiki Beach is very beautiful and is packed with swimmers from the early hours, it has lovely clean sand but the beach is getting smaller by the year and is already at the base of a couple of hotels!
The dreaded global warming!
Since Waikiki Beach faces southwest, we were able to enjoy an extraordinary sunset right from the beach in front of our hotel, then we retired to our room where we spent the evening sitting out on the lanai, having some cocktails, and listening to the surf.
Baseline

 LT-Model








 geographic relevance Figure 9.
A subjective evaluation of travelogue enrichment by the LT model-based method and the baseline method.
overall satisfaction semantic relevance diversity its most informative parts.
We also present each image s original tags and the words in text it corresponds to.
For instance, the presented images in Figure 10 (a) depict representative and diverse semantics described in the text, i.e., ocean, volcano, and beach.
Some related work has been dedicated to organizing information on the Web to provide online travel assistant services.
For instance, Jing et al. [10] proposed a travel plan assistant system which provided high-quality images relevant to given locations based on tourist sight extraction and image retrieval.
Wu et al. [21] proposed a system to generate personalized tourism summary in the form of text, image, video, and news.
In [12], a trip planning system was presented for place recommendation according to users  previous choices and tag-based place similarity.
Recently, leveraging user-contributed photo collections (e.g., Flickr [6]) has attracted lots of research efforts.
Some of them [11][18] selected representative photos to visually summarize a given landmark or scene.
Ahern et al. [1] analyzed the tags associated with photos to identify and visualize representative tags for arbitrary areas in the world; while Crandall et al. [5] utilized geo-tagged photos to discover worldwide popular places and their representative images.
In [22], geo-tagged photos were leveraged to discover landmarks and build a world-scale landmark recognition engine.
Moxley et al. [15] proposed an image tag suggestion tool based on mining of location tags from Flickr photos.
In [8], the authors proposed to generate overviews for locations by mining representative tags from travelogues and retrieving related images.
Each travelogue is assumed to be related to only one location; neither similarity between locations nor the representation of locations in the learnt topic space is considered.
Probabilistic topic models, such as latent Dirichlet allocation (LDA) [2] and its extensions, have been successfully applied to many text mining tasks.
Rosen-Zvi et al. [17] extended LDA by incorporating authors of documents as observed variables and representing authors with mixtures of topics.
Some models [4][19] aimed to discover topics in different granularity levels other than document-level.
In [16][20], locations (entities) appearing in documents were explicitly modeled as generated by topics, while in [13][14] locations served as labels associated with documents.
In a very recent work [3], the model was sensitive to both entities and relationships between entities, given textual data segmented beforehand.
In spite of the success in their respective scenarios, all the above models are not applicable to the travelogue mining scenario in this paper, as discussed in the section of Introduction.
Travelogues contain abundant location-representative knowledge, which is informative for other tourists, but difficult to extract and summarize manually.
In this paper, we have investigated the mining of location-representative knowledge from travelogues to facilitate tourists to utilize such knowledge.
We proposed a probabilistic topic model, i.e., Location-Topic model, to discover local and global topics from travelogues and characterize locations using local topics.
With this model, we could effectively (1) recommend destinations for flexible queries; (2) summarize destinations with representative tags and snippets; and (3) enrich the highlights of travelogues with images.
The proposed framework was evaluated based on two large travelogue collections, showing promising results on the above tasks.
For the future work, we plan to incorporate prior knowledge of locations to improve the unsupervised knowledge mining.
Another direction is to leverage more types of information in travelogues (e.g., opinions, travel routes, and temporal information) to meet more practical information needs such as itinerary planning.
