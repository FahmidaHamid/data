Resource Description Framework (RDF)1, a W3C standard for representing any information, and SPARQL2, a query language for RDF, are gaining importance as semantic

 http://www.w3.org/TR/rdf-syntax-grammar/ http://www.w3.org/TR/rdf-sparql-query/ Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
data is increasingly becoming available in the RDF format.
RDF data consists of triples where each triple presents a relationship between its subject (S) and object (O), the name of the relationship is given by the predicate (P), and the triple is represented as (S P O).
Such an RDF data can be represented as a labeled directed graph and it can be serialized and stored in a relational database simply as a 3-column table where each tuple in that table represents a triple in the original RDF graph.
RDF is extensively being used for representing data from the  elds of bioinformatics, life sciences, social networks, and Wikipedia as well.
Since disk-space is getting cheaper, storing this huge RDF data does not pose as big a problem as executing queries on them.
Querying these huge graphs needs scanning the stored data and indexes created over it, reading that data inside memory, executing query algorithms on it, and building the  nal results of the query.
Hence, a desired query processing algorithm is one which (i) keeps the underlying size of the data small (using compression techniques), (ii) can work on the compressed data without uncompressing it, and (iii) doesn t build large intermediate results.
A lot of work has already gone in using compression techniques in storing the data in a column-store database as well as trying to work on the compressed data without uncompressing it by lazy materialization [2, 4].
In this paper, we go one step further and propose a compressed bitcube of RDF data and a novel SPARQL join query processing approach which always works on the compressed data by producing the  nal results in a streaming fashion without building intermediate join tables.
A SPARQL join query, which can also be viewed as a Basic Graph Pattern Matching (BGP) query, or a conjunctive triple pattern query, resembles closely to an SQL join query (in fact any SPARQL join query can be systematically translated into an SQL join query [9]).
A typical SPARQL join query looks like the one shown in Figure 1.
This query shows a join between three triple patterns.
Such join queries can be broadly classi ed into three categories.
The  rst type is   queries having highly selective triple patterns3.
E.g., consider the query (?s :residesIn USA)(?s :hasSSN  123-45-6789 ).
Since SSN is a unique attribute of a person, the second triple pattern has only one triple associated with it thereby being highly selective.
The second type is   queries having triple patterns with low-selectivity but which generate few results, i.e., highly selective join results.
E.g., consider a multinational orga-
Selectivity of a triple pattern is low if there are more number of triples associated with it and vice versa.
?m rdf:type :movie .
?n rdf_type :movie .
?m :similar_to ?n } Equivalent SQL join query Note: RDF graph stored as tripletable
 tripletable AS A, tripletable AS B, tripletable AS C WHERE A.subject = B.subject AND A.predicate = ":similar_to" AND A.object = ":movie" AND B.object = ":movie" AND A.object = C.subject AND B.predicate = "rdf:type" AND C.predicate = "rdf:type"; Figure 1: An example of SPARQL join query nization BigOrg, having employees on the order of few millions all over the world.
Now consider a densely populated country like India having population close to 1.2 billion and consider a query like (?s :residesIn India)(?s :worksFor Big-Org).
Although the number of triples associated with the two triple patterns is quite high, their join will produce much fewer number of results as there are only a few employees of BigOrg in India.
The third type of queries are the ones having low-selectivity triple patterns and low-selectivity join results, i.e., the ones generating a lot of results.
For instance, a modi cation of the  rst query given above, to  nd SSNs of all the people (?s :residesIn USA)(?s :hasSSN ?y).
Most of the systems which generate various indexes on the data do well on the  rst type of queries   highly selective triple patterns.
Especially having 6-way indexes helps in picking the right set of triples at the beginning, avoiding scanning a large amount of data.
For the second type of queries   low-selectivity triple patterns, but highly selective join results   systems using join selectivity estimation or pre-computed join tables/indexes get bene ted to a certain extent.
Although as shown in our experiments, join selectivity estimation does not always help in improving the query performance in case of complex joins involving low-selectivity intermediate join results.
For the third type of queries   low-selectivity triple patterns generating a large number of results   even the state-of-the-art systems run into problems (as shown by our evaluation).
Although disk-space is growing at a much faster speed, the available main-memory still remains very small compared to it.
This creates the main bottleneck while executing queries of the second and third type mentioned above.
Hence our goal is to build a scalable query algorithm which operates on the compressed data.
Second our goal is to not generate intermediate join tables, thereby keeping the memory footprint of the system small (hence lightweight), and cope well with the second and third type of queries mentioned above.
Our key contributions in this work are:
 BitMat   to increase the size of the data that can  t in memory.
the triples during the  rst phase of SPARQL join query execution and in the second phase, performs variable binding matching across the triple patterns to obtain the  nal results (both phases use compressed BitMats without any join table construction).
the compressed data directly.
and  1.33 billion) using a set of queries published on the web by the owners of the datasets, showing a comparison with the state-of-the-art RDF storage systems   RDF-3X and MonetDB.
Our results indicate that competing methods are much better on high-selectivity queries, whereas our method outperforms them by 2-3 orders of magnitude on complex queries with low selectivity intermediate join results.
Work presented in this paper is a considerable extension of our preliminary work outlined previously in [5].
RDF data can be serialized and stored in a database and a SPARQL join query can be executed as an SQL join, hence recently a lot of database join query optimization techniques have been applied to improve the performance of SPARQL join queries.
Notably, in the past couple of years, C-Store [3], RDF-3X [16], MonetDB [21], and Hexastore [24] systems have proposed ways of optimizing SPARQL join queries.
Out of these systems, C-Store and MonetDB exploit the fact that typically RDF data has much less number of properties (predicates), thereby vertically partitioning the data for each unique predicate and sorting each partition (predicate table) on subject, object order (creating a subject-object index on each property table).
Hexastore and RDF-3X make use of the fact that an RDF triple is a  xed 3-dimensional entity and hence they create all 6-way indexes (SPO, SOP, PSO, POS, OPS, OSP).
Although Hexastore does share common indexes within these 6 indexes, e.g., SPO and PSO share the  O  index, without any compression, it su ers from 5-fold increase in the space required to store these indexes.
RDF-3X goes one step further and compresses these indexes as described in their paper [15].
RDF-3X also implements several other join optimization techniques like RDF spe-ci c Sideways-Information-Passing, selectivity estimation, merge-joins, and using bloom lters for hash joins.
Along with these systems, there are other systems being developed for RDF data storage and querying, such as, Jena-TDB [1] and Virtuoso [10].
Jena-TDB faces scalability issues while executing queries on very large datasets.
Along with these, BRAHMS [11] and GRIN [22] focus more on path-like queries on RDF data, typically which cannot be expressed using existing SPARQL syntax.
Most systems built to store and query RDF data typically use a left-deep join tree which requires materialization of the intermediate join results in case of a complex join query involving several join variables.
Merge-joins cannot always be used while performing later joins, especially when the join column of an intermediate result is not sorted.
In contrast to these, in our system instead of using sophisticated join optimization techniques, we have followed a simple rule of keeping the data compressed without materializing the intermediate join results.
This helps to keep a large amount of required data in memory.
We execute the join by following a novel algorithm, which propagates the constraints on the join-variable bindings among di erent join variables in the query.
Our technique is reminiscent of the concept of semi-joins [7, 6] as discussed further in Section 4.1.
We consider our query processing engine lightweight   lightweight on runtime memory consumption as well as optimization techniques.
We have shown results by analyzing where our system outperforms the state-of-the-art systems like RDF-3X and MonetDB.
:the_matrix :the_thirteenth_floor :the_matrix :the_thirteenth_floor :the_matrix :the_thirteenth_floor Predicate :releasedIn :releasedIn :similar_to :similar_to rdf:type rdf:type Object "1999" "1999" :the_matrix_reloaded :the_matrix :movie :movie S dimension



 c



 d

 a

 b P dimension














 O dimension S O and O S BitMats for Ps Note: a = :the_matrix, b = "1999", c = :movie, d = :the_matrix_reloaded :releasedIn c d b a :the_matrix
 :the_thirteenth_floor



 Transpose



 :similar_to c d b a








 rdf:type b c d a








 Figure 2: Example of SO, OS BitMat construction procedure for each P

 Figure 2 shows sample RDF data and gives a pictorial representation of constructing a BitMat.
Let Vs, Vp and Vo denote the sets of distinct subjects, predicates, and objects, respectively, in the RDF data.
This RDF data can be represented by a 3D bit-cube, where each dimension of the bitcube represents subjects (S), predicates (P), and objects (O).
The volume of this bitcube is Vs   Vp   Vo.
Each cell in the bitcube represents a unique RDF triple that can be formed by the combination of S, P, O positions which are the coordinates of that bit.
A bit set to 1 denotes presence of that triple in the given RDF data.
This 3D bit-cube is sliced along a dimension to get 2D matrices.
Figure 2 shows slicing along the P-dimension, which gives SO bit-matrices (BitMats).
Inverting an SO BitMat gives an OS BitMat.
We store these SO and OS BitMats for each P value.
In all we get |Vp| such SO and OS matrices.
Additionally, we slice the bitcube along S and O dimensions which gives PO and PS BitMats respectively.
Note that we do not store inverted OP and SP BitMats, since based on our experience, usage of those BitMats is rare; and even if needed their construction from the corresponding PO or PS BitMat is easier due to the relatively few number of predicates in typical RDF data.
In all we have |Vs| PO BitMats and |Vo| PS BitMats.
To summarize, for each P value we have a SO and an OS BitMat, for each S value   a PO BitMat, and for each O value   a PS BitMat (in all
 There are total |Vs|   |Vp|   |Vo| possible triples with the given Vs, Vp, and Vo sets.
But it is observed that typically RDF data contains much fewer number of triples, hence the SO, OS, PS, PO BitMats are very sparse.
We make use of this fact by applying gap compression on each bit-row of these four types of BitMats.
In gap compression scheme, a bit-row of  0011000  will be represented as  [0] 2 2 3 .
That is, starting with the  rst bit value, we record alternating run lengths of 0s and 1s.
We also store the number of triples in each compressed BitMat (this statistics is useful while executing our query algorithm as described later).
Along with this, we store two bitarrays   row and column bitarray   which give a condensed representation of all the nonempty row and column values in the given BitMat.
For example, in Figure 2, for the SO BitMat of  :similar to  predicate (marked by BitMat  2  in Figure 2), we store row bitarray  1 1  and a column bitar-ray  1 0 0 1 , and for the OS BitMat we store row bitarray  1 0 0 1 , and column bitarray  1 1  respectively.
These bitarrays are useful while performing  star join  queries (as elaborated in the Evaluation section).
We store the compressed SO, OS, PS, PO BitMats in one  le on the disk and maintain a meta le which gives the o set of each Bit-Mat inside the BitMat  le.
Due to this, addition or deletion of the triples might require moving a large amount of data, but if bulk updates are expected on the RDF data, all the BitMats can be rebuilt at once since the BitMat construction time even for very large data is very small (as shown at the end of the Evaluation section).
The above construction reveals that each unique S, P, and O is mapped to a unique position along each dimension and this position can be represented as an integer ID.
We decide this mapping with the following procedure: Let Vso represent the Vs   Vo set.
Each element in Vso, along with the elements in Vs, Vp and Vo, is assigned an integer ID as follows:   Common subjects and objects: Set Vso is mapped to a sequence of integers: 1 to |Vso|.
  Subjects: Set Vs   Vso is mapped to a sequence of integers: |Vso| + 1 to |Vs|.
  Predicates: Set Vp is mapped to a sequence of integers:
   Objects: Set Vo   Vso is mapped to a sequence of integers: |Vso| + 1 to |Vo|.
The common subject-object identi er assignment facilitates the bitwise operations in join queries wherein an S position in one triple pattern is joined over an O position in another triple pattern (e.g.
?n in the query in Figure 1).
For the present considerations, we do not handle joins across SP and PO dimensions.
Such queries are rare in the context of assertional RDF data.
None of the benchmark queries published for the large RDF datasets have queries having joins over SP or PO dimensions.
Hence overlapping S, P, O IDs except for the common S and Os do not pose a problem while processing a query.
With respect to the construction described above, the RD-FCube [14] system is conceptually closest to BitMat.
RD-FCube also builds a 3D cube of S, P, and O dimensions.
However, RDFCube s design approximates the mapping of a triple to a cell by treating each cell as a hash bucket containing multiple triples.
They primarily used this as a distributed structure in a peer-to-peer setup (RDFPeers [8]) to reduce the network tra c for processing join queries in a conventional manner.
In contrast, BitMat s compressed structure maintains unique mapping of a triple to a single bit, and also employs a di erent query processing algorithm.
Further, RDFCube has demonstrated their results on a bitcube of only up to 100,000 triples, whereas we have used more than 1.33 billion triples in this paper.
In this section we de ne two basic operations fold and unfold which are used by our join query algorithm.
Fold BitMats constructed while storing the original RDF data.
(1) Fold: fold operation represented as  fold(BitMat, Re-tainDimension) returns bitArray  folds the input BitMat by retaining the RetainDimension.
For example, if in an SO BitMat of a given predicate P, RetainDimension is set to  columns , then BitMat is folded along the subject  rows  resulting into a single bitarray, i.e., all the subject bit-rows are ORed together to give an  object  bit-array.
Intuitively, a bit set to 1 in this array indicates the presence of at least one triple with the  object  corresponding to that position in the given SO BitMat.
Without loss of generalization, this procedure can be applied to any of the SO, OS, PO, PS BitMats with  rows  or  columns  as RetainDimension.
(2) Unfold: Speci ed as  unfold(BitMat, MaskBitArray, RetainDimension) , unfolds the MaskBitArray on the Bit-Mat.
Intuitively, in the unfold operation, for every bit set to
 sition of the RetainDimension in the BitMat are cleared.
For example, unfold(BitMat,  011000 ,  columns ) would result in a bitwise AND of  [0] 1 2 3  (gap compressed representation of  011000 ) and each row of the BitMat.
Note that fold and unfold operations are implemented to operate directly on a compressed BitMat.
For example, a bitwise AND of compressed arrays   arr1 as [0] 2 3 4 and arr2 as [1] 3 4 2   can be performed by sequentially looking at their  gap values .
E.g., AND the  rst gap of arr1   2 0s and arr2   3 1s, which gives the  rst gap of 2 0s in the result.
Since the two gaps were of uneven length, there is a leftover
 arr1   3 1s, and leftover  rst gap of 1 1s from arr2, which gives second gap in the result   1 1s, so on and so forth.
Bitwise OR on the compressed bitarrays can be done with AND using simple Boolean logic (a OR b) = NOT(NOT(a) AND NOT(b)).
A bitwise NOT operation on a compressed bitarray is simply   NOT([0] 2 3 4) = [1] 2 3 4.
Before describing our join processing algorithm, we would like to note some facts about the join process.
Property 1.
Each triple pattern in a given join query has a set of RDF triples associated with it which satisfy that triple pattern.
These triples generate bindings for the variables in that triple pattern.
If the triples associated with another triple pattern containing the same variable cannot generate a particular binding, then that binding should be dropped.
In that case, all the triples having that binding value should be dropped from the triple patterns which contain that variable.
Property 2.
If two join variables in a given query appear in the same triple pattern, then any change in the bindings of one join variable can change the bindings of the other join variable as well.
Property 3.
A join between two or more triple patterns over a join variable indicates an intersection between bindings of that join variable generated by the triples associated with the respective triple patterns.
To elaborate the use of these properties, consider the query given in Figure 1.
?m and ?n appear in the same triple pattern (?m :similar to ?n).
A position marked with  ?  in the ?m
 jvar ?n ?m rdf:type :movie ?m :similar_to ?n ?n rdf:type :movie
 BitMat
 BitMat


 tp BitMat
 Figure 3: Graph G for the query in Figure 1 triple pattern is variable.
If we perform a join of (?m :similar to ?n) (?m rdf:type :movie)  rst, we get two bindings for the variable ?m viz.
:the matrix and :the thirteenth  oor and two for ?n :the matrix reloaded and :the matrix corresponding to ?m s bindings.
When we do the join between (?n rdf:type :movie)(?m :similar to ?n), we consider bindings generated for ?m and ?n after the  rst join.
After the join on ?n, binding :the matrix reloaded for ?n gets dropped, hence the triple (:the matrix :similar to :the matrix reloaded) gets dropped from the triples associated with (?m :similar to ?n) which in turn drops the the binding :the matrix for ?m.
Properties 1, 2 and 3 together establish the basis of our pruning algorithm.
We propagate the constraints on the bindings of each join variable in a given triple pattern to all other triple patterns and do aggressive pruning of the RDF triples associated with them.
First we construct a constraint graph4 G out of a given join query.
The constraint graph is built as follows:
 tp-node in G. Hence forth we use the terms  tp-node  and  triple pattern  interchangeably.
A jvar-node in G corresponds to a join variable in the query.
Hence forth we use terms  jvar-node  and  join variable  interchangeably.
and a tp-node exists in G if that join variable appears in the triple pattern represented by the tp-node.
This edge represents the dependency between triples associated with the tp-node and the join variable bindings (ref.
Property 1).
variables appear in the same triple pattern.
This undi-rected, unlabeled edge represents the dependency between their bindings (ref.
Property 2).
join variable between them.
This is an undirected, labeled edge with potentially multiple labels.
Multiple labels can appear if the two triple patterns share more than one join variables.
The labels denote the type of join between the two triple patterns   SS denotes subject-subject join, SO denotes subject-object join etc.
For a query having no Cartesian joins 5, constraint graph
 This graph is reminiscent of similar terminology used in the constraint satisfaction literature.
A Cartesian join is where there is no shared variable in a set of triple patterns, and hence the result of the query is a full Cartesian product of all triples associated with each triple pattern.
for the join query given in Figure 1.
Before starting the pruning algorithm, we initialize each tp-node by loading the triples which match that triple pattern.
In Section 3 we elaborated the construction of four types of BitMats viz.
SO and OS for each P value, PS for each O value and PO for each S value.
Assuming that a given query does not have any triple pattern with all variable positions (e.g.
?x ?y ?z) we initialize the BitMats associated with each triple pattern using the four types of stored Bit-Mats (case of all-variable triple pattern is discussed at the end of this section).
E.g., if the triple pattern in the query is of type (?s :p2 :o321) then we load only one row corresponding to  :p2  from the PS BitMat created for  :o321 .
If the triple pattern is of type (?s :p6 ?o) then we load either the SO or OS BitMat created for  :p6 .
If ?s is a join variable and ?o is not, we load SO BitMat and vice versa.
If both, ?s and ?o, are join variables, then the decision depends on whether ?s will be processed before ?o.
If a join over ?s is processed before ?o, we load SO BitMat and vice versa.
If we have a triple pattern of type (:s2 ?p :o6), then  rst we decide whether PS BitMat for  :o6  has less number of triples or PO BitMat for  :s2  has less number of triples.
If PS BitMat has less number of triples, then we load the PS BitMat by keeping only the bit corresponding to  :s2  in each row and mask out all other bits.
Note that all these operations are done directly on the compressed BitMats.
Thus at the end of initialization, each tp-node has a BitMat associated with it which contains only the triples matching that triple pattern.
For example, BitM at1 associated with (?m rdf:type :movie) has just a single row corresponding to  rdf:type  loaded from the PS BitMat created for the object value  :movie .
Now we start the pruning algorithm.
First, we consider an induced subgraph Gjvar of G containing only jvar-nodes.
By the construction of graph G, Gjvar is also always connected (see Figure 3).
Gjvar can be cyclic or acyclic.
Next, we embed a tree on Gjvar discarding any cyclic edges.
To propagate the constraints on join variable bindings (Property 2), we walk over this tree from root to the leaves and backwards in breadth rst-search manner.
At every jvar-node, we take intersection of bindings generated by its adjacent tp-nodes and after the intersection, drop the triples from tp-node Bit-Mats as a result of the dropped bindings.
It can be seen that by the construction of graph G and following the tree over Gjvar, constraints on the join variable bindings get propagated to other jvar-nodes through the tp-node BitMats (when the triples get dropped), and this propagation follows an alternating path between jvar-nodes and tp-nodes.
This procedure is elaborated in Algorithm(1).
A topological sort of an undirected tree is nothing but enumerating all the nodes from root to leaves in a breadth rst-search fashion.
For each node in the topological sorted list of join variables, we call prune for jvar (Lines 2   4 in Algorithm(1)).
A topological sort ensures that a child jvar node always gets processed after all of its ancestors.
The bitwise AND between folded bitarrays in prune for jvar(J) computes the intersection of all the bindings generated by the tp-nodes which contain J (Lines 2   5 in Algorithm(2)).
According to Property 1, for any binding dropped in the intersection, the respective triples are removed from the BitMats associated with the tp-nodes which contain J using the unfold operation (Lines 6   9 in Algorithm(2)).
getDimension returns the position of J in the BitMat of the triple pattern.
For instance, getDimension(?n, (?m :similar to ?n)) can return column or row depending on whether it is an SO or OS BitMat.
Algorithm 1 Pruning Step


 prune for jvar(J) 4: end for


 8: end for prune for jvar(K) dim = getDimension(J, T ) Algorithm 2 prune for jvar(jvar-node J)



 5: end for


 9: end for dim = getDimension(J, T ) unfold(BitMatT , MaskBitArrJ , dim) One such pass over all the jvar-nodes ensures that the constraints are propagated to the adjacent jvar-nodes from root to leaves of the tree.
For a complete propagation of constraints, we traverse jvar-nodes second time by following the reverse order of the  rst pass (Lines 5   8 in Algorithm(1)).
The leaves of the tree embedded on Gjvar appear last in queue q.
Since they are processed last in the  rst traversal over the tree, in the second traversal, we directly start with the parent nodes of these leaves (Line 5 in Algorithm(1)).
Notably, since we take intersection of the bindings in each pass, the number of triples in the tp-node s BitMat decrease monotonically as the constraints are propagated.
At the end of Algorithm(1), each tp-node contains a much reduced set of triples adhering to the constraints on join variable bindings.
Typically, when Gjvar is acyclic, this set of triples is also minimal, i.e., each triple in the BitMat of a triple pattern is necessary to generate one or more  nal results.
If Gjvar is cyclic, then the set of triples is not guarantied to be minimal.
But in any case, the unwanted set of triples get dropped in the following phase of  nal result set generation.
Our pruning method closely resembles the idea of semi-joins [7, 6].
Semi-joins also build a query graph (QG) where the nodes of the graph are relations (tables) and an edge between the two nodes indicate a join between the two relations.
A semi-join QG for a SPARQL join query can be reduced to Gjvar in BitMat s constraint graph by following simple transformation: each edge in the QG is a node with the join-variable name in Gjvar, two nodes in Gjvar have an edge between them if the corresponding edges in the QG are incident on the same node in the QG.
If a QG is proper cyclic [6], Gjvar is cyclic and if QG is a tree query then Gjvar is acyclic.
Bernstein et al. have proved in [7, 6] that for the tree queries, semi-joins can fully reduce the database for a given query, i.e., at the end of a semi-join the database has minimal tuples, whereas cyclic queries cannot be guarantied to have full reducers.
A formal proof of minimal triple set generation in case of an acyclic Gjvar in our method is rem-space constraints.
Pn In our current implementation, we load the BitMat associated with each triple pattern at the beginning of query processing and then never seek a disk access in the entire lifespan of the query.
This necessitates that for a query having n triple patterns it needs i=0 size(BitM ati) amount of memory at the beginning.
This poses limitations for queries having triple patterns with all variable positions (?x ?y ?z), as it is not feasible to load a BitMat for the all-variable tp-node containing the entire dataset in memory.
Also, due to this condition, it can happen that for certain queries with highly selective triple patterns, the memory requirements of the conventional query processors are lesser than BitMat as they do not need to load entire indexes in memory to perform joins (e.g.
RDF-3X).
Notably, BitMat s memory requirement remains linear in terms of the triples associated with the triple patterns in the query, whereas for conventional query processors it can degrade polynomially for low-selectivity multi-join queries.
Simple optimizations While performing the pruning step as elaborated before, we use some simple statistical optimization techniques.
Tree root selection: After initialization, in a join query with n triple patterns, we sort all the triple patterns  rst in the order of increasing number of triples associated with them.
If the  rst triple pattern in this list has only one join variable, we pick this join variable as the root of the tree embedded on the graph Gjvar as described before.
If it has more than one join variables, then we scan through the sorted list of triple patterns and  nd another triple pattern such that it shares a join variable with the  rst triple pattern (since constraint graph G is always connected for the queries without Cartesian joins, we are sure to  nd such a triple pattern).
We then assign this shared join variable as the root of the tree embedded on Gjvar.
This method is similar to choosing tables with least number of triples to be joined  rst in the SQL joins.
Early stopping condition: While performing the pruning at each jvar-node, at any point if the M askBitArrJ contains all 0 bits, that is a direct evidence of the query generating empty set of results.
If such a condition occurs we exit the query processing at that point telling that the query has 0 results.
This avoids unnecessary further processing of other join variables and fold/unfold operations over BitMats.
After the pruning phase, we are left with a much reduced set of triples associated with each triple pattern.
Intuitively, each BitMat of a triple pattern can be viewed as a compressed table in a relational database.
Hence, one way of producing the results could have been to simply materialize these BitMats into tables and perform standard joins over them.
But our goal is to avoid building intermediate join results by building a left-deep join tree; which precludes a
 cessor.
In our method, we build and output an entire resulting row of variable bindings, which is similar to multi-way joins.
Notably for this process, we use at most k size additional memory bu er, where k is the number of variables in the query (and hence the additional bu er size is negligible).
We keep a map of bindings for all k variables at a time, output one result when all k variables are mapped, and proceed to generate the next result (hence we call these  streaming results ).
Let us assume that a query has n triple patterns and N is the maximum number of triples in any of the n BitMats associated with the triple patterns.
For simplicity, we denote BitM ati as the BitMat associated with the ith tp-node (tpnodei).
A simple brute force approach can be as follows: Choose say BitM at1, pick a triple from it.
This triple will generate bindings for the variables in tpnode1.
Store these bindings in the map.
Next pick the  rst triple from BitM at2 and generate bindings for the variables in tpnode2.
If tpnode2 and tpnode1 share one or more join variables, check the map if the variable bindings generated by both of them are the same, if not, pick a second triple from BitM at2.
Repeat this procedure until you get the variable bindings consistent with the ones stored in the map.
Then consider BitM at3 and repeat the same procedure as described above.
Repeat this procedure until the last BitM atn in the query.
If a triple in BitM atn generates valid variable bindings, such that all k variables are mapped to the bindings, output one result.
Now start with BitM at1 again and choose the second triple, store the bindings for the variables in tpnode1, and repeat the same process.
In general, while generating variable bindings from any BitM ati, check all the variable bindings stored in the map.
We can quickly see that the worst case complexity of such a brute force approach is O(N n).
Since BitMat is a fully inverted index structure, in practice we devise following method which speeds up the above procedure by several orders of magnitude: In general a Bit-Mat having lesser number of triples generates lesser number of unique bindings for the variables in its tp-node.
This means that in the  nal results of the query, these bindings will get repeated more often in di erent result rows than other bindings (just like the product of two columns where  rst column has lesser number of rows than the other   values from the  rst column get repeated more often in the product).
Making use of this fact, we choose a BitMat as BitM at1, which has the least number of triples, to be processed  rst (similar to the way of choosing the table having least number of triples to join  rst), generate bindings for the variables in tpnode1, and store them in the map.
Next instead of picking BitM at2 randomly, we pick a tpnode2 which shares a join variable with tpnode1.
Depending on the variable bindings stored in the map, we directly locate the triples which can satisfy these bindings inside BitM at2.
Recall that BitMat being a completely inverted index structure, it is easy to locate speci c triples.
If no such triple exists in BitM at2, we discard the variable bindings in the map, go back to BitM at1, and pick the second triple from it to generate new bindings (this can happen only in case of a cyclic Gjvar).
If BitM at2 generates variable bindings consistent with BitM at1, pick tpnode3 which shares join variables either with tpnode1 or tpnode2.
Considering the constraint graph given in Figure 3, let Gtp be an induced subgraph of G having only tp-nodes and edges between them.
We make use of Gtp to make the choice of the next tp-node at every step.
Hence it can be seen that after one walk over all the tp-nodes of Gtp, if the map has all k variables mapped to bindings, we output one result.
The procedure is repeated again until all the triples in BitM at1 are exhausted.
bindings of all variables in the query unless it is a  star join .
However, in the future, depending on the nature of the constraint graph G for a given query and the variable bindings asked by the SELECT clause, it might not be required to traverse Gjvar twice (ref.
Section 4.1) thereby further improving the overall query processing time.
Note that all the procedures described previously work on a compressed BitMat.
Since the pruning phase only reduces the number of triples in the BitMat monotonically, the memory requirement of the query processor goes on reducing as the pruning progresses and the  nal phase of result generation doesn t build join tables.
To conclude the description of our procedure, we would like to point out certain key di erences of our query processing algorithm from the typical bitmap index joins.
BitMat s structure is similar to the idea of compressed bitmap indexes which are widely used to improve joins in OLAP data warehousing techniques [17, 12, 20].
But an SQL join between multiple tables over di erent columns cannot always make use of the bitmap indexes for the later joins.
This is due to the fact that after the  rst level of join, a relational query processor has to materialize the results of the previous join to carry out the next join and this materialized table does not always have indexes formed on it (unless join-indexes are precomputed based on heuristics).
As opposed to that, Bit-Mat s pruning and  nal result generation steps always use compressed BitMats, without materializing the intermediate join results.
BitMat structure and query algorithm is developed in C and is compiled using g++ with -O3 optimization  ag.
For the experiments we used a Dell Optiplex 755 PC having 3.0 GHz Intel E6850 Core 2 Duo Processor, 4 GB of memory, running 64 bit 2.6.28-15 Linux Kernel (Ubuntu 9.04 distribution), with 7 GB of swap space on a 7200 rpm disk with

 We had a wide choice to select the systems for competitive evaluation due to the availability of numerous RDF triplestores.
We experimented with Hexastore6, Jena-TDB, RDF-3X, and MonetDB.
Out of these we chose RDF-3X (v0.3.3) and MonetDB (v5.14.2)   latest versions   for our evaluation, as they could load a large amount of RDF data, gave better performance than others, and are open-source systems used by the research community for performance intensive RDF query execution.
Like BitMat, RDF-3X maps strings/URIs in RDF data to integer IDs and mainly operates on these IDs, building the entire result of a query in the integer ID format.
It converts the IDs to strings using their dictionary mapping just before outputting the results in a user readable format.
We observed that RDF-3X was taking signi cant amount of time to convert IDs into strings; in certain cases it took even more time for this conversion than the time taken for the core query execution.
Current BitMat system doesn t support a formal SPARQL query parser interface and the interface to output the results in the string format is still under preliminary development.
Hence for a fair compari-
We obtained compiled binaries of Hexastore from the authors.
son, we disabled the ID to string mapping in RDF-3X, which improved their query times a lot.
All the RDF-3X query times reported in this paper are without their ID to string mapping.
For a fair comparison, we loaded MonetDB7 by inserting the integer IDs generated out of BitMat dictionary mapping (ref.
Section 3).
Hence essentially all the MonetDB queries were performed on S, P, Os as integer IDs.
We created separate predicate tables in MonetDB by inserting the respective triples by ordering on SO values [21] and used these predicate tables in the query whenever there is a bound predicate in the triple pattern instead of the giant triple-table containing all the triples.
We chose UniProt dataset with 845,074,885 triples, 147,524,984 subjects, 95 predicates, and 128,321,926 objects [23], which is a protein dataset.
We also generated a dataset using LUBM [13]   a synthetic data generator provided by Lehigh University   with over 10,000 universities which gave 1,335,081,176 unique triples with 217,206,845 subjects, 18 predicates, and 161,413,042 objects.
LUBM is widely used by the Semantic Web community for benchmarking triple-stores.
For UniProt dataset we used 6 out of 8 queries published by RDF-3X in [16] (Q7-Q10, Q12, Q13 in our list, leaving out 2 queries which have  all-variable  triple patterns).
To increase the diversity, we also included 5 more queries (out of 9) published by the UniProt dataset owners [19] (Q1, Q4-Q6, Q11 in our list).
We removed the FILTER condition in the original Q1 as currently it is not supported by our query processor and had to modify a  bound position  in Q5, Q11 as that value did not exist in the dataset.
We modi ed two of the RDF-3X queries by removing some bound positions to reduce the selectivity of triple patterns (Q2, Q3 in our list).
For the LUBM dataset, OpenRDF has published a list of queries [18].
But many of these queries are simple 2-triple pattern queries or they are quite similar to each other.
Hence we chose 7 representative queries out of this list.
All the queries are listed in Appendix A.
For the evaluation, we measured the following parameters: (i) query execution times (cold and warm cache).
This is an end-to-end time counted from the time of query submission to the time including outputting the  nal results.
For cold cache we dropped the  le systems caches using /bin/sync and echo 3 > /proc/sys/vm/drop_caches, (ii) initial number of triples   the sum of triples matching each triple pattern in the query, and (iii) the number of results.
The evaluation is given in Tables 1, 2, and 3.
Query times were averaged over 10 consecutive runs.
Geometric mean* is the geometric mean of the query times excluding the ones on which RDF-3X failed to complete processing.
Note that our current BitMat query processing system does not use any sophisticated cache management (like Mon-etDB) and also does not mmap data les into the memory (like RDF-3X).
Due to this, as opposed to RDF-3X and Mon-etDB, in most of the queries the di erence between our cold and warm cache times was not very high.
After the evaluation, we could classify the queries into

 MonetDB was compiled using  --enable-optimization   ag.
BitMat MonetDB


 Aborted BitMat MonetDB
 #Results #Initial triples

 Aborted









 Cold cache





 Warm cache









































 Table 2: Evaluation   UniProt 845 million triples (time in seconds, best times are boldfaced)
















 Cold cache





 Warm cache





















 Geom.
Mean





 Geom.
Mean* (without Q1)





 BitMat MonetDB
 BitMat MonetDB
 #Results #Initial triples Table 3: Evaluation   LUBM 1.33 billion triples (time in seconds, best times are boldfaced)





 BitMat MonetDB


 Aborted


 BitMat MonetDB
 #Results #Initial triples

 Aborted









 Cold cache


 Warm cache



















 Geom.
Mean





 Geom.
Mean* (without Q1)











 both RDF-3X and MonetDB (in both cold and warm cache times), queries where BitMat did better than one of the systems or the reported query times were comparable to the other systems, and queries where BitMat s performance was worse than both RDF-3X and MonetDB.
Queries of the  rst type are   Q1, Q2 of UniProt and Q1, Q2, Q3 of LUBM.
Notably, UniProt Q1, Q2 had a high number of initial triples and the join results were quite un-selective.
As was our initial conjecture, BitMat did much better on such queries than RDF-3X and MonetDB.
On the other hand, LUBM Q1 and Q3 were more complex queries having a high number of initial triples associated with the triple patterns, but the  nal number of results were quite small (2528 and 0 respectively).
For these queries, the initial selectivity of the triple patterns and selectivity of the intermediate join results were quite low, but together they gave highly selective results (these queries have cyclic dependency among join variables   ref.
Section 4.1).
For these queries BitMat was upto 3 orders of magnitude faster than RDF-
without materializing the intermediate join tables.
RDF-
the system ran out of its physical memory and swap space.
We executed the same queries on RDF-3X on a higher con guration server having 16 GB physical memory.
RDF-3X processed UniProt Q1 in 858.464 sec; was observed to consume  11 GB resident memory.
For LUBM Q1, RDF-3X took 1613.178 sec and the peak memory consumption was  11 GB.
For both queries BitMat took 448.169 and 50.70 sec, and consumed  2.6 GB and  3 GB respectively on the same server.
A  star join  query was the one where many triple patterns joined on one variable, the query had only one join variable, and that variable got projected in the  nal results.
LUBM Q2, Q4, Q5 were star-join queries.
For star-joins BitMat worked much better, because our query processor doesn t need to load the BitMats of all the triple patterns in memory.
It just loads the pre-computed row or column bitarrays of each BitMat associated with the triple pattern (ref.
Section 3).
The  nal result generation phase consists of just listing out the 1-bit positions from the bitwise AND of the loaded bitarrays (similar to the bitmap index joins).
has very low selectivity and the query generates a lot of results compared to query Q4 and Q5.
RDF-3X did very well on the UniProt queries Q7-Q10, Q12, Q13.
These queries have a lot of triple patterns, with many having bound predicate and object positions which make them highly selective.
Also many of these triple patterns join on one variable where RDF-3X s method of Sideways Information Passing worked much better.
The number of results produced by these queries were highly selective too (less than 30 results for 5/6 queries).
In the case of UniProt Q4, Q5, Q7-Q10 BitMat did better than MonetDB (cold cache), but RDF-3X still outperformed BitMat (Q7-Q10 are the queries published by RDF-
times of RDF-3X and BitMat was quite small.
But in case of Q5, the di erence was quite high.
Further dissection of BitMat query processing times revealed that initialization and pruning phases were very fast, but more than 90% of the time was spent in the last phase of the result construction.
The reason behind this is   our current data structures and result enumeration algorithm are not tuned to exploit the  locality  in memory while generating the  nal results.
This query has only one join variable but all the variables in the query get projected in the results.
On the other hand, for UniProt Q11-Q13, more than 90% of the query processing time was spent in the initialization to load the BitMats associated with each triple pattern (ref.
Section 4.1).
In the future, this e ect can be alleviated by implementing a  lazy loading  of the BitMats associated with the tp-nodes   instead of loading all the BitMats at the beginning, one can wait until the very  rst join and then load only the required portion of the BitMat in the unfold operation.
To summarize the results   it was evident that for complex join queries with low-selectivity intermediate results, BitMat outperformed both RDF-3X and MonetDB by a sig-ni cant margin.
Although for queries with highly selective triple patterns generating fewer results, RDF-3X and Mon-etDB performed better.
This reemphasizes our initial goal of targeting low-selectivity queries with our novel query processing algorithm.
In view of these results, we would like to mention one spe-ci c LUBM query which turned out be an outlier (LUBM Q7).
RDF-3X aborted due to the system running out of memory on the Dell PC.
BitMat took several hours to process this query, although the processor clock showed that the query spent only  200 seconds actually executing on the processor.
MonetDB processed it in 449.048 sec.
For further investigations, we evaluated this query on the server having 16 GB of memory and we found the following:   BitMat  nished processing this query in 139.94 sec.
The peak resident memory consumption was reported to be 6.3 GB, and on an average the process consumed
 with tp-node  ?x ub:takesCourse ?z  was very large,  3.4 GB, having 288,017,530 triples (22% of the total triples) in it   largest among all the predicates.
Thus, on the 4 GB Dell PC, BitMat process spent a lot of time in the kernel waiting for the pages to be allocated.
MonetDB handled this situation well due to its better cache-memory management.
  MonetDB processed this query in 136.082 sec on the
 consumption was 9 GB, and on an average the process consumed 8.6 GB of resident memory.
  RDF-3X processed the same query in 66.139 sec on the same server, but its peak resident memory consumption was 14 GB and on an average it consumed
 the query.
This query has 442,351,492 initial triples associated with it   largest among the listed LUBM queries   and generates
 egy along with  proactive cache management  BitMat would be able to handle these type of queries in a better manner in future.
The on disk size of all 2|Vp| + |Vs| + |Vo| BitMats (ref.
Section 3) was 48 GB and 67 GB for UniProt and LUBM respectively and corresponding LZ77 compressed dictionary mappings were 3.2 GB and 1.8 GB.
These BitMat sizes include the size of the meta le too.
But note that for any given query with n triple patterns, the runtime memory re-i=1 size(BitM ati); which is typically a quirement is just much smaller fraction of the total data le size.
For RDF-3X and MonetDB the on-disk size of data les were 42 GB and
 sizes of raw RDF Ntriple  les of UniProt and LUBM were
 Pn We used an external Perl script to parse the raw triples It took  12 and build string to ID dictionary mapping.
hours to parse and build dictionary mappings of LUBM data and  9 hours for UniProt data.
After parsing the data, the SO, OS, PS, PO BitMats for UniProt and LUBM were built in 41 and 56 minutes respectively.
This process is much faster than parsing due to our method of building the compressed bit-row of a BitMat directly without building an uncompressed array  rst using the ID based triples sorted on their S, P, O positions (details of this process are omitted due to space constraints).
In this paper we demonstrated a novel method of processing RDF join queries, following a simple principle of keeping the data compressed as much as possible, without building intermediate join tables, and producing the  nal results in a streaming fashion.
Our evaluation using the state-of-the-art RDF stores like RDF-3X and MonetDB showed that while RDF-3X and MonetDB gave better performance on highly selective queries, BitMat gave much superior performance on low-selectivity queries, where sophisticated query optimization techniques did not fetch a lot of bene ts.
BitMat could deliver over 3 orders of magnitude better performance for some of the queries (e.g., LUBM Q3, warm cache).
Notably, working on the compressed data fetches bene ts when the size of the underlying data is higher and when the selectivity of triple patterns in the query and intermediate join results is lower.
This is due to our query processing algorithm which keeps the runtime memory footprint small.
On the other hand, for queries with highly selective triple patterns, processing the joins in a conventional manner can be more bene cial.
BitMat system is a prototype implementation of our query processing algorithm.
Since BitMat s basic data structure resembles compressed bitmap indexes, in the future it is processing algorithm and the conventional query processor.
The system can choose the method of processing the query based on heuristics and selectivity of the triple patterns in the query.
Along with these avenues, in the future, we plan to improve the system by further optimizing our algorithm.
