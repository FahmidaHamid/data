When you write papers, how many times do you want to make some citations at a place but you are not sure which papers to cite?
For example, the left part of Figure 1 shows a segment of a query manuscript containing some citation placeholders (placeholders for short) marked as  [?
] , where Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Figure 1: A manuscript with citation placeholders and recommended citations.
The text is from Section 2.2.
citations should be added.
In order to  ll in those citation placeholders, one needs to search the relevant literature and  nd a small number of proper citations.
Searching for proper citations is often a labor-intensive task in research paper composition, particularly for junior researchers who are not familiar with the very extensive literature.
Moreover, the volume of research undertaken and information available make citation search hard even for senior researchers.
Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations?
High quality citation recommendation is a challenging problem for many reasons.
For each citation placeholder, we can collect the words surrounding as the context of the placeholder.
One may think we can use some keywords in the context of a placeholder to search a literature search engine like Google Scholar or CiteSeerX to obtain a list of documents as the candidates for citations.
However, such a method, based on keyword matching, is often far from satisfactory.
For example, using query  frequent itemset mining  one may want to search for the  rst paper proposing the concept of frequent itemset mining, e.g.
[1].
However, Google Scholar returns a paper about frequent closed itemset mining published in 2000 as the  rst result, and a paper on privacy preserving frequent itemset mining as the second choice.
[1] does not appear in the  rst page of the results.
CiteSeerX also lists a paper on privacy preserving frequent itemset mining as the  rst result.
CiteSeerX fails to return [1] on the  rst page, either.
One may wonder, as we can model citations as links from citing documents to cited ones, can we use graph-based link prediction techniques to recommend citations?
Graph-based link prediction techniques often require a user to provide sample citations for each placeholder, and thus shifts much of the burden to the user.
And, graph-based link prediction holder.
The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document.
Our approach can recommend citations for a context e ectively, which is the innovative part of the paper.
Moreover, it can recommend a set of citations for a paper with high quality.
In addition to the theoretical contribution, we also implement a prototype system in CiteSeerX.
Figure 2 shows a real example in our demo system, where a user submits an citation context with a placeholder and the title/atract.
Among the top 6 results, the 1st, 4th, 5th and 6th ones are proper recommendations relevant and new to the manuscript; the 2nd one is the original citation; the 3rd one is a similar but irrelevant recommendation.
We also present an extensive empirical evaluation in the CiteSeerX digital library against many baselines.
Our empirical study demonstrates the e ectiveness and the scala-bility of our approach.
The rest of this paper is organized as follows.
We discuss related work in Section 2.
We formalize the problem in Section 3.
We discuss candidate retrieval methods in Section 4.
We present our context-aware relevance model for ranking in Section 5.
We report our empirical evaluation in Section 6.
Section 7 concludes the paper.
In this section, we discuss the related work on document recommendation, topic-based link prediction, and analysis of citation contexts.
There are some previous e orts on recommending a bibliography list for a manuscript, or recommending papers to reviewers.
The existing techniques generally rely on a user pro le or a partial list of citations.
Basu et al. [4] focused on recommending conference paper submissions to reviewers based on paper abstracts and reviewer pro les.
Reviewer pro les are extracted from the Web.
This is a speci c step in a more general problem known as the Reviewer Assignment Problem, surveyed by Wang et al. [26].
Chandrasekaran et al. [6] presented a technique to recommend technical papers to readers whose pro le information is stored in CiteSeer.
A user s publication records are used to model her pro le.
User pro les and documents are presented as hierarchial concept trees with prede ned concepts from the ACM Computing Classi cation System.
The similarity between a user pro le and a document is measured by the weighed tree edit distance.
Our work can also be seen as a pro le-based system, where a query manuscript is a pro le.
However, our system uses richer information than just prede ned concepts or paper abstracts.
Shaparenko and Joachims [22] proposed a technique based on language modeling and convex optimization to recommend documents.
For a large corpus, the k-most similar documents based on cosine similarity are retrieved.
However, similarity based on full-text is too slow for large digital libraries.
Furthermore, according to our experiments, similarity based on document abstract results in poor recall (cf.
Table 1).
Some previous studies recommend citations for a manuscript already containing a partial list of citations.
Speci cally, given a document d and its partial citation list r , those (cid:2) Figure 2: A demo of our context-aware citation recommendation system.
methods may encounter di culties to make proper citations across multiple communities because a community may not be aware of the related work in some other community.
A detailed natural language processing analysis of the full-text for each document may help to make good citation recommendations, but unfortunately has to incur serious scala-bility issues.
There may be hundreds of thousands of papers that need to be compared with the given manuscript.
Thus, the natural language processing methods cannot be straightforwardly scaled up to large digital libraries and electronic document archives.
The recommended citations for placeholders should satisfy two requirements.
First, a citation recommendation needs to be explainable.
Our problem is di erent from generating a bibliography list for a paper where a recommendation should discuss some ideas related to the query manuscript.
In our problem, a recommended citation for a placeholder in a query manuscript should be relevant and authoritative to the particular idea that is being discussed at that point in the query manuscript.
Di erent placeholders in the same query manuscript may need di erent citations.
Second, the recommendations for a manuscript need to take into account the various ideas discussed in the manuscript.
For example, suppose we are asked to recommend citations for a query manuscript in which one section discusses mixture models and another section discusses nonparametric distributions.
Citations to nonparametric mixture models may be ranked high since they are related to both sections in the same manuscript.
In summary, citation recommendation for placeholders (and for the overall bibliography) is a challenging task.
The recommendations need to consider many factors: the query manuscript in whole, the contexts of the citation placeholders individually and collectively, and the literature articles individually.
We need to construct an elegant mathematical framework for relevance and develop an e cient and scalable implementation.
In this paper, we present an e ective solution to the problem of citation recommendations for placeholders in query manuscripts.
Our approach is context-aware, where a con-(cid:2) (cid:2) studies try to recover the complete citation list denoted by r   r .
Collaborative  ltering techniques have been widely applied.
For example, McNee et al. [16] built various rating matrices including a author-citation matrix, a paper-citation matrix, and a co-citation matrix.
Papers which are co-cited often with citations in r are potential candidates.
Zhou et al. [27] propagated the positive labels (i.e., the existing citations in r ) in multiple graphs such as the paper-paper citation graph, the author-paper bipartite graph, and the paper-venue bipartite graph, and learned the labels of the rest documents for a given testing document in a semi-supervised manner.
Torres et al. [25] used a combination of context-based and collaborative  ltering algorithms to build a recommendation system, and reported that the hybrid algorithms performed better than individual ones.
Strohman et al. [23] experimented with a citation recommendation system where the relevance between two documents is measured by a linear combination of text features and citation graph features.
They concluded that similarity between bibliographies and Katz distance [15] are the most important features.
Tang and Zhang [24] explored recommending citations for placeholders in a very limited extent.
In particular, a user must provide a bibliography with papers relevant to each citation context, as this information is used to compute features in the hidden layer of a Restricted Boltzmann Machine before predictions can be made.
We feel this requirement negates the need for requesting recommendations.
In our study, we do not require a partial list of citations, since creating such a list for each placeholder shifts most of the burden on the user.
Thus, we can recommend citations both globally (i.e., for the bibliography) and also locally (i.e., for each placeholder, the innovative part of this paper).
Topic models are unsupervised techniques that analyze the text of a large document corpus and provide a low-dimensional representation of documents in terms of automatically discovered and comprehensible  topics .
Topic models have been extended to handle citations as well as text, and thus can be used to predict citations for bibliographies.
The aforementioned work of Tang and Zhang [24]  ts in this framework.
Nallapati et al. [18] introduced a model called Pairwise-Link-LDA which models the presence or absence of a link between every pair of documents and thus does not scale to large digital libraries.
Nallapati et al.
[18] also introduced a simpler model that is similar to the work of Cohn and Hofmann [7] and Erosheva et al. [9].
Here citations are modeled as a sample from a probability distribution associated with a topic.
Thus, a paper can be associated with topics when it is viewed as a citation.
It can also be associated with topics from the analysis of its text.
However, there is no mechanism to enforce consistency between the topics assigned in those two ways.
In general, topic models require a long training process because they are typically trained using iterative techniques such as Gibbs Sampling or variational inference.
In addition to this, they must be retrained as new documents are added.
The prior work on analyzing citation contexts mainly belongs to two groups.
The  rst group tries to understand the motivation functions of an existing citation.
For example, Aya et al. [2] built a machine learning algorithm to automatically learn the motivation functions (e.g., compare, contrast, use of the citation, etc.)
of citations by using citation context based features.
The second group tries to enhance topical similarity between citations.
For example, Huang et al. [11] observed that citation contexts can e ec-tively help to avoid  topic drifting  in clustering citations into topics.
Ritchie [21] extensively examined the impact of various citation context extraction methods on the performance of information retrieval.
The previous studies clearly indicate that citation contexts are a good summary of the contributions of a cited paper and clearly re ect the information needs (i.e., motivations) of citations.
Our work moves one step ahead to recommend citations according to contexts.
In this section, we formulate the problem of context-based citation recommendation, and discuss the preprocessing step.
Let d be a document, and D be a document corpus.
Definition 3.1.
In a document d, a context c is a bag of words.
The global context is the title and abstract of d. The local context is the text surrounding a citation or placeholder.
If document d1 cites document d2, the local context of this citation is called an out-link context with respect to d1 and an in-link context with respect to d2.
A user can submit either a manuscript (i.e., a global context and a set of out-link local contexts) or a few sentences (i.e., an out-link local context) as the query to our system.
There are two types of citation recommendation tasks, which happen in di erent application scenarios.
Definition 3.2 (Global Recommendation).
Given a query manuscript d without a bibliography, a global recommendation is a ranked list of citations in a corpus D that are recommended as candidates for the bibliography of d.
Note that di erent citation contexts in d may express different information needs.
The bibliography candidates provided by a global recommendation should collectively satisfy the citation information needs of all out-link local contexts in the query manuscript d.
Definition 3.3 (Local Recommendation).
Given an out-link local context c  with respect to d, a local recommendation is a ranked list of citations in a corpus D that are recommended as candidates for the placeholder associated with c .
For local recommendations, the query manuscript d is an optional input and it is not required to already contain a representative bibliography.
To the best of our knowledge, global recommendations are only tackled by document-citation graph methods (e.g., [23]) and topic models (e.g., [24, 18]).
However, the context-aware approaches have not been considered for global or local recommendations (except in a limited case where a bibliography with papers relevant to each citation context is required as the input).
ommendation.
Our proposed context-based citation recommendation system can take two types of inputs, a query manuscript d1 or just a single out-link local context c .
We preprocess the query manuscript d1 by extracting its global context (i.e.
the title and abstract) and all of its out-link local contexts.
Extracting the local contexts from d1 is not a trivial task.
Ritchie [21] conducted extensive experiments to study the impact of di erent lengths of local citation contexts on information retrieval performance, and concluded that  xed window contexts (e.g.
size of 100 words) are simple and reasonably e ective.
Thus, before removing all stop words, for each placeholder we extract the citation context by taking
 preprocessing is e cient.
For a PDF document of 10 pages and 20 citations, preprocessing takes on average less than 0.1 seconds.
The critical steps of the system are: (1) quickly retrieving a large candidate set which has good coverage over the possible citations, and (2) for each placeholder associated with an out-link local context and for the bibliography, ranking the citations by relevance and returning the top K. The next two sections focus on these critical steps.
Citation recommendation systems, including our system and especially those that compute intricate graph-based features [23, 27],  rst quickly retrieve a large candidate set of papers.
Then, features are computed for papers in this set and ranking is performed.
This is done solely for scalability.
Techniques for retrieving this candidate set are illustrated in Figures 3 and 4.
Here, the query manuscript is represented by a partially shaded circle.
Retrieved documents are represented as circles with numbers corresponding to the numbered items in the lists of retrieval methods discussed below.
In this section, we discuss two kinds of methods for retrieving a candidate set.
Those methods will be evaluated in Section 6.2.
The context-oblivious methods do not consider local contexts in a query manuscript that has no bibliography.
For a query manuscript d1, we can retrieve
 similar to d1.
We call this method GN (e.g., G100,

 this method Author.
Figure 4: Context-aware methods.
The single-context-based method is for local recommendation with the absence of query manuscript d1.
The hybrid method is for global recommendation and local recommendation with the presence of d1, where the  nal candidate set is the union of the candidate sets derived from each local out-link context in d1.
set, generated by some other method (e.g., GN or Author).
We call this method CitHop.
are already in a candidate set generated by some other method.
We call this method AuthHop.
Note that retrieval based on the similarity between the entire text content of documents would be too slow.
Thus, these methods appear to provide a reasonable alternative.
However, the body of an academic paper covers many ideas that would not  t in its abstract and so many relevant documents will not be retrieved.
Retrieval by author similarity will add many irrelevant papers, especially if the authors have a broad range of research interests and if CitHop or AuthHop is used to further expand the candidate set.
Context-aware methods can avoid such problems.
The context-aware methods help improve coverage for recommendations by considering local contexts in the query manuscript.
In a query manuscript d1, for each context c , we can retrieve:
 ilar to c .
We call this method LN (e.g., L100).
similar to c  (these papers cite papers retrieved by LN).
When used in conjunction with LN, we call this method LCN (e.g., LC100).
We found that method LCN is needed because frequently a document from a digital library may have an out-link context that describes how it di ers from related work (e.g.,  prior work required full bibliographies but we do not ).
Thus, while an out-link context usually describes a cited paper, sometimes it may also describe the citing paper, and this description may be what best matches an out-link context c  from a query manuscript.
The above 6 methods can be combined in some obvious ways.
For example, (L100+CitHop)+G1000 is the candidate set formed by the following process: for each context c  in a query manuscript d1, add the top 100 documents with in-link local context most similar to c  (i.e. L100) and all of their citations (i.e. CitHop) and then add the top (i.e. G1000).
In this section, we propose a non-parametric probabilistic model to measure context-based (and overall) relevance between a manuscript and a candidate citation, for ranking retrieved candidates.
To improve scalability, we use an approximate inference technique which yields a closed form solution.
Our model is general and simple so that it can be used to e ciently and e ectively measure the similarity between any two documents with respect to certain contexts or concepts in information retrieval.
Recall that a query manuscript d1 can have a global context c1 (e.g., a title and abstract, which describes the problem to be addressed) and the out-link local contexts c2, .
.
.
, ck1 (e.g., text surrounding citations) which compactly express ideas that may be present in related work.
A document d2 in an existing corpus D has a global context b1 (e.g.
title and abstract), and local in-link contexts b1, .
.
.
, bk2 (e.g., text that is used by papers citing d2) which compactly express ideas covered in d2.
In this section, we describe a principled and consistent way of measuring sim(d1, d2), de ned as the overall relevance of d2 to d1 and sim(d1, d2; c ), de ned as the relevance of d2 to d1 with respect to a speci c context c  (in particular, c  could be any of the out-link contexts ci).
Our techniques are based on Gleason s Theorem specialized to  nite dimensional real vector spaces.
Theorem 5.1 (Gleason [10]).
For an n-dimensional real vector space V (with n   3), let p be a function that assigns a number in [0, 1] to each subspace of V such that p(v1   v2) = p(v1) + p(v2) whenever v1 and v2 are orthogonal subspaces 1 and p(V ) = 1.
Then p(v) = T race(T Pv) where Pv is the projection matrix for subspace v (e.g.
Pvw is the projection of vector w onto the subspace v), and T is a density matrix   a symmetric positive semide nite matrix whose trace is 1.
Note that van Rijsbergen [20] proposed using Gleason s Theorem as a model for information retrieval, and this was also extensively studied by Melucci [17].
However, our framework is substantially di erent from their proposals since it relies on comparisons between density matrices.
Let W be the set of all words in our corpus and let |W| be the number of words.
The vector space V is |W|-dimensional with one dimension for each word.
In this framework, atomic concepts will be represented as one-dimensional vector spaces and we will treat each context (global, in-link, out-link) as an atomic concept.
Each atomic concept c will be associated a unit column vector which we shall also denote by c (one such representation can be derived from normalizing tf-idf scores into a unit vector).
The projection matrix for c is then ccT .
Our goal is to measure the probability that c is relevant to a document d and so, by Gleason s Theorem, each document d is associated with a density matrix Td.
The probability that c is relevant to 1v1   v2 is the linear span of v1 and v2.
d is then pd(c) = T race(TdccT ) = cT Tdc.
Note that similar atomic concepts (as measured by the dot product) will have similar relevance scores.
Now, the probability distribution characterized by Gleason s theorem is not a generative distribution   it cannot be used to sample concepts.
Instead, it is a distribution over is concept c relevant or not?).
Thus yes/no answers (e.g.
to estimate Td for a document d we will need some (unknown) generative distribution pgen over unit vectors (concepts).
Our evidence about the properties of Td come from the following process.
pgen independently generates k concepts c1, .
.
.
, ck and these concepts are then independently judged to be relevant to d. This happens with probability k i=1 pgen(ci)pd(ci).
We seek to  nd a density matrix Td (cid:2) that maximizes the likelihood.
The log likelihood is: k(cid:3) (cid:4) (cid:5) k(cid:3) log pgen(ci) + i=1 i=1 log cT i Tdci .
(cid:6) Now, if there is only one concept (i.e. k = 1), then the maximum likelihood estimator is easy to compute: it is Td = c1ct
 needed [3].
The computational cost of estimating Td can be seen from the following proposition: (cid:6) r i=1 titT Proposition 5.2.
The density matrix Td can be repre-i where the ti are a set of at most r or-sented as (ti   ti) = 1, and r is the thogonal column vectors with dimension of the space spanned by the ci (the number of linearly independent contexts).
There are O(r2) parameters to determine and numerical (iterative) techniques will scale as a polynomial in r2.
The detailed proof is in Appendix A.
Furthermore, the addition of new documents to the corpus will cause the addition of new in-link contexts, requiring a recomputation of all the Td.
Thus the likelihood approach will not scale for our system.
Since hundreds of thousands of density matrices need to be estimated (one for each document) and recomputed as new documents are added to the corpus, we opt to replace the exact but slow maximum likelihood computation with an approximate but fast closed form solution which we derive in this section.
We begin with the following observation.
For each concept ci, the maximum likelihood estimate of Td given that concept ci is relevant is cicT i .
It stands to reason that our overall estimate of Td should be similar to each of the cicT i .
We will measure similarity by the Frobenius norm (square-root of the sum of the squared matrix entries) and thus set up the following optimization problem: k(cid:3) minimize L(Td) = i=1 ||Td   cicT i ||2
 subject to the constraint that Td is a density matrix.
Taking derivatives, we get k(cid:3) i=1
  Td = 2 (Td   cicT (cid:6) i ) = 0, leading to the solution Td = 1 i .
Now that we have k a closed form estimate for Td, we can discuss how to measure the relevance between documents with respect to a concept.
i=1 cicT k Let Td1 and Td2 be the respective density matrices of the manuscript d1 and a document d2 from the corpus D. We de ne sim(d1, d2), the overall relevance of d2 to d1 to be the probability that a random context drawn from the uniform distribution over contexts is relevant to both d1 and d2.
After much mathematical manipulation, we get the following: (cid:6) Proposition 5.3.
Let Td1 = 1 k1 k1 i=1 cicT i and Td2 = 1 k2 k2 i=1 bibT i .
Then the relevance of d2 to d1 is: (cid:6) k1(cid:3) k2(cid:3) i=1 j=1 sim(d1, d2) =
 k1k2 (ci   bj )
 .
(1) The detailed proof is given in Appendix B.
Given query manuscript q1, we use Equation 1 to rank documents for global recommendation.
Local Recommendation If we are given a single context c  instead of a manuscript, we can still compute sim(c , d2), the relevance of a document d2   D to the context c .
Letting Td2 = 1 i , then k2(cid:3) by Gleason s Theorem, we have: i=1 bibT (cid:6) k2 k2 sim(c , d2) = T race(Td2 c cT  ) = (bj   c )
 .
(2)
 k2 j=1 We de ne sim(d1, d2; c ), the relevance of d2 to d1 with respect to context c  as the probability that c  is relevant to both documents.
Applying Gleason s Theorem twice: sim(d1, d2; c ) = T race(Td1c cT  )T race(Td2c cT  ) k1(cid:3) k2(cid:3) =
 k1k2 (ci   c )
 (bj   c )
 .
(3) i=1 j=1 Given query manuscript d1, we use Equation 3 to rank documents for recommendations at the placeholder associated with context c  in d1.
If a citation context c  is given without d1, then we use Equation 2.
We built a real system in the CiteSeerX staging server to evaluate context-aware citation recommendations.
We used all research papers published and crawled before year
 papers and the papers missing abstract/title and citation contexts, we obtained 456, 787 unique documents in the corpus.
For each paper, we extracted its title and abstract as the global citation context or text content.
Within a paper, we simply took 50 words before and after each citation placeholder as its local citation context.
We removed some popular stop words.
In order to preserve the time-sensitive past/present/future tenses of verbs and the singu-lar/plural styles of named entities, no stemming was done.
All words were transferred to lower-cases.
Finally, we obtained 1, 810, 917 unique local citation contexts and 716, 927 unique word terms.
We used all 1, 612 papers published and crawled in early 2008 as the testing data set.
We implemented all algorithms in C++ and integrated them into the Java running environment of CiteSeerX.
All experiments were conducted on a Linux cluster with 128 nodes2, each of which has 8 CPU processors of 2.67GHz and

 The performance of recommendation can be measured by a wide range of metrics and means, including user studies and click-through monitoring.
For experimental purpose, we focus on four performance measures as follows in this paper.
Recall (R): We removed original citations from the testing documents.
The recall is de ned as the percentage of original citations that appear in the top K recommended citations.
Furthermore, we categorize recall into global recall and local recall for global and local recommendation respectively.
The global recall is  rst computed as the percentage of original bibliography of each testing document d that appears in the top K recommended citations of d, and then averaged over all 1, 612 testing documents.
The local recall is  rst computed as, for each citation placeholder, the percentage of the original citations cited by c  that appear in the top K recommended citations of c , and then averaged over all 7, 049 testing out-link citation contexts.
Co-cited probability (P): We may recommend some relevant or even better recommendations other than those original ones among the top K results, which cannot be captured by the traditional metric like precision.
The previous work usually conducted user studies for this kind of relevance evaluation [16, 6].
In this paper, we instead use the wisdom of the population as the ground truth to de ne a systematic metric.
For each pair of documents (cid:6)di, dj(cid:7) where di is an original citation and the dj is a recommended one, we calculate the probability that these two documents have been co-cited by the popularity in the past as
 number of papers citing both di and dj .
number of papers citing di or dj The co-cited probability is then averaged over all K l unique document pairs for the top K results, where l is the number of original citations.
Again, we categorize this probability into a global version and a local version: the former is averaged over all testing documents and the latter is averaged over all testing citation contexts.
NDCG: The e ectiveness of a recommendation system is also sensitive to the positions of relevant citations, which cannot be evaluated by recall and co-cited probability.
Intuitively, it is desirable that highly relevant citations appear earlier in the top K list.
We use normalized discounted cumulative gain (NDCG) to measure the ranked recommendation list.
The NDCG value of a ranking list at position i is calculated as i(cid:3) 2r(j)   1 log(1 + j) , N DCG@i = Zi j=1 where r(j) is the rating of the j-th document in the ranking list, and the normalization constant Zi is chosen so that the perfect list gets a NDCG score of 1.
Given a testing document d1 and any other document d2 from our corpus D, we use the average co-cited probability of d2 with all original citations of d1 to weigh the citation relevance score of d2 to d1.
Then, we sort all d2 w.r.t.
this score (suppose Pmax is the highest score) and de ne 5-scale relevance number for them as the ground truth: 4, 3, 2, 1, 0 for documents
 bers are averaged over 1,612 test documents.
Methods




 Author L100+CitHop L1000+CitHop LC100+CitHop G1000+CitHop LC1000+CitHop Author+CitHop

 (L100+CitHop)+G1000 (LC100+CitHop)+G1000 (LC1000+G1000)+CitHop LC100+G1000+(Author+CitHop) (LC100+G1000)+AuthHop Coverage Candidate Set Size





































 in (3Pmax/4, Pmax], (Pmax/2, 3Pmax/4], (Pmax/4, Pmax/2], (0, Pmax/4] and 0 respectively.
Finally, the NDCG over all testing documents (the global version) or all testing citation contexts (the local version) is averaged to yield a single qualitative metric for each recommendation problem.
Time: We use the running time as an important metric to measure the e ciency of the recommendation approaches.
Table 1 evaluates the quality of di erent candidate set retrieval techniques (see Section 4 for notation and detailed descriptions).
Here we measure coverage (the average recall of the candidate set with respect to the true bibliography of a testing document) and candidate set size.
A good candidate set should have high recall and a small size since large candidate sets slow down the  nal ranking for all recommendation systems.
For context-aware methods, we feel LC100+G1000 achieves the best tradeo  between candidate set size and coverage.
For context-oblivious methods, G1000+CitHop works reasonably well (but not as well as LC100+G1000).
However, the retrieval time of context-oblivious methods is around 0.28 seconds on an 8-node cluster.
On the other hand, the retrieval time of context-aware methods ranges from 2 to 10 seconds on the same cluster (depending on the number of out-link contexts3 of a query manuscript).
Our goal for the  nal system is to use LC100+G1000 and to speed up retrieval time using a combination of indexing tricks and more machines (since this retrieval is highly parallelizable).
Note, however, that ranking techniques are orthogonal to candidate set retrieval methods.
Thus, when we compare our ranking methods for recommendations with baselines and related work, we will use LC100+G1000 as the common candidate set, since our eventual goal is to use this retrieval method in our system.
In this section, we compare our context-aware relevance model (CRM for short, Section 5) with other baselines in global recommendation performance, since the related work only focused on recommending the bibliography.
5 out-link contexts per testing document.
(cid:7)  1
 (cid:8)    1     (cid:6) Recommendation Quality We compare CRM with 7 other context-oblivious baselines: HITs [13]: the candidates are ranked w.r.t.
their authority scores in the candidate set subgraph.
We choose to compare with HITs because it is interesting to see the di erence between the popularity (link-based methods) and the relevance (context or document based methods).
Katz [15]: the candidates are ranked w.r.t.
the Katz i  iNi, where Ni is the number of unique paths distance, of length i between the query manuscript/context and the candidate and the path should go through the top N similar documents/contexts of the query manuscript/context, and  i is a decay parameter between 0 and 1.
We choose to compare Katz because this feature has been shown to be the most e ective among all text/citation features in [23] for document-based global recommendation.
Note that [23] used the ground truth to calculate the Katz distance, which is impractical.
l-count and g-count: the candidates are ranked according to the number of citations in the candidate set subgraph (l-count) or the whole corpus (g-count).
textsim: the candidates are ranked according to similarity with the query manuscript using only title and abstract.
This allows us to see the bene t of a context-aware approach.
di usion: the candidates are ranked according to their topical similarities which are generated by the multinomial di usion kernel [14] to the query manuscript, K( 1,  2) = t arccos2(  2) 2 exp (4 t) , where  1 and  2 are topic distributions of the query manuscript d1 and the candidate d2 respectively, t is the decay factor.
Since we only care about the ranking, we can ignore the  rst item and t.
We choose to compare with it because topic-based citation recommendation [24] is one of related work and the multi-nomial di usion kernel is the state-of-the-art tool in topic-based text similarity [8].
We run LDA [5] on each candidate set online (by setting the number of topics as 60) to get the topic distributions for documents.
mix-features: the candidates are ranked according to the weighted linear combination of the above 6 features.
We choose to compare it because in [23], a mixture approach considering both text-based features and citation-based features is the most e ective.
Figures 5 (a), (b) and (c) illustrate their performances on recall, co-cited probability and NDCG, respectively.
Among all methods, g-count is the worst one simply because it is measured over the whole corpus, not the candidate set.
Context-oblivious content-based methods like textsim and di usion come to heel, indicating that abstract/title only are too sparse to portray the speci c citation functions well.
Moreover, they cannot  nd the proper related work that uses di erent conceptual words.
The di usion is better than textsim, indicating that topic-based similarity can capture the citation relations more than raw text similarity.
The social phenomenon that the rich get richer is also common in the citation graph, since the citation features including l-count, HITs, and Katz work better than the abstract/title-based text features.
Interestingly, [23] claimed that citation features did a poor job at coverage.
But on our data, they have higher recall values than text features.
A combination of these features (mix-features) can further improve the performance, especially on the recall and NDCG, which means that if a candidate is recommended by multiple features, it (b) co-cited probability (c) NDCG Figure 5: Compare performances for global recommendation.
should be ranked higher.
An interesting  nding is that the performance of HITs (especially on NDCG) increases significantly after more candidates are returned, indicating that some candidates with moderate authoritative scores are our targets (people may stop citing the most well-known papers after they become the standard techniques in their domains and shift the attentions to other recent good papers).
Katz works the best among single features, partially because it implicitly combines the text similarity with the citation count.
It works like the collaborative  ltering, where candidates often cited by similar documents are recommended.
Finally, our CRM method leads the pack on all three metrics, implying that after considering all historical in-link citation contexts of candidates (then the problem of using di erent conceptual words for the same concept would be alleviated), CRM is e ective in recommending bibliography.
Ranking Time Time is not a major issue for most ranking algorithms except for topic-based methods, where LDA usually took tens of seconds (50   100) for each new candidate set.
Thus, topic-based methods including di usion and mix-features are not suitable for online recommendation systems.
All other ranking algorithms need less than 0.1 seconds.
Limited by space, the detailed comparisons are omitted here.
Local recommendation is a novel task proposed by our context-aware citation recommendation system.
Here, we do not compare with the above baselines because they are not tailored for local recommendation.
Instead, we evaluate the impact of the absence/presence of the global context and other local contexts, and analyze the problem of context-aware methods.
If a user only inputs a bag of words as the single context to request recommendations, we can then only use Equation 2 to rank candidates.
We name this method as CRM-singlecontext.
If a user inputs a manuscript with placeholders inside, we can then rank candidates for each placeholder using Equation 3.
We name this method as CRM-crosscontext.
Figure 6 illustrates the performance of these two kinds of local recommendations on recall, co-cited probability and NDCG.
CRM-crosscontext is rather e ective.
In Figure 5 (due to (cid:4)(cid:15)(cid:15)(cid:15)(cid:15) (cid:18) (cid:25) (cid:24) (cid:17) (cid:22) (cid:23) (cid:22) (cid:17) (cid:21) (cid:2) (cid:27) (cid:4)(cid:15)(cid:15)(cid:15) (cid:4)(cid:15)(cid:15) (cid:4)(cid:15) (cid:4) (cid:16)(cid:17)(cid:18)(cid:18)(cid:19)(cid:20)(cid:2)(cid:21)(cid:17)(cid:22)(cid:23)(cid:22)(cid:17)(cid:24)(cid:25)(cid:18) (cid:26)(cid:17)(cid:22)(cid:2)(cid:21)(cid:17)(cid:22)(cid:23)(cid:22)(cid:17)(cid:24)(cid:25)(cid:18) (cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:13)(cid:5)(cid:5)(cid:7)(cid:5)(cid:9)(cid:10)(cid:4)(cid:7)(cid:4)(cid:14)(cid:13) (cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:5)(cid:6)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:4)(cid:7)(cid:8)(cid:11)(cid:12) (cid:24) (cid:13) (cid:11) (cid:18) (cid:11) (cid:23) (cid:17) (cid:23) (cid:14) (cid:15) (cid:22) (cid:7) (cid:1)(cid:2)(cid:6) (cid:1)(cid:2)(cid:5) (cid:1)(cid:2)(cid:4) (cid:1)(cid:2)(cid:3) (cid:1) (cid:4) (cid:4)(cid:15) (cid:4)(cid:15)(cid:15) (cid:27)(cid:2)(cid:26)(cid:17)(cid:18)(cid:22)(cid:24)(cid:28)(cid:17)(cid:21)(cid:23)(cid:29)(cid:2)(cid:21)(cid:17)(cid:22)(cid:23)(cid:22)(cid:17)(cid:24)(cid:25)(cid:2)(cid:21)(cid:24)(cid:25)(cid:22)(cid:19)(cid:9)(cid:22)(cid:18) (a) scatter distribution (cid:7) (cid:7)(cid:1) (cid:7)(cid:1)(cid:1) (cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:11)(cid:16)(cid:17)(cid:18)(cid:9)(cid:16)(cid:11)(cid:13)(cid:17)(cid:13)(cid:11)(cid:14)(cid:19)(cid:9)(cid:16)(cid:14)(cid:19)(cid:13)(cid:20)(cid:21)(cid:13)(cid:12) (b) probability distribution Figure 7: Correlation between count/probability of missed/hit citations and their number of historical in-link citation contexts.
crawling and OCR issues, our testing documents have an average of 5 out-link citation contexts that point to documents in our corpus, so  top 5 per context  corresponds to  top
 is very close to CRM in global recommendation.
We know CRM-crosscontext and CRM have the same input (the same amount of information to use).
However, CRM-crosscontext tackles a much harder problem than CRM, and has to assign citations to each placeholder.
Thus, CRM-crosscontext is more capable than CRM.
CRM-crosscontext outperforms all baselines of global recommendation and thus is also superior than their local versions.
Limited by space, we omit the details here.
CRM-crosscontext is able to e ectively rank candidates for placeholders.
For example, more than 42% original citations can be found in the top 5 recommendations, and frequently co-cited papers (w.r.t.
original citations) also appear early as indicated by NDCG.
On the other hand, CRM-singlecontext uses much less information (without global context and other coupling contexts) but still achieves reasonable performance.
For example, if a user only inputs 100 words as the query, around 34% original citations can be found in the top 5 list.
One may wonder what would happen to some documents in the corpus which do not have enough in-link citation contexts in history.
Given an original citation from a query manuscript, we examine the correlation of its missing/hit probability to its number of historical in-link citation contexts in CRM, shown in Figure 7.
(b) co-cited probability (c) NDCG Figure 6: Evaluate local recommendations.
The correlation results clearly indicate that the missing/hit probability of an original citation declines/raises proportionally to its number of historical in-link contexts.
In fact, for those new corpus documents without any in-link contexts, our context-aware methods can still conduct context-oblivious document-based recommendation, except that we still enhance the speci c citation motivations of a query manuscript using its out-link local contexts.
In this paper, we tackled the novel problem of context-aware citation recommendation, and built a context-aware citation recommendation prototype in CiteSeerX.
Our system is capable of recommending the bibliography to a manuscript and providing a ranked set of citations to a speci c citation placeholder.
We developed a mathematically sound context-aware relevance model.
A non-parametric probabilistic model as well as its scalable closed form solutions are then introduced.
We also conducted extensive experiments to examine the performance of our approach.
In the future, we plan to make our citation recommendation system publicly available.
Moreover, we plan to develop scalable techniques for integrating various sources of features, and explore semi-supervised learning on the partial list of citations in manuscripts.
This work is supported in part by the National Science Foundation Grants 0535656, 0845487, and 0454052, an NSERC Discovery grant and an NSERC Discovery Accelerator Supplements grant.
All opinions,  ndings, conclusions and recommendations in this paper are those of the authors and do not necessarily re ect the views of the funding agencies.
