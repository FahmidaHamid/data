In a world where information becomes available in ever increasing quantities, document classi cation plays an important role.
Obvious applications range from search engines to spam  ltering, but also more speci c tasks can be approached with the same techniques.
In this paper we address a particular problem when the documents are reviewed by legal experts in large corporate litigation cases.
In common law judiciary systems, during the pretrial process of discovery, litigants are commanded by means of a subpoena to bring any relevant documents to the court.
In cases involving large corporations, this means that lawyers have to go through the records Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
that those are obliged to keep and produce any responsive documents (i.e. that could be of importance to the case).
The number of documents under review could easily run in the millions.
The review of documents has to be done by legal experts and is very time-consuming.
Even for human annotators the accuracy is not very high; moreover an important mismatch usually exists between annotations by different people.
It turns out that both speed and accuracy of reviewers can be improved dramatically by grouping and ordering documents.
Different technologies have been developed to support human annotators that mine the corpus structure and present documents in a natural order [3, 13].
Some take into account the textual contents of documents only.
Obviously, other levels of description, such as the group of people that worked on the document or the visual layout, could be of importance in distinguishing relevant items.
This idea could be applied in a multitude of different tasks, but classi cation towards responsiveness provides an ideal testbed since document review intuitively takes different criteria into consideration, not just the textual contents.
The publicly available Enron Corpus consists of about 250,000 emails from the accounts of 150 top managers of Enron Corporation around the time of the collapse of this U.S. energy giant.
Besides coming from a community that has been a subject to corporate litigation, emails allow one to successfully construct an alternative representation of documents.
Considerable structure may be hidden within a large body of emails from a community: the social network that is implicit in the group of people communicating one to another.
In the following, we will use the term of multi-modality to refer to different levels of document description to aid classi cation tasks.
Considering responsive documents in the Enron Corpus, we target improving classi cation performance in a one-class setting by combining classi ers of different modalities.
Being speci cally applied to the problem of distinguishing responsive documents in a corpus of emails, we hope that the same principles might be successfully applied to similar classi cation problems.
The contributions of this paper can be summarized as follows:   Working in a one-class setting we adopt a semi-supervised approach based on the Mapping Convergence framework [31].
For broader applicability we propose an alternative interpretation that allows to size down the requirement on the natural separability of positive and negative items.
  We propose an extension to the one-class evaluation framework which turns to be useful when a small number of positive training examples are available and the ratio of positives is unknown.
We use UCI and INEX08 datasets to prove its usefullness.
ables us to take advantage of the availability of multiple redundant views on the data.
We evaluate the Mapping Co-Convergence on the responsiveness task in Enron Corpus.
  We propose a way to turn the social network that is implicit in a large body of electronic communication into valuable features for classifying the exchanged documents.
  We show that a combination of text-based and social network-based modalities does improve classi cation results.
The paper is organized as follows.
In Section 2 we introduce the responsiveness task on the Enron Corpus, then we report on the cleaning steps required to preprocess the data set.
In Section 3 we present the text-based representation of documents, and one based on the implicit social network.
Then in Section 4 we discuss the one-class setting and the Mapping Convergence principle.
We extend the one-class evaluation framework and describe how to combine multiple modalities by co-training.
In Section 5 we present a number of evaluation results obtained on the INEX08 and Letter Recognition datasets and the Enron Corpus.
Section 7 enumerates the most important conclusions of our study.
Large bodies of documents, resembling the ones when reviewing for corporate litigation, are very rare to appear in the public domain.
Both privacy-related and legal issues make often such collections, in the rare case that they are assembled at all, unpublish-able.
Consequently, the Enron Corpus is, with respect to its sheer size and completeness, unique in its kind.
Containing all emails sent and received by some 150 accounts of the top management of Enron and spanning a period of several years, the total number of messages reaches about 250,000.
Almost no censorship has been applied to the contents, resulting in a vast variety of subjects ranging from business related topics to strictly personal messages.
It is no wonder that the Enron Corpus opened up completely new possibilities to the research, including subject classi cation [18], folder detection [3], social network analysis [27], hierarchy detection [25], identity modeling [10], reconstruction of threading [30] and large (email) network visualization [13].
Our approach is complementary to previous work in that we explicitly use the multi-modality of emails for classi cation.
Emails consist of text, but also implicitly instantiates a social network of people.
We model these distinct levels and combine them to classify towards responsiveness, whether or not the message is of interest with respect to a particular litigation trial.
Intuitively such a decision requires an integration of different aspects: not only the topic of an email, but also the sender and receivers are relevant.
There exist several versions of the Enron Corpus available online.
Besides the raw text corpus [8], a relational database version is also available [27].
In this database, however, some important information regarding the social network has been discarded.
This information is very important in our approach, thus we construct a database from scratch.
When large corporations get involved in litigation and receive a subpoena to produce evidence regarding some case, they have to go through enormous amounts of data to obey the court order.
The number of documents to be taken into account can run in the millions.
Review has to be done by legal experts working at a rate of
 nomic interest of speeding up and improving the process of  nding documents that are responsive (i.e.
considerable.
relevant) to the subpoena is Different studies have shown that tools that group and order documents yield good results.
In order to support human document review one would require a large set of pre-annotated data.
Several attempts have been made to manually annotate fragments of the Enron Corpus [3].
All of them are relatively small and typically annotated with subject, emotional tone or other primary properties.
Annotation with responsiveness is tedious and expensive, requires legal expertise and is usually not publicly available.
Our solution is to use the lists of government exhibits published on the website of the United States Department of Justice (DOJ) 1.
More speci cally we use the set of emails used in the trials against the two CEO s of Enron, Ken Lay and Jeff Skilling; this set actually re ects the part of the corpus that has been found of interest.
From the DOJ we obtain a list of 186 emails.
Being a very small set compared to the size of the entire corpus, we expect that the set covers most types of emails found to be relevant, yet we expect that a lot of tokens did not make it there.
The corpus potentially contains many practically identical messages that do not appear on the exhibit list.
Working with the DOJ set means that we have to work in a one-class setting.
The algorithm we will develop in the next sections will be speci cally aimed at classifying items based on a very small set of positive training examples and a large amount of unlabeled data explicitly using multiple views of the objects.
The Enron Corpus contains a large number of duplicate messages, ambiguous references to actors and other inconsistencies.
The  rst problem is due to the automatic generation of the email folders (e.g.
all documents, sent items) and emails copies appearing both in sender s and receiver s folders.
We unify identical messages using their titles and body digests.
The second problem is the existence of ambiguities and inconsistencies among the references in sender and receiver  elds.
To extract the implicit social network, we identify the references which are pointing to the same person.
For example,  Richard Buy ,  Rick Buy ,  Richard B. Buy ,  richard.buy@enron.com ,  rbuy@ect.en-ron.com ,  Buy-R , etc.
probably all refer to the same person.
We use regular expressions to extract  rstname, lastname and, when available, email address from all references.
Ee  rst reassemble references occuring in the headers of the same message.
Then we relate them to references in other messages.
An actor is a collection of references pointing to the same person.
We use the email address as a primary cue.
Secondly, for each yet unidenti ed reference we search for possible matches in the set of actors with the same last name based on  rst name, pre x, middle name and/or nicknames (from a list of common US English nicknames).
After the message deduplication and reference resolution, the resulting database contains 248,155 messages and a network of
 and inconsistency may still exist.
Key characteristics of the resulting set are displayed in Figure 1.
In the  rst plot, we see that the frequency of words follows the power law distribution.
The second plot shows the distribution of message sizes.
The main peak is around 11 words, with most mass for lengths between 10 and 300 words.
Importantly, the average email is a relatively short text, one more reason to try to use other properties in classi cation.
The third plot shows that even though 1http://www.usdoj.gov/enron/jury_trial.htm.
message size Average: 150.6



 s e g a s s e m f o #

 word




 # of words

 s e c n e r r u c c o f o #




 s e g a s s e m f o #





 messages per sender Average: 12.3



 receivers per message Average: 6.2 i s r e v e c e r f o #


 actor

 message
 Figure 1: Characteristics of the resulting data set.
there are some very active actors in the network, most are sending very few emails.
The number of receivers per message  ts the power law distribution; there are some emails with an enormous amount of receivers (up to 1000), but most communication is aimed a small group of recipients.
The most obvious level of document representation is the textual content.
It includes the body and subject  elds in a bag-of-words representation, that has proven to be well suited for classi cation.
The idea is to construct a vector where each feature represents a word in the lexicon and the feature values express some weight.
Then we propose a representation based on the role of senders and receivers of the message in the network of people exchanging information.
Intuitively this second level of description also plays a role for human annotators in deciding whether or not a document is responsive to a subpoena.
We generate the lexicon by applying a number of criteria, in particular, we choose terms with a minimum occurrence of 4 in the entire corpus and a minimum length of 3 characters.
Also, all keywords are stemmed with a Porter stemmer and lowercased.
We then construct vectors with each feature representing one word.
The document frequency df of a word w is the number of documents it occurs in, the term frequency tf is the number of word occurrences in a document d, N is the number of documents.
For the experiments, we consider three different text representations: binary, frequency and tf-idf values, de ned as follows: 1 if w occurs in d 0 otherwise binary(w,d) = j frequency(w,d) = tf tf-idf(w,d) = tf   log(N/df ).
The feature set based on a bag-of-words representation are high-dimensional (around 105,000) and the feature vectors are very sparse.
This makes it particularly suited for the SVM classi cation with a linear kernel [14].
There is a lot of redundancy in the feature vector: a document typically has a large number of cues that signal a particular classi cation.
Another problem with high-dimensional feature spaces for one-class classi er is the so-called curse of dimensionality [11].
It has been observed that noise hurts performance signi cantly, especially in the case of learning from unbalanced data.
To investigate this effect, we test two alternative ways to reducing the feature space.
The  rst is based on the feature selection where we select a subset of F words the highest document frequency.
The other way of reducing the dimensionality is by semantic clustering of features.
We use a soft clustering approach proposed in [2].
Maximal cliques in the co-occurrence graph are clustered to obtain a score indicating the probability a word belongs to each cluster.
Using this approach we obtain feature vectors of length 6,522, where each feature represents a certain semantic  eld.
Ambiguous words contribute to multiple features.
Network structures have received a lot of attention in scienti c literature and have proven to be useful in diverse  elds like sociology, biology, computer science and epidemiology [19].
With the advent of techniques to handle large graphs and the emergence of huge, real-life linked structures on the internet, structure in social networks has become a subject of intensive research.
The structure of a large email corpus, like Enron, is not homogeneous.
The lines of communication implicitly instantiate a network of actors.
By setting thresholds on the minimal number of emails per connection, one can generate graphs with different levels of connectedness.
We set this threshold to 2, making sure to keep the majority of the traf c while discarding any accidental links with no or little meaning.
Due to the power law distribution of the connection strength, this reduces the number of actors considerably, without losing much of relevant information.
We take the largest connected component (95% of the actors) of this graph, resulting in a network of 30,094 actors.
We expect our social network features to represent the position/role of correspondents in the corporate network.
It has been shown that certain properties of nodes in a communication graph can serve very well to automatically detect the social role of actors in the network [25].
We adopt and report below a set of commonly used features to represent key properties of actors [5, 25].
The communication network is a directed weighted graph G = (cid:2)V, E(cid:3), where V contains n nodes and the weight w of edge (s, t)   E re ects the activity from s to t (the number of emails sent).
The  rst feature represents the activity of the actor in the network: (v,t) E w(v, t).
1. the number of emails sent, m(v) = In hypertext classi cation two features, that represent the authority that is assigned to nodes by its peers, have proven to be very valuable.
Nodes with a high number of incoming edges from hubs are considered to be authorities, nodes linking to a large number of authorities are hubs [15].
2. hub score h(v) is given by v-th element of the principal eigenvector of AAT where A is the adjacency matrix corresponding to graph G; 3. authority score a(h) is given by v-th element of the principal eigenvector of AT A.
The next group is a set of different centrality measures to model the position of the node in network.
These depend on a undirected = (cid:2)V, E(cid:3)(cid:3) of graph G. We calculate the unweighted version G(cid:3) shortest paths in G(cid:3) using a Matlab library for working with large graphs [12].
We obtain the distance dst from node s to t and the number  st of paths from s to t. The number of paths from s to t via v is denoted as  st(v):


 s V dvs ;
 s(cid:4)=v(cid:4)=t  st(v)  st ;

 maxt d(v,t) ; s(cid:4)=v(cid:4)=t  st(v).
One more feature characterizes the connectedness in the direct neighborhood of node v:
 (v, s),(v, t), (s, t)   E(cid:3) .
2|(s,t)| (deg(v)(deg(v) 1)) : The  nal group calculates all cliques in the graph using a Matlab implementation of [6].
It includes three following features: 10. the number clq(v) of cliques the actor v is in; 11. a raw clique score where each clique in clq(v) of size n is given a weight 2n 1, CSR(v) =
 q clq(v) 2size(q) 1; 12. a weighted clique score where each clique is weighted by the sum of activities of its members, CSw(v) = size(q) 1
 m(w).
q clq(v)
 w q All scores are scaled to a value in [0, 1] range, where 1 indicates a higher importance.
Each node can be assigned an aggregated social score [25] which is a linear combination of all 12 features, with all features equally weighted.
Note we are not classifying the nodes (actors) in the social network, but messages that have been sent and received by these actors.
To translate the properties of actors to properties of emails, we construct a set of 37 features to represent each message.
A message is represented by three sets of 12 features, the  rst is the properties of the sender, the second the average of the properties of all receivers and the third is the properties of the most prominent receiver (i.e. with the highest social score).
The last feature is the number of receivers.
This set of features based on the social network implicit in the corpus represents a quanti cation of the sender and receiver characteristics of each message.
The responsiveness problem in the Enron Corpus is typical for Information Retrieval settings.
A small set of positive training examples is relatively straightforward to obtain (e.g.
the medical records of patients that are diagnosed with a particular disease or the emails on the DOJ exhibit list).
However it is often very dif cult (if not impossible) to develop a set of objects that re ects the complement of the positive class.
Three key assumptions of the problem are the following:   In a very large set of documents, we have a small set of truly positive examples P = (x1, .
.
.
, xl) completed with a large unlabeled set U = (xl+1, .
.
.
, xl+u);   The ratio between positive and negative items in the corpus is unknown, but assumed to be unbalanced where the positives are the minority class;   The positive items are assumed to be drawn from a certain distribution, whereas the negatives are everything else.
In the next subsection we de ne an Support Vector Machines algorithm that is capable of constructing hypotheses based on positive trainings examples only.
It turns out that in practice those are quite sensitive to choice of features and parameter settings and for that reason hard to implement; under tting and over tting are always close [11, 17, 24].
We therefore adopt a semi-supervised framework, Mapping Convergence by [31], that copes with the positive examples and the huge amount of unlabeled data.
Our core idea is to con dently assign examples from the unlabeled set as negatives in order to support a process of convergence towards an optimal hypothesis.
We adapt the existing one-class interpretation to start with a very sparse set of positives and still obtain a good estimate of the performance of the generated hypotheses.
Then we extend the algorithm implementing ideas from co-training [4] to combine complementary classi ers.
For Support Vector Machines, a few extensions have been proposed to enable learning in a one-class setting.
Both the Support Vector Data Description algorithm (SVDD) [29] and One Class Support Vector Machines (OC-SVM) [26] are shown to be equivalent when data vectors are scaled to unit length [28].
We will use OC-SVMs, it allows us to formulate the optimization problem as in  -SVM and keep the intuitivity of parameter  .
The main idea behind OC-SVMs is to create a hyperplane in feature space where the projections of data are separated from the origin with a large margin.
The data is separable from the origin if there exists such a vector w that K(w, xi) > 0,  i.
For the special case of a Gaussian (Radial Basis Function, or RBF) kernel, the following two properties guarantee this: For K(xi, xj) = e (cid:5)xi xj(cid:5) :  i, j K(xi, xj ) > 0  i.
K(xi, xi) = 1 (1) (2) It results in all mappings being in the positive orthant and on the unit sphere and being much tighter than for other kernels.
As it is pointed out in [26], there exists a strong connection between OC-SVMs and binary classi cation.
Supposing we have a parametrization (w,  ) for the supporting hyperplane of a data set {x1, .
.
.
, x(cid:4)}, we also have that (w, 0) is the parametrization of the maximally separating hyperplane for the labeled data set {(x1, +1), .
.
.
, (x(cid:4), +1), ( x1, 1), .
.
.
, ( x(cid:4), 1)}.
Also, supposing that we have a maximally separating hyper-plane parametrized by (w, 0) for a data set {(x1, y1) .
.
.
, (x(cid:4), y(cid:4))} and with a margin  /(cid:6)w(cid:6), we know that the supporting hyper-plane for {y1x1, .
.
.
, y(cid:4)x(cid:4)} is parametrized by (w,  ).
For the non-separable case, margin errors in the binary setting correspond to outliers in the one-class case.
To  nd the supporting hyperplane for a distribution, we solve the optimization problem of  -SVM.
Slightly extending the conventional interpretation, in a one-class setting   represents an upper bound on the fraction of outliers (margin errors) and a lower bound on the number of support vectors.
There exist two types of problems where one-class learning is particularly attractive [33].
In the  rst case, a majority set is well-sampled, but it is hard to sample from the minority class.
This is the case in [20, 21] where an ensemble of one-class classi ers is used to detect malicious traf c on a computer network.
The basic assumption is that most traf c is normal and attacks are atypical, thus reducing the problem to outlier detection.
In the second type of problems, the target class can be accurately In regular sampled, but the data appears extremely unbalanced.
h1 h2 h3 hMC h n h 3 h 1 h 0 bp b p Figure 2: The Mapping Convergence in 1-dimensional space.
multi-class classi cation, this would result in a bias towards the majority class, by rebalancing (of which one-class classi cation is the extreme) this problem can be circumvented [24].
In a comparative study by Manevitz [17], the performance of OC-SVM is compared to other algorithms that are applicable in a one-class setting, such as Rocchio, Nearest Neighbour and Neural Nets.
It turns out that OC-SVM and Neural Networks are the two most powerful algorithms.
Their main conclusion with respect to OC-SVM is that its performance is very sensitive to choice of parameters and features.
Yet, it remains computationally much less intensive than Neural Networks.
To bene t from a large set of unlabeled data, the Mapping Convergence (MC) [31] assumes the existence of a gap between negative and positive data and tries to exploit it.
A basic intuition behind the MC is that, given a hypothesis h, items that are further from the decision boundary are classi ed with a higher probability.
In Algorithm 1, this  rst approximation  N0 of the negative distribution serves as input for the converging stage to move the boundary towards the positive training examples P .
At iteration i, it trains an SVM on P and the constructed negatives N. The resulting hypothesis hi is used to classify the remaining unlabeled items.
Any unlabeled items that are classi ed as negative are added to the negative set.
The converging stage is iterated until convergence, when no new negative items are discovered and the boundary comes to a hold.
Figure 2 from [31] is particularly instructive for understanding the MC.
It shows seven clusters in 1D space, where the fourth cluster from the left is positive, but all data is unlabeled except for the dark subset of the positive cluster.
The optimal boundary is represented by the dashed lines, but a generic one-class algorithm would end up tightly  tting the positive training data on (bp, b(cid:3) p).
Algorithm 1 Mapping Convergence Require: positive data set P unlabeled data set U negative data set N =  

  P0   remaining part of U Ensure: boundary function hi

 3: i   0



 hi+1   train C2 on P and N  Ni+1   negatives from  Pi by hi+1  Pi+1   positives from  Pi by hi+1 i   i + 1
 9: end while Mapping Convergence is able to take advantage of the underlying structure of the unlabeled data and to give a good approximation of the optimal boundary.
The initial hypothesis places the boundary on (h0, h(cid:3)
 is far away from the one-class boundary.
The convergence step that follows moves the boundary to (h1, h(cid:3) 1), adding the unlabeled data that is recognized as negative by h1 to N. This process is iterated until no new data is added and the boundary remains  xed.
Each new hypothesis hi+1 maximizes the margin between hi and bp.
When the new boundary is not surrounded by any data, it retracts to the nearest point where the data does live (cf.
where the boundary moves from h2 to h3).
For the algorithm to converge, there must live no data in the volume covered by the  nal step.
For this reason, to be sure that it does not over-iterate, MC depends on a gap between positive and negative data of at least half the distance between hi 1 and bp.
Because this condition gets easier to meet as the boundary gets closer to bp, it is unlikely that the algorithm selects a boundary that is very far from the training data.
On the right-hand side, there is no gap that stops the convergence, resulting in the boundary being placed on bp.
There is a trade-off in deciding how much data to put in  N0 in the mapping stage.
On the one hand, a bigger initial set of arti- cial negatives will better cover the distribution of the negatives.
On the other, putting items in  N0 that do not belong there results in poor performance because the effect will accumulate in convergence.
Our experiments show that a proportion of 5   10% is enough to support the convergence and does not hurt the performance on labeled positives P very much.
It has been shown that the MC performance dramatically decreases when the positive training set is severely undersampled [31].
This is explained by its incapacity to detect the gap between positive and negative data when too much unlabeled items act as noise.
Iterations will not stop, causing the algorithm to over t.
To better understand the MC dynamics, we created random data in R2 with no gap between positive and negative data.
The dataset has 75 positives and 1,225 unlabeled data points, of which 1,000 are negatives and 225 are noise.
We randomly generate Gaussian centroids of positive data, surrounded by negative data.
The noise is added by switching the classi cation of 1% of the data.
80% of the data is selected randomly for training and the remaining 20% is used for testing 2.
The data as well as the MC iterations are shown in Figure 3.
Figure 3.a displays the distribution of negatives (dots), unlabeled positives (diamonds) and labeled positives (circles).
The MC takes as input P the labeled positives only, in Figure 3.b all unlabeled data U is represented as dots.
The mapping stage (iteration 0) creates a conservative hypothesis excluding only a small proportion of U; its decision boundary is in Figure 3.b.
From that the convergence proceeds (Figures 3.c to 3.e), with each iteration decreasing the number of unlabeled items from U.
The performance on the
 b.
0: mapping c. 2: converging.
.
.
d. 5: optimal e. 15: over tting f. performance curve











 convergence











 Figure 3: Random data in R2: representation of Mapping Convergence and the  P P performance curve.
positives P starts decreasing, until 15-th iteration where over tting takes place, as shown in Figure 3.e.
Meanwhile, the  fth iteration produces the classi er with a fairly accurate description of the data.
Finally, the plot in Figure 3.f shows how the hypotheses hi, i = 0, 1, 2, .
.
.
are performing on separating out a small proportion of the entire data set (x-axis), while maintaining performance on the positive subset (y-axis).
This novel performance measure is formally de ned in the following section.
Unlike the standard measures like precision/recall used on fully annotated data, the main advantage of the novel measure is its applicability in the truly one-class setting, when the true labels of U are unknown.
When working in the one-class setting, like the responsiveness in the Enron Corpus, we want to know when to stop the iterations before over tting occurs.
We will consider the convergence as a sequence of hypotheses, where each step is represented as a decision boundary in the feature space, and identify the hypothesis hi that maximizes some performance measure.
The key idea is that hi should address a small part of U that retains a large part of the positive items from P .
In that way we will  nd a hypothesis that strikes a good bias-variance balance.
 P P measure In the following, we use curves like the one in Figure 3.f to track the distinguishing power of a classi er.
The MC produces a sequence of classi ers hi, i = 0, 1, 2, .
.
.. For each hi, we plot on x-axis the percentage of the entire data set classi ed positively,  Pi/|U|.
On the y-axis, we plot the percentage of the positives that i (P )|/|P|.
We call it  P P mea-is found within that space, |P   h+ sure.
The one-class classi er h0 used in the mapping phase produces the  rst (unconnected) point in the plane.
Then we construct the curve, starting from the right-top with the mapping hypothesis and moving left and down with each step.
The curve of  P P measures can be interpreted in a ROC-like fashion.
The upper left corner represents a perfect classi er, points on the diagonal are random selection from U.
On each iteration step of MC, a classi er is generated that gives a point on a curve like Figure 3.f.
The  rst point in the convergence will be close to the upper right corner: the mapping stage is about selecting a small part of the data set containing only near-certain negatives.
From there each iteration will lead to a smaller selection (it moves left), but potentially also a lower performance on the true positives (it moves down).
A large step leftwards corresponds to a large step in the convergence: a lot of unlabeled data is identi ed as negative.
A large step downwards signals a big loss in performance.
Contrary to a genuine ROC curve, the P  P curve is not continuous, but a sequence of points (xi, yi), on the plane.
Points (0,0) an (100,100) refer to the two naive decisions of retaining none or all data in P and U.
The  rst choice of the measuring is to build a piece-wise linear function induced by points (0,0), (xi, yi), (100,100).
Unfortunately, the step-like behaviour of points (xi, yi) is not guaranteed.
As we will see in Section 5, they may have an erratic behavior that makes impossible to calculate the area under the curve (AUC).
An alternative to the AUC is to identify the point in the P  P curve that discard most of data in U, while keeping a large part of the positive data P to be classi ed correctly.
The point on the curve that is closest to (0, 100) is considered as the best classi er.
The distance measure can be weighted to assign more importance to recall or precision, in this paper we will use the Euclidean distance.
When applying the framework described above to the Enron case, we should estimate the percentage of positives returned by each hypothesis hi when no labels are available for the largest part of the data set.
To calculate the percentage of data that is returned, we can simply leave out part of the unlabeled data in the training phase and use a prediction on that to estimate the performance.
As a solution, we introduce a cross-validation step in the algorithm.
Each step in MC process is carried out with a 5-fold split over the positive data P .
We train a hypothesis on 4 parts, and predict on the remaining  fth part and the entire unlabeled set U.
This results in exactly one prediction per item in P , and then we aggregate the  ve predictions for the items in U to obtain a solid estimate of the performance of the hypothesis.
Note that LIBSVM by default predicts class labels it can produce probability estimates for each of the classes, using a method by Platt [22] that  ts a logistic function to the output of an SVM to generate posterior probabilities.
This algorithm presumes that the equal distribution of positives and negatives in training and test sets.
This is not the case in a one-class setting, nor in the convergence steps where the distribution in the training set actually changes on each step and converges to the actual one.
Therefore we altered the LIBSVM code to directly output the distance to the decision plane and use it as a measure of the con dence of prediction.
After scaling to (0,1) we can aggregate the predictions of multiple classi ers, for example by taking averages.
A more advanced way of capturing the con dence of predictions might be subject of future research.
Note that the introduction of a random  ve-way split in all stages of the convergence introduces some irregularities in the outcome of the algorithm.
Using a higher cross-validation will solve this, but a higher computational cost.
In Section 2 we de ned two complementary views of documents in the Enron Corpus.
Combining different classi ers to improve overall performance is known as ensemble learning [9].
In this section we propose to combine the MC algorithm with the idea of co-training.
First we consider a na ve way of combining predictions.
When the MC algorithm produces a sequence of classi ers, we can generate predictions on a test set on each of the steps.
In fact this is exactly what we will do to determine the position in the performance plane, using cross-validation for the positive examples.
Now suppose that we take one classi er based on view A and a second based on view B.
Both have classi ed all items in the test sets, but potentially have made errors.
The idea is that when one of the two has made an error, it can be corrected by the second.
Since each prediction is a number in the [0,1] range and represents the con dence, we can average these predictions over multiple classi- ers.
The classi er that is most certain will  win  and, in case of an error, correct the other.
An improvement of the performance will be easily recognized as a movement up or left (or both) in  P P measure.
Moving left in the performance plane means a smaller part of U, while moving up corresponds to retaining more positives from P .
As we will see in Section 5, combining classi ers in this way results in exactly these effects.
If we can establish that combining the predictions of multiple classi ers representing different modalities does indeed aid classi- cation, it would be interesting to see if we can adapt the MC algorithm to take the different views into account on each of the steps.
We will have different classi ers cooperating in a way that closely (  10%)

 i i i


 , .
.
.
, h(n)
   train C1 on P (k) Ensure: boundary functions h(1) 1: h(k)   predict with h(k) 2: pred(k)   strong negatives
   remaining part of U (k) , .
.
.
, pred(n)
 Agg(pred(0)  P (k) 4: i   0




  k   [1, .
.
.
, n] do   train C2 on P (k) and N (k) i+1 on  P (k)   predict with h(k)   strong negatives (cid:10)=   i h(k) i+1 pred(k) i+1  N (k) i+1 Agg(pred(0) i+1  P (k) i   i + 1 i+1
 11: end while  k   [1, .
.
.
, n]  k   [1, .
.
.
, n] in U (k) by  k   [1, .
.
.
, n]  k   [1, .
.
.
, n]  k   [1, .
.
.
, n]  k   [1, .
.
.
, n] by in  P (k) i  k   [1, .
.
.
, n] Algorithm 2 Mapping Co-Convergence Require: n views of the positive data set P (1), .
.
.
, P (n) n views of the unlabeled data set U (1), .
.
.
, U (n) n views of the negative data set N (1) =  , .
.
.
, N (n) =  

 Aggregation function: Agg.
i (  5%) , .
.
.
, pred(n) i+1)   remaining part of  P (k) i resembles the concept of co-training [4].
On each step of the iteration the more con dent classi er will be able to overrule the other.
Predictions are aggregated and a  xed percentage of the items is labeled as negative.
The Mapping Co-Convergence we propose is depicted as Algorithm 2.
There are three important differences between Algorithm 2 and Algorithm 1 discussed in Section 4.2.
First, it starts with not one but n different views of the data.
Second, the views interact by means of the aggregation function that combines the predictions to select the next items to label.
Finally, in the convergence phase, not all items recognized as negative are added to N, but only the ones that are agreed upon with the highest certainty are labeled, limited to 5% of the entire data set.
Note that the two-step assemble learning is not a unique way to combine different modalities.
In Web page classi cation [23], there exist other techniques to take both content and links structure into account.
Simple approaches convert one type of information to the other.
For example, in spam blog classi cation, Kolari et al. [16] concatenate outlink features with the content features of the blog.
Other techniques [32] try to jointly process all modalities.
They use the same set of latent concepts to simultaneously capture the pairwise relationships between modalities.
One-class classi cation with the joint modeling of different modalities may be a subject of further research.
We develop our framework in Python, and use the LIBSVM library [7].
We use the linear kernel for the text-based feature sets and the Gaussian kernel with the social network-based sets.
An important parameter for both is  , that bounds the number of margin errors or outliers.
By picking a small value, we ensure that the algorithm is forced to come up with algorithms that stay true as much as possible to the training data we gave it.
We pick   = 0.1 for all experiments.
For the Gaussian kernel we also have the   parameter,









 INEX08: Removing data
 LETTER RECOGNITION: Removing data







































 Figure 4:  P P curves for different sizes of positive data.
a) INEX08, b) Letter recognition.
INEX08: Number of features
 LETTER RECOGNITION: Gamma





































 Figure 5:  P P curves.
a) Feature selection in INEX08, b)   values in Letter Recognition.
controlling the smoothness of the decision boundary.
In all experiments we use the same sets of 186 positive examples obtained from the DOJ exhibit list and 10,000 randomly selected unlabeled documents.
For testing on the positives we use 5-fold cross-validation, for testing on unlabeled items we hold out 500 items.
We  rst report some  P P curves on two reference corpora: the INEX08 Wikipedia challenge corpus [1], containing text  les with subject class labels and the Letter Recognition dataset from the UCI ML repository, containing samples of handwriting classi ed with the correct symbol.
The INEX08 corpus contains 11,437 objects described by 78,412 features representing a BOW representation with tf-idf values.
The vectors are very sparse (at an average of 103 nonzero values per document), so we will use the linear kernel.
We use classes 8 and 6 as positives (21%), while all others are negative.
The Letter Recognition data set contains 20,000 instances, described by 16 features.
Since these vectors are not sparse, the Gaussian kernel is the best option.
We use classes 0, 1, 2, 3 and 4 as positives (20%) and the rest as negatives.
In both cases we use 80% of the data to train a hypothesis and the remaining 20% to test it.
Figure 4 shows the in uence of removing positive training data on both corpora.
We start with all positive data as known positives and all of the negatives as unlabeled items.
We then remove part of the positives (50%, 25%, 12.5% and 6.25%) and observe that performance degrades: larger steps and conversion to (0,0) in the extreme cases.
Figure 5.a shows the in uence of reducing the number of features for INEX08 set.
We remove words with low document frequencies.
We perform the experiment with 12.5% of the positives and 50% as noise (ratio 1/4).
Some reduction seems to be useful (10.000 features), but too much reduction leads to over tting (100 features).
The  nal experiment in Figure 5.b shows the in uence of the   parameter of a Gaussian kernel on the Letter Recognition data.
We perform the experiment with 12.5% of the positives and
 tually jumping to (0,0) immediately.
Small values give over tting, evidenced by erratic behaviour.
Now we present our experiments on classifying the Enron corpus using different representations discussed in Section 3.
First we try to get the optimal setting of parameters to obtain good classi ers for each modality, before we will try to combine their predictions.
Text-based representations.
For the text-based representations, we  rst determine what type of feature values performs better.
As shown in Figure 6.a, The performance curve in Figure 6.a shows that both binary and tf-idf values give similar results, with tf-idf being slightly better and more stable.
We can see that during the convergence performance degrades slowly, with a drop at the end.
The key is to select a classi er that is just before the drop.
Note that the algorithm is clearly beating OC-SVM.
The algorithm takes a huge  rst step in the convergence, yielding a hypotheses that separates out 75.8% of the positives in 16.8% of the data.
Because we use a one-class classi er, the dimension of the feature space might be important.
Even though with our reference corpus we had a slight increase in performance by reducing the number of features, no such thing happens with this one.
In Figure 6.b we even see that the OC-SVM performs considerably worse with less features.
The convergence itself however is not hurt.
Also the reduction of features using semantic clustering does not seem to improve anything.
Social network-based representations.
When classifying with the social network features, we have a lot less tuning to do.
The feature values are not sparse and reducing the number of features is not considered.
However, with the RBF kernel we have an additional   parameter.
Bigger values lead to under tting because of large steps in the convergence.
Smaller values lead to under t-ting: good performance until a certain point, and erratic behaviour thereafter.
The best and most stable performance is obtained with   = 0.1, see Figure 6.c.
Optimally we select 71.5% of the positives in 9.4% of the data.
Finally we present the results of combining the classi ers from the previous section.
It turns out that in both cases best results are obtained using the bag-of-words representation with all features and tf-idf values with the social network feature set.
Na ve method.
The simplest way of combining two classi ers is to take their predictions, aggregate those (in some way) and measure the performance.
For an aggregation function we simply take the average of the predictions.
In Figure 7.a, we combine the clas-si ers we get at the 12th and 13th steps on the social network curve and the 2nd and 3rd steps on the text curve respectively.
The fact that we observe a movement towards the left-top means that we get a classi cation that takes less data while retrieving more of the positives.
Mapping Co-Convergence.
This approach combines the different views on the data on each convergence step.
Every time a limited number of objects that is classi ed as negative with the highest certainty by the two classi ers combined is labeled.
The hypotheses appearing on the curve are based on a small part of the data.
The performance as shown in Figure 7.b is very satisfactory, retaining 93% of the positives while discarding 90% of the unlabeled data.
ENRON   TEXT: Feature values
 ENRON   TEXT: Number of features
 ENRON   SOCIAL NETWORK: Gamma
 all tf idf all bin all freq




















 all



































 Figure 6: Classifying Enron Corpus.
a) text-based, b) social network-based, c)   values.
classi er text (bow tf-idf) social network text + social (na ve) text + social (MCC) part of U



 part of P



 distance



 Table 1: Comparison of the different classi ers.
As explained before, we represent any curve by its  best  classi- er.
We take the Euclidian distance to the perfect classi er (0,100), to be compare different classi ers.
Table 1 reports the best clas-si ers of all curves discussed in this section.
We can see that the combination of social network-based and text-based feature sets allows to achieve the best performance.
We would like to thank Virginia Dignum from Utrecht University for early discussions and feedback on the preliminary report.
This work is partially supported by the FRAGRANCE Project co-funded by the French Association on Research and Technology (ANRT).
From the results presented above we can conclude that our approach to document classi cation by using multiple levels of description does yield excellent results.
Implementing ideas from co-training within the Mapping Convergence framework has enabled us to dramatically improve classi cation results.
First we discuss some crucial observations with respect to our framework.
Initialization of Mapping Convergence.
It appears that the mapping phase of the framework is crucial for a good result.
Any bad classi cations made will have a snowball effect and impede performance severely.
It is however necessary to include enough data in hte  rst approximation of the negative set to support convergence.
From our experiments, labeling 5-10% with a one-class classi er is usually enough to keep the convergence going.
Dependency on parameter selection.
Even though the number of parameters is not huge, the algorithms are somewhat sensitive to the selection of parameters.
Some of them we have been able to select by looking at its properties, for others however this is not so easy to do.
As can be seen from the test results on the reference corpora their in uence is sometimes quite substantial.
Instability as a result of random cross-validation.
We introduced a cross-validation step to be able to work with a corpus which does not contain labels for much of the data.
This is however another source of instability.
The split is randomly made on every step in the convergence.
After several runs we have picked average results, but there sometimes was a discrepancy of about 10% between runs.
This could be reduced by using 10-fold crossvalida-tion or higher, at a computational cost.
Lack of training data.
From our tests it appears that the size of the initial positive set is of capital importance.
Reducing the number of positive training examples has a striking effect on the overall results.
In Enron we have a very small set of positively labeled items, that we used with a random subset of the rest of the corpus.
Using it with the entire corpus might cause the unlabeled data to  ood the positives after all.
It might be that the combination of different views in Mapping Co-Convergence partly handles this problem, but that has to be veri ed.
Con dence measure for SVM.
Research on ways to assign a con dence measure to the predictions of Support Vector Machines is an active  eld.
A well-known approach by Platt [22] is actually implemented in the LIBSVM toolkit [7] we used.
Unfortunately, because of an assumption made by this approach it is not applicable in our case.
We decided to use a scaled version of the distance to the decision plane as an indicator of con dence.
This is de nately a point that can be improved.
Aggregation function.
Also the way we combine the prediction of different classi ers is quite na ve.
In fact we take the average of the predictions.
It is clear that this is an approximation and lacks solid theoretical support, but future research is necessary to come up with a better solution.
