In many cases on the Web, given a topic query, we want to know which web document (or its author) is the one to  This research was supported by IBM and also sponsored in part by the U.S. National Science Foundation under grants IIS-08-42769 and IIS-09-05215.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
initiate the topic or the  rst one to talk about the topic.
For example, someone started a rumor about a product on the Web and generated a lot of discussions on this topic.
The company would like to know who started this rumor.
Based on our knowledge, there is no current system supports this technique or service.
General search engines, such as Google, Yahoo and Bing, only return webpages which are most relevant to the query.
For example, Google Web Search supports searching by query and returns webpages sorted by the PageRank based relevancy scores.
This method cannot  nd the topic initiator.
Google News is a service that automatically clusters new articles into groups, each of which contains articles for the same topic, and provides sorting based on relevance or date.
The problem is that the clustering results are not always correct, in some cases the articles in the same group are not about the same topic.
For example, there was a news group in the Sci/Tech section which contained 111 news articles mostly about Microsoft s next version of Internet Explorer IE 8.
Within the cluster, the  rst result was titled  Microsoft IE8 To Make Stealth Sur ng Easier .
Unfortunately, the group was wrong even for the second article titled  Mozilla steps up Firefox 3 push , which discussed Firefox 3.
Some speci c search engines provide searching by query and sorting the search results by dates.
Google News Search as a good example.
However, it only supports search for a query and simply sort the results by dates.
We will show later that only using date information is far from enough.
In addition, Google News Search only supports news articles, but our framework works for the whole Internet by integrating all the information on the web, including the news, blogs, forums, newsgroups, etc.
Another drawback of existing systems is that they only support webpage level analysis.
However, we go deeper into the web document level.
When a user wants to  nd which web document is the initiator, all the three major search engines do not work to this level of details.
In this paper we introduce a new web mining and search engine technique/service - Topic Initiator Detection (TID) on the Web.
Given a topic query, the system  nds all webpages containing the query word/words.
Then it extracts the web documents within each webpage, examples of web documents are news articles, blogs, forums and newsgroup posts.
The di erence between a web document and a webpage is that a webpage may contain more than one web document.
Several web documents may appear on the same webpage.
For example, blog articles could be posted on the same blog page.
Based on web documents, information tracted.
Finally, the system returns a list of web documents (together with the author names) ranked by their possibility to be topic initiator or be the  rst to talk about the topic.
We give a formal de nition for problem of Topic Initiator Detection (TID) on Web as follows, Input: Given a topic query q and a collection D(q) of web documents which contain the query word/words.
Suppose D(q) consists of N time-stamped web documents, D(q) = {d1, d2, , dN} and the associated time information is T = {t1, t2, , tN}, where di denotes web document i(i = 1, .
.
.
, N ) and ti represents its time stamp.
Output: The web document which initiated the topic or was the  rst to talk about the topic.
An intuitive solution for the TID problem is as follows: (1) according to the topic query, return all web documents that contain the query; (2) sort the documents by time; and (3) select the  rst one as the initiator.
The performance of this intuitive method is poor, because a web document that appears early may just happen to contain the query words, but does not really talk about the topic.
Another style of method is to use link-based algorithms, such as InDegree, PageRank [16] and HITS [5], to choose the one with the highest ranking score as the topic initiator.
However, the true topic initiator may only have a small number of citations or even not be cited by any other web documents, and a following article that appears in a popular website may get a lot of citations and obtain the highest ranking score by link.
We develop InitRank to rank the possibility of a web document to be the topic initiator, based on time, content, link and some other useful information.
We  rst introduce several topic initiator indicators, and then propose a TCL graph which integrates Time-Content-Link information and design an optimization framework to compute InitRank.
Contributions of this paper are:
 or service - Topic Initiator Detection (TID) on the Web.
Given a topic query, return which web document (or its author) initiated the topic or was the  rst to discuss about the topic.
propose algorithm InitRank to automatically  nd the topic initiator.
Based on a ranking score initialization using initiator indicators, InitRank re nes the score within a optimization framework over a TCL graph.
mance of our algorithm and verify its e ectiveness and robustness.
The most related work is the research on New Event Detection (NED) [22], which is also called Novelty Detection or First Story Detection (FSD) [23].
The task of NED is to automatically detect the earliest report for each event as soon as that report arrives in the sequence of documents.
NED is the most di cult task in the research area of Topic Detection and Tracking (TDT) [1], which is an important research area in Web Mining [12].
Most NED systems basically work by comparing a document to all the documents in the past, and use a threshold on the similarity scores to detect novel stories.
If all the similarity scores are below the prede ned threshold, the document is predicted as the  rst story of a novel event [22].
We give a brief description of several systems for NED as mentioned in [6].
The UMass (Univ.
of Massachusetts) method performs clustering   implementing a modi ed version of the single-pass clustering algorithm   on the set of time-stamped documents and returns the  rst document in each cluster as the result.
The CMU approach represents document using vector space model with term weighting and uses single-pass clustering algorithm to partition stories into di erent topic groups.
The general idea is similar to the UMass method.
The UPenn (University of Pennsylvania) approach begins with clusters of size one and merges similar clusters.
Stories are compared to the preceding ones and merge their clusters when the similarity is high enough.
If a story cannot be combined with any other existing cluster, it becomes a new cluster, thus the story is a new story.
The above systems do not work for TID because there are no multiple clusters, and, most importantly, they do not go deeper to solve the problem of how to select the right  rst story within a cluster.
The approach proposed in [6] uses TF-IDF term weighting and assigns FSD-value to each story according to some rules.
It works sequentially: (1) the  rst story in the collection is always a  rst-story (FAD-value = 0), (2) the second story is evaluated by calculating a measurement of similarity based on the occurrences of terms that were in the previous story, and (3) continue these steps for each subsequent story, the FSD-value will be lower if the story contains a large number of previously unknown terms.
A story is identi ed as a  rst-story if its FSD-value is under a threshold value.
This method does not work for TID because the  rst story in time will always have the lowest FSD-value and be identi- ed as the  rst story, which, as we have already discussed, is incorrect in many cases.
In paper [23], the authors propose a two-level approach for novelty detection: (1) using a supervised learning algorithm to classify the online document stream into pre-de ned broad topic categories, and (2) performing topic-conditioned novelty detection for documents in each topic.
The limitation of the approach is that it needs training data for classi cation, thus it is a supervised method.
It is not applicable to the TID problem which is unsupervised.
Additional di erences between TID and NED are: (1) NED works sequentially, but TID is not required to work sequentially and thus be more  exible.
(2) TID is web based and contains other related information, aside from only time and text.
We present a framework system for Topic Initiator Detection on Web.
The goal is that given a topic query, we want to know which web document is the one to initiate the topic or the  rst one to talk about the topic.
Note that it s a search service that traditional search engines, such as Google, Yahoo and Bing Search, do not support.
As shown in Figure 1, the general system framework works as follows: (1) beginning with the user submitted topic query, fetch webpages that contain the query keywords; (2) Document Attribute Description Date Domain
 Title Text ThreadId ForumId ForumName Author BBStype Publication time.
Website domain of the web document URL of the web document Title of the web document Text content of the web document Identi cation of the thread Identi cation of the forum Name of the forum Name of the author Type of BBS (Bulletin Board System) SourceType MessageBoard, Blog, News, etc.
Country LinkURL LinkDomain Query Country of the website Set of link/citation URL (cid:44)a rs Set of link/citation website domains Keyword(s) of the query give a list of synonyms and transform those di erent words to a single form.
There are a lot of noisy terms on the web and many of them only appear in very few number of web documents, we remove terms with very low document frequency, e.g.
those appearing in less than 2 web documents.
Document Similarity.
We use vector space model to represent a web document and adopt the standard tf   idf weighting method.
The weight of a term is computed as tf   (1 + log(N/df )), where df is the number of web document the term appears in.
Other weighting schemes can also be used, such as PN [21] and BM 25 [20].
To estimate Sim(di, dj), the content relationship between two web documents di and dj, we use the cosine (normalized dot product) similarity measure.
This section presents algorithm to rank the web documents for TID on the Web.
We  rst de ne normalization functions, and then introduce several basic topic initiator indicators, the  nal ranking schemes (combo ranking and graph-based re nement) in detail.
Based on the sigmoid function, we design a normalization function SN Inc() as follows, SN Inc(x) =
 1 + e x/    1 (1) SN Inc(x) is a normalized increasing function.
When x > 0, the value of SN Inc(x) ranges from 0 to 1.
A good property of this function is that small x has higher impact on the score, while big x has lower impact on the score.
Parameter   controls what the function curve looks like.
The setting of   allows us to decide where we want the change of x has little impact on the change of score.
For example, if   = 5, when x is bigger than around 50, the score will be close to 1.
The SN Dec() function is de- ned as SN Dec(x) = 1   SN Inc(x), which is a normalized decreasing function.
Indicators for Ranking the Initiator This section presents several indicators for ranking the possibility of a web document to be the topic initiator.
Figure 1: Framework for Topic Initiator Detection on the World Wide Web.
extract the web documents from the webpages; (3) extract information such as author name, date, content and links, for forums, we also extract the forum id and thread id; and (4) perform topic initiator ranking algorithm InitRank and return a sorted list.
Note that step (1) can be performed e ciently based on inverted index, steps (2) and (3) can be pre-computed.
Data Source.
Since our goal is to mine the information from the whole Web, only one type of data source is not enough.
An initiator may come from a blog website, a news website or even from a newsgroup discussion.
So in our framework, the mining process is based on data sources in the whole Web which consists of a lot of di erent types of information, such as blogs, news and newsgroups.
Web Document v.s.
Webpage.
Our analysis is based on web document level instead of webpage level.
Web documents are extracted from the webpages.
Each webpage may contain one or multiple web documents.
Some webpages even contain less than one web document.
For example, some news websites divide an article into several webpages to gain more clicks.
The webpage and web document mapping describes the relationship between a webpage and a web document.
There are three kinds of Webpage-WebDocument mapping: One-One, One-Multiple and Multiple-One.
One-One maps one webpage to one web document, One-Multiple maps one webpage to multiple web documents.
For example, a blog page may contain multiple posts.
Multiple-One maps multiple webpages to one web document.
Information Extraction.
For each web document, our system extracts many related information, as listed in Table 1.
The attribute Date is the time the web document was published online, not the date entity, if there is any, extracted from the document content.
Document Preprocessing.
The web documents go through the following preprocessors: stopword-removal [4], synonymy, stemming [18] [19] and noise-removal.
Words such as  cdata ,  nbsp ,  http ,  www ,  pdf  and  html , are added to the standard stop-word list, because they are common in many webpage documents and provide little information about the topic.
To handle synonyms, such as  USA  with  U.S. , we A topic initiator starts a topic and spreads the information via many following web documents, so it should be located around the content center, i.e., similar in content with its followers.
Thus the similarities between a web document and all other web documents give us a hint on the potential of the web document to be the topic initiator.
To estimate the Centrality of a web document di, there are two types of measures: AverSim and CenterSim.
We de ne AverSim as the average similarity between di and all other web documents in the query result list.
CenterSim is de ned as the similarity between di and the center dc of all the web documents.
For vector space model, the center is the mean point of the document points; for language model, the center is background model estimated from the documents.
AverSim(di) =

 Sim(di, dj) N(cid:88) j(cid:54)=i CenterSim(di) = Sim(di, dc) N is the size of the set of web documents containing the query.
The computational complexity of CenterSim and AverSim is O(N ) and O(N 2), respectively.
So CenterSim is more e cient.
Note that achieving the highest Centrality score does not necessarily mean the web document is the topic initiator, because a following web document may contain more detailed information about the topic, and has higher Centrality score than the topic initiator.
Since the topic initiator is the beginning of the topic, it should be novel.
More speci cally, the topic initiator should not only be similar to its following web documents, but also dissimilar to its earlier web documents.
This leads to two factors which consider both time and content information: (cid:88) tj >ti ASL(di) =

 Sim(di, dj) ASEM ax(di) = argmaxtj <ti{Sim(di, dj)} (cid:189)
   original.
The possibility is low for the post to be a topic initiator, because it is unusual that someone starts a new burst of topic when reply to a topic post.
Rule 2.
For posts within the same thread of the same forum, we consider those not posted in the  rst day as not original.
We form a new attribute ThrForId, which is a merging of the ThreadId and ForumId.
In most cases, web documents which share the same ThrForId belong to the same group of discussion, and thus only the web documents from the  rst day are considered to be original.
Rule 3.
The problem for rule 2 is that even on the same day, there could be many posts.
Ideally the  rst post should be chosen, because all others are just replies.
However, if we do not know the exact time of each post, we simply decide they are all original.
Based on originality (ORIG) information, the possibility for the web document di to be the topic initiator is evaluated as follows original not original ORIG(di) = (7) Parameter   (    [0, 1]) controls the possibility of a non-original web document to be the initiator.
For simplicity, we set   = 0 to ignore any non-original web documents.
In this case, the originality factor works as a  ltering function.
Some forum or newsgroup posts are very short, but contain a lot of query keywords, and thus have high overall similarity to other web documents.
To deal with this problem, we make the assumption that a web document should be long enough to provide useful information.
Let L(di) as the length (number of words) of web document di, we de ne Document Length Factor (DLF) to utilize the above assumption.
DLF is computed as a normalized score based the document length using the SN Inc() function.
DLF (di) = SN Inc(L(di)) (8) The length of a web document usually ranges from 1 to over 3000 words, and the average length among our dataset is about 50 words.
We thus assume that a web document which contains more than around 50 words brings enough information to start a wildly spread topic.
Based on the property of the SN Inc() function,   = 7 is a good setting for our task.
The reason is that under this setting, web documents which are longer than around 50 will have a DLF score close to 1, and thus a web document with 50 words length has similar DLF score with those with 500 words, because they are all long enough.
Meanwhile, a web document with very few words, e.g.
5, will have a very small DLF score, which indicates that the web document is too short to be a topic initiator.
Distance between term occurrences has been shown to be useful for relevance weighting in retrieval [9].
Term gap gives a hint on the topic focus of the web document and we are especially interested in the query terms in the web document.
If the query terms appear close to each other in a web document, it is more con dent to say that the web document is about the query topic.
Otherwise, if they appear far away from each other, the possibility is low for the web document (2) (3) (4) (5) (6) where NL is the number of web documents that appear later than web document di.
ASL(di) is the average similarity between di and its later web documents, while ASEM ax(di) is the highest similarity between di and its earlier web documents.
We want ASL(di) to be high and ASEM ax(di) to be low, so the N ovelty of a web document di is de ned as follows, ASL(di)   ASEM ax(di) + 1 N OV E(di) =
 This function is designed to range within [0, 1], since ASL(di) and ASEM ax(di) are independent and both range from 0 to 1.
The originality factor is introduced because a topic initiator should be original.
We consider the following rules to decide whether the web document is original or not: Rule 1.
If the title of a post begins with  Re:  or other reply indicators, such as  RE: ,  Reply #99 on:  and  reply to why girls don t like drugs , we consider the post as not n 1(cid:88) i=1 to be focusing on the query topic.
We introduce the Term Allocation Compactness (TAC) score to utilize the term gap for ranking.
A term may appear in the document d for multiple times.
For a query of n terms, let qi denotes the ith (i = 1, .
.
.
, n) term of the query, mi denotes the number of appearances of term qi in the document d, Zi = 1, .
.
.
, mi, lij denotes the location of the jth (j   Zi) appearance of term qi in the document.
The value of lij ranges from 1 to L, and L is the length of the document.
De ne c as a combination of the locations of the terms in the document.
c = {l1j1 , l1j1 , .
.
.
, lnjn|ji   Zi} (9) Denote C = {c} as the set of combinations for the query in the document, and M as the number of di erent combi-nations,
 mi (10) i=1 We only consider absolute gap between terms, and ignore the relative order.
For example,  Google and IBM  is considered as the same as  IBM and Google .
To facilitate computation, the locations in c are sorted in increasing order.
Then c is re-represented cs, cs = {ls1, ls2, .
.
.
, lsn} (11) When lsi is the location of the ith term in the sorted cs.
Based on sorted combination cs, the average gap between terms is calculated as follows, AveGap(cs) =
 n   1 (lsi+1   lsi   1) (12) Select the combination with the minimum average gap, M inGap(d) = argmincs C{AveGap(cs)} Finally, TAC is calculated as a normalized score, (13) T AC(d) = SN Dec(M inGap(d)) (14) Note that although term gap is a good topic indicator, it does not necessarily mean a web document with compact query terms allocation is de nitely talking about the topic.
We still have to check the whole content of the document to see its true topic focus.
Intuitively, given the topic query and the web documents containing the query keyword(s), a web document which appears earlier should have higher possibility to be the topic initiator.
Based on this assumption, a naive approach to the estimate the possibility for the web document di to be the topic initiator is the following ranking function of the time information.
PT ime(di) = TEnd   ti TEnd   TBegin (15) where, TBegin = min{ti} and TEnd = max{ti}.
There are two major disadvantages of the naive approach: (1) if there is a noisy web document whose publication date is much earlier compared with other web documents, it will dominate the ranking function and make all other web documents have similar scores; and (2) all publication dates are equally important, however, if the web documents in the same date are all outliers (or non-relevant), this date should not account much.
We sort the dates in increasing order O = {st1, st2, .
.
.
, stP}, We propose a better method which considers both the time order and the content of the web documents within the same date.
The basic idea is to use time order instead of exact time gap for solving problem (1) and use content analysis to give relevance score/weight to the dates for solving problem (2).
P is the number of distinct dates (P   N , where N is the number of web documents).
De ne the order of time/date t as Order(t) = q, where t = stq.
Since stj is the jth sorted date, Order(stj) = j.
For a date stj, let D(stj) = {di|ti = stj} as the set of web documents whose publication date is stj, We de ne M CS as the maximum content score of those documents, M CS(stj) = argmaxdj D(stj ){ContentScore(dj)} For simplicity, Centrality is used as the ContentScore.
(16) For a date stj, de ne its importance/weight W (stj) as a score related to the Order and M CS, and normalize the Order using the SN Dec() function, W (stj) = SN Dec(Order(stj))   M CS(stj) (17) Finally, we get the ranking for web document di by earliness (EARL) as follows, EARL(di) = Order(ti)(cid:80) P(cid:80) j=1 W (stj) (18) W (stj) j=1 The limitation of directly using time information is that the  rst web document is not necessarily the topic initiator.
Because it may happen to contain these query words, but is not really talking about the topic.
Even if we consider weighting by the order and content, the current ranking function will still rank the  rst document as top 1.
We still need other factors to get the true topic initiator.
Using only time, originality, content or link in isolation gives poor performance.
If we only use originality, there could be a lot of original web documents.
If we only consider time, there could be a lot of web documents ranking high but not really talking about the query topic.
If we only consider content similarity, the topic initiator is not necessarily the web document with the highest overall similarity with other web documents, because it is possible that some following web documents contain more information about the topic, and thus have higher overall similarity with other web documents.
We propose our  rst scheme for ranking the topic initiator.
Assume the indicators, such as originality, content similarity, term gap and web document length are factors independent of each other.
Then the topic initiator can be ranked as a multiplicative model of the basic indicators, (cid:89) ComboRank = Di (19) where D(cid:48) is are the indicators, such as ORIG, DLF, TAC, EARL, LINK and CenterSum, where LINK is the normalized InDegree.
We call this approach ComboRank, such a ual indicators in a robust fashion in diverse situations.
Re nement One disadvantage of ComboRank is that there are many components, a big error in a component may have big impact on the  nal score.
To solve the problem, we propose our second approach, called InitRank (Initiator Ranking), to smooth the scores based on graph based re nement.
We  rst use some basic indicators to get ranking score initialization for each web document, for example,   r
 (20) Then re ne the scores based on a TCL graph model as described below.
We propose TCL graph to integrate the time(T), con-tent(C) and link(L) information.
Each node denotes a web document.
Add two kinds of directed edges: one type is based on the link information; another type represents the semantic relationship and information  ow, the relationship con dence is based on the content similarity and the information  ow direction is based on the time order.
The second type is used to capture the situation, as found in many real cases, where the information of a web document come from another one but it does not link to it.
Based on content similarity, we can estimate such semantic relationship.
Figure 2 shows an example TCL graph simpli ed from a It shows 13 web real query result about  Vegemite ban .
documents ordered in four dates.
The solid directed edge is the link between two web documents.
The dashed directed edge shows the hidden semantic relationship between two web documents.
The weight indicates the con dence of such relationship.
The direction represents the information  ow from one node to another.
We make the direction goes from the web document which appears later to the one which appears earlier, in order to show that the information of the later web document can be traced back to the earlier web document.
For two web documents in the same date, it s hard to distinguish the information sender and receiver.
So we assume both directions are possible.
Web document 3 is the true topic initiator.
Web document 1 appears on the  rst date, but does not really talk about the topic indicated by the query, even though it contains the query keywords (We will show how this could happen in our case study in the experiments part of the paper).
So the method solely based on time ordering will falsely rank 1 as the topic initiator.
Web document 6 has the highest overall similarity to other web documents; however, it is not the original topic initiator.
Web document 7 gets the biggest number of inlinks but is not the initiator.
In order to  nd the true topic initiator, we need to integrate the link and semantic relationship.
De nition.
In a TCL graph, V is the set of vertices/nodes, eij =< i, j > is a directed edge from node i to node j, E is the set of edges in this graph.
EL is the set of edges formed by the link(L) information and ES is the set of edges modeling the semantic(S) relationship between nodes, wij is the weight/con dence of the relationship estimated by the content similarity, the direction goes from web document i to j and i appears no earlier than j.
O(R) =   (cid:88) i V  i|| ri  i (cid:88) (cid:88) <j,i> EL <j,i> ES +  +  (cid:80) ||2     r i  i || ri   rj
  i j wij|| ri   rj
  i j ||2 ||2 (21) (22) Figure 2: An example TCL graph.
Denote  i as the in uence or importance of node i, we can estimate a node s in uence by the number of nodes which link to it and the number of nodes which shows hidden semantic relationship with it.
If the node has many inlinked nodes or semantic related nodes, its importance should be high.
Let R = {ri}(i = 1, ...,|V |), ri is the initiator ranking of node i, optimize the following objective function O(R), where W O j = <j,i> ES wji is the sum of the weights for the edges between node j and the nodes that j directs to, N O j is the number of nodes that node j links to.
i / i||2 instead of ||ri r   Among the three components in the objective function, the  rst component means that the re ned score should not deviate too much from the initialization score, we use ||ri/ i r i||2 in order to be compa-  rable to the other two components; the second component means the semantic information sent out from the initiator is similar to the information received; the third component means similar idea as the second terms but such  ow is indicated by link information.
Our goal is to  nd R = R  to minimize the cost function, R  = argmin{O(R)}.
R  is the  nal ranking score in our InitRank algorithm.
To minimize O(R), we compute its  rst-order partial derivatives,
  ri = 2    i   i ) (ri   r    i (cid:88) (cid:88) j V L i ( ri  i +2 +2    i )   rj
 j   wijrj
 j ( wijri  i j V S i ) (23) i set of nodes which have semantic link to node i.
is the set of nodes which link to node i, V S i is the Let  O(R)  ri ri = = 0, we get   (cid:80)   i r wij j V S i  i (cid:80)   +   N I i  i +   + +   +   N I i  i   +       +   N I i  i +   wij j V S i  i (cid:80) j V S i  i (cid:88) (cid:88) <j,i> EL wij <j,i> ES

 j rj wij
 j rj (24) where N I i = |V L i |.
R is initialized as {r i}, the  nal score R  is obtained by   iteratively updating all ri via Equation 24.
We could put a window which only include semantic links within such window at each time position to avoid building too big TCL graph.
Connection to Absorption Random Walk.
Equation 24 can be understood as an absorption random walk on the TCL graph.
The topic initiator will have the highest possibility to be visited.
The second and third terms in Equation 24 represent the jumping possibility from a inlink node and a semantic relevant node, respectively.
this case, we just use the initial ranking score.
Two special cases: (1)   (cid:54)= 0,   = 0,   = 0.
Equation 24 becomes ri = r   i .
In (2)   = 0,   (cid:54)= 0,   (cid:54)= 0.
In this case, we ignore the initial ranking score and only consider the link and the time-related semantic relationship.
If we de ne the importance  i as the weighted combination of the inlink nodes size and the overall semantic relationship with following nodes, i.e., (cid:80)  N I i +   wij j V S i   +    i = Equation 24 becomes a simpler version, ri =     +   +     i r (cid:88) (cid:88) <j,i> EL     +   +     + +   +   +   <j,i> ES (25) (26) rj
 j wijrj
 j Table 2: Example topics Topic name USA banned Vegemite Google IBM cloud computing universities IBM announces Blue Cloud Google enters Smart Grid and announces PowerMeter Xcel Energy announces  rst Smart Grid city For each topic, to simulate the real web search situation, we try di erent queries with variant words and query length, because di erent users may submit di erent queries for the same topic.
In total 916 queries are performed for those topics.
Ground Truth.
We manually checked the web documents of each topic to identify the topic initiator.
If there are multiple relevant web documents that appear in the earliest date and it is hard to identify a single topic initiator by the information available, we deem them as equally possible topic initiators.
(cid:80) Evaluation Measures The true topic initiator should be ranked as high as possible (ideal case is ranked as top 1).
We evaluate the performance by rank r, the ranking order of the initiator in the ranking list, if there are multiple initiator candidates, choose the best rank.
The overall performance is Rank = ri/n, the average rank of all queries (n is the number of queries).
The standard deviation is reported to show the robustness of the algorithms, and we call this Rank std for short.
PageRank and HITS only work when the link information is available.
If there is no link between any web documents of a query, all of them get the same score by the purely link-based algorithms.
In such case, we use random guess and set the ranking of the topic initiator as N/2, i.e., half the number of the web documents.
Figures 3 and 4 show the overall performance of the algorithms.
ComboRank performs better than individual indicators, both in Rank and Rank std.
InitRank achieves the best performance.
Not only being able to dig out the true topic initiator and rank it in the very top, InitRank also has very small Rank std which indicates its performance is very robust among all the queries.
This section reports experiment results on real web data to demonstrate the e ectiveness of our framework and the performance of our topic initiator ranking algorithms.
Our data are all webpages from web search result.
We investigate 332 topics related to three types of interest: product vegemite, cloud computing and smart grid.
Overall
 shows some example topics.
Figure 3: Overall performance of the algorithms.
Y-axis denotes the Rank.
To simplify experiments, set   +   +   = 1 and   =   to reduce the original three parameters to only one, i.e., s =
 parameter s. When s = 0, InitRank only use initial ranking and no re nement based on the graph random walk is used, so the performance is not good.
When s = 1, initial ranking score is totally ignored and thus dramatically degrades the performance.
When 0 < s < 1, initial ranking score and graph re nement are integrated to show good performance.
The highest performance is achieved around s   [0.05, 0.2].
Figure 6: Convergence of InitRank.
X-axis denotes the number of iteration.
Y-axis denotes  R(i.e., Delta R) and its standard deviation.
Table 3: Statistics of the two case study topics Topic 1 Topic 2 # of web documents # of webpages # of websites # of outlink websites # of outlink webpages









 Figure 7: Number of web documents per day for  Vegemite Ban .
Figure 5: Sensitivity of parameters.
X-axis denotes the value of s.
Figure 6 shows the average  R and its standard deviation (std) over all the queries at each iteration.
Because its value at iteration 1 is too big and dominates the  gure, we draw the curve beginning at iteration 2.
We can see from the result that InitRank converges very fast, only 5 iterations are enough for most cases.
We present two detailed case studies from two interesting topics: (1)  Vegemite ban  and (2)  Google IBM cloud computing .
Table 3 shows the statistics of the two topics.
Figure 7 and 8 show the number of web documents per day for them.
Figure 8: Number of web documents per day for  IBM Google Cloud Computing .
Background: Vegemite is a food paste used mainly as British and New Zealand s Marmite and to Swiss  Cenovis.
It is very popular in Australia and New Zealand.
In October
 United States.
Facts: The fact behind the spreading of the topic is that: On October 5th, 2006, a blog article [17]  rst talked about Vegemite ban, and several people commented on the blog.
However, it did not generate a burst.
A very interesting thing is that the blog author of [17] is an Australian writer best known for his speculative  ction.
This blog mentioned that  Around the same time, however, Deb Layne (Wheat-land Press), my  ne and lovely publisher, had a birthday, and I thought I can send her a jar of vegemite.
.
.
Except of course, vegemite cannot legally be imported into the States anymore because it contains Folate. Wheatland Press, founded by Deborah Layne, is an independent book publisher specializing in science  ction and fantasy, and has published works written by the blog author.
On October 21, 2006, News Australia website published a news article reporting that USA banned Vegemite [2], and generated a burst on Vegemite ban.
A lot of web documents have cited this article.
On the next day, the same website published another news article titled  US bans Vegemite  [10], some web documents have cited this one.
On October 25, 2006, USA denied imposing ban on Vegemite [3].
Even though  USA bans vegemite  has been proven to be a rumor since then; there were still many lingering spreading of the rumor after that.
In the following we quote two articles which followed the topic and analyzed where it started from.
Quote [15]: It all started out with an article in Sunday s Courier Mail by Kelvin Healey, which was taken from Danny Lannen s article in the Geelong Advertiser.
Online forums were on  re with the news of Vegemite being banned.
Quote [8]: Australians are particularly unhappy.
(Kelvin Healey,  US bans Vegemite , The Courier Mail, Oct. 22; News.com.au, Oct. 21; Tim Blair via Dylan).
If you re an American fan, act fast before eBay shuts down the auction.
The two examples show that even human may not be able [15] falsely said it to correctly identify the topic initiator.
started with [10].
[8] listed several candidates (we surmise that the author did this partially because of not knowing which one is the true origin), but [17] was missing.
Results Table 4 shows the ranking result of the true topic initiator from di erent algorithms.
If we just sort the web documents by time, blog [17] is only ranked 110th.
All web documents appearing before it are not talking about Vegemite ban.
Well-known link-based algorithms PageRank and HITS do not rank blog [17] as top 1 because there are some web documents which get much larger amount of citations.
InitRank correctly ranks blog [17] as the 1st.
Table 5 shows the snippet for the top 1 result by InitRank and ranking only by time.
We can see that the top 1 result of ranking by time just happens to have the query words  vegemite  and  ban , but does not really talk about the vegemite ban topic.
Interesting Finding: The interesting thing for the result is that we  nd that blog article [17] was actually the  rst to talk about Vegemite ban, while majority people through a standard analysis would conclude that the rumor started with the News Australia article [2] which appeared early and was cited by over 156 webpages.
Table 4: Ranking Result for  Vegemite Ban  Algorithm Rank of [17] TimeRank
 Centrality Novelty PageRank
 ComboRank InitRank







 Table 5: Top 1 result of the algorithms Algorithm Snippet TimeRank locate the vegemite and write my name on everyone .
.
.
I fought my way into parliament, and made a law banning Nuttelex [7] I just found out that Vegemite is banned in the States .
.
.
InitRank
 IBM Google Cloud Computing Background: In October 2007, IBM and Google o -cially announced to work together on cloud computing and collaborate with six USA universities.
Facts: On October 7, 2007, article [13] talked about IBM and Google would o cially announce they will work together on cloud computing.
On October 8, 2007, a lot of websites published this announcement, including the IBM o cial website [11] and many news websites.
Quote [13]: Google and International Business Machines are announcing Monday a major research initiative...
The two companies are investing to build large data centers that students can tap into over the Internet to program and research remotely, which is called  cloud computing .
Results Table 6 shows the ranking result of di erent algorithms.
[13] is correctly ranked as top 1st by InitRank.
Table 6: Ranking Result for  IBM Google Cloud Computing  Algorithm Rank of [13] TimeRank
 Centrality Novelity PageRank
 ComboRank InitRank







 Table 7 shows a quote of the top 1st result for InitRank and ranking only by time.
The result also proves that ranking based on only time information is not enough.
Interesting Finding: The interesting thing for the result is that we  nd that the IBM-Google cloud computing collaboration is o cially announced on October 8, 2007, but article [13] seems have published the news one day early.
In this paper, we introduce a new Web Mining and search Algorithm Snippet TimeRank .
.
.
eBay, Google and Yahoo can be quite different than traditional IT shops.
.
.
.
more custom solutions than its competitors, including IBM.
.
.
 Cloud computing is all about companies getting as many computers.
.
.
 [14] Google and IBM join in  cloud computing research .
.
.
InitRank technique/service - Topic Initiator Detection (TID) on the Web.
When a user is interested in a topic, and wants to know which web document initiated the topic or was the  rst to talk about this.
The service can answer this question.
We design a framework solution for TID on the Web and present InitRank, which is based on our proposed topic initiator indicators and re nement over a TCL graph, to rank the possibility of a web document to be the topic initiator.
Experiments are done with real web datasets, compared with approaches such as intuitive method of simple time sorting, well-known link-based algorithms PageRank and HITS, InitRank gets the best performance.
InitRank can  nd the correct topic initiator in some cases where even human can make mistakes.
Our experiment results have interesting  ndings: one is that we found the  rst web document related to the Vegemite (a popular food product in Australia) ban, which can help the company conducts investigation; another is an article that published the IBM Google collaboration on Cloud Computing one day before the o cial announcement.
We thank gratefully IBM Corporation for its support of this research.
We thank Ying Chen, Bin He and Zhijun Yin for helpful discussions and suggestions.
