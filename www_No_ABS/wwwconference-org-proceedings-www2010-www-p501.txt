Web search engines play an important role in satisfying users  information needs.
However, due to the openness of web search engines and the pro t potential in manipulating the search result pages, malicious use of the search en  Part of this work was done when the  rst author was on a summer internship with Microsoft Research and the Bing Search Data Mining Group.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
gine has been widely observed [11, 30, 31, 45].
Speci cally, robots1, a type of programs that issue queries and clicks automatically to web search engines can consume a large portion of the overall tra c for the search engines.
It is crucial to detect and separate these automatically generated search activities from those of genuine human users  for the following reasons.
First, program generated tra c can usually peak to a large tra c volume in a very short period of time, causing an increase of the search engine response time that degrades the human user s experience.
Second, search logs that record users  interactions with the search engines are often retained for later analysis.
Search engine logs corrupted by bot activities can mislead and even cause serious problems when important conclusions are to be drawn from these logs [37].
Third, it is usually very important to pre-process the data logs and  lter out irrelevant records [10] before carrying out other tasks such as modeling user search behaviors [1].
Speci cally, one example is learning to rank by user clicks, in which case the ranking algorithms learned from polluted data will not be e ective and useful [14, 28].
One di culty in detecting bots in search engine is the diversity in their behaviors.
There are bots that behave ethically by clearly identifying themselves in their visits [15, 21,
 engine, only a very small fraction of the bot tra cs belong to this class.
Other types of bots behave very di erently.
For example, some bots attempting to reverse engineer the search engine index would issue query terms extracted from a dictionary, i.e. consecutive queries that only di er by one or two characters, while some bots submit the same queries and sometimes click on the similar results in an attempt to boost the ranking for some speci c keywords or search results.
The diverse search behaviors are also mirrored by the genuine human users, one speci c reason being di erent users exhibit various sophistication level in using the search engine [22].
Prior researches in automated web tra c detection mainly focused on detecting bots on websites.
For example [40] used behavioral features, such as percentage of multimedia requests, average time between clicks, and total number of page requested to characterize the navigational patterns of users and then apply a decision tree algorithm to learn and determine if the user is a human or not.
[39] used sim-
to refer to automated online programs.
under Bayesian classi cation framework that demonstrated promising results for detecting crawlers inside Web-server access logs.
However, almost ironically, in these prior researches one major type of robots were the web crawlers from search engines, and work focused on the detection of automatically generated tra c targeted at search engines has been limited.
[37] classi es search sessions into typical and atypical behaviors, and showed that by  ltering out these atypical (outlier) sessions, one can improve the con -dence in the click through rate (CTR) estimation.
While it is natural that reducing data variance leads to higher con -dence of parameter estimation, the classi cation task itself mixes genuine user behavior with robots  activities.
Therefore it sheds limited light on detecting robot behaviors.
Our work is largely inspired by and built upon [4] which were based on active learning framework, and [5] which used unlabeled data to help identify the samples that need to be labeled.
Compared to these prior work, our contribution is a cost-e cient way of generating large number of initial samples and propose a semi-supervised learning framework that can improve the classi cation performance using unlabeled data in an iterative EM process.
In this paper, we propose a novel approach of combining the use of CAPTCHA [42] and some simple heuristics to generate large number of training data at essentially no additional labeling cost.
Further, we propose a semi-supervised learning approach for the bot detection problem.
Our semi-supervised learning approach is advantageous in handling the sampling bias issue during the initial training dataset generation.
This is achieved by introducing a large number of unlabeled data from randomly sampling the whole dataset, again at no labeling cost.
We demonstrate the effectiveness of our semi-supervised learning approach in distinguishing bot tra c from genuine human user tra c.
Our comparison to a fully supervised learning algorithm shows that the semi-supervised learning approach performs significantly better under human judgments.
For the scale of modern web search engines, it is costly to organize human judges to manually inspect and label the user search logs as training set for bot detection.
What is practically possible is often a tiny fraction of one day s search log.
However, since there are millions of online users, researchers have made successful attempts in utilizing these valuable resources, such as asking them to recognize scanned documents that are extremely challenging for current computer programs to handle [42] and, at the same time, use the process as a veri cation technique to determine if the user is a genuine human or some automated program.
In some systems, every user must pass the CAPTCHA challenge in order to access some speci c service, such as web email account application, online banking, etc.
For web search engines, this strategy is not viable because the goal of a web search engine is to help users retrieve useful information as quickly as possible, that one of the most important design objectives is to minimize the e orts a user spends on the search engine.
For this reason, blindly applying CAPTCHA challenge for every user is apparently not acceptable.
On the other hand, it is also necessary to send out CAPTCHA challenges to some users selectively to guard against malicious use.
Figure 1: The number of all users and users who were present with CAPTCHA pages.
Numbers of all users are normalized to the maximum number during the week.
Numbers of CAPTCHAed users are normalized to each day respectively and percentages are shown here.
Our current search engine implements a mechanism to send CAPTCHA challenges to a small fraction of users 2 mainly based on the following criteria: 1), server load status, i.e. CAPTCHA challenges are sent out when some servers are experiencing high tra c volume; 2), user behavior, when a user, tracked by a unique identi er, is behaving abnormally, such as sending in large number of queries in a very short interval; 3), IP block, such as when the tra c from a certain IP address exceeds a high threshold that all users from this address will be asked to verify their identities; 4), random selection, especially when the server continues to experience heavy tra c volume; and 5), some other thresholds have been exceeded, such as those from the baseline algorithm described later in the paper.
After correctly responding to the challenge, the user will be exempted from further veri cation for a period of time.
We have found this mechanism a good balance between user experience and system stability.
Figure 1 shows the overall tra c and the percentage of users who were presented with the challenges in the data collected during the period of July 3 to July 9, 2009.
The number of overall users  uctuates periodically on a weekly basis and peaks on Monday and Tuesday.
On average, less than 1% of overall users are requested for veri cation.
When presented with a CAPTCHA challenge, a user can either disregard the challenge ( no response ) by closing the browser or exiting the search session, or attempt to answer the challenge.
Every time a wrong answer is submitted, the server changes the image shown in the veri cation web page.
As long as the user answers correctly in one of the challenges, the user is regarded as having passed the challenge and labeled  correct response  .
If the user fails to correctly answer the challenges after all trials, the user is marked as  wrong response .
Figure 2 shows the response categorization of these users.
It partially demonstrates the e ectiveness of the veri cation mechanism since, among all
 The algorithm discussed in this paper only handles the cases where users accept cookies.
To further simplify the process, a user with the same user id is classi ed independently in di erent sessions.
Here, a session is characterized as consecutive visits from the same user and can consist of several query actions taking place within a temporal interval, say, 30 minutes.
to the CAPTCHA pages.
Since the CAPTCHA users were sampled non-uniformly, most of them were likely to be robots, and therefore large portion of the CAPTCHA pages were not responded.
For the users who do respond, the correct rate is about 80%.
the users who receive the veri cation requests, over 99.9% of them do not respond at all.
This lends to the belief that most current bots do not implement the functionality to respond to the CAPTCHA challenges, let alone the sophisticated algorithms needed to overcome them.
For users who do respond to the veri cation requests, we see that the correct response rate is roughly 80%.
The rate depends on many factors, such as the di culty of di erent CAPTCHA challenges and the user expertise levels in online usage [22] that make some users more experienced in solving the challenges than others.
For users who correctly answer the challenges, we label them as genuine human users and use their records as the  human  class in our training dataset.
Even though we believe that the majority of users who do not respond to the challenges are bots, it is still possible that some human users are turned o  by the challenge or simply do not understand how to respond.
Therefore, we use some heuristics to select a fraction that are most likely to be bots from the set of  no response  users.
The set of heuristics include 1), number of clicks in a time period, 2), number of search result pages browsed, and 3), number of IPs that the user  originates  simultaneously.
The de nition of these measurement will be further discussed in Section 3.
The users who do not respond and exceed the thresholds of the heuristic measurements are initially labeled as  bot .
From the data, we observe that user behaviors appear to be multi-modal and vary notably among the search verticals such as web search, image search, video search, etc.
In this paper, we restrict our discussion to web search only.
Figure 3 shows the categorization of web search users based on the combination of their responses to the CAPTCHA challenges and our heuristic based classi cation.
The di erent response rates compared to the overall response rate are shown in Figure 2.
Note that in these datasets we use only the most suspicious subset of the users who do not respond as  bot  samples.
By making use of the existing CAPTCHA mechanism and some simple heuristics, our training data generation process has essentially incurred  0-cost  for the developers to extract a large number of labeled data.
However, this set of data is not uniformly sampled over the whole dataset and has the following issues to use them directly for supervised training.
Figure 3: Categorize web search users based on the combination of their response to the CAPTCHA challenge and heuristics.
Note the di erent user response rate on web search vertical compared to that in Figure 2 for all users.
First, as we discussed before, the users who were selected to present the CAPTCHA challenge are sampled unevenly towards those who have large number of requests.
Second, the users who correctly answer the challenges are biased towards the users with more online experience.
Third, the samples in the  bot  class is only a partial representative of the whole  bot  class due to the high thresholds in our heuristic rules, and therefore only a small fraction of the users who do not respond are used as the  bot  samples.
It is well known that when the labeled training dataset does not re ect the underlying data distribution, the classi ers will have skewed classi cation boundaries that will lead to poor generalization capability [3, 6, 46, 47].
Obviously, the bene t of generating a large amount of data in a cost e cient way must be accompanied by the data being useful for training.
To compensate the data bias issues, we further include a large amount of unlabeled data by uniformly sampling from the whole search logs and use a semi-supervised learning approach to correct the skewed decision boundary.
We describe the details of the semi-supervised learning algorithm that we developed from Bayes network [18] (Section 4), and then we demonstrate its e ectiveness by comparing the proposal with a supervised learning algorithm based on decision tree [32] (Section 5).
It is important to note that bot detection and the user ver-i cation mechanism are closely related.
A good bot detection system will make the user veri cation mechanism more e cient and improve the experience of the genuine human users.
On the other hand, selectively verify the identity of certain users can also help a bot detection algorithm learn to better distinguish human user behaviors from those automatically generated by programs.
Due to the space limit, this intricacies between these two issues are left out of this article.
Here we brie y describe the features we use in the bot detection algorithms.
Generally speaking, the features can be categorized into two types.
First, the numerical user behavior features are the type of summarized measurements Figure 6: AllHitCount Figure 5: UserClickCount.
Figure 7: UserUniqueIPs.
derived from the whole, per-user search sessions.
The second type of features is the boolean blacklist features that are basically hand crafted rules.
The blacklist features are very powerful in identifying bots when triggered, although the frequency of these features being triggered is low in practice.
In the following we describe these di erent types of features and, for clarity, mark their value types in parentheses.
PageTrackedCount measures the number of pages that the user browses.
Empirical observations lead to the impression that bots tend to behave in two extremes.
Some bots will only submit queries and not browse any of the result pages (except the  rst one), ostensibly with the intention to increase the query frequency for certain keywords.
The other extreme sees the bots fetch all the result pages for each query, probably trying to reverse engineer the index of the search engine, while genuine human users would probably just browse the  rst few pages of the query results selectively.
Figure 4 shows the PageTrackedCount distributions of the di erent classes.
We can see that both  human  and  bot  classes are slightly skewed toward higher PageTrackedCount due to the reason we described before.
UserClickCount measures the number of mouse clicks on the search result pages.
This includes clicks on the web search results, i.e. results that are deemed relevant to the query term, and the sponsored items (i.e., advertisements).
At this point we do not distinguish these two items and sum the clicks into a single number, although counting them separately can be potentially useful in capturing the ad-fraud bots that intentionally click on advertisements placed on the search result pages.
Figure 5 shows the UserClickCount distributions of the di erent user classes.
AllHitCount measures the overall  impressions  that the user receives in addition to the search results.
Since on a web search engine the major contributor to this feature is the page views, this feature (Figure 6) is closely correlated to the PageTrackedCount depicted in Figure 4.
However, the correlation is weaker for image and video search where the impressions of the media contents are not necessarily bounded by the search result pages.
UserUniqueIPs measures the unique number of IPs a user is using.
[11] and [45] have both reported that a large number of bots can assemble a network to attack online services in a well coordinated manner, and one way to discover these attacks is by counting the number of unique IPs that are associated with each user.
Although the IP address of a user could change when the user moves from one place to another, e.g.
home v.s.
o ce, the frequency of the IP changes is typically much smaller than that of a bot net.
In our experiment we did not discover these malicious networked activities in our search logs, thus the  human  and  bot  classes are less distinguishable on this dimension than others (Figure 7).
UserUniqueQueries measures the unique number of queries issued by a single user in a search session.
There are two key observations from inspecting the distribution shown in Figure 8: 1),  bots  tend to issue either a very small number of repeated queries or a large number of unique ones, and 2), the users in the  human  class tend to have more unique queries than the overall user population.
One possibility is that the users labeled as  human  might be more experienced users that are more versed in formulating queries.
We implement the following rules (boolean) in our current bot detection algorithm: 1) RuleBlacklistForm: this rule is triggered when a user includes in the query certain obscure beled data samples are needed in order to reach comparable performances.
This property is especially appealing because, as we have seen, web search users that are presented with the CAPTCHA challenges are only a very small fraction (  1%) of the whole user set, and among these users only a very small fraction of them will respond to the challenges (Figure 2 and Figure 3).
Therefore, our labeled data are a very sparse sample of the original data corpus that seem to be more suitable with a semi-supervised than with a fully supervised learning approach.
Second, semi-supervised learning is especially advisable when the labeled data samples  distribution are skewed from the underlying data distribution, e.g.
Figure 4 - Figure 9.
According to the Bayes rule, P (Y |X) = If the training data distribution, P (X(cid:2) ), used in the supervised learning algorithm diverges from the actual data distribution P (X), the learned classi cation model P (Y |X(cid:2) ) will not generalize well over X [6, 46].
Again, our dataset is generated with known biases that make it appealing to avoid supervised learning.
A common way to incorporate the unlabeled data into the learning process is through the Expectation-Maximization (EM) algorithm[12].
The algorithm starts by using the labeled data (X L) only to bootstrap an initial classi er P (Y |X L).
This initial classi er is then applied to annotate the unlabeled data (X U ) with the posterior probabilities, P (Y |X U ), for each unlabeled observation Y .
These probabilistically labeled data are then added to the training dataset, and their posterior probabilities were used as  soft  counts for the purpose of updating the conditional probabilities in the EM iterations.
At the end of each iteration, a new classi er, P (Y |X), is obtained and this new classi er is then applied to annotate the unlabeled data.
The process is repeated until certain convergence criteria are met.
This approach is also known as a self-training algorithm since the learning algorithm is feeding back on its own classi cation outcomes [35, 36, 44].
A temporal version of this method leads to the Baum-Welsh algorithm for Hidden Markov Model (HMM) training [33].
When the feature dimension is su ciently large, co-training algorithm [3] splits the features into two subsets, assuming that each subset is distinctive enough to train a good classi- er independently.
In the co-training algorithm, the labeled data are  rst used to train each classi er.
Afterwards, unlabeled data are classi ed by each classi er and the samples with high con dence from one classi er are added to the training dataset of the other classi er.
Each classi er is trained again with the new training dataset and this process is repeated iteratively.
Two assumptions are required for this co-training algorithm to perform well:  rst, each sub-feature set is su ciently distinctive and, second, the conditional independence assumption is required for these two feature sets [29].
Relaxations to these strong independence assumptions were explored in [7, 19].
In addition to the generative models, semi-supervised learning framework has also been enhanced with discriminative methods that optimize P (Y |X) directly without explicitly model the data generation process, i.e. P (X|Y ) and P (X, Y ).
For the unlabeled data to be useful in the discriminative approach, a connection between P (X) and P (Y |X) has to be made [38].
Transductive support vector machines (TSVMs) accomplishes this by requiring that the decision boundary Figure 8: UserUniqueQueries Figure 9: Rules codes that are designed mostly for internal search engine instrumentation purposes that should be unfamiliar to most genuine human users.
2) RuleBlacklistIp: we maintain a list of IPs that are publicly identi ed as Internet spammers and labeled all the tra c from these IPs as  bot .
3) RuleBlack-listQuery: this rule is triggered when the query composition is too complicated to be manually typed in by a human user.
In this work, we combine these rules into a single feature (Figure 9) that assumes the value  1  whenever one of the rules is triggered and  0  otherwise.
A straightforward way to utilize the labeled data extracted using the techniques described above is to directly learn a bot detector in a fully supervised manner.
In our experiment, we choose a speci c implementation (J48 [43]) of the popular C4.5 algorithm [32] as our supervised learning algorithm for bot detection.
The details of the decision tree algorithm are omitted here.
Since our initial labeled dataset is extracted using CAPTCHA response and heuristics, the decision tree algorithm can  t very well the labeled training dataset.
However, as clearly seen in Figure 4 through 9, the labeled samples we extracted are indeed skewed from the true data distribution, bringing in the question that how well the classi cation boundaries learned directly from this subset will be able to generalize.
Indeed, our problem actually matches the strengths of the semi-supervised learning framework [3, 6, 46, 47], in which unlabeled data are included in the classi cation boundary learning process and the optimization of the learning process is carried out with both labeled and unlabeled data samples.
The advantages of applying semi-supervised learning to the bot detection problem are based on the following properties commonly observed in a semi-supervised learning system.
the unlabeled data, and as a result the decision boundary minimizes the generalization error on unlabeled data [41].
Our work is mostly related to the study that only one class s label is available (in our case the users who pass the CAPTCHA challenge are known to be  human ).
[13] assumes that the prior distribution P (Y ) is known and showed that it is theoretically possible to estimate P (Y |X) using Bayes rule.
[27] address this problem using two rounds of the EM algorithm.
In the  rst round, the labeled data samples are split into two parts unevenly, with the larger split being used on its own, while the smaller split, called the spy documents, is mixed with the unlabeled samples to form the opposing class of data.
A classi er is learned using the EM algorithm.
The posterior probability of the spy documents are considered as a relative standard to determine which data samples have high probability to belong to the other class, after which these data samples and the initial labeled samples are used to train a  nal classi er through a second EM algorithm.
[26] adopts a more aggressive scheme by  rst treating all unlabeled data as the opposing class to the labeled samples, and using a weighted logistic regression algorithm to learn a linear classi cation function.
As a result, this approach is not suitable for cases where the opposing classes are not linearly separable, or when the unlabeled samples overwhelmingly outnumber the labeled samples.
In this paper we develop a semi-supervised learning algorithm using Bayesian network classi er that has various advantages in handling incomplete data set, encoding causal relationship, and avoiding over tting [20].
The training of a Bayesian network is composed of two parts, i.e. the structure learning and the parameter estimation.
Given a set of training samples, the goal of structure learning is to generate a graph that best describes the causal relationships in the data.
Here, the goodness of the graph structure can be measured in many ways, ranging from the entropy, the posterior probability given the training data, to the minimum description length [34].
Regardless the metric chosen, the structure learning problem amounts to de ning a search algorithm that optimizes for the metric by systematically changing the graph structure.
Unfortunately, even for fully labeled dataset, learning the optimal Bayesian network structure is NP-hard [8] that approximation and heuristics are often adopted in practice [9,
 labeled data only.
In addition to the computational complexity concern, it is also because, during the dataset generation process, we heavily utilizes the domain knowledge that is representative of the nature of the problem that it is reasonable to believe the the structure learned from the labeled data can generalize well.
We use a structure learning algorithm similar to that of [16] in which a tree structure is formed by calculating the maximum weight spanning tree using the methods described in [9].
Semi-supervised Bayesian network parameter estimation After acquiring the Bayesian network structure, we learn the conditional probabilities for each node and its parents.
We represent each feature (Xi) and the class label (Y ) as Table 1: Terminologies and notations used in semi-supervised Bayesian network parameter learning Description Observation (features) Class label/prediction
 Bayesian classi er posterior (cid:2)
 Notation 0 :  human(cid:2)(cid:2) 1 :  bot(cid:2)(cid:2)
 d(X, Y )



 =  d(X, Y )


 d   [0, 1], i   {0, 1} wi #(X = x, Y = y)
  

 A data sample Human data Bot data Unlabeled data Training data corpus Weight for a data sample Weighted count of a sample with observation x and label y CAPTCHA trust factor Laplacian smoothing factor Total number of data samples Maximum EM iterations nodes in the Bayesian network, and the task is to learn the following posterior probability for any given observation [24]: P (Y |X0, X1, ..., Xn) = (cid:3) P (X0, X1, ..., Xn, Y ) Y P (X0, X1, ..., Xn, Y ) .
(1) n(cid:4) As can be seen in the above equation, the problem of learning the conditional probabilities is equivalent to learning the joint probability of P (X0, X1, ..., Xn, Y ).
Based on the local Markovian assumption and the chain rule of probability, this joint probability can be factorized as P (X0, X1, ..., Xn, Y ) = P (Xi|P aXi ), (2) i=0 where P aXi represents all the parents nodes of Xi.
With this factorization, the parameter learning is further simpli- ed as the process of estimating the conditional probability of a random variable Xi given its parents P axi .
Without loss of generality, we explain our parameter learning algorithm in the special case that variable Xi has only one parent node Y (i.e., Na ve Bayes).
The terminologies and notations used in the algorithm are listed in Table 1 and, whenever appropriate, we shorthand P (X = x, Y = y) as P (X, Y ) for simplicity in notation.
At the core of the parameter learning is the iterative EM algorithm, summarized in Algorithm 1, in which the posterior probabilities of the data samples can be updated using the weights learned in the preceding iteration:
 =

 (cid:3)

 ,
 #(X, Y ) +   N +   .
(3) (4) semi-supervised Bayesian network parameter learning Data: D(X, Y ) = H   B   U Result: P (Y |X) begin Initialization: Set #(X, Y ) = 0; for each d   D do if d   H then d = 1, w1 else if d   B then d = 0, w1 else if d   U then d = 0, w1 Set w0 Set w0 Set w0 d = 0, d = 1, d = 0, i = 0; Maximization: learn Bayesian classi er P 0(Y |X) as Equation (3)   (6); i + +; while i < T do Expectation: Update weights for each d   D do if d   B or d   U then else if d   H then Set w0 Set w1 Set w0 Set w1 d = P i 1(Y = 0|X); d = 1   P i 1(Y = 0|X); d = (P i 1(Y = 0|X))

 d = 1   (P i 1(Y = 0|X))

 Maximization: learn Bayesian classi er P i(Y |X) as Equation (3)   (6); i + +; Return P (Y |X); end Figure 10: Averaged change of posterior probabilities after each EM iteration.
Combining (3) and (4) we have
 (cid:3) #(X, Y ) +   Y (#(X, Y ) +  ) , where the weighted count is de ned as #(X = x, Y = y) = wi=y d .
(cid:5) d   D X = x Y = y (5) (6) Our semi-supervised learning algorithm is slightly di er-ent from many other algorithm in that we introduce a CAPTCHA trust factor (C) in the parameter learning process.
As in Algorithm 1, the soft count of each data sample is updated based on its previous label.
If a sample is initially labeled as  bot  or  unlabeled , we update its soft count with the posterior probability learned from the previous iteration.
If the sample is initially labeled as  human , i.e. the user has correctly answered the CAPTCHA challenge, its soft count is the posterior discounted by a CAPTCHA trust factor C.
This factor controls to which degree we trust the CAPTCHA system.
Even though at this moment no computer program could automatically solve the CAPTCHA challenge problem, it is possible that one can make use of manual labors to cheat CAPTCHA.
We use a large trust factor C with a belief that the CAPTCHA system is reliable in most cases.
However, when some samples have a very high posterior probability to be  bots , soft counts can still be drawn from these samples and added to the  bots  class.
Note that C     means that we absolutely trust the CAPTCHA system and therefore the identity of the  human  users can not be changed in the EM training process.
We tested various C values and found that 10 seems to be a good balance.
Figure
 Our current EM algorithm stops after a  xed number of iterations (e.g.
10).
Figure 10 shows the average change of posterior probability for the training dataset, which can also serve as a convergence criteria.
Our training data is composed of a week s web search log data on a popular search engine from July 03 to July 09,
 process described in Section 2.
For computational considerations, we further sample both the  human  and  bot  dataset from the log, and  nally extract 20000  human  records and 6000  bot  records that keep the original ratio unchanged.
For the semi-supervised learning algorithm, we uniformly sample the whole week s logs to generate an  unlabeled  dataset.
The  nal labeled/unlabeled dataset size ratio is,
 shown in Figure 4 - Figure 9.
As for testing, we generated a smaller dataset composed of 170 user sessions.
In order to be fair for all algorithms, we invited 10 web search researchers to label the full testing dataset so that each search session is judged by 3 di erent persons.
We developed a visualization tool to render the search log and display a user s search session in its original temporal order.
The visualized information includes 1) nu-meric/boolean features as de ned in Section 3; 2) the  rst 100 raw query terms; 3) referrer s link; 4) clicked links; 5) Description Numbers Total number of user sessions Number of user sessions with at least 2 judges reach agreement Number of user sessions with at least 2 judges agreed on either  human  or  bot  labels Number of user sessions with at least 2 judges agreed on  uncertain  labels Agreement percentage  for sure  percentage (i.e. either  human  or  bot )


  human : 71  bot : 31


 Table 3: Performance comparison for the supervised learning algorithm (D-Tree) and our proposed semi-supervised learning approach Algorithm D-Tree Semi-supervised Bayesian network Precision Recall F-measure





 dwell time.
For each session, a judge could choose among the three di erent labels:  human ,  bot , or  uncertain .
The statistics of the labels in the test set are summarized in Table 2.
Note that even for human, the problem of distinguishing bot from a genuine human user seems extremely di cult3.
Among all the 170 sessions, only about 60% of them that an agreement is reached by a majority of 2 over
 with the judgment labels and in total has 71  human  sessions and 31  bot  sessions, is used as the test set to evaluate various bot detection algorithms.
In our experiment, we use the J48 implementation of [32,
 are set to their default values.
For the semi-supervised learning approach, we developed our own algorithm based on the implementation of [34].
We  rst used the labeled data to learn the Bayesian network structure using the builtin Tree Augmented Na ve Bayes structure learning algorithm, and then kept this graph structure  xed for the EM algorithm to learn the conditional probability tables (CPT) for each Bayesian network node.
We used standard Laplacian smoothing in CPT estimation, with smoothing factor   = 1 throughout the experiment.
The inference process used the builtin inference algorithm [34].
To pre-process the original numerical features, we used the builtin supervised quantization algorithm [43].
More speci cally, we used the labeled dataset to learn the quan-tization bins for each feature dimension, and then applied this binning scheme to the unlabeled dataset.
Our experiments were conducted on an Intel Xeon workstation with

 user to enable cookies.
In reality, there are a large number of bots that do not accept cookies and are not considered here.
As for the bots that do accept cookie, they tend to be more sophisticated and better disguised, which contributes to the di culty of the detection problem.
Figure 11: The e ect of di erent CAPTCHA trust factors on the performance, measured by F-measures.
Figure 12: Distribution of the posterior probability for the misclassi ed samples in our approach.
data samples took about 3 minutes.
For the parameter estimation, we  xed the maximum EM iteration to 10, and the training on around 200 thousand samples took about 3   5 minutes on average.
The performance for each algorithm is summarized in Table 3, where the precision and recall are averages of the two classes.
Because the decision tree algorithm was trained with skewed training dataset, its performance on the testing set was far from comparable to the semi-supervised learning algorithm we proposed in this work.
For most of the performance measures, the semi-supervised learning approach performs over 2 times better than the supervised learning approach.
Figure 12 shows the distribution of the posterior probabilities of the mis-classi ed samples in the semi-supervised learning approach.
Note that a large portion of the errors are made close to the classi cation boundary.
Manual inspection over the mis-classi ed examples shows that the errors mostly fall under the following two cases, 1), when the user session is short, there is very little information that the algorithm can infer from; 2), there are some cases that a user issues a lot of queries and the algorithm tends to classify such a behavior as a bot.
However, human judges can tell these sessions are from genuine human users because  the queries were semantically related and the queries and clicks were coherent with each other .
In this paper, we propose a semi-supervised learning framework for detecting automatically generated web search traf-gorithms to this type of Internet scenarios is data annotation scalability, to which we propose a 0-cost approach that makes use of the existing CAPTCHA technique to extract data logs of genuine human users.
The advantage of this approach is that it is virtually cost free to harvest large amount of human annotations through the existing user veri cation mechanism.
We also propose using the simple heuristics from domain experts to generate the initial positive ( bot ) dataset.
We address the skewed sample issue of the proposed approach and formulate the problem under semi-supervised learning framework.
The proposed approach makes use of a large number of unlabeled data, evenly sampled from the whole dataset (which again is virtually 0-cost).
Compared to the supervised learning algorithms that only base on the labeled dataset, our semi-supervised learning approach performs signi cantly better in our evaluation (2 : 1).
Many aspects in the proposed approach can be further improved.
First, the feature set used in our current algorithm seems to be not distinctive enough for this problem.
For example, it may be helpful to learn users  search intention in order to classify if the user is a genuine human or a bot as most genuine users typically use search engines when they have information needs.
This could be done by further including the query content and the correlation between query and clicks into the feature set .
In essence, our experiment is still preliminary because we use only one week s data that might not be long enough to capture some bot activities.
It is therefore not clear if our model can generalize well over a longer time span.
Ideally, the learning approach should be adaptive in nature so that the system performance can be automatically improved as more and more new data are collected.
In its current form, human judgments are involved only for testing data annotation, but it is straightforward to extend the judgment e orts towards the active learning approach so that new ambivalent data, when detected, can be quickly added into the adaptation data to broaden the scope and enrich the capabilities of the system.
As mentioned before, the CAPTCHA technique and our bot detection system could be better combined.
The authors would like to thank anonymous reviewers for helpful suggestions.
The authors are also grateful to the colleagues at Microsoft who generously o ered help in the user judgment study.
Hongwen Kang thanks Christos Faloutsos, Nikolas Gloy and Jay Stokes for helpful discussions.
