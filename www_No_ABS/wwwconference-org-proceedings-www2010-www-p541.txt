Presenting a ranked list of URL anchor texts and their associated snippets in return for a user query has become a standard interface for all major commercial search engines.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
While for most simple queries the ranked list-based presentation of search results is su cient to easily  nd relevant documents, for more complicated queries it would take a user signi cantly more time to peruse the long list of returned documents and, potentially, reformulate the query multiple times.
In order to understand why existing search interfaces often fail on certain types of queries and propose an improvement, we need a closer look at the very nature of search queries.
Almost all the queries are motivated by some underlying question.
For example, if a user is searching for information about John Fitzgerald Kennedy, the easiest and the most straightforward way to do so for a person, who is used to keyword-based search, would be to pose a query, such as  john kennedy  or  kennedy.  However, such a query does not fully specify the user s information need, which often includes particular aspects of information about JFK that a user is most interested in.
For example, in this case, the underlying question that has caused a user to search, might be as broad as  Who is John Kennedy?  or as speci c as  When was Kennedy sworn as the President of the United States? .
The query-based search paradigm assumes that search engine users have su cient knowledge about the query domain and are able to  nd good di erentiator terms to make their queries speci c and precise.
In reality, however, there is still a large number of queries, which are over or under-speci ed, and it is often the case that the users are unable to  nd anything useful as a result of their  rst search, sometimes even after tedious perusal of document titles and snippets.
This has to do with the fact that in their daily life people naturally tend to use verbose or imprecise statements to express their requirements and, thus, are not used to formulating arti cial short string requests.
According to [17], formulating natural language questions is the most natural way for most search engine users to express their information needs.
Unfortunately, state-of-the-art question answering systems cannot yet accurately answer arbitrary natural language questions posed by users.
Another widely recognized de ciency of modern search systems is the lack of interactivity.
Bookstein [2] points out that information retrieval should be envisioned as a process, in which the users are examining the retrieved documents in sequence and the system can and should actively gather the user feedback to adjust retrieval.
For this reason, the interface of search engines should re ect the fact that Web search is a process, rather than an event.
This work is an attempt to emphasize the exploratory nature of Web search their queries with automatically generated natural language questions, which can be answered precisely.
Ideally, questions should re ne the query topic from multiple perspectives.
For example, presented with the query  john kennedy , an interactive question-based retrieval system can generate the following questions:  Who is John Kennedy? ,  When was John Kennedy born? ,  What number president was John F. Kennedy? ,  Who killed President Kennedy? .
Each of the above questions can be considered as a clari cation question, which puts the general query terms in a speci c context.
Our intuition is that by automatically generating clari cation questions, an information retrieval system would enable the users to interactively specify their information need.
Since the questions are generated based on the system s internal information repository, they can always be answered precisely, which is not always the case with ordinary question answering systems.
In addition to providing answers, which are guaranteed to be correct, this model of interaction also has the bene t of helping the users to quickly navigate to the information they are looking for, e ectively eliminating the need to read the documents to locate it.
Enabling interactive question-based retrieval requires major changes to all components of the retrieval process: from more sophisticated methods of content analysis to ranking and feedback.
Speci cally, an interactive question-based retrieval system must be able to locate and index the content, which can be used for question generation, generate well-formed and meaningful questions in response to user s queries and rank those questions in such a way that the most interesting and relevant questions appear at the top of the question list.
In this paper, we propose solutions to all of the above problems and evaluate our approach with a prototype system.
The rest of this paper is organized as follows.
In Section 2, we provide a brief overview of existing work.
Section 3 introduces the general concept of question-guided search.
Implementation details of the question-guided search framework are presented in detail in Section 4.
Section 5 presents the results of an experimental evaluation of our question-guided search engine prototype.
Finally, in Section 6 we give concluding remarks and identify directions for future work.
In this section, we provide an overview of research e orts in two major research areas that are the closest to our work: application of Natural Language Processing (NLP) methods to Information Retrieval (IR) and presentation of search results.
The brittleness of NLP methods have traditionally been one of the major concerns and obstacles to using them in IR.
However, it has been experimentally shown in [20] that  the speed and robustness of natural language processing has improved to the point where it can be applied to the real IR problems .
One of the research directions that has recently gained particular attention is focused on using lexical and semantic relations to improve the accuracy and completeness of search results over the traditional  bag-of-words  approaches.
The importance of lexico-syntactic relations for information retrieval was demonstrated by Wang et al.
[23] experimentally showed that indexing lexical atoms and syntactic phrases improves both the precision and recall.
Hearst [7] used the semantics of [21].
Zhai et al.
lexico-syntactic patterns to automatically extract hyponym relations between the lexical items in large corpora.
Syntactic pattern matching is also a popular technique in Question Answering (QA) to improve the performance on factoid [11] and de nitional [4] questions.
Katz et al.
[12] extracted syntactic relations between the words through dependency parsing and used these relations to retrieve potential answers to a question.
Jijkoun et al. [11] experimentally proved that a linguistically deeper method for factoid QA, based on a small number of patterns, including syntactic relations, outperforms the traditional surface text pattern-based methods.
Our work can be viewed as a new way of using NLP techniques to support question answering in an IR system that is more robust than the regular question answering methods in the sense that automatically generated questions can always be answered.
An important aspect of any information access system is e ective presentation of retrieval results.
Hearst [8] provides an extensive overview of research e orts over the past several decades that aimed at improving the usability of search.
The various alternatives to traditional presentation of search results can be classi ed into three major categories: clustering [9] [22] [1], summarization [13] and visualization.
The information retrieval community has for a long time explored a number of graphical visualization techniques as an alternative to the ranked list presentation of search results.
Fowler et al.
[5] combined visual representations of queries, retrieved documents, associative thesaurus and a network of inter-document similarities into one graphical retrieval environment.
Chalmers et al.
[3] proposed to represent bibliography articles as particles in a 3-dimensional space and apply the potential  elds modeling equations from theoretical physics to represent the relationships between the articles by their relative spatial position.
InfoCrystal [19] is a visual query language that allows the users to graphically formulate arbitrarily complex boolean and vector-space queries by organizing the graphical query building blocks into hierarchies.
Leuski et al.
[14] integrated the ranked list representation and clustering of the retrieved documents into a visual environment for interactive relevance feedback.
As a new way to present search results, our work enables the users to navigate directly into the answers they are searching for without needing to read the documents.
In the following sections, we present the general idea behind the question-guided retrieval process and examine its each individual component in detail.
The idea of question-guided search comes naturally from the fact that a search for information is often motivated by the need for answering a question.
Asking a well-formulated question is the fastest and the most natural way to express the search goal.
However, the current search technologies cannot fully support a search interface, which is based entirely on free natural language question queries.
Moreover, search engine users have already got used to the keyword-based search paradigm.
In this work, we propose a method to augment the standard ranked list presentation of search results with a question based interface to re ne initially imprecise queries.
A typical scenario for question-guided search is as follows.
After a user types in initial keyword query, the automatically generated clari cation questions can be presented next to result presentation interface, should the system decide that a query requires further speci cation.
Alternatively, users may press a button (e.g.,  Guide Me ) and see the list of questions any time they want.
In general, we envision that question-guided query re nement is likely to be very useful for exploratory search, especially for di cult, imprecise or ambiguous queries.
Clari cation questions can be short (more general) or long (more speci c) and should ideally be about di erent aspects of the query topic.
Similar to documents in the classic relevance feedback scenario, questions place the query terms in a speci c context, which may help the users  nd relevant information or initiate exploration of other topics.
However, unlike the full-sized documents, questions are much shorter and hence require less time and e ort from the users for reading and relevance judgment.
In addition to questions, users may also be presented with short answers to them, when they point to a particular question.
Users can also click on the question and be redirected to the document, containing the answer, for further information.
In this sense, questions can be considered as shortcuts to speci c answers.
We also believe that questions can more naturally engage the users into a relevance feedback cycle.
By clicking on the questions, users indicate their interest in a particular aspect of the query topic.
Therefore, based on that signal, a search system can present the next set of questions and search results, by adding the terms in the clicked question to the current query to improve results.
Although question-guided search can be used to supplement the results of any query, it may not be equally e ective for all types of queries.
Short, under-speci ed queries are the best candidates for re nement through questions.
Since question generation algorithm is based on capturing syntactic relations between the terms, queries, containing named entities are well-suited for re nement through questions as well, since re ning questions will allow to explore potential relations of the named entities in a query with other named entities in a corpus.
Overall, question-guided search is a novel way of applying natural language processing methods to improve the usability of search.
It seamlessly integrates lightweight search results navigation and contextual interactive relevance feedback into one retrieval framework.
In this section, we demonstrate how the idea of natural language question-guided retrieval process can be implemented in a search engine.
In order to experimentally evaluate the proposed idea, we have built a prototype of a QUestion-guided Search Engine, which we called QUSE.
In the following sections, we consecutively focus on each individual component of the retrieval process: indexing, retrieval, ranking and feedback.
In Section 4.1, we begin with a description of how dependency parsers can be used to index the occurrences of syntactic patterns, which can be later transformed into questions.
In Section 4.3, we describe the structure of the index, used in QUSE.
Question ranking is discussed in Section 4.4.
And,  nally, we overview the question-based relevance feedback in Section 4.6.
Due to the fact that information contained in a sentence is represented not only by its basic lexical units (words), but also by syntactic relations between them, any natural language sentence can be phrased in multiple ways, even if the meaning conveyed by all the variants is identical.
According to the linguistic theory of dependency grammars [16], any sentence can be represented as a set of dependency relations, which form a tree structure, usually called a dependency tree.
A dependency relationship is an asymmetric binary relationship between a word, called the head (or governor, parent), and another word called the modi er (or dependent, daughter).
Each term in a sentence can have several modi ers, but can modify at most one other term.
The root of a dependency tree does not modify any other words.
Verbs cannot modify any other constituents and, thus, are always the roots of dependency trees.
For example, the dependency structure of the sentence  John found a solution to the problem  is shown in Figure 1.
Figure 1: Example of a dependency tree In the example sentence in Figure 1, there are six pairs of dependency relationships, depicted by the arrows from heads to modi ers.
Each edge is labeled by the syntactic role of a modi er.
For example, the label  subj  means that the modi er in this relation is the subject of a sentence.
In order to convert the sentences in a document collection into dependency trees, we used Minipar [15], a broad coverage dependency parser.
Given an input sentence, Minipar returns its dependency tree, in which the nodes correspond to the terms in the sentence along with the syntactic and semantic labels assigned to them, and the edges represent the dependency relationships between the terms.
Minipar also classi es proper nouns into semantic categories (names of people, organizations, geographical locations, titles, currencies), based on its internal dictionary.
If we consider only syntactic and semantic labels of the nodes in a dependency tree, disregarding the speci c terms corresponding to the nodes, we will get a generalized dependency tree or syntactic pattern.
Obviously, a syntactic pattern is a compressed representation of all dependency trees with the same structure.
We will refer to the nodes of a syntactic pattern as slots.
During indexing, slots are  lled with the actual words from a matching sentence.
When the semantic role of a constituent is important, it is speci ed after the syntactic label of a node.
For example, node 1 of the generalized tree in Figure 2 has the label  subj:person , which means that a parse tree or subtree can match this particular pattern, only if there is a node at that speci c position, which is syntactically labeled as the subject of a sentence and semantically labeled as a proper name, designating a person.
Dependency trees can be used to convert any nominative sentence (or part of it) into a question.
The transformation of a nominative sentence into a question involves changes only to its syntactic structure, without any significant changes to its lexical content.
The general idea behind the question generation algorithm is that we can index the instances of syntactic patterns in a document collection along with the terms  lling the slots of these patterns and convert those instances into questions, according the patterns, slots specify what kind of lexemes can match the pattern.
In the instances, slots store the actual constituents of matching sentences and their stems.
Definition 2.
Syntactico-semantic pattern   de nes a structure on a subset of a set of slots  , given the relation of syntactic dependency.
In other words, a syntactic pattern is a set of the ordered pairs of slots:   = {( ,  ), .
.
.
, ( ,  )} such that in each pair ( ,  ),   is a head of syntactic dependency relationship and   is a modi er.
Let   = { 1,  2, .
.
.
,  } be a collection of   syntactic patterns.
Definition 3.
An Instance of a syntactic pattern   is a mapping      , where   is a set of slots belonging to some pattern      .
An instance of a syntactic pattern occurs when a sentence in the corpus matches one of the syntactic patterns.
An instance is stored and represented by pairs of words and their stems, which are  lling the slots of a matching pattern.
Definition 4.
Context of a pattern instance includes the sentence, containing a pattern instance, and the sentences immediately before and immediately after it.
The context of a sentence is saved to be later shown as an answer to the question generated from an instance.
The purpose of the context is to provide a short answer to the automatically generated question.
Definition 5.
Question template is a subset of the set of ordered slots   of a syntactic pattern      , perturbed and mixed with other terms in such a way that, when instantiated from an instance of a pattern, it conveys the semantics of a question.
In question-guided search, the purpose of the index is to store the instances of syntactic patterns.
The nature of syntactic patterns allows to use relational tables for storing them in the index.
The most important parts of the index, used for question generation are the following relations:   Dictionary of terms and stems   ( ,  ):   - the ID of a term or a stem;   - term or stem itself;   Documents in the repository  ( ,  ):   - the ID of a document;   - number of words in a document   Instances of syntactic patterns:  ( ,  ,  ,  ,  ,  ,  ) where   is the ID of an instance;   is the ID of the document, where an instance occurred;   is the ID of a sentence in the document, where an instance occurred;   is the ID of the pattern, corresponding to an instance;   is the number of the slot, which the term and its stem are  lling;   is the ID of the term,  lling the slot of a pattern instance;   is the ID of the stem,  lling the slot of a pattern instance.
Figure 2: Compressed dependency tree (syntactic pattern) to the question generation templates.
The algorithm to convert sentences into questions is illustrated with the following example sentence:  John went to school in Massachusetts , the dependency tree of which is shown in Figure 3.
Figure 3: Dependency tree of an example sentence In particular, we can manually de ne the following question templates for the syntactic pattern in Figure 2: Where did {1:stem} {0:stem} {2:term} {3:term}?
Who {0:term} {2:term} {3:term} {4:term} {5:stem}?
 Term  in the slot description of a question template means that when the actual question is generated from this template, the original form of the word from the corresponding slot of a syntactic pattern instance is used.
 Stem  means that a morphologically normalized version of a word is used.
Given our example sentence  John went to school in Massachusetts , which matches the pattern in Figure 2, the following questions can be generated from the question templates above: Where did John go to school?
Who went to school in Massachusetts?
Examples of other patterns, used in QUSE, along with the sample sentences, matching each of them, are shown in Table 1.
Terms,  lling the slots of pattern instances, are highlighted with numbered under-braces.
E cient algorithms for recognition of syntactic patterns are discussed in detail in [6].
Let   = { 1,  2, .
.
.
,  } be a collection of   documents, composed from a set of   terms   = { 1, .
.
.
,  } and their stems     = { 1 , .
.
.
,  }.
Definition 1.
Slot: given a set of syntactic labels   and a set of semantic roles  , a set of slots   is a subset of      .
A slot of a syntactic pattern is a relation ( ,  )    , where       and      .
1.
{1:s(person)} {0:i} {2:mod} {3:pcomp- n(location)} {4:mod} {5:pcomp-n(date)} 2.
{1:s(person)} {0:i} {2:obj(person)} {3:mod} {4:pcomp- n(location)} {5:mod} {6:pcomp-n(date)} 3.
{1:s(person)} {0:i} {2:obj} {3:mod} {4:pcomp- n(location)} 4.
{1:s(person)} {0:i} {2:pred} {3:mod} {4:pcomp- n(person)} 5.
{1:s(person)} {0:i} {2:pred} {3:mod} {4:pcomp- n(location)} 6.
{1:s(location)} {0:i} {2:pred} {3:mod} {4:pcomp- n(location)} Matching Sentences Columbia, SouthCarolina , the state capital, (cid:2) (cid:5) (cid:3)(cid:4)
 , where his father was professor at the (cid:5) Wilson (cid:5) (cid:2) lived (cid:2) (cid:3)(cid:4) (cid:5) in (cid:2)(cid:3)(cid:4)(cid:5) (cid:3)(cid:4)
 from (cid:2) (cid:3)(cid:4) (cid:5) (cid:2)



 (cid:3)(cid:4)


 Massachusetts .
(cid:2) (cid:5) (cid:3)(cid:4)
 Columbia Theological Seminary.
In (cid:2)(cid:3)(cid:4)(cid:5)
 (cid:2) (cid:3)(cid:4) (cid:5) , Adams (cid:5) (cid:2) (cid:3)(cid:4)
 married (cid:5) (cid:2) (cid:3)(cid:4)
 Abigail Smith (cid:2) (cid:5) (cid:3)(cid:4)
 at (cid:2)(cid:3)(cid:4)(cid:5)
 Weymouth (cid:2) (cid:5) (cid:3)(cid:4)
 , Kennedy (cid:2) (cid:5) (cid:3)(cid:4)
 had (cid:2) (cid:3)(cid:4) (cid:5)
 near   legendary status (cid:2) (cid:3)(cid:4) (cid:5)
 in (cid:2)(cid:3)(cid:4)(cid:5)
 Ireland , as the  rst (cid:5) (cid:2) (cid:3)(cid:4)
 person of Irish heritage to have a position of world power.
Voight (cid:2) (cid:3)(cid:4) (cid:5) is (cid:2)(cid:3)(cid:4)(cid:5) the father (cid:2) (cid:3)(cid:4) (cid:5) of (cid:2)(cid:3)(cid:4)(cid:5)



 actress Angelina Jolie (cid:5) (cid:2) (cid:3)(cid:4)
 (Angelina Jolie Voight is her birthname) and actor James Haven.
President Kennedy (cid:2) (cid:5) was (cid:2) (cid:3)(cid:4) (cid:5)
 assassinated (cid:2) (cid:5) (cid:3)(cid:4)
 in (cid:2)(cid:3)(cid:4)(cid:5)
 Dallas, Texas (cid:2) (cid:5) (cid:3)(cid:4)
 at 12:30 Washington, D.C.
, formally the District of Columbia and com-(cid:2) (cid:5) monly referred to as Washington, the District, or simply founded on July
 is of (cid:2)(cid:3)(cid:4)(cid:5) the United States , (cid:5) (cid:2) the capital (cid:2) (cid:3)(cid:4) (cid:5) (cid:2)(cid:3)(cid:4)(cid:5) (cid:3)(cid:4)




 p.m.
(cid:3)(cid:4)
 (cid:3)(cid:4)
 Table 1: Examples of syntactic patterns and sentences matching them
 Similar to the traditional document-based retrieval model, the goal of question ranking methods is to determine and use as many useful heuristics (features) as possible to bring potentially interesting and relevant questions up to the top of the list of clari cation questions, returned for a keyword query.
Our approach to question ranking is based on determining the position of a newly added question in the ranked list, according to several heuristics, numerically expressing the relative interestingness and relevance of questions.
Formally, given a set   = { 1,  2, .
.
.
,  } of   ranking heuristics (features), where each heuristic is a function   :      , mapping questions in the set   into the real numbers (feature values), and the two questions  1 = ( 1( 1), .
.
.
,  ( 1)) and  2 = ( 1( 2), .
.
.
,  ( 2)), represented as  tuples of feature values, a non-parametric question ranking function   is a binary function:         {0, 1} on question pairs, such that, if  ( 1,  2) = 1, then the question  1 should be ranked above  2 or, i.e., question  1 is more relevant to the query than the question  2, or  1    2.
Therefore, the ranking procedure is similar to the insertion sorting algorithm, where each new question is compared with the questions that are already in the list until a less relevant question is found or the end of the list has been reached.
When such a question is found, a new question is inserted before it.
It is important to note that, in such a setting, the order, in which the heuristics are applied, determines their relative importance for ranking.
We applied the following ranking heuristics in the order, in which they are presented below: QT:  ( ,  ), the number of query terms that occur both in the query   and the question  , generated from it.
The motivation behind this heuristic is that the questions matching more query terms are potentially more relevant to the information need.
PM:  ( ,  ,  ), the number of query terms that occur both in the query   and the slots of the pattern instance  , from which the question   was generated.
The intuition behind this heuristic is that questions generated from instances that match more query terms are more speci c, and, thus, are more aggressively guiding the users towards their search goals.
DS:  ( ,  ,  ), the retrieval score of the query   with respect to the document   that contains an instance of the pattern, from which the question   was generated.
This heuristic allows to use the scores of traditional retrieval models (vector space, probabilistic or language modeling based) for question ranking.
In our implementation, we used the popular Okapi/BM25 retrieval formula [18]:  ( ,  ) =  ,  ln   +0.5   +0.5   ( 1+1)   1((1 )+      )+      ( 3+1)   3+  where   is the total number of documents in the collection;   is the number of documents that contain a query term;   is the term s frequency in a document;   is the term s frequency in a query;   is the document s length;   is the average length of a document in the collection.
We will illustrate our non-parametric approach to question ranking with the following example.
Suppose a user submits a query   = { 1,  2,  3}, which matches three pattern instances (query terms that are matching the slots of an instance are given in brackets for each instance) in two documents, such that the instances  1 and  2 occur in the document  1 and the instance  3 occurs in the document  2.
The retrieval score of the document  2 with respect to the query   is greater than the score of the document  1,  ( ,  ,  2) >  ( ,  ,  1).
Six questions, which are summarized in Table 2, were generated from the instances  1,  2 and  3.
The query terms, contained in each question, are given in braces after each question.
The  nal ranking of the sample questions in Table 2 by instances  1  2  1[ 1,  2,  3]  2[ 1,  3]  3[ 2] questions  1( 1,  2,  3)  2( 2)  3( 2,  3)  4( 3)  5( 1,  3)  6( 2) Table 2: Matching documents, instances and generated questions for a sample query applying the non-parametric ranking heuristics   = { ( ,  ),  ( ,  ,  ),  ( ,  ,  ))} is shown in Table 3.
 





  





    ( 1)  ( 1)  ( 1)  ( 1)  ( 1)  ( 2) 1.  1 2.  3 3.  5 5.  2 6.  4 4.  6 Table 3: Non-parametric ranking of questions for a sample query
 In this section, we present an algorithm for generating a ranked list of clari cation questions for keyword queries.
Let   be a set of instances:   = {{( 11,  11 ), .
.
.
, ( 1 ,  1 )}, .
.
.
, {( 1,  1 ), .
.
.
, ( ,  )}} of   syntactic patterns  :   = {( 11,  12, .
.
.
,  1 ), .
.
.
, ( 1,  2, .
.
.
,  )} obtained after indexing a document collection.
Suppose a user poses an  term keyword query   = { 1,  2, .
.
.
,  }.
Let  ( ,  ) be a ranking function de ned on a set of ranking heuristics:   = { ( ,  ),  ( ,  ,  ),  ( ,  ,  )}.
The algorithm to generate a list of clari cation questions   ranked according to the ranking function  ( ,  ) is shown in Algorithm 1.
Algorithm 1 operates as follows.
First, a set of pattern instances   with at least one query term is obtained by querying the index (line 1).
Next, for each instance in  , the corresponding pattern and the document, where the pattern instance occurred, are obtained (lines 3 and 4, respectively).
Templates of the questions, which are focused on the query terms and include other slots of the instance, are obtained in line 5.
Next, the slots of the question templates are  lled with the terms from the corresponding slots of the pattern instance (lines 6 and 7).
Once a question is generated from the template, the values of the ranking features are calculated in lines 9-11: the number of query terms, occurring in the generated question, is obtained in line 9; the number of query terms occurring in the slots of the pattern instance, from which the question   was generated, is obtained in line 10; the score of a document containing the pattern instance  , from which the question   was generated, is obtained in line 11.
Finally, the current list of questions is Algorithm 1 Algorithm to generate a ranked list of clari cation questions   for a keyword query   Require: Keyword query,   = { 1,  2, .
.
.
,  } Require: Set of   syntactic patterns,   Require: Set of   instances of syntactic patterns,   Require: Ranking function  ( ,   )

      ( )
      ( )
      ( ,  ,  )
 for   = 0 to   do











 18: end for end for  ( ,  ) =        ( ,  ,  ) =        ( ,  ,  ) =   25( ,  ) for   = 0 to   do end if end for  [ ]    [ ]      [ ] if  ( ,  ) = 1 then  ( ,  ,  ) being searched (lines 12-17) for the question, which should be ranked below the question  , according to the ranking function (line 14).
If such a question is found at position  , the newly generated question is inserted at this position (line 15), pushing other questions towards the end of the list.
Our method for automatic question generation provides a natural way for implicit relevance feedback.
Indeed, when a question is clicked, it can be assumed that a user is interested in this question.
Suppose a user submits a query:   = { , .
.
.
,  ,  , .
.
.
,  } and, after viewing the ranked list of questions  , clicks on the question  ( ,  ,  ,  ), which was generated from the instance   = { , .
.
.
,  ,  ,  ,  , .
.
.
,  },      .
The key idea for question-based relevance feedback is that when a user clicks on the question, containing non-query terms, a system can interpret this action as an indication of the direction of interest, and all the non-query terms in the question can then be added to the original query to enrich the representation of information need.
Speci cally, the original query can be augmented with the terms from other slots of the same instance of a syntactic pattern that was matched with the original query.
Formally, a new query is   =       , where   =          ; for the example above,   = { , .
.
.
,   ,  , .
.
.
,  }   { , .
.
.
,  }.
For example, suppose a user submits a query containing a person s name and clicks on the question, generated from the pattern instance, involving a location and a date.
Both the location and the date can now be added to the original query.
The new query can then be resubmitted to the search system to generate an updated question list and search results, achieving the e ect of feedback.
In this section, we present the results of an experimental evaluation of a prototype search system with the question-evaluation is aimed at demonstrating the added value of the question-guided search process from the two major perspectives: easier and faster navigation in the search results and interactive feedback.
Within the  rst perspective, the focus is on the quality of question generation (automatically generated questions should be grammatically correct) and ranking (relevant and interesting question should be presented  rst).
The second perspective is related to how natural, interesting and interactive the question feedback is for the users (generated questions should encourage further exploration of the query topic).
We crawled, preprocessed and indexed a subset of Wikipedia, consisting of 3000 most viewed articles in 2009, combined with the biographic articles about the famous Americans 1.
Such composition of the test collection allows the users to pose a variety of interesting exploratory queries.
The test collection includes 19547 articles and its total size is around
 and index the occurrences of 32 di erent syntactic patterns, some of which are presented in Table 1.
We designed a special evaluation interface for the system and opened it to the users for a week.
The users, who participated in the evaluation, were a group of 20 engineering graduate students.
We allowed the users to select the queries from a list of prede ned queries or type their own queries directly into the search box.
After submitting their own query or clicking on a link for a prede ned one, the users were forwarded to a page with search results, which were organized into question-answer pairs.
For each query, a maximum of 30 top-ranked questions, along with the answers, have been presented for evaluation.
A snapshot of the sample result page is shown in Figure 4.
Users were asked to provide their judgments regarding the well-formedness (column  W  in Figure 4), interesting-ness (column  I  in Figure 4) and relevance (column  R  in Figure 4) of each question, by putting a check mark into the corresponding check box.
We de ned a well-formed question as a question, which is grammatically correct and meaningful to the user; an interesting question as a question, which is either unexpected or about some fact not previously known by the user, or if it generates interest in further exploration of the question topic; and a relevant question as a question relevant to the topic of a query.
We also explicitly clari ed that some questions may be interesting, but not necessarily relevant, as well as some relevant questions may not necessarily be interesting.
For example, if a user submits the query  clinton  and is willing to  nd some information about Bill Clinton, questions about Hillary Clinton are not relevant.
However, among the questions about Hillary Clinton, there can still be questions interesting to the user.
The  Answer  column in Figure 4 was intended to help the users judge the interestingness and, especially, the relevance of questions.
Well-formedness of a question is not related to its interestingness or relevance.
A question can be well-formed, even if it is not interesting or relevant.
Note that the questions in Figure 4 are presented as hyperlinks, which may be clicked on, should the user be interested in exploring the topic of the clicked question.
After clicking on a question,
 http://en.wikipedia.org/wiki/Category:Lists_of_people_by_U.S.
_state the user is presented with another ranked list of feedback question-answer pairs, generated by issuing a reformulated (feedback) query.
A maximum of 10 feedback questions have been presented for evaluation during each feedback cycle.
After running the system for a week, we collected the user judgments of 2895 questions generated for 184 queries (63 non-feedback queries and 121 feedback ones).
In order to get a more detailed picture of how the proposed retrieval framework performs on di erent types of information needs, we manually classi ed the collected queries into the three groups, which are listed below along with some sample real queries:   SQ (short queries): short (one term only), underspec-i ed and potentially ambiguous queries: e.g.,  ford ,  paris ,  illinois ;   NQ (normal queries): well-formed, generally unambiguous, exploratory queries:  michael jackson ,  bill gates ;   LQ (long queries): long (three or more terms), very speci c queries:  barry bonds babe ruth record ,  bush gulf war ;   FB (feedback queries): queries, generated by the system, when one of the questions was clicked:  cher david letterman return ,  diagnose disease reagan ronald .
The aggregated statistics of user judgments with respect to the absolute number (upper half of each cell) and the relative percentage (lower part of each cell) of clicked (C), well-formed (W), interesting (I), and relevant (R) questions to the total number (T) of questions, generated for the queries of each type, are shown in Table 4.
All queries, regardless of the type, are designated as ALL.
Table 4: User judgments for di erent query types There are several important conclusions, which could be made based on the analysis of Table 4.
First, questions corresponding to the feedback queries have the largest proportion of interesting questions.
This clearly shows the bene t of the question-based feedback strategy.
Second, the overall question click-through rate greater than 3.33% indicates that the users clicked on at least one of the 30 questions presented for each non-feedback query.
Third, relevance of the questions varies across di erent query types and is the highest for normal queries.
Therefore, unambiguous queries generate relatively more relevant questions.
The low precision of questions, generated by the long and feedback queries, can be explained by the more speci c information need corresponding to those types of queries, and hence a smaller subset of potentially relevant questions/answers.
Ambiguous queries naturally result in questions with lower precision.
Finally, the well-formedness of questions is independent of the query type and is about 80% across all query types.
After taking a high-level look at the initial user judgments, we are now ready to move on to a more detailed analysis of all components of the question-guided retrieval process.
Due to the fact that a set of questions, which can be potentially returned for a query, can be much larger than a set of documents, accurate ranking of questions in the question-guided search framework is very important.
Since there may be many relevant questions and their usefulness to the users may vary, we distinguish di erent levels of usefulness of a question and use the Normalized Discounted Cumulative Gain or nDCG [10] to measure the quality of a ranked list of questions.
The DCG at the  -th question is computed as:  ( ) = 3 if   is both interesting and relevant 2 if   is just relevant 1 if   is just interesting 0 if   is neither interesting, nor relevant Given a DCG vector   =  1,  2, .
.
.
,   computed for a list of   questions   that are generated by some ranking method and the DCG vector   =  1,  2, .
.
.
,  , which corresponds to the ideal ranking of the same question list  , a normalized DCG vector is   =  1/ 1,  2/ 2, .
.
.
,  / .
 ( ) =  ( ),  (    1) +  ( ) log2( +1) if   = 1 , otherwise where  ( ) is the grade of the  -th question   in the ranked list, which is computed as follows: {    
 In this section, we present the results of an experimental evaluation of di erent question ranking strategies described in Section 4.4 to determine the best performing non-parametric ranking function.
First, we started with the ranking functions that include only one ranking heuristic  ,  ,   at a time.
Then, we kept adding additional heuristics to the best performing ranking function at each step to determine the best performing combination of ranking heuristics.
The relative performance of di erent ranking functions is summarized in Table 5 As follows from Table 5, the best performing non-parametric question ranking function is  ( ,  ,  ).
This indicates that all three ranking heuristics are useful.
The sequence of application of ranking heuristics in the best-performing ranking function also suggests that the questions, generated from the more speci c patterns (those that match more query terms), should be ranked higher.
This can be explained by fact that the users prefer more speci c questions to the broader ones.
One of the key bene ts of the question-based retrieval process is the possibility of contextual query expansion.
We evaluated the e ectiveness of question-based feedback by comparing precision@n (Figure 5) and nDCG@n (Figure 6) across all feedback and non-feedback questions.
Non-feedback questions were presented after the users submitted their initial queries or clicked on the prede ned query.
Feedback questions were generated and presented after the users clicked on one of the initial questions and the updated initial query has been resubmitted to the system.
Since the updated query includes the original query terms, the clicked question may appear in the feedback questions, however it may not necessarily be ranked high enough to be presented to the users, since the updated query also generates other questions, which could be ranked higher than the clicked one.
The steep slope of the precision curve for the feedback



 Avg.
NDCG 0.8765
 Prec@5 Prec@10
  ( )




  ( )




  ( ,  )  ( ,  )  ( ,  ,  )














 Table 5: Performance of di erent ranking functions
 non FB Head Click how what who when where




 Inter Relev
















 n @ n o s c e r p i i





 question position



 Figure 5: Precision@n for all feedback and non-feedback questions questions in Figure 5 indicates that the question-based feedback aggressively re nes the information need by bringing up a small number of both highly relevant and interesting questions to the top of the question list.
non FB










 n @


 n






 question position



 Figure 6: feedback questions nDCG@n for all feedback and non-Figure 6 further con rms our conclusion that the question-based feedback e ectively improves question ranking by bringing the highly relevant and interesting questions to the  rst three positions of the ranked list.
The proposed novel retrieval framework opens up many interesting opportunities for exploration of user search behavior.
In this section, we aim to analyze user preferences regarding di erent types of questions.
In particular, we focus on the two speci c questions:   is there any relationship between the head word of a question and user judgments/click-through rate?
Table 6: User behavior with respect to di erent question types   is there any relationship between the length of a question and user judgments/click-through rate?
In order to answer the  rst question, we calculated the breakdown of clicked, interesting, and relevant questions across the di erent question types, which is shown in Table 6.
From Table 6, it follows that the users  nd factual questions (i.e. the  what  questions) and questions about a person (i.e. the  who  questions) to be more interesting than questions about time or location.
The same applies to click-throughs, although the di erence is less pronounced, which could be partially explained by the low absolute number of click-throughs compared to the judgments.
In order to answer the second question, in Figure 7 we plotted the distribution of clicked, interesting, and relevant questions across the questions of di erent length.
From Figure 7, it follows s n o i t s e u q f o r e b m u n








 clicked interesting relevant










 question length Figure 7: Distribution of clicked, interesting, and relevant questions over question lengths that the users mostly click on the medium length 3,4,5-word questions.
Users also  nd such medium-length questions to be more interesting and relevant than others.
This work proposed a novel idea of question-guided search process, in which a retrieval system would automatically generate and present to the users a set of potentially interesting questions, based on the search results of a query.
The search result presentation methods to improve the utility of search engines in two ways.
First, it enables the users to navigate directly into the answers, contained in search results, without needing to read the documents, when the generated questions are relevant to their information need.
Second, in case of imprecise or ambiguous queries, the automatically generated questions can naturally engage the users into a feedback cycle to re ne their information need and guide them towards their search goals as well as stimulate new interests for exploratory search.
We proposed a suite of methods for implementing the question-guided search strategy, including the methods for indexing of syntactic pattern instances, generating questions from pattern instances with question templates and ranking questions with multiple heuristics.
We implemented these methods in a prototype and evaluated it on a subset of Wikipedia.
The experimental results demonstrated that the proposed method for question-based query re nement allows the users to more easily navigate in search results and e ectively explore the results space in an interactive and natural way.
We believe that question-guided search is a very promising novel paradigm of interactive search.
Our work is only a  rst step to show its feasibility; there are many interesting directions for future research.
First, it would be interesting to further explore alternative methods for question presentation and ranking; in particular, applying learning to rank methods to optimize the ranking of questions would be very interesting.
Second, we have only explored question generation based on manually created templates; it would be interesting to develop techniques for automatic induction of interesting syntactic patterns and question generation templates.
Finally, a question-guided search engine would generate rich user history, including sequences of questions clicked by the users; such search log data o ers interesting opportunities for user intent analysis and massive implicit feedback.
This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0347933, IIS-0713581, IIS-0713571, and CNS-0834709.
We would like to thank everyone, who participated in the evaluation of QUSE, as well as the anonymous reviewers for their comments, which helped to improve the quality of this paper.
