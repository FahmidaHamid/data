Computing the size of set intersections is a fundamental problem in information retrieval, databases, and machine learning.
Given two sets, S1 and S2, where
 a basic task is to compute the joint size a = |S1   S2|, which measures the (un-normalized) similarity between S1 and S2.
The resemblance, denoted by R, is a normalized similarity measure: , where f1 = |S1|, f2 = |S2|.
f1 + f2   a a In large datasets encountered in information retrieval and databases, ef ciently computing the joint sizes is often highly challenging [3, 18].
Detecting duplicate web pages is a classical example [4, 6].
Typically, each Web document can be processed as  a bag of shingles,  where a shingle consists of w contiguous words in a document.
Here w is a tuning parameter and was set to be w = 5 in several studies [4, 6, 12].
Clearly, the total number of possible shingles is huge.
Considering merely 105 unique English words, the total number of possible 5-shingles should be D = (105)5 = O(1025).
Prior studies used D = 264 [12] and D = 240 [4, 6].
In their seminal work, Broder and his colleagues developed min-wise hashing and successfully applied the technique to duplicate   Supported by Microsoft, NSF-DMS and ONR-YIP.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Arnd Christian K nig Microsoft Research Microsoft Corporation Redmond, WA 98052 chrisko@microsoft.com Web page removal [4, 6].
Since then, there have been considerable theoretical and methodological developments [5, 8, 19, 21 23, 26].
As a general technique for estimating set similarity, minwise hashing has been applied to a wide range of applications, for example, content matching for online advertising [30], detection of large-scale redundancy in enterprise  le systems [14], syntactic similarity algorithms for enterprise information management [27], compressing social networks [9], advertising diversi cation [17], community extraction and classi cation in the Web graph [11], graph sampling [29], wireless sensor networks [25], Web spam [24,33], Web graph compression [7], and text reuse in the Web [2].
Here, we give a brief introduction to this algorithm.
Suppose a random permutation   is performed on  , i.e.,   :      , where   = {0, 1, ..., D   1}.
An elementary probability argument shows that Pr (min( (S1)) = min( (S2))) =

 (1) After k minwise independent permutations,  1,  2, ...,  k, one can estimate R without bias, as a binomial:
 k k(cid:2) (cid:4) j=1 =
 (cid:3) Var



 k (2) (3) Throughout the paper, we frequently use the terms  sample  and  sample size  (i.e., k).
In minwise hashing, a sample is a hashed value, min( j (Si)), which may require e.g., 64 bits to store [12].
In this paper, we establish a uni ed theoretical framework for b-bit minwise hashing.
Instead of using b = 64 bits [12] or 40 bits [4, 6], our theoretical results suggest using as few as b = 1 or b = 2 bits can yield signi cant improvements.
In b-bit minwise hashing, a sample consists of b bits only, as opposed to e.g., 64 bits in the original minwise hashing.
Intuitively, using fewer bits per sample will increase the estimation variance, compared to (3), at the same sample size k. Thus, we will have to increase k to maintain the same accuracy.
Interestingly, our theoretical results will demonstrate that, when resemblance is not too small (e.g., R   0.5, the threshold used in [4, 6]), we do not have to increase k much.
This means our proposed b-bit minwise hashing can be used to improve estimation accuracy and signi cantly reduce storage requirements at the same time.
For example, when b = 1 and R = 0.5, the estimation variance will increase at most by a factor of 3.
In this case, in order not to lose accuracy, we have to increase the sample size by a factor of improvement by using b = 1 will be 64/3 = 21.3.
Algorithm 1 illustrates the procedure of b-bit minwise hashing, based on the theoretical results in Sec.
2.
Algorithm 1 The b-bit minwise hashing algorithm, applied to estimating pairwise resemblances in a collection of N sets.
Input: Sets Sn     = {0, 1, ..., D   1}, n = 1 to N.
Pre-processing:

 (cid:6)(cid:7)b min ( j (Sn)), denoted by en,i,j, i = 1 to b.
Estimation: (Use two sets S1 and S2 as an example.)
i=1 1{e1,i, j
 k  Eb C1,b

 are from Theorem 1 in Sec.
2.
(cid:8) } = 1 , where C1,b and C2,b (cid:5)k = e2,i, j j=1 .
Locality Sensitive Hashing (LSH) [8,20] is a set of techniques for performing approximate search in high dimensions.
In the context of estimating set intersections, there exist LSH families for estimating the resemblance, the arccosine and the Hamming distance [1].
In [8, 16], the authors describe LSH hashing schemes that map objects to {0, 1} (i.e., 1-bit schemes).
The algorithms for the construction, however, are problem speci c.
Two discovered 1-bit schemes are the sign random projections (also known as simhash) [8] and the Hamming distance LSH algorithm proposed by [20].
Our b-bit minwise hashing proposes a new construction, which maps objects to {0, 1, ..., 2b   1} instead of just {0, 1}.
While our major focus is to compare with the original minwise hashing, we also conduct comparisons with the other two known 1-bit schemes.
 1 a  The method of sign (1-bit) random projections estimates the ar-, using our notation for sets S1 ccosine, which is cos and S2.
A separate technical report is devoted to comparing b-bit minwise hashing with sign (1-bit) random projections.
See www.stat.cornell.edu/~li/hashing/RP_minwise.pdf.
That report demonstrates that, unless the similarity level is very low, b-bit minwise hashing outperforms sign random projections.
f1f2 The method of sign random projections has received signi cant attention in the context of duplicate detection.
According to [28], a great advantage of simhash over minwise hashing is the smaller size of the  ngerprints required for duplicate detection.
The space-reduction of b-bit minwise hashing overcomes this issue.
distance LSH algorithm developed in [20] (and surveyed in [1]): Sec.
4 will compare b-bit minwise hashing with the Hamming   When the Hamming distance LSH algorithm is implemented naively, to achieve the same level of accuracy, its required storage space will be many magnitudes larger than that of b-bit minwise hashing in sparse data (i.e., |Si|/D is small).
  If we only store the nonzero locations in the Hamming distance LSH algorithm, then its required storage space will be about one magnitude larger (e.g., 10 to 30 times).
Consider two sets, S1 and S2,
 f1 = |S1|, f2 = |S2|, a = |S1   S2| (cid:3) (cid:4) Apply a random permutation   on S1 and S2:   :      . De ne the minimum values under   to be z1 and z2: z1 = min (  (S1)) , z2 = min (  (S2)) .
De ne e1,i = ith lowest bit of z1, and e2,i = ith lowest bit of z2.
Theorem 1 derives the main probability formula.
(cid:9) (cid:11) THEOREM 1.
Assume D is large.
1{e1,i = e2,i} = 1 b(cid:10) Eb = Pr i=1 where r1 = f1
 , C1,b = A1,b , f2
 + A2,b r2 = r2 r1 + r2 r1 r1 r1 + r2 r2 r1 + r2 , , = C1,b + (1   C2,b) R (4) (5) (6) C2,b = A1,b + A2,b r1 + r2 r1 [1   r1] 1   [1   r1] 2b 1 2b 1 r2 [1   r2] 1   [1   r2] , 2b A2,b = A1,b = (7) For a  xed rj (where j   {1, 2}), Aj,b is a monotonically de-For a  xed b, Aj,b is a monotonically decreasing function of rj   creasing function of b = 1, 2, 3, ....
2b .
[0, 1], reaching a limit: Aj,b = lim rj 0 .
2b (8) Proof: See Appendix A.2 D and r2 = f2 Theorem 1 says that, for a given b, the desired probability (4) is determined by R and the ratios, r1 = f1 D .
The only assumption needed in the proof of Theorem 1 is that D should be large, which is always satis ed in practice.
Aj,b (j   {1, 2}) is a decreasing function of rj and Aj,b   1 2b .
As b increases, Aj,b converges to zero very quickly.
In fact, when b   32, one can essentially view Aj,b = 0.
nation of Theorem 1.
Consider b = 1.
One might expect that A simple heuristic argument may provide a more intuitive expla-Pr (e1,1 = e2,1) =Pr (e1,1 = e2,1|z1 = z2) Pr (z1 = z2) +Pr (e1,1 = e2,1|z1 (cid:9)= z2) Pr (z1 (cid:9)= z2)



 ,


 because when z1 and z2 are not equal, the chance that their last bits are equal  may be  approximately 1
 actually consistent with Theorem 1 when r1, r2   0.
According to (8), as r1, r2   0, we have A1,1, A2,1   1
 also; and hence the probability (4) approaches 1+R
 In practice, when a very accurate estimate is not necessary, one might actually use this approximate formula to simplify the estimator.
The errors, however, could be quite noticeable when r1, r2 are not negligible; see Sec.
5.2.
Theorem 1 suggests an unbiased estimator  Rb for R:  Rb =  Eb = ,  Eb   C1,b (cid:12)
 b(cid:10) k(cid:2)
 k j=1 i=1 (cid:13) 1{e1,i, j = e2,i, j} = 1 , (9) (10) (cid:4)  Eb Var [1   C2,b] (cid:4) (cid:3)
 As we decrease b, the space needed for storing each  sample  will be smaller; the estimation variance (11) at the same sample size k, however, will increase.
This variance-space trade-off can be precisely quanti ed by the storage factor B(b; R, r1, r2): B(b; R, r1, r2) = b   Var b [C1,b + (1   C2,b)R] [1   C1,b   (1   C2,b)R]   k  Rb = [1   C2,b]
 .
(12) (cid:3) (cid:4) Lower B(b) is better.
The ratio, B(b1;R,r1,r2) B(b2;R,r1,r2) , measures the improvement of using b = b2 (e.g., b2 = 1) over using b = b1 (e.g., b1 = 64).
Some algebra yields the following Theorem.
THEOREM 2.
If r1 = r2 and b1 > b2, then A1,b1 (1   R) + R A1,b2 (1   R) + R B(b1; R, r1, r2) B(b2; R, r1, r2) b1 b2 =

 , (13) is a monotonically increasing function of R   [0, 1].
If R   1 (which implies r1   r2), then B(b1; R, r1, r2) B(b2; R, r1, r2)   b1 b2

 .
(14) If r1 = r2, b2 = 1, b1   32 (hence we treat A1,b = 0), then B(b1; R, r1, r2) B(1; R, r1, r2) R + 1   r1 Proof: We omit the proof due to its simplicity.2 = b1
 (15) Suppose the original minwise hashing used 64 bits to store each sample, then the maximum improvement of b-bit minwise hashing would be 64-fold, attained when r1 = r2 = 1 and R = 1, according to (15).
In the least favorable situation, i.e., r1, r2   0, the improvement will still be 64
 Fig. 1 plots B(64) B(b) , to directly visualize the relative improvements, which are consistent with what Theorem 2 predicts.
The plots show that, when R is very large (which is the case in many practical applications), it is always good to use b = 1.
However, when R is small, using larger b may be better.
The cutoff point depends on r1, r2, R. For example, when r1 = r2 and both are small, it would be better to use b = 2 than b = 1 if R < 0.4, as shown in Fig. 1.
where e1,i, j (e2,i, j ) denotes the ith lowest bit of z1 (z2), under the permutation  j.
Following property of binomial distribution, (cid:3) (cid:4)  Rb = Var [C1,b + (1   C2,b)R] [1   C1,b   (1   C2,b)R]
 Eb(1   Eb) [1   C2,b]

 k t n e m e v o r p m
 (11) =
 k (cid:3) [1   C2,b]
  Rb (cid:4) For large b, Var mator for the original minwise hashing:
 (cid:4) = In fact, when b   32, Var indistinguishable for practical purposes.
b  Var lim (cid:3)  Rb  Rb k converges to the variance of  RM , the esti-(cid:4) (cid:3) (cid:4)
 = Var (cid:3) .
t n e m e v o r p m
 and Var
 are numerically














 r1 = r2 = 10 10


 Resemblance (R) r1 = r2 = 0.5 b = 1 b = 2 b = 3 b = 4

 b = 1 b = 2 b = 3 b = 4














 t n e m e v o r p m
 t n e m e v o r p m
 r1 = r2 = 0.1


 Resemblance (R) b = 1 b = 2 b = 3 b = 4

 b = 1 b = 2 b = 3 b = 4






 r1 = r2 = 0.9






 Resemblance (R) Figure 1: B(64) B(b) , the relative storage improvement of using b =
 Resemblance (R)

 Experiment 1 is a sanity check, to verify: (A) our proposed estimator  Rb in (9), is indeed unbiased; and (B) its variance follows the prediction by our formula in (11).
Experiment 2 is a duplicate detection task using a Microsoft proprietary collection of 1,000,000 news articles.
Experiment 3 is another duplicate detection task using 300,000 UCI NYTimes news articles.
The data, extracted from Microsoft Web crawls, consists of 10 pairs of sets (i.e., total 20 words).
Each set consists of the document IDs which contain the word at least once.
Thus, this experiment is for estimating word associations.
Table 1: Ten pairs of words used in Experiment 1.
For example,  KONG  and  HONG  correspond to the two sets of document IDs which contained word  KONG  and word  HONG  respectively.
Word 1 Word 2



















 r1









 r2












































 B(1) and B(64) Table 1 summarizes the data and also provides the theoretical improvements, B(32) B(1) .
The words were selected to include highly frequent word pairs (e.g.,  OF-AND ), highly rare word pairs (e.g.,  GAMBIA-KIRIBATI ), highly unbalanced pairs (e.g.,  A-Test ), highly similar pairs (e.g,  KONG-HONG ), as well as word pairs that are not quite similar (e.g.,  LOW-PAY ).
We estimate the resemblance using the original minwise hashing estimator  RM and the b-bit estimator  Rb (b = 1, 2, 3).
Figure 2 presents the estimation biases for the selected 2 word pairs.
Theoretically, both estimators,  RM and  Rb, are unbiased (i.e., the y-axis in Figure 2 should be zero, after an in nite number of repetitions).
Figure 2 veri es this fact because the empirical biases are all very small and no systematic biases can be observed.
b=3
 b=1


  2  4  6 s a
 i  8

 b=2 b=1 x 10 4 b=3
 s a
 i

  5  10 b=1
 b = 1 b = 2 b = 3
 b=2
 b=2 b=1
 b = 1 b = 2 b = 3

 Sample size k


 Sample size k
 Figure 2: Empirical biases from 25000 simulations at each sample size k.  M  denotes the original minwise hashing.
Figure 3 plots the empirical mean square errors (MSE = variance + bias2) in solid lines, and the theoretical variances (11) in dashed lines, for 6 word pairs (instead of 10 pairs, due to the space limit).
All dashed lines are invisible because they overlap with the corresponding solid curves.
Thus, this experiment validates that the variance formula (11) is accurate and  Rb is indeed unbiased (otherwise, MSE will differ from the variance).
)


 ( r o r r e e r a u q s n a e
 )


 ( r o r r e e r a u q s n a e
 )


 ( r o r r e e r a u q s n a e


 b=1






 Sample size k

 b=1










 Sample size k b=1 b=2



 Sample size k b = 1 b = 2 b = 3
 Theor.
)


 ( r o r r e e r a u q s n a e

 b = 1 b = 2 b = 3
 Theor.
)


 ( r o r r e e r a u q s n a e

 b = 1 b = 2 b = 3
 Theor.
)


 ( r o r r e e r a u q s n a e


 b=1

 b = 1 b = 2 b = 3
 Theor.
Sample size k
 b = 1 b = 2 b = 3
 Theor.
b = 1 b = 2 b = 3
 Theor.
b=1




 Sample size k b=1 b=2 b=3





 Sample size k
 Figure 3: Mean square errors (MSEs).
 M  denotes the original minwise hashing.
 Theor.  denotes the theoretical variances of Var(  Rb)(11) and Var(  RM )(3).
The dashed curves, however, are invisible because the empirical MSEs overlapped the theoretical variances.
At the same k, Var(  R1) > Var(  R2) > Var(  R3) > Var(  RM ).
However,  R1 (  R2) only requires 1 bit (2 bits) per sample, while  RM requires 32 or 64 bits.
To illustrate the improvements by the use of b-bit minwise hashing on a real-life application, we conducted a duplicate detection experiment using a corpus of 106 news documents.
The dataset was crawled as part of the BLEWS project at Microsoft [15].
We computed pairwise resemblances for all documents and retrieved documents pairs with resemblance R larger than a threshold R0.
We estimate the resemblances using  Rb with b = 1, 2, 4 bits, and the original minwise hashing (using 32 bits).
Figure 4 presents the precision & recall curves.
The recall values (bottom two panels in Figure 4) are all very high and do not differentiate the estimators.
i i n o s c e r
 i i n o s c e r
 i i n o s c e r
 l l a c e

















































 b=2 b=1
 b=1 b=2 b=4
 i i n o s c e r


 Sample size (k)

 i i n o s c e r

 b=1 b=2 b=4


 Sample size (k)



 b=1
 b=1 i i n o s c e r

 b=1 b=2 b=4




 Sample size (k) Recall


 Sample size (k)

 l l a c e
 b=1 b=2 b=4




















































 b=1

 b=1

 b=1


 Sample size (k)


 Sample size (k)


 Sample size (k) Recall



 Sample size (k) b=1 b=2 b=4


 b=1 b=2 b=4


 b=1 b=2 b=4


 b=1 b=2 b=4


 Figure 4: Microsoft collection of news data.
The task is to retrieve news article pairs with resemblance R   R0.
The recall curves (bottom two panels) indicate all estimators are equally good (in recalls).
The precision curves are more interesting for differentiating estimators.
For example, when R0 = 0.4 (top right panel), in order to achieve a precision = 0.80, the estimators  RM ,  R4,  R2, and  R1 require k = 50, 50, 75, 145 samples, respectively, indicating  R4,  R2, and  R1 respectively improve  RM by 8-fold, 10.7-fold, and 11-fold.
The precision curves for  R4 (using 4 bits per sample) and  RM (using 32 bits per sample) are almost indistinguishable, suggesting a 8-fold improvement in space using b = 4.
When using b = 1 or 2, the space improvements are normally around 10-fold to 20-fold, compared to  RM , especially for achieving high precisions (e.g.,   0.9).
This experiment again con rms the signi cant improvement of the b-bit minwise hashing using b = 1 (or 2).
Table 2 summarizes the relative improvements.
In this experiment,  RM only used 32 bits per sample.
For even larger applications, however, 64 bits per sample may be necessary [12]; and the improvements of  Rb will be even more signi cant.
Note that in the context of (Web) document duplicate detection, in addition to shingling, a number of specialized hash-signatures have been proposed, which leverage properties of natural-language We denote the samples of yi by hi, where hi = {hij , j = 1 to k} is a k-dimensional vector.
These samples will be used to estimate the Hamming distance H (using S1, S2 as an example):
 [y1i (cid:9)= y2i] = |S1   S2|   |S1   S2| = f1 + f2   2a.
D 1(cid:2) i=0 Using the samples h1 and h2, an unbiased estimator of H is simply k(cid:2) j=1

 k [h1j (cid:9)= h2j ] ,
 (16) (cid:17) ([h1j (cid:9)= h2j ]) (cid:9)(cid:5)D 1 i=0 [y1i (cid:9)= y2i] (cid:11)2(cid:19)
  
 whose variance would be (cid:3) (cid:4) Var
 = = k
 (cid:14) (cid:15) [h1j (cid:9)= h2j ] (cid:18)(cid:5)D 1 i=0 [y1i (cid:9)= y2i] (cid:20) (cid:21)


 .
k2
 k
 k = (cid:4) (cid:3) (17) The above analysis assumes k (cid:11) D (which is satis ed in prac-in (17) by D k tice); otherwise one should multiply the Var
 the  nite sample correction factor.  It would be interesting to compare  H with b-bit minwise hashing.
In order to estimate H, we need to convert the resemblance estimator  Rb (9) to  Hb:  Hb = f1 + f2   2 (f1 + f2).
(18) (f1 + f2) =  Rb

 The variance of  Hb can be computed from Var  Rb (11) using the  delta method  in statistics (note that (cid:3) (cid:4) Var  Hb =Var  Rb (f1 + f2)
 (cid:3) (cid:3) (cid:4) (cid:4) =Var  Rb 4(r1 + r2)2


  2 (1+x)2 ): (cid:25)2 (cid:24)
 k2 (cid:25) (cid:24)
 k2 (19)
 (cid:25) .
(cid:4)
 (cid:3)
 (cid:23)(cid:6) (cid:22) (cid:24)  2 1 x 1+x =
 Recall ri = fi/D.
To verify the variances in (17) and (19), we conduct experiments using the same data as in Experiment 1.
This time, we estimate H instead of R, using both  H (16) and  Hb (18).
Figure 6 reports the mean square errors, together with the theoretical variances (17) and (19).
We can see that the theoretical variance formulas are accurate.
When the data is not dense, the estimator  Hb (18) given by b-bit minwise hashing is much more accurate than the estimator  H (16).
However, when the data is dense (e.g.,  OF-AND ),  H could still outperform  Hb.
We now compare the actual storage needed by  Hb and  H.
We de ne the following two ratios to make fair comparisons: (cid:3) (cid:4) (cid:3)   k   bk Var , Gb =
 Var (cid:4)   r1+r2   bk
  Hb 64k .
(20) (cid:3) (cid:3) (cid:4) (cid:4) Var
 Var  Hb text (such as the placement of stopwords [31]).
However, our approach is not aimed at any speci c type of data, but is a general, domain-independent technique.
Also, to the extent that other approaches rely on minwise hashing for signature computation, these may be combined with our techniques.
Table 2: Relative improvement (in space) of  Rb (using b bits per sample) over  RM (32 bits per sample).
For precision = 0.9, 0.95, we  nd the required sample sizes (from Figure 4) for  RM and  Rb and use them to estimate the required storage in bits.
The values in the table are the ratios of the storage costs.
The improvements are consistent with the theoretical predictions in Figure 1.
Precision = 0.9 b = 1 2



























 Precision = 0.95 b = 1 2     7.1
   10.0











 We conducted another duplicate detection experiment on a public (UCI) collection of 300,000 NYTimes articles.
The purpose is to ensure that our experiment will be repeatable by those who can not access the proprietary data in Experiment 2.
Figure 5 presents the precision curves for representative threshold R0 s.
The recall curves are not shown because they could not differentiate estimators, just like in Experiment 1.
The curves con rm again that using b = 1 or b = 2 bits,  Rb could improve the original minwise hashing (using 32 bits per sample) by a factor of
 with the curves for  RM , verifying an expected 8-fold improvement.
i i n o s c e r
 i i n o s c e r

























 b=1 b=2 b=4

 i i n o s c e r


 Sample size (k)


 b=1
 b=1 i i n o s c e r

 b=1 b=2 b=4





 Sample size (k)

























 b=1
 b=1



 Sample size (k)


 Sample size (k) b=1 b=2 b=4


 b=1 b=2 b=4


 Figure 5: UCI collection of NYTimes data.
The task is to retrieve news article pairs with resemblance R   R0.
Wb =


 The Hamming distance LSH algorithm proposed in [20] is an in uential 1-bit LSH scheme.
In this algorithm, a set Si, is mapped into a D-dimensional binary vector, yi: yit = 1, if t   Si; yit = 0, otherwise.
Wb and Gb are de ned in the same spirit as the ratio of the storage factors introduced in Sec.
2.3.
Recall each sample of b-bit minwise hashing requires b bits (i.e., bk bits per set).
If we assume each sample in the Hamming distance LSH requires 1 bit, then Wb in (20) is a fair indicator and Wb > 1 means  Hb outperforms  H.
However, as can be veri ed in Fig. 6 and Fig 7, when r1 and r2 are small (which is usually the case in practice), Wb tends to be very


 ( r o r r e e r a u q s n a e
 )


 ( r o r r e e r a u q s n a e














 b=1 b=2

 Sample size k

 )


 ( r o r r e e r a u q s n a e








 b=1 b=2

 Sample size k




 b=1
 b=2
 large, when r1, r2 are small.
However, when r1 is very large (e.g.,
 tance LSH could still outperform b-bit minwise in dense data.
By only storing the nonzero locations, Figure 7 illustrates that b-bit minwise hashing will outperform the Hamming distance LSH algorithm, usually by a factor of 10 (for small R) to 30 (for large R and r1   r2).
)


 ( r o r r e e r a u q s n a e


 Sample size k




 The previous results establish the signi cant reduction in storage requirements possible using b-bit minwise hashing.
This section demonstrates that these also translate into signi cant improvements in computational overhead in the estimation phrase.
The computational cost in the preprocessing phrase, however, will increase.
b=1 b=2

 Sample size k

 Figure 6: MSEs (normalized by H 2), for comparing  H (16) with  Hb (18).
In each panel, three solid curves stand for  H (labeled by  H ),  H1 (by  b=1 ), and  H2 (by  b=2 ), respectively.
The dashed lines are the corresponding theoretical variances (17) and (19), which are largely overlapped by the solid lines.
When the sample size k is not large, the empirical MSEs of  Hb deviate from the theoretical variances, due to the bias caused by the nonlinear transformation of  Hb from  Rb in (18).
large, indicating a highly signi cant improvement of b-bit minwise hashing over the Hamming distance LSH algorithm in [20].
We consider in practice one will most likely implement the algorithm by only storing nonzero locations.
In other words, for set Si, only ri   k locations need to be stored (each is assumed to use
 64k (per set).
In fact, we have the following Theorem for Gb when r1, r2   0.
THEOREM 3.
Consider r1, r2   0, and Gb as de ned in (20).
(cid:4) (cid:3) b   1
 2b   1 2b Proof: We omit the proof due to its simplicity.
2 Figure 7 plots W1 and G1, for r1 = r2 = 10 then Gb   8 b then Gb   64 b If R   0, If R   1, .
 4, 0.001, 0.01, 0,1 (which are probably reasonable in practice), as well as r1 = r2 = 0.9 (as a sanity check).
Note that, not all combinations of r1, r2, R are possible.
For example, when r1 = r2 = 1, then R has to be 1.
 6, 10 .
(21) (22)










 W1, r2 = r1 r1 = 1e 6 1e 4





 Resemblance (R) r1 = 0.9











 G1, r2 = r1 r1 = 0.9 r1 = 1e 6 to 0.1




 Resemblance (R)  6, 10 Figure 7: W1 and G1 as de ned in (20).
We consider r1 =  4, 0.001, 0.01, 0.1, 0.9.
Note that not all combinations
 of (r1, r2, R) are possible.
The plot for G1 also veri es the theoretical limits proved in Theorem 3.
Figure 7 con rms our theoretical results.
W1 will be extremely In the preprocessing phrase, we need to generate minwise hashing functions and apply them to all the sets for creating  ngerprints.
This phrase is actually fairly fast [4] and is usually done offline, incurring a onetime cost.
Also, sets can be individually processed, meaning that this step is easy to parallelize.
The computation required for b-bit minwise hashes differs from the computation of traditional minwise hashes in two respects: (A) we require a larger number of (smaller-sized) samples, in turn requiring more hashing and (B) the packing of b-bit samples into 64-bit (or 32-bit) words requires additional bit-manipulation.
It turns out the overhead for (B) is small and the overall computation time scales nearly linearly with k; see Fig. 8.
As we have analyzed, b-bit minwise hashing only requires increasing k by a small factor such as 3.
Therefore, we consider the overhead in the preprocessing stage not to be a major issue.
Also, it is important to note that b-bit minwise hashing provides the  exibility of trading storage with preprocessing time by using b > 1.
) c e s ( e m
 i x 104










 32 bits 1 bit




 Sample size k (# hashing)




 Figure 8: Running time in the preprocessing phrase on 100K news articles.
3 hashing functions were used: 2-universal hashing (labeled by  2-U ), 4-universal hashing (labeled by  4-U ), and full permutations (labeled by  FP ).
Experiments with 1-bit hashing are reported in 3 dashed lines, which are only slightly higher (due to additional bit-packing) than their corresponding solid lines (the original minwise hashing using 32-bit).
The experiment in Fig. 8 was conducted on 100K articles from the BLEWS project [15].
We considered 3 hashing functions:  rst, 2-universal hash functions (computed using the fast universal hash ing scheme described [10]); second, 4-universal hash-functions (computed using the CWtrick algorithm of [32]); and  nally full random permutations (computed using the Fisher-Yates shuf e [13]).
We have shown earlier that, when R   0.5 and b = 1, we expect a storage reduction of at least a factor of 21.3, compared to using computational overhead of the estimation.
In the following, we will analyze how this impacts the Here, the key operation is the computation of the number of identical b-bit samples.
While standard hash signatures that are multiples of 16-bit can easily be compared using a single machine instruction, ef ciently computing the overlap between b-bit samples for small b is less straightforward.
In the following, we will describe techniques for computing the number of identical b-bit samples when these are stored in a compact manner, meaning that individual b-bit samples e1,i,j and e2,i,j, i = 1, .
.
.
, b, j = 1, .
.
.
k are packed into arrays Al[1, .
.
.
, k b w ], l = 1, 2 of w-bit words.
To compute the number of identical b-bit samples, we iterate through the arrays; for an each offset h, we  rst compute v = A1[h]   A2[h], where   denotes the bitwise-XOR.
Subsequently, the h-th bit of v will be set if and only if the h-th bits in A1[h] and A2[h] are different.
Hence, to compute the number of overlapping b-bit samples encoded in A1[h] and A2[h], we need to compute the number of b-bit blocks ending at offsets divisible by b that only contain 0s.
The case of b = 1 corresponds to the problem of counting the number of 0-bits in a word.
We tested different methods suggested in [34] and found the fastest approach to be pre-computing an array bits[1, .
.
.
, 216], such that bits[t] corresponds to the number of 0-bits in the binary representation of t. Then we can compute the number of 0-bits in v (in case of w = 32) as c = bits[v & 0xffffu] + bits[(v (cid:13) 16) & 0xffffu].
Interestingly, we can use the same method for the cases where b > 1, as we only need to modify the values stored in bits, setting bits[i] to the number of b-bit blocks that only contain 0-bits in the binary representation of i.
We evaluated this approach using a loop computing the number of identical samples in two signatures covering a total of 1.8 billion
 bit hashing requires 1.67x the time that the 32-bit minwise hashing requires.The results were essentially identical for b = 2.
Combined with the reduction in overall storage (for a given accuracy level), this means a signi cant speed improvement in the estimation phase: suppose in the original minwise hashing, each sample is stored using 64 bits.
If we use 1-bit minwise hashing and consider R > 0.5, our previous analysis has shown that we could gain a storage reduction at least by a factor of 64/3 = 21.3 fold.
The improvement in computational ef ciency would be 21.3/1.67 = 12.8 fold, which is still signi cant.
D and D .
The storage cost could be a concern if r1 (r2) must be The unbiased estimator  Rb (9) requires knowing r1 = f1 r2 = f1 represented with a high accuracy (e.g., 64 bits).
This section illustrates that we only need to quantize r1 and r2 into Q levels, where Q = 24 is probably good enough and Q = 28 is more than suf cient.
In other words, for each set, we only need to increase the total storage by 4 bits or 8 bits, which are negligible.
For simplicity, we carry out the analysis for b = 1 and r1 = r2 = r. In this case, A1,1 = A2,1 = C1,1 = C2,1 = 1 r 2 r , and the correct estimator, denoted by  R1,r would be (cid:4)  R1,r = (2   r)  E1   (1   r), Bias (cid:3) (cid:4) (cid:3)  R1,r (cid:3) (cid:4)  R1,r

 (1   r + R)(1   R) .
k Var  R1,r = See the de nition of  E1 in (10).
Now, suppose we only store an approximate value of r, denoted by  r.
The corresponding (approximate) estimator is denoted by  R1, r: (cid:4)  R1, r = (2    r)  E1   (1    r), (cid:4) (cid:3) (cid:3) Bias  R1, r (cid:3) (cid:4)  R1, r

 (1   r + R)(1   R) Var  R1, r = k , ( r   r)(1   R) 2   r (2    r)2 (2   r)2 .
Thus, the (absolute) bias is upper bounded by | r r| (in the worst case, i.e., R = 0 and r = 1).
Using Q = 24 levels of quantization, the bias is bounded by 1/16 = 0.0625.
In a reasonable situation, e.g., R   0.5, the bias will be much smaller than 0.0625.
Of course, if we increase the quantization levels to Q = 28, the bias (< 1/256 = 0.0039) will be negligible, even in the worst case.
Similarly, by examining the difference of the variances, (cid:4) (cid:3)   Var (cid:4)(cid:26)(cid:26)(cid:26) (cid:26)(cid:26)(cid:26)Var (cid:3) | r   r| =  R1, r  R1,r (1   r + R)(1   R) (4    r   r) (2   r)2 we can see that Q = 28 would be more than suf cient.
k , Our theoretical and empirical results have con rmed that, when the resemblance R is reasonably high, each bit per sample may contain strong information for estimating the similarity.
This naturally leads to the conjecture that, when R is close to 1, one might further improve the performance by looking at a combination of multiple bits (i.e.,  b < 1 ).
One simple approach is to combine two bits from two permutations using XOR ( ) operations.
Recall e1,1,  denotes the lowest bit of the hashed value under  .
Theorem 1 has proved that E1 = Pr (e1,1,  = e2,1, ) = C1,1 + (1   C2,1) R Consider two permutations  1 and  2.
We store x1 = e1,1, 1   e1,1, 2 , x2 = e2,1, 1   e2,1, 2 Then x1 = x2 either when e1,1, 1 = e2,1, 1 and e1,1, 2 = e2,1, 2 , or, when e1,1, 1 (cid:9)= e2,1, 1 and e1,1, 2 (cid:9)= e2,1, 2 .
Thus
 T = Pr (x1 = x2) = E2

 (23) which is a quadratic equation with a solution

  


 .
(24) We can estimate T without bias as a binomial.
The resultant estimator for R will be biased, at small sample size k, due to the nonlinearity.
We will recommend the following estimator (cid:27) max{2  T   1, 0} + 1   2C1,1


 (25) The truncation max{ .
, 0} will introduce further bias; but it is necessary and is usually a good bias-variance trade-off.
We use  R1/2 to indicate that two bits are combined into one.
The asymptotic variance of  R1/2 can be derived using the  delta method  .
(cid:3) (cid:4) Var
 =

 k


 (cid:24) (cid:25)
 k2 .
(26) Note that each sample is still stored using 1 bit, despite that we use  b = 1/2  to denote this estimator.
(cid:3) (cid:3) (cid:4) (cid:4) = lim
 Var
 lim
 Var




 = 2.
(27) (Recall, if R = 1, then r1 = r2, C1,1 = C2,1, and E1 = C1,1 +
 is not too large.
For example, one can numerically show that (cid:3) (cid:4) (cid:3) (cid:4) Var
 < Var
 , if R < 0.5774, r1, r2   0 Figure 9 plots the empirical MSEs for four word pairs in Experiment 1, for  R1/2,  R1, and  RM .
For the highly similar pair,  KONG-HONG,   R1/2 exhibits superior performance compared to  R1.
For the fairly similar pair,  OF-AND,   R1/2 is still considerably better.
For  UNITED-STATES,  whose R = 0.591,  R1/2 performs similarly to  R1.
For  LOW-PAY,  whose R = 0.112 only, the theoretical variance of  R1/2 is very large.
However, owing to the truncation in (25) (i.e., the variance-bias trade-off), the empirical performance of  R1/2 is not too bad.
)


 ( r o r r e e r a u q s n a e





 )


 ( r o r r e e r a u q s n a e

 b=1

 Sample size k b = 1/2

 b=1

 b = 1 b = 1/2
 Theor.
b = 1 b = 1/2
 Theor.
)


 ( r o r r e e r a u q s n a e
 )


 ( r o r r e e r a u q s n a e


 b = 1/2


 b = 1 b = 1/2
 Theor.
b = 1
 b = 1 b = 1/2
 Theor.
