Web 2.0 has empowered users to actively interact with each other, forming social networks around mutually interesting information and publishing large amounts of useful user-generated content online.
One popular and important type of such user-generated content is the review, where users post detailed commentary on online portals about their experiences and opinions on products, events, or services.
Reviews play a central role in the decision-making Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
process of online users for a variety of tasks including purchasing products, booking  ights and hotels, selecting restaurants, and picking movies to watch.
Sites like Yelp.com and Epinions.
com have created a viable business as review portals, while part of the popularity and success of Amazon.com is attributed to their comprehensive user reviews.
As online commerce activity continues to grow [9], the role of online reviews is expected to become increasingly important.
Unfortunately, the abundance of user-generated content comes at a price.
For every interesting opinion, or helpful review, there are also large amounts of spam content, unhelpful opinions, as well as highly subjective and misleading information.
Sifting through large quantities of reviews to identify high quality and useful information is a tedious, error-prone process.
It is thus highly desirable to develop reliable methods to assess the quality of reviews automatically.
Robust and reliable review quality prediction will enable sites to surface high-quality reviews to users while bene ting other important popular applications such as sentiment extraction and review summarization [8, 7], by providing high-quality content on which to operate.
Automatic review quality prediction is useful even for sites providing a mechanism where users can evaluate or rate the helpfulness of a review (e.g.
Amazon.com and Epinions.com).
Not all reviews receive the same helpfulness evaluation [10].
There is a rich-get-richer effect [11] where the top reviews accumulate more and more ratings, while more recent reviews are rarely read and thus not rated.
Furthermore, such helpfulness evaluation is available only within a speci c Web site, and is not comparable across different sources.
However, it would be more useful for users if reviews from different sources for the same item could be aggregated and rated automatically on the same scale.
This need is addressed by a number of increasingly popular aggregation sites such as Wise.com.
For these sites, automatic review rating is essential in order to meaningfully present the collected reviews.
Most previous work [17, 10, 11, 6, 12, 15] attempts to solve the problem of review evaluation by treating each review as a standalone text document, extracting features from the text and learning a function based on these features for predicting review quality.
However, in addition to textual content, there is much more information available that is useful for this task.
Online reviews are produced by identi able authors (reviewers) who interact with one another to form social networks.
The history of reviewers and their social network interactions provide a social context for the reviews.
In our approach, we mine combined textual, and social context information to evaluate the quality of individual reviewers and to assess the quality of the reviews.
can help enhance the accuracy of a text-based quality predictor.
To the best of our knowledge, this is the  rst time that textual, author and social network information are combined for assessing review quality.
Expressed very generally, our idea is that social context reveals a lot about the quality of reviewers, which in turn affects the quality of the reviews.
We formulate hypotheses that capture this intuition and then mathematically model these hypotheses by developing regularization constraints which augment text-based review quality prediction.
The resulting quality predictor is formulated into a well-formed convex optimization problem with ef -cient solution.
The proposed regularization framework falls under the category of semi-supervised learning, making use of a small amount of labeled data as well as a large amount of unlabeled data.
It also has the advantage that the learned predictor is applicable to any review, even reviews from different sources or reviews for which the reviewer s social context is not available.
Finally, we experiment with real review data from an online commerce portal.
We test our hypotheses and show that they hold for all three categories of data we consider.
We then experimentally demonstrate that our novel regularization methods that combine social context with text information can lead to improved accuracy of review quality prediction, especially when the available training data is sparse.
The remainder of our paper is structured as follows.
We  rst formally de ne the problem in Section 2.
In Section 3 we present a text-based quality predictor which we use as our baseline.
In Section 4, we outline our proposed methods for exploiting social context, formulate our hypotheses, and provide the mathematical modeling.
In Section 5 we experimentally validate our hypotheses, evaluate the prediction performance of our methods and compare against baselines.
Finally, we go over the related work in Section 6 and conclude in Section 7.
A review system consists of three sets of three different types of entities: a set I = {i1, ..., iN} of N items (products, events, or services); a set R = {r1, ..., rn} of n reviews over these items; and a set U = {u1, ..., um} of m reviewers (or users) that have authored these reviews.
Each entity has a set of attributes T associated with it.
For an item i or a user u, Ti and Tu are sets of attribute-value pairs describing the item and the user respectively while for a review r, Tr is the text of the review.
We are also given relationships between these sets of entities.
There is a function M : R   I that maps each review r to a unique item ir = M (r); an authorship function A : R   U, that maps each review r to a unique reviewer ur = A(r); and a relation S   U   U that de nes the social network relationships between users.
Since each review is associated with a unique item, we omit the set I, unless necessary, and assume all information about the item ir (item identi er and attributes) is included as part of the attributes Tr of review r. We also model the social network relation as a directed graph GS = (U, S) with adjacency matrix S, where Suv = 1 if there is a link or edge from u to v and zero otherwise.
We assume that the links between users in the social network capture semantics of trust and friendship: the meaning of user u linking to user v is that u values the opinions of user v as a reviewer.
The information about the authors of the reviews along with the social network of the reviewers places the reviews within a social context.
More formally we have the following de nition.
(SOCIAL CONTEXT).
Given a set of reviews R, we de ne the social context of the set R as the triple C(R) = (cid:4)U, A, S(cid:5), of the set of reviewers U, the authorship function A, and the social network relation S.
The set of reviews R contains both labeled (RL) and unlabeled (RU ) reviews.
For each review ri   RL in the labeled subset of reviews we observe a numeric value qi that captures the true quality and helpfulness of the review.
We use L = {(ri, qi)}, to denote the set of review-quality pairs.
Such quality values can be obtained through manual labeling or through feedback mechanisms in place for some online portals.
Given the input data {RL   RU , C(R), L}, we want to learn a quality predictor Q that, for a review r, predicts the quality of the review.
A review r is represented as an f-dimensional real vector r over a feature space F constructed from the information in R and f   R that C(R).
So the quality predictor is a function Q : R maps a review feature vector to a numerical quality value.
Previous work has used the information in {RL, L} for learning a quality predictor, based mostly on different kinds of textual features.
In this paper, we investigate how to enhance the quality predictor function Q using the social context C(R) of the reviews in addition to the information in {RL, L}.
Our exploration for the prediction function Q takes the following steps.
First we construct a text-based baseline predictor that makes use of only the information in {RL, L}.
Then we enhance this predictor by adding social context features that we extract from C(RL).
In the last step, which is the focus of this paper, we propose a novel semi-supervised technique that makes use of the labeled data {RL, L}, the unlabeled data RU , and the social context information C(R) for both labeled and unlabeled data.
The text of a review provides rich information about its quality.
In this section, we build a baseline supervised predictor that makes use of a variety of textual features as detailed in the top part of Table 1.
We group the features into four different types.
Text-statistics features: This category includes features that are based on aggregate statistics over the text, such as the length of the review, the average length of a sentence, or the richness of the vocabulary.
Syntactic Features: This category includes features that take into account the Part-Of-Speech (POS) tagging of the words in the text.
We collect statistics based on the POS tags to create features such as percentage of nouns, adjectives, punctuations, etc.
Conformity features: This category compares a review r with other reviews by looking at the KL-divergence between the uni-gram language model Tr of the review r for item i, and the uni-gram model T i of an  average  review that contains the text of all (cid:2) reviews for item i.
This feature is used to measure how much the review conforms to the average and is de ned as DKL(Tr||T i) = w Tr(w) log(Tr(w)/T i(w)) where w takes values over the tokens of the unigram models.
Sentiment features:This category considers features that take into account the positive or negative sentiment of words in the review.
The occurrence of such words is a good indication about the strength of the opinion of the reviewer.
With this feature set F , we can now represent each review r as an f-dimensional vector r. Given the labeled data in {RL, L}, we f   R that for a review ri it pre-want to learn a function Q : R dicts a numerical value  qi as its quality.
We formulate the problem as a linear regression problem, where the function Q is de ned as a linear combination of the features in F .
More formally, the function Q is fully de ned by an f-dimensional column weight vector w, such that Q(r) = wT r, where wT denotes the transpose of the vector.
In the following, since Q is uniquely determined the by Total number of tokens.
Total number of sentences.
Ratio of unique words Average sentence length.
Ratio of capitalized sentences.
Ratio of nouns.
Ratio of adjectives.
Ratio of comparatives.
Ratio of verbs.
Ratio of adverbs.
Ratio of foreign words.
Ratio of symbols.
Ratio of numbers.
Ratio of punctuation symbols.
KL div DKL(Tr||T i) Ratio of positive sentiment words.
Ratio of negative sentiment words.
Type Text-Stat Text-Stat Text-Stat Text-Stat Text-Stat Syntactic Syntactic Syntactic Syntactic Syntactic Syntactic Syntactic Syntactic Syntactic Conformity Sentiment Sentiment Feature Name
 NumToken NumSent UniqWordRatio SentLen CapRatio








 KLall PosSEN NegSEN
 ReviewNum AvgRating In-Degree Out-Degree PageRank Author Author SocialNetwork SocialNetwork Out-degree of the author.
SocialNetwork Num.
of past reviews by the author.
Past average rating for the author.
In-degree of the author.
PageRank score of the author.
Table 1: Textual Features and Social Context Features weight vector w and vice versa, we will use Q and w interchangeably.
Our goal is to  nd the f-dimensional weight vector  w that minimizes the objective function:
 n(cid:2) L(wT ri, qi) +  wT w i=1  (w) = (1) where L is the loss function that measures distance of the predicted quality Q(ri) = wT ri of review ri   RL with the true quality value qi, n(cid:2) is the number of training examples, and     0 is regularization parameter for w. In our work, we use squared error loss (or quadratic loss), and we minimize the function n(cid:2)(cid:3) n(cid:2)(cid:3) i=1  1(w) =
 n(cid:2)
 +  wT w (2) (wT ri   qi) n(cid:2)(cid:3) i +  n(cid:2)I) n(cid:2)(cid:3) i=1  1 qiri The closed form solution for  w is given by  1(w) = (  w = arg max rirT where I is the identity matrix of size f.
i=1 w Once we have learned the weight vector w, we can apply it to any review feature vector and predict the quality of unlabeled reviews.
The solution we describe in Section 3 considers each review as a standalone text document.
As we have discussed, in many cases we also have available the social context of the reviews, that is, additional information about the authors of the reviews, and their social network.
In this section we discuss different ways of incorporating social context into the quality predictor we described in Section 3.
Our work is based on the following two premises:
 viewer.
Estimating the quality of the reviewer can help in estimating the quality of the review.
peers in the social network.
We can obtain information about the quality of the reviewers using information from the quality of their friends in their social network.
We investigate two different ways of incorporating the social context information into the linear quality predictor.
The  rst is a straightforward expansion of the feature space to include features extracted from the social context.
The second approach is novel in that it de nes constraints between reviews, and between reviewers, and adds regularizers to the linear regression formulation to enforce these constraints.
We describe these two approaches in detail in the following sections.
A straightforward use of the social context information is by extracting additional features for the quality predictor function.
The social context features we consider are shown in the bottom part of Table 1.
The features capture the engagement of the author (ReviewNum), the historical quality of the reviewer (AvgRating), and the status of the author in the social network (In/Out-Degree, PageRank).
This approach of using social context is simple and it  ts directly into our existing linear regression formulation.
We can still use Equation 2 for optimizing the function Q, which is now de ned over the expanded feature set F .
The disadvantage is that such information is not always available for all reviews.
Consider for example, a review written anonymously, or a review by a new user with no history or social network information.
Predicting using social network features is no longer applicable.
Furthermore, as the dimension of features increases, the necessary amount of labeled training data to learn a good prediction function also increases.
We now present a novel alternative use of the social context that does not rely on explicit features, but instead de nes a set of constraints for the text-based predictor.
These constraints de ne hypotheses about how reviewers behave individually or within the social network.
We require that the quality predictor respects these constraints, forcing our objective function to take into account relationships between reviews, and between different reviewers.
Social Context Hypotheses We now describe our hypotheses, and how these hypotheses can be used in enhancing the prediction of the review quality.
In Section 5 we validate them experimentally on real-world data, and we demonstrate that they hold for all the three data sets we consider.
Author Consistency Hypothesis: The hypothesis is that reviews from the same author will be of similar quality.
A reviewer that writes high quality reviews is likely to continue writing good reviews, while a reviewer with poor reviews is likely to continue writing poor reviews.
Trust Consistency Hypothesis: We make the assumption that a link from a user u1 to a user u2 is an explicit or implicit statement of trust.
The hypothesis is that the reviewers trust other reviewers in a rational way.
In this case, reviewer u1 trusts reviewer u2 only if the quality of reviewer u2 is at least as high as that of reviewer u1.
Intuitively, we claim that it does not make sense for users in the social network to trust someone with quality lower than themselves.
Co-Citation Consistency Hypothesis: The hypothesis is that people are consistent in how they trust other people.
So if two reviewers u1, and u2 are trusted by the same third reviewer u3, then their quality should be similar.
pothesis.
Formally, for a reviewer u, let hu be the n-dimensional normalized indicator vector where hu(i) = 1/|Ru| if user u has written review ri, and zero otherwise.
Then we have that  Q(u) = wT Rhu.
We can thus write the objective function as (wT ri   qi) (cid:10) n(cid:2)(cid:3) (cid:3) +  wT w  3(w) = (cid:11)(cid:5)2
 n(cid:2) (cid:4) (6) i=1

 +   Suv max u,v U where S is the social network matrix.
The optimization problem is still convex, but due to the max function, no nice closed form solution exists.
We can still solve it and  nd the global optimum by gradient descent, where the gradient of the objective function is n(cid:2)(cid:3) i=1 rirT i w   1 (cid:3) n(cid:2) n(cid:2)(cid:3) i=1 riqi +  w SuvR(hu   hv)(hu   hv) T RT w  3(w) 2 w =
 n(cid:2) +   u,v, wT R(hu hv )>0 Let H = [h1, ..., hm] be an n m matrix de ned over all reviewers and Z be a new matrix such that diag(wT RH)S   S diag(wT RH) if > 0 uv (cid:15) (cid:16)     Suv
 Zuv = otherwise (4) Now we can rewrite the gradient as  3(w) 2 w =
 n(cid:2) rirT i w   1 n(cid:2) n(cid:2)(cid:3) i=1 n(cid:2)(cid:3) i=1 riqi +  w Link Consistency Hypothesis: The hypothesis is that if two people are connected in the social network (u1 trusts u2, or u2 trusts u1, or both), then their quality should be similar.
The intuition is that two users that are linked to each other in some way, are more likely to share similar characteristics than two random users.
This is the weakest of the four hypotheses but we observed that it is still useful in practice.
We now describe how we enforce the hypotheses de ned above by designing regularizing constraints to add into the text-based linear regression de ned in Section 3.
Author Consistency: We enforce this hypothesis by adding a regularization term into the regression model where we require that the quality of reviews from the same author is similar.
Let Ru denote the set of reviews authored by reviewer u, including both labeled and unlabeled reviews.
Then the objective function becomes:  2(Q) =  1(Q) +   (Q(ri)   Q(rj))
 (3) (cid:3) (cid:3) u U ri,rj Ru Minimizing the regularization constraint will force reviews of the same author u to receive similar quality values.
We can formulate this as a graph regularization.
The graph adjacency matrix A is de ned as Aij = 1 if review ri and review rj are authored by the same reviewer, and zero otherwise.
Then, Equation 3 becomes:  2(w) = (cid:4) n(cid:2)(cid:3) (cid:3) i=1
 n(cid:2) +   Aij i<j (cid:5)2 wT ri   qi (cid:4) wT ri   wT rj +  wT w (cid:5)2 Let R = [r1, ..., rn] be an f   n feature-review matrix de ned over all reviews (both labeled and unlabeled).
Then the last regularization constraint of Equation 4 can be written as (cid:4) Aij (cid:3) i<j (cid:5)2 wT ri   wT rj (cid:2) = wT R ART w  A = DA   A is the graph Laplacian, and DA is a diagonal matrix with DAii = j Aij.
The new optimization problem is still convex with the closed form solution [21]: i +  n(cid:2)I +  n(cid:2)R ART n(cid:2)(cid:3) n(cid:2)(cid:3)  w = ( rirT qiri  1 ) i=1 i=1 Trust Consistency: Let u be a reviewer.
Given a review quality predictor function Q, we de ne the reviewer quality  Q(u) as the average quality of all the reviews authored by this reviewer as it is estimated by our quality predictor.
That is, (cid:2) (cid:2)  Q(u) = Q(r) r Ru |Ru| = wT ri r Ru |Ru| (5) We enforce the trust consistency hypothesis by adding a regularization constraint to Equation 2.
Let Nu denote the set of reviewers that are linked to by reviewer u.
We have (cid:3) (cid:3) (cid:6) (cid:7) (cid:8)(cid:9)2
  3(Q) =  1(Q) +   max u1 u2 Nu1 The regularization term is greater than zero for each pair of reviewers u1 and u2 where u1 trusts u2, but the estimated quality of u1 is greater than that of u2.
Minimizing function  3 will push such cases closer to zero, forcing the quality of a reviewer u1 to be no +  RH ZHT RT w where  Z = DZ + DZT   Z   ZT can be thought of the graph Laplacian generalized for directed graphs with DZ and DZT the diagonal matrices of the row, and column sums of Z respectively.
Co-Citation Consistency: We enforce this hypothesis by adding a regularization term into the regression model, where we require that the quality of reviews authored by two co-cited reviewers is similar.
Then, the objective function (Equation 2) becomes:  Q(x)    Q(y)  4(Q) =  1(Q) +   (cid:3) (cid:3) (cid:9)2 (cid:6) u U x,y Nu Minimizing function  4 will cause the quality difference of reviewers x and y to be pushed closer to zero, making them more similar.
We can again formulate these constraints as a graph regularizaton.
Let C be the co-citation graph adjacency matrix, where Cij = 1 if two reviewers ui and uj are both trusted by at least one other reviewer u.
Using the same de nition of matrix R and vector hu as for trust consistency, the objective function now becomes (cid:5)2 (cid:4) n(cid:2)(cid:3) (cid:3) i=1
 n(cid:2) wT ri   qi (cid:4)  4(w) = +   Cij i<j wT Rhi   wT Rhj (7) (cid:17) n(cid:2)(cid:3) Let  C be the Laplacian of graph C. The closed form solution is  w = i +  n(cid:2)I +  n(cid:2)RH CHT RT rirT riqi i=1 i=1 +  wT w (cid:5)2 (cid:18) 1 n(cid:2)(cid:3) similar to the one for the co-citation consistency.
We treat the trust network as an undirected graph.
Let B be the corresponding matrix, where Bij = 1 if Sij = 1 or Sji = 1.
Our objective function now becomes (cid:5)2 (cid:4) n(cid:2)(cid:3) (cid:3) i=1
 n(cid:2) wT ri   qi (cid:4) +   Bij i<j wT Rhi   wT Rhj +  wT w (cid:5)2 (cid:18) 1 n(cid:2)(cid:3) (8) riqi  5(w) = (cid:17) n(cid:2)(cid:3) with a similar closed form solution  w = i +  n(cid:2)I +  n(cid:2)RH BHT RT rirT i=1 i=1 In all these cases,   is a weight on the added regularization term which de nes a trade-off between the mean squared error loss and the regularization constraint in the  nal objective function.
Adding the regularization makes our problem a semi-supervised learning problem.
That is, our algorithms operate on both the labeled and the unlabeled data.
Although, only the labels of the labeled data are known to the algorithm, the unlabeled data are also used for optimizing the regularized regression functions.
This gives considerable more  exibility to the algorithm, since it is able to operate even with little labeled data by making use of the unlabeled data and the constraints de ned by the social context.
Furthermore, through regularization the signal from the social context is incorporated into the textual features.
The resulting predictor function operates only on textual features, so it can be applied even in the case where there is no social context.
In this section, we present the experimental evaluation of our techniques.
For our experiments we use product reviews obtained from a real online commerce portal.
We begin by describing the characteristics and preprocessing of our data sets.
Then, we test the hypotheses we proposed in Section 4.2.2 on these real-world datasets.
Finally, we evaluate the prediction performance of different methods and conduct some analysis.
Our experiments employ the data from Ciao UK1, a community review web site.
In Ciao, people not only write critical reviews for all kinds of products and services, but also rate the reviews written by others.
Furthermore, people can add members to their network of trusted members or  Circle of Trust , if they  nd their reviews consistently interesting and helpful.
We collected reviews, reviewers, and ratings up to May, 2009 for all products in three categories: Cellphones, Beauty, and Digital Cameras (DC).
We use the average rating of the reviews (a real value between 0 and 5) as our gold standard of review quality.
In order for the gold standard to be robust and resistant to outlier raters, we use only reviews with at least  ve ratings from different raters.
We then apply some further pruning by imposing the conditions shown in the top part of Table 2.
The purpose of the pruning is to obtain a dataset that is both large enough and has suf cient social context information.
Because we need some information about reviewers  history in order to test our Reviewer Consistency hypothesis, we require reviewers for Cellphone and Beauty to have at least two reviews each.
We also require reviewers to be part of the 1http://www.ciao.co.uk/ Cellphone Beauty Digital Camera
 min # of ratings/ review min # of reviews/reviewer min # of trust links/reviewer min # of reviews/ product
 # of reviews # of reviewers # of products # of links in Trust # of links in Link # of links in Cocitation Trust graph density Link graph density Cociation graph density Avg # of reviews/reviewer Ratio of Reciprocal links Clustering coef cient
 Social Context Quality Distribution















































 rich balanced rich skewed sparse balanced Table 2: Data Pruning Settings, Statistics, and Characteristics y t i s n e
 Cellphone Beauty Digital Camera





 Review Quality Figure 1: Density Estimate of Gold Standard Review Quality.
trust social network (with at least one link in the social network), in order to test our hypotheses and methods based on social networks.
Finally, we require for each product to have some representation in the dataset, that is, a suf ciently large number of reviews.
The pruning thresholds are selected per category, so as to obtain suf -cient volume of data.
For the Digital Cameras category, this results in a minimum amount of pruning.
Although DC reviews do not contain much social context information, we still include them here for comparison and generality purposes.
From the statistics in Table 2, we can see that Cellphone and Beauty reviews contain more rich social context information than DC reviews in the sense that the average number of reviews per reviewer is more than twice that for Digital Cameras, and the link density (de ned as D = |V |(|V | 1) for a graph with vertices V and edges E) is more than 10 times that of Digital Cameras.
We also plot the Kernel-smoothing density estimate (pdf) of the samples qi (the gold standard review quality) in Figure 1.
The distributions of qi for the three categories are quite different.
Beauty reviews are highly concentrated at rating 4, while Cellphone and DC reviews have a more balanced distribution of quality.
We summarize the characteristics of the three data sets in the bottom of Table 2.
Rel:DifferentReviewer Rel:SameReviewer p-value Cellphone

 Beauty

 Digital Camera







 Table 3: Statistics of Review Quality Difference to Support Reviewer Consistency Hypothesis
 Before evaluating the prediction performance of different algorithms, we  rst validate our four consistency hypotheses over our data sets.
For each dataset, we consider all n2 pairs of reviews (ri, rj), and we divide them into two disjoint groups: Rel:DifferentReviewer (cid:9)= uj, if ri and rj are authored by different reviewers, i.e., ui and Rel:SameReviewer if ui = uj.
In each group, for each pair (ri, rj) we compute the difference in quality, dqij = qi   qj, of the two reviews.
Since for each value dqij we also include value dqji =  dqij the mean value of dqij for both groups is zero.
We are interested in the standard deviation, std(dqij), that captures how much variability there is in the difference of quality between reviews for the two groups.
Table 3 shows the results for the different datasets.
For a visual comparison, in Figure 2 we also plot the Kernel-smoothing density estimates of the two groups.
We observe that the standard deviation of the quality difference of two reviews by the same author is much lower than that of two reviews from different authors.
This indicates that reviewers are, to some extent, consistent in the quality of reviews they write.
The  gures also clearly indicate that the density curve for Rel:SameReviewer is more concentrated around zero than Rel:DifferentReviewer for all three categories.
Moreover, two-sam-ple Kolmogorov-Smirnov (KS) test of the samples in the two groups indicates that the difference of the two groups is statistically significant.
The p-values are shown in the last row of Table 3.
The star next to the p-value means there is strong evidence (p < 0.01) that the two samples come from different distributions.
Social Network Consistency Hypotheses In order to test the three social network consistency hypotheses, namely Trust Consistency, Co-Citation Consistency and Link Con-(ui)  sistency, we look at the empirical distribution of d  Q 
 (uj), i.e., the difference in quality of two reviewers, where, similar to Equation 5 ij =  Q  (cid:2)
 (u) = ri Ru |Ru| qi (9) is de ned as the average quality of the reviews written by u in our dataset, but using gold standard quality.
Again, we group the pairs of reviewers (ui, uj) into the the following sets depending on the relationship between the two reviewers.
Rel:None: User ui is not linked to user uj, i.e., Bij = 0.
Rel:Trust: User ui trusts user uj, i.e., Sij = 1.
Rel:Cocitation: Users ui and uj are trusted by at least one other reviewer u3, i.e., Cij = 1.
Rel:Link: User ui trusts user uj, or uj trusts ui, i.e., Bij = 1.
In Figure 3, we plot the Kernel-smoothing density estimate of the d  Q  ij values for the four different sets of pairs, for the three categories.
We further show in Table 4 the moments (mean and Cellphone p-value Rel:None Rel:Trust Rel:Link Moments Mean Variance Beauty p-value Rel:None Rel:Trust Rel:Link Moments Mean Variance Digital Camera p-value Rel:None Rel:Trust Rel:Link Moments Mean Variance Rel:None ---Rel:None

 Rel:None ---Rel:None

 Rel:None ---Rel:None

 Rel:Trust

 --Rel:Trust -0.1376
 Rel:Trust

 --Rel:Trust -0.0824
 Rel:Trust

 --Rel:Trust -0.1481
 Rel:Link



 -Rel:Link

 Rel:Link



 -Rel:Link

 Rel:Link



 -Rel:Link

 Rel:Cocitation




 Rel:Cocitation

 Rel:Cocitation




 Rel:Cocitation

 Rel:Cocitation




 Rel:Cocitation

 Table 4: Statistics of Reviewer Quality Difference to Support Social Network Consistency Hypotheses.
variance) of the four density estimates and p-values of the KS-test between pairs of density estimates.
(ui)    Q  The  rst observation is that the distribution of Rel:Trust is skewed towards the negative with a negative mean.
This supports the Trust Consistency Hypothesis that when ui trusts uj, the quality of ui is usually lower than that of uj, i.e.,  Q  (uj) < 0.
The remaining three distributions are all symmetric with mean zero.
However, Rel:Cocitation and Rel:Link have a much more concentrated peak around zero, i.e., smaller variance, compared with Rel:None.
This supports the Co-Citation and Link Consistency Hypotheses that reviewers are more similar in quality (quality difference closer to zero) if they are co-trusted by others, or linked in a trust graph regardless of direction.
In the results of the KS-test, we have only one high p-value, for Rel:Link and Rel:Cocitation, while all the other pairs have p-values close to zero.
This implies that Rel:Trust, Rel:Cocitation, or Rel:Link do not come from the same distribution as Rel:None.
This observation directly connects the quality of reviewers with their relations in the social network.
The correlation between Rel:Link and Rel:Cocitation could potentially be explained by the relatively high reciprocity ratio (the percentage of links in the Trust social network that are reciprocal), and the relatively high clustering co-ef cient [14] which measures the tendency of triples to form triangles.
In summary, our experiments indicate that there exists correlation between review quality, reviewer quality, and social context.
For all the three data sets considered, the statistics support our hypotheses for designing the regularizers.
For all three datasets (Cellphones, Beauty, and Digital Cameras), we randomly split the data into training and testing sets: 50% of the products for training (Rtrain), and 50% for testing (Rtest).
We keep the test data  xed, while sub-sampling from the training data to generate training sets of different sizes (10%, 25%, 50% or 100% of the training data).
Our goal is to study the effect of different amount of training data on the prediction performance.
We draw 10 independent random splits, and we report test set mean and stan- dard deviation for our evaluation metrics.
A polynomial kernel is t i s n e
  2 y t i s n e
  2 Rel:DifferentReviewer Rel:SameReviewer Rel:DifferentReviewer Rel:SameReviewer Rel:DifferentReviewer Rel:SameReviewer y t i s n e
 y t i s n e

  1
 Review Quality Difference (a) Cellphone
  2  1
 Review Quality Difference
 (b) Beauty
  2
  1
 Review Quality Difference (c) Digital Cameras Figure 2: Density Estimates of Review Quality Difference.
Rel:None Rel:Trust Rel:Link Rel:Cocitation Rel:None Rel:Trust Rel:Link Rel:Cocitation Rel:None Rel:Trust Rel:Link Rel:Cocitation y t i s n e

  1 y t i s n e

  2  0.5 Difference in Reviewer Quality

  1

 Difference in Reviewer Quality (a) Cellphone (b) Beauty Figure 3: Density Estimates of Reviewer Quality Difference.
 1

 Difference in Reviewer Quality (c) Digital Cameras

 used to enrich the feature representation for the linear model.
We  x the parameter   of Linear Regression to the value that gives the best performance for the text-based baseline.
Then, we report the best prediction performance by tuning the regularization weight  .
We will discuss the parameter sensitivity in Section 5.3.3, while leaving the automatic optimization of parameters as future work.
We evaluate the effectiveness of different prediction methods using Mean Squared Error (MSE) over the test set Rtest of size nt, nt(cid:3) i=1 M SE(Rtest) =
 nt (Q(ri)   qi)
 MSE measures how much our predicted quality deviates from the true quality.
A smaller value indicates a more accurate prediction.
Since the graph statistics in Section 5.2 support our design of regularizers, we will examine a few text-free baselines (TBL) that are based solely on social context.
These baselines also serve as a sanity check for the experiments we report in the following section.
For the following, r denotes a test review written by reviewer ur, and  Q  (u) is the quality of reviewer u as de ned in Equation 9, when computed over the training data.
If reviewer u has no reviews in the training data,  Q  (u) is unde ned.
We consider the following baselines for predicting the quality of r.
TBL:Mean: Simply predict as the mean review quality in the training data Rtrain, i.e., Q(r) = 1 i=1 qi.
nt TBL:Reviewer: Predict as the quality  Q  the training data.
If it is not de ned, predict as TBL:Mean.
TBL:Link: Predict as the mean quality of all the reviewers connected to ur in the link graph; if no such reviewer exists in the training set, or the value is unde ned simply predict as TBL:Mean.
(ur) of the author ur in (cid:2)nt TBL:CoCitation: Similar to TBL:Link, predict as the mean quality of all reviewers connected to ur in the Co-Citation graph.
If this is not de ned predict as TBL:Mean.
We compare the four simple text-free baselines against BL:Text: the Linear Regression baseline that uses only text information.
Figure 4 shows the MSE with standard deviation where the x-axis corresponds to the different percentages of the training data we used.
We observe that none of the text-free baselines works as well as Linear Regression with textual features, suggesting that social context by itself cannot accurately predict the quality of a review.
The MSE of the text-free baselines is lower for the Beauty category, where quality distribution is highly skewed at 4, but the text-based predictor is still signi cantly better.
Out of the three social-context based baselines, TBL:Reviewer appears to provide more accurate prediction than the other two when there is rich social context (Cellphones and Beauty), but it offers marginal improvements over TBL:Mean in the case where the social context is sparse (Digital Cameras).
TBL:CoCitation consistently outperforms TBL:Link, which is in line with our observation in Table 4 that the variance of Rel:Cocitation is smaller than that of Rel:Link.
Incorporating Social Context We now compare the different techniques for review quality prediction that make use of text and social context of reviews.
We consider the following methods.
BL:Text: Linear Regression described in Section 3 (Equation 2) using only textual features.
BL:Text+Rvr: Linear Regression described in Section 4.1 using both textual, and social context features.
REG:Reviewer: Linear Regression with a regularizer under Reviewer Consistency Hypothesis (Equation 4).
d e r a u q
 n a e





 d e r a u q
 n a e






 d e r a u q
 n a e






 Percentage of Training Data
 r o r r TBL:Mean TBL:Reviewer TBL:Link TBL:Cocitation BL:Text r o r r TBL:Mean TBL:Reviewer TBL:Link TBL:Cocitation BL:Text
 r o r r TBL:Mean TBL:Reviewer TBL:Link TBL:Cocitation BL:Text



 Percentage of Training Data (a) Cellphone (b) Beauty



 Percentage of Training Data (c) Digital Cameras Figure 4: MSE of Simple Text-free Baselines V.S.
Text-only Baseline.
Cellphone BL:Text BL:Text+Rvr REG:Link REG:CoCitation REG:Trust REG:Reviewer Beauty BL:Text BL:Text+Rvr REG:Link REG:CoCitation REG:Trust REG:Reviewer Digital Camera BL:Text BL:Text+Rvr REG:Link REG:CoCitation REG:Trust REG:Reviewer











































































 Table 5: MSE of Using Social Context as Features and as Regularization vs. Text-based Baseline REG:Link: Linear Regression with a regularizer under Link Consistency Hypothesis (Equation 8).
REG:Cocitation: Linear Regression with a regularizer under Co-ciation Consistency Hypothesis (Equation 7).
REG:Trust: Linear Regression with a regularizer under Trust Consistency Hypothesis (Equation 6) It is possible to consider combinations of the different regular-izers.
This would introduce multiple   parameters (one for each regularizer), and careful tuning is required to make the technique work.
We defer the exploration of this idea to future work.
The results of our experiments are summarized in Table 5 where we show the mean MSE and the standard deviation for all techniques, over all categories, for different training data sizes.
In the parentheses we have the percentage of reduction over MSE of the text-based baseline BL:Text.
The best result (largest decrease of MSE) for each data set and each training size is emphasized in bold.
The  rst observation is that adding social context as additional features BL:Text+Rvr can improve signi cantly over the text-only baseline when there is suf cient amount of training data.
The more training data available, the better the performance.
BL:Text+Rvr gives the best improvement for training percentage of 50% and
 amounts of training data.
On the other hand, when there is little training data, the social context features are too sparse to be helpful, and it may be the case that the MSE actually increases, e.g., when training with 10% and 25% of the training data for Cellphone, and training with 10% for Digital Cameras.
There are techniques for dealing with sparse data, however, exploring such techniques is beyond the scope of this paper.
Using social context as regularization (method names starting with REG) consistently improves over the text-only baseline.
The advantage of the regularization methods is most signi cant when the training size is small, e.g.
using training percentage of 10% and
 we have limited resources for obtaining labeled training data, while there are large amounts of unlabeled data available.
Among the different regularization techniques, for both Cellphone and Beauty reviews, where there is relatively rich social context information, REG:Reviewer appears to be the most effective.
For the Cellphone dataset, REG:Reviewer outperforms BL:Text+Rvr even with 50% of training data, indicating that social context regularization can be helpful when we have rich social context and balanced data.
Among the regularization methods using the social network, REG:Trust, which is based on the most reasonable hypothesis, performs best in practice.
This means that the direction of the trust social network carries more useful information than the simpli ed undirected link graphs and co-citation graphs.
is very sparse there is still some improvement observed using regularization when the training data is small, but the improvement is not as signi cant as on the other two categories where the social context is richer; that is exactly what we expected.
In addition to the experiments on our test data, we are interested in testing our algorithms on data for which we have no social context information.
Our premise is that using regularization can help to incorporate signals from the social network to the text-based predictor, thus improving accuracy prediction even if social context is not available.
We now validate this premise.
We use the Cellphone dataset, and we consider the case where we train on 10% of the training data.
Within the test data of Cellphone, there is a subset of data (144 reviews on average across splits) that has no social context information, i.e., the author has only one review, and is not in the social network.2 Regularization methods only adjust weights on textual features and are thus applicable to those anonymous reviews too, even though these reviews do not contribute to the added regularization terms.
In Table 6, we report the percentage of improvement of four regularization methods over BL:Text.
We still observe some improvement on anonymous reviews with no social context, although as expected less than on reviews with social context.
This indicates the the generalizablity of the proposed regularization methods.
To further support the generalizablity claim, we try an extra set of experiments testing our regularization methods on a held-out set of reviews which are not used in the optmization process and for which we use only the textual features and hide their social context.
More speci cally, after learning a quality prediction function Q using 10% of the training data, we apply it to the remaining 90% of the training data, by multiplying the learned weight vector w with the text feature vectors of the held-out reviews.
From the last row in Table 6, we can clearly see that compared with the text-only baseline, all regularization methods can learn a better weight vector w that captures more accurately the importance of textual features for predicting the true quality on the held-out set.
In summary, we make the following observations.
  Adding social context as features is effective only when there is enough training data to learn the importance of those additional features.
  On the other hand, regularization methods work best when there is little training data by exploiting the constraints de- ned by the social context and the large amount of unlabeled data.
  Since regularization techniques incorporate the social context information into the text-based predictor, they provide improvements even when applied to data without any social context.
Regularization methods have one parameter   to set: the trade-off weight for the regularization term.
The value of the regularization weight de nes our con dence in the regularizer: a higher value results in a higher penalty when violating the corresponding regularization hypothesis.
In the objective functions (Equations 4, 6, 7, and 8), the contribution from the regularization term depends on   as well as the number of nonzero edges in the regularization graph.
least two reviews and a link in the social network, due to multiple consecutive pruning conditions some reviewers end up with only one review and no links in the  nal pruned subset.
r o r r
 d e r a u q
 n a e






 BL:Text REG:Link REG:Cocitation REG:Trust REG:Reviewer BL:Text REG:Link REG:Cocitation REG:Trust REG:Reviewer
 r o r r

 d e r a u q
 n a e







 Sum of Regularization Weight (a) Cellphone






 Sum of Regularization Weight (b) Beauty Figure 5: Parameter Sensitivity.
(cid:2) We de ne the sum of regularization weight as   =   ij Mij, where M can be the coauthor matrix A, the directed trust matrix S, the co-citation matrix C, or the undirected link matrix B.
Figure 5 shows how the prediction performance of regularization methods varies as we use different values of  .
We only show the parameter sensitivity for Cellphone and Beauty reviews where the social context is relatively rich.
The training data size is  xed to be 10%.
As we can see, even though Cellphone and Beauty reviews carry different characteristics, the curves follow a very similar trend: as long as we set     0.1, all regularization methods achieve consistently better performance than the baseline.
As   goes to zero, the performance converges to the text-based baseline.
In addition, the shape of the performance curve depends on the corresponding hypothesis.
For example, the optimum   for REG:Trust is larger than that of REG:Link and REG:Cociation.
Also, even with a value of   higher than the optimum, the error of the REG:Reviewer does not increase as quickly as for the other methods.
These observations are in line with the previous observations that the history of the reviewer (REG:Reviewer) and the Trust graph (REG:Trust) provide a better signal than the Co-Citation graph, or the Link graph.
The problem of assessing the quality of user-generated content has recently attracted increasing attention.
Most previous work [17, 10, 11, 6, 12, 15] has typically focused on automatically deter mining the quality (or helpfulness, or utility) of reviews by using textual features.
The problem of determining review quality is formulated as a classi cation or regression problem with users  votes serving as the ground-truth.
In this context, Zhang and Varadarajan [17] found that shallow syntactic features from the text of reviews are most useful, while review length seems weakly correlated with review quality.
In addition to textual features, Kim et al. [10] included metadata features including ratings given to an item under review and concluded that review length and the number of stars in product rating are most helpful within their SVM regression model.
Ghose and Ipeirotis [6] combined econometric models with textual subjectivity analysis and demonstrated evidence that extreme reviews are considered to be most helpful.
In [12], the authors incorporated reviewers  expertise and review timeliness in addition to the writing style of the review in a nonlinear regression model.
In our work, we extend previous work by using author and social network information in order to assess review quality.
Although user votes can be helpful as ground-truth data, Liu et al [11] identi ed a discrepancy between votes coming from Amazon.
com and votes coming from an independent study.
More speci -cally, they identi ed a  rich-get-richer  effect, where reviews accumulate votes more quickly depending on the number of votes they already have.
This observation further enhances our motivation to All Reviews with no social context Reviews with social context Held-out reviews with hidden social context # of Reviews REG:Link REG:CoCitation REG:Trust REG:Reviewer



















 Table 6: Improvement of Regularization Methods over BL:Text (Cellphone) automatically determine the quality of reviews in order to avoid such biases.
Danescu-Niculescu-Mizil et al. [5] showed that the perceived helpfulness of a review depends not only on its content but also on the other reviews of the same product.
We include one of their hypotheses, i.e. conformity hypothesis, as a feature into our model.
A recent paper [15] took an unsupervised approach to  nding the most helpful book reviews.
Although their method is shown to outperform users  votes, it is evaluated on only 12 books and thus is not clear whether it is robust and generalizable.
The problem of assessing the quality of user-generated data is also critical in domains other than reviews.
For example, previous works [2, 4] focused on assessing the quality of postings within the community question/answering domain.
The work in [2] combines textual features with user and community meta-data features for assessing the quality of questions and answers.
In [4], the authors propose a co-training idea that jointly models the quality of the author and the review.
However, their work does not model user relationships, bur rather uses all community information for exacting features.
Regularization using graphs has appeared as a type of effective method in the semi-supervised learning literature [19].
The interested reader may examine [18, 20, 3].
The resulting formulation is usually a well-formed convex optimization problem which has a unique and ef ciently computable solution.
These types of graph regularization methods have been successfully applied in Webpage categorization [16] and Web spam detection [1].
In both cases, the link structure among Web pages is nicely exploited by the regularization which, in most cases, has improved the predictive accuracy within the problem at hand.
Recently, Mei et al. [13] propose to enhance topic models by regularizing on a contextual graph structure.
In our scenario, the social network of the reviewers de nes the context, and we exploit it to enhance review quality prediction.
In this paper we studied the problem of automatically determining review quality using social context information.
We studied two methods for incorporating social context in the quality prediction: either as features, or as regularization constraints, based on a set of hypotheses that we validated experimentally.
We have demonstrated that prediction accuracy of a text-based classi er can greatly improve, when working with little training data, by using regularization on social context.
Importantly, our regularization techniques make the general approach applicable even when social context information is unavailable.
The method we propose is quite generalizable and applicable for quality (or attribute) estimation of other types of user-generated content.
This is a direction that we intend to explore further.
As further future work, social context can be enhanced with additional information about items and authors.
Information about product attributes, for example, enables estimates of similarity between products, or categories of products which can be exploited as additional constraints.
Furthermore, although a portal may lack an explicit trust network, we plan to construct an implicit network using the ratings reviewers attach to each others  reviews and then apply our techniques to this case.
Finally, rather than predicting the quality of each review, it would be interesting to adapt our techniques for computing a ranking of a set of reviews.
