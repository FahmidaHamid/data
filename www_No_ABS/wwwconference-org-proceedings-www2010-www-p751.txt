With the explosion of Web 2.0 services, more and more user-generated sentiment data have been shared on the Web.
They exist in the form of user reviews on shopping or opinion sites, in posts of blogs or customer feedback.
As a result, opinion mining has attracted much attention recently [29, 24], for example, opinion summarization [19, 26], opinion integration [25] and review spam identi cation [21], etc.
Sentiment classi cation, which aims at classifying sentiment data into polarity categories (e.g., positive or negative), is widely studied because many users do not explicitly indicate their sentiment polarity thus we need to predict it from the text data generated by users.
In literature, supervised learning algorithms [30] have been proved promising and widely used in sentiment classi cation.
However, the performance of these methods relies on manually labeled training data.
In some cases, the labeling work may be time-consuming and expensive in order to build accurate sentiment classi ers.
Furthermore, these approaches are domain dependent.
The reason is that users may use domain-speci c words to express sentiment in different domains.
Table 1 shows several user review sentences from two domains: electronics and video games.
In the electronics domain, we may use words like  compact ,  sharp  to express our positive sentiment and use  blurry  to express our negative sentiment.
While in the video game domain, words like  hooked ,  realistic  indicate positive opinion and the word  boring  indicates negative opinion.
Due to the mismatch between domain-speci c words, a sentiment classi er trained in one domain may not work well when directly applied to other domains.
Thus cross-domain sentiment classi cation algorithms are highly desirable to reduce domain dependency and manually labeling cost.
In this paper, we target at  nding an effective approach for the cross-domain sentiment classi cation problem.
Assume we have a set of labeled data from a source domain, in order to train a clas-si er for a target domain, we leverage some unlabeled data from the target domain to help.
In detail, we propose a spectral feature alignment (SFA) algorithm to  nd a new representation for cross-domain sentiment data, such that the gap between domains can be reduced.
SFA uses some domain-independent words as a bridge to construct a bipartite graph to model the co-occurrence relationship between domain-speci c words and domain-independent words.
The idea is that if two domain-speci c words have connections to more common domain-independent words in the graph, they tend to be aligned together with higher probability.
Similarly, if two domain-independent words have connections to more common domain-speci c words in the graph, they tend to be aligned together with higher probability.
We adapt a spectral clustering algorithm, which is based on the graph spectral theory [9], on the bipartite graph to co-align domain-speci c and domain-independent speci c words, which are much more frequent in one domain than in the other one.
Italic words are some domain-independent words, which occur frequently in both domains.
 +  denotes positive sentiment, and  -  denotes negative sentiment.
electronics + Compact; easy to operate; very good picture quality; looks + sharp!
I purchased this unit from Circuit City and I was very excited about the quality of the picture.
It is really nice and sharp.
It is also quite blurry in very dark settings.
I will never buy HP again.
video games A very good game!
It is action packed and full of excitement.
I am very much hooked on this game.
Very realistic shooting action and good plots.
We played this and were hooked.
The game is so boring.
I am extremely unhappy and will probably never buy UbiSoft again.
words into a set of feature-clusters.
In this way, the clusters can be used to reduce the mismatch between domain-speci c words of both domains.
Finally, we represent all data examples with these clusters and train sentiment classi ers based on the new representation.
Different from state-of-the-art cross-domain sentiment clas-si cation algorithms such as the structural correspondence learning (SCL) algorithm [6], our proposed SFA can fully exploit the relationship between domain-independent and domain-speci c words via co-aligning them on the bipartite graph to learn a more compact and meaningful representation underlying the graph.
Experiments in two real world domains indicate that SFA is indeed promising in obtaining better performance than several baselines including SCL [6] in terms of the accuracy for cross-domain sentiment classi cation.
The rest of the paper is organized as follows.
In the next section, we  rst describe the problem we study and give some de nitions.
Then we present the idea behind our proposed feature alignment approach in Section 3.
The details of our solution are presented in Section 4.
We conduct a series of experiments to evaluate the effectiveness of our proposed solution in Section 5.
Finally, we review some related works in Section 6 and conclude our work in Section 7.
Before giving a formal de nition of the problem we address in this paper, we  rst present some de nitions.
De nition 1 (Domain) A domain D denotes a class of entities in the world or a semantic concept.
For example, different types of products, such as books, dvds and furniture, can be regarded as different domains.
Take research area as another example, computer science, mathematics and physics can be also regarded as different domains.
De nition 2 (Sentiment) Given a speci c domain D, sentiment data are the text documents containing user opinions about entities of the domain.
User sentiment may exist in the form of a sentence, paragraph or article.
In either case, it corresponds with a sequence of words w1w2...wxj , where wi is a word from a vocabulary W .
In this work, we represent user sentiment data with a bag-of-words method, with c(wi, xj) to denote the frequency of word wi in xj, and the word sequential information is ignored.
Without loss of generality, we use a uni ed vocabulary W for all domains and |W| = m. Furthermore, in sentiment classi cation tasks, either single word or NGram can be used as features to represent sentiment data, thus in the rest of this paper, we will use word and feature interchangeably.
De nition 3 (Labeled / Unlabeled Sentiment Data) Given a spe-ci c domain D, the sentiment data xi and a yi denoting the polarity of xi, xi is said to be positive if the overall sentiment expressed in xi is positive (yi = +1), while xi is negative if the overall sentiment expressed in xi is negative (yi =  1).
A pair of sentiment text and its corresponding sentiment polarity {xi, yi} is called the labeled sentiment data.
If xi has no polarity assigned, it is unlabeled sentiment data.
Besides positive and negative sentiment, there are also neutral and mixed sentiment data in practical applications.
Mixed polarity means user sentiment is positive in some aspects but negative in other ones.
Neutral polarity means that there is no sentiment expressed by users.
In this paper, we only focus on positive and negative sentiment data, but it is not hard to extend the proposed solution to address multi-category sentiment classi cation problems.
Based on the de nitions described above, we now de ne the problem we try to address in this paper as follows; Problem De nition (Cross-domain Sentiment Classi cation) Given two speci c domains Dsrc and Dtar, where Dsrc and Dtar are referred to as a source domain and a target domain respectively, suppose we have a set of labeled sentiment data Dsrc = {(xsrci , ysrci )}nsrc i=1 in Dsrc, and some unlabeled sentiment data Dtar = {xtarj}ntar j=1 in Dtar.
The task of cross-domain sentiment classi cation is to learn an accurate classi er to predict the polarity of unseen sentiment data from Dtar.1 In order to solve this problem, we propose a framework which targets to achieve two subtasks: (1) To identify domain-independent features and (2) to align domain-speci c features.
In the  rst sub-task, we aims to learn a feature selection function  DI ( ) to select l domain-independent features, which occur frequently and act similarly across domains Dsrc and Dtar.
These domain-independent features are used as a bridge to make knowledge transfer across domains possible.
After identifying domain-independent features, we can use  DS( ) to denote a feature selection function for selecting domain-speci c features, which can be de ned as the complement of domain-independent features.
In the second subtask, we aims to to learn an alignment function   : R(m l)   Rk to align domain-speci c features from both domains into k prede ned feature clusters z1, z2, ..., zk, s.t.
the difference between domain speci c features from different domains on the new representation constructed by the learned clusters can be dramatically reduced.
For simplicity, we use WDI and WDS to denote the vocabulary of domain-independent and domain-speci c features respectively.
Then sentiment data xi can be divided into two disjoint views.
One view consists of features in WDI, and the other is composed of features in WDS.
We use  DI (xi) and  DS(xi) to denote the two views respectively.
one target domain.
However, our proposed method is quite general and can be easily adapted to solve multi-source domain adaptation problems.
In this section, we use an example to introduce the motivation of our solution to the cross-domain sentiment classi cation problem.
First of all, we assume the sentiment classi er f is a linear function, which can be written as   y = f (x) = sgn(xwT ), where x   R1 m and sgn(xwT ) = +1 if xwT   0, otherwise, sgn(xwT ) =  1.
w is the weight vector of the classi er, which can be learned from a set of training data (pairs of sentiment data and their corresponding polarity labels).
Consider the example shown in Table 1 to illustrate our idea.
We use a standard bag-of-words method to represent sentiment data of the electronics (E) and video games (V) domains.
From Table 2, we can see that the difference between domains is caused by the frequency of the domain-speci c words.
Domain-speci c words in the E domain, such as compact, sharp, blurry, do not occur in the V domain.
On the other hand, domain-speci c words in the V domain, such as hooked, realistic, boring, do not occur in the E domain.
Suppose the E domain is the source domain and the V domain is the target domain, our goal is to train a vector of weights w  with labeled data from the E domain, and use it to predict sentiment polarity for the V domain data.2 Based on the three training sentences in the E domain, the weights of features such as compact and sharp should be positive.
The weight of features such as blurry should be negative and the weights of features such as hooked, realistic and boring can be arbitrary or zeros if a L1 regularizer is applied on w for model training.
However, an ideal weight vector in the V domain should have positive weights for features such as hooked, realistic and a negative weight for the feature boring, while the weights of features such as compact, sharp and blurry may take arbitrary values.
That is why the classi er learned from the E domain may not work well in the V domain.
Table 2: Bag-of-words representations of electronics (E) and video games (V) reviews.
Only domain-speci c features are considered.
 ...  denotes all other words.
compact sharp blurry hooked realistic boring
 + + -+
 -...
...
...
...
...
...
...
In order to reduce the mismatch between features of the source and target domains, a straightforward solution is to make them more similar by adopting a new representation.
Table 3 shows an ideal representation of domain-speci c features.
Here, sharp_hooked denotes a cluster consisting of sharp and hooked, compact_realistic denotes a cluster consisting of compact and realistic, and blurry_boring denotes a cluster consisting of blurry and boring.
We can use these clusters as high-level features to represent domain-speci c words.
Based on the new representation, the weight vector w  trained in the E domain should be also an ideal weight vector in the V domain.
In this way, based on the new representation, the classi er learned from one domain can be easily adapted to another one.
The problem is how to construct such an ideal representation as shown in Table 3.
Clearly, if we directly apply traditional clustering algorithms such as k-means [18] on Table 2, we are not able to
 ignore all other words.
Table 3: Ideal representations of domain-speci c words.
blurry_boring compact_realistic sharp_hooked
 + + -+
 -...
...
...
...
...
...
...
align sharp and hooked into one cluster, since the distance between them is large.
In order to reduce the gap and align domain-speci c words from different domains, we can utilize domain-independent words as a bridge.
As shown in Table 1, words such as sharp, hooked, compact and realistic often co-occur with other words such as good and exciting, while words such as blurry and boring often co-occur with a word never_buy.
Since the words like good, exciting and never_buy occur frequently in both the E and V domains, they can be treated as domain-independent features.
Table 4 shows co-occurrences between domain-independent and domain-speci c words.
It is easy to  nd that, by applying clustering algorithms such as k-means on Table 4, we can get the feature clusters shown in Table 3: sharp_hooked, blurry_boring and compact_realistic.
Table 4: A co-occurrence matrix of domain-speci c and domain-independent words.
realistic compact hooked boring blurry sharp good exciting never_buy

















 So, the co-occurrence relationship between domain-speci c and domain-independent features is useful for feature alignment across different domains.
In this paper, we use a bipartite graph to represent this relationship and then adapt spectral clustering techniques to  nd a new representation for domain-speci c features.
In the following section, we will present spectral domain-speci c feature alignment algorithm in detail.
In this section, we describe our algorithm to adapt spectral clustering techniques to align domain-speci c features from different domains for cross-domain sentiment classi cation.
First of all, we need to identify which features are domain independent.
As mentioned above, domain-independent features should occur frequently and act similarly in both the source and target domains.
In this section, we present several strategies for selecting domain-independent features.
A  rst strategy is to select domain-independent features based on their frequency in both domains.
More speci cally, given the number l of domain-independent features to be selected, we choose features that occur more than k times in both the source and target domains.
k is set to be the largest number such that we can get at least l such features.
A second strategy is based on the mutual dependence between features and labels on the source domain data.
In [6], mutual information is applied on source domain labeled data to select features as  pivots , which can be referred to as domain-independent features in this papers.
In information theory, mutual information is ables.
Feature selection using mutual information can help identify features relevant to source domain labels.
But there is no guarantee that the selected features act similarly in both domains.
Here we propose a third strategy for selecting domain-independent features.
Motivated by the supervised feature selection criteria, we can use mutual information to measure the dependence between features and domains.
If a feature has high mutual information, then it is domain speci c.
Otherwise, it is domain independent.
Furthermore, we require domain-independent features occur frequently.
So, we modify the mutual information criterion between features and domains as follows, (cid:181) (cid:182) I(X i; D) = p(x, d)log2 p(x, d) p(x)p(d) , (1) (cid:88) (cid:88) d D x Xi,x(cid:54)=0 where D is a domain variable and we only sum over nonzero values of a speci c feature X i.
The smaller I(X i; D) is, the more likely that X i can be treated as a domain-independent feature.
(cid:83) Based on the above strategies for selecting domain-independent features, we can identify which features are domain independent and which ones are domain speci c.
Given domain-independent and domain-speci c features, we can construct a bipartite graph VDI , E) between them.
In G, each vertex in VDS
 corresponds to a domain-speci c word in WDS, and each vertex in VDI corresponds to a domain-independent word in WDI.
An edge in E connects two vertexes in VDS and VDI respectively.
Note that there is no intra-set edges linking two vertexes in VDS or VDI.
Furthermore, each edge eij   E is associated with a non-negative weight mij.
The score of mij measures the relationship between word wi   WDS and wj   WDI in Dsrc and Dtar (e.g., the total number of co-occurrence of wi   WDS and wj   WDI in Dsrc and Dtar).
A bipartite graph example is shown in Figure 1, which is constructed based on the example shown in Table 4.
So we can use the constructed bipartite graph to model the intrinsic relationship between domain-speci c and domain-independent features.
Besides using the co-occurrence frequency of words within documents, we can also adopt more meaningful methods to estimate mij.
For example, we can de ne a reasonable  window size .
If a domain-speci c word and a domain-independent word co-occur within the  window size , then there is an edge connecting them.
Furthermore, we can also use the distance between wi and wj to adjust the score of mij.
The smaller is their distance, the larger weight we can assign to the corresponding edge.
In this paper, for simplicity, we set the  window size  to be the maximum length of all documents.
Also we do not consider word position to determine the weights for edges.
We want to show that by constructing a simple bipartite graph and adapting spectral clustering techniques on it, we can algin domain-speci c features effectively.
In the previous section, we have presented how to construct a bipartite graph between domain-speci c and domain-independent features.
In this section, we show how to adapt a spectral clustering algorithm on the feature bipartite graph to align domain-speci c features.
In graph spectral theory [9], there are two main assumptions: (1) if two nodes in a graph are connected to many common nodes, then they should be very similar (or quite related), (2) there is a low-dimensional latent space underlying a complex graph, where two nodes are similar to each other if they are similar in the original graph.
Based on these two assumptions, spectral graph the-Figure 1: A bipartite graph example of domain-speci c and domain-independent features based on Table 4.
ory has been widely applied in many problems, e.g., dimensional-ity reduction and clustering [27, 3, 14].
In our case, we assume (1) if two domain-speci c features are connected to many common domain-independent features, then they tend to be very related and will be aligned to a same cluster with high probability, (2) if two domain-independent features are connected to many common domain-speci c features, then they tend to be very related and will be aligned to a same cluster with high probability, (3) we can  nd a more compact and meaningful representation for domain-speci c features, which can reduce the gap between domains.
Therefore, with the above assumptions, we expect the mismatch problem between domain-speci c features can be alleviated by applying graph spectral techniques on the feature bipartite graph to discover a new representation for domain-speci c features.
Before we present how to adapt a spectral clustering algorithm to align domain-speci c features, we  rst brie y introduce a standard spectral clustering algorithm [27] as follows, Given a set of points V = {v1, v2, ..., vn} and their corresponding weighted graph G, the goal is to cluster the points into k clusters, where k is an input parameter.
mij, if i (cid:54)= j; Aii = 0.
struct the matrix L = D 1/2AD 1/2.3 j Aij, and con-
the matrix U = [u1u2...uk]   Rn k.
(cid:80) (cid:80)
 j U2 ij)1/2.
into k clusters.
Based on the above description, the standard spectral clustering algorithm clusters n points to k discrete indicators, which can be referred to as  discrete clustering .
Ding and He [15] proved that the k principal components of a term-document co-occurrence matrix,
 Laplacian matrix (cid:101)L = I   L, where I is an identity matrix.
The tors.
Thus selecting the k smallest eigenvectors of (cid:101)L in [9, 3] is changes in these forms of Laplacian matrix will only change the eigenvalues (from  i to 1    i) but have no impact on eigenvec-equivalent to selecting the k largest eigenvectors of L in this paper.
compactrealisticsharphookedblurryboringneverbuygoodexciting11111111WWW 2010   Full PaperApril 26-30   Raleigh   NC   USA754(cid:184) (cid:80) which are referred to as the k largest eigenvectors u1, u2, ..., uk in step 3, are actually the continuous solution of the cluster membership indicators of documents in the k-means clustering method.
More speci cally, the k principal components can automatically perform data clustering in the subspace spanned by the k principle components.
This implies that a mapping function constructed from the k principal components can cluster original data and map them to a new space spanned by the clusters simultaneously.
Motivated by this discovery, we show how to adapt the spectral clustering algorithm for cross-domain feature alignment.
Given the feature bipartite graph G, our goal is to learn a feature alignment mapping function  ( ) : Rm l   Rk, where m is the number of all features, l is the number of domain-independent features and m   l is the number of domain-speci c features.
sponds to the co-occurrence relationship between a domain-speci c word wi   WDS and a domain-independent word wj   WDI.
(cid:183)




   Rm m of the bipartite graph, where the  rst m   l rows and columns correspond to the m l domain-speci c features, and the last l rows and columns correspond to the l domain-independent features.
struct the matrix L = D 1/2AD 1/2.
j Aij, and con-
the matrix U = [u1u2...uk]   Rm k.
 (x) = xU[1:m l,:], where U[1:m l,:] denotes the  rst m l rows of U and x   R1 (m l).
Given a feature alignment mapping function  ( ), for a data example xi in either a source domain or target domain, we can  rst apply  DS( ) to identify the view associated with domain-speci c features of xi, and then apply  ( ) to  nd a new representation  ( DS(xi)) of the view of domain-speci c features of xi.
Note that the af nity matrix A constructed in Step 2 is similar to the af nity matrix of a term-document bipartite graph proposed in [14], which is used for spectral co-clustering terms and documents simultaneously.
Though our goal is only to cluster domain-speci c features, it is proved that clustering two related sets of points simultaneously can often get better results than only clustering one single set of points [14].
If we have selected domain-independent features and aligned domain-speci c features perfectly, then we can simply augment domain-independent features with the features learned by the feature alignment algorithm to generate a perfect representation for cross-domain sentiment classi cation.
However, in practice, we may not be able to identify domain-independent features correctly and thus fail to perform feature alignment perfectly.
Similar to the strategy used in [1, 6], we augment all original features with features learned by feature alignment to construct a new representation.
A tradeoff parameter   is used in this feature augmentation to balance the effect of original features and new features.
So, for each data example xi, the new feature representation is de ned as (cid:101)xi = [xi,  ( DS(xi))], where xi   R1 m,(cid:101)xi   R1 m+k and 0       1.
In practice, the value of   can be determined by evaluation on some heldout data.
The whole process of our proposed framework for cross-domain sentiment classi cation is presented in Algorithm 1.
Algorithm 1 Spectral Domain-Speci c Feature Alignment for Cross-Domain Sentiment Classi cation Input: labeled source domain data Dsrc = {(xsrci , ysrci )}nsrc i=1 , unlabeled target domain data Dtar = {xtarj}ntar j=1 , the number of clusters K and the number of domain-independent features m.
Output: adaptive classi er f : X   Y .
to select l domain-independent features.
The remaining m   l features are treated as domain-speci c features.
(cid:184) (cid:183) (cid:183)
  DI (xsrc)  DI (xtar) and  DS =  DS(xsrc)  DS(xtar) occurrence matrix M  R(m l) l.
(cid:183) (cid:184) .
where A =



 (cid:184) .
the matrix U = [u1u2...uK ]   Rm K.
Let mapping  (xi) = xiU[1:m l,:], where xi   Rm l {([xsrci  ( DS(xsrci ))], ysrci )}nsrc
 i=1

 In this section, we will describe our experiments on two real-world datasets and show the effectiveness of our proposed SFA for cross-domain sentiment classi cation.
In this subsection, we  rst describe the datasets used in our experiments.
The  rst dataset is from Blitzer et al. [6].
It contains a collection of product reviews from Amazon.
The reviews are about four product domains: books (B), dvds (D), electronics (E) and kitchen appliances (K).
Each review is assigned a sentiment label,  1 (negative review) or +1 (positive review), based on the rating score given by the review author.
In each domain, there are
 we can construct 12 cross-domain sentiment classi cation tasks: D
 D, E   D, B   K, D   K, E   K, where the word before an arrow corresponds with the source domain and the word after an arrow corresponds with the target domain.
We use RevDat to denote this dataset.
The sentiment classi cation task on this dataset is document-level sentiment classi cation.
The other dataset is collected by us for experiment purpose.
We have crawled a set of reviews from Amazon4, Yelp5 and Citysearch6 websites.
The reviews from Amazon are about three product domains: video game (V), electronics (E) and software (S).
The reviews from Yelp and Citysearch are about the hotel (H) domain.
Instead of assigning each review with a label, we split these reviews into sentences and manually assign a polarity label for each 4http://www.amazon.com/ 5http://www.yelp.com/ 6http://www.citysearch.com/ tences and 1, 500 negative ones for experiment.
Similarly, we also construct 12 cross-domain sentiment classi cation tasks: V   H,
 H   S, H   E, S   V. We use SentDat to denote this dataset.
Sentiment classi cation task on this dataset is sentence-level sentiment classi cation.
For both datasets, we use Unigram and Bigram features to represent each data example (a review in RevDat and a sentence in SentDat).
The summary of the datasets is described in Table 5.
Table 5: Summary of Datasets Used for Evaluation.
Dataset RevDat Domain dvds kitchen SentDat electronics books video game hotel software electronics # Reviews







 # Pos







 # Neg







 # Features


 In order to investigate the effectiveness of our method, we have compared it with several algorithms.
In this subsection, we describe some baseline algorithms with which we compare SFA.
One baseline method denoted by NoTransf, is a classi er trained directly with the source domain training data.
The gold standard (denoted by upperBound) is an in-domain classi er trained with labeled data from the target domain.
For example, for D   B task, NoTransf means that we train a classi er with labeled data of D domain.
upperBound corresponds with a classi er trained with the labeled data from B domain.
So, the performance of upper-Bound in D   B task can be also regarded as an upper bound of E   B and K   B tasks.
Another baseline method denoted by LSA is a classi er trained on a new representation which augments original features with new features which are learned by applying latent semantic analysis (also can be referred to as principal component analysis) [13] on the original view of domain-speci c features (as shown in Table 2).
A third baseline method denoted by FALSA is a classi er trained on a new representation which augments original features with new features which are learned by applying latent semantic analysis on the co-occurrence matrix of domain-independent and domain-speci c features.
We compare our method with LSA and FALSA in order to investigate if spectral feature clustering is effective in aligning domain-speci c features.
We have also compared our algorithm with a method: structural correspondence learning (SCL) proposed in [6].
We follow the details described in Blitzer s thesis [5] to implement SCL with logistic regression to construct auxiliary tasks.
Note that SCL, LSA, FALSA and our proposed SFA all use unlabeled data from the source and target domains to learn a new representation and train classi ers using the labeled source domain data with new representations.
For NoTransf, upperBound, LSA, FALSAand SFA, we use logistic regression as the basic sentiment classi er.
The library implemented in [16] is used in all our experiments.
The tradeoff parameter C in logistic regression [16] is set to be 10, 000, which is equivalent to set   = 0.0001 in [5].
The parameters of each model are tuned on some heldout data in E   B task of RevDat and H   S task of SentDat, and are  xed to be used in all experiments.
We use accuracy to evaluate the sentiment classi cation result: the percentage of correctly classi ed examples over all testing examples.
The de nition of accuracy is given as follows, Accuracy = |{x|x   Dtst   f (x) = y}| |{x|x   Dtst}| , where Dtst denotes the test data, y is the ground truth sentiment polarity and f (x) is the predicted sentiment polarity.
For all experiments on RevDat, we randomly split each domain data into a training set of 1,600 instances and a test set of 400 instances.
For all experiments on SentDat, we randomly split each domain data into a training set of 2,000 instances and a test set of 1,000 instances.
The evaluation of cross-domain sentiment classi cation methods is conducted on the test set in the target domain without labeled training data in the same domain.
We report the average results of 5 random times.
In this section we compare the accuracy of SFA with NoTransf, LSA, FALSA and SCL by 24 tasks on two datasets.
For LSA, FALSAand SFA, we use Eqn.
(1) de ned in Section 4.1 to identity domain-independent and domain-speci c features.
We adopt the following settings: the number of domain-independent features l = 500, the number of domain-speci c features clusters k = 100 and the parameter in feature augmentation   = 0.6.
Studies of the SFA parameters are presented in Section 5.5 and 5.6.
For SCL, we use mutual information to select  pivots .
The number of  pivots  is set to be 500, and the number of dimensionality h in [6] is set to be 50.
All these parameters and domain-independent feature (or  pivot ) selection methods are determined based on results on the heldout data mentioned in the previous section.
Figure 2(a) shows the comparison results of different methods on RevDat.
In the  gure, each group of bars represents a cross-domain sentiment classi cation task.
Each bar in speci c color represents a speci c method.
The horizontal lines are accuracies of upperBound.
From the  gure, we can observe that the four domains of RevDat can be roughly classi ed into two groups: B and D domains are similar to each other, as are K and E, but the two groups are different from each other.
Adapting a classi er from K domain to E domain is much easier than adapting it from B domain.
Clearly, our proposed SFA performs better than other methods including state-of-the-art method SCL in most tasks.
As mentioned in Section 3, clustering domain-speci c features with bag-of-words representation may fail to  nd a meaningful new representation for cross-domain sentiment classi cation.
Thus LSA only outperforms NoTransf slightly in some tasks, but its performance may even drop on other tasks.
It is not surprising to  nd that FALSA gets signi cant improvement compared to NoTransf and LSA.
The reason is that representing domain-speci c features via domain-independent features can reduce the gap between domains and thus  nd a reasonable representation for cross-domain sentiment classi cation.
Our proposed SFA can not only utilize the co-occurrence relationship between domain-independent and domain-speci c features to reduce the gap between domains, but also use graph spectral clustering techniques to co-align both kinds of features to discover meaningful clusters for domain-speci c features.
Though our goal is only to cluster domain-speci c features, it has been proved that clustering two related sets of points simultaneously can often get better results than clustering one single set of points only [14].
From the comparison results on SentDat shown in Figure 2(b), we can get similar conclusion: SFA outperforms other methods (b) Comparison Results on SentDat.
Figure 2: Comparison Results (unit: %) on Two Datasets.
in most tasks.
One interesting observation from the results is that SCL does not work well compared to its performance on RevDat.
One reason may be that in sentence-level sentiment classi cation, the data is quite sparse.
In this case, it is hard to construct a reasonable number of auxiliary tasks that are useful to model the relationship between  pivots  and  non-pivots .
The performance of SCL highly relies on the auxiliary tasks.
Thus in this dataset, SCL even performs worse than FALSA in some tasks.
We do t-test on the comparison results of the two datasets and  nd that SFA outperforms other methods with 0.95 con dence intervals.
In this section, we conduct two experiments to study the effect of domain-independent features on the performance of SFA.
The  rst experiment is to test the effect of domain-independent features identi ed by different methods on the overall performance of SFA.
The second one is to test the effect of different numbers of domain-independent features on SFA performance.
As mentioned in Section 4.1, besides using Eqn.
(1) to identify domain-independent and domain-speci c features, we can also use the other two strategies to identify them.
In Table 6, we summarize the comparison results of SFA using different methods to identify domain-independent features.
We use SFADI, SFAF Q and SFAM I to denote SFA using Eqn.
(1), frequency of features in both domains and mutual information between features and labels in the source domain respectively.
From the table, we can observe that SFADI and SFAF Q achieve comparable results and they are stable in most tasks.
While SFAM I may work very well in some tasks such as K   D and E   B of RevDat, but work very bad in some tasks such as E   D and D   E of RevDat.
The reason is that applying mutual information on source domain data can  nd features that are relevant to the source domain labels but cannot guarantee the selected features to be domain independent.
In addition, the selected features may be irrelevant to the labels of the target domain.
To test the effect of the number of domain-independent features on the performance of SFA, we apply SFA on 12 tasks randomly selected from the two datasets, and  x k = 100,   = 0.6.
The value of l is changed from 300 to 700 with step length 100.
The results are shown in Figure 3(a) and 3(b).
From the  gures, we can  nd that when l is in the range of [400, 700], SFA performs well and stably in most tasks.
Thus SFA is robust with regard to the quality and numbers of domain-independent features.
B >DE >DK >DD >BE >BK >B6570758085Accuracy (%)82.5581.4D >EB >EK >ED >KB >KE >K7075808590Accuracy (%)84.687.1V >ES >EH >EE >VS >VH >V7072.57577.58082.585Accuracy (%)81.3478.72E >SV >SH >SE >HV >HS >H6567.57072.57577.58082.585Accuracy (%)78.2882.7WWW 2010   Full PaperApril 26-30   Raleigh   NC   USA757Table 6: Experiments with Different Domain-Independent Feature Selection Methods.
Numbers in the table are accuracies in percentage.
RevDat




































 SentDat






 Besides the number of domain-independent features l, there are two other parameters in SFA: one of them is the number of domain-speci c feature-clusters k and the other is the tradeoff parameter   in feature augmentation.
In this section, we further test the sensitivity of these two parameters on the overall performance of SFA.
We  rst test the sensitivity of the parameter k. In this experiment, we  x l = 500,   = 0.6 and change the value of k from 50 to 200 with step length 25.
Figure 3(c) and 3(d) show the results of SFA under varying values of k. Note that when the cluster number k falls in the range from 75 to 175, SFA performs well and stably.
Finally, we test the sensitivity of the parameter  .
In this experiment, we  x l = 500, k = 100 and change the values of   from



 Sentiment classi cation aims to predict the sentiment polarity of text data, e.g., text sentences and review articles, etc.
It has drawn much research attention recently.
Many machine learning techniques have been proposed for sentiment classi cation, such as unsupervised learning techniques [32], supervised learning techniques [30], graph-based semi-supervised learning techniques [17, 31], and matrix factorization techniques with lexical prior knowl- edge [23].
However, most sentiment classi ers are domain dependent.
It is challenging to adapt a classi er trained on one domain to another domain.
To address this problem, Blitzer et al. [6] proposed the SCL algorithm to exploit domain adaptation techniques for sentiment classi cation.
SCL is motivated by a multitask learning algorithm, alternating structural optimization (ASO), proposed by Ando and Zhang [1].
SCL tries to construct a set of related tasks to model the relationship between  pivot features  and  non-pivot features .
Then  non-pivot features  with similar weights among tasks tend to be close with each other in a low-dimensional latent space.
However, in practice, it is hard to construct a reasonable number of related tasks from data (as shown in Section 5.4) which may limit the transfer ability of SCL for cross-domain sentiment classi cation.
More recently, Li et al. [22] proposed to transfer common lexical knowledge across domains via matrix factorization techniques.
Domain adaptation can be referred to as a special setting of transfer learning [28], which aims at transferring knowledge across domains or tasks.
Besides sentiment classi cation, domain adaptation techniques have been widely applied to other Web applications, such as text classi cation [11, 8, 33, 10], part of speech tagging [2, 7, 20, 12], named-entity recognition and shallow parsing [12].
Most existing domain adaptation methods can be classi ed into two categories: feature-representation adaptation [11, 8, 33, 2,
  rst kind of methods is to develop an adaptive feature representation that is effective in reducing the difference between domains.
Among these works, the method proposed by Dai et al. [10] is also based on graph spectral techniques.
But the bipartite graph constructed in [10] is among features, instances and tasks.
While in this work, we build a bipartite graph between domain-independent and domain-dependent features.
Instead of constructing new feature representations, instance-weight approaches assume that some training data in the source domain are very useful for the target domain and these data can be used to train model for the target domain after re-weighting.
Theoretical analysis of domain adaptation has also been studied in [4].
In this paper, we propose a general framework for cross-domain sentiment classi cation.
In our framework, we  rst build a bipartite graph between domain-independent and domain-speci c features.
Then, we propose a spectral feature alignment (SFA) algorithm to align the domain-speci c words from the source and target domains into meaningful clusters, with the help of domain-independent words as a bridge.
In this way, the clusters can be used to reduce the gap between domain speci c words of the two domains, which is helpful for training an accurate classi er for the target domain.
Our experimental results on both document-level and sentence-level sentiment classi cation tasks demonstrate the effectiveness of our proposed framework.
In the future, we are planning to encode some lexical knowledge from the source domain to the spectral domain-speci c feature alignment framework.
The reason is that each word has its polarity category.
If we get the polarity knowledge of some words, we can adopt semi-supervised learning techniques to help learn more reasonable clusters of domain-speci c features from the bipartite graph.
In addition, we are planning to develop a more effective feature selection method to identify domain-independent features.
Finally, we are also planning to extend our proposed SFA to solve sentiment classi cation problems from multiple source domains.
Sinno J. Pan and Qiang Yang thank a grant from Microsoft Research Asia MRA08/09.EG03 and Hong Kong CERG/China-NSFC Grant N_HKUST624/09 for their support.
We aslo thank John Blitzer and Yangsheng Ji for comments on implementation of the SCL algorithm and Evan W. Xiang for providing toolkits to prepro-cess the datasets.
Independent Features.
(b) Results on SentDat under Varying Numbers of Domain-Independent Features.
(c) Results on RevDat under Varying Numbers of Feature-Clusters.
(d) Results on SentDat under Varying Numbers of Feature-Clusters.
(e) Results on RevDat under Varying Values of  .
(f) Results on SentDat under Varying Values of  .
Figure 3: Parameter Sensitivity Study of SFA on Two Datasets.
