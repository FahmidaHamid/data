With everyday increases to the size of the World Wide Web and the potential matches of queries, selecting  best  matches for the top few slots of a result page is becoming more constrained.
At the same time, users expect to  nd their relevant matches in the  rst page (if not in top 5 or 3),   The work is done while the author was visiting Google in Mountain View, CA.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
but as the diversity of users increases, so does their needs whereas the number of slots in the  rst page remains  xed.
To have a better understanding of the challenges, consider the following scenarios.
Example 1.
Consider a scenario where the result set {A, B, C, D, E, F, G, H, I, J} is retrieved, and the query is likely to be posed by one of four hypothetical users.
Suppose one such user (if posed the query) would  nd A and D relevant to his/her search; the second user would  nd G and H and the third user would  nd A and E relevant to their searches.
The fourth user would  nd no relevant document in the returned set.
An interesting question is as both the number of likely users and the size of the result set increases, what is the best strategy for ordering the results such that many users would  nd their relevant pages in the top few slots.
The problem is often more complex, and the search experiences of users generally vary depending on where relevant documents are shown in result pages.
In particular, results are often browsed from top to bottom, and it requires less e ort to  nd a relevant document, for instance, in position one, compared to other positions.
Also given an ordering, the more users  nd their relevant documents in top positions, the better score should be given to the ordering.
In other words, the overall search performance depends on both the number of users and the quality of the search experience of each user.
Example 2.
Consider the result set in Example 1 but suppose this time a reward is paid when relevant documents are retrieved.
Let the amount of payo  be proportional to the reciprocal ranks of the relevant documents, i.e. the payo  at rank i is 1/i if the document is relevant and 0 otherwise.
Suppose the payo  for a result ordering is the mean payo  for all relevant documents.
For instance, if only the  rst and the fourth documents are relevant to a particular search, the mean payo  would be ($1.00 + $0.25)/2 = $0.62.
Now consider a search scenario where in 45% of the cases {A, D} is relevant, in 12% of the cases {G,H} is relevant, in 1% of the cases {A, E} is relevant and in the remaining 42% of the cases, none of the documents are relevant.
Table 1 gives both the payo  and the weighted payo  when the results are ordered from A to J.
Under the given workload, the mean payo  is estimated to be $0.31 for the given ordering.
In real settings, typically very little is known about a large class of queries in advance.
According to some estimates, about 60% of search engines queries are new [16].
In cases where the queries are known, it is also generally di cult to Sum









 $0.11 $0.10
 payo  weighted $0.62 $0.28



 $0.13 $0.02 $0.60 $0.01 $0.00 $0.00 $0.31 Table 1: Example scenario identify in advance which subsets of the results would be relevant to which classes of searches and the frequency of searches in each class.
Hence workloads and query classes are rarely known in advance.
In this paper, we base our analysis on click-through rate estimates and statistics about search results, mainly because click-through rates, unlike user workload and query statistics, can be estimated with a good accuracy (e.g.
[15]).
If clicks are treated as votes for relevance, the mean and the standard deviation of this relevance can be estimated; this leads to further statistics about a result set or any subset of the result set.
As our contribution, we formalize the problem of diversifying search results and propose solutions to address it.
More speci cally, we cast the problem as an optimization task and present algorithms to estimate the optimization parameters.
We report on the feasibility and the accuracy of our estimations using various data sources.
We further conduct experiments using queries from di erent sources, including Wikipedia pages [19] and search engine logs, and report the e ectiveness of our algorithm in diversifying Google search results.
Our approach  ts within the general risk minimization framework of Zhai and La erty [21] in that a risk function is minimized and a variable capturing user behaviour is introduced.
Our work focuses on one particular loss function which is the variance of relevances.
The rest of the paper is organized as follows.
In the next section, we formalize the problem and our approach to diversifying search results.
The presented model has a few input parameters; Section 3 presents a few algorithms for estimating these parameters.
Section 4 presents our experimental evaluation of the work.
Related works are discussed in Section 5 and Section 6 concludes the paper.
Let S be a result set and QS be a set of searches for which S is retrieved.
Our notion of a search here includes all query aspects including spatial, temporal and lexical features.
Hence all searches are considered unique, meaning the same query expression issued by di erent users or even the same user at di erent times are considered di erent mainly because the relevant result sets can be di erent (e.g.
the desired result for  Michael Jordan  can be in one search the NBA player and in another search the U. Berkeley Prof.).
The problem of result diversi cation informally can be described as  nding a set S includes relevant (cid:2)   S such that S (cid:2) documents for as many queries as possible in QS.
Naturally result diversi cation is meaningful when S is too large to be fully browsed.
We would refer to set S as a portfolio.
In real settings, we may have some constraints on relatedness, diversity or the size of S based on page layout and statistics on the number of pages browsed or clicked per query.
(cid:2) (cid:2) Let zu be a random variable indicating the relevance of document u to queries in QS.
Suppose zu takes values in the range [0, 1].
Denote the expectation and the variance of zu respectively by E(zu) and  2(zu).
Now let Z = [z1, z2, .
.
.
, zn]T be a vector of random variables indicating the relevance of the documents in S, as just described, where n = |S|.
Denote the correlation between two variables zi and zj by  ij and form the covariance matrix of the variables associated to the result set S. The covariance matrix is symmetric with entry at row i and column j set to  (zi) (zj) ij .
More formally, let s indicate the inclusion of pages in a portfolio with a weight vector W = [w1, .
.
.
, wn]T where 0   wi   1 for i = 1, .
.
.
, n and i wi = 1.
Given a weight vector W , the expected relevance of the portfolio to queries is W T E(Z) and the variance is W T CW where C is the covariance matrix of the result set It should be noted that the expectation here gives the precision of the results, averaged over queries in QS, and the variance indicates the degree of dispersion or variation in precision between queries.
Definition 1.
A portfolio is diversi ed if its expectation is relatively high and its variance is relatively low.
This is a natural and intuitive de nition of result diversity.
Given the uncertainty around queries and the intentions of users posing them, we not only want to increase the average relevance or expectation of a portfolio but also want to reduce the variations on relevance between potential searches for which the portfolio may be returned.
With this de ni-tion, consider two portfolios W1 and W2 and suppose W1 has the same expectation as W2 but a smaller variance.
There are two possible cases where W1 can have a smaller variance.
First, variables in Z with large (small) variances may have overall smaller (larger) weights in W1 compared to W2; this combined with the fact that W1 and W2 have the same expectations would indicate that W1 overall prefers documents that are relevant to a larger number of queries over those that are relevant to only a few; hence W1 is more diverse.
Second, variables in Z can have correlations and W1 may give less weight to correlated variables than W2 which can result in a smaller variance for W1.
This would in turn indicate that W1 prefers less correlated and more diverse results.
On the other hand, diversity is meaningful if the results are relevant or the expectation is not low.
A search for a diversi ed result set can be modeled as an optimization problem where a portfolio is sought such that the portfolio variance is minimized while the expected relevance is  xed at a certain level e, i.e.
min



 (1) subject to W T E(Z) = e, W T 1 = 1 (2)
 and its transpose W T is a row vector.
E(Z) and 1 are linearly independent; if not, one of the constraints must be redundant and can be eliminated before solving the equation.
For vector W to be a solution of Eq.
1 and 2, the necessary  rst-order condition is        

     E(Z)         = e, 1T W  
   = 1 (3) (4) and   are scalers (referred to as Lagrange multi-where   pliers).
To show the uniqueness of solution for our optimization, we  rst eliminate the constraints.
If we denote [E(Z) 1]T and [e 1]T respectively by A and b, then the equality constraints in Eq.
2 can be written as AW = b.
A general approach for reducing this constraint is to choose Y   Rn 2 and V   Rn (n 2) such that AV = 0 and AY is invertible 3, and rewrite W as
  1b + V xv.
(5) see [8]).
Further details on  nding Y and V can be found elsewhere (e.g.
It is easy to see that with the setting in Eq.
5, the constraint AW = b holds for all values of xv, and that the optimization problem in Eq.
1 can be equivalently expressed as an unconstrained problem after replacing W with its equivalent expression in Eq.
5.
Definition 2.
A square matrix M   Rn n is positive de nite if X T M X > 0 for all X   Rn, and positive semi-de nite if X T M X   0.
Equivalently, M is positive de nite if and only if all of the eigenvalues are positive, and M is invertible if and only if all of the eigenvalues are nonzero (see, for example, [3]).
Clearly a positive de nite matrix is always invertible.
Theorem 1.
If V T CV is positive de nite, then there is ) satisfying the conditions of Eq.
4 and   is the unique global solution of Eq.
1 and 2.
a unique (W
 ,   ,         Proof.
This is the direct consequence of Lemma 16.1 and Theorem 16.2 of Nocedal and Wright [12].
Lemma 1.
Let C be a covariance matrix and AV = 0; V T CV is positive de nite if no component of W is a linear function of other components.
Proof.
The covariance matrix C is always positive semi-de nite, i.e. X T CX   0 for all nonzero vectors X; otherwise the variance would be negative.
Suppose there is a nonzero vector p such that pT Cp = 0.
Consider the random variable pW T = [p1W1, .
.
.
, pnWn];  2(pW T ) = (pT Cp) = 0.
This means pW T = b for some constant b.
Since p is nonzero, at least one component of p say p1 must be nonzero.
That means we can write W1 as a linear function of other
 expectation; because of the uniqueness of a solution (as shown next), for a  xed variance, the problem has a unique expectation and vice versa.
 1    1 = I where I is the m   m identity Rn m such that AA matrix.
components of W , and this contradicts our assumption that no component of W is a linear function of other components.
With the replacement of variables in Eq.
5, the preconditions of Lemma 1 and Theorem 1 hold, hence the uniqueness of an optimal solution is guaranteed.
Our experiments with hundreds of thousands of queries, as discussed in Section 4, also con rmed that a unique solution is always reachable.
Next section presents our methods for estimating the model parameters including E(Z) and C.
The optimization model, as presented in Section 2.1, has a few input parameters.
Given a query and a set of matching documents, the relevance expectation for all matching documents, i.e. E(Z), would have to be estimated before evaluating the expectation in Eq.
2.
Furthermore, to construct the covariance matrix (in turn used in Eq.
1), the variance of relevances on each result document, and the pairwise correlations between relevances of result documents have to be evaluated.
The data in our disposal for estimation is (a) click data which may be treated as votes for relevance, and (b) document content.
Consider the set of matching pages of a query, and let p and q be two arbitrary pages in the result set.
We call p and q positively correlated if there is evidence that whenever p is relevant to a search, then q is also likely to be relevant and vice versa.
Correlation statistics is important in optimizing search results, as evidenced in our formulation discussed in the previous section; but estimating correlation can be challenging mainly because relatedness and relevance are often subjective and may depend on queries.
For example, daim-ler.com and toyota.com are related with respect to query  car makers  but not so related with respect to queries  ger-man car makers  and  japan manufacturers .
One approach for estimating correlation is based on past search data; if two pages are frequently retrieved or clicked for the same query, it is likely that they would be retrieved or co-clicked in future.
A problem though is that this data is very sparse.
A large fraction of pages are never clicked or retrieved.
Even at the site level, the data is still too sparse.
In our experiment with 21 million lines of the AOL query log [13], the number of sites that were co-clicked was less than 9 million.
After removing the pairs with frequency two or less and also those with con dence 4 less than 0.10, the number was dropped to 9700 pairs.
This level of con dence was relatively low, but even at this level and with 1.2 million unique sites in the log, the chance of  nding an estimate for an arbitrary pair was less than 0.00000001.
Another approach for estimating correlation is to use the textual contents of pages; if two documents are similar in their textual contents and one is relevant to a query, the other is also likely to be relevant.
On the same basis, a large body of work in the IR community has studied di er-ent metrics for  nding similar documents (e.g.
[2]).
As an
 f(s1) con dence(s1,s2)=f(s1,s2)/max(f(s1),f(s2)).
s1 and f(s2) and s2 and the frequencies joint of frequency respectively f(s1,s2), an obscure Web page that imitates a very well-known page and scores the highest degree of textual similarity.
Hence textual similarity alone is not su cient to establish relatedness.
However, this is less of an issue in our case since correlation is only sought between pages that are retrieved (for a query) and generally obscure and low-quality pages have less chance of making it to a result set.
Even if a low quality page makes to the result set, with a strong correlation between the two (high quality and a low quality) pages, the low quality page has less chance of making to the portfolio.
To further reduce noise and to keep the number of false correlations within some bound, we use a more salient set of features to indicate if pages refer to the same set of concepts and entities.
Our set of features, in particular, included entities, numbers, query extensions and site names.
Entities are identi ed using a simple heuristic that looks for capitalized terms and phrases in sentences.
Query extensions are terms and phrases that appear in result pages and extend query terms.
For example,  city of palo alto ,  palo alto chamber of commerce  and  palo alto restaurants  are all extensions of the query  palo alto .
Query extensions are important for queries with multiple aspects, and may correlate pages on each aspect.
Numbers are included since they give quantities such as phone number, zip code, year, height and weight; these quantities may relate pages that discuss the same entities or concepts.
Extracted entities, query extensions and numbers were weighted based on their Inverse Document Frequencies 5 (IDF).
Finally site names are also included based on the observation that pages on the same site are generally related.
In fact, some search engines limit the number of pages from the same site (so-called site collapsing) as an attempt toward diversifying the results.
This observation does not hold for large portals that host millions of pages such as yahoo.com.
We deal with this problem by assigning a weight to each site which is inversely proportional to the number of pages on the site (similar to IDF weighting).
Consider two Web pages p and q in a document collection and denote their set of common features with F .
If P (f ) denote the probability of observing feature f in an arbitrarily chosen document in the collection, and assuming independence of the features in F , the probability that feature set F (say of p) is found in an arbitrarily chosen document (including q) by chance is  f F P (f ).
The larger this probability is, the smaller the correlation between p and q should be.
Of course, correlation is bounded from upward to 1, and when p and q are independent, the correlation should be zero.
Based on these observations, the correlation between p and q can be expressed as: j else (1/)( log( f F P (f )))  log( f F P (f )) < 
 C(p, q) = where  is a threshold, in our case, set to  log(1/N ) and N is the number of documents in the collection; this setting indicates the point where the expectation of  f F P (f ) drops to one.
The log function can be pushed in giving an equivalent expression for correlation as the (normalized) sum of
 ments that has t and N is the number of documents in the collection.
j











 E(  Nja) Var(  Nj)























 Table 2: Position payo  the idf values for features in F .
j C(p, q) = (1/)(

 f F  log(P (f )))
 f F else  log(P (f )) <  (6)
 Consider a result set S and let u be a document in S. If Zu denote the relevance (or payo ) of u to queries in QS, we want to estimate the expectation and the variance of Zu.
Assuming that the distribution of Zu doesn t change much over time (e.g.
Michael Jordan s NBA page remains relevant to more queries than the Berkeley Prof s home page), its expectation and variance can be estimated based on past queries.
In particular, our estimation is based on the observation that the relevance of a document to a query is directly related to the number of clicks the document is expected to receive.
Generally not all clicks are equally important; especially more relevant pages are likely to be clicked  rst.
Also long clicks, where more time is spent on a page, may be considered more important than short ones.
Hence query clicks may be ordered, and each click may be assigned a payo  proportional to its rank.
Without loss of generality, suppose the amount of payo  is set to the reciprocal rank of a click.
If the random variable Zuj denote the payo  for document u at position j, then j Zuj = 1/i url u at position j receives the ith click
 else.
Similarly, the payo  at position j, denoted by Nj, can be written as j Nj = 1/i position j receives the ith click
 else.
Table 2 gives the expectation and variance of this payo  for top 12 positions, as estimated from the AOL search log data [13].
On the other hand, Zuj directly depends on both Nj, and the bias introduced by presenting u at position j.
If random variable Xu denote this bias toward u, Zuj can be expressed as Zuj = Nj + Xu.
(7) Here Xu = Zuj   Nj gives the di erence in payo  between the case where u is present at position j and the case where u is not.
Xu can be estimated for each u based on estimates of Zuj and Nj.
It should be noted that in real settings Zuj may also correlate (either positively or negatively) with (cid:2) Zu(cid:2)j(cid:2) for pages u < j; this correlation is not easy to estimate and is not taken into account in our formulation.
The expectation and the variance of Zuj can be expressed as (cid:2) E(Zuj) = E(Nj) + E(Xu),  2(Zuj) =  2(Nj) +  2(Xu) + 2Cov(Nj, Xu) (8) where the last term in the expression of variance gives the covariance of Nj and Xu and is zero when Nj and Xu are independent.
The expectation and the variance of Nj and Xu can be estimated in advance of queries and can be plugged in Eq.
8 at query time to derive estimates of the expectation and variance of Zuj .
As shown in Figure 1-a for sites in the AOL log data 6, the bias for 80% of the sites is either zero or -0.05 and for the remaining 20% of the sites it is distributed in the range from 0.05 to 0.7.
Our formulation of bias in Eq.
7 assumes that Xu is independent of the position, hence it gives the average bias over all positions where u appears.
A breakdown of bias over positions, as shown in Figure 1-b for the AOL log data, reveals that Xu is not uniformly distributed over all positions, and rather it has a direct relationship to the positions where u appears.
With the sparsity of data however, it is di cult to estimate the bias for each site and each position; there is little click data on many URLs, and the URLs that are clicked hardly appear in more than very few positions.
To address the problem of sparsity, we use the distribution of bias over positions, estimated over all sites, to obtain an estimate of bias for a speci c site on a speci c position.
In particular, given a site u and position i, assuming that the bias toward u at position i does not di er much from the bias distribution of other sites at the same location, Xu can be scaled according to the distribution, giving a more accurate estimate at position i.
As shown in Figure 1-b for E(Xu), the scaling ratio gets close to zero for i > 10.
One last parameter to our search optimization is a target level of expectation as denoted by e in Eq.
2.
Assuming that the expected values of relevance or clicks per page are in the range [0, 1], the target expectation is also constrained to the range [0, 1].
If we denote the greatest expectation of a result set S by emax(S), the target expectation for S also cannot exceed emax(S).
For the boundary values, Eq.
1 has trivial solutions; more speci cally when e = emax(S), an optimal portfolio includes only the document(s) with the greatest expectation(s), and for e = 0, an optimal portfolio includes no document.
Definition 3.
For a given result set S, relevance estimates are mean-variance e cient if for any pair Zuj and Zvj where u, v   S, E(Zuj)   E(Zvj) i   2(Zuj)    2(Zvj).
Mean-variance e ciency is expected to hold under natural settings of search engines; an e cient engine is expected to push URLs with high expectations up in the ranking until an equilibrium is reached between the expectations and the variances.
Theorem 2.
Assuming mean-variance e ciency of the individual estimates Zuj , variance of an optimal portfolio is a monotonically increasing function of e.
it is assumed that all sites have the same chance of being shown.
e portfolio size relative



 absolute (mean) mean







 var



 Table 3: Portfolio size varying the target expectation (cid:2) as follows:  rst set W1 Proof.
Suppose the result set includes a document d0 with both expectation and variance zero; this can be, for example a document which is never shown hence it cannot receive a click.
Denote the random variable indicating the relevance of d0 by Z0.
Consider target expectations e1 and e2 where e1 > e2 and let W1 and W2 be the respective optimal portfolios.
As a contradiction, suppose  2(W1)    2(W2).
Let  = (e1   e2)/n for some positive integer n. Construct portfolio W1 = W1, then iteratively select a document di with expectation greater than ; denote the random variable indicating the relevance of di by Zi.
Reduce the associated weight wi in W1 by /E(Zi) and add /E(Zi) to the weight of d0 (to keep the norm of the weight vector 1).
Repeat the iterative step until E(W1 ) = e2.
Since initially E(W1 ) = e1 and e1 > e2, after n iterations (for an appropriate n) the termination condition would become true.
With the new W1 and because of the mean-variance e ciency of estimates for individual documents,  2(W1 ) <  2(W1).
We also have  2(W1)    2(W2), hence  2(W1 ) <  2(W2).
But this is a contradiction since W2 is an optimal portfolio for the target expectation e2; this completes the proof.
(cid:2) (cid:2) (cid:2) (cid:2) (cid:2) (cid:2) (cid:2) With a monotonic relationship, setting a target expectation is not straightforward since a higher expectation would also mean a higher variance.
To  nd a trade-o , we further studied the portfolio size as the target expectation varied.
Given statistics on the browsing behaviour of users and the fact that the vast majority of users only browse a few top-ranked results, it is reasonable to keep the portfolio size small.
Although the size of a portfolio cannot be set directly, our experiments show that the portfolio size to some degree is a byproduct of the target expectation.
Table 3 shows mean portfolio size for 10,000 queries randomly selected from Google query log as the target expectation varies from MT(2) to MT(10) where MT(i) denotes the mean of top i expectations in a result set.
The optimal portfolio size increases as the target expectation e decreases until it reaches M T (5) after which there is a reduction in size.
This is not surprising given that when e = 0, the optimal portfolio size is 0 (as discussed earlier).
Examining the changes in optimal portfolios as e increases reveals that for smaller values of e the changes are in the form of including additional documents in the portfolio, whereas for larger values of e the changes are in the form of both additions and substitutions.
Table 4 shows these changes for the same set of random queries.
The changes to optimal portfolios are overall small, indicating the robustness of the portfolio selection to changes in e.
This section provides a preliminary evaluation of our search optimization and diversi ed results.
Bias breakdown (Expectation) s e t i s f o r e b m u











 -0.1 0
 E(Xu) (a)
 s o p t a ) u

 ( / j s o p t a ) u

 (















 Position j (b)


 Figure 1: a) Bias distribution, and b) Bias breakdown over positions insertions MT(3) vs MT(2) MT(5) vs MT(3) MT(10) vs MT(5) mean


 var mean





 del./sub.
var


 Table 4: Portfolio changes
 We implemented our search optimization algorithm, as discussed in Sec.
2.1, in a tool called Diver that o ered functionalities for searching and diversifying the results.
For solving the quadratic programming optimization in our algorithm, we used Gertz s and Wright s OOQP [9] and it turned out to be fast (see Sec.
4.5 for details).
Diver used Google search, and for each query, up to 500 top results from Google were retrieved and reranked to improve diversity.
Correlations between pages were estimated based on textual features as discussed in Section 3 and the relevance statistics were estimated as in Eq.
8.
Xu was calculated at the site level and was the same for all pages on the same site, except that its was weighted for each page according to its rank in the result (as shown in Figure 1-b).
The parameter e in Eq.
2 was set to the mean of the best  ve expectations, i.e. MT(5), hence it was query and result speci c (see Section 3.3 for details on the choice of e).
For evaluation, top  ve results from Diver were compared to those from Google in terms of result relevance and the number of di erent query aspects retrieved.
For Diver, top  ve results included those that received the highest weights in the optimal portfolios derived using the optimization in Eq.
1; if there were less than  ve results in the portfolio, the rest of the results were selected from those outside the optimal portfolio but with the largest Google ranks.
To measure the improvement in the number of di erent query aspects retrieved, we selected 50 disambiguation pages from Wikipedia and used the titles of these pages as queries in both Diver and Google.
The disambiguation pages were identi ed using the query site:en.wikipedia.org  may re-
google diver






 d e v e i r t e r s t c e p s a t c n i t s d i








 at Figure 2: Unique aspects retrieved for Wikipedia queries fer to  disambiguation at Google and after removing topics with either less than 3 or more than 15 di erent aspects.
We ensured that the selected topics were real queries tried in the past by checking their frequencies in a small query log.
All queries appeared at least once; some appeared more than 4000 times.
Returned results from the two systems were passed to evaluators who were asked to assign each document in the result to its closest matching subtopic in Wikipedia; when there was no matching subtopic, the eval-uators could create their own.
Finally the systems were assessed based on the number of di erent aspects retrieved in top r. Figure 2 shows this result with r varying from 1 to 5.
Google does a relatively good job retrieving multiple query aspects and Diver slightly improves upon Google.
We found limitations in using the disambiguation pages to evaluate diversity.
First, there are many obsolete aspects listed for topics that cannot be found elsewhere on the Web.
On the other hand, more new usages of the topics often are not listed in Wikipedia.
A good example of this is person google diver







 d e v e i r t e r s e i r o g e t a c t c n i t s
 i








 at Figure 3: Unique aspects retrieved for random queries names.
A search on the Web often retrieves many unique persons with the same name but only a handful of them can be found in Wikipedia.
Second, Wikipedia pages and topics are well-indexed by search engines and are not good representative queries.
Finally, a manual assessment does not easily scale up to a large number of queries.
Our next experiment was on a randomly selected sample of queries from Google query log.
After running the queries in both Diver and Google, we looked up the returned documents in Google directory and tagged them with the category names under which each document was listed.
Two documents were treated in the same subject if they had at least one category in common.
Figure 3 shows the mean number of unique aspects retrieved at r for r = 1, .
.
.
, 5, measured as the ratio of the number of di erent categories discovered and r. In 40% of the cases, the  rst document was not tagged, leading to a ratio of 0.6 at r = 1.
Unlike the previous experiment, Diver improves upon Google in terms of the number of di erent aspects retrieved by a signi cant margin.
The previous experiments con rm our claim that the result diversity is improved in Diver.
However, diversity is meaningful only if the retrieved results are relevant.
To test for relevance, we selected from our query log a random sample of queries that were expected to have multiple aspects.
The selection criteria was (a) to include queries that had less than 4 terms and a minimum unigram log frequency greater than 3 (i.e. every term in the query appeared in at least 1000 documents), as obtained from Web 1TB 5-gram dataset [4], and (b) to exclude queries that had either a number or one of the terms weather, picture, map, yahoo, wikipedia and youtube or returned 10 or less results at Google.
This was based on the observation that both long queries and those that match very few documents or very speci c sites are less likely to bene t from diversi cation.
Also to alleviate the assessment process, we excluded queries that had all frequent terms (i.e. a minimum unigram log frequency greater than 5).
With this criteria, we selected 42 queries out of 427 random queries we examined (i.e. 10% selectivity).
As for comparison with our system, Google established one baseline for us; since our system was built on top of Google, it was important to make sure that we are improving and not worsening the results.
As another baseline, we chose Carbonell s and Goldstein s MMR [5] which combines query relevance with result novelty.
The model, referred to as Maximal Marginal Relevance, ranks documents for a given query Q based on both their similarities to the query and also their dissimilarities to other selected documents, i.e.
" #
 def = arg max di R\S  (Sim1(di, Q)   (1    ) max dj S Sim2(di, dj )) .
(9) where R is a set of documents retrieved for Q, S   R is the set of documents selected already,     [0, 1] and was set in our experiments to 0.4 based on authors recommendation, and Sim1 and Sim2 are two similarity functions.
A problem in implementing MMR is the choice of a similarity measure between documents and query; a textual similarity alone is not a good measure of relevance in the context of the Web.
To overcome this problem, we used the reciprocal rank of di in the Google result for Q as our Sim1(di, Q) function.
Sim2(di, dj) was the standard Cosine similarity between feature vectors of documents, and each feature vector included the same set of features extracted within Diver (as explained in Sec.
3).
Because of these changes, we refer to this version of MMR as MMR*.
MMR* turned out performing consistently the best in terms of the precision of the results (as discussed next) among the variations we tried (e.g.
compared to the case when Sim1(di, Q) was set to 1 for top k documents from Google for Q and zero for the rest).
The selected queries were submitted to Diver, Google, and MMR* and the top 5 results for each system were selected for user evaluation.
The evaluators were asked to assign a score of 2 for relevant pages with enough new content, 1 for relevant pages with little new content, 0.5 for relevant pages with no new content and 0 for non-relevant pages.
With this setting, a system that returned 5 relevant results all covering the same topic but with little variation would only get an average score of (2 + 1 + 1 + 1 + 1)/5 = 1.2 or less, whereas a system that returned relevant pages with more variations or di erences was likely to score higher.
This scoring quanti es a combined measure of both relevance and novelty very similar to  -nDCG metric of Clarke et al. [7], where a positive value of   indicates that novelty is rewarded in the results proportional to  .
Compared to  -nDCG, our scoring is more discrete, making the assessment a bit easier for our evaluators.
For evaluation, we took some extra steps to make sure that there was no bias against one system.
In particular, the ordering of the systems were randomized and varied from one query to next, and the evaluators didn t know which system was being shown  rst or last.
Each query was assessed by two evaluators, and the score of a system on a query was the average score assigned to its results.
Table 5 shows the score (calculated as the ratio of average score and the maximum score) recorded for each system, averaged over 42 queries.
Diver performs better than Google and the di erence is signi cant (at   = 0.05).
Diver also outperforms MMR* and the di erence is signi cant (at   = 0.01).
The di erence between Google and MMR* is Score


 Table 5: The ratio of average score and maximum score at 5 not statistically signi cant.
Checking the queries, we could see that in 60% of the cases Diver gave better results than Google, in 19% of the cases they scored the same, and only in 21% of the cases Google did better.
Compared to MMR*, Diver did better in 50%, the same in 29% and slightly worse in 21%.
To get a better feeling for the kind of results returned by Diver, Table 6 shows the results of Diver, Google and MMR* for a few queries.
These queries are not from our random query set and were posed to our system after an internal presentation of the work.
The major overhead in diversifying search results, as suggested in this paper, is solving a quadratic optimization function.
To assess this overhead, we varied the number of documents being passed to the optimizer and measured the running time.
As shown in Figure 4, the running time increases with the number of documents, but even at an input size of 500 documents, the running time was under 1 second on a modest Pentium 4 dual-core PC.
On the other hand, the input size is not expected to be large; documents with low ranks in the search engine ranking generally have low expectations.
These documents don t have much chance of making to an optimal portfolio anyway, hence they may be dropped with no or very little a ect on the optimization.
Figure 5 shows the probability that a document returned at rank i (in our case by Google) makes to an optimal portfolio.
Optimization runtime




 c e s m n i ) e m i t n u r ( g o l






 result size



 Figure 4: Optimization runtime

 Related work can be divided into the works on search diversi cation and portfolio optimization.
Carbonell and Goldstein evaluate MMR in document reordering and document summarization and report that more users prefer diversi ed results [5].
In a cost function similar to MMR, Zhai, Cohen and La erty [20] also combine novelty and relevance and use it for subtopic retrieval.
A challenge in both works is  nding a balance between relevance and novelty and adapting the functions to the settings of a search engine; since the computation of relevance scores within search engines may take into account many features including similarity, freshness, and even novelty, this double-dependence of ranks to novelty can a ect the formulation and the results.
In a method similar to MMR, Zhang et al.
[22] use an a nity graph between documents where in each step a document with the largest score is selected while other documents with an a nity relationship to the selected document are penalized.
Chen and Karger [6] propose a Bayesian model with an objective function that puts at least one relevant document in top 10.
To avoid evaluating the function for all subsets of size 10, the authors develop a greedy approach that selects one document at a time and does not change the previous selections as the algorithm progresses.
Their greedy algorithm (similar to MMR) in each step selects a document that maximizes the objective function, conditional on the assumption that none of the previous selections were relevant.
The model can be generalized to select at least k relevant documents in top 10 where k is  xed.
Agrawal et al.
[1] map queries and documents to ODP categories and propose an objective function that maximizes the probability that some document from each one of the categories a given query is assigned to is returned, conditional on the number of returned documents being  xed at some constant.
The problem, as formulated, is NP-hard and the authors provide a greedy approximation to their formulation.
Our approach does not  x k; in the context of the Web search, setting k can be challenging as it would require estimating the number of query aspects in advance.
Also many queries (and sometimes documents) cannot be found in the ODP categories, as was the case for many person name and location queries we tried; for one location query in particular, Google reported 33,400 matches but the location was not in ODP.
We adopt a numerical approach to the problem which easily scales to large result sizes.
The recent addition of a diversity task to the Web track at TREC [10] is also related to our work and emphasizes the importance of the task.
Result diversi cation in general relates to the problem of document clustering, in that a diversi ed result set may be produced by combining the relevance scores with cluster information about each document.
However, running clustering for diversi cation is an overkill; also clustering ranked documents and setting some of the parameters is not straightforward.
As a client-side solution, Radlinski and Du-mais [14]  nd for each query k other interesting queries in the log within 30min window of the query and merge top results of these queries to construct a more personalized but diversi ed result set.
The issue of diversifying search results over more structured data has also garnered some interest lately.
Vee et al.
[17] de ne a diversity ordering of attributes and a similarity measure that weights higher order attributes more heavily than attributes of lower order.
As yet in one more domain, Zwol et al.
[23] propose a method for diversifying image tags and further using those terms to  nd related but more diverse collection of results.
Our portfolio model of search is based on Markowitz s Nobel-prize winning portfolio selection [11].
In his seminal work, Markowitz notes the relationship between expected return and the associated risk in the stock market and develops an optimization model that can minimize risk for a given level of return or vice versa.
The model has been used in the areas outside  nance, but to the best of our knowledge, not much work is done to incorporate a similar notion of risk to the Web search.
With the exception of a recent independent work by Wang and Zhu [18] which studies some properties of the model when applied to a ranked list in IR, our work is the  rst that applies the model to diversify Web search results.
As in Markowitz s portfolio selection, risk manifests in Web search in the form of returning a result set which may not include a user s desired aspect.
This can be due to ambiguity in query interpretations, uncertainty about users  intentions and sometimes heuristics that may be applied within a search engine.
Our formulation of diversity tries to reduce risk by taking into account correlations between pages and that if a document is not relevant, other correlated documents are also likely to be not relevant.
We have proposed a model and an algorithm for diversifying search engine results, and have presented an evaluation and analysis of our algorithm and its results.
To the best of our knowledge, this is the  rst work that relates results quality and diversity to expected payo  and risk in clicks and provides a model to optimize these quantities.
A challenge in any search optimization including ours is deriving statistics about variables used in the model; we have presented a few methods to derive these statistics based on data and statistics that is generally available in search engines.
There is room to  nd better statistics about click-through rates and correlations which can lead to more accurate estimates and better search results.
Another interesting related direction is detecting queries that can or cannot bene t from a result diversi cation.
We tried to do a little bit of this with our heuristics in Section 4, but this area by itself, to the best of our knowledge, is open for further research.
Acknowledgments The authors would like to thank the members of NextRank group at Google for their input and the anonymous reviewers of the paper for their comments.
This research was partially supported by the Natural Sciences and Engineering Research Council.
