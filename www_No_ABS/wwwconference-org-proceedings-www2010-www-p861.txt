Access to realistic measurement data is critical to accurate research results in a variety of network domains.
Prior work on Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
wired and wireless networks has shown that the validity of experimental results depends on the accuracy of the data used [12, 16].
This is especially true for the growing area of online social networks, where evaluation of social applications can produce very different results depending on the social graphs used [35].
As researchers gain awareness of the need for real data, they are frustrated by the dif culty of performing measurements on existing social networks, many of whom now take careful steps to prevent automated crawlers.
Instead of commiting costly resources to gather real social graphs, researchers seek access to a small number of measurement-generated graphs available to the community1.
However, the continued distribution and experimental use of social graph datasets faces three signi cant challenges.
First, owners of datasets are increasingly concerned about inadvertently revealing private information with their anonymized datasets.
Recent work shows that malicious parties can recover information from anonymized graphs by de-anonymizing them using either auxiliary graphs or by identifying unique subgraphs in the anonymized graph [4, 27, 28].
Given recent privacy compromises [7], these concerns act as a strong disincentive against sharing graph datasets.
Second, the limited number of available graphs is insuf cient to generate meaningful experimental results.
Ideally, researchers would like to experiment with multiple real graphs to produce statistically con dent results.
Finally, social networks are exploding in size, and measured graphs contain millions of nodes and hundreds of millions of edges.
Even compressed, graphs from our Facebook study [35] can be over 50GBs in size.
Sharing this data requires either a multi-day download over a fast network or shipping hard-drives, neither of which scales with the demand for these graphs.
Synthetic graphs generated by measurement-calibrated graph models offer an attractive alternative to sharing large graph datasets.
Trace-driven modeling is popular in research settings where real measurement data is dif cult to gather, such as wireless networks [16,
 In the social graph context, we can  t  a graph model to a real social graph, thereby extracting a set of model parameters.
We feed these parameters into the graph model to produce randomized synthetic graphs that match the original in statistical properties.
If accurate, experimental results from these synthetic graphs will closely match those from experiments performed on the original graph.
This approach addresses all of the aforementioned challenges: synthetic graphs are randomized, hence no privacy is compromised; additional graphs can be generated on demand, thus improving statistical con dence; and model parameters are compact, hence cost of sharing models is trivially low.
But which graph model should we use?
Instead of creating a new graph model, we choose to determine if any of the numerous
 use of social graphs from our recent Facebook study [35].
two (dK [23] and Kronecker graphs [19]) are designed to capture overall structural characteristics of graphs.
These structure-driven models would be ideal for our use, but they incur very high costs in memory or computation.
To apply them to large social graphs, we must  rst use parameters to reduce their overheads, i.e. limiting dK graphs to dK-2, and reducing matrix size and number of iterations for Kronecker graphs.
Unfortunately, this also reduces model accuracy.
In contrast, intent and feature-driven models such as Nearest Neighbor and Forest Fire have signi cantly lower algorithmic and computational complexity, but are only designed to capture a single graph property.
To be inclusive, our work must consider all three types of graph models.
We must answer several other key questions before we can accept the validity of research results using synthetic graphs generated by calibrated models.
What challenges are involved in  tting graph models to real social graphs?
How accurately can model-generated synthetic graphs capture the statistical metrics of real graphs?
We refer to a model s ability to produce statistically similar graphs as its  delity.
Which models demonstrate the highest levels of  delity for today s large social graphs?
Do current graph metrics capture all of the meaningful properties of real graphs?
And  nally, can application-level results obtained by researchers using synthetic graphs match those obtained using the original graphs?
In this paper, we seek to answer these questions by exploring the feasibility of replacing real graphs with synthetic graphs generated from calibrated graph models.
We make three key contributions:
 graphs.
We explore the problem for several popular graph models, and propose a two-phase parameter search approach guided by a structural graph similarity metric.
models from literature, and evaluate how accurately each model captures statistical metrics from graphs of the Facebook social network ranging from 30,000 to 3 million edges.
We  nd that while most vary signi cantly in accuracy, two graph models (Nearest Neighbor and dK-2) are consistently accurate for most metrics across all of our test graphs.
simulations of social network applications.
We use these application-level tests as  end-to-end  metrics to test the feasibility of substituting real graphs with synthetic graphs.
Our results show that the Nearest Neighbor model produces results on synthetic graphs closely matching those of real graphs, thus con rming that model-generated synthetic graphs can be reliably used in research on social networks.
While this work focuses on social network graphs, we believe our methodology is general, and we can use similar techniques to evaluate the feasibility of measurement-calibrated graph models for Internet routers, the web, and biological graphs.
Our goal is to identify which graph models, if any, can generate synthetic graphs that are suf ciently representative of real-world social graphs to be suitable for experimental research.
We refer to the a model s ability to reproduce statistically similar synthetic graphs as its  delity.
Thus, our restated goal is to determine the  delity of current graph models, and whether any model has suf -ciently high  delity to replace real social graphs in research.
Our approach (shown in Figure 1) consists of three steps: collecting real-world social graphs,  tting graph models to target graphs, and quantifying each model s  delity by comparing the resulting Social Network (Facebook) Measurements Social Graphs Fidelity Analysis Graph Metrics Application Level Results Fit to Model Model Parameters Generator Synthetic Graph Figure 1: Our methodology for evaluating graph model  delity.
Graphs Monterey Bay, CA Santa Barbara, CA Egypt New York, NY Nodes



 Edges



 Table 1: Four representative social graphs from Facebook measurements in 2008.
They vary in size from very small (Monterey Bay, CA) to extremely large (NY, NY).
synthetic graphs against the originals using graph metrics and application benchmarks.
Here, we summarize our methodology, identify 4 challenges in the process, and describe our solutions.
Collecting Social Graphs.
Facebook is the largest social network in the world with more than 350 million users.
We use several Facebook social graphs we obtained through detailed measurements between March and May 2008 [35].
Our data includes anonymized social graphs encompassing more than 10 million users with over 940 million social links from the 22 largest regional networks, as well as several smaller regional networks.
These Facebook social graphs are attractive as target graphs for this work for two reasons.
First, our prior analysis [35] showed that they are representative of measured social graphs, i.e. they display graph properties similar to measurements of other popular social networks such as Orkut [25].
Second, the availability of a wide range of Facebook graphs means we can choose multiple graphs of different sizes.
Ultimately, we chose to use 4 representative regional networks (listed in Table 1) ranging from 6000 nodes and 30,000 edges to 300,000 nodes and 3 million edges.
Fitting Models to Target Graphs.
We compare the  delity of different models to determine their suitability as replacements for measured social graphs.
We consider several well-known graph models of social networks developed from the  elds of mathematics, physics and computer science.
For each model, we use social graphs from Facebook as targets, and determine the optimal model parameters that provide the best  t for the model and a given graph.
We then use these parameters to generate randomized graphs that attempt to match the target graph s salient graph properties.
We face three challenges in this phase of our work.
First, existing graph models cannot be used directly to generate synthetic graphs because their output is restricted in some manner.
For example, Nearest Neighbor [33] has been analytically shown to generate graphs with Power-law coef cients always > 2, and Random Walk [33] produces directed graphs that may be disconnected.
We modify these models to produce general graphs matching our social graph datasets.
For models such as dK [23] and Kronecker graphs [19], we also tune parameters to trade off accuracy for model complexity.
Model modi cations are described in Section 3.
Our second challenge lies in  tting the models to our target graphs, i.e. for a given graph G and model M, determining the optimal pa-this leads us to the third challenge:  how do we quantify similarity between graphs?  As we will explain in Section 4, we build a graph similarity metric using the dK-series [23], a structure-based graph metric that, given suf cient space, can uniquely identify a target graph.
Using this metric, we perform adaptive precision search through the parameter space until we  nd the best  t parameters.
Evaluating Model Fidelity.
Finally, once we have computed the best  t parameters for a given model and target graph, we can compute the  delity of the model with respect to a given metric.
The  nal challenge is identifying the correct metric(s) that capture the properties of interest to experimental research.
We start with a comprehensive set of accepted social graph metrics, including the power-law degree distribution, node separation, clustering coef cient and assortativity.
For each target graph, we quantify a model s  delity by measuring the Euclidean distance between the target s graph metrics and those of the synthetic graphs.
These metrics may not tell the whole story, however.
Ultimately, we do not yet understand how these graph metrics are related, or whether existing graph metrics completely capture all properties of a graph.
Therefore, the only reasonable way to determine the  delity of a graph model for experimental research is to feed the original and synthetic graphs into  application-level  tests and examine the difference in their results.
We perform a suite of simulations of well-known social network applications, including Sybil-guard [36], Reliable Email [13], and Social Shields for anonymous communication [30].
Examining the  error  between application results from original graphs and those of synthetic graphs provides an end-to-end test that answers two questions: do current graph metrics capture the features of graphs  important  to social networking research, and can researchers safely rely on synthetic graphs to produce meaningful and accurate experimental results?
In this section, we brie y describe six well-known graph models that we consider as potential models to replace real social graphs.
We divide these models into three classes based on their methodology.
We classify the classical Barabasi-Albert model [5] and the Forest Fire [20] model as feature-driven, since they focus on reproducing statistical features of a graph such as power-law distribution and dynamic changes in graph density.
Intent-driven graph models such as Random Walk [33] and Nearest Neighbor [33] focus on emulating the creation process of the original graphs.
Finally, structure-driven models capture statistics from the graph structure, allowing a corresponding generator to reproduce random graphs with the same structural constraints.
This class includes Kronecker graphs [19] and dK-graphs [23].
We omitted a number of graph models from our study.
The Watts and Strogatz model [34] generates small-world graphs, but is unsuitable for our study because it does not produce graphs with power-law degree distribution.
Other models were omitted because they are similar to models in our chosen set.
This includes variants of the Nearest Neighbor [32], Random Walk, and Forest Fire models, such as the copying model [17], the duplication divergence model [33] and the random surfer model [8].
Barabasi-Albert.
Barabasi and Albert [5] proposed the classical model which produces graphs with power-law degree distributions missing from random graphs [11].
This model proposed an incremental growth model for graph construction, and preferential attachment: the idea that new nodes tend to attach to existing nodes with nonuniform probability.
The model has two parameters: n, number of nodes in the graph, and m, number of edges introduced from each new node to existing nodes.
Given its impact on other models, we include it as a baseline measure.
Forest Fire (modi ed).
Leskovec et al. observed increases in density and decreases in diameter over time in graphs such as the patent citation graph and Internet AS connectivity graph [20].
To capture these dynamic effects, they propose the Forest Fire model, where the graph grows with each new node connecting to a set of existing nodes.
After the new node connects to an existing node, it randomly connects to some of the node s neighbors.
This process is executed recursively, imitating the  burning  of forest  res.
Since the Forest Fire model generates directed graphs, we make a simple modi cation for it to generate undirected graphs for our study.
Speci cally, we always create undirected edges and follow the edges in both directions in the  burning  process.
This model has two parameters: n, number of nodes in the graph, and p, the rate which decides the number of neighbors  burned  in each recursion.
Intent-driven models attempt to capture how power-law graphs form and grow by emulating the processes behind link formation between nodes, e.g.
formation of friendships in of ine social networks and adding links on a webpage to other sites.
Random Walk (modi ed).
The Random Walk model [33] emulates the randomized walk behavior of friend discovery in online social networks.
Each new node performs a random walk starting from a randomly chosen node in the graph.
As the walk traverses the graph, the new node probabilistically attaches itself to each visited node.
The original model creates directed graphs.
We modify the model to create undirected edges, and allow the random walk to traverse edges in any direction.
The model has three parameters: n, the number of nodes, qe, the probability of continuing the walk after each step, and qv, the probability of attaching to a visited node.
The original Random Walk model can generate disconnected graphs.
We  x qv = 1 for each new node s  rst edge, in order to ensure a generated graph with a single connected component.
Nearest Neighbor (modi ed).
Another model based on social behaviors is the Nearest Neighbor model [33].
It follows the observation that two people sharing a common friend are more likely to become friends.
Each new node added to the graph is connected to a random existing node.
Additionally, random pairs of 2-hop neighbors around the new node are connected.
The original model has two parameters: n, the number of nodes, and u, a probability that determines at each step if a new node is added or if a pair of 2-hop neighbors are connected.
Analysis shows that the original model always produces graphs with power-law exponent greater than 2 [33].
This does not match known measurements of social networks such as Facebook, YouTube, Flickr and Orkut, which all have power-law exponents between 1.5 and 1.75 [25, 35].
Thus, we modify the model by adding a parameter k. Each time a new node is added, we also connect k pairs of existing nodes randomly chosen from the graph.
Because the power-law exponent scales with the intensity of preferential attachment in random graphs, adding edges between node pairs selected uniformly at random reduces the level of preferential attachment, and thus the power-law exponent.
By extending the analysis in [33] to our modi ed model, we show that the power-law exponent   is a function of k and u:     1 + 1   , where r   = u 2(1   u)  1 +
 (k + 1)(1   u) u !
.
Egypt

 New York



 dK-1 dK-2 dK-3 Table 2: The number of values required to represent our graphs using different dK-series.
This produces the desired reduction in Power-law distribution while maintaining the intuition behind the model.
Unlike models that focus on single properties or incremental growth of the graph, structure-driven models focus purely on capturing the physical characteristics of the target graph.
Thus they have no graph formation parameters, only parameters that trade off accuracy for model complexity.
Given their design goals, these models should provide the most representative synthetic graphs.
While they are likely to capture a graph s structure, however, they are also likely to incur high costs in computation and/or memory, thus limiting their achievable accuracy in practical settings.
Kronecker Graphs.
Leskovec et al. proposed using Kronecker graphs to approximate real graphs [19].
Kronecker graphs are generated by the recursive evolution of an initiator graph.
This evolution process, called Kronecker multiplication, is able to approximate real graph structures [20].
KronFit is an algorithm that generates synthetic graphs that are structurally similar to a given target graph.
The similarity is measured by a maximum likelihood value, i.e., the probability that this model will generate a graph identical to the original.
KronFit includes parameters that tradeoff optimality and computation complexity.
The most important parameters are the size of the initiator matrix ikro, the sample size for estimating the likelihood and its gradient skro, and the maximum number of gradient descent iterations in the search gkro.
Larger parameters map to higher accuracy as well as higher complexity.
With guidance from the authors of [19], we chose the following values to keep the running time comparable to other graph models (less than 48 hours for Egypt and New York graphs on a server with 32GB of RAM): ikro = 3, skro = 500,000, gkro = 50.
dK-graphs.
Finally, dK-graphs are a systematic way of extracting subgraph degree distributions from a target graph, so that similar synthetic graphs can be generated with identical degree distributions [23].
As the value of d increases, dK incorporates degree distributions of increasingly large subgraphs.
For example, the dK-1 metric captures the node degree distribution, dK-2 captures the joint degree distribution, and dK-3 captures the clustering coef cient.
As d increases beyond 3, the distribution becomes increasingly likely to uniquely de ne the target graph [23].
dK models  running time and computation state size increase rapidly as d increases.
As shown in Table 2, the amount of state to capture a graph grows rapidly from dK-2 to dK-3, and becomes prohibitively costly for large graphs like New York.
In this work, we present results using dK-2 model for two reasons: to avoid extremely large memory requirements for large graphs like New York, and because graph generators for dK-3 do not yet exist.
We now describe our efforts to  t each graph model to Facebook social graphs.
All six models are parameterized by n, number of nodes in the graph, and four models require additional parameters.
Maximum likelihood estimation (MLE) is the best-known statistical method for  tting a statistical model to data and estimating a model s parameters.
For different parameters, it calculates the maximum probability that a parameterized model generates the data exactly matching the original, and chooses the parameters that maximizes such probability.
Applying MLE to graph model  tting, however, is very dif cult.
For large graphs like ours, there are no ef cient solutions to determine if two graphs are physically identical.
This is the well-known graph isomorphism problem whose dif culty has been proven in prior work [14].
Instead, we propose to use a parameter-search based solution by scanning the possible parameter space, and guiding the search using a statistical similarity metric between the target graph and the model generated graphs.
We choose this solution for our graph models because it produces good results within tractable computation times, despite the massive sizes of our graphs.
This is a  rst attempt at a practical solution, and we leave the search for more ef cient solutions for future study.
Implementing this solution requires us to solve two technical challenges.
First, we need a metric to measure the statistical difference between graphs, for which we propose to leverage the dK series, a graph distribution that captures subgraph degree distributions [23].
Second, we need an ef cient strategy to search through the large parameter space to quickly locate near-optimal parameters.
For this we propose a space-sampling solution with adaptive precision.
Structure-Driven Graph Comparison.
We consider the problem of quantifying the similarity between any two graphs.
A naive solution is to organize the accepted social graph metrics (Section 5.1) into a vector, each with a weight.
We can de ne the statistical difference between graphs as the distance between the vectors derived from each graph.
The problem with this approach is that it assumes our graph metrics are comprehensive, and that we can assign the  right  weight to each metric.
Rather than focusing on known graph metrics, we propose to use the dK-series as a single similarity metric to capture a graph s physical characteristics.
We do so for two reasons.
First, with increasing d, dK can progressively capture increasingly complex graph properties [23] until the graph is uniquely de ned by the dK model.
Second, the dK-series captures signi cantly more detail of graph structures than alternative metrics.
For a given d, we calculate the distance between two graphs as the square distance between the dK vectors of the two graphs.
Given the memory and time complexities of dK for higher values of d (see Table 2), we limit ourselves to the dK-2 series.
Using dK-2, parameter  tting a Nearest Neighbor model to our New York graph requires 2 days of computation on a quad CPU server.
More accurate dK-3 requires orders of magnitude more values to represent each graph, making it impractical for larger graphs.
Parameter Sampling with Adaptive Precision.
When it comes to parameter  tting models, local search algorithms such as hill climbing [31] are the most widely used solutions.
Hill climbing starts from a random (potentially poor) solution, and iteratively improves the solution by making small changes until no more improvements are found.
Hill climbing does not work well for non-convex spaces, however, since it will terminate when it  nds a local maxima.
We have experimented with hill climbing in our model  tting problem, and con rmed that it produces suboptimal results because the similarity metric (dK or others) is not strictly convex.
To overcome this problem, we apply a sampling method that  nds the best  t parameters by uniformly scanning the possible p a r
 l a e
 o t e c n a t s


 i








 k=3 k=2 k=1
 u h p a r
 l a e
 o t e c n a t s


 i








 k=1
 u (a) with k = 1, 2, 3 and coarse sampling of u.
(b) with k = 1 and  ne sampling of u.
Figure 2: Two-level parameter sampling for the Nearest Neighbor model on the Monterey Bay graph.
Using dK-2 as the graph similarity metric, (a) shows the  rst level sampling with varying values of k (values of k > 3 are not shown).
Error (distance from the target graph) is shown on the Y-axis.
After choosing k=1 from (a), (b)  ne-tunes for the parameter u.
The  nal parameters are k = 1 and u = 0.8.
parameter space given a reasonable constraint on the level of precision.
We choose this simple approach because it requires minimum information on parameter statistics.
In contrast, advanced sampling methods such as Gibbs sampling [1] require knowledge of the conditional distribution of each parameter, which is very costly to compute.
We also apply techniques to improve ef ciency and accuracy.
First, we use theoretical analysis to narrow down the parameter space based on the statistics of the real graph.
We partition the remaining space uniformly to avoid local maxima.
Second, we apply an initial coarse sampling to identify candidate parameter regions and then perform  ne-grained sampling within these regions.
Despite the simplicity of this approach, results in Section 5 con rm that it locates model parameters that produce synthetic graphs that closely approximate metrics of the original graph.
We now apply the  tting algorithm to each model.
Nearest Neighbor.
Our modi ed Nearest Neighbor model has two parameters: 0 < u < 1, and k = 1, 2, 3,   .
Since u and k determine the power-law exponent  , we also examine the resulting   to eliminate unsuitable choices of u and k. In the remaining two-dimensional parameter space, we apply a multilevel sampling method.
First, we vary k from 1 to 10 and for each k sample u coarsely with  u = 0.05.
Using the Monterey Bay graph as an example, Figure 2(a) shows the dK-2 based distance between the original and Nearest Neighbor-generated graphs.
This model produces graphs with minimal dK-2 distance from the target when k = 1.
Next, having  xed k = 1, we apply a  ne sampling on u with  u = 0.01.
Results in Figure 2(b) show that the  ne grain sampling avoids a signi cant number of local maxima.
The  nal parameters for Monterey Bay are k = 1 and u = 0.8.
Random Walk.
The modi ed model has two parameters to tune: 0 < qe < 1 and 0 < qv < 1.
Both are real numbers and their contributions are interrelated.
Our sampling takes two steps.
We start from a coarse sampling on qe with  qe = 0.1, and for each qe we sample qv with the same precision  qv = 0.1.
Using these results we identify multiple candidate intervals where the synthetic graphs are closer to the original real graph.
Next, we apply a  ne-grained sampling across these intervals with  qv = 0.01 and  qe = 0.01, and choose the best con guration that minimizes the dK-2 distance.
Forest Fire.
The Forest Fire model has only a single parameter, p the burn rate.
For each target graph, we apply a  ne-grained sampling ( p = 0.01) across its range to  nd the best  t p.
Barabasi-Albert.
This model has only one unknown parameter m, the number of edges introduced from each new node to existing nodes.
Since m is a static parameter, we compute it as m = |E|/n.
This follows naturally because given n nodes, the total number of edges in the graph is n   m.
Computational Costs.
Our experience shows that the parameter search approach is computationally tractable for large graphs.
Running experiments on a Dell 2900 server w/ 32GB of RAM, most models can be  t to the largest of our graphs (New York,
 dominated by the time required to generate candidate graphs as we search through the model parameter space.
Computing the dK-2 distributions is also a factor, but rarely contributes more than 1 hour to the total  tting time.
The time required to compare dK-2 distributions is negligible (a few ms).
Finally, we found that Kronecker graphs and the Forest Fire model are the most computationally intensive to  t.
Having extracted the best parameters for each model and Facebook graph combination, we can evaluate the  delity of the models by comparing the properties of the Facebook graphs against their synthetic counterparts.
We  rst evaluate the  delity of graph models using graph metrics described in literature.
In the rest of the paper, we identify a graph G as G = (V, E) where V is the set of vertices representing social network users, and E is the collection of undirected edges representing links among users.
We evaluate the six graph models using the Facebook graphs listed in Table 1.
For each target graph, we apply the  tting mechanism described in Section 4 to compute the best parameters for each model.
We generate 20 randomly seeded synthetic graphs from each model for each target graph, and measure the differences between them using several popular graph metrics.
We examine model  delity by computing the Euclidean distance between metrics derived from the target and model-produced graphs.
We represent node degree distribution, clustering coef cient and joint degree distribution as functions of the node social degree, and compute each metric s Euclidean distance as the average square root of the total squared errors in metric values.
All results are averages from comparing the 20 synthetic graphs against the original.
Standard deviation values are consistently low relative to the values themselves, and are omitted for clarity.
We now summarize the suite of graph metrics we use to determine the  delity of our graph models.
Social degree refers to the number of friends (or edges) each node has.
Measurements show that node degree distributions in social graphs follow a power-law the fraction P (k) of nodes in the graph having distribution, i.e.
k connections to other nodes grows as P (k)   k  , where   is a constant exponent.
We compute   by  tting a graph s degree distribution to a power-law using the method described in [9].
Joint Degree Distribution.
There are several different ways to capture the joint degree distribution, including the knn function, assortativity, and the s-metric [23].
knn computes the correlation between a node s degree and the average degree of its neighbors.
A graph s assortativity coef cient AS is a value in [-1,1] calculated as the Pearson correlation coef cient of the degrees of all connected node pairs in the graph.
A positive value means that nodes tends to connect with others with similar degrees, and a negative value means the contrary [29].
Finally, the s-metric captures the joint degree distribution of a graph as the probability that high degree nodes interconnect with each other [21].
We compute all three metrics for all synthetic and target graphs.
Given the consistency of our results, we omit results for the s-metric, which can be viewed as a subset of the assortativity distribution.
Clustering Coef cient (CC).
Clustering coef cient measures whether social graphs conform to the small-world principle [34].
It is de ned as the ratio of the number of links that exist between a node s immediate neighborhood and the maximum number of links that could exist.
For a node x with degree dx, at most dx(dx   1)/2 edges can exist among x s friends (when they form a complete clique).
Let kx be the actual number of edges among x s friends, dx(dx 1) .
A graph s CC is the clustering coef cient of node x is the mean CC of all nodes.
Intuitively, a high CC means that nodes tend to form highly connected subgraphs with their neighbors.
Node Separation.
The degree of node separation is measured through three metrics: average path length, network radius and network diameter.
Average path length refers to the average of all-pairs-shortest-paths on the social graph.
The radius and diameter are calculated using the eccentricity of each node in the social graph.
Eccentricity is de ned as the maximum shortest-path distance between a node and any other node in the graph.
Radius is the minimum of all eccentricities, while diameter is the maximum.
Because computing all-pairs-shortest-paths is computationally in-feasible given the size of our social graphs, we estimate the radius, diameter and average path length by determining the eccentricity of 1000 randomly selected nodes in each graph.
2kx
 We now describe the results of our  delity tests.
All of our graph computations are performed on a cluster of Dell Poweredge 1750 servers with dual-core Xeon processors.
Memory intensive computations were performed on 2 Dell 2900 servers, each with a quad-core Xeon CPU and 32GB of memory.
Node Degree Distribution.
We examine the node degree distribution in terms of the cumulative distribution function (CDF).
The results across the four Facebook graphs are consistent.
Hence in Figure 3 we only plot the results for Santa Barbara and New York.
We see that dK and Nearest Neighbor are the two models that produce the most accurate degree distributions, which is con rmed by the Euclidean distance results in Table 4 under  NDD.  An interesting observation is that  40% users have social degree of 10 or less, which is not captured by the Barabasi Albert model.
We also examined whether the node degree in the synthetic graphs follows the power-law distribution.
Using the power-law curve  t-Graph Santa Barbara 12,814 nodes 92,241 edges New York 377,712 nodes 3,616,873 edges Model Real Graph dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert Real Graph dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert Estimated Power-law exp.
Power-law Fitting error



























 Table 3: Examining the power-law effect in node degree distribution.
dK-2 and Nearest Neighbor are the two most accurate models.
ting method in [9], we derive the exponent and the  tting error.
Table 3 summarizes the results for Santa Barbara and New York.
Again, dK-2 and Nearest Neighbor outperform other models.
Joint Node Degree Distribution.
Next we compare the real and synthetic graphs in terms of node connectivity, particularly on metrics of joint node degree distribution (knn and assortativity).
These metrics have been used to verify whether a graph displays scale-free and small-world properties.
We represent knn as a vector over the node social degree, and use the vector-based Euclidean distance values in Table 4 to represent the statistical difference between the real and synthetic graphs.
Assortativity is a scalar value representing the same property as knn, and re ects the same relative results.
For scalar metrics like assortativity, network diameter, and average path length, we compare the actual values of both synthetic and target graphs in Table 5.
The knn and assortativity results show that the dK-2 model is consistently accurate across all four target graphs, while the Nearest Neighbor model displays some visible differences.
A closer look at the detailed knn values (omitted for brevity) shows that the Nearest Neighbor model displays a similar trend in knn as the real graphs, but produces larger knn values at nodes with higher social degree.
This implies that the model tends to interconnect nodes with higher social degrees, which can be explained by the fact that the model connects nodes to 2-hop neighbors and most popular nodes are within 2 hops from each other.
This is con rmed by its higher assortativity values in Table 5.
Clustering Coef cient.
Prior work [34] shows that social networks have a local clustering structure, i.e. neighbors of a node in the social graph tend to connect to each other as well.
This is particularly true for nodes with low social degrees.
Figure 4 shows the clustering coef cient as a function of the social degree for three networks.
We omit the results for Monterey Bay because they are highly consistent with those of Santa Barbara.
The Euclidean distance values associated with all four networks are shown in Table 4 under the column  CC.  In this case, Nearest Neighbor, Random Walk and Forest Fire are the top three models.
They all follow the general trend in the target graphs: nodes with small social degrees experience heavier clustering, and the degree of clustering decreases with the node degree.
This is unsurprising, given the model de nitions where all three encourage forming local triangles by connecting 2-hop neighbors (Nearest Neighbor) or connecting new nodes to well-connected subgraphs (Random Walk and Forest Fire).
On the other hand, the other three models (dK-2, KronFit and Barbasi-Albert)



 )


 ( s r e s
 f o %

 Forest Fire KronFit Nearest Neighbor Real Data dK-2 Random Walk Barabasi Albert
 Social Degree
 (a) Santa Barbara




 )


 ( s r e s
 f o %

 Forest Fire KronFit Real Data dK-2 Nearest Neighbor Random Walk Barabasi Albert

 Social Degree (b) New York Figure 3: CDFs of the node degrees of the real Facebook graphs and those generated by the six network models.
dK-2 and Nearest Neighbor models closely match the real data.
i t n e c i f f e o
 g n i r e t s u
 l








 Forest Fire Real Data Nearest Neighbor Random Walk KronFit dK-2 Barabasi Albert


 Social Degree

 Figure 4: CC in Santa Barbara as a function of social degree.
The best models are Forest Fire, Nearest Neighbor and Random Walk.
all produce  at clustering coef cient around 0.05 or less, and fail to capture any local clustering effects.
Node Separation Metrics.
Finally, we look at how models capture the separation between nodes through the network diameter and average path length metrics.
We  nd that dK-2 is highly accurate, while Forest Fire also performs well.
Nearest Neighbor, however, produces signi cantly more clustered graphs, resulting in shorter path lengths and a smaller network diameter.
We attribute this to Nearest Neighbor s focus on preferential attachment which increases connections between highly connected nodes.
The above results do not tell us in absolute terms how signi -cantly different synthetic graphs are compared to the original graphs.
Relatively speaking, however, we see that the dK-2 and Nearest Neighbor models provide a relatively accurate representation of the target graphs.
On the other hand, some models do not accurately capture particular individual metrics well.
The dK-2 model is especially accurate in capturing the individual and joint degree distributions, but fails to capture the key feature of local clustering.
The Nearest Neighbor model is consistently accurate in terms of the degree distribution and clustering coef cients, but is biased towards interconnecting high-degree nodes, and produces graphs with signi cantly shorter path lengths and network diameter.
This results in higher assortativity values that may diverge from graphs with more heterogeneous connections like Egypt2.
Our results are promising, because they show that despite variances across models and graphs, two models (Nearest Neighbor and dK) stand out for their ability to capture graph metrics.
If these metrics are indicative of application performance, then we expect these two to also show high  delity in our application benchmarks.
Since we do not yet understand how graph metrics impact different social applications, the  nal measure of a model s  delity must still rely on application-level benchmarks.
We implement the algorithms from several social network applications, run tests with both target graph and synthetic graphs as input, and compare the results to quantify each model s  delity.
In addition, these results allows us to identify whether the existing social graph metrics fully capture the  important features  in social networks.
We chose  Reliable Email  [13],  Sybilguard  [36] and a  Social Shield Anonymous System  [30] as representative social network applications.
All are recent research systems that leverage graph properties in social networks to address network security
 values between 0.05 and 0.25.
problems.
Compared to known graph metrics, all these application tests present new perspectives, since their performance on a particular graph cannot be easily correlated with a single graph metric3.
Examining how synthetic graphs compare in these application tests versus their original counterparts sheds light on whether today s models are accurate enough to replace actual social graphs with graph models for experimental research.
RE: Reliable Email.
RE [13] is a whitelist system for email that securely marks emails from a user s friends and friends-of-friends as non-spam messages, allowing them to bypass spam  l-ters.
Friends in a social network securely attest to each others  email messages while keeping users  contacts private.
A meaningful evaluation experiment is to examine the level of potential impact on RE users if accounts in the social network were compromised using phishing attacks.
Compromised accounts can  ood spam email through the RE system, since their spam bypasses  lters and directly reaches user s inboxes.
Our RE simulation measures the portion of the entire user population receiving spam as we increase the number of compromised accounts.
We perform these experiments on our Facebook social graphs, and plot the results for Santa Barbara, Egypt, and New York in Figure 5, and list Euclidean distance values for all 4 graphs in column 7 of Table 4.
Comparing results across all graphs, Nearest Neighbor produces the overall best results, with dK-2 and Random Walk as the next best models.
It is notable that the best model varies across our graphs, perhaps due to speci c structural features in each of the Facebook graphs.
One takeaway from this experiment is that application level results cannot be easily explained using a single graph metric.
In general, the accuracy of the RE experiment s performance on a synthetic graph is not strongly correlated with any of the metrics we track in Table 4.
Sybilguard.
A malicious user in an online community can launch a Sybil attack [10] by creating a large number of virtual identities.
These identities can then work together to provide the owner with some unfair advantage, by outvoting legitimate users in consensus systems, corrupting data in distributed storage systems, or manipulating incentive systems or reputation systems to perform fraud.
SybilGuard [36] proposes a way to detect these Sybil identities using social networks.
The main insight of this defense relies on the fact that it is dif cult to make multiple social connections between Sybil identities and legitimate users.
Because of this concrete obstacle, Sybil identities tend to form a strongly connected subgraph with a small number of links to honest users.
Using Sybilguard, a node A seeking to determine if node B is a Sybil identity sends a number of random walks.
Node B does
 Time property of graphs.
We cannot con rm this, since current Mixing Time algorithms do not scale to large graphs.
Monterey Bay 6,283 nodes 33,969 edges Santa Barbara 12,814 nodes 92,241 edges Egypt 246,692 nodes 1,618,085 edges New York 377,712 nodes 3,616,873 edges Models dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert Euclidean Distance: Target vs.
Synthetic Graphs Graph Metrics Applications

























 Knn








































































 Sybil.
Table 4: Euclidean distances between model-generated graphs and the original graphs for several graph metrics and application benchmarks.
Each point is the average of 20 synthetic to original graph comparisons.
For each metric, the value with the lowest error is underlined and in bold, while the second best model is underlined.
Overall, Nearest Neighbor is consistently accurate for most metrics.
dK-2 is highly accurate for node degree distribution (NDD) and joint node degree distribution (knn), but not for clustering coef cient (CC) and application benchmarks.
Exact metric values Diam.
Path Leng.
Graphs Monterey Bay dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert Santa Barabara dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert Egypt dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert New York dK-2 Nearest Neighbor Random Walk KronFit Forest Fire Barabasi-Albert






 -0.03





 -0.021





 -0.009





 -0.006























































 Table 5: Comparing the original graphs and their synthetic counterparts w.r.t.
assortativity, network diameter and average path length.
Results shown are the exact metric value, and results for synthetic graphs are averages over 20 graphs.
the same, and A records the number of intersections between these random walks.
If B is a Sybil identity, it is likely to be in a local subgraph with a small number of paths to A, resulting in a small number of walk intersections.
Otherwise, the number of intersections will be high.
The rate of success depends on the length of the random walks.
If the walks are too short then they might not intersect, and A would have less information about B.
Our experiment looks at the portion of random walks that result in intersections as a function of the length of random walks.
Looking at the resulting plots in Figure 6 and Table 4, we see that Nearest Neighbor again performs very well, producing the most accurate synthetic graphs for 3 of the 4 target graphs.
We note that the simple Barabasi-Albert model, which consistently produces inaccurate graphs (measured by graph metrics), actually performs relatively well in the Sybilguard tests with Euclidean distances on par or even lower than other models.
Again, our results reinforce the idea that application-level benchmarks do not easily map to known graph metrics, and they must be included in any attempts to understand the  delity of graph models.
Social shields for anonymous communication.
Puttaswamy et al. propose using social neighborhoods to protect users of anonymous communication protocols against passive logging attacks [30].
Most anonymous routing protocols provide anonymity by forwarding traf c through a random sequence of relay nodes.
In practice, however, malicious relays that observe traf c in the network over long periods can probabilistically guess the identity of the communication source.
To protect themselves, a communication source in the proposed system  rst relays traf c through a random sequence of friend nodes, such that any passive logging attack will not be able to distinguish it from its friend nodes.
The solution provides the strongest protection when the user is in a large clique in the social network [30].
Our experiment measures the size of the largest clique each user is a part of.
Again, this application exploits a graph property that is not captured by any of the previously analyzed metrics.
The closest related metric is the clustering coef cient, which quanti es the level of connectivity within each user s one-hop neighborhood.
Figure 7 shows two interesting results.
First, we see that dK-2 consistently failed to capture the formation of larger cliques in its synthetic graphs.
This is somewhat intuitive, since dK-2 captures only joint degree distribution, and not the clustering coef cient.
Given its poor correlation with the clustering coef cient (Figure 4), it is clear that dK-2 forms fewer and smaller cliques than the other models.
We assume that if a graph generator existed for the dK-3 model, it would do a much better job of capturing clustering coef- cients as well as clique properties.
Second, our results for Santa Barbara show that the Forest Fire model produces clique properties most similar to the original graph.
This is due to the large number of local connections that Forest Fire introduces with each new node.
However, this heavy local clustering signi cantly skews other metrics, making Forest Fire the least accurate of all our models for both the Sybilguard and RE tests.
In fact, Forest Fire produces so many local edges that our relatively ef cient maximal clique search algorithm failed to complete on Forest Fire graphs modeled after the Egypt and New York graphs.
For each of these graphs, our algorithm takes more than 2.5 weeks to produce a result.
In comparison, i m a p
 g n v e c e
 s r e s
 f o % )


 ( s n o i t c e s r e t n
 f o %




















 Barabasi Albert Random Walk dK-2 Real Data Nearest Neighbor KronFit Forest Fire





 % of Users Spamming (a) Santa Barbara i i m a p
 g n v e c e
 s r e s
 f o %










 Barabasi Albert Random Walk Nearest Neighbor dK-2 Real Data KronFit Forest Fire





 % of Users Spamming (b) Egypt i i m a p
 g n v e c e
 s r e s
 f o %










 Barabasi Albert Random Walk Nearest Neighbor KronFit dK-2 Real Data Forest Fire





 % of Users Spamming (c) New York Figure 5: Penetration of spam in a social graph running RE as function of number of spammers in the network.
Nearest Neighbor dK-2 Random Walk Real Data KronFit Barabasi Albert Forest Fire




 Walk Length


 (a) Santa Barbara




 )


 ( s n o i t c e s r e t n
 f o %








 )


 ( s n o i t c e s r e t n
 f o %


 Forest Fire dK-2 Nearest Neighbor Real Data Random Walk Barabasi Albert KronFit

 Walk Length


 (b) Egypt Forest Fire Random Walk Nearest Neighbor dK-2 Barabasi Albert Real Data KronFit





 Walk Length (c) New York Figure 6: Portion of all random walks resulting in intersections in Sybilguard, as a function of random walk length.
our clique search algorithm running on other models for Egypt and New York all completed in less than 30 minutes.
Thus, while we expect Forest Fire to outperform other models for the clique test in Egypt and New York, its large errors in RE and Sybilguard make it unsuitable for our purposes.
Nearest Neighbor, instead, provides a more consistent accuracy both across statistical metrics and application level results, which is also con rmed in this experiment.
Although it is not extremely precise in the number of maximum cliques, it performs consistently well over multiple datasets.
Interaction Graphs.
We have also investigated another dataset, the  interaction graph  from the New York region, as proposed in [35].
An interaction graph is a social graph in which edges that do not receive interactions between the two endpoints are culled.
In our case, interactions are de ned as wall posts or photo comment activity.
Intuitively, this process removes unimportant edges from the graph, leaving only active edges that are relevant when designing and testing  user-driven  applications.
The interaction graph for NY has 254599 nodes and 926165 edges (a 75% reduction in edges compared to the full social graph).
We do not present detailed results regarding the accuracy of the models on this graph, because they lead us to the same conclusions derived from other analyzed datasets.
For instance, both clustering coef cient and degree distribution of the interaction graph look extremely similar to those of the full social graph for Egypt.
In general, dK-2 and Nearest Neighbor consistently produce the best results on the interaction graph.
Final Considerations.
A  nal takeaway from our tests is that despite signi cant variance in model accuracy, we  nd that Nearest Neighbor consistently outperforms its competitors in producing synthetic graphs that not only capture the majority of known graph metrics, but also accurately predict the performance of application-level tests such as RE, Sybilguard, and Social Shields.
Based on our graph metric and application-level tests, we conclude that the Nearest Neighbor model is a viable candidate for researchers looking to replace real graphs with model-generated graphs.
Trace-driven models.
Trace-driven network models are popular in research areas where active measurements are dif cult to perform, including wireless networks [16], mobile networks [15], and Internet backbone traf c [6].
Researchers continue to rely on synthetic model-generated traf c traces for experimental research, even as they recognize and continue to reduce the inherent error introduced by these models [15].
OSN measurements.
Several important measurement studies of online social networks helped derive and shape the key graph metrics we use in our study.
Some of them focus on static properties by looking at data sets collected at a single time point [2, 3, 25], while others investigate dynamic properties from a series of data sets over time [18, 20, 26].
These studies found a collection of remarkable properties such as the power-law scaling characteristics, the small-world phenomena, and clustered community structures.
Graph similarity.
A number of techniques have been proposed to quantify graph similarity, including graph isomorphism, edit distance [24], common subgraphs and supergraphs, and statistical measurements of graph structure.
We chose to use a statistical approach for our study because most of the alternative methods were computationally intractable for our large graph datasets.
We began this work as a search for practical solutions to challenges we faced while distributing measured social graphs to colleagues in the research community.
While experiments using trace-driven models are common in the study of both wired and wireless networks, an analogous approach has not been applied to research on social graphs.
It became clear to us that measurement-calibrated graph modeling faced a number of challenges due to the inherent complexity and scale of graphs.
Our most important contributions are proposing this approach to experimental research on social graphs, identifying inherent challenges, and proposing a number of simple but feasible solutions.
( s r e s
 f o %









 KronFit Barabasi Albert dK-2 Random Walk Nearest Neighbor Forest Fire Real Data
 Maximal Clique Size (a) Santa Barbara )


 ( s r e s
 f o %










 Barabasi Albert KronFit dK-2 Random Walk Nearest Neighbor Real Data
 Maximal Clique Size (b) Egypt )


 ( s r e s
 f o %










 KronFit Barabasi Albert dK-2 Random Walk Nearest Neighbor Real Data
 Maximal Clique Size (c) New York Figure 7: CDF graphs showing the size of the largest clique each user belongs to in the Monterey Bay, Santa Barbara, and New York graphs.
Through empirical experimentation, we  nd that structure-driven models such as dK and Kronecker are limited by high computational and memory complexity.
The most consistently accurate model is our modi ed version of Nearest Neighbor, which despite its simple algorithm, manages to successfully capture key graph metrics of the original graphs.
It also produces generally accurate results in our application-level tests, making it a viable candidate for researchers looking to replace real graphs with model-generated graphs.
We conclude that graph models can be adequate replacements for real social graphs, and that current graph metrics cannot completely capture properties used by social network applications.
More work needs to be done to further validate and expand these initial  ndings.
A logical next step is to investigate these applications to better understand the properties they rely on for success, and if these properties correspond to yet unknown graph metrics.
We also need to verify our conclusions using simulations of more social applications.
Finally, more work needs to be done to minimize the operational overheads of structure-driven models such as dK.
Then we will learn if, given suf cient computational resources, they can generate the most representative synthetic graphs.
Acknowledgments.
We thank Jure Leskovec for sharing code and guidance on calibrating graphs in the Kronecker and Forest Fire models, and the anonymous reviewers for their helpful comments.
This work is supported in part by NSF Grants CNS-0916307, IIS-
and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect the views of the National Science Foundation.
