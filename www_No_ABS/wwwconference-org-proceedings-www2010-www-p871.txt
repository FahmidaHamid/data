The World Wide Web made information dissemination much easier and more e cient, from communication and commerce to content.
In particular, not only did more o ine content come online and access to it made easier, but generation and publication of content has become easier.
Millions of users publish blogs, self-made videos, and post comments or reviews of products and businesses such as hotels, restaurants, and others.
Users also post and answer questions, tag pictures and maps, form and nurture social networks, etc.
This giant publication system works by implicit understanding that content should be appropriate (e.g., avoid porn), legal (e.g., no illegally copied content), believed to be correct (e.g., when tagging maps and answering questions factually), or respectful of others privacy (e.g., with blogging and social networking), etc.
; explicitly, these are enforced by Terms of Service that gives the platform provider the right to remove content, cancel access, report to police, etc.
However, for a variety of reasons   nonprofessional users, monetary incentives, lack of cost to action, etc.
  such user-generated content is always suspect.
The challenge is how to identify inappropriate content at the Internet scale where millions of pieces of content being generated every day.
While sophisticated algorithms that explicitly identify inappropriate content are used in cases (e.g., with  nding copyright violations), a common solution has been to rely on the community, i.e., other users, to identify and  ag  inappropriate content.
We refer to such systems where users  ag or report inappropriate content as negative feedback systems.
Exam-or YouTube comments2.
There are many other examples.
These systems should be distinguished from others that rely on user participation, in particular those that identify popular items based on user feedback.
These use various voting schemes to tap into the wisdom of crowds and aggregate positive feedback, for say ranking content.3 Such ratings systems rely on the fact that single popular item can score thousands of ratings.
In contrast, negative feedback systems are expected to take action quickly, in some cases even after a single  ag on certain content.
The community-based policing in negative feedback systems faces a di erent challenge.
How do the systems know when a user s  ag is genuine and something to be acted upon?
Flags may be simply incorrect due to an error on the user s part (mistaken about an answer to a query), or maliciousness ( ag and remove a good review for a competitor), or even cluelessness; in some cases, there may be revenge  ags.
Known systems solve this problem by manually evaluating some or all the  ags.
There are two underlying assumptions here.
First, humans can identify a correct  ag.
Second, human testing of  ags is more scalable than human testing of the original contents.
These are reasonable assumptions in many applications.
As an example, consider YouTube comments: humans can readily spot incorrect  ags of spam, and the number of  ags is several orders of magnitude smaller than the number of either user-generated videos or comments.4 However, as the systems grow in size, even testing of all  ags becomes prohibitively expensive and becomes prone to denial of service style attacks.
The challenge is then reduced to a tradeo : number of  ags tested by humans versus the number of incorrect  ags the system misses.
Testing all the  ags by humans will be prohibitive but detect all incorrect  ags, and testing none will allow far too many incorrect  ags.
In this paper, we study this tradeo .
Consider a set of items I, and a user u who generates a sequence of  ags i1, i2, .
.
.
, iN .
These  ags correspond to the items ij that the user u deems abusive.
The  ag could either be true or false.
A  ag ij is True means that the item ij violates Terms of Service; a false  ag indicates that the item is not abusive and user committed an error (whether honestly or maliciously) when reporting it.
For each  ag, the monitoring algorithm performs one of the three actions, A = {accept, reject, test}, and the outcome is S = {accepted, rejected, positive, negative}.
The  rst two states correspond to the case when algorithm accepted the report and took appropriate action against the item (say, removed or demoted it) or rejected the  ag (did not perform any action against content) without any further testing.
The last two correspond to the case where the algorithm chooses to perform external, human-based testing and discovered
 views.
Introduction-Algorithms-Third-Thomas-Cormen/dp/

 where reported spam comments are hidden by default.
 agged by any user, there could potentially be false negatives.
This, however, is a reasonable outcome, since it indicates that the content is not seen by too many users.
re-http://www.amazon.com/ http://www.youtube.com/watch?v=4TpRAp0WWLs This  e.g.,  Report link in product the true state of the  ag.
If the algorithm chooses to test, depending on the outcome of the test the system will either accept or reject the item.
We further assume that an item is not  agged multiple times; in fact, multiple  ags are seldom seen in our applications and can be handled as a sequence of independent, single  ags.
In fact as we discuss later, very few  ags per item, is a crucial di erence between our system and traditional reputation systems.
Formally,   User strategy is a function U : A    r   {true, false} that takes as an input vector A  of past actions of the monitoring algorithm, a random vector r, and generates the next  ag.
  The monitoring algorithm R : S r   A is a function that takes as an input the vector S  of past outcomes, a random vector r and returns the action for the current  ag.
Interestingly, we do not assume any structure between items (such as, two videos were posted by the same user) or model the correlation between items with  ags (such as, reviews in poor language tend to get  ags, or videos with DJ mixes get fewer  ags).
Our approach here is to focus on users alone, and ignore the signals from the content.
This may be seen as analogous to web search where link analysis, without content analysis, gives lot of information [5, 9].
The additional motivation for us is that focusing on user analysis makes our model applicable across content types (be they video, review text or others) and general.
Finally, notice that the monitoring algorithm has no access to user strategy and it only learns something about user when it decides to test the  ag, and not in other actions.
This is in contrast with standard learning with experts framework where algorithm learns something on each action or step.
Say user u places N  ags.
Given a randomized monitoring algorithm A we measure:   tA u (N ), the expected number of tests performed by algorithm A on the sequence of N  ags by user u, and   eA u (N ), the expected number of errors   an incorrect  ag that is accepted or a correct  ag that is rejected   by the algorithm for user u.
To the best of our knowledge this is the  rst e ort to formalize the problem of monitoring negative feedback.
More formally, we study the tradeo  between tA u (N ).
  Our main contribution is the design of a randomized algorithm called Adaptive Probabilistic Testing (AP T ) to process  ags in real time as they are being submitted by users, and a detailed theoretical analysis of it.
We prove that AP T satis es the following: u (N ) and eA u   [Adversarial Users] eAP T (N )    N in expectation against any user u and any  xed  , 0       1.
The user strategy could be adversarial and user can observe actions of the monitoring algorithms once they are performed and adjust his strategy arbitrarily.
  [Standard Users] We denote by STD(p) a standard user who errs with some probability p independently on each  ag.
Let OPT be the optimal algorithm for monitoring such a user with STD(p)(N )    N .
We show that for APT algorithm we have: tAP T STD(p)(N )   4 tOPT STD(p)(N ) + o(N ).
in other words, the adaptive testing algorithm performs within constant factor of the best possible algorithm for that particular type of user.
  We present an experimental study of our algorithm with synthetic and real data collected from various Google systems.
Our algorithm satis es eu(N )    N almost always (not only in expectation), and at the same time, behaves signi cantly better than our theoretical analysis, more like tAP T STD(p)(N ) + o(N ).
STD(p)(N )   tOPT Thus the framework we use for evaluating any monitoring algorithm involves two properties: (1) the number of errors should be bounded in all cases including adversarial users, and simultaneously, (2) in a system where overwhelming majority of the users are nonmalicious, the monitoring algorithm should perform almost as well as the best possible algorithm that satis es (1).5 It is our experience that practitioners do need both of the properties above.
Systems need to be robust when it comes to spammers, but also graceful for majority of users who tend to be honest.
Notice that it is not trivial for a monitoring algorithm to satisfy these two properties simultaneously.
For example, a naive approach would be to test a user at the beginning to see if the user is standard, determine p and then thereafter run OPT for user STD(p).
This however will not satisfy the  rst property because a strategy available to a user is to pretend to be STD(0) at the beginning (e.g.
never lie) and then switch to STD(1).
In fact, our algorithm is far more graceful: if an adversarial user becomes standard only for certain consecutive number of  ags and is arbitrary elsewhere, our algorithm will automatically behave like OPT for most of that portion when user is standard.
We do not formally abstract this property for the various portions, and only focus on the two properties above over the entire sequence of  ags.
Our model is very general and can be applied to many real world negative feedback systems.
Because of its generality however it is reminiscent of many other problems.
In particular it is reminiscent of  online label prediction problems , where the task is to predict the label on each item.
But typically in such problems, there is an underlying well-structured class of hypotheses and errors are measured with respect to best in this class.
In contrast, users may have arbitrary strategy in our problems, and no structured class of hypotheses may  t their behavior.
In multiarmed bandit [2], expert learning setting with limited feedback [6], apple-tasting [7], and other learning problems, for each online item, one is given the correct label for all experts or arms after the action is performed, but in our problem, we obtain the correct label only when we test an item.
Hence our monitoring algorithms have to work more agnostically.
Our approach is also reminiscent of large class of  reputation problems  where a user s reputation is measured based on his agreement with other users or item quality and is used as weight to measure how much the system values user feedback.
Reputation problems arise in user driven systems such as Wikipedia [1, 4] and others [3, 8].
Such reputation schemes do not apply directly to negative feedback systems, since our systems do not have opportunity to adjust a users  errors based on other users.6 Despite the plethora of work in related areas, and practical motivations, to the best of our knowledge there is very little work done in the area of negative feedback systems.
In fact, the only work we are aware of is empirical paper by Zheleva et al [10] that considers a problem of computing trust score of email users reporting spam/non-spam.
Their results considers a community-based scoring and a  xed scoring function and present empirical results, however, their algorithm provides no guarantees against malicious users.
Approaching our problem from the  rst principles, it is immediately clear that the monitoring algorithm has to use randomization to determine what  ags to test.
The algorithm we design is simple and natural, keeping a testing probability pi that is adjusted based on feedback from tests, up or down based on whether tests reveal correct or incorrect  ags.
The main di culty is its analysis because of the dependence of the state of the algorithm to the entire trajectory of testing probabilities and user strategies.
In particular, the analysis of expected behavior relies on careful upper bounding the hitting times of sequences of Bernoulli trials with stopping probabilities pi1   pi2   pil .
.
.
.
The stopping probability itself changes non-deterministically, in fact in uenced by user strategy, over time in such a way that there is no explicit upper bound on expectation.
Instead, we rely on expectations of hitting times conditioned on the fact that stopping probability stays above some  xed value, and then generalize the results.
In this section we describe our algorithm and provide intuition why it works; we defer full analysis to Section 3.
We begin by presenting a monitoring algorithm which only performs two actions: test and accept.
This algorithm has applications of its own such as when leaving abusive content after it was reported is unacceptable for legal reasons.
Similarly, a monitoring algorithm which either rejects or tests has applications of its own in systems where accidentally removing content bears high cost.
Later we generalize these two algorithms into an universal monitoring algorithm with at most  1N false positive (accepts erroneous  ags) and at most  2N false negative errors (ignores correct  ags).
The high level idea behind the algorithm is as follows.
When  ag i arrives, the algorithm  ips a pi-biased coin and tests the  ag with probability pi and accepts it otherwise.
After each  ag, the new probability pi+1 is computed.
The crux is to determine the update rule for the consequent of pi s.
A naive approach would be to set p1 =   = pk = 1 for some k, and use the fraction of incorrect  ags out of k as pj for j > k. However, if user changes strategy after k  ags, this method will fail.
A di erent approach is to  x some window size w and test a fraction in each window i to estimate the number ni of incorrect  ags, and use ni/w
 STD(p)(N ) = c( , p)N tests if p >   and O(1) otherwise.
This is far fewer than what an optimal algorithm needs for an adversarial user.
it will no longer be equally visible to other users, and thus likely to become uncorrectable.
changes strategy between windows, this method will fail and make far too many errors.
Thinking about this further, one will realize that we need a more  exible way to combine testing probability, and knowledge from each testing.
Our approach is based on the following simple observation   if we test  ags with probability p, then upon discovery of a false  ag, the expected number of missed false  ags is 1 p p , independently of user strategy.
Indeed, since each false  ag is tested with probability p, the expected stopping time on a sequence of false  ags is 1 p p .
Thus, intuitively if between two consecutive discoveries of false  ags the testing probability was in the range [p , p ] then we have both lower and upper bound on the expected number of missed  ags in between as 1 p  .
The formal proof of this statement is not obvious since the actual probability p  is changing non-deterministically and is in fact not bounded away from zero in advance.
A technical achievement in our paper is to indeed develop this idea formally as in Theorem 3.1, but this intuition su ces.
Using this, we can keep track of approximately how many false items we have accepted (e.g.
number of false positive errors), and thus can chose new pi in such a way so that we satisfy the constraint on the number of false positives in expectation.
See Algorithm 1 for the full details.
p  and 1 p  p Algorithm 1 Test-Accept Algorithm Input: A stream of  ags.
Output: For each  ag we either accept or test it.
If the  ag is tested the algorithm learns its true state Description:
 false skipped  ags at L = 0;
 (a) Test  ag with probability pi and accept otherwise.
(b) If  ag is tested and the  ag is false, set L   L + 1 pi pi (c) Set the new testing probability pi+1   1  i+1 L .
Test-accept and test-reject cases are symmetric (with default action accept action being replaced by reject, and in step 2b, L increases if the  ag is true).
For completeness Algorithm 2, provides full details on test-reject algorithm.
Combining Two Cases.
The idea behind test-accept-reject algorithm is that we just run test-accept and test-reject algorithms in parallel, with only one of them being active and producing the next action.
After every step, both algorithms advance one step, and we reset active algorithm to the one which has lower probability of testing.
The complete algorithm is given on Figure 3.
We  rst analyze errors by the monitoring algorithm against an adversarial user; later, we analyze the number of tests it performs against a standard user.
Algorithm 2 Test-Reject Algorithm Input: A stream of  ags.
Output: For each  ag we either reject or test it.
If the  ag is tested the algorithm learns its true state Description:
 true skipped  ags at L = 0;
 (a) Test  ag with probability pi and reject otherwise.
(b) If  ag is tested and it is true, set L   L + 1 pi (c) Set the new testing probability pi+1   1  i+1 L .
pi Algorithm 3 Adaptive Probabilistic Testing algorithm Input: Stream of  ags, constants  1,  2 Output: For each  ag, output test, accept or reject Description: Let A and B be the test-accept and test-reject algorithms respectively.
For each  ag, the algorithms A and B are run in parallel to produce probabilities pAi and pBi .
If pAi < pBi , set algorithm A as active, B as passive.
Else, set B as active and A as passive.
Active algorithm  ips a coin performs its action and updates its state.
Passive algorithm is executed to update its pi, but the suggested action is not performed.
We begin by analyzing the test-accept algorithm.
For each  ag this algorithm tests each  ag with certain probability and accepts it otherwise.
Thus the only type of error admitted is false positives, where algorithm accepts a false  ag.
Intuitively, how many undetected false  ags there are between two detected ones?
We begin by estimating the run length until the  rst detected  ag, if the testing probabilities is some non-increasing sequence {pi}.
Lemma 3.1.
Let {ri} be a sequence of Bernoulli trials with parameter pi, where {pi} is monotonically non-for increasing sequence, and pi itself can depend on rj, j < i.
Let Q   [0, ] be the hitting time for the sequence {r0, r1, .
.
.}.
In other words random variable Q is equal to the  rst index i, such that ri = 1.
Then for any  , we have the expectation bound: E [Q|pQ    ]   (1   )/  and E [Q|pQ]   (1  pQ)/pQ (1) and further the realizations are concentrated around the expectation: Pr [Q > c/  |pQ    ]   e c and Pr [Q > c/pQ]   e c (2) Proof.
Consider sequence r( ) i = ri if pi     and is Bernoulli trial with probability   otherwise.
And suppose Q( ) is a hitting time for r( ) , such that r( ) i .
Then i EhQ( )i = EhQ( )|pQ    i Pr [pQ    ] + EhQ( )|pQ <  i Pr [pQ <  ]   EhQ( )|pQ    i f2 = 3 f3 = 4 f4 = 6 f5 = 7 True False False False True False False Tested Accepted Accepted R1 = 2 (2 missed false  ags) Tested g1 = 4 Accepted Accepted R2 = 1 (1 missed  ag) Tested g2 = 7 f6 = 9 False Tested g3 = 9 True Tested
 Figure 1: A sequence of  ags.
Gray  ag indicates that the  ag was not tested and its true state is unavailable to the algorithm.
fi indicates indices of all false  ags (both discovered and not), gi indicates realization of indices of discovered false  ags.
Ri is a realization of random variable  the number of undiscovered  ags between two sequential gi s .
where in the  rst transition we used the linearity of expectation and in the second we used the fact that for any  xed sequence P = {pi}, EhQ( )|pQ    , Pi < EhQ( )|pQ    , Pi.
On the other hand Q( ) and Q are equal to each other if pQ    .
Thus, we have similarly for probabilities we have: E [Q|pQ    ] = EhQ( )|pQ    i   EhQ( )i Pr Q     |pQ         |pQ     = Pr Q( )     Pr Q( )   c c c Now we just need to show EhQ( )i   (1    )/  and PrhQ( )   c i   e c.
Observe that Q( ) can be upper bounded by geometric random variable G   0 with parameter  .
Indeed, let us suppose gi is 1 with probability min{1,  /pi} if r( ) i = 1, and is 0 otherwise.
Unconditionally each gi is 1 with probability  .
Thus, hitting time G for {gi} is a geometric random variable, and by de nition G   Q.
Since expectation of G is (1   )/  we have the  rst part of our lemma.
The second part of equation (1) follows from the de nition of conditional expectation.
To prove the equation (2), we note that Pr [G > c/ ] = (1    ) c   +1   e c since it is exactly the probability that a sequence of Bernoulli trials with identical probability   does not hit 1 after c   steps.
Since Q( )   G in the entire space, we have the desired bound.
Theorem 3.2.
For the test-accept algorithm, the expected number of errors eu(N )    N for an adversarial user u.
Proof.
We count the expected number of undetected false positives so far after we test the ith  ag.
The crux is to consider the underlying sequence of false  ags and corresponding testing probability, and hide all the true  ags inside the probability changes pi and apply lemma 3.1.
Suppose the false  ags have occurred at positions f1, f2 .
.
.
fl.
We do not know what those fi are, but our goal is to show that for any sequence the desired lower bounds holds.
Denote ri a random variable that indicates whether i-th false  ag has been accepted without testing.
In other words ri is a sequence of bernoulli trials each occurring with probability 1   pfi .
Consider g0, g1, ...gl  where g0 = 0, and gi is an index of the ith detected false  ag.
In other words {gi} is a random subsequence of {fi} where algorithm detected false  ag.
Note that while fi are unknown, gi are the steps of the algorithm where we test and discover false  ags and thus are known.
Let Ri denote a random variable that is equal to the number of false  ags between  ags gi 1 and gi.
We illustrate all the notation we used with an example on Figure rfj .
And thus i=1 rfi , therefore it is su cient for us to esti-
Pl  i=1 Ri = Pl mate EhPl  i=1 Rii.
Note that Ri is a hitting time for the sequence of pgi 1 , .
.
.
pgi , where the sequence if over hidden false  ags pgi is not bounded a-priori.
Since our algorithm does not increase testing probability if it does not detect false  ags by Lemma 3.1 E [Ri|pgi ]   1   pgi pgi .
Further, note that for  xed pgi the expectation is bounded independently of all earlier probabilities and therefore: l  Xi=1 E [Ri|pg0 .
.
.
pgi ] =   l  l  Xi=1 Xi=1 E E [Ri|pgi ] pg1 , .
.
.
pgi 1  1   pgi pgi    N.
where the last transition follows from the step 2c of Algorithm 1, where we have pgi =  (gi 1)+1 L and thus 1 pgi .
Hence
 1 pgj pgj j=1 pgi    gi   Lgi , where Lgi = Pi Pl  i=1 To  nish the proof: 1 pgi pgi    N.
l  Xi=1 l  E [Ri] = E2 Xi=1
 E [Ri|pg1 ]3
 l  Xi=2 E [Ri|pg1 ]3

 E [R1|pg1 ] +



 E [Ri|g1, .
.
.
gi]3 Xi=1

 pgi 3
 1   pgi

 Xi l  l  l  Xi=3 E [Ri|pg1 , pg2 ]3
 Similarly to above, the same results apply to the test-reject algorithm.
Combining these two results together we have: Theorem 3.3.
For AP T monitoring algorithm the expected number of false positives is at most  2N and the expected number of false negatives is at most  2N .
Indeed, lets A denote the set of items where test-Proof.
accept algorithm was active, and let B denote the set of items where test-reject algorithm was active.
During the B phase, the test-accept algorithm did not produce any mistake (since no item was accepted), thus the expected number of errors still test-accept is  1N .
More formally, we de ne Ri as aa hitting times, of detecting false  ags, with an extra constraint that the algorithm must have been active, and the analysis carries through.
In this section, we consider standard users.
Recall that for a standard user, each  ag is incorrect with some unknown probability pu.
This models two dimensions about users in negative feedback systems.
First, even genuine users err sometimes, but it is not correlated across items, and hence we assume it is with some  xed, independent, but unknown probability pu.
Second, some of non-malicious users might be clueless and err; in such cases, the error probability pu is again considered independently random, not correlated with the items.
We abstract these as STD(p) users.
Note that p may be small as in the  rst case or large as in the second case.
What is the minimum number of tests we need to perform to guarantee at most  1N of false positive?
Since the user is random the only parameters we can tune are the number of tests T , the number of accepts A and the number of rejects R with the goal of minimizing T , since it does not matter which  ags got tested: T + A + R = N, Ap    1N, R(1   p)    2N, min T Thus if p    1 then we can accept all the  ags and not do any testing.
On the other hand if p   1    2, then we can reject all  ags and again not perform any testing.
In the general case it can be shown that the total fraction of tested  ags will be at least 1    1
 we get the total fraction of  ags that needs to be tested is at least p 1 and if  1 = 0 it becomes 1 p 2 p    2 .
p We now analyze the behavior of our algorithm and show that for a standard user the algorithm is competitive with 1 p respect to the optimal algorithm described above.
Equivalently, we prove that if p     then the expected number of tests is o(N ) and if p     then it is bounded by 4 OPT.
As empirical evaluation in Section 4 shows, the analysis below is very likely to be not tight, and providing a tighter constant is an interesting open problem.
p Theorem 3.4.
For a STD(p) user with N  ags, each false with probability p, the test-accept algorithm performs   tests if p    , and  N + c tests oth-in expectation  N erwise, where   = 4 p  and c and   are O(1).
Similarly p if p   1     test-reject algorithm performs at most  N and 4 1 p  1 p  
 Proof.
Suppose our target is   fraction of errors.
It is easy to see that the algorithm can be reformulated as follows.
At step i test with probability 1+ i , and every time the item is tested, the probability of testing resets back to 1 with probability p. The question then becomes what is the expected number of tests we will perform?
The full proof is given in the appendix.
Finally, we analyze the performance of the AP T algorithm.
Theorem 3.5.
Consider STD(p) a user u with N  ags.
The number of tests performed by the AP T algorithm is at most 4 OP T + 2 max( 1,  2)N + o(N ).
Proof.
The total number of tests is the lesser of the number of tests performed by either of the test-accept and test-reject algorithms in isolation.
Thus it is su cient to only consider  1   p   1    2, otherwise, by Theorem 3.4 the expected number of tests is o(N).
For the latter case we have, the expected number of tests is: tAP T ST D(p)(N )   4n min(1    1 p , 1    2 1   p ).
(3) ST D(p)(N )   N (1   1 If p   1/2 , the number of tests performed by the optimal
 algorithm is tOP T p  2 2).
Similarly, for p   1/2 the number of tests is bounded by: ST D(p(N )   N (1   1 tOP T 1 p  2 1), combining these two inequalities with equation (3) we have the desired result.
p    2 1 p )   n(1   1 p    2

 In this section we perform two kinds of experiments.
First, on synthetic data, we compare our algorithm with the optimal algorithm which knows user strategy in advance.
The primary goal here is to show that not only the algorithm performs only constant times as optimal, but to further demonstrate that the constant is very close to 1.
Second, we present results from running our algorithm on real data consisting of abuse reports submitted to several Google properties.
An important observation here is that since our algorithms are not using the actual content in any way, the only important quantity is the number of reports per user.
In all our experiments we assume that acceptable error level for either false positive or false negative type of errors is 0.1.
Synthetic data.
We  rst demonstrate the performance (number of tests and number of errors) of the algorithm against standard users.
To achieve this we plot the optimal s t s e t / s r o r r e f o r e b m u







 Number of tests performed by the algortihm Minimum number of tests required False negatives False positives








 y t i l i b a b o r







 Testing probability User probability of error
 False-positive-error-rate False-negative-error-rate




 The probability of error by the user number of flags reported (a) The number of tests and the number of errors admitted by AP T algorithm when user happens to be STD(p) vs that by the optimal algorithm that knows p.
(b) Performance of the AP T algorithm for STD(p) user that changes p over time.
number of tests for  xed acceptable error   = 0.1 and p changing from 0.01 to 1 for STD(p) against average number of tests performed by AP T .
For each p we assume 1000  ags and run the experiments 30 times, to get accurate estimate of the expected number of tests performed.
The results are presented in Figure 3(a).
It is clear that our algorithm is in fact much closer to the optimal than the theoretical analysis above suggests, and tAP T STD(p)(N ) + o(N ).
STD(p)(N ) is more like tOPT On the Figure 3(b) we consider a user who changes his error probability over time.
The step-function with extreme values between 0 and 1 is user real error rate.
The best optimal test rate (if the algorithm knew the underlying constant), is step function bounded by 0.5.
The line, closely following the latter, shows the testing probability for the AP T algorithm when STD(p) user keeps changing p. It is clear that AP T algorithm automatically adjusts its testing rate nicely to be nearly close to the best testing for STD(p) for whatever p the user uses for a period of time.
Experiments with real data.
In this section we present experimental results that we perform using real data, which contains a subset of abuse reports accumulated by various Google services over the period of time of about 2 years.
The dataset contained roughly about 650 randomly selected anonymized users who submitted at least 50 reports to the system (some submitted considerably more).
Their total contribution was about 230,000  ags.
The algorithm computed testing probability independently for every user, and thus our guarantees apply for every user independently.
Our goal was to measure to average testing rate as a function of the total number of  ags, since it translates to immediate reduction of amount of manual labor required.
On Figure 3(c) we plot the number of total  ags arrived into system (blue curve), vs the total number of tested  ags (green curve).
The bottom three curves show the actual fraction of admitted errors vs the acceptable error levels.
To illustrate the actual fraction of tested  ags we refer to Figure 3(d).
As one can see the testing ratio in general hovers around 0.35, which means that only roughly 1 in every 3 user  ags needs to get tested, and the remainder can be acted on automatically.
We described a simple model for monitoring negative feedback systems, and presented the AP T algorithm with expected number of errors    N for even adversarial users; for a standard user STD(p), the expected number of tests is close to that of the optimal algorithm for STD(p).
Practitioners look for such algorithms that are resistant to adversary users, while still being gracefully e cient to standard users.
We have found these algorithms useful for some of the Google systems.
u From a technical point of view, the immediate open question is if our analysis of AP T algorithm can be improved since our experiments indicate tAP T STD(p)(N ) behaves more like tOPT STD(p)(N ) + o(N ).
Further, could we extend the expected case analysis in this paper to high concentration bounds?
We are able to analyze the AP T algorithm and show that the eAP T (N ) is within twice the expectation with overwhelming probability for any user, under certain mild conditions.
This result appears in Appendix B.
We need to use martingale inequalities to prove these bounds, but they require bounded di erence, whereas hitting times of our algorithm are not bounded.
We overcome this problem by suitably modifying Azuma s inequality.
This analysis may be of independent interest.
Similar high concentration bounds for tAP T STD(p)(N ) would be of interest.
From a conceptual point of view, negative feedback systems are abundant on the Internet, and we need more research on useful monitoring algorithms.
For example, an extended model is to consider items having attributes and typical user error as some function of those attributes that we need to learn.
Similarly considering standard (e.g.
non malicious) user whose behavior evolves over time is also important.
A potential  rst step in this direction is to compare the performance of a monitoring algorithm with the optimal (but not necessarily spam resistant) algorithm that is required to have g a l f #










 Total flags Flags tested Missed Positives Missed Negatives Acceptable Error Level






 l a t o t / d e t s e t














 days days (c) The number of  ags that the system tested vs.
number of  ags vs. the number of errors the total (d) The ratio between number of tested  ags and the total number of  ags as a function of time Figure 3: Performance of the algorithm on  ags received from real users.
bounded number of error on every pre x sequence of user  ags for users that change their failure probability gracefully over time.
Another potentially interesting direction which has practical justi cation is to allow monitoring algorithms to take retroactive actions such as deciding to test an item which was accepted or rejected earlier.
Finally, a rich direction is to not consider each user individually, but group them according to their past history of reports.
This allows a reduction in the amount of testing for users who provide only a few  ags since such users can contribute a signi cant fraction of  ags in many real-world systems.
Such a grouping can be dynamic and depend on users  strategies as well as other properties, and guarantees will be relative to the group.
We hope our work here spurs principled research on monitoring algorithms for negative feedback systems that are much needed.
Authors would like to thank Nir Ailon, Raoul-Sam Daruwala, Yishay Mansour and Ruoming Pang and anonymous reviewers for interesting discussion and good feedback on the paper.
