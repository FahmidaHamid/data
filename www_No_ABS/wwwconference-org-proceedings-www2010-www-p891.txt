The rapidly increasing popularity and data volume of modern Web 2.0 content sharing applications is based on their ease of operation even for unexperienced users, suitable mechanisms for supporting collaboration, and attractiveness of shared annotated material (images in Flickr, bookmarks in del.icio.us, etc.).
For video sharing, the most popular site is YouTube1.
Recent studies have shown that tra c to/from 1http://www.youtube.com Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Figure 1: Comments and Comment Ratings in YouTube this site accounts for over 20% of the web total and 10% of the whole internet [3], and comprises 60% of the videos watched online [11].
YouTube provides several social tools for community interaction, including the possibility to comment published videos and, in addition, to provide ratings about these comments by other users (see Figure 1).
These meta ratings serve the purpose of helping the community to  lter relevant opinions more e ciently.
Furthermore, because negative votes are also available, comments with o ensive or inappropriate content can be easily skipped.
The analysis of comments and associated ratings constitutes a potentially interesting data source to mine for obtaining implicit knowledge about users, videos, categories and community interests.
In this paper, we conduct a study of this information with several complementary goals.
On the one hand, we study the viability of using comments and community feedback to train classi cation models for deciding on the likely community acceptance of new comments.
Such models have direct application to the enhancement of comment browsing, by promoting interesting comments even in the absence of community feedback.
On the other hand, we perform an in-depth analysis of the distribution of comment ratings, including qualitative and quantitative studies about Can we predict the community feedback for comments?
Is there a connection between sentiment and comment ratings?
Can comment ratings be an indicator for polarizing content?
Do comment ratings and sentiment depend on the topic of the discussed content?
These are some of the questions we investigate in this paper by analyzing a large sample of comments from YouTube.
Clearly, due to the continuing and increasing stream of comments in social sharing environments such as YouTube, the community is able to read and rate just a fraction of these.
The methods we present in this paper can help to automatically structure and  lter comments.
Analyzing the ratings of comments for videos can provide indicators for highly polarizing content; users of the system could be provided with di erent views on that content using comment clustering and aggregation techniques.
Furthermore, automatically generated content ratings might help to identify users showing malicious behavior such as spammers and trolls at an early stage, and, in the future, might lead to methods for recommending to an individual user of the system other users with similar interests and points of views.
The rest of this paper is organized as follows: In Section 2 we discuss related work on user generated content, product reviews and comment analysis.
Section 3 describes our data gathering process, as well as the characteristics of our dataset.
In Section 4 we analyze the connection between sentiment in comments and community ratings using the SentiWordNet thesaurus.
We then provide a short overview of classi cation techniques in Section 5, explain how we can apply these techniques to rate comments, and provide the results of large-scale classi cation experiments on our YouTube data set.
In Section 6 we analyze the correspondence between comment ratings and polarizing content through user experiments.
Section 7 describes dependencies of ratings and sentiments on topic categories.
We conclude and show directions for future work in Section 8.
There is a body of work on analyzing product reviews and postings in forums.
In [4] the dependency of helpfulness of product reviews from Amazon users on the overall star rating of the product is examined and a possible explanation model is provided.
 Helpfulness  in that context is de ned by Amazon s notion of how many users rated a review and how many of them found it helpful.
Lu et al. [17] use a latent topic approach to extract rated quality aspects (corresponding to concepts such as  price  or  shipping ) from comments in ebay.
In [27] the temporal development of product ratings and their helpfulness and dependencies on factors such number of reviews or e ort required (writing review vs. just assigning a rating) are studied.
The helpfulness of answers on the Yahoo!
Answers site and the in uence of variables such as required type of answer (e.g.
factual, opinion, personal advice), topic domain of the question or  priori e ect  (e.g.
Did the inquirer some apriori research on the topic?)
is manually analyzed in [12].
In comparison, our paper focuses on community ratings for comments and discussions rather than product ratings.
Work on sentiment classi cation and opinion mining such as [19, 25] deals with the problem of automatically assigning opinion values (e.g.
 positive  vs.  negative  vs.  neutral ) to documents or topics using various text-oriented and linguistic features.
Recent work in this area makes also use of SentiWordNet [5] to improve classi cation performance.
However, the problem setting in these papers di ers from ours as we analyze community feedback for comments rather than trying to predict the sentiment of the comments themselves.
There is a plethora of work on classi cation using probabilistic and discriminative models [2] and learning regression and ranking functions [24, 20, 1].
The popular SVM Light software package [14] provides various kinds of parameteri-zations and variations of SVM training (e.g., binary classi- cation, SVM regression and ranking, transductive SVMs, etc.).
In this paper we will apply these techniques in a novel context to automatic classi cation of comment acceptance.
Kim et al. [15] rank product reviews according to their helpfulness using di erent textual features and meta data.
However, they report their best results for a combination of information obtained from the star ratings (e.g.
deviation from other ratings) provided by the authors of the reviews themselves; this information is not available for all sites, and in particular not for comments in YouTube.
Weimer et al. [26] make use of a similar idea to automatically predict the quality of posts in the software online forum Nab-ble.com.
Liu et al [16] describe an approach for aggregation of ratings on product features using helpfulness classi ers based on a manually determined ground truth, and compare their summarization with special  editor reviews  on these sites.
Another example of using community feedback to obtain training data and ground truth for classi cation and regression can be found in our own work [22], for an entirely di erent domain, where tags and visual features in combination with favorite assignments in Flickr are used to classify and rank photos according to their attractiveness.
Compared to previous work, our paper is the  rst to apply and evaluate automatic classi cation methods for comment acceptance in YouTube.
Furthermore, we are the  rst to provide an in-depth analysis of the distribution of YouTube comment ratings, including both qualitative and quantitative studies as well as dependencies on comment sentiment, rating di erences between categories, and polarizing content.
We created our test collection by formulating queries and subsequent searches for  related videos , analogously to the typical user interaction with the YouTube system.
Given that an archive of most common queries does not exist for YouTube, we selected our set of queries from Google s Zeitgeist archive from 2001 to 2007, similarly to our previous work [23].
These are generic queries, used to search for web pages.
In this way, we obtained 756 keyword queries.
In 2009, for each video we gathered the  rst 500 comments (if available) for the video, along with their authors, times-tamps and comment ratings.
YouTube computes comment ratings by counting the number of  thumbs up  or  thumbs down  ratings, which correspond to positive or a negative votes by other users.
In addition, for each video we collected meta data such as title, tags, category, description, upload date as well as statistics provided by YouTube such as overall number of comments, views, and star rating for the video.
The complete collection used for evaluation had a  nal size of 67, 290 videos and about 6.1 million comments.
c n e u q e r














 N. Comments Figure 2: Distribution of Number of Comments per Video 3.5e+06 3e+06 2.5e+06 2e+06 1.5e+06 1e+06

 <-10 -10 9 8 7 6 5 4 3 2 -1









 Figure 3: Distribution of comment ratings Figure 2 shows the distribution of the number of comments per video in the collected set.
The distribution follows the expected zip an pattern, characterized by having most of the energy contained within the  rst ranked elements as well as subsequent long tail of additional low-represented elements, valid for most community provided data.
For our collection, we observe a mean value of  comm = 475 comments per video, with ratings ranging from  1, 918 to 4, 170 for a mean value of  r = 0.61.
Figure 3 shows the distribution of comment ratings.
The following two main observations can be made: On the one hand, the distribution is asymmetric for positive and negative ratings, indicating that the community tends to cast more positive than negative votes.
On the other hand, comments with rating 0 represent about 50% of the overall population, indicating that most comments lack votes or are neutrally evaluated by the community.
Preliminary Term Analysis.
The textual content of comments in Web 2.0 infrastructures such as YouTube can provide clues on the community acceptance of comments.
This is partly due to the choice of words and language used in di erent kinds of comments.
As Table 1: Top-50 terms according to their MI values for accepted (i.e. high comment ratings) vs. not accepted (i.e. low comment ratings) comments Terms for Accepted Comments favorit her hot my d love song best amaz beauti awesom voic rock she thank lol xd lt cute luv perfect wish perform hilari miss most gorgeou omg brilliant nice legend bless ador music sexi fantast heart man greatest time sweet jame talent feel avril wonder janet danc absolut watch Terms for Unaccepted Comments shut gui im jew comment die cock name asshol read ur dont ugli dick better fag white fake black faggot fuck suck u gai shit stupid bitch ass nigger hate game fat kill idiot dumb retard bad know don sorri fuckin worst y pussi crap de cunt bore loser look an illustrative example we computed a ranked list of terms from a set of 100,000 comments with a rating of 5 or higher (high community acceptance) and another set of the same size containing comments with a rating of 5 or lower (low community acceptance).
For ranking the terms, we used the Mutual Information (MI) measure [18, 28] from information theory which can be interpreted as a measure of how much the joint distribution of features Xi (terms in our case) deviate from a hypothetical distribution in which features and categories ( high community acceptance  and  low community acceptance ) are independent of each other.
Table 1 shows the top-50 stemmed terms extracted for each category.
Obviously many of the  accepted  comments contain terms expressing sympathy or commendation (love, fantast, greatest, perfect).
 Unaccepted  comments , on the other hand, often contain swear words (retard, idiot) and negative adjectives (ugli, dumb); this indicates that o ensive comments are, in general, not promoted by the community.
Do comment language and sentiment have an in uence on comment ratings?
In this section, we will make use of the publicly available SentiWordNet thesaurus to study the connection between sentiment scores obtained from SentiWord-Net and the comment rating behavior of the community.
SentiWordNet [9] is a lexical resource built on top of Word-Net.
WordNet [10] is a thesaurus containing textual descriptions of terms and relationships between terms (examples are hypernyms:  car  is a subconcept of  vehicle  or synonyms:  car  describes the same concept as  automobile ).
WordNet distinguishes between di erent part-of-speech types (verb, noun, adjective, etc.)
A synset in WordNet comprises all terms referring to the same concept (e.g.
{car, automobile}).
Terms Corresponding to Positively Rated Comments










 Positivity Terms Corresponding to Negatively Rated Comments Terms Corresponding to Positively Rated Comments y c n e u q e r
 y c n e u q e r





























 Negativity Figure 4: SentiValue histograms for term lists according to MI In SentiWordNet a triple of three senti values (pos, neg, obj) (corresponding to positive, negative, or rather neutral sentiment  avor of a word respectively) are assigned to each WordNet synset (and, thus, to each term in the synset).
The sentivalues are in the range of [0, 1] and sum up to 1 for each triple.
For instance (pos, neg, obj) = (0.875, 0.0, 0.125) for the term  good  or (0.25, 0.375, 0.375) for the term  ill .
Sen-tivalues were partly created by human assessors and partly automatically assigned using an ensemble of di erent classi- ers (see [8] for an evaluation of these methods).
In our experiments, we assign a sentivalue to each comment by computing the averages for pos, neg and obj over all words in the comment that have an entry in SentiWordNet.
A SentiWordNet-based Analysis of Terms.
We want to provide a more quantitative study of the interrelation between terms typically used in comments with high positive or negative ratings.
To this end, we selected the top-2000 terms according to the MI measure (see previous section) for positively and negatively rated comments, and retrieved their sentivalue triples (pos, neg, obj) from Senti-WordNet if available.
Figure 4 shows the histograms of sentivalues for these terms.
In comparison to terms corresponding to positively rated comments, we can observe a clear tendency of the terms corresponding to negatively rated comments towards higher negative sentivalue assignments.
Sentiment Analysis of Ratings.
Negativity Sentivalue













 Objectivity Sentivalue


 y c n e u q e r
 y c n e u q e r
 y c n e u q e r































 Positivity Sentivalue Figure 5: Distribution of comment negativity / objectivity / positivity the highest accuracy in SentiWordNet for these.
Our intuition is that the choice of terms used to compose a comment may provoke strong reactions of approval or denial in the community, and therefore determine the  nal rating score.
For instance, comments with a high proportion of o ensive terms would tend to receive more negative ratings.
We used comment-wise sentivalues, computed as explained above, to study the presence of sentiments in comments according to their rating.
To this end, we  rst subdivided the data set into three disjoint partitions:   5Neg: The set of comments with rating score r less or equal to 5, r    5.
  0Dist: The set of comments with rating score equal to 0, r = 0.
  5Pos: The set of comments with rating score greater or equal to 5, r   5.
Now we describe our statistical comparison of the in u-ence of sentiment scores in comment ratings.
For our analysis, we restricted ourselves to adjectives as we observed We then analyzed the dependent sentiment variables positive, objective and negative for each di erent partition.
Detailed comparison histograms for these sentiments are shown e u a
 n a e
 Negativity Positivity






 Partition
 Figure 6: Di erence of Mean values for sentiment categories in Figure 5.
These  gures provide graphical evidence of the intuition stated above.
Negatively rated comments (5Neg) tend to contain more negative sentiment terms than positively rated comments (5Pos), re ected on a lower frequency of sentivalues at negativity level 0.0 along with consistently higher frequencies at negativity levels   0.1.
Similarly, positively rated comments tend to contain more positive sentiment terms.
We also observe that comments with rating score equal to 0 (0Dist) have sentivalues in between, in line with the initial intuition.
We further analyzed whether the di erence of sentivalues across partitions was signi cant.
We considered comment positivity, objectivity and negativity as dependent variables.
Rating partition (5Neg, 0Dist, 5Pos) was used as the independent variable (grouping factor) of our test.
Let us denote  k s the mean value for sentiment s   {N, O, P } (negativity, objectivity and positivity respectively) for partition k   {5N eg, 0Dist, 5P os}.
Our initial null hypothesis states that the distribution of sentiment values does not depend on the partition states, i.e. the mean value of each independent variable is equal across partitions H0 :  5N eg =  0Dist .
The alternative hypothesis Ha states that the di erence is signi cant for at least two partitions.
We then used three separate one-way ANOVA (Analysis of Variance) procedures [6], a statistical test of whether the means of several groups are all equal, to verify the null hypothesis, H0, for each variable negativity (FN ), objectivity (FO) and positivity (FP ).
=  5P os s s s We selected a random sample of 15, 000 comments.
From this, we discarded comments for which sentiment values were unavailable in SentiWordNet, resulting in a  nal set of 5047 comments.
All tests resulted in a strong rejection of the null hypothesis H0 at signi cance level 0.01.
Figure 6 shows the di erence of mean values for negativity and positivity, revealing that negative sentivalues are predominant in negatively rated comments, whereas positive sentivalues are predominant in positively rated comments.
The ANOVA test does not provide information about the speci c mean values  k s that refuted H0.
Many di erent post-hoc tests exist to reveal this information.
We used the Games-Howell [6] test to reveal these inter-partition mean di erences because of its tolerance for standard deviation heterogeneity in data sets.
For negativity, the following homogeneous groups were found: { {5Neg}, {0Dist, 5Pos} }.
Finally, for positivity the following homogeneous groups were found: { {5Neg}, {0Dist}, {5Pos} }.
These results provide statistical evidence of the intuition that negatively rated comments contain a signi cantly larger number of negative sentiment terms, and similarly for positively rated comments and positive sentiment terms.
Can we predict community acceptance?
We will use support vector machine classi cation and term-based representations of comments to automatically categorize comments as likely to obtain a high overall rating or not.
Results of a systematic and large-scale evaluation on our YouTube dataset show promising results, and demonstrate the viability of our approach.
Our term and SentiWordNet-based analysis in the previous sections indicates that a word-based approach for classi- cation might result in good discriminative performance.
In order to classify comments into categories  accepted by the community  or  not accepted , we use a supervised learning paradigm which is based on training items (comments in our case) that need to be provided for each category.
Both training and test items, which are later given to the clas-si er, are represented as multi dimensional feature vectors.
These vectors can, for instance, be constructed using tf or tf   idf weights which represent the importance of a term for a document in a speci c corpus.
Comments labeled as  accepted  or  not accepted  are used to train a classi cation model, using probabilistic (e.g., Naive Bayes) or discrimina-tive models (e.g., SVMs).
How can we obtain su ciently large training sets of  accepted  or  not accepted  comments?
We are aware that the concept is highly subjective and problematic.
However, the amount of community feedback in YouTube results in large annotated comment sets which can help to average out noise in various forms and, thus, re ects to a certain degree the  democratic  view of a community.
To this end we considered distinct thresholds for the minimum comment rating for comments.
Formally, we obtain a set {( ~c1, l1), .
.
.
( ~cn, ln)} of comment vectors ~ci labeled by li with li = 1 if the rating lies above a threshold ( positive  examples), li =  1 if the rating is below a certain threshold ( negative  examples).
Linear support vector machines (SVMs) construct a hy-perplane ~w  ~x+b = 0 that separates a set of positive training examples from a set of negative examples with maximum margin.
For a new previously unseen comment ~c, the SVM merely needs to test whether it lies on the  positive  side or the  negative  side of the separating hyperplane.
We used the SVMlight [14] implementation of linear support vector machines (SVMs) with standard parameterization in our experiments, as this has been shown to perform well for various classi cation tasks (see, e.g.,[7, 13]).
We performed di erent series of binary classi cation experiments of YouTube comments into the classes  accepted  and  not accepted  as introduced in the previous subsection.
For our classi cation experiments, we considered di erent levels of restrictiveness for these classes.
Speci cally, we considered distinct thresholds for the minimum and maximum ratings (above/below +2/-2, +5/-5 and +7/-7) for comments to be considered as  accepted  or  not accepted  by the community.
We also considered di erent amounts of randomly chosen  accepted  training comments (T = 1000, 10000, 50000, AC_NEG - Rating Threshold: 5 T: 50000 THRES-0 - Rating Threshold: 5 T: 50000 i i n o s c e r













 i i n o s c e r













 i i n o s c e r













 Recall Recall Recall Figure 7: Comment Classi cation: Precision-recall curves (50000 training comments per class, rating 5) Table 2: Comment Classi cation Results (BEPs)  unaccepted  comments are shown in Figure 7.
The main observations are:















 Rating   2 Rating   5 Rating   7






 -


-
Rating   2 Rating   5 Rating   7






 -


-
Rating   2 Rating   5 Rating   7











 200000) as positive examples and the same amount of ran domly chosen  unaccepted  comments as negative samples (where that number of training comments and at least 1000 test comments were available for each of the two classes).
For testing the models based on these training sets we used the disjoint sets of remaining  accepted  comments with same minimum rating and a randomly selected disjoint subset of negative samples of the same size.
We performed a similar experiment by considering  unaccepted  comments as positive and  accepted  ones as negative, thus, testing the recognition of  bad  comments.
We also considered the scenario of discriminating comments with a high absolute rating (either positive or negative) against unrated comments (rating = 0).
The three scenarios are labeled AC POS, AC NEG, and THRES-0 respectively.
Our quality measures are the precision-recall curves as well as the precision-recall break-even points (BEPs) for these curves (i.e. precision/recall at the point where precision equals recall, which is also equal to the F1 measure, the harmonic mean of precision and recall in that case).
The results for the BEP values are shown in Table 2.
The detailed precision-recall curves for the example case of T=50000 training comments class and thresholds +5/-5 for  accepted /   All three types of classi ers provide good performance.
For instance, the con guration with T=50,000 posi-tive/negative training comments and thresholds +7/-7 for the scenario AC POS leads to a BEP of 0.7208.
Consistently, similar observations can be made for all examined con gurations.
  Trading recall against precision leads to applicable results.
For instance, we obtain prec=0.8598 for re-call=0.4, and prec=0.9319 for recall=0.1 for AC POS; this is useful for  nding candidates for interesting comments in large comment sets.
  Classi cation results tend to improve, as expected, with an increasing number of training comments.
Furthermore, classi cation performance increases with higher thresholds for community ratings for which a comment is considered as  accepted .
In this section, we will study the relationship between comment ratings and polarizing content, more speci cally tags/topics and videos.
By  polarizing content  we mean content likely to trigger diverse opinions and sentiment, examples being content related to the war in Irak or the presidential election in contrast to rather  neutral  topics such as chemistry or physics.
Intuitively, we expect a correspondence between diverging and intensive comment rating behavior and polarizing content in Youtube.
Variance of Comment Ratings as Indicator for Polarizing Videos.
In order to identify polarizing videos, we computed the variance of comment ratings for each video in our dataset.
Figure 8 shows examples of videos with high versus low rating variance (in our speci c examples videos about an Iraki girl stoned to death, Obama, and protest on Tiananmen Square in contrast to videos about The Beatles, cartoons, and amateur music).
To show the relation between comment ratings and polarizing videos, we conducted a user evaluation of the top and bottom-50 videos sorted by their variance.
These 100 videos were put into random order, and evaluated by 5 users on a 3-point Likert scale (3: polarizing, 1: rather neutral, 2: in between).
The assessments of the variance of comment ratings for the corresponding videos presidential campaign xbox 3g jew betting skybus butter lent menorah High comment rating variance nomination muslim station barack kiss space shakira grassroots zac deals hamas itunes efron obama shark Low comment rating variance turns peanut chanukah daylight casserole tmx f-18 savings snowboard 1040ez puckett defender form egan 1040a islam nice iraq celebrities kiedis tropical vlog iditarod havanese booklet


 Videos in YouTube belong to a variety of categories such as  News & Politics ,  Sports  or  Science .
Given that different categories attract di erent types of users, an interesting question is whether this results in di erent kinds of comments, discussions and feedback.
In order to study the in uence of categories on the classi cation behavior, we conducted a similar experimental series as described in section 5.
In the following paragraphs, we describe the results of classi cation of YouTube comments into the classes  accepted  and  not accepted  as introduced in the previous subsection.
In each classi cation experiment we restricted our training and test sets to comments from the same class.
We used smaller training sets than in section 5 as we had less comments available per category than for the overall dataset.
Figure 9 shows the precision-recall curves as well as the break-even-points (BEPs) for comment classi cation for the con guration T=10,000 training documents and threshold +5/-5 for accepted/unaccepted comments.
We observe that training and classifying on di erent categories leads to clear di erences in classi cation results.
While classi ers applied within the categories  Music  and  Entertainment show comparable performance, the performance drops for for  News & Politics .
This might be an indicator for more complex patterns and user relationships for that domain.
categories In this section we consider the analysis of comment rating distribution across di erent categories.
Our intuition is that some topics are more prone to generate intense discussions than others.
Di erences of opinion will normally lead to an increasing number of comments and comment ratings, a ecting the distribution.
Figure 10 shows the distribution of comment ratings for a set of selected categories from our subset.
We observe several variations for the di erent categories.
For instance, science videos present a majority of 0-scored comments, maybe due to the impartial nature of this category.
Politics videos have signi cantly more negatively rated comments than any other category.
Music videos, on the other hand, have a Figure 8: Videos with high (upper row) versus low variance (lower row) of comment ratings di erent users were averaged for each video, and we computed the inter-rater agreement using the  measure [21], a statistical measure of agreement between individuals for qualitative ratings.
The mean user rating for videos on top of the list was 2.085 in contrast to a mean of 1.25 for videos on the bottom (inter-rater agreement   = 0.42); this is quite a high di erence on a scale from 1 to 3, and supports our hypothesis that polarizing videos tend to trigger more diverse comment rating behavior.
A t-test con rmed the statistical signi cance of this result (t= 7.35, d.f.
= 63, P < 0.000001).
Variance of Comment Ratings as Indicator for Polarizing Topics.
We were also studying the connection between comment ratings and video tags corresponding to polarizing topics.
To this end we selected all tags from our dataset occurring in at least 50 videos resulting in 1, 413 tags.
For each tag we then computed the average variance of comment ratings over all videos labeled with this tag.
Table 3 shows the top and bottom-25 tags according to the average variance.
We can clearly observe a higher tendency for tags of videos with higher variance to be associated with more polarizing topics such as presidential, islam, irak, or hamas, whereas tags of videos with low variance correspond to rather neutral topics such as butter, daylight or snowboard.
There are also less obvious cases an example being the tag xbox with high rating variance which might be due to polarizing gaming communities strongly favoring either Xbox or other consoles such as PS3, another example being f-18 with low rating variance, a  ghter jet that might be discussed under rather technical aspects in YouTube (rather than in the context of wars).
We quantitatively evaluated this tendency in a user experiment with 3 assessors similar to the one described for videos using the same 3-point Likert scale and presenting the tags to the assessors in random order.
The mean user rating for tags in the top-100 of the list was 1.53 in contrast to a mean of 1.16 for tags on the bottom-100 (with inter-rater agreement   = 0.431), supporting our hypothesis that tags corresponding to polarizing topics tend to be connected to more diverse comment rating behavior.
The statistical signi cance of this result was con rmed by a t-test (t=4.86, d.f.
= 132, P = 0.0000016).
Category Music - Rating Threshold: 5, T: 10000 Category Politics - Rating Threshold: 5, T: 10000 i i n o s c e r













 i i n o s c e r













 i i n o s c e r













 Recall Recall Recall Figure 9: Classi cation Precision-Recall Curves for Multiple Categories Enterntainment Music Politics People Science y c n e u q e r








 7 6 5 4 3 2 -1







 Comment Rating Figure 10: Distribution of comment ratings for di erent categories clear majority of positively rated comments.
Mean rating score values for all categories in our database are shown in Figure 11.
r =  j We further analyzed whether the rating score di erence across categories was signi cant.
We considered comment ratings as the dependent variable, and categories as the grouping factor.
Let us denote  i r the mean rating score value for category i.
We wanted to refute hypothesis H0 :  i r,   i, j (i.e. comment ratings mean value is identical for all categories).
Our alternative hypothesis Ha states that at least two categories, i and j, feature mean rating scores that are statistically di erent.
We used one-way ANOVA to test the validity of the null hypothesis.
For this experiment we considered the complete data set, excluding comments with 0 ratings and no assigned category, for a total of 2, 539, 142 comments.
The test resulted in a strong rejection of the hypothesis H0 at signi cance level 0.01, providing evidence that mean rating values across categories are statistically di erent.
A subsequent post-hoc Games-Howell test was conducted to study pairwise di erences between categories.
Table 4 shows the homogeneous groups found.
The table identi- es category  Music  as having signi cantly higher comment ratings than any other, and categories  Autos&Vehicles ,  Gaming  and  Science  having signi cantly lower comment ratings.
While some categories are likely to be a ected by the lack of comment ratings ( Science ), the signi cantly lower comment ratings in some categories like  Gaming  g n i t a
 n a e












 u sic n t e e o
 il m


 o d e
 r a rt p l e a i n -m
 -
m e d y n i m u c w v a ti o s -n
 o e n t l o g s a ti o n s n c e
 e h icl e s







 o n e t s p r o -
t o o h p a w o o rt s m i n w s g u t o ci e e l -litic s fit s
 v e -
n t s -
n i m c a ls tivis t yl e m Category Figure 11: Mean Rating Score per Comment for di erent Categories might indicate that malign users (trolls, spammers, .
.
. )
are more dominant in these categories than in others.
In Section 4 we provided statistical evidence of the dependency of comment ratings on sentivalues.
In this section we extend the analysis to also consider categories, to check whether we can  nd a dependency of sentivalues for di er-Homogeneous Category Groups Highest Mean Music Medium Pets&Animals, Comedy, Education Mean Lowest Mean Entertainment, News&Politics Nonpro ts&Activism, Sports People&Blogs, Shows Travel&Events, Howto&Style Autos&Vehicles, Gaming, Science g n i t a
 n a e









 Negativity Objectivity Positivity













 h o o o n m e o e o d w w e t s p o n t e ci e a m u t o h o w s p r o p l e e d y s -
-fit s -
u c t o -litic s a ti o
 n t yl e
 o l o g s -
rt s n i m a ls rt n c i n a i n m e g -
e n t e c h
 il
 u m sic -
w s s -
e h icl e n i m a ti o ent categories, and provide additional ground to the claims presented in Section 7.2.
c tivis m s n n o l o g y Category r r =  K,j r r We proceeded similarly to Section 7.2.
In this case, we considered sentivalue negativity, objectivity and positivity as dependent variables, and categories as the grouping factor.
We denote  N,i the mean negativity value for category i. Analogously,  O,i and  P,i r denote mean objectivity and positivity values for category i.
We wanted to refute hypothesis H0 :  K,i ,   i, j, K   {N, O, P } (i.e. comment ratings mean value is identical for all categories).
Our alternative hypothesis Ha states that at least two categories, i and j, feature mean values that are statistically di erent.
We used three one-way ANOVA procedures to test the validity of the null hypothesis.
For this experiment we considered the complete data set, excluding comments for which senti-values were not available, for a total of 2, 665, 483 comments.
The test resulted in a strong rejection of the hypothesis H0 at signi cance level 0.01 for the three cases, providing evidence that mean sentivalues across categories are statistically di erent.
Figure 12 shows mean values for sentiments negativity, objectivity and positivity for di erent categories.
Results are in agreement with  ndings of Section 7.2 (Table 4 and Figure 11).
For instance, music exhibits the lowest negativity sentivalue and the highest positivity sentivalue.
Our interpretation of these results is that di erent categories tend to attract di erent kinds of users and generate more or less discussion as a function of the controversy of their topics.
This clearly goes along with signi cantly di er-ent ratings and sentivalues of comments associated to videos.
As a result, user generated comments tend to di er widely across di erent categories, and therefore the quality of clas-si cation models gets a ected (illustrated in section 7.1).
We conducted an in-depth analysis of YouTube comments to shed some light on di erent aspects of comment ratings for the YouTube video sharing platform.
How does community feedback on comments depends on language and sentiment expressed?
Can we learn models for comments and predict comment ratings?
Does comment rating behavior depend on topics and categories?
Can comment ratings be an indicator for polarizing content?
These are some of the questions we examined in this paper by analyzing a sample of more than 6 million YouTube comments and ratings.
Large-scale studies using the SentiWordNet thesaurus and YouTube meta data revealed strong dependencies between di erent kinds of sentiments expressed in comments, comment ratings provided by the community and topic orientation of the discussed video content.
In our classi cation Figure 12: Distribution of comment sentivalues experiments, we demonstrated that community feedback in social sharing systems in combination with term features in comments can be used for automatically determining the community acceptance of comments.
User experiments show that rating behavior can be often connected to polarizing topics and content.
Regarding future work, we plan to study temporal aspects, additional stylistic and linguistic features, relationships between users, and techniques for aggregating information obtained from comments and ratings.
We think that temporal aspects such as order and timestamps of comments and upload dates of commented videos can have a strong in u-ence on commenting behavior and comment ratings, and, in combination with other criteria, could help to increase the performance of rating predictors.
More advanced linguistic and stylistic features of comment texts might also be useful to build better classi cation and clustering models.
Finally, comments and ratings can lead to further insights on di erent types of users (helpful users, spammers, trolls, etc.)
and on social relationships between users (friendship, rivalry, etc).
This could, for instance, be applied for identifying groups of users with similar interest and recommending contacts or groups to users in the system.
We think that the proposed techniques have direct applications to comment search.
When searching for additional information in other users  comments, automatically predicted comment ratings could be used as an additional ranking criterion for search results.
In this connection, integration and user evaluation within a wider system context and encompassing additional complementary retrieval and mining methods is of high practical importance.
This work was supported by EU FP7 integration projects LivingKnowledge (Contract No.
231126) and GLOCAL (Contract No.
248984) and the Marie Curie IOF project  Mieson .
