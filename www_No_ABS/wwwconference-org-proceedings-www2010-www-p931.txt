Search engines return ranked lists of documents or websites in response to a query, with the precise forms of the ranked lists depending on the internal mechanisms of the engines.
We consider the problems of comparing and visualizing the similarity relationships between di erent search Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
algorithms.
The term search algorithm is an intentionally vague term corresponding to a mechanism for producing ranked lists of websites in response to queries.
We focus on the following three interpretations of search algorithms: (a) di erent search engines, (b) a single search engine subjected to di erent query manipulation techniques by the user, and (c) a single engine queried across di erent internal states.
A visualization of relationships among type-(a) search algorithms may be useful as it reveals which search engines users should or should not use.
For example, two search engines that output very similar ranked lists may provide redundant information to the user.
On the other hand, two search engines that output very dissimilar lists are worth examining more closely.
We emphasize that we do not consider here the issue of search quality or relevance.
The techniques developed in this paper allow users to understand the similarity relationships among search algorithms.
Evaluating retrieval quality is a separate, well-studied problem that is beyond the scope of this paper.
Visualizing relationships among type-(b) search algorithms is useful as it indicates which query manipulation techniques are worth using.
For example, a query times square may be manipulated by the user (prior to entering it in the search box) to times AND square.
Such manipulations may result in di erent ranked lists output by the search engine.
Understanding the similarities between these ranked lists may provide the user with insights into which manipulations are redundant and which are worth exploring further.
Understanding the relationships among type-(c) search algorithms is useful from the point of view of design and modi cation of search engines.
Search engines are complex programs containing many tunable parameters, each one in uencing the formation of the ranked list in a di erent way.
For example, commercial search engines frequently update the internal index which may result in di erent ranked lists output in di erent dates (in response to the same query).
It is important for both the users and the search engineers to understand how much the ranked lists di er across consecutive days and how much this di erence changes as internal parameters are modi ed.
There are several techniques for visualizing complex data such as search algorithms.
One of the most popular techniques is multidimensional scaling (MDS) [3], which transforms complex high dimensional data s1, .
.
.
, sm into 2-D vectors z1, .
.
.
, zm that are easily visualized by displaying them on a 2-D scatter plot.
Assuming that a suitable dissimilarity measure between the high dimensional data  (si, sj ) has been identi ed, MDS computes the 2-D embedding of the high dimensional data si 7  zi   R2, i = 1, .
.
.
, m that R(z1, .
.
.
, zm) = X i<j ( (si, sj )   kzi   zjk)2 .
(1) In other words, the coordinates z1, .
.
.
, zm in R2 corresponding to s1, .
.
.
, sm are selected to minimize the distortion (z1, .
.
.
, zm) = arg min R(z  1, .
.
.
, z  m).
(2) z  1,...,z  m Variations of MDS with slightly di erent distortions (1) and objective functions (2) may be found in [3].
Our contributions in this paper are (1) to develop a suitable dissimilarity function for search algorithms  (si, sj) and study its properties, (2) to examine the use of MDS in the context of visualizing search algorithms of types (a), (b), and (c), and (3) to validate it using synthetic and web data.
The analysis of search engine outputs is a large area of research within the IR community.
Most approaches evaluate rankings output by search engines against human judgment or some other ground truth [8, 10, 21, 20].
In this area, Carterette [4] recently enumerated the limitations of Kendall s tau for comparing system performance.
He proposed an alternative method for computing rank distance that accounts for dependency between items, which is computed as the solution to a minimization problem.
The resulting measure, however, has problems with interpretation and is highly dependent on the number of systems and queries being analyzed, and is used instead as input to a p-value di erence estimator for comparing system performance.
In general, increased industry and research interest in measuring dissimilarity between di erent search engines has lead to a variety of comparison tools1.
The key for comparing and visualizing ranked lists output by search engines is to de ne an appropriate distance metric.
A straightforward way to measure such distance is to compute the overlap of the two lists [13].
Such an approach is problematic, however, as it is invariant with respect to reordering or the ranked lists.
Popular distance measures between permutations are Kendall s tau, Spearman s rho, the footrule, Ulam s distance, and Cayley s distance [5].
There are several ways to extend these permutation distances to partially ranked lists output by search engines, including Hausdor  distance [5] and expected distances [1, 15].
Substantial research on the interaction between users and search engines [9, 11] shows that users  attention drops quickly from top to bottom ranks2.
One problem with many proposed dissimilarities, however, is that they do not distinguish between disagreement at top rankings and at the bottom rankings.
One exception is [7] who considers stage-wise ranking processes that generalize Kendall s tau.
This generalization, however, is not unique as it depends on the order in which the di erent ranking stages are selected.
Rank correlation coe cient such as NDCG [10] adopts inverse logarithm function as the discount rank factor.
However it is not symmetric and is intended as an evaluation measure against ground truth, not a comparison measure between ranked lists.
Another example is the inverse measure [2] that em-1e.g.www.bing-vs-google.com, www.searchrater.com
 other discount functions have been proposed as well.
phasizes disagreement at top ranks, where the weight function decays linearly with the rank.
While much previous work on visualization of search algorithms is based on document similarity, e.g.
[17], there is renewed interest in visualizing and analyzing set-level di er-ences between results from di erent search systems.
Fagin et al. [6] and Bar-Ilan et al. [2] investigate the relationship among engines by examining the pairwise distance matrix but do not make the connection with visualization.
Liggett and Buckley [14] used multidimensional scaling over ranking dissimilarity to examine search system variations due to the e ect of query expansion, where the dissimilarity was based on Spearman s coe cient.
In addition, tools like MetaCrystal [19] regard search results as a set of items and visualize the common items among di erent engines.
Temporal studies of search engines have been examined by [2] and [20] who compare search engine results across multiple time periods.
As mentioned in Section 1, e ective visualization of search algorithms using MDS depends on the quality of the dissimilarity measure  .
We describe in this section a new measure based on the expectation of the weighted Hoe ding distance on permutations and examine its properties.
We start by considering several desired properties for  (si, sj).
It should be (i) symmetric, (ii) interpretable with respect to search algorithms retrieving ranked lists of di erent lengths, (iii)  exible enough to model the increased attention users pay to top ranks over bottom ranks, (iv) computationally ef- cient, and (v) aggregate information over multiple queries in a meaningful way.
The symmetry property (i) is relatively straightforward.
Property (ii) addresses an important and often overlooked issue.
How should ranks in a short ranked list be compared with ranks in a long one?
How should we count websites that appear only in the longer (or shorter) ranked list?
Many previously proposed dissimilarity measures provide ad-hoc answers to these questions which may lower the quality of the MDS embedding.
Property (iii) refers to the di erences in attention users pay to websites listed in top vs. bottom ranks.
The dissimilarity measure should take this into account and provide a dissimilarity similar to that experienced by users, as opposed to a rank-symmetric formula.
The ef- ciency property (iv) can be critical for online use and we address that in Sec.
3.2.
Property (v) refers to the fact that  (si, sj) should aggregate information from multiple queries with each query contributing the  correct  amount to the  nal dissimilarity  (si, sj).
Dissimilarity functions examined in previous studies satisfy some but not all of these properties (see Figure 2).
In this paper we propose to de ne   using an expectation over the weighted Hoe ding distance.
The expectation and the properties of the weighted Hoe ding distance provide a clear probabilistic interpretation and ensure that properties (i)-(v) are satis ed.
We start by de ning the weighted Hoe ding distance which is a novel distance on permutations dw( ,  ).
The weight vector w provides the  exibility necessary for satisfying (iii) while the metric property satis es (i).
We then extend it to a dissimilarity  (si(q), sj(q)) over ranked lists si(q), si(j) by taking expectations with respect to the sets of permutations S(si(q)), S(sj (q)) consistent with the ranked lists.
Above, we consider search algorithms si, sj as functions from queries by si in response to the query q.
The function   is extended to search algorithms by taking another expectation, this time with respect to queries q sampled from a representative set of queries Q.
The expectations ensure that properties (ii) and (v) are satis ed.
We derive an e cient closed form for the double expectation that veri es property (iv) in Sec.
3.2 and give a pseudo-code implementation.
Formally, we de ne  (si, sj) = E q Q{ (si(q), sj (q))} = E q Q E  S(si(q))E  S(sj (q)){dw( ,  )} (3) where dw( ,  ) is a distance between permutations  ,   de- ned in Section 3.1, E  S(si(q))E  S(sj (q)) is the expectation with respect to permutations  ,   that are sampled from the sets of all permutations consistent with the ranked lists output by the two algorithms si(q), sj(q) (respectively), and E q Q is an expectation with respect to all queries sampled from a representative set of queries Q.
In the absence of any evidence to the contrary, we assume a uniform distribution over the set of queries Q and over the sets of permutations consistent with si(q), sj(q).
We proceed with a description of dw( ,  ) in Section 3.1 and then follow up in Section 3.2 with additional details regarding the expectations in (3) and how to compute them.
The weighted Hoe ding distance is a distance between permutations, here considered as permutations over the n indexed websites in the internet3.
The fact that n is extremely large should not bother us at this point as we will derive closed form expressions eliminating any online complexity terms depending on n. For simplicity we refer to the websites using the integers {1, .
.
.
, n}.
A permutation over n websites   is a bijection from {1, .
.
.
, n} to itself mapping websites to ranks.
That is  (6) is the rank given to website 6 and  1(2) is the website that is assigned second rank.
A permutation is thus a full ordering over the entire web and we denote the set of all such permutations by Sn.
We will represent a permutation by a sorted list of websites from most preferred to least, separated by vertical bars i.e.  1(1)|       | 1(n); for example, for n = 5 one permutation ranking item 3 as  rst and 2 as last is 3|5|1|4|2.
Our proposed distance dw( ,  ) is a variation of the earth movers distance4 [18] on permutations.
It may also be regarded as a weighted version of the Hoe ding distance [15].
It is best described as the minimum amount of work needed to transform the permutation   to  .
Work, in this case, is the total amount of work needed to bring each item from its rank in   to its rank in   i.e., the r-item is transported from rank k =  (r) to l =  (r) (for all r = 1, .
.
.
, n) requiring wk +       + wl 1 work (assuming k < l) where wk is the work required to transport an item from rank k to k + 1.
For example, the distance d(1|2|3, 2|1|3) is w1 + w1 due to the sequence of moves 1|2|3   |1, 2|3   2|1|3.
Another example
 websites in the internet.
In any case, this number is very large and is growing continuously.
We avoid its dynamic nature and consider it as a  xed number.
function is the minimum amount of work needed to transform one to the other, when the functions are viewed as representing spatial distributions of earth or rubble.
dw( ,  ) = d  w(u, v) = r=1

 Pv 1 t=u wt d  w(v, u) >:
 (4) (5) if u < v if u > v otherwise .
is d(1|2|3, 3|1|2) = w1 + w2 + w2 + w1 due to the sequence of moves 1|2|3   |1, 2|3   |1|2, 3   |1, 3|2   3|1|2.
Formally, the distance may be written as n d  w( (r),  (r)) where The weight vector w = (w1, .
.
.
, wn 1) allows di erentiat-ing the work associated with moving items across top and bottom ranks.
A monotonic decreasing weight vector, e.g., wt = t q, t = 1, .
.
.
, n   1, q   0 correctly captures the fact that disagreements in top ranks should matter more than disagreements in bottom ranks [9, 11, 16].
The exponent q is the corresponding rate of decay.
A linear or slower rate 0   q   1 may be appropriate for persistent search engine users who are not very deterred by low-ranking websites.
Choosing q   0 retrieves a weighting mathematically similar to the log function weighting that is used in NDCG [10] to emphasize top ranks.
A quadratic or cubic decay q = 2, 3 may be appropriate for users who do not pay substantial attention to bottom ranks.
The weight may be modi ed to wt = max(t q    , 0),   > 0 to capture the fact that many users simply do not look at results beyond a certain rank.
While it is possible to select an intuitive value of q, it is more desirable to select one that agrees with user studies.
An MDS embedding of permutations using dw appears in Figure 1 (see Section 4 for more details).
Proposition 1.
Assuming w is a positive vector, the weighted Hoe ding distance (4) is a metric.
Proof.
Non-negativity dw( ,  )   0 and symmetry dw( ,  ) = dw( ,  ) are trivial to show.
Similarly it is easy to see that dw( ,  ) = 0 i    =  .
The triangle inequality holds as dw( ,  ) + dw( ,  ) =   n
 r=1 n
 r=1 d  w( (r),  (r)) + d  w( (r),  (r)) d  w( (r),  (r)) = dw( ,  ) (6) where the inequality (6) holds due to the positivity of w.
The weighted Hoe ding distance has several nice properties that make it more appropriate for our purposes than other permutation measures.
First, it allows customization to di erent users who pay varying degrees of attention to websites in di erent ranks (typically higher attention is paid to higher ranks).
Standard permutation distances such as Kendall s tau, Spearman s rho, the footrule, Ulam s distance and Cayley s distance treat all ranks uniformly [5].
Second, it is a true metric in contrast to the generalized Kendall s tau [7].
Third, its clear interpretation allows explicit speci cation of the weight vector based on user studies.
Finally, it is computationally tractable to compute the weighted Hoe ding distance as well as its expectation over partially ranked lists corresponding to   in (8).
Figure 2 summarizes the advantages of our distance over other dissimilarities.
A ranked list output by a search algorithm forms an ordered list hi1, .
.
.
, iki of a subset of the websites {i1, .
.
.
, ik}  


































 Figure 1: MDS embedding of permutations over n = 5 websites.
The embeddings were computed using the weighted Hoe ding distance with uniform weight function wt = 1 (left), linear weight function wt = 1/t (middle) and quadratic weight function wt = 1/t2 (right).
The permutations starting with 1 and 2 (colored in red) and the permutations starting with 2 and 1 (colored in blue) become more spatially disparate as the rate of weight decay increases.
This represents the increased importance assigned to agreement in top ranks as we move from uniform to linear and quadratic decay.
Kendall/Spear [5] Fligner Kendall [7] E Kendall top k [6] E Spearman [14] InverseMeasure [2]
 (i)
 (ii) (iii) (v) (iv)

















 E Weighted Hoe ding X X Figure 2: Summary of how di erent dissimilarities satisfy properties (i)-(v) in Section 3.
{1, .
.
.
, n}.
Di erent search strategies may result in lists of di erent sizes but in general k is much smaller than n. In addition to the notation hi1, .
.
.
, iki we also denote it using the bar notation as i1|i2|       |ik|ik+1, .
.
.
, in where {ik+1, .
.
.
, in} = {1, .
.
.
, n} \ {i1, i2,       , ik} (7) indicating that the unranked items {1, .
.
.
, n} \ {i1, .
.
.
, ik} are ranked after the k items.
Partial rankings (7) are not identical to permutations as there is no known preference among the unranked items {1, .
.
.
, n} \ {i1, .
.
.
, ik}.
We therefore omit vertical lines between these items and list them separated by commas i.e., 3|2|1, 4 is equivalent to the ranked list h3, 2i which prefers 3 over 2 and ranks 1 and 4 last without clear preference between them.
It is natural to identify a ranked list hi1, .
.
.
, iki as a full permutation of the web that is unknown except for the fact that it agrees with the website ranking in hi1, .
.
.
, iki.
Denoting the set of permutations whose website ordering does not contradict hi1, .
.
.
, iki as S(hi1, .
.
.
, iki), we have that hi1, .
.
.
, iki corresponds to a random draw from S(hi1, .
.
.
, iki).
Assuming lack of additional knowledge, we consider all permutations in S(hi1, .
.
.
, iki) as equally likely resulting in  (hi1, .
.
.
, iki, hj1, .
.
.
, jli) def = E  S(hi1,...,iki), S(hj1 ,...,jli)d( ,  )
 = (n   k)!
(n   l)!
X  S(hi1,...,iki)
  S(hj1,...,jl i) d( ,  ).
(8) For example, consider the case of n = 5 with two search strategies returning the following ranked lists h3, 1, 4i =
  (3|1|4|2, 5, 1|5|2, 3, 4) =

 (d(3|1|4|2|5, 1|5|2|3|4) + d(3|1|4|5|2, 1|5|2|3|4) +       + d(3|1|4|2|5, 1|5|4|3|2) Expression (8) provides a natural mechanism to incorporate information from partially ranked lists.
It is di cult to compare directly two ranked lists hi1, .
.
.
, iki, hj1, .
.
.
, jli of di erent sizes.
However, the permutations in S(hi1, .
.
.
, iki) and S(hj1, .
.
.
, jki) are directly comparable to each other as they are permutations over the same set of websites.
The expectation (8) aggregates information over such directly comparable events to provide a single interpretable and coherent dissimilarity measure.
Figure 3 displays the MDS embedding for partial rankings using the expected   (8) (see Section 4 for more details).
The expectation de ning   in (8) appears to require insurmountable computation as it includes summations over (n   k)!
(n   l)!
elements with n being the size of the web.
However, using techniques similar to the ones developed in [15] we are able to derive the following closed form.
Proposition 2.
The following closed form applies to the expected distance over the weighted Hoe ding distance (4).
 (hi1, .
.
.
, iki, hj1, .
.
.
, jli) = n
 r=1  d(r) where (10)  d(r)=
 >>>: d  w(u, v) n l Pn
 n k Pn

 n k t=l+1 d  t=k+1 d  w(t, u) w(t, v) t=k+1 Pn n l Pn
 r   A   B r   A   Bc r   Ac   B .
s=l+1 d  w(t, s) otherwise Above, A = {i1, .
.
.
, ik}, B = {j1, .
.
.
, jl}, and u   {1, .
.
.
, k}, v   {1, .
.
.
, l} are the respective ranks of r in {i1, .
.
.
, ik} and {j1, .
.
.
, jl} (it they exist).
Proof.
A careful examination of (4) reveals that it may be written in matrix notation: d( ,  ) = tr(A AT   ) (11) where tr is the trace operator,   is the n n distance matrix with elements  uv = d  w(u, v), and A , A  are permutation matrices corresponding to the permutations   and   i.e., [A ]uv = 1 i   (u) = v. Using equation (11), we have  (hi1, .
.
.
, iki, hj1, .
.
.
, jli) = P S(hi1,...,iki) P S(hj1,...,jk i) tr(A AT   ) (n   k)!
(n   l)!
+ d(3|1|4|5|2, 1|5|4|3|2)).
(9) = tr(  Mhii (  Mhji)T ) (12)











































































 Figure 3: MDS embedding of ranked lists of varying lengths (k varies) over a total of n = 5 websites.
The embeddings were computed using the expected weighted Hoe ding s distance (8) with uniform weight function wt = 1 (left), linear weight function wt = 1/t (middle) and quadratic weight function wt = 1/t2 (right).
We observe the same phenomenon that we saw in Figure 1 for permutations.
The expected distance (8) separates ranked lists agreeing in their top rankings (denoted by di erent colors) better as the weights decay faster.
where  Mhii = P S(hi1,...,ik i) A  (n   k)!
,  Mhji = P S(hj1,...,jli) A  (13) (n   l)!
.
Note that the marginal matrices  Mhii,  Mhji have a probabilistic interpretation as their u, v entries represent the probability that item u is ranked at v. Combining (12) with Lemma 1 below completes the proof.
Lemma 1.
Let  M be the marginal matrix for a top-k ranked list hi1, .
.
.
, iki with a total of n items as in (13).
If r   {i1, .
.
.
, ik} and r = is for some s = 1, .
.
.
, k, then  Mrj =  js where  ab = 1 if a = b and 0 otherwise.
If r 6  {i1, .
.
.
, ik} then  Mrj = 0 for j = 1, .
.
.
, k and 1/(n   k) otherwise.
Proof.
For a top-k ranking hi1, .
.
.
, iki out of n items, the size of the set S(hi1, .
.
.
, iki) is (n   k)!.
Each of the permutations compatible with it has exactly the same top-k ranks.
If r   {i1, .
.
.
, ik} and r = is for some s = 1, .
.
.
, k then the number of permutations compatible with hi1, .
.
.
, iki that assign rank s to the item is (n   k)!.
Similarly, the number of consistent permutations assigning rank other than s to the item is 0.
As a result we have  Mrs = (n k)!
= 1 and  Mrj = 0 for j 6= s. If r 6  {i1, .
.
.
, ik}, the (n k)!
number of permutations consistent with the ranked list that assign rank j   {k + 1, .
.
.
, n} to the item is (n   k   1)!.
Similarly, the number of permutations that assign rank j   {1, .
.
.
, k} to the item is 0.
As a result  Mrj = 0 for j =
 n k for j = k+1, .
.
.
, n.
(n k)!
= 1 The expected distance (8) may be computed very e -ciently, assuming that some combinatorial numbers are pre-computed o ine.
Bounding k, l by a certain number m such that k, l   m   n, the online complexity is O(k + l) and the o ine complexity is O(n + m2).
Proposition 3 makes this precise.
A pseudo-code description of the distance computation algorithm is given as Algorithm 3.1.
Proposition 3.
Let hi1, .
.
.
, iki and hj1, .
.
.
, jli be top-k and top-l ranks on a total n items with k, l   m   n. Assuming that d  w(u, v) in (4) is computable in constant time and space complexity (as is the case for many polynomial decaying weight vectors w), the online space and time complexity is O(k + l).
The o ine space complexity is O(m2) and the o ine time complexity is O(n + m2).
kl = 1 n k
 m m and DE2 w(t, v), and DE2 kv = 1 s=l+1 d  Proof.
From equation 10, the o ine pre-computation m m, where Duv = requires computing Dm m, DE1 n k Pn w(u, v), DE1 d  t=k+1 d  n l Pn t=k+1 Pn w(t, s).
The space complexity for computing these matrices is O(m2).
The time complexity to compute Dm m is O(m2).
Exploiting features of cumulative sums and the matrix Dm m it can be shown that comput-m m requires O(n + m2) time.
Similarly, computing DE1 m m requires O(n + m2) time.
As a result, the to-ing DE2 tal o ine complexity is O(m2) space and O(n + m2) time.
Given the three precomputed matrices, computing the expected distance for two partially ranked lists hi1, .
.
.
, iki and hj1, .
.
.
, jli requires O(k+l) time and space.
The reasons are that given two lists, the time to identify overlapping items from two lists of size k and l is O(k + l) and that for items ranked by at least one engine, we need to use the lookup table no more than k + l times and another extra lookup for items never ranked in both.
To evaluate the proposed framework we conducted two sets of analyses.
The  rst analysis (in this section) uses synthetic data to examine properties of the embedding and compare it to alternative methods under controlled settings.
The second set of experiments (next section) includes real world search engine data.
Its goal is to demonstrate the bene t and applicability of the framework in the context of a real world visualization problem.
We start by examining the embedding of permutations over n = 5 websites.
A small number is chosen intentionally for illustrative purposes.
We consider two sets of permutations.
The  rst set contains all permutations ranking item
 mutations ranking item 2  rst and item 1 second.
Figure 1 displays the MDS embedding of these two sets of permutations based on the weighted Hoe ding s distance with constant weights wt = 1 (left), linear weight wt = t 1 (middle) and quadratic weight wt = t 2 (right).
The  rst set of permutations are colored red and the second set blue.
The uniform weight MDS embedding does not pay particular attention to di ering websites in the top ranks and so the red and blue permutations are interspersed together.
This is also the embedding obtained by using the Kendall s tau distance as in [5, 6, 12].
Moving to linearly decaying and quadratic decaying weights increases the separation between these two groups dramatically.
The di erences in web-

On-line:
 m m (Section 3).
m m and DE2 Expected-Weighted-Hoeffding( ,  ) if  mark[i] > 0 then sum   sum + D[i,  mark[i]] else 1 k1   size( ) 2 k2   size( )
 4 sum   0; 5 for i   1 to k1 6 do




 12 count   0 13 for i   1 to k2 14 do




 20 return sum; then sum   sum + DE1[k1, i] sum   sum + DE1[k2, i] if  mark[i] == 0 count   count + 1 Mark-Rank(a, b) 1 k1 = size(a) 2 k2 = size(b) 3 amark = zeros(1 .
.
.
k1), bmark = zeros(1 .
.
.
k2) 4 for i   1 to k1 5 do for j   1 to k2
 do if a[i] = b[j]


 then amark[i] = j bmark[j] = i 10 return [amark , bmark] Algorithm 3.1: Algorithm to compute expected weighted Hoe ding distance between two ranked lists   and  .
The online complexity of the above algorithm is O(k1k2).
A slightly more complex algorithm can achieve online complexity O(k1 + k2) as described in Proposition 3.
sites occupying top ranks are emphasized while di erences in websites occupying bottom ranks are de-emphasized.
This demonstrates the ine ectiveness of using Kendall s tau distance or uniform weight Hoe ding distance in the context of search engines.
The precise form of the weight - linear, quadratic, or higher decay rate depends on the degree to which a user pays more attention to higher ranked websites.
The second simulated experiment is similar to the  rst, but it contains partially ranked lists as opposed to permutations.
We form  ve groups - each one containing partially ranked lists ranking a particular website at the top.
Figure 3 displays the MDS embedding of these  ve sets of permutations based on the expected weighted Hoe ding distance  (hi1, .
.
.
, iki, hj1, .
.
.
, jli) in (8) using constant weights wt = 1 (left), linear weights wt = t 1 (middle) and quadratic weights wt = t 2 (right).
Ranked lists in each of the di er-ent groups are displayed in di erent colors.
We observe a similar conclusion with the expected distance over partially ranked lists as we did with the distances over permutations.
The  ve groups are relatively interspersed for uniform weights and get increasingly separated as the rate of weight decay increases.
This represents the fact that as the decay rate increases, disagreements in top ranks are emphasized over disagreement at bottom ranks.
We also conducted some comparisons between the weighted Hoe ding distance and alternative distance measures.
Table 1 shows how one recently proposed measure, the inverse measure [2], lacks discriminative power, assigning the same d to 1|2|3|4|5






 InverseMeasure wt = t 1 wt = t 2




















 Table 1: A comparison of the inverse measure [2] with the weighted Hoe ding distance indicates that the inverse measure lacks discriminative power as it assigns the same dissimilarity to very di erent ranked lists (n = 5).
d to 1|2|3|4|5






 n = 5






 n = 10






 n = 103






 n = 105






 n = 107






 Table 2: A comparison of weighted Hoe ding distance with cubic weight decay wt = t 3 reveals that increasing n beyond a certain size does not alter the distances between partially ranked lists.
This indicates lack of sensitivity to the precise value of n as well as computational speedup resulting from replacing n by n    n.
dissimilarity to very di erent ranked lists.
Kendall s tau and the other distances proposed in [5, 6] lack the ability to distinguish disagreement in top ranks and bottom ranks.
In particular, Kendall s tau is identical to our weighted Hoe ding distance with uniform weights (see Figures 1-3 for a demonstration of its inadequacy).
NDCG [10] and other precision recall measures rely on comparing a ranked list to a ground truth of relevant and not-relevant websites.
As such they are not symmetric and are inappropriate for computing MDS embedding based on pairwise distances.
Table 2 shows a comparison of weighted Hoe ding distances with wt = t 3 for di erent sizes of the web n.
It reveals that increasing n beyond a certain size does not alter the distances between partially ranked lists.
This indicates a lack of sensitivity to the precise value of n as well as computational speedup resulting from replacing n by n    n.
We discuss in this section three experiments conducted on real world search engine data.
In the  rst experiment we visualize the similarities and di erences between nine di erent search engines: altavista.com, alltheweb.com, ask.com, google.com, lycos.com, live.com, yahoo.com, aol.com, and dogpile.com.
We collected 50 popular queries online in each of six di erent categories: company names, questions5, sports, tourism6, university names, and celebrity names.
These queries form a representative sample of queries Q within each category over which we average the expected distance   according to (3).
Figure 4 shows several queries for each one of the topic categories.
We visualize search result sets within each of the query categories in order to examine whether the discovered similarity patterns are spe-ci c to a query category, or are generalizable across many di erent query types.
5queries from http://answers.yahoo.com
 Times Square, Sydney Opera House, Ei el Tower, Niagara Falls, Disneyland, British Museum, Giza Pyramids Categories Tourism Celebrity Names Michael Bolton, Michael Jackson, Jackie Chan, Harrison Ford, Halle Berry, Whoopi Goldberg, Robert Zemeckis Sports University Names Georgia Institute of Technology, University of Florida, Virginia Tech, University of California Berkeley Company Questions Temporal Queries AIG Bonuses, G20 major economies, Timothy Geitner, Immigration Policy, NCAA Tournament Schedule Football, Acrobatics, Karate, Pole Vault, Butter y Stroke, Scuba Diving,Table Tennis, Beach Volleyball, Marathon Goldman Sachs, Facebook, Honda, Cisco Systems, Nordstrom, CarMax, Wallmart, American Express, Microsoft How are  ying buttresses constructed, Does toothpaste expire, How are winners selected for the Nobel Prize Figure 4: Selected queries from each of the 6 query categories, and from the set used for examining temporal variations.
university

 celebrity







 1. altavista 2. alltheweb 3. ask 4. google 5. lycos 6. live 7. yahoo 8. aol 9. dogpile



 1. altavista 2. alltheweb 3. ask 4. google 5. lycos 6. live 7. yahoo 8. aol 9. dogpile



 company












 sports



 questions





 tourism










 Figure 5: MDS embedding of search engine results over 6 sets of representative queries: company names, university names, celebrity names, questions, sports, and tourism.
The MDS was based on the expected weighted Hoe ding distance with linear weighting wt = t 1 over the top 100 sites.
Circle sizes indicate position variance with respect to within category queries.
1. altavista 2. alltheweb 3. ask 4. google 5. lycos 6. live 7. yahoo 8. aol 9. dogpile celebrity celebrity 1. altavista 2. alltheweb 3. ask 4. google 5. lycos 6. live 7. yahoo 8. aol 9. dogpile Figure 6: MDS embedding of search engine results over the query category celebrity with di erent query manipulations.
The MDS was computed based on the expected distance (3) with (8) corresponding to the weighted Hoe ding distance with quadratic decaying weights (left panel) and Kendall top k distance [6] (right panel).
Each marker represents a combination of one of the 9 search engines and one of the 5 query manipulation techniques.
By comparison, the embedding of the Kendall top-k distance (right) lacks discriminative power and results in a loss of information.
Figure 5 displays the MDS embedding of each of the nine engines for the six query categories, based on the expected weighted Hoe ding distance (3) with linear weight decay.
The   quantity was averaged over the 50 representative queries from that category.
Each search engine is represented as a circle whose center is the 2-D coordinates obtained from the MDS embedding.
The radii of the circles in Figure 5 were scaled proportionally to the positional stability of the search engine.
More precisely, we scaled the radius of the circle corresponding to the i-th search engine proportionally to its distance variance over the 50 queries altav alweb lycos yahoo ask google aol dog msn stability(i) def
 j:j6=i Var q Q{ (si(q), sj(q))}.
(14) Figure 7: Hierarchical clustering dendogram for the nine search engines over the query category Companies.
Scaling the circles according to (14) provides a visual indication of how much will the position change if one or more queries are deleted or added to Q.
This can also be interpreted as the degree of uncertainty regarding the precise location of the search engines due to the averaging over Q.
Examining Figure 5 reveals several interesting facts.
To begin with there are  ve distinct clusters.
The  rst and largest one contains the engines altavista, alltheweb, lycos, and yahoo (indicated by the numeric codes 1,2,5,7).
These four search engines are clustered together very tightly in all
 and AOL (numeric codes 4, 8) who also appear in very close proximity across all 6 query categories.
The remaining three clusters contain individual engines: live, dogpile, and ask.
The clusters in the embedding do in fact mirror the technology relationships that have evolved in the search engine industry.
FAST, the company behind alltheweb, bought Lycos and was subsequently bought by Overture who also bought Altavista7.
Overture was subsequently bought by the fourth member of the cluster, Yahoo.
All four search engines in the  rst cluster have close proximity in the embedding and yet are dissimilar from the remaining competitors.
The second cluster, for Google and AOL, re ects the fact that AOL now relies heavily on Google s web search technology, leading to extremely similar ranked lists.
The remaining engines are quite distinct.
Dogpile is a meta-search engine which incorporates the input of the other major search engines.
We see that dogpile s results are roughly equidistant from both Yahoo and Google clusters for all query categories.
Figure 5 also shows that dogpile is more similar to the two remaining engines - Live and Ask.
Apparently, dogpile emphasizes pages highly-ranked by Live and Ask in its meta search more than Google and AOL and more than Yahoo, Lycos, Altavista, and alltheweb.
We present the similarity structure between the search engines in Figure 7.
The  gure displays a dendogram output by standard hierarchical bottom up clustering.
The clustering was computed based on the expected Hoe ding similarity measure for queries sampled from the query category Companies.
The dendogram in the  gure con rms the analysis above visually.
Altavista, alltheweb, lycos, and yahoo all form a tightly knit cluster.
A similar tight cluster contains google and aol.
More loosely it can also be said that google and aol are closer to the remaining search engines (ask, dogpile, and msn) than the yahoo cluster.
7http://google.blogspace.com/archives/000845.html Engine/Distance Ask Live Google Yahoo (b) +



 (c)    



 (d) and



 (e) or



 Figure 8: The expected distance of di erent query manipulations from the original query for di erent search engines.
In the second experiment we used search engine data to examine the sensitivity of the search engines to four commonly used query manipulation techniques.
Assuming that the queries contained several words w1w2       wl with l > 1, the query manipulation techniques that we considered were (a) w1w2       wl   w1w2       wl (b) w1w2       wl   w1 + w2 +       + wl (c) w1w2       wl    w1w2       wl  (d) w1w2       wl   w1 and w2 and       and wl (e) w1w2       wl   w1 or w2 or       or wl with the  rst technique (a) being the identity i.e. no query manipulation.
The embeddings of queries in the query category celebrity are displayed in Figure 6.
The left panel displays the MDS embedding based on our expected weighted Hoe ding distance with quadratic decaying weight.
As a comparison, the right panel shows the MDS based on Kendall s top k distance as described by Fagin et al. [6].
Each marker in the  gure represents the MDS embedding of a particular engine using a particular query manipulation technique which brings the total number of markers to 9   5 = 45.
Comparing the left and right panels shows that visualizing using Kendall s top k distance [6] lacks discriminative power.
The points in the right panel fall almost on top of each other limiting their use for visualization purposes.
In contrast, the points in the left panel (weighted Hoe ding distance) di erentiate among not only di erent engines but also di erent types of query manipulations.
In particular, it shows that most search engines produce two di erent clusters of results corresponding to two sets of query manipulation techniques: transformations {(a),(b), (c)} in one cluster and transformations {(d),(e)} in the other cluster.
Live and ask form an exception to that rule forming clusters {(a),(d)}, {(b),(c)}, {(e)} (live) and {(a),(b),(d),(e)}, {(c)} (ask).
Figure 8 shows the query manipulations that produce ranked lists most distinct from the original query: (c) for ask and live, (d) for google, and (e) for yahoo.
2. alltheweb 3. google 4. lycos 5. live 6. yahoo 7. aol












 Figure 9: Top: MDS embedding of search engine results over seven days for a set of queries based on temporal events.
The MDS embedding was based on the expected Hoe ding distance with linear weighting wt = 1/t over the top 50 sites.
Bottom: The dissimilarity of Yahoo results over seven days with respective to a reference day for a set of queries based on temporal events.
In the third experiment, we visualize search result sets created by replicating 7 out of the 9 search engines over 7 consecutive days resulting in 7   7 = 49 search result sets.
The search engines were queried on a daily basis during 3/25/2009 - 3/31/2009 and the returned results were em bedded in 2-D for visualization.
In contrast to the previous two experiments, we used a separate query category which was speci cally aimed at capturing time sensitive matters.
For example, we ignored tourism queries such as Ei el Tower due to their time insensitive nature and instead used queries such as Timothy Geitner or AIG bonuses which dominated the news in March 2009.
See Fig. 4 for more examples.
The embedded rankings are displayed in Figure 9 (top).
The embedding reveals that the yahoo cluster (yahoo, al-tavista, alltheweb, and lycos) shows a high degree of temporal variability, and in particular a sharp spatial shift on the third day from the bottom region to the top left region.
This could be interpreted either as a change in the index, re ecting the dynamic nature of the Web, or an internal change in the retrieval algorithms underlying the engines.
The other engines were more stable as their ranked lists changed very little with the temporal variation.
Note that as the queries were time sensitive this should not be interpreted as a measure of robustness, but rather as a stability measure for the internal index and ranking mechanisms.
Interestingly, Vaughan [20] also reports that Altavista shows temporal jumps with Google being more stable over a set of queries in 2004.
Figure 9 (bottom) shows the expected distance between the yahoo search results across the seven consecutive days and a reference point (2nd and 6th day).
As expected, for each one of the two plots, the deviation to the reference date increases monotonically with the temporal di erence.
The slope of the curve represents the degree of temporal change  (st, st+  ) between yahoo at time t and at time t +   as a function of   with respect to the reference point t.
A central assumption of this study is that MDS embed-dings can give a faithful representation of the distances in the original higher-dimensional space.
Thus, we now provide a validation of the MDS embedding as a visualization tool, diagnosing whether MDS is providing a reasonable embedding and which MDS variant should be preferable.
The most common tool for validating the MDS embedding is Shepard s plot (Figure 10, top) which displays a scatter plot contrasting the original dissimilarities (on the x axis) and the corresponding distances after the embedding (on the y axis).
Points on the diagonal represent zero distortion and a curve that deviates substantially from the diagonal represents substantial distortion.
The Shepard s plot in Figure 10 (top) corresponds to the metric MDS using the standard stress criterion as described in (2).
The plot displays low distortion with a tendency to undervalue dissimilarities in the range [0, 0.5] and to overvalue dissimilarities in the range [0.5, 0.9].
Such a systematic discrepancy between the way small and large distances are captured is undesirable.
An alternative is non-metric MDS which achieves an embedding by transforming the original dissimilarities into alternative quantities called disparities using a monotonic increasing mapping which are then approximated by the embedding distances [3].
Doing so preserves the relative ordering of the original dissimilarities and thus (assuming the embedding distances approximate well the disparities) accurately represent the spatial relationship between the points.
Figure 10 (bottom) displays the Shepard s plot for the same data embedded using non-metric stress MDS with the disparities displayed as a red line.
Despite the fact that its numeric distortion is higher, the non-metric MDS is a viable alternative to the metric MDS, and is what was used to generate the  gures in this paper.
In this paper we present a framework for visualizing relationships between search algorithms.
The framework starts by deriving an expected distance based on the earth mover s distance on permutations and then extends it to partially ranked lists by taking expectations over all permutations consistent with the ranked lists.
The expected distance   is then averaged over representative queries q   Q to obtain a dissimilarity measure for use in multidimensional scaling embedding.
The expected distance has several nice properties including being computationally e cient, customizable through a selection of the weight vector w, and interpretable.
We explore the validity of the framework using a simulation study which indicates that the weighted Hoe ding distance is more appropriate than Kendall s tau and more dis-criminative than the inverse measure.
It is also more appropriate for MDS embedding than non-symmetric precision-recall measures such as NDCG.
We also demonstrate the robustness of the proposed distance with respect to the choice of n and its e cient computation with complexity that is linear in the sizes of the ranked lists (assuming some quantities are precomputed o ine).
Experiments on search engine data reveal several interesting clusters which are corroborated by examining recent news stories about the web search industry.
We demon-

































Figure 10: Shepard plots (embedding distances as a function of the original dissimilarities) for 2D MDS embeddings corresponding to the weighted Hoe ding distance of search engine results over query category celebrity with di erent query manipulations.
The metric stress MDS [3] (top panel) produces an embedding that has a lower overall distortion than the non-metric stress MDS [3] (bottom panel).
The non-metric stress MDS, however, achieves an embedding by transforming the original dissimilarities into alternative quantities called disparities using a monotonic increasing mapping (red line in bottom panel).
strate how to visualize the positional stability by scaling the MDS markers proportionally to the total distance variance and how to visualize sensitivity of search engines to popular query manipulation techniques.
We also use the visualization framework to examine how the search results vary over consecutive days.
Search engines use complex proprietary algorithms containing many parameters that are automatically tuned based on human provided ground truth information.
As a result, it is di cult for search engineers to have a detailed understanding of how precisely their search engine works.
For external users the problem is even worse as they are not privy to the internal algorithmic details.
Our framework provides visual assistance in understanding the relationship between a search algorithm s ranked results and its dependence on internal and external parameters.
Such visualization may lead to better designed search engines as the engineers improve their understanding of how search engines depend on the internal parameters.
It may also improve the search experience as users understand better the relationship between di erent engines and their dependency on query manipulation techniques and external parameters such as time.
We thank H. Zha and H. Li for helpful comments.
This research was funded in part by NSF grant DMS-0907466.
