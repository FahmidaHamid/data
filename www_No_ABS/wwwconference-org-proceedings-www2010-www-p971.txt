As the Internet matures the amount of data available continues to increase.
The artifacts of this ever-growing media provide interesting new research opportunities that explore social interactions, language, art, politics, and so on.
Many of these new research directions require the content of the Internet to be gathered, processed and stored quickly and ef- ciently.
These e orts are often hampered by the inclusion of non-content text and images, i.e., navigation links and advertisements.
Furthermore, HTML tags and other non-content related HTML characters   images not included     State University, Manhattan, KS Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Weninger performed early work on this topic at Kansas typically comprise the majority of each page s size, and yet, web crawlers are required to download, store and compute each webpage in its entirety.
In order to e ectively manage this ever-growing and ever-changing media, content extraction methods have been developed to remove extraneous information from webpages.
When beginning this investigation, we observed that the typical webpage contains a title banner (or something similar) towards the top of the page, a list of hyperlinks on the left or right side of the page with advertisements interspersed.
Most usually the meaningful content of the page is located in the middle.
Of course, this layout is not standard among all webpages; therefore a  exible, robust content extraction tool is necessary.
T.V.
Raman recently observed that in newer webpages,  ...there is a clean architectural separation among content, visual-presentation, and interaction layers  [31].
Our observations concur with those of Raman; speci cally, we  nd that modern webpages have largely abandoned the use of structural tags within a webpage and adopted an architecture which makes use of stylesheets and <div> or <span> tags for structural information.
While this is a welcome advancement for many reasons (e.g., ease of development, more conducive to AJAX-oriented design) one aspect which has failed to keep up with these changes is content extraction methods.
Most current content extraction techniques make use of particular HTML cues such as tables, fonts, sizes, lines, etc., and since modern webpages no longer include these cues, many content extraction algorithms have begun to perform poorly.
One di erence between our approach and other related work is that we make no assumptions about the particular structure of a given webpage, nor do we look for particular HTML cues.
We only assume that a given webpage maintains some structure.
This is a nontrivial task because of the di culty in determining which part of a webpage is meaningful and which part is not.
Our solution, called Content Extraction via Tag Ratios (CETR - pronounced cedar), is partially based on previous work in Web content extraction [32].
In the CETR algorithm we construct a tag ratio (TR) array with the contention that for each line i in the array, the higher the tag ratio is for the ith line the more likely that i represents a line of content-text within the HTML document.
In this and in previous work [32], we observe that the TR array can be represented as a histogram, wherein each histogram bucket represents the tag ratio of a line in the document.
By that observation the problem is reduced to a histogram clustering task wherein appropriate clusters should content and those TR-lines which do not.
Three clustering approaches are investigated in this work.
The  rst two approaches either apply a water-level i.e., minimum cut-o  (CETR-TM) or a partition clustering approach (CETR-KM) which operate on similarities from the tag ratio alone.
We  nd that the TR-histogram is not just a set of values, but is rather an ordered sequence of values wherein additional information may be gained by examining the surrounding values and the manner in which these values evolve as the sequence is iterated through.
By this intuition, we expand our model to include the sequence information by way of an absolute derivative array.
The result is a fast, accurate and general content extraction algorithm which outperforms current, even supervised and specialized, approaches.
Contributions.
Five main contributions can be claimed in our paper:


 dimension: CETR Threshold Method (CETR-TM) and CETR k-Means (CETR-KM), and discussion as to why we believe this approach to be limited.
its application to the content extraction task.
operate on the two dimensional webpage content model.
alternate content extraction approaches across a large and varied corpus.
Orgainization.
The remainder of this paper  rst discusses related work and applications before describing the CETR algorithm and giving examples of it s use.
We pay special attention on the smoothing, two-dimensional model and clustering methods, as well as possible worst case scenarios.
Three distinct algorithms are presented in this paper.
The  rst two (CETR-TM, CETR-KM) are preliminary versions of the approach and the  nal version (CETR) is claimed to be the most advanced and best performing.
We test each algorithm against many of the techniques examined in the related works section and discuss the results.
Finally we o er our conclusions and plans for future research.
Extracting content from HTML documents has been well-studied and numerous methods have been developed.
Perhaps the most simplistic approaches are seen in handcrafted web scrapers which speci cally know how to extract article text by looking for known HTML-cues with regular expressions written in Java or Perl or with specialized tools designed for content extraction such as NoDoSE [2] or XWRAP [4].
An obvious disadvantage of this approach is that di erent rule expressions need to be manually created for each website.
Furthermore, an individual website may also change its structure or layout over time making this approach in need of continuous maintenance.
The term Content Extraction was introduced by Rahman et al. [30] in which the authors describe a basic content extraction algorithm.
Shortly thereafter Finn et al. [13] introduced the Body Text Extraction (BTE) algorithm wherein the authors extract content-text by identifying the single, continuous region which contains the most words and the least amount of HTML tags.
Gottron [14] applied the Document Slope Curves (DSC) [29] extension to the BTE algorithm to create Advanced DSC (ADSC) in which a windowing technique is used to locate document regions in which word tokens are more frequent than tag tokens.
Mantratzis et al. presented an approach to identify navigation lists by identifying DOM elements which have a high ratio of text residing in anchor tags [25].
This aptly named Link Quota Filter (LQF) approach can be applied to content extraction by it s inverse, that is, by removing the resulting link blocks from the document.
Han et al. developed the Largest Size Increase (LSI) algorithm [20] which identi es the DOM subtree which contributes most strongly to the visible content in a rendered document.
Debnath et al. developed the FeatureExtractor (FE) [11] and K-FeatureExtractor (KFE) [12] approaches based on block segmentation of the HTML document.
Each block is analyzed for particular features like the amount of text, images, script code, etc.
Content text is extracted by selecting blocks which meet some criteria, e.g.
most text.
Gottron presented an approach most similar to CETR by way of Content Code Blurring (CCB) [16], wherein content regions are identi ed by homogeneously formatted source code character sequences.
An attempt to combine di erent content extraction methods into one system was made by the Crunch framework [18,
 methods can provide better results than a single approach on its own.
A more recent ensemble method called the Com-bineE framework [15] was recently developed to more easily con gure ensembles of content extraction algorithms.
Yet another approach is to induce a wrapper from labeled examples.
One such approach was studied by Kushmer-ick [22], however this approach could not handle complex or unexpected structures.
Muslea et al. [27] present a similar approach by taking a hierarchical description of the  elds to be extracted along with user de ned labels on example documents in order to induce a set of extraction rules.
However, like the manual or pattern matching approaches mentioned above, wrapper induction techniques still require up-to-date, tediously labeled examples for each data source.
The Visual Page Segmentation (VIPS) [6] heuristically segments documents into a tree where the nodes are visually grouped blocks.
The major problem with this approach is that the result of the VIPS algorithm does not label the nodes as content or non-content.
The results presented in later sections show that if the best possible parameters are selected and a perfect mechanism is provided to label the nodes then VIPS can extract article text with a high degree of accuracy.
However, there exists no such labeling mechanism; furthermore, VIPS must partially render a page in order to analyze it including retrieving all external style sheets, etc.
Therefore, compared to other techniques, VIPS is very resource intensive.
Template detection algorithms [23, 33, 21, 10, 7, 9] are a di erent approach to content extraction in which collections of training documents based on the same template are used to learn a common structure.
Speci cally, Bar-Yossef et al.
present an approach which automatically detects templates from the largest pagelet (LP) [3].
In general template detec-parts across all web documents.
This is an accurate approach but has been found to be too time consuming and burdensome because a model must be built for each individual website and therefore for each site multiple pages known to have the same template are required.
In the CleanEval content extraction competition only a few pages are available from the same site thus mandating a more general approach.
The winner of the CleanEval task [26] splits pages by their tags into a sequence of blocks and then labels each block as content or non content using conditional random  elds with a number of block-level features.
A hybrid approach of the heuristic and supervised learning methods is the Maximum Subsequence Segmentation algorithm (MSS) by Pasternack and Roth [28] wherein they extract content by a  method of global optimization over token-level local classi ers.  Despite being a supervised learning approach, MSS seems to be less susceptible to the problems of similar approaches because it bases its learning largely on character sequence statistics rather than on speci c tags.
However, MSS still requires training and is therefore susceptible to bias from the training examples which is evident by it s results.
For example, when trained on news article data MSS can extract news article content quite well, but when that model is given data such as the CleanEval corpora the performance su ered signi cantly (results not reported).
Only after several adjustments were made were the CleanEval results reported.
There are a number of applications where content extraction is an essential task or could improve overall performance.
Pocket-sized devices with small screens such as mobile phones or PDA s are ubiquitous and therefore adapting webpages for these devices is an important task [4, 8].
Other tasks include automatically generating RSS news feeds from blogs or article pages.
The general  eld of information retrieval may bene t from this work: by removing irrelevant text from a webpage a keyword-based search is less likely to return irrelevant hits.
For example, Cai et al [5] increased IR performance when his VIPS algorithm was employed to process webpages  visual blocks separately.
VIPS is used again to aid in query expansion by segmenting webpages and selecting additional query terms from only the  best  blocks.
Speci cally, in a future task we wish to employ a general content extraction method as a preprocessing step to clean input text to subsequent steps in a pipeline.
For example, an interesting research task aided by content extraction tools could be named entity extraction, disambiguation and reconciliation wherein we wish to infer relationships between entities by their semantics and relative location in the web-graph.
In order to save such a system from the onslaught of irrelevant entities confounding the model, we realize the absolute necessity for a fast, accurate, general purpose content extraction algorithm.
Let s take, as a running example, a news article from The Hutchinson News1 that appeared on Wednesday, March 19,
 many pages on the Web; the title banner, navigation and advertisements take up most of the space on the page while the content of the page is con ned to a relatively small space in the middle.
At the bottom of the page more advertisements and images are displayed along with links to copyright and other administrative information.
Figure 1: The Hutchinson News webpage article To extract the content from this webpage a naive approach would use regular expressions to remove all of the HTML tags from the document and return the result.
This approach would achieve 100% recall, however all of the text advertisements, title, menus, etc.
would remain.
The majority of the algorithms listed in Section 2 look for HTML cues which likely indicate a content section.
For example, many algorithms look for speci c structural elements of the webpage and match these elements to a set of rules to derive the content section.
The shortcoming of these methods is that, with the widespread adoption of cascading style sheets in recent years, the structure of the webpage has become separated from the content (For an interesting review of this phenomenon see Michael Wesch s The Machine is Us/ing Us2).
As a result, modern webpages have switched from using structural tags to mostly <div> tags with the structural information provided by the style sheets.
With this change, most of the current extraction techniques perform poorly on modern webpages even if they previously performed well.
Of course, any new content extraction algorithm is still required to handle the old-style HTML markup.
With this in mind, we studied the general features that the old and new paradigms have in common, and from this investigation we  nd that the number of tags per line of HTML markup has generally remained the same even though the type and function of those tags has changed.
From this observation we developed the general concept of Tag Ratios.
Tag Ratios (TRs) are the basis by which CETR analyzes
 http://www.hutchnews.com
 http://www.youtube.com/watch?v=NLlGopyXT_g are the ratios of the count of non-HTML-tag characters to the count of HTML-tags per line.
In the likely event that the count of HTML-tags on a particular line is 0 then the ratio is set to the length of the line.
The TR algorithm is described in Algorithm 1 where D is the document being analyzed and T is the resulting histogram containing the tag ratios for each line i in D.
Before TRs are computed, script, remark and style tags are removed from the HTML document because this information would be treated as non-tag text by the algorithm and likely skew the results.
Empty lines are also removed because their inclusion would potentially hinder the performance of the clustering procedure.
Algorithm 1 Compute Tag Ratios

 D   removeScriptT ags(D) D   removeRemarkT ags(D) D   removeStyleT ags(D) for all i   1 to |D| do x  nonTagChars(Di) y  tags(Di) if y = 0 then y   1 end if Ti   x/y end for Computing the TR-histogram is a straight forward task as evident from the simplicity of Algorithm 1.
Example 1 shows a snippet of code from an article published on The Hutchinson News  website with the corresponding tag ratios.
Example 1.
Below is a brief snippet of a webpage news <div id="storyPageContent2"> <div id="author">James Smith</div> OKLAHOMA CITY - Police were told that.
.
.
.
.
.
The Oklahoman reported Sunday.
<br><br> Jones had.
.
.
article.
1.
<div id="topnav">



 6.
</div></div> The Tag Ratios for these six lines are computed as follows:





 The running time is linear in the number of HTML lines, that is, O(|D|).
Figure 2 shows the resulting TR-histogram T .
We see that between lines 220 and 260 there exist lines with a relatively high tag ratio; intuitively, we acknowledge this high tag ratio portion to be indicative of the webpage s content.
In this section we describe the threshold partitioning technique.
Originally described in [32] the principle of this approach is to determine a threshold   which discriminates




 y a r r
 o i t a
 g a







 Line Number



 Figure 2: Tag Ratios line by line from Hutchinson News webpage article TRs into content and non-content sections.
That is, any TR value greater than or equal to   should be labeled content, and conversely, any TR value less than   should be labeled not content.
The problem then becomes a task of  nding the best value for   .
Discussion on parameter tuning is in Section 6.5.
After the TR-histogram T is calculated a smoothing pass is made on the histogram.
This is done because without smoothing many important content lines might be lost.
In our experience, these lost content-lines typically include the page title, the news article byline or dateline, short or one sentence paragraphs, or other lines such that the TR is abnormally di erent relative to the surrounding lines.
As a pathological example, consider a webpage containing a document such as the American Declaration of Independence3, which contains TR-spikes corresponding to the relatively long preamble and proclamation sections.
However, many of the abuses of the king are listed in short, single sentence phrases, and relative to the rest of the document their TRs may therefore be errantly excluded in the clustering phase.
To resolve this problem we apply a Gaussian smoothing pass to T .
Standard Gaussian smoothing algorithms are generally implemented for image processing, are continuous and thus do not suit our purposes.
Therefore the algorithm used in this approach was re-implemented as a discrete function operating in a single dimension.
Equation 1 shows the construction of a Gaussian kernel k with a radius of 1 standard deviation 1 , giving a total window size of 2((cid:4) (cid:5)) + 1.
(cid:2) (cid:3)(cid:2) ki = j= (cid:2) (cid:3)  j2 2 2 , 0   i   2((cid:4) (cid:5)).
e (1) The size of and values within k vary according to   because as the variance of T increases, smoothing necessity also increases.
Next, Equation 2 shows that k is normalized to form k (cid:4) .
3e.g.
http://www.ushistory.org/declaration/document/ index.htm.
i = k ki(cid:3)(cid:2) (cid:3) j=0 kj ,(cid:4) (cid:5)   i   2((cid:4) (cid:5)).
(2) Finally, the Gaussian kernel k (cid:4) order to form a smoothed histogram (T tion 3.
is convolved with T in ) as shown in Equa-(cid:4) (cid:4) i =
 (cid:2) (cid:3)(cid:2) j= (cid:2) (cid:3) j+(cid:2) (cid:3)Ti j,(cid:4) (cid:5)   i   len(T )   (cid:4) (cid:5).
(cid:4) k (3) (cid:4) Compared to Figure 2, T , shown in Figure 3, is better suited for clustering because of the increased cohesiveness within sections and strict di erences between sections.
Furthermore, T has a lower variance because outlying peaks and valleys are smoothed.
Similarly, outliers, such as advertisements, that may occupy a single high-TTR line among many low-TTR lines, are smoothed to below the threshold.
(cid:4) y a r r
 o i t a
 g a
 d e h t o o m












 Line Number



 s o i t Figure 3: Smoothed Tag Ratios line by line of Hutchinson News webpage article  

 i     where Di (cid:4) Finally, let C be the set of content lines such that Di   i and       where   is a (cid:4) C i  T parameter and   is the standard deviation.
The parameter   is discussed further in Section 6.5.
After elements of C are selected, each content-line is stripped of all remaining HTML tags   usually paragraph and anchor tags.
Then the cleaned lines are combined and output to a  le for storage, indexing, etc.
This threshold method is hereafter referred to as CETR-TM.
(cid:4) Alternatively, we apply the k-means clustering method to group content C and non-content N lines by using T as the only similarity measure.
Empirically, we set k   3.
The resulting k clusters S1, S2 .
.
.
Sk are labeled by selecting the cluster which has its centroid closest to the origin (i.e.
zero in 1-dimensional space) Smin and assigning N   Smin.
The remaining clusters are assigned to C. The content-lines in C are stripped of all HTML tags and output.
This 1-dimensional k-means clustering method is hereafter referred to as CETR-KM.
One shortcoming of the Threshold Clustering and k-Means methods is that they view the TR histogram as a set of values rather than an ordered sequence of values, and as a result they are not sensitive to jumps in the TR histogram.
This ordered sequence information should be considered in a general purpose algorithm because signi cant jumps in the histogram (moving left to right or right to left) provide more information on the borders of the content section(s).
This section presents a unique approach to clustering 1-dimensional histograms.
We contend that by transforming the histogram data so that it may be represented in 2-dimensions we can capture the ordered nature of the his togram data and obtain more accurate results.
For this task, we de ne the two dimensions to be (1) a smoothed TR histogram (T ), and (2) the absolute smoothed derivatives of the smoothed TR histogram (  G).
(cid:4) (cid:4) To compute G,  rst smooth T in the same manner as described in Equations 1-3 to get T .
Next,  nd the derivatives for each element in the array; speci cally, we subtract (cid:4)
 i from the mean of the next   elements in order to differentiate on the moving average as shown in Equation 4 instead of line-by-line.
Note: all experiments presented in this paper use   = 3.
(cid:3)  (cid:4) (cid:4) i ) = Gi =
 i , 0   i < len(T (cid:4) (cid:4) )    .
  f
 Note that len(G) (cid:10)= len(T (4) ) 1 because G is essentially an array of di erences.
Next we (cid:4) again smooth G by way of Equations 1-3 to get G i| for each i in Finally we compute  G such that  Gi = |G (cid:4) (cid:4)
 .
These values are shown in Figure 4.
).
Instead len(G) = len(T .
(cid:4) (cid:4) (cid:4) i+j j=0 T a
 g a
 d e h t o o m
 e h t f o s e v i t a v i r e
 d e h t o o m
 e t u o s b
 l












 Line Number



 Figure 4: Absolute Smoothed Derivatives of Smoothed Tag Ratios line by line of Hutchinson News webpage article Notice that there are two spikes in Figure 4.
The  rst spike at line 220 corresponds to the beginning of the content section, and the second spike at line 267 corresponds to the end of the content section.
In any given webpage there may exist more than one content section therefore a clustering method is needed to appropriately categorize our model.
(cid:4) By combining the Smoothed Tag Ratios T from Figure 3 and the Absolute Smoothed Derivatives of Smoothed Tag Ratios  G from Figure 4 we observe that good clustering properties are revealed.
As illustrated in Figure 5, if we manually label each point to be either content ( ) or non-content (+) we see that the dense collection of points near the origin are non-content lines and the remaining points are content lines.
This 2D model presents a clear separation of content from non-content lines which can be explicitly obtained with the appropriate clustering technique.
o i t a
 g a
 d e h t o o m







 Non-Content Content





 Absolute Smoothed Derivatives of the Smoothed Tag Ratios Figure 5: Scatterplot depicting Smoothed Tag Ratios by Absolute Smoothed Derivatives of the Smoothed Tag Ratios of the Hutchinson News webpage article.
Manual labels of the data show content   and non-content + lines.
After the 2D model is created it is necessary to cluster the (cid:4) i ,  Gi) into two sets: content (C) or non con-TR points (T tent (N ).
This section describes our clustering algorithm, which is based on the k-Means algorithm originally proposed by MacQueen [24].
The standard k-Means algorithm oper-(cid:4) i ,  Gi)-points) to k clusters ates by assigning objects (i.e. (T S1, S2, .
.
.
, Sk randomly at  rst, and then by iteratively reassigning objects according to the cluster centroids  nearest neighbors.
Empirically, we set k = 3.
Our approach to clustering is similar except that one cluster has a centroid which is always set to the origin.
Specif-j ically, we de ne m i to be the centroid of Si at iteration j j and then force m
 This approach is bene cial in 2 ways: (1) it forces the remaining clusters to migrate away from the origin where the non-content points are located, and (2) it provides an easy means for labeling the resulting clusters; speci cally, the cluster with the origin-centroid will always be labeled non-content because points near the origin most likely represent non-content points, i.e. N   S1.
All remaining clusters are therefore labeled content, i.e. C   S2, .
.
.
, Sk.
There exist some implementation details which are not discussed as part of the overall algorithm formulation.
First, we do assume that a given webpage does have some tag structure.
Without HTML tags we cannot calculate the Tag Ratio array and the method will fail.
To cope with these instances we assume that tagless webpages contain only content and we return the entire text.
Second, there exist some webpages wherein the HTML markup is written in a single line.
Without multiple lines the computed Tag Ratio array would only contain one element and CETR would be forced to either return all text or no text.
Fortunately, we are able resolve this issue by detecting these instances and inserting line breaks every 65 characters.
If the 65th character is located within a tag, then the line break is inserted at the next non-tag location.
In this section we conduct experiments on real world data from various Internet corpora to demonstrate the e ective-ness of CETR.
In our experiments we use data from two sources: (1) news site data from Pasternak and Roth s 2009 WWW paper [28] on maximum subsequence segmentation (MSS) and (2) training and test data sets from the CleanEval competition.
MSS: In order to appropriately compare to the maximum subsequence segmentation method, which we were unable to obtain or implement, we retrieved identical data from Pasternak and Roth s repository4.
This dataset contains labeled webpages where labels mark the beginning and end of the content section(s).
Labels in this data set were gathered by examining a few pages per news source and a template-based wrapper was manually written.
Even with this semi-automated approach, this was still a tedious process taking nearly 12 hours to complete.
It is also noted that, in order to eliminate non-news article pages, any webpages which contained less than  fty words and symbols as well as any webpages which contained more than 20% tags were discarded.
The authors made no attempt to manually check for nor correct errors in the 24,000 wrapper-produced samples.
This data set contained 45 individual websites which were further separated into two non-overlapping sets.
(1) the Big
 (2) the Myriad 40 which were chosen randomly from from the Yahoo!
Directory.
The Myriad 40 contains  an international mix of English-language sites of widely varying size and sophistication  [28].
For our purposes we arbitrarily selected 50 documents from each of the Big 5 and 206 documents total from the Myriad 40.
Aside from these sources, we also selected 50 additional pages from the BBC and NY Times websites each because we felt that these two sources are highly popular and should be explicitly included in our evaluation.
CleanEval: The CleanEval project is a shared task for cleaning arbitrary webpages.
This was started by the ACL s SIGWAC and initially took place as a competition during the Summer of 2007.
This corpus includes four divisions: a development/training set and an evaluation set in both English and Chinese languages which are all hand-labeled.
Besides extracting content, the original CleanEval competition also asked participants to  markup  the webpage.
This task scored the participants on how well their algorithm iden-ti ed lists, paragraphs and headers; we consider this addi-
http://l2r.cs.uiuc.edu/~cogcomp/Data/MSS/ do not consider it further.
Because our approach does not require training there is no need to separate between training/development and evaluation documents.
Therefore, we e ectively have two CleanEval sets: (1) 741 English documents and (2) 713 Chinese documents.
For evaluation, standard metrics are used to evaluate and compare the performance of di erent methods.
Speci cally, precision, recall and F1-scores are calculated by comparing the results/output of each method against a hand-labeled gold standard.
Let WP be the set of words in the extraction result and WL be the set of words in the labeled extraction.
Precision and recall then follow from:





 (5) The F1-scores are computed as usual and all results are calculated by averaging each of the metrics over all examples.
We also present the scores from the Big 5, BBC and NY Times individually.
It is important to note that every word in the document is considered to be distinct even if two words are lexically the same.
One exception to this is the VIPS results, which often moves or removes text from its output; this makes it impossible to align words with the original page and therefore forces us to treat WP and WL as a bag of words, i.e., where two words are considered the same if they are lexically the same.
The bag of words measurement is more lenient and as a result VIPS scores may be further in ated.
The CleanEval competition uses a di erent approach when computing extraction performance.
Their scoring method is based on the Levenshtein distance between the extraction algorithm and the gold standard divided by the alignment length.
The Levenshtein distance is typically described as being the number of insertions and deletions of characters necessary to align two strings.
The CleanEval version of the Levenshtein distance operates on the insertion and deletion of words rather than individual characters (presumably for either conceptual clarity or computation time).
The alignment length is the number of insertion, deletion or align operations required to align two word sequences.
The Lev-enshtein distance is relatively expensive to compute, taking O(|A| |B|) time, which can be prohibitively large when |A| and/or |B| are su ciently large.
We  nd that our datasets typically include documents which are  su ciently large  (i.e., size greater than 10,000 words) and therefore we do not evaluate our performance using this metric.
In order to properly evaluate the performance of CETR we compare it s performance with several other content extraction algorithms.
Several of the algorithms described in Section 2 have been implemented in Java (FE, KFE, BTE, DSC, ADSC, LQ, LP, CCB) for the CombineE framework [15].
None of these algorithms require training, so the evaluation is done by inputting each document one-by-one into each algorithm and gathering the results.
VIPS was evaluated similarly except for two major differences.
First, VIPS was not implemented, rather the executable program was taken directly from the author s website.
Second, the output from VIPS is a set of page segments rather than extracted text.
As mentioned in Section 2, we exhaustively search for the perfect parameters for segmenting, and from the results, we exhaustively search for the best possible combination of page segments by comparing each combination with the gold standard and selecting the segment(s) with the best F1-score.
This certainly in ates the extraction performance over practical means.
MSS is neither implemented nor directly tested, instead the experiments described in this paper were deliberately designed to match those of [28].
In some instances, such as the Chinese language CleanEval, NY Times, and BBC, the MSS scores are missing because those datasets were not tested or not reported in the original work.
We are con dent that our results can be compared to MSS because we worked directly with the authors of the MSS experiments when preparing our experiments.
CETR is implemented in Java5 and is divided into three distinct algorithms.
The  rst is the one dimensional Threshold Method (CETR-TM) from Section 4.2.
The second is the one dimensional method which is clustered with k-Meansk=3 (CETR-KM) from Section 4.3.
The third iteration of this algorithm is the two dimensional method clustered with the tailored clustering technique (CETR) from Section 5.
Table 1 presents the results of the Threshold Method (CETR-TM) when given the task of extracting content from the CleanEval, Myriad 40, Big 5, NY Times and BBC data sets.
The Big 5 is broken down into it s individual sources.
Table 1: Results for CETR-TM on various domains F1-Measure Source CleanEval-Eng CleanEval-Zh CleanEval Myriad 40 NY Post Freep Suntimes Techweb Tribune Big 5 NYTimes
 Recall

 Precision



























 With the CETR-TM method we observe a very high recall rate.
This is because the threshold   is set to 1.0 , i.e.,     1.0.
Intuitively, if   is increased (e.g., 1.1 , 1.2 ) then the selectivity of the threshold would increase causing the precision to increase and the recall to decrease.
Conversely, if   is decreased (e.g., 0.9 , 0.8 ) then the selectivity of the threshold would decrease resulting in a lower precision and a higher recall.
Tuning this parameter is left to the user, and Section 6.5 discusses   in further detail.
Table 2 presents the results of the k-Means (CETR-KM) clustering method.
www.cs.illinois.edu/homes/weninge1/ F1-Measure Source CleanEval-Eng CleanEval-Zh CleanEval Myriad 40 NY Post Freep Suntimes Techweb Tribune Big 5 NYTimes
 Recall Precision
































 The CETR-KM method typically achieves either a high recall or a high precision but rarely both at the same time.
Nevertheless, these results show that CETR-KM typically outperforms CETR-TM.
Table 3 presents the results of the complete CETR algorithm.
Table 3: Results for CETR on various domains Source F1-Measure CleanEval-Eng CleanEval-Zh CleanEval Myriad 40 NY Post Freep Suntimes Techweb Tribune Big 5 NYTimes
 Recall Precision
































 These results show that the complete CETR algorithm performs far better than CETR-TM and/or CETR-KM.
There is some variability among the results, which have an F1-Measure range of 98.93% for Suntimes to 86.59% for Tech-web.
Perhaps most importantly, the CleanEval scores were high relative to the highest score in the CleanEval competition, which scored an 84.1% on the English dataset.
Remember, however, that the scoring metrics used in this paper, and in most similar literature, (precision, recall, F1) are di erent from the scoring metrics used in the CleanEval competition (Levenshtein distance).
The relatively low precision reported by NY Post, Freep and Techweb is likely due to the fact that these sources contain user comments, feedback, etc.
after each article.
CETR typically does not include short comments as content whereas the gold standard extractions include these comments.
Therefore, we contend that the actual precision of CETR is likely higher than the indicated precision.
This comment e ect becomes more evident when we see that more precise results are from sources such as NYTimes which hides comments by default, Suntimes which limits the comments to nine at a time, Tribune which limits the comments to three by default, and BBC which does not accept comments at all.
In order to judge the veracity of CETR we compare its performance with the alternative approaches described earlier in this section.
Table 4 presents these results with the winners for each data source in bold.
Some of the MSS results are not listed because the original work did not perform experiments on these data sources.
The CETR threshold method is abbreviated CETR-TM and the 1-dimensional CETR clustered with k-Means is abbreviated CETR-KM.
CETR is the highest performing algorithm in most data sets and overall.
The MSS algorithm performs highest on the Big 5 data set.
We believe this is because of the comment e ect mentioned earlier, for instance, if the low performing precision results from Table 3 were more in line with the median precision of CETR then the average F1-measure would outperform MSS and VIPS by an even greater margin.
Tables 3 and 4 clearly show that CETR is a viable and robust content extraction algorithm by performing relatively well even on non-news corpora (CleanEval) and across multiple languages (English and Chinese).
Admittedly, MSS does perform relatively well especially on the news corpora, however, we should emphasize that, unlike MSS, CETR is a completely unsupervised algorithm and therefore does not require labeled training examples.
We must also view the VIPS results with some hesitation because, as stated earlier, VIPS was evaluated assuming the perfect parameters and segments were selected for each webpage.
The results also show that occasionally CETR-TM or CETR-KM does perform the best.
We believe this to be because of nuances among website-page architecture.
For instance, NY Times and BBC websites have structures that are most conducive to CETR-TM and CETR-KM.
However, the results of broader corpora, i.e., Myriad 40 and CleanEval, show that CETR performs the best in the general case.
Even though CETR-TM does not perform the best overall, for practical purposes end users may consider its use when recall is a top priority.
By reducing the threshold co-e cient   users can see a marked increase in the recall and a sharp decrease in the precision.
This precision/recall trade-o  is shown in Figure 6.
When   = 0 the recall is always
 main, shown in Figure 6, a good tradeo  might be   = 0.5.
Finding a good threshold value is di cult because   must be empirically found for each domain.
Although CETR-TM and CETR-KM perform relatively well overall, we  nd that these methods are highly susceptible to webpages which do not have smooth tag ratio sections.
Taking the American Declaration of Independence example from earlier, we  nd that content text which is presented in lists are sometimes missed by the CETR-TM and CETR-KM methods.
This is because the threshold/clustering procedures regard the Tag Ratio array as a bag of values instead of an ordered sequence of values.
As a result low-lying ratios can be missed even after smoothing.
The complete CETR algorithm solves this problem by explicitly identifying the content s borders by way of the absolute derivative array.
With this new information the CETR algorithm is better able to identify the beginning and end of



















 e r o c
 Recall Precision



  

 Figure 6: Precision and Recall tradeo  for NY Times corpora as the threshold coe cient ( ) is increased from 0 to 2 content sections.
This information coupled with the original TR array create a novel model by which content sections can be identi ed.
Furthermore, there is no rule which states that a webpage may only have a single content section.
There exists several instances in which content is divided by a menu or an advertisement which indicates the  fold    referring to newspapers which are delivered folded in half.
Unlike many current methods, VIPS especially, CETR is not a ected by multiple content sections.
Despite the many advantages related to our algorithm, we do recognize some weaknesses.
CETR does not perform well on portal home pages.
For example, the Yahoo!
homepage contains a vast array of menus and short news descriptions; CETR has di culty discerning the content section(s) of these types of webpages.
Google News is another webpage where content is di cult to discern; CETR typically extracts far more text than what users would consider content i.e., recall is high and precision is low.
Finally, we observe that webpages which do not have advertisements or menus, such as computer science professors  homepages, do not achieve high extraction accuracy.
In these instances, CETR typically removes courses taught, patents awarded, and sometimes publications lists.
The only way around this CleanEval-Eng CleanEval-Zh CleanEval Myriad 40 Table 4: F1-measures for each algorithm in each source.
Winners are in bold.
     



  





























 NyTimes







 Big 5












 Average














 problem is to determine whether or not a given webpage contains non-content text, and then if it is determined that the webpage in question does contain non-content text invoke CETR to extract the content.
The e ectiveness of extracting content text from HTML documents using the Content Extraction via Tag Ratios (CETR) algorithm has been demonstrated.
Furthermore, results show that when compared to several other leading content extraction methods CETR performs best on average.
Besides the demonstrated e ectiveness of the algorithm, perhaps CETR s greatest strength over other methods is the simplicity of the concept, implementation and execution of the algorithm.
The complete CETR algorithm contains no parameters to adjust (k   3 and     3 works for most cases), no training to be done, and no classi er models to build; all that is required is to give, as input, an HTML document and the approximate content will be returned.
Ultimately, CETR provides a fast, accurate method for extracting content from a variety of sources with little e ort.
The task of automatic content extraction remains a hot topic especially with the colossal amount of information being added to the Internet every day.
With that in mind, there some portions of this speci c approach that need further exploration.
We intend to incorporate this method into standard search engines in order to see what e ect, if any, it has on the result relevance.
For instance, many webpages include word strings and links in order to boost their search engine rank, if we can  lter the irrelevant text from the page during indexing then it may be possible to present more relevant search results.
Another area for further investigation is the clustering algorithm used in CETR.
We do not claim that our clustering method is optimal, in fact, a linear max-margin clustering approach may give better results.
The authors sincerely thank Thomas Gottron, author of the CombineE framework and by extension some of the implementations used in this work.
We also thank Je  Paster-access, and the CleanEval team for providing a standard evaluation data set.
This work is funded by an NDSEG Fellowship award to the  rst author.
