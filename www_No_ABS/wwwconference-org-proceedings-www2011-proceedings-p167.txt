How should a publisher optimally select and display advertisements or o ers when advertisers agree to pay for chunks of clicks or conversions?
A real world example of such chunked reward mechanism is Groupon (www.groupon.com).
At Groupon, purchase vouchers are sold at heavy discount but they come into e ect only when a  xed, predetermined, number of individuals sign up within a  xed time.
Once this threshold number of sign-ups is met, the whole group of buyers receives a discount.
In the presence of multiple concurrently active o ers, the Groupon selection problem, when transformed to an online and realtime setting, is to show a small number, possibly one, of the available o ers to Groupon users.
While we use the Groupon model as an example, it is easy to generalize this scenario as an online advertising scheme where the advertiser pays the publisher only upon receiving a predetermined number of item purchases (or conversions).
Similar models already exist, most notably the dynamic cost per impression (dCPM) model offered by RightMedia Exchange (www.rightmedia.com), where the publisher is paid per-impression, as long as a  xed conversion goal is maintained.
Here, we generalize the dCPM model where the publisher gets paid when a  xed number of conversions happen within a given time period.
Our case is di erent in that, instead of guaranteeing a minimum number of conversions, the publisher agrees to a  xed number of conversions and within a speci ed time limit.
Such a payout model is more desirable for the advertiser because, assuming a successful campaign, the inventory, marketing costs, and pro ts may be estimated on a per-week or a per-quarter basis and for known quantities.
While desirable for the advertiser or the seller, our problem formulation presents sig-ni cant algorithmic challenges for the publisher or the ad serving exchange.
As we shall see in this paper, such a chunked reward format poses unique challenges for revenue maximization.
The goal of this paper is to investigate how revenue for the publisher may be optimized under a chunked reward ad pricing model.
We will show that the chunked reward model proves to be a computationally hard problem.
Theoretical performance guarantees will be developed for greedy or sensible heuristic policies for our model, and extensive ex-generation with explicit campaign run-time constraints.
More formally, we consider a scenario with k concurrent ads or coupon o ers.
Exactly one of these ads must be shown to a user.
Let the probability of a user signing up for the o er i be pi, and upon ni sign ups (i.e., ni successes), the publisher (or the ad exchange) receives a  xed, known, reward ri.
We also assume, for the sake of simplicity in analysis, that once the reward is collected, the publisher has no interest in showing the o er anymore.
With this latter assumption, we depart slightly from the Groupon.com model where the ad campaign may run for a  xed duration rather than until thresholds are met.
The goal of the publisher is now to maximize reward within a given,  xed, time frame by presenting the best sequence of ads, out of the currently active k ads.
We will show how this maximization problem is NP-hard.
We will then devise a number of heuristic selection policies and show formal approximation bounds for these.
Through extensive experimentation, we will also show how some of the adaptive greedy policies devised here work quite well in practice.
To summarize the key contributions of this paper:
 lem suitable for online advertisements.
To the best of our knowledge, this problem of optimizing revenue under chunked probabilistic reward and  xed time horizon has not been studied earlier.
of the stochastic knapsack problem [2] and, therefore, intractable to exactly maximize.
We devise a number of heuristic ad-selection policies and provide worst-case performance bounds on expected revenues for those policies.
We also prove 1/3- and 1/4-approximation bounds for some policies that are easily computable.
ad selection policies and demonstrate their strengths and weaknesses.
We also identify a policy that works quite well empirically and, at the same time, is fast to compute.
The remainder of this paper is organized as follows.
In Section 2, we give a formal setup of the problem.
We then describe some related work in Section 3.
In Section 4, we describe several greedy and modi ed greedy policies, and for the modi ed greedy policies we give worst case performance bounds.
After discussing the complexity of these policies in Section
 show that a simple greedy policy is often very good.
Finally, in Section 7, we conclude and give some directions for future work.
Assume that we have k ads that we wish to display on a particular website.
Assume that the ith ad has a CTR of pi, and if it gets at least ni clicks on the web site before a  xed time, then we get a reward of ri.
The number of users who will visit the site by that time is some random variable T .
When the tth user arrives, we must choose an ad to display in order to maximize our revenue.
We assume that we can only display one ad at a time.
We will think of the problem in the following slightly more abstract setting.
There is a machine with k arms.
Pulling the ith arm results in a success with probability pi and a failure with probability 1   pi.
The arm has an associated goal of obtaining ni successes, only on accomplishment of which, a reward of ri is given by the machine.
This reward may be obtained only once for any arm.
We are allowed a maximum of T arm pulls, where T may be random or  xed and known.
The objective is to pull a sequence of arms such that the total reward is maximized.
Note that, when pi = 1 for all i, and T is  xed and known, the problem reduces to the standard (deterministic) knapsack problem.
Since that problem is known to be NP-hard (see [7]), our problem must, in general, be NP-hard as well.
Let pe, re, and ne denote the vectors of probabilities, re-pe and FT , but we assume that we have good estimates of wards, and the goals for a set of arms.
We assume that these vectors are known beforehand, along with FT (the distribution of T ).
In practice, of course, we would not know these.
Further, we assume that whether we get a success or a failure for any arm on any trial is independent of T .
For each arm i, let Wi be the random number of times that we must pull arm i to get the nith success.
Clearly, Wi has a negative binomial distribution.
Since there are di erent parametrizations of the negative binomial, we will specify what we mean.
We will write Wi   N B(ni, pi), where pi   [0, 1] and ni   N \ {0}, to mean that, for t   N, P (Wi = t) =
 i (1   pi)t ni pni if t < ni o.w.
(1)
     t   1 ni   1 Note that EWi = ni/pi.
We assume that the Wis are mutually independent both of each other and of T .
For every time t, let the arm pulled at t be  t and the result of this pull be  t.
Once the arm to be pulled is determined, the probability of success in that pull is known and given by P ( t = 1| t = i) =p i and P ( t = 0| t = i) = 1   pi.
and let de,  e, and  e be the corresponding vector forms.
For each t, letd t be a realization of the random variable  t, A policy   is either a random or a deterministic function that chooses the arm to be pulled at time t + 1 given all the available information at time t. Thus, the arm chosen by policy   at time t + 1 may be written as  t+1 =  (pe, re, ne, FT|{( i, di)}i=1,...,t).
(2) Note that a policy only knows T through its distribution.
In the important special case when T is deterministic, its distribution is a point mass and it is thus known exactly.
We will use the notation   to denote the class of all policies.
Without loss of generality, assume that P (Wi   T ) > 0 for all i.
Let Si(t) and si(t) denote the random number of successes and the observed number of successes, respectively, of arm i in the  rst t pulls.
They are de ned as follows: Si(t) = si(t) =  j1[ j =i], (i = 1, 2, .
.
.
, k), dj1[ j =i], (i = 1, 2, .
.
.
, k), (3) (4) tX tX j=1 j=1 Choose

 p1 Pull 1   p1 p2 Pull 1   p2 r1 r1p1 r2p2 r1p1 Figure 1: Example of how   computes the expected reward for T = 2, given the values at T = 1.
Here, n1 = 1, n2 = 2, and r1p1 < r2p2 and let Se(t) and se(t) be the corresponding vector forms.
Thus, given de, the reward obtained by   is kX R( , pe, re, ne, FT| e,  e = de) = kX ER( , pe, re, ne, FT| e,  e = de) = riP (Si(T )   ni;  ).
and the expected reward is ri1[si(T ) ni], (5) i=1 i=1 We will call a policy optimal if it attains the largest reward.
Thus a policy   is optimal if   = argsup   ER( , pe, re, ne, FT| e,  e = de).
(6) Since the problem is NP-hard we cannot hope to  nd an e cient optimal algorithm.
Nevertheless, we will give an exponential time optimal algorithm.
Although it is of little practical use, it guarantees that an optimal policy exists, and moreover, in small scale experiments, it allows us to compare other policies with the optimal one.
When T has a bounded support, the optimal algorithm exists.
It can be de ned recursively as the algorithm which at time t chooses the arm     pi i  = argmax i=1,...,k   ri1{ni   si(t) = 1} +ER( , pe, re, ne   se(t)   eie , FT t 1| ) +(1   pi)ER( , pe, re, ne   se(t), FT t 1| ) o .
(7) This can be easily shown by induction on the time until T0, where T0 is the largest value that T can take with positive probability.
Note that this policy is Markovian in the sense that = ER( , pe, re, ne   se(t), FT t| ).
ER( , pe, re, ne, FT|( s, ds)s=1,...,t) R( , pe, re, ne, FT| e,  e = de) as R( ) To simplify the notation, we will often write (8) when the other parameters are understood from context.
An example demonstrating how the expected reward is computed is now presented.
Let T be known and  xed, k = 2, and let the parameters pe, re be  xed, and so chosen that ER( , pe, re, ne, FT ), it is easy to see that (a) ER(1, 2, 1) = r1p1 < r2p2.
Using ER(n1, n2, T ) as shorthand notation for r1p1, (b) ER(0, 2, 1) = 0, and (c) ER(1, 1, 1) = r2p2.
Fig.
1 shows a portion of the policy tree to be constructed by   .
The levels of the tree are alternately associated with the actions choose and pull, the former where   decides on which arm to pull, and the latter where the result of the arm pulled would be observed.
Thus, when T = 2, pulling arms
 (1  p1)(r1p1), and ER(1, 2, 2|2) = p2(r2p2) + (1  p2)(r1p1), respectively, and   chooses the more valuable arm.
The general framework of our problem is very similar to that of the multi-armed bandit (MAB) problem.
For details on MAB see, for instance, [5, 6] and the references therein.
However, that is an inference problem, where the goal is to learn the value of pe.
We assume that pe is known and are interested in optimizing the total reward by time T when the rewards are chunked in the manner described in the previous section.
Our problem is actually a variant of the stochastic knapsack problem.
An overview of the standard (deterministic) knapsack problem can be found in [7].
There are a number of stochastic extensions, see the references in [2].
Our version is most similar to the version considered in [2].
They assume that there are k items.
Each item i has a  xed and known value ri and a random weight Wi, with Wi   Fi for some distribution function satisfying Fi(0) = P (Wi   0) = 0.
One-by-one, items are placed into a knapsack that can hold at most a weight of T , where T is  xed and known.
Once an item has been inserted we  nd out how big it is.
If it  ts, then we collect the corresponding reward and we can place another item in the knapsack.
If it does not  t, then we do not get a reward and we are done.
A number of approximation algorithms are given in [2].
Our version allows for T to be random (but independent of all of the Wis) and it assumes that Wi   N B(ni, pi) for each i.
The most important di erence, however, is that in the setting of [2], once we begin inserting an item, we must insert all of it, while in our setting, at each time point we have the option to switch to a di erent item (or, in our terminology, arm).
In general, the stochastic knapsack problem (as considered in [2]) is NP-hard.
However, there are situations where the optimal policy is a simple greedy algorithm.
When all of the sizes follow an exponential distribution, such a solution is given in [3].
A di erent proof of this result is in [4].
The proof in [3] only uses the fact that the exponential distribution has the memoryless property.
Therefore, it is easily extended to the geometric distribution, which is a special case of the negative binomial.
Moreover, because the result does not depend on T , it immediately extends to our setting, where T is random and we can change arms at any time.
We now state this result.
Theorem 1.
In the context of Section 2, if for every arm i, Wi   N B(1, pi) then the optimal arm to pull at time t is the one with the largest ripi from which we have not yet received a reward.
For the optimal policy   given in Eq.
(7), the choice of the arm to be pulled at time t relies on all of the possibilities at time t + 1, each of which in turn relies on all of those at time t + 2, and so on.
In this section, we will study several policies that can be implemented without requiring such a lookahead.
We will restrict our attention to policies that satisfy the following feasibility criteria: 1. arm i would be considered at time t + 1 only if si(t) < ni and P (ni   si(t)   T   t) > 0, i.e., only if its goal is not attained yet, and there is a nonzero probability of attaining it, 2. all other parameters being equal, arm i would be cho- sen over arm j if pi > pj or ri > rj or ni < nj, and 3. if the rewards of all of the arms are multiplied by the same constant, the choice of arm will not change.
The reason for considering only policies that satisfy these criteria is that any policy that does not satisfy these, can be easily replaced by one that does and is uniformly better than the given policy.
The third criterion ensures that the policy does not depend on the units of the reward.
In this section, we will list several adaptive greedy policies.
These are policies that at time t+1 compute a speci ed index for each arm.
They then choose the arm which maximizes this index.
In light of the second feasibility criterion, we will only consider policies where the index for arm i is a non-decreasing function of pi and ri, and a non-increasing function of ni si(t), and involves all of them.
Moreover, we will assume that the index is linear in ri in order to satisfy the third feasibility criterion.
The greedy policies considered are:   1 : ripi Index    2 : Index ni si(t) 1[si(t)<ni]1[ni si(t) T t].
This is inspired by the fractional knapsack problem (see [7]).
P (Wi   T|Si(t) = si(t)).
This is a variation of  1 that decreases the index of i if the chances of attaining its goal are lower.
For large T ,  2 behaves similar to  1.
ni si(t) ripi    3 : Index riP (Wi   T|Si(t) = si(t)).
The term P (Wi   T|Si(t) = si(t)) is already a monotone decreasing function of ni   si(t) and a monotone increasing function of pi.
   4 : Index P (Wi   T|Si(t) = si(t)).
This is inspired by the simpli ed greedy algorithm given in [2].
E[Wi T|Si(t)=si(t)] ri In evaluating the indices above, it is useful to note that [Wi   t|Si(t) = si(t)]   N B(ni   si(t), pi).
An important point to make is that these indices need to be recomputed at every time point.
One can instead compute the indices only once, at the beginning.
Then, at each time t, pull the arm with the largest index (without updating the indices) for which we have not yet received a reward.
We will call such policies the non-adaptive versions and denote the non-adaptive version of  i by  i, i = 1, 2, 3, 4.
Clearly these policies do not satisfy the feasibility criteria and can be uniformly improved.
However, we should mention that they are not immediately comparable with the adaptive versions, and may, in some situations, be better.
Although the greedy policies that we de ned are easy to compute, they may be arbitrarily bad relative to the optimal algorithm.
It is easiest to see this in the case when pi = 1 for each arm i.
In this case the problem reduces to the deterministic knapsack and the policies  1,  2,  4,  1,  2, and  4 all reduce to the standard greedy algorithm for that problem.
To see that these can be arbitrarily bad see Chapter 2 of [7].
For policy  3 (and  3) consider the following setup.
There are k = T + 1 arms.
For arm 1, r1 = 2 and n1 = T .
For arm i with i = 2, .
.
.
, T + 1, ri = 1 and ni = 1.
Following  3, we would only pull arm 1 and at the end get a reward of 2, however the optimal algorithm is to never pull arm 1 and to instead pull each of the other arms once to get a reward of T , which may be arbitrarily larger than 2.
This behavior is not limited to situations where p is large.
For all of these policies, it is not di cult to construct similar situations when p   (0, 1).
However, although the greedy algorithms may be arbitrarily bad, algorithm  1 is, in fact, the optimal algorithm in the following cases: 1. when ni = 1 for all arms i = 1, .
.
.
, k (this follows by Theorem 1), 2. when any two of the parameters are identical for all arms and only one parameter changes.
Bounds Since greedy policies can be arbitrarily bad, we cannot provide worst case performance bounds for them.
However, we can provide bounds for certain modi ed versions.
First we introduce a policy that is not very useful in practice, but important for the theory:    0 : always pull the arm with maxi riP (Wi   T ) even after we have received the reward for this arm Modi ed greedy policies choose between two greedy policies at the beginning, then they stick with the same greedy policy until time T .
We will consider the following modi ed greedy policies:    5 : w.p .5, choose  0, otherwise choose  1    6 : w.p .5, choose  0, otherwise choose  2    7 : choose  1 if maxi riP (Wi   T ) < ER( 1), otherwise choose  0 wise choose  0    8 : choose  2 if maxi riP (Wi   T ) < ER( 2), other   9 : choose  4 if maxi riP (Wi   T ) <  , otherwise   choose  0.
Here       kX i 1Y


 riP (Wi   T )

 i=1 j=1 Wj
   1 , where the items are ordered as in  4.
Note that  9 is a version of the simpli ed greedy algorithm given in [2].
It is immediate that, in expectation, policy  7 is uniformly better then  0 and  1 and that policy  8 is uniformly better then  0 and  2.
T ), but this can be approximated through simulation.
Even when T is nonrandom, it may be di cult to implement policies  7 and  8.
The di culty lies in evaluating kX iX !
ER( j) = riP W(cid:4)   T i=1 (cid:4)=1 for j = 1, 2, where the Wis are assumed to be ordered as in  j.
Again, this can be approximated through simulation.
The following theorem gives bounds on the worst case performance of policies  5 8.
Theorem 2.
Assume that the Wis are mutually independent of themselves and of T .
We have ER( 5), ER( 6),

 mini P (Wi   T )   mini P (Wi   T )
 ER( )   ER( )   ER( )   mini P (Wi   T ) ER( )   1 + maxi P (Wi   T ) mini P (Wi   T )
 sup   sup   sup   sup     (9) (10) ER( 7), (11) ER( 8).
(12) Note that, by Eq.
(7), when T has a bounded support, the supremum is attained.
Clearly, after we have chosen a policy in the prescribed way, if we then use a policy that is uniformly better, then the result will still hold.
In particular, it is not di cult to come up with policies that are uniformly better than  0.
When pi = 1 for all i and T is nonrandom, the problem reduces to the deterministic knapsack and both  7 and  8 reduce to the usual modi ed greedy algorithm for the deterministic knapsack, and our bounds reduce to the bound in that case.
Proof.
In the interest of space, we will only prove the bounds in Eq.
(9) and Eq.
(11).
The proof of the rest is similar but slightly more involved.
First, consider a fractional version of our problem where instead of arm i, there are ni arms with reward ri/ni attainable after one success.
Thus we have arms (i, j) for i = 1, .
.
.
, k and j = 1, .
.
.
, ni with ni,j = 1, pi,j = pi, ri,j = ri/ni, and Wi,j   N B(1, pi).
Clearly, for any policy      , if we pull the exact same sequence of arms in the fractional case that the policy   tells us to pull for the original case, the expected reward in the fractional case will be greater than or equal to the expected reward in the original case.
By Theorem 1, the optimal policy in the fractional case is one that is greedy with index ripi/ni.
Thus, in the original problem, for any       we have R( )   R( 1) + maxi ri.
Hence ER( )   ER( 1) + max ri   ER( 1) + max   ER( 1) + n P (Wi   T ) P (Wi   T ) maxi riP (Wi   T ) mini P (Wi   T )   o riP (Wi   T ) mini P (Wi   T ) This is bounded by ER( 1), max       max
 ri i i
 .
i
 =
 mini P (Wi   T ) ER( 7) E[Wi   T ] = wP (Wi = w) +T (1   P (Wi   T )),
 w=n h and
 mini P (Wi   T ) .5ER( 1) +.5 max i i riP (Wi   T ) =
 mini P (Wi   T ) ER( 5).
Thus Eq.
(9) and Eq.
(11) hold.
In the context of ads, except in extreme situations, we would not consider ads that have very low probability of attaining their goals.
In particular, it is reasonable to assume that mini P (Wi   T )   .5.
Under this condition, the bounds reduce to

 and for m = 5, 6, 8 ER( )   ER( 7) sup   (13) ER( )   ER( m).
sup   (14) It is (15) In Section 6 of [2], a similar bound is given for  9.
shown that when T is  xed and known
 ER( 

 where   0 is the best policy among those that keep pulling a chosen arm until either the reward is given or T is reached.
Thus, these policies will keep pulling the same arm even if the probability of getting a reward with the arm becomes zero.
This is a slightly weaker result, but it holds regardless of what mini P (Wi   T ) is.
Remark 3.
Since we are interested in maximizing the expected reward, nothing that we have done would change if we assume that the rewards are random so long as they have a  nite expectation and are mutually independent, both of each other and of the Wis and T .
In this case we just replace ri by E[ri] in the discussion above.
In the context of ads, this takes into account the credit risk of the advertiser.
Qk In this section, we will analyze the time and memory requirements of the greedy policies and the optimal policy when T is  xed and known.
For simplicity we set n = maxi ni.
If we want more detailed complexity bounds, in the following discussion nk may be replaced by It is clear that  1 can be implemented with time complexity of O(kT ) and space complexity O(1).
However, note that the order of the indices does not change over time, except that sometimes an arm may stop being feasible.
Thus we can evaluate and order all of the indices at the beginning, this takes O(k) space and O(k log k) time.
Then at each t we only need to check if the arm is still feasible, thus the overall time complexity is O(k log k + T ).
i=1 ni.
The policies  2 and  3 have, essentially, the same complexity.
At time t + 1,  3 involves computing, for each arm i, P (Wi   T|Si(t) =s i(t)), which is the same as
 O(n) time.
Thus, the overall time complexity for  2 and  3 is O(knT ).
compute E [Wi   T|Si(t) =s i(t)], where The policy  4 is similar to  2 and  3 but we also need to Now, we will discuss implementing the optimal policy.
As mentioned in Section 2, this policy is exponential in T .
With additional memory, however, one may come up with pseudo-polynomial time dynamic programming algorithms similar to those used for solving deterministic knapsack problems (Section 2.6 in [7]).
With O(nk) memory, all the distinct nodes in level t + 1 of the policy tree may be precomputed and stored, and for each node at level t, just k comparisons su ce to choose the optimal arm.
To precompute these values, however, one needs to start from level t = T , and move up the policy tree, retaining the nodes from only the previous level.
Thus, each level in the policy tree requires O(knkT ) time, and the total time complexity for T levels is O(knkT 2).
With O(nkT ) memory, every node of the policy tree can be stored in memory, and so requires O(nkT ) time.
In this section, we will compare the performance of the greedy policies given in Section 4.1.
We will show that although in certain situations these policies may be bad, in practice they are often very good.
For simplicity we assume that T is  xed and known.
Nevertheless, there are many possible values for pe, re, ne, and T , all of which may result in di erent expected rewards for the di erent policies.
In order to compare the policies, we ran extensive experiments for a wide variety of parameter combinations.
To evaluate the expected reward of a policy for a particular combination of parameters, we used an approach similar to that of evaluating the optimal algorithm as described in Section 2 and Fig. 1, but here the arm to be pulled was chosen based on the index of the policy.
Another approach would be to simulate paths and then to average over these, however we would need many runs and only get approximations, while this way we get the true values.
, 1
 First, we present the results for the case when there are only two arms (k = 2).
We enumerate the expected reward for various scenarios by performing a parameter sweep.
The success probabilities are chosen from { 1 , 1}, and r2 is set to one of { 1 , 1, 4, 16}.
The units of the reward are so chosen that r1 is  xed at 1.
In addition, n1 and n2 are both varied from 1 to T , for various values of T .
When T = 300, a total of 11.25M distinct parameter combinations are covered.
Although the expected reward is guaranteed to be positive for every such combination, it can be extremely small.
All  gures are rounded o  to 8 decimal places before being used for reporting.
, 1
 , 1
 , 1


 We begin by depicting the behavior of various policies on a small portion of the parameter space.
Fig. 2 plots the expected reward obtained by   and  i, i = 1, 2, 3, 4 with only n2 varying while all the other parameters are  xed.
In both Figs.
2a and 2b, p1 = 1
 Fig. 2a has n1 set to 10, while Fig. 2b has n1 set to 20.
For arm 1 we expect to need about 4 trials per success, while for arm 2, we expect to need about 16 trials per success.
4 , p2 = 1 When n2 = 3, all the policies start o  by pulling arm
 obtaining a reward from arm 2 becomes less probable, and all the policies pull arm 1  rst, ensuring the reward of 1.
In Fig. 2b, we also see that  1 may be signi cantly worse than the rest.
Whenever n2 < n1,  1 persists with arm 2 as long as the feasibility criterion is satis ed, and obtains a zero reward with a high probability.
On the other hand,  2 and  3 appear to be consistently close to optimal in both the scenarios in Fig. 2.
We now discuss how we compare various policies and the di erences between them.
The performance of each greedy policy is measured in terms of how similar its expected reward is to that of   .
We quantify this similarity using the   Agreement: For each parameter combination, agree-following three measures: ment is a binary indicator of the expected reward of  i matching that of   .
  E ciency: This is the ratio and the higher the better.
ER( i) ER( ) .
This lies in [0, 1],   Regret: This is the di erence ER(  )   ER( i).
By , regret is non-negative, and the lower de nition of   the better.
We are now confronted with the problem of presenting the performance measures.
Clearly, showing individual values as in Fig. 2 is not feasible.
For, T = 300, even if all the 300 values of n2 are covered in a single plot, 375K such plots would be required!
We will now explain our methodology for reporting these.
The problem being considered is the following.
For k =
 may choose any n1 and n2 between 1 and T .
If n1 and n2 are both very small, no matter which order the arms are pulled in, there is a high probability of obtaining a reward of r1 + r2.
On the other hand, if they are both very large, there is very little chance of obtaining a nonzero reward, even for the optimal algorithm.
Naturally, in both of these cases, a greedy policy can easily perform about the same as the optimal one, and so these can be considered trivial cases.
Intermediate choices of parameters, however, make for a challenging situation where choosing the wrong arm has a greater impact.
While the trivial cases may be abundant in number, they are likely to be rare in practice, not being valuable to either the advertiser or the publisher.
Consequently, clubbing the trivial cases together with the challenging ones would mean that the aggregate performance measures would appear to be far better than those for cases typically encountered in the real world.
For this reason, we propose a systematic way of identifying (and excluding) the trivial cases.
Observing that the expectation and variance of the number of trials required for obtaining ni successes are  i = ni i = ni(1 pi) , respectively, we identify the di culty level based on the interval  i   2 i.
For each arm i,  i,  i, and T are used to identify the di culty level of the arm as follows:   V (very easy): T  P j(cid:7)=i( j + 2 j) >  i + 2 i pi and  2 p2 i   E (easy): T >  i + 2 i and not V   M (medium):  i   2 i < T    i + 2 i   D (di cult): T    i   2 i Note that the label V applies to either all the arms or none.
Other such de nitions, with intervals of di erent sizes and more gradation, have been tried in our experiments, but the symmetric intervals for individual arms o er simplicity and ease of interpretation.
The box-and-whisker plots in Fig. 3 show how the optimal policy fares in various scenarios with T = 300.
We observe the following: (b) n1 = 20 Figure 2: Expected Reward of various policies as n2 increases while all other parameters are  xed, with n1 set to (a) 10, and (b) 20 (a) r2   1 Figure 3: Total Expected Reward of   easy, V: very easy, k=2, T=300 for various rewards and di culty levels, D: di cult, M: medium, E: (b) r2   1 creases as the di culty decreases for the other arm.
  DD (di cult for both the arms), covers about 5.5M cases with near zero expected reward for each of them.
  If one arm has level D,   the other arm.
obtains reward only from   Level V V fetches almost the maximum possible reward of r1 + r2 for most cases.
  The variance in the reward is highest when arms with level M are involved.
This agrees with our reasoning that there is lower uncertainty for levels D and E.
Fig. 4 shows the performance of greedy policies  1 and  3 relative to   .
Note the di erence in the scales of the two plots.
We also ran the experiments for  2 and  4 but their plots are not shown due to space constraints.
In general  2 and  4 did much better than  1 but slightly worse than  3.
  All the policies, except  1, match   for any level involving D for one of the arms.
This is because in these cases, the greedy policies, just like   , realize that pulling arm with level D is futile and allocate these pulls to the other arm instead.
   1 fares poorly in all levels (except V V ), and even in case DD, where the optimal expected reward is near
 an extremely high reward,  1 may have an arbitrarily bad performance.
This is a consequence of not taking available time T into consideration.
  The only levels where  2,  3, and  4 fare signi cantly worse than optimal are M E, EM and M M, that too when the reward ratios are far from 1.
When both arms look valuable enough to be pulled, these policies  simple indices cannot match the information that   uses.
It may be noted that the lowest points in the plot represent the actual minimum for that level.
So, over all the 11.25M cases considered,  3 has a worst case e ciency of 0.6, and the Avg.
E ciency in the corresponding categories, EM and M E, is over 0.98.
Avg.
E ciency of  3 is over 0.99 in each of the remaining categories.
Although Fig. 4 shows the comparison only in terms of Ef- ciency, the plots for Agreement and Regret lead to similar conclusions.
For example, the only cases with Avg.
Regret signi cantly above 0 are M E (for r2 = 1 4 , only), EM (for r2 = 16 and r2 = 4, only), and M M. Unlike ef- ciency, regret is sensitive to the value of r2, and hence, is more di cult to interpret.
For EM , when r2 = 16, Avg.
Regret is 0.117, whereas for its symmetric counterpart, M E with r2 = 1 16 and r2 = 1 16 , it is 0.007.
One may also note that the parameter combinations in Figs.
2a and 2b correspond to di culty levels EE and M E, respectively, and therefore show how well the greedy policies perform even in the challenging portions of the parameter space.
Our next set of experiments deal with checking if the greedy policies maintain the high e ciency levels as the number of arms increases.
With T = 1000 and k varied over 2, 3, 4, and 5, a complete enumeration is no longer feasible.
Instead, we selected 125 random combinations of pi s and ri s and varied the goal for each arm from 1 to
 { 1 }, the reasons being that (a) CTR values typically lie in this range [1], and (b) for higher pi s, even an ni value of 20 would lead to the di culty level V and is not challenging enough.
, 1
 , 1

 We did not include  4 because of its extremely high computational cost.
Although other greedy policies scale linearly with the ni s and T , given that the ground truth used for measuring their performance is based on the optimal policy, and since our implementation of   assumes availability of O(nk) memory, larger values of k, ni s and T have not been considered.
Based on our experiences for the case of k = 2, we treated the challenging cases with at least one arm having level M and at least another having level E separate from the rest.
To be able to compare across the di erent k values, Fig.
5 reports the proportion of cases having e ciency over a threshold, as the threshold is varied from 0.75 to 0.99.
From Fig. 5, we see that  3 continues to outperform the rest as k increases, and maintains an e ciency of about 0.95 for nearly 95% of the challenging cases.
The present work introduces an interesting variant of the stochastic knapsack problem that may be used for goal based all-or-none pricing for online ads.
It provides feasible alternatives to the optimal but prohibitively expensive algorithm to maximize the total expected reward in this setting.
We also introduce a methodology for reporting these results, by accounting for the inherent complexity of each problem.
We showed that certain policies are assured a fraction of the optimal reward, while others, for which we have no theoretical guarantees, perform close to optimal for a wide variety of situations.
We also showed the importance of adap-tivity.
The non-adaptive policy  1 (it only ensures that a feasibility condition is satis ed) was consistently beaten, in experiments, by fully adaptive modi cations ( 2    4).
There are a number of directions for future work.
Since the probabilities may be unknown, it would be interesting to combine this with the MAB problem [5, 6], to try to estimate the probabilities even as we optimize.
Also, in practice, there may be a waiting time between when the ad is shown and when we  nd out if a click or a conversion happened, yet during this time a new user may have appeared, which ad should we display?
Another direction is estimating the fair price ri for an ad, given ni and pi as well as the other ads.
This would be useful, especially when pi and T are not known exactly.
