When navigating and seeking information in a digital document collection, the ability to identify topics with their time of appearance and see their evolution over time could be of signi cant help.
Think of a scienti c paper collection and a researcher who begins research in a speci c area.
She Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
would want to quickly overview the area, determine how topics in the area have evolved, and locate important ideas and the papers that introduced them.
Knowing a speci c concept in a paper, she wants to  nd out whether there were previous papers that discussed the concept or the topic is new.
As another example, a funding agency or people who administer a digital document collection might be interested in visualizing the landscape of topics in the collection to show the emergence and evolution of topics, bursts of topics, and the interaction among di erent topics that change over time.
These information seeking activities require the ability to identify topics with their time of appearance and to follow their evolution over time.
In this paper, we describe our unique approach to providing the basic technologies to achieve such a goal.
Our approach is applicable to a time-stamped document collection with an underlying document network.
Such document collection format encompasses a wide range of digital text available over the Web recently.
Examples are scienti c paper collections, text collections with underlying social networks such as blogs and twitter, and in general the web documents with hyperlinks.
In this paper, we demonstrate the utility of our approach by applying it to a scienti c paper collection.
We will use the word paper and document interchangeably.
Our approach emphasizes on discovering the topology of topic evolution inherent in a corpus.
The topology inherent in the corpus carries surprisingly rich information about the evolution of topics as it is demonstrated in this paper (Figure 1, 2, 3, 4, and 5).
We de ne a topic as a quantized unit of evolutionary change in content, and identify topics along with the time that they start to appear in the corpus.
We do this by visiting each paper in the corpus chronologically and decide if the paper initiates a topic by requiring that it has a textual content that is not explained by previously discovered topics and that this textual content persists in a signi cant number of later papers.
After obtaining topics by the chronological scan, we build graphs whose nodes are topics and whose edges re ect cross-citation relation between topics.
Globally, this generates a map showing the landscape of topics over time as in Figure 1 obtained from the ACM corpus.
The map shows a rich topology.
For example, the population of topic nodes in network research grows fast in the later years without a signi cant body of ancestors before, while the compilers or graphics research areas exhibit steadier evolution over time.
We can also  nd an individual topic evolution graph for a given seed topic as contain multiple threads indicating that the seed topic has been in uenced by multiple  elds.
The relationship between these threads may change over time as well.
The contribution of our paper is that (1) by de ning a topic as a quantized unit of evolutionary change in content, we obtain a topic evolution graph without imposing topological restrictions on the graphs nor imposing restrictions on the time distribution of topic nodes.
This is in contrast to a body of previous works [14] [18] [2] [1] [6] [5].
Such previous works either divide a corpus into time slots and  nd a  xed number of topics in each time slot or assume a predetermined topology for topic evolution such as a chain-like topology.
(2) We obtain a large-scale topic evolution graph from ACM corpus.
Also various local evolution graphs deduced from the topic relationships are explored.
In general, previous works either report on topic evolution graphs with a chain-like topology or evolution graphs in small-scale.
It seems that the topology of the evolution graph as complex and varied as ours was not reported previously.
(3) Our approach uniquely incorporates the underlying document network such as the citation network into the topic evolution discovery.
In the rest of the paper, Section 2 and 3 explain our algorithm.
Section 4 shows the experimental result with future work in Section 5.
Section 6 discusses the related work and Section 7 concludes.
We are interested in getting the summarized view of the corpus which shows the evolution of topics over time.
A topic in a corpus is a semantically coherent content that is shared by a signi cant number of documents in the corpus.
As time  ows, a topic goes through evolutionary change.
As the change accumulates, at some point in time, a document or a set of documents within the topic initiates a content that di ers appreciably from the original content.
Such content may die out or be shared by a signi cant number of later documents.
In the latter case, we could quantize this sig-ni cant change as a new topic.
Yet, the new topic is born in the context of the original topic.
By connecting the new topic with the previous topics that provided the context for the new topic, we can see the evolutionary process.
Taking this view, our approach captures the evolution of topics in a corpus by  rst identifying signi cant changes in the content evolution as topics and then connecting each topic with the previous topics that provided the context.
Each topic is associated with the time the corresponding change is introduced in the corpus.
As a result, we get a graph over topic nodes where nodes are associated with time.
Note that we usually need some regularizations to model the evolution of topics.
For example, the common approaches taken by previous works are to quantize time into a number of time slots and connecting topics across time slots and/or to assume a chain-like topology for a topic evolution.
In our case the regularization is that we quantize evolutionary change into topics whenever such change satis es the requirement of novelty and signi cance.
By novelty, we require that the new content di ers appreciably from the original contents providing the context.
By signi cance, we require that the new content is adopted by a su cient number of later documents.
The relative advantage of our modeling choice is that the topic nodes can be inhomogeneously placed in time and we do not impose restriction on the topology of the topic graphs, allowing the topology in the evolution inherent in the corpus to appear.
Technically, we use the mixture of word distributions [17] [7] [20] [14] to formulate the problem.
A corpus is a collection of documents.
A unigram vocabulary of a corpus is a set of all unigrams that appear in the corpus.
Given a corpus, a word distribution   is a multinomial distribution over the words in the unigram vocabulary V of the corpus.
We denote the probability of producing a word w by the word distribution   as p(w| ).
As   is a multinomial distribution, the distribution satis es the constraint Pw V p(w| ) = 1.
The probability of a document d by a word distribution   is de ned as the probability of independently producing each occurrence of the unigrams in d by   and is denoted as p(d| ).
In a document d, let wd,i denote the ith occuring unigram in d and let Nd be the number of unigram occurrences in d.
i=1 p(wd,i| ).
One particular word distribution we will repeatedly use is the one that maximizes the probability of the corpus.
We call it the background model of a corpus and denote it by  .
We can use Lagrangian multipliers to verify that p(w| ) = where cw is the number of occurrences of a word w in the corpus and V is the unigram vocabulary of the corpus.
Then, p(d| ) = QNd Pw V cw cw The probability of a document d by a mixture of word distributions  1,  2, ...,  k with the corresponding mixture coef-i=1 Pk j=1  jp (wd,i| j)   cients  1,  2, ...,  k is de ned asQNd with the constraintsPk j=1  j = 1 and  j   0 for j = 1, ..., k.
Each word in the document is produced by a probability that is a linear combination of the word distributions.
In this paper, we will repeatedly use the type of mixture where one of the word distributions is a background model   with a  xed mixture coe cient b, while the mixture coe cients for the remaining word distributions  1,  2, ...,  k are determined by maximizing the probability of the document by the mixture.
We denote the probability of a document d by this type of mixture as p(d| 1, ...,  k;  , b).
By the above de nition, it is given as p(d| 1, ...,  k;  , b) = k  jp (wd,i| j)!
+ bp (wd,i| )!
(1) Nd Yi=1 (1   b) where  1, ...,  k = Xj=1 Yi=1 (1   b) Nd argmax  1,..., k  jp (wd,i| j)!
+ bp (wd,i| )!
k Xj=1 for k   1.
We also de ne the trivial case of k = 0 as p(d|;  , b) = p(d| ).
In addition to the documents of a corpus, our approach requires the publication date of each document and a network over the documents where an edge between two documents indicates that they are semantically related with probability much higher than random chance.
In this paper, our evalution is on a scienti c paper collection, and we use the citation network over the papers.
We index the documents in the corpus in chronological order as d1, d2, ..., where if i < j then the publication date of di is earlier than or equal to that of dj.
chronologically.
At each document, we test whether the document initiates a content that di ers enough from the previously identi ed topics and is shared signi cantly by later documents.
If so, we generate a new topic.
We use a tuple (ds,  s, F ) to characterize a topic   in our topic discovery.
Here  .ds is the paper that initiates the topic   .
We call it the start paper of   .
 . s is a word distribution that represents the content of the start paper  .ds and  .F is a set of papers that appreciably carry the content of the start paper.
The exact de nition of these terms will be given in the following topic de nition.
In order to de ne a topic in terms of the previously de ned topics, assume that we have chronologically scanned the documents from d1 up to dt 1 and found k topics  0, ...,  k 1.
We then examine the document dt to decide whether it initiates a new topic.
Let the word distribution  t represent the content of dt.
We de ne  t by requiring that the mixture of  t and the background model   maximizes the probability of dt.
 t = argmax p(dt| t;  , b)  t = argmax  t Ndt Yi=1 ((1   b) p(wdt,i| t) + bp(wdt,i| )) (2) The background model   in the mixture absorbs the words that appear in dt by random chance so that  t gets high support for the words that di erentiate dt from the rest of the corpus.
The mixture coe cient b for   is  xed and is a parameter we set.
We then  nd the documents that carry the new content introduced by dt.
In order to measure how much a document f carries the content of dt that di ers from the previously identi ed topics  0, ...,  k 1, we use the document probability gain g (f,  t, { 0. s, ...,  k 1. s}) de ned as g (f,  t, { 0. s, ...,  k 1. s}) = log p(f | 0. s, ...,  k 1. s,  t;  , b) p(f | 0. s, ...,  k 1. s;  , b) .
(3) The denominator p(f | 0. s, ...,  k 1. s;  , b) computes the probability of the document f using the mixture of  s s from the previous topics and the background model   while the numerator p(f | 0. s, ...,  k 1. s,  t;  , b) additionally uses  t from dt in its mixture to compute the probability of f .
The document probability gain is non-negative as long as { 0. s, ...,  k 1. s} is not empty (k > 0) because the optimization domain of p(f | 0. s, ...,  k 1. s;  , b) is a subspace of that of p(f | 0. s, ...,  k 1. s,  t;  , b).
Note that in order for the document probability gain to be high, (1)  t should be di erent enough from  s s of the previous topics, otherwise p(f | 0. s, ...,  k 1. s;  , b) approaches p(f | 0. s, ...,  k 1. s,  t;  , b), (2) the document f should contain the content that can be produced with good probability by  t but not by the  s s from the previous topics.
In  nding the documents that appreciably carry  t, we use the documents that cite dt as a candidate pool.
We let F be the set of q documents whose document probability gain is the top q largest among the documents that cite dt.
Here q is a parameter to be set.
We call the documents in F as top followers of dt.
In order to test whether dt initiates a content that di ers enough from the previously identi ed topics and is shared signi cantly by later documents, we check the conditions Eq.4 - 6.
Here Ca, Cf and m are parameters.
g (f,  t, { 0. s, ...,  k 1. s})   Ca g (f,  t, { 0. s, ...,  k 1. s})   Cf Xf  F  {dt} Xf  F  f   F , g (f,  t, { 0. s, ...,  k 1. s})   m (4) (5) (6) Eq.4 requires that the improvement in the log probability of dt and the top followers due to  t over the  s s of the previous topics is lower-bounded by Ca.
We separately require that such improvement in the log probability restricted to the top followers is lower-bounded by Cf in Eq.5, because the document probability gain g (f,  t, { 0. s, ...,  k 1. s}) for a top follower f   F is a more reliable indication of whether  t initiates a new topic than the document probability gain for dt, g (dt,  t, { 0. s, ...,  k 1. s}).
The reason is that the high probability words in  t computed by Eq.2 are either the words related to the topic of dt or the noisy words unrelated to the topic whose background probability is low.
While the document probability gain for dt gets the contribution from both groups of words, the document probability gain for a top follower gets the contribution mostly from the topic-related words, hence, more reliable, because the noisy words in dt is not likely to repeat in another document connected to dt by a citation network.
Eq.6 ensures that each top follower contributes in carrying the new content of dt by setting a lower-bound on the document probability gain.
If these conditions are met, we generate a new topic  k with  k.ds = dt,  k. s =  t,  k.F = F .
Note that we use the  lookaheads  that are the later documents of dt in determining whether to initiate a new topic at dt so that we do not introduce non-signi cant or noisy topics.
Before the algorithm scans the  rst document d1, we initialize  0 with the background model   so that the algorithm starts with nonempty previous topics.
Optimization: The optimization problems of Eq.2 and Eq.1 with k   2 are solved using the logarithm of the original optimization functions.
Eq.2 is solved by Lagrangian multipliers.
Inequality constraints ( j   0) are considered in applying Lagrangian multipliers, because unlike the maximum likelihood estimate of the background model  , the solution in Eq.2 only with the equality constraint may lie outside the inequality constraint boundaries.
Eq.1 resembles the optimization problem arising in PLSI, but it is an easier problem because the word distributions are  xed.
In particular, Eq.1 is a convex optimization for which e cient algorithms exist [4].
Our implementation iteratively moves the estimation point for ( 1, ...,  k) in the direction of the gradient projected onto the constrained domain.
The next estimation is made by  nding the point along the chosen direction with maximum function value using Newton s method.
The running time is very reasonable for a large-scale corpus as seen in Section 4.
After discovering topics, we discover the relationships between topics in order to track the topic evolution.
We discover the relationships between topics by  rst obtaining the member documents of each topic and then for each pair of ments.
For a topic   , we include its start paper  .ds and the papers that cite  .ds as its member papers.
In addition, the papers that are textually close to   are included as its member papers.
In order to textually represent the topic   ,  . F is de ned as the word distribution whose mixture with the background model maximizes the probability of the top followers  .F and the start paper  .ds and is given by  . F = argmax   Yf  .F  { .ds} p(f | ;  , b).
We use  . F instead of  . s because  . F is less prone to noise as it is the word distribution based on the aggregation of papers.
Also, Inequality 5 and 6 ensure that the papers in  .F faithfully carry the content of  .ds.
To determine whether a paper d is textually close enough to qualify as a member paper of   , we use the document probability gain g (d,  . F , {}) (Eq.3) normalized by the number of words in the document d.
g (d,  . F , {}) /Nd =
 Nd log p(d| . F ;  , b) p(d|;  , b) Here g (d,  . F , {}) is negative for a paper d not related to the topic   , while for a paper d related to   , g (d,  . F , {}) is positive.
We include a paper d as a member paper of   if g (d,  . F , {}) /Nd     where   is a positive parameter.
Thus, if we denote the set of member papers of   as  .M ,  .M is given as  .M = { .ds}   {d|d cites  .ds}   {d|g (d,  . F , {}) /Nd    }.
Once we obtain the member papers for each topic, we  nd the relationships between topics.
For a pair of topics, we use their cross citation count as their relationship data.
The cross citation count between  i and  j is de ned as |{(da, db)|da cites db or db cites da, da    i.M, db    j.M }|.
Using the cross citation count we derive a metric that represents the strength of the relationship between the pair of topics.
By applying a threshold to the metric, we generate a graph of topics.
Let n1 and n2 be the number of member papers in topics  1 and  2 and let c be their cross citation count.
Then there are n1n2 pairs of papers di and dj where di    1.M and dj    2.M .
We say that there are n1n2 cross pairs between  1 and  2.
The metric we use to represent the strength of the relationship between the topics  1 and  2 is based on the following log likelihood ratio.
log p(c cross citation count| 1 and  2 are related) p(c cross citation count| 1 and  2 are random) (7) The numerator of the log is the probability to generate c cross citation count between  1 and  2 when the two topics are related, while the denominator is the corresponding probability when the two topics are randomly selected with respect to each other.
In each case, we assume that the cross citation edges are generated by a binomial process.
That is, there are n1n2 cross pairs as trials and in each trial a citation edge is independently generated with a  xed Bernoulli probability.
When the two topics are related, we use p1 as the Bernoulli probability of the binomial process.
When the two topics are random, we use p0 as the Bernoulli probability.
The probabilities p1 and p0 are estimated as follows.
The corpus has N papers and E citation links.
A citation link is

 N (N  1) = d N  1 .
We set p0 = d treated as an undirected edge.
Let d be the average degree of a paper.
By de nition, d = 2E N .
When the two topics  1 and  2 are randomly selected with respect to each other, it is reasonable to assume that if we pick a paper di from  1.M and dj from  2.M , the probability that the pair di and dj has a citation edge is the probability that a random pair of papers in the corpus has a citation edge, which is given as N  1 .
To estimate p1 we make the following argument.
A paper di has a number of neighbor papers in the citation network.
However, the neighbor papers are not the exhaustive set of papers that are related to di.
There are other papers related to di in the sense that di and another paper discuss the similar subject or an idea is transferred between them.
We let Ri be the number of papers that are related to di.
We also let R be the average of Ri s.
By de nition of R, the number of related pairs of papers in the corpus is N  R
 E citation edges are contained within the related pairs of papers.
The probability of a related pair of papers having a citation edge is then given as R .
When the two topics  1 and  2 are related, we could assume that a cross pair of papers di and dj from the two topics are related.
Thus, we set p1 = d R .
We use R as a parameter as we don t know its value.
It is a parameter which we have an intuitive interpretation for.
N  R = d

 We now compute the log likehood ratio (Eq.7).
Among n1n2 cross pairs of papers between the two topics  1 and  2, c trials generate a citation link while n1n2   c trials do not.
Translating this into a binomial process with p1 and p0 respectively, log p(c cross citation count| 1 and  2 are related) p(c cross citation count| 1 and  2 are random) 1(1   p1)n1n2 c 0(1   p0)n1n2 c = log `n1n2 c  pc `n1n2 c  pc = log p1 p0 + log 1   p0 log 1 p0 1 p1 1 p1 !
1   p1  c   n1   n2   1 p1  as it is a constant over pairs + log 1 p0 log p1 p0 + log 1 p0 Removing  log p1 of topics, yields p0 r ( 1,  2; R) = c   n1   n2   log 1 p0 1 p1 log p1 p0 + log 1 p0 1 p1 (8) which we call the relationship strength metric.
The value log 1 p0 1 p1 p1 p0 1 p0 1 p1 is mathematically inbetween the probabili-+log log ties p1 and p0.
Thus, r ( 1,  2; R) can be interepreted as the cross citation count c discounted by the expected cross citation count n1   n2 log 1 p0 1 p1 log p1 p0 +log 1 p0 1 p1 when cross citation links are generated by a probability inbetween p0 and p1.
Note that such discount grows proportional to the topic pair size n1n2.
We generate a link between two topics  1 and  2 by imposing a threshold to the relationship strength metric r ( 1,  2; R)     where   is a parameter.
We applied our algorithm to the collection of papers in the ACM corpus from the year 1952 to the year 2007.
There are 129,544 papers in the collection with 291,122 citation links, for an average degree of 4.49.
For each paper, we use its title and abstract as its text.
The text is stemmed by Porter Stemmer [9].
The algorithm to detect topics was run with the parameters   = 0.8, q = 10, Ca = 360.0, Cf = 210.0 and m = 7.0.
743 topics were discovered.
The running time was 1 hour and 17 seconds on a common desktop with intel E2160 processor.
The parameters  , q, Ca, Cf and m are used to determine the granularity of evolutionary change in detecting topics.
We empirically determined their values by checking that the  rst few examples of the topic conditions Eq.4 - 6 perform the intended function.
The cost of such adjustment was very small compared to the corpus size.
The remaining parameters  , R and   are used to generate topic evolution graphs from the raw topic relationship data.
Because the topic relationship data with cross citation counts have multitudinal information that are not entirely captured by a single graph representation, we vary the parameters  , R and   as control knobs to explore the topic evolutionary graphs.
After  nding the topics, we obtained their member papers with the textual threshold parameter for member papers   = 0.7.
We now have the relationship between topics represented by the cross citation counts.
To turn this relationship into a graph of topic nodes, we computed the relationship strength metric r ( i,  j; R) (Eq.8) with R = 200 for each pair of topics  i and  j.
The topic evolution graph in Figure 1 1 was obtained by applying a threshold value   = 75 to r ( 1,  2; R).
The small rectangles in the graph with numbers in them represent topics.
The numbers are the topic ids and run chronologically from 1 to 743.
An edge exists between a topic node pair if r ( 1,  2; R)    .
The Y axis in the left of the graph represents time.
Though the corpus is from the year 1952 to 2007, the majority of topics are obtained in the time span from year 1972 to 2003.
This is because in the early years of the ACM corpus the population of papers is sparse.
Also we didn t discover many topics in the latest years of the ACM corpus because our method requires a sig-ni cant number of follower papers that cite the start paper of a topic.
We think that this problem can be remedied either by lowering the values of the parameters q, Ca, Cf and m, or by replacing the citation network with a document network that does not require as much time in generating edges.
The graph is shown with time span from the year 1972 to
  rst requiring that each topic node should be placed at the publishing year of its start paper, and then letting the  dot  application in the graphviz tool [8] draw the graph.
The  dot  draws an acyclic graph by minimizing edge crossing in a 2D layout [8].
2 In general, a lower threshold value   brings in more edges to the graph for more relationship structures.
But, such dense edges obscure the core structures of the graph in the
 topic relationship, we used a high value of threshold   = 75 in Figure 1 to have fewer edges.
As a result, many other
 get a better view, you may want to magnify the pdf  le.
 dot  are usually well connected and thus closely related to each other.
However, the nodes that are next to each other without any connection are not related.
They are placed next to each other simply because of the default behavior of  dot .
plore some of those structures later in the paper.
In the graph, 235 nodes out of the 743 nodes are isolated.
Figure 1 is a partial snapshot of the graph focused on the region with the large connected components.
By inspecting the abstract of the start paper and the tf.idf summary of the top follower papers of each topic, we manually labeled the  ve largest connected components as shown at the bottom of the graph.
These connected components are (1) graphics, (2) database, (3) architecture, compiler and programming lanauge, (4) network and distributed system, and (5) natural language processing.
Note the di erence and variety in the topology of these connected components.
For example, the connected component for network and distributed systems has two weakly connected subgraphs.
The left subgraph labeled sr contains topic nodes for  reliable secure protocols at the presence of faulty processes  and  cryptography  in distributed systems.
The right subgraph labeled sn contains topic nodes for network research.
Subgraph sn starts with the topic nodes around mid 80s and grows into a very popolated area with many topic nodes later.
There are some earlier topic nodes in network research that do not yet appear in the subgraph sn due to the high threshold   for edges, but there is an overall trend of increasing population over time.
On the other hand, the connected components for graphics, database, and architecture and PL exhibit steadier distribution of topic nodes over time.
Figure 2: The topic evolution graph for Database We now zoom in to one of the large connected components and see how the topology in a  ner resolution re ects the topic evolution trend in the area.
Figure 2 shows the Table 1: Textual information of selected topics in Database id title of the start paper

 functional dependencies
 relational databases














 for database concurrency control
 locking and partial rollbacks using write-ahead logging


 points and rectangles



 indexing large time series databases




 connected component for database in detail.
Also, Table 1 shows the titles of the start paper of selected topics in Figure 2 to give a more concrete idea on the textual content of the topics.
We explain the graph in Figure 2 by dividing it into 5 subgraphs as suggested by the visualized connectivity.
In reading the description below, refer to Table 1 for more detail.
Subgraph sd1 on the left is on the theoretical foundations of database systems such as data models and relational algebra.
It includes  entity-relationship model in 1976 (topic id 50),  discussion on third normal form (topic id 65), and  4th normal form (topic id 74).
The discussion on data dependency continues through topics 89, 102, and 184, with topic id 184 on  a new normal form for nested relation .
The thread of topics 61, 83, 187, 188, and 204 in Subgraph sd1 is relatively separated from the topics covered above.
This thread shows that logic programs were actively discussed in database design.
For example, topic id 83 discusses  logic program based queries over relational database .
Note that most of the topic nodes in subgraph sd1 reside in 1970 s and 80 s.
But there is a topic node 502 in 1999.
The topic node 502 is about  query rewriting for semistructured data , which seems to re ect the evolution of topics.
Subgraph sd2 is on building database systems.
Topic nodes 54 and 59 in
 later in time, topic nodes 378, 408, 432, and 435 in 1995 to 1997 are on  data warehouse  discussing view maintenance and data cubes, OLAP etc.
tems.
For example, it contains  consistency and locks  (topic id 64),  nonblocking commit protocols  (topic id 103),  multilevel atomicity  (topic id 125), and  transaction recovery with  ne granularity locking and partial rollbacks  (topic id
 sd2 and sd3 is on a distributed database system(SDD-1), combining the system building aspect of Subgraph sd2 and the distributed system nature of Subgraph sd3.
Subgraph sd4 is on data structure of data storage for e cient search.
The subgraph demonstrates content evolution over time with  multidimensional binary search tree  (topic id 47) in 1975,  R-tree  in 1984 (topic id 133),  R*-tree  in 1990 (topic id 257) for spatial search, and more recently,  nearest neigh bor queries  (topic id 379) and  distance browsing  (topic id 503) in spatial databases, discussion in time-series databases (topic id 350, 593), and discussion in moving object databases (topic id 515 and 535).
Subgraph sd5 is on e cient query processing with tuple size estimation, histograms, etc (topic id 134, 142, 407, and 476).
More recent topics in Subgraph sd5 are on query estimation for online datastream (topic id 595 and 637).
Overall, the observation on the connected component for databases shows that (1) the content cohrerence of evolving topics is re ected in the connectivity pattern of the graph, (2) dynamic change in topic node population along each sub-graph over time seems to re ect reality, (3) The content evolution along the topic thread is visible.
Figure 3: Partial snapshot of the global topic evolution graph with more edges showing the merge of connected components As we bring in more edges to the global topic evolution map (Figure 1) by lowering the threshold  , the existing connected components absorb more isolated nodes.
Also, the connected components get connected to each other.
For example, when we lower   to 40 to bring in more edges, we observe that the connected component of databases becomes connected to the connected component of distributed systems and network.
Figure 3 shows the snapshot of such a merge at   = 40.
Figure 3 contains network, distributed systems, and the subset of topic nodes from databases, from right to left.
Comparing with Figure 1, one can see that the subgraph representing network research is getting richer in Figure 3.
Its subgraph sn2, on the right starting in year
 layers to the more recent BGP routings.
Its subgraph sn1 on the left starting more recently in year 1994 is mostly on mobile, ad-hoc networks.
The connection between databases and distributed systems is mainly made by the nodes in Subgraph sd3 in databases (Figure 2) being connected to the nodes in distributed systems, which makes sense because Subgraph sd3 in databases is about concurrency control in database systems.
In fact, at   = 40, the connected components for architecture and PL and for databases and for network and distributed systems are all connected, while the connected component for graphics is still isolated.
Figure 1 mainly shows the large connected components of the topic graph with   = 75.
The nodes not shown in Figure 1 are isolated nodes and nodes forming small structures.
Examples of the small structures not shown in Figure 1 are the thread of topics in association rule mining, frequent item mining, and the thread for topics in web search.
When we bring in more edges, these small structures grow to show richer topic evolution patterns.
We will see an example in the next subsection.
Also, when we bring in more edges, the isolated nodes either are absorbed into the existing structures or they form new connected components.
Examples of such new connected components we have seen are the topic thread for computer science education and the topic thread for CAD.
ics We now investigate how to  nd the topic evolution graphs for individual topics.
All topic evolution graphs in this subsection are obtained by starting from a single topic node as a seed and discovering the earlier topic nodes from which the seed node has possibly evolved.
There are nontrivial technical challenges involved.
A single set of parameters ( , R,  ) for determining topic edges is not universally adequate to reveal the evolution structure for individual topics.
For example, the parameters ( , R,  ) used in Figure 1 is e ective in revealing the evolution structure of dense areas but it does not discover the structures for sparse area.
On the otherhand, the parameter values adequate to discover the structures of sparse areas may leave the dense area too dense to decipher the structure.
In this subsection, rather than focusing on  nding the single best parameter values, we explore the parameter space and present multiple examples of graphs obtained with varying parameter values.
A simple breadth rst search is quite e ective in discovering the topic evolution graphs for a seed topic (Figure 4 and Figure 5(a)).
But we also present a case that needs smarter graph expansion strategy (Figure 5(b1)-(b3)).
Solving these technical challenges and  nding a uni ed and automated way to discover the individual evolution graphs is left for future work.
Nonetheless, the examples covered here demonstrate that the topic graphs obtained by the relatively simple methods information about the corpus that are sometimes previously unknown to us.
To discover a topic evolution graph from a seed topic, we apply a breadth rst search starting from the seed node but only following the edges that lead to topic nodes earlier in time.
In order to follow the edges in one direction in time, we treat the edges between topic nodes as directed edges.
For an edge that connects topics i and j, if the time of topic i is earlier than that of topic j, the edge is a directed edge from topic j to the earlier topic i.
In this subsection, we used a lower value for the textual similarity threshold parameter   = 0.2 than the value used in Figure 1, in order to have more member papers in topics so that we do not su er from sparseness of cross citations.
The textual information for topics is obtained by manual inspection of the start paper and tf.idf summary of the top follower papers.
Figure 5: (a) The topic evolution graph for Topic 648, (b1)-(b3) The topic evolution graphs for Topic 506 of as a decentralized multicast protocol in the application layer.
The thread has a directed change with the evolutionary  avor in that as time goes by, multicast is studied from lower network layer to higher layers, from extended LAN to IP layer to application layer.
Chronologically, Topic 183 discusses  reliable multicast in the presence of failures, message ordering, and scalability .
The start paper of Topic 262 discusses  multicast routing in datagram internetworks and extended LAN .
Topic 393 covers  IP multicast protocol (SRM) .
Topic 411 talks about  receiver-driven multicast .
Topic 486 is about  using key graphs for secure group communication scalable(logarithmic) for dynamic group change .
Topic 540 introduces  End system multicast  arguing for  the need to provide multicast not in the IP layer but in the higher layer of the network .
Some of the topics covered in other weaker threads in Figure 4(c) are  domain name system (Topic 225),  local service protocol in ad-hoc networks (Topic 564 and 225),  network topology and the power-law internet topology (Topic 505).
Figure 5(a) shows the topic evolution graph for Topic 648 with parameters R = 300,   = 400.
The start paper of Topic 648 is about  making C programs type-safe by pointer analysis guaranteeing memory safety .
The three threads in Figure 5(a) are, from left to right, the thread on  type , the thread on  storage reclamation and garbage collection , and the thread on  pointer analysis .
Note that the middle thread reaches far earlier years of the ACM corpus compared to the other threads.
In such early years, garbage collection is discussed in the context of the list structure of LISP (Topic 3, 23, and 60).
Figure 4: The topic evolution graphs for Topic 622 Figure 4(a) shows the topic evolution graph for topic 622 obtained by breadth rst search with topic edge parameters R = 500,   = 500.
Figure 4(b) and (c) are the graphs similarly obtained but with lower values of   to bring more topic nodes into the graph.
The values 200 and 140 are used for   in Figure 4(b) and (c) respectively.
Topic 622 is about  peer to peer system with distributed hash table .
Its start paper is the paper that introduces Chord.
Chord is a peer-to-peer system with distributed hash table that scales logarithmi-cally.
With high link threshold, Figure 4(a) shows that we discovered 5 earlier nodes for topic 622.
These earlier nodes are well connected to each other forming a single thread.
As we lower the link threshold   gradually, we bring in more nodes forming new threads (Figure 4(b) and (c)).
We  rst take a look at the thread consisting of topic nodes 183, 262, 393,
 vived in Figure 4(a).
The thread is about multicast, which makes sense because  peer-to-peer system  can be thought for Topic 506.
Topic 506 is about link-based web search with its start paper title being  Authoritative sources in a hyperlinked environment .
Topic 506 is located in the region where edges are relatively sparse.
Figure 5(b1) shows the snapshot of the small connected component that contains topic 506, with the same parameters used for the global topic evolution map in Figure 1.
In order to obtain the topic evolution graph for 506 such as in Figure 5(b2) or (b3), we applied a threshold to the bare cross citation count c instead of to the relationship strength metric r ( 1,  2; R) = c   n1   n2   log 1 p0 1 p1 log p1 p0 +log 1 p0 1 p1 to determine whether a pair of topics has an edge between them.
The reason is that the metric requires that the cross citation count c be greater than the second term in order for the metric to be positive.
While the second term is e ective in discounting the unfair advantage of a topic pair with large size having more cross citation count, the requirement is too stringent for Topic
 ure 5(b2) shows the topic evolution graph for 506 discovered by drawing more member papers to each topic (  = 0.2) and applying a threshold 400 to cross citation counts.
Compared to Figure 5(b1), more earlier topic nodes are discovered in Figure 5(b2).
These nodes are well connected to each other.
Topic nodes 1, 213, and 349 are information retrieval topics and topic nodes 433, 441, 474, and 475 are topics in web search.
When we lowered the cross citation count threshold from 400 to 200 in order to enrich the existing thread and to  nd other relevant threads, we encountered a problem.
With the lower link threshold, some topic nodes bring in a lot of less relevant nodes into the thread.
For example, the breadth rst search found a topic node on web-caching which in turn brings a lot of new nodes in caching and memory.
To prevent this problem, we employed a simple branch pruning strategy that prunes a branch consisting of the nodes expanded from a single node if this branch has very little connection to the rest of the graph.
The details of the pruning strategy are omitted.
Figure 5(b3) shows the result of the experiment after the second step of the breadth rst search.
The existing thread has the additional topic node 413 which is about compression of inverted index for fast information retrieval.
The middle thread consists of topic nodes close to NLP that are somewhat relevant to information retrieval such as document clustering (Topic 311), lexical segmentation (Topic
 contains the discussion in hypertext system in the late 80 s such as hypertext system implementation (Topic 166 and 224) and formal de ntion of hypertext system using petri- net (Topic 232).
The technical challenges we would like to address in the future are mining the relationships between topics, topic evolution thread discovery and textual mining on evolution threads.
Another promising direction for future work is to build a navigational application based on our algorithm.
When we navigated the topic evolution graphs obtained, we often discovered from the graphs concrete information that was either unknown to us or only vaguely known.
This experience suggests the utility of our topic evolution graphs as a navigational aid as well as an e ective global summary for the corpus.
Recently, various approaches [15] [14] [18] [2] [19] [1] [6] [5] have been proposed to model topic evolution in a time-stamped corpus.
These approaches use variants [19] [2] [1] [6] of LDA [3] or variants [14] [5] of PLSI [7] or clustering on feature space of tf idf to model text.
The works on topic evolution also di er in their modeling choice of how to ac-comodate the transition of topic content over time and the topological change in topic evolution.
[2] divides the documents in a corpus into a number of time slots and applies LDA in each time slot while letting the hyperparameters change over time through Gaussian noise.
[19] extends LDA by using per-topic Beta distribution to generate the time-stamp of each document.
The discovered topics are more narrowly distributed in time showing the dynamic change in their population over time.
[18] generates clusters at discrete time points with aging that discounts the contribution of old data points.
It then uses the overlap of clusters from di erent time points to determine the transition or emergence or disppearance of clusters over time.
[14] divides the documents by time slot and applies a variant of PLSI to extract the topics.
It then uses KL-divergence based similarity between topic models to derive the topic evolution graphs.
[1] processes a batch of documents at a time and applies LDA while using the topic models learned from previous time slices as a prior for the current model.
[5] successively applies PLSI to a batch of documents at a time.
It folds in new words using Bayesian inversion of probability as well as using traditional folding in of new documents to evolve the topic model.
[6] applies LDA-based model to the documents in each time slot.
In  nding topics for a time slot, their model considers the previous documents cited by the documents in the time slot as well with a mechanism to give more weight to a relevant document.
Our work di ers from the previous work in that our approach is designed with more emphasis on revealing the topology of topic evolution inherent in the corpus and it leverages the network underlying the corpus in a unique way.
We discover topics without dividing the corpus into time slots by conceptually de ning a topic as a quantized unit of signi cant change in topic evolution.
This allows topics to be discovered with non-homogeneous distribution over time that are inherent in the corpus as shown in Figure 1.
Topics are then connected by the relationship derived from the citation network to form a topic evolution graph.
In contrast, previous works [6] [2] [18] [14] [5] [1] divide the corpus into time slots to discover topics, and [2] [5] [1] restrict the topology of a topic evolution graph by letting each topic thread form a chain.
There are previous works that discover topics without imposing time restriction [19] [15], or that do not impose much topological restriction in connecting topics [6] [18] [14].
However, these works still do not demonstrate the rich topology of topic evolution as shown in the  gures in our evaluation.
Our work is built on the premise that the words relevant to a topic are distributed over documents such that the distribution is correlated with the underlying document network such as a citation network.
Speci cally, in our topic discovery methodology, in order to test if a multinomial word dis-Dirichlet allocation.
Journal of Machine Learning Research, (3):993 1022, 2003.
[4] S. Boyd and L. Vandenberghe.
Convex Optimization.
Cambridge University Press, 2004.
[5] A. Gohr and A. Hinneburg.
Topic evolution in a stream of documents.
In SDM, 2009.
[6] Q.
He, B. Chen, J. Pei, B. Qiu, P. Mitra, and C. L.
Giles.
Detecting topic evolution in scienti c literature: How can citations help?
In CIKM, 2009.
[7] T. Hofmann.
Probabilistic latent semantic indexing.
In
 [8] http://graphviz.org.
[9] http://tartarus.org/ martin/PorterStemmer/.
[10] Y. Jo, C. Lagoze, and C. L. Giles.
Detecting research topics via the correlation between graphs and texts.
In
 [11] C. X. Lin, B. Zhao, Q. Mei, and J. Han.
Pet: A statistical model for popular events tracking in social communities.
In SIGKDD, 2010.
[12] G. S. Mann, D. Mimno, and A. McCallum.
Bibliometric impact measures leveraging topic analysis.
In JCDL, 2006.
[13] Q. Mei, D. Cai, D. Zhang, and C. Zhai.
Topic modeling with network regularization.
In WWW, 2008.
[14] Q. Mei and C. Zhai.
Discovering evolutionary theme patterns from text - an exploration of temporal text mining.
In SIGKDD, 2005.
[15] S. Morinaga and K. Yamanishi.
Tracking dynamics of topic trends using a  nite mixture model.
In
 [16] R. Nallapati, A. Ahmed, E. P. Xing, and W. W.
Cohen.
Joint latent topic models for text and citations.
In SIGKDD, 2008.
[17] B. Shaparenko and T. Joachims.
Information genealogy: Uncovering the  ow of ideas in non-hyperlinked document databases.
In SIGKDD, July 2007.
[18] M. Spiloipoulou, I. Ntoutsi, Y. Theodoridis, and R. Schult.
Monic - modeling and monitoring cluster transitions.
In SIGKDD, 2006.
[19] X. Wang and A. McCallum.
Topics over time: A non-Markov continuous-time model of topical trends.
In SIGKDD, 2006.
[20] C. Zhai, A. Velivelli, and B. Yu.
A cross-collection mixture model for comparative text mining.
In
 tribution derived from a document constitutes a new topic, the following heuristic is used.
We check that the distribution is exclusively correlated to the document network by requiring it to be signi cantly present in other documents that are network neighbors of the given document while suppressing the nondiscriminative words using the background model.
Such correlation is previously used in [10] to discover topic terms.
The strategy of using the background model in the mixture model to absorb the nondiscriminitive words is employed in a number of previous works [20] [14].
In order to measure the contribution of a word distribution on a document over the existing word distributions, we used a log odd ratio test(Eq.3).
We inherit such form of log odd ratio test from [17].
Many available text data have a network associated with them.
Examples are citation networks or various social networks.
The importance of utilizing the network associated with text data is recently recognized in topic detection [16] [12] [13], in topic evolution detection [6], and in social network mining [11] etc.
[16] incorporates the citation link generation into the generative process of LDA.
[13] uses a potential that encourages the neighboring documents to be similar in their topic distribution.
[12] uses citation statistics to derive various relationship measures among topics such as topical di usion, diversity and tranfer.
[11] solves the problem of tracking the evolution of a single event using a model that utilizes the similarity propagated through the network and time.
In this paper, we propose a new approach to discover the evolution of topics over time in a time-stamped document collection.
Our approach emphasizes on capturing the topology of topic evolution that is inherent in the corpus.
The evaluation of our algorithm on the ACM corpus demonstrates that the topology of the topic evolution discovered by our algorithm is very rich and carries concrete information on how the corpus has evolved over time.
Our result suggests a wealth of interesting future work including the technical challenges we faced during the evaluation and the possibility of building an application for summarizing and navigating a research paper collection.
The work is supported by NSF Grant DUE-0840744 NSDL Technical Network Services: A Cyberinfrastructure Platform for STEM Education and Air Force Grant F56-8350.
We thank ACM for providing the metadata of the ACM corpus.
We thank anonymous reviewers for their comments.
