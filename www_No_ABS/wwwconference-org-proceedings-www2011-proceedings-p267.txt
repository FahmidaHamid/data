Internet news portals provide an increasingly important service for information dissemination.To ensure good performance they need to provide a number of essential capabilities to the reader: Clustering: Given the high frequency of news articles   in considerable excess of one article per second even for quality English news sites   it is vital to group similar articles together such that readers can sift through relevant information quickly.
Timelines: Aggregation of articles should not only occur in terms of current articles but it should also take previous news into account.
This is particularly important for stories that are just about to drop o  the radar   Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
a single article on the Haiti earthquake a year after the event may not carry much statistical weight, yet when combined with past information it may be categorized e ciently into the bigger context of related news.
Mindshare: When viewing events it is useful to gauge their importance in terms of whether the story is currently  trendy , and to assess how popular it has been in the past.
Services such as Google Trends or Yahoo!
Buzz provide similar functionality.
Content analysis: We would like to group content at three levels of organization: high-level topics, individual stories, and entities.
For any given story, we would like to be able to identify the most relevant topics, and also the individual entities that distinguish this event from others which are in the same overall topic.
For example, while the topic of the story might be the death of a rock star, the identity Michael Jackson will help distinguish this story from similar stories.
Online processing: As we continually receive news documents, our understanding of the topics occurring in the event stream should improve.
This is not necessarily the case for simple clustering models   increasing the amount of data will simply increase the number of clusters, without necessarily improving their quality   but it holds for topic models.
Yet topic models are unsuitable for direct analysis since they do not reason well at the level of individual events.
The above desiderata are often served by separate algorithms which cluster, annotate, and classify news.
Such an endeav-our can be costly in terms of required editorial data and engineering support.
Instead, we propose a uni ed statistical model to satisfy all demands simultaneously.
Our approach relies on a hybrid between clustering algorithms and topic models.
We use clustering to address the issue of aggregating related news into the same story, focusing on the  who,   when,  and  where  of the story.
At the same time, topic models allow us to identify the  what  and  how,  tying new stories to related events from the past.
While topic models and clustering are usually represented as opposing approaches in the literature, we show here that they can be combined into a powerful hierarchical framework.
Furthermore, we use time-dependent dynamic cluster assumptions to make our model adaptive to the ever changing nature of the news stream.
At its heart is a nonparametric Bayesian approach called the Recurrent Chinese Restaurant Process [3].
As a side bene t our model yields temporal intensity tracking, i.e. it addresses the issue of constructing a novel multiresolution model that represents streaming text and we show how inference can be made e cient.
The standard topic model representation is  at, applying a single set of topics across all documents.
But in blogs and news articles, there are at least two relevant dimensions, which topic models con ate.
High-level topics   such as  basketball  or  nancial crises    each pertain to many distinct storylines, such as the bankruptcy of Lehman Brothers in 2008.
The distinction between topics and storylines is not simply a question of granularity: topics feature a loose and shifting cast of characters over a long timescale, while sto-rylines tend to occur over a compact timespan and focus on a few key actors and events.
Within a given storyline, an author may choose to emphasize a small subset of all potentially relevant topics; for example, an article describing the Lehman Brothers collapse might focus on either the political or the economic aspects of the story.
We present a model capable of reasoning about an unbounded number of storylines, in relation to higher-level topics, named entities (e.g.
people and locations), and time.
The strength of each individual storyline rises and falls in a non-parametric model based on the Recurrent Chinese Restaurant Process [3].
The model distinguishes between storylines and high-level topics in several other respects.
First, while storylines and topics both generate the words in an article, only storylines are imbued with a distribution over named entities.
Second, documents are modeled as an admixture of topics, but we permit only a single latent story-line indicator which de nes a prior on the topic proportions.
We assume that we have access to a continuous stream of news articles which needs to be processed instantly, and which is too large to a ord periodic analysis of the entire collection of news articles.
We assume, in particular, that we have an incoming stream of news in excess of one new article per second, and that we would like to build a system capable of processing this amount of data.
To streamline the presentation for the purpose of this paper, we ignore issues such as the identity of a news source, possible attached categorical metadata, or links present within the individual news articles.
While these issues are all critical for practical deployment, they would detract from the key algorithm presented in this paper and are hence omitted.
Nevertheless, it is easy to add such metadata to our model by  upstream conditioning  [22], i.e. by using side information such as links, newspapers, authors, or categories to improve topic and cluster estimates.
Similarly, co-clicks, popularity of articles, etc.
can provide highly valuable information regarding the coherence of stories.
We begin by introducing our model s time-dependent clustering component, based on the Recurrent Chinese Restaurant Process (RCRP) [3].
This is depicted in Figure 1 on the left, for which some terminology is in order: shaded variables are observed data, blank ones are latent (not observed).
Arrows encode dependence, i.e. the distribution of children depends on their parents.
The graphs themselves are acyclic.
Plates are the statistical equivalent of a for loop.
They refer to instantiations of the contained random variable for all values of the associated index.
The basic idea is that stories (i.e. clusters) are composed of articles and that the popularity of stories varies over time.
Figure 1: Left: Recurrent Chinese Restaurant Process for clustering; Middle: Latent Dirichlet Allocation; Right: Storylines.
timelines of news.
A key departure from prior work is our emphasis on computationally scalable inference.
Our ultimate goal is an online system that can run continuously, with time and memory costs that are constant in the length of the history.
To this end, we develop a stochastic Monte Carlo inference procedure that collapses out many of the model s latent variables, yet remains trivially parallelizable.
Our experiments demonstrate both the scalability and accuracy of our approach.
In particular, we compare against the manual annotations from the Topic Detection and Tracking shared task [5], obtaining state of the art results (without using any of the hand annotations typically employed for parameter tuning on this task) that validate the design choices embodied by our model.
To demonstrate scalability, we apply our approach to the commercial news stream of a large internet news site.
Our experiments demonstrate both the scalability and accuracy of our approach.
In particular, we compare against the manual annotations from the Topic Detection and Tracking shared task [5], and editorially-labeled clusters from commercial news stream of a large internet news site.
Outline: We begin with a high-level overview of the problem of news clustering and analysis.
This is followed by detailed yet relatively equation free description of the statistical approach we take.
This section is su cient to gain the intuition needed for understanding the inner workings of the model.
Subsequently, we provide a more thorough mathematical presentation of the steps required in obtaining such an estimate.
A discussion of the data types required for inference and implementation details follows.
We conclude with our experimental results and a discussion of future work.
For an in-depth analysis of the inference procedure, more detailed statistical and technical implications, we refer the reader to [1, 2].
Generative statistical models enable rich inference over an intuitively appealing probabilistic representation for documents and text.
But just as human readers are overwhelmed by the number of potentially relevant documents, existing generative models can be deployed only on narrowly-selected collections.
Standard Bayesian inference does not scale easily to web-size data [31, 12]; however, we believe that the scalability of topic models such as Latent Dirichlet Allocation is also limited on a more fundamental level by the un-sumption for today s popularity   consequently, if we do not see articles related to a given story for a while, our prior assumption to see any further articles decreases.
The RCRP not only models these assumptions, but also has the advantage of not assuming a  xed number of stories.
Instead, it allows for a certain probability of seeing a new cluster at any given time.
Figure 1 provides a graphical depiction of this dependence.
The observed words arise from a multinomial distribution, i.e. a bag of words model that is associated with the story at hand.
Over time, the prevalence of stories changes, and the present article is drawn from this changing story distribution.
As a result, we obtain both an estimator over the story distributions and a model of the stories.
However, simply clustering articles based on this model will not work: as we obtain increasingly large amounts of data by recording articles over time, our algorithm will not learn anything new from the composition of articles related to a story.
In other words, if we had two years of data rather than one to build our model, we would simply obtain twice as many clusters (assuming that the average arrival rate of new stories is constant) but no further information regarding the language model underlying this process.
In order to address the clustering issue, we shall integrate our RCRP story model with the strengths of the Latent Dirichlet Allocation model [11].
The LDA exploits long-range relations between words by assuming that documents are composed of topics.
Under LDA, to model the content of a document it su ces to estimate the topics that occur in it.
A graphical description of LDA is given in the middle diagram of Figure 1.
The generative process  rst draws a topic mixture over topics for a given document.
This topic mixture corresponds to a mixture of multinomial word distributions, from which words are subsequently drawn.
This means that, rather than assigning a single multinomial distribution to each cluster, we now have a mixture of multinomials for each document.
This approach has been shown to yield content models that correlate well with human judgments [13].
Unfortunately, when applying LDA to news articles we face a problem: LDA o ers us little guidance in terms of how to aggregate articles into stories.
That is, while LDA is very useful in identifying the content of an article such as  -nance ,  basketball ,  middle east , etc., it fails to distinguish between, say, two di erent athletes committing adultery, since both are likely concerned with the same set of topics.
Unlike clustering however, LDA can incorporate more data without dramatically increasing the size of the latent space.
Moreover there are e ective models for dealing with topic drift over time [10].
In other words, LDA excels in almost all aspects where clustering fails, and vice versa.
We now describe how we integrate the strengths of story clustering and LDA topic modeling.
As a new story develops, we draw from a mix of topics to describe its content, in the same fashion as LDA.
However, we do not identify a story with a single article.
Instead, we assume that articles are written based on the story.
That is, we treat each story as a cluster from which individual articles are drawn.
Such an approach combines the strengths of clustering and topic models.
We use topics to describe the content of each cluster, and then we draw articles from the associated story.
This is a more natural  t for the actual process of how news is created: after an event occurs (the story), several journalists write articles addressing various aspects of the story.
While their vocabulary and their view of the story may di er, they will by necessity agree on the key issues related to a story (at least in terms of their vocabulary).
Hence, to analyze a stream of incoming news we need to infer a) which (possibly new) cluster could have generated the article and b) which topic mix describes the cluster best.
To make the described approach work requires some mod-i cations.
First, story prevalence has to be dynamic, changing with time   this can be addressed by the storylines model of [3].
Second, we need the clusters to gain a more detailed description as we observe more data.
This is achieved by allowing for cluster-speci c corrections to the topic mixture.
While words may be drawn from distributions corresponding to either the storyline or a high-level topic, all entities are drawn from a distribution corresponding to the story-line.
While entities are obviously strongly correlated with topics, their choice is somewhat orthogonal to the story itself (e.g.
the name of the dying rockstar or the name of the philandering athlete).
The rationale is that high-level topics are more general than any individual entity, while storylines are characterized most distinctively by the people, places, and organizations they describe.
This assumption may occasionally be violated for extremely in uential entities who are ubiquitous within a high-level topic, but it provides a crucial anchor that helps us to di erentiate storylines from topics, while sidestepping some of the problems encountered by similar generative models over terms and entities [24].
As an added bene t, a separate model for entities provides us with useful data for annotating clusters by capturing the dramatis personae.
A critical feature for disambiguating storylines is time.
Storylines comes and go, and it makes little sense to try to associate a document with storyline that has not been seen over a long period of time.
One approach to modeling time would be parametric: we could associate a probability distribution with the timestamps of documents in any given storyline [30].
However, di erent stories often have radically di erent temporal characteristics, and multimodality is not uncommon, as we observe e.g.
in Figure 4.
We turn to the Recurrent Chinese Restaurant Process [3], which generalizes the well-known Chinese Restaurant Process (CRP) [27] to model partially exchangeable data like document streams.
The RCRP provides a nonparametric model over storyline strength, and permits sampling-based inference over a potentially unbounded number of stories.
Documents are assumed to be divided into epochs (e.g., one hour or one day); we assume exchangeability only within each epoch.
For a new document at epoch t, a probability mass proportional to   is reserved for generating a new sto-ryline.
Each existing storyline may be selected with probability proportional to the sum mst + m st, where mst is the and m st is the prior weight for storyline s at time t.
More formally, denote by std the story index associated with document d at epoch t. We can compactly write std|s1:t 1, st,1:d 1   RCRP( ,  ,  ) (1) to indicate the following distribution: P (std|s1:t 1, st,1:d 1)  m ts + m td   ts existing story new story (2) As in the original CRP, the count m td is the number of documents in storyline s at epoch t, not including d. The temporal aspect of the model is introduced via the prior m  st, which is de ned as ts m  st = e      ms,t .
(3)  =1 This prior de nes a time-decaying kernel, parametrized by   (width) and   (decay factor).
When  =0 the RCRP degenerates to a set of independent Chinese Restaurant Processes at each epoch; when   = T and   =   we obtain a global CRP that ignores time.
In between, the values of these two parameters a ect the expected life span of a given component, such that the lifespan of each storyline follows a power law distribution [3].
The associated graphical model is given on the left in Figure 1.
The variables are as follows: t d (d, i) sd wdi  s  0 time document position i in document d story associated with document d word i in document d word distribution for story s prior for word distributions For each time period t   {1, .
.
.
, T} do For each document d in time period t do i.
Draw the storyline indicator: std via ii.
If std is a new storyline draw a distribution std|s1:t 1, st,1:d 1 over words  s| 0 iii.
For each i in document draw wdi    std This time-dependent clustering process constitutes the  rst component of the storylines model.
The second component is given by a topic model.
The Latent Dirichlet Allocation model [11] is described in the middle of Figure 1.
We have the following variables and associated generative process:   d  d (d, i) zdi wdi  0  k Dirichlet prior over topic distributions document topic distribution for document d position i in document d topic associated with word at (d, i) word at (d, i) Dirichlet prior over word distributions for topics word distribution for topic k
 (a) Draw word distribution  k from word prior  0
 (a) Draw topic distribution  d from Dirichlet prior   (b) For each position (d, i) in d do i.
Draw topic zdi for position (d, i) from topic distribution  d ii.
Draw word wdi for position (d, i) from word distribution  zdi The key di erence to the basic clustering model is that we continue improving our estimate of the stories as we receive more data.
We now combine clustering and topic models into our proposed storylines model by imbuing each storyline with a Dirichlet distribution over topic strength vectors with parameters   (they encode mean and precision).
For each article in a given storyline, the topic proportions  d are drawn from this Dirichlet distribution.
Words are drawn either from the storyline or one of the topics.
This can be modeled by adding an element K + 1 to the topic proportions  d.
If the latent topic indicator zn   K, then the word is drawn from the topic  zn ; otherwise it is drawn from a distribution linked to the storyline  s.
Topic models usually focus on individual words, but news stories often center around speci c people and locations [21].
For this reason, we extract named entities from text in a pre-processing step, and model their generation directly.
Note that we make no e ort to resolve names  Barack Obama  and  President Obama  to a single underlying semantic entity, but we do treat these expressions as single tokens in a vocabulary over names.
The variables are as follows: t d (t, d) (d, i) std st edi zdi wdi  s
  s  td    s  0  k  0 time document document d in epoch t position i in document d (word or entity) story associated with document d aggregate cluster variables at time t entity at position i in document d topic at position i in document d word at position i in document d Entity distribution for story s prior for entity distributions topic distribution for story s topic distribution for document d at time t Dirichlet prior over topic distributions word distribution for story speci c  topic  for story s prior for story-speci c word distributions word distributions for topics k Dirichlet prior over word distributions for topics The full generative process is stated below.
The key modi- cations are that we need to interleave the RCRP with LDA.
For each time period t from 1 to T do (forward in time)
 (a) Draw the storyline indicator: (b) If std is a new storyline, std|s1:t 1, st,1:d 1   RCRP ( ,  ,  ) i.
Draw a distribution over words ii.
Draw a distribution over named entities  snew|G0   Dir( 0)  snew|G0   Dir( 0)  snew   Dir( ) (c) Draw the topic proportions:  td|std   Dir( std ) (d) Draw the words wtd|std   LDA std ,{ 1,  ,  K,  std}  (e) Draw the named entities etd|std   Mult( std ) Here LDA std ,{ 1,  ,  K,  std}  indicates a probability distribution over word vectors in the form of a Latent Dirichlet Allocation model [11] with topic proportions  td and topics { 1,  ,  K,  std}.
The hyperparameters  0,  0,  0 are all symmetric Dirichlet.
Our goal is to compute online the posterior distribution P (z1:T , s1:T|x1:T ), where xt, zt, st are shorthands for all of the documents at epoch t ( xtd =  wtd, etd ), the topic indicators at epoch t and story indicators at epoch t. Markov Chain Monte Carlo (MCMC) methods which are widely used to compute this posterior are inherently batch methods and do not scale well to the amount of data we consider.
Furthermore they are unsuitable for streaming data applications.
Instead, we apply a sequential Monte Carlo (SMC) method known as particle  lters [15].
Particle  lters approximate the posterior distribution over the latent variables up until document td.
When document td arrives, the posterior is updated and the posterior approximation is maintained as a set of weighted particles each represent a hypothesis about the hidden variables; the weight of each particle represents how well the hypothesis maintained by the particle explains the data.
The structure of the algorithm is described in Figure 2.
The algorithm processes one document at a time in the order of arrival.
This should not be confused with the time stamp of the document.
For example, we can chose the epoch length to be a full day but still process documents inside the same day as they arrive (although they all have the same timestamp).
The main ingredient for designing a particle  lter is the proposal distribution Q(ztd, std|xtd, past).
Usually this proposal is taken to be the prior distribution P (zt, st|past) since computing the posterior is hard.
We take Q to be the posterior which minimizes the variance of the resulting particle weights [15].
Unfortunately computing this posterior is intractable, thus we use MCMC and run a Markov chain over (ztd, std) whose equilibrium distribution is the sought-after posterior.
The derivation for the sampling equations of ztd, std is given in [1, 2].
Sampling topic indicators: For the topic of word i in document d and epoch t, we sample from: P (ztdi = k|wtdi = w, std = s, rest) C i tdk + sk +  C i s. + (K+1) C i C i td.
+ 1 C i kw +  0 C i k. +  0W = (4) where rest denotes all other hidden variables,C i tdk refers to the count of topic k and document d in epoch t, not including the currently sampled index i; C i sk is the count of topic k with story s, C i kw is the count of word w with topic k (which indexes the story if k = K + 1; traditional dot notation is used to indicate sums over indices (e.g.
C i tdk).
Note that this is just the standard sampling equation for LDA except that the prior over the document s topic vector   is replaced by it s story mean topic vector.
td.
=  k C i Sampling story indicators: The sampling equation for the storyline std on a high level decomposes as follows: P (std|s td t :t, ztd, etd, wK+1 td , rest) = (5) t :t) P (std|s td     Prior P (ztd|std, rest)P (etd|std, rest)P (wK+1     Emission td   where the prior follows from the RCRP (2), wK+1 are the set of words in document d sampled from the story speci c language model  std , and the emission terms for wK+1 , etd are simple ratios of partition functions.
Since we integrated out  , the emission term over ztd does not have a closed form solution and is computed using the chain rule as follows: td td |std, rest)   P (ztd|std = s, rest) = ntd i=1 P (ztdi|std = s, z td,(n i) td , rest) (6) where the superscript  td, (n   i) means that we exclude all words in document td that came after position i.
The terms in the product can be computed using (4).
We alternate between sampling (4) and (5) for 15 iterations (as we showed in [1, 2], increasing the number of iterations beyond 15 does not help).
Unfortunately, even then the chain is too slow for online inference, because of (6) which scales linearly with the number of words in the document.
In addition we need to compute this term for every active story.
To solve this we use a proposal distribution q(s) = P (std|s td t :t)P (etd|std, rest) whose computation scales linearly with the number of entities in the document.
We then sample s  from this proposal and compute the acceptance ratio r which is simply r = P (ztd|s , rest)P (wK+1 P (ztd|std, rest)P (wK+1 td td |s , rest) |std, rest) .
Thus we need only to compute (6) twice per MCMC iteration.
Another attractive property is that the proposal is constant and does not depend on ztd, thus, we precompute it once for the entire MCMC sweep.
Finally, the unnormal-ized importance weight for particle f ,  f after td is updated as[1, 2]:  f    f P (xtd|zf td, sf td, x1:t 1), (7) which has the intuitive explanation that the weight for particle f is updated by multiplying in the marginal probability of the new observation xtd, which we compute from the last
 nally, if the e ective number of particles  t 2 falls below a threshold we stochastically replicate each particle based on its normalized weight.
To encourage diversity in those replicated particles, we select a small number of documents (10 in our implementation) from the recent 1000 documents, and do a single MCMC sweep over them, and then  nally reset the weight of each particle to uniform.
Hyperparameters: The hyperparameters of the model are  ,  0,  0, and  0.
For our experiments we set  0 = .1, Illustration of the particle  ltering algorithm.
The state of each  lter represents the hypothesis maintained by each particle before processing document td.
The state is shown as a 3-layer hierarchy: top layer is the topics, then the stories and  nally the documents.
Each story maintains a distribution over topics, and each document is associated with a single story.
Each  lter runs an MCMC algorithm over each document in parallel and then update the  lter weight.
The  lters  weights are then normalized to arrive at the posterior distribution shown to the right of the  gure.
 0 = 0.01, and  0 = .001.
The Dirichlet prior   is uniform over topics but with a di erent value for the storyline topic.
That is, we choose 0.1 K+1 for the high-level topics and
 K+1 for the storyline speci c term.
For the RCRP, we allow the new-storyline hyperparameter  t to be epoch-speci c, we apply a Gamma(5,10) prior and sample after every batch of
 eters, we set   = 3 and   = 0.5   results were robust across a range of settings.
For all experiments, we use 8-particles running on an 8-core machine.
Implementation and Storage Implementing our SMC algorithm for large datasets poses runtime and memory challenges.
To address these, we must  rst understand the algorithm s requirements.
Operations in our algorithm can be divided into two categories: in the  rst category, we have particle  ltering operations that work on single particles; these operations constitute the bulk of our algorithm.
Because these particles can be worked on individually, we spawn one worker thread per particle to achieve fast parallel inference.
The second category contains just the particle resampling operation.
This operation is fundamentally di erent from the others, as it involves copying entire particles rather than simple updates to individual particles.
We handle particle resampling using a master thread, during which all other threads sleep.
Given these needs, our algorithm requires a data structure that supports fast updates of individual particles  data, as well as fast copying of particles.
In particular, the latter ensures that the time spent in the master thread is negligible compared to the worker threads, giving our implementation high parallel e ciency.
It should be obvious that the naive implementation, where each particle has its own set of arrays for storage, is wholly unsuited for the latter operation   our runtime would be crippled by incessant memory copying.
Worse, memory requirements would grow linearly in the number of particles, making large data streams impractical even for modest numbers of particles.
Inheritance trees: We overcome these problems with an idea from Canini et al. [12], in which particles maintain a memory-e cient representation called an  inheritance tree .
In this representation, each particle is associated with a tree vertex, which stores the actual data.
The key idea is that child vertices inherit their ancestors  data, so they need only store changes relative to their ancestors, in the form of a dictionary or hash map.
To save memory, data elements with value 0 are not explictly represented unless necessary (e.g.
when a parent has nonzero value).
New vertices are created only when writing data, and only under two circumstances:  rst, when the particle to be changed shares a vertex with other particles, and second, when the particle to be changed is associated with an interior vertex.
In both cases, a new leaf vertex is created for the particle in question.
This representation dramatically reduces memory usage for large numbers of particles, and also makes particle replication a constant runtime operation.
The tradeo  however, is that data retrieval becomes linear time in the depth of the tree, although writing data remains (amortized) constant time.
This disadvantage can be mitigated via tree maintenance operations, in which we prune branches without particles and then collapse unnecessary long branches   refer to Figure 3 for an example.
With tree maintenance, data retrieval becomes a practically constant time operation.
Thread safety: Thus far, we have only described Canini et al. s version of the inheritance tree [12], which is not thread-safe.
To see why, consider what happens when particle 1 is Doc.tdStateStateSamplestdSampleztdSamplestdSampleztdFilter1Filter FUpdatefilterweightUpdatefilterweightNormalizeweightsPosteriorResampleFiltersifnecessaryWWW 2011   Session: Spatio-Temporal AnalysisMarch 28 April 1, 2011, Hyderabad, India272Figure 3: Inheritance tree operations in the context of our SMC algorithm.
Numbers within a vertex represent associated particles.
Each vertex s hash map is represented by a table, connected by a dotted line.
associated with the parent vertex of particle 2.
If a thread writes to particle 1 while another is reading from particle 2, it may happen that the second thread needs to read from the parent vertex.
This creates a race condition, which is unacceptable because our algorithm is multi-threaded.
To ensure thread safety, we augment the inheritance tree by requiring every particle to have its own leaf in the tree.
This makes particle writes thread-safe, because no particle s vertex is an ancestor of any other particle s vertex, and writes only go to the assigned vertex, never to ancestors.
Furthermore, every particle is associated with only one worker thread, so there will never be simultaneous writes to the same particle.
On the other hand, data reads, even to ancestors, are inherently thread-safe and present no issue.
To maintain this requirement, observe that particle-vertex associations can only change during particle resam-pling, which we handle with a master thread (all other threads become inactive).
So immediately after resampling, we branch o  a new leaf for every particle at an interior node.
Once this is done, the individual  lter threads may be run safely in parallel.
In summary, the inheritance tree has four operations:
 given particle f .
retrieve or write to data elements i for particle f .
and then collapses unnecessary long branches.
These operations are demonstrated in Figure 3.
The worker threads only use update operations, while master thread re-sampling uses the copy operation.
After resampling but before restarting the worker threads, the master thread invokes the maintain operation to reduce the inheritance tree s size, followed by the branch function on every particle to make the tree thread-safe.
Extended inheritance trees: Our thread-safe inheritance tree supports most of our data storage needs.
However, parts of our algorithm require storage of sets of objects, rather than integer values.
For example, our story sampling equation (5) needs the set of stories associated with each named entity, as well as the number of times each story-to-entity association occurs.
To support these operations we implemented an extended inheritance tree that integrates the our thread-safe inheritance tree with the inverted representation of [31].
Details of this novel data structure and its incorporation with the sampler can be found in [1, 2] Purging unnecessary data: Because the RCRP prior only looks at a  nite time window from t     to t, we only need particle data from that window in memory, while antecedent data can be safely purged to disk.
The same observation applies to the incoming word and entity streams; we only need data from that window in memory.
These features keep our algorithm s memory usage constant if   is assumed constant, which is critical for online execution.
Moreover, our bounded memory requirements allow us to handle datasets of the scale seen in our experiments.
Our experiments assess our approach for both accuracy and scalability.
Our goal is to provide the user with a representation of news stories that would allow them to  nd relevant stories easily and satisfactorily.
We consider two datasets: news articles from Yahoo!
and the TDT5 dataset[26].
We  rst describe the two datasets and then give three evaluations: clustering accuracy, structured browsing, and  nally, an ablation study to understand the contribution of every component of our model.
Yahoo!
News Dataset.
We examine our model on English news samples of varying sizes extracted from Yahoo!
News over a two-month Filter threadsupdateparticlesRoot1games: 1officials: 3league: 423(empty)league: 5minister: 1games: 0season: 2Initial tree(ready for threads)Root1games: 1officials: 3league: 423(empty)league: 5games: 3minister:7games: 0season: 20 = get(1, games )set(2, games ,3)set(3, minister ,7)ResamplingcopiesparticlesRootgames: 1officials: 3league: 42,13(empty)league: 5games: 3minister: 7games: 0season: 2copy(2,1)Pruneunused branchesRootgames: 1officials: 3league: 42,13(empty)league: 5games: 3minister: 7games: 0season: 2Collapse long branchesRootgames: 1officials: 3league: 42,13league: 5games: 3minister: 72,1games: 3season: 2league: 5maintain_prune()maintain_collapse()Createnew leavesRootgames: 1officials: 3league: 43minister: 7games: 3season: 2league: 5branch(1)branch(2)12(empty)(empty)New initial tree(ready for threads)Rootgames: 1officials: 3league: 43minister: 7games: 3season: 2league: 512(empty)(empty)WWW 2011   Session: Spatio-Temporal AnalysisMarch 28 April 1, 2011, Hyderabad, India273period.
Details of the news samples are listed in Table 1.
We use a sophisticated named entity recognizer [34] which disambiguates and resolves named entities to Wikipedia entries in the data preprocessing step.
In addition, we remove common stop-words and tokens which are neither verbs, nor nouns, nor adjectives from the news articles.
For the purpose of modeling, we divide each of the samples into a set of 12-hour epochs according to the article publication date and time.
TDT5 Dataset.
The Topic Detection and Tracking dataset (TDT) [26] contains several hundred thousand news articles, along with storyline relevance judgments from human raters.
We use a month of data from the TDT5 corpus, comprising 46,793 documents.
Of these, 1771 are annotated as belonging to one of 37 storylines; the others do not participate in evaluation, but must still be processed and should not distract the system from properly processing the annotated documents.
Named entities are extracted using the Stanford NER system [18].
After applying a stoplist of 500 words [28], the vocabulary of both words and named entities is pruned to the 3000 most frequent terms.
We evaluate the clustering accuracy of our model over the Yahoo!
news dataset.
The dataset contains 2525 editorially judged  must-link  (45%) and  cannot-link  (55%) article pairs included in the modeling samples.
The must-link article pairs refer to the pairs of articles that are considered related to the same story, whereas cannot-link pairs are those considered not related.
Table 1: Details of Yahoo!
News samples and corresponding clustering accuracies of our method (Story) and the baseline (LSHC) evaluated 2525 labeled must-link and cannot-link pairs.
No.
Sample # Words # Entities Story
 size








 Accuracy Accuracy








 For the sake of evaluating clustering, we compare against a variant of a single-link clustering baseline [14].
This simple baseline is the best performing system on TDT2004 task and was shown to be competitive with Bayesian models [33].
This method scales linearly with the number of all previously seen documents, however, in [25], the authors showed that using locality sensitive hashing (LSH), one can restrict the subset of documents examined with little e ect of the  nal accuracy.
Here, we use a similar idea, but we even allow the baseline to be  t o ine.
First, we compute the similarities between articles via LSH [20, 19], then construct a pairwise similarity graph on which a single-link clustering algorithm is applied to form larger clusters.
The single-link algorithm is stopped when no two clusters to be merged have similarity score larger than a threshold tuned on a separate validation set.
In the remainder of this paper, we simply refer to this baseline as LSHC.
We ran both our method and the baseline on the datasets listed in Table 1.
For our method, we used 200 topics.
From Table 1, we see that our method outperformed the baseline on all the datasets tested.
We believe that this outcome stems from the fact that the baseline method does Table 2: Clustering accuracies of our method on various samples vs. number (K) of topics.
No.
K=50 K=100 K=200 K=300









 not make use of any statistical topic-document models that could help link two articles related to the same story but written with rather di erent wordings.
Note that all clustering algorithms working at the surface-word level would su er from this problem.
Moreover, the fact that our online, single-pass algorithm is competitive with an o line algorithm, is an encouraging result due to the streaming nature of our application.
To study the importance of the number of topics to our model, we performed another set of experiments with different numbers of topics.
Table 2 shows that the number of topics matters in the clustering accuracy of our method but it becomes less of a concern if we use a su cient number of topics   see [2, 1] for more details.
                                                                                                                                                                                      Figure 4: Some example storylines and topics extracted by our system.
For each storyline we list the top words in the left column, and the top named entities at the right; the plot at the bottom shows the storyline strength over time.
For topics we show the top words.
The lines between storylines and topics indicate that at least 10% of terms in a storyline are generated from the linked topic.
We illustrate the utility of our model by describing some of the topics and stories and then showing how this representation can support structured browsing.
The storylines in Figure 4 include the UEFA soccer championships, a tax bill under consideration in the United States, and tension between India and Pakistan.
Our model identi es connections between these storylines and relevant high-level topics: the UEFA story relates to a more general topic about sports; both the tax bill and the India-Pakistan stories relate to the Politics topics, but only the latter story relates to the topic about civil unrest.
Note that each storyline contains a plot of strength over time; the UEFA storyline is strongly mul-timodal, peaking near the dates of matches.
This demonstrates the importance of a  exible nonparametric model for time, rather than using a unimodal distribution.
One way for end-users to take advantage of the organi-                                                                                                              Figure 5: An example of structure browsing a ordances provided by our model.
Starting with the story about India-Pakistan tension, the user may request similar stories by topic, obtaining a story about the Middle East con ict (left), which shares topics such as politics, unrest and diplomacy.
Alternatively the user may request topically similar stories, but require that the term  nuclear  be present   this leads to a storyline about the North Korean nuclear weapons program.
Table 3: Evaluation of complete model, with varying number of topics.
Lower scores are better.
# Topics Cdet









 Table 4: Ablation test, removing key features from the 100-topic model.
ablated feature time entities topics storyline word distributions Cdet







 zation obtained by our model is to browse the collection of high-level topics and then descend to speci c stories indexed under each topic   like opening a physical newspaper and going directly to the sports page.
However, our model provides a number of other a ordances for structured browsing which were not possible under previous approaches.
Figure 5 shows two examples.
First, users can request similar stories by topic: starting from a story about India-Pakistan tension, the system returns a story about the Middle-east con ict, which also features topics such as diplomacy, politics, and unrest.
In addition, users can focus their query with speci c keywords or entities: the right side of Figure 5 shows the result of querying for similar stories but requiring the keyword nuclear to have high salience in the term probability vector of any story returned by the query.
Similarly, users might require a speci c entity   for example, accepting only stories that are a topical match and include the entity Vajpayee.
This combination of topic-level analysis with surface-level matching on terms or entities is a unique contribution of our model, and was not possible with previous technology.
We evaluate against human-annotated storylines in the TDT5-May dataset to assess the contribution of speci c components of our system: the recurrent Chinese restaurant process for modeling time, multilevel modeling of top-Figure 6: Time Scalability ics and storylines, and the incorporation of named entities.
We assess the importance of each component by measuring the e ect of removing it on the accuracy of the  rst story detection  task, which is the problem of correctly identifying the  rst document in each annotated storyline; this is considered to be the most di cult problem in the TDT task [7].
We use the TDT scoring software and report the standard metric, which is the macro-averaged minimum cdet cost.
Our system outputs the probability that each story is new, this probability is taken to be P (std = new) averaged over particles.
The minimum cdet cost is a measure that combines both false alarms and misses in a single number.
Lower numbers are better [26].
Evaluation is performed in a streaming setting, meaning that the system must make a judgement for each document shortly after it is received   rather than after viewing the entire dataset.
Our quantitative evaluation of storyline quality is shown in Tables 3 and 4.
Experimenting with a range of topics, we  nd the strongest results around 100.
The average time to process one document ranged from 50ms for K = 50 to
 tance tree.
Fixing K = 100, we then tested the contributions of each feature of our model (Table 4).
Each key aspect of our system improves performance: the two-level topic/storyline model strongly outperforms a model that ignore topics (line 3 of Table 4); the RCRP model outperforms the CRP model, demonstrating the importance of modeling time (line 1); named entities improve performance (line 2) and the story speci c language model also enhances outcome (line 4).
Our second evaluation concerns scalability.
Our goal is a system that can be deployed over a long time period, continually processing data with time and memory costs that are constant in the length of the history.
As shown in Figure 6, our system scales well as the history grows: after an initial buildup period, the time taken by our system to process 1000 documents increases very slowly as the total number of documents processed by the system increases.
After the buildup period, the average time to process a document stabilized around 60 ms per document for K = 100 (the residual growth is due to the increasing number of stories).
Finally we would like to mention that our method is completely unsupervised, in contrast to many TDT systems which tune their parameters over a training dataset from an earlier TDT run.
Our use of TDT5 here was merely to evaluate the contribution of each component of our model.
Modeling: Our approach is non-parametric over stories, allowing the number of stories to be determined by the data.
Similarly, Zhang et al. [33] describe an online evaluate on the same  rst-story detection task that we consider here.
This work equates storylines with clusters, and does not model high-level topics.
Non-parametric clustering has been combined with topic models, with the cluster de ning a distribution over topics [32, 29].
We di er from these approaches in several respects: we incorporate temporal information and named entities, and we permit both the storylines and topics to emit words.
There have been several e orts to augment topic models with time [10, 30].
These approaches allow topic strength to vary smoothly; however, they do not incorporate a multilevel model that distinguishes individual storylines from high-level topics, and thus must treat topic evolution as a relatively slow phenomenon.
Consequently, such models have been applied successfully to long-term corpora (over several years or even decades), but have not been shown to successfully model rapidly-changing corpora such as news or blogs.
Inference: Recent work on topic models has focused on improving scalability; we focus on sampling-based methods, which are most relevant to our approach.
Yao et al. use Gibbs sampling but do not resample the topics for documents outside a  xed temporal window [31]; we di er by developing a Monte Carlo sampling approach that handles the streaming setting explicitly.
Banerjee and Basu also develop online sampling equations, but they instantiate the parameters   and   whereas we marginalize them [9].
Our approach is most in uenced by the particle  lter of [12], from which we take the idea of e ciently storing the hidden variable history.
We present a scalable probabilistic model for extracting storylines in news and blogs.
The key aspects of our model are (1) a principled distinction between topics and storylines, (2) a non-parametric model of storyline strength over time, and (3) an online e cient inference algorithm over a nontrivial dynamic and non-parametric model.
We see many promising directions for future research.
One particularly appealing possibility would be a hierarchical nonparametric model of topics and storylines that features multiple levels of increasing detail.
Acknowledgments We thank the anonymous reviewers for their helpful comment, and Yahoo!
Research for the datasets.
This work is supported in part by grants NSF IIS- 0713379, NSF DBI-0546594 career award, ONR N000140910758, DARPA NBCH1080007, AFOSR [7] J. Allan, V. Lavrenko, and H. Jin.
First story detection in TDT is hard.
In CIKM, 374 381, 2000.
[8] C. E. Antoniak.
Mixtures of dirichlet processes with applications to bayesian nonparametric problems.
The Annals of Statistics, 2(6):1152 1174, 1974.
[9] A. Banerjee and S. Basu.
Topic models over text streams: A study of batch and online unsupervised learning.
In Proceedings of SDM, 2007.
[10] D. Blei and J. La erty.
Dynamic topic models.
In W. W.
Cohen and A. Moore, editors, ICML, 2006.
[11] D. Blei, A. Ng, and M. Jordan.
Latent Dirichlet allocation.
[12] K. R. Canini, L. Shi, and T. L. Gri ths.
Online inference of topics with latent dirichlet allocation.
In AISTATS, 2009.
[13] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D.
Blei.
Reading tea leaves: How humans interpret topic models.
In Neural Information Processing Systems, 2009.
[14] M. Connell, A. Feng, G. Kumaran, H. Raghavan, C. Shah, and J. Allan.
UMass at TDT 2004.
In TDT 2004 Workshop Proceedings, 2004.
[15] A. Doucet, N. de Freitas, and N. Gordon.
Sequential Monte Carlo Methods in Practice.
Springer, 2001.
[16] G. Doyle and C. Elkan.
Accounting for burstiness in topic models.
In ICML, 2009.
[17] M. Escobar and M. West.
Bayesian density estimation and inference using mixtures.
JASA 90, 1995.
[18] J. R. Finkel, T. Grenager, and C. Manning.
Incorporating non-local information into information extraction systems by gibbs sampling.
In ACL, pages 363 370, 2005.
[19] A. Gionis, P. Indyk, and R. Motwani.
Similarity Search in High Dimensions via Hashing.
In VLDB, 1999.
[20] T. Haveliwala, A. Gionis, and P. Indyk.
Scalable Techniques for Clustering the Web.
In WebDB, 2000.
[21] G. Kumaran and J. Allan.
Text classi cation and named entities for new event detection.
In SIGIR,2004.
[22] D. M. Mimno and A. McCallum.
Topic models conditioned on arbitrary features with dirichlet-multinomial regression.
In UAI, 2008.
[23] T. P. Minka.
Estimating a Dirichlet distribution.
Technical report, MIT, 2003.
[24] D. Newman, C. Chemudugunta, and P. Smyth.
Statistical entity-topic models.
In KDD, pages 680 686, New York,
 [25] S. Petrovic, M. Osborne and V. Lavrenko.
Streaming First Story Detection with application to Twitter.
In NAACL,
 [26] NIST.
http://www.itl.nist.gov/iad/mig/tests/tdt/ 2004/workshop.html.
[27] J. Pitman.
Exchangeable and partially exchangeable random partitions.
Probability Theory, 102(2), 1995.
[28] G. Salton.
The SMART Retrieval System Experiments in Automatic Document Processing.
Prentice-Hall, 1971.
FA9550010247, and Alfred P. Sloan Research Fellowsh to EPX.
