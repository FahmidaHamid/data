Video summarization is the process of generating the montage of a given video that indicates its main theme and contents.
Nowadays, the fast development of the Web has resulted in the explosive growth of video resources, which makes video summarization a very important technology for e cient access of video contents.
Among those large amount of Web video resources, some are accompanied with additional types of information, such as texts, and the exploration of textual information has been shown to bene t the video summarization task [20, 12, 31].
However, there are also signi cant amount of videos that do not possess those extra information.
For them, summarization models relying on both video and text features cannot be applied directly.
Early video summarization approaches [20, 5, 33, 29], depicted in Figure 1(a), usually just make use of video resources alone, and conduct the learning process based on video features.
Those approaches assume that video features can well determine whether certain shots should be included in an ideal montage, or whether a subset of shots as a whole constitutes an ideal montage.
However, due to the complex structure and representation of video shots, it is generally di cult to design e ective features from video that can capture the semantic importance of video shots precisely.
More recent approaches [4, 24, 26], depicted in Figure 1(b), realize the importance of other types of resources, espe- cially textual information.
Those approaches achieve significant improvement and demonstrate that text features can give a clear and accurate description of the corresponding shots.
However, they require both video and text features George returns to CTU where he orders the medics to revive the unconscious Paula, who has vital information.
Palmer becomes aware of Eric s deception.
Kim takes Megan to the hospital.
(b) Learning to Summarize using both Video and Text Features George returns to CTU where he orders the medics to revive the unconscious Paula, who has vital information.
(c) Transfer Learning for Video Summarization Training Data Testing Data Figure 1: Three di erent strategies for video summarization in montage prediction.
Considering that current video resources, such as those from YouTube 1, are mostly without adequate textual data, the above approaches can only be applied in some limited scenarios.
To address the above mentioned problems, we propose a transfer learning approach, depicted in Figure 1(c).
It makes use of both video and textual data in the training process; and video data alone are required in the testing/summarization process.
More speci cally, we formulate the summarization task as a subset selection problem.
As shown in Figure 2, our basic assumption is that in the training process, each shot is accompanied by certain textual information.
We can specify two summarization tasks based on video and text, respectively.
Both of them actually aim at learning the same structure indicating which candidate subset should be selected to generate the summary.
This makes the knowledge transfer from text to video possible: It is observed that there exists a close relationship between videos and their corresponding text materials.
Speci cally, those video-based features and textual features are highly correlated, which can be described by a transfer matrix.
This means that candidate montages probably di er from each other to a similar degree as the corresponding subsets of text information, usually represented in the form of sentence subsets or word subsets.
Therefore, by integrating the textual data into the learning process, we can expect a more powerful summa-rization model containing the knowledge transferred from textual data.
In this paper, we will focus on dealing with a special type of videos of TV series, they tend to have the following characteristics: 1) TV series are usually accompanied with rich textual information such as transcripts and dramas, which are closely related to the content of the TV series; and 2) For each episode of a TV series, it often begins with a montage, Video Knowledge Transference Montage Summary Text Summary Same Structure There s a nuclear device, under terrorist control, that s on US soil.
I m just as concerned with the dangers of panic and mass hysteria.
You two.
Cut it out.
There s a nuclear device, under terrorist control, that s on US soil.
Quantico wants permission to relay the nuclear threat to their mid-level tacticians.
I m just as concerned with the dangers of panic and mass hysteria.
So where are we on everything else?
Text Figure 2: The mechanism of our transfer learning approach in other words, video summary, of the last episode, thus provides ideal training data for the summarization task.2 Notice that our assumption relies on the requirement that video and corresponding textual information are closely related.
For other types of videos, such as  eld sports, indoor sports, home movies, whose accompanied textual information is not always so close related to the video contents, di erent kinds of methods need to be developed.
Our paper aims at making good use of videos  associated textual data   available during the training process   to help the summarization task based on video data only.
To
 content-light comments, brief descriptions of video producers and actors, or short vacuous outlines.
ally are not accompanied by textual information, and they also lack inner story structures.
bone of our training method augmented with a set of constraints generated from the textual data.
This forms the proposed transferable structured learning framework.
In the process of constraint generation, a text summarization model is trained to evaluate the importance of candidate montages in terms of the corresponding textual information.
Then through the model, textual data are encoded into constraints which are supposed to give a more detailed description of the di erence between candidate montages.
Thus, the learning process of the video summarization is guided by the knowledge from the associated text summa-rization.
As a result, the text information is transferred into knowledge on video summarization, enhancing the learning process of video summarization.
Cutting plane algorithm is adapted to solve the resulting optimization problem [30].
Finally, in summarizing a new video, the learned summa-rization model is used for montage prediction using video features only.
A set of experiments have been conducted to demonstrate the e ectiveness of the proposed summarization method.
Firstly, our approach was shown to give a remarkable improvement compared with several state-of-the-art methods.
It also showed that the proposed constraints, which are supposed to transfer the textual data into knowledge on the summarization task based on only video features, contribute to a more e ective video summarization model.
Finally, we also demonstrated that the e ectiveness of the pre-learned text summarization model has a signi cant impact on the performance of the video summarization task.
We now list the main contributions of our work:
 enhance video summarization in the context of transfer learning: textual information is only used for training the summarization model, and summarization of new videos relay on video features alone;
 work, leading to a more e ective summarization model, which use constraints to incorporate knowledge transfer from textual data.
The rest of the paper is organized as follows: in Section 2 we present previous works related to video summarization and transfer learning.
The video summarization task is formulated as a structured learning problem in Section 3.
In Section 4, we discuss in detail how to transfer videos  related textual data into knowledge on how to generate montage just based on video features.
Then we address how to solve the resulting optimization problem in Section 5; detailed description of the features we use in the learning process is also included in this section.
Section 6 presents the experimental results along with some case studies.
Finally, We conclude the paper with some pointers to future research directions in Section 7.
Summarization [15, 25, 5, 10] has enjoyed considerable development in other areas, such as information retrieval.
In particular, video summarization attracts increasingly interests nowadays.
It can be generally categorized into keyframe-based summarization and skimming-based summarization.
Keyframe-based summarization has been studied for quite a long time.
These approaches usually represent each shot by keyframes and extract a certain number of them or their corresponding shots as the montage of given videos.
According to the techniques on keyframes extraction, they can be further classi ed as follows: sampling-based, shot-based, and graph-based.
Sampling-based approaches, such as video magni er [20] and Minivideo [25], usually extract keyframes from videos in a random or uniform manner.
Shot-based approaches  rstly segment video into shots and extract one or several keyframes from each of them based on low-level video features, such as color or motion.
S. X. Ju et al. [12], once employ motion and gesture as the criterion for keyframe extraction.
Advanced human behaviors are also studied to extract more meaningful and representative keyframes.
Attention models, employed in [21], prove to be a reasonable criteria for keyframe extraction.
Recently, several graph-based approaches [31, 33] have been proposed.
These approaches view a given video as a graph with each node representing a standard unit.
Then various graph-based algorithms such as clustering, singular value decomposition, or principle component analysis, are applied to extract keyframes.
Skimming-based summarization enjoys a fast development in recent years.
These approaches make use of the whole shot s information, employ video features of relatively high level, pay special attention to the montage s length, and mostly perform an extra smoothing step to make the  nal montage more nature.
Amir et al. [5] employ video features based on audio to capture the semantic information of shots.
Several rush summarization approaches [29, 8] focus on the fast-moving object and camera events of videos.
Moreover, shot-change patterns are also used for dialog events extraction in [16].
Most of those approaches mainly deal with video of special domains, as they can make use of the domains  speci c information.
For example, Russell [23] employs a presentation structure for video s from weekly forum.
Several others approaches also pay special attention to videos of talks [9], news [10] or sports [32].
Those above approaches mostly focus on the generation of various kinds of video features, so as to capture the video s structure or distinguish each shots from each other.
In spite of their encouraging achievements, above approaches have to face the great di culty that video features may fail to capture the shots  characteristics accurately due to the overwhelming di culties in representing videos e ec-tively.
Recent years, other types of resources, especially textual data are employed to help video summarization.
Some approaches aim at enhancing the semantic characters of videos by generating concept entity [4] or short synopses [24].
Huang et al.
[10] aim at generating semantically meaningful montage by integrating text information.
A novel video summarization method proposed by Chen et al.
[4] employs text features extracted from speech transcript to generate concept entities.
The Informedia Project [24] generates short synopses partly based on pre-extracted keywords.
Others [3, 26, 36] exploit transcript information to decide the scenes  boundary.
Our approach goes a step further by transferring the text information into the learned model for the summarization task based on video features only.
In this way, our approach can be applied to a wide range of videos on the web, even if they are not accompanied by textual data.
Another  eld closely related to our work is transfer learning.
Transfer learning approaches mainly aim at make use of knowledge of source domain to solve problems of target domain.
According to the form of transferred knowledge, these approaches can to classi ed into four categories: instance-based, feature-based, parameter-based and relationship-based.
Instance-based transfer learning [7, 11] reuses certain parts of data in source domain to help learning in the target domain in the manner of re-weighting.
Feature-based approaches [28] mainly focus on how to transfer the feature representation for di erent domains.
Early approaches usually solve the task by constructing rules of each domain and learning to translate rules of di erent domains.
For example, Rule Transfer, a transfer algorithm proposed in [27], prove to be e cient for transferring knowledge between two kinds of games.
In recent approach, knowledge of more kinds of forms is enabled to be transferred.
A novel approach, translated learning, [6] employs a dictionary concerning two domains, and learn to transfer knowledge of common features of di erent domain through a Markov chain.
mixture coaching, proposed in [28], learns to transfer feature representation in the opposite direction, from target to source, and makes prediction based on features from both domains.
Parameter-based approaches, such as response coaching [28] and informative vector machine [14], assume that there exists some parameters shared by both source and target tasks.
Another class of methods, relationship-based transfer learning, assumes that there exists similar relationship among data in source and target domains.
For example, Mihalkova et al. [19] employ Markov logic network to capture the possible similar relationship.
Yu et al.
[34] included latent variables in a structure learning framework, which is applicable to the cases where loss function is independent of latent variables.
Our approach belongs to this category, and furthermore, requires no dependence relation between loss function and features belonging to di erent domains.
We formulate the video summarization problem as follows.
Given a video v = {v1, v2, .
.
.
, vn}   V, where vi represents a shot of the video, V is the space of all videos.
The goal of the summarization task is to predict a shot subset y from the space of all possible shot subsets Y, in other words, y   v. In video summarization, the subset y is also called a montage of the video v.
In the training process, we assume that certain corresponding textual information t = {t1, t2, .
.
.
, tm}   T of the video v is also available.
Here ti denotes the corresponding textual data of shot vi, for example the corresponding transcript of the shot, and T denotes the space of all the text.
Given a video v, a textual summary l can also be obtained by collecting the corresponding textual data of its montage y.
We use x to denote the video-text pair (v, t), and z their corresponding summary (y, l).
Assume that we are given a set of labeled training data: {(x(i), z(i))|i = 1, .
.
.
, n}, x(i) = (v(i), t(i)), z(i) = (y(i), l(i)), where y(i) represents the ground-truth video summary of the video v(i) and l(i) the corresponding text summary of t(i).
We want to emphasize that in our transfer learning framework, the labeled training data (x, z) for both video and its corresponding text are used for the training process, while for summarizing a new video, we will generate video summary without using the textual information.
Now we formulate the problem of video summarization as that of learning a discriminant function, F (v, y) = V   Y 7  R, which is intended to measure the degree to which the subset y considered as suitable summary for the video v. Then we generate the summary of a given video v as the subset y , maximizing F (v, y) over all possible subsets y   Y, i.e., y  = argmax F (v, y).
y Y (1) To make the above framework feasible, we represent each pair (v, y) by a feature vector  (v, y).
For simplicity, the discriminant function F (v, y) is assumed to be linear in terms of the feature vector  (v, y), i.e., F (v, y) = wT  (v, y).
(2) As will be shown, the weight vector w is estimated by  t-ting both the video training data and the text training data.
We also employ the following loss function to quantify the penalty of using a predicted summary  y as an approximation of the ground-truth summary y:  (y,  y) : Y   Y   R.
In our study, a loss function related to F1 measure is applied:  (y,  y) = 1   2pr p + r , p = < y,  y > <  y,  y > , r = < y,  y > < y, y > .
(3) Here, given two subsets a and b, < a, b > denotes the number of common elements they share.
Our general idea is to extend structural SVM into the transfer learning scenario through introducing an additional set of constraints; and these constraints are meant to encode and transfer the knowledge learnt from an auxiliary and related task.
First, we describe brie y the structural SVM method proposed in [35, 15] to train a robust model for a summarization task.
Given a training set {(x(i), z(i)) | i =
 tor w for the discriminant function F (v, y) = wT  (v, y) through the following quadratic programming problem: Optimization Problem 1.
(Structural SVM) min w, 0

 kwk2+ c n n
 i=1  i, (4) subjected to:  i,  y   Y \ y(i),  i   0, wT  (v(i), y(i))   wT  (v(i), y) +  (y(i), y)    i.
In Equation (4), the parameter c controls the tradeo  between the model complexity 1 i=1  i, the sum of the slack variables  i.
The constraints for the optimization problem enforce the requirement that the ground-truth summary y(i) should have a higher function value than other alternatives y   Y, and y 6= y(i).
Text Summary Text Groud-truth Montage Video Learning to Summarize Text Text Summarization Model Constraints Generation Video A Transferrable Structure Learning Framework Video Summarization Model Making Prediction Predicted Montage Training Process Testing Process Figure 3: The  owsheet of our video summarization approach
 rization Generally, text-based summarization is a much easier task than video-based summarization.
Thus, the characteristics of x = (v, t) can be better captured by the features generated based on text information.
On the other hand, it is quite di cult to obtain e ective representation for video shots.
Let us consider the constraints in the above-mentioned optimization problem: wT  (v(i), y(i))   wT  (v(i), y) +  (y(i), y)    i (5) These constraints employ  (y(i), y) to measure the di er-ence between candidate montages.
As mentioned in the previous section, this only describes the di erence between two subsets in terms of the F1 measure.
Therefore, it is desirable to more accurately capture the more subtle di erence between the ideal subset and an alternative in a quantitative way.
The corresponding text materials for video shots, which are known to be closely correlated with the video content, can provide a much detailed and accurate description for the di erence between candidate montages as we will show next.
Our basic idea is to use the videos  related textual data to form an additional set of constraints, which is supposed to help training a better video summarization model.
Recall that we have the set of training examples, {(x(i), z(i))|i = 1, .
.
.
, n}, x(i) = (v(i), t(i)), z(i) = (y(i), l(i)), We will  rst learn a text summarization model, i.e., we seek to learn a discriminant function, P(t, l) : T   L   R, which measures the degree to which the subset l is a suitable summary for the textual information set t.
Again, a feature vector  (t, l) is employed to describe each pair (t, l).
The discriminant function P(t, l) is assumed to be linear in the feature vector  (t, l), which can be expressed as follows: P(t, l) =  wT  (t, l).
(6) To transfer the textual data into knowledge on summa-rization based on video features only, we emphasize the similar distribution of video features and text features by introducing the following additional constraints.
New Constraint: wT  (v(i), y(i))   wT  (v(i), y) +(  wT  (t(i), l(i))    wT  (t(i), l))    i, (7) where  w is obtained from a good text summarization model mentioned above.
According to our assumption that  (v, y) and  (t, l) are closely related, it is believed that  wT  (t, l) can better represent the di erence between candidate montages y, as long as we employ a relatively accurate text summarization model, in other word, a suitably estimated  wT .
In fact, the text features make  wT  (t(i), l(i))   wT  (t(i), l) a word-level metric, which is generally better than shot-level metric.
In this way, the text information is transferred to learn a more suitable weight vector for the video feature vector  (v, y).
In fact, the above constraints can also be applied to a much wider situation.
Suppose there exists a one-to-one mapping between units in source and target domains.
Then  wT  (t, l) can provide another measure for the di erence between candidates y.
Thus these constraints will bene t the model training in the target domain, especially when the task is easier in the source domain.
of Video For di erent types of video, their relation with the associated textual information can vary a lot.
TV series, of course, are known to be also closely related to the transcripts.
However, for other types of videos, the corresponding textual information does not necessarily have a close relation with the videos.
Take home movies as an example, a considerable part of the associated text are usually comments about the details described in a single shot.
For  eld sports, textual information frequently describes the background, history, gossip, and so on, thus also fail to re ect the content of corresponding shot.
Under this situation, it may be a good idea to also incorporate some measurement of the strength of the relation, so as to give a higher weight to the transfer of the more correct knowledge.
Using r(y, l) to denote the degree of  tness of one subset of shots with the corresponding textual information, the following constraints can be used instead: wT  (v(i), y(i))   wT  (v(i), y) +(  wT r(y(i), l(i)) (t(i), l(i))    wT r(y, l) (t(i), l))    i, In our experiment, we just focus on dealing with TV series, the implementation of the above type of constraints is left to future work.
Employing the additional constraints we discussed, we propose to train a summarization model using both text and video features through the following optimization problem: Optimization Problem 2.
min w, 0

 kwk2 + c n n
 i=1  i, (8) subjected to:  i,  y   Y \ y(i) :  i   0,

    wT  (t(i), l))    i.
The space Y of all possible subsets is complex.
We solve the optimization problem (8) following the general cutting plane algorithm [30, 35].
The space Y of all possible subsets is complex.
In order to solve the optimization problem de ned in Equation (8) e ciently, we employed the cutting plane algorithm [30, 35].
It iteratively adds constraints until the problem has been solved with a desired tolerance .
We start with a group i, for i = 1, .
.
.
, n. Then, we of empty working sets yi, y iteratively  nd the most violated constraints  y,  y , for each (v(i), y(i)) corresponding to the two constraints in Equation (8), respectively.
They are then added to the corresponding working sets, and w is updated with respect to the new combined working set.
The learning algorithm is presented in Algorithm 1.
It is guaranteed to halt within a polynomial number of iterations [30].
For each iteration, we need to solve argmax P(v(i), y(i), y)   argmax  (y(i), y) + wT  (v(i), y) y Y y Y for  (y(i), y) =  (y(i), y), or  wT  (t(i), l(i))    wT  (t(i), l).
A greedy algorithm, described in Algorithm 2, is proposed to solve this problem where we repeatedly select the shot y

 i =  , y for i = 1, 2, .
.
.
, n do i =   for i = 1, .
.
.
, n Algorithm 1 Cutting plane algorithm
 2: yi =  , y 3: repeat





     wT ( (v(i), y(i))    (v(i), y)) H(y) =  (y(i), y)    
 Compute:  y = argmaxyH(y),  y (y) Compute actual slack:  i = max{0, maxy Wi H(y), max if (H( y) >  i + ) or (cid:16)H (y) = (  wT  (t(i), l(i))    wT  (t(i), l))     ( y) >  i + (cid:17) then = argmaxyH (y)} y y

 i









 Add constraint to working set yi   yi   { y}, i   y y w   Optimize over  i(yi   y0 i) i   { y


 15: until no working set has changed during iteration.
end if end for } Algorithm 2 Greedy algorithm for shot subset selection

 3: for i = 1, 2, .
.
.
, k do
 5: end for 6: return  y y   argmaxy / yP(v, y,  y   {y}) satisfying the condition that  y   {y} is the shot set with the highest score.
The algorithm ends with an extracted shot set of size k. This algorithm has the same approximation bound as the greedy algorithm proposed by Khuller et al.
[13] to solve the budgeted maximum coverage problem, that is to say, a (1   1 e )approximation bound.
According to [13, 30,

 According to (1) and (2), we generate the summary for a given new video v using: argmax P(v,  , y)   argmax F (v, y) = argmax wT  (v, y), y Y y Y y Y which is a special case of Algorithm 2.
Our training and testing data are prepared as follows: For each episode, we collect the following data: the episode itself, the corresponding transcript, the preview, and the drama.
In our experiments, the preview is employed as the summary of its corresponding episode, and the drama is used as the ground-truth summary of the transcript.
The data set in our experiments contains 100 episodes derived from several popular U.S. TV shows 3 with the statistics presented in Table 1.
We perform 5-fold cross validation to determine the parameters in the experiments.
The reported performance is averaged over all 5-folds.
Number of Episodes Average Episode Length Average Preview Length Resolution Dataset
 40 min 39 s 1 min 58 s
 In our experiment, the video features  (v, y) are generally keyframe-based.
IBM s IMARS [1] is used to split single video into shots and extract one keyframe for each shot.
For each keyframe,  ve kinds of features, including lab histogram [18], Law [18], human color vision [18], GIST [22] and SIFT [17], are extracted, leading to a feature vector with 1514 dimensions.
We use the following  ve kinds of text features  (t, l): word frequency, position, thematic word, length and n-gram [15], leading to a feature vector with 2090 dimensions.
We apply two measures, F1 and Simi to evaluate the performance.
The metrics provides by TRECVID is not completely employed, as they are not focus on summarization measurement.
F1 measurement is widely used in summarization eval-In F1 metric, the predicted summary  y and the uation.
ground-truth summary y are compared directly and the precision, recall, F1 scores are calculated as follows: F1(y,  y) = 2pr p + r , p = < y,  y > <  y,  y > , r = < y,  y > < y, y > .
To give a more detailed and precise evaluation of the performance, we measure the di erence between the ground-truth video summary and candidate video summary through the similarities between their shots.
Let vsmi(vi, vj) to denote the similarity of two shots vi and vj, which is determined by the similarities of their corresponding keyframes.
Suppose the corresponding keyframes of vi and vj are denoted as {ki1, ki2, .
.
.
, kim} and {kj1, kj2, .
.
.
, kjn}, respectively, we calculate vsmi(vi, vj) through: vsmi(vi, vj) =
 m m
 i=1 max j=1,...,n psmi(ki, kj), where psmi(ki, kj) denotes the similarity of two keyframes, measured by a widely used image comparison tool ImageMag-ick 4 based on the pixel di erence.
In order to compare the predicted summary  y = { v1,  v2, .
.
.
,  vk and the ground-truth summary y = {v1, v2, .
.
.
, vk}, we de- ne Simi(y,  y) as follows: Simi(y,  y) =
 k k
 i=1 max j=1,...,k
 vsmi(vi,  vj)
 In this section, we list several supervised and unsupervised methods that are used for comparison in our experiments.
4www.imagemagick.org
 We choose two approaches based on video data only and three approaches based on both video and textual data as our baselines.
SVM: SVM is widely used to train a binary classi er.
We use SVM to classify summary shots and non-summary shots based on video features only.
Clustering [31]: It builds a graph based on the similarities between extracted keyframes.
A time-constrained clustering algorithm is proposed to classify all shots into groups, and then select the most representative one of each cluster to form montages.
Mixed-Clustering [36]: Mixed-Clustering makes a further step over the above one by calculating the mixed similarity scores based on both video and textual data.
KP-Transcript [26]: This approach views the video sum-marization task as a knapsack problem.
It aims at maximizing segment score derived from corresponding transcripts and constrained by segment duration.
Mixed-Str-SVM: Mixed-Str-SVM shares a similar structured learning framework with our approach, but with no constraints for knowledge transfer involved.
It makes use of both video and text features in montage prediction.
Trans-Str-SVM: This is our proposed approach.
Table 2 gives the performance of our approach and the baselines.
As shown in Table 2, Clustering gives the worst performance.
We think this is due to the following reasons: First, it only utilizes on the visual similarities between di erent shots.
Second, current approaches can hardly give an accurate measure for the similarities between shots or keyframes based on video or image features only.
As a result, it can not obtain accurate summaries since two shots with similar colors can actually tell quite di erent stories.
SVM performs a little better than Clustering, which demonstrates the advantage of supervised learning.
Moreover, it proves that it is quite di cult to distinguish summary shots from non-summary shots.
Approaches that utilize both video and text features perform notably better than with those approaches using only video features.
For example, Mixed-Clustering results in a remarkable improvement over the performance of Clustering, thereby shows the great bene ts brought by textual data.
In particular we may conclude that the shots similarity may be better measured by taking text information into consideration, since main themes are usually easier to extract from textual data.
KP-Transcript con rms that it is reasonable to view subset selection problems as knapsack problems by obtaining much better result.
Notice that in this method, text features actually play a major role in determining which shots are to be selected as summary for the given video, while the video features are mainly used for constraint generation.
Mixed-Str-SVM performs the best among all these approaches.
The improvement shows the effectiveness of structured learning, especially in dealing with textual data, and provides the possibility for transferring text knowledge for video summarization under our framework.
Our method outperforms the two methods based on video features only, achieves an improvement of 33.4% and 19.0% over Clustering in terms of F1 and Simi metrics, respec-
Clustering Mixed-Clustering KP-Transcript Mixed-Str-SVM Trans-Str-SVM
 Simi











 Table 3: Performance of di erent models measured by F1 Str-SVM Trans-Str-SVM Str-SVM(G) Trans-Str-SVM(G) Str-SVM(B) Trans-Str-SVM(B) Fold1 Fold2 Fold3 Fold4 Fold5 Total



































 The additional tag (G) denotes the results on test cases easy for text summarization, (B) denotes the results on others.
Table 4 uses same tags.
Table 4: Performance of di erent models measured by Simi Str-SVM Trans-Str-SVM Str-SVM(G) Trans-Str-SVM(G) Str-SVM(B) Trans-Str-SVM(B) Fold1 Fold2 Fold3 Fold4 Fold5 Total



































 tively.
Moreover, it outperforms SVM by 23.4% and 9.3% in term of F1 and Simi, respectively.
This also demonstrates that text information is useful for video summarization.
On the other hand, compared with our approach, there is a considerable performance gain for Mixed-Clustering, KP-Transcript, and Mixed-Str-SVM.
This can be attributed to the fact that text information are available in the test phrase for these methods, while in our setting no text data is available in the test phrase.
Therefore, they can be viewed as the upper-bound of the proposed method.
We include these methods for a comprehensive evaluation.
According to the above results, we can also  nd that F1 and Simi show the similar trend in performance measurement.
Therefore, in the following experiments, the performance is only measured by F1 metric.
To identify how the knowledge in the text domain help our video summarization task, we provide two models through a strategy selection based on our approach.
We use Str-SVM and Trans-Str-SVM to denote the model trained with only video features and the model trained with both video and text features, respectively.
To better understand how text information can aid video summarization, we divide the data set into two parts based on the performance of the text summarization model on each test case.
Speci cally, the boundary is determined by an empirical threshold under F1 metric.
The results on those two data sets are also presented, and (G) denotes the result on test cases for which text summary can be easily obtained while (B) denotes the result for the rest.
According to Table 3 and Table 4, we  nd that Trans-Str-SVM gains the advantage over Str-SVM in all  ve folds measured by F1, and also outperforms Str-SVM in most cases measured by Simi.
Trans-Str-SVM improve the performance over Str-SVM by 15.7% and 11.2% in average measured by F1 and Simi, respectively.
Further more, there is a relatively smaller increase in terms of Simi, this may due to the fact that the Simi metric emphasizes the similarities between keyframes.
However, image similarity can not accurately describe the e ective structures in the summarization task.
The transfer of text knowledge, on the other hand, pay more attention to the structure of video.
The quality of textual summary also has a in uence on the performance of Trans-Str-SVM.
For those episodes where text summarization model performs relatively well, Trans-Str-SVM generally achieves a much greater performance gain over Str-SVM than the average case, makes an improvement of 20.2% and 15.3% in terms of F1 and Simi metrics, respectively.
For those episodes di cult for text summarization, Trans-Str-SVM obtains comparable or a litte worse performance than Str-SVM.
As a result, we believe that useful knowledge can be transferred through the proposed constraints when text summaries can be obtained accurately.
To a large extent, its accuracy determines how much bene- ts we can get from knowledge transfer.
This also demonstrates that video features and corresponding text features are highly correlated.
According to the above results, we can now come to the conclusion that the constraints generated from textual data can substantially contribute to video summarization.
They indeed transfer text information into knowledge on how to summarize based on video features only.
To further demonstrate the signi cant bene ts brought by knowledge transferred from textual data, we select one test case and compare the performance of Str-SVM and Good-Trans1 Good-Trans2 Bad-Trans1 Bad-Trans2
 Simi







 Trans-Str-SVM manually.
The keyframes of ground-truth montage, keyframes of montage predicted by Str-SVM, and keyframes of montage predicted by Trans-Str-SVM are presented in Figure 4.
From Figure 4 we can  nd that Str-SVM tends to extract the keyframes owning similar hue with the ground-truth summary.
They may involve no human  gures that share similar backgrounds with the keyframes in the ground-truth summary, or present several scenes holding diversity colors or stripes.
However, those keyframes usually are not the right answer, as video summary prefers keyframes with single scene and person.
Trans-Str-SVM may fail to learn the color diversity presented by the Str-SVM.
However, it extracts more accurate keyframes.
This can be attributed to the structure information it learns from knowledge transfer.
Keyframes extracted by Trans-Str-SVM are generally in consistent with the previous mentioned principles of video summary(video summary prefers keyframes with single scene and person).
We believe that those principles are closely related with the video s structures.
In this section, we test the in uence of text summariza-tion model on the performance of our approach.
Four video summarization models are trained separately on di erent training sets or using di erent parameters in model training.
We denote Good-Trans1, Good-Trans2 to be the text sum-marization model with better performance in text summa-rization task, and Bad-Trans1, Bad-Trans2 to be the model getting a relatively worse result.
Table 5 presents the performance of our approaches employing the above four models.
According to Table 5, the model Good-Trans1 and Good-Trans2 make an average increase of 2.7% and 2.7% over the model Bad-Trans1 and Bad-Trans2 measured by F1 and Simi, respectively.
This illustrates that transferring text information in a correct manner is very important.
When using unreliable text summarization models, the corresponding constraints may provide an inaccurate measurement of the di erence between candidate montages.
In fact, this can play quite a negative e ect on adjusting weighting for those video features.
Thus we can get a worse video summariza-tion model.
In this paper, a novel approach is proposed to make use of textual data to help the video summarization task using only video data.
In order to transfer text information to help the summarization based on video features only, we construct a transferable structured learning framework to train a video summarization model with a set of constraints generated from textual data, which is intended for knowledge transfer.
In particular, a text summarization model is  rstly trained and then used to generate constraints which transfer the knowledge from textual summarization and give a more detailed and precise measurement of the di erences among shots.
Experimental results demonstrate the e ectiveness of our approach.
The performance of our method achieves a remarkable improvement over a set of state-of-the-art supervised and unsupervised methods which indicates that the constraints can transfer knowledge from the textual data effectively.
In future work, we plan to transfer text information to knowledge for video summarization in other ways.
For example, a text-video dictionary can be employed or generated from web resources such as Flickr, to transfer text features into video features.
Moreover, state-of-the-art video tagging approaches can be employed to generate textual data from videos or keyframes.
The text features generated from those data can be ensured to be more consistent with corresponding video features, which can be expect to bene t video summarization a lot.
We thank the anonymous reviewers for their valuable and constructive comments.
Gui-Rong Xue thanks the support of NSFC project (No.
60873211), RGC/NSFC project (No.
of CAD & CG (No.
A0801), Zhejiang University.
Part of the work was supported by NSF Grant IIS-1049694.
