The transfer of discriminative knowledge between heterogeneous domains is one of the most important task in many web-based application, such as content-based search and semantic indexing for text and multimedia documents.
The word features of the text representation are much easier to interpret as compared to image features.
On the contrary, there is often a tremendous semantic gap between visual features and the concepts in the image domain.
These characteristics make it easier to interpret and solve the classi cation problem in the text domain.
The challenges of image classi cation are particularly evident, when the amount of training data available is limited.
In such cases, the semantic learning process is further hampered by the paucity of labels.
Classi ers naturally work better with features that have semantic interpretability, because class labels are also usually designed on the basis of application-speci c semantic criteria.
This implies that text features are inherently friendly to the classi cation process in a way that is often a challenge for image representations.
In the case of images, it is desirable to obtain a feature representation which relates more directly to semantic concepts; a process which will improve the quality of classi cation.
Furthermore, this must often be achieved with the use of only a limited amount of labeled image data.
This naturally motivates an approach for utilizing the abundant data in the text domain in order to improve image classi cation.
This is achieved by a semantic knowledge transfer process belief systems, 2 Health, 2 Mathematics and logic, 1 Thought and philosophy, 1 Technology and the applied science, 4 Natural and the physical sciences, 9 History and events, 11 Society and social sciences, 12 Geography and places, 14 Culture and the arts, 30 Biographies and persons, 15 Figure 1: Subjects covered by Wikipedia content.
The Wikipedia documents are archived by a number of categories in these subjects, which provides a variety of categorized text documents.
by which an indirect feature representation for images is constructed which extracts the semantic concepts from text features.
Furthermore, the relationship of the class labels to the text features is used during this transfer process.
We will show that the transfer of the rich semantic information in the source text domain to the target image domain provides much more e ective learning algorithms.
While labeled text is widely available, labeled images are often expensive to obtain, and are generally scarcely available.
For example, millions of categorized text articles are freely available in online web text collections such as Wikipedia, covering a wide range of subjects from culture and the arts, geography and places, history and events, to natural and the physical science (see Figure 1).
To help readers browse through the articles, Wikipedia articles are indexed by thousands of categories in these subjects 1.
This provides us with a large number and variety of categorized text documents.
In many real web and social media applications, it is possible to obtain co-occurrence information between text and images.
For example, in web pages, the images co-occur with text on the same web page.
Similarly, there is a tremendous amount of linkage between text and images on the web, through comments in image sharing sites, posts in a social networks, and other linked text and image corpora.
It is reasonable to assume that the content of the text and the images are highly correlated in both scenarios.
While such co-occurrence information may be noisy on an individual basis, we hypothesize that the co-occurrence information may be su ciently rich on an aggregate basis.
This information provides a semantic bridge, which can be exploited in order to learn the correspondence between the features in the di erent domains.
This learned bridge is then leveraged in order to translate the semantic information in the text features into the image domain.
We seek to develop an algorithm for transferring knowledge between di erent domains [16] [12] [7].
It is applied to the multimedia domain in order to leverage the semantic labels in text corpora to annotate image corpora with scarce labels.
Such algorithms typically transfer knowledge between heterogeneous feature spaces [7][18][17] instead of
 http://en.wikipedia.org/wiki/Portal:Contents/Categorical index homogeneous feature spaces [16].
The approach may also use some auxiliary information from the target domain in order to further improve accuracy.
Heterogeneous transfer learning is usually much more challenging due to the unknown correspondence across the distinct feature spaces.
In order to bridge across two distinct feature spaces, the key ingredient is a semantic  translator  which can explain the correspondence between text and image feature spaces through the use of a feature transformation.
This transformation is used for the purpose of e ective web image classi cation and semantic indexing.
As discussed earlier, this process is achieved with the use of co-occurrence data that is often available in many practical settings.
In contrast to previous work, [7][18][17], the translator proposed in this paper can directly establish the semantic correspondence between text and images even if they are new instances of the image data with unknown correspondence to the text documents, or if the co-occurrence data is independent of the labeled source text instances.
This increases the  exibility of the algorithm and makes it more widely applicable in many practical applications.
In order to perform the knowledge transfer process, we create a new topic space into which both the text and images are mapped.
Both the correspondence information and auxiliary image training set are used to learn the translator, which links the instances across heterogeneous text and image spaces.
We follow the principle of parsimony, and encode as few topics as possible in order to translate between text and images for regularization.
This principle has a preference for the least complex model, as long as the text and image correspondence can be well explained by the learned translator.
After the translator is learned, the semantic labels can be propagated from any labeled text corpus to any new image by a process of cross-domain label propagation.
While auxiliary information from the target domain is also used for improving accuracy, one characteristic of our translator is that it is particularly robust in the presence of a very small number of auxiliary examples.
The remainder of this paper is organized as follows.
In Section 2, we formulate the translation problem and show how the labels of text corpus can be propagated to image corpus.
Section 3 explains the learning procedure of the semantic translator.
In Section 4, we use a proximal gradient based algorithm to optimize the formulation in Section
 main.
The experimental comparisons to related algorithms are presented in section 6.
The conclusion and summary is presented in Section 7.
  and   In this section, we will introduce the notations and de ni-tions, as well as the problem de nition for the transfer learn  be the source and target feature ing process.
Let   spaces, which have a dimensionality of   and   respectively.
For the purpose of this paper, the source space corresponds to the text domain, and the target space corresponds to the image domain.
In the source (text) space, we have a set of  ( ) text documents in    .
Each text document is repre- , 1        ( ).
This sented by a feature vector  ( ) text corpus has already been annotated with class labels  ( ) =     {+1, 1}       }  1        ( ) , where  ( )  ( )   ,  ( )   {( ) Translator T(X1(s),X(t)) Translator T(X2(s),X(t)) fT(X(t))   Translator T(X3(s),X(t)) X1(s) X2(s) X3(s) Text Corpus 1 +1 X(t) Test Image Figure 2: Illustration of semantic label propagation from text to images by the learned translator.
The output is the discriminant function    ( ) ( ) .
is the binary label.
The binary assumption is made for no-tational convenience.
This assumption is without loss of generality, because the extension to the multi-class case is straightforward.
The images are represented by feature vectors  ( ) in the target space     .
The task is to transfer the feature structure of the source (text) space to the target space (image) space, while taking into account the labeling relationships in the source space, and the correspondence information between the source and target space.
The goal of the transformation process is to provide a classi er for the target (image) domain in the presence of scarce labeled data for the latter domain.
        ,   ( {( ))}  ( )   ,  ( )  ( )   ,  ( )   ,  ( )  ( ) ) .
For brevity, we use  ,  to denote   its corresponding image feature vector  ( ) In order to perform the knowledge propagation from the text to the image domain, we need a bridge, which relates the text and image information.
A key component which provides such bridging information about the relationship between the text and image feature spaces is a co-occurrence set   = .
For the text document  ( ) in the co(   occurrence set, we denote the co-occurrence frequency by   .
Such co-occurrence information is copiously available in the context of web and social network data.
In fact, it may often the case that the co-occurrence information between text and images can be more readily obtained than the class labels in the target (image) domain.
For example, in many web collections, the images may co-occur with the surrounding text on the same web page.
Similarly, in web and social networks, it is common to have implicit and explicit links between text and images.
Such links can be viewed more generally as co-occurrence data.
This co-occurrence set provides the semantic bridge needed for transfer learning.
 ( )   ,  ( ) ( )    ( )   ,  ( )  1        ( ) Besides the linkage-based co-occurrence set, we sometimes also have a small set  ( ) = of labeled target instances.
This is an auxiliary set of labeled target instances, and its size is usually much smaller than that of the set of labeled source examples.
In other words, we have  ( )    ( ).
As we will see, the auxiliary set is used in order to enhance the accuracy of the transfer learning process.
  {( } ) One of the key intermediate steps during this process is the design of a translator function between text and images.
This translator function serves as a conduit to measure the linking strength between text and image features.
We will show that such a conduit can be used indirectly in order to propagate the class labels from text to images.
The transla-  as well as image tor   is a function de ned on text space   space   It assigns a real value to one pair of text and image instances to weigh their linking strength.
This value can be either positive or negative, representing either positive or negative linkages.
Given a new image  ( ), its label is determined by a discriminant function as a linear combination of the class labels in  ( ) weighted by the corresponding translator functions      .
  as   :         ( )  ( )   =1 ( ) )  ( )   ,  ( ) (    ( ) =  ( )     (1)  ( ) provides the In the above relationship, the sign of   class label of  ( ).
Hence, the key to translating from text to images is to learn a translator which can properly explain the correspondence between text and image spaces.
This overall process is illustrated intuitively in Figure 2.
Since the key to an e ective transfer learning process is to learn the function   , we need to formulate an optimization problem which maximizes the classi cation accuracy obtained from this transfer process.
First, we will  rst set up the optimization problem more generally without assuming any canonical form for   .
Later, we will set up a canonical form for the translator function in the form of matrices which represent topic spaces.
The parameters of this canonical form will be optimized in order to learn the translator function.
We propose to optimize the following problem to learn the semantic translator: (  ( )   =1 min
         ( ( )  ( )   ) +      ,      ( ( )   ,  ( )   ) ) (  
 )
 (2) Here,   and   are balancing parameters, which de ne the relative importance of co-occurrence data and auxiliary data in the objective function.
The above expression measures the e ectiveness of the translation process, and the e ectiveness can be divided into di erent components with corresponding balancing parameters.
  The  rst term is the empirical loss of prediction made by the discriminant function   on the auxiliary training set.
Based on the large margin principle, the loss can be minimized by maximizing the margin  ( )     ( ( )   ).
Thus, in Equation (2) we use a loss function  ( ) that prefers large positive margins and penalizes large negative ones.
  In the second term, the summation is taken over   weighted by co-occurrences  ,  with a monotonically decreasing function   ( ).
Here,  ( ) outputs a small value when   is large and vice versa.
Note that a pair of  ( )   with large co-occurrence number  ,  will be weighed more when minimizing this term.
In other words, by minimizing this term, translator function has larger output on a pair of target and source instances with larger co-occurrence number in the observation set   and vice versa.
This is consistent with and  ( )   instances probably share the same labels so we expect the translator function has a large output to propagate the labels between them.
  The last term   (  ) regularizes the learning of the translator in order to improve the generalization performance.
This term is particularly useful, when the auxiliary examples are scarce.
This term will be extended in the following section when establishing the translator function.
We note that the above optimization problem is formulated in general form, with the use of a generic translator function, and generic loss functions.
Since we wish to optimize the translator function, we need to de ne a speci c model for the translator function, and also materialize the loss functions in algebraic form.
In the next sections, we will address these issues and then solve the underlying optimization problem.
In this section, we will design the canonical form of the translator function in terms of underlying topic spaces.
This provides a closed form to our translator function, which can be e ectively optimized.
Topic spaces provide a natural intermediate representation which can semantically link the information between the text and images.
One of the challenges to this is that text and images have inherently di er-ent structure to describe their content.
For example, text is described in the form of a vector space of sparse words, whereas images are typically de ned in the form of feature vectors such as color histograms or other texture features, each of which may not be semantically meaningful of its own record.
To establish their connection, one must discover a common structure which can be used in order to link them.
A text document usually contains several topics which describe di erent aspects of the underlying concepts at a higher level.
For example, in a web page depicting a bird, some topics such as the head, body and tail may be described in its textual part.
At the same time, there is a co-occurring bird image illustrating them.
By mapping the original text and image feature vectors into a space with several unspeci ed topics, they can be semantically linked together by investigating their co-occurrence data.
By using this idea, we construct two transformation matrices to map text and images into a common (hypothetical) latent topic space with dimension  , as in the previous work [11], which makes them directly comparable.
The dimensionality is essentially equal to the number of topics.
We note that it is not necessary to know the exact semantics of latent topics.
We only attempt to model the semantic correspondence between the unknown topics of text and images.
The learning of e ective transformation matrices (or, as we will see later, an appropriate function of them) is the key to the success of the semantic translation process.
These matrices are de ned as follows.
  ( )       :          ,  ( )   (cid:15)    ( ) ( )     ( )       :          ,  ( )   (cid:15)    ( ) ( )   (3) (4) The translator function is de ned as a function of the source and target instances by computing the inner product in our (5) hypothetical topic space, which is implied by these transformation matrices )   (  ( )
   =  ( )      ( )  ( ) ( ) ,  ( ) =     ( ) ( )   =  ( )  ,   ( ) ( )    ( )         Here  ,  and   denote the inner product and transpose operations respectively.
Clearly, the choice of the transformation matrices (or rather the product matrix   ( )  ( )) impacts the translator function   directly.
Therefore, we will use the notation   in order to brie y denote the matrix   ( )  ( ).
Clearly, it su ces to learn this product matrix   rather than the two transformation matrices separately.
The above de nition of the matrix   can be used to rewrite the discriminant function as follows: ( )    ( ) =  ( )   =1    ( )   ( )    ( )   (6) The above expression for the discriminant can be substituted in the objective function of the optimization problem for the translator function.
In addition, we can use the conventional squared norm to regularize the translator   on two transformations respectively: ((cid:11)(cid:11)(cid:11)  ( ) (cid:11)(cid:11)(cid:11)2 ) (cid:11)(cid:11)(cid:11)  ( ) (cid:11)(cid:11)(cid:11)2
 +



 Here, the expression   represents the Frobenius norm.
Then, we can use the aforementioned substitutions in order to rewrite the objective function of Eq.
(2) as follows:   (  ( )  ((cid:11)(cid:11)(cid:11)  ( )  =1

 (cid:11)(cid:11)(cid:11)2
 +    ( ( )  ( )   ) ( ) (cid:11)(cid:11)(cid:11)  ( ) +     ) (cid:11)(cid:11)(cid:11)2

 + )    ,     ( )       ( )   min  =  ( )  ( )   (7) The goal is to determine the value of  , which optimizes the objective function in Eq.
(7).
We note that this objective function is not convex.
This implies that the optimum valu of   may be hard to  nd with the use of straightforward gradient descent techniques, which can easily get stuck in local minima.
Fortunately, it is possible to learn   directly from Eq.
(7) by the trace norm as in [14] [1].
It is de ned as follows: ((cid:11)(cid:11)(cid:11)  ( ) (cid:11)(cid:11)(cid:11)2 ) (cid:11)(cid:11)(cid:11)  ( ) (cid:11)(cid:11)(cid:11)2
 +
 (8)
 inf  =  ( )  ( )

 The trace norm is a convex function of  , and can be computed as the sum of its singular values.
The trace norm is di erent from the conventional squared norm for regularization purposes, and is actually a surrogate of matrix rank [6], and minimizing it can limit the dimension   of the topic space.
In other words, minimizing the trace norm results in the fewest topics to explain the correspondence between text and images.
This implies that concise semantic transfer with fewer topics is more e ective than tedious translation on cross-domain correspondence between text and images, as long as the learned translator complies with the observations (i.e., the co-occurrence and auxiliary data).
This is consistent with the parsimony principle, which states preference for the least complex translation model.
A parsimonious choice is also helpful in avoiding over tting problems which may arise in scenarios where the number of auxiliary training examples are small.
(7) can be rewritten as follows with the use of the trace norm: (  ( )   =1 min
        ( ( )  ( )   ) +   ) (  
    ,     ( )       ( )   )
 (9) We note that this objective function has a clear closed form and has a number of properties, which can be leveraged for optimization purposes.
In the next section, we discuss the methodology for optimization of this objective function.
In order to optimize the objective function above, we  rst need to decide which functions are used for  ( ) and  ( ) in Eq.
(9).
Recall that these functions are used to measure compliance with the observed co-occurrence and the margin of discriminant functions  ( ) on the auxiliary data set, respectively.
In this case, we use the well known logistic loss function   ( ) = log {1 + exp ( )} for the  rst function, and the exponentially decreasing function   ( ) = exp ( ) for the second.
This materializes the entire expression in a closed form algebraic format, which is easy to optimize.
After performing the aforementioned substitutions the objective function represented in Eq.
(9) is a nonlinear one.
One possibility for optimizing an objective function of the form represented in Eq.
(9) is to use the method of Srebro et al.
[14].
The latter work showed that the dual problem can be optimized by the use of semi-de nite programming (SDP) techniques.
Although many o -the-self SDP solvers use interior point methods and return a pair of primal and dual optimal solutions [5], they do not scale well with the size of the problem.
The work in [1] proposes a gradient based method which replaces the non-di erentiable trace norm with a smooth proxy.
But the smoothed approximation to   may not guarantee that the obtained minima still correspond to fewest topics for semantic translation.
Alternatively, a proximal gradient method is proposed in [15] to minimize such nonlinear objective functions with the use of a trace norm regularizer.
We will use such an approach in this paper.
In order to represent the objective function of Eq.
(9) more succinctly, we introduce the function   ( ) as follows.
  ( ) =     ( )      ( )   +      ,     ( )     ( )   (  ( )   =1 ( )) (  
 ) (10) Then, the objective function of Eq.
(9) can be rewritten as   ( )+ .
In order to optimize this objective function, the proximal gradient method quadratically approximates it by Taylor expansion at current value of   =   and Lipschitz coe cient   as follows:   ( ,   ) =   (  ) +    (  ) ,       +      2
  
 (11) We can further introduce the notation   in order to organize the above expression:   =      1  (  ) (12) We can use   the write the expression of Eq.
(11) as Algorithm 1 Proximal Gradient Solver for (9).
input Co-occurrence set  , text corpus  ( ), auxiliary
 training set  ( ), and balancing parameters   and  .
repeat repeat Initialize      0.
Set   =      1  (  ).
      Update   +1     diag     diag ( ) VT gives the SVD of   .
Set       ( until   (  +1) +    +1      (  +1,   ).
      + 1.
)
 + Here




 until Convergence or maximum iteration number achieves.
follows:  
      2   +  +  (  )  1   ( ,   ) = The gradient   (  ) can be computed as follows: ))   (  ) =  (    ( )  ) ( }  ( )   ( )   { ( 2     =1  ( )    ( )  ,     ( )   =1        ( )    ( )        ( )     ,   ( ) +     
   (  ) 2
 (13)    ( )   ( )   where   ( ) = of  ( ) and  ( ) with respect to  .
  1 +   (14) and   ( ) =    are the derivatives Algorithm 1 summarizes the proximal gradient based method to optimize the expression in Eq.
(9).
As shown,   can be updated by minimizing   ( ,   ) with  xed   iteratively.
This can be solved by singular value thresholding [6] as line 4 in Algorithm 1.
Note that as pointed out in [15], the convergence of the proximal gradient algorithm can be accelerated by making an initial estimate of   and increasing it by a constant factor   until   (  +1) +   +1      (  +1,   ).
At this point, it is deemed that we are su ciently close to an optimum solution, and the algorithm terminates.
A variety of transfer learning methods have been proposed in prior work [16][13][12].
The problem of learning semantic translators from text to images can also be seen as a kind of transfer learning method from heterogeneous data in di erent feature spaces.
For example, [18] proposes heterogeneous transfer learning, which uses both user tags and related document text as auxiliary information to extract a new latent feature representation for each image.
However, it does not utilize the text labels to enrich the semantic labels of images, which may restrict its performance when the image labels are very scarce.
On the other hand, translated learning [7] attempts to label the target instances through a Markovian chain.
A translator is assumed to be available between source and target data for correspondence.
However, given an arbitrary new image, such a correspondence is not always directly available between any text and image instances.
In this case, a generative model is used in the Markovian chain to construct feature-feature co-occurrence.
This model is not reliable when co-occurrence data is noisy Birds horses Category Num.
of pos.
ex.
Num.
of neg.
ex.
birds buildings cars cat dog horses mountain plane train waterfall



















 and sparse.
On the contrary, we explicitly learn a semantic translator, which directly links and propagates semantic labels from text to images even if the semantic correspondence is not available beforehand for a new image.
It avoids over tting into the noisy and sparse co-occurrence data by imposing the prior of fewest topics on semantic translation.
It is also worth noting that learning translator across het-erogenous domains is di erent from the conventional heterogeneous learning, such as multi-kernel learning [2] and co-training [4].
In heterogeneous learning, each instance must contain di erent views.
On the contrary, when translating text to images, it is not required that an image has an associated text view.
This makes the problem much more challenging.
The correspondence between text and images is established by the learned translator, and a single image view of an input instance is enough to predict its label by the translator.
Finally, we distinguish the proposed translator from the other latent models.
Previous latent methods, such as Latent Semantic Analysis [9], Probabilistic Latent Semantic Analysis [8] and Latent Dirichlet Allocation [3], are restricted to latent factor discovery from the co-occurrence observations.
On the contrary, in this paper, the goal is to establish semantic bridge so that the discriminative labeling information can be propagated between the source and target spaces.
To the best of our knowledge, it is one of the  rst algorithms to address such heterogeneous label transfer problem via a parsimonious latent topic space.
It is worth noting that even with unknown correspondence to source instances, it can still label the new instance by predicting its correspondence based on the learned translator.
In this section, we compare the proposed semantic translator to a pure image classi cation algorithm with an SVM classi er, and other existing transfer learning methods proposed in [18][7].
We will show that our approach provides superior results to both the methods, especially when the amount of auxiliary data is very limited.
The data sets consist of a collection of Flickr and Wikipedia web pages, since Flickr contains rich media content and Wikipedia has rich text documents.
We use 10 categories to evaluate the e ectiveness on the image classi cation task.
To collect text and image collections for experiments, the names of these 10 categories are used as query keywords to crawl web pages from Flickr web site and Wikipedia.
Both Flickr and Wikipedia contain many categorized web pages buildings mountain cars cat dog plane train waterfall Figure 3: Examples of crawled images over the different categories.
for these 10 categories.
Figure 3 illustrates some examples of crawled images in these categories, and Table 1 shows the number of crawled documents from these web pages for each category.
Flickr is an image sharing web site, where the users can share images with their friends and other users, and make textual tags and comments on the shared images.
For Wikipedia, the relevant web pages in the subcategories are also crawled.
In each crawled web page, the images and the corresponding text documents are used to establish correspondence between text and images.
For images, visual features are extracted to describe these images.
A visual vocabulary with 500 visual words is constructed to represent images.
These include the 500 dimensional bag of words based on SIFT descriptors [10].
For the text documents, all the tokens are extracted and stemmed from documents in Wikipedia and tags in Flickr, and their frequencies are used as textual features.
Table 3 shows the top-10 tokens extracted in the crawled text documents associated with the images over the 10 categories.
We can  nd that most of text content is closely related to these categories at the semantic level.
These tokens also give us some impression about the latent topics underlying these categories in the text domain.
For each category, the images are manually annotated by human to collect the ground truth labels for evaluation purposes as shown in Table 2.
Nearly the same number of background images are collected as the negative examples.
These background images do not contain the objects of the categories.
These image categories are not exclusive which means that one image can be annotated to be positive examples by more than one category.
We tested the accuracy and sensitivity of our transfer learning approach with respect to a number of algorithms.
In order to validate the performance of our proposed translator from text to images (TTI), we compared our approach with three other baseline algorithms for the image classi cation task.
These baselines are as follows: Category Number of crawled pairs Category Number of crawled pairs birds buildings cars cat dog horses mountain plane train waterfall









 Table 3: The Top-10 tokens in the crawled text documents associated with the images over the 10 categories.
It shows that most of text content is closely related to these categories at the semantic level.
The tokens also give us some idea about the latent topics underlying these categories in the text domain.
building sky night city architecture birds bird nature sky animal wildlife water clouds water  ight blue building animal sunset blue sea skyline cars car street road locomotive automobile tra c vehicle city police train cat cat kitty kitten animal cute pet feline pet nature white dog dog beach puppy pet running animal water blue cute nature horses horse foal nature bravo brazil brasil argentina cloud sky animal mountain mountain landscape nature cloud sky snow blue lake water tree plane airplane aircraft plane aviation airport  ying jet sky  ight  ghter train train railroad locomotive railway rail engine steam track sky bridge waterfall water sea sky sunset beach cloud blue ocean nature landscape
 si ers based on the visual features extracted from images.
This method does not use any of the additional information available in corresponding text in order to improve the e ectiveness of target domain classi cation.
The method is also susceptible to the case when we have a small number of test instances.
This is another transfer learning algorithm, which performs the translation by minimizing risk (TLRisk) [7].
The algorithm transfers the text labels to image labels via a Markovian chain.
It learns a probabilistic model to translate the text labels to image labels by exploring the occurrence relation between text documents and images.
We note however, that such an approach does not use the topic-space methodology which is more useful in connecting heterogeneous feature spaces.
gorithm is the best  t to our scenario with heteroge-nous spaces compared to other transfer learning algorithms such as [13][12] on a homogeneous space.
This methods has also been reported to achieve superior e ectiveness results.
It maps each image into a latent vector space where an implicit distance function is formulated.
In order to do so, it also makes use of the occurrence information between images and text documents as well as images and visual words.
To facilitate this method into our scenario, user tags in Flickr are extracted to construct the relational matrix between images and tags as well as that between tags and documents.
Images are represented in a new feature space on which the images can be classi ed by applying the  nearest neighbor classi er (here   is set to be 3) based on the distances in the new space.
We refer to this method as HTL.
In the experiments, a small number of example images are randomly selected for each category as labeled instances in the auxiliary training set  ( ) for the classi ers.
The remaining are used for testing the quality of the knowledge propagation through the classi cation application.
Thus, only a small number of examples are used, which makes the problem very challenging from the training perspective.
This process is repeated  ve times.
The error rate and the associated standard deviation for each category is reported in order to get an idea of the e ectiveness of the classi ers obtained through knowledge transfer process.
We also use varying number of co-occurred text-image pairs to construct the classi er, and compare the corresponding results with related algorithms.
All the parameters are tuned based on a 2-fold cross-validation procedure on the selected training set, and the parameters with the best performance are selected to train the models.
We compare the performance of di erent algorithms with varying number of training images.
For each category, the same number of images from the other categories are used as the negative examples.
Then error rate is shown for each category to measure the classi cation performance.
Since each image can be assigned more than one label, the error rate is computed in binary case.
We note that a smaller number of auxiliary training examples is also the most interesting case for our algorithm, because it handles the cases where the presented images do not have much past knowledge in the domain for the classi- cation process.
In order to validate this point further, we plot Figure 4, which compares the average error rates over all categories with varying number of auxiliary training exam-The smallest error rate for each category is in bold.
(a) Two training images Category birds buildings cars cat dog horses mountain plane train waterfall Image only




















 TLRisk




















 (b) Ten training images Category birds buildings cars cat dog horses mountain plane train waterfall Image only




















 TLRisk




















 Table 5: The number of topics (i.e., the rank of matrix  ) used for translation in topic space with two and ten training examples.
Category Two trn.
ex.
Ten trn.
ex.
birds buildings cars cat dog horses mountain plane train waterfall

















 ples.
It illustrates the advantages of our methods when there are an extremely small number of training examples.
This is consistent with our earlier assertions that our approach can work even in the paucity of auxiliary training examples, by exploring the correspondence between text and images.
In Tables 4(a) and 4(b), we compare the error rate of di er-ent algorithms for each category with two and ten auxiliary training images respectively.
We note that Table 4(a) (a) shows the results with a much smaller number of auxiliary training examples, and our proposed scheme performs much better than the baselines in almost all cases.
Also, Table 5 lists the number of topics (i.e., the rank of matrix  ) used for translation in topic space with two and ten training examples.
It shows that for most of categories with only a small number of topics, the learned translator







 e t a r r o r r



 Image Only
 TLRisk


 Number of auxiliary training examples





 Figure 4: Average error rate of di erent algorithms with varying number of training images.
model works very well.
This also provides evidence of the advantages of the parsimony principle in semantic translation.
However, this criterion is not absolute or unconditioned, but with the premise that the observed training examples and auxiliary co-occurrences are well explained by the learned model.
For complex categories with many aspects, it often uses more topics to establish the correspondence between the heterogeneous domains.
For example, as the appearances of   = 0.1   = 0.5   = 1.0   = 2.0








 t e a r r o r r








 Image Only
 TLRisk
 t e a r r o r r e e g a r e v












  



 Number of co occurred text image pairs
 Figure 5: Parametric Sensitivity - average error rate with di erent Parameters   and   on 10 auxiliary training examples.
Figure 6: Average error rate of di erent algorithms with varying number of text documents as source instances.
 buildings  are largely varying and often has lots of variants, more topics are needed to explain the correspondence than the categories with relatively uniform appearances.
But as long as the training data can be explained, the models with fewer topics are preferred.
These above results are obtained by using 2, 000 pairs of co-occurred text and images.
We know the number of co-occurred text-image pairs play an important role to connect these two heterogeneous domains in cross-domain label propagation.
Therefore, it is instructive to examine the effect of increasing the pair number.
In Figure 6, we illustrate the e ectiveness of di erent algorithms with varying number of text-image pairs.
The number of pairs is illustrated on the horizontal axis, whereas the error rate is illustrated on the vertical axis.
As we can see, the error rate of the TTI algorithms decreases with an increasing number of pairs since more correspondence information is explored to bridge text and image domains.
We also note that their improvements are more signi cant than other algorithms when more text-image pairs are involved.
In the experiments, the parameters   and   (used to decide the importance of auxiliary data and co-occurrence data from the objective function in (9)) are selected from {0, 0.5, 1.0, 2.0} and {0.1, 0.5, 1.0, 2.0}, respectively.
To illustrate the parametric sensitivity, Figure 5 illustrates the average error rate over all variations of   and   on 10 auxiliary training examples.
When   = 0, the average error rate is high, since no co-occurrence data is used in this case.
When   becomes large, the error rate decreases rapidly.
On the other hand,   adjusts the weight of auxiliary data with a reasonably large value.
From the  gure, it is evident that the smallest error is achieved at   = 1.0 and   = 0.5.
It is also evident from the results that the method achieves fairly stable behavior across many di erent values of   and  .
This implies that the method can be used in a robust way across a wide range of parameters.
In this paper, we presented a method to transfer knowledge across di erent domains in order to design an e ec-tive method for image classi cation.
This method is designed in order to alleviate the dual issues of scare labels and high semantic gaps which are inherent in the image domain.
The transfer process is designed with the use of a translator function, which can convert the semantics from text to images very e ectively via the cross-domain label propagation.
We show that the translator can be learned from the co-occurrence of text and images as well as a small size of training images with the use of a parsimonious representation with fewest topics.
We present proximal gradient methods to e ciently optimize the translator function.
For prediction, the semantic labels of the text corpus can be propagated to images by the learned translator.
We show superior results of the proposed algorithm for the image clas-si cation task as compared with state-of-the-art heterogeneous transfer learning algorithms.
Acknowledgment Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-09-2-0053.
The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the o cial policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government.
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
