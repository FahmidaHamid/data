Commerce search is gaining signi cance as more users spend increasing amounts of time searching for products on web.
Typically, the shopping verticals of commercial search engines or web portals maintain a catalog (or index) of products, and surface a consideration set of products from the catalog that best match the user query.
This is akin to the result set the user sees in traditional web search.
Unlike web search, the main goal of a consideration set is to aid the user in completing her transaction.
Figure 11 underscores the importance of a consideration set in commerce search where the entropy of clickthrough rates in the top 10 positions is very high.
In fact, it is close to the normalized maximum value of 1.0.
This suggests that the user is more likely to browse (or click) multiple results in di erent positions for commerce queries.
Another fundamental di erence between commerce search and web search is the presence of structure in both data and queries.
Typically the catalog is composed of products in a well-structured or semi-structured format and/or other rich metadata.
For example, a digital camera is associated with attribute information such as brand, model, color, etc.
In fact, this key aspect of the data can be exploited in commerce search.
On the query side, commercial search queries are good examples of (partially) structured queries wherein a set of attribute values relevant to the product being searched can potentially be extracted from the query string.
For example, a user query such as 10mp Nikon Digital Camera denotes the user s intent to see digital cameras from a particular manufacturer (Nikon) having certain desired features (10 megapixel resolution).
An important characteristic of the queries in commerce search is that the user does not always associate a commercial query with her intent to buy a product immediately.
Often, she issues many queries with di erent keywords (attributes of a product) and their combinations before she decides to buy a product.
Further, many of these exploratory queries are often imprecise and incomplete in expressing the
 months of query and click data from the search log of a commercial search engine.
s e t a r h g u o r h t k c i l c f o y p o r t n












 Probability of commercial intent Figure 1: The correlation between the likelihood of commercial intent in a query and the entropy of clickthrough rates in the top 10 positions user s information need.
This arises because of two reasons -
example, she may not be aware of the fact that Samsung only makes 55  lcd tvs and not 52  lcd tvs, or for that matter there are no 50  lcd tvs made by any manufacturer; and 2) not all keywords (attributes) that describe a product are used in a search query.
The number of keywords typically depends on the intent of the user such as how close is she to buying a product.
Let us elaborate with an example.
A user looking for digital cameras of a certain resolution might start her search session with 10mp digital cameras and then work her way toward issuing more speci c queries to the search engine such as Nikon Coolpix 10mp digital camera or browsing on the result page to the product she is interested in.
Note that in both these queries, a large fraction of the product attributes are still not speci ed.
Continuing with the example, there are multiple products in the Coolpix series with 10 megapixel resolution - the S3000 and S4000 with di erent colors and price points.
To substantiate our point, we de ne the notions of speci ed and unspeci ed attributes that we will use in our study to characterize the space of relevant products for a user query.
In our example, the speci ed attributes are brand, model line, and resolution, while the unspeci ed attributes correspond to color, optical zoom, and model number.
The above user query model motivates the development of new techniques for generating e ective consideration set of products in response to user queries.
Before we summarize our techniques for e ective consideration set generation, we need to introduce another important aspect of the user query model that we assume and exploit in our study.
Though the query acts as a point of reference to compute the closeness of a product with respect to the speci ed attributes, there is nothing similar to deal with the unspec-i ed attributes.
User preferences over di erent attributes will help us  ll this void.
A user preference function is typically de ned over the attribute value space for any given attribute.
Even though the user does not explicitly and exactly specify her information need through the user query, she will value the consideration set more if the search engine surface products with attribute values that are close to or exceed her preferences.
Again, consider the example of the digital camera.
If the S3000 has (nearly) the same attribute values as the S4000 but comes with a lower cost, then surfacing the lower priced camera could be considered a good search result by the user.
Extending this to multiple attributes each with di erent preference functions makes the problem challenging.
Contributions of this study In this study, we propose methods for generating the consideration set in commerce search that exploit both the speci- ed and unspeci ed attributes and user preferences to maximize user satisfaction.
We observe that each product can be represented as an attribute vector and each query as a list of attribute values.
It is natural to require that the surfaced products be close to the speci ed attribute values in the query, and diverse with respect to the unspeci ed attributes.
This motivates the following Max-Sum Dispersion problem.
We are given n nodes in a metric space, each node denoting a product and cost of a node being inversely proportional to its relevance with respect to speci ed attributes in the query.
The distance between two nodes (with respect to unspeci ed attributes) captures the notion of novelty or coverage.
The objective is to select a set of nodes with total cost not more than a given budget, and maximize the sum of the pairwise distances between the selected nodes.
This problem is known to be NP-hard, and no constant factor approximation was known previously.
We give the  rst 2-approximation algorithm for this problem along with some implementation results.
We note that our formulation is general enough to admit attributes and their values with associated importance scores.
These scores admit a natural preference a user might have over an attribute (value) with respect to the other attributes (values).
In fact, we incorporate the relative importance of attributes and their values in our experiments and show that our methods extend to these settings as well.
The work on diversi cation of search results has looked into similar objectives as ours where the likelihood of the user  nding at least one result relevant in the result set forms the basis of the objective function.
One of the early in uen-tial work on diversi cation is that of Maximal Marginal Relevance (MMR) presented by Carbonell and Goldstein in [5].
In their work, a trade-o  between novelty (a measure of diversity) and the relevance of search results is made explicit through the use of two similarity functions, one measuring the similarity among documents, and the other the similarity between document and query.
A parameter controls the degree of trade-o .
Vee et al [15] study diversi cation in the context of structured database with applications to online shopping.
In their work, items can be represented as a set of features or attributes, and the objective is to select a set of items that are as diverse as possible according to a lexicographical ordering of attributes.
In contrast, we propose an algorithm that admits general distance functions that satisfy the metric property and user preferences over attribute values in addition to attribute importance.
Further, we provide strong theoretical guarantees about the performance of our algorithm.
Zhai and La erty [18] formalize the notion that it is in general insu cient to simply return a set of relevant results and that the correlations among the results are also important.
Speci cally, they propose a risk minimization framework for information retrieval that allows a user to de ne an arbitrary loss function over the set of returned documents.
Bookstein [4] and Chen and Karger [7] both consider information retrieval in the context of ambiguous queries.
The basic idea in these works is that documents should be se-ument being relevant conditioned on the documents that come before.
Radlinski, Kleinberg, and Joachims [11] propose a learning algorithm to compute an ordering of search results from a diverse set of orderings.
By iterating through all documents in each of the positions while holding  xed the documents in the other positions, they attempt to learn a  best  ranking of documents using user clicks.
Agrawal et al. [1] take a di erent approach that makes use of a taxonomy for classifying queries and documents, and creates a diverse set of results in accordance to this taxonomy.
In approximation algorithms literature, all previous works on Max-Sum Dispersion considered the Uniform-cost version (when each node has unit cost) of the problem.
Another closely related problem is that of Max-Min Dispersion where the objective is to maximize the minimum pairwise distance of the selected nodes subject to a budget constraint.
Both the Uniform-cost Max-Sum and Uniform-cost Max-Min Dispersion problems are NP-hard [8, 17].
Ravi, Rosenkrantz, and Tayi [12] give a 2-approximation for Uniform-cost Max-min dispersion, and shows that it is NP-hard to obtain an approximation ratio better than 2.
For Uniform-cost Max-sum dispersion, they provide an e cient heuristic with approximation guarantee of 4, Hassin et.
al. [9] give a di erent algorithm achieving an approximation ratio of 2.
Birnbaum et.
al. [3] show, using factor-revealing LPs, that the e -cient heuristic proposed in [12] has in fact an approximation ratio of 2 for Uniform-cost Max-sum dispersion.
Finally, Rosenkrantz, Tayi, and Ravi [13] consider the Max-min dispersion problem and give a 2-approximation for the general case, along with other extensions.
We conclude this section by mentioning that there is an extensive literature on multiattribute utility theory and its applications in commerce search that is closely related to our work.
The interested reader is referred to the article by Wallenius et.
al. [16] for an excellent survey of this  eld.
First, we observe that each product can be characterized as an attribute vector, and given a query, attributes may be classi ed as speci ed or unspeci ed (with respect to the query).
Moreover, an attribute is either monotone or single peaked.
For example, the price and resolution of a camera are monotone attributes.
Everyone wants a cheap camera with high resolution.
On the other hand, attributes like color are single-peaked.
We assume that a user wants a consideration set of products that are close to the speci ed attribute values, and at the same time diverse with respect to the unspeci ed attributes.
Therefore we can model the above mentioned situation as follows.
We are given an undirected complete graph G = (V, E), a weight function d : E   R, and a cost function c : V   R. Each node represents a product, the weight of an edge captures the distance between the corresponding products with respect to unspeci ed attributes, and the cost of a node captures the distance of the corresponding product from the speci ed attribute values.
Throughout the rest of the paper, we will use the terms weight of an edge (x, y) and distance between nodes x, y interchangeably.
Our objective is to select a set of nodes S with small total cost c(S) = v S c(v) and high dispersion Disp(S).
The function Disp(S) captures how far away the nodes in S are from each other.
Two most natural measures of dispersion are Max-min Dispersion and Max-sum Dispersion.
Max-min dispersion of a set of nodes S is de ned as minu,v S{d(u, v)}, that
 is, the minimum pairwise distance between the nodes.
On the other hand, Max-sum dispersion of a set S is de ned x,y S d(x, y), that is, the sum of the pairwise distances as of the nodes.
Intuitively, dispersion of a consideration set should increase as more products are added to the set.
However, we note that the Max-min dispersion function is not monotone: Consider two sets S   S(cid:4) .
It may happen that Max-min dispersion of S is strictly greater than that of S(cid:4) .
Since Max-sum dispersion does not su er from this potential drawback, we de ne Disp(S) to be the Max-sum dispersion of S.
Recall that we want to select a set of nodes with small cost and large dispersion.
One possible way to formulate the problem is to maximize Disp(S)       c(S), where   is some prede ned constant.
Note that the problem can then be described as a LP relaxation.
Maximize
 d(u, v)xuv     c(v)yv
 u,v V v V xuv   min{yu, yv}  u, v   V
 (1) (2) Each yv is an indicator variable denoting whether or not the node v has been included in the consideration set S.
Furthermore, the variable xuv is set to 1 if and only if yu = yv = 1.
Let { xuv,  yu,  yv} denote the optimal solution to the above LP.
Pick an   uniformly at random from the interval [0, 1].
For each node v   V , include v in the consideration set i       yv.
It is easy to check that this rounding scheme preserves the optimal objective value in expectation.
Hence the optimal solution can be found in polynomial time.
However, this formulation su ers from a drawback.
It may so happen that the total cost of the selected nodes is too high, and this will defeat the intuition that the surfaced products should be close to the speci ed attributes.
The best way to get around this di culty is to maximize Disp(S) subject to the constraint that c(S) is at most some prede ned budget B.
It will be termed as the Max-Sum dispersion problem, and the main result of our paper is a 2-approximation algorithm for this setting, provided the edge weights satisfy triangle inequality.
For completeness, we note that without triangle inequality, the problem may be hard to approximate: When all distances are either 0 or 1 and all nodes have unit cost, the problem is equiva- lent to  nding the B-densest subgraph, that is,  nding a set of B nodes such that the induced subgraph has maximum number of edges, and the best known approximation ratio is O(|V |1/4) [2].
While the max-sum dispersion formulation addresses the issue of potentially unbounded cost of the consideration set, it has its own drawback related to the quality of unspec-i ed attribute values of the products it surfaces.
Consider the query brand = Nikon; Color = black; Category = digital camera.
Note that resolution of the camera is an unspeci ed attribute, and our algorithm will ensure that the products are as diverse as possible with respect to resolution.
However, resolution is also a monotone attribute: We should be surfacing only high resolution cameras.
Consider two di er-ent cameras x and y with di erent resolutions.
The distance if B = 0 then Output  ; end if B = 1 then Output any node x   V ; end Find an edge (x, y)   argmax {d(u, v) : u, v   V }; B(cid:4)   B   2; V (cid:4)   V \ {x, y}; Output {x, y}   UniformGreedy(V (cid:4), B(cid:4) Algorithm 1: Greedy algorithm with uniform costs ); between the two, d(x, y) does not capture the monotonicity.
Still we can remedy the situation by considering a new distance function d(cid:4) (x, y) = d(x, y) + w(x) + w(y), where w(x) (resp.
w(y)) denotes the importance of the resolution value of x (resp.
y).
In this case, a camera with higher resolution has higher importance.
We note that this works even with multiple monotone attributes.
If the underlying distance d(u, v) is a metric, then d(cid:4) (u, v) is also a metric and therefore all our theoretical guarantees hold for d(cid:4) (u, v) as well.
Another potential drawback of the max-sum dispersion objective is that the algorithm may select a product that is far away from the natural consideration set.
In such a situation, however, the outlier will have a high cost, and the algorithm will quickly exhaust the budget if too many such outliers are selected.
Hassin et.
al. [9] proposed a simple 2-approximation for Uniform-cost Max-Sum dispersion (see Algorithm 1).
The procedure UniformGreedy takes the set of nodes V and a budget B as inputs,  nds the edge (x, y) with highest weight, outputs its two endpoints, and calls itself recursively with V \ {x, y} and a budget of B   2.
In this section, we extend their result to give a 2 approximation for general Max-Sum dispersion where di erent nodes can have di erent costs.
First, consider the special case when the cost function takes only small number of di erent values.
In other words, suppose there is a partition of the set of nodes V into k buckets V0, .
.
.
Vk 1, such that all nodes in the same bucket have same costs.
Let (cid:4)w = (w0, .
.
.
, wk 1) be the demand vector whose ith component denotes the number of nodes from bucket Vi selected by the optimal solution.
We can guess the vector (cid:4)w in time O(nk), where n = |V |, and it is a polynomial in n as long as k is a constant.
In Section 4.2, we describe an extension to UniformGreedy that  nds a subset of nodes S subject to the constraint induced by vector (cid:4)w. Furthermore, we show (see Theorem 4.1) that it is a 2-approximation to the optimal solution.
The proof of Theorem 4.1 closely follows the argument in [9].
Next, consider the general situation when all the cost values can potentially be di erent from each other.
Fix some  > 0, and suppose we are allowed to overshoot the budget by a factor of at most (1 + O()).
Without any loss of generality, the maximum node cost is bounded from above by B, the budget.
Further, the minimum node cost is bounded Greedy(V, (cid:4)w) if wi = 0 for all i   {0, .
.
.
, k   1} then Output  ; end if wl = 1 for some l and wi = 0 for all i (cid:10)= l then Output any node xl   Vl; end Let (xi, xj) be the feasible edge with maximum weight; Let xi   Vi, and xj   Vj ; if i = j then i   wi   2; w(cid:4) i   Vi \ {xi, xj}; V (cid:4) else i   wi   1; w(cid:4) j   wj   1; w(cid:4) i   Vi \ {xi}; V (cid:4) j   Vj \ {xj}; V (cid:4) end for all t (cid:10)= i, j do V (cid:4) t = Vt ; w(cid:4) t = wt ; end
 = V (cid:4) V (cid:4) 0, .
.
.
, w(cid:4) = (w(cid:4) (cid:4)w(cid:4) Output {xi, xj}   Greedy(V (cid:4), (cid:4)w(cid:4) Algorithm 2: Greedy Algorithm for small number of distinct costs k 1.
; k 1).
; ); from below by B/n, since we can include in our solution all the nodes below this threshold with a total additional cost of B.
Now discretize the range from B/n to B in powers of (1 + ), getting L levels, and round every cost value down to the nearest level below it.
Clearly, (B/n)(1 + )L = B, implying L = (log n   log )/ log(1 + )   (log n)/2, for suf- ciently small .
Let  c(x) denote the rounded cost value of node x, for all x   V .
Thus, the set of all nodes has been e ectively partitioned into (log n)/2 buckets, and we can guess how many nodes from each bucket is selected by the optimal solution in O(n(log n)/2 ) time.
We can now run
 the greedy algorithm from Section 4.2 to get a 2 approximation.
Furthermore, note that for any subset of nodes S, if v S c(v)   (1 + )B.
 c(S) = In Section 4.3, we proceed to discretize the cost values even further in such a way that brings down the running time to a polynomial in n while preserving the approximation factor.
v S  c(v)   B, then c(S) = We begin with a simple de nition.
Definition 4.1.
Suppose the set of nodes V has been partitioned into k buckets V0 .
.
.
Vk 1.
Furthermore, let (cid:4)w = (w0, .
.
.
, wk 1) denote the demand vector.
An edge (xi, xj)   E, where xi   Vi, xj   Vj is feasible if 1) i = j and wi   2, or 2) i (cid:10)= j and wi, wj   1.
For a given partition of V into buckets V0 .
.
.
Vk 1, and demand vector (cid:4)w = (w0, .
.
.
, wk 1), let OPT(V, (cid:4)w) denote the subset of nodes with maximum dispersion that includes exactly wi nodes from bucket Vi, for all i   {0, .
.
.
, k   1}.
Theorem 4.1.
Algorithm Greedy is a 2-approximation to OPT(V, (cid:4)w).
(cid:4)w. Suppose the claim holds for any demand vector strictly dominated by (cid:4)w. To get some intuition behind the proof, consider the  rst edge (xi, xj) selected by Greedy, and suppose both its endpoints are also included in OPT(V, (cid:4)w).
Now consider some other node u (resp.
v) selected by OPT(V, (cid:4)w) (resp.
Greedy(V, (cid:4)w)).
Since (xi, xj) has maximum weight amongst the feasible edges, d(u, xi) + d(u, xj)   2d(xi, xj).
Since the edge weights satisfy triangle inequality, d(v, xi) + d(v, xj)   d(xi, xj).
Thus, we have d(u, xi) + d(u, xj)   2(d(v, xi) + d(v, xj)), and we can use induction hypothesis = V \ {xi, xj} and the adjusted demand vec-on the set V (cid:4) tor (cid:4)w(cid:4) (see Algorithm 2) to get the desired 2-approximation.
The complete proof is described in appendix.
wl cl   The complete procedure is outlined in Algorithm 3.
Let V0 denote the set of nodes with cost at most B/n.
As explained in Section 4.1, we can output all the nodes in V0.
Now dis-cretize the range from B/n to B in powers of (1+), getting L levels, and round every cost value down to the nearest level below it.
Let  c(x) denote the rounded cost value of node x, for all x   V \ V0.
It results in the set V \ V0 being partitioned into L buckets V1 .
.
.
VL, where L = (log n)/2.
Let wl denote the number of nodes selected from bucket Vl in the optimal solution.
De ne  cl =  c(x) for any node x   Vl.
Let , for some integer tl   {0, .
.
.
, (log n)/3   1}.
Under these circumstances, we will round the total cost of the nodes selected from bucket Vl down to tl(3B/ log n).
In other words, we are pretending that the contribution from each bucket is an integral multiple of 3B/ log n. We may underestimate the contribution from each bucket by at most 3B/ log n. Since there are (log n)/2 buckets, we can overshoot the budget by at most ((log n)/2)   (3B/ log n) = B.
Thus, we get the following Lemma (complete proof appears in appendix).
tl(3B/ log n), (tl + 1)(3B/ log n)   Lemma 4.2.
The set of nodes returned by the modi ed greedy algorithm has a total cost of at most (1 + O())B, for su ciently small  > 0.
Note that we need to guess L = (log n)/2 integers t1 .
.
.
tL such that each tl   {0, .
.
.
, (log n)/3   1}, and l tl = (log n)/3.
It was shown in [6] that there are only polynomi-ally many tuples satisfying the above mentioned condition.
For the sake of completeness, we prove the statement of Lemma 4.3 in appendix.
Lemma 4.3.
[6] For any  xed  > 0, the number of di erent L-tuples (t1, .
.
.
, tL) is polynomial in n whenever L = (log n)/2, tl = (log n)/3, and each tl is a non-negative integer.
l=1
 We can thus guess the correct demand vector in polynomial time and run Greedy to get a 2-approximation.
Lemma 4.4.
Algorithm ModifiedGreedy gives a 2-approximation to the optimal solution.
Proof.
Suppose the optimal solution satisfying the budget constraint includes exactly kl nodes from bucket Vl.
ModifiedGreedy guesses in polytime a demand vector (cid:4)w(cid:4) = l   kl, for each l   {0, .
.
.
, L}.
(w(cid:4) ModifiedGreedy returns the set Greedy(V, (cid:4)w(cid:4)), and Theorem 4.1 implies that it is a 2-approximation to OPT(V, (cid:4)w(cid:4) ).
L) such that w(cid:4) 0, .
.
.
, w(cid:4)
 ModifiedGreedy(V, B) V0   {v   V | c(v)   B/n}; V (cid:4)   V \ V0; Partition V (cid:4)   into L buckets V1, .
.
.
, VL such that for each l   {1, .
.
.
, L}, Vl   v   V (cid:4) | (B/n)(1 + )l 1 < c(v)   (B/n)(1 + )l for l   {1, .
.
.
, L} do   ;  cl   (B/n)(1 + )l 1; for v   Vl do  c(v) =  cl; end tl     end For each l   {1, .
.
.
, L}, guess 0, 1, .
.
.
, (log n)/3   1 for l   {1, .
.
.
, L} do
 l=1 s.t.
tl = (log n)/3; Select the largest integer wl s.t.
wl cl   (tl + 1)(3B/ log n); l   min(wl,|Vl|); w(cid:4) end

 Let w(cid:4) Let (cid:4)w(cid:4)   {w(cid:4) S   Greedy(V = V0   V1   .
.
.
  VL, (cid:4)w(cid:4) Output S 2, .
.
.
, w(cid:4) 0, w(cid:4)
 ); Algorithm 3: Modi ed Greedy Algorithm For General Cost Functions Since the function Disp(S) is monotone in S, dispersion of the set OPT(V, (cid:4)w(cid:4)) is at least the optimal objective value subject to the budget constraint.
The lemma follows.
Combining the previous lemmas, we get the main result of this section.
Theorem 4.5.
ModifiedGreedy runs in polytime for any  xed su ciently small  > 0, overshoots the budget by a factor of at most (1 + O()), and is a 2-approximation to the optimal solution.
In this section, we do an empirical evaluation of our model and algorithms described in the earlier sections.
We primarily evaluated the performance of our algorithm, Modified-Greedy along two dimensions - the quality of results, and the diversity of the products with respect to the unspeci ed attributes in the query.
We used to two di erent ranking functions to compare our algorithms against.
In the  rst setup, we used a prototype commerce search engine serving results from an index of about 30 million products taken from the Bing shopping catalog2.
We use the technique described in [14] to annotate the query with attribute information.
For the second ranking function, we used the shopping vertical of a commercial search engine.
In both the cases, we compared the quality of the results produced by our algorithms with those produced by the respective ranking functions.
In order to make the comparison, we need attribute information related the products in the consideration set in each case.
In the prototype 2http://shopping.bing.com e i r e u q f o r e b m u
















 Number of attributes in query Figure 2: The histogram of queries with number of extracted attributes y c n e u q e r










 Number of important attributes Figure 3: The histogram of number of categories with a given number of attributes search engine, the search index was generated using the full text descriptions and the structured attribute information of the products.
Thus, the consideration set produced by the baseline structured ranking function is already enriched with attribute information.
In the case of the shopping vertical, we extracted the attributes of the products in the consideration set from the product pages surfaced by the search engine and we mapped the attributes into the space of important attributes used in our prototype commerce search engine.
As our query set we used a random sample of 3200 queries from the shopping vertical of Bing.
We extracted the attribute value pairs for these queries.
Naturally, our annotator extracted structured information to di erent degree from these queries.
Figure 2 shows the histogram of queries with number of structured attributes extracted from them.
Further, we categorized these queries using a Nai ve Bayes classi er and found them to be spread across 173 categories in our product taxonomy.
Another important statistic is that the average length of the query in our test set was 4.4 terms.
In all the experiments, we considered the important attributes of a category, i.e., attributes that are frequently asked by the user or occur in a large fraction of the products3 in the index.
In our case, we set the selectivity threshold to

 us an idea of the number of unspeci ed attributes that our algorithm has to deal with.
3also known as selectivity of the attribute
 tions In the prototype search engine, we used a ranking function that exploits structure in both queries and products.
It is a simple combination of both structure and text.
It exploits structure where it s present (in the query) and falls back to using textual relevance when no structure could be extracted from the query.
The relevance with respect to the text is based on the well-known IR features like BM25F, Proximity of query terms in the product description, and fractional match of query terms.
The key measure that is used in computing the structural relevance is the distance between corresponding attribute values.
We denote the distance between any two attribute values u and v for a given attribute i as di(u, v).
Speci cally, we use the likelihood of an average user preferring v to u as the similarity4 between u and v. This measure can be applied to both categorical and numeric attributes.
For numeric attributes, we de ne di(u, v) = min(1.0, |u v| u ).
For categorical attributes, we adopt the method described in [10] which is based extracting user preferences for attribute values from browse trails on the web.
We note that this distance function does not satisfy all the properties of a metric.
Next, we describe the application of the distance function in our experiments.
We begin with the baseline ranking function.
For the speci ed attributes, the relevance of the product with respect to the query is computed using the L1-norm of the distance between the corresponding attribute values, i.e., c(p) = d(q, p) = i di(u, v) where u and v are respectively the values of attribute i in the query q and product p. We use this scoring function along with the textual relevance function as the baseline ranking function.
We compare the performance of ModifiedGreedy against this function.
Handling Attribute Dependencies Though we don t explicitly deal with attribute dependencies in this study, our algorithms admit more complex distance functions that incorporate attribute dependencies.
For example, the algorithms proposed in [10] can be used to learn user preferences over a set of attribute values.
To keep the problem tractable, this computation could be limited to sets of popular attributes and their values.
We now move on to the implementation of Modified-Greedy.
We will describe how we use the baseline ranking function and the distance between unspeci ed attributes to implement ModifiedGreedy.
We begin with user preferences.
Note that here we did not incorporate any user preferences in the baseline structured ranking function.
To do so, we need to modify the distance function as j dp i (u, v) =
 di(u, v) if v   u otherwise for an attribute i that is monotone-upward, i.e., we don t penalize products with attribute value v greater than the corresponding attribute value u in the query.
A similar function is de ned for monotone-downward functions as well.
For single-peaked attributes, the distance stays unmodi ed.
4simi(u, v) = 1   di(u, v) f o r e b m u n e g a r e v a d e t h g i e
 s e t u b i r t t a Baseline Structured Ranking ModifiedGreedy










 Number of attributes in query


 Figure 4: The weighted average number of di er-ent attributes for queries grouped by the number of speci ed attributes in the query We use the modi ed distance function along with the textual relevance function used in the baseline structured ranking function to generate the  lter set of products.
Note that the  lter set is a intermediate set of relevant products on which we run ModifiedGreedy to generate the consideration set for the user.
Typically, the size of the  lter set is much larger (e.g., 300) than the size of the consideration set (e.g., 10).
Note that the above distance function does not capture monotonicity.
To circumvent this problem, we incorporate relative importance of attributes and their values into the distance function as described in Section 3.
The computation of importance values is not the focus of this study and we therefore assume that the importance values are pre-computed and available to our algorithm.
Selection of the vector (cid:4)w We also made a few changes to ModifiedGreedy to make it run e ciently in the prototype search engine.
The key step is the guessing of the integers ti for i = 1, 2,   , L. For each bucket we toss a coin with bias 1   ci C where C is the total cost of all the buckets.
Constructing (cid:4)w this way has the desired property of picking elements from buckets with smaller costs and thereby resulting in products which are closer to the user query (in terms of speci ed attributes).
Ranking Function We ran a set of experiments to compare the performance of ModifiedGreedy against the baseline ranking function described in the previous section.
In all these experiments, we used the top 10 results surfaced for each query in our query set.
Along with the basic attribute information such as name of the product, manufacturer and price, we extracted information associated with all the important attributes of each product.
In the  rst experiment, we compared the amount of important yet diverse information surfaced by both the algorithms.
To do so, we compute the number of di erent attributes surfaced in the result set weighted by their importance.
Figure 4 illustrates the weighted average number of attributes for di erent number of speci ed attributes in the query.
As the  gure shows, the more general the query (corresponds to less number of attributes), the better Mod-ifiedGreedy performs with respect to the baseline.
As the queries get more speci c (i.e., have more attributes), both the algorithms perform equally well.
Next, we compare the relevance of the consideration set Baseline Number of attributes Min Max














 Avg




 ModifiedGreedy Min Max









 Avg




 Table 1: Relevance of the consideration set with respect to the speci ed attributes of the best, worst, and average document in the set, averaged over all queries in test set in both cases by computing the minimum, maximum, and the average distance of the products from the query over the speci ed attributes.
These three measures give us a sense of how close is the consideration set to the query in terms of the best, worst and the average result in the set.
Table 1 highlights the relative performance of both the algorithms.
Again, we compare the corresponding values by bucketing the queries into buckets based on the number of attributes extracted from the query.
As the results show, ModifiedGreedy indeed performs as well as the baseline with respect to the nearness of the surfaced products to the query in terms of the speci ed attributes, i.e., the attributes recognized in the query.
This observation seems to hold across all range of queries, from speci c to general.
Finally, we performed a qualitative evaluation by conducting a user study using the Amazon Mechanical Turk plat-form5.
To compare the e ectiveness of the consideration set in both cases, we selected two criteria to measure, viz., the best product for each query and the usefulness of the un-speci ed attribute values in the consideration set in helping the user in her decision to buy the product.
As part of the evaluation, we sampled 240 queries and presented the human judge the top 10 results along with a budget (equal to the price of the largest product in the consideration set) and the unspeci ed attributes associated with the query and asked the judge to select the product she is most likely to buy and chose the unspeci ed attributes (if any) that aided in her decision.
We presented each query to 11 judges.
To get the best product, we simply selected the product with the largest number of votes among the judges.
Ties were broken randomly.
We then asked the judges to pick the better product among the two best products (one for each algorithm).
This gives us a measure of the e ectiveness of each consideration set in surfacing the product the user is more likely to buy.
In fact, the judges found the best product surfaced by ModifiedGreedy bettered the best product surfaced by the baseline algorithm 58.6% of the queries.
With respect to the second criteria, we also presented the judges with the best unspeci ed attribute (again one for each algorithm) and asked them to choose the better one.
Again, the judges concluded something similar.
Modified-Greedy was surfacing more relevant information related to the unspeci ed attributes compared to the baseline.
Speci -cally, it did better in 67.8% of the queries.
Coincidentally, in the  rst evaluation, the judges did not  nd the unspeci ed attributes helpful in around 21.7% of the queries while the corresponding fraction for the consideration sets generated by ModifiedGreedy was 10.6%.
This suggests that the 5https://www.mturk.com/mturk/welcome tribute values in the consideration set generated by ModifiedGreedy and the shopping vertical of a commercial search engine Figure 6: The fraction of the top-10 attribute values covered by the consideration set.
This fraction is computed as a average over all the unspeci ed attributes consideration set generated by our algorithm not only surfaces relevant results to the user s query but also surfaces more helpful information not directly associated with the speci ed attributes in the query.
Vertical We ran the same set of 3200 shopping queries against a shopping vertical of a commercial search engine and scrapped the results of each query.
Beyond basic knowledge of the ranking function of the search engine having access to the attributes values of the products, we did not know anything else about the ranking function itself.
We simply treated it as a black-box ranking function.
To measure the amount of diversity in the consideration set with respect to the unspeci ed attributes, we aggregated the attribute values for each product in the consideration set over all queries, again grouping the queries according to the number of speci ed attributes in them.
Figure 5 shows that the average number of unspeci ed attribute values surfaced by ModifiedGreedy is larger than the corresponding number for the commercial shopping vertical.
Interestingly, we did not observe a similar di erence when we compared Mod-ifiedGreedy with the baseline structured ranking function.
We also performed a user study to compare the relevance of the consideration sets produced by the shopping vertical and ModifiedGreedy.
As in the experiment in the last section, we compared the best products surfaced in the each of the consideration sets.
Again, the best product of Modi-fiedGreedy was chosen 60% of the time over the best result produced by the shopping vertical.
The di erence is more pronounced in the case of the best unspeci ed attribute (i.e., the unspeci ed attribute chosen by the user to be most helpful in their decision to buy the product) in both cases.
In fact, 78% of the users preferred the the unspeci ed attribute surfaced by ModifiedGreedy to be more helpful than the corresponding unspeci ed attribute in the consideration set in the shopping vertical.
In the experiments described in the previous section, we focused on coverage of important unspeci ed attributes where we measured the importance using the product data in the index.
Further, we did not consider any di erence in importance of the attribute values themselves.
In this section, we describe a set of experiments, where we measured the number of intents covered by the consideration sets.
Here, we de ne an intent to be the set of attribute values associated with a query.
For example, when the user issues a query 17 inch laptops, one of her intents could be to buy a $700 17 inch dell latitude laptop.
Another could be to buy a $500
 distribution over the attribute values for a given product.
Bed type (Importance) Bed Type (Importance) Platform (0.23) Bunk (0.19) Teen (0.17) Toddler (0.11) Loft (0.11) Trundle (0.07) Kids (0.04) Sleigh (0.03) Canopy (0.03) Adjustable (0.01) Table 2: Relative importance of attribute values for the attribute bed type in the category beds In this experiment, we don t explicitly compute this distribution.
Instead we approximate it by computing the top attribute values for each of the important attributes the user is likely to consider in her search.
Here we assume independence between attributes and hence we treat the likelihood of the user picking one attribute value independent of her choice of values for other attributes.
We adopt the de nition of attribute value importance described in [10] wherein the importance is measured in terms of the user s likelihood of switching the value during the course of her browsing the web for the product.
Thus, an important attribute value in the query is likely to exist in the product she eventually buys.
For completeness, we include an example here.
Table 2 shows the important attribute values for bed type in the category home furnishings|furniture|beds.
Thus, for each consideration set generated for a query in our test set, we compute the weighted sum of the attribute values covered by the set using each value s importance as its weight.
Again, to be able to compare both structured ranking functions with the shopping vertical, we mapped the attribute values extracted from the product pages to the corresponding attribute values used in the search index of our prototype search engine.
This mapping was done using the Jaccard similarity6 measure and picking the tar-


analysis for a greedy remote-clique algorithm using factor-revealing lps.
Algorithmica, 55(1):42 59, 2009.
[4] A. Bookstein.
Information retrieval: A sequential learning process.
Journal of the American Society for Information Sciences (ASIS), 34(5):331 342, 1983.
[5] J. G. Carbonell and J. Goldstein.
The use of MMR, diversity-based reranking for reordering documents and producing summaries.
In SIGIR, pages 335 336,
 [6] C. Chekuri and S. Khanna.
A ptas for the multiple knapsack problem.
In SODA, pages 213 222, 2000.
[7] H. Chen and D. R. Karger.
Less is more: probabilistic models for retrieving fewer relevant documents.
In SIGIR, pages 429 436, 2006.
[8] E. Erkut.
The discrete p-dispersion problem.
Eur.
J.
Opnl.
Res., 46:48 60, 1990.
[9] R. Hassin, S. Rubinstein, and A. Tamir.
Approximation algorithms for maximum dispersion.
Oper.
Res.
Lett., 21(3):133 137, 1997.
[10] D. Panigrahi and S. Gollapudi.
Result enrichment in commerce search using browse trails.
In WSDM, 2011.
[11] F. Radlinski, R. Kleinberg, and T. Joachims.
Learning diverse rankings with multi-armed bandits.
In International Conference on Machine Learning (ICML), 2008.
First presented at NIPS07 Workshop on Machine Learning for Web Search.
[12] S. S. Ravi, D. J. Rosenkrantz, and G. K. Tayi.
Heuristic and special case algorithms for dispersion problems.
Operations Research, 42(2):299 310, 1994.
[13] D. J. Rosenkrantz, G. K. Tayi, and S. S. Ravi.
Facility dispersion problems under capacity and cost constraints.
J. Comb.
Optim., 4(1):7 33, 2000.
[14] N. Sarkas, S. Paparizos, and P. Tsaparas.
Structured annotations of web queries.
In SIGMOD, pages
 [15] E. Vee, U. Srivastava, J. Shanmugasundaram, P. Bhat, and S. Amer-Yahia.
E cient computation of diverse query results.
In ICDE, pages 228 236, 2008.
[16] J. Wallenius, J. S. Dyer, P. C. Fishburn, R. E. Steuer, S. Zionts, and K. Deb.
Multiple criteria decision making, multiattribute utility theory: Recent accomplishments and what lies ahead.
Management Science, 54(7):1336 1349, 2008.
[17] D. W. Wang and Y.-S. Kuo.
A study on two geometric location problems.
Inf.
Process.
Lett., 28(6):281 286,
 [18] C. Zhai and J. D. La erty.
A risk minimzation framework for information retrieval.
Info.
Processing and Management, 42(1):31 55, 2006.
Proof.
We use induction on the demand vector (cid:4)w. For the base case, if exactly one component of (cid:4)w is 1 and every other component is zero, there is nothing more to prove.
Suppose the claim is true for any demand vector that is strictly dominated by (cid:4)w. The greedy algorithm selects the edge (xi, xj), xi   Vi, xj   Vj before recursively calling itself on (V (cid:4), (cid:4)w(cid:4) ).
Let S(cid:4), O(cid:4), S, O denote the set of nodes Figure 7: The fraction of the top-10 attribute values covered by the consideration set.
This fraction is computed as a weighted average over all the un-speci ed attributes get attribute value with the highest similarity score above a threshold (0.5 in our case).
In another experiment, we further weigh the contribution of attribute values of an attribute with the importance of the attribute.
Figures 7 and 6 show the relative performance of all baseline structured ranking function, ModifiedGreedy, and the shopping vertical for the respective settings of with and without attribute importance.
As both the  gures show, ModifiedGreedy surfaces more useful attribute values to the user in the consideration set compared to both the baseline structured ranking function and the shopping vertical in the case when the contribution of attribute values of an attribute are weighted by the importance of the attribute.
In the other case where all attributes are treated uniformly, ModifiedGreedy outperforms the other algorithms when the queries are more general.
In this study, we propose an algorithm for generating the consideration set in commerce search that exploits both the speci ed and unspeci ed attributes and user preferences to maximize user satisfaction.
Speci cally, we show that this setting admits a natural notion of user satisfaction that requires that the surfaced products be close to the speci ed attribute values in the query, and diverse with respect to the unspeci ed attributes which we solve using a max-sum dispersion formulation.
This problem is known to be NP-hard, and no constant factor approximation was known previously.
We give the  rst 2-approximation algorithm for this problem along with extensive empirical analysis involving both real-world data and user studies.
The directions for future work include studies on the e ect of di erent demand vectors and budgets on the quality of the  nal solution.
