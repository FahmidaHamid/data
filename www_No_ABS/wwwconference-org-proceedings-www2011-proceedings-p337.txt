The ability to quantify semantic relatedness of texts underlies many fundamental tasks in natural language processing, including information retrieval, word sense disambiguation, text clustering, and error correction.
Previous approaches to computing semantic relatedness used various Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
linguistic resources, such as WordNet, Wikipedia, or large-scale text corpora for methods like Latent Semantic Analysis (LSA).
Yet all of these approaches essentially considered the underlying linguistic resource as a static collection of texts or concepts.
In this paper we argue that there is an additional source of rich information about semantic relatedness of words, which can be revealed by studying the patterns of word occurrence over time.
Consider, for example, words such as  war  and  peace .
While these words are clearly related, they might rarely be mentioned in the same documents.
However, they are likely to be mentioned roughly around the same time (say, in different articles posted during the same day, or in adjacent days).
In this work, we use the New York Times archive spanning over 130 years.
For each word, we construct the time series of its occurrence in New York Times articles.
We posit that if there is a correlation between the time series of two words, then the meanings of the two words are related.
In principle, there are a number of cases when temporal information could o er a complementary source of signal, which is not captured by other models.
Synonyms (that is, words with similar meanings) are rarely used in the same article since an author usually sticks to one set of terms, yet they can be used by di erent authors in di erent articles describing the same events.
Looking at their coordination in time allows us to leverage the opinions of multiple authors collectively.
As another example, consider pairs of words that form stock phrases, such as  luxury car .
Taken individually, the two words in each pair have very di erent meanings, and are likely to be judged as such by existing methods.
On the other hand, these words are indeed related, and the frequency of their use over time exhibits nontrivial correlation.
Especially interesting are pairs of words that have implicit relationships such as  war  and  peace  or  stock  and  oil , which tend to correlate in frequency of use over time.
Figures 1 and 2 depict these correlations in time.
The proposed method, Temporal Semantic Analysis, captures such correlations, and is able to better estimate semantic relatedness than methods that only use static snapshots of linguistic resources.
The contributions of this paper are threefold.
First, we propose to use temporal information as a complementary source of signal to detect semantic relatedness of words.
Speci cally, we introduce Temporal Semantic Analysis (TSA), which leverages this information and computes a re ned metric of semantic relatedness.
Second, we construct a new dataset for semantic relatedness of words, which we have judged with the help of Amazon s Mechanical Turk service.
such as ESA [16].
However, while ESA uses a static representation of each concept, we use the concept dynamics  its behavior over time, represented by the time series of the concept occurrence.
Thus, instead of representing a word with a vector of unit concepts, vectors of time series are manipulated, where each time series describes concept dynamics over time.
Our hypothesis is that concepts that behave similarly over time, are semantically related.
Such a rich representation of words (adding the extra temporal dimension) could facilitate the discovery of implicit semantic relationships between the original words.
As we will show experimentally, the naive approach of directly computing temporal correlation between words (without the concept vector representation) is not e ective.
Thus, our TSA method consists of three main steps:
 cept repository of choice (e.g., Wikipedia or Flickr image tags), represent a word as a set of associated concepts with weights (Section 2.1).
using a corpus of choice (e.g., New York Times archive), quantify concept occurrence for each time period (e.g., a day) and build its time series (Section 2.2).
dynamics:  nally, scale each concept s time series according to the concept s original weight from item 1 above (Section 2.3).
In our representation, each word is mapped into a vector of concepts   a concept vector.
For each concept a static weight is computed.
We consider several such representations over multiple folksonomies:
 knowledge repositories on the Web, which is written collaboratively by millions of volunteers around the world, and almost all of its articles can be edited by any user.
Wikipedia is available in dozens of languages, while its English version is the largest of all with more than 500 million words in over three million articles.
Vector space models based on this ontology have been used by many works for semantic relatedness [16, 32].
In those representations, each entry in the concept vector is a TFIDF-based function of the strength of association between the word and the concept in Wikipedia.
ing community.
Photo submitters have the option to add metadata to each image in the forms of natural language tags.
This feature enables searchers to  nd images related to speci c topics.
The natural concepts in this representation are the Flickr tags.
bookmarking service, with the possibility to search and explore new bookmarks.
The service had, by the end of 2008, more than 5.3 million users and 180 million unique bookmarked URLs.
Del.icio.us users can tag each of their bookmarks with free text, and we use these tags as concepts.
Figure 1: Time series (1870-1988) of the words  war  (red) and  peace (blue).
The words correlate over time.
Figure 2: Time series (1870-1988) of the words  stock  (red) and  oil (blue).
The words correlate over time.
In contrast with the previous standard benchmark, WS-353, our new dataset has been constructed by a computer algorithm (also presented below), which eliminates subjective selection of words.
We make the new dataset publicly available for further research in the  eld.
Finally, empirical evaluation shows that TSA exhibits superior performance compared to the previous state of the art method (ESA), and achieves higher correlation with human judgments on both datasets.
We propose Temporal Semantic Analysis (TSA), which is composed of two novel components: a new approach, described in this section, for representing the semantics of natural language words, and a new method, described in Section 3, for computing the semantic relatedness between words.
Our method is based on associating each word with a weighted vector of concepts.
Such concepts can be derived from crowd intelligence folksonomies such as Wikipedia, co-tagging in Flickr, or from online bookmarking services such FrequencyTimeTimeFrequencyWWW 2011   Session: Semantic AnalysisMarch 28 April 1, 2011, Hyderabad, India338(cid:10)  ts1,  ts2 n} 1, .
.
.
, ts1 m} 1, .
.
.
, ts2 Procedure Semantic Relatedness(t1, t2) (1)C(t1) = {ts1 (2)C(t2) = {ts2 (3)R(t1, t2)   0 (4)Repeat M in(m, n) times (5) (6) R(t1, t2)   R(t1, t2) + Q(  ts1,  ts2) (7) C(t1)   C(t1)\{  ts1} (8) C(t2)   C(t2)\{  ts2} (9)Return R(t1, t2) (cid:11) = arg max(cid:104)ts1,ts2(cid:105) C(t1) C(t2) Q(ts1, ts2) Figure 4: A greedy algorithm for computing the semantic relatedness between two words.
The procedure assumes the availability of a function Q that determines relatedness between a pair of time series tsi associated with two concepts.
To compute semantic relatedness of a pair of words we compare their vectors (as de ned in Section 2.3) using measurements of weighted distance between multiple time series, combined with the static semantic similarity measure of the concepts.
This approach, therefore, integrates both temporal and static semantic behavior of the words.
The ESA method for computing semantic relatedness is based on the assumption that related words share highly-weighted concepts in their representations.
The TSA approach does not assume so.
We only assume that highly-weighted concepts of the related words are related.
1, .
.
.
, c1 1, .
.
.
, c2 Suppose we are trying to  nd the relatedness between words t1 and t2.
Assume that t1 is mapped to a set of concepts C(t1) = {c1 n} and t2 is mapped to C(t2) = {c2 m}.
Suppose we have a function Q that determines relatedness between two individual concepts using their dynamics (as de ned in Section 2.2).
Assuming w.l.o.g n   m, we can de ne the relatedness R between t1 and t2 as the maximal sum of pairwise concept relatedness over all ordered subsets of size n of C(t2): (cid:88) R(t1, t2) = max jl (1...(m n)) l=1,...,n Q(c1 l , c2 jl ) (2) This exhaustive search over all possible pairs is, however, in-feasible.
Therefore we take an alternative greedy approach, which is formally described in Figure 4.
The procedure at each step  nds a pair of time series with the highest relatedness Q (line 5 in the algorithm), removes them and proceeds (lines 7 and 8).
Iteratively, the relatedness R(t1, t2) is computed as the sum of relatedness of the matching concepts (line 6).
This procedure complexity is O(n   m   max(|ts|)), where |ts| is the length of the time series representing the concepts.
The relatedness Q between two concepts is determined by comparing their dynamics.
Our basic assumption is that related concepts correlate in their temporal behavior.
For comparing the concepts associated time series, we use two Figure 3: Each word is represented by a weighted vector of concept time series (produced from a historical archive H).
The weight wi of each concept corresponds to the concept  importance  w.r.t.
the original word.
Let c be a concept represented by a sequence of words wc1, .
.
.
, wck.
Let d be a document.
We say that c appears in d if its words appear in the document with a distance of at most   words between each pair wci, wcj, where   is a proximity relaxation parameter (in the experiments we set   = 20).
That is, a concept appears in a document if there is a window of size   where all the concept words appear.
For example, for the concept c    Great Fire of London    we say that the c appears in a document d, if the words  Great ,  Fire ,  of ,  London  appear in the document with a distance of at most   between each word.
Let t1, .
.
.
, tn be a sequence of consecutive discrete time points (e.g., days).
Let H = D1, .
.
.
, Dn be a history represented by a set of document collections, where Di is a collection of documents associated with time ti.
We de ne the dynamics of a concept c to be the time series of its frequency of appearance in H: Dynamics(c) = (cid:104)|{d   D1|appears(c, d)}| |{d   Dn|appears(c, d)}| (cid:105)
 , .
.
.
, |Dn| (1) In the experiments described in this paper we used New York Times articles since 1870 for history.
Each time point is a day, and the collection of documents associated with a day is the set of articles appearing on that day.
poral Signals Our approach is inspired by the desire to augment text representation with massive amounts of temporal world knowledge.
Hence, we represent a word as a weighted mixture of concept time series, where the weights correspond to the concept  importance  w.r.t.
the original word (Figure 3).
In common semantic representations (such as ESA [16]) a word is represented as a weighted vector of concepts (derived from Wikipedia articles).
In ESA, each vector entry contains a single (static) TFIDF weight, which expresses the strength of association of the word and the concept.
Our TSA method extends ESA so that each entry in the vector corresponds to a time series, computed as described above.
correlation and dynamic time wrapping (DTW).
In statistics, cross correlation is a method for measuring statistical relations, e.g., measuring similarity of two random variables.
A common measurement for this purpose is the Pearson s product-moment coe cient which is de ned as: corr(X, Y ) = cov(X, Y )  X  Y  X  Y (xi    x)(yi    y) = n(cid:80) (cid:80)N i=1(xi   x)2 i=1 (cid:113) 1
 (cid:113) 1

 = (3) (cid:80)N i=1(yi   y)2 In signal processing, cross-correlation is used as a measure of similarity of two signals as a function of a time-lag applied on one of the signals   a variation of the Pearson coe cient to di erent time delays between two time series in Figure
 of similar time series in volume, with consideration of time shifts.
In our representation, where words are represented as time series, words whose frequencies correlate in volume, but with a time lag, will be identi ed as similar.
When we wish to evaluate the correlation of the two words  time-series, we compare the time series starting from the  rst time point they both started appearing, until the time point when one of the words stopped appearing.
For example, the word  computer  did not appear during the 1800s, and started to appear only around 1930.
Therefore, when we compare it to the word  radio , we calculate the cross correlation only during the period starting at 1930.
The DTW algorithm [5] measures the similarity between two time series that may di er in time scale, but similar in shape.
In speech recognition, this method is used to identify similar sounds between di erent speakers whose speech speed and pitch might be di erent.
The algorithm de nes a local cost matrix C   R|ts1| |ts2| of two time series ts1 and ts2 as Ci,j = (cid:107)ts1[i]   ts2[j](cid:107), i   (cid:104)1 .
.
.|ts1|(cid:105), j   (cid:104)1 .
.
.|ts2|(cid:105) (4) where (cid:107)ts1[i] ts2[j](cid:107) is a distance metric between two points of the time series.
Given this cost matrix, DTW constructs an alignment path that minimizes the cost over this cost matrix.
This alignment p is called the  warping path , and de ned as a sequence of points pairs p = (pair1, .
.
.
pairk), where pairl = (i, j)   (cid:104)1 .
.
.|ts1|(cid:105)   (cid:104)1 .
.
.|ts2|(cid:105) is a pair of indexes in ts1 and ts2 respectively.
Each consequent pair preserves the ordering of the points in ts1 and ts2, and enforces the  rst and last points of the warping path to be the  rst and last points of ts1 and ts2.
For each warping path p we compute l=1 C(pairl).
The DTW is de ned to be its cost as c(p) =(cid:80)k the minimum optimal warping path DT W (ts1, ts2) = min{c(p)|p   P |ts1| |ts2|} (5) where P are all possible warping paths.
A dynamic programming algorithm (similar to the one in Figure 6) is usually applied to compute the optimal warping path of the two sequences.
This similarity measurement, as opposed to time series cross-correlation distance (cf.
Section 3.2.1) is much more  exible, hence we decided to experiment with it as well.
As the meaning of the words changes over time, more recent concept correlation are more signi cant than past correlation.
Therefore, when measuring the distance between two individual time series, higher weights to recent similarities should be given.
We apply several linear and nonlinear weighting functions to the above time series distance functions (see Section 5.2.4).
Let f (i, j) be such a function, whose parameters are two time points i, j.
Thus, we modify DTW de nition of (cid:107)ts1(i)   ts2(j)(cid:107) to (cid:107)ts1(i)   ts2(j)(cid:107)   f (i, j) (6) and the covariance de nition in the cross-correlation distance now changes to cov(ts1, ts2)   cov(ts1, ts2)+ f (i, j)   [(ts1[index]   E(ts1))   (ts2[delayedIndex]    E(ts2))] (7) We described how our TSA method represents words as concepts (Section 3.1), and how the temporal dynamics of the concept usage over time can be used to compute semantic relatedness (Section 3.2).
We now turn to the experimental evaluation of our approach.
We implemented our TSA approach using the New York Times archive (1863-2004).
For each day we had an average of 50 abstracts of articles, which after parsing yielded 1.42 GB of texts with a total of 565,540 distinct words.
In this section we describe the methodology we used in our experiments and then describe a novel algorithm for automatically creating benchmarks for word relatedness tasks.
Both ESA and TSA were implemented on the concepts extracted from the folksonomies presented in Section 2.1, and therefore use the same vector representations.
This will allow us to isolate the performance of the temporal dimension in the TSA semantics.
Methods compared: We compare our algorithm and representations to the state of the art semantic representation   Explicit Semantic Analysis (ESA), which has been shown to be signi cantly superior to other approaches [16].
This approach projects words into a high-dimensional space of concepts derived from Wikipedia.
Using machine learning techniques, it represent the meaning of a word as a weighted vector of Wikipedia-based concepts.
Each concept in the vector is weighted by relevance to the word.
Assessing the relatedness of words in this space is done by utilizing cosine distance   a conventional metric of comparison of high-dimensional vectors.
Evaluation metrics: As in prior published studies, in our evaluation we use Spearman correlation coe cient to compare the predicted relatedness scores with human judge-ments.
The comparison is applied on both our algorithm and representations and the current state of the art.
Statistical Signi cance: We compare the rank correlation coe cient of our method, rank1, to the competitive methods (1)similarity(ts1, ts2) = 0 (2)cov(ts1, ts2) = 0 (3)For delay = { delaymin .
.
.
delaymax} (4) For index = {0 .
.
.
M in(|ts1|,|ts2|)} (5) (6) (7) (8) (9)Return similarity(ts1, ts2) delayedIndex = index + delay cov(ts1, ts2)   cov(ts1, ts2) + (ts1[index]   E(ts1))   (ts2[delayedIndex]   E(ts2)) corr@delay(ts1, ts2)   cov(ts1,ts2) N ts1  ts2 similarity(ts1, ts2)   M ax(similarity(ts1, ts2), corr@delay(ts1, ts2)) Figure 5: Time series cross correlation Procedure DTW(ts1, ts2, C) (1)n   M in(|ts1|,|ts2|) (2)dtw(ts1, ts2)   new [|ts1|   |ts2|] (3)For i = {1 .
.
.
n} (4) (5) (6)For i = {1 .
.
.
n} (7) For j = {1 .
.
.
n} (8) (9)Return dtw(n, n) dtw(i, 1)   dtw(i   1, 1) + c(i, 1) dtw(1, i)   dtw(1, i   1) + c(1, i) dtw(i, j) = (cid:107)ts1(i)   ts2(j)(cid:107) + M in(dtw(i   1, j), dtw(i, j   1), dtw(i   1, j   1)) Figure 6: Dynamic time warping algorithm rank coe cient, rank2, and calculate statistical signi cance, using the following standard formula: p = 0.5   ErrorF unction( (8) |z1   z2|  
 2  (cid:113) 2 (cid:82) x 0 e t2 ), and ErrorF unction(x) = 2  where N is the number of word pairs the dataset, zi = 0.5   ln( 1+ranki dt is the 1 ranki standard Gauss error function.
  Evaluating word relatedness is a natural ability humans have and is, therefore, considered a common baseline.
To assess word relatedness, we use the WS-353 benchmark dataset, available online [14], which contains 353 word pairs.
Each pair was judged, on average, by 13-16 human annotators.
This dataset, to the best of our knowledge, is the largest publicly available collection of this kind, which most prior works [16, 37, 36, 35] use in their evaluation.
As an e ort to provide additional evaluation data in this problem domain, we created a new dataset1 to further evaluate our results upon.
We present a principled method to create additional datasets, as opposed to the WS-353 benchmark where the word pairs were extracted manually.
We propose to draw the word pairs from words which frequently occur together in large text domains.
The relatedness of these pairs of words is then evaluated using human annotators, as done in the WS-353 dataset.
Selecting word pairs to evaluate: To create a balanced dataset of both related words and unrelated words, we applied the following procedure: Let W be a set of all words in the New York Times news articles.
As we wish to compare between entities, we intersect this collection with entities extracted from DBpedia.
We further proceed with removing stop words and rare words (words appearing less than 1000
 http://www.technion.ac.il/ kirar/Datasets.html over the entire time period), and stemmed the remaining words.
We annotate this collection as W (cid:48).
For each word pair (ai, aj)   W  W  , their point-wise mutual information (PMI) is computed over the entire set of the articles, i.e., a group G of all possible word pairs, ordered by their PMI values G = {(a1, b1), .
.
.
, (an, bn) |P M I(ai, bj)   P M I(ai+1, bj+1), (ai, bj)   W     W  } where P M I is de ned as: P M I(ai, aj) = log p(ai, aj) p(ai)p(aj) (9) (10) |W (cid:48) W (cid:48)| Eventually, given a pre-de ned number n of desired test -th pair from the G(cid:48) ordering is cho-pairs, every sen.
Formally, we construct the  nal set |gj   G, j   |W D = {g (11) (cid:48)   W (cid:48)|} n
 n Intuitively, this process performs a strati ed sampling, containing both frequently and infrequently co-occurring words, with decent coverage of the entire spectrum of co-occurrence values (as measured by mutual information).
Obtaining human ratings from Amazon MTurk workers: The human  ground truth  judgements were obtained by using the Amazon s Mechanical Turk workers, in batches of 50 word pairs per assignment, resulting in 280 word pairs labeled overall.
Up to 30 workers per batch were assigned, with the average of 23 MTurk workers rating each word pair, on average.
Ten (distinct) pairs from WS-353 dataset were injected into each batch, in order to provide a calibration baseline to discard poor-quality work.
Additionally, a simple  captcha  requiring to solve a simple math problem was given to each worker.
As a result, the work of the annotators with ratings that correlated less than 50% on the WS-353 subset of the batch, or those that failed the  captcha  was discarded through this procedure).
We  rst report the main experimental results comparing TSA to ESA on the WS-353 and MTurk datasets described above.
Then, we analyze the performance of TSA in more detail on the WS-353 dataset to gain more insights into the e ects of the di erent system parameters.
In this section we compare the results of TSA to known similarity measurements.
The section  rst provides empirical evidence that temporal signals contribute to measuring semantic relatedness of words, and then we show that our representation as a vector of concepts combined with temporal data outperforms previous temporal similarity techniques.
The comparison results of TSA on the WS-353 dataset are reported in Table 1.
TSA results shown in the table are computed using cross correlation with a quadratic weighted function as the distance metric between single time series.
Table 1: TSA algorithm vs. ESA (WS-353 dataset) Algorithm Correlation with humans ESA-Wikipedia [16]
 TSA (Section 3)


 As reported in Table 1, TSA performs signi cantly better compared to the ESA-Wikipedia approach, with p < 0.05.
We also evaluate the performance of ESA-Wikipedia and TSA, on the additional dataset we created (we refer to it as the MTurk dataset).
The results are presented in Table
  rming that temporal information is useful on other datasets.
Table 2: TSA algorithm vs. state-of-the-art (MTurk Dataset) Algorithm Correlation with humans ESA-Wikipedia [16] TSA (Section 3)


 Some works [8] proposed measuring semantic similarity of queries through temporal correlation analysis alone   without expending to a vector of semantic concepts.
We therefore compare to additional two baselines: Word-Similarity using cross correlation and Word-Similarity using DTW as the distance measurement of the time-series of the two words.
The results using the WS-353 and Mturk dataset can be seen in Table 3.
In both datasets TSA signi cantly outperformed the baselines.
This suggests that temporal vector similarity combined with static similarity is essential.
Table 3: TSA algorithm vs. temporal word similarity (WS-353 dataset) Algorithm Dataset WS-353 MTurk Word-Similarity (cross correlation) Word-Similarity (DTW) TSA (Section 3)






 This section analyzes the performance of TSA for varying settings to gain more insights into the advantages and limitations of the TSA method.
To further analyze the performance of our algorithm we conducted experiments to test on which type of word pairs our algorithm outperforms the state of the art to this end.
We chose to focus on word frequency.
We investigated whether our algorithm performs better on frequent or rare words.
We measured frequency in both domains   Wikipedia and New York Times.
In order to evaluate the joint frequency of a pair of words, we combine their frequency by three types of measurements: minimum frequency of the two words, average, and maximum frequency of the two words.
We divide the word pairs into three buckets, each containing an equal number of data points.
We compute Spearman correlation separately in each bucket.
The results for the minimum criteria for the New York Times and Wikipedia corpora are reported in Tables 4 and
 and maximum frequency measurements.
The results show that TSA performs signi cantly better than ESA on low-frequency words.
This can be attributed to the fact that ESA is based on statistical information about words and concepts, which requires su cient number of occurrences.
Low-frequency words do not have enough statistical data, hence any additional signal, such as the temporal behavior of the words, can improve the performance.
Table 4: Grouping word pairs by NYT word frequency (WS-353 dataset) Type of Bucket ESA Correlation TSA Correlation with humans with humans Low Medium High





 Table 5: Grouping word pairs by Wikipedia word frequency (WS-353 dataset) Type of Bucket ESA Correlation TSA Correlation with humans with humans Low Medium High





 The results on the Mturk Dataset comparing ESA-Wikipedia, and TSA are reported in Table 6.
While the absolute values of the TSA and ESA correlations with humans are lower, the trend persists: TSA signi cantly outperforms ESA, particularly on words with low frequency.
The lower absolute ratings, despite performing best-of-practice  ltering of poor-quality MTurk work [31], and as explained in Section 4.2.
Table 6: Grouping word pairs by Wikipedia word frequency (Mturk dataset) Type of Bucket ESA Correlation TSA Correlation with humans with humans Low Medium High





 Size of Temporal Concept Vector
 In this subsection we experiment with several sizes of the temporal concept vector in several di erent natural representations.
In many of the folksonomy domains presented in Section 2.1, we are able to obtain only vectors of about 10 concepts (based on API limitation in Flickr and Del.icio.us), i.e., for each word we are not able to produce all the words and their co-occurrence weight, but only the word s related tags.
Due to this limitation, a traditional cosine measurement cannot be computed between those partial vectors   as each vector contains di erent concepts.
We de ne a size of a concept vector to be the number of concepts.
The main advantage of the distance measurement we de ned in Section 3.1 is the ability to measure distance between vectors with di erent concepts representation, and even vectors of di erent sizes.
We ran the experiments on various vector sizes.
We deduce from the results (as appear in Table 7) that the optimal vector size is 10.
Additional improvements for larger vector sizes might be achieved with additional feature selection.
Table 7: E ect of concept vector size on performance (WS-353) Vector Size


 Correlation with humans





 In this subsection we experiment with several distance functions, that are applied during the measurement of the semantics distance of the temporal concept vectors.
Cross correlation outperforms DTW in each setting, where TSA with cross-correlation performance is 0.80, and with DTW it drops to 0.74.
This indicates that, for the purpose of measuring similarity of concept s vectors, correlations in time series volume are more signi cant than measuring general similarity in time series structure (as in DTW).
Several weighting functions can be applied on the words  time series to produce higher weighting to more recent correlations (as we discussed in Section 3.2.3).
In this work, we de ne several variations for a weighing function f (t1, t2).
This function receives two time points of two time series, and is used to weigh the distance between the time-series at these points.
The functions we experiment upon are:
 which weighs all time points equally.
which is a power model of weight, in which volume differences in more recent time points are weighted higher based on the power of the function.
We have experimented on n = 1, 2.
which is an exponential model of weight, in which volume di erences in recent time points are weighted exponentially higher.
The results of the performance for the TSA algorithm (with cross correlation distance function over WS-353) are presented in Table 8.
The results provide evidence for the need to weigh the recent changes in time series distance measurement higher than the ancient changes.
While linear, quadratic, and exponential temporal weighting functions perform similarly, the quadratic performs best, and we use it for all the experiments described in this paper.
A few Table 8: E ect of temporal weighting function Temporal Weighting Function Correlation with humans Constant Linear Quadratic Exponential



 examples to illustrate those changes in performance can be seen in Table 9.
It is clear from the rankings presented in the table, that quadratic weighting yields more signi cant correlation with human ranking than the constant weighting function.
The correlation of such words, such as  Mars  and  water  in 1900 should be weighted di erently from the correlation they exhibit in 2008, when NASA images suggested the presence of water on Mars.
Table 9: Temporal weighting in uence Word 1 Word 2 Humans TSA-Const TSA-Quadratic Mars peace water plan Rank

 Rank

 Rank



 In order to gain more intuition on which cases TSA approach should be applied, we provide real examples of the strengths and weaknesses of our methods compared to the state of the art ESA method.
The results are derived from the application of the TSA algorithm with cross correlation and a quadratic weighting function as the distance metric between single time series.
Synonyms are the  rst type of words for which the TSA method seems to outperform the ESA method.
The reason for that is that synonyms have similar patterns of occurrence over time, as writers in the news corpus tend to use them interchangeably.
Therefore, the two synonyms time-series in the same corpus strongly correlate over time.
On the other hand, ESA represents each word as a vector of concept s Wikipedia article the number of distinct authors is limited, and therefore, the language model, and, as a consequence, the use of di erent synonyms is quite limited.
For this reason, the TFIDF values of the synonyms in the ESA representation tend to be quite di erent.
A sample of those cases can be seen in Table 10, where we present for each pair of synonyms the ranking given by human judgements, the ESA rank and the TSA rank.
The rankings are based on the rank of the similarity of the pair of words out of the 353 pairs in the WS-353 dataset.
Table 12: Implicit Relations Word 1 Word 2 closet summer disaster cup cup clothes drought area tableware liquid canyon landscape tiger jaguar Humans ESA TSA Rank Rank













 Rank






 Table 10: Synonyms Word 1 Word 2 asylum madhouse coast boy shore lad problem challenge Human ESA TSA Rank Rank Rank











 are not always straight forward to humans.
For example, the correlation between  drink  and  car .
In the news, many times alcohol drinking correlates with car accidents, however humans tend not to  nd them related at all.
Another representative example is  psychology  and  health .
These words are considered very related by humans, however no true correlation in the news was found between the two words.
More information about these examples can be seen in Table 13.
As our method also captures co-occurrences of words in a single article (as we construct time-series aggregated over all articles on a certain date), phrases can also be identi ed well.
ESA represents each word as a vector of Wikipedia concepts, weighted by the TFIDF of the word in the concept s article.
Therefore, when measuring similarity of  hundred  and  percent  the similarity score is quite low - as the words appear in di erent articles and acquire completely di erent meanings in di erent contexts.
Therefore, word phrases like  hundred-percent  are not identi ed well by ESA.
More of those examples are presented in Table 11.
Table 11: Word Phrases Word 1 Word 2 Humans ESA TSA Rank Rank





 Rank


 percent series luxury hundred game car Implicit relations are one of the di erentiating strengths of the TSA representation and the new distance metric we presented.
For example, causality relations, such as  summer causes draught , are easily detected using correlation of the words  time-series.
Relations of  type-of  (such as canyon is a type of landscape) are also relations we have found to be common when TSA outperforms ESA.
We attribute that to the fact that many words in Wikipedia are associated to the general concepts (in our example  landscape ) and therefore, when measuring the distance between the concepts  TFIDF vectors, the relation of each sub-object (such as  canyon ) declines.
Table 12 presents additional examples of pairs belonging to these relations and the ranking of human judgments, ESA and TSA algorithms for the WS-353 dataset.
Although we have seen many results in which TSA performs better than ESA, we also present in this work some examples in which TSA performs worse.
One of the strength of the algorithm sometimes also serves as its weakness.
Although this phenomenon is not too common, TSA identi es very complex implicit relations, which Table 13: Complex Implicit Relations Word 1 Word 2 Humans ESA TSA Rank Rank





 health Rank drink psychology car Some problems of our representation arise from the corpus we have selected to represent the concept s temporal behavior.
The corpus is, unfortunately, sparse in certain topics   mostly speci c topics such as technology and science (see Table 14).
Therefore, correlations between words such as  physics  and  proton  are not identi ed well.
A possible solution for this problem would be to add other sections of the New-York-Times news (such as sports, technology and science), and weigh the words frequency by the appearance in each one of those sections (so small sections will not be  discriminated ).
Considering adding additional temporal corpus like blogs, tweets and so on, might also be useful.
Unfortunately, we did not have access to this kind of data at the time.
Table 14: News Corpus Bias Word 1 Word 2 physics network boxing proton hardware round Humans ESA TSA Rank Rank


 Rank







 Automatically estimating word similarity (WS) and semantic relatedness (SR) have been fundamental problems for decades, and have been addressed by diverse techniques in cognitive science, computational linguistics, arti cial intelligence, and information retrieval.
For example, in computational linguistics, applications of WS include word sense disambiguation, information retrieval, word and text clustering [7].
This section  rst brie y reviews previous established approaches to the WS and SR problems; we then oratively generated content (CGC) such as Wikipedia, and  nally frame our approach in the context of previous work on using temporal information for WS and other problems.
Until recently, computing semantic relatedness of natural language texts (ranging from a single word to a document in length) required encoding vast amounts of commonsense and domain-speci c world knowledge.
Prior work pursued three main directions: comparing text fragments as bags of words in vector space [2], using handcrafted lexical resources such as WordNet [13], and using Latent Semantic Analysis (LSA) [10].
The former technique is the simplest, but performs sub-optimally when the compared texts share few words, for instance, when the texts use synonyms to convey similar messages.
Unfortunately, this family of techniques are not appropriate for comparing individual words.
Lexical databases such as WordNet [13] or Roget s Thesaurus [29] encode relations between words such as synonymy, hypernymy.
Multiple metrics have been proposed for computing relatedness using properties of the underlying graph structure of these resources [7, 20, 3, 28, 24, 21,
 resources is that it requires signi cant expertise and e ort, and consequently such resources cover only a small fragment of the language lexicon.
Speci cally, such resources contain few proper names, neologisms, slang, and domain-speci c technical words.
Furthermore, these resources have strong lexical orientation and mainly contain information about individual words but little world knowledge in general.
In contrast, LSA [10], a purely statistical technique, leverages word cooccurrence information from a large unlabeled corpus of text.
LSA does not rely on any human-organized knowledge; rather, it  learns  its representation by applying Singular Value Decomposition (SVD) to the words-by-documents cooccurrence matrix.
LSA is essentially a di-mensionality reduction technique that identi es a number of most prominent dimensions in the data, which are assumed to correspond to  latent concepts .
Meanings of words and documents are then compared in the space de ned by these concepts.
Latent semantic models are notoriously di cult to interpret, since the computed concepts cannot be readily mapped into natural concepts manipulated by humans.
Another statistical approach is estimating semantic relatedness of words through  distributional similarity  [23, 9] - that is, the similarity of the contexts in which the words occur.
In this paper we deal with  semantic relatedness  rather than  semantic similarity  or  semantic distance , which are also often used in the literature.
In their extensive survey of relatedness measures, Budanitsky et al [7] argued that the notion of relatedness is more general than that of similarity, as the former subsumes many di erent kind of speci c relations, including meronymy, antonymy, functional association, and others.
They further maintained that computational linguistics applications often require measures of relatedness rather than the more narrowly de ned measures of similarity.
For example, word sense disambiguation can use any related and not just similar words from the context.
Some works [30, 26] proposed to use the Web as a source of additional knowledge for measuring similarity of short text snippets.
A major limitation of this technique is that it is only applicable to short texts, because sending a long text as a query to a search engine is likely to return few or even no results at all.
More closely related to our work, Gabrilovich et al. [16] presented an approach to WS that relied on exploiting Wikipedia for  Explicit Semantic Analysis  or ESA, and have demonstrated high correlation with human annotators.
Strube et al. [32] also used Wikipedia for computing semantic relatedness.
As many datasets have important temporal dimensions (e.g., stock quotes, sensor readings, search engine query popularity), there exist numerous techniques to analyze and mine time series data.
In particular, Vlachos et al. [33] and subsequent work identi ed similar objects based on their trajectories through time series analysis.
Among many known approaches to time series similarity we consider Dynamic Time Warping (DTW) [6], which we use as one of the methods for identifying words with similar trajectories.
Gruhl et al. [18] and others [22] analyzed temporal information di usion in blogosphere, including the temporal patterns in word popularity.
Efron [11] considered term popularity in a document collection, to assign better term weights for document ranking.
More similar to our work, but in the context of analyzing temporal search engine query logs (which often exhibit strong temporal regularities [4]), some work [8, 25, 39] proposed a method for detecting semantically similar queries through temporal correlation analysis.
More generally, time series analysis has been used previously to detect similar topic patterns [34], among many other applications.
However, to the best of our knowledge, temporal information has not yet been used to improve general word relatedness estimation.
In the related context of searching evolving document collections, several prior studies focused on versioned document retrieval models, where the objective is to e ciently access previous versions of the same document [38, 19].
Elsas and Dumais [12] studied the dynamics of document content change with applications to document ranking.
Research on topic detection and tracking (TDT) analyzed the evolution of stories and topics over time [1].
Gabrilovich et al. [15] studied the dynamics of information novelty in evolving news stories.
Olston and Pandey [27] introduced the notion of information longevity to devise more sophisticated crawling policies.
While our work also makes use of temporally evolving statistics of a document collection, our goal is di erent in that we seek to identify related words based on temporal patterns, rather then improve performance on a speci c application such as ranking or web crawling.
Furthermore, our work presents a novel way of representing terms in a vector space of concept time series, which can then be compared with time-series similarity measurements as building blocks.
Finally, we provide a ways of combining the static and temporal information for computing relatedness - resulting in a signi cantly more accurate estimation of relatedness than using either signal in isolation.
We proposed a novel approach to computing semantic relatedness with the aid of a large scale temporal corpus.
We use the New York Times archive that spans over a large period of time, and which, to the best of our knowledge, have duced two innovations over the previous words  semantic relatedness methods:  rst, a new method, Temporal Semantic Analysis, for representing the semantics of natural language terms, and a new method for measuring semantic relatedness of terms, using this representation.
The algorithm is robust in that it can be naturally tuned to assign di erent weights to time periods, and can be used for studying language evolution over time.
Our empirical evaluation con rms that using TSA leads to signi cant improvements in computing words relatedness over two large datasets.
Compared with the previous state of the art, TSA yields statistically signi cant improvements in correlation of computed relatedness scores with human judgements.
We also provide an algorithm for the automatic construction of new datasets of measuring semantic relatedness of words, and provide additional dataset to the community for further research in the  eld.
We believe that more accurate identi cation of word relatedness provided by TSA, will enable more intelligent search, improve text classi cation accuracy, and enable other tasks that normally require understanding of subtle relationships between words.
