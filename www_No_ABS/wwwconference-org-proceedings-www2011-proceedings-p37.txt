With the exponential growth of information on the Web, search engine has become an indispensable tool for Web users to seek their desired information.
However, it is never Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
easy for users to formulate a proper query to search because query is usually very short [7] and words are ambiguous [23].
Furthermore, users sometimes cannot express their search intent precisely due to the lack of domain-speci c knowledge.
Therefore, how to help users formulate a suitable query has been recognized as a challenging problem.
To overcome this problem, a valuable technique, query recommendation, has been employed by most commercial search engines, such as Google 1, Yahoo!2, and Bing3 to improve usability.
Query recommendation aims to suggest queries that may better re ect users  information needs to help them  nd what they need more quickly.
Traditional query recommendation approaches mainly focused on recommending alternative queries with close search intent to the original query.
Query logs are widely used in these approaches [2, 15, 23], where similar queries are identi ed based on users  historical behavior and used as recommendations for each other.
However, to only take rel-evance/similarity into account may generate redundant recommendations.
For example, when a user issues a query  abc , the system may recommend him/her  abc television  and  abc tv , which are both very relevant to  abc  but of the equivalent meaning.
Recommending such queries at the same time will decrease the recommendation quality since they provide almost the same information to users.
Therefore, it is important to provide diverse as well as relevant query recommendations.
By reducing the redundancy, we are able to cover multiple potential search intents of users and minimize the risk that users will not be satis ed.
In addition, previous query recommendation approaches mostly relied on measuring the similarity between queries in the Euclidean space, either based on query terms or click-through data.
However, there is no convincing evidence that the query space is Euclidean.
Inspired by the research work on document modeling [26, 27], it is more natural and reasonable to assume that the query space is a manifold, either linear or nonlinear.
The local geometric structure is essential to reveal the relationship between queries.
In our study, we  nd that ranking queries in terms of the intrinsic global manifold structure [26, 27] is superior to the pairwise distance in the Euclidean space.
In this paper, therefore, we propose to recommend diverse and relevant queries based on the intrinsic query manifold.
We propose a novel uni ed model, named manifold ranking with stop points, for query recommendation.
Specif-1http://www.google.com/ 2http://www.yahoo.com/ 3http://www.bing.com/ over query manifold, which can naturally make full use of the relationships among queries to  nd relevant and salient queries.
Meanwhile, we introduce the stop points into query manifold to capture the diversity during the ranking process.
The stop points are points that stop spreading their ranking scores to their nearby neighbors during the manifold ranking process.
By turning ranked queries into stop points, the ranking scores of other queries close to these stop points (i.e., queries which share similar search intent with the ranked queries) will be naturally penalized during the ranking process based on the intrinsic query manifold.
Therefore, our approach can generate query recommendations by simultaneously considering both diversity and relevance between queries in a uni ed way.
Like traditional manifold ranking algorithm, the new proposed ranking approach also shows a nice convergence property.
We conducted extensive experiments to evaluate the proposed approach based on a large collection of query logs from a commercial search engine.
Empirical experimental results show that our approach can e ectively generate highly diverse as well as closely related query recommendations.
The main contributions of our approach can be summarized as follows: (1) we  rst exploit the intrinsic global query manifold structure to measure the similarity between queries; (2) we propose a novel ranking approach, i.e., manifold ranking with stop points, for query recommendation which addresses relevance and diversity simultaneously in a uni ed way; (3) the proposed ranking approach has a nice convergence property; (4) we show that our query recommendation approach is superior to other baseline methods in producing diverse and relevant query recommendations.
The rest of the paper is organized as follows.
Section 2 introduces related work.
Section 3 describes our proposed approach in detail.
Experimental results are discussed in Section 4 and conclusion is made in Section 5.
Query recommendation.
Query recommendation has been employed as a core utility by many industrial search engines, which focuses on improving queries raised by users.
The intention of query recommendation is closely related to query expansion [24, 7, 21], query substitution [12] and query re nement[14, 9].
The main di erence is that query recommendation aims to recommend full queries submitted by previous users.
Most of the work on query recommendation is focused on measures of query similarity, where query log data has been widely used in these approaches.
Click-through information conveyed in query log is often leveraged to measure the similarity of queries.
The basic assumption is that queries sharing more clicked URLs are considered more similar [4].
A query-URL bipartite graph can be constructed from the click-through data with the vertices on one side corresponding to queries and on the other side to URLs.
Beeferman et al. [2] applied agglomera-tive clustering algorithm to the click-through bipartite graph to identify related queries for recommendation.
Wen et al.
[23] proposed to combine both user click-through data and query content information to determine the query similarity.
Li et al. [15] recommended related queries by computing the similarity between queries based on query-URL vector model and leveraging a hierarchical agglomerative clustering method to rank similar queries.
Ma et al. [16] developed a two-level query recommendation method based on two bipartite graphs (user-query and query-URL bipartite graphs) extracted from the click-through data.
However, most previous work only focused on recommendation relevance, while not explicitly addressed the problem of diversity.
Mei et al.
[18] tackled this problem using a hitting time approach based on the query-URL bipartite graph.
Their approach can recommend more diverse queries by boosting long tail queries.
However, the weakness of their approach is that it would sacri ce the relevance considerably when improving the diversity, and many long tail queries recommended to users may not be familiar to them.
Di erent from existing approaches to query recommendation, our approach exploits the intrinsic global manifold structure to measure the similarity between queries and employs a novel ranking approach to address relevance and diversity simultaneously.
Diversity rank.
Beyond relevance, diversity has also been recognized as a crucial criteria in ranking[5, 28, 17, 13,
 dundant information as possible, and cover as many aspects as possible.
Among the existing research work, Maximal Marginal Relevance (MMR) [5] is the most well-known method used for result set diversi cation.
It has been widely used in the text summarization community.
MMR utilizes a greedy strategy that iteratively selects the best scoring object, and then updates remaining object scores by computing a penalty based on the similarity of each object with the selected object.
The closest work to ours is Grasshopper proposed by Zhu et al. [28], which applies an absorbing random walk on the graph.
In order to achieve diversity, it turns the selected object into an absorbing state and then selects the next object based on the expected number of visits to each node before absorption.
Although our work share similar idea with this approach in handling ranked objects, they are largely different in ranking strategy.
Grasshopper uses two di erent measures, i.e., stationary distribution and expected number of visits, to select the top ranked object and the remaining objects.
In contrast, all the objects are ranked with a consistent strategy (i.e., using their ranking scores) in our approach.
A recent improvement on diversity ranking using random walk based approach is DivRank [17].
DivRank employs a time-variant random walk process, which uses the rich-gets-richer mechanism in ranking.
However, the main drawback is that the computation of the expected number of visits is intractable and has to resort to some approximation strategies.
Di erent from these above approaches, we introduce the notion of stop points in manifold ranking and propose a uni ed model which simultaneously consider both relevance and diversity for query recommendation.
Manifold ranking.
The manifold ranking algorithm  rst constructs a weighted network on the data, and assigns a positive ranking score to the input query and zero to the remaining points which are to be ranked with respect to the input query.
Then all points spread their ranking scores to their nearby neighbors via the network until a global stable state is reached.
Points without the input one are ranked according to their  nal ranking scores.
Manifold ranking was used to rank data with respect to the intrinsic global manifold structure collectively revealed by a huge amount of data [26, 27].
It has been applied ing is needed in essentials.
For example, He et al.
[10] leveraged manifold ranking to measure relevance between the query and database images for image retrieval.
Wan et al.
[22] applied the manifold ranking process to utilize the relationships between the topic and the sentences for text summarization.
However, so far there is no related work on applying manifold ranking for query recommendation.
To the best of our knowledge, this paper is the  rst article attempt to utilize manifold ranking for query recommendation.
Given a set of data points (i.e. queries) X = {q0, q1, .
.
.
, qn}
 m , the  rst point q0 is the input query and the rest of the points qi (1   i   n) are the candidate queries.
Hereafter, query and point will not be discriminated unless otherwise speci ed.
Let d : X   X   R denote a metric on X (e.g.
Euclidean distance), where d(qi, qj ) is the distance between qi and qj .
Let f : X   R denote a ranking function which assigns to each point qi (0   i   n) a ranking value fi.
We can view f as a vector f = [f0, .
.
.
, fn]T .
We also de ne a vector y = [y0, .
.
.
, yn]T , in which y0 = 1 for the input query q0 and yi = 0 (1   i   n) for all the candidate queries.
In our work, queries are assumed to be sampled from a low-dimensional manifold which is embedded in the high-dimensional ambient space.
Our recommendation approach is then based on such a query manifold structure.
Here we build a k-nearest-neighbor query graph using the click-through information in query logs to model the local query manifold structure.
The click-through data can help us  nd similar queries.
The basic idea is that if two queries share many clicked URLs, they have similar search intent to each other [15].
Therefore, we model queries in terms of query-URL vectors, instead of query-term vectors.
We represent each query qi as a L2-normalized vector, where each dimension corresponds to one unique URL in the click-through data.
Speci cally, given a query qi (0   i   n), the j-th element of the feature vector of qi is (cid:2) eij (cid:3) m k=1 e2 ik j q i = if qi clicked uj; (1)
 otherwise, where m denotes the total number of unique URLs in the click-through data and eij denotes the weight for the pair of query qi and its clicked URL uj .
Here we follow the CF-IQF weighting scheme [8] and de ne the weight eij = cfij   log(n/qfj ), where cfij denotes the total click frequency on uj given qi, qfj denotes the total number of unique queries which have clicked uj , and n denotes the total number of unique queries in the query log.
The distance between two queries qi and qj is then measured by the Euclidean distance between their normalized feature vectors i   qk (cid:4)(cid:3)m d(qi, qj) = j )2.
k=1 (qk (2) any two points with an edge if they are among the k nearest neighbors to each other (k = 50 in our case).
In this way, we are able to preserve the sparse property of the query manifold.
We de ne an a nity matrix W for the query manifold, where  d(qi,qj )2 2 2 , wij = e (3) if there is an edge linking qi and qj , and wii = 0 as there are no loops in the graph.
Here   is empirically set to 1.25.
A traditional manifold ranking process [26] over the query manifold can be described as follows:

  1/2 in which D is the diagonal matrix with (i, i)-element equal to the sum of the i-th row of W .
where   is a parameter in [0, 1).
i denote the limit of the sequence of {f   each point qi according its ranking scores f ranked  rst).
i }.
Rank (t)   i (largest In the above ranking process, all the points spread their ranking scores to their neighbors via the weighted graph.
The spread process is repeated until a global stable state is achieved, and all the points are ranked according to their  nal ranking scores.
With the traditional manifold ranking process, we can obtain relevant and salient queries for recommendation given the input query.
To explicitly address the diversity of query recommendation, we introduce stop points into query manifold and propose a novel ranking approach, named manifold ranking with stop points.
The stop points are a special type of points on query manifold, which stop spreading their ranking scores to their neighbors during the manifold ranking process.
Intuitively, we can imagine the stop points as the  black holes  on the manifold, where no ranking scores would be able to  escape  from them.
By turning ranked queries into stop points, the ranking scores of other queries close to the stop points (i.e., queries which share similar search intent with the ranked queries) will be naturally penalized during the ranking process based on the intrinsic query manifold.
Here we derive the new iteration algorithm for manifold ranking with stop points.
Let T denote the set of stop points, and R denote the set of free points (all the data points excluding the stop points).
The normalized matrix S in traditional manifold ranking can then be reorganized as a block (cid:6) matrix , and the original iteration equation (cid:5) (cid:5) in step 2 can be written as

 (cid:6)(t+1) fR fT (cid:5) (cid:6)(cid:5) (cid:6)(t) fR fT (4) =   + (1    )

 (cid:5) (cid:6) yR yT , With the de nitions above, we construct the k-nearest-neighbor query graph as follows.
Firstly, each query is represented as a data point on the manifold.
We then connect where fR and fT denote the ranking scores of points in set R and T respectively, and yR and yT denote the prior on the points in set R and T respectively.
points, we set SRT = ST T = 0, then we get the new iteration equation for manifold ranking with stop points: (cid:6)(t+1) (cid:5) (cid:5) fR fT (cid:6)(t) (cid:6)(cid:5) (cid:6) fR fT

 (cid:5) =   + (1    ) yR yT .
(5) As we turn the queries already selected for recommendation into stop points, the ranking scores of stop points are no longer useful for us since the stop points would not be selected later again.
All we care about is the ranking scores of the free points in set R. Therefore, we only need to compute fR with the iteration equation (t+1) f
 =  SRRf (t) R + (1    )yR, (6) where the parameter   speci es the relative contributions to the ranking scores from neighbors and the initial ranking scores.
It is important to know that, with the stop points introduced, the new iteration algorithm still has a nice convergence property, which means it can achieve a global stable state.
This convergence property is shown in Theorem 1.
Theorem 1.
The sequence {f (t) R } converges to Proof.
Without loss of generality, suppose f (0) R = yR. By iteration equation (6), we have  1 yR.
R = (1    )(I    SRR)   f (cid:3)t 1 tyR + (1    ) f (t) R = ( SRR) i=0 ( SRR)iyR.
Let P = D 1 as follows: RRWRR, P is the similarity transformation of SRR
  1/2

  1/2


  1/2


  1/2
 hence P and SRR have the same eigenvalues.
Let   be an eigenvalue of P , according to the Gershgorin circle theorem, we have |    Pii|  (cid:3)|R| j=1,j(cid:3)=i |Pij|, (cid:3)|R| where |R| is the size of the free point set.
Note that Pii = 0 and j=1,j(cid:3)=i |Pij|   1, so we have | |   1.
Since 0     < 1 and i=0( SRR)i = | |   1, then limt ( SRR)t = 0, limt  (I    SRR)  1.
Hence, we have (cid:3)t 1 f  R = lim t  f (t) R = (1    )(I    SRR)  1yR.
We can use this closed form to compute the ranking scores   f R for all the free points directly.
In large scale real-world problems, however, an iterative algorithm is preferable due to computational e ciency.
Based on the ranking algorithm above, we  nally obtain our query recommendation approach.
We  rst construct a k-nearest-neighbor query graph to model the query manifold structure based on query logs and set all the query points as free points.
Giving an input query, we apply the proposed ranking algorithm, i.e., manifold ranking with stop points, over the query manifold until a global stable state is achieved, and rank the queries according to their ranking scores.
The free point with the largest ranking score (except the input query) will be selected as a recommendation, and set as a stop point in the following iteration.
The process iterates until a pre-speci ed number of recommendations acquired.
The recommendation algorithm using manifold ranking with stop points is shown in Algorithm 1.
Note that it would be time consuming if we directly apply the ranking algorithm on the whole query manifold.
Since most queries are irrelevant to the input query, we can use a width  rst search strategy to construct a sub-manifold to save the computational cost.
Algorithm 1 Query Recommendation using Manifold Ranking with Stop Points Input: q - the input query   - all the other queries K - recommendation size S - normalized a nity matrix of the query manifold T - stop point set R - free point set Output: Top K recommendation query set U Initialization: U =  , T =  , R =  


 obtain SRR based on S, T and R.
=  SRRf (t) iterate f (t+1) started with f (0) R = 0, where   is a parameter in [0,1).
select the query qk with the largest ranking score (except the input query) as a recommendation, U = U   {qk}.
turn query qk from free point into stop point, T = T  {qk} and R = R   {qk}.
R + (1    )yR until convergence


 6: end for


 Our experiments are based on the Microsoft 2006 RFP dataset4 which contains about 15 million queries (from US users) that were sampled over one month in May, 2006.
For each query, the following details are available: a query ID, the query itself, the user session ID, a time-stamp, the clicked URL, the rank of that URL and the number of results.
We cleaned the raw data by ignoring non-English queries, converting letters into lower case, and replacing all non-alphanumeric characters in each query with whitespace.
To further reduce the noise in clicks, the click-through between a query and a URL with a frequency less than 3 was removed.
After cleaning, we obtained the click-through data with totally 191,585 queries, 251,427 URLs and 318,947 edges.
On average, each query clicks 1.66 distinct URLs, and each URL is clicked by 1.27 distinct queries.
4http://research.microsoft.com/users/nickcr/wscd09/ tween 700 and 15,000 for evaluation.
This restriction is to avoid the navigational queries (for which the query recommendations may not be so useful) or very speci c queries (for which there are no recommendations) [3].
To evaluate the performance of our approach, called Mani stop for short, for query recommendation, we adopt four baselines for comparison:   Naive: It represents each query as a URL vector shown in Equation (1) and directly measures the Euclidean distance between queries.
For a given query q, queries with smallest distance scores are ranked higher and selected as recommendations.
Di erent from other baseline, this model only considers relevance for recommendation without emphasizing diversity.
  Hitting time[18]: It recommends queries by using the hitting time from candidate queries qs to the test query q as a measure for ranking.
The hitting time from node i to node j in a random walk is the expected number of steps before node j is visited starting from node i, and it decreases when the number of paths from i to j increases and the lengths of the paths decrease.
The basic idea of Hitting time is to boost long tail queries for recommendation.
  MMR (Maximal Marginal Relevance) [5]: MMR measures the relevance and diversity independently and provides a linear combination, called  marginal relevance , as the metric.
Formally,
 def = Arg max qi R\S   (1    ) max qj S [ Sim1(qi, q) Sim2(qi, qj )], (7) where R is a set of candidate queries, S is the subset of queries in R which is already recommended, Sim1 and Sim2 are both similarity metrics between queries and   is a parameter for linear combination.
When  =1 it computes the standard relevance-ranked list, while when  =0 it computes a maximal diversity ranking.
For a given query q, MMR will iteratively recommends queries with the largest  marginal relevance .
  Grasshopper (Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking)[28]: The Grasshopper model leverages an absorbing random walk over the query graph.
The model starts with a teleporting random walk P : P =  (cid:7)P + (1    )1r
 , where (cid:7)P is the raw transition matrix, r is the user-(8) supplied initial distribution, and   is a parameter to control the tradeo .
When   = 1 it ignores the user-supplied prior ranking r, while when   = 0 it returns to the ranking speci ed by r. The query with the largest weight is selected as the  rst recommendation, which is then set as an absorbing state.
The model then reruns the random walk with absorbing states, and selects the next query based on the expected number of visits to each node before absorption.
To make a fair comparison, we need to tune the parameters for baseline approaches.
Since both Naive and Hitting time involve no parameter tuning, we only need to tune parameter   for two approaches (MMR and Grasshopper).
Based on one held-out data with respect to the metrics introduced in the latter part of this section, we tested the two approaches using 11   values (i.e. 0, 0.1, 0.2,   , 1) and selected the best   value for MMR (  = 0.6) and Grasshopper (  = 0.9).
In our experiments, we  xed the parameter   in our method (Mani stop) at 0.99, consistent with the experiments performed in [26, 27].
Here we  rst present the comparison of recommendations generated by our approach and baseline methods.
Table 1 shows two samples from our test queries including their top 10 recommendations generated by  ve methods.
From the results we clearly see that Naive approach tends to recommend closely related but somewhat redundant queries.
For example, for the test query  abc , we can  nd equivalent recommendations like  abc tv  and  abc television , or recommendations sharing very close meaning like  abc news ,  abc breaking news  and  abc world news .
We can also  nd redundant examples in the recommendations for the test query  yamaha , e.g.,  yamaha motor ,  yamaha motorcycle , and  yamaha motorcycles .
Since Naive method only considers relevance, it will inevitably produce many redundant recommendations.
Meanwhile, we can easily  nd that the three other baseline approaches (Hitting time, MMR, Grasshopper) recommended queries with better diversity.
However, there is still some redundancy in these approaches.
For example, for the test query  abc ,  abc tv  and  abc television  are both recommended by Hitting time and MMR, while for test query  yamaha ,  yamaha motor  and  yamaha motorcycles  are both recommended by Grasshopper.
Moreover, the recommendation provided by Hitting time may not so closely related to the original query, e.g.
recommendation of  espn sports  with respect to  abc , and recommendation of  bluebook motorcycle  with respect to  yamaha .
It may also bring up noisy long tail queries which hurt user experience, like recommendation  yahama  for query  yamaha .
Among all these approaches, we observe that our Mani stop approach obtains best performance, where more diverse as well as closely related queries can be found in its recommendation results.
Evaluating the quality of query recommendation is di cult, since there is usually no ground truth of recommendations.
Therefore, we  rst conduct automatic evaluation over query recommendation for more objective comparison between di erent approaches.
The Open Directory Project (ODP)5 and a commercial search engine (i.e., Google) are leveraged in this automatic evaluation.
Besides, there is no evaluation metric that seems to be universally accepted as the best for measuring the performance of algorithms that aim to obtain diverse rankings [20].
Therefore, we adopt the following three metrics (Relevance, Diversity and Q-5http://www.dmoz.org/ query Naive abc shows abc television abc tv Hitting time abc shows abc television associated builders and
 abc shows abc breaking news associated builders and Grasshopper abc tv abc news abc family Mani sink abc tv abc news abc nightline abc abc news abc breaking news contractors abc tv news stories contractors abc nightline abc tv abc shows abc breaking news abc family associated builders and abc family abc sports abc world news world news tonight abc soap operas yamaha america yamaha motor corp yamaha motor yamaha motor co yamaha motorcycle yamaha motors yamaha motorcycles yamaha quads yamaha snowmobiles yamaha scooters abc news abc world news tonight abc family channel espn sports abc nightline yahama yamaha america yamaha motor corp yamaha motor co yamaha motor yamaha motorcycle yamaha snowmobiles yamaha quads yamaha outboard motors bluebook motorcycles abc television abc family abc sports abc daytime goodmorning america yamaha america yamaha atv parts yamaha boat motors yamaha motor corp yamaha snowmobiles yamaha motor yamaha drums yamaha guitars yamaha motorcycles yamaha atvs nightline goodmorning america abc sports abc daytime national news yamaha motor yamaha america yamaha motor corp yamaha motorcycles motorcycles yamaha marine yamaha atv yamaha motorcycle parts yamaha snowmobiles yamaha quads yamaha contractors abc shows abc daytime goodmorning america abc sports abc soap operas yamaha motor yamaha motor corp yamaha america yamaha marine yamaha atv yamaha snowmobiles yamaha drums yamaha guitars yamaha quads yamaha boat motors measure) to help evaluate the relevance and diversity in recommendation.
Relevance.
We adopt the same method used in [1] to evaluate the relevance of recommended queries.
Speci cally, we measure the relevance of two queries based on the similarity between their corresponding categories provided by
 , let C and C Given two queries q and q denote the corresponding set of top k (k = 10 in our case) ODP categories from Google Directory6, respectively.
We de ne the similarity between two categories c   C and c as the length (cid:7) of their longest common pre x l(c, c ) divided by the length (cid:7) of the longest category of c and c .
More concisely, denoting the length of a category c with |c|, the similarity between (cid:7) two categories c and c (cid:7)   C is (cid:7) (cid:7) (cid:7) (cid:7) Sim(c, c ) = |l(c, c (cid:7) )| max{|c|,|c(cid:7)|} .
(9) For instance, the similarity between the two categories  Arts/Television/News  and  Arts/Television/Stations/North America /United States  is 2/5, since they share the common pre x  Arts/Television  and the length of the longest category is  ve.
We then evaluate the relevance between two queries by measuring the simialrity between the most similar categories of the two queries among C and C .
Speci cally, the relevance between query q and q is then de ned as (cid:7) (cid:7) r(q, q (cid:7) ) = max (cid:7) c C,c(cid:2) C(cid:2) Sim(c, c ).
(10) For an input query q, the relevance of its recommendations is then de ned as rel(q) =

 (cid:8) q(cid:2) U (cid:7) ), r(q, q (11) where U denotes the recommendation set and |U| is the number of queries in U .
Diversity.
We measure the diversity of recommended queries based on the di erences between their top ranked search results provided by Google.
Speci cally, given two 6http://www.google.com/dirhp queries q and q , we compute the proportion of di erent URLs among their top k ( k = 10 in our case) search results by (cid:7) (cid:2) (cid:7) ) = d(q, q (cid:7) 1   |o(q,q(cid:2))|
 k (cid:7) if(q (cid:7)= q ); otherwise, (12) where o(q, q top k search results of query q and q query q, the diversity of its recommendations is de ned as ) is the number of overlapped URLs among the .
Then for an input (cid:7) (cid:9)(cid:3) (cid:3) q U
 q(cid:2) U d(q, q(cid:7)) div(q) = (13) where |U| is the number of queries in the recommended query set U .
, Note that we do not adopt an opposite measure, e.g., the overlap of the top ranked search results, to evaluate the relevance between two queries since that kind of relevance is not desired by query recommendation.
Obviously, it is not reasonable to assign high relevance scores to queries with similar search results as the input query and recommend them to users.
Therefore, we evaluate relevance with a topical similarity de ned above.
Q-measure.
The two metrics above measure the relevance and diversity of recommendations respectively.
It is better to have a measure which can combine these two aspects to have a comprehensive evaluation of the e ectiveness of recommendation methods.
Here we borrow the F-measure scheme and introduce a new metric to assess relevance and diversity tradeo , referred as Q-measure.
Formally, it is de ned as the weighted harmonic mean of relevance and diversity: Q(q) = = (1 +  2)   rel(q)   div(q)  2   rel(q) + div(q) (1 +  2)  2 div(q) + 1 rel(q) , (14) where   is a parameter which can be used to control the tradeo  between relevance and diversity.
If the value of In our experiment settings, we have   = 1 which equally emphasize importance of relevance and diversity.
e c n a v e e
 l


 Naive Hitting_time
 Grasshopper Mani_stop




 Size of recommendations

 Figure 1: Average Relevance of Query Recommendation over Di erent Recommendation Size under Five Approaches.
y t i s r e v
 i






 Naive Hitting_time
 Grasshopper Mani_stop







 Size of recommendations Figure 2: Average Diversity of Query Recommendation over Di erent Recommendation Size under Five Approaches.
e r u s a e m  
 Naive Hitting_time
 Grasshopper Mani_stop









 Size of recommendations Figure 3: Average Q-measure of Query Recommendation over Di erent Recommendation Size under Five Approaches.
The automatic evaluations were conducted over a variety of recommendation size (up to top 10) and the results are presented in Figure (1-3).
Figure 1 shows the average relevance values of query recommendations under  ve di erent methods.
As expected, for all recommendation methods, the average relevance value gradually decreases when the recommendation size increases.
We notice that Mani stop can achieve better performance in relevance as compared with MMR and Naive methods, which directly measure the relevance between queries in a Euclidean space.
It demonstrates the e ectiveness of the intrinsic query manifold in capturing the relevance between queries.
We also note that the relevance of Grasshopper drops very quickly as the recommendation number increases, which makes its relevance performance not very stable.
Besides, the relevance value of Hitting time is lower than that of all the other four approaches on average.
The major reason is that Hitting time employs the expected hitting time from other queries to the give query to rank recommendations, so that it can boost long tail queries for recommendation.
However, the recommendations with low hitting time to the given query may not be necessarily closely related, and thus hurt the relevance performance.
The average diversity values of query recommendations under the  ve di erent approaches are shown in Figure 27.
Not surprisingly, the diversity of Naive is the lowest one in the  ve approaches, since Naive only focuses on recommending queries according to their relevance with the input query.
Hitting time obtains better diversity by boosting the long tail queries for recommendation, but the improvement is limited.
Both MMR, Grasshopper and Mani stop can receive higher diversity value by explicitly address the diversity in ranking.
Among these three approaches, Mani stop obtains the highest diversity on average (0.872) by introducing the stop points into query manifold.
Figure 1 and Figure 2 show the relevance and diversity of di erent approaches, respectively.
To better evaluate the overall e ectiveness of di erent approaches, we show the Q-measure (a weighted harmonic mean of relevance and diversity) in Figure 3.
From the results, we can see that Hitting time works better than the Naive approach.
Both MMR and Grasshopper can further outperform Hitting time, while Grasshopper obtains better results than MMR by leveraging a  soft  penalization.
Among the  ve approaches, the proposed Mani stop approach consistently outperforms the other four baseline approaches in terms of Q-measure.
We conduct t-test (p   value <= 0.05) over the results and  nd that the performance improvement is signi cant as compared with the baselines.
These above results clearly con rm that our Mani stop approach can e ectively generate highly diverse as well as closely related query recommendations.
As our approach is based on the query manifold structure (i.e. k-nearest-neighbor query graph), we also study the impact of parameter k, which de nes the number of nearest neighbors when constructing the graph and plays an important role in terms of both e ectiveness and e ciency.
to that it is meaningless with the diversity metric.
r u s a e m  












 Q measure@3 Q measure@5 Q measure@8 Q measure@10








 k Figure 4: Q-measure over Di erent Recommendation Size (3,5,8,10) with k varying from 10 to 100.
Figure 4 shows the performance of our approach in terms of Q-measure under di erent recommendation sizes when varying k from 10 to 100 .
The X-axis is parameter k, while the Y-axis is the Q-measure value measured by the automatic evaluation.
We can see that as increasing the number of k, the Q-measure value exhibits a rise, until k = 50.
It indicates that, if value of k is too small, the relationship between queries may be not be well revealed by the manifold structure.
If k > 50, there is a slight decrease of the performance.
The reason is that when k > 50, the possibility that noisy queries are becoming added into queries  neighborhood is increased, which will potentially a ect the quality of query recommendation results.
We further conduct manual evaluation for comparing different recommendation methods.
For each query, we create a recommendation pool by merging the topmost (e.g., 10 in our work) recommendations from all the methods.
Then we invite 3 human judges, with or without computer science background, to label the recommendations in the pool manually.
For each test query, the human judges are required to identify the relevant recommendations and further group them into clusters according to their search intent.
We create a label tool as shown in Figure 5 to help ease the labeling process.
For each test query, the human judges are presented with the recommendation pool (the left panel), and the search results of the query from a commercial search engine (the right panel).
When labeling a recommendation, the human judge  rst takes a relevance assessment on the recommendation.
If the recommendation is irrelevant to the test query at all, he just skips it.
If relevant, he then needs to compare this recommendation with previous labeled recommendations and mark it in the same column as the one with the same search intent, or mark it in a new column (as a new intent), otherwise.
The human judges are allowed to use a search engine of their choice for better understanding the meaning of the query and the recommendations.
Note here there are three major cases in which two queries are considered as sharing the same search intent: (1) they are equivalent expressions, e.g.,  post code  and  zip code ; (2) one query is the subconcept of the other, e.g.
 abc breaking news  and  abc news ; (3) they are closely related with each other and share many similar search results, e.g.
 travel directions  and  travel maps .
Since the labeling task is costly, we just randomly pick 50 queries for manual evaluation.
I(cid:8)
 With the human labeled data, here we evaluate the quality of the recommendations produced by di erent approaches (1)  normalized Disusing the following two measures: counted Cumulative Gain ( -nDCG) [6] which has been widely used in the TREC Web track 8 diversity task; and (2) Intent-Coverage.
 -nDCG.
The  -nDCG, which rewards diversity in ranking, is a new version of the nDCG [11], the normalized Discounted Cumulative Gain measure.
When   = 0, the  -nDCG measure corresponds to the standard nDCG, and when   is closer to 1, the diversity is rewarded more in the metric.
The key di erence between  -nDCG and nDCG is that they use di erent gain value.
For each recommendation, the gain value G(k) of  -nDCG is de ned as follows: Ji(k)(1    ) Ci(k 1) , i=1 G(k) = (15) where Ci(k   1) is the number of relevant recommendations found within the top k   1 recommendations for intent i, Ji(k) is a binary variable indicating whether the recommendation at rank k belongs to intent i or not, and I is the total number of unique intents for each test query.
The computation of  -nDCG exactly follows the procedure described in [6] with   = 0.5.
Intent-Coverage.
The Intent-Coverage measures the proportion of unique intents covered by the top k recommended queries for each test query.
Since each intent represents a speci ed user information need, higher Intent-Coverage indicates larger probability to satisfy di erent users.
Note that the Intent-Coverage is di erent from the diversity measure used in automatic evaluation, since only relevant recommendations to the test query will be considered in Intent-Coverage.
Therefore, Intent-Coverage can better re ect the diversity quality of recommendations than the diversity measure in automatic evaluation.
The Intent-Coverage is formally de ned as: I(cid:8) Intent   Coverage(k) =

 Bi(k), (16) i where Bi(k) is a binary variable indicating whether the intent i found within the top k recommendations or not, and I is the total number unique intents for each test query.
In our experiments, we compare the performance of di er-ent methods in terms of  -nDCG@5,  -nDCG@10, Intent-Coverage@5, and Intent-Coverage@10.
Table 2 reports the performance of di erent recommendation approaches under manual evaluation.
The numbers in the parentheses are the relative improvements compared with baseline methods.
From Table 2, we can see that Naive obtains the lowest Intent-Coverage and also shows a poor overall performance as measured by  -nDCG, since it only consider the relevance in recommendation.
Hitting time approach obtains better performance than Naive approach as it implicitly addresses the diversity in query recommendation by boosting long tail queries in top ranked results.
Both MMR and Grasshopper approaches can further outperform Hitting time approach
 given input query and the right panel shows the the search results of the query from a commercial search engine.
The columns A i in the left panel denotes the i-th intent of the test query.
A recommended query belonging to the i-th intent of the test query will be marked in the corresponding column.
Table 2: Performance of recommendation results over a sample of queries under  ve di erent approaches.
Performance metrics  -nDCG@5,  -nDCG@10, Intent-Coverage@5 and Intent-Coverage@10 are shown.
Numbers in parentheses indicates relative % improvement over Naive/Hitting time/MMR/Grasshopper.
Paired t-tests are performed, and results which show signi cant improvements (p-value < 0.05) are marked  .
Naive Hitting time
 Grasshoppper Mani stop  -nDCG@5

   /*/*/*) (7.4
   (11.4
   (10.7
   (16.9   /8.8 /3.8/*/*) /3.1/-0.6/*)   /4.9   /5.5 ) /*/*/*)  -nDCG@10

   (7.1
   /0.5/*/*) (7.7
     /4.1/3.5 (11.5
     (17 /8.6   /9.2 /*)   /4.9 /*/*/*) Intent-Coverage@5

   (16
     (28 /10.3
   (24.3
   (45.3 /7.2/-2.9/*)   /13.5   /25.3 /*/*)   /16.9 /*/*/*) Intent-Coverage@10

   (9.3
   (9.1 /0/*/*)
   (14.9
   (24.1   /13.5   /13.7 /5.1/5.3/*)   /8 ) ) ) on  -nDCG by explicitly addressing recommendation diversity.
It seems that as a  soft  version of MMR, Grasshopper can achieve better performance than MMR when recommendation size is large.
Compared with the four baseline methods, our Mani stop approach achieves the best performance in terms of all measures, which is consistent with results reported in the automatic evaluation.
We also conduct the t-test (p   value < 0.05) and  nd that the improvements over all baseline methods are signi cant.
It shows that by exploiting the intrinsic global query manifold structure and employing manifold ranking with stop points, we can recommend highly diverse as well as closely related queries.
In this paper, we address the problem of recommending relevant and diverse queries.
We propose a novel uni ed model, named manifold ranking with stop points, to solve this problem.
Our approach leverages a manifold ranking process over query manifold, which can naturally make full use of the relationships among queries to  nd relevant and salient queries.
Meanwhile, we introduce the stop points into query manifold to capture the diversity during the ranking process.
In this way, our approach can generate query recommendations by simultaneously considering both diversity and relevance between queries in a uni ed way.
Like traditional manifold ranking algorithm, the new proposed ranking approach also shows a nice convergence property.
Both automatic and manual evaluations are conducted to demonstrate the e ectiveness of our approach.
The extensive empirical results clearly show that our approach can outperforms all the four baseline methods (Naive, Hitting time, MMR and Grasshopper) in recommending highly diverse as well as closely related query recommendations.
For the future work, we would like to explore di erent ways for modeling the query manifold structure and investigate how it may a ect the recommendation performance.
It would also be interesting to apply the proposed approach to a variety of applications, e.g., image retrieval, expert  nd-ing, and product recommendation, where both diversity and relevance are demanded in ranking.
This research work was funded by the National Natural Science Foundation of China under Grant No.
61003166 and Grant No.
60933005.
