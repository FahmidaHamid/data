Any time we modify a web search or information retrieval algorithm, we would like to compare it to previous or competing algorithms.
Traditionally this is done with metrics, such as discounted cumulative gain (DCG) [9], Bpref [2], expected reciprocal rank (ERR) [5], rank biased precision (RBP) [12], which are computed from human relevance judgments, which can be costly.
More recently, quality metrics based on user interaction including click-through distributions have been introduced; however using these online metrics directly to test new search models can be risky, because unsuccessful con gurations may hurt user experience while being tested to be able to collect the required user interaction data.
In addition, state-of-the-art click models can make only predictions for high frequency query-URL pairs, and so can be over-con dent in their estimates.
Therefore, an inexpensive evaluation method reusing the existing judgments, and session logs from the previous search models is highly desirable.
We provide a method of reusing existing relevance judgments and combining them with the relevance estimates obtained by a user browsing model.
Our experiments focus speci cally on estimating the di erence in Discounted Cumulative Gain (DCG) [9] between two search models (we will refer to this as  DCG), since this metric is widely used in the community.
This approach is applicable to any evaluation metric for which we can calculate the mean and variance from random variables (such as Average Precision [3]), and we will elaborate on general applicability of our method.
In the literature, there are many user browsing models to estimate the relevance of documents from the user session logs.
Some of these methods are based on the idea of providing pairwise preferences for pairs of documents.
Joachims and Radlinski s methods are perhaps the most well-known of those belonging to this family [11, 14, 10].
They use heuristics like skip-above or skip-next to provide pairwise preference judgments for pairs of documents, which are not corrupted by presentation bias.
Presentation bias is based on the fact that the users do not examine all the documents presented to them, and the click-through distributions are biased towards the higher ranked documents.
There are also models that aim to provide the absolute relevance values [7,
 namic Bayesian Network (DBN) based model by Chapelle and Zhang [6].
In this work, to evaluate the relevance estimates from session logs we use the Chapelle-Zhang DBN model.
The next section gives a brief overview of this model, of our choice.
In summary, we propose an evaluation metric for an unseen result set con guration, using the editorial data and clicks from previous con gurations.
Using this, we estimate the  DCG between two result sets, namely between the baseline model, and the unseen con guration.
This paper is structured as follows.
The next section gives a brief overview of the user browsing model used.
In Section 3, we give a mapping from the outcome of the click model to editorial labels, and a method to do query and position based smoothing to substitute for missing relevance values.
Then we present our experimental setup and provide details about the dataset used in the experiments.
Afterwards, we show evaluations of the proposed smoothed-DBN model showing that it overwhelmingly outperforms (correlation of 0.74 versus 0.29) previously published methods when used over a random sample of queries, and approaches inter-annotator agreement.
Finally we conclude the paper with a brief discussion of directions for future work.
User browsing models use web search interactions including searches and clicks to estimate the relevance of documents to search queries.
Some consider pairwise interactions [11, 14, 10] to estimate relative relevance.
Others use click-through patterns on one or more of the results to estimate relevance that is comparable across queries and URLs.
The DBN model by Chapelle and Zhang [6] deals with the result set as a whole.
As well as the order of the results, it also takes into account the e ects of other URLs, similar to the approach in the cascade model [7, 8].
The reasoning is as follows: a relevant result presented along with very relevant results gets few clicks, whereas the same document would have gotten many more clicks if it had been presented along with less relevant ones.
The DBN models the examination probability of a particular document as a function of its rank in the result set, as well as the quality of the other documents in the set.
The model does not require any relevance judgments for training, and it can assign relevance scores to query-URL pairs where the URL has been seen several times for the given query, and the motivation is to infer two latent variables, de ned as attractiveness and satisfaction.
Let e, c and s be binary random variables, indicating whether the user has examined, clicked to, or satis ed by a given search result, respectively.
Chapelle & Zhang de ne attractiveness as the perceived relevance of a search result, formally p(c = 1|e = 1), the probability that the user would click on the search result given that she examined it.
Further, they de ne satisfaction as the landing page relevance, p(s = 1|c = 1), the probability that the user would be satis- ed with the result given that she clicked on it.
After inferring these two latent variables, they model the relevance of a search result as attractiveness satisfaction.
The derivation and the complete details of the DBN model is out of the scope of this paper, but to provide motivation to the readers we would like to present the simpli ed version of it.
In fact, it is notable is that this very basic simpli ed-DBN model that does not require any optimization performs only slightly worse than the DBN model optimized via expectation maximization.
Given a ordered result set, assume that the user examines the results from top to bottom, one by one, without skipping any of them.
Also assume that the user keeps going deeper into the result set until she is satis ed, and stops afterwards.
These two assumptions imply the following, e1 = .
.
.
= el = 1, and el+1 = el+2 = .
.
.
= 0, in words, all the results until the last clicked one are examined, and the rest, the ones after the last clicked result are not examined.
Then the latent variables attractiveness and satisfaction can simply be obtained by counting the following three values for each URL; N u last click, namely the number of times the URL is examined, clicked, and is the last click of a search query1.
Using these three values, the latent variables attractiveness (au) and satisfaction (su) for each URL are de ned as click and N u view, N u au = N u N u click view , su = N u last click N u click (1) Intuitively, au is the ratio of times that it is clicked versus the times that it gets clicked or skipped2.
Similarly, su of a URL is the ratio of times that the users end their search (hence, do not go back to the search result page and click to another result) after clicking this particular result.
Overall, the DBN model not only provides better estimation accuracy than earlier models, it is also by nature well connected to earlier models.
The examination model, and the cascade model are special cases of this model, and with the way the examination likelihood (or N u view in the sim-pli ed model) is de ned, DBN inherently implements the skip-above pairs type of ideas like Joachims s work.
Also note that in the following there is nothing speci cally adjusted or  ne-tuned for this particular model; hence, one can plugin any other user browsing model that gives absolute relevance values.
In the experimental results section, we will show that similar results can be obtained with ordinal regression as well.
We would like to evaluate a new ranking model by comparing with a baseline, and looking at the di erence in the chosen metric.
The steps consist of (1) express the change in the metric in terms of a function of the means and variance of a probability density function over the metric (2) mapping the estimates from the click-based model to judgments for the metric by  tting a distribution to data in the intersection (3) computing estimates for the remaining missing values using query and position based smoothing.
The intersection is the portion of the query-URL pairs that we have both editorial judgments and the user browsing model estimates.
Note that, although derivation of the performance evaluation method in the beginning of this section is given for DCG [9] only, the techniques in the rest of the paper apply to any other performance metrics based on human
 so there is no de nition of a session.
Therefore, the queries are handled independently, and the last click is considered per query basis.
that for a URL the view count is not increased unless a lower ranking result is clicked.
derivation of MAP as a random variable has been given by Carterette et al. [3].
For each pair of new and baseline model pairs, we will evaluate the  DCGn for n=5 and DCG formula is given by DCGn = reli log2(i + 1) (2) where normally reli values are the relevance scores by human judges in a 5-scale grades.
These grades later translated into a numeric scale to be used in the DCG formulation.
At this point, following the derivation in [4], we introduce a random variable gi representing the relevance of the document i, and rewrite the DCG formula as nXi=1 nXi=1 DCGn = gi log2(i + 1) (3) We would like to introduce this random variable because estimating a particular grade from user browsing model based estimates is quite hard, whereas estimating the probability density function (PDF) of a prede ned parametric form is relatively more reliable.
This will be clear in the next section.
Given gi, the grade probability of document i, the probability distribution p(gi) is multinomial.
Let {a1, .
.
.
, a5} be the set of numeric scale values to be used in DCG estimation that correspond the 5-scale grades, such that a1 > a2 > a3 > a4 > a5.
We de ne pij such that pij = p(gi = aj) j=1 pij = 1.
The expected grade E(gi), for 1   j   5 andP5 and its variance V ar(gi) are given by E{gi} =P5 V ar{gi} =P5 j=1 pijaj j=1 pij a2 j   E{gi}2 (4) Using E{gi} and V ar{gi}, we write the mean and variance of the DCG.
The mean is given by E{DCGn} = E{gi} log2(i + 1) (5) nXi=1 Assuming the relevance of each document is independent of each other (hence, Cov(gi, gj) = 0), the variance of DCG is given by V ar{DCGn} = V ar{gi} (log2(i + 1))2   E{DCGn}2 (6) nXi=1 To evaluate the new model, we sample from the queries that this new model a ects (which, in general, may or may not be the all queries).
For a particular query in this set, let D and D  be the set of results obtained with the search engine when the model to be tested is turned on (new con gu-ration) and o  (the baseline model), respectively.
We want to evaluate E{ DCGDD  }.
However, simply evaluating the DCG for these two sets independently and looking into the di erence (that is  DCGDD  n =
 n ) is not optimal here, and one should compute the E{ DCGDD  } directly.
(For example, if a particular document i is at the same rank both in D and D , the variance of gi for this document should cancel out and not a ect the V ar{ DCGDD  } and V ar{ DCGDD  n   DCGD  }.)
n n n n e r o c s


























 x (DBN relevance estimate) Figure 1: DBN relevance estimates vs.
editorial scores in PEGFB scale (Perfect - Excellent - Good - Fair - Bad) i and rD  Given rD the ranks in each result set, we de ne the discounts for all documents conditioned on both result sets D and D : i i =( dD
 i +1) log2(rD
 i   n if 1   rD if D does not have document i (7) Finally using the discounts for each result set, one can write the  DCG over the union of the documents in both result sets
 n E(gi) (dD i   dD  i ) (8) ) = Xi D D  In the next two sections, we will focus on the estimation of E(gi) for all the query-URL pairs from di erent sources; editorial data, user browsing model, and query-position based smoothing.
Judgments To understand the relationship between DBN relevance estimates and editorial judgments, we inspect the intersection of the editorial and the DBN relevance data, namely the query-URL pairs for which we have both an editorial judgment and a DBN relevance estimate.
This is shown in Figure 1, where each point represents a query-URL and the vertical and horizontal axes are the DBN relevance estimate x and the editorial scores, respectively.
A noise jitter is added to the vertical axis for visual purposes.
Note that DBN scores tend to correlate with editorial scores, so these quantities are predictive of each other.
Since they are not perfectly predictive, we will relate them to each other in terms of probability estimates, as will given next.
to Editorial Judgments Although it is possible to use the relevance estimates from the browsing model to estimate the  DCG using only session logs, incorporating the existing editorial data into the evaluation process is of great interest because click models can estimate relevance only for queries that appear several times in the query logs (at least 10 times in the DBN model).
Therefore they tend to have a bias towards more frequent queries.
Since the infrequent queries are still a signi cant fraction of tra c to the search engine, an evaluation method based only on more frequent queries might be overcon dent







 p(g) with depth 1 p(g) with depth 5 Table 1: Inter-editor agreement matrix






































 Figure 3: The prior grade distribution p(g) of the estimates provided.
One can solve this problem by combining editorial judgments from infrequent queries into the evaluation process.
We denote the relevance estimates from the DBN browsing model as xi.
Note that since the DBN is a probabilistic model, and the relevance estimates that it provides are in the range [0,1].
Therefore, if one needs to combine the DBN estimates with some existing editorial data, one needs to compute a mapping between the relevance estimates of the DBN and the editorial grade scale {a1, .
.
.
, a5}.
Note that we could map the DBN scores to the editorial grade scale, or the reverse.
In this case we chose to map to the editorial grade scale for ease of understanding for those familiar with working with DCG based evaluation.
Note that there is no loss of information in mapping in either direction, since we can work in probability density functions.
To obtain this mapping, we inspect the intersection of the editorial and the DBN relevance data, the query-URL pairs that we have both editorial grades and DBN estimates, shown in Figure 1 and described above.
Given its DBN relevance estimate and the numeric scale values of the editorial grades {a1, .
.
.
, a5}, our aim is to estimate the grade probability distribution of any query-URL pair.
To estimate the multinomial grade distribution p(gi) for each xi, hence to obtain pij values, one should  rst estimate the prior distribution of grades p(g) and the grade conditional DBN estimates p(x|g), and employ Bayes rule.
Since g is a discrete valued random variable, obtaining p(x|g) is quite straightforward by  tting individual models for each grade separately.
We  t beta distributions to the DBN results of each grade, as shown in Figure 2.
The prior grade distribution p(g) is the marginal of the p(gi).
This multinomial distribution can simply be obtained by counting the samples for each grade.
However, note that this should be done for the same ranking function in a position aware manner.
For example, if p(g) is going to used in the estimation of DCG1, one should use the histogram of the top ranked judged documents.
Similarly for the estimation of DCG5, one should use the histogram of the documents in top 5.
For the judged dataset we have, both of these are shown in Figure 3.
Using Bayes rule, one can estimate the posterior to be p(g|x)   p(x|g) p(g) with a missing normalization constant.
Figure 4 shows p(g|x) for both prior distributions.
The posterior probability p(g|x) with the grade prior probabilities obtained from the whole editorial data is given in Figure 5, evaluated at 10 points uniformly sampled along x.
Table 2: Mean and variance of each grade distribution estimated from the inter-editor agreement matrix E(g) V ar(g)














 On the other hand, estimating the grade probability from editorial data is relatively simpler.
The most straightforward approach is, for a particular query-URL, to select pij =
 the grades are exact, that is the grade pdf p(g) has zero variance.
In fact, that is the assumption by Carterette and Jones [4], when they combine editorial data and click-based relevance estimates.
However, it is well known that this assumption is not valid, since the judges hardly ever fully agree with each other [1, 16, 15].
To be able to relax this assumption, we measured the inter-editor agreement for a query-URL data set that has three judgments given by three di erent editors independently, to account for the variance of each grade3.
The query-URL dataset consists of 5 URL per query for 334 random queries, resulting in 1670 unique query-URLs and 5010 judgments in total.
The three judges for each query-URL are randomly assigned from a pool of 42 individuals.
Cross-comparing the grades for the same query-URL pairs given by di erent editors, one ends up with a 5 5 matrix as shown in Table 1, where the disagreement can be measured by the non-diagonal entries.
Given the grade, one can use the rows (or columns) of the inter-editor agreement matrix to estimate the underlying multinomial grade probability distribution by normalizing by the sum of each row.
E(g) and V ar(g) values of these multinomial distributions for each grade in the DCG estimation is summarized in Table 2, and these are the values we will use in our experiments.
In general, the user browsing model and the preexisting editorial data cannot provide relevance estimates for all documents for the selected set of queries.
To estimate the missing judgments, we use a query and position based smoothing over the existing relevance values, obtained either through editorial data or the user browsing model.
In Table 3 we see an illustration of the grades of top  ve documents, where the relevance judgments are given in a  ve scale of Perfect Excellent Good Fair and Bad, and   rep-
di erent than the one we use for the evaluation of the method in the Experimental Results section.
p(x|bad) p(x|fair) p(x|good) p(x|excellent) p(x|perfect)




































 Figure 2: beta distributions for p(x|g) p(bad|x) p(fair|x) p(good|x) p(excellent|x) p(perfect|x)



































 Figure 4: p(g|x) evaluated with respect to the p(g) of depth 1 (dashed) and 5 (solid) as shown in Figure 3.
Table 3: An illustration of grades of top 5 documents for di erent queries, where missing values are shown with  .
For q1 query-based smoothing seems the most natural, and we might guess that the missing grade is Bad.
For q5 a position-based smoothing is more natural - assuming the document at rank 1 has typical relevance for a rank 1 document, and so on.
q4 q3 q2 q5 q1




 resents missing judgments.
For the cases where we have a su cient number of grades for the query and the available grades are close to each other (like in q1 and q2), query-based smoothing is more e cient.
Where there are a few or perhaps no grades (like in q4 and q5) and/or the available grades have a large variance (like in q3), position-based smoothing becomes more e cient.
Therefore, we want a hybrid smoothing model that combines these two models in a query-dependent manner.
For simplicity, the derivation here is given for only one of the result sets, D. One should repeat the same for D , and for the documents that both appear in D and D , use the average of these two estimates.
Let us de ne two basic smoothing methods.
Position-based smoothing: De ne pD r (g) as the average grade pdf at rank r, averaged over all queries and document set D, which is given by pD r (g) = n 1 r Xd @ rank r in D p(gd) (9) Query-based smoothing: De ne pD grade pdf in D for the query q, given by q (g) as the average pD q (g) = n 1 q Xd in D for q p(gd) (10) Here nr and nq denote the number of available relevance judgments for rank r or query q, respectively.
Query and position based smoothing: One important observation is that the query based smoothing performs much better when (1) there are more than a couple grades for that query (2) the grades are similar or the same.
On the other hand, although it is more robust and performs equally well regardless of the number of available grades per query, position based smoothing performs worse than query based smoothing on average.
Over a random set of queries, the percentage of missing judgments per query is rarely uniform, tail queries may have only a few judged or click estimated documents, whereas more frequent queries have much more available click estimated relevance values due to much bigger user interaction data.
Since the performance of these two methods vary from query to query, we de ne a hybrid smoothing model that combines these two smoothing methods in a query-dependent manner to get the best of the both.
The hybrid method is a weighted combination of the two, and adapts the weights depending on the number (and distribution) of the available relevance values.
For this we de ne a meta-parameter   that gives what exactly the su cient number of grades and large variance mean.
In Section 5.1 we give a leave-one-out cross-validation method to learn the optimum value of   from the data directly.
Consider the following formulation ps(g) = wq pD r (g) + (1   wq)pD q (g) (11) where wq are query-dependent weights.
Intuitively, we want wq to increase (hence bias towards query-smoothing) when, the number of available grades for that query increases and the available grades have a lower variance.
For a particular query q, we de ne the mean relevance as the average of the available (graded or click-estimated) relevance values  D q (g) =
 N D Xd in D for q E(gd) (12) where N D is the number of documents in D. Hence, we select the weight as a function of the variance and the number of the available relevance values for this query































































 x=0.4



 x=0.9



 x=0.3



 x=0.8



 x=0.2



 x=0.7



 x=0.1



 x=0.6



 x=0.5



 x=1

 Figure 5: p(g|x) at 10 uniformly sampled points along x (for depth = 1) dq = 1 N 2(cid:0)Pd D(E(gd)    D wq = e dq / 2 q )2 + V ar(gd)(cid:1) (13) Here we have a parameter  , and for   = 0 and      , (10) converges to position and query-based smoothing, respectively.
The optimal value of   can be selected by using leave-one-out cross validation over the existing relevance values and using Fibonacci search (or similar).
Note that for each novel-baseline search con guration model pair, the optimal value of   varies, depending on the percentage of the missing judgments in the whole dataset as well as how the missing judgments are distributed over queries.
If the missing judgments are somewhat evenly distributed over the query set, the optimal   turns out to be smaller.
Conversely, if the missing judgments are concentrated for a portion of the dataset, that is many queries have very few or no missing judgments whereas some other have a lot, the optimal   value turns out to be higher.
We will show experiments and estimation error results regarding the proposed smoothing model.
So far we have presented three ways of relevance information: the editorial data, relevance estimates of the user browsing model, and missing relevance values estimated by query-position smoothing.
To combine these we use the following order:
 Table 1, normalize and use as the editorial grade pdf p(g).
browsing model and use p(x|g)p(g)
 vance estimate due to insu cient session logs for this query use the query-position smoothing given in Equation (11).
Before presenting the experimental results we start with providing the details of the data sets used in the experiments and the overall experimental setup.
For this experiment we de ne 5 tests.
Each test is a baseline-novel search engine pair, and each test will be conducted on the set of queries that the corresponding novel model a ects hence, the query sets in each test are not random splits.
In our experiments, all tests are for evaluating new version of a query rewriter (QRW).
In these tests, the novel search engine con guration (which returns the document set D) is basically the search engine with new QRW model that has yet to be tested online, and the baseline con guration (that returns the document set D ) is the search engine with the existing QRW model.
Although we design our tests to evaluate QRW methods, the model can be used for other binary query classi ers designed for specialized experience, for example, the classi ers that are used for triggering the shopping, local or news modules.
A positive E( DCGDD  ) means that the new QRW model is more successful than the baseline.
The query set consists of 1785 uniformly sampled queries from the a ected queries of each QRW model.
The sizes of the query sets are di erent for each test, proportional to the coverage of the a ected queries by each QRW model.
Hence, for all the queries in the dataset, the test con guration is likely to (and most likely does) give search results di erent from the baseline system with the old QRW model.
The number of test queries a ected by the test con gu-rations, and the total number of unique query-URL pairs (at top 5 ranks) for each model as well as number of available editorial judgments are given in Table 4.
Note that the number of unique query-URL pairs is not simply 5   number of queries, since the total number of documents needed per query is the union of URLs returned for the novel con g-uration and the baseline con guration.
The more aggressive the model is, the more new URLs it will introduce, and therefore, the bigger the average number of URLs per query (D   D ) becomes.
The judged query-URL pairs constitute to 34.5% of the unique query-URL pairs needed for the 5 tests.
These are editorial judgments in the PEGFB scale that have been collected independently for previous experiments within the last three months.
The DBN user browsing model is trained on three months of user click logs of a major search engine, and it produces an output for the queries-URL pairs that have been examined at least 10 times (N u view   10 for the simpli ed DBN model presented in Section 2.1).
For the query-URL pairs without an editorial judgment, the percentage of query-URLs with a DBN relevance estimate is around 30.5% of all the required ones in our dataset.
We use the data described above and given in the 3rd and test set of 1785 unique queries unique queries unique query-URL judged query-URL DBN query-URL Test1 Test2 Test3 Test4 Test5




















 model and estimate the  DCG for the  ve tests.
To measure the accuracy of our evaluation method, we compare it to fully editorially calculated DCG values.
For this we collected editorial grades on the same PEGFB scale that covers all the unique query-URLs needed for all tests (2nd row in Table 4), and use it as our evaluation data.
Furthermore, to underline the fact that the judges do not agree with each other as also mentioned in Section 3.3, we get this full query-URL set judged three times with random judge assignments from a pool of 49 individuals.
We will use the multiple fully judged query-URL datasets to measure how well a fully editorial judgment based  DCG value correlates to another independent fully editorial judgment based  DCG value, and we will show that our method signi cantly approaches this inter-annotator correlation value.
The accuracy of the smoothing technique and the performance of the proposed method and comparisons with an earlier method as well as correlations with fully editorial judgment based  DCG values will be presented here.
First we show a comparison of the three smoothing methods presented in Section 3.2, showing that all improve over Zhang and Chapelle s unsmoothed DBN models, and then showing how to select an optimal interpolation weight.
Since the derivation also builds on the formulation by Carterette and Jones [4], we also provide some comparisons with this method to underline the di erences of our approach.
We show our method signi cantly outperforms theirs on a random sample of queries in all frequency ranges.
Finally we show the performance of our evaluation method for  ve different search engine tests and compare the results with fully editorially judged  DCG.
In this section we evaluate the performance of query-based smoothing, position-based smoothing, and query/position based smoothing, by looking at their accuracy at predicting the relevance judgements scores for individual query-URL pairs.
Recall from Section 3.3 that the ordinal relevance judgments are mapped to the scores {1, .
.
.
, 5}.
We can make a prediction of each query-URL pair, and compare to the true relevance grade score, then calculate the mean-squared error (MSE) over all predictions.
We compare the mean-squared error (MSE) and error probability densities of these three smoothing models in leave-one-out cross-validation sense.
The leave-one-out procedure is as follows:
 document from the dataset (denote this as gj).
t u o   e n o   e v a e

 l


  10  5


  



 Figure 6: Leave-one-out MSE for the range of   values (horizontal axis in the logscale)
 estimator (8), (9) or (10), to estimate the grade of the
 left out document dj (denote the estimate asbgj).
ej = gj  bgj
 report average squared error E{e} over all samples.
One can use the leave-one-out procedure to evaluate the smoothing method, as well as  nding the optimal granularity parameter  .
Figure 6 shows the leave-one-out MSE for varying   values.
Note that for   = 0, Equation (11) converges to the position based smoothing, which gives 0.9911 MSE.
Similarly, for      , Equation (11) converges to query based smoothing with a MSE of 0.9452.
At   = 0.55 the hybrid smoothing method in (11) gives a MSE of 0.7286, leading to a signi cant 23   26% improvement in estimation error with respect to the other two methods.
Again for the same leave-one-out evaluation, Figure 7 compares the error pdf of three methods, query smoothing, position smoothing and query-position smoothing with   = 0.55.
Investigating the error pdf s one can see that both query and position smoothing lead to systematic errors, a number of peaks in the pdf that are not centered at zero.
The error pdf s are obtained by standard kernel density estimation with Gaussian kernel function.
The variance of the Gaussian kernel is selected to be 0.1 for all three methods.
One can argue that it is possible to get rid of these peaks by using a wider Gaussian kernel, but the error samples themselves clearly show the systematic error behavior as well.
On the other hand, the hybrid query-position smoothing method leads to a more Gaussian-like error distribution with a single peak at zero, resulting in a much lower error entropy in smoothing.
In Table 5 we see a summary of the MSE under different smoothing methods.
Best results are obtained by query/position based smoothing, the interpolation of the other two.
The best results are with the parameter   set to 0.55.
We use this smoothing method (with the optimum parameter) for the remainder of our experiments, and refer to it as smoothed-DBN.
In this section we show that our smoothed DBN model signi cantly outperforms models from previous work, and approaches inter-annotator agreement.
est error gives best results.
Best results are with interpolated smoothing.
Query-smoothed Position-smoothed Query/Position-smoothed

 (  = 0.55)

 Since the formulation in Section 3 builds on the evaluation method by Carterette and Jones [4], we would like to point out the novelties of our approach and provide some comparisons.
To model the relevance using clicks, they use ordinal regression between the grades (same  ve-level scheme that we use) and the clickthrough rates of the document and the clickthrough rates of the documents presented above and below.
As they show in their results this model works very well; however, the shortcoming of this approach is that it requires a lot of user interactions, therefore, they limit their training set to queries that have at least 200 impressions in their session log data.
Similarly, they use queries with at least 500 impressions as their test data.
Evaluation based on only frequent queries may tend to be overcon dent of the underlying metric, since the infrequent queries that are neglected in the evaluation constitute a signi cant portion of the overall query volume.
Therefore, while collecting editorial judgments for evaluation, its a common practice in the community to evaluate the ranking models with a uniformly sampled random set of queries, and that is what we use as the test data of our method in our experiments.
In our uniformly random query sample of 1785 queries, the number of queries that have more than
 the training and testing cases in Carterette and Jones paper only constitute for a tiny portion of the uniformly sampled query set we use.
On the other hand with the DBN model, it is possible to obtain reliable relevance estimates with as few as 10 impressions, and for the queries with less than 10 impressions we rely on the available editorial data and the smoothing model.
With fewer impressions, of course the statistical variance of these DBN relevance estimates are higher than the ones obtained via more impressions, but the estimates from 10 impressions prove to be useful since they correlate well with the fully editorially evaluated  DCG as we will show in the next section.
One other signi cant di erence is that Carterette and Jones use the clicks that are collected over the ranking models that are being compared; on the other hand, we only use the clicks collected over the earlier ranking models to avoid the cost of running an online test.
Here, we rely on the fact that the previous ranking models also return some documents in common to get some portion of the click interaction data for free, however comes in with the unavoidable cost of some missing data.
Another important point here is that since we only use preexisting judgments for evaluation, and do not request any additional judgments.
When they combine the click-based estimates with editorial judgments, Carterette and Jones select the query-URL samples to be judged after observing the results retrieved by the two ranking models to be compared, and they give an algorithm to get judgments for the most informative query-URL samples in the data set.
On the other hand, the editorial data that we use for the same purpose is only some preexisting query position position query  3 x 10








 ) r o r r e ( p
  4  3  2  1
 error



 query position smoothing error


  4


  4


  4  3  2  1




 position smoothing error  3  2  1




 query smoothing error  3  2  1




 Figure 7: Leave-one-out estimation error probability densities for three smoothing methods (top), and the error samples of the leave-one-out estimation.
judgments that are collection of independently conducted editorial tests as the validation or testing sets of other research projects, and our aim is to reutilize these existing judgments.
Therefore, the judgments that we use are not tuned for the most informative samples (which di erentiate the two compared con gurations the most) by any means.
In fact, on average 23% of the preexisting editorial and click-estimated relevance data in our training set belongs to documents that are ranked at the same position in D and D , which do not even di erentiate the two compared con gurations at all.
Although these samples are useful at the smoothing step to estimate the missing relevance judgments, they do not directly a ect  DCG.
In summary, our approach is signi cantly di erent in the sense that we require no online test, and do not request any editorial data and work with only preexisting click and editorial data.
We experimented with the ordinal regression method by Carterette and Jones to provide comparisons.
Since their method is particularly designed to work with queries with large numbers of user interactions, we varied the comparisons to the subsets with more than k impressions in the dataset for k = 200, 50, 10, 0.
We present the average cross-correlation with fully editorially estimated  DCG results over these subsets of the query data for 5 test con gurations in the in Table 6.
The ordinal regression method is slightly more accurate than our smoothed-DBN for k   200, presumably because both ordinal regression and DBN models have enough samples to converge and there is very little or no need for smoothing.
As less and less frequent queries are included into the query set, our model is signi cantly superior, since the DBN model converges a lot faster and requires a lot less impressions, and the smoothing model is helping to  ll in the missing relevance values.
Over a random sample of queries (k   0) Carterette and Jones has a cross-correlation of only 0.29 with the fully labeled data, while the smoothed-itorially estimated data for di erent query sets.
Carterette and Jones  method degrades when we include queries with fewer impressions, while our smoothed method is robust to samples including rare queries.
All di erence in cross-correlation are statistically signi cant.
Query Set   200 impressions   50 impressions   10 impressions   0 impressions Carterette & Jones Smoothed-DBN







 DBN maintains a cross-correlation of 0.74.
The performance for our method is also decreasing as the query set consists of less and less frequent queries, which is of course expected, but the magnitude of the decrease is much less, from 0.78 to

 Comparing the estimated performance  gures against a fully editorially evaluated values is the simplest way to measure the accuracy of the evaluation method which perhaps is the most commonly used approach in most click-based user modeling or click-based evaluation papers.
Here we take one more step by also comparing how well two fully editorially evaluated  DCG correlate on the same dataset.
As clearly seen from Table 1, the editorial relevance judgments are far from being exact, the editors show some signi cant disagreement.
This disagreement induces a signi cant variance on the DCG and  DCG values evaluated from them, and therefore these are far from being an exact ground truth for evaluation.
In this view, the correlation between two  DCG values evaluated independently collected relevance judgments from a random set of individuals can be regarded as an upper bound for any click-based estimation model can reach.
Therefore in the following, we compare the correlation between our method and the editorially obtained values to editorial-to-editorial correlation.
To evaluate the accuracy of the overall method, we compare  DCG values we obtain against  DCG values evaluated from the fully judged validation set for  ve di er-ent test con gurations.
Table 7 presents the  DCG values given by the three fully judged sets and the smoothed-DBN, the bolded results show statistically signi cant ones with p-value less than 0.05.
We also compare the results of the smoothed-DBN against two baseline methods that use the preexisting click-estimated relevance values and the grades, individually.
In these baselines, we use the assumption that all unjudged documents are bad (pi1 = 1) and all unavailable click models have 0 clickthrough rate, hence bad relevance.
We summarize the cross-correlation between delta-DCG predictions and the fully-judged editorial data in Table 8.
For these 5 test con gurations, the average pairwise correlation of fully judged  DCG results is 0.81, and the average pairwise correlation between our method and fully judged results is 0.74.
In some cases the  nal goal is not to measure the actual value of the  DCG, but to choose the better of two test search ranking con gurations.
In this case, the actual DCG values are not as important as the sign of  DCG.
We repeat the above analysis for the estimation of the random variable sign( DCG).
The average correlation between all the fully judged results is 0.73.
The same value for all fully judged results and our method is 0.69.
In summary, our model reaches similar/same level of correlation almost as much as two fully judged sets correlate to each other with about 65% missing judgments.
After all, the correlation with fully judged sets is the accuracy merit for the smoothed-DBN; however, considering in terms of the individual tests, we can say that both editors and smoothed-DBN agrees that the QRW model in Test 4 does not give any statistically signi cant di erence against its predecessor, and the QRW model in Test 3 increases and the one in Test 5 decreases the overall performance.
Although no full agreement, in Test 1 and Test 2, the smoothed-DBN is correlated with the overall editorial assessment as well.
We propose an inexpensive o ine evaluation method that uses the preexisting click logs and judged data collected from earlier models.
Despite the fact that we use user interactions to model the relevance, we call this an o ine method since no online tests are conducted, no click logs are collected for the tested con gurations.
Instead we only use the clicks from earlier models, and exploit the fact that the models have some shared results in common, and interpolate for the missing data using smoothing.
We use Chapelle & Zhang DBN model to obtain click-based relevance estimates.
Comparisons with ordinal regression based method by Carterette & Jones show that DBN model provides similar/same accuracy for frequent queries, and it provides a much better estimates for infrequent queries.
Since we only work with click data collected from earlier models and editorial data collected independently for other studies, ending up with signi cant amount of missing data is unavoidable.
To  ll in for the missing relevance judgments we provide a smoothing technique that combines position and query based smoothing depending on the available grades on any particular query.
We show that this hybrid smoothing technique performs better than both query and position based smoothing, and despite the high percentage of missing judgments, the resulting method is signi cantly correlated with DCG values evaluated using fully judged datasets.
In this paper, we use the editorial and click data that has been collected within the last three months and assume that there are no signi cant time varying patterns within this time period.
In general, reusability of preexisting judgments and past session logs for evaluation is questionable since they may lead to bad results for many cases when there are time varying patterns in the data.
For example, on the new data one can see many new queries (for example  iphone 4 ), and radically di erent intents for some old queries, such as the query iphone  meaning  iphone 4  not  iphone 3  anymore, or the query  ipad  being a typo a year ago, but not anymore.
The tested con gurations in this paper are QRW models (query stemming or query segmenting etc.
), and the underlying characteristics of these language models do not exhibit signi cant time varying patterns.
However in general, there should be an expiration date for the reusability of the preexisting judgments and clicks.
model detects an improvement in the Test,  -  indicates that the model detects a degradation, and  ?  indicates that the model cannot detect a signi cant di erence.
 DCG estimates are shown in brackets.
