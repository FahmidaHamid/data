Retrieval e ectiveness measures continue to represent a primary method for assessing the performance of search engines.
These measures are computed by  rst executing a set of test queries against a search engine.
The top results returned for each query are manually judged with respect to that query.
These  editorial  judgments then form the basis for computing the various retrieval e ectiveness measures, providing an overall indication of the search engine s performance.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
A large number of the retrieval e ectiveness measures has been described in the research literature.
The nDCG measure remains the standard measure for Web search evaluation, while other traditional measures, such as average precision, are still widely used in other areas [11].
Researchers have recently proposed a number of new measures that are intended to address some of the perceived shortcomings of nDCG and other traditional measures, particularly in a Web context.
One group of proposed measures, the cascade measures, are intended to better re ect user behavior [10] by considering the relationship between successive documents in a result list [4, 12, 20].
Under traditional e ectiveness measures, including nDCG and average precision, the relevance of a retrieved document is considered separately from others in the results list.
Even when the results list contains documents that are nearly identical, each document is given full credit in the computation of the measure.
Considerations of novelty suggest a modi cation of this principle, in which redundancy is explicitly penalized in a results list by judging documents in the context of those already seen by the user.
These considerations led to the development of e ectiveness measures based on a cascade model of user behavior.
Under this model, the user is assumed to scan a results list from the top downwards, eventually stopping because either their information need is satis ed or their patience is exhausted.
Another group of proposed measures, the intent-aware measures, are intended to better re ect the diversity of information needs underlying a query by considering di erent aspects and interpretations of that query [1, 7, 8, 14].
The intent-aware measures decompose a query into distinct intents, each with an associated probability.
Documents may be judged with respect to each intent, allowing retrieval measures to be computed separately for each speci c intent.
The value of the overall intent-aware measure is computed as a linear combination of the intent-speci c measures, weighted according to the associated probabilities.
As we will show later in the paper, the cascade and intent-aware measures are closely related.
Many of the intent-aware measures share core features in common with the cascade measures, and all cascade measures are natural candidates for adaptation to an intent-aware framework.
While evidence suggests that the cascade and intent-aware measures generally operate as intended, they are still poorly understood and are not fully validated.
Retrieval e ectiveness measures may be validated in terms of properties such as their informativeness, discriminative power, robustness, and consistency with user behavior and (MEM) proposed by Aslam et al. [2] analyzes the informativeness of a retrieval e ectiveness measure by viewing the value of the measure as a constraint on the distribution of documents.
Using MEM, Aslam et al. evaluate the informativeness of several traditional measures, including average precision.
Given an e ectiveness measure, called the target measure, the most likely distribution is inferred by MEM.
By comparing the inferred distribution to the actual distribution, Aslam et al. demonstrate that the overall quality of a target measure may be quanti ed.
The inferred distribution may also be used to estimate the value of other retrieval measures, permitting comparisons between them.
Under the traditional measures studied previously by MEM, the relevance of a retrieved document is considered separately from others in a results list.
This leads to the expected value of these measures to be represented in a closed form, providing polynomial constraints accepted by MEM.
However, as the cascade model of relevance and also multiple intents of queries are the bases of the measures we study in this work, the complex formulations of these target measures can not be used in MEM directly.
Our computation of the expected values for these cascade and intent-aware measures produces a recursive formulation, which we solve through the application of dynamic programming and adopt it to the maximum entropy setting.
Our  ndings indicate that, although our extended MEM works well in predicting the value of the cascade/ traditional measures, it works even better when the intent-aware equivalents form the target measures.
In other words, the intent-aware measures are found to be more informative than their non-intent-aware equivalents.
Moreover, among the intent-aware measures we consider in this paper, ERR IA and NRBP measures are found to be the most informative.
Overall, the intent-aware measures are found to outperform others in terms of both inferring precision-recall curves and predicting the values of other retrieval measures.
In the next section, we review related work, with a focus on existing methods for validating retrieval e ectiveness measures.
Section 3 provides a detailed description of the cascade and intent-aware measures we study in this paper.
In Section 4, we extend MEM to the cascade and intent-aware measures, including a description of the associated optimization problems and their solution.
Finally, Section 5 reports experimental results based on runs created through the TREC 2009 Web track.
Many approaches to validating e ectiveness measures appear in the literature.
Buckley and Voorhees [3] studied the stability of traditional e ectiveness measures, empirically examining how the behavior of a measure changes when query expressions are varied.
Sakai [17] studied the robustness of e ectiveness measures, examining the consistency of measures in the presence of system bias and pool bias.
Informativeness and discriminative power are two of the most widely considered properties for the validation of e ec-tiveness measures [15] The informativeness of an e ective-ness measure refers to its ability to quantify the quality of a retrieval list.
The discriminative power of an e ectiveness measure refers to its ability to di erentiate the performance of retrieval systems.
The Maximum Entropy Method (MEM) proposed by Aslam et al [2] assesses the informativeness of traditional retrieval measures by estimating the probability of relevance for items on a results list, allowing the value of other measures for the same results list to be accurately inferred.
Among the traditional measures studied by Aslam et al., average precision was found to be the most informative.
Their  ndings indicate that one can accurately infer the distribution of relevant and non-relevant documents in a results list given only the value of average precision and the total number of relevant documents returned for the query.
The discriminative power of retrieval e ectiveness measures is often assessed using signi cance tests.
Sanderson et al. [18] make comparisons between the reliability of various retrieval measures using a number of signi cance tests.
Sakai [16] proposes a simple method for assessing the dis-criminative power of e ectiveness measures.
The method computes a signi cance test between every pair of experimental runs and reports the percentage of pairs that are signi cant at some  xed signi cance level.
Most of the above work concerns only traditional e ective-ness measures, such as average precision.
The newer cascade and intent-aware measures are not considered.
In this paper we extend the work of Aslam et al.[2] to encompass these measures.
Discriminative power and other properties of these measures are examined elsewhere [6].
Under traditional measures, such as average precision (AP) and nDCG, the relevance of a retrieved document is treated independently of others in the results list.
The cascade measures [4, 12, 20] which are based on the cascade model of user behavior [10], attempt to remedy this de ciency by explicitly penalizing redundancy in the results list.
Moreover, traditional measures often consider relevance in terms of a single broad interpretation of a query.
The intent-aware measures [1, 4, 7, 8] attempt to remedy this de ciency by explicitly considering di erent interpretations of an ambiguous query and/or di erent aspects of an underspeci ed query [8].
When placed into an intent-aware framework, the cascade measures provide a natural vehicle for measuring both novelty and diversity [6, 7].
In the description that follows, we treat the cascade and intent-aware measures in a uni ed fashion.
We assume a given query has M intents.
Each intent has an associated probability  j, 1   j   M , indicating the probability that a user entering the query is seeking information related to intent j. Non-intent-aware versions of the measures correspond to the case that M = 1 and  1 = 1.
Let d1d2... be the ranked list of documents returned in response to some query.
Chapelle et al. [4] imagine users read the results list in order, stopping when they  nd the information they seek.
Let qi j be the probability that a user who is interested in intent j will be satis ed with document di.
The probability that a user interested in intent j will stop at document i can be expressed as a gain value, Qi j, as follows: i 1Y Qi j = qi j (1   qk j ) (1) Values for qi j in this paper are estimated following the approach proposed by Clarke et al. [7], which is based on binary k=1 j is de ned as  gi relevance assessments available in our test collection [5].
Un-j where g   {0, 1} is the der this approach, qi binary relevance and 0 <     1 is a constant.
With respect to this approach, ci j is de ned as the number of documents ranked before position i that are judged relevant to intent j.
The gain value Qi j is then calculated as: k=1 gk j =Pi 1 i 1Y j ) =  gi j Qi j = qi j (1   qk (1    gk j ) =  gi j(1    )ci j (2) k=1 k=1 The gain value may be discounted by a factor Di that depends on rank, accounting for the extra e ort required to scan lower ranks and for the possibility that the user will abandon the query without  nding anything satisfactory.
This discounted gain value is thus Gi j/Di.
Consequently, the cascade e ectiveness measure, denoted as Cj, with respect to an intent j and for the top N documents can be computed by summing the gain values for each document: j = Qi

 Cj = Gi j = i=1 i=1 Qi j Di (3) In terms of a user model,   is viewed as a probability that a user would be satis ed with a judged relevant document; 1  could alternatively be viewed as representing the user s tolerance for redundancy.
As   is decreased, the user becomes more willing to accept documents about previously seen intents.
Various forms of cascade measures that have been proposed in prior work employ di erent versions of the discount value Di.
The two cascade measures studied in this paper are the expected reciprocal rank measure (ERR) [4] and the rank-biased precision measure (RBP) [12].
ERRj employs a linear discount of Di = i; and RBPj employes an exponential discount of Di = (1/ )i 1, where 0       1.
i=1 From the perspective of a user model, the ratio Di/Di+1 represents the probability that a user examining the document at rank i will continue on to examine the document at rank i + 1.
For the exponential discount, this probability is constant at all ranks Di/Di+1 =  .
For the linear discount, the probability increases at deeper ranks.
Given a query with M intents, Clarke et al. [7, 8], Agrawal et al. [1], and Chapelle et al. [4] all model diversity by assigning a probability  j, 1   j   M , to each intent, indicating the probability that a user entering the query is seeking information related to intent j.
Agrawal et al. [1] propose intent-aware versions for a family of measures, each based on a traditional measure such as nDCG or AP.
The traditional measure is applied to each intent independently and the results are combined to give the expected value of the measure across all users.
With respect to combining novelty and diversity together, Clarke et al. [8] and Chapelle et al. [4] suggest measuring novelty

 i=1 ERRj = RBPj = j(1    )ci j  gi i j(1    )ci j  i 1  gi , (4)  DCG = independently for each intent and then combining individual intent scores into a single overall score according to the diversity underlying the query.
We use this de nition to produce the intent-aware version of either traditional or cascade measures, consistent with the de nition of Agrawal et al. [1].
The overall intent-aware score I over a query with M intents may be expressed as follows:
  jSj (5)
 j=1 where Sj represents the value of a cascade measure (i.e. Cj from Equation 3) or the value of a traditional measure (i.e.
Tj such as nDCG or AP) computed over the intent j.
With respect to this de nition, two intent-aware measures studied in this paper, ERR IA and NRBP, are de ned as the intent-aware versions of respectively ERR and RBP by combining Equations 4 and 5 as follows:
  j   ERRj = j(1    )ci j  gi i (6)

 j=1

 j=1  j

 i=1
  j   RBPj =  j j(1    )ci j  i 1  gi j=1 j=1 i=1 The third intent-aware measure, studied in this paper, is  -DCG according to the de nition proposed by Clarke et al.
in [7].
In this de nition, the discount function of traditional nDCG is applied, while instead of using the graded values of traditional nDCG the cascade model is used to compute the gain values as detailed above.
They justify  -DCGj as the weighted linear combination of novelty scores computed over each individual intent j, which can be incorporated to the intent-aware form presented in Equation 5 in order to compute  -DCG across all intents:


  j    DCGj =  j j=1 j=1 i=1  gi j(1    )ci log2 i j (7) Overall, the interest of this paper is to study the informativeness of these newer measures.
We compare these measures to traditional average precision (AP), as well as an intent-aware version of average precision (AP IA).
In the next section, we present the theory behind our work, followed by an experimental evaluation based on a test collection created by the TREC 2009 Web Track [5].
The maximum entropy method (MEM), as described by Aslam et al. [2], attempts to reconstruct a distribution of relevance values for the documents in a results list, in part by applying the value of an e ectiveness measure as a constraint on the distribution.
This approach uses entropy [9] as a measure of uncertainty.
The approach attempts to compute the distribution that maximizes entropy, subject to a set of constraints, which Aslam et al. view as a reasonable estimate of the actual distribution.
Using this approach, Aslam et al.
evaluate various traditional e ectiveness measures, including average precision.
Noting how closely a measure can predict the relevance of documents in a retrieval list and estimate other measures for that list, they attempt to quantify the informativeness of a measure according to its ability to predict the actual relevance distribution.
to Cascade and Intent-Aware Measures The maximum entropy framework is extended in this work in order to analyze the informativeness of the cascade and the intent-aware measures.
Consider a list of documents corresponding to the output of a search engine for a given query.
The maximum entropy method looks for the answer to this question: Can we predict the probability of seeing a document at some rank that is relevant to a given query?
The  rst extension is applied here with respect to the cascade and intent-aware measures by changing the question to predict the probability of seeing a document at some rank that is relevant to speci c intents of a query.
Given a list of N documents retrieved for a query with M intents, there are 2M N possible patterns for the relevant documents in the retrieved list.
Maximizing entropy given a set of constraints dictates the most random distribution (i.e. the most reasonable distribution) over these possible lists.
Under the independence assumption, the probability distribution over the relevance values associated to a document list of depth N for a query with M intents can be written as follows: P1,...,M (r1, ..., rN ) = p1(r1)...pM (r1)...p1(rN )...pM (rN ) where pj(ri) is the probability that the document at rank i is relevant to the jth intent of the query.
For notational convenience, pj(ri) is referred to as pi j in the paper.
Since P1,...,M (r1, ..., rN )) is a product distribution, then:

 H(P1,...,M (r1, ..., rN )) = H(pi j) where H(pi j) is the binary entropy: i=1 j=1 H(pi j) =  pi j log(pi j)   (1   pi j) log(1   pi j) It is noted that the above formulations hold for the general case with multiple intents (i.e.
for the intent-aware measures).
In order to apply it to the non-intent-aware versions of the cascade measures, we set M = 1 and j = 1.
Hence, in the remainder of the paper, j is omitted for non-intent-aware cascade measures.
In order to ensure that the distribution has the appropriate expectation, the expected number of relevant documents is considered as a constraint to MEM.
In this paper, we assume that we are given the expected number of relevant documents (Rj ret) for each intent j.
In addition, the expected value of the target measure is provided to MEM as a constraint.
In order to formulate the setting, we need to formulate the expected value of each measure in terms of the pi j values.
For the traditional measures, the expected value of a measure can be simply calculated based on the probability of relevance values with respect to a single query [2].
Once the expected value of a measure is calculated, it is passed to the optimization problem as a constraint.
Calculating the expected value of the cascade measures and the intent-aware measures such that they can be passed as polynomial constraints to an optimizer is not as straightforward as it is for the traditional measures.
The reason is due to the application of the cascade model in the measures  formula for modeling novelty through considering the dependency of documents in the results list.
The measures in u-enced by the cascade model, include ERR, RBP, ERR IA, NRBP, and  -DCG.
Other intent-aware measures, such as AP IA, do not re ect novelty, and they only consider diversity.
Therefore, they can be simply calculated by averaging over the performance of individual intents.
The performance of each intent can be calculated using the formulations provided in Aslam et al. [2].
In this paper, the cascade and intent-aware measures are adjusted to MEM by calculating their expected value recursively using a dynamic programming algorithm as described below.
Intent-Aware Measures The calculation operates by building a table for each intent j (or for a single intent in case of a non-intent-aware measure).
An example of this table is shown in Figure 1.
The ith row of the table corresponds to the expected value of the measure for the top i documents with respect to intent r,j, represents the expected value of the measure at rank i while r documents (among the top i documents) are relevant to the intent.
The table is  lled by increasing rows, top to bottom in order to take into consideration the inter-dependency of document relevance through a document gain value that depends on the relevance of documents appearing higher in the results list.
At each point, the value for a cell of the table is calculated based on the value of the cells in the previous row and with respect to the probability of relevance value (i.e. pi j) of the document at the current rank (i.e. i), as follows: ( T i r,j = r 1,j +  Gi j) + (1   pi j) T i 1 r,j j (T i 1 pi
 if i   r if i < r (8) Figure 2: Maximum entropy setup for a target cascade or intent-aware measure.
j is derived from the discounted gain value Gi where  Gi j as explained in Section 3.
The rationale behind this recursive formula comes from the earlier idea of measuring novelty.
To achieve the aim of measuring novelty for each intent of a query, redundancy is penalized through the application of the cascade model in which the dependency of documents in a results list is considered.
Given that r out of the top i documents are relevant, there are two possibilities at rank i: 1) the document at rank i is relevant (re ected by the  rst part of the recursive formula in Equation 8), or 2) it is not relevant (re ected by the second part of the formula).
j The  rst part of the recursive formula considers the chance of seeing a relevant document.
The expected value of the measure under this condition is the probability of seeing a relevant document (i.e. pi j) multiplied by the value of the measure at the previous rank with one less relevant document (i.e, T i 1 r 1,j) plus the gain at this point that a relevant j(1 )ci document is seen.
From Equations 1 and 2, Qi j = r   1, since the number of relevant documents is with ci r   1 up to this point in the recursive calculation.
The gain value is then discounted by Di as described in Section 3.
Therefore, The second part of the formula considers the chance of seeing a non-relevant document (1   pi j).
In this case, no gain is obtained and the e ective value of the measure will remain the same as the value at the previous rank with r relevant documents (i.e. T i 1 r,j ).
j = (1    )(r 1)/Di in Equation 8.
 Gi j = pi ret,j For a given intent j, the number of relevant documents among the top N documents is given as Rj ret, the expected value of the measure is estimated as T N .
It is noted that Rj if the measure is a cascade measure, there will be one table for the query (j = 1), and therefore the expected value of the measure is EC = T N Rret where Rret is the given number of documents relevant to the query.
For an intent-aware measure with M intents, there will be one table per intent.
The expected value of the measure for the top N documents is then calculated as a linear combination of the expected values of the individual intents:
 j=1



 Rj ret,j (9) Following the conventions of the TREC 2009 Web Track, the probability assigned to each intent is assumed to be equal, and therefore  i = 1 M in the equation above.
The formulations presented above all form constraints to the optimization problem.
Constrained nonlinear optimization [13] is used to solve these optimization problems.
The general form of the optimization problem is shown in Figure 2, where EC and EI are the expected values for the target cascade measure or the intent-aware measure respectively, which is passed to the optimization problem as a constraint along with the expected number of relevant documents for each intent (or for a single intent in case of the non-intent-aware measures).
Recursive calculation of EC and EI leads to a polynomial formulation for each measure.
In order to obtain the polynomial expected value, the closed form of the recursive formulation is calculated for each measure.
This is performed once for each measure by traversing the recursive formula and outputting the parameters in string form at each step until the base case is reached.
The resulting string is stored as the polynomial form of the expected value for the measure.
The parameters in the string are then substituted with their corresponding values at each iteration of the optimization program.
The recursive form of the measure helps us to  nd the derivative for the second constraint recursively, passing it to the optimization setting in the same way as the original constraint.
Overall, the problem of evaluating the cascade measures and the intent-aware measures using the MEM may be expressed as follows:   MEM for the Cascade Measures: Given a list of N documents with respect to a single query, one is told that the expected number of relevant documents for the query is Rret.
The expected value of the target cascade measure is also given as EC , and one is asked the probability of relevance of each document to each intent under the independence assumption.
By applying MEM, one can determine the probability distribution (called the probability-at-rank-query distribution) that maximizes entropy.
  MEM for the Intent-Aware Measure: Given a list of N documents with respect to M intents of a query, one is told that the expected number of relevant documents for intent j is Rj ret.
The expected value of an intent-aware measure is also given as EI , and one is asked the probability of relevance of each document with respect to each intent under the independence assumption.
By applying MEM, one can determine the probability distribution (called the probability-at-rank-intent distribution) that maximizes entropy.
(b) RBP Figure 3: Maximum entropy setup for the target cascade measures.
The cascade measures that are targeted in this work are RBP and ERR.
The expected value of each cascade measure can be formulated according to the recursive formula above (Equation 8).
The gain and discount values used in the recursive formula are obtained according to the general de nition of the measures presented in Equation 4.
Casting the expected value of measures into the general form of MEM (Figure 2) will give us the problem formulations depicted in Figure 3.
ERRret and RBPret listed in Figures 3a and 3b are respectively the given values of ERR and RBP for a list of N documents that are passed to the optimizer.
The intent-aware measures targeted in this paper are the ones in uenced by both novelty and diversity: ERR IA, NRBP, and  -DCG.
Applying the recursive formula from Equation 8 and averaging over intents through Equation 9 for these measures and passing each to the general form of MEM (Figure 2) give us the problem formulations depicted in Figures 4a, 4c, and 4b.
Again note that the gain and discount values used in the recursive formula are obtained according to the general de nition of the measures presented earlier in Equations 6 and 7.
ERR IAret, N RBPret, and  DCGret listed in Figures 4a to 4c are respectively the given values of ERR IA, NRBP, and  -DCG with respect to M intents and for a list of N documents that are passed to the optimizer.
Using the maximum entropy probability distribution (probability -at-rank-query distribution for cascade measures and probability -at-rank-intent distribution for intent-aware measures) obtained by solving the optimization problem corresponding to a given measure, we should be able to infer the quality of a retrieval list along with the value of other cascade and intent-aware measures.
As noted by Aslam et al. [2] and Yilmaz et al. [19], if a target measure can accurately predict the relevance probability of a document to a query, one should be able to estimate the actual precision-recall curve and also predict the value of other measures for the list given the value of the target measure.
They use this approach to evaluate the informativeness of traditional retrieval measures.
In this section, we report the results of equivalent experiments on the cascade and intent-aware measures.
Speci cally, we evaluate the performance of the cascade and intent-aware measures in terms of i) estimating precision-recall curves and ii) predicting the values of other cascade and intent-aware measures.
(a) ERR IA (b) NRBP (c)  -DCG Figure 4: Maximum entropy setup for the target intent-aware measures.
The experimental data collected by diversity task of TREC
 The corpus for this track was crawled from the general Web in early 2009 and contains roughly one billion Webpages1.
As part of the Web Track, the TREC organizers developed 50 queries, with explicitly de ned intents, which were given to the track participants for execution over their systems.
After executing the queries, the participants submitted ranked lists of documents to the TREC organizers, submitting a total of 49 experimental runs.Hired assessors made binary relevance judgments with respect to each intent.
For our experimental study, all measures are computed to retrieval depth N = 10.
Following the conventions of the Web Track, a default value of   = 0.5 is used for all measures, and a default value of   = 0.8 is adopted for NRBP and RBP (a relatively patient user).
To compute non-intent-aware measures, we ignore the distinction between intents and treat a document as relevant to a query if it is relevant to any intent.
We sorted the 49 submitted runs based on the total number of relevant documents returned for each.
The top 30 runs were selected for our experiments; the remaining runs did not appear to achieve su cient performance to permit a meaningful analysis.
Similarly, we sorted the 50 queries based on the total number of documents judged as relevant for each.
The top 25 queries were selected for our analysis; the remaining queries did not appear to have su cient relevant documents to permit a meaningful analysis.
As seen in Figures 3 and 4, there are  ve optimization problems to be solved for each query of each run (one for each measure).
These  ve optimization problems have been solved for each of the 25 selected queries and for each run in the set of 30 selected runs, giving a total of 5 25 30 = 3, 750 optimization problems.
For each triple <measure, query, run>, the probability distribution values (i.e. the pi j values) are obtained by solving the maximum entropy optimization problem corresponding to that measure and query and with respect to the measures calculated from the run.
Using these values, the precision-recall curve for each triple of <measure, query, run> is estimated and compared against the corresponding actual precision-recall curve.
Further details regarding this process can be found in Aslam et al. [2].
In Figure 5, we demonstrate an example of the predicted precision-recall curves with respect to the maximum entropy distribution for the intent-aware measures against the precision-recall curve of the actual results list.
For any run with a total of Rret relevant documents returned for a query, the recall points after Rret/R are not of the interest to us (points after 0.14 for the example shown in the  gure).
It can be seen in Figure 5 that AP IA and ERR IA estimate the actual precision-recall curve better than the other
 ERR IA NRBP  -DCG AP IA







 Figure 5: An example of the inferred precision-recall curve for the intent-aware measures against the actual precision-recall curve.
two measures.
Of course, this observation holds only for this example.
To make general observations, we need to consider the measures across the 25 queries and 30 runs.
In order to evaluate how well a measure can estimate the actual precision-recall curve, we calculate the root mean square error (RMS) and the mean absolute error (MAE) between the estimated and actual precision-recall curves at the points where the recall changes and then averaged over the runs.
Table 1 summarizes the errors in inferring the precision-recall curve with respect to the intent-aware and cascade measures.
The results based on AP and AP IA are also reported in the table for comparison purposes.
Both errors are signi cantly lower for the intent-aware measures as compared to the cascade measure and the traditional measure (AP).
Consistent with Figure 5, again AP IA and ERR AI appear to be good predictors, along with the rest of the intent-aware measures.
The pi j values of the maximum entropy distribution based on a target measure may be used to infer the expected value of other measures.
In order to evaluate the performance of such predictions, the value of each predicted measure for a run is averaged over the 25 queries.
As an example, for run #1 and the target measure ERR IA, the optimization problem shown in Figure 4a is solved 25 times, each time based on the constraints obtained for one of the 25 queries.
The results of each optimization problem are then used to estimate the value of other measures.
For instance, NRBP is calculated from the result of each optimization problem and then averaged over the 25 queries.
Finally, in order to evaluate how well ERR IA can predict NRBP, for each run, Kendall s  

 Kendall s  









 w.r.t.
ERR IA



  -DCG AP IA





 w.r.t.
NRBP



 w.r.t.
AP IA w.r.t.
 -DCG














  -DCG


  -DCG


 Table 3: Kendall s   correlation and the relative errors for prediction w.r.t.
the cascade measures comparing with the ones for prediction w.r.t.
AP.
w.r.t.
ERR w.r.t.
RBP w.r.t.
AP


































 Kendall s  

 the obtained NRBP value is compared against the average of the actual NRBP value calculated using the 25 queries for that run.
This process is repeated for each run and each target measure, in order to estimate the value of other measures.
Note that each cascade measure is used to predict the rest of cascade measures plus AP (as an example of a traditional measure) while each intent-aware measure is used to predict the rest of the intent-aware measures including
 Figures 6 to 10 plot the inferred value (corresponding to the maximum entropy distribution obtained based on each target measure) against the actual value for each run averaged over the queries.
We report Kendall s   to measure the correlation of an inferred measure and its actual value.
Values range from 1 to +1, with +1 indicating perfect agreement and 1 indicating the opposite.
In addition to evaluating the prediction results in terms of ranking, value-based error measures (i.e. RMSR and MARE) are used to evaluate the results.
Root mean square relative error (RMSR) and mean average relative error (MARE) are chosen in order to measure the relative error values across the results of various retrieval measures.
Tables 2 and 3 summarize the correlation and error measures for the results of the intent-aware measures and the cascade measures respectively.
There is relatively high correlation among the cascade-based intent-aware measures shown in the plots.
For instance, in Figure 6c,  -DCG values inferred based on the maximum entropy distribution corresponding to ERR IA (as the target measure) appear to be highly correlated with the actual value of  -DCG along 30 runs.
They are correlated with a Kendall s   value of 0.905 according to Table 2.
On the other hand, the cascade measure ERR, for which no notion of diversity is considered, results in a maximum entropy distribution with a lower correlation reported between the inferred DCG and the actual value of DCG along the 30 runs (Kendall s   = 0.851 according to Table 3).
Similar results are obtained from prediction based on NRBP versus prediction based on RBP.
This outcome could indicate that, although the extended MEM works well in predicting the value of the cascade/traditional measures based on a target cascade measure, it works substantially better when the intent-aware measures are used as target measures.
In other words, the intent-aware measures are found to be more informative than their non-intent-aware versions (cascade metrics).
Moreover, among the intent-aware measures, ERR IA and NRBP are found to be the more informative measures in predicting the value of other measures.
Comparing this set of results with the one reported earlier in Table 1, it appears that AP IA can compete with the intent-aware measures in terms of predicting the precision-recall curve of the actual list.
This is true to a lesser extent, according to Table 2, when AP IA is compared against the rest of the measures for inferring other retrieval measures.
This could be due to the fact that average precision directly incorporates precision, and therefore it naturally should be a good predictor of the precision-recall curve.
Overall, the intent-aware measures that re ect both novelty and diversity are found to be informative in terms of both inferring the precision-recall curve and predicting other retrieval measures.
In this paper, we extend the MEM framework to cascade and intent-aware measures.
By determining the extent to which a target measure can predict an actual retrieval list and estimate the value of other measures on that list, we may quantify the informativeness of that measure.
The runs created through the TREC 2009 Web track provide a vehicle for exploring and validating the cascade and intent-aware measures using the MEM framework.
Our experimental comparison contains two parts: i) inferring the actual precision-recall curve based on the maximum entropy distribution obtained from a target measure, and ii) estimating the value of other measures based on the relevance probability values obtained with respect to a target measure.
Our results indicate that the intent-aware measures that re ect both novelty (through the application of the cascade model) and diversity are informative in terms of predicting both the precision-recall and the value of other measures.
They are found to be more informative than their cor-(b) NRBP (K s   = 0.835) (c)  -DCG (K s   = 0.905) Figure 6: Prediction based on ERR IA, TREC 09.
(a) AP IA (K s   = 0.830) (b) ERR IA (K s   = 0.814) (c)  -DCG (K s   = 0.854) Figure 7: Prediction based on NRBP, TREC 09.
(a) AP IA (K s   = 0.815) (b) ERR IA (K s   = 0.802) (c) NRBP (K s   = 0.809) Figure 8: Prediction based on  -DCG, TREC 09.
(a) AP (K s   = 0.767) (b) RBP (K s   = 0.824) (c) DCG (K s   = 0.851) Figure 9: Prediction based on ERR, TREC 09.
(a) AP (K s   = 0.785) (b) ERR (K s   = 0.797) (c) DCG (K s   = 0.849) Figure 10: Prediction based on RBP, TREC 09.
evaluation of IR techniques.
ACM Transactions on Information Systems, 20(4):422 446, 2002.
[12] A. Mo at and J. Zobel.
Rank-biased precision for measurement of retrieval e ectiveness.
ACM Transactions on Information Systems, 27(1):1 27,
 [13] M. Powell.
A fast algorithm for nonlinearly constrained optimization calculations.
Numerical Analysis, pages 144 157, 1978.
[14] F. Radlinski, M. Szummer, and N. Craswell.
Metrics for assessing sets of subtopics.
In 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages
 [15] S. Robertson, E. Kanoulas, and E. Yilmaz.
Extending average precision to graded relevance judgments.
In
 Research and Development in Information Retrieval, pages 603 610, 2010.
[16] T. Sakai.
Evaluating evaluation metrics based on the bootstrap.
In 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, page 532, 2006.
[17] T. Sakai.
On the robustness of information retrieval metrics to biased relevance assessments.
Information and Media Technologies, 4(2):547 557, 2009.
[18] M. Sanderson and J. Zobel.
Information retrieval system evaluation: e ort, sensitivity, and reliability.
In
 Research and Development in Information Retrieval, pages 162 169, 2005.
[19] E. Yilmaz and S. Robertson.
On the choice of e ectiveness measures for learning to rank.
Information Retrieval, 13(3):271 290, 2010.
[20] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson.
Incorporating user behavior information in IR evaluation.
In SIGIR 2009 Workshop on Understanding the User: Logging and Interpreting User Interactions in Information Retrieval, 2009.
[21] Y. Zhang, L. Park, and A. Mo at.
Click-based evidence for decaying weight distributions in search e ectiveness metrics.
Information Retrieval,
 responding non-intent-aware cascade measures, which only consider novelty.
As seen in the  rst part of our experiments, AP IA appears to be competitive with the rest of the intent-aware measures in terms of its ability to estimate precision-recall curves, while it does not appear to be as good at predicting the values of cascade measures.
We may explain this observation from the fact that AP directly incorporates precision, and therefore it naturally should be a good predictor of the precision-recall curve.
We based our default   and   parameters on those used in the TREC 2009 Web track.
As future work, we plan to further explore the performance of the measures by varying the value of these parameters.
Also, future TREC Web Tracks will help us to further evaluate our techniques.
Finally, we hope to evaluate the cascade and intent-aware measures in terms of the user behavior seen in search logs [20, 4].
