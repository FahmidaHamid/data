Set expansion refers to the practical problem of expanding a small set of  seed  entities, into a more complete set by discovering other entities that also belong to the same  concept set .
Here a  concept set  can be any collection of entities that conceptually form a set that people have in mind, and  seeds  are the instances of entities in the set.
As an example, a person wanting to discover all camera brand names may give a small number of well-known brand names like  Canon  and  Nikon  as seeds, the set expansion techniques would leverage the given data sources to discover other camera brands, such as  Leica ,  Pentax  and  Olympus  that are also camera brands.
Set expansion systems are of practical importance and can be used in various applications.
For instance, web search engines may use the set expansion tools to create a comprehensive entity repository (for, say, brand names of each product category), in order   Work done at Microsoft Research   Work done at Microsoft Research.
(dongxin@google.com).
Now with Google Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
to deliver better results to entity-oriented queries.
As another example, the task of named entity recognition can also leverage the results generated by set expansion tools [13].
Considerable progresses have been made in developing high-quality set expansion systems.
The most relevant efforts include Google Sets [1], which employs proprietary algorithms to do set expansions.
However, due to its proprietary nature, algorithms and data sources behind Google Sets are not publicly available for future research endeavors.
Another prominent line of work is the SEAL system [16, 17,
 customized text wrappers based on the input seeds in order to extract candidate entities from web pages in a precise manner.
They then use a graph-based random walk to rank candidates entities based on their closeness to the seeds on the graph.
While they have demonstrated that this customized data extraction/ranking process can produce results with high quality, the necessary online data extraction can be costly and time-consuming.
Here we study the problem of conducting set expansion using general web data sources without resorting to online data extractions speci c to the given seeds.
In particular, we look at two common types of web data: the HTML lists extracted from web pages by web crawls (henceforth referred to as the Web Lists) and the web search query logs (the Query Logs).
We model both types of data using bipartite graphs to provide a uni ed computation model.
Such general-purpose web data can be highly useful for set expansion tasks: they are very diverse in nature, with rich information that covers most domains of interest.
In addition, since these general data are not domain/seed speci c, they can be pre-processed and optimized for ef ciency purposes.
However, these general web data can be inherently noisy.
Random walk or other similarity measures along may not be suf cient to distinguish true results from the noises, especially when the number of seeds are limited.
As we observe in our experimental evaluations, that random walk based ranking techniques used in previous work perform poorly on the general-purpose Web Lists or Query Logs and produce results with low precision/recall.
Partly because of that, previous approaches [16, 17, 18, 19] use seed-speci c and page-speci c wrappers to reduce the candidate set to a smaller and much cleaner subset over which the random walk based ranking techniques work reasonably well.
However, we note that this additional data extraction process is at the cost of overall architectural complexity and system responsiveness.
Unlike previous approaches, we in this work propose a general framework that only uses general-purpose web data without resorting to online data extractions speci c to the given seeds.
In particular, while previous random walk based approaches leverage the intuition that candidates close to the given seeds in the graph struc-we take an alternative tack and propose to measure the quality of an expanded set of entities relative to the given set of seeds in a more straightforward and comprehensible way.
Intuitively, a set of expanded results is  good  if it has two key properties: (1) the set of produced entities are similar to the given seeds; (2) the set of produced entities are coherent in the sense that they represent a consistent concept.
We abstract the intuitions and de ne quality of the result set as the sum of two component scores: the Relevance of a set of entities that measures their similarity with the given seeds, and Coherence of the set of entities produced which is how closely the entities in this set are related to each other.
Based on this quality measure, we further develop a class of iterative set expansion algorithms for which we call SEISA (Set Expansion by Iterative Similarity Aggregation).
We show that SEISA is robust and effective in producing expanded sets over noisy web data sources.
The rest of the paper is organized as follows.
We summarize the related work in the space of set expansion in Section 2.
We then formally de ne the problem of set expansion using the new quality metric we propose in Section 3.
In Section 4 we detail our iterative set expansion algorithms.
Finally we present our experimental results in Section 5 and conclude this paper in Section 6.
There is a signi cant body of related work in the broad space of information extraction and named entity extraction.
We will only summarize work most relevant to set expansion due the limit of space.
Wang and Cohen [16, 17, 18, 19] developed the SEAL system for set expansion, using a two-phase extraction/ranking architecture.
In the  rst extraction phase, they build for each web page customized wrappers using maximal left/right context that would enclose all given seeds, which are in turn applied on the web page from which they are constructed to extract candidate terms in addition to the given seeds.
In the second ranking phase, web pages, wrappers and candidate terms are modeled as nodes in the graph, and random walk techniques are used to rank candidates based on their structural proximity to the seeds in the graph.
In comparison, our approach ranks a set of candidates as a whole based on its relevance and coherence, and does not require page-speci c and seed-speci c data extraction process.
Agichtein et al. [4] introduce the Snowball system that bootstraps from a small number of input tuples, by  rst obtaining typical contextual patterns of the seed tuples from the web pages, which are used in turn to extract more tuples.
While Snowball is well suited for extracting certain types of structured data like binary relations, it may not work well for set expansion due to its reliance on textual context patterns (sets can be viewed as unary relations of tuples, whose context can be much more dynamic and less predictable than that of binary relations).
Etzioni et al. [6, 7] develop the KnowItAll system that automatically extracts facts from the web using textual patterns like  cities such as Paris, London and New York  to extract candidate entities.
Candidates are then ranked in a bootstrapping manner using statistical information gathered from the search engine such as PMI over hit counts.
Talukdar et al. [14] study the problem of set expansion from open text.
They propose to automatically identify trigger-words which indicate patterns in a bootstrapping manner.
Ghahramani et al. [8] uses Bayesian inference to solve the problem of set expansion.
It has been shown in [18] that this candidate ranking mechanism is comparable to random walk in quality of the results of the expanded set.
Canon Nikon Leica

 List01 List02 List03 List04 List05 Canon Nikon Leica

 camera japan laptop dealer mpg (a) Web List Data (b) Query Log Data Figure 1: Bipartite graph data model Set expansion is also somewhat related to the problem of class label acquisition [15, 20] where the goal is to propage a set of class labels to data instances using labeled traning examples.
While the set expansion problem can be modeled as propagating class labels associated with seeds to candidate entities, we observe that a large number of training examples is necessary in order for these techniques to be effective.
Finally Google Sets [1] does set expansions using propriety algorithms which are not publicly available.
In this work we target general web data sources.
Speci cally, we look at lists extracted from the HTML web pages (the Web List data), and the web search query logs (the Query Log data).
In each case, we model the data as bipartite graphs as in Figure 1, with candidate terms being nodes on one side (henceforth referred to as term-nodes) and their contexts on the other side.
Since we use textual terms in the web data as candidate entities for the expanded set, from this point on we will be using the word  term  interchangeably with  entity .
Web List Data.
For Web List as in Figure 1a, each unique web list crawled from the web, ( List01 ,  List02 , etc), is modeled as a node on the right-hand-side, while each term that appears in those web lists is modeled as a term-node on the left hand side.
In this example, the underlined nodes  Canon  and  Nikon  on the left are the seed terms, while the remaining terms, including  Leica ,  VW  and  BMW , are possible candidate terms.
There is an edge connecting a term-node with a list-node if that term is a members of the list.
For example, the list-node List01 connects to  Canon ,  Nikon  and  Leica , indicating that all three terms are members of List01, which is probably a web list on some web page that enumerates a list of camera brands.
While it is possible to resort to additional information of the web data to assign different weights to each edge (using the quality of the page from which the list is extracted, for example), we in this work adopt a simpler approach that only assigns a uniform weight of 1 to each edge in the Web List graph.
We in our experiments  nd this approach to work quite well.
Query Log Data.
Query log data is modeled as in Figure 1b.
Here for each keyword query, we break up the query into two parts, the term and the context.
The context is a pre x or suf x of the query up to 2 tokens, and the term is the remainder of the query.
Each term is again modeled as a graph node on the left, the context is modeled as a node on the right.
There are various ways in which we can model edges in the graph for query log data.
In this work we assign weight of the edge be-query term and query context, which is de ned in De nition 1.
DEFINITION 1.
Let    ( ) be the probability that term   occur in the query log,    ( ) be the probability that context   occur in the query log, let    ( ,  ) be the probability that the term   and context   co-occur in the query log.
The Mutual Information H(t, c) is de ned as:  ( ,  ) =    ( )   ( )    ( , ) Furthermore, we only keep the edge between a pair of nodes if the Mutual Information between the term and the context (or the weight on the edge) is positive, and additionally, the co-occurrences of the term and the context is frequent enough to be above certain threshold.
Figure 1b is an example of the resulting bipartite graph after this simple processing.
In general, we feel that this bipartite graph model is straightforward and general enough to be applied and extended to other types of data sources.
With this bipartite data model, intuitively the overall task of doing set expansion given a set of seeds can to an extent be viewed as the problem of  nding term-nodes that are similar to the given seed-nodes, using the right hand side nodes as the features.
In order to measure similarities between the term-nodes, common similarity metrics, like Jaccard Similarity [11] and Cosine Similarity [11] as de ned below, can all be used.
DEFINITION 2.
[11] Let  ,   be two term-nodes on the left hand side.
Let   and   be the two sets of right side nodes that connect to node   and  , respectively.
The Jaccard Similarity of   and  ,   denoted as  ( ,  ), is de ned as  ( ,  ) =   .
DEFINITION 3.
[11] Let  ,   be two term-nodes on the left hand side.
Let   and   be the weight vectors that indicate the weights of the edges that connect web lists to node   and  , respectively.
The Cosine Similarity of   and  , denoted as  ( ,  ), is de ned as  ( ,  ) =     .
We use the following two examples as simple illustrations of the Jaccard similarity and Cosine similarity.
EXAMPLE 1.
We  rst illustrate the computation of Jaccard similarity of two term-nodes in our bipartite graph model.
In Figure 1a, the term-node  Canon  connects to list nodes   = { List01 ,  List02 }; while the term-node  Leica  connects to nodes   = { List01 ,  List02 ,  List03 ,  List04 }.
By De nition 2, the Jaccard similarity between the   seed-nodes  Leica  and  Canon  is   =
 = 0.5.
Similarly, the similarity between  Leica  and the other
 seed-node  Nikon  is also 2
 On the other hand, the Jaccard similarities between  VW  and both of the seed-nodes  Canon  and  Nikon  are 0 = 0.
There-
fore, using the Jaccard Similarity de nition, the term  Leica  is more similar to both seeds than the term  VM .
= 0.5.
Next we show how Cosine similarity between term-nodes is computed.
In Figure 1a, the term-node  Canon  connects to   ={ List01 ,  List02 }, its edge weight vector  Canon  is thus (1, 1, 0, 0, 0).
By the same token, the edge weight vector for  Leica  is  Leica  = (1, 1, 1, 1, 0).
According to De nition 2, the Cosine similarity between nodes  Leica  and  Canon  is     = 2 = 0.71.
Similarly, the similarity between  Leica  and the other seed-node  Nikon  is also
 = 0.71.
(a) A set with high relevance (b) A set with high coherence Figure 2: Quality of an expanded set The Cosine similarities between  VW  and both of the seed-nodes  Canon  and  Nikon  are 0 due to the lack of overlap in the right side list-nodes.
We again have the term  Leica  to be more similar to seeds than the term  VM .
While in this section we only discuss two most commonly used similarity metrics, the Jaccard Similarity and the Cosine Similarity, we emphasize that the set expansion framework to be introduce in detail in Section 4 is general and extensible enough that any other similarity metrics can be easily plugged in to be used.
Furthermore, in our experimental evaluations, we  nd that the performances of set expansion using both similarity metrics are reasonably good, underlining the generality of the framework we propose.
While the previous work uses techniques like random walk to rank individual terms based on their graph structure similarity to the given seeds, we see the expanded set of entities as a whole and propose a simple and intuitive metric to measure the quality of the expanded set, as will be detailed in this section.
The  rst observation we have is that, the more similar the expanded entities are to the given seed entities, the better quality the expanded set.
This is intuitive because after all the task of set expansion is to  nd entities that are in the same  concept set  as the seeds, which by de nition should be somewhat similar to the seeds.
We formalize this observation with the following de nition of relevance to capture the similarity between the expanded set and the seed set.
DEFINITION 4.
Let   be the universe of entities,       be the expanded set, and       be the seed set.
Let   :         [0, 1] be the function that measures the similarity of any two entities.
The relevance of   with respect to   is de ned as:  ( ,  ) =

        ( ,  ) To better illustrate this de nition of relevance, we use Figure 2 to graphically demonstrate the quality of the expanded set.
In both Figure 2a and Figure 2b, the two solid dots in the middle represent the given seed set  , while the circles surrounding these two dots are the derived entities that constitute the expanded set  .
The similarity of any two entities is then represented as the distance of these two dots in the graph.
It is clear that in both of these two  gures, the expanded set of entities as circled by the dashed oval are very similar (or graphically speaking, close) to the two given seeds.
So in terms of our relevance metric, both of the two sets in Figure 2a and Figure 2b have high relevance to the given seeds.
However, we observe that this de nition of relevance alone does not fully capture the quality of the expanded set.
The reason for a consistent  concept set  that are very similar to the given seeds, there could be cases where a set of entities are similar to the seeds but not consistent enough to be a coherent concept set.
As an example, in Figure 2a, while the expanded entities as denoted by the circles are close to the given seeds, they are relative dispersed in the space and may not form a consistent  concept set  as required by set expansion.
On the other hand, the the expanded entities in Figure 2b are not only equally close to the given seeds as in Figure 2a, they are also much closer to each other to form a consistent  concept set .
Thus, the expanded entities in Figure 2b may be a better candidate for the expanded set than the entities in Figure 2a.
To capture the intuition in Figure 2b that the closer the entities in the expanded set are to each other, the more coherent and thus better the set as a whole is, we formally de ne the notion of coherence in the following.
DEFINITION 5.
Let   be the universe of entities,       be the expanded set,   :         [0, 1] be the function that measures the similarity of any two entities.
The coherence of   is de ned as:



  =1  >   ( ,  )  ( ) = where  ,      .
Based on the observation that both relevance and coherence contribute to the quality of an expanded set, we de ne the quality of the expanded set as the weighted sum of relevance and coherence as follows.
DEFINITION 6.
Let   be the universe of entities, Let       be the expanded set,       be the seed set.
Let 0       1 be the constant weight factor.
The quality of the expanded set   with respect to the seed set  ,  ( ,  ), is de ned as:  ( ,  ) =      ( ,  ) + (1   )    ( ) Here,   is a constant weight that balances the emphasis between relevance and coherence.
In our experiments that we will detail in Section 5, we  nd   = 0.5 to be a good value in practice, and this intuitive yet simple de nition of quality works well in the extensive experiments that are conducted.
With the de nition of quality metric, we formally state the problem as follows.
Given the universe of candidate terms   and some seeds      ; given a similarity function   :         [0, 1] that measures the similarity of any two terms, identify the expanded seed set  ,       and is of size  , such that the objective function  ( ,  ) is maximized, where  ( ,  ) =      ( ,  ) + (1   )    ( ) Intuitively, the expanded seed set, or the ESS, is the core component of the concept set that we want to expand, and consists of entities that we know with high con dence that belong to the desired concept set.
We say an ESS is good if its quality score is high.
Once a good ESS (denoted as  ) is derived, individual terms   can then be ranked based on   and the seed set   using the ranking function  ( ,  ,  ), which is again a straightforward combination of relevance score and coherence score as follows.
 ( ,  ,  ) =  ( ,  ) +  ( ,  ) (1)  
  =1 (1    )
  =1 where       and      .
However, we show that the problem of  nding the optimal   of size   with maximum quality score is NP-hard.
THEOREM 1.
Given the seed set  , the problem of  nding an entity set   of size   that maximizes the objective function  ( ,  ) is NP-Hard.
The hardness of this problem can be proved by reduction from the maximum clique problem [9].
Details of the proof can be found in Appendix 8.1.
Given Theorem 1 which states that it is NP-Hard to  nd the optimal expanded seed set (ESS), we in this section propose two greedy algorithms, the static thresholding algorithm and the dynamic thresholding algorithm, that iteratively re ne a candidate ESS   of size   to maximize  ( ,  ) (De nition 6).
Both algorithms are built on top of an automatic score thresholding technique.
In this section, we  rst outline two algorithms, then describe the automatic score thresholding method, and  nally, we discuss the connection of our proposed algorithms and the standard random walk based approaches.
Iterative Similarity Aggregation On the high level, the static thresholding algorithm  xes the size of ESS   at the beginning, and then iteratively searches for terms in   to maximize  ( ,  ); while the dynamic thresholding algorithm re nes both the size of   and contents of   at the same time in each iteration.
The static thresholding algorithm starts with a good guess of ESS, then iteratively improve the quality metric as de ned in De -nition 6 by replacing one entity in the ESS of the previous iteration, until the computation of ESS converges and a local maximum of the quality score is reached.
The pseudo-code of the algorithm is described in Algorithm 1.
Static Thresholding Algorithm The static thresholding algorithm takes two parameters, the set of seed entities,  , and the   with all candidate   as left side nodes.
We start by computing the relevance score of each term with the seeds  ( ,  ), as de ned in De nition 4, in the  rst for loop.
We then rank the terms according to their relevance scores, and pick top   ranked terms as the initial estimate of the ESS,  0, where the threshold value   is determined by a thresholding analysis of the score distribution that will be detailed in Section 4.2.
In the subsequent iterations in the while loop, we iteratively compute the new candidate ESS   based on  1 of the previous iteration and progressively improve the overall quality score of the ESS until a local maximum is reached.
Speci cally, in each iteration  , we compute the relevance score of each candidate term   with the previous ESS  1,  ( ,  1), and the corresponding ranking function  ( ) which is a weighted combination of the relevance score with the  , and the relevance score with  1.
We then sort the candidate terms by    =  1,  ( ).
Let the top ranked   terms be   we replace the lowest ranked term in  1 with the top ranked term         that is not in  1, and continue the iteration; otherwise we have converged and will stop and return  1 as the result of ESS.
 , if   We use the following running example to demonstrate how the static thresholding algorithm works to compute ESS, which can then be used to rank candidate terms for set expansion.
Static_Thresholding ( ,  ) for each   in  .  do  _ [ ]    ( ,  ) end for sort   by  _ [ ] desc     Pick_Threshold( _ [ ])  0   the top   ranked terms by  _ [ ]     1 while true do for each   in  .  do  _ [ ]    ( ,  1)  ( )        _ [ ] + (1    )    _ [ ] end for sort   by  ( ) desc     the top   terms by  ( )
    =  1 then if   let       let       be the last ranked term in  1     ( 1   { })   { }      1 break   be the top ranked term not in  1 else end if   + + end while return  
 EXAMPLE 2.
Let   = { ,  ,  ,  ,  ,  } be the set of 6 terms that we consider in this example, in which   = { ,  } is the input seed set.
Let the pairwise similarity matrix   be (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)





 (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)



















 (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) where each entry stands for the similarity score of the two corresponding nodes using some similarity metric   :         [0, 1].
For each term      , we  rst compute its relevance score with the seed set,  ( ,  ).
For example,  ( ,  ) = (  ( ,  ) +  ( ,  )) = 0.75,  ( ,  ) = 1



   ( ,  )) = 0.75, etc.
This gives rise to the  _ [] = {0.75, 0.75, 0.7, 0, 7, 0.65, 0.6}.
Next we invoke the thresholding algorithm (to be detailed in Section 4.2), which analyzes the score distribution to  nd a natural threshold point that separates the high scoring entities from the remaining background entities.
We use the number of entities above the threshold as the estimate of the size of ESS.
In this particular case, suppose the thresholding algorithm returns   = 4, which gives us  0 = { ,  ,  ,  }.
In the  rst iteration (  = 1), we compute for each term       its similarity score with the previous estimate of ESS,  0, to derive  ( ,  0).
For example,  ( ,  0) = 1

   ( ,  ) +   ( ,  ) +   ( ,  )) = 0.75, while  ( ,  0) =
 (  ( ,  ) +   ( ,  ) +   ( ,  ) +   ( ,  )) = 0.7, so on and so forth.
This leads to  _ [] = {0.75, 0.7, 0.6, 0.6,

 Given the weight constant   = 0.5 as used in the quality metric,    _ [] =    _ [] + 1 the ranking scores  [] = 1

 {0.75, 0.725, 0.65, 0.65, 0.7, 0.55}.
Sorting the terms in   again,

 we get a different ordering ( ,  ,  ,  ,  ,   ).
Let    ,  } be the top ranked   = 4 terms in the new ordering.
Given
 that  
 ranked term in  
 In the second iteration (  = 2), we recompute the similarity score  _ [] = {0.725, 0.7, 0.825, 0.55, 0.8, 0.375}.
Combining that with  _ [] = {0.75, 0.75, 0.7, 0.7, 0.65, 0.6} we have the new ranking scores  [] = {0.7375, 0.725, 0.7625,



 the iteration and use the ranking scores  [] to generate a ranked list of terms, ( ,  ,  ,  ,  ,   ).
THEOREM 2.
The computation of   in Algorithm 1 is guaranteed to converge, thus the while loop in Algorithm 1 is bound to terminate.
Theorem 2 states the nice property that after a  xed number of iterations, the computation of   will converge and stops changing for subsequent iterations.
Here to outline the intuition of the proof of the convergence of our algorithm, we note that in our computation of ESS, we are implicitly maximizing the quality score of ESS.
We show that this quality function will monotonically increase in each iteration, until reaching a local maximum, at which point it will converge and stop.
Details of the proof can be found in Appendix 8.2.
While the algorithm is bound to converge, we don t have an upper bound on the number of iterations it may take before it stops.
However in our experiments, we observe that it converges quickly and typically takes only a small number of iterations (less than 10).
The reason we term this algorithm static thresholding is due to the way the estimated size of ESS,  , is determined.
In this static thresholding algorithm, once threshold   is computed in the  rst iteration, it will stay the same in subsequent iterations.
In the following section, we will present a different variant of the algorithm in which   changes from iteration to iteration.
While the static thresholding algorithm described in Section 4.1.1 is proven to converge, its use of the static threshold (the parameter   in Algorithm 1) as computed in the  rst iteration may not accurately re ect the actual size of the ESS.
It can be the case that in subsequent iterations with iterative score computation it becomes clear that based on the new score distribution, the new threshold value   which is interpreted as the size of the ESS   is signi cantly different from the initial estimate derived from the score distribution for the  rst iteration.
To overcome this issue, we in this section propose a dynamic thresholding algorithm that iteratively use the new threshold value of the current score distribution to adjusts the estimated size of ESS.
The algorithm is described in detail in Algorithm 2.
The structure of this algorithm is similar to Algorithm 1.
We  rst compute the relevance score between each candidate term   and the seeds.
We then again invoke the thresholding procedure to  nd a good threshold value  0, and use the top ranked  0 terms as the initial ESS,  0.
In each subsequent iteration, we again rank each term  , using the ranking function  ( ).
Based on the new score distribution computed using  ( ), we re-invoke the automatic thresholding procedure to determine a new estimate of size of ESS,  .
Observe that instead of using the initial threshold  0 computed in the  rst iteration as in the static thresholding algorithm, Dynamic_Thresholding ( ,  ) for each   in  .  do  _ [ ]    ( ,  ) end for  0   Pick_Threshold( _ [ ]) sort   by  _ [ ] desc  0   the top ranked  0 terms by  _ [ ]     1 while     MAX_ITER do for each   in  .  do  _ [ ]    ( ,  1)  ( )        _ [ ] + (1    )    _ [ ] end for     Pick_Threshold( ( )) sort   by  ( ) desc     the top ranked   terms by  ( )
         + +   end while return   Prob.
Figure 4: Automatic thresholding to segment images image on the left has a hand in the foreground and a dark background.
The task of image segmentation is to separate the foreground hand from the background to get the image on the right hand side.
Each pixel in the image has a gray scale value that again is assumed to follow two distributions: those belong to the foreground and those belong to the background.
In the computer graphics literature, a number of thresholding algorithms have been proposed and shown to be effective, including the Iterative Threshold Selection [12] and Otsu s thresholding [10].
We in this work adopt the Otsu s thresholding to  nd the good threshold point that naturally separates those high scoring terms from those background terms.
Formally, Otsu s threshold is de- ned as follows.
DEFINITION 7.
Let  1,  2 be the probabilities of the two classes separated by a threshold  , and  2 2 be the variances of these two classes.
The weighted sum of the variances of the two classes
 is   ( ) =  2 old   is de ned as the one that minimizes   ( ), or equivalently,   = arg min  ( ) =  1( ) 2 1( ) +  2( ) 2 1,  2 (  ( )).
Score   Figure 3: Thresholding to separate two score distributions we recompute the threshold based on the new score distribution.
This dynamic thresholding technique adapts to the changes in the score distribution and may be able to re ect the size of the ESS more accurately.
In practice we observe that this algorithm slightly outperforms the static thresholding algorithm.
However, since we are dynamically changing the thresholding value in the dynamic thresholding algorithm, we cannot guarantee the convergence property as in the static thresholding algorithm.
Therefore we place a loop-termination condition which is the maximum number of iterations to execute for ef ciency consideration.
In practice we use small number of iterations (e.g., 5) and observe reasonable performance.
The subproblem we look at in this section, is to automatically determine the natural threshold that best separates two underlying score distributions from one score distribution.
This problem arises when we have a set of scores, each of which represents an estimation of the likelihood of the term being a member of the  concept set  we are trying to uncover.
The assumption here is that those terms that really belong to the  concept set  will have higher scores, and follows some kind of score distribution as in the right curve in Figure 3; while those that do not belong to the set will have lower scores, but also follow a score distribution as the left curve in Figure 3.
Under this assumption, the problem becomes the classical score thresholding problem, and we tap into existing literature to solve this thresholding problem.
In particular, the same problem arises in image segmentation in the computer graphics, where the goal is to separate the foreground image from the background images.
For example, in Figure 4, the Brie y, Otsu s technique sees the two sets of scores separated by the threshold as two clusters.
It is based on the observation that the threshold that best separates the two clusters is the point with the least intra-cluster variances.
Therefore, Otsu s threshold-ing uses the sum of the two intra-cluster variances as the objective function, and searches for the point that minimizes the sum of the intra-cluster variances as the threshold.
We observe that in our experiments, this thresholding technique outperforms alternative thresholding techniques we experimented with.
Given a sorted list of scores, the Otsu s threshold can be computed linearly.
Having described the details of our algorithm, here we discuss some major differences compared with existing random walk based approaches.
Our algorithms in its essence take an iterative score computation process, which is very similar to, say random walk based ranking algorithms which also compute scores iteratively.
How is our iterative score computation algorithm different from the general random walk?
More importantly, as we have noted, we observe in our experiments that our algorithm outperforms the random walk based approaches.
It is thus interesting to explore the differences between our approach and the random walk based algorithms that lead to the divergence in performance.
First, we would like to point out the key similarities between our iterative score computation and random walk algorithms.
If we were to build a     similarity matrix   where each entry ( ,  ) denotes the similarity of terms   and   on the left-side of the bipartite graph, this matrix essentially represents a graph between terms (instead of graph between terms and contexts).
In addition, in each iteration where we aggregate score computation, we are essentially doing a matrix multiplication       + (1   )        in which and     is a (0, 1) vector that represents the ESS in the previous iteration.
The computation framework bears some similarity with random walk based approach.
However, we note that there are at least three important aspects that differentiate our algorithm from the random walk based approaches.
First, we use a score thresholding mechanism as discussed in Section 4.2 to  nd a good ESS (equivalently, the multiplication vector   ), which tends to be very small in size (comparing to the set of universal terms), as opposed to normal random walk that does not have such score thresholding.
As the result, we only propagate probability (or, scores) through con dent nodes.
Secondly, we normalize scores in each iteration differently.
The aggregate score we compute is a weighted sum of the score computed against the given seeds and the score computed against the ESS in each iteration (catching both relevance and coherence), which is different from typical random walks that only take care of relevance to the seeds.
Lastly, we treat each entity in the ESS equally and reset their weights to be  1  after each iteration, which is again different from random walks.
Given the speci c set-expansion application we are targeting at, where each entity in the set should be conceptually treated equally, the reset of weight is reasonable given a robust automatic score thresholding mechanism.
It turns out in our experimental evaluation that these aspects are critical to achieving reasonable set expansion performances.
As we have alluded before, we experiment with two types of data, the web lists and the query log.
Both of them are modeled as bipartite graph as described in Section 3.1.
In particular, for the web list data, we use crawlers to extract around 6 million lists in HTML pages from the web, and the resulting bipartite graph has
 of user queries from Bing, and the resulting bipartite graph has 94, 859, 646 edges.
To evaluate the effectiveness of our algorithms, we select four sets of concepts over which set expansion experiments are conducted, namely, country names, colors, camera brands and mattress brands.
The  ground truth , or the set of entities that is considered to belong to each concept set are determined as follows.
For country names and colors, we resort to Wikipedia and use the list of countries in [2] and the list of atomic web browser colors in [3], respectively.
For camera brands and mattress brands, we obtained the manually created lists from domain experts.
We selected these four categories because (1) they are across different domains; and (2) they have different degree of dif culty for set expansion.
In order to compare the performance of the our algorithms and the existing techniques, we randomly pick 6 entities from the ground truth data for each concept set as the input starting seeds.
Since each algorithm returns a ranked list of results, we evaluate the performance of these competing algorithms by measuring the preci-sion/recall values of each algorithm at different rank positions.
We conduct three broad groups of experiments to thoroughly understand the effectiveness of our set expansion algorithms.
In the  rst set of experiments we compare the algorithms proposed in this work with the state-of-art random-walk based ranking algorithms used for set expansion, over the same web list/query log data.
We implemented two random-walk based ranking algorithm, the regular random-walk with  xed teleport probability as used in [17, 18,
 essentially is a variant of the regular random-walk, which penalizes popular nodes by customizing the teleport probability based on the edge-degree of each node.
In both cases the random walk is performed on the bipartite graph model as described in Section 3.1.
In addition, we conduct a second group of experiments that compares our algorithms with the two existing software set expansion systems, namely the SEAL set expansion system [16] and the Google Sets system [1] through their respective public web portals.
Since SEAL interface only accepts up to 3 seeds, we pick 3 entities as seeds in this set of experiments.
Lastly, we drill down to the dynamic thresholding algorithm (the behavior of the static thresholding algorithm is similar) proposed in this work, and present a set of experiments in which we vary various parameters.
Speci cally, we vary the parameter  , the number of seeds used, and the similarity metric used, etc.
This allows us to analyze and to better understand the performance characteristics of our algorithm in response to the changes in parameter values.
Country germany japan belgium denmark canada spain poland sweden norway austria peru croatia netherlands hungary italy czech republic Color blue red white green yellow brown black purple pink gray orange violet silver grey gold greyish green switzerland slovakia china slovenia bronze beige navy tan Camera olympus nikon kodak pentax canon casio fuji panasonic leica fuji lm sony ricoh vivitar sigma Mattress serta sealy simmons beautyrest sealy posturepedic stearns foster bassett tempur pedic sears spring air gold bond jobri universal furniture champlain konica minolta samsung minolta polaroid vistaquest rollei fbg coapt systems riverside furniture hyla vacuum broyhill lifestyle solutions Table 1: Top-20 results by Dynamic Thresholding algorithm Table 1 lists the top 20 ranked results produced by dynamic thresholding algorithm for the four domains that we experiment with.
The static thresholding algorithm reports similar results, as we will see later in the precision/recall curves.
In each domain, those terms in boldface are the input seeds.
The underlined terms are the results that do not belong to the ground truth set and thus counted as incorrect results; while the remaining terms are correct results expanded from the input seeds.
From Table 1, we can see that in the top-20 ranked results, the  Country  and  Camera  domains have perfect precision.
 Color  domain has only one incorrect result  greyish green  (which although being a real composite color, is nonetheless not included in our ground truth set which only includes atomic colors).
The top-
are incorrect, including product line names ( sealy posturepedic  and  beautyrest ), and furniture retailers ( universal furniture  and  riverside furniture ).
l l a c e




 l l a c e


 Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp
 Precision (a) Country Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp
 Precision (c) Camera
 l l a c e






 l l a c e





 Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp
 Precision (b) Color Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp

 l l a c e






 l l a c e
 Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp
 Precision (a) Country Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp

 l l a c e






 l l a c e

 Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp
 Precision (b) Color
 Dynamic(cid:3)Threshold Static(cid:3)Threshold Random(cid:3)Walk Random(cid:3)Walk(cid:3)Adsp



 Precision (d) Mattress
 Precision (c) Camera



 Precision (d) Mattress
 Figure 5: Comparison with Random-Walk based approach using Web List data Figure 6: Comparison with Random-Walk based approach using Query Log data Observing the patterns of the incorrect results in the ranked list, we further adopt a token-based subset/superset- ltering heuristics to remove results that are likely to be incorrect.
Speci cally, we remove a result in the ranked list if there is another result that ranks higher in the list whose token set is a subset/superset of the current result.
The intuition here is that for a pair of results whose token sets are subset-superset, the concepts that they represent also tend to constitute a semantic superclass-subclass hierarchy.
Given that we are expanding the given seeds to produce a coherent concept set, one and at most one of the two entities in the superclass-subclass hierarchy can be correct.
Therefore, we only pick the result that ranks higher (a more con dent prediction) and ignore sub-set/superset results that rank lower.
As an example, since  sealy  ranks higher in the list for  Mattress , we will not consider the su-perset result  sealy posturepedic  that ranks lower, which is only a product line, or a subclass of the desired superclass concept, the manufacturer/brand name  sealy .
Similarly  greyish green  will not be considered since the subset  green  is also a result that ranks higher.
This simple heuristic turns out to be useful and boosts the precision/recall results.
We apply this token-based subset/superset- ltering for all algorithms/systems in our experiments.
In this section we present the performance comparison between the iterative thresholding algorithm proposed in this work, and the random walk based ranking algorithms.
For the random walk based approaches, we report experimental results for both the regular random walk [17, 18, 19], and the Adsorption random walk [5, 15, 20], on the bipartite graph as modeled in Section 3.1.
They are reported as the  Random Walk  and  Random Walk Adsp  curves.
We set teleport probability as 0.2.
Figure 5 shows the performance results on the Web List data for each of the four domains that we experimented with.
Note that while Dynamic Thresholding is slightly better than Static Thresh-olding, both approaches signi cantly outperform random walk based approaches.
This con rms that simple random walk based approach is not able to handle noisy data well, as we analyzed in Section 4.3.
Figure 6 presents the same experiments run on the Query Log data.
The general trends observed in Figure 5 is con rmed: Dynamic Thresholding and Static Thresholding outperforms the random walk based approaches quite signi cantly.
We note, however, that the precision/recall results observed on Query Log data are not as good as the those on Web List data.
This is not entirely surprising, as the query log tends to be much noisier than the HTML web lists.
We additionally conduct a second group of experiments that compares our algorithms against two existing systems, SEAL [16] and Google Sets [1] 1.
As we stated previously, since we do not have complete details of the implementation of the algorithms or the back-end data set used by either of the SEAL/Google Sets systems, the performance numbers do not directly compare.
However, we still feel that reporting these results is useful in that it puts the performance of our algorithm in the context of start-of-art existing systems, and helps us to understand the usefulness of the algorithms.
Figure 7 summarizes the performance comparison between Dynamic Thresholding that we propose, and SEAL/Google Sets.
Since web interface for Google Sets can take up to 5 seeds, while SEAL can only allow for 3 seeds as input, in this set of experiments we only use 3 seeds as input to all three algorithms.
In addition, as Google Sets only returns 50 results at most, its performance curve is incomplete (especially for the  Country  domain, where we report precision/recall up to the top-ranked 500 terms given that the
 on 10/01/2010 l a c e
 / n o i s i c e r











 Precision Recall l l a c e r / n o i s i c e r p








 DT(cid:3)3(cid:3)seeds Google(cid:3)Set(cid:3)3(cid:3)seeds SEAL(cid:3)3(cid:3)seeds

 Precision



 alpha

 (b) Color (a) Camera Precision Recall


 alpha

 (b) Mattress l l a c e











 l l a c e











 l l a c e

 l l a c e





















 DT(cid:3)3(cid:3)seeds Google(cid:3)Set(cid:3)3(cid:3)seeds SEAL(cid:3)3(cid:3)seeds
 Precision (a) Country DT(cid:3)3(cid:3)seeds Google(cid:3)Set(cid:3)3(cid:3)seeds SEAL(cid:3)3(cid:3)seeds

 Precision (c) Camera DT(cid:3)3(cid:3)seeds Google(cid:3)Set(cid:3)3(cid:3)seeds SEAL(cid:3)3(cid:3)seeds

 Precision (d) Mattress
 l l a c e









 Jaccard Cosine Figure 8: Sensitivity analysis to   #(cid:3)seed(cid:3)=(cid:3)2 #(cid:3)seed(cid:3)=(cid:3)4 l l a c e











 Precision



 Precision

 (a) Vary similarity metric (b) Vary # of seeds Figure 9: Sensitivity analysis mance in most cases, and adopt   = 0.5 in all remaining experiments.
Next, in Figure 9a we vary the similarity metric used in our Dynamic Thresholding algorithm for the  Camera  domain.
Recall that our algorithm is a general framework that can work with any similarity measurements.
In this experiment we report performance of our algorithm using both Jaccard similarity and the Cosine similarity metric.
As can be seen in Figure 9a, while both approaches perform reasonably well, the Cosine similarity is slightly better.
Similar trends are also observed in experiments over other domains, con rming the performance advantage of using Cosine similarity.
Therefore, we only report performance using Cosine similarity metric in all other experiments.
Finally, in Figure 9b, we vary the number of input seeds and report the corresponding set expansion performance.
Speci cally, given the 6 seeds we used for each of the four domains in the previous experiments, we pick all possible 2-seed combination out of these 6 seeds, which gives us a total of 15 such combinations.
Similarly we pick all possible 4-seed combination out of the 6 seeds which again gives us 15 possibilities.
We then use all 15 combinations of 2/4 seeds as input, to test the performance of our Dynamic Thresholding algorithm.
The results are reported in Figure 9b.
The overall trend that stands out in this  gure is that the performance of our algorithm with 4 seeds is in general much better and more stable than the case where only 2 seeds are used as input.
Observe that to the lower left corner, there are two instances of 2-seed combinations that lead to extremely low precision/recall.
This suggests that our algorithm is more robust when a reasonable number of seeds are given, and the performance may  uctuate with very few number of seeds, largely depending on the quality of the seeds given.
However, we believe that it is not really hard to  nd 4 input seeds in virtually any domain, thus not a stringent requirement in general, ensuring the usefulness of our algorithm.
Figure 7: Comparison with Google Sets and SEAL ground-truth set is much larger for  Country ).
The general observation is that while Dynamic Thresholding and Google Sets perform roughly the same in  Country  and  Color , Dynamic Thresh-olding has a slightly better precision/recall curve for  Camera  and  Mattress .
Furthermore, in each of the four domains Dynamic Thresholding seems to outperforms the SEAL system.
Nevertheless, we once again emphasize that this is by no means an implication of the relative performance of these algorithms.
The difference in performance may simply because of the different data sets each system uses.
It does suggest, however, that the algorithm we develop is competitive against existing set-expansion systems.
Sensitivity Analysis to Parameters
 To better understand the performance characteristics of our proposed approaches, we in this section conduct sensitivity analysis to understand the impact of various parameters to our algorithm.
Again, we use the dynamic thresholding algorithm as example.
Figure 8 depicts the performance of the Dynamic Thresholding algorithm with varied parameter   in domain  Camera  and  Mattress .
Recall that in De nition 4,   is the essentially the weight parameter used to balance the quality metric of the expanded seed set (ESS) between relevance and coherence.
  = 0 means that we only consider the coherence of ESS, while   = 1 indicates that only relevance is taken into account.
Any value of   in between suggests a combination of both of these two metrics.
In both Figure 8a and Figure 8b we can see that when   gets extreme values (0 or 1, meaning only one of the relevance and coherence metrics is considered), precision/recall performance numbers suffer.
On the other hand,   values in between boosts the performance of our algorithm.
While we only report performance for domain  Camera  and  Mattress  here, similar trends are observed in other domains.
In general, we observe that   = 0.5 usually gives the best perfor-In this paper we studied the problem of using general-purpose web data (web lists and query logs) to expand a set of seed entities.
We proposed a simple yet effective quality metric to measure the expanded set, and designed two iterative thresholding algorithms to rank candidate entities.
We validated our approach using experiments conducted on multiple domains, and concluded that our algorithm outperforms existing techniques for set expansion on noisy web data.
