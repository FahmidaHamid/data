Virtually any website that serves content from a database uses one or more script to generate pages on the site, leading to a site considering of several clusters of pages, each generated by the same script.
Since a huge number of surface-web and deep-web sites are served from databases, including shopping sites, entertainment sites, academic repositories and library catalogs, these sites are natural targets for information extraction.
Structural similarity of pages generated from the same script allows information extraction systems to use simple rules, called wrappers, to e ectively extract information from these webpages.
Wrapper systems are commercially popular, and the subject of extensive research over the last two decades wrappers [2, 3, 6, 10, 17,
 portant application of wrapper techniques is the population of structured databases, our research goal goes beyond this to the production of a sophisticated web of linked data, a web of concepts [11].
A key challenge to ful ll this vision is Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
the need to perform web-scale information extraction over domains of interest.
The key di erence between wrapper induction and web-scale wrapper induction is the form of the input.
For a traditional wrapper induction task, a schema, a set of pages output from a single script, and some training data are given as input, and a wrapper is inferred that recovers data from the pages according to the schema.
For web-scale extraction, a large number of sites are given as input, with each site comprising the output of an unknown number of scripts, along with a schema.
A clear result of the new problem de nition for web-scale extraction is that per-site training examples can no longer be given, and recent work on unsupervised extraction seeks to meet this challenge [12, 15, 16, 23].
An equally important, but less-recognized result of the new problem de nition is the need to automatically organize pages of the site into clusters, such that a single, high-quality wrapper can be induced for each cluster.
Conceptually, each cluster corresponds to the output of one of the scripts that created the site.
Alternatively, if manual work is done to select which pages to wrap, the bene t of unsupervised extraction techniques is e ectively lost, since nontrivial editorial work must still be done per site.
(Even though techniques with the limited scope of extracting from lists [16, 23] do not explicitly need such a clustering, the knowledge that many lists on a site have the same structure can substantially improve the extraction accuracy and recall of these techniques.)
While substantially less-well-studied than wrapper induction, the resulting problem of structurally clustering web pages for extraction, has in fact been studied [7, 8], and summarized in a recent survey [13].
However, at the current state-of-the art, a fundamental issue remains: existing techniques do not scale to large websites.
Database-generated websites suitable for extraction routinely have millions of pages, and we want the ability to cluster a large number of such websites in a reasonable amount of time.
The techniques covered in a recent survey [13] do not scale beyond few hundred webpages.
In fact, most of these techniques based on similarity functions along with agglomerative hierarchical clustering have a quadratic complexity, and cannot handle large sites.
The XProj [1] system, which is the state of the art in XML clustering, has a linear complexity; however, it still requires an estimated time of more than 20 hours for a site with a million pages1.
DB1000DTD10MR6 dataset, and the documents themselves are much smaller than a typical webpage.
able techniques for clustering websites.
We primarily rely on URLs, in conjunction with very simple content features, which makes the techniques extremely fast.
Our use of URLs for structural clustering is novel.
URLs, in most cases, are highly informative, and give lots of information about the contents and types of webpages.
Still, in previous work [7], it was observed that using URLs similarity does not lead to an e ective clustering.
We use URLs in a fundamentally di erent way.
We share the intuition in XProj [1] that pairwise similarity of URLs/documents is not meaningful (we illustrate this in Sec 2.2).
Instead, we need to look at them holistically, and look at the patterns that emerge.
In this work, we develop a principled framework, based on the principles of information theory, to come up with a set of scripts that provide the simplest explanation for the observed set of URLs/content.
Below, we summarize the contributions of our work.
tering of websites.
formation theory, that allows us to leverage URLs effectively, as well as combine them with content and structural properties.
ity in the number of webpages, that scales easily to websites with millions of pages.
over several entire websites spanning four content domains, and demonstrate the e ectiveness of our techniques.
We believe this is the  rst experimental evaluation of this kind, as all previous systems have either looked at small synthetic datasets, or a few small sample clusters of pages from websites.
We  nd that, for example, we were able to cluster a web site with 700,000 pages in 26 seconds seconds, an estimated 11,000 times faster than competitive techniques.
In this section, we introduce the clustering problem and give an overview of our information-theoretic formulation.
The discussion in this section is informal, which will be made formal in subsequent sections.
Websites use scripts to publish data from a database.
A script is a function that takes a relation R of a given schema, and for each tuple in R, it generates a webpage, consisting of a (url,html) pair.
A website consists of a collection of scripts, each rendering tuples of a given relation.
E.g., the website imdb.com has, among others, scripts for rendering movie, actor, user, etc.
In structured information extraction, we are interested in reconstructing the hidden database from published webpages.
The inverse function of a script, i.e. a function that maps a webpage into a tuple of a given schema, is often referred to as a wrapper in the literature [2, 18, 17, 20, 25, 26].
The target of a wrapper is the set of all webpages generated by a common script.
This motivates the following problem: Website Clustering Problem : Given a website, cluster the pages so that the pages generated by the same script are in the same cluster.
The clustering problem as stated above is not yet fully-speci ed, because we haven t described how scripts generate the urls and contents of webpages.
We start from a very simple model focusing on urls.
A url tells a lot about the content of the webpage.
Analogous to the webpages generated from the same script having similar structure, the urls generated from the same script also have a similar pattern, which can be used to cluster webpages very e ectively and e ciently.
Unfortunately, simple pairwise similarity measures between urls do not lead to a good clustering.
E.g.
consider the following urls: u1 u2 u3 u4 : site.com/CA/SanFrancisco/eats/id1.html : site.com/WA/Seattle/eats/id2.html : site.com/WA/Seattle/todo/id3.html : site.com/WA/Portland/eats/id4.html Suppose the site has two kinds of pages : eats pages containing restaurants in each city, and todo pages containing activities in each city.
There are two  scripts  that generate the two kind of pages.
In terms of string similarity, u2 is much closer to u3, an url from a di erent script, than the url u1 from the same script.
Thus, we need to look at the set of urls holistically, and cannot rely on string similarities for clustering.
Going back to the above example, we can use the fact that there are only 2 distinct values in the entire collection in the third position, todo and eats.
They are most like script terms.
On the other hand, there are a large number of values for states and cities, so they are most likely data values.
We call this expected behavior the small cardinality e ect.
Data terms and script terms can occur at the same position in the url.
E.g., the same site may also have a third kind of pages of the form: site.com/users/reviews/id.html.
Thus, in the  rst position we have the script term users along with list of states, and in second position we have reviews along with cities.
However, if one of the terms, e.g reviews, occurs with much higher frequency than the other terms in the same position, it is an indication that its a script term.
We call this expected behavior the large component e ect.
We note that there are scenarios when a very frequent data item might be indistinguishable from script term according to the large component e ect.
We show how to disambiguate script terms and data terms in such cases using semantic constraints in Section 5.4.
In order to come up with a principled theory for clustering urls, we take an information theoretic view of the problem.
We consider a simple and intuitive encoding of urls generated by scripts, and try to  nd an hypothesis (set of scripts) that o er the simplest explanation of the observed data (set of urls).
We give an overview of this formulation in the next section.
Using an information-theoretic measure also allows us to incorporate addition features of urls, as well as combine them with the structural cues from the content.
We assume, in the simplest form, that a url is a sequence of tokens, delimited by the  /  character.
A url pattern is a sequence of tokens, along with a special token called      .
The number of     is called the arity of the url pattern.
An example is the following pattern: www.2spaghi.it/ristoranti/*/*/*/* It is a sequence of 6 tokens: www.2spaghi.it, ristoranti,  ,  ,   and  .
The arity of the pattern is 4.
Encoding URLs using scripts We assume the following generative model for urls: a script takes a url pattern p, a database of tuples of arity equal to arity(p), and for each tuple, generates an url by substituting each   by corresponding tuple attribute.
E.g., a tuple (lazio, rm, roma, baires) will generate the url: www.2spaghi.it/ristoranti/lazio/rm/roma/baires Let S = {S1, S2,  S k} be a set of scripts, where Si consists of the pair (pi, Di), with pi a url pattern, and Di a database with same arity as pi.
Let ni denote the number of tuples in Di.
Let U denote the union of the set of all urls produced by the scripts.
We want to de ne an encoding of U using S.
We assume for simplicity that each script Si has a constant cost c and each data value in each Di has a constant cost  .
Each url in U is given by a pair (pi, tij ), where tij is a tuple in database Di.
We write all the scripts once, and given a url (pi, tij), we encode it by specifying just the data tij and an index to the pattern pi.
The length of all the scripts is c   k. Total length of specifying all the data equals (cid:2) i     arity(pi)   ni.
To encode the pattern indexes, the number of bits we need equals the entropy of the distribution i ni by N , the entropy of cluster sizes.
Denoting the sum is given by Thus, the description length of U using S is given by i nilog N ni (cid:2) (cid:2) .
(cid:3)
 ni +   (cid:3) i ck + ni log i The MDL Principle arity(pi)   ni (1) Given a set of urls U , we want to  nd the set of scripts S that best explain U .
Using the principle of minimum description length [14], we try to  nd the shortest hypothesis, i.e. S that minimize the description length of U .
The model presented in this section for urls is simplistic, and serves only to illustrate the mdl principle and the cost function given by Eq .(1).
In the next section, we de ne our clustering problem formally and in a more general way.
We now formally de ne the mdl-based clustering problem.
Let W be a set of webpages.
Each w   W has a set of terms, denoted by T (w).
Note that a url sequence (cid:2)(cid:2)  site.com/a1/a2/ .
.
.
can be represented as a set of terms {(pos1 = site.com), (pos2 = a1), (pos3 = a2), } In section 3.1, we will describe in more detail how a url and the webpage content is encoded as terms.
Given a term t, let W (t) denote the set of webpages that contain t. For a set of pages, we use script(W ) to denote  w W T (w), i.e. the set of terms present in all the pages in W .
A clustering is a partition of W .
Let C = {W1,    , Wk} be a clustering of W , where Wi has size ni.
Let N be the size of W .
Given a w   Wi, let arity(w) = |T (w)   script(Wi)|, i.e. arity(w) is the number of terms in w that are not present in all the webpages in Wi.
Let c and   be two  xed parameters.
De ne mdl(C) = ck + ni log i arity(w) (2) w W (cid:3) (cid:3)
 ni +   We de ne the clustering problem as follows: Problem 1.
(Mdl-Clustering) Given a set of webpages W ,  nd the clustering C that minimizes mdl(C).
In Sec.
4, we formally analyze Eq.
(2) and show how it captures some intuitive properties that we expect from URL clustering.
(cid:2) (cid:2) w W arity(w) = (cid:2) |w|   (cid:2) equals N log N   (cid:2) Eq.
(2) can be slightly simpli ed.
Given a clustering C as above, let si denote the number of terms in script(Wi).
i nisi.
Also, the Then, i ni log ni.
By re-entropy moving the clustering independent terms from the resulting expression, the Mdl-Clustering can alternatively be formulated using the following objective function: (cid:3) i ni log N ni w W   (C) = ck   (cid:3) mdl ni log ni     nisi (3) i i
 The abstract problem formulation treats each webpage as a set of terms, which we can use to represent its url and content.
We describe here the representation that we use in this work: URL Terms As we described above, we tokenize urls based on  /  character, and for the token t in position i, we add a term (posi = t) to the webpage.
The sequence information is important in urls, and hence, we add the position to each token.
For script parameters, for each (param, val) pair, we construct two terms: (param, val) and (param).
E.g.
the url site.com/fetch.php?type=1&bid=12 will have the following set of terms: { pos1=site.com, pos2=fetch.php, type, bid, type=1, bid=12}.
Adding both (param, val) and (param) for each parameter allows us to model the two cases when the existence of a parameter itself varies between pages from the same script and the case when parameter always exists and its value varies between script pages.
Many sites use urls whose logical structure is not well separated using  / .
E.g., the site tripadvisor.com has urls like www.tripadvisor.com/Restaurants-g60878-Seattle_ Washington.html for restaurants and has urls of the form www.tripadvisor.com/Attractions-g60878-Activities-S-eattle_Washington.html for activities.
The only way to separate them is to look for the keyword  Restaurants  vs.
 Attractions .
In order to model this, for each token t at position i, we further tokenize it based on non-alphanumeric characters, and for each subterm tj, we add (posi = tj ) to the webpage.
Thus, the restaurant webpage above will be represented as { pos1=tripadvisor.com, pos2=Restaurants, is that the term pos2=Restaurants will be inferred as part of the script, since its frequency is much larger than other terms in co-occurs with in that position.
Also note that we treat the individual subterms in a token as a set rather than sequence, since di erent urls can have di erent number of subterms in general, and we don t have a way to perfectly align these sequences.
Content Terms We can also incorporate content naturally in our framework.
We can simply put the set of all text elements that occur in a webpage.
Note that, analogous to urls, every webpage has some content terms that come from the script, e.g.
 Address:  and  Opening hours:  and some terms that come from the data.
By putting all the text elements as webpage terms, we can identify clusters that share script terms, similar to urls.
In addition, we want to disambiguate text elements that occur at structurally di erent positions in the document.
For this, we also look at the html tag sequence of text elements starting from the root.
Thus, the content terms consist of all (xpath, text) pairs present in the webpage.
We analyze some properties of Mdl-Clustering here, which helps us gain some insights into its working.
Local substructure Let opt(W ) denote the optimal clustering of a set of webpages W .
Given a clustering problem, we say that the problem exhibits a local substructure property, if the following holds : for any subset S   opt(W ), we have opt(WS) = S, where WS denotes the union of webpages in clusters in S.
Lemma 4.1.
Mdl-Clustering has local substructure.
Local substructure is a very useful property to have.
If we know that two sets of pages are not in the same cluster, e.g.
di erent domains, di erent  letypes etc., we can  nd the optimal clustering of the two sets independently.
We will use this property in our algorithm as well as several of the following results.
Small Cardinality E ect Recall from Sec.
2.2 the small cardinality e ect.
We formally quantify the e ect here, and show that Mdl-Clustering exhibits this e ect.
We denote by W (f ) the set of webpages in W that contain term f .
Theorem 1.
Let F be a set of terms s.t.
C = {W (f ) | f   F} is a partition of W and |F|   2 c.
Then, mdl(C)   mdl({W}).
A corollary of the above result is that if a set of urls have less than 2 c distinct values in a given position, it is always better to split them by those values than not split at all.
This precisely captures the intuition of the small cardi-nality e ect.
For |W| (cid:7) c, the minimum cardinality bound in Theorem 1 can be strengthened to 2 .
Large Component E ect In Sec.
2.2, we also discussed the large component e ect.
Here, we formally quantify this e ect for Mdl-Clustering.
Given a term t, let f rac(t) denote the fraction of webpages that have term t, and let C(t) denote the clustering {W (t), W   W (t)}.
Theorem 2.
There exists a threshold   , s.t., if W has a term t with f rac(t) >   , then mdl(C(t))   mdl({W}).
For |W| (cid:7) c,   is the positive root of the equation  x + x log x + (1  x) log(1  x) = 0.
There is no explicit form for   as a function of  .
For   = 2,   = 0.5.
Thus, for   = 2, if a term appears in more than 0.5 fraction of URLs, it is always better to split the term into a separate component.
For clustering,   plays an important role, since it controls both the small cardinality e ect and the large component e ect.
On the other hand, since the number of clusters in a typical website is much smaller than the number of urls, the parameter c plays a relatively unimportant role, and only serves to prevent very small clusters to be split.
In this section, we consider the problem of  nding the optimal MDL clustering of a set of webpages.
We start by considering a very restricted version of the problem: when each webpage has only 1 term.
For this restricted version, we describe a polynomial time algorithm in Sec 5.1.
In Sec 5.2, we show that the unrestricted version of Mdl-Clustering is NP-hard, and remain hard even when we restrict each webpage to have at most 2 terms.
Finally, in Sec 5.3, based on the properties of Mdl-Clustering (from Section 4) and the polynomial time algorithm from Sec 5.1, we give an ef- cient and e ective greedy heuristic to tackle the general Mdl-Clustering problem.
We consider instances W of Mdl-Clustering where each w   W has only a single term.
We will show that we can  nd the optimal clustering of W e ciently.
Lemma 5.1.
In Opt(W ), at most one cluster can have more than one distinct values.
Thus, we can assume that Opt(W ) has the form {W (t1), W (t2),  , W (tk), Wrest} where W (ti) is a cluster containing pages having term ti, and Wrest is a cluster with all the remaining values.
Lemma 5.2.
For any term r in any webpage in Wrest and any i   [1, k], |W (ti)|   |W (r)|.
Lemma 5.1 and 5.2 give us an immediate PTIME algorithm for Mdl-Clustering.
We sort the terms based on their frequencies.
For each i, we consider the clustering where the top i frequent terms are all in separate clusters, and everything else is in one cluster.
Among all such clusterings, we pick the best one.
In this section, we will show that Mdl-Clustering is NP-hard.
We will show that the hardness holds even for a very restricted version of the problem: when each webpage w   W has at most 2 terms.
We use a reduction from the 2-Bounded-3-Set-Packing problem.
In 2-Bounded-3-Set-Packing, we are given a 3-uniform hypergraph H = (V, E) with maximum degree 2, i.e. each edge contains 3 vertices and no vertex occurs in more than 2 edges.
We want to determine if H has a perfect matching, i.e., a set of vertex-disjoint edges that cover all the vertices of H. The problem is known to be NP-complete [4].
We refer an interested reader to Appendix B for further details about the reduction.
Algorithm 1 RecursiveMdlClustering Input: W , a set of urls Output: A partitioning C


 4: else
 6: end if return  W (cid:2) Cgreedy return {W} RecursiveMdlClustering(W (cid:2) ) In this section, we present our scalable recursive greedy algorithm for clustering webpages.
At a high level our algorithm can be describe as follows: we start with all pages in a single cluster.
We consider, from a candidate set of re ne-ments, the one that results is the lowest mdl score.
Then, we look at each cluster in the re nement and apply the greedy algorithm recursively.
The following are the key steps of our algorithm:   (Recursive Partitioning) Using the local substructure property (Lemma 4.1), we show that a recursive implementation is sound.
  (Candidate Re nements) We consider a set of candidate re nements, and pick the one with lowest mdl.
Our search for good candidates is guided by out intuition of large component and small cardinality properties.
We show that our search space is complete for single term web pages, i.e. the recursive algorithm returns the optimal clustering of single term web pages as given in Sec 5.1.
  (E cient MDL Computation) The key to e ciency is our technique that can compute the mld scores of all candidate re nements in linear time using a single scan over webpages.
To achieve this, we analyze the functional dependencies between terms in di erent clusters.
We give details for each of the key steps below.
Let W be a set of input webpages to our clustering algorithm.
If we know that there is a partition of W such that pages from di erent partitions cannot be in same cluster, then we can use the local substructure property (Lemma 4.1) Algorithm 2 FindGreedyCandidate Input: W , a set of urls Output: A greedy partitioning C if mdl cost improves, null otherwise






 8: end for


 such that ai appears in the most number of urls in W    i 1 Ct = {W (t), W   W (t)}, where W (t) = {w|t   T (w)} C   C   {Ct} (cid:3)=1W (a(cid:3)).
(cid:3)=1Wa(cid:2)    i 1 (cid:3)=1Wa(cid:2), Wrest = W    k Ui = Wai Ck = {U1, U2, .
.
.
, Uk, Wrest} C   C   Ck 12: for 2 < k   kmax do


 16: end for
 18: // return best partition if mdl improves


 22: else
 24: end if return Cbest return null to independently cluster each partition.
We call any partition a re nement of W .
We consider a set of candidate re nements, chosen from a search space of  good  re nements, greedily pick the one that results in the highest immediately reduction in mdl, and recursively apply our algorithm to each component of the re nement.
We stop when no re ne-ment can lead to a lower mdl.
Our search for good candidate re nements is guided by our intuition of the large component and the small cardinality properties.
Recall that if a term appears in a large fraction of webpages, we expect it to be in a separate component from the rest of the pages.
Based on this, for each term t, we consider the re nement {W (t), W   W (t)}.
We consider all terms in our search space, and not just the most frequent term, because a term t1 might be less frequent than t2, but might functionally determine lots of other terms, thus resulting in a lower mld and being a better indicative of a cluster.
A greedy strategy that only looks at two-way re nements at each step may fail to discover the small cardinality e ect.
We illustrate this using a concrete scenario.
Suppose we have 3n webpages in W , n of which have exactly one term t1, n others have t2 and the  nal n have a single term t3.
Then,   ({W}) = c   3n log(3n)       0 mdl since a single cluster has no script terms.
Any two-way re nement has cost ({W (ai), W   W (ai)}) = 2c   n log n   2n log 2n    n mdl     It is easy to check that mdl of any two-way re nement is ({W}) for a su ciently large n and   = 1.
larger than mdl Hence, our recursive algorithm would stop here.
However, from Lemma 5.1, we know that the optimal clustering for the above example is {W (a1), W (a2), W (a3)}.
Motivated by the small cardinality e ect, we also consider the following set of candidate re nements.
We consider a greedy set cover of W using terms de ned as follows.
Let a1, a2, .
.
.
be the ordering of terms such that a1 is the most frequent term, and ai is the most frequent term among webpages that do not contain any al for l < i.
We  x a kmax and for 2 < k   kmax, we add the following re nement to the set of candidates : {U1, U2,    , Uk, W  k i=1Ui}, where Ui denotes the set of web pages that contain ai but none of the terms a(cid:3), (cid:5) < i.
We show that if kmax is su ciently large, then we recover the algorithm of Sec 5.1 for single term web pages.
Lemma 5.3.
If kmax is larger than the number of clusters in W , Algorithm 5.3 discovers the optimal solution when W is a set of single term web pages.
In order to  nd the best re nement of W from the candidate set, we need to compute the mdl for each re nement.
If we compute the mdl for each re nement directly, the resulting complexity is quadratic in the size of W .
Instead, we work with the mdl savings for each re nement, which is de ned as  mdl(C) = mdl (C)   mdl
     We show that, by making a single pass over W , we can compute the mdl savings for all the candidate re nements in time linear in the size of W .
If C = {W1, W2, .
.
.
, Wk}, then it is easy to show that  mdl =  c + |Wi| log |Wi| + |Wi|   (si   s) (cid:3) (cid:3) i i where, si is the size of script(Wi) and s is the size of script(W ).
Since every script term in W is also a script term in Wi, note that (si   s) is the number of new script terms in Wi.
We now show how to e ciently compute (si   s) for all clusters in every candidate partition in a single pass over W .
Thus if the depth of our recursive algorithm is (cid:5), then we make at most (cid:5) passes over the entire dataset.
Our algorithm will use the following notion of functional dependencies to e ciently estimate (si   s).
Definition 1 (Functional Dependency).
A term x is said to functionally determine a term y with respect to a set of web pages W , ify appears whenever x appears.
More formally, x  W y   W (x)   W (y) (4) We denote by F DW (x) the set of terms that are functionally determined by x with respect to W .
First, let us consider the two-way re nements {W (t), W   W (t)}.
Since t appears in every web page in W (t), by (cid:2) de nition a term t is a script term in W (t) if and only (cid:2)   F DW (t).
Similarly, t does not appear in any web if t page in W   W (t).
Hence, t is a script term in W   W (t) (cid:2)   F DW ( t); we abuse the FD notation if and only if t and denote by F DW ( t) the set of terms appear whenever t does not appear.
Therefore, script(W (t)) = F DW (t), and script(W   W (t)) = F DW ( t).
(cid:2) (cid:2) (cid:2) (cid:2) F DW (t) = {t The set F DW (t) can be e ciently computed in one pass.
We compute the number of web pages in which a single term (cid:2) (n(t)) and a pair of terms (n(t, t (cid:2)|n(t (5) To compute F DW ( t), we  nd some web page w that does not contain t. By de nition, any term that does not appear in T (w) can not be in F DW ((cid:14)= t).
F DW ( t) can be computed as {t (cid:2)|t (cid:2)   T (w)   n   n(t) = n(t (cid:2) )) appears.
)} )   n(t, t ) = n(t, t where, n = |W|.
Now, look at k   way re nements.
Given an ordering of terms {a1, a2, .
.
.
, akmax }, our k-way splits are of the form {U1, U2, .
.
.
, Uk 1, W    iUi}, where Ui denotes the set of web pages that contain ai but none of the terms a(cid:3), (cid:5) < i.
Therefore (again abusing the FD notation), script(Ui) = F DW ( a1    a2  .
.
.  ai 1   ai).
The  nal set does not contain any of the terms a(cid:3), (cid:5) < k. Hence, script(W    iUi) = F DW ( k 1 i=1  ai).
)} (6) The F D sets are computed in one pass over W as follows.
We maintain array C such that C(i) is the number of times ai appears and none of a(cid:3) appear 1   (cid:5) < i.
For each non script term in W , we maintain an array Ct such that Ct(i) is the number of times t appears when ai appears and none of a(cid:3) appear 1   (cid:5) < i.
Similarly, array R is such that R(i) = |W|   (cid:2) i Rt is an array such that Rt(i) = |W (t)|   (cid:2) (cid:3)=1 C((cid:5)).
For each non script term t in W , i (cid:3)=1 Ct((cid:5)).
The required F D sets can be computed as: F DW (( (cid:3) 1 i=1 ai)   a(cid:3)) = {t|C((cid:5)) = Ct((cid:5))} i=1 ai) = {t|R((cid:5)) = Rt((cid:5))}
 F DW ( (cid:3) (7) (8) Our problem formulation does not take into account any semantics associated with the terms appearing in the urls or the content.
Thus, it can sometime choose to split on a term which is  clearly  a data term.
E.g.
Consider the urls u1, u2, u3, u4 from Section 2.2).
The split Ceats = {W (eats), W  W (eats)} correctly identi es the scripts eats and todo.
However, sometimes, there are functional dependencies in the URLs that can favor data terms.
E.g.
there is a functional dependency from Seattle to WA.
Thus, a split on Seattle makes two terms constant, and the resulting description length can be smaller than the correct split.
If we have regions and countries in the urls in addition to states, the Seattle split CSeattle is even more pro table.
If we have the domain knowledge that Seattle is a city name, we will know that its a data term, and thus, we won t allow splits on this value.
We can potentially use a database of cities, states, or other dictionaries from the domain to identify data terms.
Rather than taking the domain centric route of using dictionaries, here we present a domain agnostic technique to overcome this problem.
We impose the following semantic script language constraint on our problem formulation : if t is a script term for some cluster W , then it is very unlikely that t is a data term in another cluster W .
This constraint immediately solves the problem we illustrated in the above example.
CSeattle has one cluster (W (Seattle)) where WA is a script term and another cluster where WA is a data term.
If we disallow such a solution, we indeed rule out splits on data terms resulting from functional dependencies.
(cid:2) (cid:2)   script(W (t)), then t Hence, to this e ect, we modify our greedy algorithm to use a term t to create a partition W (t) if and only if there does not exist a term t that is a script term in W (t) and a data term is some other cluster.
This implies the following.
(cid:2)   F DW (t).
Moreover, First, if t both in the two-way and k-way re nements generated by our greedy algorithm, t can be a data term in some other (cid:2) is not in script(W ).
Therefore, we cluster if and only if t can encode the semantic script language constraint in our greedy algorithm as: (cid:2) split on t if and only if F DW (t)   script(W ) (9) In Algorithm 5.3, the above condition a ects line number 5 to restrict the set of terms used to create two-way partitions, as well as line number 11 where the ordering is only on terms that satisfy Equation 9.
We  rst describe the setup of our experiments, our test data, and the algorithms that we use for evaluation.
Datasets As we described in Sec.
1, our motivation for structural clustering stems from web-scale extraction.
We set up our experiments to target this.
We consider four different content domains : (a) Italian restaurants, (b) books, (c) celebrities and (d) dentists.
For each domain, we consider a seed database of entities, which we use, via web search to discover websites that contain entities of the given types.
Fig. 1 shows the websites that we found using this process.
E.g.
for Italian restaurants, most of these are websites specialize in Italian restaurants, although we have a couple which are generic restaurant websites, namely chefmoz.com and tripadvisor.com.
Overall we have 43 websites spanning the 4 domains.
For each website, we crawl and fetch all the webpages from those sites.
The second column in the table lists the number of webpages that we obtained from each site.
Every resulting site has several clusters of pages.
E.g, for restaurant websites have, along with a set of restaurant pages, a bunch of other pages that include users, reviews, landing pages for cities, attractions, and so on.
Our objective is to identify, from each website, all the pages that contain information about our entities of interest, which we can use to train wrappers and extraction.
For each website, we manually identi ed all the webpages of interest to us.
Note that by looking at the URLs and analyzing the content of each website, we were able to manually identify keywords and regular expressions to select the webpages of interest from each site.
We use this golden data to measure the precision/recall of our clustering algorithms.
For each clustering technique, we study its accuracy by running it over each website, picking the cluster that overlaps the best with the golden data, and measuring its precision and recall.
Algorithms We will consider several variants of our technique: Mdl-U is our clustering algorithm that only looks at the urls of the webpages.
Mdl-C is the variant that only looks at the content of the webpages, while Mdl-UC uses both the urls and the content.
In addition to our techniques, we also look at the techniques that are described in a recent survey [13], where various techniques for structural clustering are compared.
We pick a technique that has the best accuracy, namely, which uses a Jaccard similarity over path sequences between web-









i i n o s c e r p

 recall Figure 2: Precision-Recall of Mdl-U by varying   pages, and uses a single-linkage hierarchical clustering algorithm to cluster webpages.
We call this method CP-SL.
Fig. 1 lists the precision/recall of various techniques on all the sites, as well as the average precision and recall.
We see that Mdl-U has an average precision of 0.95 and an average recall of 0.93, supporting our claim that urls alone have enough information to achieve high quality clustering on most sites.
On some sites, Mdl-U does not  nd the perfect cluster.
E.g., in chefmoz, a large fraction of restaurants (around 72%), are from United States, and therefore Mdl-U thinks its a di erent cluster, separating it from the other restaurants.
Mdl-UC, on the other hand, corrects this error, as it  nds that the content structure in this cluster is not that di erent from the other restaurants.
Mdl-UC, in fact, achieves higher average precision and recall than Mdl-U.
On the other hand, Mdl-C performs slightly worse that Mdl-U, again con rming our belief that urls are often more informative and noise-free than the content.
Fig. 1 also includes the precision/recall numbers for CP-SL.
CP-SL algorithm is really slow, so to keep the running times reasonable, we sampled only 500 webpages from each website uniformly at random, and ran the algorithm on the sample.
For a couple of sites, the fraction of positives pages was so small that the sample did not have a representation of positives pages.
For these sites, we have not included the precision and recall.
We see that the average precision/recall, although high, is much lower that what we obtain using our techniques.
Dependency on  : Recall that the   parameter controls both the small cardinality and large compenent e ect, and thus a ects the degree of clustering.
A value of   = 0 leads to all pages being in the same cluster and   =   results in each page being in its own cluster.
Thus, to study the dependency on  , we vary alpha and compute the precision and recall of the resulting clustering.
Fig. 2 shows the resulting curve for the Mdl-U algorithm; we report precision and recall numbers averaged over all Italian restaurant websites.
We see that the algorithm has a very desirable pr characteristic curve, which starts from a very high precision, and remains high as recall approaches 1.
Figure 3 compares the running time of Mdl-U and CP-SL.
We picked one site (tripadvisor.com) and for 1   (cid:5)   c e s ( e m i t






 Jaccard






 # of webpages Figure 3: Running Time of Mdl-U versus CP-SL








 ) c e s ( e m i t





 # of webpages (thousands)


 Figure 4: Running Time of Mdl-U 60, we randomly sampled (10   (cid:5)) pages from the site and performed clustering both using Mdl-U and CP-SL.
We see that as the number of pages increased from 1 to 600, the running time for Mdl-U increases from about 10 ms to about 100 ms. On the other hand, we see a quadratic increase in running time for CP-SL (note the log scale on the y axis); it takes CP-SL about 3.5 seconds to cluster 300 pages and 14 (= 3.5   22) seconds to cluster 600 pages.
Extrapolating, it would take about 5000 hours (  200 days) to cluster 600,000 pages from the same site.
In Figure 4 we plotted the running times for clustering large samples of 100k, 200k, 300k, 500k and 700k pages from the same site.
The graph clearly illustrates that our algorithm is linear in the size of the site.
Compared to the expected running time of 200 days for CP-SL, Mdl-U is able to cluster 700,000 pages in just 26 minutes.
There has been previous work on structural clustering.
We outline here all the works that we are aware of and state their limitations.
There is a line of work [1, 5, 9, 21, 22] that looks at structural clustering of XML documents.
While these techniques are also applicable for clustering HTML pages, HTML pages are harder to cluster than XML documents because there are more noisy, do not con rm to simple/clean DTDs, and are very homogeneous because of the  xed set of tags used in HTML.
At the same time, there are properties speci c to HTML setting that can be exploited, e.g.
the URLs of the pages.
There is some work that speci cally target structural clustering of HTML pages [7, 8].
Several measures of structural similarity for webpages have been proposed in the literature.
A recent survey [13] looks at many of these measures, and compares their performance for clustering webpages.
However, as mentioned in Section 1, current state-of-the art structural clustering techniques do not scale to large websites.
While, one could perform clustering on a sample of pages from the website, as we showed in Section 6, this can lead to poor accuracy, and in some cases clusters of interest might not even be represented in the resulting sample.
In contrast, our algorithm can accurately cluster sites with millions of pages in a few seconds.
In this work, we present highly e cient and accurate algorithms for structurally clustering webpages.
Our algorithms use the principle of minimum description length to  nd the clustering that best explains the given set of urls and their content.
We demonstrated, using several webpages, that our algorithm can run at a scale not previously attainable, and yet achieves high accuracy.
