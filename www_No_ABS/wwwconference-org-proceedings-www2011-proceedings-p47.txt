Mining query logs on a large scale has been shown to be tremendously useful for web search and web applications.
Query logs have been successfully explored for a variety of Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
purposes such as core ranking, automatic query expansion, web caching, user modeling, matching ads, and more.
One of the most direct and visible applications of query log mining is query recommendations in its multiple forms, from dynamic query suggestions  as you type  to related searches displayed on the search-engine results page.
Yet, in spite of their relative success, all query-log based methods exhibit the same weakness: they cease being as useful when reaching the long tail.
As a result, methods that operate at the level of individual queries cannot, as of today, handle rare or onetime queries and this leads to a signi cant coverage issue on the long tail.
One approach could be to simply ignore the long tail and concentrate on serving best head and torso needs.
This approach, we believe, would probably be a mistake.
Indeed, Goel et al. [12] conducted a thorough analysis of the  anatomy of the long tail,  and showed that most  ordinary people ..[have].. extraordinary tastes,  i.e. all of us exhibit a certain level of eccentricity.
Even more importantly, it turns out that supporting the tail boosts the head by providing users a convenient one-stop shop for both their mainstream and niche interests.
Following this view, we believe that it is critical to appropriately handle long-tail queries, especially in the query-recommendation family of applications.
In this paper, we focus on related query recommendation, one of the tasks for which the long-tail issue is the most visible.
We propose to address the long-tail problem by leveraging query templates [1], which are query constructs that abstract and generalize queries.
Our key idea is to identify rules between templates as means for suggesting related queries.
The rationale for our approach is based on the fact that many individual queries share the same query intent while focusing on di erent entities.
Hence, their related queries also share similar structures.
As an example, assume that the queries  Los Angeles hotels ,  New York hotels  and  Paris hotels  have been abstracted into the common query template  <city> hotels .
In addition, if  Los Angeles restaurants  is a query recommendation for  Los Angeles hotels  and similarly  New York restaurants  is a recommendation for  New York hotels , we can extract the general rule:  <city> hotels   <city> restaurants  Using such a rule, we could then o er for the query  Yancheng restaurants , the suggested query  Yancheng hotels  even if both are rare queries and had never been observed before.
The key challenge in this approach is to ensure that while signi cantly improving coverage for query recommendations, we maintain a certain level of quality in order to satisfy acceptable as long as the user has enough valid recommendations to select from.
In contrast, not o ering any suggestion damages the user experience, as lack of coverage remains a signi cant issue: we have observed on a Yahoo!
query log sample that 34M queries, out of 95M, had no consecutive query.
Thus, in our sample, any query recommendation system that leverages query reformulation would not be able to derive any information for more than 36% of the queries.
In this paper we make the following contributions:   We introduce the concept of rules between query templates, which can be used to infer recommendations for rare or previously unseen queries.
We apply the concept of template rules on the query ow graph [5].
At a high level, the concept is general and can be used to enhance other query-log constructs.
  We explain how to build templates and more speci -cally how to extract template rules.
Note that unlike prior work on templates, which used semi-supervision on restricted domains [1, 16], we can a ord to work at a general, large-scale level because we extract rules rather than standalone templates and ambiguity is therefore automatically reduced (see Section 4.5).
  We introduce the query-template  ow graph as an enrichment of the query ow graph with templates.
We then describe how to use the query-template  ow graph for the task of query recommendation.
  We provide an experimental evaluation, which shows that using a query-template  ow graph instead of a query ow graph construct consistently improves the quality of recommendations, especially for long-tail queries.
We verify our claims using manual evaluation, as well as a novel automated large-scale evaluation process over millions of tested recommendations.
Query recommendations.
Most query-recommendation methods use similarity measures obtained by mining (i) the query terms, (ii) the clicked documents, and/or (iii) the user sessions containing the queries.
Typical methods use a combination of these factors.
Query recommendation based on clicked documents.
Baeza-Yates et al. [3] propose a measure of query similarity and use it to build methods for query expansion.
Their technique is based on a term-weight vector representation of queries, obtained from the aggregation of the term-weight vectors of the URLs clicked after the query.
Wen et al. [27] also present a clustering method for query recommendation that is centered around the query-click graph [4].
The query-click graph is also utilized for  nding related documents using random walks [8],  nding related keywords for advertising [11], query rewriting through co-citation generalization [2] and ranking related queries using the notion of hitting time [19].
Query recommendation based on query reformulations.
Many e ective approaches focus on the analysis of user query sessions [10, 14, 30].
Fonseca et al. [10] propose a query recommendation system based on association rules.
Another attempt of extracting information from the query log is due to Zhang and Nasraoui [30], who represent each user session by a complete graph where consecutive queries are connected with an edge of a prede ned weight d. Nonconsecutive queries are connected by an edge weighted with the product of the weights on the walk connecting them.
Recent papers have shown that not only the previous query, but also the long-term interests of users, are important for understanding their information needs [18, 22].
Jones et al. [14] introduce the notion of query substitution: for each query, a set of similar queries is obtained by replacing the whole query or its sub-phrases.
White et al. [28, 29] use the query rewrites observed in a query log to generate query recommendations.
Sadikov et al. [23] have recently proposed to cluster the re nements of a user query by performing a random walk on a query-document graph that incorporates both session and click information.
One limitation for query recommendation that is common to the above works is that recommendations cannot be made for queries that were not seen before.
Additionally, the quality of recommendations declines for infrequent queries.
Name-entity and template extraction using query logs.
A few researchers have addressed the problem of extracting name-entities and hierarchy knowledge-bases from query logs [9, 21].
The main idea of these papers is to use the query log in order to extract entities and populate lexicons and/or ontology data.
Since in this paper we are not concerned with building an entity hierarchy, such a line of research is orthogonal and complementary to ours.
Few works attempt to tag query terms [13, 16], an important step towards generating templates for queries.
These works apply semi-supervised approaches for tagging with prede ned categories.
Recently, Agarwal et al. [1] introduced query templates as means to detect query intent and query attributes for a speci c domain.
They propose a semi-supervised approach for learning relevant templates in a domain, where both the domain attributes and seed queries are manually provided.
Unlike these works, we aim at broad-coverage recommendation using templates for any query, with neither prede ned categories nor speci c domains.
Templates in Natural Language Processing Finally we note that templates and rules between them have been successfully used in a number of NLP tasks such as information extraction [26], taxonomy population [24], question answering [17] and machine translation [6].
In this section we review the concepts of query logs and the query ow graph, which are central concepts in our paper.
Query-log analysis is typically used by search engines to build models of the interaction of the users with the search engine, and it is applicable to a wide range of applications.
A typical query log L is a set of records (cid:3)qi, ui, ti, Vi, Ci(cid:4), where qi is a query submitted to the search engine; ui is an anonymized identi er for the user who submitted the query; ti is a timestamp; Vi is the set of documents returned as results to the query; and Ci is the set of clicked documents.
A useful concept is a physical session, or simply session: a sequence of queries of one particular user within a speci c timeout t .
A typical timeout threshold often used in query-log analysis is t  = 30 minutes.
The query ow graph, which was introduced by Boldi et al. [5], is a graph structure that aggregates information contained in a query log.
At an intuitive level, a query ow graph is a graph in which nodes are queries, and edges (qi, qj) with large weights indicate that query qj is a very likely reformulation of query qi.
Formally, a query ow graph is de ned as a directed graph Gq = (Q0, Eqq, sqq) where:   Q0 = Q   {s, t} is the set of distinct queries Q submitted to the search engine plus two special nodes s and t, representing a starting state and a terminal state of any user search task;   Eqq   Q0   Q0 is the set of directed edges;   sqq : Eqq   (0..1] is a weighting function that assigns to every pair of queries (qi, qj)   Eqq a score sqq(qi, qj).
In the basic construction of the query ow graph, two queries qi and qj are connected by an edge if there is at least one session in the query log in which qj follows qi.
The edge weight w may depend on the application; one simple de nition is to use as weight the frequency of the transition (qi, qj) out of all instances of qi.
While there are more sophisticated versions of query ow graph the de nition of edge weights is orthogonal to the focus of this paper and so we set query ow graph edge weights to be transition probabilities.
One application of the query ow graph, suggested by Boldi et al., is query recommendation.
The idea is fairly natural: for a query qi recommend queries qj for which the weight of the edge (qi, qj) in the query ow graph is high enough.
Various alternatives for picking those queries were studied.
Those alternatives were inspired by ideas based on PageRank and personalized PageRank.
An obvious limitation of using the query ow graph for query recommendation is that no recommendation can be made for queries that were not seen before.
This is the limitation that we try to alleviate with the present work.
In this section, we describe our query-template approach for handling the long-tail problem of query logs.
We formally describe query templates and template construction.
We then introduce relations between templates, and present a recommendation algorithm based on a query ow graph enhanced with templates.
Our notion of query templates is similar to the one used by Agarwal et al. [1]; the main di erences are that (i) we de ne templates over a hierarchy of entity types, and (ii) we de ne a global set of templates for the whole query log and we do not restrict our templates on speci c domains (say, travel, weather, or movies).
We start our discussion by considering the set of all queries Q = {q1, q2, .
.
.}
that appear in a query log of a search engine, where each query, q   Q is a sequence of search terms, q = w1 .
.
.
wr.
For a query q we de ne a tokenization to be a grouping of the terms of q into consecutive sequences of terms.
For example, all possible tokenizations of  chocolate cookie recipe  are: (chocolate)(cookie)(recipe), (cho-colate)(cookie recipe), (chocolate cookie)(recipe), and (chocolate cookie recipe), where parentheses are used only for indicating the token boundaries.
Very often, not all possible tokenizations of a query are meaningful.
In addition to the query log, we also assume that we have access to an additional source of information: a generalization hierarchy H = (E, R) on a set of entities E. As usual, the set of generalizations R   E   E contains a pair ei   ej if the entity ej is a semantic generalization of the entity ei (R induces a DAG on E).
We also say that ei is a type of ej.
For example, R may contain the following pairs: chocolate   food chocolate   drink cookie   dessert chocolate cookie   dessert dessert   food food   substance recipe   instruction .
.
.
We overwrite the notation to denote ei   ej if there is a sequence of generalizations in R that leads from entity ei to entity ej in the hierarchy H. So, in the above example, we have chocolate   substance, since chocolate   food   substance.
If ei   ej, we write d(ei, ei) to denote the shortest path from ei to ej using the generalizations of R.
So, d(chocolate,substance) = 2.
Now let q = w1 .
.
.
wr be a query with a tokenization q = k1 .
.
.
ks.
Assume that the i-th token of the query, ki is present as an entity in the generalization hierarchy H, namely ki = ei   E, and that ei   ej, for some other entity ej.
We de ne a template t(q | ki   ej) of q by replacing the entity ki with a placeholder of type <ej> in the query q.
In other words t(q | ki   ej) =k 1 .
.
.
ki 1<ej>ki+1 .
.
.
ks.
The angle brackets < > are used to indicate that a substitution by a placeholder has taken place.
We denote the set of templates of q by T (q) ={ t1, t2, .
.
.
}.
In our running example, some of the possible templates for the query q = chocolate cookie recipe  are: <food> cookie recipe <drink> cookie recipe <food> recipe <substance> recipe chocolate cookie <instruction> We de ne kq(t) to be the token ofq that was replaced when generating t and e(t) to be the placeholder in t.
In our example, kq( <food> recipe ) is  chocolate cookie  and e( <food> recipe ) is  food .
A template is utilized by instantiating it with a token whose type matches the type of the template placeholder.
We denote by IH(t | k) the query generated by instantiating template t with token k based on the generalization hierarchy H. For example, IH( <food> recipe  |  soup ) is  soup recipe .
The result of instantiating an invalid token of an unmatched type is null.
The power of template instantiation lies in its ability to generate queries that were not observed before, as a cross product of templates and entities.
For each template t   T (q) of a query q we associate a score sqt(q, t).
This score provides a measure of goodness of the template t as a generalization of the query q.
Intuitively, for our query  chocolate cookie recipe , the template  <food> recipe  should have a higher score than  <substance> recipe .
This captures the notion that the risk being to overgeneralize.
In addition,  <food> recipe  should have a higher score than  <drink> cookie recipe , so as to give preference to more semantically valid templates.
Rules among templates enable the inference of meaningful recommendations for queries that have been seen very rarely, even for the  rst time.
Once queries have been generalized to templates, the next step is to mine for frequent transitions among templates, which we call template rules.
The transition  <city> hotels   <city> restaurants  shown in the introduction is an example of a template rule.
The power of template rules lies in their ability to provide inference rules by aggregating over many di erent individual query transitions.
Thus, template rules eliminate noise very e ectively and capture meaningful transitions.
More discussion on template rules follows in the rest of this section.
A central concept in this paper is the query-template  ow graph, which extends the query ow graph of Boldi et al [5].
The query-template  ow graph is a graph whose nodes represent not only the queries Q but also the templates T .
In addition to the query-to-query edges Eqq, there are also edges Eqt that specify the mapping from a query q to its templates T (q), and also edges Ett between templates.
More formally, the query-template  ow graph is a graph Gt = (Q0, T, Eqq, Eqt, Ett, sqq, sqt, stt), where   Q0, as in the query ow graph, is the set of queries plus the starting and terminal queries;   T is the set of all templates formed by the queries of Q, (cid:2) that is, T = q Q T (q);   Eqq is the set of query-to-query transition edges, as in the query ow graph;   Eqt is the set of query-to-template mapping edges, that is, edges between queries q   Q and templates of T (q);   Ett is the set of template-to-template transition edges, to be de ned shortly;   sqq are query-to-query scores on the edges of Eqq, as in the query ow graph;   sqt are query-to-template scores on the edges of Eqt mentioned above, to be de ned shortly;   stt are template-to-template scores on the edges of Ett; stt(t1, t2) is a measure of goodness that template t2 is a good transition from template t1, to be de ned shortly; We next describe how the parts of the query-template  ow graph are constructed in this paper, namely, the set of templates T , the query-to-template and template-to-template edges, Eqt and Ett, and their respective scores sqt and stt.
As discussed above, templates are constructed with the help of a generalization hierarchy H over entities.
In our implementation, H is the WordNet 3.0 hypernymy hierarchy [20] and the Wikipedia category hierarchy, connected together via the yago1 induced mapping [25].
This hierarchy features more than 1,750,000 entities with more than
 our approach can employ any method for generating an entity hierarchy, e.g.
from query logs [9, 21].
All the distinct queries in the query ow graph are processed.
For each query q we construct the set of templates T (q) as follows: First we normalize q by converting all characters to lower case, collapsing continuous spaces, removing non-printable characters, etc.
Then, every word n-gram up to length 3 in q is considered as a token for replacement by a placeholder.
For every n-gram k, we add to T (q) all the templates formed by replacing k with each of its generalizations in H (if any).
We note that n-grams that consist of stop-words are ignored.
In addition to the above general scheme, we found that the coverage of the produced templates can be improved by taking care of a small number of special cases, which appear often in query logs, when an n-gram is not found in the yago hierarchy.
If there are context words around such an n-gram (the n-gram is not the whole query) we further look into the following cases:   If the n-gram is an email address, we generate an email-typed template, e.g.,  ggg@yahoo.com instant message  becomes  <email> instant message .
  If the n-gram is a url, we generate a url-typed template, e.g.
 nbc.com login  becomes  <URL> login .
  If the n-gram contains numbers, we generate a numerically-typed template, e.g.,  555-7777 address becomes  <000-0000> address .
  Otherwise, if the n-gram is a noun-phrase, we create a post x-typed template, e.g.,  luxury cars sale  becomes  <?-cars> sale .
Query-to-template edges Eqt are used to specify the mapping from a query q to the templates T (q).
That is, we have (q, t)   Eqt if and only if t   T (q).
As already mentioned, some templates in T (q) are more con dent generalizations than others, a notion captured by the score sqt(q, t) as the goodness of the edge (q, t)   Eqt.
Let q be a query and t   T (q) be a template for q.
To compute the score sqt(q, t) we  rst compute a score Sqt(q, t), and we then normalize all scores Sqt(q, t) to ensure that the total score from the query q sums to 1.
For the unnormalized scores Sqt(q, t) we use the following rules, which we found to work well in practice.
  If t was obtained by substituting the token kq(t) with an entity e in the hierarchy H then, intuitively, the higher e is located in the hierarchy, the less con dent we are in the template quality.
Consequently, we select our score to be exponentially decaying with the distance d(kq(t), e) between the token and the generalization e in the hierarchy: Sqt(q, t) =  d(z,e) where   was set to 0.9 in our experiments.
www.mpi-inf.mpg.de/yago-naga/yago/ templates t of a query q we set Sqt(q, t) = 0.5.
  Finally, for all post x-typed templates t of a query q we set Sqt(q, t) = 0.1.
In addition, if q(cid:3) is a query and (q, q(cid:3) convenience of notation, we set Sqt(q, q(cid:3) Sqt(q, q(cid:3) ) = 0.
Normalization is then performed by setting , where t   T (q)   Q Sqt(q, t) (cid:3) sqt(q, t) = t(cid:2) T (q) Q Sqt(q, t(cid:3)) )   Eqq then, for ) = 1, otherwise, It is important to note that both the template construction procedure and the query-to-template score function are not dependent on query occurrences in the query log, only on the hierarchy H. Hence, whenever we encounter a new query q that is currently not in Q0 (an unseen query), we can add it to Q0 as well as its mapping edges to already existing templates in T in an online fashion.
The set of edges Ett forms the set of template rules that were introduced in Section 4.2.
Template rules describe the possible transitions between templates for suggesting related templates.
Recalling our running example, a possible transition could be  <food> recipe   healthy <food> recipe .
Each template rule in Ett is accompanied by a score stt, which captures the con dence of generating valid related queries by instantiating the corresponding templates.
Consider two templates t1 and t2 in the query-template  ow graph.
A directed edge (t1, t2) is in Ett if and only if:   e(t1) = e(t2), i.e., they have the same placeholder type.
  there is at least one edge (q1, q2) in the query ow graph between queries q1 and q2 such that t1   T (q1), t2   T (q2) and kq1 (t1) = kq2 (t2), that is the substituted token in q1 and in q2 is the same.
We call such query-to-query edges support edges and denote the set of support edges for (t1, t2) by Sup(t1, t2).
As an example, an edge  sandwich recipe   healthy sandwich recipe  in the query ow graph would give rise to the edge  <food> recipe   healthy <food> recipe  in the query-template  ow graph.
To compute the score stt(t1, t2) between the templates t1 and t2, we sum over the scores of all supporting pairs of queries, and we normalize so that the total score out of each template sums to 1: St(t1, t2) = stt(t1, t2) = (cid:4) (q1,q2) Sup(t1,t2) St(t1, t2) (cid:3) t St(t1, t) .
sqq(q1, q2),
 We next discuss how to utilize the query-template  ow graph in order to provide query recommendations.
Given an input query q that was submitted by a user, our task is to recommend other queries q(cid:3) that are likely to be helpful for the user.
Our output is a ranked list of candidate related-queries, ordered by their con dence score (highest  rst).
are those for which there is a directed edge (q, q(cid:3) The related query con dence score r(q, q(cid:3) Using the query ow graph, the candidate related queries ) inE qq.
) is sqq(q, q(cid:3) ).
Similarly, utilizing the query-template  ow graph, the candidate related queries are those for which there is either a directed edge (q, q(cid:3) ) in Eqq or there is a mapping edge (q, t) and a template-to-template transition edge (t, t(cid:3) ), such that IH(t(cid:3) | kq(t)) = q(cid:3) , that is the instantiation of template t(cid:3) with the mapping token of q to t results in q(cid:3) .
For example, one recommendation for the query  Adele Astaire  is  Adele Astaire biography , based on the mapping edge  Adele Astaire   <artist>  and the transition edge  <artist>   <artist> biography .
Notice that the recommended query q(cid:3) does not have to be in Q, i.e. it does not have to appear in the query log.
This is because q(cid:3) is generated as an instantiation of a template with a token extracted from the input query (which itself could be an unseen query).
The con dence score r(q, q(cid:3) ) based on the query-template  ow graph is: r(q, q(cid:3) sqt(q, t) stt(t, t(cid:3) ) =s qt(q, q(cid:3) ) sqq(q, q(cid:3) (cid:4) )+ ).
t T (q)   (t,t(cid:2)) Ett   IH(t(cid:2)|kq (t))=q(cid:2) Since both scores sqt(q, t) and stt(t, t(cid:3) ) are normalized, r(q, q(cid:3) ) is always between 0 and 1.
In particular, it can be interpreted as the probability of going from q to q(cid:3) by one of the feasible paths in the query-template  ow graph.
We note that in our experiments we rank  rst queries that were seen before as related (sqq(q, q(cid:3) ) > 0), assuming that they are more reliable recommendations.
Choosing the right template or set of templates for a query is a di cult task, since many queries consist of more than one term, and many terms are ambiguous.
For example, in  jaguar transmission fluid  it is unclear on which term the focus is.
Additionally,  jaguar  has several meanings based on which the query could be generalized.
Hence, prior work on query templates focused on semi-supervised approaches in speci c domains, where the task is simpler [1,
 of template rules for all kinds of queries.
For our advantage, extracting rules instead of single templates diminishes the problems mentioned above.
First, shared terms between queries typically coincide with the query focus.
Second, aggregating occurrence statistics for rules emphasizes the correct sense of ambiguous terms under the query context.
For example, the query transition  jaguar transmission fluid   jaguar used parts  indicates that  jaguar  is the focus.
In addition, by observing other transitions, e.g.
 toy-ota transmission fluid   toyota used parts , the template rule  <car> transmission fluid   <car> used parts  is brought forward as the frequent rule, diminishing the e ect of other possibly extracted rules, such as  <feline> transmission fluid   <feline> used parts .
One nice feature of the proposed method is that it can be implemented e ciently in a map-reduce environment, such as the hadoop platform2.
For building the query-template  ow graph, templates may be generated independently for each query, together with their mapping scores, at hadoop nodes that keep the hierarchy H in memory.
Template nodes are then aggregated at reduce time.
If H is too big to  t
 http://hadoop.apache.org/ query-template  ow graph used in our experiments (query statistics are the same for the query ow graph and the query-template  ow graph).
templates



 (<album>)
 (<institution>)



 (craigslist)
 (youtube) # nodes # edges avg in/out degree max out-degree max in-degree queries Table 2: Accuracy of each con guration and each sample-set based on manual evaluation qfg qtfg set-A 98.48% 97.84% set-B 97.65% 98.86% set-C
   in main memory, it may be partitioned among the hadoop nodes.
In this case, di erent n-gram pieces of one query may be processed by di erent hadoop nodes and be aggregated at the end.
Finally, the template-to-template edges and scores may also be computed in a map-reduce fashion as a sequence of joins we omit the details.
On the other hand, generating query recommendations needs to be an online process.
For a computationally viable solution, the idea is again to split the query-template  ow graph among di erent back-end nodes, mapped according to queries and query templates.
A new query is  rst parsed by a broker to generate the set of possible templates.
Then the templates are sent to the appropriate back-end nodes to compute candidate recommendations and scores, which are returned to the broker in order to be aggregated and produce the  nal ranking.
In this paper, we tested the proposed query-template  ow graph on the task of query recommendation.
We compared between two con gurations: (i) recommendations using the query ow graph, denoted qfg; and (ii) recommendations using the query-template  ow graph, denoted qtfg.
We conducted two experiments.
The  rst experiment follows the typical manual evaluation of query recommendation methods [3, 10, 18], where the quality of recommendations is assessed by human judges.
However, manual evaluation suffers from limitations, such as human labor and scale.
Therefore, as a second experiment, we propose a novel automatic evaluation approach for the query recommendation task.
Implementation For constructing the query ow graph, and consequently the query-template  ow graph, we used a development query log from which we sampled user sessions.
For testing the graphs, sessions were sampled from a later query log, denoted the test-query-log (see each experiment for the exact test-set construction details).
Table 1 presents statistics for the two graphs.
From the  gures we see that 56 candidate templates are generated on average for each query, and this re ects the number of template-to-template edges generated.
While the query-template  ow graph performance is signi cantly better than the query ow graph (see the following sections), this generation is still noisy and many of the generated templates and edges between templates are incorrect, though with lower scores.
In future work, we plan to improve our identi cation of correct templates and edges that should be generated for a given query or a given edge in the query ow graph.
The average degree of a node is very small, but the variance is huge.
While about a third of the nodes (both for queries and templates) have no outgoing edges and about a similar percentage have no incoming edges, the maximum in and out degrees are very large.
For templates, some of the large degrees are due to very general templates, while some queries (and consequently templates) are simply very frequent in sessions, as shown in the table.
Following the typical evaluation procedure in prior work, the authors evaluated a random sample of query recommendations for the two tested con gurations, qfg and qtfg.
For each con guration we randomly sampled 300 queries that occurred in the test-query-log, and which were followed by a consecutive query within a single session.
Each system suggested related queries, from which one recommendation was chosen randomly out of the top 10 recommendations.
In total, 300 pairs of an input query and one of its recommendations were sampled for each con guration, denoted set-A.
In addition, 100 pairs were randomly chosen from the 300 pairs sampled for qfg.
We then extracted, for the input queries in these pairs, the recommendations by qtfg that were ranked in the same position as those in the query ow graph pairs.
This sample, denoted set-B, compares more directly between the two methods on the same input queries and the same recommendation ranked positions.
Finally, 100 queries, for which no recommendation could be provided by qfg (queries that were not seen before), were randomly sampled from the test-query-log, and one of the top 10 recommendations by qtfg was randomly selected for each.
This sample, denoted set-C, assesses the extension capabilities of the query-template  ow graph.
In total, 800 query-recommendation pairs were evaluated.
Two of the authors blindly evaluated these pairs, with an overlap of 100 queries for agreement assessment.
The measured agreement between the two judges is 93% and the corresponding Kappa statistic [7] is 0.37, which is typically viewed as  fair  agreement [15].
No decision was made by the authors for about 10% of the examples.
Table 2 presents the results of the manual evaluation.
The accuracy of both methods is very high, with a slight gain for the query ow graph in general (set-A).
Yet, when looking at proposed related queries for the same input queries at the same ranked position (set-B), we see that actually it is qtfg that has a slight gain over qfg.
Nevertheless, the power of qtfg over qfg is demonstrated by the results on the set of unseen queries (set-C): for this set, even though the overall performance of qtfg drops slightly it still remains very high (94.38%), while the qfg could not provide any recommendation at all.
While manual evaluation provides a good quality assessment for query recommendation systems, it has several lim-dations should be considered as valid related queries.
This di culty is re ected in the mediocre Kappa value achieved for the inner annotator agreement.
Furthermore, the two annotators reported that though almost all recommendations seem  related , some seem more useful than others, a quality that is hard to measure.
A second limitation of the manual evaluation is that the amount of examples that are analyzed manually typically sums up to no more than several hundred examples [3, 8, 10, 30], which may not be statistically indicative (compared to query-log sizes).
Finally, manual evaluation is a slow process, which poses a real problem when several rounds of evaluation are needed during the development of a new system or algorithm.
A di erent approach is to directly evaluate a tested recommendation method in a search engine via bucket testing.
Yet, this is not a feasible option for the majority of researchers in the  eld.
To overcome the limitations of the current evaluation approaches, we propose a novel automatic evaluation based on a previously unseen query log.
This query log is partitioned into sessions and from each session the list of consecutive query pairs are extracted.
Our assumption is that if a user entered two queries, {qi, qi+1}, one after the other in the same session, then the second query may be viewed as a valid related query for the  rst query.
Furthermore, we assume that since a user explicitly entered qi+1 as related to qi, qi+1 should be considered more related to qi compared to other possible related queries.
Thus, a recommendation system should also provide qi+1 as a recommendation to qi, and rank it high.
These seem reasonable assumptions, as they are also behind recommendation methods that learn from query logs, such as the query ow graph.
Our proposed automatic evaluation is to test how many of the pairs {qi, qi+1}, extracted from the new query log, are also proposed by the tested recommendation system.
Furthermore, these recommendations should be ranked high.
This evaluation setup has the advantage of being fast, as it is automatic.
In addition, it may be applied to a large number of pairs (several millions in our experiment bellow).
Finally, there are no agreement issues between annotators, since any related query generated by real users is taken as a valid example   as the gold standard.
We note that this evaluation ignores other recommendations for a given query by the tested system, which may also be valid.
Thus, the absolute quality of recommendations is not directly assessed.
Yet, the proposed evaluation is very useful as a relative comparison between di erent recommendation systems, since when testing on a large volume of recommendations that were useful for users, each system should propose (and rank high) a reasonable amount of them.
This evaluation measures how well a recommendation system answers the need for related queries as explicitly formulated by a large number of real users.
We extracted pair occurrences from the sampled sessions in our test-query-log (see Section 5.1).
We experimented with two versions of this test-set.
In the  rst version, denoted all-pairs, all pairs of consecutive queries were extracted from each session, summing up to 3,134,388 pairs.
In the second version, denoted  rst-last, we generated just one pair from each session, which consists of the  rst and the last query in the session.
In this setup, which contains 4,591,044 pair occurrences, we assume that the last query in a session is the real target query of the user, and we test if the systems can propose it given the  rst query in the session.
To con rm that these gold-standard pairs indeed describe related queries, 100 pairs were sampled from the  rst-last dataset and were blindly evaluated by two of the authors together with the 800 examples judged in Section 5.2 (900 in total).
The accuracy achieved for this sample is 100%, that is all pairs are related queries.
This result further supports our choice of user generated pairs as a valid test-set for related query recommendation.
We measured the following  gures over the performance of each tested con guration:   How many of the tested pairs could be proposed by the system (upper-bound coverage).
  How many pairs were ranked in the top 100 recommendations of the system.
  How many pairs were ranked in the top 10 recommendations of the system (the typical list visible for users).
  How many pairs were ranked highest ( rst place).
  Mean Average Precision (MAP) of the test-pairs, viewing no more than 100 recommendations per query (if the pair is not in the top 100, its precision is 0).
  Average position (rank) of test-pairs, only for those that appear in the top 100 recommendations.
Some of the pairs occur in our dataset more than once.
We thus report the above  gures both for pair occurrences and for unique pairs (considering each pair only once).
While our initial target for using the query-template  ow graph was to address the long tail of infrequent queries and unseen queries, we also noticed that the query-template  ow graph helps to better rank recommendations in general.
Thus, in addition to the qfg and qtfg con gurations we experimented, on the  rst-last data-set, with a con guration of query-template  ow graph restricted only for re-ranking recommendations by query ow graph, denoted qtfg-rerank.
Tables 3 and 4 present the results for the all-pairs and  rst-last test-sets.
The  rst result seen from the tables is that by utilizing our current implementation of the query-template  ow graph the coverage of the test-pairs increases relatively by 22-24% (depending on the test-set).
If only unique pairs are considered, the increase is even more substantial, by about 45% for both test-sets.
This shows that the query-template  ow graph signi cantly increases the coverage for the long tail of infrequent or unseen queries.
For example, for the input query  1956 dodge lancer , qtfg ranked  rst the test-pair { 1956 dodge lancer ,  1956 dodge lancer for sale } using rules such as  1956 <car>   1956 <car> for sale , while qfg could not suggest any related query for  1956 dodge lancer .
Other examples are presented in Table 5.
When focusing on the top-10 recommendations per query, the di erence in test-pair coverage is widening.
This result pair occurrences qfg qtfg relative increase total in test-set upper-bound coverage # in top-100 # in top-10 # ranked highest
 avg.
position
 (22.65%) 709832 (16.97%) 531854 (9.49%) 297462 (2.86%)



 (28.17%) 882851 (25.49%) 799001 (20.74%) 649939 (10.01%) 313638

 unique pairs



 qfg qtfg relative increase total in test-set upper-bound coverage # in top-100 # in top-10 # ranked highest
 avg.
position
 (13.28%) 366047 (12.06%) 332487 (8.41%) 231811
 (2.86%)


 (19.38%) 533960 (17.25%) 475323 (13.52%) 372481 (6.5%) 179093





 Table 4: Results on the  rst-last data-set for each con guration pair occurrences qfg qtfg-rerank qtfg total in test-set upper-bound coverage # in top-100 # in top-10 # ranked highest
 avg.
position
 (27.63%) 1268579 (18.78%) 862232 (10.22%) 469165 (3.21%) 147501

 qfg
 (27.63%) 1268579 (24.79%) 1137920 (19.37%) 889383 (9.32%) 427828

 unique pairs qtfg-rerank
 (33.85%) 1554282 (28.59%) 1312408 (21.53%) 988568 (10.11%) 464260

 relative increase qtfg vs. qfg



 qtfg relative increase qtfg vs. qfg total in test-set upper-bound coverage # in top-100 # in top-10 # ranked highest
 avg.
position
 (15.51%) 598114 (13.7%) 528333 (9.23%) 355843 (3.33%) 128267


 (15.51%) 598114 (14.48%) 558255 (11.63%) 448383 (6.01%) 231797


 (22.62%) 872377 (18.86%) 727220 (14.13%) 545062 (6.92%) 266862





 is also re ected by the higher MAP values when utilizing query-template  ow graph.
The di erence is at its most when looking at the highest ranked recommendation, where the number of user-generated related queries that are ranked  rst is twice as much for unique queries and more than thrice as much for pair occurrences.
This surprising result indicates that query-template  ow graph not only provides recommendations for unseen or rare queries, but it also helps to better rank recommendations that are proposed by the query ow graph.
For example, qtfg ranked  rst the test-pair { gangrene ,  gangrene symptoms } using rules such as  <pathology>   <pathology> symptoms , while qfg ranked it at 23rd place.
The surprising result that the query-template  ow graph helps to better rank the query ow graph recommendations is also expressed by the performance of the qtfg-rerank con guration, where a signi cant improvement in performance is achieved just by re-ranking the query ow graph output by the query-template  ow graph.
This is further emphasized by looking at the average position of recommendations in the top-100.
While qtfg recommendations are better positioned on average than qfg, qtfg-rerank achieves the best average positions for user-generated related queries.
The reason for lower positions in qtfg than in qtfg-rerank is due to additional unseen queries for which only qtfg could provide recommendations.
For these queries the average position is lower than for the seen ones, as expected, and hence the lower overall average position.
To further understand the behavior of the query-template  ow graph vs. the query ow graph, we conducted several analyses on the results of the  rst-last test-set.
We  rst looked at the potential of the query-template  ow graph to extend the query ow graph for nodes in the query ow graph that have no outgoing edges.
The queries in these nodes were seen before (in the development query log, from which the query ow graph was constructed), but no related queries were observed for them (they were only observed as related queries for other queries).
There are 33,883,564 such queries (36% of the nodes), and the query-template  ow graph managed to propose at least one related query for
 of the query-template  ow graph for previously unseen query relations.
Next, we analyzed the change in test-pairs ranking when for the input query, showing the large verity of rules that were learned by our approach.
Example rule used Rank Pair { 8 week old weimaraner ,  8 week old weimaraner puppy } { a thousand miles notes ,  a thousand miles piano notes } { aaa office twin falls idaho ,  aaa twin falls idaho } { 1910 swimsuit ,  1910 swimsuit mens } { air force titles ,  air force ranks } { beach cameras ,  live beach cameras } { guangzhou flights ,  guangzhou map } { i am legend action figure ,  i am legend } { name for salt ,  chemical name for salt } 8 week old <breed>   8 week old <breed> puppy 1910 <clothing>   1910 <clothing> mens <single> notes   <single> piano notes aaa office <city>   aaa <city> <military service> titles   <military service> ranks <geo formation> cameras   live <geo formation> cameras <capital> flights   <capital> map <fiction> action figure   <fiction> name for <compound>   chemical name for <compound>








 Table 6: Ranking di erences of suggestions for the  rst-last test-set qfg qtfg total in test-set # in top-100 # in top-100 but not in the top-100 of the other # in top-10 but not in the top-10 of the other # better positioned when both in top-100






 (28.38%) 206373 (28.25%) 205430 (35.05%) 254919 (1.42%) (3.07%) (13.47%) ) % (

 p o t t a s r i a p t s e t #




















 query length (words) ) % (

 p o t t a s r i a p t s e t #











 query frequency


 Figure 1:  rst-last pair coverage at top-10 suggestions vs. query length Figure 2:  rst-last pair coverage at top-10 suggestions vs. query frequency the input query length (in words) varies.
From the graph presented in Figure 1, we see that the query ow graph shows a typical decline in performance for longer queries, as these are less frequent and thus have less history to rely on.
However, things are di erent when using the query-template  ow graph.
It actually performs better when the input queries have a few additional words, since they act as a disambiguating context for ranking appropriate recommendations (see Section 4.5).
In our third analysis, presented in Figure 2, we looked at the change in test-pairs ranking quality when the input query frequency varies.
The plot in Figure 2 shows that both con gurations  nd it hard to suggest test-pairs as recommendations for queries that occur only once in the test set.
This behavior agrees with the manual evaluation, which showed lower suggestion quality for unseen queries.
There is a substantial increase in performance for queries that occur twice and then a decline as the frequency increases, since frequent queries have a more diverse history and its hard to predict the best suggestions from this history.
Yet, we see that the query-template  ow graph consistently outperforms the query ow graph, managing to pick more appropriate suggestions for frequent queries.
For example, for the query  jack johnson lyrics , which occurred 14 times in the test-set, qtfg ranked the pair { jack johnson lyrics ,  jack johnson music } 5th, while qfg ranked it 43rd.
Finally, we analyzed the di erences in ranks of the same suggestions by the query ow graph and the query-template  ow graph.
This analysis, presented in Table 6, shows that many test-pairs that either cannot be processed by the query ow graph or are ranked very low, are positioned in the top-
indicates that the query-template  ow graph handles well recommendations that cannot be processed by the query ow graph.
Furthermore, only very few pairs (3%) are ranked signi cantly higher by the query ow graph than by the query-template  ow graph (in the top-10), which means that the query-template  ow graph hardly hurts the ranking of well ranked recommendations by the query ow graph.
This is despite the fact that about 13% of the test-pairs are ranked  ow graph.
These improvements are not signi cant, as they are mainly at the lower part of the top-100 ranks and are usually not displayed to users.
To conclude, the analysis shows that utilizing the query-template  ow graph on top of the query ow graph improves consistently the ranking of user-generated related queries, without any signi cant loss in performance for a speci c type of queries.
We introduced the concepts of rules between query templates and the query-template  ow graph as an abstraction and a generalization approach for relations between queries.
This novel approach is useful for addressing the long tail of rare or previously unseen queries in various search-related tasks.
Yet, it is also helpful in discovering important relations between frequent queries, for example for better ranking possible suggestions in query recommendation.
We conducted two query-recommendation experiments, a manual evaluation and a novel large-scale automated evaluation.
The manual evaluation showed that both the query-template  ow graph and the baseline query ow graph are very adequate as methods for query recommendations.
Yet, our automatic evaluation over millions of query-recommen-dation pairs showed that the query-template  ow graph consistently outperforms the query ow graph by ranking higher recommendations that were explicitly chosen by users.
More importantly, the query-template  ow graph provides good suggestions for many unseen queries, for which the query ow graph could not provide any suggestion.
In future work, we plan to apply the query-template  ow graph on other search-related tasks and to further explore its structure and behavior.
In addition, we want to improve the quality of rule extraction.
Finally, we would like to investigate the identi cation of edges from templates to queries, which capture relations such as between di erent towns in a district and the single airport that serves them.
