Recently we have witnessed more and more usage of structured data by search engines, and a most visible feature is the direct answers to fact lookup queries.
For example, for queries {Barack Obama date of birth}, {morocco capital} and {ps3 release date} (we use  {}  to represent a query), Google returns direct answers above the regular search results (as in Figure 1).
Such direct answers appear for queries looking for many kinds of facts, including directors of movies, spouses of celebrities, and population of {Barack Obama date of birth}: {morocco capital}: {ps3 release date}: Figure 1: Three examples of Google s direct answers.
The first two answers are correct and the third is wrong.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
cities.
Such answers bring great convenience to users searching for facts on the web, because it is no longer necessary for the user to inspect the snippets, click into a page and find the answer in the text.
It will not be surprising if direct answers can change users  search habits from searching for web pages to searching for facts.
Although there is clear user need, the fact lookup direct answers do not appear on major search engines until recently, which indicates that building a mature fact lookup engine is a challenging task.
Today the precision and coverage of the fact lookup answers on major search engines are still quite limited.
For example, the third answer in Figure 1 is incorrect as PlayStation 3 has been on the market for four years.
On the other hand, Google does not show direct answers for many fact lookup queries.
For example, it does not show such an answer for {britney spears birth date}, although it shows an answer for {britney spears date of birth}.
In this paper we introduce FACTO, a fact lookup engine based on structured data from the web.
FACTO is designed to work within the search engine, and it should only show answers for queries looking for facts.
Comparing with the fact lookup engines of Google and Ask.com, FACTO achieves higher precision and comparable query coverage (higher than Google and lower than Ask.com), although it is built by a very small team of two people in less than a year.
According to the best of our knowledge, this is the first paper that describes an end-to-end system for answering fact lookup queries in search engines.
Answering natural language questions has been studied in information retrieval and natural language processing [3][11][17] [18].
However, this is very different from identifying and answering fact lookup queries in web search.
Web users are lazy and seldom submit natural language queries.
More importantly, these question answering approaches cannot distinguish fact lookup queries from other queries, and they usually treat every query as a question.
In web search only a small portion of queries are fact lookup queries.
If we assume every query is looking for facts, very likely we will provide irrelevant answers to most of them.
Unlike the above question answering approaches, FACTO relies on structured data extracted from the web.
The main philosophy of FACTO is to extract entity-attribute-value triples from tables on the web, and use such data to answer queries that ask for an attribute of an entity.
FACTO extracts data from tables containing attribute-value pairs, such as the table in Figure 2 (a).
We choose to use attribute-value tables because such tables widely exist on the web, and the number is much larger than that of two-dimensional tables (such as the table in Figure 2 (b)).
Attribute-value tables provide very rich data covering all kinds of domains, which enables us to build a generic fact lookup engine.
However, it is not sufficient to simply extract attribute-value pairs from tables, because the entity involved is seldom in the same table.
We propose a novel approach to infer the main entity of a web page, by matching web search queries with the titles and contents of the search result pages clicked for these queries.
By combining Fact lookup answer of FACTO Figure 3: User Interface of FACTO ly the entity-attribute pairs with high authority scores are considered by FACTO.
We conduct an empirical study for each new approach proposed in this paper.
We also compare the precision and query coverage of FACTO, Google, and Ask.com, and observe that FACTO achieves higher precision and comparable query coverage.
A snapshot of FACTO is shown in Figure 3.
FACTO can be accessed at http://lepton.research.microsoft.com/facto/, with examples at http://lepton.research.microsoft.com/facto/about.htm.
The remaining of this paper is organized as follows.
Related work is discussed in Section 2.
In Section 3 we present our approaches for extracting attribute-values and entities from the web.
Section 4 describes the query answering component, including how to find equivalent names of entities and attributes.
Experimental results are presented along with the technologies in different sections.
We conclude this study in Section 5.
Answering fact lookup questions has been widely studied in information retrieval and natural language processing, with a good survey in [17].
Studies in this area focus on natural language questions, such as  When was James Dean born  and  Who founded the Black Panthers organization .
Different approaches have been proposed, including those based on knowledge base and ontology [11], those based on document retrieval plus answer extraction [18], and those relying on search engines [3].
However, answering natural language questions is very different from identifying and answering fact lookup queries in web search.
First, web users are lazy and seldom submit natural language queries,   they tend to send queries like {morocco capital} instead of {where is the capital of morocco}.
It is difficult to infer whether a question is being asked and what is being asked from such a short query.
Second and more importantly, the above question answering approaches cannot distinguish fact lookup queries from other queries and assume all queries are fact lookup queries.
But in web search only a small portion of queries are fact lookup queries, while the majorities are navigational queries, entity queries, transactional queries, etc.
If we assume every query is a fact lookup query, very likely we will provide irrelevant answers to most of them.
Web table analysis has been widely studied.
Many studies focus on how to distinguish  genuine  tables from the tables for HTML layout purposes.
Wang and Hu [20] propose a machine learning based approach for genuine table classification, which uses features such as mean and standard deviation of numbers of rows and columns.
Similar features are used in the approach by Cafarella et al. [4], which discovers tables together with their column headers, so that tables on the web can be converted into tables in a rela-(a) An attribute-value table on Wikipedia page of Costco (b) A two-dimensional table on seattle.mariners.mlb.com Figure 2: An example of attribute-value table and that of two-dimensional table entities with attribute-value pairs, we create a very large repository of entity-attribute-value triples, which is a most convenient way to store facts in a domain-independent way1.
There are significant challenges in almost every step of building this data repository, such as how to identify attribute-value tables, how to find main entity of each page, and how to remove noise from the data.
With the repository of entity-attribute-value triples, FACTO can provide direct answers to queries consisted of an entity and an attribute, such as {Madonna s birth date}.
The major challenges in query answering include how to answer queries in which users use an entity name or attribute name that is different from our data, how to judge if a query really contains an entity-attribute pair or just happens to be consisted of an entity name and an attribute name, and how to select the value with highest confidence.
Here is a list of major contributions we made in this paper.
  We build FACTO, an end-to-end system for answering fact lookup queries in web search, which achieves higher precision and comparable query coverage comparing with fact lookup engines of Google and Ask.com.
We evaluate the performance of each of the many components of FACTO, as well as that of the whole system.
  We propose a new method to extract entity-attribute-value triples from web pages.
Although attribute-value tables can be identified using existing methods, such data is not useful unless we find the corresponding entities, which seldom appear in the tables.
We find the important entities in web pages by analyzing the web search queries and the pages clicked for them.
We can also extract entities from pages with few or no clicks utilizing the fact that many pages from the same web site have identical format.
Our experiments show FACTO achieves high accuracy in extracting data from the web.
  A major challenge in answering fact lookup queries is how to judge if a query is really looking for some fact about an entity, or happens to have an entity name and an attribute name.
We apply Authority-Hub analysis [13] by treating entity-attribute pairs as authorities and data sources as hubs.
An ent-ity-attribute pair provided by many data sources will have high authority score, and a data source providing many authoritative entity-attribute pairs will have high hub score.
On-
http://wiki.freebase.com/wiki/Data_dumps.
HTML tables in classification.
Tengli et al. [19] propose approaches for distinguishing attribute cells and value cells in tables, in order to extract tabular data with semantics.
Crestan and Pantel study how to classify attribute-value tables in their very recent work [7].
However, they do not have an accurate way to find the entity that is the subject of each attribute-value table.
We solve this problem by matching queries with clicked pages and utilizing sets of pages with same format, which achieves high accuracy.
There are also studies on extracting information from very large document collections.
SnowBall [1] starts from a small set of seed facts and keeps expanding it by learning natural language patterns and applying them on text.
Pa ca et al. propose an approach with the same philosophy [16], and use it to extract one million facts of birth year of people using a seed set of ten facts.
These facts are sorted by their confidences and their accuracy ranges from 98.5% (facts with highest confidences) to 75% (facts with lowest confidences).
KNOWITALL [9] expands the set of facts using information on the web, by submitting appropriate queries to search engines.
TextRunner [2][21] extracts 7.8M facts which are assertions like (Edision, invented, light bulbs) from 9M web pages, with accuracy 80.4%.
WEBTABLES [5] extracts 154M data tables from billions of documents crawled by Google, and allows users to search these tables and find related attributes or tables.
FACTO shares some common philosophy with some approaches mentioned above, such as using machine learning to distinguish tables containing attribute-values, and exploring attribute correlations which is also studied in [5].
But in general FACTO is very different as it is an end-to-end fact lookup engine, which starts from data extraction from web and can answer fact lookup queries with high precision.
FACTO contains two major components: Data extractor that extracts entity-attribute-value triples from the web, and query answering engine which identifies the queries looking for facts about entities and answers with the data extracted.
In this section we will describe the data extractor.
The architecture of the data extractor is shown in Figure 4.
The first component is the table classifier which distinguishes attribute-value tables from the vast majority of other tables.
The problems of identifying  genuine  data tables has been widely studied in [4][7][10][19][20], and we adopt a machine learning based approach similar to that in [4], as described in Section 3.2.
The second component is the URL pattern summarizer, which finds sets of pages with same format [22].
Many web sites contain large sets of pages with same format.
We treat each set of pages of same formats as a data source.
By analyzing data from different pages in a data source, we can filter out many false positive attribute-value tables, as described in Section 3.5.
The third component is the entity extractor that is described in Section 3.4.
It extracts the main entity of each web page by analyzing web search queries with clicks on this page or pages of same format.
For a page that is clicked as a web search result, its main entity can often be found by matching user queries with the page title, header, or contents.
For example, a user may search for {Costco company} and click http://en.wikipedia.org/wiki/Costco (the page containing the table in Figure 2 (a)), and we can infer the main entity for this page is  Costco , which appears in both the query and the page title.
For pages that receive few or no clicks, we identify their main entities by analyzing pages with the Web pages Table Classifier Attribute-value Tables
 Pattern Summarizer Set of pages of same format Entity Extractor Entity & attribute-values Figure 4: Overview of Data Extractor same format and learning how to extract entity names by building HTML wrappers.
The final step is to join the entities with the attribute-values extracted from tables, in order to create a large repository of entity-attribute-value triples.
Before describing our approach, we first explain why we choose attribute-value tables as our main data source.
According to our observation, a significant portion of tables on the web are attribute-value tables, with first column containing attribute names and second column containing values.
Cafarella et al. [5] have shown the web contains a huge number of relational tables, i.e., two-dimensional tables providing values of multiple entities on multiple attributes.
We perform an empirical study to compare the amount of both types of tables.
We randomly select 1000 HTML tables from all indexed pages of Bing, and manually identify each type of tables from them.
The statistics of each type of tables are shown in Table 1.
The number of attribute-value tables is about 4 times that of relational tables (6.6% vs. 1.6%).
This is fairly consistent with the number of attribute-value tables (744M) extracted by our table classifier and the number of relational tables reported by Cafarella et al. in their 2008 paper [5].
On the other hand, each relational table contains about twice more data elements.
We also find 63% of data in relational tables are numerical, while this ratio is only 9% in attribute-value tables.
In general, there is a very large amount of data in attribute-value tables, and the data amount is similar to that of two-dimensional tables.
These two types of tables contain different types of data,   there are more textual data in attribute-value tables and more numerical data in two-dimensional tables.
We use attribute-value tables as our data source because most fact lookup queries look for nonnumerical facts, such as people or locations.
Table 1: Comparing attribute-value tables and relational tables Total number Percentage (in randomly selected tables) Avg.
# entities Avg.
# attributes Avg.
# data elements Percentage of numerical data elements Attr-value tables Relational tables
 web pages)
 Cafarella et al. [5])










 The problem of how to distinguish data tables and non-data tables has been studied in [4][7][10][20], and we just adopt the approach guishing attribute-value tables.
The feature set includes features such as numbers of nonempty cells in the first and second columns, mean and standard deviation of number of nonempty cells in each row, those of text length of cells in each column, and percentage of nonempty cells and distinct cells in each column.
We use SVM to train classifiers for distinguishing attribute-value tables, based on a manually labeled training set containing 138 positive examples (attribute-value tables) and 523 negative ones.
libSVM [6] is used with four types of kernels: Linear, Polynomial, RBF and Sigmoid.
We use the 1000 randomly selected HTML tables to test the accuracy of our table classifier.
They are manually labeled and 66 of them are attribute-value tables.
SVM with a linear kernel achieves highest F1 of 0.759 (precision 0.696 and recall 0.833), and thus we use it in FACTO.
It is very common for a web site to contain many pages in the same format, which contain same or similar sets of attributes for different entities.
For example, celebrina.com has a page for each company (e.g., http://www.celebrina.com/madonna.html for Madonna).
These pages have the same format and each of them contains an attribute-value table with the same set of attributes.
Such sets of uniformly formatted pages also exist on many popular web sites such as Wikipedia, facebook and dpreview.
They are very useful in data extraction, as we can learn the format from some pages and use that to extract data from other pages.
We use an approach from our previous work [22] for finding pages of the same format, and we briefly describe it here for completeness.
In [22] it is found that web pages from a domain can be automatically divided into many groups simply based on their URLs.
This approach has high accuracy in finding sets of pages in the same format,   experiments show that in about 96% of cases two URLs in the same group have same format.
This approach is based on URL patterns, which are regular expressions that can match example, http://www.imdb.com/name/nm*/bio is a URL pattern, which matches with all IMDB URLs of people s biographies.
Because each URL pattern usually contains a set of pages containing same or similar sets of attributes for different entities, we treat each URL pattern as a data source.
Many of the following analyses will be applied on data from each data source separately.
strings.
with For
 The attribute-values extracted from a web page are not useful unless we can identify which entity they are about, which usually needs to be extracted from somewhere else in the page.
According to our observation and experiments (see Section 3.6), the attribute-values from a page are usually about the entity that this page talks about, and we call such entity as the main entity of the page.
It is a challenging task to identify the main entity of a page, as a page may contain many entities.
For each web page, usually there is only one entity that the web search users query for, which is the main entity of the page.
Therefore, the main entity can often be found by matching user queries with clicks on this page with the page contents.
For example, a user may search for  Britney Spears music  and click on http://www.last.fm/music/Britney+Spears, whose title is  Britney Spears   Discover music, videos, concerts, & pictures at Last.fm .
The longest common substring of the query and the title is  Britney Spears , which is the entity that this page is about.
More importantly, every artist s page on www.last.fm (i.e., every page following URL pattern http://www.last.fm/music/*) has a title like  (artist s name)   Discover music, videos, concerts, & pictures at Last.fm .
If we can find this from a few such pages based on user queries, we can extract the main entity from every such page.
FACTO has two components for entity extraction: Wrapper builder and wrapper ranker.
The wrapper builder is responsible for finding the main entity of a page given a user query with click(s) on that page, and building a wrapper for it.
It tries to match the query with the page title and the text in each <h1> element.
If no match is found, it tries to match with each <h2>, and so on, until it finds a match or has tried each element in the page.
When matching a piece of text (e.g., title of a page) with a query, we look for the longest sequence of words in the text, so that each word appears in the query except stop words (e.g.,  a ,  the ,  is ,  for ).
This longest sequence is treated as the entity name, and a regular expression is built by replacing this entity name with  (*) , which indicates the part to be extracted.
FACTO uses wrappers based on HTML tag-paths [15].
For example, when matching query {Britney Spears music} with her page on Last.fm, FACTO builds two wrappers, one from title and one from header.
The one from title is  <html><head><title>(*)   Discover music, videos, concerts, & pictures at Last.fm , and the one from header is  <html><body><div><div><div><h1>(*) .
Both can extract  Britney Spears  as the entity name.
The second component for entity extraction is wrapper ranker.
Usually many wrappers are built for web pages from a data source.
Some of them are incorrect for different reasons, such as some user queries not containing the complete entity name.
We need to rank these wrappers and choose the entity extracted from the best wrapper applicable to each page.
, (1) , (2) | ( )| In the ideal case wrappers should be ranked by precision and recall.
But we cannot directly measure them because we do not use labeled data.
We try to measure precision and recall based on the pages from which entities can be extracted using user queries.
 ( )	= | | ( ),match( , ) | For each web page   and query   with clicks on   , let match( , ) be the entity found by matching   with  .
Let  ( ) be the set of distinct queries with clicks on  .
For any entity   extracted from  , we estimate the probability of   being correct as which is the portion of queries in  ( ) from which   can be inferred as the entity.
Let   be the set of web pages of a data source.
Let   be a wrapper for   and  ( ) be the entity extracted by applying   on  .
The precision of   can be calculated by  ( )  precision( ) =    , ( )  | | , ( ) | The recall of   can be calculated by  ( )  recall( ) =    , ( )  | | ,	 ,	 ( ) | , (3) The score of wrapper   is defined as the harmonic mean of preci-score( )= 2 precision( ) recall( ) precision( ) recall( ) .
(4) For each web page, we extract the main entity using the wrapper with highest score.
The last step of data extraction is to extract attribute-value pairs from attribute-value tables, and combine them with the main entity from each page.
It is rather straightforward to extract attributes and values from the first and second column of an attribute-value table.
However, our table classifier only considers the contents in sion and recall, i.e., an individual table, which is often insufficient to make the right decision.
Some tables classified as attribute-value tables are false positives.
We find the majority of these false positives fall into two categories.
The first category contains tables with same contents that appear on many pages, such as the table in Figure 5 (a).
The second category contains tables containing a set of values without attributes, such as the table in Figure 5 (b).
Log in Help About us Contact us Customer services Store locations (a) Britney Spears Jennifer Lopez Jessica Simpson Paris Hilton Madonna Jessica Alba (b) Figure 5: Examples of false positives of attribute-value tables Both types of false positives can be detected by computing statistics of attribute-value pairs extracted from pages in a data source.
For each attribute appearing in a data source, we compute (1) entropy of all values of this attribute, and (2) number of pages containing this attribute.
If the values of an attribute have very low entropy, this attribute contains little information and is very likely to belong to the first category.
If an attribute appears in a small number of pages, it is unlikely to be a real attribute, which should exist in multiple entities.
We ignore an attribute if it appears in less than five pages or its values have entropy less than

 Value Extraction Here we report the experiment results on extracting entity and attribute-values.
We extract 744M attribute-value tables from 20B web pages in Bing s index on 2010/06/22.
These tables come from 417M web pages.
To extract entities we use all query-click logs from U.S. market during 2008/08/01 to 2009/05/31, which contains each search query and all URLs clicked for it by any user.
Among the 417M web pages containing attribute-value tables, we extract an entity from each of 93.3M pages, which contain 164M attribute-value tables and 749M attribute-value pairs.
No entity is extracted from majority of pages because majority of pages are not in English and thus have no or very few clicks in the logs of U.S. market.
After removing noise according to Section
 We first test the accuracy of FACTO in entity extraction.
We manually inspect 50 uniformly randomly selected entities, and use Amazon Mechanical Turk (MTurk) to inspect 500 of them, in order to see if they are the main entities in the corresponding web pages.
An example MTurk question is shown in Figure 6.
Each question is answered by three workers of MTurk, and all answers of  cannot access page  are ignored unless all three workers say so.
The accuracy according to voted result for each entity is shown in Table 2, and the average accuracy is 97.4%.
Table 2: Accuracy of Entity Extraction Relevant Cannot access page

 MTurk Manual

 Irrelevant Tie



 Excluding cases of  tie  and  cannot access page  #cases Avg.
accuracy 95% conf.
interval of accuracy

 [96.1%, 98.8%] We also test whether the extracted tables contain attribute-values relevant to the specified entity.
Again we manually inspect

 tains attribute-values of the specified entity.
The results are shown in Table 3, and average accuracy is 89.2%.
Table 3: Accuracy of Attribute-Value Tables Relevant Irrelevant Tie No such page, table, or entity MTurk Manual







 Excluding cases of  tie  and  No such page, table, or entity  #cases Avg.
accuracy 95% conf.
interval of accuracy

 [86.6%, 91.9%] Here we show some typical attribute-value tables of two randomly chosen entities.
The first entity is  Ameritrade , which has ten tables extracted, and here are two of them: http://www.linkedin.com/comp-anies/ameritrade?lnk=vw_cprofile Headquarters Greater Omaha Area Industry Financial Services Type Public Company Status Operating Subsidiary Company Size 1001-5000 employees Founded
 http://www.ameritrade.
com Website http://www.wikinvest.com/wiki/Am eritrade




 Debt to Equity   4.1 Interest Coverage Ratio  








 The second entity is  Atlantic City , with 156 attribute-value tables extracted, and here are two examples.
http://eppraisal.com/localpages/ new%20jersey/atlantic%20city.aspx Average Temp in January: Average Temp in July: 76 degrees Average Snowfall: 16.3 inches Average Precipitation: 45.11 inches Average Sunny Days: 31.7 degrees
 http://fr.weather.yahoo.com/usnj/ usnj0015/index_c.html Humidit :
 N/13 km/h Vent: 16.09 km Visibilit :
 Point de ros e: Pression: 1019 mb Lever du soleil : 6:49 Coucher du soleil 18:50

 After extracting entity-attribute-value data from the web, we store them in a relational database with an index built on entity and attribute.
Given a web search query, FACTO tries every way to interpret it as a fact lookup query by considering part of the query as an entity and part as an attribute.
A major challenge is how to judge if a query is really a fact lookup query, or happens to contain an entity name and an attribute name.
For example, FACTO extracts an entity  Red  with an attribute  hair  from http://dcanimated.wikia.com/wiki/Red, and thus it will provide an answer for query {red hair}, which is not a fact lookup query.
We Figure 6: A question for entity relevance on Mechanical Turk use Authority-Hub analysis [13] by treating entit as authorities and data sources as hubs.
An entity-attribute pair as authorities and data sources as hubs.
An entity provided by many data sources will have high authority score, and provided by many data sources will have high authority score, and a data source providing many authoritative entity-attribute pairs a data source providing many authoritative entity will have high hub score.
We select the entity-attribute pairs with attribute pairs with high authority scores as the ones to be answered by FACTO.
high authority scores as the ones to be answered by Another challenge is that users often use entity users often use entity/attribute names .
For example, our data-that are different from those in our data.
For example, our dat base contains  date of birth  of  Britney Spears , and users may y Spears , and users may query with {britney date of birth} or {dob of britney spears}.
We query with {britney date of birth} or {dob of britney spears}.
We use an approach for detecting similar queries [8] [8] to detect equivalent entity names, because entity names are often often used as queries.
In contrast, attribute names are not directly used as queries, and In contrast, attribute names are not directly used as queries, and we detect equivalent attribute names based on the observation that d on the observation that an entity should have same or similar values on the same attribute an entity should have same or similar values on the same attribute in two data sources.
If two attributes are often associated with the in two data sources.
If two attributes are often associated with the same value for same entity, they are likely to be equivalent.
same value for same entity, they are likely to be equivalent.
For each possible interpretation, FACTO queries the queries the database using the entity and attribute and also their equivalent forms.
If using the entity and attribute and also their equivalent forms.
If certain combinations of entity and attribute exist in the database, xist in the database, FACTO retrieves the corresponding values and select selects a value with highest confidence among all retrieved values, which is returned highest confidence among all retrieved values, which as the answer of the query.
This procedure is illustrated in Figure as the answer of the query.
This procedure is illustrated in
 .
We will describe each component in this section.
We use a rule-based approach for interpreting queries.
There exist based approach for interpreting queries.
There exist more advanced approaches on query parsing such as [14].
But we more advanced approaches on query parsing such as choose a simple, rule-based approach because approaches like based approach because approaches like [14] require manually labeled training data that is not available to require manually labeled training data that is not available to us.
Moreover, web users tend to use short and simple queries in-us.
Moreover, web users tend to use short and simple queries i stead, and a more advanced query parsing approach is unlikely to stead, and a more advanced query parsing approach is unlikely to improve the query coverage significantly.
We use e to denote an entity and a an attribute.
an attribute.
The following five rules are used in FACTO to convert a query into an entity to convert a query into an entity ,  (what|who|when attribute pair:  e a ,  e s a ,  (the)?
a of e , |where) (is|are|was|were) (the)?
a of (the)?
e ,  ,  (what|who|when| where) (is|are|was|were) e s a .
We also have four specific rules We also have four specific rules for  how long ,  how old ,  when was someone born  and for  how long ,  how old ,  when was someone born  and  where was someone born .
Given a user query, FACTO finds all possible interpretations possible interpretations of by matching it with every rule in every poss-entity-attribute pairs by matching it with every rule ible way.
FACTO also maintains the list of all attributes in main also maintains the list of all attributes in main memory, so that any interpretation without an existing attribute is memory, so that any interpretation without an existing attribute is discarded.
All remaining interpretations will go through the fo All remaining interpretations will go through the following procedure for query answering.
There are many queries that happen to contain an entity name and There are many queries that happen to contain an entity name and Figure 7: Overview of Query Answering System Query Answering System an attribute name.
For example, from from http://en.wikipedia.org/ wiki/Sand_(film) FACTO extracts an entity  Sand  with an extracts an entity  Sand  with an attribute  language , and it may provide may provide an answer for query {sand language}, which actually looks for a book called  The actually looks for a book called  The Language of Sand .
We say an entity--attribute pair is valid if (1) this attribute exists for this entity, and (2) the query constructed by this attribute exists for this entity, and (2) attribute asks for the specific concatenating the entity and the attribute attribute of the entity.
For example, ( C China ,  capital ) is a valid  ,  language ) and ( Seattle , entity-attribute pair, while ( Sand ,   When the intent of a query is not obvious, we infer  WA ) are not.
When the intent of a query it from the clicked URLs of Bing.
We observe that if an entity-attribute pair is attribute pair is provided by many , it is more likely to be a valid pair.
This problem is data sources, it is more likely to be a valid pair analogous to Authority-Hub analysis Hub analysis [13] by considering entity-attribute pairs as authorities and data sources as hubs.
An entity-attribute pairs as authorities and data sources as hubs.
An entity attribute pair provided by many data sources is likely to be valid attribute pair provided by many data sources and popular, and should have high authori have high authority score.
Similarly, a data source providing many authoritative entity-attribute pairs data source providing many authoritative enti We implement the iterative method should have high hub score.
We implement the iterative using MapReduce on a distri-for computing authority-hub scores using MapReduce buted environment of Microsoft [12].
.
Initially each data source we compute the scores of entity-has score of 1.
In each iteration we compute the s ose of the data sources.
After each itera-attribute pairs and then those of the data sources.
tion we compute the relative change of ge of scores, which is defined the vector of scores of all entity-attribute pairs.
We keep based on the vector of scores of all entity iterating until the relative change drops below 0.001, which r change drops below 0.001, which requires five iterations in this problem.
We test the accuracy of this approach by studying if it assigns We test the accuracy of this approach by higher score to valid entity-attribute pairs attribute pairs than to invalid ones.
We randomly select 25 entities with at least 10 attributes, and select a randomly select 25 entities with at least 10 attributes, and select a an invalid attribute for each entity.
In this way we get 25 valid and an invalid attribute for each entity valid entity-attribute pairs and 25 invalid ones.
For each va pairs and 25 invalid ones.
For each valid pair and each invalid pair (either for same entity or not), we test if our and each invalid pair (either for same entity or not), we approach assigns higher score to the valid pair.
The accuracy is approach assigns higher score to the valid pair.
defined as the percentage of cases our approach is correct.
The cases our approach is correct.
The accuracy is 85.74%.
There are 509M entity-attribute pairs.
After inspecting the data, attribute pairs.
After inspecting the data, the 73M pairs with highest authority scores as the entity-we use the 73M pairs with highest authority scores attribute pairs that FACTO will provide answers for.
will provide answers for.
One major challenge in using extracted data to answer queries is One major challenge in using extracted that users may use an attribute name that is different from our that users may use an attribute name that is different from our data.
Similarly, different web sites often use different names for data.
Similarly, different web sites often use different names for the same attribute, such as  birth date  and  date of birth ,  direc-the same attribute, such as  birth date  and  date of birth ,  dire tor  and  directed by .
In this section we study how to identify section we study how to identify equivalent attributes from different web sites, which will help us equivalent attributes from different web sites, which will help us match the attribute names in user queries with those in our data.
match the attribute names in user queries with those in our data.
Our key observation is that if an entity appears in multiple data Our key observation is that if an entity appears in multiple data sources, it should have same or similar values on two equivalent same or similar values on two equivalent attributes.
Therefore, if two attributes from two data sources attributes.
Therefore, if two attributes from two data sources usually have same value on same entity, they are likely to be usually have same value on same entity, they are likely to be equivalent.
However, we do not know if two entities in different equivalent.
However, we do not know if two entities in different data sources are the same, even if they have same name.
Thus we e, even if they have same name.
Thus we also need to infer equivalence between entities based on their also need to infer equivalence between entities based on their attributes.
This problem has been studied in [23] [23], which aims at finding equivalent attribute names across two databases.
It infers the simi-equivalent attribute names across two databases.
It infers the sim larities between attributes according to whether they have same larities between attributes according to whether they have same value on same entity, and infers the similarities between entities value on same entity, and infers the similarities between entities based on whether they have same value on same attribute.
This based on whether they have same value on sam Two attributes are Count %Correct Example of two attributes Any Both in English



 One in English

 Neither in English

  population  vs.  population (estimate)  of  zephyrhills   family  vs.  familie  (Dutch) of  paeonia      (waiting) vs.
      (standby) of  nokia 6260 slide  problem is then converted into Authority-Hub analysis [13] by treating attribute-pairs as authorities and entity-pairs as hubs.
We implement a variant of this approach using MapReduce and apply it to the entity-attribute-value triples extracted from the Web.
For each pair of attributes (or each pair of entities), we compute their similarity based on their values on different entities (or attributes).
An iterative procedure is used, which assigns initial similarity of 1 to all pairs of entities with same name, and keeps computing the similarities between attributes and those between entities based on each other.
We ignore pairs of entities with different names because (1) there are too many entities and it is impossible to compute for each pair of them, and (2) our goal is to detect equivalence between attributes.
The above approach can detect equivalent attributes in different data sources.
To answer queries we need to know equivalent attribute names independent of any data source.
For example, we need to know  dob  and  date of birth  are equivalent, instead of  dob  from a data source and  date of birth  from another.
This is done by aggregating the equivalence relationships between attributes from different data sources.
We define the similarity between two attribute names   and  	 as the weighted average of similarities between pairs of sim( = )=    ,  attribute   and , where   is  , =log ( , )| . = . .
attributes with these two names in different data sources:  ,   ,     ,  the name of (7) Here we evaluate the equivalent attributes detected.
The total number of attributes in different data sources is 1.15M.
FACTO analyzes the 2.99M pairs of attributes (each in a different data source), with each pair having the same value on at least one entity.
It finds 1.26M pairs of attributes with similarity above 0.5.
We evaluate the accuracy of equivalent attributes by uniformly Table 5: Synonymous attribute names address location phone price weight telephone list price adresse (French) phone number cost direcci n (Spanish) admissions regular price street address alamat (Malay) tel general phone our price your price headquarters general informa-license tion gewicht (Dutch) poids (French) peso (Spanish) waga (Polish) peso con batteria in dotazione (Italian) tr ng l ng (Vietnamese) adresa (Romanian) telephone# address 1 tel fono (Span-paperback hardcover masa cia a (Polish) masa (Polish) mailing address ish) office barnesand-noble weight kg town information us list price vikt (Swedish) randomly selecting 50 pairs of attributes and manually label if each pair are truly equivalent.
Each pair of attributes come from different data sources, and in 6 of the 50 pairs the attributes have same name.
We do not use MTurk because many attribute names are not in English, and MTurk workers may not be diligent enough to find out their meanings.
Google Language Detection (http://www.google.com/uds/samples/language/detect.html) is used to detect the language of a non-English attribute name.
The categorized results are shown in Table 4, together with an example in each category.
The accuracy is 88%, and the false positives come from attributes with similar sets of values.
For example,  overall  and  gameplay  from two game web sites are considered equivalent because both are for ratings of games.
The above experiment is about equivalence of attributes in data sources.
We also check accuracy of synonyms for attribute name, which are independent of data sources.
Table 5 shows the top 10 synonymous attribute names (ranked by similarity) of the four attribute names appearing in most data sources:  address ,  phone ,  price ,  weight .
We can see most synonyms have same or similar meanings with the original attribute.
Besides equivalent attribute names, we also need to find equivalent entity names because users often use different entity names from those in our database.
Equivalent entity names are generated by finding similar queries of each entity name.
The query similarity is computed using a traditional approach in [8] and we briefly describe it here.
We convert each query into a vector, by considering each URL as a dimension and the number of clicks on each URL as the value on that dimension.
The similarity between two queries is just the cosine similarity between their vectors.
We only consider pairs of queries with similarity at least 0.5 when finding equivalent names of entities.
The equivalent entity names are pre-Table 6: Result of Equivalent Entity Detection Two entities are Portion Example Exactly same One belongs to the other

  australian job  vs.  job in australia   flightless bird  vs.  large flightless bird  One is a certain aspect of the other
  will county  vs.  map of will county  Different entities
  1972 chevrolet suburban  vs.  1968 chevrolet suburban  computed and stored in an indexed database table.
We uniformly randomly select 100 equivalent names of entities and manually label if they are truly equivalent, as shown in Table 6.
Given a web search query, FACTO generates every possible combination of entity   and attribute  .
For each combination, it queries the database to get all values, together with URL of the page providing each value.
FACTO also tries to replace a with every equivalent attribute name of a, or replace e with every equivalent entity name of e. For example, if a query is {britney birthdate}, FACTO will consider  britney  as an entity and  birthdate  as an attribute.
Besides getting values for this combination, it will replace  birthdate  with  date of birth  and  dob , and  britney  with  britney spears .
FACTO does not replace both entity and attribute at the same time, in order to avoid generating queries with different semantic meanings and to reduce computation cost.
After retrieving a set of values for a query, FACTO needs to select the best value as the answer and shows all URLs providing this answer.
Among the values retrieved, some are for the original  , ,  ( )  query-independent importance of a URL used by Bing Search, The goal of result aggregation is to select a value with highest confidence as the answer.
entity   and attribute  , while some others are for an equivalent entity   or equivalent attribute  .
Let  ( , ) and  ( , ) be the similarities between   and   and that between   and  .
For each value  , FACTO stores the URL of the page containing  , and the StaticRank of that URL.
(StaticRank is a measure for which is similar to PageRank.)
The weight of a value   from URL   is defined as  ( )= ( ),	if	 		is	of	 	and	 ;  ( ) ( , ),	if	 		is	of	 .  ( ) ( , ),	if	 		is	of	 ; of them being correct.
Given all values  , ,  from URLs  , ,  for  	and	 , we define the score of each value   as  ( )= ( )+  ,  If we see the same value from different web domains2, we are more confident that this value is correct.
Some values may be similar to each other, which also improves the confidence of each Similarity between different values is defined based on their types.
FACTO considers seven types of values: (1) string, (2) date/time, (3) numerical, (4) duration, (5) length, (6) area, (7) weight.
For example,  6.4 lb  is parsed as a value of weight, and  July 1, 1978  as a value of date/time.
A value is of type (1) if it does not belong any other type.
The similarity between each type of values is defined based on our experiences with data3.
After computing the score of each value, FACTO selects the value with highest score as the answer to the query.
It also collects all values consistent with the selected value (having similarity no less than 0.9), and show these values when the user clicks  Show All .
Finally we study the performance of FACTO in answering web search queries, and compare that to the fact lookup engine of Google, and that of Ask.com which puts more focus on NLP-based question answering.
We use a server with two Intel Xeon
 base server, which runs Windows Server 2008 and SQL server
 server.
To test the performance on web search queries, we randomly sample 134K distinct queries from the query logs of Bing from Jan 2008 to March 2010, and the probability of a query being selected is proportional to its query frequency in this period.
We send each query to FACTO, Google and Ask.com to retrieve the fact lookup answers when available.
Examples of fact lookup answers of Google are shown in Figure 1, and that of Ask.com is shown in Figure 8.
We crawled Google on 2010/08/10, Ask.com
 hoo.co.jp  for http://blogs.yahoo.co.jp/kazami_0922.
fined as  ( , )= 1 4  | | | | | |,0 .
The similarity The similarity between strings   and   is  ( , )=  1 4   ( , )  . . ,0 .
The similarity between dif-between values of type (4) to (7) is defined in the same way.
ferent date/time values is zero.
The similarity between different types of values is zero.
Figure 8: Fact lookup answer of Ask.com for query {Tom Hanks date of birth} on 2010/08/20, and use the indexed documents of Bing on
 other samples of queries (which should be used by Google and Asko.com as well), but not the query sample used for testing.
In order to judge the relevance of fact lookup answers accurately and consistently, a comprehensive set of guidelines are needed.
First, we categorize the queries into five types as in Table 7, in which the first two types are fact lookup queries.
We do not consider unit conversion queries and definition queries in our study, because it requires very different (and usually specially designed) approaches to answer them.
If the intent of a query is not obvious, we infer that form the clicked URLs of Bing.
Table 7: Types of queries Type EA The query asks for an aspect/attribute/fact Meaning about an entity F The query is about a general fact without a particular entity Example how old is george bush current prime interest rate
 The query is about an entity without any full moon band specific aspect/attribute O Other types of queries for which the user seeks answers or recommendations, including questions w\o definitive answers, definition queries and unit conversion queries W The direct answer should not be triggered because the user is not seeking any direct answer (e.g., navigational query) how to write a thesis paper download free movies The following guidelines are strictly followed when evaluating the correctness of each direct answer.
A correct answer must have the following three properties:
 ple fact, the answer should be true.
If the query asks for a list of items, the answer should contain majority of items (if different items have similar importance) or the several most important ones.
This is judged based on information at the time of crawling.
(Some questions may not have a known definitive answer, such as {Tom Cruise height}.
If an answer is popularly used in many websites, we consider it to be correct.)
sult URL is about the same entity if the query contains an entity, or about the question being asked.
In some rare cases the first result of a search engine is obviously irrelevant, we consider an answer to be consistent if it is consistent the most clicked result URL of Bing.
irrelevant contents.
However, it is OK to contain some related contents not being asked.
For example, if a user searches for birth date of someone, and the answer contains birth place as well, it is still considered as relevant.
An answer indicating  I don't know the answer  is considered as wrong answer.
If the intent of a query is not obvious and we cannot understand it from the result URLs of the corresponding search engine or Bing's clicked URLs, we say we cannot judge the correctness.
kup queries on Google, Ask.com, FACTO and FACTO without using equivalent attributes.
We can see FACTO achieves higher precision and its query coverage is between Google and Ask.com.
Detailed results are described below.
Table 8: Accuracy of fact lookup answers of Google, Ask.com, FACTO and FACTO w\o equivalent attributes Google Ask.com
 Precision


 FACTO w\o equv-attr
 #correct answers



 The accuracy of fact lookup answers of Google is shown in Table 9, where accuracy is defined by  #correct / (#correct + #wrong) .
We also show the precision of fact lookup queries (EA+F), which is the number of fact lookup queries answered correctly divided by the number of queries triggering an answer that provides a fact.
The false positives are caused by treating a non-fact-lookup query as a fact lookup query.
Google has very few false positives, which shows it confines the triggering of its fact lookup answer very well.
However, the value of its answer is sometimes inconsistent with the attribute.
For example, for query {how old is george bush} it answers  george bush date of birth   1946 , although 1946 is not a date and the right answer should be  July 6, 1946 .
Majority of errors are simply incorrect answers, such as answering {ps3 release date} with  2010 .
We keep the result pages of Google with fact answers for our testing queries at http://lepton.research.microsoft.com/facto/doc/google_answer.zip.
Table 9: Accuracy of fact lookup answers of Google Type All




 #queries correct wrong

















 cannot judge accuracy











 fact lookup queries correct wrong

 false positive precision

 The accuracy of fact lookup answers of Ask.com is shown in Table 10.
Ask.com shows fact lookup answer more often than Google and FACTO, which might be the cause for its lower precision.
For example, it answers {food new york mets stadium} with  The New York Mets play at Citi Field , which is not what the user asks for.
It gives impressive results for some hard fact lookup queries.
For example, for query {raven symone gives birth} it answers  Raven-Symon  is not and has never been pregnant according to reports , which shows it knows what has not happened besides what has.
The result pages of Ask.com with fact answers Table 10: Accuracy of fact lookup answers of Ask.com Type All




 #queries correct wrong

















 cannot judge accuracy











 fact lookup queries correct wrong

 false positive precision

 can http://lepton.research.microsoft.com/facto/doc/ask_answer.zip.
accessed be at Table 11 shows the accuracy of FACTO.
FACTO provides correct answer more often than the other two, although it has some false positives.
For example, it answers {british publisher} with the publisher of book  The British .
We summarize the results of the three engines at http://lepton.research.microsoft.com/facto/doc/ fact_answer_results.xlsx, and FACTO can be accessed at http://lepton.research.microsoft.com/facto/.
We also test the performance of FACTO without using equivalent entity names and that without using equivalent attribute names, as shown in Table 12 and Table 13.
We can see equivalent entities are very important to FACTO, as both precision and query coverage drops when running without them.
When not using equivalent attributes, precision increases and query coverage drops.
Table 11: Accuracy of fact lookup answers of FACTO Type All




 #queries correct wrong























 cannot judge accuracy





 fact lookup queries correct wrong

 false positive precision

 Table 12: FACTO w\o equivalent entities Type All
 #queries correct wrong





 cannot judge accuracy



 fact lookup queries correct wrong

 false positive precision

 Table 13: FACTO w\o equivalent attributes #queries correct wrong cannot judge accuracy



 Type All






 fact lookup queries correct wrong

 false positive precision

 We analyze what kinds of errors are made by each engine.
The errors are categorized as follows: (1) Wrong answer, such as answering  English  for {Turkey language}.
(2) Incomplete answer, such as answering  2009  for {Diablo 3 release date}.
(3) Answer is not for the query, such as answering  santa mo-nica college founded   1929  for {how santa monica college was founded}.
(4) Entity query: The query is an entity and does not contain any attribute, such as {Microsoft publisher}.
(5) Navigational query, such as {Lil Wayne myspace}.
(6) Should not trigger answer for query, such as {watch free movies}.
(7) Answer indicates  I don t know , such as answering  Conversion from hours to miles is not available.  The types of errors made by Google, Ask.com and FACTO are shown in Table 14.
We can see wrong answers do not account for majority of the errors, and many errors are caused by failing to




 100-200ms 200-300ms 300-400ms 400-500ms 500-600ms >600ms Response Time Figure 9: Response time of queries with answers understand the query correctly.
This suggests that query understanding is at least as challenging as information extraction.
[2] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead and O. Etzioni.
Open information extraction from the Web.
IJ-
[3] E. Brill, S. Dumais and M. Banko.
An analysis of the AskMSR question-answering system.
In EMNLP 02.
[4] M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and Y.
Zhang.
Uncovering the relational web.
WebDB 08.
[5] M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and Y.
Zhang.
WebTables: Exploring the power of tables on the Web.
VLDB 08.
[6] C.-C. Chang and C.-J.
Lin.
LIBSVM: a library for support vector machines, 2001. www.csie.ntu.edu.tw/~cjlin/libsvm [7] E. Crestan and P. Pantel.
Web-scale knowledge extraction Table 14: Types of errors by Google, Ask.com and FACTO from semi-structured tables.
WWW 10.
Type of error Wrong answer Incomplete answer Answer is not for the query Entity query Navigational query Should not trigger answer Answer  I don t know 

 Google Ask.com



















 The average response time of FACTO is 10.6ms for every query, and 381ms for every query with an answer returned.
Figure 9 shows the distribution of response time for queries with answers.
We perform Student s t-test on the precision and coverage of the three engines plus FACTO w\o equivalent attributes (denoted by *).
Table 15 contains the results, which shows the probabili-
ty of FACTO having higher precision than Google is 0.695, and that against Ask.com is 0.999.
The query coverage of the fact lookup engines are in the range of 0.05% to 0.2%, which actually correspond to a large number of queries considering there are >4B searches per day.
We also believe fact lookup answers will gradually change users  habits from searching for web pages to searching for facts.
Table 15: T-test on accuracy/coverage of different engines.
Each cell in upper-right part contains the probability of engine of the row having higher precision than engine of the column.
Lower-left part contains that for coverage.
One-tail probability
 *
 Google Ask.com

 * Google Ask.com











 Coverage
 r e c i s i o n

 In this paper we describe FACTO, an end-to-end system for answering fact lookup queries with comparable performance with the fact lookup engines of popular search engines.
Our future plan includes extracting more types of data to increase query coverage and precision, considering the temporal aspects of extracted information to provide more timely answers, and performing deep analysis on the data to improve data quality.
