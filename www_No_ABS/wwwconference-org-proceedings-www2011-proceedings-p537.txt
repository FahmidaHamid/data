Online social networking services have brought to the public a new style of social lives parallel to our day-to-day o ine activities.
Popular social network sites, such as Facebook, Linkedin and Twitter have already gathered billions of extensively acting users and are still attracting thousands of Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
enthusiastic newbies each day.
Doubtlessly, social networks have become one of today s major platforms for building friendship and sharing interests.
service item user friendship interest Figure 1: A social network graph.
The connections consist of both (unipartite) edges within the user-user friendship network and bipartite user-item interactions in the interest network.
Fundamental to all social network services is the goal to e ectively model the interests of a user and the friendship between users [21].
On the one hand, by capturing a user s interests and accordingly exploiting the opportunity to serve her/him with potentially interesting service items (e.g.
news, games, advertisements, products), one can improve the satisfaction of a user s participation and boost the revenue of a social network site as well (e.g.
via product purchases, virtual transactions, advertisement clicks).
On the other hand, connecting people with common interests is not only important for improving existing users  loyalty, but also helps to attract new costumers to boost the site s tra c.
In fact, friendship prediction (a.k.a.
link prediction) and interest targeting (a.k.a.
service recommendation) are two important tools available in almost all the major social network sites.
Both activities which occur routinely in a social network have accrued a tremendous wealth of interaction traces, both among users (i.e. friendship network) and between users and service items (i.e. interest network).
Figure 1 depicts a typical topology of a heterogeneous graph in the context of social networks.
Modeling user interests and friendship in social networks raises unique challenges to both research and engineering communities.
The information about a user s behaviors is often scattered in both friendship and interest networks, involving other users that are closely connected to the user damental mechanism that drives the dynamics of networks is the underlying social phenomenon of homophily [18]: people with similar interest tend to connect to each other and people of similar interest are more likely to be friends.
Traditional user pro ling approaches often do not take full advantage of this fact.
Instead they either employ feature engineering to generate handcrafted meta-descriptors as  ngerprint for a user [26, 5] or they extract a set of latent features by factorizing a user s registered pro le data; for example, by means of sparse coding [12] or latent Dirichlet allocation [2].
These approaches could be inaccurate because neither user friendship nor user behavior information is taken into account.
Recent approaches resort to collaborative  ltering (CF) techniques [3, 23, 1, 10] to pro le user interests by collaboratively uncovering user behaviors, where users are assumed to be unrelated to each other.
While CF performs well in recommendation systems where decisions are mainly made individually and independently, it could fail in the context of social networks where user interactions substantially in uence decision making [7, 18].
Modeling friendship is equally challenging.
A typical social network is a graph both large and sparse, involving hundreds of millions of users with each being connected to only a tiny proportion of the whole virtual world.
This property rules out traditional spectral algorithms for graph mining [19, 20] and calls for algorithms that are both e cient to handle large scale connections and capable of reliably learning from rare, noisy and largely missing observations.
Unfortunately, progress on this topic to date is limited [13].
This paper exploits the important role homophily plays in social networks.
We show that friendship and interest information is highly correlated (i.e. closely-connected friends tend to have similar interests) and mutually helpful (i.e.
much higher performance for both friendship prediction and interest targeting could be achieved if coupling the two processes to exploit both sources of evidence simultaneously).
We present a friendship-interest propagation (FIP) model that integrates the learning for interest targeting and friendship prediction into one single process.
The key idea in FIP is to associate latent factors with both users and items, and to de ne coupled models to encode both interest and friendship information.
In particular, FIP de- nes a shared latent factor to assure dynamical interaction between friendship network and interest network during the learning process.
In doing so, FIP integrates both interest and friendship networks to connect a user to both items of potential interest and other users with similar interests.
FIP hereby provides a single uni ed framework to address both link prediction and interest targeting while enjoying the resources of both sources of evidence.
Experiments on Yahoo!
Pulse demonstrate that, by coupling friendship with interest, FIP achieves much higher performance on both tasks.
The contributions of this work are three-fold:
 that propagates two di erent types of evidence through heterogeneous connections.
work, discuss the  exibility in the choices of loss objectives (e.g.
(cid:2)2, logistic regression, Huber s loss) and regularization penalties (e.g.
sparse coding, (cid:2)2 penalties) and we benchmark di erent variants in a real-world social networking system;
 scheme for bias correction based on pseudo-negative sampling to avoid over tting, and we also deliver an optimization package that allows distributed optimization on streaming data.
Outline:  2 describes the background.
 3 presents the detailed FIP model and our distributed implementation.
 4 reports experiments and results.
 5 reviews related work and  6 summarizes the results.
We begin by brie y reviewing the state-of-the-art.
This will come in handy as we will link them to our model in  3.
Modeling dyadic interactions is the heart of many web applications, including link prediction and interest targeting.
Typically, a pair of instances from two parties (such as users and items), i   I and j   J , interact with each other with a response yij   Y.
The mapping {(i, j)   yij where i   I, j   J } constitutes a large matrix Y   Y|I| |J | , of which only a tiny proportion of entries are observable; the goal is to infer the value of a missing entry y i j, given an incoming pair ( i,  j).
Essentially, the observed interactions de ne a graph, either unipartite (when I = J ) or bipartite.
The task amounts to propagating the sparse observations to the remainder (unobserved) part of the matrix.
For convenience we will henceforth refer to i as user and j as item unless stated otherwise.
Interest targeting, or (service) recommendation, works with a bipartite graph between two di erent parties, e.g.
user i and item j.
It aims at matching the best item j to a given user i.
We consider collaborative  ltering (CF) approaches, which tackle the problem by learning from past interactions.
  Neighborhood models.
A popular approach to CF is based on the principle of locality of dependencies, which assumes that the interaction between user i and item j can be restored solely upon the observations of neighboring users or items [24, 17].
Such neighborhood-based models therefore propagate similar items to a particular user (item-oriented) or recommend a particular item to similar users (user-oriented).
Basically, it predicts the interest of user i to item j by averaging the neighboring observations.
For instance, the user-oriented model uses: i(cid:2) i  ii(cid:2) yi(cid:2)j (cid:2) (cid:2)  yij = i(cid:2) i  ii(cid:2) , where  ii(cid:2) measures the similarity, e.g.
Pearson correlation coe cient, between user i and its neighbor i (cid:4)    i.
Latent factor models.
This class of methods attempt to learn informative latent factors to uncover the dyadic interactions.
The basic idea is to associate latent factors,1
 contains a constant component so as to absorb user/item-speci c o set into latent factors.
 i   R k for each item j, and assume a multiplicative model for the interaction response p(yij|i, j) = p(yij|  (cid:5) i  j ;  ).
This way the factors could explain past interactions and in turn make prediction for future ones.
This model implicitly encodes the Aldous-Hoover theorem [6] for exchangeable matrices   yij are independent from each other given  i and  j .
Parameter estimation for the model reduces to a low-rank approximation of the matrix Y that naturally embeds both users and items into a vector space in which the inner product   (cid:5) i  j directly re ect the semantic relatedness.
Latent factor models have gained tremendous successes in recommendation systems and have even become the current state-of-the-art for CF [10, 1].
A known drawback for such models is that, because it is learned only upon past interactions, the generalization performance is usually poor for completely new entities, i.e. unseen users or items, for which the observations are missing at the training stage.
This scenario is well-known as the  cold-start problem  in recommendation systems.
The recently proposed regression based latent factor model (RLFM) [1] addresses this problem by incorporating entity features into latent factor learning.
The key idea is to use observable features to explain the learned latent variables (e.g.
by regression or factorization).
Suppose for each user and each item, there are observable features, xi for i (e.g.
user s demographic information, self-crafted registration pro les) and xj for j (e.g.
content of a document, description of a product), as shown in Figure 2, RLFM [1] assumes the following dependencies: yij   p(yij|   i   p( i|xi)  j   p( j|xj ) (cid:5) i  j ;  ).
Neighborhood based latent factor models.
It is natural to combine the neighborhood models and latent factor models.
A recent example is discussed [9], where the basic idea is to apply the locality of dependencies directly to the latent factors, for example: i(cid:2) i  ii(cid:2)  i(cid:2) i(cid:2) i  ii(cid:2) yij   p(yij|   (cid:5) i  j;  ).
(cid:2) (cid:2)  i = (1) This model2 which is quite similar to [9] was deployed on the Net ix data yielding signi cantly better performances over both pure-neighborhood and pure latent factor models.
Friendship (link) prediction recommends users to other users in the hope of acquainting people who were previously not connected in the network (or even unfamiliar with each other).
Unlike interest targeting, the user network is unipar-(cid:4) tite.
For a pair of users (i, i ) the observation whether they are connected is a binary value Sii(cid:2) .
Link prediction crucially in uences both the tra c and the revenue of a social network and it is hence recognized as one of the key tasks in social network analysis.
Ideally, our goal is to learn a distribution over jointly exchangeable matrices (e.g.
by applying the Aldous-Hoover factorization theorem).
For reasons of practicality we pick a  nite-dimensional factorization instead, which we shall discuss in the next section.
Before we do so, let us brie y review existing approaches.
Some of them employ random walk methods [14, 22] or spectral graph algorithms [19, 20].
ix jx   i   j jx   j ijy
 y (a) ix   i (b) ijy
 y 'ix   'i 'iis
 s Figure 2: Graphical representations of (a) regression based latent factor model (RLFM) and (b) friendship-interest propagation model (FIP).
Random Walk.
A random walk on the graph S is a reversible Markov chain on the vertexes I.
The transi-(cid:4) tion probability from the vertex i to vertex i is de ned (cid:4)|i) = sii(cid:2) /di.
Here di denotes the degree of vertex i; p(i (cid:4) sii(cid:2) the connection weight between nodes i and i .
Vertexes are considered close whenever the hitting time is small or whenever the di usion probability is large.
Spectral Algorithms.
For the given network S, the un-normalized Laplacian is de ned by L = D   S, where D is a diagonal matrix with Dii = di.
Spectral algorithms di use the connections by maximizing the spectral smoothness to obtain the intrinsic kinship de ned by the dominant eigen-vectors of the Laplacian sii(cid:2) (cid:2)ui   ui(cid:2)(cid:2)2 = 2U LU(cid:5), where U = [u1, .
.
.
, u|I|].
(2) (cid:3) i,i(cid:2)

 We now consider interest targeting and link prediction in the context of social network, where evidence for both interest and friendship are available, allowing us to solve both tasks in a single framework.
The rationale is that friendship and interest information are to some degree correlated,3 i.e.
the network exhibits homophily [18] and the propagation of friendship and interest would be mutually reinforcing if modeled jointly.
In this section we present our model of friendship-interest propagation (FIP).
We start with a probabilistic formulation, discuss di erent variants of the model and its implementation within an optimization framework, and then distinguish our model from existing works.
The nontrivial correlation between interest and friendship motivates joint modeling of both sources of evidence.
As shown in Figure 2, the friendship-interest propagation(FIP) model simultaneously encodes the two heterogeneous types of dyadic relationships: the user-item interactions {yij|i   I, j   J }, and user-user connections {sii(cid:2)|i, i (cid:4)   I}.
Our model is built on latent factor models.
interest correlation (Pearson score, max 1.0) between two directly-linked friends is 0.43, much higher than average.
item dyads, yij , we assume that for each user i and item j there exist observable properties xi (e.g.
a user s self-crafted registration  les) and xj (e.g.
a textual description of a service item)4.
Moreover, we also assume that there exist some subtle properties which cannot be observed directly, such as a user s interests, a service item s semantic topics.
We denote these latent features by  i for i and  j for j respectively.
We assume the response yij depends on both types of features (i.e. observable and latent): yij   p(yij| i,  j , xi, xj,  ),  i   p( i|xi)  j   p( j|xj ) where   denotes the set of hyper-parameters.
To design a concrete model, one needs to specify distributions for the dependencies,  i|xi,  j|xj, and yij|xi, xj,  i,  j .
This model is essentially an integration of collaborative  ltering [1] and content  ltering [4].
On the one hand, if the user i or item j has no or merely non-informative observable features such that we have access to only their identity and past interactions, the model degrades to a factorization-style collaborative  ltering algorithms [23].
On the other hand, if we assume that  i and  j are irrelevant, for instance, if i or j is totally new to the system such that there is no interaction involving either of them as in a cold-start setting, this model becomes the classical feature-based recommendation algorithms [3, 31, 4], which predict the interaction response yij purely based on the observed properties of i and j, and are commonly used in, e.g.
webpage ranking [31], advertisement targeting [3], and content recommendation [4].
Modeling Friendship Evidence.
We now extend the interest model to incorporate the social friendship-connection information among users.
For this purpose, we de ne a random walk process for user-user networking.
But unlike traditional random walk models [14, 22], we assume a user i is fully characterized by her observable features xi and latent factor  i, and devise the following model for user-user transition:  i   p( i|xi,  ) and sii(cid:2)   p(sii(cid:2)| i,  i(cid:2) , xi, xi(cid:2) ,  ), (3) (cid:4) where sii(cid:2) re ects an observed state transition from i to i .
Unlike in random walk models where proximity in a graph is simply used to smooth secondary estimators of parameters (e.g.
reachability, hitting times), we make direct use of it to model the latent variables  i.
Note that whenever we restrict the norm of  i (e.g.
by (cid:2)2 regularization) and when (cid:5) i  i(cid:2) to assess similarity, we we use an inner product model   approximately recover the graph Laplacian of Eqn.
(2).
In this way our model integrates two di erent methodologies   collaborative  ltering and random walks.
It is di er-ent from traditional random walk models in which transition probability is de ned solely based on graph topologies.
It is also di erent from traditional CF models in that it is de- ned on unipartite dyadic relationships.
By doing so, this integrated model not only allows learning of latent factors to capture graph topologies, but it also alleviates certain critical issues in random walks: for example, it naturally handles heterogeneous graphs (e.g.
a compound graph consisting of both unipartite and bipartite connections such as Figure 1), and it also makes applicable computationally-e cient
 simply default to the expected value of the latent variables, which is easily achieved in a probabilistic model.
sequential learning algorithms (e.g.
stochastic gradient descent), avoiding directly manipulating large matrices.
Friendship-Interest Propagation model.
Based on the above descriptions, we  nally summarize the overall FIP model in Figure 2 and the table below.
Note that the tuples (i, xi,  i) now play  double duty  in encoding interest (cid:4) , sii(cid:2) ) interactions (i, j, yij) and friendship connections (i, i simultaneously.
Learning shared factors from coupled relationships gives us both more evidence and more constraints to work with, and in turn leads to better generalization.
The Friendship-Interest Propagation (FIP) model.
  i   I   j   J   i   I, j   J   i, i (cid:4)   I  i   p( i|xi,  )  j   p( j|xj,  ) yij   p(yij| i,  j, xi, xj,  ) sii(cid:2)   p(sii(cid:2)| i,  i(cid:2) , xi, xi(cid:2) ,  )
 So far we deliberately described the FIP model in terms of general dependencies between random variables to make it explicit that the model is quite a bit more general than what can be achieved by an inner product model.
Here, we specify the model within an optimization framework.
For computational convenience we assume linear dependencies between xi and  i plus a noise term5 .
This means  i = Axi + i where E [i] = 0.
 j = Bxj + j where E [j] = 0.
(4) (5)  is typically assumed to be Gaussian or Laplace.
Whenever nonlinearity in x is desired we can achieve this simply by using a feature map of x and an associated kernel expansion.
Finally, we assume that the dyadic response (e.g.
yij ) depends on latent features only through the inner product (cid:5) (e.g.
  i  j) and on observable features through a bilinear product (e.g.
x (cid:5) i W xj) [4].
That is: yij   p(yij|fij ) where fij =   (cid:5) i  j + x sii(cid:2)   p(sii(cid:2)|hii(cid:2) ) where hii(cid:2) =   (cid:5) i W xj.
m and xj   R Here, assume xi   R m n and M   R m m provide a bilinear form which captures the a nity between the observed features for the corresponding dyads.
We also impose Laplace or Gaussian priors on W and M .
One advantage of using an (cid:2)1 (i.e. Laplace) prior is that it introduces sparsity, which makes (6) equivalent to sparse-coding [12] and thus improves both compactness and predictiveness of the learned latent factors  .
Given observed responses for the dyads {(i, j)   Oy} and {(i, i )   Os}, the problem of minimizing the negative log-(cid:4) posterior of FIP boils down to the following objective: (cid:5) i  i(cid:2) + x (cid:5) i M xi(cid:2) .
n, the matrices W   R min  y (cid:2)(yij, fij ) +  s (cid:2)(sii(cid:2) , hii(cid:2) ) +  I  ( i|xi) +  J (6) (cid:3) (cid:3) i I (i,j) Oy (cid:3) (cid:3) j J (i,i(cid:2)) Os  ( j|xj ) +  W  [W ] +  M  [M ] +  A [A] +  B [B], where  s are trade-o  parameters, (cid:2)( , ) denotes a loss function for dyadic responses.
The term  ( |x) =  [ ] +
 It indicates the deviation of the user/item pro les from its cold-start estimates Axi and Bxj respectively.
(cid:2)2, (cid:2)1 norm).
The term  x(x,  ) regularizes   by  tting the observed feature x, as de ned by (6).
This type of regularization are equivalent to applying content factorization (e.g.
LSI, NMF, LDA) to the feature x in terms of a factor   and bases A  1 or B  1.
The motivations for a computational framework instead of direct probabilistic inference are mainly twofold: First, the two formulations are somewhat equivalent   the distribution of the dyadic response (e.g.
yij ) and its dependence on the prediction (e.g.
p(yij|fij )) can be encoded precisely through the choice of loss functions; likewise, the prior over the observations or parameters could also be readily translated into the regularization penalties.
Secondly, computational models allow more scalable algorithms, e.g.
via stochastic gradient descent, whereas probabilistic reasoning often requires Monte Carlo sampling or quite nontrivial vari-ational approximations.
In our case, both y and s are binary, i.e. yij , sii(cid:2)   { 1}.
We performed an extensive study in our experiments comparing a large variety of di erent loss functions.
For the convenience of optimization, we limit ourselves to di eren-tiable (in many cases, also convex) loss functions (see also Figure 3 for details): Least Mean Squares: This is the most popularly-used loss in matrix factorization.
It minimizes the Frobenius norm of the prediction residue matrix and leads to a SVD-style algorithm.
We have the loss (cid:2)2(y, f ) = (1   yf )
 .
(7) Lazy Least Mean Squares: This is a slight modi cation of (cid:2)2 loss for the purpose of classi cation [30].
Basically, it is an iteratively truncated version of the (cid:2)2 loss via ll2(y, f ) = min(1, max(0, 1   yf )
 ).
(8) It has been shown that this loss approximates the clas-si cation error rate in the example space [30].
Logistic regression: This is the loss used in a binary exponential families model.
It is given by log(y, f ) = log[1 + exp( yf )].
(9) Huber loss: This is the one-sided variant of Huber s robust loss function.
It is convex and continuously di eren-tiable via  (y, f ) =
 2 max(0, 1   yf )2,   yf,

 if yf > 0.
otherwise.
(10) (cid:4) (cid:4)   loss: Unlike other loss functions, which are all convex upper bound of the 0-1 loss, the   loss [25] is non-convex.
Both theoretical and empirical studies have shown appealing advantages of using non-convex loss over convex ones, such as higher generalization accuracy, better scalability, faster convergence to the Bayes limit [25, 30].
We implement the following version:  (y, f ) =
 2 max(0, 1   yf )2,
 2 max(0, 1 + yf )2, if yf > 0.
otherwise.
(11)
 s s o l

  1
 yf
 l2 log Huber Psi
 Figure 3: Least mean squares ((cid:2)2), logistic (log), Huber and  loss (Psi).
We use these four and the lazy (cid:2)2 (omitted since its shape in parameter space is essentially identical to (cid:2)2) loss for binary classi cation.
A key challenge for learning latent factors from dyadic interactions is that the observations are extremely sparse with almost exclusively positive interactions observable.
That is, we typically do not observe explicit information that user i does not like item j.
Rather, the fact that we have not observed (i, j) suggests that i might not even know about j.
In other words, absence of a preference statement or a social link should not be interpreted absolutely as negative information.
At the same time, unless we have access to negative signals, we will almost inevitably obtain an estimator that is overly optimistic with regard to preferences (e.g.
predict positive for all the interactions).
To balance both requirements we draw uniformly from the set of unobserved tuples (i, j) (cid:4) and (i, i ) respectively and we require that, on average, observed pairs are preferred to unobserved ones.
In practice, since we use a stochastic gradient algorithm for minimization, for every positive observation, e.g.
yij = 1, we randomly sample a handful set of missing (unobserved) entries {yij(cid:2)}j(cid:2)=1:m, and treat them as negative examples (e.g.
yij(cid:2) =  1,) with credibility 1/m each.
Since the sampling procedure is random, the set of pseudo-negatives changes at each iteration and consequently each missing entry is treated as a potentially very weak negative instance.
Minimizing (6) is a nonconvex problem regardless of the choice of the loss functions and regularizers due to its use of bilinear terms.
While there are convex reformulations for some settings, they tend to be computationally ine cient for large scale problems   the convex formulations require the manipulation of a full matrix which is impractical for anything beyond thousands of users.
Moreover, the relationships between users change over time and it is desirable to have algorithms which process this information incrementally.
This calls for learning algorithms that are e cient and amendable to dynamic updating so as to re ect upcoming data streams, rendering less attractive those o ine learning algorithms such as classical SVD-based CF algorithms or spectral link prediction methods that involve manipulation of large-scale matrices.
This requirement becomes more important for FIP than for traditional latent factor models scale coupled interactions and feature observations.
We established algorithms for distributed optimization based on the Hadoop MapReduce framework.
The basic idea is to decompose the objective in (6) by optimizing with respect to yij and sii(cid:2) independently in the Map phase, and to combine the results for  i in the Reduce phase.
Stochastic Gradient Descent.
We brie y describe a stochastic gradient descent algorithm to solve the optimization of (6).
The algorithm is computationally e cient and decouples di erent users.
For a detailed discussion see [33].
The algorithm loops over all the observations and updates the parameters by moving in the direction de ned by negative gradient.
For example, for each (i, j, yij )   Oy: -  i    i       ((cid:2) (cid:4) (cid:4) [ i]) (yij , fij ) j +   -  j    j       ((cid:2) (cid:4) (cid:4) (yij, fij ) i +   [ j]) - W   W       ((cid:2) (cid:4) (cid:5) (cid:4) (yij, fij )xix j +   To update on feature observations, for each i   I: -  i    i       ( i   Axi +   (cid:4) [ i]) - A   A       ((Axi    i)x (cid:5) (cid:4) i +   where the subscripts of the trade-o  parameters   are omitted but clear from the context,   is the learning rate6.
Note that the gradient of (cid:2)1 regularizer is the discontinuous sign function.
We approximate it by a steep soft sign function: 1 exp( x) 1+exp( x) , where   is a positive number controlling  (x) = the ramp of  (x) (we use   = 100).
Feature Hashing.
A key challenge in learning FIP from large-scale data is that the storage of parameters as well as observable features requires a large amount of memory and a reverse index to map user IDs to memory locations.
In particular in social networks with hundreds of millions of users the memory requirement would easily exceed what is available on today s computers (100 million users with 100 latent feature dimensions each amounts to 40GB of RAM).
We address this problem by implementing feature hashing [28] on the space of matrix elements.
In particularly, by allowing random collisions and applying hash mapping to the latent factors (i.e.  ), we make possible most-needed latent factors to remain in-memory, and in turn allow storing, accessing and updating (i.e. the stochastic gradient descent algorithm) to perform at su cient speeds.
model is related to the models discussed in  2.
We end this section with a brief discussion on how our Our  rst observation is that our FIP model indirectly in-(cid:4) duces a kernel for the friendship network graph: k(i, i ) = (cid:5) i  i(cid:2) via the learned embedding  s .
This is similar to the   information di usion kernel for graph [8, 11] in that both kernels inherent a Riemannian manifold for I de ned by the friendship network S, rather than a  at Euclidean space as in traditional CF models (e.g.
neighborhood [24], factorization [23], RLFM [1]).
However, it is also worth mentioning that the FIP induced kernel is di erent from di usion kernels in that (i) our feature mapping  i is obtained from latent-factor based random walk model rather than topology-based random walk; and (ii) our model de nes a compact low-rank manifold rather than a manifold that is potentially of in nite dimensionality [8, 11].
factor of 0.9 after each iteration, as suggested by [9].
Traditional latent factor CF models [1, 23] work in Euclidean space where user factors  i are assumed identically and independently distributed:  i   N (0,  2I).
Our model relates  i with one another by modeling the social network graph.
This is equivalent to a row-correlated matrix-Gaussian     MN (0,     I), where   = [ i, .
.
.
,  |I|] (cid:5) , MN is a matrix-variate Gaussian, and     R de nes the row-covariance (user-user covariance).
By inexplicably modeling the friendship manifold, our model hereby generalizes traditional latent factor CF models from Euclidean space to Riemannian space, in a way analogous to how diffusion kernels generalize Gaussian kernels.
Note that, although the neighborhood based latent factor model [9] also induces a manifold structure for I, this manifold is virtual as it is directly constructed from Euclidean representations that essentially re ect the same amount of information.
Our model generalizes this model by exploiting the true connections from social networks.
The FIP also di ers from traditional link prediction algorithms.
Actually, it borrows the idea of latent factor CF models to model the transition probability in terms of latent factors, making possible that (i) latent factors can be learned from network topologies; and (ii) connections can be propagated collaboratively through the interaction between latent factors.
Essentially, our approach establishes an integrated network of interest and friendship that connects people with similar interests, and upon which both friendship and interests could be e ciently propagated.
We demonstrate the FIP model on Yahoo!
Pulse in terms of both interest targeting and friendship prediction.
Yahoo!
Pulse (pulse.yahoo.com) is a social network site that allows users to create pro les, connect to friends, post updates, and respond to questions, as in other social networks.
More importantly, it provides a mechanism for users to share interests, i.e. users can upload, download, install applications, and invite friends to try interesting applications.
Our motivation is to utilize the user-user friendship network and user-application interest network from Yahoo!
Pulse, so as to simultaneously propagate interest and friendship.
We examine data collected on Yahoo!
Pulse for about one year, involving hundreds of millions of users and a large collection of applications, such as games, sports, news feeds,  nance, entertainment, travel, shopping, and local information services.
Figure 4 shows the degree distribution of this data set.
The data is very sparse and almost half of the users only have one friend connections and do not like any of the applications (they are essentially not using the network).
Our goal is to propagate evidence to establish reliable connections both among users and between users and applications.
We use a subset of Yahoo!
pulse data.
The data set has
 and 29M interest interactions.
There is a signi cant di er-ence in the densities of the two networks in this data set.
As the item set is pretty small, the interest network is relatively dense   each user likes 23.5 items on average.
In contrast, as the user population is large, the friendship network is extremely sparse: on average, each user only has 4.9 friends out of the total 1.2 million.
Table 1: item oriented We compared the following models: neighborhood model (SIM), regression based latent factor model (RLFM), neighborhood based latent factor model (NLFM), and friendship-interest propagation (FIP).
For the latter we distinguish by choice of regularizer  [ ] and loss function (cid:2) as described in  3.3.
Models












 loss (cid:2)2 lazy (cid:2)2 logistic Huber
 (cid:2)2 lazy (cid:2)2 logistic Huber



























 (cid:2)2 (cid:2)2 (cid:2)2 (cid:2)2 (cid:2)2 (cid:2)1 (cid:2)1 (cid:2)1 (cid:2)1 (cid:2)1 nDCG@5












 Table 2: Friendship prediction performance.
We used the identical setting as in Table 1.
The best results are printed in boldface.
Models










 loss (cid:2)2 lazy (cid:2)2 logistic Huber
 (cid:2)2 lazy (cid:2)2 logistic Huber























 (cid:2)2 (cid:2)2 (cid:2)2 (cid:2)2 (cid:2)2 (cid:2)1 (cid:2)1 (cid:2)1 (cid:2)1 (cid:2)1 nDCG@5











 Both interest targeting and link prediction lead to a ranking of entities (e.g.
items and users that the system may recommend) according to a score function.
In our context this means that the scores fij and hii(cid:2) induce a ranking.
Hence it is natural to use ranking metrics to assess performance.
We consider the following three scores: AP is the average precision.
AP@n averages the precision of the top-n ranked list of each query.
AR is the average recall of the top-n rank list of each query.
nDCG or normalized Discounted Cumulative Gain is the normalized position-discounted precision score.
It gives larger credit to top-ranked entities.
In all three metrics we use n = 5 since most social networks and recommendation sites use a similar number of items for friend and application suggestions; it is also the standard recommendations size used in the current system.
For our evaluation we use cross-validation, where we randomly partition the data into two equally sized pieces and use one for



 ) s r e s u # ( y t i s n e d






 ) s r e s u # ( y t i s n e d



 degree (#friends)



 degree (#interests) Figure 4: Degree distributions of Yahoo!
Pulse friendship (top) and interest (bottom) networks.
training and the other for testing.
All three measures are computed on testing data only, and they are averaged over  ve random repeats.
In this section, we report the results on interest targeting (i.e. application recommendation).
We adopt a fairly strict evaluation by assessing the top results out of a total preference ordering of the item set for each user.
In particular, for each user i, we consider all the 386 items as candidates; we evaluate the recommendation performance by assessing the quality of the top-5 items based on the comparison between ground truth (the actual list of the applications that user i liked) and the top-5 ranked shortlist outputted by each model.
For comparison, we take three popular CF models as baseline: the item-oriented neighborhood model (SIM), the regression based latent factor model (RLFM) [1], and the combination of them (referred to as neighborhood based latent factor model or NLFM [23]).
SIM and RLFM use interest information; NLFM use both friendship information and interest information.
We test the baselines and di erent variants of FIP model, each of which is referred to in terms of the name combination of a loss and a regularization (e.g.
FIP((cid:2)2, (cid:2)2)).
Table 1 demonstrate the overall results, i.e. the mean value of metrics averaged over 5 random runs.
As the scale of the data is quite large, the predictive variance is very small (less than 0.002) and it is therefore not reported.
For the relatively dense interest network, all the reported models in our system actually achieve satisfactory performance in interest propagation.
For most models, both the nDCG@5 and AP@5 scores are above 0.7, that is, out of the  ve recommended items, on average 3.5 are truly  relevant  (i.e. actually being liked by the user).
Such performance is su ciently satisfactory for propagating the 386 approved services in the current interest network.
With such good performance, there is really not much room for further improvement.
However, we still observe that noticeable improvements are obtained by the FIP models.
Speci cally, @


 n






 latent dimensionality




 @


 n

 credit of friendship

 @


 n





 Log Huber Psi




 hold out data (%)

 Figure 5: Service recommendation performance (nDCG@5) as a function of latent dimensionality (left), friendship credibility (mid) and the proportion of holdout data (right).
@


 n




 latent dimensionality

 @


 n











 @


 n


 credit of interest




 hold out data (%)

 Figure 6: Friend prediction performance (nDCG@5) as a function of latent dimensionality (left), interest credibility (mid) and the proportion of holdout data (right).
in terms of the nDCG@5 scores (similar comparisons apply to other metrics), FIP outperform the SIM model by up to
 All improvements are signi cant (according to t-test with con dence threshold 0.01).
Among the 5 loss options for FIP, the one-sided Huber loss, the lazy (cid:2)2 loss and logistic regression perform equally well (with Huber slightly better).
Surprisingly, the noncon-vex   loss performs very poorly, even worse than (cid:2)2.
We attribute this to the non-convexity of the   loss   while non-convex losses perform superiorly in learning linear classi ers [25, 30], they could totally fail in learning the bilinear form of latent factor models because of the strong nonconvexity.
With respect to the two types of regularization, we observe that for each loss the (cid:2)1 regularizer almost consistently outperforms (cid:2)2.
As (cid:2)1 regularization leads to compact (i.e. sparse) latent factors by assigning submissive latent dimensions to exactly 0, this observation suggests that sparseness can improve the informativeness of latent factors (being sparser implies smaller description length) and in turn leads to superior performance.
One of our claims is that friendship information is helpful for interest targeting.
An interesting test would be to check how the credibility (i.e.  s) of the friendship in u-ences the performance of interest prediction.
We report this result in Figure 5.
We can see that, as we increase  s, the performance  rst increases (it peaks at 0.5, or 1.0 for  ) and thereafter it starts to drop.
This observation coincides with our intuitions: friendship information is truly useful for interest propagation, it helps interest targeting with discounted credit; yet, if too much weight is given to friendship, the latter may pollute the interest evidence and in turn harm interest targeting performance.
We also test the e ects of two parameters: the dimension-ality of latent factors k, and the proportion of holdout testing data.
Results are reported in Figure 5.
For most losses, between 10 and 20 latent factors are su cient for prediction.
Also, with the exception of the   loss, the performance is



 @


 n




 @


 n with BC without BC

 Log Huber Psi


 Log Huber Psi Figure 7: Recommendation performance in terms of nDCG@5 with and without bias-correction (BC) when applied to service recommendation (left) and friendship prediction (right).
quite stable to both parameters.
This observation validates our hypothesis of the   loss not being very amenable to e -cient optimization: as latent dimension increases, more local optima are created and the   loss performs worse; likewise, as training data becomes more sparse, the   loss may be trapped in worse local optima.
An important procedure in our implementation is the bias-correction, i.e. generating pseudo-negative samples to correct the selection bias, as described in  3.4.
We demonstrate the e ects by comparing results obtained with and without this procedure in Figure 7.
The comparisons are striking, indicating that latent factor models, if trained without negative examples, turn to over t the training observations and misleadingly predict  positive  for most dyads.
Our algorithms, by sampling missing interactions and using them as very weak negatives, guide the latent factor to capture the dyadic interactions while avoiding being fooled by the positive-only observations.
We conducted similar evaluations on friendship propagation (i.e. link prediction).
As the user population in the users as candidate friends and generate a total ordering of the whole user set for each user to evaluate the prediction performance; similarly, the models relying on neighborhood information (e.g.
SIM and NLFM) are no longer tractable as they require computations quadratic in the number of users.
To this end, we use a di erent evaluation mechanism: for each user i, we randomly sample M users that are not connected to i, we mix them with the set of users that i actually connects to.
We then use this probe-polluted set as candidates, upon which the ranking performance is computed.
In our experiments, we use M = 300 random probes per user.
We report the overall results in Table 2, where RLFM is used as the baseline model.
The friend-network is extremely sparse (0.0039% density).
Propagating friendship based on such sparse evidence is much more di cult, for example, RLFM only achieves 17% AP@5 and nDCG@5, which means out of the top-5 recommendations, less than 1 is truly relevant.
Yet, we observe a signi cant improvement, as high as a 40% gain in nDCG@5, when FIP is used.
This observation indicates that there is strong evidence of homophily in the Pulse social network such that users with similar interests are truly interested in each other.
By leveraging the relatively dense interest evidence to assist the extremely sparse friendship graph, FIP achieves much higher performance in friendship predictions.
Regarding the loss functions, this time the (cid:2)2 loss performs the best.
We hypothesize that for much sparser friendship networks, losses that are more suitable for classi cation tasks tend to over t the observed connections by making prematurely hard decisions to exclude connections that are not observed at the training stage.
Similarly, because the (cid:2)1 regularizer makes the latent factor sparser (some components of   are shrunk to be exactly 0), it also turns to make hard predictions and in turn performs worse.
Indeed, we observe signi cantly better performance for the (cid:2)2 loss and/or regularizer, which turn to make smoother decisions.
The e ects of parameter settings on this task are reported in Figure 6.
We observe similar trends as in the previous task, although the performance is more sensitive: the performance changes faster as latent dimensionality increases or training data decreases.
This matches the bias-variance analysis of statistic learning: as friendship connections are extremely sparse, we are typically dealing with a small-sample-size estimation, for which decrease of training data (e.g.
increase holdout proportion) or increase of model complexity (e.g.
increase latent dimensionality) will inevitably lead to the increase of either bias or variance or both, and therefore the models are more likely to over t the training observations and in turn generalize poorly.
As before, Figure 7 (right) shows that bias correction sig-ni cantly improves the performance.
Note that the di er-ence is not as large as in interest targeting.
This is likely due to the observation sparsity: in the sparser friendship network, two users that were not observed in the training set still have a good chance to be friends, which means many pseudo-negatives could be false-negatives.
Collaborative  ltering (CF) and link prediction were previously studied separately in two di erent research communities.
The proposed FIP model bridges these two methodologies with a uni ed model.
Essentially, FIP embeds all the users and items into the same space (e.g.
Euclidean, simplex) so that the distances between two entities (e.g.
user-item, user-user) re ect the relatedness (e.g.
interest, friendship) between them, and hence, provides a uni ed treatment for both interest targeting and link prediction.
Existing approaches to link prediction di use the sparse connections using topology-based random walk [14, 22, 32] or spectral graph algorithms [19, 20], both of which involve expensive manipulation of large matrices.
FIP borrows the idea of latent factor models in collaborative  ltering [23, 1, 10] and it shows connections to random walk based mod els.
As a side e ect we obtain computationally attractive algorithms for e cient random walks.
Traditional CF techniques exploit past records of user behavior for future prediction based on either neighborhood based or latent factor based methods.
The neighborhood latent factor model [9] merged these two models and reported signi cant performance improvement on the Net ix data.
Though promising, the network structure exploited in this combined model [9] is a virtual one, constructed using the same evidence for learning latent factors.
Our FIP model extends this model to allow the actual social network structure to be captured in latent factor learning.
Along another line, the recently proposed regression based latent factor model (RLFM) [1] incorporates node (user or item) features to improve recommendation performance in the cold-start scenario.
The FIP model also generalizes RLFM [9] in a way analogous to how information di usion kernels [8, 11] generalize the Gaussian kernels.
Basically, instead of working in the Euclidean space as RLFM does, FIP induces a limited-dimensional Riemannian manifold de ned by the topologies of both the unipartite friendship network and the bipartite user-service interaction network.
The FIP model has a close connection with recent works on collective matrix factorization [15, 29, 27, 32], where the tasks of learning relational data were also formulated in terms of factorizing multiple matrices.
The current work continues our prior investigations on this topic and further examines interest and friendship propagation in the context of social networks, a task urgently motivated by emerging demands from social network services [21].
The techniques developed in this work also advances the state-of-the art from several aspects: (i) besides the dyadic relational data (i.e. edges), we also attempt to leverage the rich information conveyed by the node features using regression model similar to RLFM [1] or factorization models similar to sparse coding [12]; in this way, FIP integrates latent factor models [10, 23, 1] and predictive bilinear models [31, 4]; (ii) we present distributed optimization algorithms, address bias correction, discuss and benchmark di erent loss objectives and regular-izers; and our work provides one of the  rst large-scale examinations of interest-friendship propagation in a real social network system.
One work relevant to ours is the social recommendation approach proposed by [16], where the trust relationships among users are used to improve cold-start recommendation.
This model can be seen as a special case of our FIP model by assuming (i) no node feature xi or xj; (ii) (cid:2)2 loss objective; and (iii) asymmetric factor based random walk model, i.e. the transition probability is modeled as a multiplicative function of user-factor and a basis.
Also, this work did not address the task of user relationships (e.g.
trust, both tasks with a more general framework and conducts large-scale evaluation on a social networking system.
E ectively modeling interest and friendship and accordingly recommending services and/or suggesting friends are fundamental to all social network services.
In this paper, we have shown that the interest and friendship information is highly relevant and mutually helpful.
We established a joint friendship-interest propagation model that leverages both evidences to address both tasks in one uni ed framework.
The FIP model bridges collaborative  ltering in recommendation systems and random walk in social network analysis with a coupled latent factor model.
We conducted extensive experiments to benchmark di erent variants of FIP in the Yahoo!
Pulse social networking system.
Two directions of future research appear attractive: The FIP model o ers a latent factor for each user that captures both interest and friendship information.
We plan to leverage such deeper user pro les to detect interest communities (i.e. grouping users according to interest with user-friendship in mind) and to identify the macro-behavior (i.e. the global e ect as a result of individual actions) of each interest group.
We also plan to investigate the underlying mechanism of how the interactions between users impact individual decision making in the context of social networks.
