The acquaintance structure underlying a social network contains a wealth of information about the network itself, and many data mining tasks can be accomplished from this information alone (e.g., detecting outlier nodes, identifying interest groups, estimating measures of centrality etc.
[24, 13]).
Many of these tasks translate into graph mining problems and can be solved through suitable (sometimes, variants of standard) graph algorithms that often assume that the graph is stored into the main memory.
However, this assumption is far from trivial when large graphs are dealt with, and this is actually the case when social networks are considered; for instance, current estimates say that the indexable web contains at least 23:59 billion pages1, and in 2008 Google announced to have crawled 1 trillion unique URLs: the successor lists for such a graph would require hundreds of terabytes of memory!
The situation is somewhat similar in other social networks; for example, as of October
 friendship relations.
The objective of this paper is to  nd effective techniques to store and access large graphs that can be applied fruitfully not only to web graphs but also to social networks of other kinds.
The considerations above explain why this problem is lately emerging as one of the central algorithmic issues in the  eld of information retrieval [10, 7]; it should also be noted that improving the compression performance on a class of networks, apart for its obvious practical consequences, implies (and requires) a better understanding of the regularities and of the very structure of such networks.
Here and in the following, we are thinking of compressed data structures.
A compressed data structure for a graph must provide very fast amortised random access to an edge (link), say in the order of few hundreds of nanoseconds, as opposed to a  compression scheme , whose only evaluation criterion is the number of bits per link.
While this de nition is not formal, it excludes methods in which the successors of a node are not accessible unless, for instance, a large part of the graph is scanned.
In a sense, compressed data structures are the empirical counterpart of succinct data struc-1http://www.worldwidewebsize.com/ 2http://www.facebook.com/press/info.php?statistics ber of bits equal to the information-theoretical lower bound, providing access asymptotically equivalent to a standard data structure.
The idea of using a compressed data structure to store social networks was already successfully exploited with application to web graphs [6], showing that such graphs may be stored using less than 3 bits/link; this impressive compression ratio is mostly obtained by making good use of two simple properties that can be experimentally observed when nodes are ordered lexicographically by
 (cid:15) similarity: nodes that are close to each other in the order tend to have similar sets of neighbours; (cid:15) locality: most links are between nodes that are close to each other in the order.
The fact that most compression algorithms exploit these (or analogous) properties explains why such algorithms are so sensible to the way nodes are ordered; the solution of ordering nodes lexico-graphically by URL is usually considered good enough for all practical purposes, and has the extra advantage that even the URL list can be compressed very ef ciently via pre x omission.
Analogous techniques, which use additional information besides the graph itself, are called extrinsic.
One natural and important question is whether there exist any intrinsic order of the nodes (i.e., one that does not rely on any external data) that produces comparable, or maybe even better, compression ratios.
This is particularly urgent for general social networks, where the very notion of URL does no longer apply and  nding a natural extrinsic order is problematic [7,



 The general problem we consider may be stated as follows: a graph-compression algorithm A takes (the adjacency matrix of) a graph as input and stores it in a compressed data structure; the algorithm output depends on the speci c numbering chosen for the nodes.
We let (cid:26)A .G; (cid:25)/ be the number of bits per link needed by A to store the graph G under the given node numbering3 (cid:25) W VG !
jVGj.
The overall objective is to  nd a numbering O(cid:25) min-imising (cid:26)A .G; O(cid:25)/.
In the following, we shall always assume that a graph G with n nodes has VG D n, so a node numbering is actually a permutation (cid:25) W n !
n.
Of course, the problem has different solutions depending on the speci c compression algorithm A that is taken into consideration.
In the following, we shall focus on the so-called BV compression scheme [6] used within the WebGraph framework, which incorporates the main ideas adopted in earlier systems and is a de facto standard for handling large web-like graphs.
In particular, the framework strongly relies on similarity and locality to achieve its good compression results; for this reason, we believe that most compressed structures that are based on the same properties will probably display a similar behaviour.
As noted in [7], even a very mild version of the above-stated op-timisation problem turns out to be NP-hard, so we can only expect to devise heuristics that work well in most practical cases.
Such heuristics may be intrinsic or extrinsic, depending on whether they only use the information contained in the graph itself or they also depend on some external knowledge.
In the class of intrinsic order heuristics, [19] proposes to choose the permutation (cid:25) that would sort the rows of the adjacency matrix
 f 0; 1; : : : ; n (cid:0) 1g.
AG in lexicographic order.
This is an example of a more general kind of solution:  x some total ordering (cid:30) on the set of n-bit vectors (e.g., the lexicographic ordering), and let (cid:25) be the permutation that would sort the rows of the adjacency matrix AG according to4 (cid:30).
Another possible solution in the same class, already mentioned in [19] and studied more deeply in [4], consists in letting (cid:30) be a Gray ordering.
Recall that [14] an n-bit Gray ordering is a total order on the set of the 2n binary n-bit vectors such that any two successive vectors differ in exactly one position.
Although many n-bit Gray ordering exist, a very effective one (i.e., one that is manageable in practice because it is easy to decide which of two vectors come  rst in the order) is the so-called re ective n-bit Gray ordering, which was used in [4].5 Chierichetti et al. [7] propose a completely different intrinsic approach based on shingles that adopts ideas used for document similarity derived from min-wise independence.
The compression results they get are comparable to those achieved through Gray ordering [4].
In the same paper they also discuss an alternative compression technique (called BL) that provides better ratios; however, while interesting as a compression scheme, BL does not provide a compressed data structure recovering the successors of a node requires, in principle, decompressing the whole graph.
Recently, Safro and Temkin [21] presented a multiscale approach for the network minimum logarithmic arrangement problem: their method searches for an intrinsic ordering that optimises directly the sum of the logarithms of the gaps (numerical difference between two successive neighbours).
Although their work is not aimed at compression, their ordering is potentially useful for this task if combined with a compression scheme like BV.
Indeed, some preliminary tests show that these orderings are promising especially on social networks; however, the implementation does not scale well to datasets with more that a few millions of nodes and so it is impractical for our purpose.
As far as extrinsic orderings are concerned, a central r le is played by the URL-based ordering in a web graph.
If G is a web graph, we can assume to have a permutation (cid:25)U of its nodes that sorts them according to the lexicographic URL ordering: this extrinsic heuristic dates back to [3] and, as explained above, turns out to give very good compression, but it is clearly of no use in non-web social networks.
Another effective way to exploit the host information is presented in [4], where URLs from the same host are kept adjacent (within the same host, Gray ordering is used instead).
It is worth remarking that all the intrinsic techniques mentioned above produce different results (and, in particular, attain different compression ratios) depending on the initial numbering of the nodes, because they work on the adjacency matrix AG.
This fact was overlooked in almost all previous literature, but it turns out to be very relevant: applying one of these intrinsic reordering to a randomly numbered graph (see Table 7) produces worse compression ratios than starting from a URL-ordered web graph (see Table 6).
This problem arises because even if the intrinsic techniques described above do not explicitly use any external information, the initial order of a graph is often obtained by means of some external information, so the compression performances cannot be really
 adjacency matrix contains duplicated rows.
This issue turns out to have a negligible impact on compression and will be ignored in the following.
ordering, we will simply omit the adjective  re ective  in the following.
we will speak of coordinate-free algorithms for those algorithms that achieve almost the same compression performances starting from any initial ordering; this adjective can be applied both to compression algorithms and to orderings+compression algorithm pairs.
From an experimental viewpoint, this means that, unlike in the previous literature, we run all our tests starting from a random permutation of the original graph.
We suggest this approach as a baseline for future research, as it avoids any dependency on the way in which the graph is presented initially.
The only coordinate-free compression algorithm we are aware of6 is that proposed by Apostolico and Drovandi in [1];7 they exploit a breadth rst search (BFS) to obtain an ordering of the graph and they devise a new compression scheme that takes full advantage of it.
Their algorithm has a parameter, the level, which can be tuned to obtain different trade-offs between compression performance and time to retrieve the adjacency list of a node: at level 8 they attain better compression performances than those obtained by BV with Gray orderings and have a similar speed in retrieving the adjacency list.
Even in this optimal setting, though, their approach is outperformed by the one we are going to present (see Table 5).
Finally, Maserrat and Pei [15] propose a completely different approach that does not rely on a speci c permutation of the graph.
Their method compresses social networks by exploiting Eulerian data structures and multi-position linearisations of directed graphs.
Notably, their technique is able to answer both successor and predecessor queries: however, while querying for adjacency of two nodes is a fast operation, the cost per link of enumerating the successors and predecessors of a node is between one and two orders of magnitude larger than what we allowed.
In other words, by the standards followed in this paper their algorithm does not qualify as a compressed data structure.
We must also remark that the comparison given in [15] of the compression ratio w.r.t.
WebGraph s BV scheme is quite unfair: indeed, the authors argue that since their algorithm provides both predecessors and successors, the right comparison with the BV scheme requires roughly doubling the number of bits per link (as the BV scheme just returns successors).
However, this bound is quite na ve: consider a simple strategy that uses the set Esym of all symmetric edges, and let Gsym D .V; Esym/ and Gres D .V; E n Esym/.
To be able to answer both successor and predecessor queries one can just store Gsym, Gres and Gres transposed.
Using this simple strategy and applying the ordering proposed in this paper to the datasets used in [7] we obtain better compression ratios.
results: In this paper we give a number of algorithmic and experimental (cid:15) We identify two measures of  tness for algorithms that try to recover the host structure of the web, and report experiments on large web graphs that suggest that the success of the best coordinate-free orderings is probably due to their capability of guessing the host structure.
(cid:15) Since the existing coordinate-free orderings do not work well
 proaches to web-graph compression, not quoted here, either fail to compress social networks, or are strongly dependent on the initial ordering of the graph.
sion (10 15%) when starting from URL ordering or from a random permutation, except for the altavista-nd dataset, which however is quite pathological.
on social networks, we propose a new algorithm, called Layered Label Propagation, that builds on previous work on scalable clustering by label propagation [18, 20]; the algorithm can reorder very large graphs (billions of nodes), and unlike previous proposals, is free from parameters.
(cid:15) We report experiments on the compression of a wide array of web graphs and social networks using WebGraph after a reordering by Layered Label Propagation; the experiments show that our combination of techniques provides a major increase in compression with respect to all currently known approaches.
This is particularly surprising in view of the fact that we obtain the best results both on web graphs and on social networks.
Our largest graph contains more than 600 millions nodes one order of magnitude more than any published result in this area.
We remark that our new algorithm has also been applied with excellent results to the Minimum Logarithmic Arrangement Problem 8.
Most of the intrinsic orderings proposed so far in the literature are unable to produce satisfactory compression ratios when applied to a randomly permuted graph, mainly because they mostly fail in reconstructing host information as we discussed in the last section.
To overcome their limitations, we can try to approach this issue as a clustering problem.
However, this attempt presents a number of dif culties that are rather peculiar.
First of all, the size of the graphs we are dealing with imposes to use algorithms that scale linearly with the number of arcs (and there are very few of them; see [8]).
Moreover, we do not possess any prior information on the number of clusters we should expect and their sizes are going to be highly unbalanced.
These dif culties strongly restrict the choice of the clustering algorithm.
In the last years, a new family of clustering algorithms were developed starting from the label propagation algorithm presented in [18], that use the network structure alone as their guide and require neither optimisation of a prede ned objective function nor prior information about the communities.
These algorithms are inherently local, linear in the number of edges, and require just few passes on the graph.
The main idea of label propagation algorithms is the following: the algorithms execute in rounds, and at the beginning of each round every node has a label representing the cluster that the node currently belongs (at the beginning, every node has a different label).
At each round, every node will update its label according to some rule, the update order being chosen at random at the beginning of the round; the algorithm terminates as soon as no more updates take place.
Label propagation algorithms differ from each other on the basis of the update rule.
The algorithm described in [18] (hereafter referred to as standard label propagation or just label propagation) works on a purely local basis: every node takes the label that occurs more frequently in its neighbourhood9.
Metaphorically, every node in the network chooses to join the largest neighbouring community (i.e., the one to which the maximum number of its neighbours belongs).
As labels propagate, densely connected groups of nodes quickly reach 8http://www.mcs.anl.gov/~safro/mloga.html.
The authors had been provided a preliminary version of our code to perform their tests.
rent label of the node is one of the most frequent in its neighbour-hood, in which case the label is simply not changed.
groups are created throughout the network, they continue to expand outwards until it is possible to do so.
At the end of the propagation process, nodes having the same labels are grouped together as one community.
It has been proved [22] that this kind of label propagation algorithm is formally equivalent to  nding the local minima of the Hamiltonian for a kinetic Potts model.
This problem has a trivial globally optimal solution when all the nodes have the same label; nonetheless, since the label-propagation optimisation procedure produces only local changes, the search for maxima in the Hamiltonian is prone to becoming trapped at a local optimum instead of reaching the global optimum.
While normally a drawback of local search algorithms, this characteristic is essential to clustering: the trivial optimal solution is avoided by the dynamics of the local search algorithm, rather than through formal exclusion.
Despite its ef ciency, it was observed that the algorithm just described tends to produce one giant cluster containing the majority of nodes.
The presence of this giant component is due to the very topology of social networks; to try to overcome this problem we have tested variants of the label propagation that introduce further constraints.
One of the most interesting is the algorithm developed in [2], where the update rule is modi ed in such a way that the objective function being optimised becomes the modularity [17] of the resulting clustering.
Unfortunately, modularity is not a good measure in very large graphs as pointed out by several authors (e.g., [9]) due to its resolution limit that makes it hardly usable on large networks.
Another variant, called Absolute Pott Model (APM) [20], introduces a nonlocal discount based on a resolution parameter (cid:13).
For a given node x, let (cid:21)1; : : : ; (cid:21)k be the labels currently appearing on the neighbours of x, ki be the number of neighbours of x having label (cid:21)i and vi be the overall number of nodes in the graph with label (cid:21)i ; when x is updated, instead of choosing the label (cid:21)i maximizing ki (as we would do in standard label propagation), we choose it as to maximise (see Algorithm 1) ki (cid:0) (cid:13).vi (cid:0) ki /: Observe that when (cid:13) D 0 the algorithm degenerates to label propagation; the reason behind the discount term is that when we decide to join a given community, we are increasing its density because of the ki new edges joining x to existing members of the community, but we are at the same time decreasing it because of vi (cid:0) ki non-existing edges.
Indeed, it can be shown that the density of the sparsest community at the end of the algorithm is never below (cid:13)=.
(cid:13) C 1/.
Algorithm 1 The APM algorithm.
(cid:21) is a function that will provide, at the end, the cluster labels.
For the sake of readability, we omitted the resolution of ties.
Require: G a graph, (cid:13) a density parameter
 2: for all x: (cid:21).x/ x, v.x/ 1 3: while (some stopping criterion) do for i D 0; 1; : : : ; n (cid:0) 1 do
 j(cid:21) for every label `, k`

 ` argmax` k`

 decrement v.(cid:21).
(cid:25).i /// (cid:21).
(cid:25).i // O

 increment v.(cid:21).
(cid:25).i /// end for
 11: end while (cid:0)1.`/ \ NG .
(cid:25).i //j (cid:0) (cid:13).v.`/ (cid:0) k`/  ` Figure 1: An example of the distribution of cluster sizes computed by APM.
This algorithm demonstrated to be the best candidate for our needs.
However it has two major drawbacks.
The  rst is that there are no theoretical results that can be used to determine a priori the optimal value of (cid:13) (on the contrary, experiments show that such an optimal value is extremely changeable and does not depend on some obvious parameters like the network size or density).
The second is that it tends to produce clusters with sizes that follow a heavy-tailed decreasing distribution, yielding both a huge number of clusters and clusters with a huge number of nodes (see Figure 1).
Thus to obtain good compression performances we have to decide both the order between clusters and the order of the nodes that belong to the same cluster.
In this section we present a new algorithm based on label propa-More formally, let a sequence (cid:13)0; (cid:13)1; (cid:13)2; : : : and an initial or-
to other scalable clustering techniques: we have experimented with several alternatives [8], and APM is by far the most interesting candidate.
gation that yields a compression-friendly ordering.
A run of the APM algorithm (discussed in the previous section) over a given graph and with a given value of the parameter (cid:13) produces as output a clustering, that may be represented as a labelling (mapping each node to the label of the cluster it belongs to).
An important observation is that, intuitively, there is no notion of opti-mality for the tuning of (cid:13): every value of this parameter describes a different resolution of the given graph.
Values of (cid:13) close to 0 highlight a coarse structure with few, big and sparse clusters, while, as (cid:13) grows, the clusters are small and dense, unveiling a  ne-grained structure.
Ideally, we would like to  nd a way to compose cluster-ings obtained at different resolution levels.10 This intuition leads to the de nition of Layered Label Propagation (LLP); this algorithm is iterative and produces a sequence of node orderings; at each iteration, the APM algorithm is run with a suitable value of (cid:13) and the resulting labelling is then turned into an ordering of the graph that keeps nodes with the same label close to one another; nodes within the same cluster are left in the same order they had before.
To determine the relative order among different clusters, it is worth observing that the actual label produced by the label propagation algorithm suggests a natural choice: since every cluster will be characterised by the initial label of the leader node (the node which  ooded that portion of graph; see Algorithm 1), we can sort the clusters according to the order that the leader nodes had.
sequence of orderings (cid:25)1; (cid:25)2; : : : W VG !
jVGj and a sequence of labelling functions (cid:21)0; (cid:21)1; : : : W VG !
jVGj as follows: (cid:21)k is obtained by running the APM algorithm on the graph G with parameter (cid:13)k; then we let (cid:25)kC1 be the ordering de ned by x  kC1 y iff (cid:21)k .x/ D (cid:21)k .y/ V (cid:25)k .x/  (cid:25)k .y/: (cid:25)k .
(cid:21)k .x// < (cid:25)k .
(cid:21)k .y// or ( The rationale behind this way of composing the newly obtained clustering with the previous ordering is explained above: elements in the same cluster (i.e., with the same label) are ordered as before; for elements with a different label, we use the order that the corresponding labels (i.e., leader nodes) had before.
The output of LLP actually depends on two elements: the initial ordering (cid:25)0 and the choice of the parameters (cid:13)k at each iteration.
Regarding the choice of the (cid:13)k s, instead of trying to  nd at each iteration an optimal value for the parameter we exploit the diverse resolution obtained through different choices of the parameter, thus  nding a proper order between clusters that suitably mixes the clusterings obtained at all resolution levels.
To obtain this effect, we choose every (cid:13)k uniformly at random in the set11 (cid:0)i ; i D 0; : : : ; Kg.
Since the APM algorithm is run at f0g [ f2 every step on the same graph G, it turns out that it is easier (and more ef cient) to precompute the labelling function output by the APM algorithm for each (cid:13) in the above set, and then to reuse such labellings.
The surprising result is that the  nal ordering obtained by this mutilresolution strategy is better than the ordering obtained by applying the same strategy with K different clusterings generated with the same value of (cid:13) chosen after a grid search for the optimal value (as shown in Table 1), and a fortiori on the ordering induced by one single clustering generated with the optimal (cid:13).
Moreover the  nal order obtained is essentially independent on the initial permutation (cid:25)0 of the graph (as one can see comparing Table 6 with Table 7).
One may wonder if this iterative strategy can be applied also to improve the performances of other intrinsic orderings.
Our experiments rule out this hypothesis.
Iterating Gray, lex, or BFS orderings does not produce a signi cant improvement.
Layered label propagation lends itself naturally to the task-de-composition parallel-programming paradigm, which may dramatically improve performances on modern multicore architectures: since the update order is randomised, there is no obstacle in updating several nodes in parallel.
Our implementation breaks the set of nodes into a very small number of tasks (in the order of thousands).
A large number of threads picks up the  rst available task and solves it: as a result, we obtain a performance improvement that is linear in the number of cores.
We are helped by WebGraph s facilities, which allows us to provide each thread with a lightweight copy of the graph that shares the bitstream and associated information with all other threads.
As an attempt to explain the good compression results obtained, we propose an empirical analysis showing how pro cient layered label propagation is in recovering the host information even when
 be of no practical use on large networks, because it would only yield a complete fragmentation of the graph.
Name Amazon
 Enron Hollywood LiveJournal Flickr indochina (hosts) uk (hosts) eu in indochina it uk













 Fixed LLP












 (+3%) (+3%) (+6%) (+7%) (+4%) (+4%) (+12%) (+6%) (+14%) (+22%) (+14%) (+26%) (+26%) Table 1: Comparison between LLP with different values of (cid:13) and LLP with the best value of (cid:13) only.
Values are bits per link.
starting from a random permutation.
Indeed, the results presented in [4] suggest that what is really important in order to achieve good compression performances on web graphs is not the URL ordering per se, but rather an ordering that keeps nodes from the same host close to one another.
For this reason, we will be naturally interested to measure how much a given ordering (cid:25) respects the partition induced by the hosts, H .
The  rst measure we propose is the probability to have a host transition (HT): HT.H ; (cid:25)/ D PjVGj(cid:0)1 iD1  (cid:0)H  (cid:25) (cid:0)1.i (cid:0) 1/ (cid:1) (cid:0)1.i / ; H  (cid:25) jVGj (cid:0) 1 where   denotes the usual Kronecker s delta and H  x  is the equivalence class of node x (i.e., the set of all nodes that have the same host as x): this is simply the fraction of nodes that are followed, in the order (cid:25), by another node with a different host.
Alternatively, we can reason as follows: the ordering induces a re nement of the original host partition, and the appropriateness of a given ordering can be measured by comparing the original partition with the re ned one.
More formally, let us denote with Hj(cid:25) the partition induced by the re exive and transitive closure of the relation (cid:26) de ned by x (cid:26) y   j(cid:25).x/ (cid:0) (cid:25).y/j D 1 and H  x  D H  y : Intuitively, the classes of Hj(cid:25) are made of nodes belonging to the same host and that are separated in the order only by nodes of the same host.
Notice that this is always a re nement of the partition
 The second measure that we have decided to employ to compare partitions is the Variation of Information (VI) proposed in [16].
De- ne the entropy associated with the partition S as: P .S / log.P .S // where P .S / D jSj jVGj H.S / D (cid:0) X


 and the mutual information between two partitions as:
 where P .S; T / D jS\Tj jVGj .
The Variation of information is then
 P .S; T / log P .S; T /
 V I.S ; T / D H.S / C H.T / (cid:0) 2 I.S ; T /I notice that, in our setting, since Hj(cid:25) is always a re nement of H , we have I.H ; Hj(cid:25) / D H.H / and so VI simpli es into V I.H ; Hj(cid:25) / D H.Hj(cid:25) / (cid:0) H.H /: Armed with these de nitions, we can determine how much different intrinsic orderings are able to identify the original host structure.
We computed the two measures de ned above on a number of web graphs (see Section 8) and using some different orderings described in the literature; more precisely, we considered: (cid:15) Random: a random node order; (cid:15) Natural: for web graphs, this is the URL-based ordering; for the other non-web social networks, it is the order in which nodes are presented, which is essentially arbitrary (and indeed produces compression ratios not very different from random); (cid:15) Gray: the Gray order explained in [4]; (cid:15) Shingle: the compression-friendly order described in [7]; (cid:15) BFS: the breadth rst search traversal order, exploited in [1]; (cid:15) LLP: the Layered Label Propagation algorithm described in this paper.
The results of this experiment are shown in Table 2; comparing them with the data of Table 7 (that shows the compression performances starting from a truly random order), it is clear that recovering the host structure from random is the key property that is needed for obtaining a real coordinate-free algorithm.
However, the only ordering proposed so far that is able to do this is breadth rst search, and its capability to identify hosts seems actually a side effect of the very structure of the web.
In the rest of the paper, we use BFS as a strong baseline against which our new results should be compared.12 As a  nal remark, we should observe that in some cases (e.g., see the uk dataset in Table 2) layered label propagation yields better compression than breadth rst although it is less pro cient in recovering host information; in a sense, our algorithm is in some cases able to unveil a  ner structure than that of hosts, thus outperforming both the natural (URL-based) and the breadth rst ordering.
For our experiments, we considered a number of graphs with various sizes and characteristics; most of them are (directed or undi-rected) social graphs of some kind, but we also considered some web graphs for comparison (because for web graphs we can rely on the URLs as external source of information).
Almost all the datasets can be downloaded from the LAW site13 (or from other public or free sources) and have been widely used in the previous literature to benchmark compression algorithms.
The Java code for our new algorithm is distributed at the same URL under the GNU General Public License.
We give a name to each dataset, and show between parentheses the name of the dataset on the LAW site (see also Table 3 and 4):
 ture that we expect in social networks, an algorithm as simple as a breadth rst search can identify meaningful clusters (see again the BFS column of Table 7), and this leaves room for improvement.
13http://law.dsi.unimi.it/ (cid:15) Hollywood (hollywood-2009): One of the most popular undirected social graphs, the graph of movie actors: vertices are actors, and two actors are joined by an edge whenever they appeared in a movie together.
(cid:15) DBLP (dblp-2010): DBLP14 is a bibliography service from which an undirected scienti c collaboration network can be extracted: each vertex of this undirected graph represents a scientist and two vertices are connected if they have worked together on an article.
(cid:15) LiveJournal (ljournal-2008): LiveJournal15 is a virtual community social site started in 1999: nodes are users and there is an arc from x to y if x registered y among his friends (it is not necessary to ask y permission, so the graph is directed).
We considered the same 2008 snapshot of LiveJour-nal used in [7] for their experiments16.
(cid:15) Amazon (amazon-2008): This dataset describes similarity among books as reported by the Amazon store; more precisely the data was obtained17 in 2008 using the Amazon E-Commerce Service APIs using SimilarityLookup queries.
(cid:15) Enron (enron): This dataset was made public by the Federal Energy Regulatory Commission during its investigations: it is a partially anonymised corpus of email messages exchanged by some Enron employees (mostly part of the senior management).
We turned this dataset into a directed graph, whose nodes represent people and with an arc from x to y whenever y was the recipient of (at least) a message sent by x.
(cid:15) Flickr: Flickr18 is an online community where users can share photographs and videos.
In Flickr the notion of acquaintance is modelled through contacts; we used an undi-rected version of this network, where vertices correspond to users and there is an edge connecting x and y whenever either vertex is recorded as a contact of the other one.
(cid:15) For comparison, we considered  ve web graphs (eu-2005, in-2004, indochina-2004, it-2004, uk-2007-05) of various sizes (ranging from about 800 thousand nodes to more than 650 million nodes), available at the LAW web site.
(cid:15) Finally, the altavista-nd (altavista-2002-nd): graph was obtained from the Altavista dataset distributed by Yahoo!
within the Webscope program (AltaVista webpage connectivity dataset, version 1.019).
With respect to the original dataset, we pruned all dangling nodes ( nd  stands for  no dangling ).
The original graph, indeed, contains 53:74% dangling nodes (a preposterous percentage [23]), probably because it also considers the frontier of the crawl the nodes that have been discovered but not visited.
We eliminated (one level of) dangling nodes to approximate the set of visited nodes, and also because dangling nodes are of little importance in compression.20 14http://www.informatik.uni-trier.de/~ley/db/ 15http://www.livejournal.com/
 17http://www.archive.org/details/amazon_similarity_ isbn/
 mental results on the Flickr graph.
ature, is not a good dataset.
As we already remarked, most likely all nodes in the frontier of the crawler (and not only visited nodes) were added to the graph, and the giant component is less than 4% of the whole graph.
eu in indochina it uk















 Shingle






 Gray






 Natural






 Random






 Table 2: Various measures to evaluate the ability of different orderings to recover host information.
Smaller values indicate a better recovery.
Name Amazon
 Enron Hollywood LiveJournal Flickr Nodes





 Edges





 Table 3: Social graph description.
Name eu in indochina indochina (hosts) it uk (hosts) uk altavista-nd Year







 Nodes







 Edges







 Table 4: Web graph description.
Each graph was compressed in the BV format using WebGraph [6]21 and we measured the compression performance using the number of bits/link actually occupied by the graph  le.
We also compared LLP+BV with the compression obtained using the algorithm proposed by Apostolico and Drovandi [1] at level 8 starting from a randomly permuted graph; the results, shown in Table 5, provide evidence that LLP+BV outperforms AD in all cases, and in a signi cant way on social networks.
This is particularly relevant, since the compression algorithm of AD is designed to take full advantage of a speci c ordering (the breadth rst search) and is the only known coordinate-free alternative we are aware of.
In our comparison, contrarily to all other tables, we used the full compression power of the BV format, as our intent is to motivate LLP+BV as a very competitive coordinate-free compression algorithm.
In the rest of the paper, as we already explained, we have turned off intervalisation, as our purpose is to study the effect of different permutations on locality and similarity: this explains why
 isation and put a limit of 3 to the length of the possible reference chains (see [4] for details on the r le of this parameter).
Observe that the latter two settings tend to deteriorate the compression results, but make decompression extremely ef cient even when random access is required.
Name Amazon
 Enron Hollywood LiveJournal Flickr indochina (hosts) uk (hosts) eu in indochina it uk altavista-nd















 (+36%) (+10%) (+28%) (+53%) (+37%) (+26%) (+26%) (+27%) (+6%) (+7%) (+11%) (+21%) (+36%) (+114%)













 Table 5: Comparison between LLP+BV compression (for this particular table, the full set of compression tecniques available in WebGraph has been used, including intervalisation) and the algorithm proposed by Apostolico and Drovandi (AD) at level
 the bits per link found in Table 5 are smaller than elsewhere in the paper.
A comment is needed about the bad performance the Apostolico  Drovandi method on the altavista-nd dataset.
Apparently, the size of the dataset is such that scrambling it by a random permutation causes the method to use a bad naming for the nodes, in spite of the initial breadth rst visit.
In our previous experiments, the Apostolico Drovandi method did not show variations of more than 20% in compression due to random permutations, but clearly the issue needs to be investigated again.
Tables 6 and 7 present the number of bits per link required by our datasets under the different orderings discussed above and produced starting from the natural order and from a random order (the percentages shown in parenthesis give the gain w.r.t.
breadth rst search ordering).
Here are some observations that the experimental results suggest: (cid:15) LLP provides always the best compression, with an average gain of 25% with respect to BFS, and largely outperforms both simple Gray [4] and shingle orderings [7].
Some simple experiments not reported here shows that the same happen for transposed graphs: for instance, uk is compressed at 1:06 compressed data structure available today.
(cid:15) LLP is extremely robust with respect to the initial ordering of nodes and its combination with BV provides actually a coordinate-free compressed data structure.
Other orderings (in particular, Gray and shingle) are much more sensitive to the initial numbering, especially on web graphs.
We urge researchers in this  eld to always generate permutations starting from a randomised copy of the graph, as  useful  ordering information in the original dataset can percolate as an artifact in the  nal results.
(cid:15) As already remarked elsewhere [7], social networks seem to be harder to compress than web graphs: this fact would suggest that there should be some yet unexplained topological difference between the two kinds of graphs that accounts for the different compression ratio.
Despite the great improvement in terms of compression results our technique remains highly scalable.
All experiments are performed on a Linux server equipped with Intel Xeon X5660 CPUs (2:80 GHz, 12 MB cache size) for overall 24 cores and 128 GB of RAM; the server cost about 8 900 EUR in 2010.
Our Java implementation of LLP sports a linear scaling in the number of arcs with an average speed of (cid:25) 80 000 000 arcs/s per iteration.
The overall time cost of the algorithm depends on the number (cid:13) s and on the stopping criterion.
With our typical setting the overall speed of the algorithm is (cid:25) 800 000 arcs/s.
The algorithm is also very memory ef cient (it uses 3n integers plus the space required by the graph22) and it is easy to distribute, making a good candidate for huge networks.
Indeed, most of the time is spent on sampling values of (cid:13) to produce base clusterings,23 and this operation can be performed for each (cid:13) in a fully parallel way.
Applying LLP to a web graph with 1 billion nodes and 50 billions arcs would require few hours in this setting.
For comparison, we also tried to compress our dataset using the alternative versions of LLP described in Section 4: in particular, we considered APM (with the optimal choice of (cid:13)) and the combination APM+Gray (that sorts each APM cluster using Gray).
Besides the number of bits per link, we also analysed two measures that quantify two different structural properties: (cid:15) the average gap cost (i.e., the average of the base-2 logarithms of the gaps between the successors of a node: this is an approximate measure of the number of bits required to write the gaps using a universal variable-length code); this measure is intended to account for locality: the average gap cost is small if the ordering tends to keep well-connected nodes close to one another;24 (cid:15) the percentage of copied arcs (i.e., the number of arcs that are not written explicitly but rather obtained by reference from a previous successor list); this is intended to account for similarity: this percentage is small if the ordering tends to keep nodes with similar successor lists close to one another.
ory, but the cost becomes O.n log n/.
the number of nodes, rather than in the number of arcs, and has little impact on the overall run time.
version of the standard gap measure used in the context of data-aware compressed data structures [11].
The results obtained are presented in Table 8.
In most cases APM copies a smaller percentage of arcs than APM+Gray, because Gray precisely aims at optimising similarity rather than locality; this phenomenon is less pronounced on web graphs, where anyway the overall number of copied arcs is larger; looking at the average gap cost, all clustering methods turn out to do a better job than Gray in improving locality (data not shown in the table).
LLP usually copies less arcs than APM+Gray, but the difference is often negligible and de nitely balanced by the gain in locality.
We would like to point out that, at least when using the best compression currently available (LLP+BV), the average gap cost is de nitely more correlated with compression rates than the average distance cost, that is, the average of the logarithms of the (absolute) difference between the source and target of each arc (see Figure 2).
Indeed, the correlation coef cient is 0:9681 between bits per link and average gap cost and 0:1742 between bits per link and average distance cost.
In [7] the problems MLOGA and MLOGGAPA consist exactly in minimising the average distance and the average gap cost, respectively: that authors claim that both problems capture the essence of a good ordering, but our extensive experimentation suggests otherwise.
As a  nal remark, it is worth noticing that similarity and locality have a different impact in social networks than in web graphs: in web graphs the percentage of copied arcs is much larger (a clue of the presence of a better-de ned structure) and in fact it completely determines the number of bits per link, whilst in social networks the compression ratio is entirely established by the gain of locality (measured, as usual, by the average gap cost).
We have presented highly scalable techniques that improve compressed data structures for representing web graphs and social networks signi cantly beyond the current state-of-art.
More importantly, we have shown that coordinate-free methods can outperform state-of-art extrinsic techniques on a large range of networks.
The clustering techniques we have devised are scalable to billions of nodes, as they just require few linear passes over the graphs involved.
In some cases (e.g., the uk dataset) we bring down the cost of a link to almost 1:8 bits.
We remark again that our improvements are measured w.r.t.
the BFS baseline, which is itself often an improvement when compared to the existing literature.
Finally, we leave for future work a full investigation of the compression ratio that can be obtained when fast access is not required.
For instance, uk compressed by LLP+BV at maximum compression requires only 1:21 bits per link better, for instance, than the Apostolico Drovandi method with maximum compression (1:44).
Some partial experimental data suggests that we would obtain by far the highest compression ratio currently available.
The experiments that we report required several thousands of hours of computation: we plan to make available the results both under the form of WebGraph property  les (which contain a wealth of statistical data) and under the form of comprehensive graphical representations.
