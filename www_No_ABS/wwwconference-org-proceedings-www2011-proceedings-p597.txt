Online social networks have become increasingly popular in the recent decade which gave rise to an increasing need in analyzing their properties and comparing them to one Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
another.
Many properties of online social networks are considered important.
These include, for example: their user age distribution, net activity, connectivity, and many more.
The literature on attaining such parameters is vast and any e ort to reference all of it is bound to fail.
We cite here only a handful of directly related works as examples.
In [12, 9] the authors present a way to detect proximity between two users.
In [20] the authors analyze the degree distribution, clustering property, degree correlation, and evolution over time for three networks.
That said, the total number of users (or users in a certain demographic) seems to be one of the most crucial factors in deriving the worth and overall performance of social networks.
These  gures are also critical for business development issues, choosing between networks for advertisement campaigns or for launching social applications.
Although countless sites, blogs, and other reports often present such numbers, these are usually based on reports of the social networks themselves or on tra c [2]).1 analysis and are not guaranteed to be accurate (e.g.
Moreover, since each network reports slightly di erent  g-ures, it is almost impossible to compare between them.
For example, Facebook reported lately on crossing the 500 million active users mark.
However, it is unclear what constitutes as an  active user .
Thus, it is extremely important to be able to accurately and reliably estimate the size of social networks in a uni ed and unbiased way (without the networks  consent or control).
Since online social networks provide public interfaces, it is possible to traverse their members  network externally.
By  crawling  the network, we can collect statistics on any of the above characterizes.
In a similar spirit, in [3, 4] the authors suggested ways to measure various parameters of search engines by interacting with their public search interface only.
One approach, is to crawl the network extensively.
In other words, since online social networks can be modeled as undirected unweighted graphs (where the users are nodes and edges are connections/friendships) we can perform a breadth rst search (BFS) on the graph.2 This method is impractical when dealing with online social networks since the communication and computational burden of such an undertaking would probably be prohibitive.
The second approach is to sample users uniformly at ran-
timate for connected sub-graphs (For example, 20-30 year olds living at the US).
of connected users for every user, Thus, acting like a neighbor list representation of the graph.
tics can be estimated.
This includes estimating the size of the network using methods such as mark-and-recapture.
These methods have the advantage of requiring only O( n) users to be sampled to get a good estimate (where n is the overall number of users).
Their disadvantage though, is that users must be sampled uniformly.
Since online social networks interfaces usually do not provide this functionality, it must be simulated by other API queries which give for each user the list of their neighbors.
Alas, producing a single uniformly chosen user might require many such queries.
This is explained in the next section.
In this work we show that there is no need to sample users uniformly.
In fact, by using the sampling bias we reduce the number of required samples dramatically.
For example, for networks with a Zip an-like degree distribution our algorithms require only O(n1/4 log(n)) samples to converge.
Moreover, since the bias does not have to be corrected, each such sample requires considerably less API queries.
Surprisingly, our algorithm is extremely simple and gives provable error guarantees with high probability.
Experiments with our algorithm performed over a wide range of real and synthetic data corroborate that it converges signi cantly faster and to a more accurate estimate than uniform sampling based approaches.
As a side note, simple variants of our algorithm also give e cient sub-linear algorithm for estimating the size of transitive closures in graphs and for estimating the size of search-indexes as in [3].
However, this is a matter of farther research and is beyond the scope of this paper.
The rest of this work is organized as follows.
Section 2 surveys related work.
Our algorithms are presented and analyzed in Sections 3 and 4.
In Section 5 we report our experimental results and conclude in Section 6.
Various proofs and discussions are included in the Appendix.
From this point on we consider the general problem of estimating the size of undirected graphs.
The graph representation of online social networks is the obvious one.
Each node refers to one user and an edge is present between two nodes if their corresponding users are  friends  in the social network.
Although our algorithms are correct for general graphs, they are especially suited for graphs which naturally occur in large social networks.
In [17] the authors provide a possible solution for another problem which could be used to solve the problem at hand as well.
They present an algorithm for estimating the number of attributes in a database.
The algorithm samples rows from the database uniformly at random.
It then estimates the total number of attributes using the collected information of how many times each attribute was picked.
This is identical to a well known problem in statistics called  estimating the number of unseen types .
Their algorithm can be applied to estimating the size of graphs if the graph is represented as a database table containing two rows per edge, one for each adjacent node.3 Clearly, the number of distinct attributes in this database is n, the number of nodes.
The algorithm presented in [17] is guaranteed to take at most r = O(n) samples.
However, in graphs, unlike in databases,
 since random walks on graphs sample edges uniformly.
it is possible to sample nodes (analogously, attributes) uniformly at random which can dramatically decrease the number of samples.
In ecology, a method known as mark and recapture is used to estimate population sizes.4 It relies on the same phenomenon as the so called  birthday paradox  e ect.
Informally, after sampling r nodes uniformly at random we expect to encounter C   r2/2n collisions (nodes already picked).
Thus, n can be estimated by r2/2C.
Surprisingly, taking only O( n) samples can guarantee that this estimate for n is rather accurate.5 In [7] the authors present a maximum likelihood estimator for this problem and show that their estimator converges almost surly when the number of samples increases.
In [6, 10] the authors extend these methods to nonuniform, but known, distributions.
That said, to use these methods one must sample nodes uniformly at random from a graph, which is not straight forward.
To see how this can be done we remind the reader a few basic facts from spectral graph theory.
A random walk on an undirected graph with n nodes {v1, .
.
.
, vn} is de ned as such: start from an arbitrary node, then move to a neighboring node uniformly at random and repeat.
After many such steps, the probability of being at any node vi is close to pi = di/D where di is the degree of node vi and D = i=1 di is the sum of all node degrees in the graph.
This is called the stationary distribution of the random walk on the graph.
The number of random walk steps needed for the stationary distribution to be reached depends on the mixing rate property of the graph (see a survey by Lov asz [14]).
Fortunately, social network graphs and small world graphs are known to have good mixing rates.
We therefore assume that nodes can be repeatedly sampled from the stationary distribution without much computational overhead.
Pn Using these properties of random walks, one can sample nodes also uniformly at random by using, for example, rejection sampling.
To be precise, a node vi is  rst sampled according to the stationary distribution.
Then, with probability 1/di it is kept.
With probability 1  1/di it is rejected.
Clearly the set of kept nodes is uniformly sampled.
However, since we only expect to accept a node with probability n/D, to sample  ( n) un-rejected nodes would require an expected r =  (D/ n) biased samples.
Several rejection sampling ideas and other methods for turning the node sampling distribution to uniform were suggested for speci c graphs.
Namely, the bipartite graph between search queries and search results [5, 3].6 Another approach was considered in [8].
The authors present a modi ed Metropolis-Hastings random walk which transitions from node vi to an adjacent node vj with probability 1/ max(di, dj ).
With the remaining probability, it stays in vi.
Due the symmetry in the transition probabilities it can be shown that the stationary distribution of this walk is uniform on the nodes.
However, the mixing rate of this walk can be signi cantly worse than that of the original graph, and so, it is unclear when it is expected to outperforms rejection sampling, i.e., require fewer random walk steps.
include capture-recapture, capture-mark-recapture, mark-recapture, and mark-release-recapture.
search services but their approach is suitable for this task as well.
the  German tank problem  [1].
It was supposedly used during world-war II to estimate the number of German tanks based on manufacturing numbers found on those captured by the allied forces.
In its mathematical formulation, elements with serial ID s are sampled uniformly without replacement and the objective is to provide an estimate for the total number of elements.
This is not applicable to our scenario since the users do not have serially allocated and publicly available ID s.
Estimating the number of nodes in a graph was also studied.
In [11] the authors estimate the size of a tree.
Their motivation was to estimate the running time of a backtracking programs.
Later [18] extends their argument to acyclic graphs.
Finally [15] extends this idea to general undirected graphs.
However, the running time of the the latter is unbounded in the worst case and expected to be more than the number of nodes in the graph.
Recently, in [19] the authors try to estimate the size of social networks in a setup very similar to ours .
However, they either require that the users be sampled uniformly or use the algorithm from [15] which their experiments show is impractical.
In this section we present our graph size estimator.
We start by taking r samples {x1, .
.
.
, xr} independently from the stationary distribution of the graph, i.e., each node vi is sampled with probability pi = di/D where D =Pn We de ne three variables that the algorithm keeps track of.
First, the number of collisions.
A collision is a pair of identical samples.
More precisely, de ne Yi,j to be 1 if xi = i=1 di.
xj and 0 else.
C = Pi<j Yi,j.
Second,  1 is the sum of all sampled node degrees  1 = Pr as the sum of the reciprocal degrees,  1 =Pr Using linearity of expectation and the fact that the samples are taken independently it is easy to compute their expectation.
i=1 dxi .
Last, we de ne  1 i=1 1/dxi .
r
 E [C] = E [Xi<j Xi=1 Xi=1
 r Yi,j] = r 2!
n Xi=1 p2 i n
 Xi=1 n n dxi ] = rE [dx1 ] = r pidi = rD
 pi/di = Xi=1 p2 i Xi=1 rn
 .
From the above three equations we can isolate n and get that: n = r

 r2E [C]


   Intuitively, if C,  1 and  1 are all close to their expected values then the estimator  1 1/2C should be close to n as well.7 This is stated in the following Corollary.
slightly di erent estimator.
Namely,  n = ( 1 1   r)/2C.
This however is di erent from  1 1/2C only by a factor of O(r/nc).
Corollary 1.
For any degree distribution and C,  1, and  1 de ned as above we have the estimator:  n ,  1 1

 (1) which guarantees for any     1/2 and     1: Pr[n(1    )    n   n(1 +  )]   1     as long as the number of samples, r, satis es: i=1 p3 i i=1 p2 i )2 + Pn i=1 1/pi  2 n2 ) r   rc   O(
  pPn i=1 p2 i + Pn  2 (Pn Proof.
The proof goes by  rst calculating the variance of C,  1 and  1.
Then, using Chebyshev s inequality we require that each of them gives an  /4 approximations to their expected values with probability at least 1   /3.
This gives us a lower bound on the required number of samples.
See Appendix A for more details.
Since the probability of each for deviating from its expected value is at most  /3, the probability of one or more of them deviating is at most   (the union bound).
This gives us that with probability at least 1     all three variables are  /4 close to their respective expectations.
We then use the facts that  n/n   (1 +  /4)2/(1    /4)   1 +   and  n/n   (1    /4)2/(1 +  /4)   1     to complete the proof for     1/2.
Note that for regular graphs, where the degree distribution is uniform, this estimator is identical to the  birthday paradox  estimator presented in the introduction.
Moreover, it will also require O( n) samples to converge (to see this substitute 1/n for pi).
In order to argue that our algorithm is suitable for sizing social networks we have to assume something about their node degree distributions.
In [16], [20] and in [8] the authors argue that, in several networks, the nodes  degrees exhibits di erent kinds of heavy tail distributions.
Mainly: Exponential, Double-Pareto and Zip an.
Here we analyze, as an example, the Zip an distribution.
Similar analyses can be performed for the other distributions as well.
If the nodes  degrees are distributed according to a Zip an distribution with maximum degree of dm and parameter   = 2 we have: P r(d = j) = ; j = 1, .
.
.
, dm , j 2

 where H =Pdm moment of the degree distribution is de ned as M  = E [d ] and the  rst few moments of the Zip an distribution are: j=1 j 2    2

 log dm dm
 d2 m

 .
We also assume that the moments of the observed degree distribution are close to those of the generating distribution.
This is true for large graphs by the strong law of large numbers (SLLN).
This gives us that Pn n 1(M1)  .
Substituting the above into Corollary 1 and using the fact that dm =  ( n) we get that rc   O(n1/4 log(n)).
Therefore, only O(n1/4 log(n)) samples su ce for our estimator to be accurate.
Note the signi cant reduction in the number of samples over the uniform distribution.
For example, for i=1 p  i   n = 109,  n   30, 000 while n1/4 log(n)   6000.
M  One surprising aspect of this estimator is that it works for subgraphs as well.
Let X   be the subset of samples X who are also in the subgraph (not necessarily connected).
We perform the same random walk (over the original graph) and compute the same parameters C  ,    1, which are de ned as above but for X   instead of X.
The subgraph  1/2C  .
The proof provided above size is estimated by   works for this case as well.
The only change is that D is replaced by D  which is the sum of node degrees in the sub-graph.
However, since D (and therefore D  as well) cancels itself in the analysis our estimator remains unchanged.
However, it turns out that it is usually more e cient to  rst estimate the size of the entire graph and then estimate the subgraphs  relative size.
From the above we have that E [ 1] = rn/D, similarly for the subgraph, E [   1] = r n /D  (n  being the number of nodes in the subgraph).
Isolating n  we get: n  = n r r 

  1]
 E [ 1]   n
  1
  1 If the number of samples is large enough, the last step is cor-
rect since r /r   E [r /r] = D /D and since E [  and E [ 1]    1.
Under most conditions, the ratio estimator requires only a constant number of samples to converge.
Thus, the main computational e ort is in estimating n, which is surprisingly lower than that of directly estimating n .
To see this, let rc and r  c be the number of samples needed to estimate the sizes of the graph and the subgraph respectively.
Since, in a random walk we only hit a node in the subgraph with probability D /D, we are expected to require Dr  c samples from the subgraph.
Thus, if r  c/D    rc/D the second method is preferable.
Note that this holds in the natural situation that the nodes  degree distributions of the graph and subgraph are similar.
c/D  random samples to obtain r 
 In this section we present another estimator which is based on counting non-unique elements instead of collisions.
On the one hand, it tends to be consistently, yet marginally, more accurate.
On the other hand, its proof is much more involved.
We thus choose to present the estimator along with its performance (in the experimental results section) without providing a proof of its correctness.
An element in the sample is considered non-unique if it was sampled at least once before.
This is slightly di er-ent from counting collisions.
For example, in the sequence {1, 2, 3, 1, 1} there are two non-unique elements (the last two 1 s) but three collisions (x1 = x4, x1 = x5, and x4 = x5).
The intuition is that counting non-unique elements is less sensitive to errors in which a speci c node is oversampled.
This is because the non-unique count is linear in the number of times each item was sampled whereas the collision count is quadratic.
We estimate n by  n which is the unique solution to the following  xed point equation:  n = r    C +  n
 r Xi=1
 dxi (cid:18)1   dxi  1  nr (cid:19)r (2) Note that r,  C ,  1 and dxi are all observed quantities.
To n

 pi Now, consider that (1   pi)r = E [ see why this is correct,  rst note that the expected number i=1 (1   pi)r.
of non-unique elements is E [  C] = r   n +Pn Xi=1 (1   Also, D   rn .
Making these substitutions into the expectation expression gives the above  xed point equation.
Intuitively, the size estimate  n is chosen such that the observed number of non-unique elements is equal to its expectation.
As a remark, if the node distribution is uniform, this estimator is identical to the maximum likelihood estimator [7].
(1   pi)r]   Xi=1
 dxi dxi
 )r .
r
 r


 In order to test our estimators  accuracy we  rst experimented with three networks whose exact sizes are known.
A synthetic network; a synthetically constructed graph consisting of 1 million nodes whose degree distribution is Zip an with parameter   = 2 and maximal degree dm =
 The Digital Bibliography and Library Project; we used the Digital Bibliography and Library Project s (DBLP) entire database.8 Edges in the graph were associated with co-authorship of at least one paper.
The resulting graph included 845, 211 nodes, each with at least one edge (authors with no coauthors were omitted).
The Internet Movie Database; we useed public Internet Movie Database s (IMDB) entire database.9 Edge connections between actors were established according to joint participation in at least one movie or TV episode.
The resulting graph included 1, 955, 508 nodes.
We produced three types of curves.
All three were plotted as functions of the percent of sampled nodes.
Error curves present the normalized absolute estimation error, i.e., |n    n| /n where n is the true size of the network and  n is our estimate of it.
Con dence interval curves give for each estimator the 5 th and 95 th percentile value from 10, 000 independent estimations.
In other words, 90% of the estimated sizes fell between the lower and upper curves.
Lastly, comparison curves present the ratio between the errors of the non-unique element estimator and that of the collision estimator.
It is important to stress that this ratio is between the normalized absolute estimation errors and not between the estimated values.
All presented plots were produced by averaging over 10, 000 independent experiments.
Examining the error curves depicted in Figure 1, the superiority of degree sampling estimation (both non-unique and collisions based) over uniform sampling estimation is well observed.
In particular, for the synthetic network, uniform sampling estimation requires 5 times as many samples as required by degree sampling estimation (0.5% vs. 2.5%), to ensure a normalized absolute estimation error of less than
 mation required almost 3 times more samples than required by degree sampling estimation (0.3% vs. 0.8%) to ensure a normalized absolute error of less than 10%.
trier.de/xml/.
berlin.de/pub/misc/movies/database/.
ftp://ftp.fu-


















i ] e z s k r o w t e n o t e v i t a e
 l [ r o r r e e t u o s b
 l i ] e z s k r o w e n t o t e v i t l a e
 [ r o r r e e t l u o s b











 ] i e z s k r o w e n t o t e v i t l a e
 [ r o r r e e t l u o s b
 Synthetic network   Estimation error Unif.
dist.
  collision Unif.
dist.
  non unique Deg.
dist.
  collision Deg.
dist.
  non unique



 Number of samples [Percentage of network size] DBLP network   Estimation error Unif.
dist.
  collision Unif.
dist.
  non unique Deg.
dist.
  collision Deg.
dist.
  non unique

 Number of samples [Percentage of network size]



 IMDB   Estimation error Unif.
dist.
  collision Unif.
dist.
  non unique Deg.
dist.
  collision Deg.
dist.
  non unique


 Number of samples [Percentage of network size]


 i ] e z s k r o w t e n o t e v i t a e
 l [ n o i t a m i t s e e z
 i









 i ] e z s k r o w t e n o t e v i t l a e
 [ n o i t a m i t s e e z
 i












 ] i e z s k r o w e n t o t e v i t l a e
 [ n o i t a m i t s e e z
 i Synthetic network   Confidence interval Unif.
dist.
  non unique 95% Deg.
dist.
  non unique 95% Deg.
dist.
  non unique 5% Unif.
dist.
  non unique 5%



 Number of samples [Percentage of network size] DBLP network   Confidence interval Unif.
dist.
  non unique 95% Deg.
dist.
  non unique 95% Deg.
dist.
  non unique 5% Unif.
dist.
  non unique 5%

 Number of samples [Percentage of network size]



 IMDB   Confidence interval Unif.
dist.
  non unique 95% Deg.
dist.
  non unique 95% Deg.
dist.
  non unique 5% Unif.
dist.
  non unique 5%


 Number of samples [Percentage of network size]


 Figure 1: Error curves - absolute normalized size estimation errors vs. the percent of sampled nodes for three networks: a 1-million-node synthetic network (top), a network constructed from the Digital Bibliography and Library Project (DBLP) database (middle), and a network obtained from the Internet Movie Data Base (IMDB) database (bottom).
Figure 2: Con dence interval curves - relative estimated size vs.
the percent of sampled nodes for three networks: a 1-million-node synthetic network (top), a network constructed from the Digital Bibliography and Library Project (DBLP) database (middle), and a network obtained from the Internet Movie Data Base (IMDB) database (bottom).
Similar observations regarding the estimation error are also notable examining the con dence interval curves depicted in Figure 2.
These curves also demonstrate that there is an inherent asymmetrical bias towards size overestimation.
This is probably because both estimators are inversely proportional to the number of collisions or non-unique elements.
For example, a 50% discrepancy between the observed number of collisions and its expectation can

 o i t a r r o r r




























 o i t a r r o r r
 o i t a r r o r r
 Synthetic network   Performance comparison Unif.
dist.
  non unique to collision error ratio Deg.
dist.
  non unique to collision error ratio



 Number of samples [Percentage of network size] DBLP Network   Performance comparison Unif.
dist.
  non unique to collision error ratio Deg.
dist.
  non unique to collision error ratio


 Number of samples [Percentage of network size]


 IMDB   Performance comparison Unif.
dist.
  non unique to collision error ratio Deg.
dist.
  non unique to collision error ratio


 Number of samples [Percentage of network size]


 Figure 3: Comparison curves - absolute relative error ratio between the non-unique and collision estimators vs. the percent of sampled nodes for three networks: a 1-million-node synthetic network (top), a network constructed from the Digital Bibliography and Library Project (DBLP) database (middle), and a network obtained from the Internet Movie Data Base (IMDB) database (bottom).
result in 100% size overestimation but only in 35% size underestimation.
In all the curves above and for all three networks the non-unique elements estimator slightly outperformed the collisions based estimator.
This phenomenon is visible when examining the comparison curves depicted in Figure 3.
For example, for DBLP, the non-unique elements estimator provides a 5% reduced relative error over the collision based estimator when 3% of the network is sampled.
The reader should note that the actual size estimates in this case di er by only 0.25%.
In all the aforementioned experiments we estimated the sizes of networks (whose sizes were already known) up to precision of a few single percents.
We observed that both collision and non-unique based estimators perform well and that degree based sampling is signi cantly preferred to uniform sampling.
In the next section we estimate the size of a subnetwork within Facebook and size of their entire network.
We used two crawls performed on Facebook by the Authors of [8, 13].
The  rst crawl consisted of 984, 830 uniformly sampled users collected during April 2009 [8].10 The second crawl was performed during October 2010 and consisted of 988, 116 users [13].
This crawl performed a simple random walk on the Facebook graph and therefore selected users with probability proportional to their degree.11
 Since the actual size of Facebook is not known (other than Facebook s own reports) we  rst estimate the size of a sub-graph whose size is known.
We selected a random subset of
 this sub-population using the  rst algorithm in Section 3.2.
This is done for two reasons.
First, to test the subgraph size estimation algorithm.
Second, to make sure that Facebook s network s topology and statistics are suitable for our estimators.
We present an error curve, a con dence interval curve, and a comparison curve in Figure 4.
Note that the x-axis here gives the percent of nodes sampled from the subnetwork and not the entire network as before.
These results corroborate that our subgraph size estimators behave almost identically to the complete graph estimators.
This was expected since their analysis is essentially identical.
A more important discovery is that the network topology and node degree distribution of Facebook are indeed suitable for our estimators to perform well.
We now estimate the size of the entire Facebook network.
Presenting accuracy plots in this case is not possible since the true size of Facebook is not known.
The uniform Facebook sample collected during April 2009, contains 2053 collisions and 2052 non-unique elements.
Substituting these into Equations (1) and (2) yields estimates of 237, 197, 785 and 236, 984, 623 users respectively.
The very same month, Facebook ([2]) reported of having  more than 200 million active users  and  more than 250 million active users  three months later.
The crawl that was performed during October 2010 contained 4099 collisions and 4064 non-unique users.
This gives estimates of 475, 566, 857 and 475, 864, 724
 http://odysseas.calit2.uci.edu/research/.
every mth sampled node, where m > 30 is adequate for every practical use (see [8] and references therein).
] e z s k r o w t e n o t e v i t a e
 l [ r o r r e e t u o s b
 l











 Facebook   Estimation error Unif.
dist.
  collision Unif.
dist.
  non unique Deg.
dist.
  collision Deg.
dist.
  non unique

 Number of samples [Percentage of network size]








 i ] e z s k r o w t e n o t e v i t l a e
 [ n o i t a m i t s e e z
 i



 o i t a r r o r r





 Facebook   Confidence interval Unif.
dist.
  non unique 95% Deg.
dist.
  non unique 95% Deg.
dist.
  non unique 5% Unif.
dist.
  non unique 5%

 Number of samples [Percentage of network size]


 Facebook   Performance comparison Unif.
dist.
  non unique to collision error ratio Deg.
dist.
  non unique to collision error ratio

 Number of samples [Percentage of network size]





 Figure 4: Absolute relative estimation error (top), con dence intervals (middle), and estimation relative error ratio (bottom) vs. the percentage of samples taken from a 1 million user subnetwork of Facebook.
respectively.
Facebook at the same time reported of having  more than 500 million active users .
This is summarized in Table 1.
Notice that the second crawl sampled half as many users relative to the network size at the time it was made.
Yet, it produced roughly twice as many collisions.
This is be-Table 1: Crawl details and consequent size estimates of the entire Facebook network for April 2009 and October 2010.
April 2009 October 2010 Sampling distribution Number of samples Number of collisions Number of non-unique Collision estimator Nun-unique estimator Facebook report uniform





 degree





 cause degree proportional sampling is expected to generate more collisions than uniform sampling.
The discrepancy between our estimates and the o cial reports by Facebook stems from two main reasons.
On the one hand, the crawler cannot distinguish between active and non active users.
Thus our estimates include nonactive users which causes an over estimation.
On the other hand, our crawler cannot pass through users whose privacy settings hide their list of friends, which causes underestimation.
Thus, our estimates and Facebook s reports give slightly di erent  gures.
While Facebook counts  active  users, we estimate the number of Facebook users whose list of friends is visible to the crawler regardless of their activity.
Since the crawler has no indication of users  activity and since it is unclear what Facebook de nes as  active , we cannot o set this e ect.
However, we can try to estimate the number of blocked users (those whose privacy settings block the crawler).
This can be approximated using the fraction of such users in other users  friends lists which yields an estimate of 650   106 users, active and nonactive.
Interestingly, for a large enough number of samples (e.g., 30% of the network s size), uniform sampling estimation out performs degree sampling estimation.
This phenomenon repeats itself for all three known size network we examined.
We provide an error curve, a con dence interval curve, and a comparison curve in Figure 5 for the synthetic network only but this time extending the number of samples all the way to 100%.
We presented two algorithms for estimating the size of graphs.
Both algorithms rely on nodes being samples from the graph s stationary distribution.
We showed both analytically and experimentally that, for social-networks and other small world graphs, these algorithms considerably outperform uniformly sampling nodes.
They consistently provide more accurate estimates while using a smaller number of samples.
This result is even more outstanding since uniformly sampling nodes is strictly harder than sampling them according to the stationary distribution.
However, there is still room for improvement.
While performing the random walk in order to sample nodes from the stationary distribution, we encounter many nodes whose degrees or collisions we never use.
Using this information can possibly reduce the number of random walk steps even fur-Synthetic network   Estimation error Unif.
dist.
  collision Unif.
dist.
  non unique Zipf.
dist.
  collision Zipf.
dist.
  non unique







 i ] e z s k r o w t e n o t e v i t a e
 l [ r o r r e e t u o s b
 l










 i ] e z s k r o w t e n o t e v i t l a e
 [ n o i t a m i t s e e z
 i












 o i t a r r o r r




 Number of samples [Percentage of network size] Synthetic network   Confidence interval Unif.
dist.
  non unique 95% Zipf.
dist.
  non unique 95% Zipf.
dist.
  non unique 5% Unif.
dist.
  non unique 5%


 Number of samples [Percentage of network size]
 Synthetic network   Performance comparison Unif.
dist.
  non unique to collision error ratio Zipf.
dist.
  non unique to collision error ratio



 Number of samples [Percentage of network size]


 Figure 5: Absolute relative estimation error (top), con dence intervals (middle), and estimation relative error ratio (bottom) vs. the percent of sampled nodes for a 1 million node synthetic network whose node degree distribution is Zip an.
Note the large number of samples relative to the network size.
ther.
However, analyzing the entire random walk is hard since this introduces dependencies on the graph topology in a nontrivial way.
We thank Minas Gjoka for being extremely helpful and providing the Facebook crawls used in the simulations.
We also thank Ronny Lampel, Yoelle Maarek and Ravi Kumar for useful discussions.
