In recent years there has been an explosion in the amount of data available for social network analysis.
It has become relatively common to study social networks consisting of tens of millions of nodes and billions of edges [12, 13].
Since the amount of data to be analyzed has grossly outpaced advances in the memory available on commodity hardware, computer scientists have once again turned to parallel algorithms for data processing.
Mapreduce [10] and its open source implementation, Hadoop [19], have emerged as the standard platform for large scale distributed computation.
In this paper we will give algorithms for computing one of the fundamental metrics for social networks, the clustering coe cient [18], in the MapReduce framework.
The clustering coe cient measures the degree to which a node s neighbors are themselves neighbors.
The  eld of sociology gives us two theories as to the importance of this measure.
The tighter the community, the more likely it is for any given member to have interacted with and know the reputation of any other member.
Coleman and Portes [8, 15] argue that if a person were to do something against the social norm, the consequences in a tightly-knit community would be greater because more people would know about the o ending action and more people would be able to sanction the individual who committed it.
This phenomenon helps foster a higher degree of trust in the community and also helps the formation of positive social norms.
Thus if the clustering coe cient of an individual indicated that they were in a tightly-knit community, that individual may be able to better take advantage of the higher levels of trust amongst his/her peers and more positive social norms in his/her community.
Burt [5] contends that individuals may bene t from acting as a bridge between communities.
The theory of structural holes argues that having a relationship with two individuals that do not otherwise have a relationship allows the mediator several advantages.
The mediator can take ideas from both people and come up with something entirely new.
The person in the middle could also take ideas from one of his/her contacts and use them to solve problems that the other is facing.
Burt also argues that the e ects of this type of information brokerage do not extend past one-hop in either direction from the mediator [6].
Since the clustering coe cient of a node measures the fraction of a nodes neighbors that are also neighbors, it captures the extent to which a node acts as a mediator between his/her friends.
Thus, if a person s clustering coe cient indicates that many of their neighbors, are not neighbors themselves, that person might brokerage.
We provide two simple and scalable algorithms for counting the number of triangles incident on each node in a graph using MapReduce.
As we will see in the next section, given these counts, and the degree of each node, computing the clustering coe cient is straightforward.
Previous work dealt with the massive scale of these graphs by resorting to approximate solutions.
Tsourakakis et al. [17] used sampling to give an unbiased estimate of the number of triangles in the whole graph.
Becchetti et al. [2] adapt the notion of min wise independent permutations [3] to approximately count the number of triangles incident on every node.
We show how a judicious partition of the data can be used to achieve comparable improvements in speed without resorting to approximations, and allows us to compute exact clustering co-e cient for all nodes.
One of the main technical challenges in computing clustering coe cients that we address is dealing with skew.
A salient feature of massive social network data is that there are typically several nodes with extraordinarily high degree [1].
In parallel computations this often leads to some machines taking a much longer time than others to complete the same task on a di erent chunk of the input.
Practitioners are all to familiar with the  curse of the last reducer , in which 99% of the computation  nishes quickly, but the remaining 1% takes a disproportionately longer amount of time.
For all of the algorithms we propose, we prove worst case bounds on their performance.
Speci cally, we show our algorithms perform very well under very skewed data distributions.
Whereas for the naive algorithm the running time on di erent partitions of the data may di er by a factor of 20 or more, the algorithms we propose partition the data across the available nodes almost equally, resulting in much more uniform running times.
Since memory is often a limiting factor when performing computation on large datasets, our second algorithm also allows the user to tradeo  the RAM required per machine with the total disk space required to run the algorithm.
For example, by increasing the total amount of disk space necessary by a factor of 2 allows us to reduce the maximum memory necessary per machine by a factor of 4, while performing the same amount of work in aggregate.
Since RAM is typically a much more expensive resource than hard disk space, this leads to a favorable tradeo  for the end user.
The problem of computing the clustering coe cient, or almost equivalently counting triangles, in a graph has a rich history.
The folklore algorithm iterates over all of the nodes and checks whether any two neighbors of a given node v are themselves connected.
Although simple, the algorithm does not perform well in graphs with skewed degree distributions as a single high degree node can lead to an O(n2) running time, even on sparse graphs.
Chiba and Nishizeki [7] studied this problem in detail and introduced an optimal sequential algorithm for this problem.
Their procedure, K3, runs in O(n) time on planar graphs, and more generally in O(m (G)) time where  ( ) denotes the arboricity of the graph.
(Refer to [7] for further details and de nitions.)
In his thesis, Schank [16], gives a further description and analysis of a number of di erent algorithms for this problem.
To battle the fact that the graph may be too large to  t into memory Coppersmith and Kumar [9] and Buriol et al. [4] propose streaming algorithms that estimate the total number of triangles with high accuracy all while using limited space.
Becchetti et al. [2] solve the harder problem of estimating the number of triangles incident on each node.
Their algorithm takes O(log n) sequential scans over the data, using O(n) memory to give an approximate number of triangles incident on every node.
Perhaps the work most closely related to ours is that of [17].
The authors give a randomized MapReduce procedure for counting the total number of triangles in a graph and prove that the algorithm gives the correct number of triangles in expectation.
They also bound the variance and empirically demonstrate the speedups their algorithm achieves over the naive approach.
Our algorithms achieve comparable speedups, but there are two key di erences between our work and theirs.
First, we give an exact algorithm, whereas they give one that gives the correct answer in expectation.
Second, and more importantly, our algorithms give the total number of triangles incident on each node whereas they give the total number of triangles in the entire graph.
Having the number of triangles incident on each node is crucial for computing the clustering coe cient of each node, a metric of importance in the study of social networks.
Let G = (V, E) be an unweighted, undirected simple graph and let n = |V | and m = |E|.
Denote by  (v) the set of neighbors of v, i.e.  (v) = {w   V | (v, w)   E}.
Let dv denote the degree of v, dv = | (v)|.
The clustering coe cient [18] for a node v   V in an undirected graph is de ned by, cc(v) =
 The numerator in the above quotient is the number of edges between neighbors of v. The denominator is the number of possible edges between neighbors of v. Thus, the clustering coe cient measures the fraction of a nodes neighbors that are the themselves connected.
There is also an equivalent way to view the clustering coe cient.
For a pair (u, w) to contribute to the numerator, they must both be connected to v and they must both be connected to each other.
Thus {u, v, w} must form a triangle in G. A triangle in G is simply a set of three nodes {u, v, w}   V such that (u, v), (v, w), (u, w)   E. Thus, the algorithms we give in this paper to compute the clustering coe cient will do so by counting the number of triangles incident on each node using MapReduce.
Computing the degree of each node for the denominator using MapReduce is straightforward, see [11] for details.
MapReduce has become a de facto standard for parallel computation at terabyte and petabyte scales.
In this section we give the reader a brief overview of computation in the MapReduce computing paradigm (see [14, 19] for more information).
|{(u, w)   E | u    (v) and w    (v)}| .
`dv   pairs and the computation proceeds in rounds.
Each round is split into three consecutive phases: map, shu e and reduce.
In the map phase the input is processed one tuple at a time.
This allows di erent tuples to be processed by di er-ent machines and creates an opportunity for massive paral-lelization.
Each machine performing the map operation, also known as a mapper, emits a sequence of (cid:104)key; value(cid:105) pairs which are then passed on to the shu e phase.
This is the synchronization step.
In this phase, the MapReduce infrastructure collects all of the tuples emitted by the mappers, aggregates the tuples with the same key together and sends them to the same physical machine.
Finally each key, along with all the values associated with it, are processed together during the reduce phase.
Here too, the operations on data with one key are independent of data with a di erent key and can be processed in parallel by di erent machines.
Although MapReduce has allowed for computation on massive data sets to be performed on commodity hardware, it is not a silver bullet, and designers of MapReduce algorithms must still be careful not to overstep the bounds of the system.
The authors of [11] give a theoretical model of MapRe-duce computation and describe the following limitations.
  Machine Memory The size of the input to a MapRe-duce computation is typically too large to  t into memory.
Therefore, the memory used by a single mapper or a reducer should be sublinear in the total size of the input.
  Total Memory In addition to being limited in the memory available to a single machine, the total space used by a MapReduce computation must also remain bounded.
While some duplication of the data is allowed, the total space used should always be o(n2) for an input of size n, thus prohibiting exorbitant data duplication [11].
  Number of Rounds Finally, since each MapReduce round can involve shu ing terabytes of data across different machines in a data center, the number of rounds taken by a MapReduce algorithm should remain constant, or at worst logarithmic in the input size.
We begin by presenting the folklore algorithm for computing the number of triangles in the graph.
We call the process of generating paths of length 2 among the neighbors of node v,  pivoting  around v. The algorithm works by pivoting around each node and then checking if there exist an edge that will complete any of the resulting
 We note that each triangle {u, v, w} is counted 6 times total (once as {u, v, w},{u, w, v},{v, u, w},{v, w, u},{w, u, v}, and {w, v, u}).
The analysis of the running time is straight-v).
In constant degree graphs, where dv = O(1) for all v, this is a linear time algorithm.
However, even a single high degree node can lead to a quadratic running time.
Since such high degree nodes are often found in real world large graphs, this algorithm is not practical for real-world, massive graphs.
forward, as the algorithm runs in time O(P v V d2 for u    (v) do Algorithm 1 NodeIterator(V,E)







 for w    (v) do
 if ((u, w)   E) then
 To see how to reduce the running time of the above algorithm, note that each triangle is counted six times, twice by pivoting around each node.
Moreover, those pivots around high degree nodes generate far more 2-paths and are thus much more expensive than the pivots around the low degree nodes.
To improve upon the baseline algorithm Schank [16] proposed that the lowest degree node in each triangle be  responsible  for making sure the triangle gets counted.
Let (cid:31) be a total order on all of the vertices, with the property that v (cid:31) u if dv > du, with ties broken arbitrarily (but consistently).
Algorithm 2 NodeIterator++(V,E)







 for u    (v) and u (cid:31) v do if ((u, w)   E) then for w    (v) and w (cid:31) u do
 The algorithm restricts the set of 2-paths generated from v s neighbors, to be only those where both endpoints of the
 presented in Algorithm 2.
Although this is a subtle change, it has a very large impact both in theory and in practice.
Lemma 1 ([16]).
The running time of Algorithm NodeIt-erator++ is O(m3/2).
  We note that this is the best bound possible.
Consider the  lollipop graph  consisting of a clique on n nodes and a   n = O(n) edges in the graph.
Furthermore, there are no triangles among any of the nodes in the path, but there is a triangle between any three nodes in the the clique.
Thus path on the remaining nodes.
There are m = `  the graph has`    =  (n3/2) triangles.
  + n   n
 n


 The algorithms presented in the previous section implicitly assume that the graph data structure  ts into memory of a single machine.
For massive graphs, this is no longer case and researchers have turned to parallel algorithms to remedy this problem.
Although parallel algorithms allow for computation on much larger scales, they are not a panacea, and algorithm design remains crucial to success of algorithms.
In particular, space requirements of algorithms do not disappear simply because there are more machines available.
For large enough n, O(n2) memory is still an unreasonable request for n   108, an n2 space algorithm requires approximately 10 petabytes of storage.
algorithms in the previous section to the MapReduce framework.
We begin with the NodeIterator algorithm.
The implementation of the algorithm is as follows:   Round 1: Generate the possible length two paths in the graph by pivoting on every node in parallel.
  Round 2: Check which of the length two paths generated in Round 1 can be closed by an edge in the graph and count the triangles accordingly.
As we noted before, in the NodeIterator algorithm, a single high degree node can lead to the generation of O(n2)
 to MapReduce.
The general approach is the same as above, and we present the exact algorithm in Algorithm 3.
Note that in Round 2, in addition to taking the output of Round 1, the algorithm also takes as input the original edge list.
The algorithm di erentiates between the two di erent types of input by using a special character  $  that is assumed not to appear anywhere else in the input.
for (u, w) : u, w   S do emit (cid:104)v; (u, w)(cid:105) if v (cid:31) u then emit (cid:104)u; v(cid:105) Algorithm 3 MR-NodeIterator++(V,E)














 if Input of type (cid:104)v; (u, w)(cid:105) then if Input of type (cid:104)(u, v); (cid:105) then emit (cid:104)(u, w); v(cid:105) emit (cid:104)(u, v); $(cid:105) if $   S then for v   S   V do emit (cid:104)v; 1(cid:105)
 We analyze the total time and space usage of the MapRe-duce version of the NodeIterator++ algorithm.
Correctness and overall running time follow from the sequential case.
The next lemma implies that the total memory required per machine is sublinear.
Thus, no reducer gets  ooded with too much data, even if there is skew in the input.
Lemma 2.
The input to any reduce instance in the  rst round has O(   m) edges.
  Proof.
For the purposes of the proof, partition the nodes into two subclasses: the high degree nodes, H = {v   V : dv     m} and the low degree nodes, L = {c   V : dv <   m}.
Note that the total number of high degree nodes is     m).
Therefore a high degree node at most 2m/ can have at most |H| = O( m) high degree neighbors.
If v   H, the reducer which receives v as a key will only receive u    (v) where du (cid:31) dv.
Thus this reducer will receive at   most neighbors O( m) edges.
On the other hand, if the key   in the reduce instance is a low node, then its neighborhood has size at most m by de nition.
m = O( The sequential analysis also implies that: Lemma 3.
The total number of records output at the end of the  rst reduce instance is O(m3/2).
The bound in the Lemma above is a worst case bound for a graph with m edges.
As we show in Section 5, the actual amount of data output as a result of the Reduce 1 is much smaller in practice.
Finally, note that the number of records to any reducer in round 2 for a key (u, v) is at most O(du + dv) = O(n).
On graphs where this is value is too large to  t into memory, one can use easily distribute the computation by further partitioning the output of Round 2 across r di erent machines, and sending a copy of the edge to all r of the machines.
We omit this technicality for the sake of clarity and refer the interested reader to [11] for exact details.
In this section we present a di erent algorithm for counting triangles.
The algorithm works by partitioning the graphs into overlapping subsets so that each triangle is present in at least one of the subsets.
Given such a partition, we can then use any sequential triangle counting algorithm as a black box on each partition, and then simply combine the results.
We prove that the algorithm achieves perfect work e ciency for the triangle counting problem.
As the partitions get  ner and  ner, the total amount of work spent on  nding all of the triangles remains at O(m3/2); instead there are simply more machines with each doing less work.
Overall the algorithm e ectively takes any triangle counting algorithm that works on a single machine and distributes the computation without blowing up the total amount of work done.
We begin by providing a high level overview.
First, partition the nodes into   equal sized groups, V = V1 V2 .
.
. V  where   > 0 with Vi   Vj =   for i (cid:54)= j. Denote by Vijk = Vi   Vj   Vk.
Then let Eijk = {(u, w)   E : u, w   Vijk} be the set of edges between vertices in Vi, Vj and Vk, and let Gijk = (Vi   Vj   Vk, Eijk) be the induced graph on Vijk.
Let G =  i<j<kGijk be the set of graphs.
An instance of a graph Gijk contains 3/  fraction of the vertices of the original graph, and, in expectation a O(1/ 2) fraction of the edges.
Since each node of a triangle must be in one part of the partition of V , and G contains all combinations of parts of V , every triangle in graph G appears in at least one graph in the set G. The algorithm performs a weighted count of the number of triangles in each subgraph in G. The weights correct for the fact that a triangle can occur in many subgraphs.
The MapReduce code for the partition step is presented in Algorithm 4.
Note that we do not restrict the algorithm used in the reducer, we only insist that the triangles be weighted.
We begin by proving the correctness of the algorithm Lemma 4.
Each triangle gets counted exactly once after weighting.
Proof.
Consider a triangle (w, x, y) whose vertices lie in Vh(w), Vh(x) and Vh(z).
If the vertices are mapped to distinct partitions, i.e., h(w) (cid:54)= h(x), h(w) (cid:54)= h(y), and h(x) (cid:54)= h(z) then the triangle will appear exactly once.
If the vertices are mapped to two distinct partitions, e.g., h(x) = h(w) but h(x) (cid:54)= h(y), the triangle will appear in   2 times.
For example, suppose that   = 4, h(x) = h(y) = 1 and h(z) = 3, i   h(u) j   h(v) for a   [0,     1] do for b   [a + 1,     1] do Algorithm 4 MR-GraphPartition(V,E,  )

















 for c   [b + 1,     1] do if {i, j}   {a, b, c} then emit (cid:104)(a, b, c); (u, v)(cid:105).
Input (cid:104)(i, j, k); Eijk   E(cid:105) Count triangles on Gijk for every triangle (u, v, w) do z       2 Scale triangle (u, v, w) weight by 1/z z  `h(u) z   1 if h(u) = h(v) = h(w) then elif h(u) = h(v) | h(v) = h(w) | h(u) = h(w) then   + h(u)(    h(u)   1) +` h(u) 1  

 then the triangle will appear in graphs G0,1,3 and G1,2,3.
In the case h(x) = h(y) = h(w) one can verify that the triangle  +h(x)( h(x) 1)+` h(x) 1 appears exactly`h(x)   times.
The  rst term represents graphs Gijk with i < j < h(x), the second those with i < h(x) < k and the last those with h(x) < j < k.
We now dive deeper into the role of the tunable parameter,  .
At a high level,   trades o  the total space used by the algorithm (as measured at the end of the map phase) with the size of the input to any reduce instance.
We quantify this tradeo  below: Lemma 5.
For a setting of  :   The expected size of the input to any reduce instance is O( m  2 ).
  The expected total space used at the end of the map phase is O( m).
Proof.
To prove the  rst bound,  x one subset Gijk   G.
The subset has |Vijk| = 3n   vertices in expectation.
Consider a random edge e = (u, v) in the graph, there is a 3/  probability that u   Vijk, and a further 3/  probability that v   Vijk.
Therefore, for a random edge, it is present in Gijk with probability 9/ 2.
The  rst bound follows by linearity of expectation.
For the second bound, observe that there are O( 3) di erent partitions, and thus the total number of edges emitted by the map phase is O( 3   m  2 ) = O(m ).
The previous lemma implies that increasing the total space used by the algorithm by a factor of 2 results in a factor of 4 improvement in terms of the memory requirements of any reducer.
This is a favorable tradeo  in practice since RAM (memory requirements of the reducers) is typically much more expensive than disk space (the space required to store the output of the mappers).
Finally, we show that our algorithm is work e cient.
Theorem 1.
For any       m the total amount of work Enron web-BerkStan as-Skitter LiveJournal Twitter Directed Undirected Nodes 3.7 x 104 6.9 x 105 1.7 x 106 4.8 x 106 4.2 x 107 Edges 3.7 x 105 7.6 x 106 11 x 106 6.9 x 106 1.5 x 109 Edges 3.7 x 105 1.3 x 107 22 x 106 8.6 x 107 2.4 x 109 Table 1: Statistics about the data sets used for algorithm evaluation.
Proof.
As we noted in section 3 the running time of the best algorithm for counting triangles is O(m3/2).
Lemma
 tioned across  3 machines, each with a 1/  fraction of the input.
Computing the graph partition takes O(1) time per   output edge.
The total number of edges output is O(m ) = O(m3/2) for   < m. The running time on each reducer  3/2    m  2   m3/2    3 is O
 .
Summing up over the O( 3) graph partitions, we recover the O(m3/2) total work.
In this section we give an experimental evaluation of the MapReduce algorithms de ned above.
All of the algorithms were run on a cluster of 1636 nodes running the Hadoop implementation of MapReduce.
We used NodeIterator++ as the triangle counting subroutine in MR-GraphPartition.
We use several data sets to evaluate our algorithms all of which are publicly available.
The Twitter data set is available at [12], and the rest are from the SNAP library1.
In our experiments we consider each edge of the input to be undi-rected to make the input graphs even larger, showcasing the scalability of the approach.
Thus if an edge (u, v) appears in the input, we also add edge (v, u) if it did not already exist.
Table 1 shows how many edges each graph has before and after undirecting the edges.
As discussed in Section 3 both the NodeIterator and NodeIt-erator++ algorithms work by computing length 2-paths in the graph and then checking if there is an edge in the graph that completes the 2-path to form a triangle.
If there exists a node of linear degree in the input, the NodeIterator algorithm would output roughly  (n2) 2-paths, whereas the NodeIterator++ algorithm always checks at most O(m3/2) length 2 paths.
In order to see if these worst case claims apply in real-world data, Table 2 shows the number of 2-paths generated by both algorithms.
In the case of LiveJournal and Enron, NodeIterator++ outputs an order of magnitude fewer 2-paths than NodeIterator.
In the case of Twitter, the reduction is almost three orders of magnitude.
As one might expect, the drastic di erences in the number of 2-paths generated has a large impact on the running time of the algorithms.
In the case of Twitter, the NodeIterator algorithm generates 247 trillion 2-paths.
Even if they were encoded using only 10 bytes each, this would amount to 2.5 petabytes which is prohibitive to compute performed by all of the machines is O(m3/2).
1http://snap.stanford.edu (b) Twitter Figure 1: Figures 1(a) and 1(b) show the distribution of the clustering coe cients for LiveJournal and Twitter.
Enron web-BerkStan as-Skitter LiveJournal Twitter NodeItr 51.13 x 106 56.0 x 109 32 x 109 14.5 x 109 246.87 x 1012 NodeItr++ Reduction 2.92 x 106 176 x 106 189 x 106 1.36 x 109 3.0 x 1011




 Table 2: The number of 2-paths generated (and shu ed) by the NodeIterator and the NodeItera-tor++ algorithms along with the reduction factor.
web-BerkStan as-Skitter LiveJournal Twitter   NodeIterator NodeIterator++ GP



 > 840 > 100




 Table 3: The running times of all algorithms in minutes.
with.
The NodeIterator++ algorithm only generates 301 billion 2-paths, which takes about 2 hours.
In the case of LiveJournal computing the 2-paths via NodeIterator takes about 50 minutes on average, whereas NodeIterator++ does this in under 2 minutes.
To compare the two algorithms, and contrast them against the naive approach, we give the overall running times of all three on various real-world data sets in Table 3.
In their work on scaling clustering coe cient Tsourakakis et al [17] give an algorithm that approximates the number of triangles in the entire graph.
The authors compare their algorithm to the NodeIterator algorithm and across a wide variety of data sets show that they get speedups ranging from 10 to 100 at approximately 2% cost in accuracy.
We show that large speedups can also be obtained without any loss in precision.
The algorithm we propose also has the added bene t of computing the number of triangles incident on each node, something that is impossible using the Doulion algorithm [17].
Figure 1 shows the distributions of the clustering coe -cients in the range (0, 1) for the undirected versions of Live-Journal and Twitter.
We removed nodes that have clustering coe cients of exactly 0 or 1 because they tend to be sporadic users of both services that happen to have a few friends that are either completely unconnected or completely connected.
Note that there is a larger proportion of nodes with higher clustering coe cients in LiveJournal than in Twitter.
Thus it is more likely that two people connected to a given node are also connected in LiveJournal than in Twitter.
One possible explanation for this is that people use LiveJournal more for social interactions and such networks often have high clustering [18].
Twitter on the other hand, may be used more for information gathering and dissemination than for the type of social interaction going on in LiveJournal.
One problem that is inherently familiar to practitioners working on large data sets is the skew of the data.
It is well known, for example, that the degree distribution in many naturally occurring large graphs follows a power law [1], and almost every graph has a handful of nodes with linear degree.
This phenomenon manifests itself in computations on large datasets as well.
It is rather common to have 99% of the map or reduce tasks  nish very quickly, and then spend just as long (if not longer) waiting for the last task to succeed.
Figures 2(a), 2(b) and 2(c) further explain the di erences in wall clock times by showing the distribution of reducer completion times for a typical run of the pivot step of NodeIt-erator, NodeIterator++ and the main step of MR-GraphPartition.
Figure 2(a) illustrates this  curse of the last reducer,  by showing a heavy-tailed distribution of reducer completion times for the NodeIterator algorithm.
The majority of tasks  nish in under 10 minutes, but there are a handful of reducers that take 30 or more minutes and one that takes roughly
 tion times received high degree nodes to pivot on.
Since for a node of degree d, NodeIterator generates O(d2) 2-paths, these reducers take a disproportionately long time to run.
Figures 2(b) and 2(c) show that the NodeIterator++ and MR-GraphPartition algorithms handle skewed much better.
(b) NodeIterator++ (c) GraphPartition Figure 2: Figures 2(a), 2(b) and 2(c) show the distribution of the reducer wall clock times for typical runs of the pivot step (Round 1) for NodeIterator and NodeIterator++ algorithms, and the main round of the MR-GraphPartition algorithm.
respectively.
All runs in this plot were generated using 200 reducers.
Figure 3: The disk space vs. memory tradeo  for the MR-GraphPartition algorithm as empirically measured on the LiveJournal dataset.
The   values range from 4 to 31.
The distribution of reducer completion times is much more concentrated around the mean.
In Section 4.2.1 we showed that the MR-GraphPartition algorithm allowed the user to trade o  between the memory required per machine and the total disk space required to store the intermediate result in the shu e round.
We present this tradeo  for the LiveJournal graph in Figure 3, which plots the two quantities for various values of   ranging from 4 to 31.
The graph follows almost exactly the tradeo  implied theoretically by Lemma 5.
Initially as   increases, the total increase in disk space is small but the reduction in memory required per reducer is quite dramatic.
As   increases further, the decrease in memory is small on an absolute scale, but leads to large and larger increases in disk space required.
In addition to allowing the user to tune the algorithm for the speci c parameters of the cluster hardware, the di erent setting of the parameters changes the overall running time of the job.
In Figure 4 we present the running time of the MR-GraphPartition algorithm for di erent values of  .
The The running time of the MR-Figure 4: GraphPartition algorithm on the LiveJournal dataset.
The graph did not  t into memory of a reducer at   < 13.
Error bars indicate 95% con -dence intervals.
plot averages  ve di erent measurements of  .
Although the measurements are noisy, since there are many other jobs running on the cluster at the same time, there is a clear decrease in running time as   increases initially.
We attribute this to the fact that at high   the reduce step may need to use more memory than available and page out to disk, which ceases to be a problem as the input size decreases at higher values of  .
As   continues to increase, this bene t disappears and the total running time slowly increases with  .
This is a combination of two factors.
First, for high   several reduce tasks are scheduled sequentially on the available machines and are performed serially.
Second, full work e ciency is achieved on worst case instances those where the number of length two paths is  (m3/2).
Since we are not typically faced with the worst case instances, increasing   leads to a slight increase in the total amount of work being done.
In this work we presented two algorithms for e ciently computing clustering coe cients in massive graphs.
Our al-approach, MR-NodeIterator++, is a specialized algorithm designed explicitly for computing the clustering coe cient of every node.
The second approach, MR-GraphPartition, describes a more general framework that takes any sequential triangle counting algorithm as a black box and adapts it to the MapReduce setting.
In fact, MR-GraphPartition can easily be extended to counting other small subgraphs (for example K2,3) in a distributed setting.
To count a sub-graph H on h nodes, again partition the nodes into   equal sized sets; create all possible subsets of size h from the   partitions, and distribute the  h subgraphs across di erent machines.
Both the MR-NodeIterator++ and the MR-GraphPartition algorithms outperform the naive MapReduce implementation for computing the clustering coe cient.
The improvement is consistently at least a factor of 10 and often is sig-ni cantly larger.
(For example, it is impossible for us to assess the running time of the naive algorithm on the Twitter graph.)
These speedup improvements are comparable to those obtained by [17] and [2], but on a much harder problem.
The output of our algorithms gives the exact number of triangles incident on every node in the graph.
A signi cant part of the improvement in the running time comes from the fact that the proposed algorithms do a much better job of distributing the computation evenly across the machines, even in the presence of large skew in the input data.
This allows them to avoid the  curse of the last reducer,  whereby the majority of machines in a distributed job  nish quickly, and a single long running job determines the overall running time.
The two algorithms we propose also have di erent theoretical behavior and exploit di erent aspects of the MapRe-duce infrastructure.
The MR-NodeIterator++ algorithm uses MapReduce to compute in parallel the intersection between all possible two paths and edges in the graph.
Here, the computation itself is trivial, and the infrastructure is used to parallelize this simple approach.
The MR-Graph-Partition uses the infrastructure to divide the graph into overlapping subgraphs, while ensuring that no triangle is lost.
The reducers then execute the sequential algorithm, albeit on a much smaller input.
