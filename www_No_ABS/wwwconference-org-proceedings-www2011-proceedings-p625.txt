The neighbourhood function NG .t / of a graph returns for each t 2 N the number of pairs of nodes hx; yi such that y is reachable from x in less that t steps.
It provides data about how fast the  average ball  around each node expands.
From the neighbourhood function, several interesting features of a graph can be estimated, and in this paper we are in particular interested in the effective diameter, a measure of the  typical  distance between nodes.
Palmer, Gibbons and Faloutsos [10] proposed an algorithm to approximate the neighbourhood function (see their paper for a review of previous attempts at approximate evaluation); the authors distribute an associated tool, snap, which can approximate the neigh-bourhood function of medium-sized graphs.
The algorithm keeps track of the number of nodes reachable from each node using Flajolet  Martin counters, a kind of sketch that makes it possible to compute the number of distinct elements of a stream in very little space.
A key observation was that counters associated with different streams can be quickly combined into a single counter associated with the concatenation of the original streams.
In this paper, we describe HyperANF a breakthrough improvement over ANF in terms of speed and scalability.
HyperANF uses the new HyperLogLog counters [5], and combines them ef ciently by means of broadword programming [8].
Each counter is made by a number of registers, and the number of registers depends only on the required precision.
The size of each register is doubly logarithmic in the number of nodes of the graph, so HyperANF, for a  xed precision, scales almost linearly in memory (i.e., O.n log log n/).
By contrast, ANF memory requirement is O.n log n/.
Using HyperANF, for the  rst time we can compute in a few hours the neighbourhood function of graphs with more than one billion nodes with a small error and good con dence using a standard workstation with 128 GB of RAM.
Our algorithms are implement in a tool distributed as free software within the WebGraph framework.1 Armed with our tool, we study several datasets, spanning from small social networks to very large web graphs.
We isolate a statistically de ned feature, the index of dispersion of the distance distribution, and show that it is able to tell  proper" social networks from web graphs in a natural way.
HyperANF is an evolution of ANF [10], which is implemented by the tool snap.
We will give some timing comparison with snap, but we can only do it for relatively small networks, as the large memory footprint of snap precludes application to large graphs.
called HADI [7] has been presented.
HADI runs on one of the  fty largest supercomputers the Hadoop cluster M45.
The only published data about HADI s performance is the computation of the neighbourhood function of a Kronecker graph with 2 billion links, which required half an hour using 90 machines.
HyperANF can compute the same function in less than  fteen minutes on a laptop.
The rather complete survey of related literature in [7] shows that essentially no data mining tool was able before ANF to approximate the neighbourhood function of very large graphs reliably.
A remarkable exception is Cohen s work [3], which provides strong theoretical guarantees but experimentally turns out to be not as scal-able as the ANF approach; it is worth noting, though, that one of the proposed applications of [3] (Online estimation of weights of growing sets) is structurally identical to ANF.
All other results published before ANF relied on a small number of breadth rst visits on uniformly sampled nodes a process that has no provable statistical accuracy or precision.
Thus, in the rest of the paper we will compare experimental data with snap and with the published data about HADI.
In this section, we present the HyperANF algorithm for computing an approximation of the neighbourhood function of a graph; we start by recalling from [5] the notion of HyperLogLog counter upon which our algorithm relies.
We then describe the algorithm, discuss how it can be implemented to be run quickly using broad-word programming and task decomposition, and give results about its memory requirements and precision.
HyperLogLog counters, as described in [5] (which is based on [4]), are used to count approximately the number of distinct elements in a stream.
For the purposes of the present paper, we need to recall brie y their behaviour.
Essentially, these probabilistic counters are a sort of approximate set representation to which, however, we are only allowed to pose questions about the (approximate) size of the set.2 1 be a hash function Let D be a  xed domain and h W D !
2 mapping each element of D into an in nite binary sequence.
The function is  xed with the only assumption that  bits of hashed values are assumed to be independent and to have each probability 1
 of occurring  [5].
1, let ht .x/ denote the sequence made by For a given x 2 2 the leftmost t bits of h.x/, and ht .x/ be the sequence of remaining bits of x; ht is identi ed with its corresponding integer value in the range f 0; 1; : : : ; 2t (cid:0) 1g.
Moreover, given a binary sequence w, .w/ be the number of leading zeroes in w plus one3 (e.g., we let (cid:26)
 .00101/ D 3).
Unless otherwise speci ed, all logarithms are in (cid:26) base 2.
The value E printed by Algorithm 1 is [5][Theorem 1] an asymptotically almost unbiased estimator for the number n of distinct elements in the stream; for n !
1, the relative standard deviation (that is, the ratio between the standard deviation of E and n) is at m, where  m is a suitable constant (given most  m= m  1:06= p p
 the number of unique elements in a stream [1].
HyperLogLog is a practical counter that starts from the assumption that a hash function can be used to turn a stream into an idealised multiset (see [5]).
C, but (cid:26) is a somewhat standard notation for the ruler func-denote (cid:26) tion [8].
Algorithm 1 The Hyperloglog counter as described in [5]: it allows one to count (approximately) the number of distinct elements in a stream.
 m is a constant whose value depends on m and is provided in [5].
Some technical details have been simpli ed.
h W D !
2 1, a hash function from the domain of items i hb.x/; end; // function add C(cid:0)hb.x/(cid:1)(cid:9) (indexed from 0) and set to (cid:0)1 function add.M : counter; x: item/ begin M  i   max M  i  ; (cid:26) (cid:0)M  j  (cid:0)1 Z (cid:16)Pm(cid:0)1










 11 begin







 foreach item x seen in the stream begin return E D  mm2Z function size.M : counter/ add(M ,x) end; print size.M / end; // function size jD0 2 ; in [5])4.
Moreover [4] even if the size of the registers (and of the hash function) used by the algorithm is unbounded, one can limit it to log log.n=m/ C !.n/ bits obtaining almost certainly the same output (!.n/ is a function going to in nity arbitrarily slowly); overall, the algorithm requires .1C o.1//(cid:1) m log log.n=m/ bits of space (this is the reason why these counters are called HyperLogLog).
Here and in the rest of the paper we tacitly assume that m (cid:21) 64 and that registers are made of dlog log ne bits.
The approximate neighbourhood function algorithm described in [10] is based on the observation that B.x; r/, the ball of radius r around node x, satis es B.x; r/ D [ x!y B.y; r (cid:0) 1/: Since B.x; 0/ D f x g, we can compute each B.x; r/ incrementally using sequential scans of the graph (i.e., scans in which we go in turn through the successor list of each node).
The obvious problem is that during the scan we need to access randomly the sets B.x; r (cid:0) 1/ (the sets B.x; r/ can be just saved on disk on a update  le and reloaded later).
Here probabilistic counters come into play; to be able to use them, though, we need to endow counters with a primitive for the union.
Union can be implemented provided that the counter associated with the stream of data AB can be computed from the counters associated with A and B; in the case of Hyper-LogLog counters, this is easily seen to correspond to maximising the two counters, register by register.
counters.
Since the analogous formula for Flajolet Martin counters is 0:78= m, for a  xed precision HyperANF requires twice as many registers as ANF, but their size is exponentially smaller.
p the algorithm keeps one HyperLogLog counter for each node; at the t-th iteration of the main loop, the counter c v  is in the same state as if it would have been fed with B.v; t /, and so its expected value is jB.v; t /j.
As a result, the sum of all c v s is an (almost) unbiased estimator of NG .t / (for a precise statement, see Theorem 1).
Algorithm 2 The basic HyperANF algorithm in pseudocode.
The algorithm uses, for each node i 2 n, an initially empty Hyper-LogLog counter ci .
The function union.
(cid:0);(cid:0)/ maximises two counters register by register.
c (cid:0) , an array of n HyperLogLog counters function union.M : counter; N : counter/ end add v to c v  foreach i < m begin M  i   max.M  i  ; N  i  / end; // function union foreach v 2 n begin









 end;
 t 0;
 12 do begin










 24 until no counter changes its value.
m c v ; foreach v !
w begin m union.c w ; m/ end; write hv; mi to disk s P v size.c v /; Print s (the neighbourhood function NG .t /) foreach v 2 n begin end; Read the pairs hv; mi and update the array c (cid:0)  t t C 1 We remark that the only sound way of running HyperANF (or ANF) is to wait for all counters to stabilise (e.g., the last iteration must leave all counters unchanged).
As we will see, any alternative termination condition may lead to arbitrarily large mistakes on pathological graphs.5
 Up to now, HyperANF has been described just as ANF with Hy-perLogLog counters.
The effect of this change is an exponential reduction in the memory footprint and, consequently, in memory access time.
We now describe the the algorithmic and engineering ideas that made HyperANF much faster, actually so fast that it is possible to run it up to stabilisation.
Union via broadword programming.
Given two HyperLogLog counters that have been set by streams A and B, the counter associated with the stream AB can be build by maximising in parallel the registers of each counter.
That is, the register i of the new counter is given by the maximum between the i-th register of the  rst counter and the i-th register of the second counter.
in the number of reachable pairs as a termination condition, but this trick makes the tail of the function unreliable.
Each time we scan a successor list, we need to maximise a large number of registers and store the resulting counter.
The immediate way of obtaining this result requires extracting the value of each register, maximise it with the other corresponding registers, and writing down the result in a temporary counter.
This process is extremely slow, as registers are packed in 64-bit memory words.
In the case of Flajolet Martin counters, the problem is easily solved by computing the logical OR of the words containing the registers.
In our case, we resort to broadword programming techniques.
If the machine word is w, we assume that at least w registers are allocated to each counter, so each set of registers is word-aligned.
Let (cid:29) and (cid:28) denote right and left (zero lled) shifting, &, j and   denote bit-by-bit not, and, or, and xor; x denotes the bit-by-bit complement of x.
We use Lk to denote the constant whose ones are in position 0, that is, the constant with the lowest bit of each k-bit k, 2k, .
.
.
subword set (e.g, L8 D 0x01010101010101010101).
We use Hk (cid:28) k (cid:0) 1, that is, the constant with the highest bit of to denote Lk each k-bit subword set (e.g, H8 D 0x8080808080808080).
 It is known (see [8], or [12] for an elementary proof), that the following expression k y WD(cid:16)(cid:0) ..x j Hk / (cid:0) .y & Hk // j x   y(cid:1)   .x j y/ & Hk : x <u performs a parallel unsigned comparison k-by-k-bit-wise.
At the end of the computation, the highest bit of each block of k bits will be set iff the corresponding comparison is true (i.e., the value of the block in x is strictly smaller than the value of the block in y).
Once we have computed x <u k, we generate a mask that is made entirely of 1s, or of 0s, for each k-bit block, depending on whether we should select the value of x or y for that block: (cid:1) (cid:0) Lk  j Hk    .x <u k y/ m D(cid:16)(cid:0).x <u k y/ (cid:29) k (cid:0) 1 j Hk This formula works by moving the high bit denoting the result of the comparison to the least signi cant bit (of each k-bit block).
Then, we or with Hk and subtract 1 from each block, obtaining either a mask with just the high bit set (if we were starting from 1) or a mask with all bits sets except for the high bit (if we were starting from 0).
The last two operation  x those values so that they become 00(cid:1)(cid:1)(cid:1) 0 or 11(cid:1)(cid:1)(cid:1) 1.
The result of the maximisation process is now just x & m j y & m.
This discussion assumed that the set of registers of a counter is stored in a single machine word.
In a realistic setting, the registers are spread among several consecutive words, and we use multiple precision subtractions and shifts to apply the expressions above on a sequence of words.
All other (logical) operations have just to be applied to each word in sequence.
All in all, by using the techniques above we can improve the speed of maximisation by a factor of w=k, which in our case is about 13 (for graphs of up to 232 nodes).
This actually results in a sixfold speed improvement of the overall application in typical cases (e.g., web graphs and b D 8), as about 90% of the computation time is spent in maximisation.
Parallelisation via task decomposition.
Although HyperANF is written as a sequential algorithm, the outer loop lends itself to be executed in parallel, which can be extremely fruitful on a modern multicore architecture; in particular, we approach this idea using task decomposition.
We divide the iteration on the whole set of nodes into a set of small tasks (in the order of the thousands), where each task consists in iterating on a contiguous segment of nodes.
A pool of threads picks up the  rst available task and solves it: as a result, we obtain a performance improvement that is linear in the helped by WebGraph s facilities which allow us to provide each thread with a lightweight copy of the graph that shares the bitstream and associated information with all other threads.
Tracking modi ed counters.
It is an easy observation that a counter c that does not change its value is not useful for the next step of the computation: all counters using c during their update would not change their value when maximising with c (and we do not even need to write c on disk).
We thus keep track of modi ed counters and skip altogether the maximisation step with unmodi ed ones.
Since, as we already remarked, 90% of computation time is spent in maximisation, this approach leads to a large speedup after the  rst phases of the computation, when most counters are stabilised.
For the same reason, we keep track of the harmonic partial sums of small blocks (e.g., 64) of counters.
The amount of memory required is negligible, but if no counter in the block has been modi- ed, we can avoid a costly computation.
Systolic computation.
HyperANF can be run in systolic mode.
In this case, we use also the transposed graph: whenever a counter changes, it signals back to its predecessors that at the next round they could change their values.
Now, at each iteration nodes that have not been signalled are entirely skipped during the computation.
Systolic computations are fundamental to get high-precision runs, as they reduce the cost of an iteration to scanning only the arcs of the graph that are actually moving information around.
We switch to systolic computation when less than half of the counters change their values.
Very little has been published about the statistical behaviour of ANF.
The statistical properties of approximate counters are well known, but the values of such counters for each node are highly dependent, and adding them in a large amount can in principle lead to an arbitrarily large variance.
Thus, making precise statistical statements about the outcome of a computation of ANF or Hyper-ANF requires some care.
The discussion in the following sections is based on HyperANF, but its results can be applied mutatis mu-tandis to ANF as well.
Consider the output ONG .t / of algorithm 2 at a  xed iteration t.
We can see it as a random variable ONG .t / DX i2n where6 each Xi;t is the HyperLogLog counter that counts nodes reached by node i in t steps; what we want to prove in this section is a bound on the relative standard deviation of ONG .t / (such a proof, albeit not dif cult, is not provided in the papers about ANF).
First observe that [5], for a  xed a number of registers m per counter, the standard deviation of Xi;t satis es pVar Xi;t   jB.i; t /j  m; where m is the guaranteed relative standard deviation of a Hy-perLogLog counter.
Using the subadditivity of standard deviation (i.e., if A and B have  nite variance,pVar A C B  pVar A  C pVar B ), we prove the following ONG .t / of Algorithm 2 at the t-th it-
f 0; 1; : : : ; n (cid:0) 1g, so i 2 n means that 0  i < n.
THEOREM 1.
The output Xi;t Pr eration is an asymptotically almost unbiased estimator7 of NG .t /, that is E  ONG .t /  D 1 C  1.n/ C o.1/ for n !
1; q Var  ONG .t /  NG .t / (cid:0)5 where  1 is the same as in [5][Theorem 1] (and j 1.x/j < 5 (cid:1) 10 as soon as m (cid:21) 16).
Moreover, ONG .t / has the same relative standard deviation of the Xi  s, that is  m: NG .t / PROOF.
We have that E  ONG .t /  D E(cid:2)P orem 1 of [5], E Xi;t   D jB.i; t /j .1 C  1.n/ C o.1//, hence the q
  rst statement.
For the second result, we have: Var  ONG .t /  (cid:3).
By The-pVar Xi   jB.i; t /j i2n Xi;t
  m i2n D m: NG .t / NG .t / i2n NG .t / Since, as we recalled in Section 3.1, the relative standard deviation m satis es m  1:06= m, to get a speci c value  it is suf cient to choose m (cid:25) 1:12=2; this assumption yields an overall space requirement of about p
 2 n log log n bits (here, we used the obvious upper bound jB.i; t /j  n).
For instance, to obtain a relative standard deviation of 9:37% (in every iteration) on a graph of one billion nodes one needs 74:5 GB of main memory for the registers (for a comparison, snap would require 550 GB).
Note that since we write to disk the new values of the registers, this is actually the only signi cant memory requirement (the graph can be kept on disk and mapped in memory, as it is scanned almost sequentially).
Applying Chebyshev s inequality, we obtain the following: COROLLARY 1.
For every ", " ONG .t / NG .t /
 (cid:21) 1 (cid:0) 2 "2 : m In [5] it is argued that the HyperLogLog error is approximately Gaussian; the counters, however, are not statistically independent and in fact the overall error does not appear to be normally distributed.
Nonetheless, for every  xed t, the random variable ONG .t / seems to be unimodal (for example, the average p-value of the Dip unimodality test [6] for the cnr-2000 dataset is 0:011), so we can apply the Vysochanski -Petunin inequality [13], obtaining the bound " ONG .t / NG .t / Pr
 (cid:21) 1 (cid:0) 42
 m In the rest of the paper, to state clearly our theorems we will always assume error " with con dence 1 (cid:0)  .
It is useful, as a practical
 gible bias on ONG .t / as an estimator for NG .t /: the other estimators that will appear later on will be quali ed as  (almost) unbiased , where  almost  refers precisely to the above mentioned negligible bias.
# # of the neighbourhood function we can assume a relative error of km with con dence 1(cid:0) 4=.9k2/ (e.g., 2m with 90% con dence, or 3m with 95% con dence).
As an empirical counterpart to the previous results, we considered a relatively small graph of about 325 000 nodes (cnr-2000, see Section 6 for a full description) for which we can compute the exact neighbourhood function NG .
(cid:0)/; we ran HyperANF 500 times with m D 256.
At least 96% of the samples (for all t) has a relative error smaller than twice the theoretical relative standard deviation 6:62%.
The percentage jumps up to 100% for three times the relative standard deviation, showing that the distribution of the values behaves better than what the theory would guarantee.
As advocated in [10], being able to estimate the neighbourhood function on real-world networks has several interesting applications.
Unfortunately, all published results we are aware of lack statistical satellite data (such as con dence intervals, or distribution of the computed values) that make it possible to compare results from different research groups.
Thus, in this section we try to discuss in detail how to derive useful data from an approximation of the neighbourhood function.
The distance cdf.
We start from the apparently easy task of computing the cumulative distribution function of distances of the graph G (in short, distance cdf ), which is the function HG .t / that gives the fraction of reachable pairs at distance at most t, that is, HG .t / D NG .t / maxt NG .t / : In other words, given an exact computation of the neighbourhood function, the distance cdf can be easily obtained by dividing all values by the largest one.
Being able to estimate NG .t / allows one to produce a reliable approximation of the distance cdf: and con dence 1 (cid:0)  , that is THEOREM 2.
Assume NG .t / is known for each t with error " Pr NG .t / (cid:21) 1 (cid:0)  :
 Let OHG .t / D ONG .t /= maxt ONG .t /.
Then OHG .t / is an (almost) unbiased estimator for HG .t /; moreover, for a  xed sequence t0, t1, OHG .tk / is : : : , tk(cid:0)1, for every " and all 0  i < k we have that known with error 2" and con dence 1 (cid:0) .k C 1/ , that is, " ONG .t / #
 i2k Pr OHG .ti / HG .ti /

 PROOF.
Note that if
 holds for every t, then a fortiori 1 (cid:0) "  max ONG .t /= max NG .t /  1 C " t t (because, although the maxima might be  rst attained at different values of t, the same holds for any larger values).
As a consequence, 1 (cid:0) 2"  1 (cid:0) "

  OHG .t / HG .t /
 1 (cid:0) "
 The probability 1 (cid:0) .k C 1/  is immediate from the union bound, as we are considering k C 1 events at the same time.
Note two signi cant limitations:  rst of all, making precise statements (i.e., with con dence) about all points of HG .t / requires a very high initial error and con dence.
Second, the theorem holds if HyperANF has been run up to stabilisation, so that the probabilistic guarantees of HyperLogLog hold for all t.
The  rst limitation makes in practice impossible to get directly sensible con dence intervals, for instance, for the average distance or higher moments of the distribution (we will elaborate further on this point later).
Thus, only statements about a small,  nite number of points can be approached directly.
The second limitation is somewhat more serious in theory, albeit in practice it can be circumvented making suitable assumptions about the graph under examination (which however should be clearly stated along the data).
Consider the graph G made by two k-cliques joined by a unidirectional path of ` nodes (see Figure 2).
Even neglecting the effect of approximation, G can  fool  Hyper-ANF (or ANF) so that the distance cdf is completely wrong (see Figure 1) when using any stopping criterion that is not stabilisa-tion.
Figure 1: The real cdf of the graph in Figure 2 (+), and the one that would be computed using any termination condition that is not stabilisation (*); here ` D 10 and k D 260.
Figure 2: Two k-cliques joined by a unidirectional path of ` nodes: terminating even one step earlier than stabilisation completely miscalculates the distance cdf (see Figure 1); the effective diameter is `C 1, but terminating even just one step earlier than stabilisation yields an estimated effective diameter of 1.
Indeed, the exact neighbourhood function of G is given by:
 .t C 1/(cid:0)2k C ` (cid:0) t if t D 0

 (cid:16) (cid:1) (cid:0) 2k C 2k2  (cid:0) 2k C 3k2

 NG .t / D if 1  t  ` if ` < t.
The key observation is that the very last value is signi cantly larger than all previous values, as at the last step the nodes of the right clique become reachable from the nodes of the  rst clique.
Thus, if
 to compute the cdf will be smaller by (cid:25) k2 than the actual value, causing a completely wrong estimation of the cdf, as shown in Figure 1.
Although this counterexample (which can be easily adapted to be symmetric) is de nitely pathological, it suggests that a particular care should be taken when handling graphs that present narrow  tubes  connecting large connected components: in such scenarios, the function NG .t / exhibits relatively long plateaux (preceded and followed by sharp bumps) that may fool the computation of the cdf.
The effective diameter.
The  rst application of ANF was the computation of the effective diameter.
The effective diameter of G at   is the smallest t0 such that HG .t0/ (cid:21)  ; when   is omitted, it is assumed to be   D :9.9 The interpolated effective diameter is obtained in the same way on the linear interpolation of the points of the neighbourhood function.10 Since that the function OHG .t / is necessarily monotone in t (independently of the approximation error), from Theorem 2 we obtain: COROLLARY 2.
Assume ONG .t / is known for each t with error " and con dence 1 (cid:0)  , and there are points s and t such that OHG .s/ 1 (cid:0) 2"     OHG .t /

 : Then, with probability 1 (cid:0) 3  the effective diameter at   lies in  s : : t  .
Unfortunately, since the effective diameter depends sensitively on the distance cdf, again termination conditions can produce arbitrary errors.
Getting back to the example of Figure 2, with a suf ciently large k, for example k D 2`2 C 5` C 2, the effective diameter is ` C 1, which would be correctly output after ` C 1 iterations, whereas even stopping one step earlier (i.e., with t D `) would produce 1 as output, yielding an arbitrarily large error.
snap, indeed, fails to produce the correct result on this graph, because it stops iterating whenever the ratio between two successive iterates of NG is suf ciently close to 1.
Algorithm 3 Computing the effective diameter at   of a graph G; Algorithm 2 is used to compute ONG.
end; foreach t D 0; 1; : : : begin compute ONG .t / (error ", con dence 1 (cid:0)  ) if (some termination condition holds) break




 (cid:0) such that ONG .D (cid:0)  nd the largest D
 C such that ONG .D  nd the smallest D

   with con dence 1 (cid:0) 3  output  D
 end;

 (cid:0) /=M   .1 (cid:0) 2"/
 /=M (cid:21)  .1 C 2"/ Algorithm 3 is used to estimate the effective diameter of a graph; albeit this approach is reasonable (and actually it is similar to that
 the last step because of hash collisions in HyperLogLog counters, but this will happen with a controlled probability.
latter is de ned for all graphs whereas the former makes sense only in the strongly connected case.
diameter is an unbiased estimator of the interpolated effective diameter, whereas the same does not hold for the effective diameter.
adopted by snap, although the latter does not provide any con -dence interval), unless the neighbourhood function is known with very high precision it is almost impossible to obtain good upper bounds, because of the typical  atness of the distance cdf after the
 condition different from stabilisation should always be taken with a grain of salt because of the discussion above.
The distance density function.
The situation, from a theoretical viewpoint, is somehow even worse when we consider the density function hG .
(cid:0)/ associated with the cdf HG .(cid:0)/.
Controlling the error on hG .
(cid:0)/ is not easy: LEMMA 1.
Assume that, for a given t, OHG .t / is an estimator of HG .t / with error " and con dence 1(cid:0) .
Then O hG .t / D hG .t / 2" with con dence 1 (cid:0) 2 .
PROOF.
With con dence 1 (cid:0) 2 ,
 hG .t / D OHG .t / (cid:0) OHG .t (cid:0) 1/  .1 C "/HG .t / (cid:0) .1 (cid:0) "/HG .t (cid:0) 1/  hG .t / C 2"; and similarly O hG .t / (cid:21) hG .t / (cid:0) 2".
t t t t
 t hG .t / C 2"DG hG .t / X t hG .t / (cid:0) 2"DG X Note that the bound is very weak: since our best generic lower bound is hG .t / (cid:21) 1=n2, the relative error with which we known a point hG .t / is 2"n2 (which, of course, is pretty useless).
Moments.
Evaluation of the moments of hG .
(cid:0)/ poses further
 problems.
Actually, by Lemma 1 we can deduce that with con dence 1(cid:0) 2DG ", where DG is the diameter of G, which implies that the expected value of O hG .
(cid:0)/ is an (almost) unbiased estimator of the expected value of hG .(cid:0)/.
Nonetheless, the bounds we obtain are horrible (and actually unusable).
The situation for the variance is even worse, as we have to prove
 that we can use Var  hG   as an estimator to Var hG  .
Note that for a  xed graph G, HG is a precise distribution and Var hG   is an actual number.
Conversely, O
 hG  ) is a random OHG is an (almost) unbi-variable11.
By Theorem 2, we know that ased pointwise estimator for HG, and that we can control its concentration by suitably choosing the number m of counters.
We are going to derive bounds on the approximation of Var hG   using the OHG .t / up to ODG (i.e., the iteration at which HyperANF values of stabilises): OHG .t / is an LEMMA 2.
Assume that, for every 0  t  ODG,
 estimator of HG .t / with error " and con dence 1(cid:0) ; then, Var  hG   is an estimator of Var hG   with error hG (and hence Var  "  8"



 Var hG   Var hG   and con dence 1 (cid:0) .DG C 1/ .
OH in  0 : : DG   im-PROOF.
Assuming error " on the values of plies con dence 1 (cid:0) .DG C 1/ .
Since ODG  DG < 1, and by
 random variables O hG is a sequence of (stochastically dependent) hG .0/, O hG .1/, .
.
.
Var  t t t t t t 2 O
 hG .t / hG .t / D 0 for t > ODG we have (t ranges in  0 : : DG  ): hG   DX 2
 hG .t / (cid:0)(cid:16)X 2 t 2(cid:0)hG .t / C 2"(cid:1) (cid:0)(cid:16)X t hG .t / (cid:0) 2"

  Var hG   C 2" (cid:0)DG C E hG  (cid:1)  Var hG   C 4"D2
  Var hG   C 8"D3
 t 2 C 4"E hG  
 t t t t t where E hG   is the average path length.
Similarly (cid:0) 4"2D4

 hG   (cid:21) Var hG   (cid:0) 8"D3 Var 
 Hence the statement.
The error and con dence we obtain are again unusable, but the lemma proves that with enough precision and con dence on OHG .
(cid:0)/ we can get precision and con dence on Var hG  .
The results in this section suggests that if computations involve the moments the only realistic possibility is to resort to parametric statistics to study the behaviour of the value of interest on a large number of samples.
That is, it is better to compute a large number of relatively low-precision approximate neighbourhood functions than a small number of high-precision ones, as from the former the latter are easily computable by averaging, whereas it is impossible to obtain a large number of samples of derived values from the latter.
As we will see, this approach works surprisingly well.
The main purpose of computing aggregated data such as the distance distribution is that we can try to de ne indices that express some structural property of the graph we study, an obvious example being the average distance, or the effective diameter.
One of the main goal of our recent research has been  nding a simple property that clearly distinguishes between social networks deriving from human interaction (what is usually called a social network in the strong or proper sense: DBLP, Facebook, etc.)
and web-based graphs, which share several properties of social networks, and as the latter arise from human activity, but present a visibly different structure.
In this paper we propose for the  rst time to use the index of dispersion (cid:27) 2=(cid:22) (a.k.a.
variance-to-mean ratio) of the distance distribution as a measure of the  webbiness  of a social network.
We call such index the spid (shortest-paths index of dispersion)12 of G.
In particular, networks with a spid larger than one are to be considered  web-like , whereas networks with a spid smaller than one are to be considered  properly social .
We recall that a distribution is called under or over-dispersed depending on whether its index of dispersion is smaller or larger than 1, so a network is properly social or not depending on whether its distance distribution is under-or over-dispersed.
The intuition behind the spid is that  properly social  networks strongly favour short connections, whereas in the web long connection are not uncommon: this intuition will be con rmed in Section 6.
this would be the index of dispersion of the distance distribution, but we guessed that the acronym IDDD would not have been as as successful.
Figure 3: Cumulative density function of 100 values of the spid computed using HyperANF on cnr-2000.
For comparison, we also plot random samples of size 100 and 10 000 drawn from a normal distribution.
As discussed in the previous section, in theory estimating the spid is an impossible task, due to the inherent dif culty of evaluating the moments of hG .(cid:0)/.
In practice, however, the estimate of the spid computed directly on runs of HyperANF are quite precise.
From the actual neighbourhood function computed for cnr-2000 we deduce that the graph spid is 2:49.
We then ran 100 iteration of HyperANF with a relative standard deviation of 9:37%, computing for each of them an estimation of the spid; these values approximately follow a normal distribution of mean 2:489 and standard deviation 0:9 (see Figure 3).
We obtained analogous concentration results for the average distance.
In some pathological cases, the distribution is not Gaussian, albeit it always turns out to be uni-modal (in some cases, discarding few outliers), so we can apply the Vysochanski -Petunin inequality.
We will report some relevant observations on the spid of a number of graphs after describing our experiments.
We ran our experiments on the datasets described in Table 2: (cid:15) the web graphs are almost all available at http://law.dsi.
unimi.it/, except for the altavista-2002 dataset that was provided by Yahoo!
within the Webscope program (Al-taVista webpage connectivity dataset, version 1.0, http:// research.yahoo.com/Academic_Relations);13 (cid:15) for the social networks: hollywood-2009 (http://www.
imdb.com/) is a co-actorship graph where vertices represent actors; dblp-2010 (http://www.informatik.uni-trier.
de/~ley/db/) is a scienti c collaboration network where each vertex represents a scientist and two vertices are connected if they have worked together on an article; in ljournal-2008 (http://www.livejournal.com/) nodes are users and there is an arc from x to y if x registered y among his friends (it is not necessary to ask y permission, so the graph is directed); amazon-2008 (http://www.archive.
org/details/amazon_similarity_isbn/) describes similarity among books as reported by the Amazon store; en-ron is a partially anonymised corpus of email messages exchanged by some Enron employees (nodes represent people and there is an arc from x to y whenever y was the recipient of a message sent by x);  nally in flickr (http:
 literature, is not a good dataset.
The dangling nodes are 53:74%  an impossibly high value [11], and an almost sure indication that all nodes in the frontier of the crawler (and not only visited nodes) were added to the graph, and the giant component is less than 4% of the whole graph.
amazon-2008 indochina-2004 altavista-2002 Kronecker (177 K nodes, 2 B arcs) snap HyperANF 5 s 9.5 m 1.83 m 4.62 h -1.2 h HADI (90 machines) HyperANF 2.25 m 30 m Table 1: A comparison of the speed of snap/HADI vs. Hyper-ANF.
The tests on snap were performed on our hardware.
Both algorithms were stopped at a relative increment of 0:001.
The timings of HADI on the M45 cluster are the best reported in [7], and both algorithms ran three iterations.
We remark that a run of HyperANF on the Kronecker graph takes less than  fteen minutes on a laptop.
//www.flickr.com/14) vertices correspond to Flickr users and there is an edge connecting x and y whenever either vertex is recorded as a contact of the other one.
tion conditions different from stabilisation do not alter the computed values.
Figure 4: A plot showing the strong linear correlation between the average distance and the effective diameter.
At the best of our knowledge, this is the  rst paper where such a wide and diverse set of data is studied, and where features such as effective diameter or average path length are computed on very large graphs with precise statistical guarantees.
All experiments are performed on a Linux server equipped with Intel Xeon X5660 CPUs (2:80 GHz, 12 MB cache size) for overall

 A brief comparison with snap and HADI timings is shown in Table 1.
Essentially, on our hardware HyperANF is two orders of magnitudes faster than snap.
Our run on the Kronecker graph is one order of magnitude faster than HADI s (or three orders of magnitude faster, if you take into consideration the number of machines involved), but this comparison is unfair, as in principle HADI can scale to arbitrarily large graphs, whereas we are limited by the amount of memory available.
Nonetheless, the speedup is clearly a breakthrough in the analysis of large graphs.
It would be interesting to compare our timings for the altavista-2002 dataset with HADI s, but none have been published.
It is this speed that makes it possible, for the  rst time, to compute data associated with the distance distribution with high precision and for a large number of graphs.
We have 100 runs with relative standard deviation of 9:37% for all graphs, except for those on the altavista-2002 dataset (13:25%).
All graphs are run to stabilisation.
Our computations are necessarily much longer (usually, an order of magnitude longer in iterations) than those used to compute the effective diameter or similar measures.
This is due to the necessity of computing with high precision second-order statistics that are used to compute the spid.
Previous publications used few graphs, mainly because of the large computational effort that was necessary, and no data was available about the number of runs.
Moreover, we give precise con dence intervals based on parametric statistics for data depending on the second moment, such as the spid something that has never done before.
We gather here our  ndings.
A posteriori parameters are highly concentrated.
According to our experiments, computing the effective diameter, average distance and spid on a large number of low-precision runs generates highly concentrated distributions (see the empirical standard deviation in Table 2).
Thus, we suggest this approach for computing such values, provided that there is enough evidence that termina-
Effective diameter and average distance are essentially linearly correlated.
Figure 4 shows a scatter plot of the two values, and the line 2x=3 C 1.
The correlation between the two values has always been folklore in the study of social networks, but we can con rm that on both social and web networks the connection can be exactly expressed in linear terms (it would be of course interesting to prove such a correlation formally, under suitable restrictions on the structure of the graph).
This fact suggests that the average distance (which is more principled from a statistic viewpoint, and parameter-free) should be used as the reference parameter to express the closeness between nodes.
Moreover, experimentally the standard deviation of the effective diameter in a posteriori computations is usually signi cantly larger than that of the average distance.
Incidentally, the average distance of the altavista-2002 dataset is 16:5 slightly more than what reported in [7] (possibly because of termination conditions artifacts).
It is dif cult to give a priori con dence intervals for the effective diameter with a small number of runs.
Unless a large number of runs is available, so that the precision of the approximation of the neighbourhood function can be signi cantly lowered, it is impossible to provide interesting upper bounds for the effective diameter.
The spid can tell social networks from web graphs.
As shown in Table 2, even taking the standard deviation into account spids are pretty much below 1 for social networks and above 1 for web graphs; host graphs (not surprisingly) behave like social networks.
Note that this works both for directed and undirected graphs.
Figure 5 shows the spid values obtained for our datasets plotted against the graph size, and also witnesses that there is no correlation (a similar graph, not shown here, testi es that spid is also independent from density).
Figure 6 shows that there is some slight correlation between the spid and the average distance: nonetheless, there is no way to tell networks from our dataset apart using the latter value, whereas the under or over-dispersion of the distance distribution, as de ned by the spid, never makes a mistake.
Of course, we expect to enrich this graph in time with more datasets: we are particularly interested in gathering very large social networks to test the spid at large sizes.
As a sanity check, we have also computed on several datasets the spid of the giant component (see Table 3), which turns out to be very similar to the spid of the whole graph.
We see this as a
 amazon-2008 dblp-2010 enron ljournal-2008 flickr hollywood-2009 indochina-2004-hosts uk-2005-hosts cnr-2000 eu-2005 in-2004 indochina-2004 uk@10E6 uk@10E7 it-2004 uk-2007-05 altavista-2002 Type social (u) social (u) social (d) social (d) social (u) social (u) host (d) host (d) web (d) web (d) web (d) web (d) web (d) web (d) web (d) web (d) web (d) Nodes
















 Arcs
















 spid . (cid:27) /
















 ad . (cid:27) /
















 ied . (cid:27) /
















 ed
















 Table 2: Our main data table.
 Type  describes whether the given graph is a web-graph, a proper social network, or the host quotient of a web graph (u=undirected, d=directed).
The graphs uk@10E6 and uk@10E7 are obtained by visiting in a breadth rst fashion uk-2007-05 starting from a random node.
They simulate smaller crawls of a larger network.
We show spid, average distance and interpolated effective diameter a posteriori with their empirical standard deviation, and intervals for the effective diameter with 85% con dence for a comparison.
Type Name social (u) amazon-2008 social (u) dblp-2010 social (d) enron social (u) ljournal-2008 social (u) hollywood-2009 web (d) cnr-2000 web (d) eu-2005 web (d) in-2004 indochina-2004 web (d) web (d) it-2004 web (d) uk-2007-05 Nodes (%)










 Arcs










 spid . (cid:27) /










 ad . (cid:27) /










 ied . (cid:27) /










 ed










 Table 3: A table parallel to Table 2, but containing data for giant connected components.
Items not appearing here are either too small, or have too small a giant component for the results to be signi cant.
clear sign that the spid is largely independent of the artifacts of the crawling process.
Direction should not be destroyed when analysing a graph.
We con rm that symmetrising graphs destroys the combinatorial structure of the network: the average distance drops to very low values in all cases, as well as the spid.
This suggests that there is important structural information that is being ignored.
We also note that since all web snapshot we have at hand are gathered by some kind of breadth rst visit, they represent balls of small diameter centred at the seed: symmetrising the graph we cannot expect to get an average distance that is larger than twice the radius of the ball.
All in all, the only advantage of symmetrising a graph is a signi cant reduction in the number of iterations that are needed to complete a computation of the neighbourhood function.15 To give a more direct idea of the level of precision of our diameter estimation, we computed the actual diameter at   for the cnr-2000 dataset, and plotted it against the interval estimation obtained by HyperANF that the correct parallel framework for implementing a diffusing computation is a synchronous parallel system where computation happens at nodes and communication is sent from node to node with messages.
Such a framework, Pregel, has been recently developed at Google [9].
In a Pregel implementation of HyperANF, every computational node sends its own counter as message to its predecessors if it changed from the previous iteration, waits for incoming messages from its successors, and computes the maximisation procedure on the received messages.
Due to the small size of Hy-perLogLog counter (exponentially smaller than the Flajolet Martin counters used by ANF), the amount of communication would be very small.
Although in this paper, we preferred to focus on the computation of the spid, we remark that HyperANF can also be used to build the radius distribution described in [7], or the related closeness centrality.
HyperANF lends itself naturally to distributed implementations.
However, contrarily to the approach taken by HADI [7], we think
 altavista-2002 dataset refers to the effective diameter for the symmetrised version of the graph.
HyperANF is a breakthrough improvement over the original ANF techniques, mainly because of the usage of the more powerful Hy-perLogLog counters combined with their fast broadword combination and systolic computation.
HyperANF can run to stabilisation very large graphs, computing data with statistical guarantees.
We consider, however, the introduction of the spid of a graph the main conceptual contribution of this paper.
HyperLogLog is datasets compared with their size (i.e., number of nodes, horizontal): red squares correspond to social networks, blue diamonds to web graphs and black circles to host graphs.
Figure 7: Effective diameters at   for the cnr-2000 dataset; red bullets show the real effective diameter, whereas green crosses show the upper and lower extreme of the con dence interval obtained running 100 HyperANF with m D 128.
Lecture Notes in Computer Science, pages 605 617.
Springer, 2003.
[5] Philippe Flajolet,  ric Fusy, Olivier Gandouet, and Fr d ric Meunier.
Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm.
In Proceedings of the 13th conference on analysis of algorithm (AofA 07), pages
 [6] J.
A. Hartigan and P. M. Hartigan.
The dip test of unimodality.
Ann.
Statist., 13(1):70 84, 1985.
[7] U Kang, Charalampos E. Tsourakakis, Ana Paula Appel, Christos Faloutsos, , and Jure Leskovec.
HADI: Mining radii of large graphs.
ACM Transactions on Knowledge Discovery from Data, 2010.
[8] Donald E. Knuth.
The Art of Computer Programming.
Pre-Fascicle 1A.
Draft of Section 7.1.3: Bitwise Tricks and Techniques, 2007.
[9] Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski.
Pregel: a system for large-scale graph processing.
In SIGMOD  10: Proceedings of the 2010 international conference on Management of data, pages
 [10] Christopher R. Palmer, Phillip B. Gibbons, and Christos Faloutsos.
Anf: a fast and scalable tool for data mining in massive graphs.
In KDD  02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 81 90, New York, NY, USA, 2002.
[11] Sebastiano Vigna.
Stanford matrix considered harmful.
In Andreas Frommer, Michael W. Mahoney, and Daniel B.
Szyld, editors, Web Information Retrieval and Linear Algebra Algorithms, number 07071 in Dagstuhl Seminar Proceedings, 2007. http://arxiv.org/abs/0710.1962.
[12] Sebastiano Vigna.
Broadword implementation of rank/select
 Mat.
Statist.
21 (1979), 23 35].
Teor.
Veroyatnost.
i Mat.
Statist., 27:26 27, 157, 1982.
queries.
In Catherine C. McGeoch, editor, Experimental Algorithms.
7th International Workshop, WEA 2008, number
 Springer Verlag, 2008.
[13] D. F. Vysochanski  and Yu.
 I.
Petun n.
Remark:  Proof of the Figure 6: A plot showing the spid against the average distance using the same conventions of Figure 5.
instrumental in making the computation of the spid possible, as the latter requires a number of iterations that is an order of magnitude larger than those required for an estimate of the effective diameter.
Flavio Chierichetti participated to the earlier phases of this work.
We want to thank Dario Malchiodi for fruitful discussions and hints.
