Online social networks have many bene ts as a medium for fast, widespread information dissemination.
They provide fast access to large scale news data, sometimes even before the mass media Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
as in the case of the announcement of death of Michael Jackson [34].
They also serve as a medium to collectively achieve a social goal.
For instance with the use of group and event pages in Facebook, events such as  Day of Action  protests reached thousands of protestors [16].
While the ease of information propagation in social networks can be very bene cial, it can also have disruptive effects.
One such example was observed during the recent shootings at Fort Hood, Texas, when a soldier inside the base sent out messages via Twitter as the event unfolded.
Her incorrect reports of multiple shooters and shooting locations quickly spread through the social network and even to the mass media where it was reported on television broadcasts [22].
Another example is the spread of misinformation on swine  u in Twitter [35].
The spread of misinformation in this case reached a very large scale causing panic in the population.
Although social networks are the main source of news for many people today, they are not considered reliable due to such problems.
Clearly, in order for social networks to serve as a reliable platform for disseminating critical information, it is necessary to have tools to limit the effect of misinformation.
In the presence of a misinformation cascade, we aim to  nd a near-optimal way of disseminating  good information  that will minimize the devastating effects of a misinformation campaign.
For instance in the case of [35, 22], we seek ways of making sure that most of the users of the social network hear about the correct information before the bad one, making social networks a more  trustworthy  or  reliable  source of information.
In addition to the implication our work has in limiting the effect of misinformation, our methods can be applied to any two simultaneously spreading competing campaigns.
In this work, we study the problem of minimizing the number of people that adopt the misinformation and prove that even though the general problem does not exhibit the submodular property, certain restricted versions of it are in fact submodular.
We exploit this property to provide ef cient solutions with approximation bounds.
We also evaluate the performance of our algorithm on a number of close-knit regional networks obtained from the Facebook social network comparing its performance with some well-known heuristics including degree centrality.
We show that in many cases, heuristics have performance comparable to the more computationally intense greedy method.
Since in the real world, decisions about how to deploy a limiting campaign need to be made with incomplete data, we also consider the case where the states of only a fraction of the nodes in network can be observed.
We show that, although the naive solution to the optimization problem in this setting is intractable, using matrix tree theorem [27] and the fact that the speci c problem is supermodular [39], a polynomial time solution can be used where polynomial time is de ned in terms of calls to an oracle function.
However, this solution is still expensive that is based on generating random spanning trees on a set of likely to have been infected nodes to predict the missing information.
We show that in most cases, this method has good performance, i.e. decisions made as to who to  rst in uence by the limiting campaign under uncertain data still result in effective inoculation.
We start with a brief overview of information propagation in social networks in Section 2.
In Section 3, we introduce our model of communication and formalize the in uence limitation problem.
In Section 4 we prove NP-hardness and submodularity of in uence limitation.
Submodularity guarantees approximation bounds for a greedy algorithm presented in Section 4.3.
In Section 5, we provide the experiments that compare the performance of the greedy solution with various heuristics.
In Section 6, we explore the problem in the presence of missing information and propose an algorithm for predicting the missing data and limiting the spread of misinformation in that setting.
Finally, Section 7 concludes the paper.
The identi cation of in uential users in a social network is a problem that has received signi cant attention in recent research.
For the in uence maximization problem, given a probabilistic model of information diffusion such as the Independent Cascade Model, a network graph, and a budget k, the objective is to select a set A of size k for initial activation so that the expected value of f (A) (size of cascade created by selecting set A) is maximized [12, 37].
Early works relied on heuristics such as node degree and distance centrality [42] to select the set A.
Although the problem of  nding an optimal solution in this model is NP-hard, there is a greedy algorithm that yields a spread that is within 1   1/e of optimal [24].
This solution depends on Monte Carlo simulations which are computationally expensive.
Work has been done to improve the performance of this greedy algorithm [8, 31, 26, 9], but scalability remains a signi cant challenge.
In addition to the scale issues, these de nitions of in uential users ignore certain aspects of real social networks such as the existence of competing campaigns.
In this work we consider different models of communication that incorporate different aspects of real social networks.
Similar to [24, 31], we identify a problem that involves detecting  in uential nodes  and study the feasibility of a solution to this problem.
However, our problem formulation is more general since we model the existence of competing cascades dissipating in a network.
The existence of competing campaigns has been captured by a number of studies recently.
Dubey et al. [13] study the problem as a network game focusing on quasi-linear model and consider various cost, bene t and externality functions for competing  rms.
They study the existence of Nash Equilibrium (NE) and show that it is unique if there is enough competition between  rms or if their valuations of clients are anonymous.
Bharathi et al. [4] augment the Independent Cascade Model to capture the existence of competing campaigns in a network.
Their diffusion model is similar to ours and captures the timing issues that are crucial in competing campaigns optimization problems.
The algorithmic problem de- ned in [4] is: when there is more than one campaign dissipating in a network and each campaign can select a set of early adopters so as to maximize their bene t, what is the best strategy for the players?
This work studies the problem from both the  rst and last player s perspectives and shows that the problem of selecting the early adopters for the last player is submodular.
They also introduce a fully polynomial time approximation scheme for the  rst player when the network structure is a tree.
Carnes et al. [7] consider the same problem from the last player s perspective and use a diffusion model where nodes of the network choose the campaign to adopt w.r.t.
their distance to the early adopters of the campaigns and another model where the nodes make a uniform random choice among its active neighbors.
The experimental results show that the greedy approach performs better than the heuristics.
They also experimentally show that the best strategy for the  rst player is to choose high degree nodes.
Kostka et al. [29] study competing campaigns as a game theoretical problem and show that being the  rst player, i.e.
the  rst to decide, is not always advantageous.
Both [7, 4] use diffusion models where the two campaigns propagate exactly the same way, i.e.
the probability of diffusion on a certain edge is the same for both campaigns and both start at the same time.
In our work, we study the case where the competing campaigns have different acceptance rates and one is in response to the other, and therefore the campaign of the last player is started with a delay.
Also, different from previous work we address the problem of in uence limitation as opposed to maximization.
The problem of limiting the effect of misinformation in a social network can be seen as similar to the problem of epidemics and inoculation.
There are many studies on the spread of infections and immunization [41, 3, 25].
A recent work on identifying in uen-tial people in a social network [28] uses SIS (susceptible-infected-susceptible), SIR (susceptible-infected-recovered) models [2, 11, 21] and concludes that the in uence of a node is more dependent on its location in the network than the number of connections it has.
This work captures the notion of being  immunized  but the immunization is limited to the node that is inoculated by external means.
Conversely, we consider the case where once a node is inoculated, it can inoculate more people (by virally spreading the  good  information).
Inoculation has also been studied in the game theory literature.
Meier et al. [33] study inoculation games in social networks.
The problem is posed in terms of virus propagation where the owner of each node decides whether or not to protect itself.
Here inoculation has a direct effect only on the inoculated node, meaning that the  good  information does not propagate.
The decision to  protect  oneself is a distributed process, each node decides for itself and aims to maximize its own function whereas we consider the problem of  nding the best solution for the community.
A social network can be modeled as a directed graph G = (N, E) consisting of nodes N and edges E. In the context of in uence spread, N can be viewed as the users of the social network.
A node w is a neighbor of a node v if and only if there is ev,w   E, an edge from v to w in G. In addition to this, pv,w is assigned to each edge ev,w which is used to model the direct in uence v has on w.
Independent cascade model (ICM) is one of the most basic and well-studied diffusion models that has been used in different contexts [14, 32, 17, 19].
In the ICM, a process starts with an initial set of active nodes A0, and unfolds in discrete steps.
When node v  rst becomes active in step t, it has a single chance to activate each currently inactive neighbor w; it succeeds with probability pv,w.
If v succeeds, then w will become active in step t + 1; but whether or not v succeeds, it cannot make any further attempts in subsequent rounds.
The process runs until no more activations are possible.
If w has incoming edges from multiple newly activated nodes, their attempts are sequenced in an arbitrary order.
We now introduce the Multi-Campaign Independent Cascade Model (MCICM) which models the diffusion of two cascades evolving simultaneously in a network.
Let C (for  campaign ) and L (for  limiting campaign ) denote the two cascades.
The initial set of active nodes for cascade L (C) is denoted by AL (AC).
When a node chance to activate each currently inactive neighbor w in campaign L (or C) and it succeeds with probability pL,v,w (or pC,v,w) given that no neighbor of w tries activating w in the competing campaign at the same step.
We also refer to pL,v,w (or pC,v,w) as the probability of the edge ev,w being live.
If there are two or more nodes trying to activate w at a given time step, at most one of them can succeed.
In independent cascade, when w has several newly activated neighbors, their attempts are sequenced in arbitrary order.
However in our studies, we will assume that there is a natural order to the two campaigns, more speci cally one is  good  while the other is the  bad  campaign and if the  bad information  and the  good information  reach a node w at the same step,  good information  takes effect.
Once a node becomes active in one campaign, it never becomes inactive or changes campaigns and the process continues until there is no newly activated node in either campaign.
We also consider another model of diffusion in which the probabilities of each edge being live is independent of the campaign.
In this setting we only associate one probability pv,w with each edge ev,w.
No matter which information reaches a node v, v forwards this information to its neighbor w with probability pv,w.
Although this model is not a perfect  t for the inoculation of misinformation, it is a good  t for modeling competing campaigns where the two information cascades are more likely to be of similar  quality  and the nodes would agree to the campaign that reaches out to them  rst.
Consider for example two news articles L and C about the same event spreading in a social network.
The probability of a user forwarding article L and C is more dependent on the news itself rather than which agency the news is from.
Similar to the Multi-Campaign Independent Cascade model, there are three states a node can be in; inactive, in campaign L, in campaign C and once a node becomes active in either L or C, it cannot change its state.
As before, we assume that in the case of simultaneous trials of activation at a node, campaign L is ordered before C. We call this model Campaign-Oblivious Independent Cascade (COICM).
COICM is similar to the diffusion model used in [4].
However we assume that one of the campaigns is prioritized over the other in the case of simultaneous activation trials whereas independent and exponentially distributed continuous random variables are assigned to each edge as delay in [4] to ensure there are no simultaneous activation trials.
The earlier studies using similar diffusion models support the validity of MCICM and COICM.
However whether such models re ect the real in uence spread in social networks is still an open problem.
In future work, we plan to investigate this problem by studying the behavior in real social networks.
While a substantial amount of research has been done in the context of in uence maximization, a problem that has not received much attention is limiting the in uence of a misinformation campaign.
One strategy to deal with a misinformation campaign is to limit the number of users who are willing to accept and spread it.
We will assume the Multi-Campaign Independent Cascade Model described in Section 3.1 as the model of communication.
W.l.o.g.
we will assume that the spread of in uence for campaign C starts from one node na and is detected with delay r and at that point campaign L is initiated.
However the algorithms can be easily extended to the case where C starts from a set of nodes and the proofs of NP-hardness and submodularity still hold for this case.
Depending on the context that the in uence limitation problem is introduced in, we need to consider different objective functions.
The objective can be to try and  save  as many nodes as possible, to limit the lifespan of the  bad  information campaign or to maximize the effect of the  good  campaign in the presence of the  bad  campaign.
In this paper, we will focus on minimizing the number of nodes that end up adopting campaign C when the information cascades from both campaigns are over.
We refer to this problem as the eventual in uence limitation problem (EIL).
Given a network and the Multi-Campaign Independent Cascade Model de ned in Section 3.1, suppose that a campaign C spreading bad information is detected with delay r. Given budget k, select AL as seeds for initial activation with the limiting campaign L such that the expected number of nodes that adopt campaign C,  (AC ) is minimized.
Let IF (AC ) denote the in uence set of C in the absence of L, i.e the set of nodes that would accept campaign C if there were no limiting campaign.
We de ne the function  (AL) to be the size of the subset of IF (AC ) that campaign L prevents from adopting campaign C. Then, the in uence limitation problem is equivalent to selecting AL such that the expectation of  (AL) is maximized.
Note that we are not necessarily interested in the number of inoculated nodes but the inoculated nodes that would be infected otherwise.
We will refer to this set of nodes as saved.
We now outline a solution to a simpli ed version of this problem where there is only a single source of information for C, meaning |AC | = 1.
We refer to this node as the adversary node or na As it may be much easier to convince a user of the truth than a falsehood, we also assume that the limiting campaign information is accepted by users with probability 1 (pL,v,w = 1 if there is an edge from v to w and pL,v,w = 0 otherwise).
We refer to this notion as high-effectiveness property.
Even with these restrictions, EIL is NP-hard and therefore  nding the optimal solution is expensive.
However as we will establish in Section 4.2, the problem is submodular which guarantees that we can provide approximation bounds with a simple hill climbing approach.
Later we will investigate a more general form of this problem where we allow arbitrary values for pL,v,w and show that this problem is no longer submodular.
THEOREM 4.1.
EIL is NP-hard even with the high effectiveness property.
PROOF.
Consider an instance of the NP-complete Set Cover problem, de ned by a collection of subsets S1, S2, ..., Sm for a universe set U = {u1, u2, ..., un}; we wish to know whether there exist k of the subsets whose union is equal to U.
We show that this can be viewed as a special case of EIL.
Given an arbitrary instance of the Set Cover problem, we de ne a corresponding directed bipartite graph with n + m + 1 nodes: there is a node i corresponding to each set Si, a node j corresponding to each element uj, and a directed edge (i, j) whenever uj   Si.
In addition, there is an adversary node a and a directed edge (a, j) for all uj with activation probability pa,j = 1.
The Set Cover problem is equivalent to deciding if there is a set AL of k nodes in this graph with  (AL)   n+k when we become aware of campaign C at time step 0 (when a itself is active in campaign C but has not contacted any of its neighbors yet).
Note that for the instance we have de ned, activation is a deterministic process, as all probabilities for adversary to infect its neighbors are 0 or 1.
Initially activating the k nodes corresponding to sets in a Set Cover solution results in saving all n nodes corresponding to the ground set U, and if any set AL of k nodes has  (AL)   n + k, then the Set Cover problem must be solvable.
A function f (.)
is said to be submodular or have  diminishing returns  if it satis es the following property: f (S   v)   f (S)   i.e. the marginal gain from adding an element to a set S is at least as high as the marginal gain from adding the same element to a su-perset of S. As proved by Nemhauser, Wolsey, and Fisher [10, 36], for submodular and monotone functions, the greedy hill-climbing algorithm of starting with the empty set, and repeatedly adding an element that gives the maximum marginal gain approximates the optimum solution within a factor of (1   1/e).
Here we will prove that the in uence limitation problem is submodular when the limiting campaign has the high effectiveness property.
We omit the proofs of monotonicity of EIL due to space limitations but to give an intuition for monotonicity we note that having more nodes to initially activate in campaign L can never have a negative effect under the models of diffusion we study.
Since in uence spread over G is a stochastic process, the in u-ence function for a set of nodes is tricky to de ne.
Following the same approach presented in [24], we view an event of a newly activated node v attempting to activate its neighbor w and succeeding with pC,v,w as  ipping a coin with bias pC,v,w.
It does not matter whether the coin is  ipped at the moment when v tries to activate w, or if it was pre- ipped and stored to be examined at the time when v tries to activate w. So while considering a speci c instance of in uence spread, we can pre- ip all the coins to determine which edges of the graph G are live (meaning if the start node of this edge were to be activated, it would succeed in activating its neighbor) or blocked (meaning the attempt would be unsuccessful).
In this setting, the spread of  bad campaign  C can be modeled as graph G  = (N  , E ).
where N   represents the set of nodes that are reachable from adversary node na via live edges and E  represents the set of live edges amongst the nodes in N  .
Consider the graph of 10 nodes represented in Figure 1(a).
Assume that by pre- ipping the coins, we end up with probabilities such that the solid lines are live edges and dotted lines are blocked edges.
In this case a campaign starting from adversary node 0 would reach nodes 0, 1, 2, 3 if there was no limiting campaign.
A  rst look at this graph (or the general EIL in general) suggests that in order to save node 3, we need to make sure both 1 and 2 should be saved (or 3 should be saved directly).
Super cially, it would seem that submodularity is no longer viable.
Since saving only 1 or 2 would not be suf cient to save 3, but their combination would.
However, a closer look at this problem reveals that we do not need to secure all the possible paths to a node from an adversary but just the shortest path.
If L can reach 3 before C, 3 can never be infected.
For instance, for the campaign in Figure 1(a), if campaign L reaches node 1 by r = 1, it will be saved.
In this case the good campaign will reach node 3 at r = 2 and even if node 2 is not saved, that still guarantees that node 3 will be saved.
Next we provide the formal proof of submodularity for EIL.
Note that the proof depends on the high-effectiveness property of the good campaign.
Later on, we will show that when this property does not hold, EIL is not, in general, a submodular function.
CLAIM 1.
In MCICM with the high effectiveness property a node w can be saved if and only if   v such that v   AL and |SPG(v, w)| + r   |SPG  (na, w)| where SPG(v, w) denotes a shortest path from node v to w in graph G.
PROOF.
1.
If   v such that v   AL and |SPG(v, w)| + r   |SPG  (na, w)|, then w is saved: Assume that such a v exists but w could not be saved.
This is only possible if the bad campaign C reaches w strictly before L since otherwise w would be saved at ts = |SPG(v, w)| + r. So there must exist a path PG  (na, w) from na to w such that |PG  (na, w)| < |SPG(v, w)| + r. Since |SPG(v, w)|+r   |SPG  (na, w)|, |PG  (na, w)| < |SPG  (na, w)|.
This means there is a shorter path from na to w in G  than the shortest path which is a contradiction.
w cannot be saved: Assume contrary, i.e.   v s.t.
SPG(v, w)   SPG  (na, w) and w is saved.
If w is saved, at least one of the nodes in one of those shortest paths must have been activated in L since otherwise C would propagate on one of those paths to w and infect it.
W.l.o.g.
let a shortest path from na to w consist of nodes na, n1, n2..., ni, w and nj   SPG  (na, w) be a node activated in L, nj can only be activated in L if L reaches nj at ts   j because SPG  (na, j) = na, n1, n2, ..., nj 1, nj.
Therefore   v   AL s.t.
|SPG(v, nj)| + r   |SPG  (na, nj)|.
Since |SPG(nj, w)|   |SPG  (nj, w)|, |SPGv, w| + r  |SPG(v, nj)| + SPG(nj, w)| + r   |SPG  (na, nj)| + |SPG  (nj, w)|   |SPG  (na, w)|.
This contradicts with the initial assumption that   v s.t.
|SPG(v, w)| + r   |SPG  (na, w)|.
THEOREM 4.2.
EIL is submodular when the limiting campaign L has high-effectiveness property PROOF.
Consider the inoculation graph G  = (N  , E ) s.t.
N   = {u|u   N   u /  I} and E  = {(u, v)|v   Su} where Su = {v|v   N     |SPG(u, v)| + r   |SPG  (na, v)}| and I is the set of nodes that are infected by time step r. Based on Claim
 maximizing the number of nodes reachable from the set AL in G  and as established in [24], this is submodular.
Unfortunately, the general EIL problem where L does not have the high-effectiveness property is not in general submodular.
Consider the graphs in Figure 1.
Assume an instance of EIL where G  representing the spread of in uence for the good campaign L consists of nodes 1,2,5,6 and the edges e5,1, e6,2.
In this case, f (5) = 1, f (6) = 2, f (5, 6) = 1, 2, 3 since by using 5(6) as a seed, L can save node 1(2).
(Since e1,3(e2,3) is not a live edge for L, the good campaign will never reach node 3, and 3 will be infected by node 2(1) at the next time step.)
On the other hand if AL = {5, 6} both 2,3 will be saved and since these are the only two nodes that could infect 1, node 1 will also be saved.
This example shows that EIL without the high-effectiveness property is not in general submodular.
Finally, consider the Campaign-Oblivious Independent Cascade introduced in Section 3.1 where the probabilities on the edges are campaign-independent.
In this case we associate only one probability pv,w with each edge ev,w.
This model  ts competing campaigns where the two campaigns are trying to get users to adopt very similar products or ideas.
In this case users are as likely to adopt campaign L as they would adopt campaign C. Note that, this model does not rely on either one of the campaigns being good or bad and therefore can be applied to any two competing campaigns.
CLAIM 2.
EIL is submodular for Campaign-Oblivious Independent Cascade Model.
PROOF.
Since a node can only be activated in one campaign, an edge ev,w will only be visited at most once.
Therefore, using the same idea presented in 4.2, we can pre- ip all the coins to determine which edges are live or blocked for an instance of in u-ence dissemination from campaigns C and L. Consider the graph Glive = (N, E ) where E  is the set of live edges in E. Both L and C can be modeled as propagating on this graph.
Let N   denote the nodes that are reachable from adversary na via live edges.
In this case, in uence limitation problem is equivalent to maximizing the number of nodes reachable from AL in G  where G  = (N  , E ) s.t.
N   = {u|u   N   u /  I} and E  = {(u, v)|v   Su} where and I is the set of nodes that are infected by time step r. Since reachability problem is submodular [24], so is EIL on COICM.
(a) Spread of C.
Solid lines are live and dotted lines are blocked edges.
The adversary node 0, without an opposing campaign, reaches 1,2 and 3 (b) The shortest path structure for spread of in u-ence for the bad campaign Figure 1: General In uence Spread
 Since EIL for M CICM where the limiting campaign has high effectiveness property or for COICM in general are submodular and monotone, the hill climbing approach provides a (1   1/e) approximation [10, 36] for these problems.
Figure 2 provides this greedy algorithm that yields an AL for which  (AL) is within
 set of adversaries Sa, delay r and budget k, i.e. number of nodes to initially activate in campaign L. According to our problem de ni-tion Sa consists of only one node na, however the proofs can easily be generalized to hold for multiple initial adversaries.
Since independent cascade is a stochastic process, computing   for a given set of nodes requires running a large number of simulations (#sim) as demonstrated in steps (6,7).
The procedure InfLimit (G,Sa,r,S,v) decides liveness of edges in G based on the probability associated with that edge and simulates the in uence limitation given that the set of adversaries that C starts from is Sa, the adversary campaign is caught with delay r, the nodes we have already chosen to initially activate in campaign L is S and the node that we are evaluating the in uence of is v. This method returns the marginal gain of v i.e.
number of people v could save but the set S could not, where the nodes that could be saved by v are the nodes that have a shorter path from v than of any node in set Sa.
Considering the large scale of social networks today and the complexity of the EIL, even the greedy approach that is a polynomial time algorithm is too costly to be used in real social networks.
Therefore, we seek alternatives that can potentially compare well with the greedy approach which, as we have proved, is guaranteed to be a good approximation.
We consider three different heuristics.
The  rst heuristic we consider is the degree centrality which has been used in early work to target  in uential people  [42].
versaries and r is the delay and k is the number of seeds} for each vertex v   N   AL do
 3: for i = 1 to k do






 sv = 0 for j = 1 to R do sv+ = Inf Limit(G, Sa, r, AL, v) sv = sv/R Figure 2: Greedy algorithm to select the set for initial activation The second heuristic we consider is called early infectees and entails choosing seeds that are expected to be infected at time step r. This is equivalent to reaching out to nodes that would be infected early on but after L is started, since those nodes are likely to create a large cascade for campaign C. In order to calculate this heuristic, we run #sim simulations of infection spread from Sa and select nodes AL in decreasing order, where the nodes are ordered w.r.t.
the number of simulations they were infected at time step r.
The third heuristic is largest infectees.
This heuristic is very similar to the early infectees but rather than choosing the nodes that are expected to be infected early on, it chooses seeds that are expected to infect the highest number of nodes if they were to be infected themselves.
In this case we only consider nodes that would be infected after time step r. In order to calculate this heuristic, we run #sim simulations of infection spread from Sa and at each simulation we increase the value vali of a node ni that is infected after time step r by the number of nodes nj s.t.
ni is on the shortest path from an adversary in Sa to nj.
We select nodes AL in decreasing order of vali.
Note that both early infectees and largest infectees are computationally more intensive to compute than degree centrality.
However they are still far less expensive than the greedy method that involves shortest path computations.
Though not directly applicable due to different natures of the problems, the large body of research in in uence maximization [8, 26] can be leveraged from to obtain a larger pool of heuristics for EIL.
We leave a more extensive evaluation including such heuristics as future work.
Here we evaluate the performance of the greedy algorithm w.r.t.
the three heuristics.
Note that since in uence propagation is a stochastic process, in order to evaluate value of each seed set with an error   with high probability, we need to perform Monte Carlo simulations polynomial in 1/  and the number of nodes of the network [23].
This is one of the major scalability issues inherent in this type of problem.
However, in our speci c problem each simulation involves the expensive computation of shortest paths which is crucial to EIL and this makes EIL even more computationally intense then the in uence maximization problems [24, 31].
We ran experiments choosing the adversary uniformly randomly.
As part of our experiments, we also evaluated how factors like the degree centrality of the adversary, delay of campaign L, and the weight distribution for pC,v,w and pL,v,w in uence our choice of best  t algorithm.
This requires performing the computationally expensive simulations for each choice of such parameters.
Taking these factors into consideration we performed experiments on 4 regional network graphs obtained from Facebook that exhibit properties such as power-law degree distribution, high clustering and positive assortativity [43].
The data sets are as follows: 2009 snapshot of Santa Barbara regional network with 26455 nodes and 453132 edges (bidirectional edges count as two edges); 2008 snapshot of the same network with 12814 nodes and 184482 edges; 2009 snapshot of the Monterey Bay regional network with 14144 nodes and 186582 edges; and 2008 snapshot of the same network with 6117 nodes and 62750 edges.
In Figure 3 we present our evaluation of the 4 methods on MCICM when L has the high effectiveness property and pC,v,w values of
 the percentage of the population that was saved.
The x-axis represents the number of nodes that are initially activated in L. Figure 3(a) demonstrates the case where delay = 20% i.e. the ratio of the delay of the algorithm L to the duration of the campaign C is
 portion of the population.
Figure 3(b) shows the rapid decay of the is started a large portion of the population is already infected.
Next, we evaluate MCICM where L does not have the high effectiveness property.
In this case, the greedy algorithm is too costly to perform since many of the optimizations we performed for the earlier two cases cannot be applied.
Considering the results obtained from the earlier two sets of experiments, we conclude that, at least for close-knit social networks, the heuristics introduced above result in a good performance.
Therefore we evaluated how well they perform on a slightly larger social network to see if there was consistency in their behavior.
Figure 5 presents the test results for the Santa Barbara 2009 data set.
Figure 5(a) presents the case where the limiting campaign is started early (delay is 20%) but campaign C is more dominant (0   pC,v,w   0.5 for all edges) than L (0   pL,v,w   0.1 for all edges) in the sense that nodes are more likely to adopt C than L. Figure 5(b) presents the opposite case where L is more dominant than C. In both cases, the degree centrality and largest infectees heuristics have similar behavior while early infectees performs worse than both.
The savings are much larger for Figure 5(b) compared to Figure 5(a).
We also note the similarity of Figure 5(b) with Figure 3(a), and claim that even if campaign L does not have the high effectiveness property, if it is more dominant than C, it is still likely to save a large population.
(a) delay = 20% (b) delay = 50% Figure 3: Evaluation on SB08 for MCICM with high effectiveness property performance of all the approaches in the case where delay is 50%.
Here we omit the case where delay is 70% since all of the algorithms were doing poorly, especially degree centrality had near-0 value.
We conducted the same experiments on the other data sets for which the result were similar.
We also conducted experiments setting pC,v,w values to 0.5 instead of 0.1.
Though the percentage saved is smaller for all algorithms, the trend w.r.t.
increasing k was similar.
Due to space limitations, we omit the graphs for these experiments.
It is evident that for MCICM when L has the high effectiveness property, the biggest determining factor is how late the limiting campaign L is started.
When L is started early, all the methods perform well whereas when the delay is large, all the algorithms perform poorly.
For larger delays, greedy performs better than the other algorithms but the portion of the population saved is so small in all cases that this improvement is not signi cant.
(a) delay = 20% (b) delay = 20%, deg(na)   40 Figure 4: Evaluation on SB08 for COICM In Figure 4 we present our evaluation of the 4 methods on COICM using the Santa Barbara 2008 data set and setting pv,w values to
 infectees and degree centrality heuristics perform similar to the greedy method.
Due to space limitations we omit the graph presenting the performance of the approaches when the delay is large.
However as an example we note that when the delay is 50% and the number of seeds is 10, the greedy method performs 8%, 36% and 116% percent better than largest infectees, degree centrality and early infectees respectively.
The reason for the decay of performance of degree centrality heuristic w.r.t.
the delay is that degree centrality is purely a structural heuristic so the expectation of being infected is not computed for the seeds in AL.
When L is started too late, the highly connected nodes and their neighbors are more likely to have already been activated in campaign C. Comparing Figure 3(a) and 4(a), we observe the importance of high effectiveness since for the latter an average of 72% of the population can be saved with 10 seeds whereas the former shows consistent savings of 90-95% even with only one seed.
Figure 4(b) presents the case where the delay of L is 20% and the adversary that C starts from, has degree   40.
All methods are less effective when the start node of C is a highly connected node, since a highly connected adver-(a) delay = 20%, 0   pC,v,w   0.5, 0   pL,v,w   0.1 (b) delay = 20%, 0   pC,v,w   0.1, 0   pL,v,w   0.5 Figure 5: Evaluation on SB09 for MCICM There are crucial lessons we can extract from the tests we performed: 1) In almost all cases, largest infectees performs comparable with the greedy algorithm while being far less computationally intense.
The early infectees heuristic, on the other hand, performs poorly since it strictly targets nodes that are expected to be infected at time step r. In many cases even the simpler heuristic of degree centrality is a better alternative.
2) Parameters such as the delay of L, the connectedness of the adversary na are crucial to identify correctly to choose the right method for determining in uential nodes for limiting a bad campaign C. For instance, when the delay is large, degree centrality is not a good option whereas it performs well for small delays.
Having suf cient information about such parameters can help identify the best method for EIL.
So far, we have focused on the problem where AC and the delay r are known.
Therefore, we can provide an approximation algorithm with error bounds for the expected case.
However, such precise data is not easy to attain.
Practically, decisions must be made in the face of missing information.
Therefore, we study a more realistic formulation of the EIL where the information about the current state of the nodes is incomplete and an approximate value for the number of currently infected nodes is known.
The question we address is:  Can effective inoculation be performed in the presence of incomplete data and how fast does the performance degrade w.r.t.
the amount of missing data?  Consider a speci c instance of propagation of a bad campaign C.
known) at which point the limiting campaign L is to be started.
Let the set of active, inactive and newly activated nodes for campaign C at round r be denoted  ,   and   respectively.
Assume further that we are given the sets  given and  given where  given     and  given    , i.e. we know for only a subset of nodes if campaign C reached out to and activated them by the time campaign L is to be started.
Note that we assume   is completely unknown, i.e.
the current infectors are unknown and the value of r is not given.
The main idea we employ in this section is to provide a good prediction of the sets  ,   and   given  given,  given and ca (an approximate value for | |).
Let the predicted sets be  pred,  pred,  pred.
We then use these sets to create a new instance of EIL to provide a solution to the in uence limitation problem under uncertain data.
In Section 6.1, we introduce our prediction algorithm.
Later in Section 6.2, we present the solution to the in uence limitation problem under uncertain data using the results of the prediction method.
Finally, Section 6.3 presents an evaluation of the methods.
Identifying   and   The  rst step of the prediction is to predict   and  . Identifying   and   is crucial for three reasons: 1) They will be used to further identify the newly activated nodes  , 2) nodes that are predicted to be already active in campaign C will be eliminated from the set of nodes to save, so inaccurately predicting inactive nodes to be active might result in not saving nodes that could be saved otherwise and 3) predicting active nodes to be inactive might result in targeting nodes that provide no savings which is a waste of resources.
Since we are given  given, a subset of the nodes active in C and ca, the total number of nodes active in C, our aim in this section is to  nd the other ca   | given| nodes that are most likely to be active in campaign C. This can be posed as an optimization prob , the set of ca   | given| nodes lem, more precisely  nding  add that maximizes the number of possible scenarios of spread of C including all nodes in  given and no node from  given.
  where Gadd De ne Gadd = (Nadd, Vadd) where Nadd =  add    given and Eadd = {(u, v)|(u, v)   E   u   Nadd   v   Nadd}.
For a set  add, the number of cascade scenarios including nodes in  add and  given (and no other node) can be calculated as the number of   is the connected component spanning trees in Gadd of Gadd that includes all the nodes in  given (if no such component   is an empty graph).
This follows from the fact that exists Gadd the spread of a cascade starting from one node under MCICM or COICM follows a tree structure.
Therefore, the value function of a set  add can be computed by counting the number of such spanning trees it would be able to produce and offsetting the value of each spanning tree by the multiplication of the weights of the edges of that spanning tree (to favor more likely scenarios of cascades) in the following way: f ( add) = X
 pC,u,v (1) T  T (Gadd  ) (u,v) T where | add| = ca   | given| and T (Gadd) is the set of pos- .
The ca   | given| nodes that are sible spanning trees in Gadd most likely to be active can be detected by solving the optimiza-  = arg max f ( add).
Note that the number tion problem:  add of spanning trees of a graph can be exponential and therefore enumerating them is infeasible.
Luckily, Kirchoff [27] introduced a method for counting the spanning trees which later was developed in a computationally useful form in [6].
For counting the number of spanning trees of a graph G with n nodes, the algorithm uses an nxn matrix D = di,j called the degree matrix.
The entries of this matrix are: di,i = P wj,i, di,j =  wi,j if ni and nj are neighbors and di,j = 0 otherwise.
Deleting the nth row and column from D, we get the reduced matrix D .
The determinant of D  is then the number of directed spanning trees in G [6].
Creating one such D   and setting the weights wi,j to pC,i,j, we matrix for graph Gadd can  nd the number of spanning trees and compute the value of a speci c  add set.
Same observation has been stated in a recent study for inferring networks of diffusion [18].
However this does not solve all scalability issues as there are `|N | | given| | given|   possible  add sets to evaluate.
| add|  ) Q(u,v) T p  C,u,v is supermodular where p  The next observation we make is that  add   can be found using a supermodular function.
A function f (.)
is supermodular if it satis es: f (S   v)   f (S)   f (T   v)   f (T ), for all elements v and all pairs of sets S   T [39].
We will show f ( add)  = PT  T (Gadd C,u,v = pC,u,v/pmin and pmin is the smallest nonzero pC,u,v in G (to set p  C,u,v   1 for each edge).
Note that as we are augmenting each probability with the same value, maximizing f ( add)  for a  xed size | add| also maximizes f ( add).
Consider two sets S and T s.t.
S   T and let S  (T  ) denote the subset of nodes in S (T ) that form a connected component that include all  given.
We de ne  ) for S   u ( or T   u).
Supermodularity in similar subsets Su the general case can be shown by a proof that shows each spanning tree  lost  for the set S   u by removing node u can be augmented   and shown to be lost when node by the set of nodes in Tu u is removed from the set T   u.
As the augmented tree has a higher weight, the loss in larger for T   u.
The details of this proof is omitted due to space limitations.
Instead we will demonstrate     T     u =  , f ( add)  is for the case Su supermodular:     S    u = Tu     Su   (Tu C,v,ni (or Pv T p  THEOREM 6.1. f ( add)  is a supermodular function.
PROOF.
Consider two sets S and T s.t.
S   T .
Let the number of spanning trees (offset by the multiplication of the edges p  C,u,v) induced by the set S   ni ( or T   ni) that has ni as a leaf be KS,1 (KT ,1) and the number of spanning trees induced by the set S   ni (T   ni) that has ni as an internal node be KS,2 (KT ,2).
Let Pv S p  C,v,ni ) be denoted by degS(ni) (degT (ni)).
Note that since S   T , degS(ni)   degT (ni).
By removing ni from set S   ni, f   will decrease from KS,1 + KS,2 to KS,1 degS (ni) , whereas the reduction from set T   ni to T will reduce f   from KT ,1 + KT ,2 to KT ,1 degT (ni) .
Since every spanning tree that involves nodes in S   ni and has ni as an internal node can be augmented with the nodes in the set T   S while still forming a spanning and since p  C,u,v   1 for each edge, we have KS,2   KT ,2.
Similarly, KS,1   KT ,1.
Factoring in degT (ni) , we can conclude that the fact that 1   degT (ni) )KT ,1.
This
 shows that f (S   ni)    f (S)    f (T   ni)    f (T ) .
degS (ni) )KS,1   KT ,2 + (1   degS (ni)   1  



 Our goal is to maximize a supermodular function with cardinal-ity constraints, as we are seeking to detect a set of ca   | given| nodes that would maximize f (.) .
As evident from the de nition, if a function f (.)
is supermodular,  f (.)
is submodular and therefore maximizing a supermodular function is equivalent to minimizing a submodular function.
Unlike maximizing submodular functions, maximizing supermodular functions (or minimizing submod-ular functions) in general is shown to be polynomial-time solvable [20].
However maximizing supermodular functions with cardinal-ity constraints are known to be NP-hard.
The problem of minimizing a general submodular function under a cardinality constraint work graph,  given, given are the incomplete sets of active and inactive nodes and ca is an approximate value of | | }









  pred =  pred   {argmaxn Nchoose {deg(n) pred }} Figure 6: Heuristic to identify  pred lnn , 1 is known to be inapproximable within o(pn/logn) [38].
Svitk-ina et al. also provide a sampling-based solution that provides approximation guarantees of the same order as this lower bound [38].
The algorithm is a (5p n 2 ) bicriteria decision proce- dure.
That is, given a feasible instance, it outputs a set U with lnn B and w(U )   W/2 with probability at least p, f (U )   5p n where the cost of high p is more iterations.
Although it is hard to even approximate an arbitrary supermodular function with cardi-nality constraints, certain functions can be solved exactly in polynomial time.
Maximizing f ( add)  can be easily reduced to a su-permodular knapsack problem with cardinality constraints which is shown to be one such function [15].
Unfortunately, polynomial time in this setting is de ned based on the number of calls to an oracle function which answers whether a given set X belongs to the base polyhedron associated with function f. We also note that even for the case where there is a polynomial time algorithm for maximizing f ( add) , this problem is still computationally expensive for today s online social networks since the value function depends on computing a determinant which is an O(n3) problem.
As the optimization problem for identifying   and   proves to be computationally expensive, we next investigate heuristics to tackle this problem.
A good heuristic should 1) provide a coherent answer, i.e. the nodes in  pred should form a connected component so that there is at least one arborescence (directed spanning tree) in the subgraph induced by nodes in  pred, 2) have as many arbores-cences as possible, therefore getting close to the optimal solution.
Figure 6 provides details of the heuristic we used to predict  pred.
As our  rst goal is to provide a feasible solution, in the  rst step, we choose a Steiner tree in G that covers all nodes in  given to make sure the infected nodes will form a connected component (Lines 2-
ci c node, this computation can be easily done in polynomial time whereas the general minimum Steiner tree problem is NP-hard.
As the next step we choose additional nodes which will induce a large number of arborescences.
We do this by adding nodes that are connected to the largest number of nodes that are already predicted to be active.
Note that, the more incoming edges a node has from active nodes, the more chances for it to be activated.
Clearly, choosing nodes this way will result in choosing nodes that have a large number of possible parents in a directed tree.
After identifying the set  pred, we predict the inactive nodes to be:  pred = N  pred.
Section 6.3 provides an evaluation of this heuristic.
Identifying  pred The  rst, rather naive approach we implemented to predict  pred was to select the most central node in set  pred; i.e. the node that has the shortest average path to all the other nodes in  pred and to perform a breadth rst-search from this node in Gpred (subgraph of G containing only the nodes in  pred and their interconnects) to create a tree of information spread and to use the leaves of that tree as the newly activated nodes.
Our experiments revealed that the in uentials identi ed using this method have poor performance which led us to identify the next method of prediction.
Considering an in uence epidemic starting from a single node under IC, MCICM or COICM diffusion models forms a tree since a node can be activated by at most one node in campaign C, we investigate the performance of generating a random spanning tree on graph Gpred to identify  pred.
By constructing random spanning trees in a  nite space of ST spanning trees on graph Gpred and a probability distribution   on ST , we can sample a spanning tree st with probability  (st).
Therefore, this method is more likely to pick scenarios of information diffusion that are more likely to happen.
The algorithms for constructing random spanning trees can be categorized into two families, ones that are based on computing determinant [30] and ones that are based on random walks on the graph.
Random walk based algorithms can be further categorized as ones that work for undirected graphs [5, 1] and directed graphs [45, 44].
Of those algorithms all except for [44] run within the cover time, i.e.
the expected time it takes for the random walk to reach all the vertices.
In this work, we use the algorithm provided in [44] which generates random spanning trees within mean hitting time (faster than the cover time) and provides solutions for directed graphs.
Using this algorithm, a random spanning tree can be sampled according to the probability distribution on all spanning trees with probabilities proportional to the weights of the trees.
Since we have no means of distinguishing the importance of edges, in our experiments we assigned the same weight to all the edges of the network which results in a unweighted graph but given more data, for instance the relative importance of friendships, this method will bias towards more likely scenarios where information  ows on edges with higher weights.
After generating a random spanning tree on Gpred, we select the leaves of the tree as the newly activated nodes.
Note that this method is a heuristic.
One could enumerate all possible spanning trees on Gpred and identify nodes that repeatedly appear at the highest depth of the tree as newly activated nodes.
However this is a computationally expensive solution.
We leave  nding a scalable solution for   while providing accuracy guarantees as an open problem.
After constructing the sets  pred,  pred,  pred, we identify ALP , the set of k nodes to in uence by campaign L in graph G, in the following manner: Create graph Greduced = (Nreduced, Ereduced) where Nreduced =  pred    pred and Epred = {(u, v)|(u, v)   E   u   Nreduced   v   Nreduced}.
We then use the algorithm presented in Figure 2 on graph Greduced where AC =  pred and delay r = 0.
This method would provide 1   1/e approximation guarantees for the case where  pred =  ,  pred =   and  pred =  .
We call this method Predictive Hill Climbing Approach (PHCA).
The true value of ALP ( (ALP )) can be computed by calculating the expected number of nodes ALP would save in Greal where Greal = (Nreal, Ereal) s.t.
Nreal =       and Ereal = {(u, v)|(u, v)   E   u   Nreal   v   Nreal} where the set of adversaries is AC =   and delay r is 0.
Accuracy, precision and recall statistics of the prediction algorithm are given in Table 1.
Accuracy refers to the ratio of the nodes whose true states are correctly identi ed.
Precision refers to the ratio of nodes that are active (or for Y : newly activated) to those that are identi ed as active and recall refers to the ratio of nodes identi ed as active to the total number of active nodes.
Amount of unknown data is modeled by using the parameter pknown which denotes the probability that the state of a node is known.
Decreas-
pknown = 0.1 X
 pknown = 0.01 X
 Precision





 Table 1: Prediction Statistics Accuracy Recall











 ing values of pknown would result in larger amounts of missing information.
The prediction algorithm provides good accuracy (especially since many nodes of the graph are inactive) but precision and recall numbers decay with the amount of unknown data.
Note, however, that we are interested in the performance of EIL under uncertain data rather than the accuracy, precision and recall statistics.
We will now show that poor recall statistics do not necessarily translate to poor performance for PHCA.
We have studied 165,643 cascade scenarios under the MCICM with high effectiveness property choosing the starter of campaign C uniformly randomly and setting pC,v,w values to 0.1 in the Mon-terey Bay 2008 network .
Each of these scenarios required performing in the order of 100,000 simulations to retrieve and evaluate in uential nodes selected by each method.
Figure 7 presents the results.
In the presence of missing information, where the newly activated nodes in the network are unknown, using the greedy algorithm is not possible without prediction.
In this case, the best available baseline algorithm to compare our method to is the degree centrality heuristic where seeds are chosen from the high degree nodes that are not known to be infected.
The X-axis in both Figures 7(a) and 7(b) presents the number of seeds whereas the Y-axis presents the relative performance of the algorithm w.r.t.
the performance of the greedy method with complete data, i.e. ratio of the number of the nodes saved using the respective method to the number of nodes that would be saved by the greedy method were we to have complete data ( ,  ,  ).
Figure 7(a) presents the performance of the predictive hill climbing approach(PHCA) and the degree cen-trality(Deg(i)) heuristic under various amounts of missing information for the case where the limiting campaign L is started with 30% delay.
Figure 7(b) provides similar data for the case when the delay is 70%.
As it is evident from both  gures, the performance of predictive hill climbing approach is mostly resilient to missing information.
Especially when delay is small, PHCA performs within 96-90% of the performance with complete data and when delay is large it drops to 75% in the worst case (pknown = 0.01).
The performance of the degree centrality heuristic without using prediction is not as robust and has consistently worse performance than PHCA.
When the delay is small and a large number of seeds can be chosen, this heuristic performs well but when the delay is large, the performance compared to the greedy method (with complete data)  uctuates and consistently underperforms.
(a) delay = 30% (b) delay = 70% Figure 7: In uence Limitation using incomplete data We also considered another variation on EIL with missing data where an outsider can observe  new activity  in the network, i.e.
one can detect only the newly activated nodes.
This problem formulation  ts systems such as Twitter [40] where one can easily retrieve new tweets on a topic but retrieving the entire history of tweets on that topic is impractical.
Interestingly, our experiments on the Facebook Monterey Bay 2008 network revealed that, even without using a prediction algorithm, i.e. assuming all the nodes except the newly activated ones are inactive, the limiting campaign performs within 99% of what would be achieved with complete data when the delay r is small, and within 92-96% when the delay is large.
However, we believe deeper analysis and experimentation is needed to generalize this  nding before concluding of a prediction method is not necessary for this case.
In future work, we plan to investigate this problem further.
In this work we performed an extensive study of the problem of limiting the spread of misinformation in a social network.
We investigated ef cient solutions to the following question: Given a social network where a (bad) information campaign is spreading, who are the k  in uential  people to start a counter-campaign if our goal is to minimize the effect of the bad campaign?
We call this eventual in uence limitation problem.
We proved that this problem is NP-hard and therefore an exact solution is infeasible for large scale social networks.
We also showed that two variations of this problem on two different communication models are submodular and therefore a greedy method is guaranteed to provide a 1/(1   e) approximation.
Although the greedy algorithm is a polynomial time algorithm, it is still too costly for large scale social networks.
Therefore, we also experimentally studied the performance of the greedy algorithm, comparing it with 3 different heuristics one of which is degree centrality.
We showed that in many cases the performance of heuristics, even the simple degree centrality heuristic, is comparable to the greedy algorithm.
We explored different aspects of the problem such as the effect of starting the limiting campaign early/late, or the properties of the adversary and how prone the population is to accepting either one of the campaigns.
We also studied the more realistic problem of in uence limitation in the presence of missing information.
We introduced an algorithm called predictive hill climbing approach which  rst predicts the current state of all the nodes of the network given the states of a fraction of the nodes and then uses the hill climbing approach to choose the set of  in uentials  using the predicted data.
We introduced an optimization algorithm to choose the set of nodes that are most likely to have been infected by the  bad campaign .
We show that, even though the naive solution to this problem is exponential, using matrix-tree theorem, and supermodularity of the speci c problem at hand, one could provide a polynomial time algorithm for this problem.
However, the method requires using an oracle function that is not available for our problem.
Therefore we seek heuristics to solve the prediction problem.
Our method relies on generating random spanning trees to capture  likely cascade scenarios .
Our experiments show that for most cases, the predictive hill climbing approach provides good performance, within 96-90% of the performance that would be achieved with no missing information.
Although for small delays the performance is consistently within 96-90%, for large delays the performance degrades to 75% when the amount of missing information increases dramatically, i.e. states of the nodes are known with only 0.01 probability.
This work is partially supported by NSF Grant IIS-0847925.
We would also like to thank OIT at UCSB for TRITON support and Manuel Gomez-Rodriguez for his comments.
