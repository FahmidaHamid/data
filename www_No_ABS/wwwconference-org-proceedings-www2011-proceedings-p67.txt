Current federated web search systems, like any other system, have many internal system parameters that need to be  xed prior to releasing it to real users.
Fixing the operating point   the set of system parameter values   can be a time consuming process in the standard approach: i) train an algorithm, ii)  x system param-eters/thresholds (operating points), iii) conduct multi-week online experiments, iv) evaluate the results.
A characterization curve of a system, on the other hand, provides a performance curve of a basic component over the entire span of possible operating points.
For example, a transistor characteristics curve can be used to  nd what the the voltage will be if the current is at a speci c level, and how this curve changes with temperature.
This information is then used by engineers to design new circuits and predict its presentation.
In a similarly way, in federated search, a change in operating point usually results in a change in the vertical rankings and hence a change in average quality metric distributions at speci c positions and overall user engagement metrics.
Today estimates for engagement metric impact are measured by doing online experiments over weeks of data.
In this paper we present a methodology that uses randomization of results that enables us to create operating curves for the expected performance of the algorithm for all possible operating points.
Furthermore it enables us to answer many design related questions such as:   Can we predict the online behavior of the system if we change an internal threshold without performing new online experiments?
  Can we understand the online impact of a speci c feature in the model at various operating points?
be used for estimating models, can we show which one gives us better online user engagement metrics in an of ine way?
  In any machine learning process the engineer has to struggle with issues of query sampling and weighting of training records.
Is there a systematic approach to quantifying which sampling or weighting scheme leads to better user engagement pro le?
In Section 2 we summarize research relevant to our work.
We provide an overview of our federated web search architecture, metrics, and experimental setup in Section 3.
In Section 4 we describe our characterization methodology and provide the algorithmic details.
Next we demonstrate the methodology by applying it to answer numerous questions about our federated web search algorithm, which is described in Section 6.
Receiver Operating Characterization (ROC) curves were introduced in early signal detection literature [23] and have since been used extensively in psychology [17], pattern recognition and machine learning [11, 12], computer vision [16] and numerous other  elds.
Essentially the idea is to characterize the performance of a system over the entire set of feasible operating points.
This characterization curve can then be used later to predict behavior or explain the behavior if the operating point changes for some reason.
The trade-off curves are typically between probability of mis-detection and probability of false alarm since, which are then used to compute total probability of error or cost.
In the  eld of information retrieval, there are numerous papers that that present plots for trade-offs of various types.
For example, Collins [3, 4] presents results that show the impact of varying query expansion parameters.
Craswell et.
al [9], characterize the impact of crawl selection methods on retrieval effectiveness.
While these papers discuss trade-off and of ine evaluation they are not about about online performance characterization from of ine experiments.
Evaluation literature in information retrieval community depends heavily on sample corpus and human judgments [24, 13], which re ect the views of the human judges and not on what will happen with respect to user engagement.
This is crucial since it is very dif cult for a judge to know what the user wanted in the case of tail or location-speci c queries.
In machine learning literature, researchers have shown how to choose optimal operating point for speci c metrics.
Joachim [15] shows how to optimize for for various loss functions such as ROC area, Precision/Recall Break-Even Point (PRBEP).
However, while this is an of ine optimization and evaluation method, it is not representing or predicting online engagement behavior of users.
Federated web search [21] is an active  eld of research.
Si and Callan [22] discuss approaches to incorporating vertical search engine effectiveness.
Diaz et al [1, 6] present machine learned approaches to trigger verticals and adapting the triggering based on click feedback.
We [18] introduced the notion of using clicks as labels for machine learning and present online performance results.
In these papers, online results are presented at only one operating point.
Online behavior models [5, 2, 14, 7] estimated from click-logs can be used to predict the user satisfaction in online sessions.
These are helpful to estimate the performance (without collecting human judgments) of an online experiment after it has been performed on real online traf c.
Finally, the notion of randomization of result presentation has been used to collect click-preferences for web results [19].
However, this work does not propose further use for characterizing performance.
Figure 1: Screenshot of the Bing SERP showing vertical search results at different slots.
In this paper we propose a method that allows us to predict online metrics over the entire range of feasible of system operating points.
This is in contrast to online experiments that i) usually take weeks to get statistically signi cant results, and ii) cannot be run for numerous operating point due to lack of online traf c.
Collecting click-logs of online experiments where we randomize the presentation of verticals allows us to simulate online traf c for various different operating points with potentially different search result pages for the same query.
We start with a brief overview of how our system places verticals on the SERP and how we currently perform online experiments.
When a user query arrives at the search engine, it is sent to the web result generator and to a vertical selection component that determines which verticals should be triggered.
Some of the verticals that are selected at this step generate content in response to the query.
The content from the verticals that respond are then sent with the web results to a ranking and placement module.
This module is responsible for determining where the verticals are placed on the SERP.
The SERP is then rendered and sent to the user.
Figure 1 gives an examples of a SERP rendered by Bing.
Verti-q Vertical Selection Web
 d n a g n i k n a
 t n e m e c a l



 .
.
.
Vk 1 Vk Figure 2: A high level overview of how the SERP is generated in response to a user query.
cals are frequently shown at speci c slots, in part due to aesthetic appeal and often due to the way pages are rendered on the user s browser.
The common slots are at the top of page (TOP), middle of page (MOP) and bottom of page (BOP).
The location for the MOP slot is picked so that it appears just above the  fold  (the line on the page below which the user can not view the content without scrolling) for most users.
The set of slots is denoted by S = {TOP, MOP, BOP}.
The approach we describe in this paper is not constrained to any speci c way of picking locations for the slots and hence we can think of the set of slots as being S = {s1, s2, .
.
.
, sk} (see Figure 3).
User Query s1 Algo Block 1 s2 Algo Block 2 ...
Algo Block k   1 sk Figure 3: A simpli ed version of the SERP showing slots where the verticals can be placed.
A  xed number of web results can be placed between two consecutive slots.
We now mention how the ranking and placement module (Figure 2) assigns verticals to the slots.
Once a subset of the verticals generate content in response to a query, they are ranked and placed as described below.
Corresponding to each vertical V there is a model  V that ranks it.
Given the feature vector for the query q, the model outputs a score  V (q).
The idea is that if  V (q) >  V (q(cid:2) ), then the vertical is more relevant for query q than q(cid:2) .
The vertical is assigned to the
 :  V (q) = slot  V (q) given by TOP if  V (q)    V,TOP MOP if  V,TOP >  V (q)    V,MOP BOP otherwise.
The constants  V,TOP and  V,MOP are referred to as the TOP and MOP thresholds for V respectively.
In general, if the SERP has k slots, then there will be k   1 thresholds  V,1,  V,2, .
.
.
,  V,k 1 that are needed to place the vertical on the SERP.
We used two approaches to build the model  V .
The  rst was using human judgments and the second was using pairwise click preference judgments.
The modeling technique itself was gradient boosted decision trees [10].
This work is described in more detail in our paper [18].
Whenever we need to experiment with a new ranking algorithm, we assign a small set of users to a  ight and use this ranking algorithm to generate the SERP for this set of users.
For example, if we want to evaluate a new model to rank a vertical V , we take the production ranker and replace the model used to rank V with the new one to obtain the modi ed ranker for the  ight.
We call this the treatment  ight.
The metrics for this  ight are then compared to a control  ight which uses the production ranker.
We usually compare the performance of the treatment  ight to the control  ight over the same period of time to avoid interference from temporal changes in user behavior (for example from holidays, weekend effect, breaking-news events).
Another kind of  ight that we use is an auditioning  ight.
On this  ight, the users are shown the verticals that trigger for a query at a random slot on the SERP (instead of using a model score to place the vertical).
This  ight is mainly used to gather online user engagement behavior data for various vertical placement con gu-rations.
This allows us to predict user engagement for a speci c con guration in a of ine setting.
Finally, each  ight is constituted of a set of impressions I = {I1, I2, .
.
.
, Im}, where each impression I contains the following pieces of information: i) the query q that generated the impression, ii) the verticals that were shown on the SERP and the slots where they were shown, iii) the components on the SERP that got clicked by the user.
For every  ight, we analyze a set of metrics for each vertical to see how they differ from the control.
As mentioned before, a typical experiment involves changing the model used for ranking one vertical V .
In this case, some of the metrics we can analyze are:   Coverage: This is the fraction of SERP impressions for which the vertical triggered.
This is typically the same on the treatment and control  ight unless we also perform vertical suppression.
  Coverage at a slot s: This is the fraction of impressions where the vertical was shown at slot s out of all the impressions where the vertical triggered.
Unlike the overall coverage of the vertical, this can be directly in uenced by changing the model or thresholds used to place the vertical.
  Clickthrough: This is the fraction of times that the vertical was clicked out of all the impressions (not just those on which the vertical was shown).
This is of particular interest  c they get.
Ideally, they would like to see this metric to be as high as possible.
We also frequently look at clickthrough at slot s which is the fraction of times that the vertical was clicked when it was shown at s out of all the impressions.
  Vertical Clickthrough Rate (CTR): This is the fraction of times that the vertical was clicked out of the impressions where the vertical was shown on the SERP.
The vertical CTR at slot s is de ned as the fraction of times the vertical was clicked out of the impressions where the vertical was shown at s.
Usually if we increase the vertical coverage at TOP, the vertical CTR increases, since components shown higher up on the page are more likely to be clicked.
Also, if the model score positively correlates to relevance, then as we increase the coverage at TOP, the vertical CTR at TOP decreases.
This is because we allow the vertical at TOP for more and more impressions where it scored lower.
Thus to make comparing vertical CTRs meaningful, we must  rst ensure that there is a good coverage match at various slots and then compare the vertical CTRs at the slots.
We then expect the  ight with the better model to have higher vertical CTR at TOP and lower vertical CTR at BOP since it is more likely to assign higher scores to more relevant queries.
  Normalized Vertical CTR: This is the fraction of times that the vertical was clicked out of the impressions where either the vertical or some other result below it was clicked.
The idea is that if the user clicks on the vertical or on some other result below it, it means that with high likelihood, the user noticed the vertical.
This is based on the eye tracking study of Radlinski and Joachims [20].
The normalized vertical CTR at slot s is de ned as the fraction of the number of times the vertical got a click when shown at s out of the impressions where the vertical was shown at s and either the vertical or some result below it got a click.
  Sliding Normalized Vertical CTR at Slot s: This is normalized CTR based on the impressions with score between  V,s and  V,s+ s in the limit  s   0.
In other words, this is the normalized CTR based on the impressions with the lowest vertical scores that were allowed to be shown at slot s. This is similar to metrics used by others [25, 2].
We formally de ne a metric as a function f : I   R. For example, the vertical clickthrough at TOP is de ned as ClickthroughV,TOP(I) = |{I   I : IsSlotV,TOP(I)   ClickV (I)}|
 , where IsSlotV,s(I) is true if V was shown at slot s in I. and ClickV (I) is true if the V was shown in the impression I and the user clicked on V .
De ne ClickBelow(I) to be true if V was shown in impression I and some result below V got clicked.
Then we can de ne normalized CTR at slot s as NormCTRV,s(I) = |{I   I : IsSlotV,s(I)   ClickV (I)}| |{I   I : IsSlotV,s(I)   (ClickV (I)   ClickBelowV (I))}| , where  V is the model used to rank V and  V,s is the threshold for slot s (see Section 3.2).
The sliding normalized CTR can be de ned as (cid:7)NormCTRV,s(I) = |{I   I :  V,s    V (I) <  V,s +  x   IsSlotV,s(I)   ClickV (I)}| .
lim  x 0 |{I   I :  V,s    V (I) <  V,s +  x   IsSlotV,s(I)   (ClickV (I)   ClickBelowV (I))}| A problem with the above de nition of sliding normalized CTR is that we need to have in nite number of impressions, which is not realistic.
In practice, we will have to approximate it by looking at a small  nite window of score around the threshold.
The size of this window would depend on the size of the  ight and the con dence we want to have in the value we estimate.
We now describe how we estimate the various metric values of ine using click logs from an auditioning  ight.
Suppose we have a ranker  V for a vertical V and we would like to estimate how the values of the online user engagement metrics change as we adjust V : I   R (i = the thresholds  V,1,  V,2, .
.
.
,  V,k 1.
Let f i 1, 2, .
.
.
, m) be a set of metrics that we are interested in estimat- ing.
Denote by Iaudition the set of impressions from an auditioning  ight, and  V (I) the score produced by the speci c ranker  V for the impression I.
For simplicity of notation de ne  V,0 =   and  V,k =  .
For any new ranker and corresponding thresholds for each slot, we can always compute the ranker score and use the thresholds to compose a  nal SERP for each impression in the auditioning  ight, The question is how do we predict the user engagement metrics for this speci c composition.
Now, since the auditioning  ight has impressions that are all possible compositions of vertical placements in slots, and corresponding user engagement data, we can simulate a  ight engagement data corresponding to a new ranker and thresholds by selecting only the impressions for each query in the auditioning  ight that are identical to the composition generated by the new ranker and thresholds.
This idea is summarized by the following proposition.
PROPOSITION 1.
Suppose on the auditioning  ight, the vertical V was placed uniformly at random at one of the k slots.
Also suppose that users pick queries at random from some  xed distribution and issue them to the search engine and that users interact with the SERP without any context of the previous queries they issued.
Then the set I = {I   Iaudition :  V,i    V (I) <  V,i 1 where I places V in slot si} simulates a treatment  ight that ran in parallel the auditioning  ight with 1/k fraction of the users as on the auditioning  ight and where  V was the ranker for V and the thresholds were  V,1,  V,2, .
.
.
,  V,k 1.
PROOF.
Suppose a query q was received by the search engine.
Suppose s =  V ( V (q)) is the slot where we would place the vertical if we used ranker  V and the thresholds  V,1,  V,2, .
.
.
,  V,k 1 (the treatment  ight).
On the auditioning  ight, with probability
 on the auditioning  ight has a 1/k probability of the vertical ending up in the same slot as on the treatment  ight.
Assuming then that each query is picked from the same distribution, this proves the proposition.
Using the proposition, we can predict the metric fV for the treatment  ight to be fV (I), where I is de ned in the proposition.
Based on Proposition 1 we give the following algorithm that predicts how adjusting the TOP threshold for vertical V in uences the value of various metrics.
e u a
 c i r t e
 Flight


 t a


 d e z i l a m r o
 TOP Threshold Clickthrough at TOP Figure 4: A plot showing a metric fV,TOP as a function of the TOP threshold generated using the pseudo-code in Section 4.1.
This is generated by plotting (cid:10) V (Ii), fV,TOP(Ii)(cid:11).
Figure 5: A plot showing the trade-off of two metrics as the TOP threshold is varied.
From a  ight, we only get to observe one point on this trade-off curve.
tioning  ight.
tical V triggered.
sorted list of impressions.
(a) Let Ii = {I1, I2, .
.
.
, Ii}.
(b) Predict the value of metric fV,TOP to be fV,TOP(Ii) if the TOP threshold were set to  V (Ii).
This gives us the user engagement metric values at TOP for the various TOP thresholds.
We can now plot each metric as a function of the TOP threshold as shown in Figure 4.
If we are interested in the trade-offs of two particular metrics, say ClickthroughV,TOP and NormCTRV,TOP, we can also plot the trade-off curve for them.
First, using the proposition, we predict ClickthroughV,TOP and NormCTRV,TOP for every possible value x of the score.
With a slight abuse of notation, we represent the respective values by ClickthroughV,TOP(x) and NormCTRV,TOP(x).
We can now plot the curve of (cid:10)ClickthroughV,TOP(x), NormCTRV,TOP(x)(cid:11) as a function of x as shown in Figure 5.
A  ight corresponds to point on this trade-off graph.
If we wanted to plot the trade-off graphs using  ights, we would have to  ight the model with lots of different TOP thresholds and then interpolate, something that is not practical.
For the purpose of characterizing the trade-offs of a model or for comparing two different models, the metrics at TOP turn out to be the most valuable since we get a lot more click feedback at TOP (and hence the con dence in the estimates tends to be much stronger) and also since the verticals are very likely competing with the best web results.
Metrics at Various Slots One common problem we have is controlling the quality of the verticals shown at various slots.
For example, we want to show a vertical at TOP only when we expect it to be of better or comparable quality than the web results.
Otherwise we want to move it down lower on the page to a slot where the value it provides is higher or comparable to the web results below it.
Suppose we want to show the vertical V at the slot s for query q only if we expect the normalized CTR (or some other quality metric) at s is at or above some constant  .
The problem is that a model  V need not necessarily predict the normalized CTR for the query at slot s. For example, a model trained on human judgments might predict a score on the same scale that the human judges used.
This does not have a natural mapping to normalized CTR.
Although we can train a model that predicts normalized CTR using user click logs, even in that case, we would like to see how changing   effects other metrics.
Another problem of interest even if we target the metric directly might be studying the impact of changing the modeling process (for example, changing the loss function, or changing the weighing of impressions) of ine without having to  ight each model.
We would also like to compare the trade-offs of models trained using different labels (like click logs and human judgments) and pick the best one.
In order to do this, we need a function that maps the model score to the metric we are interested in.
To address these problems, we look at the  sliding  version of the quality metric we are interested in.
One example was given in Section 3.5, where we de ned the sliding version of the normalized
 The below pseudo-code computes the k 1 thresholds that target (a) Let Is   Iaudition be the set of impressions where V was in slot s from the auditioning  ight.
(b) Compute the score  V (I) for each I   Is.
(c) Sort the impressions by the score.
Let I1, I2, .
.
.
Im be the sorted list of impressions.
(d) For i = w, w + 1, .
.
.
, m (w is picked based on the con dence we need in the estimates): i.
Let Ii = {I1, I2, .
.
.
, Ii}.
Let I (cid:2) i be the last w impressions from this list.
ii.
Predict that the value of f at the model score  V (I) (cid:2) i) at slot s. That is, we estimate the is equal to f (I a value of   for a sliding metric f:
 window of w impressions with score just greater than or equal to that score.
iii.
If the estimate of f at s dips below  , output  V (I) as the threshold  V,s for slot s and continue to the next slot.
For the purposes of conducting experiments with our approach, we used an auditioning  ight where 1% of the traf c was assigned to it.
The impressions we used for estimating online user engagement metrics were obtained from a two week period of this  ight.
We present our results for three verticals, Non-Navigational (Non-Nav) News, Image and Commerce verticals.
The models were trained on either the user click logs (click-based models) or on human judgment labels collected using our Human Relevance System (HRS) system.
The models we built were using an implementation of gradient boosted decision trees [10].
We used an L2 regression loss function for the modeling, except in Section 6.5 where we study the impact of changing the loss function on online user engagement metrics.
The algorithm parameter values used are described along with each experiment in the results section.
The impressions used to train our click-based models for these verticals were obtained from a two week period on the auditioning  ight (this period was different from the two weeks used to gather impressions for of ine user engagement metric estimation).
We retained the impressions where the vertical was shown at TOP and MOP.
To assign a label to each impression, the vertical was compared to the  rst web block (Figure 3).
If the vertical was shown at TOP and got a click, but not the web block below it, the impression was labeled 1.
If the vertical did not get a click when shown at TOP, but the  rst web block did, then the impression is labeled 0.
Also, if the vertical was shown at MOP and got clicked, but not the  rst web block, then the impression is labeled 1, but with a higher weight (of 5).
This is because the user explicitly skipped over the  rst web block to get to the vertical.
The impressions are then log-weighed to ensure that the most frequent head queries do not completely dominate the training process.
Then the impressions are reweighed again to ensure that the head and tail both have the same weight.
The human judgment labels that we used for the News, Image and Commerce verticals had around 4,300, 39,000 and 9,000 judgments respectively.
The judges were shown just the SERP with web results and the vertical content and were asked to grade the vertical on a scale of 0 to 3, indicating whether they think the vertical should not be shown in response to the query or whether the right slot is BOP, MOP or TOP respectively.
Further details of this approach are described in extensive detail in [18], but are not that relevant for the purpose of the methodology described in this paper.
Once we estimate the trade-off curves for a pair of user engagement metrics of ine for two different experiments (like one that uses a click-based model and another that uses a HRS model), we would also like to know if the differences in the trade-offs are statistically signi cant.
This is very important since we need to evaluate if we need a longer auditioning  ight to make our predictions meaningful.
To do so, we use bootstrapping [8].
For this, we generate n samples of the auditioning  ight by sampling the impressions uniformly at random with replacement.
For each sample, we calculate the value of the metrics at various scores.
Suppose the of ine bootstrap estimates of some metric f at a slot threshold of x for slot s are a1, a2, .
.
.
, an.
Then we predict the value of the metric f at slot s with threshold x to be the median of the n values.
We compute the 90% con dence interval by  rst sorting the ai and then picking lower value of the interval to be at the 5th percentile of the sorted list and the higher value at the 95th percentile.
Note that since in operating curves both axis are noisy, we produce both x and y con dence intervals around the estimates.
We used an n of


 We now demonstrate the methodology presented in Section 4 by applying it to  ve experiments that we conducted to answer speci c questions about our federated web search algorithm.
First we explore the impact of trading-off the aggregate quality of the results that are shown to the user versus clickthrough rate.
The second study is looks deeper into the impact of worst result quality (instead of aggregate) shown to the user for a spe-ci c threshold versus clickthrough rate.
We next demonstrate how to use the characterization curves to study the online user engagement impact of a feature in an of ine way.
This is followed by an experiment where we study the impact of various query sampling strategies in the modeling process.
And  nally we apply the methodology to study the impact of loss functions on the online engagement metrics.
Ideally, our vertical partners would like their verticals to be shown on as high a slot as possible.
On the other hand, we want to ensure that if the web results provide more utility than the vertical, then the vertical is not shown above them.
One metric of prominence that the vertical teams frequently look at is clickthrough.
A metric that measures the quality of the vertical is the normalized CTR, which measures how the vertical performs compared to the web results below it.
We compare how different models for some major verticals trade-off these two metrics at TOP.
We use the algorithm in Section 4.1 to estimate the value of these metrics as a function of the TOP threshold and then plot the trade-off graph for each model.
Using bootstrapping, we can also calculate the error margins for the estimate of both these metrics.
As we decrease the TOP threshold, we let the vertical appear at TOP for more queries.
This will lead to increased clickthrough to it.
At the same time, we expect that since we are letting the vertical at TOP for less relevant queries (assuming the model score correlates positively to relevance), the normalized CTR will drop.
This is shown in Figures 6-8 for three different models.
For each curve we also show the error margins for our estimates on clickthrough and normalized CTRs at TOP using horizontal and vertical error bars.
The X-axis has been scaled so that the maximum possible clickthrough to 1.
The Y-axis has been linearly transformed to be between 0-1.1 We plot the graphs for three models for each vertical.
The curve labeled "Vertical Con dence" denotes the trade-off of a model that just outputs the vertical con dence (a feature provided by the answer partner as an indicator of how relevant they think their vertical is to a query) as the score.
Vertical con dence is usually one of the most important features in our click-based and HRS models, but is calculated without considering the other web results present on the SERP.
But when the verticals are ranked by the ranking and placement module of the search engine, this information can be incorporated into the ranking process as features.
In some sense, the
 curves for the click-based and HRS-based models show that improved utility that this component provides.
In general, one can see that for the same amount of clickthrough, the click-based model achieves a better value of normalized CTR at TOP compared to the HRS model or just using vertical con dence.
(cid:10)ClickthroughV,TOP( V,TOP), NormCTRV,TOP( V,TOP)(cid:11) on this curve, where  V,TOP is the TOP threshold picked for the  ight.
The operating curve, in contrast, provides us with a more complete picture of the performance of the algorithm by reporting the engagement metrics for the entire range of operating points (thresholds).
Non Nav News Click based Model HRS based Model Vertical Confidence
 .
.
.
.
.
.
Fraction of Realizable Clickthrough at TOP Figure 6: Normalized CTR for Non-nav News.
Image Click based Model HRS based Model Vertical Confidence
 .
.
.
.
.
.
t a


 d e z i l a m r o
 d e a c s e
 l


 t a


 d e z i l a m r o
 d e a c s e
 l





 Fraction of Realizable Clickthrough at TOP Figure 7: Normalized CTR for Image.
of Quality Control We can use the approach presented in Section 4.2 to calculate the thresholds that target a certain value of a sliding quality metric.
For example, we can use this to target a value   of the sliding Commerce Click based Model HRS based Model Vertical Confidence


 t a


 d e z i l a m r o
 d e a c s e
 l
 .
.
.
.
.
.
Fraction of Realizable Clickthrough at TOP Figure 8: Normalized CTR for Commerce.
normalized CTR.
Using the approach of Section 4.1, we can analyze how adjusting   changes the clickthrough to the vertical at TOP.
We can in fact calculate the overall clickthrough from all slots too.
But we restrict ourselves to TOP for simplicity.
The trade-off curves for Image are shown in Figure 9 based on a sliding window of w = 1000 impressions.
In general, we expect that a better model will assign a higher score to impressions where the vertical gets a click but not the web results below it and assign a lower score to queries whose impressions have a click on the web result below the vertical, but not on the vertical itself.
Thus we expect a better model to achieve a higher value of sliding normalized CTR at the highest ranges of it scores while at the same time achieve a lower value of normalized CTR at the lowest ranges of the score.
In particular, if  Random is a model that assigns a random number as the score for a query, its sliding normalized CTR will be a constant (equal to NormCTRV,TOP(Iaudition), since it will place the vertical at TOP for random queries irrespective of the TOP threshold).
Thus its clickthrough vs. sliding normalized CTR curve will be a  at horizontal line.
The better a model is, the more the curve will be rotated clockwise.
Also of particular interest is the graph with clickthrough at TOP on the X-axis and the normalized CTR and sliding normalized CTR at TOP on the Y-axis.
For a given value   of for the sliding normalized CTR threshold, this graph allows us to read out the click-through that we will obtain and also the normalized CTR, the collective quality of the impressions for which we let the vertical at TOP.
This graph is presented for the click-based model for Image in Figure 10.
A common problem that we face is quantifying the impact of adding new features to the modeling process.
For example, before we spend the resources to implement and deploy a new feature, we would like to quantify how much will it help improve our predictions.
We can look at the difference in regression errors reported during the modeling process when we train models with and without the new features.
But there is no natural way to map this to changes in  ight metrics.
In this case, we can use our approach to predict the impact on the metrics of ine.
In our feature vector, we have as features the scores of various vertical classi ers.
For example, when a query is received, the the Image
 .
.
.
.
.
.
Click based Model HRS based Model Vertical Confidence With Web Features Without Web Features


 t a


 d e z i l a m r o
 d e a c s e
 l
 .
.
.
.
.
.
t a


 d e z i l a m r o
 g n d i i l l
 d e a c s e












 Fraction of Realizable Clickthrough at TOP Fraction of Realizable Clickthrough at TOP Figure 9: Sliding normalized CTR for Image.
Figure 11: Dropping vertical classi er features degrades the clickthrough vs. normalized CTR trade-off.
Image Rescaled Normalized CTR at TOP Rescaled Sliding Normalized CTR at TOP

 .
.
.
.
.
.
Fraction of Realizable Clickthrough at TOP Figure 10: Normalized CTR and sliding normalized CTR for Image as a function of clickthrough.
Image classi er returns the probability that the query has image intent.
We also have classi ers for some of the most important verticals.
In Figure 11 we report the impact of dropping all the vertical classi er scores from the training process on the Image vertical when the model was trained on click data.
In the case of Image, the vertical con dence happens to be the same as the Image classi er (and hence we dropped the vertical con dence too when we dropped the classi er scores) and is the most important feature when used.
The plot shows that dropping these features results in a statistically signi cant drop in the performance of the model.
When training rankers for vertical search results using clicks as labels, one needs to make sure that there is a fair representation of head, body and tail queries in the training data.
The frequency distribution of queries that trigger results from each vertical varies considerably across verticals and hence it is important to characterize this distribution of queries in the training data for each vertical individually.
Furthermore, it is important to  nd the relative repre-sentativeness (or weighting) of head and tail queries in the training data that would optimize some user engagement metric such as the normalized CTR at TOP across speci c query segments as well as across all queries.
The model operating curves described earlier gives us a straight forward methodology to choose an optimal weighting combination for head and tail queries.
We typically collect query impressions from over a two week time-frame on the auditioning  ight (referred to in Section 5.2) to be able to train a ranker for a vertical.
We assign each impression in this training data the normalized logarithmic query frequency over the 2 weeks as the base weight.
In order to perform a sweep of possible weighting combinations across head and tail queries, we assign the head queries (with greater than 10 query impressions in 2 weeks) a weight of   and the tail queries (the remaining queries) a weight of (1 ).
For each   value, we pick an optimal model after comprehensive parameter sweeps during the training process.
We then plot the model operating curves for the models corresponding to each   value between 0 and 1.
These curves use query impressions from an evaluation set collected from a two week time-frame in the future on the same  ight from which the training impressions were obtained.
Furthermore, we could also  lter these impressions into head and tail and plot separate model operating curves for the two segments of the evaluation set to see how the weighting impacts performance in each segment.
We performed the head versus tail weighting experiments for three verticals   News, Commerce and Image.
Figure 12 shows the model operating curves of a Non-Navigational News ranker for head weights of   = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) on all query impressions in the evaluation set.
Figures 13 and 14 show the operating curves on the head and tail evaluation segments respectively.
As can be seen, the value of   for which we get the optimal model operating curve over most of the clickthrough ranges is 0.2 overall.
At   = 0.2, the base weights (logarithmic query frequency) for head queries are assigned a weight of 0.2 and the tail queries are assigned a weight of 0.8, which happens to make the tail impressions 4X as representative as it was originally relative to the head impressions.
Interestingly, this is optimal even on the head segment beating a model trained only on head queries (which is represented by the operative curve for the head weight of   = 1.0) indicating the incremental value of having tail impressions to train a model for ranking head queries.
As for the tail segment, the operating Non Nav News on Head


 t a


 d e z i l a m r o
 d e a c s e
 l Alpha (Head Weight)







 .
.
.
.
.
.
t a


 d e z i l a m r o
 d e a c s e
 l Alpha (Head Weight)







 .
.
.
.
.
.
Fraction of Realizable Clickthrough at TOP Fraction of Realizable Clickthrough at TOP Figure 12: Normalized CTR for Non-nav News for head weights between 0 and 1.
Figure 13: Normalized CTR for Non-nav News on head queries for head weights between 0 and 1.
curve for   = 0.2 runs neck and neck with the curve for   = 0.0 (training only on tail impressions) and does better at certain operating points on the curve, indicating the incremental value of using head query impressions when training a ranker for tail queries if one wishes to operate at these points on the curve.
The operating curves for the other two answers, Commerce and Image (not shown in the paper), also showed "sweet-spots" over distinct   values giving the modeler a choice of head and tail weights he or she could use depending on what segment of queries he or she wants to cater to or what user engagement metric he or she is shooting for.
Candidate models (or the predictions they make) can only be compared once a loss function is decided upon.
But what evidence is used to select the best loss function for the problem domain?
One can conduct time consuming online experiments, but here we show a characterization curve can be used to compare the different options of ine without experimenting on real users.
To demonstrate this, we trained two models models using different loss functions and used a characterization curve to compare them.
One model used the L2 loss function and the other used a logistic loss function.
The training process used was described in Section 5.2 including a parameter sweep.
The click labels were generated from the process described in Section 5.2.
Operating curves were generated for both models using impressions from a different 2 week time-frame of the auditioning  ight.
These curves are shown in Figure 15 and we can see that although the model trained with L2 appears to be marginally better than that trained with logistic loss, the difference is not statistically signi cant.
Thus, the methodology allows us to weed out such hypotheses without conducting an online experiment.
Flights the predicted and observed values of clickthrough on the two  ights were  8.6% and  3.6%.
The negative values indicate that the observed values were lower than the predicted values.
The relative differences between the predicted and observed values of normalized CTR on the two  ights were  3.2% and  6.8%.
We showed how we can use impressions from an auditioning  ight to perform many of ine experiments to study the impact of system operating point changes on click metrics.
The characterization curves give an comprehensive picture of the federated web search system over the entire range of operating points and on at one speci c operating point like online experiments typically do.
In addition the characterization curves can be generated of ine and so are not time-consuming like the online experiments.
The results in Section 6.6 show that the of ine predictions we make are reasonably accurate.
We demonstrated the methodology by applying it to answer various questions about our federated web search system.
We used it to understand the trade-off of clickthrough and quality, impact of spe-ci c features on clickthrough, impact of various query sampling strategies on user engagement metrics, and,  nally, the impact of modeling loss functions on engagement metrics.
One improvement we can do to our auditioning  ight is that we can reduce the amount of auditioning needed for head queries.
For some of the most common queries, we can stop auditioning verticals once we hit a certain number of impressions.
Although our approach works for a large number of natural metrics that we look at, it can not be used to predict session based metrics.
It works well only for any SERP level vertical metrics.
Also we won t be able to accurately predict metrics for low-coverage verticals since they have very few impressions.
