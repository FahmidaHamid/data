Twitter is a micro-blogging service that counts with millions of users from all over the world.
It allows users to post and exchange 140-character-long messages, which are also known as tweets.
Twitter is used through a wide variety of clients, from which a large portion   46% of active users1   correspond to mobile users.
Tweets can be published by sending emails, sending SMS text-messages and
 http://blog.twitter.com/2010/09/evolving-ecosystem.html Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
directly from smartphones using a wide array of Web-based services.
Therefore, Twitter facilitates real-time propagation of information to a large group of users.
This makes it an ideal environment for the dissemination of breaking-news directly from the news source and/or geographical location of events.
For instance, in an emergency situation [32], some users generate information either by providing  rst-person observations or by bringing relevant knowledge from external sources into Twitter.
In particular, information from o cial and reputable sources is considered valuable and actively sought and propagated.
From this pool of information, other users synthesize and elaborate to produce derived interpretations in a continuous process.
This process can gather,  lter, and propagate information very rapidly, but it may not be able to separate true information from false rumors.
Indeed, in [19] we observed that immediately after the 2010 earthquake in Chile, when information from o cial sources was scarce, several rumors posted and re-posted on Twitter contributed to increase the sense of chaos and insecurity in the local population.
However, we also observed that information which turned out to be false, was much more questioned than information which ended up being true.
This seems to indicate that the social network somehow tends to favor valid information, over false rumors.
Social media credibility.
The focus of our research is the credibility of information spread through social media networks.
Over 20 years ago, Fogg and Tseng [10] described credibility as a perceived quality composed of multiple dimensions.
In this paper we use credibility in the sense of believ-ability:  o ering reasonable grounds for being believed 2.
We  rst ask users to state if they consider that a certain set of messages corresponds to a newsworthy event (as opposed to being only informal conversations).
Next, for those messages considered as related to newsworthy events, we ask another group of users to state if they believe those messages are likely to be true or false.
Our main objective is to determine if we can automatically assess the level of credibility of content posted on Twitter.
Our primary hypothesis is that there are signals available in the social media environment itself that enable users to assess information credibility.
In this context we de ne social media credibility as the aspect of information credibility that can be assessed using only the information available in a social media platform.
http://www.merriam-webster.com/dictionary/credible based on supervised learning, and the  rst step is to build a dataset for studying credibility on Twitter.
We  rst extract a set of relevant discussion topics by studying bursts of activity.
Then, each topic is labeled by a group of human assessors according to whether it corresponds to a newsworthy information/event or to informal conversation.
After the dataset is created, each item of the former class is assessed on its level of credibility by another group of judges.
This is described in Section 3.
Next, we extract relevant features from each labeled topic and use them to build a classi er that attempts to automatically determine if a topic corresponds to a newsworthy infor-mation/event, and then to automatically assess its level of credibility.
This is described in Section 4.
Finally, Section 5 presents our conclusions and directions for future work.
The next section outlines previous work related to our current research.
The literature on information credibility is extensive, so in this section our coverage of it is by no means complete.
We just provide an outline of the research that is most closely related to ours.
Credibility of online news in traditional media and blogs.
The perception of users with respect to the credibility of online news seems to be positive, in general.
People trust the Internet as a news source as much as other media, with the exception of newspapers [8].
Therefore, and in part due to this, the Internet is the most important resource for news in the US among people under the age of 30, according to a survey in 2008 [23], and second only to television in the case of general audiences.
Among online news sites, blogs are considered less trustworthy than traditional news sites.
A survey in 2005 showed that, even among young people, blogs are seen as signi -cantly less trustworthy than traditional news sites [34].
An exception seem to be users with political interests, which rate the credibility of blogs sites high, particularly when they are themselves heavy blog users [14].
Twitter as a news media.
While most messages on Twitter are conversation and chatter, people also use it to share relevant information and to report news [13, 22, 21].
Indeed, the majority of  trending topics   keywords that experiment a sharp increase in frequency  can be considered  headline news or persistent news  [16].
The fact that Twitter echoes news stories from traditional media can be exploited to use Twitter, e.g.
to track epidemics [17], detect news events [28], geolocate such events [27], and  nd controversial emerging controversial topics [24].
Recently Mathioudakis and Koudas [18] described an online monitoring system to perform trend detection over the Twitter stream.
In this paper we assume that a system for trend detection exists (we use [18]) and focus on the issues related to labeling those trends or events.
Twitter has been used widely during emergency situations, such as wild res [6], hurricanes [12],  oods [32, 33, 31] and earthquakes [15, 7].
Journalists have hailed the immediacy of the service which allowed  to report breaking news quickly   in many cases, more rapidly than most mainstream media outlets  [25].
The correlation of the magnitude of real-world events and Twitter activity prompted researcher Markus Strohmaier to coin the term  Twicalli scale 3.
Credibility of news on Twitter.
In a recent user study, it was found that providing information to users about the estimated credibility of online content was very useful and valuable to them [30].
In absence of this external information, perceptions of credibility online are strongly in uenced by style-related attributes, including visual design, which are not directly related to the content itself [9].
Users also may change their perception of credibility of a blog posting depending on the (supposed) gender of the author [3].
In this light the results of the experiment described in [29] are not surprising.
In the experiment, the headline of a news item was presented to users in di erent ways, i.e. as posted in a traditional media website, as a blog, and as a post on Twitter.
Users found the same news headline signi cantly less credible when presented on Twitter.
This distrust may not be completely ungrounded.
Major search engines are starting to prominently display search results from the  real-time web  (blog and microblog postings), particularly for trending topics.
This has attracted spammers that use Twitter to attract visitors to (typically) web pages o ering products or services [4, 11, 36].
It has also increased the potential impact of orchestrated attacks that spread lies and misinformation.
Twitter is currently being used as a tool for political propaganda [20].
Misinformation can also be spread unwillingly.
For instance, on November 2010 the Twitter account of the presidential adviser for disaster management of Indonesia was hacked.4 The hacker then used the account to post a false tsunami warning.
On January 2011 rumors of a shooting in the Oxford Circus in London, spread rapidly through Twitter.
A large collection of screenshots of those tweets can be found online.5 Recently, the Truthy6 service from researchers at Indiana University, has started to collect, analyze and visualize the spread of tweets belonging to  trending topics .
Features collected from the tweets are used to compute a truthiness score for a set of tweets [26].
Those sets with low truthiness score are more likely to be part of a campaign to deceive users.
Instead, in our work we do not focus speci cally on detecting willful deception, but look for factors that can be used to automatically approximate users  perceptions of credibility.
We focus on time-sensitive information, in particular on current news events.
This section describes how we collected a set of messages related to news events from Twitter.
We use Twitter events detected by Twitter Monitor [18]7 during a 2-months period.
Twitter Monitor is an online monitoring system which detects sharp increases ( bursts ) in the frequency of sets of keywords found in messages.
http://mstrohm.wordpress.com/2010/01/15/ measuring-earthquakes-on-twitter-the-twicalli-scale/
 http://thejakartaglobe.com/home/government-disaster-\ -advisors-twitter-hacked-used-to-send-tsunami-warning/408447
 http://www.exquisitetweets.com/collection/abscond/152

 http://truthy.indiana.edu/ http://www.twittermonitor.net/ or mostly comments or conversation (labeled as class CHAT).
For each topic we also asked evaluators to provide a short descriptive sentence for the topic.
The sentence allow us to discard answers without proper justi cation, reducing the amount of click spammers in the evaluation system.
For every burst detected, Twitter Monitor provides a key-word-based query.
This query is of the form (A B) where A is a conjunction of keywords or hashtags and B is a disjunc-tion of them.
For instance, ((cinco   mayo)   (mexican   party   celebrate)) refers to the celebrations of  cinco de mayo  in Mexico.
We collected all the tweets matching the query during a 2-day window centered on the peak of every burst.
Each of these subsets of tweets corresponds to what we call a topic.
We collected over 2,500 such topics.
Some example topics are shown in Table 1.
Table 1: Example topics in April to July 2010.
A tweet on a topic must contain all of the boldfaced words and at least one of the non-boldfaced ones.
Peak Keywords News recycle, earth, save, reduce, reuse, #earthday

 notebook, movie, makes, cry, watchin, story


  ood, nashville, relief, setup, victims, pls sues, ntp, tech, patents, apple, companies Conversation

 goodnight, bed, dreams, tired, sweet, early hangover, woke, goes, worst, drink, wake In the table we have separated two broad types of topics: news and conversation, following the broad categories found in [13, 22].
The fact that conversation-type of messages can be bursty is a case of endogenous bursts of activity that occur this type of social system [5].
There are large variations on the number of tweets found in each topic.
The distribution is shown in Figure 1.
In our  nal dataset, we kept all the cases having at most 10,000 tweets, which corresponds to 99% of them.
s e s a c f o r e b m u




















 Number of tweets Figure 1: Distribution of tweets per topic.
Our  rst labeling round was intended to separate topics which spread information about a news event, from the cases which correspond to personal opinions and chat.
In other words, we separate messages that are of potential interest to a broad set of people, from conversations that are of little importance outside a reduced circle of friends [2].
For this task we used Mechanical Turk8, where we asked evaluators to assist us.
We showed evaluators a sample of 10 tweets in each topic and the list of keywords provided by Twitter Monitor, and asked if most of the messages were
 http://www.mturk.com Figure 2: User interface for labeling newsworthy topics.
As shown in Figure 3.2, we provided guidelines and examples of each class.
NEWS was described as statements about a fact or an actual event of interest to others, not only to the friends of the author of each message.
CHAT was described as messages purely based on personal/subjective opinions and/or conversations/exchanges among friends.
Randomly we selected 383 topics from the Twitter Monitor collection to be evaluated using Mechanical Turk.
We grouped topics at random, in sets of 3, for each task (called  human intelligence task  or HIT in Mechanical Turk jargon).
During ten days evaluators were asked to assess HITs, and we asked for 7 di erent evaluators for each HIT.
Evaluations that did not provide the short descriptive sentence were discarded.
A class label for a topic was assigned if 5 out of 7 eval-uators agreed on the label.
In another case we label the instance as UNSURE.
Using this procedure, 35.6% of the topics (136 cases) were labeled as UNSURE, due to insu cient agreement.
The percentage of cases labeled as NEWS was

 Next we focus on the credibility assessment task.
To do this, we ran an event supervised classi er over the collection of 2,524 cases detected by Twitter Monitor.
We will discuss details of this classi er in Section 4.
Our classi er labels a total of 747 cases as NEWS.
Using this collection of instances, levels for each case.
For each one we provided a sample of 10 tweets followed by a short descriptive sentence that help them to understand the topic behind those tweets.
In this evaluation we considered four levels of credibility: (i) almost certainly true, (ii) likely to be false, (iii) almost certainly false, and (iv)  I can t decide .
We asked also eval-uators to provide a short sentence to justify their answers, and we discarded evaluations lacking that justi cation sentence.
An example of this task is shown in Figure 3.3.
We asked for 7 di erent assessments for each HIT.
Labels for each topic were decided by majority, requiring agreement of at least 5 evaluators.
Figure 3: User interface for assessing credibility.
In a preliminary round of evaluation, almost all of the cases where labeled as  likely to be true , which turned out to be a very general statement and hence useless for our purposes.
Hence, we removed the  likely to be true  option, forcing the evaluators to choose one of the others.
The percentage of cases identi ed as  almost certainly true  was 41% (306 cases),  likely to be false  accounted for 31.8% (237 cases),  almost certainly false  accounted only for 8.6% (65 cases), while 18.6% (139 cases) were considered uncertain by evaluators, labeling these cases as  ambiguous .
On this section we discuss how, given a stream of messages associated to certain topics, we can automatically determine which topics are newsworthy, and then automatically assign to each newsworthy topic a credibility label.
Our main hypothesis is that the level of credibility of information disseminated through social media can be estimated automatically.
We believe that there are several factors that can be observed in the social media platform itself, and that are useful to asses information credibility.
These factors include:  the reactions that certain topics generate and the emotion conveyed by users discussing the topic: e.g.
if they use opinion expressions that represent positive or negative sentiments about the topic;   the level of certainty of users propagating the infor-if they question the information that is mation: e.g.
given to them, or not;   the external sources cited: e.g.
if they cite a speci c URL with the information they are propagating, and if that source is a popular domain or not;   characteristics of the users that propagate the information, e.g.
the number of followers that each user has in the platform.
We propose a set of features to characterize each topic in our collections.
These include some features speci c to the Twitter platform, but most are quite generic and can be applied to other environments.
Many of the features follow previous works including [1, 2, 12, 26].
Our feature set is listed in Table 2.
We identify four types of features depending on their scope: message-based features, user-based features, topic-based features, and propa-gation-based features.
Message-based features consider characteristics of messages, these features can be Twitter-independent or Twitter-dependent.
Twitter-independent features include: the length of a message, whether or not the text contains exclamation or question marks and the number of positive/negative sentiment words in a message.
Twitter-dependent features include features such as: if the tweet contains a hashtag, and if the message is a re-tweet.
User-based features consider characteristics of the users which post messages, such as: registration age, number of followers, number of followees ( friends  in Twitter), and the number of tweets the user has authored in the past.
Topic-based features are aggregates computed from the previous two feature sets; for example, the fraction of tweets that contain URLs, the fraction of tweets with hashtags and the fraction of sentiment positive and negative in a set.
Propagation-based features consider characteristics related to the propagation tree that can be built from the re-tweets of a message.
These includes features such as the depth of the re-tweet tree, or the number of initial tweets of a topic (it has been observed that this in uences the impact of a message, e.g.
in [35]).
We trained a supervised classi er to determine if a set of tweets describes a newsworthy event.
Labels given by Mechanical Turk evaluators were used to conduct the supervised training phase.
We trained a classi er considering the three classes but performing a cost-sensitive learning process, increasing the relevance for the prediction of instances in the NEWS class.
We considered a cost matrix into account during the training process ignoring costs at prediction time.
We built a cost-sensitive tree, weighting training instances according to the relative cost of the two kinds of error, false positives and false negatives.
The cost matrix weighted misclassi cations containing the NEWS class as 1.0, Scope Feature Description Msg.
User REGISTRATION AGE





 Topic COUNT TWEETS

































 Prop.
PROPAGATION INITIAL TWEETS



 Length of the text of the tweet, in characters .
.
.
in number of words Contains a question mark   ?  .
.
.
an exclamation mark   !  .
.
.
multiple question or exclamation marks .
.
.
a  smiling  emoticon e.g.
:-) ;-) .
.
.
.
.
.
a  frowning  emoticon e.g.
:-( ;-( .
.
.
.
.
.
a personal pronoun in 1st, 2nd, or 3rd person.
(3 features) Fraction of capital letters in the tweet Number of URLs contained on a tweet Contains a URL whose domain is one of the 100 most popular ones .
.
.
one of the 1,000 most popular ones .
.
.
one of the 10,000 most popular ones Mentions a user: e.g.
@cnnbrk Includes a hashtag: e.g.
#followfriday .
.
.
a stock symbol: e.g.
$APPL Is a re-tweet: contains  RT   The day of the week in which this tweet was written The number of positive words in the text .
.
.
negative words in the text Sum of  0.5 for weak positive/negative words,  1.0 for strong ones The time passed since the author registered his/her account, in days The number of tweets at posting time Number of people following this author at posting time Number of people this author is following at posting time 1.0 i  the author has a  veri ed  account .
.
.
a nonempty  bio  at posting time .
.
.
a nonempty homepage URL at posting time Number of tweets Average length of a tweet The fraction of tweets containing a question mark   ?  .
.
.
an exclamation mark   !  .
.
.
multiple question or exclamation marks .
.
.
emoticons smiling or frowning (2 features) .
.
.
a personal pronoun in 1st, 2nd, or 3rd person.
(3 features) .
.
.
more than 30\% of characters in uppercase The fraction of tweets containing a URL .
.
.
user mentions .
.
.
hashtags .
.
.
stock symbols The fraction of tweets that are re-tweets The average sentiment score of tweets The fraction of tweets with a positive score .
.
.
with a negative score The fraction of tweets with a URL in one of the top-100 domains .
.
.
in one of the top-1,000 domains .
.
.
in one of the top-10,000 domains The number of distinct URLs found after expanding short URLs The fraction of occurrences of the most frequent expanded URL The number of distinct short URLs The number of distinct hashtags The fraction of occurrences of the most frequent hashtag The number of distinct users mentioned in the tweets The fraction of user mentions of the most frequently mentioned user The number of distinct authors of tweets The fraction of tweets authored by the most frequent author The average of AUTHOR REGISTRATION AGE The average of AUTHOR STATUSES COUNT .
.
.
of AUTHOR COUNT FOLLOWERS .
.
.
of AUTHOR COUNT FRIENDS The fraction of tweets from veri ed authors .
.
.
from authors with a description .
.
.
from authors with a homepage URL The degree of the root in a propagation tree The total number of tweets in the largest sub-tree of the root, plus one The maximum and average degree of a node that is not the root (2 feat.)
The depth of a propagation tree (0=empty tree, 1=only initial tweets, 2=only re-tweets of the root) and its per-node average (2 features) The max.
size of a level in the propagation tree (except children of root) and misclassi cations involving only the CHAT and UNSURE classes as 0.5.
We also used a bootstrapping strategy over the training dataset.
A random sample of the dataset was obtained using sampling with replacement considering a uniform distribution for the probability of extracting an instance across the three classes.
A sample size was de ned to determine the size of the output dataset.
We perform bootstrapping over the dataset with a sample size percentage equals to 300% Correctly Classi ed Instances Kappa statistic Mean absolute error Root mean squared error Relative absolute error Root relative squared error





 Table 4: Results for the classi cation of newsworthy topics.
Class


 W. Avg.
TP Rate FP Rate Prec.
Recall




















 and feature normalization.
We perform also a 3-fold cross validation strategy.
We tried a number of learning schemes including SVM, decision trees, decision rules, and Bayes networks.
Results across these techniques were comparable, being best results achieved by a J48 decision tree method.
A summary of the results obtained using the J48 learning algorithm is shown in Table 3.
The supervised classi er achieves an accuracy equal to 89 %.
The Kappa statistic indicates that the predictability of our classi er is signi cantly better than a random predictor.
The details of the evaluation per class are shown in Table 4.
As we can observe, the classi er obtains very good results for the prediction of NEWS instances, achieving the best TP rate and FP rate across the classes.
An F-measure equivalent to a 92% illustrate that specially for this class the clas-si er obtains a good balance for the precision-recall tradeo .
Before performing the automatic assessment of credibility, we analyze the distribution of features values.
To do this we perform a best-feature selection process over the 747 cases of the NEWS collection, according to the labels provided by the credibility task.
We used a best rst selection method which starts with the empty set of attributes and searches forward.
The method selected 15 features, listed in Table 5.
Table 5: Best features selected using a best  rst attribute selection strategy.
Min Max Mean StdDev



















 -2




























 -0.038























 As Table 5 shows, the  rst four features consider characteristics of users such as how long they have been Twitter users, the number of tweets that they have written at the posting time, and the number of followers/friends that they have in the platform.
The next ten features are aggregated features computed from the set of tweets of each news event.
Notice that features based on sentiment analysis are very relevant for this collection.
Other relevant features consider if the message includes a URL, a user mention, or a question mark.
The last feature considers information extracted from the propagation tree that is built from the re-tweets.
To illustrate the discriminative capacity of these features we deploy box plots for each of them.
In this analysis we distinguish between cases that correspond to the  almost certainly true  class (labeled as class A), and the  likely to be false  and  almost certainly false  (labeled as class B).
We exclude from the analysis cases labeled as  ambiguous .
The box plots are shown in Figure 4.
As Figure 4 shows, several features exhibit a signi cant di erence between both classes.
More active users tend to spread more credible information, as well as users with newer user accounts but with many followers and followees.
Sentiment based features are also very relevant for the credibility prediction task.
Notice that in general tweets which exhibit sentiment terms are more related to non-credible information.
In particular this is very related to the fraction of tweets with positive sentiments, as opposed to negative sentiments, which tend to be more related to credible information.
Tweets which exhibit question marks or smiling emoticons tend also to be more related to non-credible information.
Something similar occurs when a signi cant fraction of tweets mention a user.
On the other hand, tweets having many re-tweets on one level of the propagation tree, are considered more credible.
We trained a supervised classi er to predict credibility levels on Twitter events.
To do this we focus the problem on the detection of news that are believed to be almost certainly true (class A), against the rest of news (class B), excluding topics labeled as  ambiguous .
In total, 306 cases correspond to class A and 302 cases correspond to class B, achieving a data balance equivalent to 50.3 / 49.7.
With this balanced output we can evaluate the predictability of the credibility data.
We tried a number of learning algorithms with best results achieved by a J48 decision tree.
For the training/validation process we perform a 3-fold cross validation strategy.
A summary of the classi er is shown in Table 6.
Table 6: Summary for the credibility classi cation.
Correctly Classi ed Instances Kappa statistic Mean absolute error Root mean squared error Relative absolute error Root relative squared error





 As Table 6 shows, the supervised classi er achieves an accuracy of 86 %.
The Kappa statistic indicates that the predictability of our classi er is signi cantly better than a random predictor.
The details of the evaluation per class are shown in Table 7.
The performance for both classes is similar.
The F1 is high, indicating a good balance bet-




































































.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
  
 .
.
.
.
.
.
Figure 4: Box plots depicting the distribution for classes A ( true ) and B ( false ) of each of the top 15 features.
Table 7: Results for the credibility classi cation.
Class A ( true ) B ( false ) W. Avg.
TP Rate FP Rate Prec.
Recall












 ween precision and recall.
The last row of Table 7 shows the weighted averaged performance results calculated across both classes.
Best features.
To illustrate the top features for this task, we analyze which features were the most important for the J48 decision tree, according to the GINI split criteria.
The decision tree is shown in Figure 5.
As the decision tree shows, the top features for this task were the following:   Topic-based features: the fraction of tweets having an URL is the root of the tree.
Sentiment-based features like fraction of negative sentiment or fraction of tweets with an exclamation mark correspond to the following relevant features, very close to the root.
In particular we can observe two very simple classi cation rules, tweets which do not include URLs tend to be related to non-credible news.
On the other hand, tweets which include negative sentiment terms are related to credible news.
Something similar occurs when people use positive sentiment terms: a low fraction of tweets with Figure 5: Decision tree built for the credibility classi cation.
(A =  true , B =  false ).
credible news.
  User-based features: these collection of features is very relevant for this task.
Notice that low credible news are mostly propagated by users who have not written many messages in the past.
The number of friends is also a feature that is very close to the root.
  Propagation-based features: the maximum level size of the RT tree is also a relevant feature for this task.
Tweets with many re-tweets are related to credible news.
These results show that textual information is very relevant for this task.
Opinions or subjective expressions describe people s sentiments or perceptions about a given topic or event.
Opinions are also important for this task that allow to detect the community perception about the credibility of an event.
On the other hand, user-based features are indicators of the reputation of the users.
Messages propagated trough credible users (active users with a signi cant number of connections) are seen as highly credible.
Thus, those users tend to propagate credible news suggesting that the Twitter community works like a social  lter.
In this section we study how speci c subsets of features perform for the task of automatic assessment of credibility.
To do this we train learning algorithms considering subsets of features.
We consider 4 subsets of features grouped as follows:  Text subset: considers characteristics of the text of the messages.
This includes the average length of the tweets, the sentiment-based features, the features related to URLs, and those related to counting elements such as hashtags, user mentions, etc.
This subset contains 20 features.
  Network subset: considers characteristics of the social network of users.
This subset includes features related to the authors of messages, including their number of friends and their number of followers.
This subset contains 7 features.
  Propagation subset: considers the propagation-based features plus the fraction of re-tweets and the total number of tweets.
This subset contains 6 features.
  Top-element subset: considers the fraction of tweets that respectively contain the most frequent URL, hashtag, user mention, or author: 4 features in total.
We train a J48 decision tree with each subset feature as a training set.
The instances en each group were splitted using a 3-fold cross validation strategy, as in the previous experiments.
Best features.
In Table 8 we show with boldface the best results for each metric and class.
These results indicate that among the features, the propagation subset and the top-element subset are very relevant for assessing credibility.
We observe that text and author-based features are not enough by themselves for this task.
Regarding non-credible news, high true positive rates are achieved using propagation features which indicate that graph patterns are very relevant to detect them.
On the other hand, credible news are in general more di cult to detect.
The top-element subset of features achieves the best results for this class indicating that social patterns measured through these features are very useful for this class.
Table 8: Experimental results obtained for the classi cation of credibility cases.
The training step was conducted using four di erent subsets of features.
Text subset TP Rate FP Rate Prec.
Recall





















 Class

 W. Avg.
W. Avg.
W. Avg.
W. Avg.
Network subset








 Propagation subset











 Top-element subset

















 To illustrate the dependence among these features according to the credibility prediction task, we calculate scatter plots for each feature pair considered in this phase.
We show these plots in Figure 6.
As Figure 6 shows, most feature-pairs present low correlation, showing that the linear dependence between pairs of features is very weak.
Something di erent occurs when sentiment-based features are analyzed, showing dependences among them.
Regarding the class distribution, we can observe that every pair shows good separation properties, a fact that explains our results in credibility assessment.
Users online, lack the clues that they have in the real world to asses the credibility of the information to which they are exposed.
This is even more evident in the case of inexperienced users, which can be easily mislead by unreliable information.
As microblogging gains more signi cance as a valid news resource, in particular during emergency situations and important events, it becomes critical to provide tools to validate the credibility of online information.
On this paper, we have shown that for messages about time-sensitive topics, we can separate automatically newsworthy topics from other types of conversations.
Among several other features, newsworthy topics tend to include URLs and to have deep propagation trees.
We also show that we can assess automatically the level of social media credibility of newsworthy topics.
Among several other features, credible news are propagated through authors that have previously written a large number of messages, originate at a single or a few users in the network, and have many re-posts.
For future work, we plan to extend the experiments to larger datasets, to partial datasets (e.g.
only the  rst tweets posted on each topic), and to explore more deeply other factors that may lead users to declare a topic as credible.
There are interesting open problems in this area, including studying the impact of the target pages pointed to by the URLs, or the impact of other factors of context that are displayed and non-credible information, respectively.
each row represents a feature, from top to bottom: registration age, statuses count, number of followers, number of friends, tweets with URLs, sentiment score, positive sentiment, negative sentiment, shared URLs, shared author, tweets user mention, tweets with question marks, tweets with emoticon smiles, tweets with  rst pronoun, and max RT tree level size.
The order in the columns goes from right to left.
in Twitter (e.g.
the number of followers of each poster, the avatar used, etc.)
on the assessments of credibility users do.
Acknowledgments.
We would like to thank Michael Mathiou-dakis and Nick Koudas for lending us assistance to use the Twitter Monitor event stream.
Carlos Castillo was partially supported by the Spanish Centre for the Development of Industrial Technology under the CENIT program, project CEN-20101037,  Social Media  (http://cenitsocialmedia.es/).
