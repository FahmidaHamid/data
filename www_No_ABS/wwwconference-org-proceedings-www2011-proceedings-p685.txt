Online video chat services have become increasingly popular, and include such systems as Chatroulette [9], BlurryPeo-ple [5], RandomDorm [34], Omegle [31], vChatter [24], etc.
While most of these services have been introduced only recently (less than one year ago), statistics show that membership in for example Chatroulette has grown by 500% since the beginning of 2010.
Furthermore, tens of thousands of users are online in Chatroulette at any point of time, 24 hours a day.
In July 2010, 1.3 million US users [22] and 6.3 million users in total [39] are estimated to have visited Chatroulette.
A common feature of such online video chat services is that they randomly match pairs of users via video connections.
In the most popular cases, users are anonymous from each other, and need not supply any overt information to the Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
system or other users that may identify them, such as their name or pro le.
Users of such systems can quickly move on to the next random pairing by clicking  Next .
In addition, such services are typically o ered for free, and are easy to use, which enhances their popularity.
A key problem encountered in such anonymous video chat systems is that many of the users may engage in sexually explicit behavior such as  ashing/revealing themselves with full frontal nudity.
This misbehavior drives away other users that are not interested in viewing such obscene content, thus limiting such systems from achieving their full popularity.
In addition,  asher-type misbehavior has potential legal rami- cations since many of the viewers attracted to such video chat systems are minors [14].
Our observations on a typical Saturday night indicate that as many as 20-30% of Cha-troulette users are minors.
Even though users must con rm that they are at least 16 years old and agree not to broadcast obscene, o ending or pornographic material, it is nearly impossible to enforce this due to the lack of login or registration requirements of such anonymous systems.
One approach to deal with misbehavior is to de-anonymize users, on the hypothesis that requiring users to reveal their identities will then force users to behave.
For example, vchatter uses such an approach on Facebook, where only Facebook users may participate in its Facebook application, and the Facebook name of users in the video chat are displayed during each video pairing session.
This approach of using de-anonymization to address misbehaving users has several drawbacks.
First, it undercuts one of the primary motivations that attracts users to the most popular roulette-type video services, namely that people can participate anonymously for fun.
Second,  ashers can circumvent this approach by registering false/dummy pro les.
An alternative approach to dealing with misbehavior is to analyze the video sessions to detect obscene content, and thus  ag misbehaving users.
Some video chat sites have taken manual crowd-sourcing to detect misbehaving users, i.e. images/screen captures are sent to real humans who are paid to manually identify whether the user is a  asher or not [9].
Such approaches incur a high economic cost and thus are not scalable.
Moreover, such approaches are not necessarily applied uniformly, i.e. only images that are  reported  (Chatroulette for example has a  Report  button to  ag misbehaving users) are actually inspected.
Our analysis indicates that not all misbehaving users are reported, and some users who are not misbehaving are still reported as such by pranksters.
A more promising and scalable approach is automated detection of misbehaving users, by employing image recognition algorithms.
In particular, we investigate a novel solu-an array of evidences that image recognition algorithms re ne.
To the best of our knowledge, this is the  rst work investigating this problem space.
This paper claims the following key contributions:   we identify and analyze the issues introduced by online video chat systems, present statistical studies of its users, and demonstrate that existing pornography detection algorithms don t work well in this problem space.
  we describe a holistic solution to detect  ashers and obscene content that (1) adapts this problem space to a set of evidence that image recognition algorithms re ne, including evidence of a face, eyes and nose, etc., (2) introduce our own novel motion-based skin detector to supplement classi cation of misbehaving users, and (3) we fuse these individual evidences by building a probabilistic model using Dempster-Shafer Theory (DST) [36] to obtain a joint classi cation of each user as misbehaving or not.
  we evaluate the e ectiveness of our fused classi er system in terms of its precision and recall by employing real world data sets obtained from Chatroulette.com.
The research reported in this work is in close collaboration with Chatroulette.
We thank Andrey Ternovskiy, the founder of Chatroulette.com, for providing us with these otherwise unobtainable internal data traces.
The observations, algorithm design and system evaluations are carried out based on the dataset.
It is worth noting that though we use the Chatroulette system as a running example in this paper, our approach is generally applicable to all roulette-type video chat services.
In the following, we describe related work, present a statistical analysis of users using the Chatroulette dataset, explain the limitations of existing techniques, and glean key observations from the user data.
We then describe the architecture of our fused classi er system - SafeVChat, including the design requirements, and individual system components.
Next, we describe our motion-based skin detection algorithm.
We then detail the probabilistic model used to fuse together the results of the individual classi ers leveraging Dempster-Shafer Theory.
Finally, we provide a rigorous evaluation of the e ectiveness of our joint classi cation system using the data traces obtained from within Chatroulette.com.
Content-based image analysis has been an active research area for over a decade (see [12, 28] for some surveys).
In particular, skin-color modeling and skin detection techniques have been successfully used for detecting pornographic content with high accuracy in Internet videos [25, 19, 27, 7, 6,
 icantly for di erent illumination, background, camera characteristics, and ethnicity, which are particularly true in online video chat images.
A recent survey study [26] concludes that skin detection methods may only be used as a prepro-cessor for obscene content detection, and other content types such as text [19] and motion analysis [21] may be incorporated to improve accuracy.
In this paper, we propose a novel, motion-based skin detection method that analyzes several consecutive video frames.
This method achieves much better recall and precision than PicBlock, a widely-used pornographic image blocker [32].
Figure 1: Categorization and percentage of di erent types of users in the Chatroulette dataset.
Besides skin detection, our work also leverages an array of image recognition techniques, including face, nose, eye, mouth, and upper body detectors, which are supported in the latest version of OpenCV library [20].
An extensive survey of these techniques is outside the scope of this paper.
We focus on adapting these techniques to our online video chat images and identifying the strengths and limitations of each individual technique.
Such information is then utilized in the fusion process.
A number of (ensemble) methods have been proposed to combine or fuse multiple classi ers in order to reach a  -nal classi cation decision.
Two representative methods are Dempster-Shafer Theory (DST) [36] and AdaBoost [15].
Compared to AdaBoost, DST supports multilevel decisions, and further considers the reliability of the evidence provided by each classi er.
DST has been applied in a variety of classi cation settings, such as [29, 3, 10, 30].
The key challenge of using DST is to de ne the appropriate mass function (or basic probability) assignment, which is dependent on speci c applications.
One of the main contributions of this work is how we design mass function and apply DST to detecting obscene content in online video chat services.
In this section, we analyze a real-world dataset obtained from Chatroulette.
After an initial statistical analysis of this dataset, we then investigate a state-of-the-art commercial software for pornography detection.
We identify the limitations of this technique and present the key observations derived from our dataset with regard to obscene content and misbehaving users.
We have obtained a real-world dataset, provided by the founder of Chatroulette Andrey Ternovskiy, containing screenshot images from 20,000 users.
Each screenshot image has a
 we categorized these 20,000 users into two types: o ensive and normal users.
O ensive or misbehaving users are de- ned based on the following criteria.
If a user broadcasts obscene content or behaviors (o ensive content), or intentionally shows his/her naked chest without face in front of the webcam (potential o ensive content), or broadcasts pornographic advertisements (advertisement content), then the user is de ned as an o ensive user.
On the contrary, normal users are chatters who stay fully clothed.
A majority of normal users show their faces in front of webcams (we call the content that these users broadcast as normal content) and some others point their webcams to their clothed chest normal content).
In addition to these two types of content, there is a special type of normal users who completely block their webcams, or point their webcams to a static scene (e.g.
a room corner or ceiling), or broadcast pre-prepared interesting videos etc.
(we call the content that these normal users broadcast as other content).
Figure 1 (original dataset) shows the categorization and distribution of di erent types of users and content in the Chatroulette dataset.
Since Cha-troulette took screenshots for all the users at approximately the same time and randomly selected 20,000 users as well as their screenshot images, we believe the Chatroulette dataset is representative of Chatroulette users  characteristics.
Existing techniques for detecting objectionable content are mostly content-based, which typically utilize both image recognition and text analysis techniques to  lter out pornographic webpages.
Di erent from detecting pornographic webpages, detecting objectionable content in the context of online video chat systems cannot rely on text message analysis, since the content communicated between chatters does not pass through the central server and obtaining text messages that were exchanged is not practical.
Furthermore, some chatters may have a conversation using audio devices instead of typing messages.
Consequently, we focus on investigating whether state-of-the-art image recognition techniques, which are used for pornographic website detection, can be applied to objectionable content detection in the context of online video chat systems.
Speci cally, we used PicBlock 4.2.3 (Pornographic Image Blocker), a state-of-the-art commercial software [32], to classify 1,000 user screenshot images which were randomly selected from the Chatroulette dataset.
Surprisingly, even though PicBlock usually achieves high accuracy when detecting pornographic images on websites, it performed poorly on the screenshot images of online video chat users   the precision and recall for correctly detecting misbehaving users were only 0.253 and 0.390, respectively.
This is mainly due to the large diversity in illumination, sensing quality of web cameras used by di erent chatters, ethnicity (African, Asian, Caucasian and Hispanic groups), and variations in individual user characteristics such as age, sex, what is prominently displayed in the image, and so on.
Other factors such as appearances (makeup, hairstyle and glasses etc.
), backlighting, shadows and motion also have signi cant in uence on skin-color appearance.
Indeed, these issues have also been investigated in a recent survey study [26], which concluded that the skin detection methodology can only be used as a preprocessor for obscene content detection.
Using the Chatroulette dataset, we have conducted further analysis and identi ed some discriminative characteristics that are speci c in online video chat services.
(1) Misbehaving users on online video chat systems usually hide their faces because they are not willing to compromise their identity privacy.
(2) Di erent from regular pornographic images, misbehaving users may not completely expose themselves.
For example, some misbehaving users may only expose their genitals in front of the webcam and stay partially clothed.
(3) Chatters who present their faces in front of webcams are mostly normal users because showing both the body trunk and the face of a user requires the user placing his/her webcam far from the user, but chatters who do not show their faces may not be  ashers.
(4) A fair amount of chatters do (a) (b) (c) Figure 2: A few screenshot images from Chatroulette.
not show their faces clearly.
For example, only a partial face is presented in front of the webcam.
(5) Webcams are usually set up in a stable way, i.e., a majority of chatters do not keep moving or adjusting their webcams.
In the following sections, we take advantage of these observations to design our obscene content detection system.
The primary goal of our system is to detect users who abuse online video chat services, namely those who display obscene content.
In this section, we  rst discuss the system design requirements and assumptions about Chatroulette s capabilities in terms of providing our system with data for analysis purposes.
Following that, we describe the architecture of our SafeVchat system.
To design our obscene content detection system, there are several important requirements.
First, in terms of correctly classifying a misbehaving user, the precision should be high because in a system such as Chatroulette, all users classi- ed as misbehaving will be subject to further costly review by crowdsourcing.
We therefore want to be precise in only classifying users as misbehaving who are truly misbehaving, as any false positives incur an additional economic cost.
We also want our recall to be high, since if we miss a misbehaving user via a false negative, these users will appear in the chat system.
The resulting cost is twofold: adult users will be put o  by the obscene content and may leave the system; further, minors would be subject to inappropriate content.
In an online video chat system, for system scalability, the video stream is transmitted in a peer-to-peer manner after a pair of users have been matched by the server.
It is prohibitively expensive to obtain and monitor users  complete video content from a centralized server.
To detect o ensive content, the Chatroulette server is however able to partially obtain a user s video content by periodically taking sequential screenshot images from the user.
The sequential screenshot images are taken at a prede ned  xed time interval.
Currently, in one period, three screenshot images are taken.
Therefore, our obscene content detection system is designed based on the assumption that systems like Chatroulette are capable of providing this capacity.
Our key approach is to use evidence from multiple clas-si ers to strengthen the overall classi cation of whether a user is misbehaving or not.
We consider the following types of evidence while designing our detection system, namely evidence of the presence of face, eyes, nose and mouth, upper body evidence and skin exposure evidence.
Many of the individual classi ers that we will use are based on detecting a human face, since our observations from Section 3 indicate that misbehaving users hide their faces, while chatters who reveal their faces are typically normal users.
However, user.
Otherwise, these screenshot images will be passed to the facial detectors, upper body detector and skin exposure detector.
These detectors use their respective classi ers to re ne evidence from the three screenshot images and pass their re ned evidences to the fusion component.
The fusion component harnesses the re ned evidences to make a probabilistic decision and output how likely the user is classi ed as a misbehaving user.
We mentioned earlier that existing color-based skin detection techniques do not work well for the screenshot images of online video chat users.
Speci cally, we tested two existing methods: the Adaptive real-time skin detector from OpenCV [11, 20], and PicBlock, a commercial software for pornography detection.
Neither method performed well on our video chat dataset (see Section 7 for details).
This is mainly due to the diversity of skin color appearance (e.g., illumination, webcam characteristics) and skin-color like ob-jects/backgrounds (e.g., yellowish sofa or white wall with yellowish lighting).
As a result, real user skin may not be detected, while non-skin objects/backgrounds may be mis-classi ed as skin leading to low  asher detection accuracy in online video chat services.
We also observe that in online video chat sessions, the moving parts in images are usually the region of interest   e.g., normal users moving their heads and clothed body parts, or  ashers moving their naked body parts or touch naked parts with hand.
Users with larger  non-face skin  exposure in such  target regions  are more likely to be  ashers.
The adaptive skin detector does consider motion in images, but its skin color palette does not capture the diversity of skin colors in online video chat images.
In addition, it uses the optical  ow method, which assumes only small motion in video frames.
This assumption may not hold in online video chat services with large number of concurrent users   capturing high frequency video frames is infeasible and screenshots can only be captured with larger time intervals (e.g., 10 seconds in our dataset).
Therefore, user movement between two consecutive screenshots can be signi cant.
In this work, we propose a novel motion-based skin detection method to detect obscene content and misbehaving users in online video chat systems.
Our method consists of four major components: (1) calculates the target map (which contains the target region) via motion in consecutive screenshots; (2) uses a new skin detector with di erent skin color palettes to detect  non-face skin  in the target region; (3) calculates the skin proportion, which is the ratio of non-face skin area to target region; and (4) determines the misbehaving probability based on the skin proportion.
Given two consecutive screenshots of a user, we de ne the target map to identify changes (motion).
This is achieved through image subtraction, i.e., subtracting the pixel values at corresponding positions of the two images.
For example, if a normal user moves his/her hand against the background wall, both the hand and the wall are included in the target region by image subtraction, and the target region contains both skin (i.e. hand) and non-skin (i.e. wall).
If a  asher touches his/her naked part with hand, the target region contains mostly skin (i.e. hand and naked parts).
Therefore, via image subtraction, target map captures the region we are interested in for better detection accuracy.
Figure 3: SafeVchat: System architecture for detec-ing misbehaving users in online video chat services.
a face detector needs to be augmented by additional facial evidences, since many of the following scenarios may occur based on our observations:   In the process of taking a screenshot for a user, the user s face might be blocked by the user s hands.
Thus this face evidence alone is not able to be re ned by our system.
(See Figure 2(a)).
  A user may intentionally show his face partially and our system can only re ne partial facial evidences.
For example, Figure 2(b) shows that only eyes are present in the screenshot image.
  Even though we use the state-of-the-art face detector to re ne face evidence, there is still the possibility that the face cannot be detected (See Figure 2(c)).
Figure 3 shows the architecture of our SafeVchat system.
The system contains  ve main components including a darkness and static scene  lter, facial detectors, an upper body detector, a skin exposure detector (motioned-based skin detector) and a fusion component.
The darkness and static scene  lter is used for identifying users who operate their webcams in the dark or point their webcams to a room corner.
The remaining users are subject to fused classi cation.
Facial detectors contain a set of individual classi ers - face, eye, nose and mouth classi ers.
The upper body detector is used for identifying whether an upper body is present.
The classi ers that our system uses for facial and upper-body detection are provided by the OpenCV library and their outputs are all binary values to indicate presence or absence.
The motion-based skin detector is a motion and skin color based classi er which we designed for determining the probability that a user is a misbehaving chatter.
Finally, the fusion component of SafeVchat is used for combining evidences from facial detectors, the upper body detector and the motion-based skin detector to re ne and make a  nal probabilistic decision.
The fusion component is based on Dempster-Shafer Theory, and will be discussed in detail in Section 6.
The work  ow of our system is as follows: When the three sequential screenshot images of a user are fed into our detection system from Chatroulette, the darkness and static scene detector  rst determines whether the user is using his or her webcam in the dark or point his or her webcam to a Image B Target Map T

 Normal User
 Misbehaving User
 Figure 4: Example of normal and misbehaving users.
For each user, A and B are two screenshot images of the user; T is the target map; A + T and B + T are original images overlaid with target region (white), non-face skin (red), and the detected face (green).
Skin proportion (SP ) are 0.143 and 0.941 respectively, thus a good indicator for di erentiating normal and misbehaving users.
To avoid the noise introduced by individual pixels, we calculate the di erence between tiles, and each tile is a rectangle containing multiple pixels.
Speci cally, each image is divided into N   N tiles (N is an integer).
For each tile Trc (r, c   [1, N ]), let xi(i   [1, n]) be one of the n pixels in that tile, we calculate the tile s average intensity of RGB channels as follows: T rc =  n i=1(Rxi + Gxi + Bxi)/(n   3), (1) where Rxi , Gxi , Bxi are the R, G, B color channel intensities of pixel xi, respectively.
Therefore, the target map of any two given images contains N  N elements, each representing the di erence between two corresponding tiles  average RGB intensities.
An element is set to 1 if the absolute di erence is above the threshold, and 0 otherwise.
The threshold (set to 9 in our experiemts) is determined based on the average di erence of manually picked static images (i.e., no movement).
Target maps are further improved by  lling holes and removing glitches via morphological  lter (e.g., erosion and dilation operation) [13].
See examples in Figure 4.
For each user, we can obtain multiple screenshots, resulting in multiple target maps.
To select the best target map, we consider the size of the target region.
Most video chat users keep a relatively stable pose and move only part of their body, such as head, hand or lower body.
Thus a good target map should contain a target region that is large enough to be a body part while most part of the map is stable or static.
Let T Amin be the area of the smallest possible body part, which is derived from the training data and is set to 10% of image size in our experiments.
We select the best target map for each user as follows: (a) if there are target maps with target region bigger than T Amin, select the one with the smallest target region; (b) if all target maps  target regions are smaller than T Amin, select the target map with the largest target region.
Next, we detect exposed skin in the target region.
Unlike typical pornographic or commercial images, skin color in online video chat images varies in a wide range due to diverse illumination conditions.
Skin color does not always appear yellow or orange, and actually falls out of this range most of the time.
Under di erent lighting, angle, and re ection from computer screen, skin color could appear pink, brown, blue, or even green in video chat images.
To address this problem, we consider three di erent skin color palettes.
Palette 1 is directly derived from the adaptive real-time skin detector in OpenCV, which could identify most yellow and orange skin.
Palette 2 adds the  pinkish  color to Palette 1 to cover most Caucasian skin colors under white lighting.
This is achieved by converting RGB images to the HSV color space and detecting pixels with a Hue value in the range of [0, 60] and [300, 360].
Palette 3 focuses on the skin color of  ashers, which usually has darker illumination.
This is derived from a training dataset that contains only  ashers with manually marked skin area.
Figure 5 shows the skin proportion (SP ) (Section 5.3) of normal and misbehaving users under these three skin color palettes.
An ideal skin palette should result in low SP values for normal users and high SP values for misbehaving users.
As shown in the  gure, Palette 1 performs well for normal users (shown in green); Palette 3 performs well for misbehaving users (shown in red); and Palette 2 s performance is in the middle.
Since no single palette is perfect, we propose an approach combining the three palettes, which will be discussed in Section 5.4.
To distinguish normal users from misbehaving users, we de ne the Skin Proportion (SP ) as follows: SP = area of non-face skin/area of target region (2) Face skin area is determined using the frontal face detector in OpenCV.
If no face is detected, all the detected skin is non-face skin.
If there is a face, then non-face skin refers to all the skin below jaw of detected face.
For each user, we select the best target map (Section 5.1), and compute two SP values, one for each original image.
Since a larger SP value represents higher probability of being a  asher, to ensure that we identify all possible  ashers, we choose the larger SP value as the  nal SP value for each user.
Let SP (x) be the skin proportion value of a given user x, we need to determine the probability p(x) of user x being a misbehaving user.
Since a user is either misbehaving or not, the distribution is binomial.
A standard linear regression model which assumes normal distribution of the dependent variable is not appropriate.
Instead, we use the binary logistic regression model, and the probability of success in the x e d n i r e s






















 Palette 2









 Palette 3









 Skin proportion Figure 5: Skin proportion of normal (green) and misbehaving (red) users under three di erent skin color palettes.
Palette 1 performs well for normal users; Palette 3 performs well for misbehaving users; and Palette 2 s performance is in the middle.
outcome (i.e., being a  asher) can be expressed as follows: log p(x) 1   p(x) =   +     SP (x) (3) As discussed in Section 5.2, we leverage three di erent measures (skin color palettes) to capture a user s skin exposure percentage (i.e., skin proportion).
Therefore, the theoretical model above can be further expressed as log p(x) 1   p(x) =  + 1  SP1(x)+ 2  SP2(x)+ 3  SP3(x), (4) where SP1(x), SP2(x) and SP3(x) represent the skin proportion of user x in the three di erent measures.
Note that the three skin color palettes may be highly correlated and pose multicollinearity threats to our binary logistic regression model.
To address this issue, we utilize Principal Component Analysis (PCA) to transfer multiple correlated variables into a smaller number of uncorrelated variables.
Based on our experimental results (Section 7), we extract one component, called the skin exposure component (SKC), to represent the three aforementioned measures of skin exposure: log p(x) 1   p(x) =   +     SKC(x) (5) SKC(x) is a linear function of normalized SP scores ZSPi : SKC(x) = a   ZSP1 (x) + b   ZSP2 (x) + c   ZSP3 (x) (6) where ZSPi (x) = (SPi(x)   avg(SPi(X)))/stdev(SPi(X)).
Here, avg(SPi(X)) and stdev(SPi(X)) are the average and standard deviation of the skin proportions of all users in a training dataset using the i-th skin color palette.
In addition to motion-based skin detection, we further boost system performance by considering di erent facial and upper body feature classi ers and combine all these classi- ers using the Dempster-Shafer Theory of evidence.
According to our observations described in Section 3, most misbehaving users do not show their faces.
With further investigation, we  nd the presence of particular image characteristics like the face, eyes, nose, mouth, and upper body are all highly correlated with non-o ensive content.
Since individual classi ers look for di erent facial features of a face, their results could be seen as independent evidences of the face appearance.
With our proposed classi er (i.e. motion-based skin detector) and all facial (i.e face, eye, nose, mouth) and upper body classi ers , we model these characteristics as a set of evidences that are used for obscene content detection.
Each classi er is responsible for re ning an individual evidence.
In the following section, we will show how we harness these evidences to predict the likelihood of obscene content based on a few sequential screenshot images of a chatter.
Dempster-Shafer theory provides a representation of hypotheses through the de nition of Belief function (Bel).
The belief function is derived from a mass function, also called basic probability assignment (m).
Assume that   represents all possible hypotheses and the basic probability assignment (m) is de ned for every element A of 2  (A is also called the focal element), where 2  is the set of all subsets of  .
The value of the mass function m(A)   [0, 1] and (cid:1) (7) m( ) = 0 (cid:2)A 2  m(A) = 1 where   is the empty set.
To illustrate this, we consider the example of misbehaving user detection.
Assume a classi er is used for re ning the evidence and the hypothesis space contains two hypotheses - a user is a  asher chatter (HF ) and a user is a normal chatter (HN ).
We know that 2  = { , {HF }, {HN }, {HF , HN }} and m({HF }) + m({HN }) + m( ) + m({HF , HN }) = 1.
The belief function of hypothesis A, derived from basic probability assignment m, is de ned as a mapping from 2  to the values in the interval [0, 1] Bel(A) = (cid:3)B A m(B) (8) In the Dempster-Shafer theory, the belief value of hypothesis A is typically interpreted as the con dence level of the hypothesis.
To illustrate how the belief function works, we continue with the misbehaving user detection example above.
Suppose the evidence that the classi er re nes can support hypothesis HF with 0.7 reliability and cannot support hypothesis HN , i.e., the mass functions for set {HF } and {HN } are m({HF }) = 0.7 and m({HN }) = 0.
Further, we can calculate the mass function for set {HF , HN }, i.e., m({HF , HN }) = 0.3 using Equation 7.
To obtain the con dence level of hypothesis HF , we use Equation 8, i.e., Bel(HF ) = 0.7.
The de nition of mass functions is dependent upon spe-ci c applications and is not provided by the Dempster-Shafer Theory.
For image processing applications, the most widely used mass functions are derived either from probability at the pixel level [30] or from the distance to class centers [4].
In fault diagnosis applications, mass functions are de ned based on subjective quanti cation [33].
Certainly, these quanti cations are imprecise.
In our application, we de ne mass functions as the performance of classi ers when the outcome of the classi ers are binary values; otherwise, mass functions are de ned as detection probability.
Recall that we harness some facial classi ers and an upper-body classi- er to re ne evidences from user s screenshot images, and the outcome of each classi er is a binary value.
Since di er-ent binary classi ers have di erent classi cation precision, the reliability of a piece of evidence is signi cantly dependent upon the classi cation precision of the corresponding classi er.
For example, binary classi er Ci re nes evidence
 ever, binary classi er Ci may make mistakes when re ning evidence ei, i.e., the precision is not equal to 1.0 when a binary classi er determines whether the evidence is present or not.
Consequently, we use the precision of a binary classi- er to de ne the corresponding mass functions.
Our system also uses the motion-based skin detector and its outcomes are the probability that a user is a  asher as well as the probability that a user is a normal chatter.
Therefore, we use these probabilities to de ne mass functions instead of using the classi cation precision.
In this application, obscene content detection is a binary classi cation; therefore, our hypothesis space only contains two hypotheses - a user is a normal chatter, and a user is a misbehaving chatter.
Here we formulate these two hypotheses as HN and HF respectively.
Furthermore, di erent evidences are used to support di erent hypotheses.
Facial evidences (including the presence/absence of face, eyes, nose and mouth) and the upper-body evidence are only used for supporting hypothesis - HN , and these evidences provide no supports for the opposite hypothesis - HF .
On the other hand, the skin exposure evidence can be used for supporting both hypothesis HN and hypothesis HF .
The main di er-ence for these two types of evidences is that the beliefs of hypotheses that derive from the former evidences are non-additive and the beliefs of evidences that derive from the latter are additive because the motion-based skin detector is built upon probability theory while the other detectors are based on evidence theory [17].
The following illustration gives an easy-to-understand introduction to the calculation of mass functions based on two di erent kinds of evidence - face evidence and skin evidence - which support two di erent hypotheses.
Assume the face classi er s precision for correctly detecting the face evidence is 0.95 (i.e., when the face classi er identi es a face in a screenshot image, there is actually a face present in the image), and its precision for falsely detecting the face evidence is 0.32 (i.e., when the face classi er does not identify a face in a screenshot image, there is actually a face present in the image).
Based on the de nition of mass functions, we can calculate the mass functions based on face evidence as follows.
(a) when the face classi er detects a face: mf =1({HN }) = 0.95, mf =1({HF }) = 0, and mf =1({HN , HF }) = 0.05; (b) when the classi er does not detect a face: mf =0({HN }) = 0.32, mf =0({HF }) = 0, and mf =0({HN , HF }) = 0.68.
Di erent from the mass function based on face evidence, the mass functions based on skin evidence are de ned as follows.
Assume the motion-based skin detector identi es a user as a normal chatter with 0.87 probability and as a misbehaving chatter with 0.13 probability.
Then the mass functions based on skin exposure evidence are ms({HN }) =
 Based on the mass functions above, we further calculate the belief functions for hypothesis HN and HF , and get the following results.
When a face is detected by the face detector, the belief that a user is a normal chatter is 0.95 and the belief that a user is a misbehaving chatter is 0, i.e., Bel(HN ) = 0.95 and Bel(HF ) = 0.
Conversely, the belief that a user is a normal chatter is 0.32 and the belief that a user is a misbehaving user is 0 when the face detector does not detect a face, i.e., Bel(HN ) = 0.32 and Bel(HF ) = 0.
Apparently, these belief functions are non-additive (i.e., Bel(HN ) + Bel(HF ) < 1).
On the other hand, when the skin detector indicates a user is a normal chatter Table 1: An example of evidence fusion.
mf =1(HN ) = 0.95 mf =1(HN , HF ) = 0.05 ms(HN ) = 0.87 ms(HF ) = 0.13

     0.1235
 Figure 6: Sequential screenshot images of a user.
with 0.87 probability and the user is a misbehaving chatter with 0.13 probability, the belief that a user is a normal chatter and the belief that a user is a misbehaving chatter are 0.87 and 0.13 respectively, i.e., Bel(HN ) = 0.87 and Bel(HF ) = 0.13.
Notice that the belief functions are additive in this case.
In this application, two hypotheses are supported by multiple evidences.
Therefore, multiple evidences need to be combined in an e ective way.
The example in Section 6.1 indicates that hypothesis HN is supported by face evidence and skin exposure evidence.
In order to obtain the belief of hypothesis HN from multiple evidences, we utilize a rule of combination that Shafer suggested [36], which allows mass functions to be combined in the following way: mi1j(H) = (cid:2)A (cid:1) B=H(cid:3)=  mi(A)   mj(B)
 (9) where A, B, H   2  and A (cid:9)= B (cid:9)= H, mi and mj denote the mass functions based on evidence i and evidence j respectively.
i 1 j and mi1j represent the new combined evidence and the corresponding mass function based on the new evidence, respectively.
To illustrate this, we continue the example introduced in Section 6.1 and assume that the face classi er identi es that a face is presented in a screenshot image.
The combination results are summarized in Table 1.
For each cell, we take the corresponding focal elements, intersect them and multiply their corresponding basic probability assignment.
In the combined evidence, there are two focal elements - {HN } and {HF }.
The combined mass functions are calculated as follows: m(f =1)1s({HN }) =

 = 0.9926 m(f =1)1s({HF }) =

 = 0.0074 Based on the new combined evidence (f = 1) 1 s, we further calculate the belief of hypothesis HN and the belief of hypothesis HF , which are Bel(HN ) = 0.9926 and Bel(HF ) = 0.0074.
Thus, the user is more likely to be a normal chatter according to the combined evidence.
As introduced in Section 4, the system has the capacity of taking multiple screenshots for any user with a 10-second interval.
This capacity allows our software to re ne evidences in a more reliable way.
In the current implementation, our software uses a sequence of three screenshot images from a user to make a decision.
One of the advantages is that three sequential screenshot images of a user can reduce decision errors.
Figure 6 shows three sequential screenshot images of a user.
Our experiment indicates that the face shot image because of the smoke, and the mouth detector cannot re ne mouth evidence from the  rst two screenshot images because of the hand with a cigarette.
However, the user s facial evidences are present in the third screenshot image.
To address this issue, the decision making of our software uses the rule of maximum belief.
Assume the belief values of hypothesis HN , derived from three screenshot images, are Bel1(HN ), Bel2(HN ) and Bel3(HN ).
The belief value of hypothesis HN for the user then is Bel(HN ) = max{Bel1(HN ), Bel2(HN ), Bel3(HN )}.
Here, we use the maximum belief of hypothesis HN because it can greatly reduce the false positive rate and keep the false negative rate at a reasonable level.
In this section, we evaluate the proposed solution for detecting obscene content and misbehaving users in online video chat services.
We  rst describe the experimental setup and present the results of individual binary classi ers.
We then focus on evaluating the performance of our motion-based skin detector and DST-based fusion technique.
As described in Section 3, screenshot images from 20,000 Chatroulette users are used in our experiments.
We  rst  l-ter out images that are dark or contain static scenes.
Due to the limited space, details of this  ltering process are omitted, and we refer interested readers to [2].
The categorization of the remaining 15,000 users are shown in Figure 1 ( ltered dataset).
These images are then randomly split into two subsets of equal size   one for training and the other one for testing.
Since the binary classi ers (i.e., facial detectors and upper body detector) have already been well trained in the OpenCV library, we only use the training dataset to re ne the reliabilities of these classi ers  evidences (the mass functions of these classi ers  evidences).
Both the motion-based skin detector and DST-based fusion system are trained and tested using the training and testing datasets, respectively.
Our system utilizes a number of binary classi ers from the OpenCV library to re ne evidences.
As described in Section 6, these classi ers do not give accurate detection, and a reliability value has to be assigned to each evidence that the corresponding classi er re nes.
Recall that we de- ne the reliability value as the precision of the corresponding binary classi er.
Intuition suggests that the performance of our fusion system is greatly dependent upon the reliability value assignment.
Therefore, we randomly select 1,000 users  screenshot images from the training dataset to re ne the reliability of each classi er, and place the selected screenshot images back to the training dataset (i.e., random selection with replacement).
We repeat this operation K times (K = 10) and use the mean value as the reliability value for the corresponding evidence.
As shown in Table 2, the standard deviations of these evidences are fairly small, Table 2: Mass Functions for Facial and Upper Body Evidences mx=1(HN ) mx=0(HN ) x face eye nose mouth upper body









 stdevx=1 stdevx=0









 Table 3: Correlations among 3 Predictors


 ** Correlation is signi cant at the 0.01 level (2-tailed).
thus the mass functions which are used for the combination of facial and upper body evidences are relatively stable.
As described in Section 5, we consider three di erent skin color palettes and calculate the corresponding skin proportion SP measures using motion-based target maps.
The SP measures are then combined in our binary logistic regression model to calculate the probability of a user being a  asher.
Correlation Analysis and PCA.
Recall that we use PCA to transfer the three correlated SP variables into one uncorrelated variable.
Using the screenshot images in the training dataset, we  rst apply the method (described in Section 5) to calculate users  skin proportion SP in three di erent measures and then performed a correlation analysis of the three SP measures for each user.
As shown in Table 3, the correlations are signi cant, and PCA is indeed needed in order to avoid multicollinearity threats to our regression model.
We used the PCA procedure in PASW18.0 (SPSS) [37] to transform the three measures of skin exposure.
Kaiser Criterion (Eigenvalue > 1) was followed when selecting components and a scree plot (Figure 8) was used to con rm the dimensions identi ed.
We extracted one component (skin exposure component SKC) to represent the three SP measures according to the Eigenvalues and the elbow point iden-ti ed in the scree plot.
When subsequently building our binary logistic regression model, we only needed to consider SKC, which is a linear function of the normalized SP scores: SKC(x) = 0.362   ZSP1 (x) + 0.384   ZSP2 (x) + 0.349   ZSP3 (x) (10) Model Construction.
Since the training process of motion-based skin detector can be performed o ine, we used the binary logistic regression procedure in the statistical package SYSTAT 13 [38].
Maximum Likelihood Estimation with EM algorithm was utilized in estimating the proposed model coe cients.
The resulting regression model is as follows: log p(x) 1   p(x) =  0.775 + 1.114   SKC(x) (11) Consider the Hosmer Lemeshow test [18], which is a test of goodness of  t for logistic regression model with contin-l e u a v n e g
 i











 Component Number Figure 8: Scree plot: Eigenvalues of extracted principal components.
i n o s c e r










 MSED w/ PCA MSED w/o PCA SafeVchat PicBlock i i n o s c e r








 MSED w/ PCA MSED w/o PCA SafeVchat PicBlock





 Recall Recall (a) Classi cation for normal users (b) Classi cation for misbehaving users Figure 7: Performance comparison of obscene content detection.
Our motion-based skin exposure detector (MSED) outperforms PicBlock, and our DST fusion based solution (SafeVchat) has the best performance.
uous predictors, and a statistically non-signi cant test result ( 2) indicates a good  t of the model.
Our proposed model provided a signi cantly good  t to the training data ( 2 = 12.318, df = 8, p = 0.138)1 (p < 0.05).
Speci cally, our skin exposure composite is a statistically signi cant predictor for the probability of a user being a  asher.
This was tested with Wald s test [40], which is a statistical test of signi cance for individual variable.
The test statistics (W ald = 43.108, df = 1, p = 0.000) indicated that the in u-ence of SKC is statistically signi cant (p < 0.05).
Model Performance.
Using the model constructed above, we then evaluated the model s performance in terms correctly classifying normal and misbehaving users.
Figure 7 shows the precision-recall curves of di erent methods.
As shown in Figure 7(a), when we only consider an individual skin color measure (no PCA), the performance for classifying normal users is slightly worse than that of the combination of three skin color measures using PCA.
In contrast, Figure 7(b) shows that the performance for classifying misbehaving users remains approximately the same no matter whether PCA is used or not.
To understand the reason behind this, we review the example in Figure 5.
When we consider skin color in a small space, many misbehaving users are mixed with normal users.
As the skin color space increases, a majority of misbehaving users are detected with more skin exposures and meanwhile a smaller number of normal users are also falsely detected.
In Figure 5, there is an interesting observation   the normal users become identical and misbehaving users are still mixed with normal users when skin color space is enlarged.
Therefore, a straightforward phenomenon shown in Figure 7 is that considering three measures improves the classi cation performance for normal users but not for misbehaving users.
We now evaluate the performance of overall system, which uses DST to fuse the evidences of facial, upper body, and motion-based skin exposure detectors.
The overall system is evaluated using the testing dataset, and the precision-recall curves for correctly classifying normal and misbehaving users are plotted in Figure 7.
We can see that SafeVchat improves signi cantly in both precision and recall over other techniques.
We  rst compare SafeVchat with a state-of-the-art commercial software for detecting pornographic content   Porno-1df and p denote degree of freedom and p-value, respectively.
graphic Image Blocker (PicBlock 4.2.3).
To the best of our knowledge, it uses both skin-color detection and text analysis techniques to determine whether an image on a website is objectionable.
Its output is a binary value, i.e., either o en-sive or normal content.
We used the same testing dataset to compare our system with PicBlock 4.2.3.
As shown in Figure 7, the classi cation performance of PicBlock is much worse than our motion-based skin exposure detector and our DST-based fusion system.
There are several reasons behind this.
(1) There is no text information that can be used in online video chat systems.
Therefore, the function of text analysis in PicBlock cannot help.
(2) PicBlock does not consider motion between images, and may falsely identify some background as objectionable content.
(3) Some misbehaving users do not show their entire body trunk.
Instead, only a small proportion of skin region can be detected, which may bypass PicBlock s detection.
The fused system also outperforms our motion-based skin exposure detector.
The reasons are the following.
To reduce skin exposure detection errors, the skin exposure detector uses the face detector provided in the OpenCV library to crop the skin region on a face.
However, the face detector cannot be operated in a perfect condition (e.g.
a partial or side face cannot be detected).
Therefore, some facial skin regions that cannot be identi ed by the face detector may be considered as large skin exposure, increasing the detection error in this condition.
Using DST-based fusion, when the skin exposure detector mis-classi es a face-presented user, facial and upper body evidences, to some extent, can correct the classi cation errors and thus boost the performance of overall system.
We also observe in Figure 7 that the precision for classifying normal users remains high (almost 1.0) until the recall is above 0.75.
The reason is that when there are positive facial or upper body evidences, the user is very likely to be a normal user (e.g., users who show their faces are very likely to be normal).
However, when no such positive evidences are identi ed, more classi cation errors are possible (e.g., users who do not show their faces can be either normal or misbehaving), and the classi cation precision of the overall system approaches that of the skin exposure detector.
On the other hand, the precision for classifying misbehaving users remains fairly stable (approximately 0.7) when the recall is in the range of [0.5, 1.0].
The reason is that when skin color space increases, more misbehaving users will be identi ed, meanwhile normal users who wear cloth with similar color to the skin color space may be misclassi ed as  ashers.
As can, to some extent, reduce the number of these misclas-si ed users.
Therefore, the classi cation precision remains relatively stable when the recall increases from 0.5 to 1.0.
This paper describes a system for detecting obscene content and identifying misbehaving users in online video chat services such as Chatroulette.
A novel, motion-based skin detection method has been introduced.
The results of individual binary classi ers such as face, eye, and nose detectors as well as our probabilistic motion-based skin detector are fused together into an overall decision classifying the user as misbehaving or not, based on Dempster-Shafer Theory.
By using Chatroulette s real-world dataset, we have conducted a series of experiments, and evaluated our system performance.
Compared to the current state-of-the-art software, PicBlock 4.2.3, our system achieves signi cantly higher clas-si cation precision and recall.
While our system has been speci cally designed in the context of Chatroulette online video chat services, it can be extended to other webcam based online video chat systems.
Our preliminary observations of several webcam based online chat rooms (such as Chat for Free [8], Goth Chat City [16] and iWebcam [1] etc.)
show that the content transmitted and user behaviors in these online chat rooms are similar to Chatroulette.
One di erence is that the obscene content and behaviors are more common in such online chat rooms.
We plan to experimentally explore how well our system performs in the context of online video chat rooms.
A software demo video is available in our project website [2].
We thank Andrey Ternovskiy and Kirill Gura for much help with the deploying of SafeVchat.
