With the rapid growth of World Wide Web, it has been well recognized that classical relevance based search engines may fail in satisfying users due to the lack of understanding the true intents behind the search queries[25].
For example, when a user submits a This work is accepted when the first author visiting Microsoft Research Asia Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Sparse Hidden-Dynamics Conditional Random Fields for User Intent Understanding Jun Yan2 Ning Liu2 Shuicheng Yan3 Zheng Chen2
 Center, Zhichun Road Beijing, China {junyan,leiji, Ningl, zhengc}@microsoft.com
 Engineering, National University of Singapore shuicheng.yan@gmail.com query of  Swimming , it is unclear whether the user is interested in the sport in water or the famous movie  Swimming  without understanding the user s search intent.
As shown in recent studies [4-7, 14, 36], user historical search behaviors such as issued queries and clicked URLs [41] could provide rich information for user intent understanding.
The continuous behaviors of the same user are often semantically correlated.
For example, if the user has issued a query of  Lauren Ambrose  the actor of the movie  Swimming , right before the query of  Swimming , it is likely that the user has the intent of  Find Entertainment information  behind the query.
Similarly, if the user issues some queries related to sports information before  Swimming , it is likely that the user has the intent of  Find information on sports .
Therefore, learning user intent based on a sequence of user search behaviors could help search engines to produce better results than treating each user query individually.
learning algorithms used for sequential Classical machine structuralized data analysis focused on two categories of dependencies, namely, user behavioral dependence, such as using sliding window method [45], and class label dependence, such as using Conditional Random Fields (CRFs) and Hidden Markov Model (HMM) [5, 20, 36].
However, both categories are quite limited when applied for user intent understanding.
On one hand, there may exist billions of different user search behaviors, if we expect to model the sequential dependencies of these user search behaviors, the huge number of parameters shall require a huge training dataset, which is however of high cost or even impossible in real applications.
On the other hand, if we model the dependency over the class labels, the coarse granularity of intent class definitions may lead to serious information loss, which may make the results imprecise.
For example, suppose we have two intent labels,  plan a travel  and  find image , they have different dependency relationships for different query sequences.
If the query sequence is  cheap ticket to Seattle  and  Britney Spears , the class label of the second query does not depend on the class label of the first query, However, if the query sequence is  cheap ticket to Seattle  and then  Seattle image , the class labels between them may have high dependency.
Therefore, the coarse intent labels in previous provides little information for predicting the next user intent label.
The main challenges in modeling sequential user behaviors for intent prediction include:
 search behaviors are limited, since neither user behaviors nor class intent understanding.
sequential user labels are suitable for
 to severe information loss to reflect the true user intents, while each requires an explainable substructure.
intent generally through three characters.
First, In this work, we propose a novel Sparse Hidden-Dynamics Conditional Random Fields (SHDCRF) model for user intent learning from his/her sequential search behaviors, which are also referred to as search sessions.
The proposed SHDCRF model has the following incorporating hidden-dynamics variables instead of modeling user behavioral dependence and coarse level intent labels dependence, we can capture the true user intent dynamics in the user search session.
Second, through using a supervised learning strategy to learn sparse relations between the hidden variables and intent class labels, the hidden state variables become explainable, which could help human editors define new finer scale intent labels.
Third, we force the dependency on hidden states variables, which could be efficiently trained and inferred for SHDCRF model.
In terms of computation cost, the model parameters of SHDCRF could be estimated by employing the L-BFGS algorithm [23].
In addition, the SHDCRF model allows natural incorporation of unlabeled data for semi-supervised learning.
Experimental results show that the proposed SHDCRF model provides much more accurate intent prediction results in user search sessions than those existing state-of-the-art algorithms such as Support Vector Machines (SVM), Conditional Random Fields (CRFs) and Latent-Dynamic Discriminative Models (LDCRF).
The rest of this paper is organized as follows.
In Section 2, we provide a short review of the related work.
Then we present the problem formulation for user intent understanding in Section 3.
In Section 4, we present the formulation of the SHDCRF model and the training and inference procedures for SHDCRF model.
Section
 conclude the paper in Section 6.
There are extensive research efforts dedicated to learning user intents from their online behaviors.
Existing methods are generally proposed from two perspectives, namely Non-Context-Aware [34,
 Context-Aware methods aim to learn users  intents from their current behaviors such as current search query and clicked URL [1,
 ambiguous, various techniques have been proposed for feature enrichment, i.e. query expansion [34, 35, 37, 22, 19, 11, 3, 9, 15].
While Context-Aware methods assume that the adjacent user behaviors are semantically related and have the same or closely related user intents.
For example, several recent studies [4-7, 14, 36] propose to organize user behavioral sequence as temporal time series and learn user intent from them.
Generally, it has been well recognized in literature that Context-Aware algorithms generally perform better than Non-Context-Aware approaches.
Cao et al.[5, 6] propose the variable length Hidden Markov Model (vlHMM) and CRFs model to learn user intent from the user search session.
Both the vlHMM and CRFs model strongly rely on the Markov property assuming that the next user intent depends only on the current user intent and the next user behavior.
As studied in [7], user behavior and user intent at a certain time could have complicated relations and high-order dependency.
However, higher-orders CRFs model could not be computed efficiently since the computational cost increases exponentially with its order.
Sutton et al. [38] propose a skip-chain CRFs which tries to relax strong Markov assumption by adding long-distance edges.
But as studied in [16], it needs a lot of human knowledge to determine which long-distance edge should be added.
Sarawagi et al. [32] propose a Semi-Markov CRFs model.
But it can only deal with segment-based higher order feature.
One common technique to simplify the complex dependency relations is to incorporate a new intermediate layer of hidden state variables.
Trinh et al.[24] and Peng et al.[28] propose Neural conditional random fields (NCRFs) and Conditional Neural Fields (CNFs) respectively, both of which could capture high level features by adding hidden layers.
However, their models do not consider the hidden variables dependence.
Ariadna et al. [30] propose a Hidden-state Conditional Random Fields (HCRF) model for object recognition.
The HCRF model has also been successfully employed for phone classification [12], ECG classification [42].
However, these HCRF models can only be applied to label segmented sequence.
Sutton et al. [39] propose a Dynamic Conditional Random Fields (DCRF) to model sequence data.
DCRF model could learn complex interactions and higher-order Markov dependence between user behaviors and user intents.
However, learning a DCRF model with hidden variables shall result in a very difficult optimize problem [29].
Morency et al. [29] present a Latent-Dynamic Conditional Random Field (LDCRF) model, which also incorporates hidden-dynamic variables for Continuous Gesture Recognition.
However, the LDCRF model does not automatically learn the relations between class label and hidden state variables.
In fact, in order to make the training and inference phases of the LDCRF model tractable, Morency et al. [29] impose a constraint that each class label has a disjoint set of associated hidden state variables.
The Sparse Hidden-Dynamic Conditional Random Field (SHDCRF) model overcomes the shortage of LDCRF and does not require the user to manually assign hidden states to each class before the training phase.
Instead, it automatically learns to allocate the hidden states to each class optimally.
Yu et al.[43] present a Deep-Structured Conditional Random Fields model that uses a layer-wise intermediate representations in deep hidden layers.
Compared to Deep-Structured CRFs, SHDCRF only incorporates one hidden layer such that the training of the model is more efficient.
Our proposed SHDCRF model is different from various previous studies in many aspects.
In contrast to CRFs, which models the intent label dependence, SHDCRF learns the intermediate hidden-dynamics between intent class labels and user behavior variables.
In contrast to Latent-Dynamic Conditional Random Fields (LDCRF)[29], SHDCRF proposes to learn the sparse relations between the hidden variables and intent labels instead of specifying by human in LDCRF.
In addition, we propose a supervised learning strategy to learn the sparse relations between the hidden variables and intent labels to make the hidden state variables explainable.
learn discriminative learning strategy to

 In this section, we introduce the notations to be used in the remaining part of this paper.
defined as a sequence of observed user behaviors x(cid:2869),x(cid:2869) xT where each observed user behavior (cid:1876)(cid:3047)(cid:4666)1(cid:3409)(cid:1872)(cid:3409)(cid:1846)(cid:4667) consists of a query (cid:1869)(cid:3047) and a set of URLs (cid:1873)(cid:3047) clicked by the user after issuing query (cid:1869)(cid:3047).
Let (cid:1851) be the set of all possible user intent class labels, each user behavior (cid:1876)(cid:3047)(cid:4666)1(cid:3409)(cid:1872)(cid:3409)(cid:1846)(cid:4667) has an intent label (cid:1877)(cid:3047)(cid:1488)(cid:1851).
The user intent between a sequence of observations (cid:2206)(cid:3404)(cid:4668)(cid:2206)(cid:2778),(cid:2206)(cid:2779), ,(cid:2206)(cid:2176)(cid:4669) and a sequence of intent labels (cid:2207)(cid:3404)(cid:4668)(cid:2207)(cid:2778),(cid:2207)(cid:2779), ,(cid:2207)(cid:2176)(cid:4669) from the training understanding problem is defined as  learn a mapping function set. 


 (cid:3398)(cid:1313)(cid:2947)(cid:1313)(cid:3118)(cid:2870)(cid:2978)(cid:3118) L(cid:4666) (cid:4667)(cid:3404)  (cid:4666)(cid:2934),(cid:2935)(cid:4667) p(cid:3556)(cid:4666)x,y(cid:4667)logp(cid:2947)(cid:4666)y|x(cid:4667) For clarity and self-containedness, we begin with a brief recap of the standard Conditional Random Fields (CRFs).
CRFs is one of the most commonly used solutions for sequential data classification.
In order to estimate the parameters in CRF model, we essentially try to maximize the following objective function: where the first term in Eqn.
(1) is to maximize the log-likelihood of term is the regularization term to avoid over fitting, which imposes the training data.
  is the vector of model parameters, the second a zero prior on all the parameter values.
  is used for penalizing large parameter values.
(cid:4666)(cid:1876),(cid:1877)(cid:4667) indicates a sequence of observations with the corresponding sequence of class labels.
(cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667) and (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667) indicate the empirical distribution and conditional distribution through CRF respectively.
Figure 1 shows the graphical structure of CRF model.
However, as mentioned above, modeling user intent label dependence is likely to be imprecise due to the weak dependency between the coarse level intent labels of two consecutive behaviors.
(1) y1 y2

 yT
 Figure 1.
The graphical structure of classical CRF model.
Motivated by the limitation of CRF model, we propose a new probabilistic graphical model named Sparse Hidden-Dynamic Conditional Random Fields (SHDCRF), for sequential data labeling.
Figure 2 shows the graphical structure of the SHDCRF model.
The SHDCRF model incorporates a vector of hidden variables (cid:1860)(cid:3404)(cid:4668)(cid:1860)(cid:2869),(cid:1860)(cid:2870), ,(cid:1860)(cid:3021)(cid:4669), each (cid:1860)(cid:3047) takes its value in (cid:1834) where (cid:1834) is the finite set of all possible hidden states.
These hidden variables are not observable in the training examples.
(cid:3035) to (2) (3) (4) feature and (6).
Figure 2.
The graphical structure of SHDCRF model.
As shown in Figure 2, we can define the hidden-dynamics conditional probabilistic model as follows: parameters corresponding respectively.
Therefore, model parameters in SHDCRF denote by Z(cid:2869)(cid:4666)h(cid:4667)(cid:3404)  (cid:1857)(cid:1876)(cid:1868)  (cid:3043)(cid:3038)(cid:2880)(cid:2869) (cid:2935)(cid:4593) Z(cid:2870)(cid:4666)x(cid:4667)(cid:3404)  (cid:1857)(cid:1876)(cid:1868)  (cid:3044)(cid:3038)(cid:2880)(cid:2869) (cid:2918)(cid:4593) the feature functions for the sequence of intent class labels, hidden variables and user behaviors.
In the above equations, we assume that the total number of feature functions is p for the sequence of intent class labels and hidden variables, and q for the sequence of (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667)(cid:3404)  (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667) where (cid:1860)(cid:3404)(cid:4668)(cid:1860)(cid:2869),(cid:1860)(cid:2870), ,(cid:1860)(cid:3021)(cid:4669) ,each (cid:1860)(cid:3047) is a member of (cid:1834) .
Here (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667) and (cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667) are defined as: (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:3404) (cid:2869)(cid:3027)(cid:3117)(cid:4666)(cid:3035)(cid:4667)(cid:1857)(cid:1876)(cid:1868)  (cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1877),(cid:1860)(cid:4667) (cid:3043)(cid:3038)(cid:2880)(cid:2869) (cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667)(cid:3404) (cid:2869)(cid:3027)(cid:3118)(cid:4666)(cid:3051)(cid:4667)(cid:1857)(cid:1876)(cid:1868)  (cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860),(cid:1876)(cid:4667) (cid:3044)(cid:3038)(cid:2880)(cid:2869) where Z(cid:2869)(cid:4666)h(cid:4667) and Z(cid:2870)(cid:4666)h(cid:4667) are the partition functions, (cid:1833)(cid:3038) and (cid:1832)(cid:3038) are hidden variables and user behaviors.
(cid:2019)(cid:3038) and (cid:2010)(cid:3038) are the model function (cid:1832)(cid:3038) and (cid:1833)(cid:3038) (cid:1993)(cid:3404)(cid:4668)(cid:2019)(cid:2869),(cid:2019)(cid:2870), (cid:2019)(cid:3044),(cid:2010)(cid:2869),(cid:2010)(cid:2870), ,(cid:2010)(cid:3043)(cid:4669) with p + q parameters.
The partition functions Z(cid:2869)(cid:4666)h(cid:4667) and Z(cid:2870)(cid:4666)h(cid:4667) are defined as in Eqn.
(5) (cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1877)(cid:1314),(cid:1860)(cid:4667) (cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1876)(cid:4667) In the above equations, y(cid:1314) is a sequence of intent class labels (cid:1877)(cid:1314)(cid:3404)(cid:4668)(cid:1877)(cid:1314)(cid:2869),(cid:1877)(cid:1314)(cid:2870), ,(cid:1877)(cid:1314)(cid:3021)(cid:4669) , (cid:1860)(cid:1314) is a sequence of hidden variables (cid:1860)(cid:1314)(cid:3404)(cid:4668)(cid:1860)(cid:1314)(cid:2869),(cid:1860)(cid:1314)(cid:2870), ,(cid:1860)(cid:1314)(cid:3021)(cid:4669), each (cid:1877)(cid:1314)(cid:3047) and (cid:1860)(cid:1314)(cid:3047) is a member of Y and H Note that (cid:1833)(cid:3038) and (cid:1832)(cid:3038) are the feature functions for the sequences, (cid:1833)(cid:3038)(cid:4666)(cid:1877),(cid:1860)(cid:4667)(cid:3404)  (cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:3047),(cid:1860)(cid:3047)(cid:4667) (cid:3021)(cid:3047)(cid:2880)(cid:2869) (cid:1832)(cid:3038)(cid:4666)(cid:1860),(cid:1876)(cid:4667)(cid:3404)  (cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667) (cid:3021)(cid:3047)(cid:2880)(cid:2869) where (cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:3047),(cid:1860)(cid:3047)(cid:4667) is the state feature function, and (cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667) contains both the state feature function (cid:1871)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667) and transition (cid:1872)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667) .
Here (cid:1872)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667) is used relations between x and y, it also makes the likelihood function model.
Although L(cid:2869) regularization term is very popular in most
 However, although incorporating a intermediate layer of hidden state variables could help simplify the complex dependency non-convex.
In order to avoid bad locally optimal solution and yield sparse relations between the hidden states and the desired outputs, we add an entropy based regularization term in SHDCRF which could be rewritten as: respectively.
transition the the hidden-dynamic function in to capture function (7) (8) (5) (6) existing sparse learning literature, such as sparse PCA [46], dictionary learning [47].
However, L(cid:2869) regularization term typically makes the objective function non-differentiable.
Moreover, L(cid:2869) parameters (cid:1993) as: (cid:1838)(cid:4666)(cid:1993)(cid:4667)(cid:3404)  (cid:3398)(cid:1313)(cid:3064)(cid:1313)(cid:3118)(cid:2870)(cid:3097)(cid:3118)(cid:3398)(cid:2009)(cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667) (9) (cid:4666)(cid:3051),(cid:3052)(cid:4667) regularization term is not suitable for probability model.
Following the graphical structure as shown in figure 2, we define the objective function in SHDCRF model to learn the model (cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667) (cid:1868)(cid:3064)(cid:4666)(cid:1877)(cid:4662)|(cid:1860)(cid:4662)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:3064)(cid:4666)(cid:1877)(cid:4662)|(cid:1860)(cid:4662)(cid:4667)
 In this subsection, we aim to learn the model parameters by maximizing the objective function in Eqn.
(9), given the training (10) Therefore, to estimate the model parameters, our goal is to optimize the objective function and learn the optimal model where the first two terms of the formulation are similar with those in Eqn.
(1), and the third term aims to minimize the conditional entropy between hidden states variables and class labels.
The conditional probability distribution of class labels given hidden states.
With the reduction of the conditional entropy, the uncertainty of the class labels given hidden states is also decreasing.
As a special case, when the conditional entropy probability for intent class labels given each hidden state is also equal to zero.
In other words, each hidden state corresponds to only one intent class label, constructing the substructure of the intent.
Moreover, we ensure the sparseness of the relations between the hidden variables and the intent class labels so as to make the hidden state variables explainable.
The conditional (cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667) is minimized to be zero, the entropy of conditional entropy (cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667) is defined as: (cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667)(cid:3404)(cid:3398)    (cid:3035)(cid:4662)(cid:1488)(cid:3009) (cid:3052)(cid:4662)(cid:1488)(cid:3026) parameters (cid:1993)(cid:1499) with (11).
(cid:1993)(cid:1499)(cid:3404)(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3064)(cid:1838)(cid:4666)(cid:1993)(cid:4667) set that consists of n labeled sequences (cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439),(cid:1861)(cid:3404)1..(cid:1866).
 L(cid:4666) (cid:4667) (cid:2921) (cid:3404) (cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:1832)(cid:3038)(cid:4666)(cid:1876)(cid:3036),(cid:1860)(cid:1314)(cid:4667) (cid:3398) (cid:2921) (cid:2870) (cid:3398)(cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:1314)|(cid:1876)(cid:3036)(cid:4667)(cid:1832)(cid:3038)(cid:4666)(cid:1876)(cid:3036),(cid:1860)(cid:1314)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924)  L(cid:4666) (cid:4667) (cid:2921) (cid:3404) (cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:1833)(cid:3038)(cid:4666)(cid:1877)(cid:3036),(cid:1860)(cid:1314)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3398) (cid:2921) (cid:2870) (cid:3398)(cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:1314)|(cid:1876)(cid:3036)(cid:4667)(cid:1833)(cid:3038)(cid:4666)(cid:1877)(cid:3036),(cid:1860)(cid:1314)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3035)(cid:4593),(cid:3052)(cid:4593) (cid:3397)p(cid:4666)y(cid:4662)|h(cid:4662)(cid:4667)(cid:4666) (cid:2921)(cid:3398)(cid:3533)p(cid:4666)y(cid:4663)|h(cid:4662)(cid:4667) K(cid:4666)(cid:2935)(cid:4663),(cid:2918)(cid:4662)(cid:4667) (cid:4667) Since the SHDCRF model contains the hidden layer, the objective function is not a convex, and thus there generally does not exist closed form solution.
In this work, we employed a gradient-based method to search for the locally optimal parameters.
We first give the partial derivatives of the objective function: (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3035)(cid:4593),(cid:3052)(cid:4593) (cid:3035)(cid:4593) (cid:3035)(cid:4593) (12) (11) (cid:2935)(cid:4663)(cid:1488)Y (13)  L(cid:4666) (cid:4667) (cid:2921) (cid:3404) In these two equations h(cid:4593) and ,y(cid:4593) are sequence variables, and each element in h(cid:4593) and y(cid:4593) is a member of H and Y respectively.
In equation (13), y(cid:4662) and h(cid:4662) are also the members of Y and H respectively, the pair (cid:4666)y(cid:4662),h(cid:4662)(cid:4667) is given by parameter  (cid:2921) under the constraint of feature function (cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:4662),(cid:1860)(cid:4662)(cid:4667)(cid:3404)1.
(Note that (cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:4662),(cid:1860)(cid:4662)(cid:4667) is the feature function corresponding with parameter (cid:2010)(cid:3038)).
K(cid:4666)y(cid:4663),h(cid:4662)(cid:4667) in Eqn.
(13) equals to the unique k(cid:1499) given y(cid:4663) and h(cid:4662) , where k(cid:1499) satisfied the constraint: (cid:1859)(cid:3038)(cid:1499)(cid:4666)(cid:1877)(cid:4663),(cid:1860)(cid:4662)(cid:4667)(cid:3404)1.
The partial derivatives for model parameters   in Eqn.
(12) and (13) cannot be calculated directly, since h(cid:4593) ,y(cid:4593),x(cid:2919) and y(cid:2919) are all sequence variables, Therefore, in order to efficiently calculate Eqn.
(12) and (13) using Viterbi path [40], we rewrite Eqn.
(12) and (13) by disengaging sequence variables according to the graphical structure shown in Figure 2, with details given in Eqn.
(14) and (15).
(cid:3047)(cid:2880)(cid:2869)..(cid:3021)(cid:3284) (cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439) (cid:3533) (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:3036)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3047)(cid:2880)(cid:2869)..(cid:3021)(cid:3284) (cid:3398) (cid:2921) (cid:2870) (cid:3398)(cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036)(cid:3439) (cid:3533) (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036)(cid:4667)(cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:3036)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3047)(cid:2880)(cid:2869)..(cid:3021)(cid:3284)  L(cid:4666) (cid:4667) (cid:2921) (cid:3404) (cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439) (cid:3533) (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1877)(cid:3047)(cid:3036)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3398)(cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3036)(cid:3439) (cid:3533) (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1877)(cid:3047)(cid:4667)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1877)(cid:3047)(cid:3036)(cid:4667) (cid:2919)(cid:2880)(cid:2869)..(cid:2924) (cid:3047)(cid:2880)(cid:2869)..(cid:3021)(cid:3284) (cid:3398) (cid:2921) (cid:2870)(cid:3397)p(cid:4666)y(cid:4662)|h(cid:4662)(cid:4667)(cid:4666) (cid:2921)(cid:3398)(cid:3533)p(cid:4666)y(cid:4663)|h(cid:4662)(cid:4667) K(cid:4666)(cid:2935)(cid:4663),(cid:2918)(cid:4662)(cid:4667) (cid:4667) (cid:2935)(cid:4663)(cid:1488)Y the above equations, (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667) , (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667) , (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036)(cid:4667) and (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036)(cid:4667) are Take the computation of (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)and (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667) for examples.
Each position t(cid:3408)1 at the labeled sequence (cid:4666)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667), we define |(cid:1834)|(cid:3400)|(cid:1834)| matrix (cid:1839)(cid:3047)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439).
(cid:1839)(cid:3047)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3404)(cid:3427)(cid:1839)(cid:3047)(cid:3435)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3431), details as in Eqn.
(cid:1839)(cid:3047)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:3404)(cid:1857)(cid:1876)(cid:1868)(cid:4668)(cid:2030)(cid:3047)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:4669) (16) (cid:2030)(cid:3047)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:3404) (cid:3533) (cid:2019)(cid:3038)(cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869),(cid:1860)(cid:3047)|(cid:1876)(cid:3047)(cid:3036)(cid:4667) (cid:3397) (cid:3533) (cid:2010)(cid:3038)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047)|(cid:1877)(cid:3047)(cid:3036)(cid:4667) (cid:3038)(cid:2880)(cid:2869)..(cid:3044) (cid:3038)(cid:2880)(cid:2869)..(cid:3043) At position t(cid:3404)1 , we define the |(cid:1834)| vector random variable (cid:1839)(cid:2869)(cid:3435)(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3404)(cid:3427)(cid:1839)(cid:2869)(cid:3435)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3431) by Eqn.
(17).
(cid:1839)(cid:2869)(cid:3435)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:3439)(cid:3404)(cid:1857)(cid:1876)(cid:1868)(cid:4668)(cid:2038)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:4669) (17 conditional probabilities.
All of them can be estimated efficiently by Viterbi path [40].
random variable denoted by (16).
(14) (15) four the In ) the and Then probability conditional (cid:2038)(cid:3047)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:3404) (cid:3533) (cid:2019)(cid:3038)(cid:1871)(cid:3038)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3047)(cid:3036)(cid:4667) (cid:3397) (cid:3533) (cid:2010)(cid:3038)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047)|(cid:1877)(cid:3047)(cid:3036)(cid:4667) (cid:3038)(cid:2880)(cid:2869)..(cid:3043) (cid:3038)(cid:2880)(cid:2869)..(cid:3044) p(cid:2947)(cid:4666)h(cid:2930)|x(cid:2919),y(cid:2919)(cid:4667) p(cid:2947)(cid:4666)h(cid:2930)(cid:2879)(cid:2869),h(cid:2930)|x(cid:2919),y(cid:2919)(cid:4667) can be calculated below, i.e.
(cid:4671)(cid:3028)(cid:3400)(cid:4670)  (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:3404)(cid:1853)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667)(cid:3404)(cid:4670)(cid:1839)(cid:2869)(cid:3021)(cid:3400)  (cid:3400)(cid:1835)(cid:4671)(cid:3028) (cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667) (cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667) (cid:1839)(cid:2869)(cid:3021)(cid:3400)  (cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667) (cid:3400)(cid:1835) (cid:3047)(cid:4593)(cid:2880)(cid:2870)..(cid:3047) (cid:3047)(cid:4593)(cid:2880)(cid:3047)..(cid:3021)(cid:3284) (cid:3047)(cid:4593)(cid:2880)(cid:2870)..(cid:3021)(cid:3284) (cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:2879)(cid:2869)(cid:3404)(cid:1853),(cid:1860)(cid:3047)(cid:3404)(cid:1854)|(cid:1876)(cid:3036),(cid:1877)(cid:3036)(cid:4667) (cid:4671)(cid:3028)(cid:3400)(cid:1839)(cid:3047)(cid:4670)(cid:1853),(cid:1854)(cid:4671)(cid:3400)(cid:4670)  (cid:3404)(cid:4670)(cid:1839)(cid:2869)(cid:3021)(cid:3400)  (cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667) (cid:3400)(cid:1835)(cid:4671)(cid:3029) (cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667) (cid:1839)(cid:2869)(cid:3021)(cid:3400)  (cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667) (cid:3400)(cid:1835) (cid:3047)(cid:4593)(cid:2880)(cid:3047)..(cid:3021)(cid:3284) (cid:3047)(cid:4593)(cid:2880)(cid:2870)..(cid:3021)(cid:3284) where I is a |(cid:1834)| dimensional vector with all the elements equal to
 p(cid:2947)(cid:4666)h(cid:2930)|x(cid:2919)(cid:4667) have the similar form, we omit the details for saving time complexity of the gradient estimating are both (cid:1841)(cid:4666)(cid:1838)(cid:1499)(cid:1840)(cid:2870)(cid:4667), space.
The gradient of the objective function could be estimated using equations (14) and (15).
For each training sample, the space and (cid:3047)(cid:4593)(cid:2880)(cid:2870)..(cid:3047)(cid:2879)(cid:2869) (18) where L is the length of the sequence, N is the number of hidden states.
In our experiments, we use an L-BFGS [23] method to optimize the objective function.
The pseudo-code of the training algorithm is as follows: (19) The Output: optimal model Training Algorithm Input: A training set consisting of n labeled sequences (cid:3435)x(cid:2919),y(cid:2919)(cid:3439),i(cid:3404)1..n.
 (cid:3404)(cid:4668) (cid:2869), (cid:2870), (cid:2927), (cid:2869), (cid:2870), , (cid:2926)(cid:4669).
Initialize the model parameters   randomly.
For each parameter  (cid:2919)(cid:1488)  or  (cid:2919)(cid:1488) , estimate the partial Use the L-BFGS method to update  .
derivatives using Eqn.
(14) and (15).
Algorithms: parameters , Repeat steps 2 and 3 until convergence.
Another key challenge facing by many classification models  training process is the scalability issue in dealing with large-scale user behavioral data.
Our proposed SHDCRF model is easy to be implemented in a parallel manner under a map reduce framework [17].
In detail, we partition the training data into multiple subsets and distribute each subset to a processer.
In the map stage, each processer calculated the gradient of objective function for each training sequence by equation (14) and (15) respectively.
In the reduce stage, each processer merges all gradient values and update the model parameters.
The two stages are repeated until converge.
(20) via maximizing the conditional model,
 After the model parameters being estimated, for a new test sequence x, the most probable label sequence y(cid:1499) could be inferred where the model parameters   are learned from the training dataset.
Since the conditional probability p(cid:2947)(cid:4666)y|x(cid:4667) can be rewritten (cid:1877)(cid:1499)(cid:3404)(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3052)(cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667) (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667)(cid:3404)  (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667) (cid:1877)(cid:1499)(cid:3404)(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3052)  (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667) (cid:3035) label (cid:1877)(cid:3047)(cid:1499) can be computed as (cid:1877)(cid:3047)(cid:1499)(cid:3404)(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3052)(cid:3295)  (cid:1868)(cid:3064)(cid:4666)(cid:1877)(cid:3047)|(cid:1860)(cid:3047)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:4667) (cid:3035)(cid:3295)(cid:1488)(cid:3009) (21) (22) Thus, for each position t in the test sequence x, the most probable as in Eqn.
(21).
By combing the two equations of (20) and (21), we obtained the Eqn.
(22).
(23) And, the two terms in equation (23) can be estimated using Eqn.
(3) and (4) respectively.
(cid:3035)
 Model (24) The variables A and Z in the above equation are defined as in Eqn.
(25) One advantage of the proposed SHDCRF model is that it allows the natural incorporation of unlabeled data for training.
Recall the first term of the objective function (Eqn.
(9)) in SHDCRF model is the log-likelihood as in Eqn.
(24).
(cid:3404)  (cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1827) (cid:3398)  (cid:1868)(cid:3556)(cid:4666)(cid:1876)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1852)   (cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667) (cid:2934),(cid:2935) (cid:2934),(cid:2935) (cid:2934) (cid:3397)  (cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667)(cid:3404)   (cid:1857)(cid:1876)(cid:1868)(cid:4666)  (cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:4667) (cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1876)(cid:4667) (cid:4667) (cid:3404)(cid:1827)(cid:1852) (cid:3044)(cid:3038)(cid:2880)(cid:2869) (cid:3043)(cid:3038)(cid:2880)(cid:2869) (cid:3035)(cid:4593)  (cid:3397)  (cid:1857)(cid:1876)(cid:1868)(cid:4666)  (cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:1314)(cid:4667) (cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1876)(cid:4667) (cid:4667) (cid:3044)(cid:3038)(cid:2880)(cid:2869) (cid:3043)(cid:3038)(cid:2880)(cid:2869) (cid:3035)(cid:4593),(cid:3052)(cid:4593) In the classical supervised learning configuration, p(cid:3556)(cid:4666)x(cid:4667) and Z are computation of p(cid:3556)(cid:4666)x(cid:4667) and Z could be naturally extended to of feature F(cid:2921) and G(cid:2921) computed based on labeled data are good estimation of the expectation of F(cid:2921) and G(cid:2921) computed based on the involving unlabeled data.
It is only assuming that the expectation only estimated by using labeled data.
However, whole dataset including both unlabeled and labeled data [35].
Through incorporating the unlabeled data to calculate the Eqn.
(25), we can learn the model parameters in a semi-supervised manner.
(25) the the

 In this section, we use the real user search sessions logged by a popular commercial search engine to empirically validate the effectiveness of the proposed SHDCRF model for user intent classification.
We first elaborate on the experiment configurations on dataset, metrics and baselines.
Then, we introduce the extensive experimental results along with the algorithmic sensitivity analysis.
Finally, some case studies shall be illustrated to show how the SHDCRF model helps real world applications.
Dataset.
We collect a set of 5,629 real user search sessions from a commercial search engine.
The average session length is
 sequence length in the dataset, which contains 56,733 unique queries and 224,893 unique URLs.
Five human editors are asked to label each query in all the 5,629 user search sessions using 8 different user intent labels, which are  plan travel ,  communicate with others ,  Shopping  ,  Find a fact ,  Entertainment ,  Find Online Services  ,  Download file  and  Others .
Table 2 shows the distribution of intent labels in the dataset.
Each record in the dataset is a user behavioral sequence, which include both queries and clicked URLs issued through the query.
Table 3 uses an example to show the format of the data for experiments.
In this work, we use the classical n-gram feature in the Bag of Words model (BOW) [21] to represent each query and extract title for clicked URLs [31].
Then each user behavior is represented as a
 divided into five folds, each time four of them are used for training and the remaining one for testing.
The results reported in the remaining part of this paper are the average of the five runs.
Session Length Num Table 1.
The statistics of session length in the dataset.
>24



 Table 2.
The distribution of intent labels in the dataset.
Label plan travel communicate Shopping Find fact Entertainment Online Services Download file Others Percentage







 Table 3.
An exemplar data record in the dataset.
User Session Query Clicked URLs Label




  

 Resorts Atlantic City Casino Hotel hotels in atlantic city   trey songz freestyle http://www.resortsac.co m/  plan travel http://www.resortsac.co http://www.achotelexper m/hotel ts.com/ http://tickets.amtrak.com /itd/amtrak   http://www.youtube.com /watch?v=afZPuYujLA0 http://www.youtube.com /watch?v=fULaXDW6v
   plan travel plan travel plan travel   entertainment entertainment       Evaluation metrics.
In this work, we use the classical Precision, Recall and F-Measure to evaluate the effectiveness of deifferent classification models, where the Precision, Recall and F-Measure are defined as follows: # correctly classified   Category i )( # classified Recall = F-Measure = *2 Precision = queries #* queryies category # total queries queries # correctly queries classified queries total # precision precision *   recall recall Baselines.
Our proposed SHDCRF model is compared with three baseline models, which are Support Vector Machine (SVM) [8], which assumes the queries in a user search session are independent, the classical Conditional Random Field (CRF) [20], which considers the sequential information and the Latent-Dynamic Conditional Random Fields (LDCRF) [29], which assigns a disjoint set of hidden state variables to each class label in advance.
In addition, we also compared the proposed SHDCRF model in a semi-supervised problem configuration, which is named as the semi-supervised SHDCRF model (denoted as SHDCRF*) through incorporating 20,000 unlabeled user search sessions.
The detailed configuration for each baseline model is:   SVM.
In our experiments, we use SVM-light [18] as the toolbox for model training and testing.
The SVM model is trained using a linear kernel.
The parameter C is determined by cross-validation and the results reported in all experimental results are the parameter configurations for the best results   CRF.
The Conditional Random Field model we used for experiments is a single chain structured model [20], and the regularization term in CRFs is determined by cross-validation.
In our experiments, we use the CRF-Suit [26] as the tool to obtain the experiment results.
  LDCRF.
The Latent-Dynamic Conditional Random Fields (LDCRF) [29] was trained by varying the number of hidden states per label, say, from 2 to 6 states per label, and the regularization term in LDCRF was determined by cross-validation to achieve the best performance for comparative study.
The experiments are conducted using a 5-fold cross-validation and the experimental results reported in this subsection are the average of five runs.
The performances of different algorithms for user intent classification are shown in Table 4, where all the models with hidden variables (LDCRF, SHDCRF, SHDCRF*) are set to have the same number of hidden variables, which is set to be 32 in this group of experiments.
The parameter   in the SHDCRF and SHDCRF* models is set to be {0.001, 0.005, 0.01, 0.05, 0.1} respectively and the best performance of different parameter settings are reported to compare with the baselines.
As shown in Table 4, our proposed SHDCRF and its semi-supervised configuration, i.e. SHDCRF* models, outperform the baselines.
The performance of SVM model is the worst among all the baselines since the SVM model does not utilize any context information for classification.
In terms of F-measure, our proposed SHDCRF model can relatively improve the performance as high as 12.4% in contrast to the classical CRF model, and 3.5% in contrast to LDCRF model.
Through incorporating the unlabeled data, we could obtain the better empirical data distribution and the proposed SHDCRF* model improves the performance as high as 1.3% in contrast the SHDCRF model Table 4: Performances of different algorithms for user intent understanding.
Method




 Precision









 Recall









 F-Measure









 To verify the statistical significance of our experiments, we perform the paired t-test (2-tail) over the F-measure of the experimental result.
As shown in Table 5, all the t-test results are less than 0.01, which means the improvements of SHDCRF and SHDCRF* models are statistically significant in contrast to the baselines.
t-Test

 Table 5: Paired t-Test (2-tail) results.
There are two importance parameters to be analyzed in our proposed model.
First, the number of hidden states is an important parameter in both LDCRF and SHDCRF models.
In Figure 3, we show the performance of LDCRF, SHDCRF and SHDCRF* models with different number of hidden states, which are 16, 24,
 SHDCRF* models achieve better performance than LDCRF in most cases.
The only exception is that when the hidden states number is small, their performances are comparable.
This means that with a reasonable large number of hidden variables, our proposed model can consistently outperform the baseline model.
From this figure, we can also observe that the effect of this parameter almost converges when we increase the number of hidden variables, i.e. the performance of our proposed model gets stable with the increasing of the hidden variable number.
e r u s a e
  







 #Hidden States


 Figure 3.
Performance of LDCRF, SHDCRF and SHDCRF* with different number of hidden states.
Another parameter we need to exploit is the parameter , which is first introduced in Eqn.
(9) and used to determinate the strength of the sparse relations between the hidden variables and intent class labels.
The experiment results using different values, {0, 0.001,
 given in Figure 4 and 5 respectively.
From these results, we can observe that the parameter , without being set to be 0, has very limited impact on the proposed models if the number of hidden states is given.
Among the results with slight differences, we assign the parameter value 0.1 or 0.05, which empirically gives the best performance.
However, when setting the performance becomes pretty bad.
It can be interpreted that the model parameters trapped into the bad local maxima during the learning phrase.
From Figure 4 and 5, it can be concluded that the sparsity condition in SHDCRF is essential for avoid trapping into the bad local maxima and ensuring good experiment results.
to be 0, e r u s a e
  






  = 0.1  = 0.05  =0.01  =0.005  =0.001  =0




 #Hidden States Figure 4.
Parameter sensitivity analysis in SHDCRF model.
r u s a e
  






  = 0.
 = 0.
 = 0.
 = 0.
 = 0.
 =0 #Hidden States Figure 5.
Parameter (cid:2745) sensitivity analysis in SHDCRF* model.
hidden variables.
The gray of the cell at i(cid:2929)(cid:2930) row j(cid:2929)(cid:2930) column reflects the conditional probability of hidden variable h(cid:2920) given the intent class label y(cid:2919),which can be denoted as p(cid:4666)h(cid:2920)|y(cid:2919)(cid:4667).
In addition, we analyze the relations between intent class labels and hidden variables with parameter   setting to be 0 and 0.05 respectively, when our SHDCRF model incorporates 24 hidden variables.
In Figure 6 and 7, there are two charts both containing 8*24 small cells, which indicates 8 intent class labels and 24 variables with parameter (cid:2745) = 0.
Figure 6.
Relations between intent class labels and hidden variables with parameter (cid:2745) = 0.05.
Figure 7.
Relations between intent class labels and hidden p(cid:4666)h(cid:2920)|y(cid:2919)(cid:4667)(cid:3408)0.1 with In figure 8 and 9, we show the black cells in the two charts if the corresponding conditional probability parameter   setting to be 0 and 0.05 respectively.
It clearly shows that sparsity condition in SHDCRF could generate sparser relations between intent class labels and hidden variables than without the condition.
hidden variables with parameter (cid:2745) = 0.
Figure 8.
Sparse Relations between intent class labels and hidden variables with parameter (cid:2745) = 0.05.
Figure 9.
Sparse Relations between intent class labels and
 Variables i.e.
the hidden variables, explainable.
In One of the major advantages of learning the sparse structure between hidden variables and intent labels is to make the substructure, this subsection, we use the case studies to analyze the hidden state variables in SHDCRF model.
Through these case studies, we show that the hidden substructure discovered by SHDCRF model could help us define new finer scale user intent labels.
To simplify the case studies and without loss of generality, we arbitrarily trained the SHDCRF model using 24 hidden variables.
The analysis for the hidden states in SHDCRF model is reported in Table 6.
In this Table, the column  Label  indicates the user intent labels, which could be  plan travel ,  communicate ,  Shopping  ,  Find a fact ,  Entertainment ,  Find Online Services  ,  Download file  or  Others .
The column  H  indicates the index of hidden state variables in the model.
For each hidden state h(cid:2919), we sample some queries that satisfy (cid:1860)(cid:3036)(cid:3404)(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3035)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667).
Then we give some example queries in the  Example Queries  column of this table.
In addition, three human editors are asked to tag these sampled queries with a new label, which is in the column  Tag .
The column  Acc%  shows the accuracy of these sampled queries that can be tagged with the new label.
Take the intent label  Plan travel  as an example, in SHDCRF model, it is divided into two substructures  Map  and  Rental & book hotel  responded to the two hidden states respectively.
As shown in the column   Acc % , the accuracies of sampled queries which can be tagged with   Map  and  Rental & book hotel  is 85.5% and 86% respectively.
As shown in Table 6, we can also find some other interesting substructures discovered by SHDCRF model.
For instances the intent label  Shopping , in SHDCRF model, it is divided into  Home shopping , which indicates buying something for family and home, and  Entertainment shopping  , which indicates that user wants to concert etc.
In this paper, we present a novel Sparse Hidden-Dynamic Conditional Random Fields model for user intent understanding from user search logs.
The SHDCRF model aims to learn a substructure for each intent label which is used to model the intermediate dynamics between user intent labels and user behavioral variables.
In addition, we propose to learn sparse relations between the hidden variables and intent labels in SHDCRF to scale up the computation and make the hidden state variables explainable.
Extensive experimental results show that the proposed SHDCRF model gives much more accurate intent prediction results in user search sessions than some existing state-of-the-art methods including Support Vector Machines, CRFs, etc.
Latent-Dynamic Discriminative Models Table 6: Substructure discovered using SHDCRF.
