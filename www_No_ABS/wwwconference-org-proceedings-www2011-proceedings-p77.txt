Finding information on the web often amounts to  nding the right URL.
Proactively suggesting links that are relevant to users  current information needs is therefore likely to lead to higher user satisfaction and allow the users to accomplish their goals faster.
In this paper we propose two novel link suggestion approaches for the two main ways to  nd information online, namely, web search and browsing.
In response to a navigational query [3], search engines strive to provide the URL to which the user likely wants to navigate.
However, many navigational queries still have some amount of ambiguity.
For example, when submitting the query  P.F.
Chang s  (a chain of Chinese restaurants in the U.S.), the user may be interested in  nding the closest Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Figure 1: Example of link suggestions in a search result page for the navigational query  P.F.
Chang .
restaurant, checking the menu, booking a table, or ordering a takeaway.
The search engine cannot possibly determine the right alternative given the very short query.
What it can do, however, is to surface direct links to the most likely of these options, by showing them beneath the main URL, www.pfchangs.com (cf.
Figure 1).
These suggested links are known as quick links, and today all major search engines o er this functionality.
Link suggestion was also found to be useful in many scenarios beyond web search, including web page pre-fetching [14] and site navigation [20].
As a result, link suggestion systems have received considerable attention in a variety of commercial systems, from major search engines to companies distributing pre-fetching technology1.
Previous work on link suggestion mainly focused on exploiting logged user browsing behavior to achieve strong performance.
Quick link suggestion uses clicks logged in toolbar data to determine relevance [5].
Similarly, pre-fetching systems often use site-level access logs to suggest links for pre-fetching [14].
Although these techniques are adequate for sites with su cient tra c, performance can su er when such data is scarce or does not exist at all.
For example, quick links are available for popular restaurant chains such as  P.F.
Chang s , but not available for  Tarzana Armenian Deli .
Unfortunately, su cient tra c is often a luxury possessed by a relatively small number of sites; low tra c is the norm.
To address this limitation, we broaden the scope of link suggestion techniques beyond tra c-based solutions.
In the context of quick links, we extend tra c-based models to include non-tra c signals based on page and site layout.
We also cluster sites to leverage similarities between categories
 p.f.
changsearchP.F.
Chang's China Bistro (Nasdaq: PFCB) P.F.
Chang's China Bistro.
... News/Events | Investor Relations | Privacy Policy | Terms of Use | About Us | Site Maphttp://www.pfchangs.comLocationsWarrior Card InfoChef s CornerCareersOrder OnlineNews & EventsContact UsOur BarWWW 2011   Session: Web MiningMarch 28 April 1, 2011, Hyderabad, India77of sites.
For example, restaurant web sites should include a  menu  quick link.
Together, our techniques allow us to perform quick link suggestion for a much larger set of sites; in principle, we can provide a quick link to virtually any page.
We also introduce a task similar to pre-fetching, which we call dynamic quick links.
While the use of static quick links is limited to Web search, in dynamic quicklinks, we condition the recommendation of quick links on the page the user is currently reading.
One way to present dynamic quick links is by providing a tool (e.g., a pop-up window) suggesting links to browse next.
Tra c-based models similar to those used in pre-fetching can be used for dynamic quick link suggestion.
We also exploit both site-level and link-level clustering to improve performance.
The contributions of this paper are threefold.
First, we extend the existing quick links paradigm and propose an approach to compute quick links for any web site, including tail sites as well as brand new sites.
Second, we formulate the notion of dynamic quick links, and propose an approach for assisted browsing, e ectively anticipating which link the user will choose next from any given page.
In both of this scenarios, we propose methods that are applicable to any site, including those for which little or no historical browsing data is available.
This becomes possible due to our use of non-tra c-based data based on page and site structure, as well as clustering of Web sites to address data sparsity.
Finally, our experimental evaluation con rms the validity of our method using editorially labeled data as well as real-life search and browsing data from a major US search engine.
The static quick links task refers to the problem of selecting and ranking links for a user entering a web site.
It is de ned by a set of sites, S. Each site, s   S, has a set of candidate quick links, U(s).
In our work, U(s) consists of all the links contained on the web site s homepage p. For each u   U (s), there is an unobserved binary relevance rs(u)   {0, 1}.
Given s   S, we would like to select and rank a set of k urls from U(s) so as to maximize the relevance of this set.
We remain agnostic about performance measures of relevance, studying several in our experiments.
The dynamic quick links task refers to conditioning our selection and ranking of k urls on the url u(cid:48)   U (s) that the user is currently browsing.
A key question in developing link suggestion algorithms is whether to make them query dependent or not.
Choosing the query dependent route is bene cial for Web search, as it uses additional information contained in the query.
Yet this also comes at a cost, as this approach increases the amount of computation to be done for each query, a critical consideration for search engines handling hundreds of millions of queries each day.
Query independent approaches are also more general, as they can equally apply to browsing scenarios, where no explicit query is available.
For the sake of uniformity and computational e ciency, in this paper we opted to focus on query independent approaches.
We adopt a machine learning approach to address the static quick links task.
In general, machine learning approaches learn a relationship between task instances and a desired target value.
In our situation, each u   U(s) is an instance and the desired target value is its relevance, rs(u).
In order to generalize between instances, machine learning approaches compute abstract features of each instance and learn the relationship between these features.
Learning is performed by using a small set of training instances which have labeled target values; in our case, we assume access to a small set of sites S t   S whose URLs have the relevance values, rs(u), provided.
Such an approach requires that we de ne two things: how to compute instance features and how to model the relationship to the target.
We followed two principles when designing features.
First, obviously we would like features of u which correlate with rs(u).
Second, because we are interested in strong performance on both head and tail sites, we would like features well-represented in both head and tail sites.
For example, restricting ourselves to click-based features results in poor representations for tail sites.
We refer to features well-represented in both head and tail sites as common features, and features better represented in head sites (tending to be sparse in tail sites) as head features.
It is worth noting that head features carry critical information about how popular links are or how often links are used [5].
For each u   U(s), we generate three sets of common features.
URL-based features are extracted from a URL address of u, for example, the depth of the URL path, the type of URL  le extensions (e.g., html, jpg, php), etc.
Anchor text-based features are extracted from anchor text used for u in the homepage p of that site, for example, how many named entities are in anchor text w, how many nouns or verbs are in anchor text, etc.
Notice that these are functions of the text not term features often used in the information retrieval and text classi cation literature; we do this to provide generalization across di erent types of sites.
DOM block-based features are extracted from the Document Object Model (DOM) block b (of homepage p) that u belongs to; for example, the ratio of bytes of text to the number of links in b, the position of b in DOM block order, etc.
For each candidate quick link, we generate two sets of head features.
Link structure-based features are extracted from hyperlink structures of the whole Web graph, for example, the number of incoming links to u.
User behavior-based features are extracted from user behavior data such as toolbar logs, for example, the number of visits to u over a certain period of time.
We expect head features to be extremely sparse or nonexistent for tail sites.
In subsequent sections, we will refer to the features of u as  s u.
We would like to model the relationship between a candidate quick link s features,  s u and its relevance, rs(u).
To accomplish this, we cast our task as a regression problem.
That is, we would like to learn a function h whose domain is the site and candidate quick link and whose range is rel-(h(s, u)   rs(u))2 E(h,S t) = (1) (cid:88) (cid:88) s St u U (s) The general regression problem is to select a function  h such that,  h = argminh HE(h,S t) (2) The hypothesis space, H, is the set of possible functions which  t a particular functional form.
In order to perform learning, we need to de ne H and describe how we search it.
In our work, each h   H is de ned as a decision tree forest [16] composed of m trees such that, h(s, u) =  0f 0( s u) +   +  mf m( s u) (3) where f i is a regression tree,  s u represents the features generated for candidate u of site s, and  i is a parameter controlling the contribution of fi to the prediction.
Regression trees are appropriate for our task because they can address both numerical and categorical features and have been shown to be highly e ective for ranking tasks [28].
Because  nding the exact solution to Equation 2 for our hypothesis space is NP-Complete, we apply Friedman s Gradient Boosted Decision Tree (GBDT) algorithm to search the space [16].
The GBDT algorithm searches H using a boosting approach.
GBDT begins with an initial function f 0 that is usually an average value of labels of all training samples.
The subsequent trees, f i, iteratively minimize the L2 loss with respect to the residuals of the predicted values and the target values.
Each weight, wi is a monotonically decreasing function of i, parametrized by a value,  , referred to as the learning rate.
In addition to  , the model has two other parameters: the number of trees and the number of nodes per tree.
We induce a ranking of quick links U(s) by computing  h(s, u) for each u   U (s) and ranking by the prediction.
In the previous section, we performed ranking of quick links for each site separately, and no information was shared between similar sites.
We now turn to exploring the similarities between di erent sites.
Consider two sites s and s(cid:48), both of which are restaurants.
We know that, for sites of the class  restaurant , quick link candidates with anchor or URL text containing the term  menu  should receive the same relevance.
That is, given two quick link candidates from sites in the same class, similar candidates will have similar relevance.
In order to exploit site classes, we need to classify sites in the  rst place.
We accomplish this by clustering sites using a term-based representation.
Let ws be the |V|   1 term vector for site s; terms are extracted from anchor text and URL paths of links in pages.
Sites are then clustered using the di usion wavelet approach introduced in [26].
This method works by constructing a term-term co-occurrence matrix from the bag-of-word representations of sites, i.e., by T TT where T is the |S| |V|  collection  matrix.
Then, using the di usion wavelet algorithm [9], we obtain wavelet  topic bases .
Each topic basis,  i, is a |V|   1 vector capturing the behavior of terms in a particular class.
We assign a site s to the class determined by argmaxi(cid:104) i, ws(cid:105).
The advantage of this approach is that it does not require that a  xed number of clusters be speci ed in advance.
However, other clustering approaches can be applied as well.
This provides a partitioning of sites, allowing us to learn class-speci c models.
For each class c   C, we are interested in training a class-speci c model, hc, which leverages similarities between sites.
In order to train a class-speci c model, we adopt the Tree-based Domain Adaptation (Trada) algorithm [7].
Trada begins by training a generic model as in Section 3.1.2.
The algorithm then modi es the generic model to minimize the loss function with respect to target values in a target domain; in our case, this target domain is a class of sites.
That is, we are going to minimize our loss function, constraining ourselves to those instances in class c,  hc = argminhc HcE(hc,S t c) (4) c is a set of relevance-labeled sites of class c.
where S t Equation 4 is equivalent to Equation 2 except for the set of training instances and the hypothesis space.
De ning Hc becomes the critical part of this technique.
As mentioned earlier, our algorithm needs to use features of quick link candidates that allow similar candidates to receive similar predictions.
Unfortunately, neither the common features nor the head features capture the semantic similarity of pairs of candidates.
We manage this by using term features.
The idea here is that while common and head features provide evidence for quick link relevance in general (e.g.,  highly visited candidates are relevant ), term features provide class-speci c evidence (e.g.,  for restaurants, candidates whose url contains  menu  are relevant ).
As a result, we de ne Hc such that hc(s, u) =  h(s, u) +  0f 0 c (wu) +   +  m(cid:48) f m(cid:48) c (wu) (5) where  h is the generic model approximated in Section 3.1.2, which is  xed for all hc   Hc, and wu represents the bag of words associated with candidate u.
Except for the addition of  h(s, u), Equation 5 is identical to Equation 3 and, as a result, Trada searches Hc using the boosting approach described in Section 3.1.2.
Because Equation 5 uses fairly sparse term features, we need a substantial amount of training data for each class.
If the number of classes is large, then collecting many manual labels for sites in every cluster can be expensive.
In order to gather su cient training data, we bootstrap by automatically labeling unlabeled sites and quick link candidates.
That is, for each cluster, we  rst use the generic model to predict relevance scores of links in unlabeled sites.
Next, we assign pseudo-labels to the links, e.g., links in the top 30% are relevant while links in the bottom 30% are non-relevant.
Finally, we apply the Trada algorithm with these pseudo-labels.
The main advantage of this approach is that we can cheaply leverage an enormous number of homepages on the Web.
This approach demonstrated strong performance in the context of vertical selection [1].
The dynamic quick links task allows us to adjust the ranking of links depending on the context of the user.
In this case, context is de ned by the current page u   U(s).
Consider a user reading the  menu  page of a restaurant.
If we have observed many other visitors navigating to the  directions  page immediately after reading this page, then there in this context.
In order to address the sparsity of browsing data for tail sites, we leverage information from semantically related quick link candidates.
To accomplish this, we cluster quick link candidates within our site classes C. Our quick link clustering algorithm will use term-based representations, resulting in clustering links with related text (e.g.,  directions  and  location ).
Speci cally, we use anchor text and words in URL paths.
Although we could perform an unsupervised clustering method as in Section 3.1.3, here we have unique data that we can exploit to direct the clustering.
We hypothesize that, given two sites in the same site class, two links are semantically similar if they share a similar number of visits.
So, given two arbitrary restaurants, the two  menu  quick links should receive comparable numbers of visits.
In practice we normalize the number of visits by the number of site visits in general so that we can compare links from head and torso sites.
Our representation is term-based, our supervision is a real valued number (as explained above), and our clustering method is supervised Latent Dirichlet allocation (LDA) [2].
Supervised LDA projects each training instance into a k-dimensional  topic space , represented as a multinomial distribution over topics; that is, for each u, we have a distribution p(c|u) over all c   C.
Once we have abstract representations of links, we investigate browsing behaviors between links or the representations.
We make a Markov assumption about link transition: the class of the next link to be browsed depends only on the class of the current link.
Assume B is our browsing data encoded as url transitions.
We compute the empirical distribution of transition probabilities from quick link class ci to cj as, (cid:80) (cid:80) u u(cid:48) B p(ci|u)p(cj|u(cid:48)) u u(cid:48) B p(ci|u)p(ck|u(cid:48)) ck (cid:80) Pij = While the estimated random walk matrix represents only one step of browsing, it may be bene cial to encode multiple steps of browsing.
This is because some users may prefer shortcuts from one link to another link that is several hops away instead of going through several intermediate links that most users follow.
To model this, we construct a new random walk matrix as follows: T(cid:88) a=1


  a 1Pa where Z is a normalization factor,   is a shrinkage parameter, and T is the maximum number of steps to consider.
Given this quick link transition matrix, we are now going to rank the quick link candidates.
Assume zu = [p(c0|u), p(c1|u),  , p(ck 1|u)] is the k 1 topic vector of the current url the user is reading.
We  rst compute scores for the possible classes of the next quick link as,  zu = RTzu To  nd links relevant to u, we compute cosine similarity between  zu and topic vectors of each v   U (s).
Because this similarity captures only textual properties of quick link candidates, we combine the cosine similarity with the GBDT prediction, which is based on additional types of features (cf.
Sections 3.1.1 and 3.1.2): f (s, u, v) =   h(s, v) + (1     )(cid:104) zu, zv(cid:105) where   is a parameter.
Candidate links are then ranked by f (s, u, v).
We constructed two sets of homepages: a collection of pages from popular Web sites with rich user tra c information from search logs or toolbar data (head set), as well as a collection of less visited pages (tail set).
Note that in our experiments, candidates for link suggestion in a given site is restricted to links available in its homepage, thus we often use the words homepage and site interchangeably.
For the head set, we randomly sampled 5,153 sites among popular web sites for which (a) Yahoo!
Search was providing quick links on search result pages, and (b) there was su cient user tra c information from toolbar logs and click logs.
For the tail set, our goal was to construct a collection of homepages that were not as heavily visited.
We were not aware of an existing index of all homepages online from which to sample.
Thus, we constructed our dataset by extracting the most relevant search results for navigational queries.
We  rst sampled 100,000 million queries from query logs of Yahoo!
Web Search and applied an internal navigational query classi er trained on manually labeled examples.
For more details on the classi er, see [10].
For each query classi ed as a navigational query, we identi ed its most clicked url.
We refer to such urls as homepages.
We then restricted to those homepages that were not heavily visited (less than 1,000 clicks recorded in a month-worth of click logs), but have more than 10 outgoing-links (otherwise link suggestions are not very usuful).
We collected a total of 14,332 homepages from tail sites.
Simple statistics over the head and tail sets are shown in Table 1.
Note that typically there are a large number of candidate links from the homepage of a given site: on average a head site has 173 links on its homepage.
Table 1: Statistics of head and tail sets #sites Avg.
#links Head set Tail set Labeled Unlabeled Total





 per site

 Dynamic quicklinks experiments used the same dataset and split as static quicklinks.
We considered the bigger set as the training data   note that the original labels were not relevant here.
We took one month of Yahoo!
toolbar logs and extracted all browsing sessions which included the relevant homepages.
We segmented the sessions according to rules similar to those used in [5].
For example, a session is terminated if any two consecutive clicks are longer than 10 minute apart or a Back button is clicked.
URLs that were not in the candidate set were removed from the sessions.
In principle, performance of link suggestions on head sites can be evaluated using actual user tra c information.
But since we do not have su cient user tra c information for collections for manual annotations, where editors were asked to select up to 10 useful destinations for each site among all links in its homepage.
The selected links were considered to be relevant, the rest were considered to be non-relevant.
In terms of metrics, we considered four standard evaluation metrics in information retrieval: precision, recall, F1 score (the harmonic mean of precision and recall), and mean average precision (MAP).
Following previous work [5], we assumed 8 links will be provided as quick links for a homepage on the search result page.
Accordingly, we used 8 as the cut-o  point for all metrics and report precision, recall, F1 and MAP at 8 (P@8, R@8, F1@8 and MAP@8, respectively).
We performed the permutation test and considered an improvement statistically signi cant if p-value < 0.05.
In order to evaluate our class-based model, we focused our experiments on clusters that were large enough to have e ective smoothing among pages in that cluster.
Out of the 49 clusters in our dataset, the top 10 clusters cover more than 60%, each with over 314 sites.
Note that in a real system, the number of considered sites could be much larger and the number of reasonably-sized clusters might increase as well.
Since it would be more di cult to obtain manual labels for the dynamic link suggestion task, evaluation used tra c information that was available for sites in the head set.
We predict the next link to be visited given the current link in each user session in the test data.
Given the motivation for this task, we evaluated the quality of the top 4 predicted link in each case.
Let Q4 u   U (s)   u be the set of predicted links.
If B  is our observed browsing data, then de ne our browsing metric as, (cid:80) u v B  I(v   Q4 u)

 where I is the indicator function.
We consider three baseline systems for the static quick-links task.
Two baselines represent simple ways of estimating the  popularity  of a candidate link.
One estimate is derived from the Web graph and ranks candidate quicklinks according to the number of incoming links from the entire web.
We refer to this baseline as # inlinks.
Another estimate of popularity can explicitly use visitation data.
In this case, candidate quicklinks are ranked according to the number of visits registered in one month of Yahoo!
Toolbar logs.
We refer to this baseline as # visits.
A much stronger comparison point is the state-of-the-art technique recently introduced by Chakrabarti et al.
[5] (see Section
 outperform other methods such as ranking by the number of visits recorded toolbar logs.
We denote this system as Greedy.
We tested three versions of our static quicklinks models.
The basic Gbdt model was trained with two sets of features.
Runs labeled Gbdt-C use only common features, while runs labeled Gbdt-HC use both common and head features.
As a baseline for adaptive modeling, we consider an adaptive model which uses additional pseudolabeled data and features without clustering sites.
That is, we performed the adaptation algorithm described in Section 3.1.3, but treated the entire dataset as one cluster.
This baseline is labeled Adapt-all.
Runs labeled Adapt-cls build class-speci c models with the additional pseudolabeled data and features.
For Gbdt models, the shrinkage factor was set to 0.05, and other parameters were chosen by 10-fold cross validation.
Our class-based model required additional parameters.
Recall from Section 3.1.3, once we clustered homepages into di erent classes, the original Gbdt-C model is applied to each cluster in the tail unlabeled set to obtain pseudo-labels.
10% of the homepages in the unlabeled tail set were set aside as the held-out set to estimate the two parameters for new regression trees, i.e., the number of trees and the number of nodes.
They were chosen to minimize the loss function with respect to the pseudo-labels on the held-out set.
Tree adaptation using the TRADA algorithm was performed on the rest 90% of homepages in the tail unlabeled set.
For dynamic quicklinks, our random walk parameters, i.e.
  and   were tuned on the training data except for T which we set to 3.
Topic models for clusters are learned using the collapsed Gibbs sampler [18].
We  x free parameters as follows: the number of topics K = 20, the Dirichlet hyper-parameter   = 0.01 and the variance of the response variables  2 = 0.09.
We evaluated the performance of our algorithms separately on head and tail sites.
On head sites, we compare the proposed technique with the three baseline systems (# inlinks, # visits, Greedy).
Since some of these systems relied heavily on head features, we expect them to be strong baselines.
Because these baselines rely on features that are scarce in tail sites, we compare performance to Gbdt-HC which, as we will see, can be considered a strong baseline.
Table 2(a) summarizes performance on head sites.
The two simple baselines, # inlinks and # visits, perform better than random but signi cantly underperform Greedy.
However, Gbdt-HC, which uses both head and common features, yielded the best performance across all four metrics.
Interestingly, the Gbdt model using only common features (Gbdt-C) performed comparably to Greedy.
Table 2(b) summarizes performance on tail sites.
The Gbdt models used to obtain these results were trained on the labeled head set.
Consequently, these results show that our model can generalize from head sites to tail sites.
We also performed 10 fold cross-validation on the tail labeled set and observed almost identical trends.
We ran separate experiments to evaluate the performance of our class-based adaptation algorithm.
These results are presented in Table 3.
Adapt-cls outperforms Gbdt-C for eight out of ten clusters.
In order to con rm that our improvements were not the result of merely adding term-based features, we compared performance to Adapt-all, a model which incorporates the additional term-based features without site clustering.
As we can see from Table 3, Adapt-all did not yield any improvement over Gbdt-C.
In fact, aggregated over these ten clusters, Adapt-cls outperforms both Gbdt-C and Adapt-all across all four metrics, sta-indicates a statistically signi cant di erences from both of two weak baselines, # inlinks and # visits while a   indicates a statistically signi cant di ernece from a strong basline, Greedy.
(a) head sites (b) tail sites Method
 # inlinks
 # visits

 Greedy
 Gbdt-C Gbdt-HC 0.4361 
















 Table 3: MAP@8 of quick link selection for each class (cluster) of sites.
A   indicates a statistically signi cant improvement over both Gbdt-C and Adapt-all in aggregation.
Performance on other metrics follows the same trends.
Cluster Gbdt-C Adapt-all Adapt-cls









 total
































 tistically signi cant in three cases.
This provides empirical evidence for our hypothesis  similar types of sites have links corresponding to similar functions as good links .
In our dynamic quicklinks task, we adopted the best static quicklink model, Gbdt-HC.
For each site class c, we trained Gbdt-HC with all labeled examples except for those in c and tested on c. We removed the current URL from the candidate set for each test trail.
Experimental results are shown in Table 4.
In seven clusters, the dynamic link suggestions outperformed the static link suggestions by large margins.
In the other three clusters, the static link suggestions and the dynamic link suggestions showed little di erence.
In aggregation, the dynamic link suggestions yielded much better performance than the static link suggestions, and the improvements are statistically signi cant.
This demonstrates that modeling user browsing patterns over types of links achieves more accurate link suggestions.
For reference, we also conducted experiments examining the upper-bound of performance over head sites.
That is, for sites where we have su cient user browsing data over individual links, can we reasonably predict where a user is going to go next based on what other users have done in this same site?
Maybe there is a huge variance among users in this dynamic scenario, that there is not much room for improvement?
To this end, we computed a transition matrix over individual links for each site, counting transitions among
 Method Gbdt-HC 0.4308 Gbdt-C








 Table 4: Results of link suggestions considering user browsing patterns according to types (clusters) of sites.
 static  and  dynamic  represent the best performing model in the previous section, i.e., Gbdt-HC and the random walk-based approach, respectively.
The numbers are B4 values.
A   indicates statistically signi cant improvement on  static  in aggregation.
cluster (#transitions)









 total (1094) (4694) (6827) (4840) (3269) (32522) (840) (3449) (7267) (2739) (67541) static










 dynamic










 physical links in user trails.
The next links were predicted as the most likely outgoing links from the current position.
Note that in this case a transition matrix built for a given site is completely not applicable to another site.
We performed 10-fold cross-validation for each site in the test set, using browsing patterns learned over one subset of users to predict for unseen visitors to the same site.
On average, we obtained an upper-bound of 0.654 in B4 value.
Indeed, on each particular site, there is a reasonable amount of regularity in terms of browsing behavior.
This suggests that we still have plenty of room for improvement.
Although we should also note that some of this regularity might be site-speci c and does not generalize to other sites.
In contrast, our models were built over fairly general representations, speci c only to a given site cluster.
The models, once learned, can be applied to any unseen sites classi ed into an existing site cluster, just as we applied our model to the test sites that were unseen in the training phase.
Our results for static quick links are compelling because they imply that tra c information, while su cient, is not necessary for strong performance.
This result also means that existing tra c-based approaches can be improved with belongs.
len(s) is the length of text in s, measured in bytes.
(a) Gbdt-HC Feature # of visits to u # of u in p # of incoming links to u len(u) the depth of the path of u Are u and p in the same domain?
len(b) / len(p) # of links in b / # of links in p # of links in b / len(b) # of images in b / # of links in b Weight









 (b) Gbdt-C Feature # of u in p len(u) len(b) / len(p) # of links in b / # of links in p # of links in b / len(b) # of images in b / # of links in b Are u and p in the same domain?
the depth of the path of u the position of b in p len(anchor text of u) Weight









 content-based features.
Furthermore, this result suggests that tra c-based approaches can be replaced entirely by an approach that uses only common features.
Consequently, web sites or search engines can present quick links even without having to maintain a Web graph or toolbar data.
It is worth noting that performance numbers on the head set (Table 2(a)) are lower than on the tail (Table 2(b)).
However, Table 1 demonstrates that there were fewer candidate links in tail sites compared to head sites, which e ectively makes the problem slightly easier.
We were also interested in the importance of the di erent features in Gbdt models.
Tables 5(a) and 5(b) show the top 10 most important features of Gbdt-HC and Gbdt-C, respectively.
The feature importance is computed based on how much each feature contributes to the loss reduction [16].
The top features of Gbdt-HC and Gbdt-C largely overlap, except for two head features (# of visits and # of incoming links to u) available only in Gbdt-HC.
Among the important common features, those based on page layout or link position dominate.
This indicates that homepages indeed tend to be designed with useful links made more salient to users, and that our features can e ectively capture such layout-based cues.
In order to explain the e ectiveness of common features, we compared the distributions of relevant and non-relevant links for head and tail sites.
We expect common features to behave similarly for both head and tail sites while head features should behave very di erently.
If the distributions are completely di erent, then it is unlikely we can apply a model learned on the head sites to the tail sites.
Figure 2(a) shows the distributions of the top 6 common features.
Note that the distributions in tail sites often follow similar contours as those in the head sites.
In contrast, Figure 2(b) shows the distributions of the two most important head features in Gbdt-HC (recall that all other important features for Gbdt-HC were common features).
Here the distributions in the tail sites are quite di erent from those in the head sites, lacking the di erentiation between relevant and non-relevant sets.
In fact, most head features in the tail set take a zero value due to the absence of user tra c information from which head features were extracted.
Our results for dynamic link suggestion, although only evaluated on head sites, demonstrate the e cacy of clustering links within a site class.
Because the lower dimensional representation of a link, zu, is not dependent on tra c information, we should be able to extend models to tail sites within a cluster.
Nevertheless, performing the tail evaluation for this task requires more work.
Editorial data is problematic because of the subjectivity in assessing a contextual suggestion (i.e., editors would have to assume the role of a user reading a certain page).
However, there may be some combination of editorial and log data that provides good evaluation.
Despite having focused on quick link suggestion for our experiments, it is worth inspecting the link clusters.
Table 6 shows an example of an estimated topic model for a site cluster related to  educational institutions .
Each link cluster looks reasonable.
For example, links about scholarship or  nancial aids make one topic (#14), while links about faculty or sta  directories make another topic (#10).
In addition to inspecting link clusters, we can inspect dominant transitions between clusters within a class of sites.
Figure 3 shows an example of dominant transitions in a random walk matrix estimated from sites related to  sports teams .
We can observe some interesting patterns, for example, users visiting links about ballparks subsequently visit links about ticket sale with some probability.
Also, links about multimedia clips follow links about fan forums.
These transitions look reasonable enough to be easily understood.
Our work explores two ways of generalizing quick link selection, and proposes link suggestion methods that are applicable to any web site.
Several prior studies demonstrated the usefulness of link suggestion.
Juvina and Herder [20] used a user study to show that link suggestions help users navigate the web in a structured manner.
White et al. [27] showed that di erent types of suggestions (e.g., query suggestion vs. link suggestion) are better suited for di erent types of tasks.
Link suggestion has been applied to web site design and organization.
Perkowitz and Etzioni [23] addressed automatically generating index pages that contain useful links, re ecting evolving site usage patterns.
Srikant and Yang [25] studied the scenario when the real location of a page can be di erent from where users expect it to be, and presented an algorithm selecting the expected locations.
Doerr et al. [12] related to  educational institutions  by the supervised LDA model.
Terms are stemmed by the Porter stemmer.
Topic Topic terms



















 email mail login webmail e employ job hr career human school counti org museum scienc download project org test softwar research administr bookstor univers presid student servic center career health gov counti us educ court class schedul cours blackboard regist event new calendar emerg newsroom faculti sta  contact directori us academ program degre school graduat admiss appli student prospect undergradu map campu direct visitor tour student aid  nanci current scholarship us contact about polici privaci life z campu student hous calendar academ event orient registrar librari athlet univers scienc school rss new how feed get alumni give parent friend famili and Kranakis et al. [21] proposed algorithms for suggesting shortcuts between pages in a web site by analyzing web logs.
Chakrabarti et al. [5] were the  rst to discuss quick link generation as link suggestion.
The authors approached the quick link selection problem as a combinatorial optimization problem, since the space on search engine result pages is limited, and only high value link suggestions should be surfaced there.
This study proposes an algorithm that is within a factor of (1   1/e) from the optimum.
Link suggestion has been also applied to URL pre-fetching.
Duchamp demonstrates that a popularity-based pre-fetching protocol can signi cantly reduce latency and bandwidth usage [14].
Subsequent work explored more e cient implementations [8], personalization [11], and longer-term modeling [13, 15].
This line of work requires availability of web site access logs, which can be scarce for tail sites or nonexistent for newly created sites.
The main limitation of the previous studies is that their techniques cannot be used for tail sites where there is not enough historical tra c information, such as site access logs or toolbar logs.
In this work, we tackle this problem by introducing a feature-based model that can be e ective even without such statistics.
Features we de ne are inspired by web page segmentation and template extraction studies [6, 22, 19, 24, 17, 4].
Structural features have also been proposed in a number of other studies.
Lin and Ho [22] and Gupta et al. [19] proposed algorithms to extract content blocks from HTML pages using a DOM (Document Object Model)-based approach and an information theoretic approach, respectively.
In the context of template extraction, Gibson et al. [17] studied the nature and prevalence of templates on the Web, introducing a randomized template extraction algorithm.
Chakrabarti et al. [4] formulated smoothing of a classi er for scoring DOM blocks and showed that their approach is e ective for tasks Figure 3: Example of transition between topics corresponding to clicked links.
These topics are estimated from sites related to  sports teams .
Terms are stemmed by the Porter stemmer.
such as web page classi cation or duplicate detection.
Although we use similar features to those introduced in the above studies, our work is di erent in that their techniques focus on DOM blocks while we emphasize the links in those blocks.
We learn a feature-based model using the Gradient Boosted Decision Tree (GBDT) algorithm proposed by Friedman [16].
More recently, this method was adapted for ranking by Zheng et al. [28].
We have demonstrated that tra c-based link suggestion solutions, while e ective, can be signi cantly improved using non-tra c-based data as well as clustering.
These results imply not only that existing link suggestion systems can be improved, but also that their coverage can be extended to tail sites whose lower popularity often results in poorer performance on them.
There are several possible areas of future research.
As mentioned earlier, if we are going to evaluate dynamic quick link algorithms for tail sites, we need to develop techniques for moving beyond tra c-based evaluation.
We also think that there could be several improvements to our modeling, in terms of features, algorithms, and clustering.
We believe our approaches, though, suggest a compelling future research direction focusing on abstracting site and link semantics.
Our clustering of sites and links was heavily motivated by a hypothesis that groups of sites form cohesive classes of concepts (e.g.
 restaurants ,  universities ), within which there exist prototypical link classes (e.g.
for the  restaurants  concept,  menu ,  directions , and  reservations  links).
Our results support this hypothesis, and extensions to our models should certainly be explored.
The ballparkstadiumparkticketseatpriceteamcuppitcheryahoofantasibaseballrosterteamactivstatplayertransactfanforumclassicmediavideomorecommunfoundatdownloadteamcoachstaffWWW 2011   Session: Web MiningMarch 28 April 1, 2011, Hyderabad, India84utility of these abstractions can also go beyond simple link suggestion; we can imagine a system more intelligently reasoning about a class of sites and prototypical links in response a speci c user information need (e.g.,  nd me menus for restaurants within 3 blocks ).
