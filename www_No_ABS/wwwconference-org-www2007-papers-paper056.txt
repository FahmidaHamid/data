Modern businesses rely on accurate counts of web page-views and clicks to calculate growth rates and market share.
Per-user page-views in the millions (per month) and other suspicious statistics lead to the belief that a signi cant amount of tra c originates from robots.
Failure to remove robots can mislead businesses about their growth metrics.
Furthermore, we see higher temporal (month to month or day to day) variance from the robot population so failure to remove them also degrades statistical power [20] in comparing  elded systems to their beta counterparts.
In order to remove robots on a more principled basis, we need to have a better characterization of the distribution of click behavior.
One method of removing robots is to identify them with outliers and remove outliers.
Outlier removal using distributional methods proceeds by  tting a model to the observed distribution and then selecting a tail probability (say 0.1%) to use as a de nition of an outlier.
Using the model, we can then translate that probability into a statistically founded threshold of clicks and remove all  users  that exceed that threshold.
Currently, businesses use very conservative ad-hoc thresholds for robot removal.
Knowing the distribution more precisely would allow them to be more aggressive in removing robots and thus produce more accurate and stable metrics for business.
Most previous work in web tra c distribution modeling has been done for network caching and relay applications [7,
 that has been done did not appear to test a wide variety of distribution families and some authors used continuous distributions [9, 1], which are not appropriate for discrete (click and pageview) distributions.
Finally, the work appears not to have used rigorous statistical methodology.
For instance, many authors simply plot the observed distribution in log-log space (log of frequency of x versus log(x)) and then proceed to  t a straight line, using Pearson s correlation coef- cient [16] to measure goodness of  t.
This is completely incorrect because typically the points corresponding to low pageview and click counts represent millions of users and other points may only represent a single user.
Thus, at the very least, a weighted regression is called for.
Even so, out-liers can have large impact on regression  tting methods and no indication was given that robust regression methods were used.
Instead of regression, we use the maximum likelihood method (MLE) which is much more robust.
In our methodology, we try to  t a large number of distribution families to the data using the robust maximum-likelihood estimation [16] (MLE) method.
Then we evaluate the goodness of  t using the (log) likelihood of the data given the  tted distribution.
Since some models have more parameters that can over t the data, we use the Bayesian Information Criterion (BIC, [17]) correction to log-likelihood.
Essentially this measure requires that more complex models  pay for their complexity  by providing a better  t to the data.
In the case where we have nested models the likelihood-ratio test [16] is also used to see what other distributions are statistically indistinguishable from the winning one - thereby forming a set of winners.
Although inappropriate for count data we also use two continuous distributions (Inverse Gaussian [9], log-normal [10]) that have been used in prior work so as to compare their scores to the theoretically correct discrete distributions.
Graphical methods (plotting the data) are a great aid in statistics because in two dimensions the human eye is great at pattern detection.
However, in distribution modeling where some points have drastically di erent weight than others, the plots can be quite misleading.
Nevertheless, properties such as curvature can be discerned from plots and this is important for this paper since if the observed distribution has a curved form in log-log space, it favors scale-sensitive models over scale-free power law distributions.
The rest of the paper is organized as follows: section 2 summarizes previous work in modeling distributions for the web and other data sets with various discrete distributions (Zip an, Poisson, Negative Binomial) and simple mixture models.
Section 3 gives density functions for the candidate distributions we use in this paper.
Section 4 spells out our approach for parameter estimation and model comparison, section 5 describes the ten datasets and section 6 presents results for the various  tted distributions.
Power-laws, Zipf distributions and Pareto distributions have become somthing of a fad recently, being very popular in explaining all manner of data (city sizes, galaxy sizes [10], words: [23], incomes [21]).
We begin by carefully distinguishing these.
The term  power law  is inappropriately general for our application since it goes well beyond distributions to describe any functional relationship between y and x where y = axk, hence we will not use this term.
Zipf distributions are discrete distributions over a  nite set (1...N ) with probability mass function f (x; s, N ) =  (N ) 1 xs where

 is is a normalizing constant.
For s > 1 this distribution is normalizable even if N =   and becomes the Zeta distribution [19].
Some authors restrict the term  Zipf distribution  for the case s = 1 and call other cases  Zipf-like  or  Zip an .
Zipf distributions (s = 1) such as word distributions, have the scale-free property: i=1 Scale-free: A distribution f is scale free if for all values of x, the probability of 2x is half the probability of x.
If a distribution is not scale-free, we will term it as scale-sensitive.
Applied to word rankings this means that for all ranks, the probability of seeing a word at that rank is twice the probability of seeing a word with twice the rank (Zipf s law [23]).
Pareto [21] distributions are continuous analogues of the Zipf, with density given by f (x; k) = k 1 Previous work can be classi ed according to the types of distributions that were tried and also more subtly, the meaning of the x axis: in some work, the x axis refers to the value xk+1 .
of a random variate (e.g.
number of clicks) whereas in others it refers to a rank of that variable.
Table 1 summarizes previous work with respect to this classi cation.
Laherrere and Sornette [10] take yet another approach, modeling rank as a function of the random variate.
The Inverse-Gaussian [9], Log-Normal [10] and Weibull [10] distributions are continuous distributions yet they are being used to model discrete data.
The paper of Huberman et al.
[9] claims to have discovered a  strong law of sur ng : that the distribution of clicks is distributed as Inverse Gaussian (IG).
However, we believe this claim to be too strong.
In particular, for our datasets, we have found other distributions to o er statistically signi cantly better  t to the data than the Inverse Gaussian.
We doubt there is a single distribution that will  t all kinds of web sur ng let alone constitute a  law of sur ng .
Huberman also claims the Inverse Gaussian has theoretical motivation and argues that the utility of a web surfer mirrors that of economic options whose prices are known to follow an Inverse-Gaussian distribution.
They also show that the page-view distribution of URLs is  tted by an Inverse-Gaussian.
Their approach seems not to have tried a lot of distributions, let alone discrete ones - it seems only that they have considered the Inverse Gaussian because of its theoretical underpinning and then proceeded to see if it gives a good enough  t to the data.
Huberman s work is the only other we are aware of that points out the curvature of the distribution in log-log space thus deprecating the scale-free power law distributions.
Previous authors did not rule out better  ts by scale-sensitive curved forms in log-log space: they only demonstrated that they got a good-enough  t by a line in log-log space and thus concluded the distribution must be power-law.
Laherrere and Sornette justify their choice of Weibull because  tails of pdfs of products of a  nite number of random variables is generically a stretched exponential  [6].
However, it appears to us that there is a serious problem with their methodology.
They evalute goodness of  t in log-log space using Pearson s correlation coe cient which is incorrect since the plotted points with lower rank represent many more points than those with higher ranks.
Finally, a note on previous work in mixture and zero-adjusted models.
User web behavior can be naturally partitioned based on the presence or absence of a click, in other words zero click vs. nonzero click behavior.
The zero click class might be thought of as primarily containing robot traf- c, and the nonzero click class principally  human  tra c.
At the very least, if we see that a model  ts the data well except at the zero point, we can conclude that the excess of users with zero clicks may be due to a secondary phenomenon: that of robots.
This is the approach of mixture models in which the zero class is modeled by a (single-valued degenerate) distribution and the positive clicks are modeled by a discrete positive-valued distribution.
Such models are called zero-altered or zero-in ated and have been used in numerous applications to model data with an excess of zeroes or where the zero-class of the underlying process has special meaning (number of defects [11], number of dental cavities [14], and number of car crashes [12]).
In all of these domains, zero has a special meaning in that it is usually the default scenario indicating lack of an accident or problem.
Best t distribution Zipf k = 1 Measure Frequency Population Weibull Page-views Zipf-like, various k Entity Word City
 Search Queries Frequency Weibull
 Session Session User Page-views Clicks Clicks Page-views ZAZM X-axis Rank Population Rank Page-views Frequency Breslau [3] Rank Page-views Frequency Huberman et al. [9] Inverse Gaussian Frequency Huberman et al. [9] Inverse Gaussian Clicks ZAZM: Zero-altered Zipf-Mandelbrot Clicks Frequency Page-views Frequency Y-axis Frequency Zipf [23] Frequency Abdullah [1] Scarr, Ali (this paper) Scarr, Ali (this paper) Laherrere, Sornette [10] Author Table 1: Prior work in distribution modeling can be partitioned into those modeling the random variate x versus those modeling the rank of x.
The web papers di er subtly in that some are modeling clicks per user or clicks per session or searches per query.
Since this paper is concerned with modeling count data, in the form of user clicks, we limit our attention to discrete distributions; in particular the Negative Binomial [2], Zipf [23], Zipf-Mandelbrot [13] (of which Zipf is a special case), Logarithmic Series [22] and Yule-Simon [18].
The Poisson distribution which best  ts data when its mean and variance are equal is not used due to the fact our click data (table 2) have variance-to-mean ratios far in excess of 1.
The Inverse Gaussian [9], Weibull [10] and Log-Normal [10] distributions, which are continuous, are only included for comparison as they have been used to model web click behavior in the past.
In addition a class of simple mixture models are also considered, motivated by the fact that zero-click users may behave di erently to nonzero click users.
A number of  users  are actually robots that ping the Yahoo site every day to test if they are connected to the internet.
This is a di erent generative phenomenon than that of regular human search and hence justi es using a mixture model.
Zero-altered or zero-in ated models [8, 11, 14] for count data are mixture models that assume with probability p the only possible observation is 0 and with probability 1  p a discrete random variable is observed.
The zero-altered model mixes a degenerate distribution with point mass of 1 at zero with a truncated count distribution for example truncated Poisson, truncated Negative Binomial or any discrete distribution bounded below by 1.
On the other hand the zero-in ated model accounts for some of the zeros through the non-degenerate distribution ( e.g.
Poisson, Negative Binomial, etc...) and some through the degenerate (zero) distribution.
We now present formulae for the probability mass and density functions of the various distributions discussed above, by considering a random variable X.
Zero-Altered (ZA): The probability mass or density function for a zero-altered mixture model is de ned as: fX (x; p,  ) = 8< :
 x < 0 p x = 0 (1   p)fY (x;  ) x > 0 (1) where 0   p   1,   = ( 1,  2, .
.
.
,  k) is a vector of parameters and fY (x;  ) is any valid probability distribution on [1, ).
From the above, the random variable X takes the value zero with probability p and values greater than zero with probabilities (1   p)fY (x;  ).
Zero-In ated (ZI): The probability mass or density function for a zero-in ated mixture model is de ned as: fX (x; p,  ) = 8< :
 x < 0 p + (1   p)fY (0;  ) x = 0 (1   p)fY (x;  ) x > 0 (2) is any valid probability distribution on [0, ).
where 0   p   1,   is a vector of parameters and fY (x;  ) Negative Binomial (NB) (discrete): X   N B(k,  ), the probability mass function is: k (  (x + k)  (k) (x + 1) )k(1  k )x, x   0 (3) f (x; k,  ) = where k > 0 and     0.
To  t a  zero-altered  Negative Binomial model (ZANB) the probability mass function of a zero-truncated Negative Binomial distribution is required, this is de ned as:   + k   + k f0(x; k,  ) = f (x; k,  ) 1   f (0; k,  ) , x > 0 (4) From equations (1, 3, 4) the Zero-Altered Negative Binomial ZAN B(p, k,  ) probability mass function is: f (x; p, k,  ) = 8>< >:
 p (1   p)  +k )k(1  k  (x+k)( k  (k) (x+1)(1 ( k  +k )x  +k )k) x < 0 x = 0 x > 0 (5) Zipf (Z) (discrete): X   Z(s, N ), the probability mass function is: f (x; s, N ) =  (N )
 xs , x   (0, N ] (6) normalising constant.
where s > 0, N is  nite and  (N ) = 1/PN
 is is a Zipf-Mandelbrot (ZM) (discrete): X   ZM (s, q, N ), i=1 the probability mass function is:
 f (x; s, q, N ) =  (q, N ) (x + q)s , x   [0, N ] where s > 0, q   0, N is  nite and  (q, N ) = 1/PN is a normalising constant.
Clearly, setting q = 0 yields the Zipf distribution in equation (6) so nonzero q indicates curvature in log-log space.
Logarithmic-Series (LS) (continuous): X   LS(k), (7)
 (i+q)s i=1 the probability mass function is: f (x; k) = (8) Yule-Simon (YS) (discrete): X   Y S(k), the prob-log(1   k) , x > 0, k   (0, 1)  1 kx x ability mass function is: f (x;  ) =  B(x,   + 1), x > 0,   > 0 (9) where B(, ) is the Beta function.
Inverse-Gaussian (IG) (continuous): X   IG( ,  ), the probability density function is: f (x;  ,  ) = r   2 x3 exp(  (x    )2 2 2x ), x   0 (10) with ,  ,   > 0.
Log-Normal (LN) (continuous): X   LN ( ,  ), the probability density function is: x  2 2  
 2  exp  (log(x)    )2  , x   R (11) f (x;  ,  ) = with     R and   > 0.
The Log-Normal distribution is the probability distribution of any random variable whose logarithm possesses a Normal or Gaussian distribution.
In other words if Z   N ( ,  2) is a Normally distributed random variable then X = eZ has a Log-Normal distribution.
Weibull (W) (continuous): X   W ( ,  ), the probability density function is: f (x;  ,  ) =     ( x   ) 1 exp( ( x   ) ), x   0,  ,   > 0 (12) Zero-altered or zero-in ated models can be constructed using equations (1, 2) and any of the probability mass or density functions above.
Since we are particularly interested in the zero class, zero-altered models only are used for those distributions where x > 0, such as Zipf, Logarithmic series etc...
We now describe the method used to  t the distributions proposed in section 3 to our observed data.
In addition we also discuss how to compare di erent  tted models.
Maximum likelihood estimation e.g.
[16, 5], a standard statistical modeling technique, is used to  t the models.
It possesses a number of desirable properties and is a widely used parameter estimation tool.
Once various models have been  tted they can be compared using the Bayesian Information Criterion (BIC) [17] and graphical methods such as Cumulative Distribution plots.
Suppose we have an independant and identically distributed (i.i.d.)
sample XXX = (X1, X2, .
.
.
, Xn) with joint probability density or mass function f (xxx| ), where the data xxx = (x1, x2, .
.
.
, xn) and the parameter vector   = ( 1,  2, .
.
.
,  k).
Given observed values Xi = xi, i = 1, .
.
.
, n the likelihood is: L( |xxx) = f (xi| ) n Yi=1 (13) In the case of discrete data the likelihood measures the probability of observing the given data as a function of  .
The maximum likelihood estimate (MLE)  , is the value of   that maximises the likelihood i.e. makes the observed data  most likely .
In practice it is usually easier to equivalently maximize the log-likelihood: l( |xxx) = n Xi=1 log{f (xi| )} (14) As an example, consider an i.i.d.
sample from a Poisson distribution with parameter  , then: f (xi| ) =  xi e  xi!
 i = 1, .
.
.
, n From equation (13) the likelihood is: L( |xxx) =  xi e  xi!
n Yi=1 (15) (16) Since it is easier to work with the log-likelihood, from equation (14) we have: l( |xxx) = log( ) n Xi=1 xi   n    n Xi=1 log(xi!)
(17) Di erentiating the log-likelihood in equation (17) with respect to   and setting to zero gives:
 l ( ) =
   n Xi=1 xi   n = 0 (18) Expressing equation (18) in terms of   gives the familiar MLE   = x.
In the above example, we maximized the log-likelihood with respect to a single parameter  .
More generally, the model may contain several parameters, in which case we compute partial derivatives and set each in turn to zero.
Depending on the particular distribution used in the likelihood, a closed form solution may or may not exist.
For the Poisson distribution the MLE of   has an algebraic solution that is the sample mean of the data.
In cases where there is no closed form solution an iterative method is employed using a modi cation of the Broyden, Fletcher, Goldfarb and Shanno quasi-Newton algorithm [4] within the statistical software package R [15].
Although not discussed in detail here it is also possible to compute variances based on the Fisher Information [16] and hence con dence intervals for the maximum likelihood estimates described above.
The log-likelihood in equation (14) can be computed for the various parametric models of interest and used as a basis for model comparison.
This makes sense as the model that has the largest log-likelihood is considered to be the most  likely  given the observed data.
However since we are investigating models with di ering numbers of parameters, rather than comparing log-likelihoods directly we use the Bayesian Information Criterion (BIC) [17].
The BIC penalizes models with more parameters so that to  win  in Property X-axis Number records Mean Median Variance Variance to Mean Ratio Minimum #Clicks Maximum #Clicks Session-level data User-level data Clicks/session






 Clicks/user






 Table 2: Summary of our data sets.
a BIC-sense, the extra parameter needs to justify its addition with a commensurate increase in log-likelihood.
The Bayesian Information Criterion (BIC) [17] is de ned as: BIC =  2l( |xxx) + k log(n) (19) where l( |xxx) is the maximized log-likelihood, n is the number of observations and k = | | is the number of model parameters.
We wish to minimize the BIC with respect to the estimated model parameters.
As can be seen from equation (19) the BIC attaches a penalty to the addition of extra parameters, forcing it to prefer lower order models especially for large n.
In the related work section we argued why linear  tting in log-log space where one axis denotes frequency is poor methodology.
In this section we describe the bene ts and pitfalls of the Cumulative-Probability (Cum-Prob) plot.
Cum-Prob plots: A Cumulative-Probability (cum-prob) plot is related to a QQ plot.
To compute the x coordinate of the i th point, the area under the histogram ( e.g.
of clicks) up to and including the i th distinct value is divided by the full area under the histogram.
In other words, x values are empirical CDF (Cumulative Distribution Function) values.
The y values are obtained by  rst doing a MLE best t but this time referring to the theoretical CDF.
Let ci be the i th distinct click value over the set C of all possible click values, then CDF is a function taking random click values to the zero-one set: CDF : C   [0,1].
Thus both x and y values are normed to the [0,1] interval.
We show results for our two primary datasets: their basic parameters are in table 2.
The  rst dataset is for our main web search engine: a random sample of sessions collected from a week s worth of data.
A session is terminated by the standard 30 minutes of inactivity.
The second dataset is a random sample of per user clicks, integrated over one month of activity for that user on another Yahoo website.
Thus dataset 2 consists of a mixture of sessions.
Figures 1 and 2 show histograms of the data.
We have zoomed in to the top 10 click values since the rest of the histogram has very low values.
Note that for the per-session web search dataset, one-click occurs more frequently than any other value and in particular, more than zero clicks.
A zero-click session is possible since a session can have pageviews but zero clicks.
If one were to use an exponential discrete model, one might expect the zero probability to be higher Figure 1: Sessionized data: frequency of web-search clicks per-session (top 10 values).
Figure 2: User-level data: frequency of Yahoo website clicks per-user (top 10 values).
than the probability for one click.
Thus the data indicate immediately the need to try for a mixture model with a special component at zero - so-called Zero-Adjusted mixture models.
For the per-user month-level dataset ( gure
 the user-level data is from another website (not web search) so it is not as if multiple sessions from  gure 1 are being included in  gure 2.
Ten other datasets from various other Yahoo verticals1 were also analyzed to see if our results are generalizable.
In order to get verticals that span the gamut of user behavior

 then randomly picked a website in each decile of the sorted list.
We begin by examining both datasets in log-log space ( g-ures 3 and 4).
Log-log space brings out di erences in the various exponential distribution families that are just not apparent in usual histograms because all such distributions have long tails.
The other advantage of log-log space is that curvature can be easily visually spotted and this usually indicates the data is not from a scale-free distribution (scale-free has linear form in log-log space).
Figure 4: Per-user clicks (integrated over 1 month) for another large Yahoo website exhibit curvature in log-log space.
The main three results of our paper are supported by these results:   Curvature: In log-log space our data is curved as indicated by the fact that the best  tting distribution, Zipf-Mandelbrot, by theory has a curved form in log-log space.
This is also visually con rmed.
Another way to quantify the amount of curvature is to examine the con dence interval around the MLE (maximum likelihood estimate) of the q parameter in the Zipf-Mandelbrot distribution.
For web search, we obtained q = 2.0 0.01 and for the per-user dataset we obtained q = 0.9   0.002.
Thus in both cases, q = 0 (which would indicate no curvature) is emphatically excluded.
Curved forms in log-log space do not have the scale-free property of pure Zip an (power-law) distributions - they have a natural, distinguished scale.
( e.g.
[10]).
  ZAZM: The particular model form with best BIC  t is the ZAZM (Zero-Adjusted Zipf-Mandelbrot) model for both datasets.
  Zero-Altered Mixture: Zero-altered mixture models do better than their non-mixture counterparts, irrespective of the distribution type.
This is consistent with our hypothesis that a di erent generative component (robots) are major contributors to the zero-click observations.
Next, we examine the ten other datasets (table 3).
First note the (Log-Likelihood) LL column: this shows that the best  t for each website was obtained by the Zero-Altered Zipf-Mandelbrot mixture - not any other mixture.
Second, Figure 3: Per-session clicks for search engine results exhibit curvature in log-log space.
Observations are dark asterisks;  tted models are curves or lines.
The  gures show the distribution of data, a non-mixture Zipf-Mandelbrot, a mixture Zipf-Mandelbrot and a mixture Zipf - we cannot  t a non-mixture Zipf since it is unde ned for x = 0.
The  gures also show a  linear  t  regression line.
This line is obtained by doing linear regression on the plotted points.
Note how it provides a poor  t for points near log(clicks) = 0 which are just the points that represent millions of users (or sessions).
This is the fundamental problem with doing line tting of the plotted points: all points are equally treated whereas in reality some points correspond to millions of users and others may correspond to a single user.
Although visual examination of the data seems to indicate curvature, a more rigorous con rmation would be to  t models assessed by BIC values ( gures 5 and 6).
If the winning model has, by theory, a curved form in log-log space, then it would con rm the visual observation.
The  gures show the smallest (best  tting) BIC values consistently come from the zero-altered mixture models, with zero-altered Zipf-Mandelbrot (ZAZM) being the best  t for our data.
**********************************************************0123402468Log log plot of observed dataZAZ(red), ZM(blue), ZAZM(green)log( #clicks )log(freq)**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************02468051015Log log plot of observed data with ZAZ, ZM, ZAZM and Linear fitted modelslog( #clicks )log(freq)(ZAZ)(ZM)(ZAZM)Linear FitDecile Best parsimonious Best  t






  t (BIC)






 Sample size







 Table 3: Best  tting models for Yahoo websites chosen to span all Yahoo websites in terms of user volume.
Even though deciles 5, 8 and 10 had plenty of data, they had too few distinct values for number of clicks to allow  tting of parameterized models.
is an adjusted measure of  t) one would never lose ( tting-wise) by using ZAZM (this is clear when one recalls that the ZAZ model is just a special case of the ZAZM model).
Having established that, at least for our datasets, the Zipf-Mandelbrot (and its Zero-Altered counterpart) o er a very good  t2, we now examine supporting graphical methods: the Quantile-Quantile (QQ) plot and Cumulative-Probability (Cum-Prob) plot.
Figure 7 shows the cum-prob plot for the winning ZAZM model on the web per-session data.
The cumulative distribution plots are less vulnerable to outliers because they plot areas rather than values of the random variates themselves.
An outlier has a large value but typically does not constitute a large percentage of the overall mass (clicks summed across all users).
Hence its e ect is reduced in the cum-prob plot.
We can reexamine graphically our comparison between the best BIC scoring distribution (ZAZM - Figure 7) and one that was not competitive (Inverse Gaussian - Figure 8).
One can clearly see the ZAZM provides a good  t between theory and observation whereas the Inverse Gaussian touted by [9] as providing a strong law of sur ng does not hold for Yahoo web-search data.
Figures 9 through 14 provide graphical con rmation of the LL/BIC results.
ZAZM is the best  t with ZAZ being the runner-up and the other doing much less well.
The inappropriate continuous distributions (Inverse Gaussian, Weibull and Log-Normal) are included here only because they have been used in prior literature [9, 10, 1], which did not investigate Zipf-Mandelbrot or mixtures.
Negative-Binomial (NB) is included because clicks are discrete counts and NB is often used for over-dispersed (variance-to-mean ratio > 1) data.
Log-Normal is shown because it is the  rst distribution people think of when they think of skewed data (and it also has a curved form in log-log space).
However, it is not discrete and it did not perform nearly as well as the Zipf-Mandelbrot.
a particular form of distribution, only that of the distributions we have tried, such and such a distribution provides the best  t.
Figure 5: Sessionized web data BIC  t values (low values are better): Zero-alteration makes a big difference and within that class, the Zipf-Mandelbrot is the best.
Figure 6: User-level BIC values (lower is better): Zero-altered models (ZA..) do better than zero-in ated (ZI..) or unadjusted (e.g.
ZM).
Within the zero-adjusted set, Zipf-Mandelbrot does the best.
note that for many of these minor websites, BIC indicates the addition of a parameter in going from Zipf to Zipf-Mandelbrot is not  worth it .
In other words, some of these websites do have a linear form in log-log space.
However, even in these cases, the best  t per se (measured by LL) is provided by ZAZM.
And since LL measures pure  t (BIC Bayesian Information Criteria (BIC) for different models(BIC)Modelzawbnbzizmzmzalnzaigzazzayszanbzalszazm5800000600000062000006400000Bayesian Information Criteria (BIC) for different models(BIC)Modelzawbzalnnbzaigzanbzalszazzizmzmzayszazm7.6 e+077.8 e+078.0 e+078.2 e+07Figure 7: Web session data: Cumulative distribution plot.
Winning model: Zero-Altered Zipf-Mandelbrot.
Figure 9: Second dataset: Cumulative distribution plot.
Winning model: Zero-Altered Zipf-Mandelbrot.
Figure 8: Web session data: Cumulative distribution plot.
Losing model from strong law of sur ng: Zero-Altered Inverse Gaussian.
Figure 10: Second dataset: Drop the Mandelbrot correction (Zero-Altered Zipf ).
************************************************************************************************************************************************************************************0.20.40.60.81.00.20.40.60.81.0Observed vs. expected cumulative probs ZAZM p, (q,s) independent(the line y=x included for comparison)ObservedExpected************************************************************************************************************************************************************************************0.20.40.60.81.00.20.40.60.81.0Observed vs. expected cumulative probs ZAIG (the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAZM p, (q,s) independent(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAZ(the line y=x included for comparison)ObservedExpectedFigure 11: inappropriate Gaussian.
man s strong law of sur ng.
Second dataset: A losing model: continuous Zero-Altered Inverse Inverse Gaussian was used in Huber-Figure 13: Second dataset: A losing continuous model: Zero-Altered Weibull.
Weibull was used by Laherrere et al..
Figure 12: Second dataset: A losing discrete model with over-dispersion (Zero-Altered Negative Binomial).
Negative-Binomial is a reasonable guess since it is discrete and has long tail.
Figure 14: Second dataset: Another losing continuous model: Zero-Altered Log-normal.
Log-normal is one of the simplest models that has curved form in log-log space but it is continuous.
***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAIG (the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZANB p, m independent(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.9Observed vs. expected cumulative probs ZAWB(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.9Observed vs. expected cumulative probs ZALN(the line y=x included for comparison)ObservedExpected7.
CONCLUSIONS Prevailing wisdom is that the distribution of web clicks and pageviews follows a scale-free power law distribution.
However, we have found that a statistically signi cantly better description of the data is the scale-sensitive Zipf-Mandelbrot distribution and that mixtures thereof further enhances the  t.
Previous analyses have three disadvantages: they have used a small set of candidate distributions, analyzed out-of-date user web behavior (circa 1998) and used questionable statistical methodologies.
Although we cannot preclude that a better  tting distribution may not one day be found, we can say for sure that the scale-sensitive Zipf-Mandelbrot distribution provides a statistically signi -cantly stronger  t to the data than the scale-free power-law or Zipf on a variety of verticals from the Yahoo domain.
The distribution has a de nite curved form in log-log space which in turn indicates it is not scale free.
Secondly, we have shown that better results are obtainable using a mixture model which treats the zero-class as special.
This is warranted because the generative process of zero clicks might contain a signi cant proportion of robot  users  and thus would be di erent than the generative process for nonzero clicks (containing mostly human users).
Since we have compared zero-adjusted mixture models to non-mixture models we have taken care to use the BIC log-likelihood scoring method since it makes some adjustments for varying complexity of the models.
Finally, we have argued that the practice of  tting plotted points in log-log space is incorrect methodology and is sensitive to outliers.
We instead propose using the Cumulative-Probability plots which plot empirical cumulative distributions against theoretical cumulative distributions.
We plan to use the thresholds resulting from these methods to set probabilistically founded threshold levels for removing out-liers and robots and thus to enjoy more stable and accurate metrics.
