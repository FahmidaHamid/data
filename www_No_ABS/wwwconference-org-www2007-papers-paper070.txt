Internet advertising  ourishes as the ideal choice for both small and large businesses to target their marketing campaigns to the appropriate customer body on the  y.
An Internet advertiser, e.g.
ebay, provides an advertising commissioner, e.g.
ValueClick, with its advertisements, allocates a budget, and sets a commission for each customer action, such as clicking an advertisement, bidding in an auction, or making a purchase.
The Internet publishers, e.g.
mys-pace.com, motivated by the commission paid by the advertisers, contract with the commissioner to display advertisements on their Web sites.
The main orchestrators in this setting are the commissioners, who are the brokers between publishers and advertisers, and whose servers are the backstage for targeting and budgeting.
Whenever a surfer visits a publisher s site, the surfer is referred to one of the servers of the commissioner, which picks an advertisement, and embeds it in the publisher s site on the surfer s Browser.
If the surfer clicks the advertisement on the publisher s site, the surfer is referred to the commissioner, who logs the click for accounting purposes, and clicks-through the surfer to the advertiser s site.
Since publishers earn revenue on impressions (rendering advertisements), as well as clicks they drive to advertisers, there is an incentive for dishonest publishers to in ate the number of impressions and clicks their sites generate [3, 5,
 ulate clicks on the advertisements of their competitors to deplete their advertising budgets [32, 39], which limits the exposure of their competitors  advertisements.
Fraudulent tra c results in bad reputation for the commissioner, and sometimes in paying forfeitures for advertisers [29, 30].
Hit in ation fraud jeopardizes not only the Internet advertising industry, but rather the entire Internet [32].
Hit in ation has been a concern to advertising commissioners since their conception [47]1.
Most of the research investigates publishers  fraud, since the discussion can be generalized to advertisers  fraud.
Three main approaches have been proposed to detect hit in ation attacks.
The  rst classical fraud detection approach employed a set of tools that judge publishers based on how the advertisements behave on their sites, such as how the ratio of impressions to clicks (the Click Through Rate) for each ad-
where advertisers do not pay commission on some of the received tra c, was satisfactorily solved in [40].
vertisement di ers from the network-wide norms that are supposedly exclusively known for commissioners [28].
However, fraudsters can make use of a speci c publishers  site architecture [36], sample the network-wide metrics of advertisements, acquire knowledge about the metrics of the advertisements loaded on their site, and can hence fool the classical tools.
In addition, the classical approach cannot detect malicious intentions, and is designed to discard low-quality tra c2, even if it is legitimate.
Aggressively discounting low-quality tra c  underpays  both honest publishers and commissioners for a lot of the tra c their servers deliver.
The second cryptographic approach [5, 38] asks for the cooperation of surfers to identify fraudulent tra c.
This entails changing the advertising network model to be nontransparent to all surfer, which is unscalable.
Moreover, for the solution to be e ective, the commissioner has to uniquely identify surfers, which compromises surfers  privacy.
We proposed the third data analysis approach in [34].
Data analysis tools perform statistical analysis on aggregate data of surfers  temporary identi cation, e.g.
Cookie IDs and IP addresses.
Analysis of cookie IDs and IPs is more privacy friendly than the cryptographic approach.
Cookies store no personal information, and they can be blocked, or periodically cleared [33].
An IP is usually assigned to surfers temporarily, and could be shared by several surfers.
The philosophy of the data analysis approach is not to change the industry model, and to obfuscate individual surfers  identities, but still achieve satisfactory levels of fraud detection.
Data analysis techniques [34, 35] can identify speci c patterns that characterizes fraudulent tra c [32].
For instance, to uncover a primitive hit in ation attack where a script continuously simulates clicks on advertisements, we proposed a simple Bloom-based [4] algorithm to detect duplicates in a stream of impressions or clicks [34].
Experiments on a real network were revealing.
Interestingly, one of the advertisements was clicked 10,781 times by the same cookie ID in one day.
Hence, this approach can reveal malicious intentions, and thus complement the classical tools that cannot distinguish low-quality tra c from fraudulent tra c.
The classical and the cryptographic approaches did not distinguish between attacks made by one publisher or a group of publishers forming a coalition.
Meanwhile, making this distinction is at the heart of the data analysis approach.
By forming coalitions, fraudsters share large pools of attacking machines.
Hence, they avoid the cost and overhead of launching highly distributed attacks individually.
In addition, coalition attacks are more di cult to detect since the patterns of fraudulent tra c are shared by numerous publishers.
Therefore, coalition attacks can easily circumvent the classical tools.
However, the data analysis approach looks for signs of publishers  coalitions.
In [35], we devised an algorithm that detects a speci c attack [3].
In this paper, we devise a generalized technique that detects all forms of coalition attacks.
We  rst build on published theoretical results to devise the Similarity-Seeker algorithm that discovers coalitions of size 2.
We then extend Similarity-Seeker to discover coalitions of arbitrary sizes.
Interestingly, when implemented on a real network, Similarity-Seeker detected numerous coalitions of various sizes launching attacks with a variety of techniques.
The rest of the paper is organized as follows.
We start by setting
 probability that it yields a sale.
the stage for discovering coalition attacks in Section 2.
We describe two brute force algorithms in Section 3.
Section 4 develops one of the brute force algorithms into Similarity-Seeker that discovers coalitions made by pairs of sites.
The generalized problem of detecting coalitions of several sites is explored in Section 5.
We comment on the  ndings on a real network in Section 6.
We discuss the related work in Section 7, and conclude with our future work in Section 8.
The basic premise of tra c analysis is to draw correlations between the attacking machines, identi ed by IPs and cookies, and fraudsters.
To circumvent the data analysis techniques, fraudulent publishers should dilute the strong correlation between their sites and the machines from which the attacks are launched.
This can be done either on the side of the attacking machines or on the side of the attackers  sites.
Hence, the spectrum extremes of the hit in ation attacks are:   To dilute the correlation on the machine(s)  side, non-coalition attack are performed by one fraudster, while obliterating or frequently changing the identi cation of the machine(s), or using a huge number of machines.
  To dilute the correlation on the sites  side, coalition attack are performed by many fraudsters, where any machine can be used to simulate tra c to any site.
If we detect both non-coalition and coalition attacks, then any attacker on the spectrum is also detectable.
Hence, the attackers will have no leeway around the fraud detection system, and the hit in ation problem is ultimately solved.
Understandably, the di culty of detecting a non-coalition attack increases as the number of machines from which the attack is launched increases.
In its simplest form, launching an attack from one machine, identi ed by one cookie-ID, can be detected trivially by checking for duplicate impressions and clicks [34].
However, launching an attack from multiple machines is much harder to detect, since the detection algorithm has to examine the relationship between each publisher and all the machines generating tra c.
Although the straightforward use of network anonymiza-tion, e.g.
tor.e .org, is attractive for inexperienced fraudsters, it is not e ective.
Those services were designed to protect surfers  privacy.
Hence, they block surfers  cookies.
Therefore, using network anonymization can be trivially detected by monitoring the percentage of cookie-less tra c per publisher and investigating publishers whose tra c deviates from the norm.
Similarly, on real networks, we notice some novice fraudsters generating a lot of tra c from ISPs that assign virtual IP addresses to surfers, such as AOL r(cid:2).
However, the ranges of IPs of those ISPs are well known, and again, the ratio of the tra c of any publisher received from those ISPs is highly stable across all the publishers.
Hence, such attacks are easily detected by examining the ratio of the tra c received from ISPs assigning virtual IPs as compared to the entire publisher s tra c.
This is where the complexity of launching scalable attacks becomes clear.
Hard-to-detect attacks from several machines could be costly and unscalable to launch.
To have a normal percentage of cookie-less tra c, and a normal percentage of non-virtual ISPs  IPs, fraudsters are motivated Resources of Fraudster 1 Resources of Fraudster 2 ......
Resources of Fraudster Q Site of Fraudster 1 Site of Fraudster 2 ......
Site of Fraudster Q (a) In non-coalition attacks, every fraudster generates tra c to its site only.
Resources of Fraudster 1 Resources of Fraudster 2 ......
Resources of Fraudster Q Site of Fraudster 1 Site of Fraudster 2 ......
Site of Fraudster Q (b) In coalition attacks, every fraudster generates tra c to its and other sites in the coalition.
Figure 1: Non-Coalition versus Coalition Attacks to either own the attacking machines, or to control the machines of real surfers through Trojans in order to use the IPs and cookies of real surfers.
In other words, launching scalable attacks entails high cost or requires some Trojan-writing skills, since out-of-the-box Trojans are easily detected with antivirus softwares, and hence are unscalable.
Thus, the correlation between the di culty of launching an attack and the di culty of detecting it is understandable.
To avoid the cost of launching scalable hit in ation attacks, fraudsters shift their strategy from launching distributed non-coalition attacks, to launching coalition attacks where many fraudsters share their resources (machines used in the attack).
That is, the machines are used to generate tra c for several sites.
For instance, assume attacker A can generate u hits from each machine it controls, without being rejected by the commissioners  non-coalition radars.
Another attacker, B, can simulate another u undetectable hits per controlled resource.
Assuming no overlap between A s and B s resources, it is more scalable and cost e ective to timeshare the resources, and generate 2u hits for each publisher, instead of doubling the number of resources.
The argument can be extended for larger coalitions as illustrated in Figure 1.
Forming coalitions serves two main purposes.
The  rst purpose is to increase the tra c and the revenue while maintaining the same cost per fraudster.
The second purpose is to blur the relationship between the identities (IPs and cookie IDs) of the attacking machines and the attackers  sites.
Launching coalition attacks does not need specialized Web development skills.
It only needs minimal resources, and the knowledge of other fraudsters.
For commissioners, detecting coalition attacks is challenging since the tra c coming from any attacking machine does not exclusively go to one publisher (Figure 1).
Rather, each attacking machine contributes a small portion to the traf- c of each fraudster in the coalition.
This hinders non-coalition radars from associating any publisher with a reasonable amount of resources.
Therefore, forming coalitions is the perfect solution for fraudsters to launch scalable attacks from machines whose identi cations are weakly correlated with their sites.
To detect coalition attacks, the commissioner has to search for publishers  sites with highly similar tra c.
A reliable  n-gerprint of a site s tra c is the set of IP addresses generating the tra c3.
Given the set of distinct IPs visiting each site, the commissioner has to search for sites whose tra c entries are generated from roughly the same set of IPs4.
Since the tra c entries of di erent publishers are interleaved in the log  le, the tra c is scanned  rst to obtain the set of IPs visiting each site, which is then stored in a separate  le.
For each publisher, A, let SA be the set of IPs visiting A. De ne SB analogously.
Several measures of the similarity between SA and SB exist [16], including the Dice  
 coe cient, 2

 |SA SB| .
The goal is to discover and the Jaccard coe cient, all pairs of sites whose similarity exceeds some threshold, s.
Fortunately, as shown in Section 6, any two legitimate sites have negligible similarity.
|SA|+|SB| ; the cosine coe cient,

 We start by describing the two brute force alternatives in Sections 3.1, and 3.2.
Since e ciently detecting coalitions is hard, we develop the second algorithms further to detect coalitions of pairs of sites in Section 4.
In Section 5, we extend the algorithm to detect coalitions of arbitrary sizes.
All-Pairs The  rst algorithm considers every possible pair of sites.
For every pair, A and B, the All-Pairs algorithm calculates any of the three aforementioned similarity coe cients between their sets of IPs by determining the size of the intersection (SA   SB) using a sort-merge procedure.
Assuming the main memory cannot accommodate the entire tra c, but can accommodate the tra c of any two sites, then the sort-merge procedure can be done in memory.
However, the tra c is read from disk O(D) times, where D is the number of publishers  sites.
An average-sized commissioner has around 50,000 sites.
The tra c of each site can be presorted.
The presorting step is O(D|S| log(|S|)), where S is the largest IP set.
The number of distinct IPs visiting a site in a day is around
 duces the total in-memory complexity from O(D2|S|2) to O(D|S|(D + log(|S|))).
thus we assume the source IPs of the tra c are not spoofed.
Counteracting spoo ng has been studied in [15].
equally treated.
However, the entire discussion can be trivially generalized for the case where special IPs, such as those coming from a speci c location or known to belong to Internet Service Providers (ISPs), are given di erent weights.
(cid:2)  





 Optimizing All-Pairs.
The All-Pairs algorithm can be enhanced by using the triangle inequality.
This limits the optimization to the Jaccard |SA SB| , the Jaccard dissimilarity of SA coe cient since 1  |SA SB| (cid:2) and SB, satis es the triangle inequality.
That is, for any (cid:1) (cid:1) three sites A, B, and C, it is true that +


 .
The dissimilarity based on the other two coe cients does not satisfy the triangle inequality.
The optimization entails looking for a third site, C, such that A and B can be judged similar or dissimilar using the similarity already calculated for the pair A and C, and the pair B and C in a dynamic programming scheme.
Thus, when testing A and B for having similarity exceeding the threshold s, if there exists a site, C, such that , then (1   s)  

 (1   s)  

 (cid:1) (cid:2)




 , and A and B are similar.
(cid:3)(cid:3)(cid:3)(cid:1) (cid:2)(cid:3)(cid:3)(cid:3), then A and B are dis-(cid:1) (cid:2) Conversely, if there exists a site, C, such that (1   s)  


 similar because (1   s)  

 (cid:1)
 (cid:2) .
(cid:2) (cid:1) + (cid:1) (cid:2) (cid:1)   (cid:2)


 However, as shown in Section 6, legitimate sites have slim similarity, which makes this optimization ine ective for faster discovery of sites with similar tra c.
All-IPs Instead of performing a sort-merge for every possible pair of sets of IPs, the All-IPs algorithm performs a sort-merge for all the sets together.
Hence, all sites sharing a speci c IP are identi ed, and All-IPs calculates the similarity of pairs that have a common IP.
Counting the number of IPs shared by any pair of sites requires one scan on the sorted data.
However, the sort-merge is done out-of-memory5.
Hence, the tra c is read from disk O(log(D)) times.
The  rst presorting scan has in-memory complexity of O(D|S| log(|S|)).
Then, O(log(D)) scans are made for merging, each has an in-memory complexity of O(D|S|), yielding a total in-memory complexity of O(D|S|(log(D) + log(|S|))).
The All-IPs brute force algorithm has been proposed before for measuring set similarity in the context of  nding similar  les and domains on the Internet [9, 10, 13].
In a collection of D documents, each document is represented by the set of all statements of a speci c length the document contains.
This is analogous to our problem of  nding highly similar sets of IPs among a large collection of sets.
Optimizing All-IPs.
To compact the sets, Broder et al. [9, 10, 13] proposed sampling.
Reducing the size of S decreases the computations.
Sampling results in the sort-merge producing fewer IPs as well as fewer sites sharing any IP.
However, random sampling from the two sets SA and SB greatly skews the size of the intersection.
In the worst case, random sampling can lead to an empty intersection even (cid:4)|SA|   |SB|, though the actual intersection is not empty [17].
Therefore, the sampling technique used should scale down |SA  SB| by the same factor it scales down |SA| + |SB|, or |SA   SB|, to conserve the Dice, cosine, or Jaccard coe -5[46] gives a good survey on external memory algorithms.
cients, respectively.
Three hash-based sampling techniques were proposed by Broder et al. to conserve the similarity.
Since our  nal algorithm is based on the All-IPs brute force algorithm, we discuss the three techniques in Section 4.
In this section, we develop All-IPs to e ciently detect all coalitions of size 2.
In Section 4.1, we describe the sampling of IPs visiting sites, and derive the sample size that guarantees a speci c error.
In Section 4.2, we propose Similarity-Seeker for detecting sites with similar tra c.
The IP sets should be sampled, such that any pair of sites, A and B, whose IP sets, SA and SB, have a similarity exceeding a threshold, s, is discovered with high probability.
For any set S de ned on domain     N , let   :       be a permutation of   chosen uniformly at random; g :     N be an arbitrary injection; and M ODy(S) be the subset of the elements that are 0 modulo y. Yy,g, (S), de ned as M ODy(g( (S))), is a similarity-sensitive sample of S. In [9], |Yy,g,  (SA) Yy,g,  (SB )| |Yy,g,  (SA) Yy,g,  (SB )| was shown to be an unbiased estima-
|SA SB| .
Although [9, 10, 13] tor of the Jaccard coe cient, tackled the problem in the context of Jaccard similarity only, we like to point out the generality of the sampling method.
|Yy,g,  (SA) Yy,g,  (SB )| |Yy,g,  (SA)|+|Yy,g,  (SB )| is an unbiased es-It is easy to show 2   |Yy,g,  (SA) Yy,g,  (SB )| |Yy,g,  (SA)| |Yy,g,  (SB )| timator of the Dice coe cient, and is an unbiased estimator of the cosine coe cient Broder et al. proposed other techniques to estimate Jac-card similarity in speci c.
For any set, S, from a totally ordered domain, let M INz(S) be the subset containing the smallest z elements in S if |S|   z; otherwise M INz(S) = S.
Let SA and SB be the sets of IPs visiting sites A and B, respectively.
De ne Zz, (SA) as M INz( (SA)), and de ne Zz, (SB) analogously.
It was shown in [13] that an unbiased estimator of the Jaccard coe cient of SA and SB is given by The number of samples generated by the former method, based on Yy,g, (S), grows with |S|, which could be inconvenient; while the number of sample generated by the latter method, based Zz, (S), is  xed.
On the other hand, the former method is easier to calculate, and can be used for all similarity coe cients.
However, it is di cult to derive any guarantees on the quality of both estimators.
|M INz (Zz, (SA) Zz, (SB )) Zz, (SA) Zz, (SB )| |M INz (Zz, (SA) Zz, (SB ))| .
 1 i In [10], a MinHash-based Jaccard estimator was adopted.
Since this is the only estimator that we can draw error guarantees for, we use it in the sequel.
Let  i be a permutation of   chosen uniformly at random.
For a set S = {s1, s2, .
.
.}
on  , let min i (S) be   (min( i(s1),  i(s2), .
.
.
)).
Therefore, for sets SA and SB, min i (SA) = min i (SB) if and only if min i (SA  SB)   (SA  SB).
Since  i is chosen uniformly at random, then all the elements in (SA   SB) are equi-probable to become min i (SA   SB).
Hence, min i (SA) = min i (SB) with probability
 |SA SB| , which is the Jaccard coe cient of SA and SB.
Therefore, we can calculate an unbiased estimate of the Jaccard coe cient of the pair SA and SB as follows.
Construct a set of n independent uniform permutations,  1,  2, .
.
.
,  n, and calculate min i (SA) and min i (SB) for 1   i   n. The unbiased estimate is Count(i| min i (SA)=min i
 given by permutations (samples) where both minimums agree.
, which is the ratio of n
 Estimator.
In advertising networks, having guarantees on the quality of the results is very crucial.
The commissioner has to know, with very low error rate, sites whose tra c are highly similar.
Discarding sites erroneously reduces the commissioner s revenue, while charging advertisers for fraudulent tra c puts the commissioner at risk.
However, no robust error analysis for the estimators in [9, 10, 13] was provided.
Since each permutation generates one sample, to discuss error analysis, we use the two terms interchangeably.
We calculate the minimum number of samples, n, for a spe-ci c con dence interval estimate on the Jaccard coe cient of SA and SB using the sampling method proposed in [10].
For each permutation  i, we model comparing the samples, min i (SA) and min i (SB), as an independent Bernoulli trial with unknown success probability, p = A Bernoulli random variable is 1 with probability p, and is 0 with probability (1  p).
Based on n samples, the Maxi-(cid:5) xi mum Likelihood Estimator of p is given by  p = n , where the variance of  p is given by p(1 p) .
Since the distributions of the samples are independent, and identical, the central limit theorem and the law of large numbers state, for large n,  p is approximately normally distributed.
Thus, if K  denotes the cdf of the standard normal distribution between   and  , then both Pr (cid:7) < K /2 n (cid:6)  p p   K /2 < (cid:7) p(1 p)/n are equal to 1    .
and Pr  K  <  p p  p(1 p)/n (cid:6) (cid:8) n (cid:10)  p(1   p) |SA SB| is given by  p   K /2
 (cid:8)  p(1   p) Therefore, a two-sided (1   ) approximate con dence in, and a one-terval for (cid:9) sided (1    ) approximate con dence interval is given by  p   K  One-sided con dence intervals are more interesting for our application, since we are looking for pairs of sites with Jaccard coe cient exceeding a threshold s. Therefore, the minimum size, n, of the permutations family that would guarantee (1    ) con dence that the estimator exceeds s is n = .
Clearly, the sample size is not  p(1    p) (cid:2)2 (cid:1) (cid:11) (cid:12) , 1 n .
K   p s (cid:16) K 
 (cid:15)2 (cid:13)(cid:14) bounded in the general case.
However, if p is to be estimated within a margin of error of , then this bounds n by [7].
Statistically, this is interpreted as: with probability at least 1    , the estimator is no more than  less than the user threshold, s, i.e., Pr ( p > s   )   1    .
For instance, n = = 423 permutations guarantees (cid:11)(cid:1) (cid:2)2 (cid:12)

 that a pair of sites is estimated to be similar is truly similar within an error of  = 0.04 with probability 0.95.
Random permutations are needed to implement this Jac-card estimator.
Representing each truly random permutation requires |N| log |N| bits, where practically |N| = 264 [24].
This motivated Broder et al. to de ne min-wise independent (MWI) permutations in [11].
F is a family of MWI permutations on a domain     N , if when   is chosen Procedure: Samples-Select(Integer n) begin Construct  i using the technique in [12]; //Constructing permutations for (Integer i = 1, i   n, i + +){ }// end for //Tra c separation for each Tra cEntry e = (Site ID, IP ){ } //Selecting samples for each Site ID{ Write e to the tra c  le of Site ID on disk; IP Site-Samples[n] = empty array; for each Tra cEntry e = (Site ID, IP ){ for (Integer j = 1, j   n, j + +){ if ( j (IP ) <  j (Site-Samples[j])){ } Site-Samples[j] = IP ; }// end for }// end for let Site ID be stored at row i in Samples Write Site-Samples to Samples[i] on disk; }// end for end; Figure 2: The Samples-Select Procedure.
at random from F, then for any x    , Pr(min( ( )) =  (x)) = 1| | .
That is, all the elements of any domain,  , have equal chances to be the minimum element of the image of   under any  .
However, MWI families are impractical, | | o(| |) since the cardinality of any such family is at least e [11].
In our case,   is the IP domain.
To achieve easy computation, Indyk proposed a relaxed version of MWI,  MWI [24], de ned as follows.
For any domain     N , and x   (N    ), if   : N   N is chosen at random from F, then Pr( (x) < min( ( ))) = 1 
 if x    , then We rewrite this in a simpler form as, (cid:3)(cid:3)(cid:3)   | | .
That is, all the ele-(cid:3)(cid:3)(cid:3)Pr( (x) = min( ( )))   1| |
 ments of any domain,  , have almost equal chances to be the minimum element of the image of   under any  .
[24] provides a construction of compact  MWI permutations.
An even more practical variation of MWI is pairwise independent (PWI) permutations [11].
F is a family of PWI permutations if for any {s1, s2, t1, t2}    , s1 (cid:13)= s2, t1 (cid:13)= t2, if   is chosen at random from F then Pr(( (s1) = t1)  ( (s2) = | | (| | 1) .
Although [11] showed that PWI families t2)) = can be viewed as  MWI families with  as large as log  , in practice, they have many implementations and are widely used [12].
For instance, the performance of linear independence, of the form  i(x) = aix + bi modulo c, is acceptable in real life if     {1, .
.
.
, c},  , c    , c is a prime, ai and bi are chosen at random, and ai (cid:13)= 0 [11, 6].
[12] provided an e cient way to construct a family of  MWI permutations using a random invertible Boolean matrix, coupled with a redundant input representation, yielding an  of 0.25 in the worst case, and 0.06 in practice.
We choose this implementation [12] due to its guaranteed tolerable error.
Now, we have a reliable sampling technique that e ciently estimates the similarity of huge sets of IPs visiting sites.
Moreover, for a given error bound and con dence, we know the number samples to collect.
We use this sampling method to describe the Similarity-Seeker algorithm.
The algorithm starts by collecting samples using the technique in [12].
It uses every permutation to discover all pos-Algorithm: Similarity-Seeker (Double s, ,  , Integer l) begin (cid:11)(cid:1) (cid:12) (cid:2) 2 ; K 
 //Calculating the number of samples Integer n = //Creating an array of samples IP Samples[|Sites|][n] = empty D   n array; Samples-Select(n); //Allocating space for C Set <SiteID, SiteID, Integer> C = empty set; for (Integer j = 1, j   n, j + +){// IPs loop //Allocating space for H HashTable <IP, SiteList> H = empty hash table; for (Integer i = 1, i   |Sites|, i + +){// Sites Loop //Populating H with lists of sites sharing an IP let Site ID be the SiteID at Samples[i] let IP be the IP at Samples[i, j] Insert Site ID into H[IP ].SiteList; }// end for //Incrementing similarity of sites sharing the jth IP for each SiteList SL in H{ if (|SL| < l){// Sites do not share a popular IP for each two sites, A and B, in SL{ if ((A, B)   C){ C[(A, B)].Integer ++; if (sn   1   C[(A, B)].Integer > sn){ } Output (A, B) as similar; Insert (A, B, 1) into C; } else{ } }// end for } }// end for }// end for end; Figure 3: The Similarity-Seeker Algorithm.
sible pairs of sites that share an IP in their samples.
The algorithm employs three data structures: Samples, C, and H. Samples is a D   n array whose rows represent sites, columns represent permutations, and cells represent samples for sites under permutations.
The Samples array is populated by the Samples-Select procedure described next.
C is a set of triplets: two site IDs and the number of shared samples.
C tracks pairs of sites with shared samples exceeding sn, where s is the similarity threshold.
H is a hash table of tuples: a sample ID, and a list of sites sharing it.
To populate Samples, the Samples-Select, as sketched in Figure 2, procedure makes two preliminary scans on the tra c data.
In the  rst scan, Samples-Select separates the tra c of each site in an individual  le that can  t in memory.
For each site, Samples-Select makes a second pass where it hashes each tra c entry using all the permutations.
After collecting all the samples, the  le is written back to the right row in Samples.
We only require the memory to accommodate one row or one column of Samples at a time.
This increases the scalability of the proposed algorithm.
The in-memory complexity of Samples-Select is O(D|S|n).
Once Samples is populated, Similarity-Seeker reads it in a column-major manner.
For every column (permutation), a hash table, H, is temporarily constructed for holding D samples and their corresponding lists of sites sharing each sample.
The hash table is populated after one scan on the column, and the list of sites sharing each IP is populated.
A scan is then made on H. Any list that contains a large number, l, of sites sharing a sample is discarded.
This sample is probably the IP address of an Internet Service Provider (ISP) or a Network Address Translation (NAT) box, that is shared by hundreds of computers.
Otherwise, for each pair in a list sharing a sample, a corresponding element is created with a number of shared samples of 1, and is inserted in C, if it does not already belong to C, or is incremented otherwise.
At any time, if the incremented pair satis es the similarity s, Similarity-Seeker outputs this pair as similar.
The algorithm for discovering all pairs with a similarity exceeding s with con dence   and error  is presented in Figure 3.
In addition to the two scans on the tra c date made by Samples-Select, Similarity-Seeker makes only one scan on Samples.
Since Samples is typically smaller than the entire tra c, then the tra c is considered to be scanned only thrice, as compared to the O(log(D)) in the All-IPs algorithm, where D is around 50,000.
Calculating the in-memory complexity of Similarity-Seeker is more involved.
To establish a bound on the in-memory complexity of processing one column by Similarity-Seeker, we have to consider two extreme cases.
The  rst is where l   1 sites share the same sample, and all the other D (l 1) samples are shared by two sites.
The second is where all the D samples are clustered to form lists of length l   1.
Then the complexity of processing one column is O .
The complexity of any non-extreme case is clearly a linear combination of O and O(lD).
Since the lD factor dominates, and there are n columns, the total complexity of Similarity-Seeker is O with a tiny hidden constant.
In practice, the complexity is less due to the dissimilarity of IPs visiting sites, and hence less clustering of sites.
We examine the relationship between the estimated sites  similarity and the parameter l in Section 6.
Interestingly, some site pairs retain their similarity, no matter how small l is set.
l2 + D, lD l2 + D (cid:15)(cid:15) max lD 2 (cid:14) (cid:14)
 (cid:14) (cid:14) (cid:15) (cid:15) Due to the smaller number of sets, D, in our case (50,000 instead of all the Internet documents) we assumed that all the D samples from one permutation can  t in memory.
Thus Similarity-Seeker avoids the out-of-memory sort-merge performed by All-IPs with all the associated I/O and computational overheads.
Similarity-Seeker can e ciently detect coalitions of pairs of sites.
However, as we discuss in Section 5.1, attackers form coalitions of sizes exceeding 2 to stay under the radar level by giving up some greed.
Hence, we extend Similarity-Seeker to detect larger coalitions in Sections 5.2, and 5.3.
Coalition Size A group of attackers, of size Q, can make hard-to-detect coalitions by not sharing all the resources together.
Instead, each publisher would share each resource it controls with only q random attackers.
Hence, as shown by Theorem 1 the pairs  similarities drop, while still gaining from coalitions.
Theorem 1.
A group of attacking publishers, of size Q, where each publisher shares each resource it controls with only q < Q random publishers, reduces similarity between
 pairs from 1 to q(q+1) Proof.
Assume each site in the attacking group controls r resources, the Jaccard coe cient is used for similarity, and that a resource shared by A with B is not re-shared by B.
For any two sites, A and B, SA SB is given by the resources shared by A with B, the resources shared by B with A, and the resources shared by all the other Q   2 nodes with both A and B.
This is equal to 2r q
 Q 1 .
SA  SB is given by the resources controlled by r(q + 1) q A, the resources controlled by B, and the resources shared by all the other Q   2 nodes with either A or B.
This is equal to 2r + 2r(Q   2) q

 q(q+1)
 Hence the Jaccard coe cient is given by The resources directing tra c to each site is r(q + 1), after forming the coalition, instead of the r resources controlled
 by each site.
Q 1 + r(Q  2)  (q)(q 1)   r(Q   2)   (q)(q 1) Corollary 1.
By increasing the size of a coalition, attackers can sustain similarity between pairs at a low level, while still increasing their gains.
.
(cid:1)
 (cid:2) 2(s+1) (cid:4) Proof.
To keep pairs  similarity below a detectable level,   (2Qs + 1)2   (4Qs2 + 1) + (1   s)2 s, the gain of each publisher is q, where q is given by (2Q   3)s   1 + Hence, similarity between pairs can be sustained at any level, s, while increasing the gain, q, by forming larger groups,
 i.e. increasing Q.
Q, the Jaccard coe cient is less than 1  Q .
Fortunately, as shown in Section 6, any two legitimate sites have negligible similarity.
Therefore, even if attackers form large coalitions, the subtle similarity between pairs is still above the norm, and is still detectable by Similarity-Seeker.
From Theorem 1, if q > 1 and q =   Pairs of Sites Increasing the size of coalitions from pairs to large groups shifts our focus from searching for pairs of sites with highly similar tra c to searching for groups with moderate similarity.
For two main reasons, the solution in Section 4 has to be extended for coalitions of arbitrary sizes.
First, outputting one set of several sites, such that each pair of sites have similar tra c, establishes the evidence for the fraudsters  malicious intention.
It is very unlikely, for any random group of sites of size Q, that all possible pairs are similar.
If any two publishers  sites can be mistakenly judged to have similar tra c with probability  .
Then, mistakenly judging that Q random sites are involved in a coalition attack has a probability of   .
For instance, if   = 0.1, then erroneously judging a small group of 5 random sites to be in coalition has a probability of 10  10.
Second, outputting one set of several sites is more concise than outputting all the possible pairs in that set.
For instance, if 3 coalitions of size 50 fraudsters are discovered, it is more convenient for the management to verify a list of 3 entries, each of size 50, than to examine 3   50 49
 entries, each of size 2.
In addition, the output conciseness gives a more panoramic picture of the coalition and facilitates manual investigations like checking for common features of the sites, such as contract date, and earning rate.
Therefore, we need to condense the detected pairs of sites into groups of sites.
sites  similarity can be modeled as an undirected graph whose nodes represent sites, and whose edges connect pairs of sites with similar tra c.
The objective is to search for, instead of just edges, all maximal cliques in this huge graph.
Sites  Similarity Graph Let G = (V, E) be the sites  similarity graph, where the set of nodes V represent sites, and E is a set of edges connecting pairs of sites if and only if their similarity is at least s. We will be interchangeably referring to sites and nodes representing them.
For a subset W   V , G(W ) = (W, E(W )) with E(W ) = {(v, w)   W   W|(v, w)   E} is called a subgraph of G induced by W , and G is called a supergraph of G(W ).
G(W ) is said to be a clique if and only if (v, w)   E   v, w   W .
G(W ) is a maximal clique if it is not a proper subgraph of another clique.
The objective is to  nd all maximal clique in the sites  similarity graph G. The graph  G = (V,  E) is a complementary graph of G if  E = {(v, w)   V   V |(v, w) /  E}.
A subset W   V is an independent set if and only if (v, w) /  E   v, w   W .
W is a maximal independent set if it is not a proper subgraph of another independent set.
Finding all maximal clique in a graph G is equivalent to  nding all maximal independent sets in  G.
Two seminal algorithm for enumerating all cliques are provided in [14].
The  rst algorithm is a basic recursive branch and backtrack technique.
The second algorithm is an optimized variation that prunes the search space faster.
(cid:1) (cid:2)


 (cid:1) (cid:2)

 The complexity of the second algorithm is O [43].
Among all the variations [19, 26, 43, 44] of the algorithms
 in [14], [43] was able to reach a complexity of O by integrating the output function into the algorithm.
This is
 3 cliques 6 [37].
optimal, since a graph can contain up to 3 The maximal cliques enumeration algorithm in [43] has polynomial storage requirements, and is optimal in the worst case.
However, since the sites  similarity graph is extremely sparse as shown in Section 6, quantifying the complexity in terms of the number of maximal cliques in the graph is crucial to our application.
Although this involves complex analysis [43], the original experiments in [14] showed the average time to identify a maximal clique, i.e., a coalition attack, is not dependent on the size of the graph or the number of maximal cliques.
We recommend the implementation of [43] due to its optimal worst case complexity, though other algorithms established complexity bounds in terms of the number of maximal cliques in the graph, i.e., the number of coalitions.
For instance, [44] combined the pruning techniques in [2] and [14] to  nd all the maximal independent sets with a complexity of O(|V ||E| ), where |V |, |E|, and   are the numbers of nodes (cid:4)|V |).
(sites), edges, and cliques in the graph.
This algorithm was improved in [19] to O(a(G)|E| ), where a(G)   O(

 We have devised Similarity-Seeker for detecting coalitions of pairs of attackers, and then extended it for coalitions of arbitrary sizes.
To check the validity and the e ectiveness of our development, we comment on our comprehensive set
 Two nodes are connected if they belong to di erent triplet.
Hence, there are 3x cliques, each of size 3.
Variation of Number of Site Pairs with Similarity Excluding IPs shared by l or more sites l >= 1000 sites 30 sites 100 sites 20 sites 50 sites 10 sites 40 sites s r i a
 f o r e b m u


















 Figure 4: The Number of Pairs (Logarithmic Scale) Having a Speci c Similarity.
Similarity



 of experiments using real data.
We describe our experience with building a fraud detection system at Fastclick, Inc., a ValueClick company.
For proof of concept, we analyzed a data sample of 54,045,873 tra c entries using Similarity-Seeker to discover site pairs with similar tra c.
To reduce the noise, we excluded all the IPs that visited a large number of sites, because such IPs probably belong to NAT boxes.
We repeated the experiment and progressively increased the parameter l, the number of sites beyond which the IPs are disregarded, from 10 to 1,000.
The results are plotted in Figure 4, with a logarithmic scale on the vertical axis.
It is interesting to note that when l was 1,000, 98.94% of the pairs had less than 1% of similarity.
When l was 10,
 the similarity between sites  IP sets is negligible.
For each run, Similarity-Seeker output all the pairs with similarity more than 10%, and we fed the output to the cliques enumeration algorithm [43] to discover larger coalitions.
In particular, when l, the number of sites beyond which the IPs are disregarded, was 10, Similarity-Seeker output
 tained 5 coalitions of size 3.
As l increased, more pairs were discovered.
For instance, when l was 30, 189 pairs of sites were found.
Since more IPs shared by the disjoint components were considered, the cliques enumeration algorithm was able to connect some of these components into bigger coalitions.
However, many of the cliques were overlapping.
It became clear that the cliques discovered are highly overlapping and noisy, since popular IPs that are shared by many legitimate sites are not discarded.
To reduce the noise, we increased the minimum tra c similarity gradually beyond 0.1.
When s reached 0.5, for the same l of 40, the output of Similarity-Seeker comprised 406 pairs of sites that translated into exactly 1 perfect clique of size 29.
That was clearly a coalition attack, as discussed below.
As l increased to 40, 647 pairs were found.
As l, the number of sites beyond which the IPs are disregarded, increases, more legitimate pairs are output as similar, since popular IPs shared by those sites are not discarded.
To overcome this problem, it is advisable to increase the minimum required tra c similarity, s, as we increase l.
We increased l to 50 sites and increased s gradually up to
 can be manually identi ed, there was the original coalition of size 29, sharing 15 sites with another coalition of size 22.
There were 8 other disjoint coalitions of sizes between 3 and 10, some isolated pairs, and a star of size 6.
Among all the discovered 680 sites, there were strong evidences that more than 93% of them were real fraudsters.
Most of the coalitions had all their sites signed up with the commissioner around the same date.
The noticeable characteristic of the coalitions of size 29 and 22 was that their tra c was of moderate size, yet coming from IPs all over the world.
After further investigations, we found that the Referer  elds in the HTTP requests were coming from pages that do not have the commissioner s advertisements; and sometimes no advertisements at all.
We suspect the attackers had some form of readily available tra c through a network of Trojans, and that they do not work for the domains they signed up for.
When the commissioner sent the account activation e-mails7, the publishers somehow acquired the attached activation secrets and activated the accounts.
Since the activation secrets are stored in a hashed form, the attackers must have compromised some machines on those domains, and hence, acquired the attached secrets.
For some of the isolated pairs, we noticed almost simultaneous tra c entries for the two sites of the coalition.
We suspect that those attacks were launched using the attack in [3] that will be discussed shortly in Section 7.
As l grew further, the cliques enumeration algorithm did not connect those coalitions, but rather started to output groups that share extremely popular IPs.
We checked those IPs on www.arin.net/whois, and they are ISP-owned IPs.
We recommend setting l and s initially to small values, say 5 sites and 0.1 similarity.
From there, the commissioner can tune the values of l and s according to the noise in the results, as described above.
The noise is usually manifested in the number of isolated pairs, small coalitions, and overlapping coalitions.
From our experience, an appropriate value for the error  is s 10 , and the plausible range for the con dence   is between 0.01 to 0.1.
The work related to ours can be classi ed into two categories.
The  rst category is the recent research in discovering densely connected subgraphs in huge graphs.
The second category is our previous work on coalition attacks.
Due to the NP-hardness of the cliques enumeration problem, it was recently attempted to approximate this problem as discovering densely connected subgraphs or clusters.
the publisher an email on the domain signed up, with a secret key.
The publisher can activate the account only using this secret key.
This ensures that the publisher has an email account on the domain (s)he signed up for.
Three main approximations were proposed.
The conductance measure [8, 18, 22, 27] of a subgraph is a measure of the number of the external edges (bridges to the rest of the graph), in comparison to the internal edges of the subgraph, and the internal edges of the rest of the graph.
The second cluster editing approximation [41] bounds the number of edges to be added or deleted to transform a subgraph into an isolated clique.
The third approximation [1] bounds the average internal degree of nodes.
However, discovering any cluster with a bound on any of the three approximations is proved to be NP-complete [23, 42].
Being as hard as the original problem, the approximations are of limited use.
The algorithm in [21] e ciently discovers dense clusters.
However, the algorithm associates no connectivity metrics with the identi ed clusters.
It hence provides no guarantees on the clustering quality.
The algorithm cannot answer queries about clusters with a speci c threshold on a connectivity metric, since the identi ed clusters can be either split or combined to satisfy the query threshold.
Although the algorithm is suitable for link spam detection, it is not applicable in money-sensitive fraud detection.
We have previously proposed an algorithm to detect the coalition attack identi ed in [3].
The attack in [3] involves a coalition of a dishonest publisher, P , with a dishonest Web site, S. S s page will have a script that runs on the surfer s machine when its page loads, and automatically redirects the surfer to P  s Web site.
P will have two versions of its Web page, a non-fraudulent page; and a fraudulent page.
The non-fraudulent page is a normal page that displays the advertisement, and the surfer is totally free to click it or not.
The fraudulent page has a script that runs on the surfer s machine when it loads, and automatically clicks the advertisement.
P selectively shows the fraudulent page when the Web site that referred the surfer to P is S.
The attack silently converts every innocent visit to S to a click on the advertisement in P  s page.
Several factors make the attack virtually impossible to detect.
First, if the commissioner directly visits P  s page, the non-fraudulent page will be loaded.
Second, the commissioner cannot know the Referer  elds of the HTTP requests to publishers.
To identify S, the commissioner has to check all the Internet sites, which is infeasible.
Third, the attack is done in an automatic way that is hidden from the surfer.
In [35], we proposed a solution this sophisticated coalition attack via a collaboration between commissioners and ISPs.
By analyzing the aggregate stream of HTTP requests, the ISP can detect sites that are usually visited before a speci c site, without violating the surfers  privacy.
Bearing in mind the size and the speed of HTTP requests made to ISPs, the problem boils down to identifying associations between HTTP requests that are not widely separated in a tra c stream.
We devised the Streaming-Rules algorithm to detect associations among stream elements.
Although the solution proposed in [35] is e ective, it is very speci c to the attack in [3].
The solution is not e ec-tive against other coalition attacks.
For instance, if each attacker in the coalition controls a network of surfers  machines through Trojans, then HTTP requests for attackers could be widely separated in the ISP HTTP stream, and hence, are not detected by the solution in [35].
However, the general solution proposed in this paper detects coalition attacks in their full generality, including the attack in [3] without the need to the ISPs  support.
We have proposed a generalized solution for detecting generalized coalition attacks of hit in ation.
Since sites  tra c is highly dissimilar, any similarity is usually suspicious.
We modeled the problem of detecting fraud coalitions in terms of the set similarity problem.
We built on several published theoretical results to propose our Similarity-Seeker algorithm that uncovers coalitions of site pairs.
We then extended the detection algorithm to detect coalitions of any size by  nding all maximal cliques in a graph.
On real network data, 93% of the detected sites were provably fraudsters.
This shows how accurate our model and algorithm are regardless of how the attack is designed.
However, several publishers can collude to attack more than one commissioner.
Each commissioner can only know the tra c of its publishers, and no commissioners can detect the similarity between the tra c of the attackers.
Then, to detect attacks that span several advertising networks, we anticipate the development of new specialized auditing, or  detective , entities that are trusted by commissioners.
Our future work focuses on two directions.
The  rst direction is to compare several cliques enumeration using real data.
Although [43] has an optimal worst case bound, other algorithms can outperform it for extremely sparse graphs.
The algorithms in [19, 20, 31, 43] were never experimentally compared, to the best of our knowledge.
The second direction of our future work is to extend the algorithms to the streaming environment.
Bearing in mind that an average-sized commissioner has around 50,000 publishers  sites, and receives around 70M tra c entries per hour, it is desirable to detect coalition attacks using only one scan on the data.
Acknowledgment We thank Dr. Jerry Qi Zheng for helping us with acquiring the real data, and for his useful discussions.
