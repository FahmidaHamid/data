The DUST problem.
The web is abundant with dust: Di erent URLs with Similar Text [17, 10, 20, 18].
For example, the URLs http://google.com/news and http://news.
google.com return similar content.
Adding a trailing slash or /index.html to either returns the same result.
Many web sites de ne links, redirections, or aliases, such as allowing the tilde symbol   to replace a string like /people.
A single web server often has multiple DNS names, and any can be typed in the URL.
As the above examples illustrate, dust is typically not random, but rather stems from some general rules, which we call DUST rules, such as      /people , or  /index.html  at the end of the URL can be omitted.
 A short version of this paper appeared as a poster at experimental results, is available as a technical report [2].
 Supported by the European Commission Marie Curie International Reintegration Grant and by Bank Hapoalim.
 This work was done while the author was at the Technion.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Uri Schonfeld  Dept.
of Computer Science University of California Los Angeles, CA 90095, USA shuri@shuri.org dust rules are typically not universal.
Many are artifacts of a particular web server implementation.
For example, URLs of dynamically generated pages often include parameters; which parameters impact the page s content is up to the software that generates the pages.
Some sites use their own conventions; for example, a forum site we studied allows accessing story number  num  both via the URL http:// domain/story?id=num and via http://domain/story_num.
Our study of the CNN web site has discovered that URLs of the form http://cnn.com/money/whatever get redirected to http://money.cnn.com/whatever.
In this paper, we focus on mining dust rules within a given web site.
We are not aware of any previous work tackling this problem.
Standard techniques for avoiding dust employ universal rules, such as adding http:// or removing a trailing slash, in order to obtain some level of canonization.
Additional dust is found by comparing document sketches.
However, this is conducted on a page by page basis, and all the pages must be fetched in order to employ this technique.
By knowing dust rules, one can reduce the overhead of this process.
Knowledge about dust rules can be valuable for search engines for additional reasons: dust rules allow for a canonical URL representation, thereby reducing overhead in crawling, indexing, and caching [17, 10], and increasing the accuracy of page metrics, like PageRank.
For example, in one crawl we examined, the number of URLs fetched would have been reduced by 26%.
We focus on URLs with similar contents rather than identical ones, since di erent versions of the same document are not always identical; they tend to di er in insigni cant ways, e.g., counters, dates, and advertisements.
Likewise, some URL parameters impact only the way pages are displayed (fonts, image sizes, etc.)
without altering their contents.
Detecting DUST from a URL list.
Contrary to initial intuition, we show that it is possible to discover likely dust rules without fetching a single web page.
We present an algorithm, DustBuster, which discovers such likely rules from a list of URLs.
Such a URL list can be obtained from many sources including a previous crawl or web server logs.1 The rules are then veri ed (or refuted) by sampling a small number of actual web pages.
At  rst glance, it is not clear that a URL list can provide reliable information regarding dust, as it does not include actual page contents.
We show, however, how to use a URL
 to search engines via protocols like Google Sitemaps [13].
tions, which are similar to the  replace  function in editors, and parameter substitutions.
A substring substitution rule       replaces an occurrence of the string   in a URL by the string  .
A parameter substitution rule replaces the value of a parameter in a URL by some default value.
Thanks to the standard syntax of parameter usage in URLs, detecting parameter substitution rules is fairly straightforward.
Most of our work therefore focuses on substring substitution rules.
DustBuster uses three heuristics, which together are very e ective at detecting likely dust rules and distinguishing them from invalid ones.
The  rst heuristic leverages the observation that if a rule       is common in a web site, then we can expect to  nd in the URL list multiple examples of pages accessed both ways.
For example, in the site where story?id= can be replaced by story_, we are likely to see many di erent URL pairs that di er only in this substring; we say that such a pair of URLs is an instance of the rule  story?id=     story_ .
The set of all instances of a rule is called the rule s support.
Our  rst attempt to uncover dust is therefore to seek rules that have large support.
Nevertheless, some rules that have large support are not dust rules.
For example, in one site we found instances such as http://movie-forum.com/story_100 and http:// politics-forum.com/story_100 which support the invalid rule  movie-forum     politics-forum .
Another example is  1     2 , which emanates from instances like pic-1.
jpg and pic-2.jpg, story_1 and story_2, and lect1 and lect2, none of which are dust.
Our second and third heuristics address the challenge of eliminating such invalid rules.
The second heuristic is based on the observation that invalid rules tend to  ock together.
For example in most instances of  1     2 , one could also replace the  1  by other digits.
We therefore ignore rules that come in large groups.
Further eliminating invalid rules requires calculating the fraction of dust in the support of each rule.
How could this be done without inspecting page content?
Our third heuristic uses cues from the URL list to guess which instances are likely to be dust and which are not.
In case the URL list is produced from a previous crawl, we typically have document sketches [7] available for each URL in the list.
These sketches can be used to estimate the similarity between documents and thus to eliminate rules whose support does not contain su ciently many dust pairs.
In case the URL list is produced from web server logs, document sketches are not available.
The only cue about the contents of URLs in these logs is the sizes of these contents.
We thus use the size  eld from the log to  lter out instances (URL pairs) that have  mismatching  sizes.
The di culty with size-based  ltering is that the size of a dynamic page can vary dramatically, e.g., when many users comment on an interesting story or when a web page is personalized.
To account for such variability, we compare the ranges of sizes seen in all accesses to each page.
When the size ranges of two URLs do not overlap, they are unlikely to be dust.
Having discovered likely dust rules, another challenge that needs to be addressed is eliminating redundant ones.
For example, the rule  http://site-name/story?id=     http://site-name/story_  will be discovered, along with many consisting of substrings thereof, e.g.,  ?id=     _ .
However, before performing validations, it is not obvious which rule should be kept in such situations  the latter could be either valid in all cases, or invalid outside the context of the former.
We are able to use support information from the URL list to remove many redundant likely dust rules.
We remove additional redundancies after performing some validations, and thus compile a succinct list of rules.
Canonization.
Once the correct dust rules are discovered, we exploit them for URL canonization.
While the canonization problem is NP-hard in general, we have devised an e cient canonization algorithm that typically succeeds in transforming URLs to a site-speci c canonical form.
Experimental results.
We experiment with DustBuster on four web sites with very di erent characteristics.
Two of our experiments use web server logs, and two use crawl outputs.
We  nd that DustBuster can discover rules very e ectively from moderate sized URL lists, with as little as
 validate or refute each rule.
Our experiments show that up to 90% of the top ten rules discovered by DustBuster prior to the validation phase are found to be valid, and in most sites 70% of the top 100 rules are valid.
Furthermore, dust rules discovered by Dust-Buster may account for 47% of the dust in a web site and that using DustBuster can reduce a crawl by up to 26%.
Roadmap.
The rest of this paper is organized as follows.
Section 2 reviews related work.
We formally de ne the dust detection and canonization problems in Section 3.
Section
 Buster and the canonization algorithm appear in Section 5.
Section 6 presents experimental results.
We end with some concluding remarks in Section 7.
The standard way of dealing with dust is using document sketches [6, 11, 7, 22, 9, 16, 15], which are short summaries used to determine similarities among documents.
To compute such a sketch, however, one needs to fetch and inspect the whole document.
Our approach cannot replace document sketches, since it does not  nd dust across sites or dust that does not stem from rules.
However, it is desirable to use our approach to complement document sketches in order to reduce the overhead of collecting redundant data.
Moreover, since document sketches do not give rules, they cannot be used for URL canonization, which is important, e.g., to improve the accuracy of page popularity metrics.
One common source of dust is mirroring.
A number of previous works have dealt with automatic detection of mirror sites on the web [3, 4, 8, 19].
We deal with the complementary problem of detecting dust within one site.
A major challenge that site-speci c dust detection must address is e ciently discovering prospective rules out of a daunting number of possibilities (all possible substring substitutions).
In contrast, mirror detection is given pairs of sites to compare, and only needs to determine whether they are mirrors.
Our problem may seem similar to mining association rules [1], yet the two problems di er substantially.
Whereas the input of such mining algorithms consists of complete lists of items that belong together, our input includes individual items from di erent lists.
The absence of complete lists renders techniques used therein inapplicable to our problem.
One way to view our work is as producing an Abstract Rewrite System (ARS) [5] for URL canonization via dust rules.
For ease of readability, we have chosen not to adopt the ARS terminology in this paper.
URLs.
We view URLs as strings over an alphabet   of tokens.
Tokens are either alphanumeric strings or non-alphanumeric characters.
In addition, we require every URL to start with the special token ^ and to end with the special token $ (^ and $ are not included in  ).
A URL u is valid, if its domain name resolves to a valid IP address and its contents can be fetched by accessing the corresponding web server (the http return code is not in the
 returned document.
DUST.
Two valid URLs u1, u2 are called dust if their corresponding documents, doc(u1) and doc(u2), are  similar .
To this end, any method of measuring the similarity between two documents can be used.
For our implementation and experiments, we use the popular shingling resemblance measure due to Broder et al. [7].
DUST rules.
We seek general rules for detecting when two URLs are dust.
A dust rule   is a relation over the space of URLs.
  may be many-to-many.
Every pair of URLs belonging to   is called an instance of  .
The support of  , denoted support( ), is the collection of all its instances.
Our algorithm focuses primarily on detecting substring substitution rules.
A substring substitution rule       is speci ed by an ordered pair of strings ( ,  ) over the token alphabet  .
(In addition, we allow these strings to simultaneously start with the token ^ and/or to simultaneously end with the token $.)
Instances of substring substitution rules are de ned as follows: Definition 3.1 (Instance of a rule).
A pair u1, u2 of URLs is an instance of a substring substitution rule      , if there exist strings p, s s.t.
u1 = p s and u2 = p s.
For example, the pair of URLs http://www.site.com/ index.html and http://www.site.com is an instance of the dust rule  /index.html$     $ .
The DUST problem.
Our goal is to detect dust and eliminate redundancies in a collection of URLs belonging to a given web site S. This is solved by a combination of two algorithms, one that discovers dust rules from a URL list, and another that uses them in order to transform URLs to their canonical form.
A URL list is a list of records consisting of: (1) a URL; (2) the http return code; (3) the size of the returned document; and (4) the document s sketch.
The last two  elds are optional.
This type of list can be obtained from web server logs or from a previous crawl.
The URL list is a (nonrandom) sample of the URLs that belong to the web site.
For a given web site S, we denote by US the set of URLs that belong to S. A dust rule   is said to be valid w.r.t.
S, if for each u1   US and for each u2 s.t.
(u1, u2) is an instance of  , u2   US and (u1, u2) is dust.
A DUST rule detection algorithm is given a list L of URLs from a web site S and outputs an ordered list of dust rules.
The algorithm may also fetch pages (which may or may not appear in the URL list).
The ranking of rules represents the con dence of the algorithm in the validity of the rules.
Canonization.
Let R be an ordered list of dust rules that have been found to be valid w.r.t.
some web site S. We would like to de ne what is a canonization of the URLs in US using the rules in R. To this end, we assume that there is some standard way of applying every rule     R, so that applying   to any URL u   US results in a URL  (u) that also belongs to US (this assumption holds true in all the data sets we experimented with).
The rules in R naturally induce a labeled graph GR on US: there is an edge from u1 to u2 labeled by   if and only if (u1, u2) is an instance of  .
Note that adjacent URLs in GR correspond to similar documents.
For the purpose of canonization, we assume that document dissimilarity respects at least a weak form of the triangle inequality, so that URLs connected by short paths in GR are similar URLs too.
Thus, if GR has a bounded diameter (as it does in the data sets we encountered), then every two URLs connected by a path are similar.
A canonization that maps every URL u to some URL that is reachable from u thus makes sense, because the original URL and its canonical form are guaranteed to be dust.
A set of canonical URLs is a subset CUS   US that is reachable from every URL in US (equivalently, CUS is a dominating set in the transitive closure of GR).
A canonization is any mapping C : US   CUS that maps every URL u   US to some canonical URL C(u), which is reachable from u by a directed path.
Our goal is to  nd a small set of canonical URLs and a corresponding canonization, which is e ciently computable.
Finding the minimum size set of canonical URLs is intractable, due to the NP-hardness of the minimum dominating set problem (cf.
[12]).
Fortunately, our empirical study indicates that for typical collections of dust rules found in web sites, e cient canonization is possible.
Thus, although we cannot design an algorithm that always obtains an optimal canonization, we will seek one that maps URLs to a small set of canonical URLs, and always terminates in polynomial time.
Metrics.
We use three measures to evaluate dust detection and canonization.
The  rst measure is precision  the fraction of valid rules among the rules reported by the dust detection algorithm.
The second, and most important, measure is the discovered redundancy the amount of redundancy eliminated in a crawl.
It is de ned as the difference between the number of unique URLs in the crawl before and after canonization, divided by the former.
The third measure is coverage: given a large collection of URLs that includes dust, what percentage of the duplicate URLs is detected.
The number of duplicate URLs in a given URL list is de ned as the di erence between the number of unique URLs and the number of unique document sketches.
Since we do not have access to the entire web site, we measure the achieved coverage within the URL list.
We count the number of duplicate URLs in the list before and after canonization, and the di erence between them divided by the former is the coverage.
One of the standard measures of information retrieval is recall.
In our case, recall would measure what percent of all correct dust rules is discovered.
However, it is clearly impossible to construct a complete list of all valid rules to compare against, and therefore, recall is not directly measurable in our case.
Our algorithm for extracting likely string substitution rules from the URL list uses three heuristics: the large support heuristic, the small buckets heuristic, and the similarity like-these heuristics are e ective on websites of varying scopes and characteristics.
Large support heuristic.
Large Support Heuristic The support of a valid DUST rule is large.
For example, if a rule  index.html$     $  is valid, we should expect many instances witnessing to this e ect, e.g., www.site.com/d1/index.html and www.site.com/d1/, and www.site.com/d3/index.html and www.site.com/d3/.
We would thus like to discover rules of large support.
Note that valid rules of small support are not very interesting anyway, because the savings gained by applying them are negligible.
Finding the support of a rule on the web site requires knowing all the URLs associated with the site.
Since the only data at our disposal is the URL list, which is unlikely to be complete, the best we can do is compute the support of rules in this URL list.
That is, for each rule  , we can  nd the number of instances (u1, u2) of  , for which both u1 and u2 appear in the URL list.
We call these instances the support of   in the URL list and denote them by supportL( ).
If the URL list is long enough, we expect this support to be representative of the overall support of the rule on the site.
Note that since | supportL(     )| = | supportL(     )|, for every   and  , our algorithm cannot know whether both rules are valid or just one of them is.
It therefore outputs the pair  ,   instead.
Finding which of the two directions is valid is left to the  nal phase of DustBuster.
Given a URL list L, how do we compute the size of the support of every possible rule?
To this end, we introduce a new characterization of the support size.
Consider a sub-string   of a URL u = p s.
We call the pair (p, s) the envelope of   in u.
For example, if u =http://www.site.
com/index.html and   = index , then the envelope of   in u is the pair of strings  ^http://www.site.com/  and  .html$ .
By De nition 3.1, a pair of URLs (u1, u2) is an instance of a substitution rule       if and only if there exists a shared envelope (p, s) so that u1 = p s and u2 = p s.
For a string  , denote by EL( ) the set of envelopes of   in URLs that satisfy the following conditions: (1) these URLs appear in the URL list L; and (2) the URLs have   as a substring.
If   occurs in a URL u several times, then u contributes as many envelopes to EL( ) as the number of occurrences of   in u.
The following theorem, proven in the full draft of the paper, shows that under certain conditions, |EL( )   EL( )| equals | supportL(     )|.
As we shall see later, this gives rise to an e cient procedure for computing support size, since we can compute the envelope sets of each substring   separately, and then by join and sort operations  nd the pairs of substrings whose envelope sets have large intersections.
Theorem 4.1.
Let   6=   be two nonempty and non-semiperiodic strings.
Then, | supportL(     )| = |EL( )   EL( )|.
A string   is semiperiodic, if it can be written as   =  k    for some string  , where | | > | |, k   1,  k is the string obtained by concatenating k copies of the string  , and     is a (possibly empty) pre x of   [14].
If   is not semiperiodic, it is non-semiperiodic.
For example, the strings  /////  and  a.a.a  are semiperiodic, while the strings  a.a.b  and  %////  are not.
Unfortunately, the theorem does not hold for rules where one of the strings is either semiperiodic or empty.
For example, let   be the semiperiodic string  a.a  and   =  a .
Let u1 = http://a.a.a/ and let u2 = http://a.a/.
There are two ways in which we can substitute   with   in u1 and obtain u2.
Similarly, let   be  a.  and   be the empty string.
There are two ways in which we can substitute   with   in u1 to obtain u2.
This means that the instance (u1, u2) will be associated with two envelopes in EL( )   EL( ) and with two envelopes in EL( )   EL( ) and not just one.
Thus, when   or   are semiperiodic or empty, |EL( )   EL( )| can overestimate the support size.
On the other hand, such examples are quite rare, and in practice we expect a minimal gap between |EL( )   EL( )| and the support size.
Small buckets heuristic.
While most valid dust rules have large support, the converse is not necessarily true: there can be rules with large support that are not valid.
One class of such rules is substitutions among numbered items, e.g., (lect1.ps,lect2.ps), (lect1.ps,lect3.ps), and so on.
We would like to somehow  lter out the rules with  misleading  support.
The support for a rule       can be thought of as a collection of recommendations, where each envelope (p, s)   EL( )   EL( ) represents a single recommendation.
Consider an envelope (p, s) that is willing to give a recommendation to any rule, for example  ^http://     ^ .
Naturally its recommendations lose their value.
This type of support only leads to many invalid rules being considered.
This is the intuitive motivation for the following heuristic to separate the valid dust rules from invalid ones.
If an envelope (p, s) belongs to many envelope sets EL( 1), EL( 2),.
.
.
, EL( k), then it contributes to the intersections EL( i)   EL( j ), for all 1   i 6= j   k. The substrings  1,  2, .
.
.
,  k constitute what we call a bucket.
That is, for a given envelope (p, s), bucket(p, s) is the set of all substrings   s.t.
p s   L. An envelope pertaining to a large bucket supports many rules.
Small Buckets Heuristic Much of the support of valid DUST substring substitution rules is likely to belong to small buckets.
Similarity likeliness heuristic.
The above two heuristics use the URL strings alone to detect dust.
In order to raise the precision of the algorithm, we use a third heuristic that better captures the  similarity dimension , by providing hints as to which instances are likely to be similar.
Similarity Likeliness Heuristic The likely similar support of a valid DUST rule is large.
We show below that using cues from the URL list we can determine which URL pairs in the support of a rule are likely to have similar content, i.e., are likely similar, and which are not.
The likely similar support, rather than the complete support, is used to determine whether a rule is valid or not.
For example, in a forum web site we examined, the URL list included two sets of URLs http://politics.domain/ story_num and http://movies.domain/story_num with different numbers.
The support of the invalid rule  http:// yet since the corresponding stories were very di erent, the likely similar support of the rule was found to be small.
How do we use the URL list to estimate similarity between documents?
The simplest case is that the URL list includes a document sketch, such as the shingles of Broder et al. [7], for each URL.
Such sketches are typically available when the URL list is the output of a previous crawl of the web site.
When available, documents sketches are used to indicate which URL pairs are likely similar.
When the URL list is taken from web server logs, documents sketches are not available.
In this case we use document sizes (document sizes are usually given by web server software).
We determine two documents to be similar if their sizes  match .
Size matching, however, turns out to be quite intricate, because the same document may have very di erent sizes when inspected at di erent points of time or by di erent users.
This is especially true when dealing with forums or blogging web sites.
Therefore, if two URLs have di erent  size  values in the URL list, we cannot immediately infer that these URLs are not dust.
Instead, for each unique URL, we track all its occurrences in the URL list, and keep the minimum and the maximum size values encountered.
We denote the interval between these two numbers by Iu.
A pair of URLs, u1 and u2, in the support are considered likely similar if the intervals Iu1 and Iu2 overlap.
Our experiments show that this heuristic is very e ective in improving the precision of our algorithm, often increasing precision by a factor of two.
In this section we describe DustBuster our algorithm for discovering site-speci c dust rules.
DustBuster has four phases.
The  rst phase uses the URL list alone to generate a short list of likely dust rules.
The second phase removes redundancies from this list.
The next phase generates likely parameter substitution rules and is discussed in the full draft of the paper.
The last phase validates or refutes each of the rules in the list, by fetching a small sample of pages.
Our strategy for discovering likely dust rules is the following: we compute the size of the support of each rule that has at least one instance in the URL list, and output the rules whose support exceeds some threshold M S. Based on Theorem 4.1, we compute the size of the support of a rule       as the size of the set EL( )   EL( ).
That is roughly what our algorithm does, but with three reservations: (1) Based on the small buckets heuristic, we avoid considering certain rules by ignoring large buckets in the computation of envelope set intersections.
Buckets bigger than some threshold T are called over owing, and all envelopes pertaining to them are denoted collectively by O and are not included in the envelope sets.
(2) Based on the similarity likeliness heuristic, we  lter support by estimating the likelihood of two documents being similar.
We eliminate rules by  ltering out instances whose associated documents are unlikely to be similar in content.
That is, for a given instance u1 = p s and u2 = p s, the envelope (p, s) is disquali ed if u1 and u2 are found unlikely to be similar using the tests introduced in Section 4.
These techniques are provided as a boolean function LikelySimilar which returns false only if the documents of the two input URLs are unlikely to be similar.
The set of all disquali ed envelopes is then denoted D , .
(3) In practice, substitutions of long substrings are rare.
Hence, our algorithm considers substrings of length at most S, for some given parameter S.
To conclude, our algorithm computes for every two sub-strings  ,   that appear in the URL list and whose length is at most S, the size of the set (EL( )   EL( )) \ (O   D , ).
for   = 0 to S do if (|B| = 1 or |B| > T) continue for each pair of distinct tuples t1, t2   B do if (LikelySimilar(t1, t2)) add (t1.substring, t2.substring) to IT for each substring   of r.url of length   do p := pre x of r.url preceding   s := su x of r.url succeeding   add ( , p, s, r.size range/r.doc sketch) to ST





















 t :=  rst tuple in R add tuple (t.substring1, t.substring2, |R|) to RT Figure 1: Discovering likely DUST rules.
Our algorithm for discovering likely dust rules is described in Figure 1.
The algorithm gets as input the URL list L.
We assume the URL list has been pre-processed so that: (1) only unique URLs have been kept; (2) all the URLs have been tokenized and include the preceding ^ and succeeding $; (3) all records corresponding to errors (http return codes in the 4xx and 5xx series) have been  ltered out; (4) for each URL, the corresponding document sketch or size range has been recorded.
The algorithm uses three tables: a substring table ST, an instance table IT, and a rule table RT.
Their attributes are listed in Figure 1.
In principle, the tables can be stored in any database structure; our implementation uses text  les.
In lines 5 10, the algorithm scans the URL list, and records all substrings of lengths 0 to S of the URLs in the list.
For each such substring  , a tuple is added to the substring table ST.
This tuple consists of the substring  , as well as its envelope (p, s), and either the URL s document sketch or its size range.
The substrings are then grouped into buckets by their envelopes (line 11).
Our implementation does this by sorting the  le holding the ST table by the second and third attributes.
Note that two substrings  ,   appear in the bucket of (p, s) if and only if (p, s)   EL( )   EL( ).
In lines 12 16, the algorithm enumerates the envelopes found.
An envelope (p, s) contributes 1 to the intersection of the envelope sets EL( )   EL( ), for every  ,   that appear in its bucket.
Thus, if the bucket has only a single entry, we know (p, s) does not contribute any instance to any rule, and thus can be tossed away.
If the bucket is over owing (its size exceeds T ), then (p, s) is also ignored (line 13).
In lines 14 16, the algorithm enumerates all the pairs ( ,  ) of substrings that belong to the bucket of (p, s).
If it seems likely that the documents associated with the URLs matching) (line 15), ( ,  ) is added to the instance table IT (line 16).
The number of times a pair ( ,  ) has been added to the instance table is exactly the size of the set (EL( ) EL( ))\ (O   D , ), which is our estimated support for the rules       and      .
Hence, all that is left to do is compute these counts and sort the pairs by their count (lines 17 22).
The algorithm s output is an ordered list of pairs.
Each pair representing two likely dust rules (one in each direction).
Only rules whose support is large enough (bigger than M S) are kept in the list.
The full draft of the paper contains the following analysis: Proposition 5.1.
Let n be the number of records in the URL list and let m be the average length (in tokens) of URLs in the URL list.
The above algorithm runs in  O(mnST 2) time and uses O(mnST 2) storage space, where  O suppresses logarithmic factors.
Note that mn is the input length and S and T are usually small constants, independent of m and n. Hence, the algorithm s running time and space are only (quasi) linear.
By design, the output of the above algorithm includes many overlapping pairs.
For example, when running on a forum site, our algorithm  nds the pair ( .co.il/story?id= ,  .co.il/story_ ), as well as numerous pairs of substrings of these, such as ( story?id= ,  story_ ).
Note that every instance of the former pair is also an instance of the latter.
We thus say that the former re nes the latter.
It is desirable to eliminate redundancies prior to attempting to validate the rules, in order to reduce the cost of validation.
However, when one likely dust rule re nes another, it is not obvious which should be kept.
In some cases, the broader rule is always true, and all the rules that re ne it are redundant.
In other cases, the broader rule is only valid in speci c contexts identi ed by the re ning ones.
In some cases, we can use information from the URL list in order to deduce that a pair is redundant.
When two pairs have exactly the same support in the URL list, this gives a strong indication that the latter, seemingly more general rule, is valid only in the context speci ed by the former rule.
We can thus eliminate the latter rule from the list.
We next discuss in more detail the notion of re nement and show how to use it to eliminate redundant rules.
Definition 5.2 (Refinement).
A rule   re nes a rule  , if support( )   support( ).
That is,   re nes  , if every instance (u1, u2) of   is also an instance of  .
Testing re nement for substitution rules turns out to be easy, as captured in the following lemma (proven in the full draft of the paper): Lemma 5.3.
A substitution rule         re nes a substitution rule       if and only if there exists an envelope ( ,  ) s.t.
  =   and     =  .
The characterization given by the above lemma immediately yields an e cient algorithm for deciding whether a substitution rule         re nes a substitution rule      : we simply check that   is a substring of  , replace   by  , and check whether the outcome is    .
If   has multiple occurrences in  , we check all of them.
Note that our algorithm s input is a list of pairs rather than rules, where each pair represents two rules.
When considering two pairs ( ,  ) and ( ,    ), we check re nement in both directions.
Now, suppose a rule         was found to re ne a rule      .
Then, support(       )   support(     ), implying that also supportL(       )   supportL(     ).
Hence, if | supportL(       )| = | supportL(     )|, then supportL(       ) = supportL(     ).
If the URL list is su ciently representative of the web site, this gives an indication that every instance of the re ned rule       that occurs on the web site is also an instance of the re nement        .
We choose to keep only the re nement        , because it gives the full context of the substitution.
One small obstacle to using the above approach is the following.
In the  rst phase of our algorithm, we do not compute the exact size of the support | supportL(     )|, but rather calculate the quantity |(EL( )   EL( )) \ (O   D ,  )|.
It is possible that         re nes       and supportL(       ) = supportL(     ), yet |(EL( )   EL(   )) \ (O   D  ,    )| < |(EL( )   EL( )) \ (O   D , )|.
In practice, if the supports are identical, the di erence between the calculated support sizes should be small.
We thus eliminate the re ned rule, even if its calculated support size is slightly above the calculated support size of the re ning rule.
However, to increase the e ectiveness of this phase, we run the  rst phase of the algorithm twice, once with a lower over ow threshold Tlow and once with a higher over ow threshold Thigh.
While the support calculated using the lower threshold is more e ective in  ltering out invalid rules, the support calculated using the higher threshold is more e ective in eliminating redundant rules.
The algorithm for eliminating re ned rules from the list appears in Figure 2.
The algorithm gets as input a list of pairs, representing likely rules, sorted by their calculated support size.
It uses three tunable parameters: (1) the maximum relative de ciency, MRD, (2) the maximum absolute de ciency, MAD; and (3) the maximum window size, MW.
MRD and MAD determine the maximum di erence allowed between the calculated support sizes of the re ning rule and the re ned rule, when we eliminate the re ned rule.
MW determines how far down the list we look for re nements.
if (already eliminated R[i]) continue for j = 1 to min(MW, |R|   i) do if (R[i].size   R[i + j].size > max(MRD   R[i].size, MAD)) break if (R[i] re nes R[i + j]) eliminate R[i + j]





 else if (R[i + j] re nes R[i]) then eliminate R[i] break Figure 2: Eliminating redundant rules.
The algorithm scans the list from top to bottom.
For each rule R[i], which has not been eliminated yet, the algorithm scans a  window  of rules below R[i].
Suppose s is the calculated size of the support of R[i].
The window size is chosen so that (1) it never exceeds M W (line 4); and (2) the di erence between s and the calculated support size of the M RD   s and M AD (line 5).
Now, if R[i] re nes a rule R[j] in the window, the re ned rule R[j] is eliminated (line 7), while if some rule R[j] in the window re nes R[i], R[i] is eliminated (line 9).
It is easy to verify that the running time of the algorithm In our experiments, this algorithm is at most |R|   M W .
reduces the set of rules by over 90%.
So far, the algorithm has generated likely rules from the URL list alone, without fetching even a single page from the web site.
Fetching a small number of pages for validating or refuting these rules is necessary for two reasons.
First, it can signi cantly improve the  nal precision of the algorithm.
Second, the  rst two phases of DustBuster, which discover likely substring substitution rules, cannot distinguish between the two directions of a rule.
The discovery of the pair ( ,  ) can represent both       and      .
This does not mean that in reality both rules are valid or invalid simultaneously.
It is often the case that only one of the directions is valid; for example, in many sites removing the substring index.html is always valid, whereas adding one is not.
Only by attempting to fetch actual page contents we can tell which direction is valid, if any.
The validation phase of DustBuster therefore fetches a small sample of web pages from the web site in order to check the validity of the rules generated in the previous phases.
The validation of a single rule is presented in Figure
 of URLs from the web site and decides whether the rule is valid.
It uses two parameters: the validation count, N (how many samples to use in order to validate each rule), and the refutation threshold,   (the minimum fraction of counterexamples to a rule required to declare the rule invalid).
2: positive := 0 3: negative := 0

 u := a random URL from L on which applying R results in a di erent URL v := outcome of application of R to u fetch u and v if (fetch u failed) continue if (fetch v failed or DocSketch(u) 6= DocSketch(v)) negative := negative + 1








 15: return true else positive := positive + 1 if (negative    N ) return false Figure 3: Validating a single likely rule.
In order to determine whether a rule is valid, the algorithm repeatedly chooses random URLs from the given test URL list until hitting a URL on which applying the rule results in a di erent URL (line 5).
The algorithm then applies the rule to the random URL u, resulting in a new URL v. The algorithm then fetches u and v. Using document sketches, such as the shingling technique of Broder et al.
[7], the algorithm tests whether u and v are similar.
If they are, the algorithm accounts for u as a positive example attesting to the validity of the rule.
If v cannot be fetched, or they are not similar, then it is accounted as a negative example (lines 9 12).
The testing is stopped when either the number of negative examples surpasses the refutation threshold or when the number of positive examples is large enough to guarantee the number of negative examples will not surpass the threshold.
One could ask why we declare a rule valid even if we  nd (a small number of) counterexamples to it.
There are several reasons: (1) the document sketch comparison test sometimes makes mistakes, since it has an inherent false negative probability; (2) dynamic pages sometimes change signi cantly between successive probes (even if the probes are made at short intervals); and (3) the fetching of a URL may sometimes fail at some point in the middle, after part of the page has been fetched.
By choosing a refutation threshold smaller than one, we can account for such situations.
Figure 4 shows the algorithm for validating a list of likely dust rules.
Its input consists of a list of pairs representing likely substring transformations, (R[i]. , R[i]. ), and a test URL list L.
For a pair of substrings ( ,  ), we use the notation   >   to denote that either | | > | | or | | = | | and   succeeds   in the lexicographical order.
In this case, we say that the rule       shrinks the URL.
We give precedence to shrinking substitutions.
Therefore, given a pair ( ,  ), if   >  , we  rst try to validate the rule      .
If this rule is valid, we ignore the rule in the other direction since, even if this rule turns out to be valid as well, using this rule during canonization is only likely to create cycles, i.e., rules that can be applied an in nite number of times because they cancel out each others  changes.
If the shrinking rule is invalid, though, we do attempt to validate the opposite direction, so as not to lose a valid rule.
Whenever one of the directions of ( ,  ) is found to be valid, we remove from the list all pairs re ning ( ,  )  once a broader rule is deemed valid, there is no longer a need for re nements thereof.
By eliminating these rules prior to validating them, we reduce the number of pages we fetch.
We assume that each pair in R is ordered so that R[i].  > R[i]. .
for j = 1 to i - 1 do if (R[j] was not eliminated and R[i] re nes R[j]) create an empty list of rules LR eliminate R[i] from the list break















 else if (ValidateRule(R[i].    R[i]. , L)) if (ValidateRule(R[i].    R[i]. , L)) if (R[i] was eliminated) continue eliminate R[i] from the list add R[i].    R[i].  to LR else add R[i].    R[i].  to LR Figure 4: Validating likely rules.
The running time of the algorithm is at most O(|R|2 + N |R|).
Since the list is assumed to be rather short, this running time is manageable.
The number of pages fetched is O(N |R|) in the worst-case, but much smaller in practice, since we eliminate many redundant rules after validating rules they re ne.
The canonization algorithm receives a URL u and a list of valid dust rules R. The idea behind this algorithm is very simple: it repeatedly applies to u all the rules in R, until there is an iteration in which u is unchanged, or until a predetermined maximum number of iterations has been reached.
For details see the full draft of the paper.
As the general canonization problem is hard, we cannot expect this polynomial time algorithm to always produce a minimal canonization.
Nevertheless, our empirical study shows that the savings obtained using this algorithm are high.
We believe that this common case success stems from two features.
First, our policy of choosing shrinking rules whenever possible typically eliminates cycles.
Second, our elimination of re nements of valid rules leaves a small set of rules, most of which do no a ect each other.
Experiment setup.
We experiment with DustBuster on four web sites: a dynamic forum site, an academic site (www.ee.technion.ac.il), a large news site (cnn.com) and a smaller news site (nydailynews.com).
In the forum site, page contents are highly dynamic, as users continuously add comments.
The site supports multiple domain names.
Most of the site s pages are generated by the same software.
The news sites are similar in their structure to many other news sites on the web.
The large news site has a more complex structure, and it makes use of several sub-domains as well as URL redirections.
The academic site is the most diverse: It includes both static pages and dynamic software-generated content.
Moreover, individual pages and directories on the site are constructed and maintained by a large number of users (faculty members, lab managers, etc.)
In the academic and forum sites, we detect likely dust rules from web server logs, whereas in the news sites, we detect likely dust rules from a crawl log.
Table 1 depicts the sizes of the logs used.
In the crawl logs each URL appears once, while in the web server logs the same URL may appear multiple times.
In the validation phase, we use random entries from additional logs, di erent from those used to detect the rules.
The canonization algorithm is tested on yet another log, di erent from the ones used to detect and validate the rules.
Web Site Forum Site Academic Site Large News Site Small News Site Log Size Unique URLs







 Table 1: Log sizes.
Parameter settings.
The following DustBuster parameters were carefully chosen in all our experiments.
Our empirical results suggest that these settings lead to good results (see more details in the full draft of the paper).
The maximum substring length, S, was set to 35 tokens.
The maximum bucket size used for detecting dust rules, Tlow, was set to 6, and the maximum bucket size used for eliminating redundant rules, Thigh, was set to 11.
In the elimination of redundant rules, we allowed a relative de ciency, MRD, of up to 5%, and an absolute de ciency, MAD, of 1.
The maximum window size, MW, was set to 1100 rules.
The value of MS, the minimum support size, was set to 3.
The algorithm uses a validation count, N, of 100 and a refutation threshold,  , of 5%-10%.
Finally, the canonization uses a maximum of
 to determine similarity between documents.
Detecting likely DUST rules and eliminating redundant ones.
DustBuster s  rst phase scans the log and detects a very long list of likely dust rules.
Subsequently, the redundancy elimination phase dramatically shortens this list.
Table 2 shows the sizes of the lists before and after redundancy elimination.
It can be seen that in all of our experiments, over 90% of the rules in the original list have been eliminated.
Web Site Forum Site Academic Site Large News Site Small News Site Rules Detected



 Rules Remaining after 2nd Phase



 Table 2: Rule elimination in second phase.
In Figure 5, we examine the precision level in the short list of likely rules produced at the end of these two phases in three of the sites.
Recall that no page contents are fetched in these phases.
As this list is ordered by likeliness, we examine the precision@k; that is, for each top k rules in this list, the curves show which percentage of them are later deemed valid (by DustBuster s validation phase) in at least one direction.
We observe that, quite surprisingly, when similarity-based  ltering is used, DustBuster s detection phase achieves a very high precision rate even though it does not fetch even a single page.
In the forum site, out of the 40 50 detected rules, over 80% are indeed valid.
In the academic site, over 60% of the 300 350 detected rules are valid, and of the top 100 detected rules, over 80% are valid.
In the large news sites, 74% of the top 200 rules are valid.
This high precision is achieved, to a large extent, thanks to the similarity-based  ltering (size matching or shingle matching), as shown in Figures 5(b) and 5(c).
The log includes invalid rules.
For example, the forum site includes multiple domains, and the stories in each domain are di er-ent.
Thus, although we  nd many pairs http://domain1/ story_num and http://domain2/story_num with the same num, these represent di erent stories.
Similarly, the academic site has pairs like http://site/course1/lect-num.
ppt and http://site/course2/lect-num.ppt, although the lectures are di erent.
Such invalid rules are not detected, since stories/lectures typically vary in size.
Figure 5(b) illustrates the impact of size matching in the academic site.
We see that when size matching is not employed, the precision drops by around 50%.
Thus, size matching reduces the number of accesses needed for validation.
Nevertheless, size matching has its limitations  valid rules (such as  ps     pdf ) are missed at the price of increasing precision.
Figure 5(c) shows similar results for the large news site.
When we do not use shingles ltered support, the precision at the top 200 drops to 40%.
Shingles-based  ltering reduces the list of likely rules by roughly 70%.
Most of the  ltered rules turned out to be indeed invalid.
o i s i c e r p






 log#4 log#3 log#2 log#1









 top k rules (a) Forum, 4 di erent logs.
n o i s i c e r p






 With Size Matching No Size Matching





 top k rules n o i s i c e r







 Without shingles-filtered support With shingles-filtered support





 top k rules (b) Academic site, impact of size matching.
(c) Large news site, matching, 4 shingles used.
impact of shingle Figure 5: Precision@k of likely dust rules detected in DustBuster s  rst two phases without fetching actual content.
n o i s i c e r p






 log#4 log#3 log#2 log#1



 number of validations




 n o i s i c e r p





 number of validations log#4 log#3 log#2 log#1

 (a) Forum, 4 di erent logs.
(b) Academic, 4 di erent logs.
Figure 6: Precision among rules that DustBuster attempted to validate vs. number of validations used (N).
Validation.
We now study how many validations are needed in order to declare that a rule is valid; that is, we study what the parameter N in Figure 4 should be set to.
To this end, we run DustBuster with values of N ranging from 0 to 100, and check which percentage of the rules found to be valid with each value of N are also found valid when N=100.
The results from conducting this experiment on the likely dust rules found in 4 logs from the forum site and 4 from the academic site are shown in Figure 6 (similar results were obtained for the other sites).
In all these experiments, 100% precision is reached after 40 validations.
Moreover, results obtained in di erent logs are consistent with each other.
At the end of the validation phase, DustBuster outputs a list of valid substring substitution rules without redundancies.
Table 3 shows the number of valid rules detected on each of the sites.
The list of 7 rules found in one of the logs in the forum site is depicted in Figure 7 below.
These 7 rules or re nements thereof appear in the outputs produced using each of the studied logs.
Some studied logs include 1  3 additional rules, which are insigni cant (have very small support).
Similar consistency is observed in the academic site outputs.
We conclude that the most signi cant dust rules can be adequately detected using a fairly small log with roughly 15,000 unique URLs.
Coverage.
We now turn our attention to coverage, or the percentage of duplicate URLs discovered by DustBuster, in the academic site.
When multiple URLs have the same document sketch, all but one of them are considered duplicates.
In order to study the coverage achieved by DustBuster, we use two di erent logs from the same site: a training log and a test log.
We run DustBuster on the training log in order to Web Site Forum Site Academic Site Large News Site Small News Site Valid Rules Detected



 Table 3: The number of rules found to be valid.
 .co.il/story_     .co.il/story?id=   \&LastView=\&Close=       .php3?     ?   .il/story_     .il/story.php3?id=   \&NewOnly=1\&tvqz=2     \&NewOnly=1   .co.il/thread_     .co.il/thread?rep=   http://www.../story_     http://www.../story?id=  Figure 7: The valid rules detected in the forum site.
learn dust rules and we then apply these rules on the test log.
We count what fraction of the duplicates in the test log are covered by the detected dust rules.
We detect duplicates in the test log by fetching the contents of all of its URLs and computing their document sketches.
Figure 8 classi es these duplicates.
As the  gure shows, 47.1% of the duplicates in the test log are eliminated by DustBuster s canonization algorithm using rules discovered on another log.
The rest of the dust can be divided among several categories: (1) duplicate images and icons; (2) replicated documents (e.g., papers coauthored by multiple faculty members and whose copies appear on each of their web pages); (3)  soft errors , pages, empty search results pages, etc.
for technical assistance.
We thank Israel Cidon, Yoram Moses, and Avigdor Gal for their insightful input.
