Search is maturing to take advantage of taggers, annotators and extractors that associate entities and relations (ER) with text.
E.g., recent personal information management systems [7] represent the extracted data explicitly as an ER graph, and enable powerful searches over the textual graph data model.
A typical graph fragment is shown in Figure 1.
A very useful search paradigm that has surfaced in many forms recently [19, 20, 16, 3] is proximity search or spreading Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
activation.
E.g., a person who works in IBM on XML can be sought by issuing the  schema-light  query type=person NEAR company "IBM", paper "XML".
Note that the relation works-for has not been used, and we can further reduce schema information by, e.g., relaxing paper "XML" to * "XML"), which will also use, e.g., emails containing  XML .
In general, fully o ine static ranking is not feasible in this application domain, because the match predicates can be diverse, even if limited to words.
Figure 1: A typical ER graph The NEAR operator broadly follows a personalized Pagerank-like [11, 12] semantics.
A random surfer model is constructed as in Pagerank [5], with two modi cations:   The surfer does not follow edges out of a node uniformly at random.
Edges have associated types; types are associated with di erent walk probabilities [3, 17,
 of all relations should not be the same, and a balance must be struck between query-speci c and global node prestige.
  When the surfer jumps or teleports, he jumps to a node that satis es a match predicate, e.g., a paper containing  XML  or a company with  IBM  in the text description, and not a node uniformly at random from the whole graph.
Using standard notation, the ER graph is denoted G = (V, E), the  conductance  of edge (u, v)   E is Pr(v|u), i.e., the probability that the random surfer walks from u to v, and is written as element (v, u) in a |V |   |V | conductance matrix C, whose columns sum to 1.
0 <   < 1 is the probability of walking (as against jumping) in each step.
The teleport vector is r; r(u) is positive only for nodes that match some query word.
r, being a multinomial distribution, has unit L1 norm.
The |V |   1 personalized Pagerank vector (PPV) for teleport vector r is written as pr, and is the solution to pr =  Cpr + (1    )r.
(1) We will omit r when unnecessary or clear from the context.
Personworks-forPapercitedin-reply-towrotesentEmailreceivedCompanyWWW 2007 / Track: SearchSession: Personalization5711.1 The problem Spreading activation has been proposed for searching in graphs for over a decade [19, 20, 16].
ObjectRank [3] was among the  rst large-scale implementations of proximity search.
In ObjectRank, a PPV is precomputed for each word in the corpus vocabulary and stored in decreasing order of node scores (which means node IDs must be stored too, taking 8|V | bytes if int and float are used).
Object-Rank supports a few monotone score-combining functions for multi-word queries.
A multi-word query is executed by an e cient merge of per-word PPVs.
Balmin et al. [3, Section 6] demonstrated, through a relevance feedback survey, that ObjectRank captured a su ciently powerful class of scoring/ranking functions.
Vocabulary grows quickly with corpus size and can easily reach a million.
Precomputing a million PPVs will take too long, even though ObjectRank uses some clever tricks to reduce PPV computation time for  almost-acyclic  graphs.
The public ObjectRank demo appears to maintain a disk cache of word PPVs which are used as and when possible.
If a query  misses  in cache, it is rejected and the missing PPVs are computed o ine for possible later use (see Appendix A).
Cache space is the other issue.
A cache of reasonable size will generally be much smaller than the vocabulary size times |V |.
To save space, ObjectRank truncates the PPVs if a PPV element is smaller than some threshold.
The effects of truncation on multi-word queries have not been thoroughly studied before, to our knowledge.
In Section 2.4 we show that multi-word ranking accuracy su ers signi cantly if we truncate PPVs enough to match the size of a basic text index.
Personalized Pagerank (2002) was invented two years before ObjectRank (2004), but, surprisingly, there has been no open work1 to exploit PPVs to solve the performance challenges in the ER graph search framework.
Langville and Meyer write in their well-known survey [13]:  If the holy grail of real-time personalized search is ever to be realized, then drastic speed improvements must be made, perhaps by innovative new algorithms. 
 Our goal is to preprocess the ER graph must faster than computing all word PPVs, and yet answer queries much faster than a query-time ObjectRank computation, while consuming additional index space comparable to a basic text index.
To this end, our key ideas, elaborated into a sketch of our system in Section 3, are as follows:   Based on query logs, choose words and other entity nodes for which PPVs are pre-indexed (Section 4).
These are called hub nodes.
  Do not compute exact PPVs, but exploit the random walk trick of Fogaras et al. [10] to store approximate PPVs in the form of  ngerprints (FPs)  Section 5.
  Given a query, identify a small active subgraph whose PPVs must be computed, bordered by blocker nodes with cached  ngerprints (Section 6).
four hits
 the query objectrank "personalized pagerank" and two hits for the query objectrank ppv.
Teoma/ExpertRank personalizes at topic level.
www.google.com/psearch is not for ER graphs (yet).
for   Adaptively load only a portion of the  ngerprints of the blocker nodes, to save memory and computation signi cantly (Section 7).
  Iteratively estimate the required PPVs based on the small active subgraph, faster than running Pagerank on the whole graph, and report the top results (Section 8).
In addition, we provide many practical guidelines to exploiting partially indexed PPVs in the context of ER graph search.
Both our indexing and search steps are, to some extent,  anytime algorithms  in the sense that we can abandon them at any time and get increasing quality with the e ort invested.
Some indicators of HubRank performance: On our testbed, HubRank preprocesses and indexes 52 times faster than whole-vocabulary PPV computation.
A text index occupies
 PPVs are truncated to 56 MB, precision compared to true Pagerank drops to 0.55; in contrast, HubRank has precision 0.91 at 63 MB.
HubRank s average query time is 200 
 11 seconds on average.
While Pagerank has been personalized in various ways since the  rst papers by Jeh and Widom (J&W) [12] and Haveliwala [11], hub selection is largely unexplored, especially in the context of ER graph search.
Moreover, we know of no public work that uses query log statistics to pick hubs.
Fogaras et al. [10] not only prove that complete, full-precision personalization is doomed to use quadratic space, but they also give a simple, practical alternative: a Monte Carlo approximation of PPVs in the form of a  ngerprint (FP).
FPs are critical to our success, but we go farther in a few ways.
First, we compute FPs only for a few, carefully-chosen nodes.
Second, we adaptively control the resolution to which we compute and use various FPs.
Third, because they keep FPs for each node, Fogaras et al. can compute a PPV estimate for node u based on the FPs at only the out-neighbors of u.
Because we may have a bigger active subgraph, we must resort to an iterative PPV computation.
Our active subgraph expansion draws from a common intuition of in uence decaying with distance, because tele-port deadens long-range in uence [1, 9, 8].
Very recently, Berkhin [4] has independently suggested an active subgraph expansion method called the  bookmark coloring algorithm  (BCA) which is similar to our proposal, but he used PPVs, not FPs, and did not optimize the hub set using query logs.
We will compare our method with BCA in Section 8.
The basic (personalized) Pagerank recurrence is expressed in Equation (1).
J&W [12] showed two far-reaching but easily-veri ed results that we cite below.
Linearity.
pr =  Cpr + (1    )r solves to pr = (1    )(I    C) 1r, which is linear in r. Therefore, a linear combination of teleports is satis ed by the same linear combination of corresponding Pageranks: p r =  pr and pr1+r2 = pr1 + pr2 ; (2) (CiteSeer is words and 55000 nodes and 40577 words).
just one example of the text-embedded-in-graph model that is becoming ubiquitous, e.g., in personal information management [7], but obtaining real query logs would be challenging.)
for any scalar  .
(The above holds for any real   and any vectors r1 and r2, not just valid teleport vectors.)
In Section 3 we will propose a graph representation where each query word w will be a node, connected to entity nodes where the word appears.
Computing a PPV for the word w will amount to setting a teleport vector r =  w in which  w(w) = 1 and  w(u) = 0 for all nodes u 6= w. Given a multi-word query, if we have available the PPV p w for each word w, we can compute the  nal scores of each node as a linear combination of per-word PPVs.
In general for word or entity node u, p u is also called PPVu.
PPV Decomposition.
With PPVu de ned as above,
 (u,v) E PPVu =  C(v, u) PPVv +(1    ) u, (3) or, more compactly, Q =  QC + (1    )I, where the uth column of Q is PPVu and I is the |V |   |V | identity matrix.
The decomposition property is useful when we wish to compute an estimate of PPVu from cached approximations to PPVv for out-neighbors v of u.
Fogaras et al. [10] proved the very important negative result that if a hub set H   V is used, exact storage of all PPVs of H will take  (|H||V |) bits, no matter how clever a compression mechanism is devised.
(If H has a node for each word in the corpus vocabulary, |H||V | is unacceptably large.)
They also showed related bounds where some error could be tolerated.
Then they proposed Monte Carlo PPV estimates: instead of computing an exact PPVu, simulate the random surfer as follows:
 tribution: Pr(  =  ) =  (1    ).
in node v, say.
The walk is repeated numWalks times, where numWalks is a tuned parameter, and a frequency histogram FPu over terminating nodes v is created.
J&W as well as Fogaras et al.
showed that PPVu(v) = Pr(random surfer  nishes at node v).
For a  xed error tolerance in PPVs, a  xed numWalks suf- ces.
An additional bene t is that FP computation involves largely integer arithmetic, faster than heavy  oating-point computation required to solve Equation (1).
In experiments, Fogaras et al. computed FPs for every node, keeping numWalks small to maintain speed.
When PPVu was needed, they used the decomposition property (3) unrolled exactly once to get the bene t of FPs stored at all out-neighbors v of u.
In our case FPs are not stored at all nodes, so we must work harder to reconstruct PPVu (Section 8).
The ER data graph.
The CiteSeer corpus we obtained has 709173 words over 1127741 entity nodes.
Our system can scale to such sizes and beyond, but query-time personalized Pagerank computation (1) is typically 30 50 times slower.
For a thorough comparative evaluation of Object-Rank and HubRank, we picked papers and authors in Cite-Seer prior to 1994.
This subset has about 172000 words over 74000 nodes (which is more in line with the two data Figure 2: Typical Zip an distribution of word frequencies over almost two million queries.
The query log.
We obtained 1.9 million queries from Cite-Seer, with an average of 2.68 words per query.
Word frequencies are distributed in a typical Zip an manner shown in Figure 2.
We used samples of typical size 10000 from the  rst 100000 queries as test data for long experiments, while all but the  rst 100000 queries were used to train and tune our indices.
This sampling procedure (unlike uniform random sampling) made sure that we are not bene ting from splitting a query session with shared words into the training and test set.
To reduce longer-range dependencies our test queries are chronologically before training queries.
Hardware and software.
Experiments were run on a 4-CPU 2.2 GHz Opteron server with 8 GB RAM and Ultra-320 SCSI disks.
All code was written in Java (64-bit JDK1.5) and exploited the trivial parallelism across queries on all four CPUs.
Unless otherwise speci ed, Pagerank iterations used   = 0.8 and were stopped when L1 di erence between iterates dropped below 10 6.
Comparing scores and rankings.
When evaluating accuracy, we will measure two score-related and two rank-related indicators of quality [10], comparing the test algorithm with  full-precision  ObjectRank.
L1 score di erence: If p is the full-precision PPV, and we estimate  p, then k p   pk1 is a reasonable  rst number to check.
However, it is not scale-free.
I.e., for a larger graph, we must demand a smaller di erence.
Moreover, it is not a faithful indicator of ranking  delity [14].
Precision at k: p induces a  true  ranking on all nodes v, while  p induces a distorted ranking.
Let the respective top-k sets be Tk and  Tk.
Then the precision at k is de ned as |Tk    Tk|/k   [0, 1].
Clipping at k is reasonable, because, in applications, users are generally not adversely a ected by erroneous ranking low in the ranked list.
Relative average goodness (RAG) at k: Precision can be excessively severe.
In many real-life social networks, y = 100317x-0.7151100100010000100000110100100010000RankFreqWWW 2007 / Track: SearchSession: Personalization573took about 40 CPU-hours to complete the set, or about 14 seconds per query.
The accuracy is low in this range, even for the relatively forgiving RAG measure.
From Figure 3, it would appear that while truncating at 41 nodes results in poor accuracy, a few hundred nodes per word may be adequate.
But this will not scale; if we tried to process the whole CiteSeer corpus with 709173 words, retaining even
 More problematic is that we need to calculate all word PPVs in the  rst place, before we can truncate them.
Otherwise, we will need to reject queries and calculate the requisite PPVs o ine (Appendix A).
HubRank o ers a practical solution to this dilemma.
In this section, we  rst describe (our adaptation of) the ObjectRank scoring model.
Then we give an overview of how a query is executed; this naturally leads to hub selection and query optimization issues.
These speci c technical problems are solved in the rest of the paper.
As mentioned before, the ER graph has many (node and) edge types: paper-cited-paper, author-wrote-paper, etc.
Edge types are denoted t, taken from a small set T .
Each edge e = (u, v) has an associated type t(e).
Associated with each edge type t is a number  (t)   1.
Thus the weight of edge e is  (t(e)).
In the preprocessing step, we index the text of every node of the ER graph.
We use Lucene [2] to create an inverted index from words to entity node IDs.
A query is a set of distinct words.
To process the query, the ER graph is augmented with one node for each query word, connected to entity nodes where the word appears, with special edges of type  word-to-entity .
A word node appears in W only if it matches at least one entity; thus, no word node is a dead end.
For the moment assume that no entity node is a dead end.
Figure 4: TypedWordGraph with active subgraph, blockers and losers illustrated (see Section 3.2 for de nition of blockers and losers).
The conductance of edge (u, v)   E is now de ned as
 C(v, u) =  (t(u, v)) (u,w) E  (t(u, w)) (4) Figure 3: Truncation reduces the ranking accuracy of ObjectRank signi cantly.
near-ties in Pagerank values are common.
If the true scores of  Tk are large, our approximation is doing ok.
One proposal is (note that  p is not used):

 RAG(k) = v   Tk v Tk p(v) p(v)   [0, 1] Kendall s   : Node scores in a PPV are often closely tied.
Let exact and approximate node scores be denoted Sk(v) and  Sk(v), where the scores are forced to zero if v 6  Tk and v 6   Tk.
A node pair v, w   Tk    Tk is concordant if (Sk(v)   Sk(w))(  Sk(v)    Sk(w)) is strictly positive, and discordant if it is strictly negative.
It is an exact-tie if Sk(v) = Sk(w), and is an approximate tie if  Sk(v) =  Sk(w).
If there are c, d, e and a such pairs respectively, and m pairs overall in Tk    Tk, then Kendall s   is de ned as p(m   e)(m   a) c   d   [ 1, 1].
  (k, u) = (Unlike Fogaras et al., we do not limit to pairs whose scores di er by at least 0.01 or 0.001, so our   is generally smaller.)
Throughout, we use a fairly stringent k = 100 for evaluation.
Since ObjectRank stores PPVs sorted by decreasing Page-rank value, node IDs need to be stored explicitly.
If Page-rank values are stored as floats, each entry requires 8 bytes, so, if all word-PPVs were stored, about 102 GB would be required.
To put this in perspective, a Lucene [2] text index takes only 56 MB, which is only a 0.00056 fraction of 102 GB.
For our CiteSeer subset, this means that, on average, for each word, we can store the top 41 nodes of 74000.
This corresponds closely with numbers in the range of 7 84 reported in the ObjectRank paper [3].
How does truncation a ect scoring and ranking accuracy, compared to the  full-precision  ObjectRank?
For each query, we separately computed PPVs for each word in the query, truncated these word PPVs, then combined them (using the linearity property of PPVs, see Section 2.1).
In Figure 3 we plot RAG, precision and Kendall s   of ObjectRank with PPVs truncated at various ranks (x-axis).
10000 queries were sampled for testing; ObjectRank

 ements are zero.
One may also choose nonuniform teleport to the words, if they are not considered equally important.
Equation (1) is now applied with C and r de ned as above.
Word rareness.
Note that an  inverse document frequency  (IDF) [18] e ect is built into the design.
Suppose the query has one rare and one frequent word.
Each gets half the Page-rank of d, but the rare word passes on the Pagerank to a few entity neighbors, each getting a large share.
The frequent word is connected to many entity nodes, each getting a tiny share of its Pagerank.
For this reason we felt no need to experiment with di erent teleports to query words.
Dead-ends and irreducibility.
A subgraph NW   N is reachable from W , the rest can be ignored for a speci c query and our graph is e ectively W   NW .
To make this irreducible and aperiodic, we add fake edges from entity nodes in NW to a sink entity node s. This eliminates dead ends in NW .
s has a self-loop.
The score of s is ignored.
Equations (1) and (4) continue to apply after these modi cations, and give meaningful scores (except to s).
Learning   automatically.
Assigning weights  (t) for each edge type t might seem like an empirical art from the above discussion.
Indeed, ObjectRank and related systems use manually-tuned weights.
However, there has been recent progress [17, 6] in learning   or C automatically from pairwise preferences (or partial orders) between nodes.
Here we will assume that   is provided to our system by a weight learner [6] prior to indexing.
At startup, our system preloads only the entity nodes N (including the sink node s).
This would be impractical for Web-scale data, but is reasonable for ER search applications.
Only the graph skeleton is loaded, costing us only about eight bytes per node and edge.
Entity graphs with hundreds of millions of nodes and edges can be loaded into regular workstations.
In ongoing work, we are considering disk-based skeletons; see Section 9.
Word nodes are not preloaded.
Given a keyword query to execute, we instantiate the query words as nodes in W and attach each word node to the entity nodes where the word appears.
This gives us a setup time not too large compared to IR engines.
To answer the query, we need the PPVs of the nodes corresponding to the query words.
As shown in Figure 4, we need to work on the entity sub-graph reachable from d through the word nodes.
A total teleport mass of 1  rst reaches the word nodes, then diffuses out to N .
Every node on the way attenuates the total out ow of Pagerank by a factor   < 1.
Therefore, we expect the e ect of distant nodes on a word PPV (that we wish to compute) to be decay rapidly indicated by the gradual shading of the active region in Figure 4.
We stop expanding the active subgraph at two kinds of boundary nodes: blocker2 nodes B   H whose PPVs have been precomputed and stored, and loser nodes that are too far from the word nodes to assert much in uence on the
 cause they block the expansion.
word PPVs.
Given we want |B| (cid:28) |V |, unless we exploit loser nodes our active subgraphs will be too large.
Thus we set up some kind of a  boundary value problem : the active subgraph is bounded everywhere with blocker and loser nodes, whose PPVs remain  xed.
We estimate the PPVs for the remaining active nodes, including query words nodes that are not blockers.
Then we linearly combine word PPVs into the  nal score vector, which is then sorted to return the top answer nodes.
In this section we present approaches to choosing a subset PrMLE(w) = fw/P ability fPr(w) = (fw +  )/P of word and entity nodes that we call the hub set.
Let W0 be the full corpus vocabulary and fw be the frequency of word w   W0 in the query log (after discarding words not in W0).
We can model the log as inducing a multinomial distribution over words and  nd the probabilities that maximize the likelihood of the observed log: w0 fw0 .
(MLE is maximum likelihood estimate.)
In general, even a large log will not touch all words in W0, and many w   W0 will get assigned PrMLE(w) = 0.
Given the long-tailed nature of query logs (Figure 2), this is a problem, because the above model will assign strictly zero probability of seeing a word in W0 that did not occur in the log.
This is a standard problem in NLP [15, Section 6.2.2], and handled by smoothing the word distribution.
Lidstone smoothing is simple and e ective: propose a smoothed prob (f  +  ), and tune parameter 0 <   < 1 so as to maximize the likelihood of a  held-out  portion of the query log.
We omit the fairly standard details, except to summarize the bene ts in Figure 17 at the end of the paper.
6: while frontier 6=   do






 14: sort word and entity nodes by decreasing score(u) let frontier = {w} and priority(w) =fPr(w) remove some u from frontier and mark visited accumulate priority(u) to score(u) for each visited neighbor v do accumulate priority(u)   C(v, u) to score(v) for each unvisited neighbor v do let priority(v) = priority(u)   C(v, u) add v to frontier attach node w to the preloaded entity graph create an empty set of visited nodes Figure 5: Greedy estimation of a measure of merit to including each node into the hub set.
We wish to minimize the average time taken to compute PPVs for all active nodes over a representative query mix.
PPV computation time is likely to be directly related to the number of active nodes, but the connection is not mathematically predictable.
Even if we were to make simplifying assumptions (such as a  xed number of iterations), the problem of picking the best subset reduces to hard problems like dominating set or vertex cover.
Even quadratic-or cubic-time algorithms on CiteSeer-scale graphs may be near-linear-time heuristics.
An indirect approach is to try to arrest active graph expansions with blocker nodes as quickly and e ectively as possible.
I.e., we want to pick a small number of hub nodes that will block expansions from a large number of frequent query word nodes.
If a node is a good hub, it will be reachable along short paths from frequent query word nodes.
This leads to the greedy hub ordering approach shown in Figure 5; it might be regarded as a fast if crude approximation to the push step in BCA [4].
Because teleport is strongest at word nodes and then diffuses out to entities with a loss incurred at every step, it may appear that the originating word nodes have all the advantage in ranking highest in the merit list.
However, the correct intuition is that queries about a link-cluster in the graph will share a theme but not necessarily words.
Over many queries, these individual words may not  oat to the top, but entity nodes at the con uence of many short paths from thematic words will.
This is con rmed in Figure 6.
The order returned by the algorithm in Figure 5 is a nontrivial mix.
Words do crowd the top ranks but soon they are overtaken by entity nodes; in fact, the fraction of words steadily dwindles until nearly all entity nodes are exhausted.
A natural question arises here: Is the nontrivial word-entity mix essential to fast query-processing, or is it an artifact of our heuristic ordering?
If latter, a suitable word PPV caching scheme associated with ObjectRank might be adequate.
Figure 6: For reasonable hub set sizes, entity nodes are highly desirable compared with word nodes; the best case is a nontrivial mix.
To avoid a large number of lengthy runs with di erent sizes of H, we measure surrogates of actual running time: the number of active, blocker and loser nodes as we pick pre xes of di erent sizes from the ordering returned from Figure 5.
(The de nitions of blocker and loser are made precise in Figure 9 in Section 6.)
In Section 8 we establish that these are indeed correlated to running time.
We wish to compare two orders: the mixed order returned by the code in Figure 5, and the order with all entity nodes removed.
In Figure 7, we see that allowing entity nodes into the mix signi cantly reduces the number of active nodes and losers, and increases the number of blockers.
Smaller active set is better because there are fewer PPVs to iterate over.
Larger blocker set is better, because there are more Figure 7: Allowing entity hub nodes improves the prospects of fast, accurate PPV estimation.
PPVs that are pinned to  xed values.
Smaller loser set is better, because fewer PPVs are crudely approximated (see Section 8).
Note that the number of blocker rises, then falls as the hub set size |H| is increased.
This is because, for large |H|, blockers are found closer to the origin nodes.
We can now greedily pick nodes with the largest merit scores, where we will compute FPs.
The score associated with a node u in Figure 5 re ects a combination of how often u is reached from a query, and how strong the connection is.
The latter factor tells us how strongly the FP of u is going to a ect the accuracy of answering a typical query.
Intuitively, if u is in the backwaters, a very low-resolution FP computed at u will su ce, whereas if u is a celebrity, a high-resolution FP is desirable.
In the interest of a clean, precise analysis, Fogaras et al. [10] used the same numWalks at all nodes, but, while building a system, we are at liberty to use diverse numWalks at di erent nodes.
If we assume that one random walk takes roughly a  xed time, and we have a budget of a total number of walks, we should allocate more walks to celebrity nodes and fewer walks to the backwaters.
A straightforward approach is to allocate the budgeted number of walks in proportion to the score of nodes computed in Figure 5.
We can allocate walks in small batches, and terminate the process whenever we run out of space, time or patience.
this simple policy overall in terms of space, time and accuracy.
The space bene t might be explained by the fact that the space required by a FP increases sublinearly with numWalks (Figure 8), making skewed numWalks more attractive than uniform numWalks.
Most FPs are quite sparse.
remove hu, si from frontier if u 6  A then add u to A if  ngerprint FPu is found in index then

 3: let frontier be a max priority queue
 5: while frontier is not empty do











 add u to B load(FPu, s,  ) for blocker u for each child v of u do add hv, s  C(v, u)i to frontier else if s <   (abandon threshold) then add u to L load a trivial FP for loser u else Figure 8: The space taken by the average FP grows slightly sublinearly with increasing numWalks.
 numHits  is the number of distinct end nodes touched by numWalks walks, which determines the storage required (int+short per-record).
Total FP computation time was 10 CPU-hours.
Contrast this with an estimated (via word samples) 526 CPU-hours to compute all word PPVs (even if we truncate them thereafter).
Unless otherwise speci ed, we picked |H| =
 dex.
When a keyword query is submitted, we perform an expansion process similar to that in Figure 5 to collect the active nodes A, except that we also identify blocker nodes B   H whose FPs have been indexed, and loser nodes   which are so distant from the originating node w that even the largest element of PPV( ) is unlikely to in uence PPV(w) much.
As in earlier work on local Pagerank computation [9, 8] we judge if PPVv is  unlikely to in uence  PPVu much via a heuristic.
Ideally, we should check if the conductance from u to v is small, but that amounts to solving part of the PPV estimation problem (which is what BCA [4] does).
Chien et al. [9, Section 3.1] propose a one-pass weight-dissipation type algorithm similar to ours, except that, in the interest of speed, we further omit conductance via multiple paths, noting only the largest conductance path from u to v. (We use all edges while iteratively computing active PPVs.)
Figure 9 shows the  rst stage of query processing: collecting the active subgraph.
There is one more critical hurdle to negotiate before our basic idea works out.
The big advantage of regular Page-rank/ObjectRank is that only one Pagerank vector needs to be computed, whereas we must iteratively estimate PPVs Figure 9: Query-time active subgraph expansion.
For load routines see Section 7.
at all active nodes.
If we convert the cached FPs into full-length PPVs and compute full-length PPVs all over the active subgraph, the sheer handicap we will face by way of  oating point operations will forestall any substantial gains from HubRank.
The key trick is to extend the pruning action of step 12 in Figure 9, from discarding whole FPs to discarding parts of FPs as well.
FPs are stored using Berkeley DB; the key is a node ID and the data is a sequence of (numHits, nodeID) records, sorted in decreasing order of numHits.
As we scan down the list, we keep a cumulative hit count sumHits.
At any point during the loading scan load(FPu, s,  ), if we  nd a node v for which s numHits(v) sumHits <  , we abandon the rest of FPu, and rescale the loaded pre x to be a PPV estimate with unit L1 norm.
Note that FPs are stored to diverse resolutions in the  rst place, but that is wrt an aggregated query mix; for a speci c new query, we may need to load them to a very di erent set of resolutions.
Figure 10: Modest values of   su ce to dramatically reduce the average  ll  (nonzero element count) of FPs loaded over all active nodes, and the number of  oating-point operations per iteration.
This dynamic pruning has dramatic e ect on keeping the loaded PPVs sparse, as can be seen from Figure 10.
With-
by a large margin consistently.
Luckily, as we shall see in Figure 13,   has minimal e ects on accuracy over a broad range.
Loading FPs for a loser node v is easy: we just initialize the PPV to xv.
We do the same for active nodes, but they are then free to  oat, while loser PPVs are pinned.
Once the active subgraph is set up, we run the  dynamic programming  version of J&W s PPV estimation algorithm, keeping blocker and loser PPVs  xed.
This just boils down to iteratively invoking Equation (3) as an iterative assignment: u   X (i) (u,v) E
  C(v, u) [PPV (i 1) v + (1    ) u.
(5) A randomized ordering of us worked best.
Convergence.
J&W prove that iterative PPV updates will converge.
It follows that if we pin blockers to true PPVs, active PPVs will converge to true values.
However, in our case, we fetch from disk FPu, which is a fairly crude estimate of PPVu, which we further truncate.
Because FPs at di er-ent blockers were obtained via di erent random walks, they may be potentially inconsistent.
We argue in Appendix B that given a set of blocker FPs, there exists a PPV assignment to active nodes consistent with the FPs, and iterative PPV updates will take us there.
Unfortunately, unlike the elegant single-FP case analysis of Fogaras et al., we cannot prove that at convergence we get unbiased or otherwise mathematically meaningful PPV estimates; this is left to experimental evaluation.
Figure 11 validates over four (of millions of) queries that convergence is never a problem in practice.
Figure 12: Over 2 3 orders of magnitude, the time for iterative PPV estimation is almost linear in the number of active nodes.
beating ObjectRank time by a substantial margin?
Figure 13 shows that, as   is increased, the overall time taken by HubRank drops dramatically, basically tracking Figure 10.
In contrast, all accuracy indicators remain essentially  at.
The ranking stability persists across 100  increase in   and a 29-fold decrease in FP footprint (note the x-axis is a log-(We found   = 3   10 6 the best value for our scale).
testbed.)
In contrast, observe in Figure 3 how ranking quality drops sharply on a linear x-axis representing index size.
Figure 11: Fast convergence of active PPVs.
Running time.
Figure 12 plots a scatter of time-to-converge against the number of active nodes.
Over a wide range, running time is strongly related to the number of active nodes.
This validates our hub selection approach in Section 4.3 and corroborates Figure 7.
Effect of   on overall speed and accuracy.
Clearly   reduces the memory footprint and computational load of HubRank, but the critical question is, how accurate is the resulting ranking?
And can that accuracy be obtained while Figure 13: HubRank accuracy and running time as a function of  , the threshold for abandoning FPs.
Figure 14 presents average HubRank and ObjectRank query processing times in one chart, against the number of words in a query.
HubRank time is more jittery, but, for short queries, some 20 40 times faster than ObjectRank computed at query time.
The gap is most pronounced at the very frequent short (1 3 word) queries.
Effect of increasing cache size.
Average query time for HubRank drops steeply as the FP cache size increases.
For our testbed, the steep decrease happens right around the same size as the Lucene text index (Figure 15).
But long before the  knee  is reached, HubRank times are less than

 As the FP cache is enlarged, it accommodates more FPs with smaller numWalks, as per our  ngerprint computation
 (average and standard deviation) and relative query frequency against the number of words in a query.
Figure 15: E ect of cache size on HubRank query execution time (average and standard deviation).
policy in Section 5.
As shown in Figure 16, this does very modest damage (less than 0.5%) to precision and   , even as query processing time drops by over a factor of 4.
Smoothing.
Figure 17 shows the bene cial e ects of workload-smoothing on accuracy and speed.
Smoothing ensures that hubs are picked reasonably close to word nodes that do not even appear in the training query log.
This improves both speed and accuracy.
Comparison with BCA.
BCA [4] can be regarded as a re ned version of Figures 5 and 9.
To maintain precise guarantees, BCA starts with a residual of r(w) at word node w, and progresses using push steps.
A push from node u transfers 1    times its current residual to its score, and the remain ing   fraction of its current residual to its out-neighbors  residuals.
BCA has to maintain a heap for the node residuals.
Using a Fibonacci heap, BCA is somewhat slower than HubRank in our preliminary experiments (Figure 18), but both are substantially faster than ObjectRank.
Summary.
We presented HubRank, a workload-driven indexing and dynamic personalized search system for ER graphs.
Figure 16: As cache size increases, we include lower quality FPs, but the drop in ranking accuracy is very modest.
#words  NoSmoothTime SmoothTime NoSmoothPrec SmoothPrec


 .81 .85


 .85 .91


 .85 .92


 .85 .91 Figure 17: HubRank time and precision with and without smoothing, at   = 10 6.
Our index preprocessing is 52 times faster than Object-Rank; our index is comparable to a text index and 0.056% the size of a full ObjectRank index.
Our query time is
 indexing and search are  anytime algorithms  in the sense that we can abandon them at any time but get increasing quality with the e ort invested.
At present HubRank scales to the size of DBLP and CiteSeer, with several millions nodes and almost a million words.
Failure analysis.
We separated sample queries where all words were blocked (empty active set, complete word FPs loaded) vs. queries with nontrivial active sets.
Ranking quality in these two query categories were essentially identical wrt precision and RAG, but surprisingly, wrt   , empty active sets are worse (Figure 19)!
This hints that limited precision of word FPs may be a signi cant impediment to higher accuracy; iterative PPV calculation is not too crude Figure 18: BCA has somewhat higher overhead than HubRank.
subgraph Nonempty Empty #queries

 Avg Prec Avg RAG Prec   .801 .864 .996 .986 .878 .742 Figure 19: Ranking quality in empty and nonempty active subgraphs.
in comparison.
We should revisit the proportional budget allocation policy of Section 5.
The active set over ows RAM once in   2000 queries owing to a dearth of blockers; we need sentinel blockers or a reliable ObjectRank fallback.
Unresolved issues and ongoing work.
We would like to give a theoretical guarantee of the score quality similar to Fogaras et al..
In view of Lempel and Moran s results [14], it is unclear if we can give any guarantee of ranking quality in the Pagerank framework.
We are working on graph clustering and indexing techniques to reduce disk seeks while expanding the active subgraph and loading blocker PPVs, while the graph is largely on disk.
We would also like to support broader classes of predicates on nodes, perhaps involving structured attributes and cached views over and above word matches.
We would like to report top-k without evaluating the whole active set to convergence.
Acknowledgments.
Thanks to C. Lee Giles for CiteSeer data, to Manish Gupta and Amit Phatak for cleaning the data, to Yannis Papakonstantinou for access to Object-Rank source code, and Pavel Berkhin for discussions.
