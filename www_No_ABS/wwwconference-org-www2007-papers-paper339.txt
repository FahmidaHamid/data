The Web is a fascinating object, very extensively studied since a few years.
Among the many research problems it opens, an interesting one is the topological issues, i.e.
describing the shape of the Web [11].
Understanding the hyperlink structure allowed the design of the most powerful search engines like Google, famous because it uses the PageRank algorithm from Brin and Page [21], and the design of other ranking methods like HITS from Kleinberg [15], and cyber-communities detection [19, 14], and many other applications.
However, we only know parts of the Web.
The crawlers are software that automatically browse the Web and cache the  most relevant  information, especially the documents URL and their hyperlinks.
This is recursively performed, the analysis of crawled pages allowing to get new valid URLs.
But bandwidth limitations, HTML errors, unreferenced pages, removed or modi ed pages, and the existence of dynamic pages (generated from requests in Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Fabien de Montgol er LIAFA - Universit e Paris 7

 fm@liafa.jussieu.fr URL or from a session mechanism) make very hard, if not impossible, to output  the  Web: crawlers instead produce partial and biased images.
This is not even a snapshot of the Web, since the crawling takes a long time while the pages are changing rapidly.
From theses observations of the Web, one can try to infer the properties of the  real  Web, the underlying object, but it is hard since the biases are not well known.
So we can not say that the properties of The Web graph are known, but only that some properties of Web crawls are known.
The object we deal with in this paper are therefore the Web crawls, and not the Web itself.
In Section 2 we recall some of the most commonly admitted properties of Web crawls.
In order to explain why the graphs have these properties, many hypotheses from sociology or computer sciences  elds have been proposed.
Many models describe random graphs and some of them (see Section 3) are speci cally designed to model Web Graphs, i.e.
the hyperlink structure of the Web.
The authors usually compare measurements on their random graphs with existing crawls of the Web and conclude how accurate their model is [4, 6, 17, 18, 7, 2, 9, 10, 17, 18].
We also propose a model generating random Web crawls, and show that our random crawls share a lot of properties with real crawls.
But our approach here is quite di erent from the random Web graph models.
Indeed we do not try to model the page writing process, using sociological assumptions about how the people link their own pages to the existing ones.
We try to model the pages crawling process itself instead.
So we do not suppose that pages are linked preferentially to well-known pages, nor that the links of a page are likely a copy of the links of another pages, or such kind of things.
We instead postulates only two things about a crawl:   The in and out-degree of the pages follow Zipf laws (aka power laws), and   the graph is output by a crawler We present our model in details in Section 4.
In Section 5 we show that our random crawls have most of the main crawl properties presented in Section 2.
Web crawls can be quite large objects (for instance Google currently claims more than 8 billion pages in database) but are very sparse graphs, since the average degree is around
 are not necessarily connected, since a connecting page may They have very few sources (pages with no incoming links, either submitted by peoples or unlinked after a while) and a lot of sinks (pages not crawled yet or with no external hyperlink).
Small-World graphs, de ned by Watts and Strogatz [24] and studied by many authors since, are graphs that ful ll the following two properties: 1. the characteristic path length (average distance be tween two vertices) is small: O(log n) or O(log log n) 2. the clustering coe cient (probability that two vertices sharing a common neighbor are linked) is high: O(1).
This de nition can be applied to directed graphs when omitting arc direction.
The Web (in fact the crawls) characteristic path length seems small (about 16 clicks [8] or 19 [3]), consistant with the O(log n) axiom.
Its clustering co-e cient is high.
Existing computations of its exact value di er, but it is admitted that it is greater than 0.1, while random graphs (Erd os-R enyi, see Section 3.1) with the same average degree have clustering coe cient p   0.
Crawls diameter (maximum distance between two pages) is potentially in nite because a dynamic page labeled by n in URL may refer to a dynamic page labeled by n + 1, but since Web crawlers usually perform BFS (see Section 4.1) the diameter of crawls may be actually small.
Zipf laws (a.k.a power laws) are probability laws such that log(P rob(X = d)) =       log(d) If the in-degree (respectively out-degree) distribution of a graph follows a Zipf law, P rob(X = d) is the probability for a vertex to have in (resp.
out) degree d.
In other words, the number of vertices with degree d is k.d  (k depends on the number of vertices n).
A graph class such that the degree of almost all graphs follow a Zipf law is called scale-free because some parameters like   are scale invariant.
Scale-free graphs have been extensively studied [4, 6,
 between objects (proteins, peoples, neurons...) or other network properties seem to have the scale-free property.
For Web crawls, a measure from Broder & al.
[8] on a 200 000 000 pages crawl show that the in and out-degrees follow Zipf law.
The exponents are  in = 2.1 for in-degree and  out = 2.72 for out-degree.
Bow Tie structure According to Broder, Kumar et al [8] the Web has a Bow Tie structure: a quarter of the page are in a Giant Strongly Connected Component (GSCC), a quarter are the  in  page, leading to the GSCC but not linked from there, another quarter are the  out  pages reachable from the GSCC but not linking to it, and the last quarter is not related to the GSCC.
This famous assertion was reported even by Nature [23] but, since four years, an increasing number of people suspects it is a crawling artifact.
According to the same survey, the distribution of the size of strongly connected components follows a Zipf law with exponent roughly 2.5.
Another well-known property of crawls is the existence of cores.
A core is a dense directed bipartite subgraph, consisting in many hubs pages (or fans) pointing many authorities.
It is supposed [19, 16] that such cores are the central structure of cybercommunities, set of pages about the same topics.
The authorities are the most relevant pages, but they do not necessarily point one each other (because competition, for instance) but the hubs list most of them.
Starting from this assumption, HITS algorithm [15] ranks the pages containing a given keyword according to a hub factor and an authority factor.
Kumar et al. [19, 18] enumerate over 200,000 bipartite cores from a 200,000,000 pages crawl of the Web.
Cores sizes (counting hubs, authorities, or both) follow Zipf laws of exponent between 1.09 and 1.4.
Another ranking method, the most popular since it does not depends on given keywords, is Google s PageRank factor [21].
It is an accessibility measure of the page.
Brie y, the PageRank of a page is the probability for a random surfer to be present on this page after a very long surf.
It can be computed by basic linear algebra algorithms.
PageRank distribution also follows a Zipf law with the same exponent as the in-degree distribution [22].
Pages with high PageRank are very visible, since they are e ectively popular on the Web and are linked from other pages with high PageRank.
A crawler therefore easily  nds them [5, 20] while it may miss low-ranked pages.
This is indeed a useful bias for search engine crawlers!
For a long time the most used random graph model was Erd os-R enyi model [13].
The random graph depends on two parameters, the number of vertices n and the probability p for two vertices to be linked.
The existence of each edge is a random variable independent from others.
For suitable values (p = d/n), E.-R. graphs indeed have characteristic path length of O(log n) but very small clustering (p = o(1)) and degree distribution following a Poisson law and not a Zipf law.
Therefore they do not accurately describe the crawls.
Other models have then be proposed where attachment is not independent.
Most random Web graph models [4, 6, 17, 18, 7] propose an incremental construction of the graph.
When the existence of a link is probed, it depends on the existing links.
That process models the creation of the Web across time.
In some models all the link going from a page are inserted at once, and in other ones it is incremental.
The  rst evolving graph model (BA) was given by Barabasi and Albert [4].
The main idea is that new nodes are more likely to join to existing nodes with high degrees.
This model is now referred to as an example of a preferential attachment model.
They concluded that the model generates graphs whose in-degree distribution follows a Zipf law with exponent   = 3.
earized Chord Diagram (LCD), was given in [6].
In this model a new vertex is created at each step, and connects to existing vertices with a constant number of edges.
A vertex is selected as the endpoint of the an edge with probability proportional to its in-degree, with an appropriate normalization factor.
In-degrees follow a Zipf law with exponent roughly 2 when out-degrees are 7 (constant).
In the ACL [2] model, each vertex is associated a in-weight (respectively out-weight) dependent of in-degree (respectively out-degree).
A vertex is selected as the endpoint of the an edge with probability proportional to its weight.
In these models edges are added but never deleted.
The CL-del model [9] and CFV model [10] incorporate in their design both the addition and deletion of nodes and edges.
A model was proposed by [17] to explain other relevant properties of the Web, especially the great number of cores, since the ACL model generates graphs which on average contain few cores.
The linear growth coping model from Kumar& al.
[18] postulates that a Web page author shall copy an existing page when writing its own, including the hyperlinks.
In this model, each new page has a master page from which it copies a given amount of links.
The master page is chosen proportionally to in-degree.
Other links from the new page are then added following uniform or preferential attachment.
The result is a graph with all properties of previous models, plus the existence of many cores.
These models often use many parameters needing  ne tune, and sociological assumptions on how the Web pages are written.
We propose a model based on a computer science assumption: the Web graphs we know are produced by crawlers.
This allow us to design a simpler (it depends only on two parameters get from experiments) and very accurate model of Web crawl.
In this section, we present the crawling strategies and derive our Web crawl model from them.
It aims to mimic the crawling process itself, rather than the page writing process as web graph models do.
Let us consider a theoretical crawler.
We suppose the crawler visits each page only once.
The bene t is to avoid modeling the disappearance of pages or links across time, because the law it follows is still debatable (is the pages lifetime related to their popularity, or to their degree properties?)
When scanning a page, the crawler gets at once the set of its outgoing links.
At any time the (potentially in nite) set of valid URL is divided into
 the corresponding pages were visited and their outgoing links are known
 probed yet
 existing or non-HTML  le (some search engines index them, but they do not contain URL and are not interesting for our purposes)
 The crawling algorithm basically choose and remove from its Unvisited set an URL to crawl, and then adds the outgoing unprobed links of the page, if any, to the Unvisited set.
The crawling strategy is the way the Unvisited set is managed.
It may be:   DFS (depth rst search) The strategy is FIFO and the data structure is a stack   BFS (breadth rst search) The strategy is LIFO and the data structure is a queue   DEG (higher degree) The most pointed URL is chosen.
The data structure is a priority queue (an heap)   RND (random) An uniform random URL is chosen We suppose the crawled pages are ordered by their discovery date.
For discussing structural properties, the crawled pages only are to be considered.
Notice that the  rst three strategies can only be correctly implemented with a single computed.
The most powerful crawlers are distributed on many computers and their strategy is hard to de ne.
It is usually something between BFS and Random.
Our model shall mimic a crawler strategy.
It works in two steps:  rst constructing the set of pages, then adding the hyperlinks.
Constructing the set of pages.
Each page p has two  elds: its in-degree din(p) and its out-degree dout(p).
In the  rst step of the crawl constructing process, we set a value to each of them.
The in-degree and out-degree are set according to two independent Zipf laws.
The exponent of each law is a parameter of the model, therefore our model depends on two parameters  in (for in-degree) and  out (for out-degree).
These values are well known for real crawls: following [8], we have  in = 2.1 and  out = 2.72.
We shall have to chose the pages at random according to their in-degree.
For solving the problem, n pages (the maximal size of the crawl) are generated and their in and out-degrees are set.
Then, a set L is created, where each page p is duplicated din(p) times.
The size of this set is the maximal number of hyperlinks.
Each time we need to choose a page at random according to the in-degree law, we just have to remove one element from L.
Constructing the hyperlinks.
Now the pages degrees are preset, but the graph topology is not yet de ned.
An algorithm, simulating a crawling, shall add the links.
There are indeed four algorithms, depending on which crawling strategy shall be simulated.
The generic algorithm is simply:
 as crawled



 ) g o l ( d e w a r c s e c i t r e v f o r e b m u











 16/x**2.6
 18.05/x**2.62
 19.26/x**2.615







 Out-degree (log) l ) g o l ( d e w a r c s e c i t r e v f o r e b m u










 13.77/x**2.6
 15.4/x**2.13
 16.48/x**2.1





 In-degree (log)


 Figure 1: Out-degree distribution at three steps of a BFS Figure 2: In-degree distribution at three steps of a
 The Unvisited Set is seeded with one or more pages.
The way it is managed depends on which crawling strategy is simulated, i.e. which algorithm is chosen:   For DFS algorithm, the Unvisited Set is a stack (FIFO)   For BFS algorithm, it is a queue (LIFO)   For DEG algorithm, it is a priority queue (heap)   For RND algorithm, a random page is extracted from the Unvisited Set Because the average out-degree of a page is large enough, the crawling process will not stop unless almost all pages have been crawled.
The progress of the crawl (expressed in percent) is the fraction of crawled pages over n. As it approaches n, some weird things will occur as no more unknown pages are allowed.
In our experiments (see the next section) we sometimes go up to 100% progress but results are more realistic before 30%; when the crawl can expand toward unkown pages.
Our model di ers radically from preferential attachment or copy models because the neighborhood of a page is not set at writing time but at crawling time.
So a page is allowed to point known or unknown pages as well.
We present here simulation results using the di erent strategies and showing how the measurements evolve across time.
Thanks to the scale-free e ect, the actual number of pages does not matter, since it is big enough.
We have used several graphs of di erent sizes but with the same exponents  in = 2.1 and  out = 2.72 (experimental values from [8]).
And unless otherwise speci ed, we present results from BFS, the most used crawling strategy, and simulations up to 20,000,000 crawled pages.
At any step of the crawl, the actual degree distribution follows a Zipf law of the given parameters (2.1 and 2.72) with very small deviation (see Figures 1 and 2).
This result is independent from the crawl strategy (BFS, etc.)
It demonstrates that our generated crawls really are scale-free graphs.
The distribution of path length (Figure 3) clearly follows a Gaussian law for BFS, DEG and RAND strategies.
This distribution is plotted at progress 30% but it does not change a lot accros time, as shown in Figure 5.
DFS produces far greater distances between vertices, and the distribution follows an unknown law (Figure 4).
DFS crawls diameter is about 10% of the number of vertices!
This is because DFS crawls are like long tight trees.
It is why DFS is not used by real crawlers, and this paper focuses on the three other crawls strategies.
The clustering (Figure 6), computed on 500,000 pages simulation) is high and do not decrease too much as the crawl goes bigger.
Our crawls de nitely are small-world graphs.
The relative size of the four bow-tie components (SCC, IN, OUT and OTHER) are roughly the same for BFS, DEG and even RAND (but not DFS) strategies (Figure 7).
When using only one seed, the size of the largest SCC converges toward two thirds of the size of the graph.
These proportions thus di er from [8] crawl observations since the  in  and  others  parts are smaller.
But with many seeds (it may be seen as many pages submitted to the crawler portal) the size of the  in  component is larger and can be up to one quarter of the pages.
Our model replicates indeed very well genuine crawls bow-tie topology.
We used Agrawal practical algorithm [1] for cores enumeration (notice that the maximal core problem is NP-complete).
Figure 10 gives the number of core of a given minimal size for a crawl up to 25 000 vertices.
As shown, the number of cores is very dependent from exponents of Zipf laws, since high exponents mean sparser graphs.
It means that our simulated crawls contain many core, as real crawls do.
Figure 9 shows that the number of (4, 4)-cores (at least four hubs and four authorities) is proportional to n and after a while stays between n/100 and n/50.
n o i t a u p o
 ) g o l ( n o i t l a u p o
 2.5e+06 2e+06 1.5e+06 1e+06










 Path length Figure 3: Distribution of path length for BFS and DEG and RAND




















 Path length (log) Figure 4: Distribution of path length for DFS (log/log scale) r e t e m a
 / h t g n e l h t a p e g a r e v




 Diameter BFS, apl DEG, apl RAND, apl BFS, d DEG, d RAND, d log(N)/log(average degree)




 Average path length

 Number of vertices crawled Figure 5: Evolution of diameter and average path length across time for BFS, DEG and RAND t i n e c i f f e o c g n i r e t s u
 l





 0.750029-0.0454094*log(x) 0.546027-0.0293349*log(x)




 Number of vertices crawled Figure 6: Evolution of clustering coe cient across time




 ) n o i t r o p o r p ( e z s


 i










 ) n o i t r o p o r p ( e z s


 i








 Number of vertices crawled






 Number of vertices crawled

 Figure 7: Evolution of the size of of the largest SCC (left) and of the OUT component (right) across time.
One seed, up to 500,000 pages in the crawl
 crawling speed Figure 10 shows the PageRank distribution (PageRank is normalized to 1 and logarithms are therefore negative).
We have found result similar to Pandurangan et al. observations [22]: the distribution is a Zipf law with exponent 2.1.
The crawl quickly converges to this value.
Figure 11 shows the sum of the PageRank of the crawled pages across time (the PageRank computed at the end of the crawl, so that it must vary from 0 at beginning to 1 when crawl stops).
In a very few steps, BFS and DEG strategies  nd the very small amount of pages that contains most of the total PageRank.
This property of real BFS crawlers is known since Najork and Wiener [20].
Our results can be compared to Boldi et al crawling strategies experiments [5].
Figure 12 shows another dynamical property: the discov-It is the probability for the extremity of a link ery rate.
of being already crawled.
It converges toward 40% for all strategies.
This is an interesting scale-free property: after a while, the probability for a URL to point a new page is very high, about 60%.
This  expander  property is very usefull for true crawlers.
This simulation shows it does not depends only on the dynamical nature of the web, but also from the crawling process itself.
hubs auth cores


























 hubs auth cores


























 )
 ,
 ( s e r o c f o n o i t c a r













 Number of vertices crawled




 Figure 9: (4,4) Cores distribution Figure 13 focuses on a well known topological property of crawls, that our simulations also produces, the very high number of sinks regardless of crawl size.
Notice that their existence is a problem for practical PageRank computation [21].
In other words, the large  out  component of the bow-tie is very broad and short... Eiron et al survey the Web frontier ranking [12].
As said in Section 2, a good crawl model should output graphs with the following properties: 1. highly clustered 2. with a short characteristic path length
 4. with many sinks
  nal graph) are crawled early Figure 8: Number of small cores (over 1000 pages) 6. with a bow tie structure
 -18 -16 -14 -12 -10 8 6 4 PageRank values(log) Figure 10: PageRank distribution 1.68*10**(-8)*x+0.062*log(x)-0.39












 -20





 l ) g o l ( d e w a r c s e c i t r e v f o r e b m u
 k n a
 e g a
 l a t o


 2e+06 4e+06 6e+06 8e+06 Number of vertices crawled 1e+07 1.2e+07 1.4e+07 1.6e+07 1.8e+07 2e+07 Figure 11: PageRank capture % l ( x e t r e v d o n a g n e e s f o y t i l i i b a b o r p s e c i t r e v s k n s f i o n o i t c a r


















 4.4*log(x)-31.46

 5.59*log(x)-51.81
 4.4*log(x)-31.46
 5e+06 1e+07 1.5e+07 2e+07 Number of vertices crawled Figure 12: Evolution of the discovery rate 0.644592+-0.0235404*log(x)

 0.791703-0.0323677*log(x)


 2e+06 4e+06 6e+06 8e+06 Number of vertices crawled 1e+07 1.2e+07 1.4e+07 1.6e+07 1.8e+07 2e+07 Figure 13: Evolution of the proportion of sinks (pages with no crawled successors) among crawled pages Property 3 of course is ensured by the model, but the other ones are results of the generating process.
The basic assumption of degree distribution, together with the crawling strategy, is enough to mimic the properties observed in large real crawls.
This is conceptually simpler than other model that also have the same properties like the Copy model [17].
The Bow Tie structure we observe di ers from [8] since the largest strongly connected component is larger.
But together with the other topological properties measured, it proves that we reproduce quite well the topology of real crawls with our very simple model.
It is nice, because we have fewer assumption than [6] or [18].
Our approach is di erent from the Web graph models, that mimic the page writing strategy instead of the page crawling, but give similar result.
It points out that we need more numerical or other measures on graph in order to analyze their structure.
BFS, RAND and DEG strategies are the most used in simple crawlers.
We show that they produce very similar results for topological aspects.
For dynamical aspects (PageRank capture for instance) BFS and DEG seems better, but are harder to implement in a real crawler.
DFS is de nitely bad, and for this reason is not used by crawlers.
Parallel crawler use, however, more sophisticated strategies that were not modeled here.
So our random Web crawls model can be compared with the existing random Web graph models [4, 6, 17, 18, 7, 2, 9,
 assumptions about how the pages are written, but on an assumption on the law followed by the pages degrees and, for the structural properties, on only one assumption that the graph is output by a crawler.
The design is then quite di erent from the design of the random Web graph models, but the results are the same.
We can interpret this conclusion in a pessimistic way: it is hard to tell what are the biases of the crawling.
Indeed we have not supposed that the Web graph has any other speci c property than degrees following a Zipf law, and yet our random crawls have all properties of real crawls.
This means that one can crawl anything following a Zipf law, not only the Web, and output crawls with the speci c properties of the Web crawls.
So the comparison of the result of a Web graph model with real crawls could be not enough to assert that the model captures properties of the Web.
