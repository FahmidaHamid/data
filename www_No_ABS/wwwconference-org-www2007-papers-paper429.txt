The hidden Web has been growing at a very fast pace.
It is estimated that there are several million hidden-Web sites [18].
These are sites whose contents typically reside in databases and are only exposed on demand, as users  ll out and submit forms.
As the volume of hidden information grows, there has been increased interest in techniques that allow users and applications to leverage this information.
Examples of applications that attempt to make hidden-Web information more easily accessible include: metasearchers [14,
 rectories [7, 13] and Web information integration systems [10,
 Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
many hidden-Web sources whose data need to be integrated or searched, a key requirement for these applications is the ability to locate these sources.
But doing so at a large scale is a challenging problem.
Given the dynamic nature of the Web with new sources constantly being added and old sources removed and mod-i ed, it is important to automatically discover the search-able forms that serve as entry points to the hidden-Web databases.
But searchable forms are very sparsely distributed over the Web, even within narrow domains.
For example, a topic-focused best rst crawler [9] retrieves only 94 Movie search forms after crawling 100,000 pages related to movies.
Thus, to e ciently maintain an up-to-date collection of hidden-Web sources, a crawling strategy must perform a broad search and simultaneously avoid visiting large unproductive regions of the Web.
The crawler must also produce high-quality results.
Having a homogeneous set of forms that lead to databases in the same domain is useful, and sometimes required, for a number of applications.
For example, the e ectiveness of form integration techniques [16, 25] can be greatly diminished if the set of input forms is noisy and contains forms that are not in the integration domain.
However, an automated crawling process invariably retrieves a diverse set of forms.
A focus topic may encompass pages that contain searchable forms from many di erent database domains.
For example, while crawling to  nd Airfare search interfaces a crawler is likely to retrieve a large number of forms in di erent domains, such as Rental Cars and Hotels, since these are often co-located with Airfare search interfaces in travel sites.
The set of retrieved forms also includes many non-searchable forms that do not represent database queries such as forms for login, mailing list subscriptions, quote requests, and Web-based email forms.
The Form-Focused Crawler (FFC) [3] was our  rst attempt to address the problem of automatically locating online databases.
The FFC combines techniques for focusing the crawl on a topic with a link classi er which identi es and prioritizes links that are likely to lead to searchable forms in one or more steps.
Our preliminary results showed that the FFC is up to an order of magnitude more e cient, with respect to the number of searchable forms it retrieves, than a crawler that focuses the search on topic only.
This approach, however, has important limitations.
First, it requires substantial manual tuning, including the selection of appropriate features and the creation of the link classi er.
In addition, the results obtained are highly-dependent on the quality of the set of forms used as the training for the may drift away from its target and obtain low harvest rates.
Given the size of the Web, and the wide variation in the hyperlink structure, manually selecting a set of forms that cover a representative set of link patterns can be challenging.
Last, but not least, the set of forms retrieved by the FFC is very heterogeneous it includes all searchable forms found during the crawl, and these forms may belong to distinct database domains.
For a set of representative database domains, on average, only 16% of the forms retrieved by the FFC are actually relevant.
For example, in a crawl to locate airfare search forms, the FFC found 12,893 searchable forms, but among these, only 840 were airfare search forms.
In this paper, we present ACHE (Adaptive Crawler for Hidden-Web Entries),a new framework that addresses these limitations.
Given a set of Web forms that are entry points to online databases,1 ACHE aims to e ciently and automatically locate other forms in the same domain.
Our main contributions are:   We frame the problem of searching for forms in a given database domain as a learning task, and present a new framework whereby crawlers adapt to their environments and automatically improve their behavior by learning from previous experiences.
We propose and evaluate two crawling strategies: a completely automated online search, where a crawler builds a link classi er from scratch; and a strategy that combines o ine and online learning.
  We propose a new algorithm that selects discriminating features of links and uses these features to automatically construct a link classi er.
  We extend the crawling process with a new module that accurately determines the relevance of retrieved forms with respect to a particular database domain.
The notion of relevance of a form is user-de ned.
This component is essential for the e ectiveness of online learning and it greatly improves the quality of the set of forms retrieved by the crawler.
We have performed an extensive performance evaluation of our crawling framework over real Web data in eight representative domains.
This evaluation shows that the ACHE learning strategy is e ective the crawlers are able to adapt and signi cantly improve their harvest rates as the crawl progresses.
Even starting from scratch (without a link clas-si er), ACHE is able to obtain harvest rates that are comparable to those of crawlers like the FFC, that are constructed using prior knowledge.
The results also show that ACHE is e ective and obtains harvest rates that are substantially higher than a crawler whose focus is only on page content  these di erences are even more pronounced when only relevant forms (i.e., forms belong to the target database domain) are considered.
Finally, the results also indicate that the automated feature selection is able to identify good features, which for some domains were more e ective than features identi ed manually.
The remainder of the paper is organized as follows.
Since ACHE extends the focus strategy of the FFC, to make the paper self-contained, in Section 2 we give a brief overview of
  hidden-Web source  interchangeably.
the FFC and discuss its limitations.
In Section 3, we present the adaptive-learning framework of ACHE and describe the underlying algorithms.
Our experimental evaluation is discussed in Section 4.
We compare our approach to related work in Section 5 and conclude in Section 6, where we outline directions for future work.
The FFC is trained to e ciently locate forms that serve as the entry points to online databases it focuses its search by taking into account both the contents of pages and patterns in and around the hyperlinks in paths to a Web page.
The main components of the FFC are shown in white in Figure 1 and are brie y described below.
  The page classi er is trained to classify pages as belonging to topics in a taxonomy (e.g., arts, movies, jobs in Dmoz).
It uses the same strategy as the best rst crawler of [9].
Once the crawler retrieves a page P, if P is classi ed as being on-topic, its forms and links are extracted.
  The link classi er is trained to identify links that are likely to lead to pages that contain searchable form interfaces in one or more steps.
It examines links extracted from on-topic pages and adds the links to the crawling frontier in the order of their predicted reward.
  The frontier manager maintains a set of priority queues with links that are yet to be visited.
At each crawling step, it selects the link with the highest priority.
  The searchable form classi er  lters out non-searchable forms and ensures only searchable forms are added to the Form Database.
This classi er is domain-independent and able to identify searchable forms with high accuracy.
The crawler also employs stopping criteria to deal with the fact that sites, in general, contain few searchable forms.
It leaves a site after retrieving a pre-de ned number of distinct forms, or after it visits a pre-de ned number of pages in the site.
These components and their implementation are described in [3].
Below we discuss the aspects of the link classi er and frontier manager needed to understand the adaptive learning mechanism of ACHE .
Since forms are sparsely distributed on the Web, by prioritizing only links that bring immediate return, i.e., links whose patterns are similar to those that point to pages containing searchable forms, the crawler may miss target pages that can only be reached with additional steps.
The link classi er aims to also identify links that have delayed bene t and belong to paths that will eventually lead to pages that contain forms.
It learns to estimate the distance (the length of the path) between a link and a target page based on link patterns: given a link, the link classi er assigns a score to the link which corresponds to the distance between the link and a page that contains a relevant form.
In the FFC, the link classi er is built as follows.
Given a set of URLs of pages that contain forms in a given database domain, paths to these pages are obtained by crawling backwards from these pages, using the link: facility provided by search engines such as Google and Yahoo!
[6].
The backward crawl proceeds in a breadth rst manner.
Each level shown in blue; and the modules shown in white are used both in the FFC and in ACHE .
l+1 is constructed by retrieving all documents that point to documents in level l. From the set of paths gathered, we manually select the best features.
Using these data, the classi er is trained to estimate the distance between a given link and a target page that contains a searchable form.
Intuitively, a link that matches the features of level 1 is likely to point to a page that contains a form; and a link that matches the features of level l is likely l steps away from a page that contains a form.
The goal of the frontier manager is to maximize the expected reward for the crawler.
Each link in the frontier is represented by a tuple (link, Q), where Q re ects the expected reward for link: Q(state, link) = reward (1) Q maps a state (the current crawling frontier) and a link link to the expected reward for following link.
The value of Q is approximated by discretization and is determined by: (1) the distance between link and the target pages  links that are closer to the target pages have a higher Q value and are placed in the highest priority queues; (2) the likelihood of link belonging to a given level.
The frontier manager is implemented as a set of N queues, where each queue corresponds to a link classi er level: a link l is placed on queue i if the link classi er estimates l is i steps from a targe page.
Within a queue, links are ordered based on their likelihood of belonging to the level associated with the queue.
Although the goal of frontier manager is to maximize the expected reward, if it only chooses links that give the best expected rewards, it may forgo links that are sub-optimal but that lead to high rewards in the future.
To ensure that links with delayed bene t are also selected, the crawling frontier is updated in batches.
When the crawler starts, all seeds are placed in queue 1.
At each step, the crawler selects the link with the highest relevance score from the  rst nonempty queue.
If the page it downloads belongs to the target topic, its links are classi ed by link classi er and added to a separate persistent frontier.
Only when the queues in the crawling frontier become empty, the crawler loads the queues from the persistent frontier.
An experimental evaluation of the FFC [3] showed that FFC is more e cient and retrieves up to an order of magnitude more searchable forms than a crawler that focuses only on topic.
In addition, FFC con gurations with a link classi er that uses multiple levels performs uniformly better than their counterpart with a single level (i.e., a crawler that focuses only on immediate bene t).
The improvements in harvest rate for the multilevel con gurations varied between 20% and 110% for the three domains we considered.
This con rms results obtained in other works which underline the importance of taking delayed bene t into account for sparse concepts [11, 22].
The strategy used by the FFC has two important limitations.
The set of forms retrieved by the FFC is highly heterogeneous.
Although the Searchable Form Classi er is able to  lter out non-searchable forms with high accuracy, a qualitative analysis of the searchable forms retrieved by the FFC showed that the set contains forms that belong to many di erent database domains.
The average percentage of relevant forms (i.e., forms that belong to the target domain) in the set was low around 16%.
For some domains the percentage was as low as 6.5%.
Whereas it is desirable to list only relevant forms in online database directories, such as BrightPlanet [7] and the Molecular Biology Database Collection [13], for some applications this is a requirement.
Having a homogeneous set of the forms that belong to the same database domain is critical for techniques such as statistical schema matching across Web interfaces [16], whose e ectiveness can be greatly diminished if the set of input forms is noisy and contains forms from multiple domains.
Another limitation of the FFC is that tuning the crawler and training the link classi er can be time consuming.
The process used to select the link classi er features is manual: terms deemed as representative are manually selected for each level.
The quality of these terms is highly-dependent on knowledge of the domain and on whether the set of paths obtained in the back-crawl is representative of a wider segment of the Web for that database domain.
If the link classi- er is not built with a representative set of paths for a given database domain, because the FFC uses a  xed focus strategy, the crawler will be con ned to a possibly small subset of the promising links in the domain.
With the goal of further improving crawler e ciency, the quality of its results, and automating the process of crawler setup and tuning, we use a learning-agent-based approach to the problem of locating hidden-Web entry points.
Learning agents have four components [23]:   The behavior generating element (BGE), which based on the current state, selects an action that tries to maximize the expected reward taking into account its goals (exploitation);   The problem generator (PG) that is responsible for suggesting actions that will lead to new experiences, even if the PageFormDatabaseCrawlerLinkClassifierPageClassifierDomain-SpecificFormClassifierFormsRelevant Forms(Link,Relevance)LinksMost relevant linkAdaptiveLinkLearnerFeatureSelectionForm pathFrontierManagerSearchableFormClassifierSearchable FormsForm FilteringWWW 2007 / Track: SearchSession: Crawlers443Algorithm 1 Adaptive Link Learner

 paths = collectP aths(relevantF orms, length) {Collect paths of a given length to pages that contain relevant forms.}
f eatures = F eatureSelector(paths) {Select the features from the neighborhood of links in the paths.}
linkClassif ier = createClassif ier(f eatures, paths) {Create new link classi er.}
updateF rontier(linkClassif ier) {Re-rank links in the frontier using the new link classi er.}
6: end if The policy used by the frontier manager is set by the link classi er.
In ACHE , we employ the adaptive link learner as the learning element.
It dynamically learns features automatically extracted from successful paths by the feature selection component, and updates the link classi er.
The e ectiveness of the adaptive link learner depends on the accuracy of the form ltering process; on the ability of the feature selector to identify  good  features; and on the e -cacy of the frontier manager in balancing exploration and exploitation.
Below we describe the components and algorithms responsible for making ACHE adaptive.
In the FFC, link patterns are learned o ine.
As described in Section 2.1, these patterns are obtained from paths derived by crawling backwards from a set of pages that contain relevant forms.
The adaptive link learner, in contrast, uses features of paths that are gathered during the crawl.
ACHE keeps a repository of successful paths: when it identi es a relevant form, it adds the path it followed to that form to the repository.
Its operation is described in Algorithm 1.
The adaptive link learner is invoked periodically, when the learning threshold is reached (line 1).
For example, after the crawler visits a predetermined number of pages, or after it is able to retrieve a pre-de ned number of relevant forms.
Note that if the threshold is too low, the crawler may not be able to retrieve enough new samples to learn e ectively.
On the other hand, if the value is too high, the learning rate will be slow.
In our experiments, learning iterations are triggered after 100 new relevant forms are found.
When a learning iteration starts, features are automatically extracted from the new paths (Section 3.3).
Using these features and the set of path instances, the adaptive link learner generates a new link classi er.3 As the last step, the link learner updates the frontier manager with the new link classi er.
The frontier manager then updates the Q values of the links using the new link classi er, i.e., it re-ranks all links in the frontier using the new policy.
The e ectiveness of the link classi er is highly-dependent on the ability to identify discriminating features of links.
In ACHE , these features are automatically extracted, as described in Algorithm 2.
The Automatic Feature Selection (AFS ) algorithm extracts features present in the anchor, URL, and text around links that belong to paths which lead to relevant forms.
of levels used in the link classi er.
Figure 2: Highlight of the main components involved in the adaptive aspect of a learning agent.
bene t is not immediate, i.e., the decision is locally subop-timal (exploration);   The critic that gives the online learning element feedback on the success (or failure) of its actions; and   The online learning element which takes the critic s feedback into account to update the policy used by the BGE.
A learning agent must be able to learn from new experiences and, at the same time, it should be robust with respect to biases that may be present in these experiences [20, 23].
An agent s ability to learn and adapt relies on the successful interaction among its components (see Figure 2).
Without exploration, an agent may not be able to correct biases introduced during its execution.
If the BGE is ine ective, the agent is not able to exploit its acquired knowledge.
Finally, a high-quality critic is crucial to prevent the agent from drifting away from its objective.
As we discuss below, ACHE combines these four elements to obtain all the advantages of using a learning agent.
Figure 1 shows the high-level architecture of ACHE .
The components that we added to enable the crawler to learn from its experience are highlighted (in blue).
The frontier manager (Section 2.2) acts as both the BGE and PG and balances the trade-o  between exploration and exploitation.
It does so by using a policy for selecting unvisited links from the crawling frontier which considers links with both immediate and delayed bene t.
The Q function (Equation 1) provides the exploitation component (BGE).
It ensures the crawler exploits the acquired knowledge to select actions that yield high reward, i.e., links that lead to relevant forms.
By also selecting links estimated to have delayed reward, the frontier manager provides an exploratory component (PG), which enables the crawler to explore actions with previously unknown patterns.
This exploratory behavior makes ACHE robust and enables it to correct biases that may be introduced in its policy.
We discuss this issue in more detail in Section 4.
The form  ltering component is the critic.
It consists of two classi ers: the searchable form classi er (SFC)2; and the domain-speci c form classi er (DSFC).
Forms are processed by these classi ers in a sequence: each retrieved form is  rst classi ed by the SFC as searchable or non-searchable; the DSFC then examines the searchable forms and indicates whether they belong to the target database domain (see Section 3.4 for details).
successful actionsknown actionunknown actionCriticOnline learningPGBGEsuccessful actionsknown actionunknown actionCriticBGEPGOnlineLearningsuccessful actionsknown actionunknown actionCriticBGEPGOnlineLearningWWW 2007 / Track: SearchSession: Crawlers444Algorithm 2 Automatic Feature Selection

 features selected in the three feature spaces  anchor, URL and around

 termSet = getTermSet(featureSpace, paths) {From the paths, obtain terms in speci ed feature space.}
termSet = removeStopWords(termSet) stemmedSet = stem(termSet) if featureSpace == URL then










 topKTerms= getMostFrequentTerms(stemmedSet, k) {Obtain the set of k most frequent terms.}
for each term t   topKTerms do for each term t0   stemmedSet that contains the sub-string t do addFrequency(stemmedSet,t,t0) {Add frequency of t0 to t in stemmedSet.}
end for end for end if selectedFeatures = getNMostFrequentTerms(termSet) {Obtain a set of the top n terms.}
16: end for Initially, all terms in anchors are extracted to construct the anchor feature set.
For the around feature set, AFS selects the n terms that occur before and the n terms that occur after the anchor (in textual order).
Because the number of extracted terms in these di erent contexts tends to be large, stop-words are removed (line 5) and the remaining terms are stemmed (line 6).
The most frequent terms are then selected to construct the feature set (line 15).
The URL feature space requires special handling.
Since there is little structure in a URL, extracting terms from a URL is more challenging.
For example,  jobsearch  and  usedcars  are terms that appear in URLs of the Job and Auto domains, respectively.
To deal with this problem, we try to identify meaningful sub-terms using the following strategy.
After the terms are stemmed, the k most frequent terms are selected (topKTerms in line 8).
Then, if a term in this set appears as a substring of another term in the URL feature set, its frequency is incremented.
Once this process  nishes, the k most frequent terms are selected.
The feature selection process must produce features that are suitable for the learning scheme used by the underlying classi er.
For text classi cation Zheng et al. [29] show that the Na ve Bayes model obtains better results with a much lower number of features than linear methods such as Support Vector Machines [20].
As our link classi er is built using the Na ve Bayes model, we performed an aggressive feature selection and selected a small number of terms for each feature space.
The terms selected are the ones with highest document frequency (DF)4.
Experiments conducted by Yang and Pedersen [27] show that DF obtains results comparable to task-sensitive feature selection approaches, as information gain [20] and Chi-square [12].
AFS is very simple to implement, and as our experimental results show, it is very e ective in practice.
The form  ltering component acts as a critic and is responsible for identifying relevant forms gathered by ACHE .
It assists ACHE in obtaining high-quality results and it also
 in a collection where a given term occurs.
enables the crawler to adaptively update its focus strategy, as it identi es new paths to relevant forms during a crawl.
Therefore, the overall performance of the crawler agent is highly-dependent on the accuracy of the form ltering process.
If the classi ers are inaccurate, crawler e ciency can be greatly reduced as it drifts way from its objective through unproductive paths.
The form ltering process needs to identify, among the set of forms retrieved by the crawler, forms that belong to the target database domain.
Even a focused crawler retrieves a highly-heterogeneous set of forms.
A focus topic (or concept) may encompass pages that contain many different database domains.
For example, while crawling to  nd airfare search interfaces the FFC also retrieves a large number of forms for rental car and hotel reservation, since these are often co-located with airfare search interfaces in travel sites.
The retrieved forms also include non-searchable forms that do not represent database queries such as forms for login, mailing list subscriptions, and Web-based email forms.
ACHE uses HIFI, a hierarchical classi er ensemble proposed in [4], to  lter out irrelevant forms.
Instead of using a single, complex classi er, HIFI uses two simpler classi ers that learn patterns of di erent subsets of the form feature space.
The Generic Form Classi er (GF C) uses structural patterns which determine whether a form is searchable.
Empirically, we have observed that these structural characteristics of a form are a good indicator as to whether the form is searchable or not [3].
To identify searchable forms that belong to a given domain, HIFI uses a more specialized clas-si er, the Domain-Speci c Form Classi er (DSF C).
The DSFC uses the textual content of a form to determine its domain.
Intuitively, the form content is often a good indicator of the database domain it contains metadata and data that pertain to the database.
By partitioning the feature space, not only can simpler classi ers be constructed that are more accurate and robust, but this also enables the use of learning techniques that are more e ective for each feature subset.
Whereas decision trees [20] gave the lowest error rates for determining whether a form is searchable based on structural patterns, SVMs [20] proved to be the most e ective technique to identify forms that belong to the given database domain based on their textual content.
The details of these classi ers are out of the scope of this paper.
They are described in [4], where we show that the combination of the two classi ers leads to very high precision, recall and accuracy.
The e ectiveness of the form  ltering component is con rmed by our experimental evaluation (Section 4): signi cant improvements in harvest rates are obtained by the adaptive crawling strategies.
For the database domains used in this evaluation, the combination of these two classi ers results in accuracy values above 90%.
We have performed an extensive performance evaluation of our crawling framework over real Web data in eight representative domains.
Besides analyzing the overall performance of our approach, our goals included: evaluating the e ectiveness of ACHE in obtaining high-quality results (i.e., in retrieving relevant forms); the quality of the features automatically selected by AFS ; and assessing the e ectiveness of online learning in the crawling process.
Airfare Auto Book Hotel Job Movie Music Rental airfare search used cars books search hotel availability job search movie titles and DVDs music CDs car rental availability Density Norm.
Density







 Table 1: Database domains used in experiments and density of forms in these domains.
The column labeled Norm.
Density shows the density values normalized with respect to the lowest density value (for the Movie domain).
Database Domains.
We evaluated our approach over the eight online database domains described in Table 1.
This table also shows the density of relevant forms in the domains.
Here, we measure density as the number of distinct relevant forms retrieved by a topic-focused crawler (the baseline crawler described below) divided by the total number of pages crawled.
Note that not only are forms very sparsely distributed in these domains, but also that there is a large variation in density across domains.
In the least dense domain (Movie), only 94 forms are found after the baseline crawler visits 100,000 pages; whereas in the densest domain (Hotel), the same crawler  nds 19 times as many forms (1857 forms).
Crawling Strategies.
To evaluate the bene t of online learning in ACHE , we ran the following crawler con gura-tions:   Baseline, a variation of the best rst crawler [9].
The page classi er guides the search and the crawler follows all links that belong to a page whose contents are classi ed as being on-topic.
One di erence between baseline and the best rst crawler is that the former uses the same stopping criteria as the FFC; 5   O ine Learning, the crawler operates using a  xed policy that remains unchanged during the crawling process this is the same strategy used by the FFC [3];   O ine-Online Learning, ACHE starts with a pre-de ned policy, and this policy is dynamically updated as the crawl progresses;   Online Learning, ACHE starts using the baseline strategy and builds its policy dynamically, as pages are crawled.
All con gurations were run over one hundred thousand pages; and the link classi ers were con gured with three levels.
E ectiveness measure.
Since our goal is to  nd search-able forms that serve as entry points to a given database domain, it is important to measure harvest rate of the crawlers based on the number of relevant forms retrieved per pages
 priate stopping criteria, the best rst crawler gets trapped in some sites, leading to extremely low harvest rates [3].
Figure 3: Number of relevant forms returned by the di erent crawler con gurations.
crawled.
It is worthy of note that harvest rates reported in [3] for the FFC (o ine learning) took into account all searchable forms retrieved a superset of the relevant forms.
Below, as a point of comparison, we also show the harvest rates for the di erent crawlers taking all searchable forms into account.
Figure 3 gives, for each domain, the number of relevant forms retrieved by the four crawler con gurations.
Online learning leads to substantial improvements in harvest rates when applied to both the Baseline and O ine con gura-tions.
The gains vary from 34% to 585% for Online over Baseline, and from 4% to 245% for O ine-Online over Of ine.
These results show that the adaptive learning component of ACHE is able to automatically improve its focus based on the feedback provided by the form  ltering component.
In addition, Online is able to obtain substantial improvements over Baseline in a completely automated fashion requiring no initial link classi er and greatly reducing the e ort to con gure the crawler.
The only exception is the Movie domain.
For Movie, the most sparse domain we considered, the Online con guration was not able to learn patterns with enough support from the 94 forms encountered by Baseline.
E ect of Prior Knowledge.
Having background knowledge in the form of a  good  link classi er is bene cial.
This can be seen from the fact that O ine-Online retrieved the largest number of relevant forms in all domains (except for Rental, see discussion below).
This knowledge is especially useful for very sparse domains, where the learning process can be prohibitively expensive due to the low harvest rates.
There are instances, however, where the prior knowledge limits the crawler to visit a subset of the productive links.
If the set of patterns in the initial link classi er is too narrow, it will prevent the crawler from visiting other relevant pages reachable through paths that are not represented in the link classi er.
Consider, for example the Rental domain, where Online outperforms O ine-Online.
This behavior may sound counter-intuitive, since both con gurations apply online learning and O ine-Online starts with an advantage.
The initial link classi er used by O ine-Online was biased, and the adaptive process was slow at correcting this bias.
A closer examination of the features used by O ine-Online shows that, over time, they converge to the same set of features of Online.
The Online, in contrast, started with no bias and was able to outperform O ine-Online in a window of 100,000 pages.
Forms retrievedWWW 2007 / Track: SearchSession: Crawlers446Figure 4: Relative performance of O ine-Online over Baseline.
The domains are ordered with respect to their densities.
The presence of bias in the link classi er also explains the poor performance of O ine in Rental, Book and Airfare.
For these domains, O ine-Online is able to eliminate the initial bias.
ACHE automatically adapts and learns new patterns, leading to a substantial increase the number of relevant forms retrieved.
In the Book domain, for instance, the initial link classi er was constructed using manually gathered forms from online bookstores.
Examining the forms obtained by the O ine-Online, we observed that forms for online bookstores are only a subset of the relevant forms in this domain.
A larger percentage of relevant forms actually appear in library sites.
ACHE successfully learned patterns to these sites (see Table 2).
Another evidence of the e ectiveness of the adaptive learning strategy is the fact that Online outperforms O ine for four domains: Airfare, Auto, Book, and Rental.
For the latter two, Online retrieved 275% and 190% (resp.)
more forms than O ine.
This indicates that a completely automated approach to learning is e ective and able to outperform a manually con gured crawler.
The Link Classi er and Delayed Bene t.
Figure 4 shows the relative performance between the O ine-Online con guration of ACHE and Baseline, with respect to both relevant forms and searchable forms.
Here, the domains are ordered (in the x axis) by increasing order of density.
Note that for the sparser domains, the performance di erence between ACHE and Baseline is larger.
Also note that the gains from delayed bene t are bigger when the performance is measured with respect to relevant forms.
For example, in the Book domain, O ine-Online retrieves almost 9 times more relevant forms than Baseline.
The performance di er-ence is much smaller for searchable forms O ine-Online retrieves only 10% more searchable forms than Baseline.
This can be explained due to the fact that searchable forms are much more prevalent than relevant forms within a focus topic.
The numbers in Figure 4 underline the importance of taking delayed bene t into account while searching for sparse concepts.
Delayed bene t also plays an important role in the e ec-tiveness of the adaptive learning component of ACHE .
The use of the link classi er forces ACHE to explore paths with previously unknown patterns.
This exploratory behavior is key to adaptation.
For example, in the Book domain (see (a) Auto (b) Book (c) Movie Figure 5: Number of forms retrieved over time.
Table 2), since the initial link classi er has a bias towards online bookstores, if ACHE only followed links predicted to yield immediate bene t, it would not be able to reach the library sites.
Note, however, that the exploratory behavior can potentially lead the crawler to lose its focus.
But as our experimental results show, ACHE is able to obtain a good balance, being able to adapt to new patterns while maintaining its focus.
Crawler Performance over Time.
To give insight about the behavior of the di erent crawler con gurations, it is useful to analyze how their harvest rates change over time.
Here, we only show these results for Book, Auto and Movie in Figure 5.
Similar results were obtained for the other domains.
Note that the number of relevant forms retrieved by Online and Baseline coincide initially, and after the crawler starts the learning process, the performance of Online improves substantially.
A similar trend is observed for O ine and O ine-Online the number of relevant forms retrieved by O ine-Online increases after its policy is  rst updated.
AutoBookMovieWWW 2007 / Track: SearchSession: Crawlers447Another interesting observation is that the rate of increase in the number of relevant forms retrieved is higher for the con gurations that use online-learning.
The increase in the number of forms retrieved over time is a good indication of the e ectiveness of online learning for a particular domain.
For Movie (Figure 5(c)), after crawling
 ter that, the number of forms remain (almost) constant (a single additional form is found in the last 30,000 pages).
Because so few relevant forms are found, Online is not able to learn due to insu cient evidence for the link patterns.Note that the lines for Baseline and Online coincide for the Movie domain.
In the Book domain, which is also sparse but less so than Movie, Online was able to learn useful patterns and substantially improve the harvest rate.
As shown in Figure 5(b), the  rst learning iteration happened after 50,000 pages had been crawled much later than for the other denser domains.
For example, for Auto the  rst iteration occurs at 17,000 pages (Figure 5(a)).
The Auto domain provides a good example of the adaptive behavior of ACHE in denser domains.
These results show that, even starting with no information about patterns that lead to relevant forms, these patterns can be learned dynamically and crawler performance can be improved in an automatic fashion.
However, the sparser the domain is, the harder it is for the crawler to learn.
For Online to be e ective in a very sparse domain, a crawler that is more focused than Baseline is needed initially.
The performance improvement obtained by the adaptive crawler con gurations provides evidence of the e ectiveness of the automatic feature selection described in Section 3.3.
As an example of its operation, consider Figure 6, which shows the terms selected by AFS in 4 learning iterations for the Auto domain using the Online con guration.
Note that AFS is able to identify good features from scratch and without any manual intervention.
For both Anchor and Around feature sets, already in the  rst iteration, relevant terms are discovered which remain in subsequent iterations (e.g., car, auto), although their frequency changes over time.
For instance, the term  auto  has the highest frequency in the  rst iteration, whereas  car  has the highest frequency after the second iteration.
Unlike Anchor and Around, the URL feature set is not so well-behaved.
Because URLs contain uncommon terms and more variability, the patterns take longer to converge.
As Figure 6 shows, after the  rst iteration the AFS selects terms that disappear in subsequent iterations (e.g.,  index  and  rebuilt ).
In addition, the frequencies of terms in the URL are much lower than in the other feature spaces.
A  nal observation about the automatic feature selection is that by analyzing how the features evolve over time, and change at each learning iteration, insights can be obtained about the adaptive behavior of ACHE .
For example, as Table 2 illustrates, for the O ine-Online in the Book domain, the features selected for the initial link classi er are clearly related to online bookstores (e.g., book, search and book-stor).
As new relevant forms are encountered, new terms are introduced that are related to library sites (e.g., ipac,6 librari, book, search, and catalog).
6ipac is a system used by some library sites to search their catalogs.
(a) URL (b) Anchor (c) Around Figure 6: Features automatically extracted in di er-ent iterations of adaptive learning for Online in the Auto domain.
There is a rich literature in the area of focused crawlers (see e.g., [1, 3, 8, 9, 11, 22, 24, 19]).
Closely related to our work are strategies that apply online-learning and that take delayed bene t into account.
We discuss these below.
Delayed Bene t.
Rennie and McCallum [22] frame the problem of creating e cient crawlers as a reinforcement learning task.
They describe an algorithm for learning a function that maps hyperlinks to future discounted reward: the expected number of relevant pages that can be found as a result of following that link.
They show that by taking delayed rewards into account, their RL Spider is able to e ciently locate sparse concepts on a predetermined universe of URLs.
There are several important di erences between the RL Spider and ACHE .
First and foremost, the RL Spider does not perform a broad search over the Web.
It requires as input the URLs of all sites to be visited and performs a focused search only within these predetermined sites.
Second, it is not adaptive the classi er maintains a  xed policy during the crawl.
Finally, although their classi er also learns to prioritize links and it considers links that have delayed ben-e t, the learning function used in ACHE is di erent: the link classi er estimates the distance between a link and a relevant form (see Section 2.1).
The importance of considering delayed bene t in a focused crawl was also discussed by Diligenti et al. [11].
Their Feature Selection-URL, AroundFeature Selection AnchorFeature Selection-URL, AroundWWW 2007 / Track: SearchSession: Crawlers448Iteration 0 (initial features)





 book,search,addal,natur,hist search,index,book search,adv,lib,index,ipac search,lib,ipac,pro l,catalog search,lib,ipac,pro l,catalog search,lib,ipac,pro l,catalog Selected Features Anchor book,search,addal,bookstor,link search,book,advanc,librari,engin search,book,advanc,librari,catalog search,librari,catalog,advanc,book librari,search,catalog,advanc,book librari,search,catalog,advanc,book Around book,search,onlin,new,bookstor book,search,titl,onlin,author book,search,librari,titl,author librari,search,book,catalog,titl librari,search,catalog,book,titl librari,search,book,catalog,public Table 2: Features selected during the execution of the O ine-Online in Book domain.
Context Focused Crawler (CFC) learns a hierarchy of concepts related to a search topic.
The intuition behind their approach is that topically relevant pages can be found by using a context graph which encodes topics that are directly or indirectly related to the target topic.
By performing a backward crawl from a sample set of relevant pages, they create classi ers that identify these related topics and estimate, for a given page r, the distance between r and a topic-relevant page.
Unlike ACHE , the focus of the CFC is solely based on the contents of pages it does not prioritize links.
If a page is considered relevant, all links in that page will be followed.
Like the RL Spider, the CFC uses a  xed focus strategy.
The FFC [3] combines ideas from these two approaches.
It employs two classi ers: one that uses page contents that focuses the search on a given topic; and another that identi- es promising links within the focus topic.
An experimental evaluation over three distinct domains showed that combining the page contents and hyperlink structure leads to substantial improvement in crawler e ciency.
The di erences between FFC and ACHE are discussed in Section 3.
Unlike these approaches, ACHE adaptively updates its focus strategy as it learns from new experience.
There is an important bene t derived from combining delayed bene t and online-learning: following links that have delayed bene- t forces the crawler to explore new paths and enables it to learn new patterns.
This is in contrast to strategies based on immediate bene t that exploit actions it has already learned will yield high reward.
This exploratory behavior makes our adaptive learning strategy robust and enables it to correct biases created in the learning process.
This was observed in several of the domains we considered in our experimental evaluation (Section 4).
In the Book domain, for instance, the crawler with a  xed policy (O ine) was trapped in a subset of promising links related to online bookstores, whereas ACHE was able to eliminate this bias in the  rst learning iteration and learn new patterns that allowed it to also obtain relevant forms from online library sites.
Online Learning Policies.
Chakrabarti et al. [8] proposed an online learning strategy that, similar to ACHE uses two classi ers to focus the search: a baseline page classi er that learns to classify pages as belonging to topics in a taxonomy [9]; and the apprentice, a classi er that learns to identify the most promising links in a topic-relevant page.
Their motivation to use the apprentice comes from the observation that, even in domains that are not very narrow, the number of links that are irrelevant to the topic can be very high.
Thus, following all the links in a topic-relevant page can be wasteful.
The baseline classi er captures the user s speci cation of the topic and functions as a critic of the apprentice, by giving feedback about its choices.
The apprentice, using this feedback, learns the features of good links and is responsible for prioritizing the links in the crawling frontier.
Although ACHE also attempts to estimate the bene t of following a particular link based on the crawler experience, there is an important di erence.
Because the apprentice only follows links that give immediate bene t, biases that are introduced by the online learning process are reinforced as the crawl progresses the crawler will repeatedly exploit actions it has already learned will yield high reward.
In contrast, as discussed above, by considering links that may lead to delayed bene t, ACHE has a more exploratory behavior and will visit unknown states and actions.
It is worthy of note that the goal of our link classi er is complementary to that of the apprentice it aims to learn which links lead to pages that contain relevant forms, whereas the goal of the apprentice is to avoid o topic pages.
In addition, we are dealing with concepts that are much sparser than the ones considered by Chakrabarti et al.. For example, the density of the domains considered in [8] varied from 31% to 91%, whereas for concepts we considered density values range between 0.094% and 1.857%.
Nonetheless, our approach is likely to bene t from such an apprentice, since it would reduce the number of o topic pages retrieved and improve the overall crawling e ciency.
Integrating the apprentice in our framework is a direction we plan to pursue in future work.
Aggarwal et al. [1] proposed an online-learning strategy to learn features of pages that satis es a user-de ned predicate.
They start the search with a generic crawler.
As new pages that satisfy the user-de ned predicates are encountered, the crawler gradually constructs its focus policy.
The method of identifying relevant documents is composed by di erent predictors for content and link structure.
Manual tuning is required to determine contribution of each predictor to the  nal result.
In addition, similar to Chakrabarti et al. their strategy only learns features that give immediate bene t.
Another drawback of this approach is its use of a generic crawler at the beginning of its execution.
Because a generic crawler may need to visit a very large number of pages in order to obtain a signi cant sample, the learning costs may be prohibitive for sparse domains.
As a point of comparison, consider the behavior of the online crawler for the Movie domain (Section 4).
Even using a focused crawler, only 94 relevant forms are retrieved in a 100,000 page crawl, and these were not su cient to derive useful patterns.
A much larger number of pages would have to be crawled by a general crawler to obtain the same 94 forms.
We have presented a new adaptive focused crawling strategy for e ciently locating hidden-Web entry points.
This strategy e ectively balances the exploitation of acquired knowledge with the exploration of links with previously unknown duced in the learning process.
We have shown, through a detailed experimental evaluation, that substantial increases in harvest rates are obtained as crawlers learn from new experiences.
Since crawlers that learn from scratch are able to obtain harvest rates that are comparable to, and sometimes higher than manually con gured crawlers, this framework can greatly reduce the e ort to con gure a crawler.
In addition, by using the form classi er, ACHE produces high-quality results that are crucial for a number information integration tasks.
There are several important directions we intend to pursue in future work.
As discussed in Section 5, we would like to integrate the apprentice of [8] into the ACHE framework.
To accelerate the learning process and better handle very sparse domains, we will investigate the e ectiveness and trade-o s involved in using back-crawling during the learning iterations to increase the number of sample paths.
Finally, to further reduce the e ort of crawler con guration, we are currently exploring strategies to simplify the creation of the domain-speci c form classi ers.
In particular, the use of form clusters obtained by the online-database clustering technique described in [5] as the training set for the classi- er.
Acknowledgments.
This work is partially supported by the National Science Foundation (under grants IIS-0513692, CNS-0524096, IIS-0534628) and a University of Utah Seed Grant.
