The progress of information technology has made it possible to store and access large amounts of data.
However, since people think in different ways and use different terminologies to store information, it becomes hard to search each other s data stores.
With the advent of the Internet, which has enabled the integrated access of an ever-increasing number of such data stores, the problem becomes even more serious.
The Semantic Web aims to use semantics in the retrieval process, where the semantics is captured in ontologies or at the very least in concept hierarchies.
The task then is to  nd pairs of concepts from different meta-data schemas that have an equivalent meaning, a problem known as ontology matching.
This problem has been extensively studied in the Semantic Web and elsewhere, see [1, 2, 3, 4, 5] for recent survey papers.
However, in many realistic domains, it is impossible to give precise concept de nitions, and consequently no crisp notion of concept equivalence exists.
Below we will illustrate this in the music-domain (an important commercial domain on the Web), where musical genres are inherently imprecise.
Such imprecision is a fundamental aspect of many other domains as well.
Ontology matching must then be rede ned to  nding a concept with the closest meaning in the other schema when an equivalent one does not exist.
We then require mechanisms that are able to  nd approximate correspondences rather than exact ones.
The  rst contribution of this paper is to de ne a notion of approximate ontology matching between inherently imprecise domain concepts (section 2).
In section 3 we re ne this de nition with a weighting function to ensure that the approximation method does not simply allow any mappings, but that correct approximations are favoured over incorrect ones.
As the second main contribution of this paper, in section 3.3, we instantiate this weighting function with a Google-based scheme, and show in section 4 through experiments in the music domain that this weighting scheme has indeed the desired behaviour of increasing recall without loosing precision.
Before moving to the technical part of the paper, we  rst brie y introduce the domain of musical genres, and will argue why this is an appropriate domain for investigating techniques for approximate ontology mapping.
The variety and size of music content on the Internet (even when restricted to legal distributions) make it dif cult to  nd music of interest.
It is often cumbersome to retrieve even a known piece of music, given the large number of music providers, each with their own music schema.
Finding the right music that  ts a user s preferences is dependent on semantic integration over different music provider s schemas.
Music content providers usually classify the music they offer into classes for easy access.
These classes are organised in a hierarchy.
We refer to the classi ed entities as instances.
Usually these are artists, albums, compilations, other kinds of releases, songs, etc.
The majority of the terms used to identify a class of music entities are music genres and styles, for example Blues, Jazz,... with further distinctions such as American Blues, Electronic Jazz,....
Imprecision in Music Schemas.
Music genres and styles are intrinsically ill-de ned, see [6, 7].
There is no single authority that can decide for a music entity which genre or style it belongs to.
For example, it s becoming increasingly dif cult to categorise the newly emerging musical styles that incorporate features from multiple genres.
Also, the attempts to classify particular musicians in a single genre are sometimes ill-founded as they may produce music in a variety of genres over time or even within a single piece.
There are no objective criteria that sharply de ne music classes.
Genre is not precisely de ned.
When asked, people will classify the same music entity in different genres, with an agreement of only in the 30-40% region.
As a result, different providers often classify the same music entities (artists, albums, songs...) differently.
Widely used terms like Pop and Rock do not denote the same sets of artists at different portals, [6].
That is also the case for even more speci c styles of music like Speed Metal.
In our experiments when testing with instance data, we restricted to the artists shared by MusicMoz and Artist Direct Network, i.e.
artists that are present and classi ed in both portals.
In the sequel we refer to them as MM and ADN, respectively.
As an example, from the class named Rock (including its subclasses) in MM there are 471 shared classi ed artists, in ADN there are 245, and 196 shared artists are classi ed under Rock in both of them.
Hence, from all the artists classi ed under Rock in at least one of the two portals, only about 38% (196 out of 520) is classi ed under Rock in both portals, see  gure 1.
This example shows that there is a high degree of imprecision in the music domain.
Therefore we expect that reasoning methods that look for exact matches are not useful, and approximate methods are more appropriate.
Representation of hierarchical Music Schema.
As with any semantic concept, musical genres can in principle be de ned either intensionally (as a set of rules that de ne when an entity belongs to the genre or not), or extensionally (as the set of all music entities that belong to the genre).
With rare exceptions such as Music Genome1 and Wikipedia2, who aim to provide intensional 1http://www.pandora.com/mgp.shtml
 Figure 2: Two music genres: Although the labels are equivalent, Experimental, they represent different classes.
de nitions of musical genres, the extensional approach is the most widely used in practice.
Hence, for our purpose, we assume an extensional treatment for the genres and styles as sets of music entities.
Consequently, a music classi cation is a collection of music concepts described with English language terms, where instances are being classi ed.
It can be modelled as a concept hierarchy.
However, it is not suf cient to use only the concept labels to identify the concepts, since, their position in the schemas in uences their meaning as well.
Figure 2 illustrates this with an example from existing music schemas.
Although the labels are equivalent (namely Experimental), they represent different classes.
We adopt the approach proposed in [8] to make the meanings explicit by conjoining the concepts with their superconcepts in the hierarchy.
This then makes the meaning of the two concepts in the example explicit as Electronic   Experimental Jazz   Big Band   Experimental Data Selection.
Music metadata schemas on the Internet mostly exist in the form of a navigation path through the music offered.
A meta-data schema isn t always offered next to the music, but a visitor can interactively navigate through different pages that list the music.
We consider this structure of navigation paths together with the labelling on the links and pages as the meta-data schema of that provider.
After considering several music provision sites, we selected seven of them and extracted the schema (navigation path), as shown in  gure 3.
Also, we have extracted the music schema present in the free online encyclopaedia Wikipedia.
Ontology matching is often phrased as  nding equivalences A   B between concepts A and B from separate hierarchies.
We take a slightly more general view, namely that of  nding subsumption relations A   B between concepts from separate hierarchies (with  nding equivalences as an obvious special case of mutual subsump-tions).
The representation of concepts as a conjunction of propositional symbols described in the previous section implies that concepts have the form B = B1   .
.
.
  Bk
 sumption check
 (1)
 and consequently we replace conjunction by intersection relation, and implication by a subset relation.
# concepts max.
depth
 http://www.cdnow.com/ MusicMoz http://musicmoz.org/ Artist Direct Network http://artistdirect.com/ All Music Guide http://www.allmusic.com/ Artist Gigs http://www.artistgigs.com/ CD Baby http://www.cdbaby.com/ Yahoo LaunchCast http://launch.yahoo.com/













 Figure 3: The extracted music schemas.
can be split into a set of subproblems: (A   Bi) ^ i (2) with each subproblem checking if A is a subclass of one of the conjuncts Bi.
The original subsumption problem (1) is satis ed if and only if all the subproblems from (2) are satis ed.
This approach is independent from the choice of solver to use for solving the individual subproblems A   Bi.
This might be an in-tensional reasoner (using axioms for A and Bi)), or an extensional checker (checking the instance sets of A and Bi), or a simple lexical approach (using the textual descriptive labels of A and Bi).
In our experiments later in this paper, we will in fact use the lexical approach, but our approach to approximate ontology matching is independent of this choice.
Now we introduce the idea of approximation.
In our approximation, we allow a few of the subproblems from (2) to be unsat-is able, while still declaring the original problem (1) satis able.
The (relative) number of satis able subproblems is a measure of how strongly the subclass relation between the two given formulas hold.
If for only a few of the subproblems the relation (2) doesn t hold, we may say that the relation (1) almost holds.
We can express the strength at which the relation (1) holds as the ratio between the number of false subproblems (subproblems for which the subclass relation doesn t hold) and the total number of subproblems.
We call this ratio the sloppiness and we use the letter s to denote its value: s(A   B) = |{i : A 6  Bi}|
 k (3) Here |{i : A 6  Bi}| denotes the number of unsatis ed subprob-lems that are ignored, and k is the total number of subproblems.
We will later refer to this sloppiness value as the uniform weighting.
In the example of  gure 4, the subsumption A   B1 B2 B3 is not classically valid, because A 6  B3, but it is valid at a sloppiness level of 1/3.
Some properties that can be easily seen are: PROPERTY 1.
Only classically valid subsumptions hold at s =
 PROPERTY 2.
Any subsumption holds at s = 1.
text, and simply speak of the sloppiness value s.
Figure 4: Example of an approximate subsumption relationship: A   B with B = B1   B2   B3.
From these two properties it follows that our approximation only makes sense for conjunctive formulae B1 .
.
.
Bk with k > 1.
A trivial conjunctive formula with only one conjunct is not approx-imable: the only sloppiness values for such a formula are either 0 or 1, limiting us to either classical subsumption or trivial acceptance of the subsumption.
But such a formula would correspond to a hierarchy of depth 1, which is a case not very interesting for our problem anyway.
PROPERTY 3.
The set of subsumptions that hold at a given sloppiness value grows monotonically with increasing sloppiness value.
Properties 1-3 together tell us that at small s-values, the approximation will be too strict, and no (actually: only classically valid) mappings will be found; while at large s-values, the approximation will be too loose, and many invalid mappings (actually: all combinations) will be found.
In the next subsection, we will discuss how we can in uence the approximation level (the s-values) at which valid and invalid mappings are found.
We can in uence the sloppiness level at which a mapping is discovered with a heuristic weighting function: each subproblem A   Bi is given a weight wi, and the heuristic sloppiness value  s(A   B) is then the summed weight of all the subproblems that needed to be ignored for the subsumption to be accepted:  s(A   B) = X {i:A6 Bi} wi.
Notice that the non-heuristic s-value (the uniform weighting) is a special case of the heuristic  s-value, namely with equal values for all wi.
How should we make a heuristic weighting function, and how should we judge if one weighting heuristic is better than another?
As before, let A   B with B = B1   B2   B3 be a potential ontology match (= cross-hierarchy subsumption relation) that does not hold classically.
To judge the quality of a weighting heuristic, we must distinguish two cases: desirable matches: although A   B does not hold classically, it might nevertheless be a useful mapping.
(This might be known from intended (informal) meaning of A and B, or because we have a gold standard that contains this match).
level of sloppiness (i.e. by ignoring only a small number of well-chosen subproblems) undesirable matches: conversely, we might know from the intended meaning of A and B that indeed A   B should not be derived.
In this case, we would want A   B to be derived only at a very high level of sloppiness (i.e. only after ignoring many subproblems).
Put in other words, a heuristic weighting function should min-imise the sloppiness required to derive desirable matches, and max-imise the sloppiness required to derive undesirable matches.
This would ensure that when gradually increasing the allowed sloppiness level, we begin to discover desirable matches quickly, while only including undesirable matches very late in the process.
This would have the effect that when increasing the s-value, we have an early increase of recall, but a late decrease of precision.
It is easily seen that the uniform weighting scheme from formula (3) amounts to choosing equal values for all wi (namely 1/k), and hence to making a random choice for subproblems to ignore.
In general, however, the subterms Bi have different signi cance in capturing the meaning of B.
We would expect any reasonable heuristic to improve over a random choice, and ignoring more sig-ni cant subterms should go with a penalty, i.e. should require a higher wi for such signi cant subterms.
A requirement on an informed heuristic form choosing the weights wi is therefore: REQUIREMENT 1.
The heuristic weights wi should be chosen in such a way that:   if A   B is a desirable match, then  s(A   B) < s(A   B)   if A   B is an undesirable match, then  s(A   B) > s(A   B).
Consider again the example of  gure 4, and let A   B be a desirable match.
Suppose that we heuristically choose the following weights: w1 = 0.7, w2 = 0.2, w3 = 0.1.
Then this heuristic accepts A   B already at sloppiness level 0.1, i.e.  s(A   B) = 0.1, since only A   B3 with weight 0.1 needs to be non-classically assumed to hold (even though classically it doesn t).
The uniform weighting scheme would result in s(A   B) =
 form weighting performs less well than the example heuristic values, since the required increase in recall (i.e. accepting A   B) is only achieved at a higher sloppiness value (1/3 instead of 0.1).
If on the other hand A   B would be an undesirable match, the uniform weighting would be preferred.
Clearly, the recall and precision of the the above approximations mapping approach relies on choosing the right weights wi.
The main intuition behind the choice for a good weighting function is that the weight assigned to the subproblem A   Bi should re ect how much information the concept Bi provides about the concept B.
The level of informativeness can be observed as a  semantic closeness  between the concepts Bi and B.
Intuitively, a concept Bi that is semantically close to B should be more relevant, and have a higher weight, than a concept that is semantically more distant.
In the next section, we will consider a weighting heuristic based on such a notion of semantic distance.
The weighting scheme we consider in this section takes advantage of the vast knowledge available on the web by using a Google-based dissimilarity measure.
We utilise a dissimilarity measure, called Normalised Google Distance (NGD), introduced in [9].
NGD takes advantage of the number of hits returned by Google to compute the semantic distance between concepts.
The concepts are represented with their labels which are fed to the Google search engine as search terms.
Given two search terms x and y, the the normalised Google distance between x and y, NGD(x, y), can be obtained as follows NGD(x, y) = max{log f (x), log f (y)}   log f (x, y) log M   min{log f (x), log f (y)} (4) where f (x) is the number of Google hits for the search term x, f (y) is the number of Google hits for the search term y, f (x, y) is the number of Google hits for the tuple of search terms x y and M is the number of web pages indexed by Google5.
Intuitively, NGD(x, y) is a measure for the symmetric conditional probability of co-occurrence of the terms x and y: given a webpage containing one of the terms x or y, NGD(x, y) measures the probability of that webpage also containing the other term6.
Example: We will determine the normalised Google distance between the search terms  jazz  and  rock  that correspond to concepts Bjazz and Brock, respectively.
The number of hits for the search term  jazz  is given by f (jazz) = 196 000 000 and for the search term  rock  by f (rock) = 723 000 000.
Furthermore, Google returns f (jazz, rock) = 119 000 000 web pages in which both  jazz  and  rock  occur.
Therefore, NGD(jazz, rock) =

 For a subsumption check A   B with B = B1   .
.
.
  Bk, the Google-based weighting scheme is de ned as follows: First, we compute the normalised Google distances.
di = NGD(Bi, B) We normalise these values to the [0, 1] interval
 d i = di Pk j=1 dj (5) Subsequently, the normalised distance values are converted into similarities si = 1   d
 i Finally, from the similarity values the weights for the subproblems are derived.
wi = si Pm j=1 sj
 billion pages (M   1010).
Google is known to show non-monotonic behaviour, i.e. adding more words in the search query may increase the number of hits instead of decrease it.
Yet, such cases are exceptions and did not affect the results of our experiments
 search terms ( jazz  and  rock ) with the general scope-term  music , in order to avoid homonym problems such as  rock  in its geological sense.
The general NGD formula (4) (taken from [9]) can be slightly simpli ed because of the special form of our queries.
As said, we compute the NGD values using (4) di = NGD(Bi, B) max{log f (Bi), log f (B)}   log f (Bi, B) log M   min{log f (Bi), log f (B)} = (6) wheref (Bi), f (B) and f (Bi, B) give the number of hits for Bi, B and (Bi, B), respectively.
The Google query for Bi is comprised of search terms that are also present in the Google query for B.
Therefore, f (Bi)   f (B).
This inequality implies that max{log f (Bi), log f (B)} = log f (Bi) and min{log f (Bi), log f (B)} = log f (B).
For the same reason (namely that all terms from Bi also occur in B), the Google query for B coincides with the Google query for the tuple (Bi, B) which means f (Bi, B) = f (B).
If we consider the last few deductions we can rewrite (6) in the following manner di = NGD(Bi, B) = log f (Bi)   log f (B) log M   log f (B) or di = NGD(Bi, B) = (7) where N = log M   log f (B) and Ni = log f (Bi)   log f (B) If we consider (7) we can rewrite the expressions given with (5) Ni
 as follows: We use these formulas to test for the subsumption relation CountryADN   Bluegrass GospelMM .8 (8) As described in section 2, we have to solve the following subprob-lems: Country   Country Country   Bluegrass Country   Bluegrass Gospel In order to solve the individual subproblems, we apply the following method:   Concepts with the same label have the same meaning   If one label is derived from another by adding extra words, we assume that the  rst is a more speci c concept than the second, hence the second concept subsumes the  rst.
This is a reasonable assumption because additional words usually restrict an expression s meaning.
Given this, we obtain the following results for the subproblems Country   Country Country   Bluegrass Country   Bluegrass Gospel Using the uniform weighting scheme, subsumption (8) is acceptable at a sloppiness-level of 0.66, since 2 out of 3 subproblems are found not to hold, and must be ignored in order for the subsumption to go through : true (Country on both sides) : false : false Next, we apply the Google-based weighting scheme on the same set of subproblems.
We compute the NGD values as described in section 3.4:
 d i = Ni
 = Ni Pm j=1 Nj Nj
 Pm As we can see the value of d0 i does not depend on N. Therefore, we can use the following expression to compute the Normalised Google Distance j=1 d1 = NGD(Country, d2 = NGD(Bluegrass, Country   Bluegrass   Bluegrass Gospel) Country   Bluegrass   Bluegrass Gospel) Country   Bluegrass   Bluegrass Gospel) d3 = NGD(Bluegrass Gospel, di = mNGD(Bi, B) = log f (Bi)   log f (B) = log f (Bi) f (B) where mNGD stands for modi ed Normalised Google Distance.
The advantage of NGD over the NGD is that mNGD no longer depends on M, the size of the Google index, for which we would have to guestimate a value.
In this section we illustrate the process of approximate ontology matching with weighting.
In our examples we consider the subsumption relations between two pairs of styles from MusicMoz and ArtistDirectNetwork portals, as shown in Figures 5 and 6.
Example 1: Country and Bluegrass gospel.
The  rst step is to transform the concepts into formulas.
The individual concepts are represented with their labels, and conjoined with the representation of their parents, as discussed in section 1.2.
This yields the following formulas representing the meaning of Country from ADN and Bluegrass Gospel from MM: CountryADN = Country Bluegrass GospelMM = Country   Bluegrass   Bluegrass Gospel After providing each of the terms with the scope-term  music , the Google queries  Country  music,  Bluegrass  music,  Bluegrass Gospel  music and  Country   Bluegrass   Bluegrass Gospel  music return 467 000 000, 24 000 000, 338 000 and 261 000 hits, respectively.
Consequently, we  nd the following mNGD distances: d1 = 7.44147 d2 = 4.39942 d3 = 0.164622.
Using these values we derive the weights for the subproblems, as described in section 3.4: w1 = 0.190081 w2 = 0.336629 w3 = 0.493144 Since the last two of the three subproblems must be ignored for the match to go through, the heuristic  s-value is 0.81.
Hence, in this example,  s-value > s-value, which, since the match is actually undesirable, is an improvement (of 15 %-points) over the uniform weighting, in line with the requirement stated in section 3.1.
and hence constitutes an undesirable mapping.
Goal of the experiment: In our experiment, we want to measure both if approximation improves over simple classical matching ( rst goal), and if approximation with heuristic weighting improves over uniform weighting (second goal).
Data acquisition: We used genre hierarchies that we extracted from the meta-data schemas underlying the classi cations of the Artist Direct Network (ADN), MusicMoz (MM) and Wikipedia music portals, as discussed in section 1.2.10 One of the concept was always from MM, and the other concept in 30% of the cases from ADN and 70% of the cases from Wikipedia Construction of a Gold Standard: We randomly selected 50 pairs of concepts, and manually assessed whether a mapping (= a cross-hierarchy subsumption relation) between the concepts within each selected pair holds or not.
In other words, we classi ed each of the 50 pairs as either a desirable or an undesirable matching relation, as de ned in section 3.2.
This yielded 9 desirable mappings and 41 undesirable mappings.11 Evaluation criteria: For measuring the  rst goal (improvement of approximate matching over classical matching), we use standard recall and precision measures: heuristic matching is better than classical matching if recall improves (more desirable matches are found), while precision does not decrease much (not many undesirable matches are found).
For measuring the second goal (improvement of heuristic weighting over uniform weighting), we use Requirement 1 from section 3.2.
Heuristic weighting is better than uniform weighting if desirable matches are found with a lower heuristic  s-value then the uniform s-value, and conversely for undesirable matches.
Choice of subproblem solver: Remember that we regard ontology matches as cross-hierarchy subsumptions, and that our approximation method reduces subsumption-queries A   B to a set of subproblems A   Bi.
Of course, each of these atomic subprob-lems still needs to be solved.
As in section 3.6, we apply a simple lexical method that establishes whether A is (strictly) subsumed by Bi by checking whether the set of words in label of A is (properly) contained in the set of words of the label of Bi (since additional words usually restrict an expression s meaning).
exact matching When comparing approximate matching with exact matching, we cannot simply compare two scores.
After all, the behaviour of approximate matching is a function of the sloppiness-level s.
Figure 7 shows a recall-precision plot of uniformly weighted approximate matching.
Since exact matching corresponds to approximate matching at s = 0, the graph also shows the precision and recall of exact matching: 22% recall at 100% precision.
This score is to be expected of the simple lexical matcher that we employ to solve subproblems: lexical matching will fail to  nd many semantically desirable matches (hence the low recall of 22%), but when it  nds a match, it is indeed a correct one (precision of 100%).
As expected of approximate matching, increasing values of s increase recall and decrease precision.
(Figure 7 quantitatively shows what was already qualitatively predicted by properties 1 and 2 from section 2).
Concerning the comparison with exact matching,  gure 7 shows that a slight increase in s-levels is indeed useful (a jump in recall
 tained enough additional information for us to construct a Gold Standard.
cross-hierarchy pairs, the large majority are undesirable matches.
Figure 5: Matching concepts Country and Bluegrass gospel Figure 6: Matching concepts Black metal and Heavy metal Example 2: Black metal and Heavy metal.
The previous example showed the behaviour of the weighting on an undesired match.
In this example we examine the weighting behaviour on a desired match.
Figure 6 shows the following matching problem: Black metalADN   Heavy metalMM .9 (9) The separate subproblems in this case are: Rock   Heavy metal   Black metal   Rock Rock   Heavy metal   Black metal   Hard Rock   Heavy metal   Black metal   Black metal The  rst and the third subsumptions are correct established lexi-cally, but the second is found false.
As a consequence the uniform weighting scheme would need a sloppiness-level of s = 0.33 to establish (9).
Using NGD we obtain the following weight values for each of the subproblems: w1 = 0.25462 w2 = 0.25588 w3 = 0.48950.
This allows (9) to be established at a heuristic sloppiness-level of  s = 0.25588.
This is 8 %-points lower than the required s-value, and hence an improvement, since (9) is a desired mapping.
To validate our approach of improving approximation by weighting, we conducted experiments using data from the music domain.
In this section, we describe the experiments and summarise and discuss the results.
Desirable mappings Total Total


 Better


 Equal Worse





 Figure 9: NGD weighting compared to uniform weighting.
 Better  and  Worse  mean NGD weighting is better (worse) than uniform weighting.
that after the  s = 0.5-level, the precision starts to drop very sharply (to 83% at  s = 0.75 and even to 51% at  s = 0.79).
This suggests that on other data-sets, the near perfect score at  s = 0.53 may not be repeated.
Further experiments on other data-sets are required to verify this.
An overall summary of the results of our experiments is given in  gure 9.
Informed weighting yielded an improvement over uninformed weighting in almost all cases.
The main gain is in a better performance on undesirable matches (i.e. improving precision).
The most detailed analysis of our experimental data is shown in  gures 10 and 11.
There we plot for all 41 undesirable mappings ( gure 10) and for all 9 desirable mappings ( gure 11) the minimal s and  s-values at which these mappings are found.
Figure 10 shows that for almost all undesirable mappings, the informed  s-values are higher than the s-value, meaning that the informed weighting scheme is more resistant to accepting such undesirable mappings (explaining its dramatically better precision scores in the preceding recall/precision plots).
Figure 11 shows that for the desirable mappings, both weighting schemes behave roughly equally well.
Taken together, we can conclude that the main gain of the informed weighting scheme is that it manages to avoid a drop in precision while maintaining the high recall-levels of the uninformed scheme.
Figure 10 also shows that the lion-share of the gains by the informed weighting are made at the s = 0.5 level, where the uninformed weighting accepts an incorrect mapping.
These are often cases with labels of length 2 (such as  hard rock     gospel rock ).
In this example,  gospel  is the most informative term, but since the uninformed weighting randomly chooses a subproblem to ignore, it may choose to ignore  gospel , ending up with  hard rock     rock  as the only subproblem, which does indeed hold.
The informed Google weighting would instead realise that  gospel  is the most informative term, hence choose to ignore  rock  instead, ending up with  hard rock     gospel  as the remaining subproblem, and correctly refusing to accept this mapping.
Note that this is not simply a matter of a preferring adjectives over nouns, consider for example the case  Country Gospel     Christian Gospel , which is an intended mapping, and can only be established by preferring the noun  Gospel  over the adjective  Christian .
A truly semantically informed weighting scheme is required, no simple lexical  x will do.
Although we claim that our approach is entirely novel (both the idea of approximating ontology mappings by ignoring subprob-lems, and the idea of using the Google distance as a weighting heuristic for this problem), the different components of our approach can be found elsewhere.
Existing ontology mapping methods typically use one or a combination of the following approaches: terminological: use the labels of the entities to derive matches be-Figure 7: Recall/precision graph of approximate matching with uniform weighting at increasing sloppiness-levels.
Exact matching happens at s=0.
Figure 8: Recall/precision graph of approximate matching with Google weighting at increasing sloppiness-levels.
from 22% to 33% at a negligible loss of precision when increase s from 0.2 to 0.33).
The graph also shows that there is no point in increasing the uniform s value beyond 0.5, since at that point full recall is achieved.
However, this has happened at the cost of a signi cant loss in precision (down to 63% at s = 0.5).
Overall,  gure 7 tells us that approximate matching at small s-values does indeed usefully improve over exact matching, even when randomly choosing the subproblems to be approximated (= the uniform weighting).
In the next section, we will evaluate to what extend an informed weighting heuristic can improve over this.
The behaviour of the Google-weighted approximate matching is shown in the recall-precision plot of  gure 8.
This is near perfect recall-precision plot, which shows that we can increase the  s-values (and hence increase the recall) until we have achieved perfect recall, all at the cost of only a negligible loss in precision of 2%, at  s = 0.53.
The precision only starts to drop signi cantly after a perfect recall has been achieved.
This plot compares very favourably with that of the uniform weighting scheme in  gure 7, where a perfect recall could only be achieved at a disappointing 63%.
Interestingly enough, both the uniform and the Google weighting achieve 100% recall around the  s = s = 0.5 level (i.e. ignoring about half the subproblems), but apparently the Google weighting is very successful at maintaining exactly those subproblems which prevent a drop in precision (while the uniform weighting simply ignores random subproblems).
One observation which questions the stability of our results is

 a very simple version of these methods.
structural: use the structure of the ontology, i.e. the relatedness among entities within the matching ontologies, see [14, 10,
 can be seen as a simple way of using logical structure of the ontology.
instance-based: use the overlap or relatedness between classi ed instances, see [15].
background knowledge: use external resources such as thesaurus, dictionaries or more complicated ontologies to perform the task, see [16, 17, 18, 19].
As mentioned in section 2, any of the above approaches to ontology matching could be deployed in solving our A   Bi subproblems.
Our Google-based weighting method resembles recent work in the  eld of Knowledge Acquisition which exploits the Web as a source for discovering new knowledge, for example to discover relations between concepts in various ways: using the number of hits when querying a search engine like Google, by analysing co-occurrence of concepts within large text corpora, or by exploiting patterns to construct queries to check for a relation (see [20, 21, 22, 9] for recent work).
Other attempts exist to deal with vaguely de ned domain concepts, most notably by using fuzzy-logic representations (e.g.
[23]), and more recently by using rough sets [24].
An inherent price of these approaches is that they require signi cant extra modelling effort to capture the imprecision of the concepts, either in terms of fuzzy membership functions or in terms of rough-set upper and lower-bounds.
The advantage of our approximation approach is that it applies directly to semantically lightweight hierarchies typically found in imprecise domains without any further need for complicated and expensive domain modelling.
Finally, we point out that the  con dence-factor  which is part of the format used by the Ontology Alignment Evaluation Initia-tive12 only expresses the degree of con dence in the truth of a crisp matching relation, and does represent an approximation of such a matching relation.
In this study, we have addressed the problem of discovering approximate matching relations between concepts from different concept hierarchies.
Such approximate matching relations are required in many domains where concepts are ill-de ned, and any attempt to  nd precise equivalences (or even precise subsumptions) will fail because of the imprecise nature of the concepts.
Our method is directly applicable to the lightweight hierarchies found in practice in imprecise domains.
We have given a declarative de nition for approximate ontology-mapping with a variable degree of approximation.
We have shown how this approximation degree (for which we used the term sloppiness level) can be in uenced by a semantic similarity measure that we derive from the Normalised Google Distance.
In order to validate our theoretical proposal, we harvested from the Web realistic concept hierarchies that represented musical genres, we constructed a small Gold Standard corpus of mapping candidates, and we performed experiments to test the precision and recall of our approximation method.
Our results show that the Google-based semantic measure signi cantly outperforms an uninformed measure.
12http://oaei.ontologymatching.org/ Figure 10: Google-based versus uniform weighting on undesirable matches Figure 11: Google-based versus uniform weighting on desirable matches
 establish the submappings that together build up an approximate mapping.
In the music domain, we have used simple lexical techniques to establish these submappings, but this can be replaced with more complicated mapping techniques, while the essential idea of our sloppiness value and the weighting function can still be applied unchanged.
In very general terms, our method makes use of the huge amount of knowledge that is implicit in the current Web, and exploits this knowledge as a heuristic for establishing approximate mappings between ill-de ned concepts.
