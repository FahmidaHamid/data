The World Wide Web has had a tremendous impact on society and business in recent years by making information instantly and ubiquitously available.
The Semantic Web is seen as an extension of the WWW, a vision of a future Web of machine-understandable documents and data.
One its main instruments are the annotations, which enrich content with metadata in order to ease its automatic processing.
The traditional paradigm of Semantic Web annotation  Part of this work was performed while the author was visiting Yahoo!
Research, Barcelona, Spain.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
(i.e., annotating Web sites with the help of external tools) has been established for a number of years by now, for example in the form of applications such as OntoMat [20] or tools based on Annotea [23], and the process continues to develop and improve.
However, this paradigm is based on manual or semiautomatic annotation, which is a laborious, time consuming task, requiring a lot of expert know-how, and thus only applicable to small-scale or Intranet collections.
For the overall Web though, the growth of a Semantic Web overlay is restricted because of the lack of annotated Web pages.
In the same time, the tagging paradigm, which has its roots in social bookmarking and folksonomies, is becoming more and more popular.
A tag is a relevant keyword associated with or assigned to a piece of information (e.g., a Web page), describing the item and enabling keyword-based classi cation of the information it is applied to.
The successful application of the tagging paradigm can be seen as evidence that a lowercase semantic Web1 could be easier to grasp for the millions of Web users and hence easier to introduce, exploit and bene t from.
One can then build upon this lowercase semantic web as a basis for the introduction of more semantics, thus advancing further towards the Web 2.0 ideas.
We argue that a successful and easy achievable approach is to automatically generate annotation tags for Web pages in a scalable fashion.
We use tags in their general sense, i.e., as a mechanism to indicate what a particular document is about [4], rather than for example to organize one s tasks (e.g.,  todo ).
Yet automatically generated tags have the drawback of presenting only a generic view, which does not necessary re ect personal interests.
For example, a person might categorize the home page of Anthony Jameson2 with the tags  human computer interaction  and  mobile computing , because this re ects her research interests, while another would annotate it with the project names  Halo 2  and  MeMo , because she is more interested in research applications.
The crucial question is then how to automatically tag Web pages In many environments, de ning a user s in a personalized way.
viewpoint would rely on the de nition of an interest pro le.
However, these pro les are laborious to create and need constant maintenance in order to re ect the changing interest of the user.
Fortunately, we do have a rich source of user pro ling information available: everything stored on her computer.
This personal Desktop usually contains a very rich document corpus of personal information which can and should be exploited for user personalization!
There is no need to maintain a dedicated interest pro le, since the
 the Semantic Web by adding simple meaning gradually into the documents and thus lowering the barriers for reusing information.
2http://www.dfki.de/ jameson while it also tracks her history.
Based on this observation, we propose a novel approach for a scalable automatic generation of annotation tags for Web pages, personalized on each user s Desktop.
We achieve this by aligning keyword candidates for a given Web page with keywords representing the personal Desktop documents and thus the subject s / author s personal interest, utilizing appropriate algorithms.
The resulting personalized annotations can be added on the  y to any Web page browsed by the user.
The structure of the paper is as follows: In Section 2 we discuss previous and related work.
Section 3 describes the core algorithmic approaches for personalized annotation of Web pages by exploiting Desktop documents.
In Section 4 we present the setup and empirical results of our evaluation.
Finally, prior to concluding, we brie y discuss possible applications of our approach in Section 5.
This paper presents a novel approach to generate personalized annotation tags for Web pages by exploiting document similarity and keyword extraction algorithms.
Though some blueprints do exist, to our knowledge there has been no prior explicit formulation of this approach, nor a concrete application or empirical evaluation, as presented in this paper.
Nevertheless, a substantial amount of related work already exists concerning the general goal of creating annotations for Web pages, as well as keyword extraction.
The following sections will discuss some of the most important works in the research areas of annotation, text mining for keyword extraction, and keyword association.
Brooks and Montanez [4] analyzed the effectiveness of tags for classifying blog entries and found that manual tags are less effective content descriptors than automated ones.
We see this as a support for our work, especially since our evaluation from Section 4 proves that the tags we create do result in high precision for content description.
They further showed that clustering algorithms can be used to construct a topical hierarchy amongst tags.
We believe this  nding could be a useful extension to our approach.
Cimiano et.
al. [10] proposed PANKOW (Pattern-based Annotation through Knowledge on the Web), a method which employs an unsupervised, pattern-oriented approach to categorize an instance with respect to a given ontology.
Similar to P-TAG, the system is rather simple, effortless and intuitive to use for Web page annotation.
However, it requires an input ontology and outputs instances of the ontological concepts, whereas we simply annotate Web pages with user speci c tags.
Also, PANKOW exploits the Web by means of a generic statistical analysis, and thus their annotations re ect more common knowledge without considering context or personal preferences, whereas we create personalized tags.
Finally, the major drawback of PANKOW is that it does not scale, since it produces an extremely large number of queries against one target Web search engine in order to collect its necessary data.
The work in [11] presents an enhanced version of PANKOW, namely C-PANKOW.
The application downloads document abstracts and processes them offline, thus overcoming several shortcomings of PANKOW.
It also introduces the notion of context, based on the similarity between the document to be annotated and each of the downloaded abstracts.
However, it is reported that annotating one page can take up to 20 minutes with C-PANKOW.
Our system annotates Web pages on the  y in seconds.
Note that the tasks are not entirely comparable though, since our system does not produce ontology-based annotations, but personalized annotation tags.
Further, our notion of context is much stronger, since we consider documents from the personal Desktop, which leads to highly personalized annotations.
Finally, C-PANKOW uses the proper nouns of each Web page for annotation candidates, and thus annotation is always directly rooted on the text of the Web page.
On the other hand, the algorithms we propose in this paper generate keywords that not necessarily appear literally on the Web page, but are in its context, while also re ecting the personal interests of the user.
Dill et.
al. [14] present a platform for large-scale text analytics and automatic semantic tagging.
The system spots known terms in a Web page and relates it to existing instances of a given ontology.
The strength of the system is in the taxonomy based disambiguation algorithm.
In contrast, our system does not rely on such a handcrafted lexicon and extracts new keywords in a fully automatic fashion, while also supporting personalized annotations.
Text data mining is one of the main technologies for discovering new facts and trends about the currently existing large text collections [21].
There exist quite a diverse number of approaches for extracting keywords from textual documents.
In this section we review some of those techniques originating from the Semantic Web, Information Retrieval and Natural Language Processing environments, as they are closest to the algorithms described in this paper.
In Information Retrieval, most of these techniques were used for Relevance Feedback [29], a process in which the user query submitted to a search engine is expanded with additional keywords extracted from a set of relevant documents [32].
Some comprehensive comparisons and literature reviews of this area can be found in [17, 30].
Efthimiadis [17] for example proposed several simple methods to extract keywords based on term frequency, document frequency, etc.
We used some of these as inspiration for our Desktop speci c annotation process.
Chang and Hsu [7]  rst applied a clustering algorithm over the input collection of documents, and then attempted to extract keywords as cluster digests.
We moved this one step further, by investigating the possibilities to acquire such keywords using Latent Semantic Analysis [13], which results in more qualitative clusterings over textual collections.
However, this turned out to require too many computational resources for the already large Desktop data sets.
The more precise the extracted information is, the closer we move to applying NLP algorithms.
Lam and Jones [26] for example used summarization techniques to extract informative sentences from documents.
Within the Semantic Web / Information Extraction area, we distinguish the advances achieved within the GATE system [12, 27], which allows not only for NLP based entity recognition, but also for identifying relations between such entities.
Its functionalities are exploited by quite several semantic annotation systems, either generally focused on extracting semantics from Web pages (as for example in KIM [25]), or more guided by a speci c purpose underlying ontology (as in Artequakt [1]).
While not directly related to the actual generation of semantic entities, keyword association is useful for enriching already discovered annotations, for example with additional terms that describe them in more detail.
Two generic techniques have been found useful for this purpose.
First, such terms could be identi ed utilizing co-occurrence statistics over the entire document collection to annotate [24].
In fact, as this approach has been shown to yield good results, many subsequent metrics have been developed to best assess  term relationship  levels, either by narrowing the analysis for clusters [31], etc.
We have also investigated three of these techniques in order to identify new keywords related to a set of terms that have been already extracted from the Web page which requires annotation.
Second, more limited, yet also much more precise term relationships can be obtained from manually created ontologies [6], or thesauri, such as WordNet [28].
We distinguish three broad approaches to generate personalized Web page annotations, based on the way user pro les (i.e., Desktops in our case) are exploited to achieve personalization: (1) a document oriented approach, (2) a keyword oriented approach, and (3) a hybrid approach.
The general idea of the document oriented approach is as follows.
For a given Web page, the system retrieves similar documents from the personal Desktop.
This set of related personal documents re ects user s viewpoint regarding the content of the Web page.
Hence, the set will be used to extract the relevant annotation tags for the input Web page.
The generic technique is also depicted in the algorithm below.
Algorithm 3.1.1.
Document oriented Web annotations generation.
Find similar Desktop documents DSi, i   {1, .
.
.
, nd}




 Extract its relevant keywords.
We will now detail the speci c algorithms for accomplishing steps 2 and 4 in the following two subsections.
We consider two approaches for this task, namely Cosine Similarity (i.e., nearest neighbor based search), and Latent Semantic Analysis (i.e., clustering based search).
In both cases, we apply the algorithms only to those terms with a Desktop document frequency above 10 and below 20% from all Desktop documents of the current user.
This is necessary in order to avoid noisy results (i.e., documents with many words in common with an input page p, yet most of these being either very general terms, or terms not describing the interests of the surfer).
Moreover, such optimizations signi cantly improve computation time.
Cosine Similarity.
Nearest neighbor search, based on TFxIDF (Term Frequency multiplied by Inverse Document Frequency), outputs a similarity score between two documents utilizing the following simple formula: Sim(Di, Dj) = v(Di)   v(Dj) = Xt Di Dj wi,t   wj,t (1) where v(Di), v(Dj) are the term vectors describing documents Di, Dj using TFxIDF, as within the Vector Space Model [3], and wk,t represents the actual TFxIDF weight associated to term t within document Dk.
Latent Semantic Analysis.
Similar to clustering, LSA can compute document to document similarity scores.
However, we decided to abandon experimenting with this algorithm, as it turned out to be too computationally expensive for regular Desktops consisting of several dozens of thousands of indexable items.
Nevertheless, since the quality of its results might be high, we are currently investigating several techniques to optimize it or to approximate its results in order to enable evaluating it in a further work.
Introduction.
Keyword extraction algorithms usually take a text document as input and then return a list of keywords, which have been identi ed by exploiting various text mining approaches.
To ease further processing, each keyword has associated a value representing the con dence with which it is thought to be relevant for the input document.
Once such keywords have been identi ed for the input set of relevant Desktop documents (i.e., as determined using the previously described techniques), we propose to generate annotations for the original input Web page by sorting all these generated terms utilizing their con dence levels, and then taking the Top-K of them.
We investigated three broad approaches to keyword extraction with increasing levels of granularities, denoted as  Term and Document Frequency  (keyword oriented),  Lexical Compounds  (expression oriented) and  Sentence Selection  (summary oriented).
Term and Document Frequency.
As the simplest possible measures, TF and DF have the advantage of being very fast to compute.
Moreover, previous experiments showed them to yield very good keyword identi cation results [5, 17].
We thus associate a score with each term, based on the two statistics (independently).
The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term  rst appears more towards the end of the document.
This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [5].
The complete TF based keyword extraction formula is as follows: T ermScore =  1
 +   nrW ords   pos nrW ords


 (2) where nrW ords is the total number of terms in the document, pos is the position of the  rst appearance of the term, and T F is the frequency of each term in the considered Desktop document.
The identi cation of suitable expansion terms is even simpler when using DF: The terms from the set of Top-K relevant Desktop documents are ordered according to their DF scores.
Any ties are resolved using the above mentioned TF scores [17].
Note that a hybrid TFxIDF approach is not necessarily ef cient, since one Desktop term might have a high DF on the Desktop, yet it may occur rarely on the Web.
For example, the term  RDF  could occur frequently accross the Desktop of a Semantic Web scientist, thus achieving a low score with TFxIDF.
However, as it is encountered more rarely on the Web, it would make a better annotation than some generic keyword.
Lexical Compounds.
Anick and Tipirneni [2] de ned the lexical dispersion hypothesis, according to which an expression s lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.
Although there exist quite several possible compound expressions, it has been shown that simple approaches based on noun analysis produce results almost as good as highly complex part-of-speech pattern iden-ti cation algorithms.
We thus inspect the selected Desktop documents (i.e., at step 1 of the generic algorithm) for all their lexical compounds of the following form: { adjective?
noun+ } line, at Desktop indexing time, for all the documents in the local repository.
Moreover, once identi ed, they could be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.
Sentence Selection.
This technique builds upon sentence oriented document summarization: Having as input the set of Desktop documents highly similar to the input Web page, a summary containing their most important sentences is generated as output.
Thus, sentence selection is the most comprehensive of these approaches, as it produces the most detailed annotations (i.e., sentences).
Its downside is that, unlike the  rst two algorithms, its output cannot be stored ef ciently, and thus it cannot be precomputed offline.
We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [26]: SentenceScore =




 ] The  rst term is the ratio between the square amount of signi cant words within the sentence and the total number of words therein.
A word is signi cant in a document if its frequency is above a threshold as follows: T F > ms = 8< :




 , if N S < 25 , if N S   [25, 40] , if N S > 40 with N S being the total number of sentences in the document (see [26] for more details).
The second term is a position score.
We set it to 1/N S for the  rst ten sentences, and to 0 otherwise.
This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.
However, it is known that longer documents usually do include overall descriptive sentences in the beginning [16], and these sentences are thus more likely to be relevant.
The  nal term is an optional parameter which is not used for this method, but only for the hybrid extraction approaches (see Section 3.3).
It biases the summary towards some given set of terms, which in our case will correspond to the terms extracted from the input Web page.
More speci cally, it represents the ratio between the square number of set terms present in the sentence and the total number of terms from the set.
It is based on the belief that the more terms in the set are contained in a sentence, the more likely will that sentence convey information highly related to the set, and consequently to the Web page to annotate.
In the keyword oriented approach we start from extracting some keywords from the Web page to annotate.
This set of keywords re ects a generic viewpoint on the document.
In order to personalize this viewpoint, we then align these keywords with related terms from the personal Desktop, as in the algorithm presented below.
Algorithm 3.2.1.
Keyword oriented Web annotations generation.
Extract relevant keywords from p




 Find related keywords on the local Desktop.
Step 1 can be accomplished using the algorithms from Section
 we propose for the keyword alignment process, i.e., the  nding of similar keywords on the Desktop corpus, namely (1) term co-occurrence statistics and (2) thesaurus based extraction.
Term Co-occurrence Statistics.
For each term, one could easily compute offline those terms co-occurring with it most frequently in a collection, such as user s Desktop, and then exploit this information at run-time in order to infer keywords highly correlated with the content of a given Web page.
Our generic Desktop level co-occurrence based keyword similarity search algorithm is as follows: Algorithm 3.2.2.
Co-occurrence based keyword similarity search.
Offline computation:



 Compute SCki,kj , the similarity coef cient of (ki, kj) For each keyword kj Online computation:


 the set of keywords extracted from the input Web page.
S   S   T SC(k), where T SC(k) contains the Top-K terms most similar to k
 5a: 5b:
 Let Score(t)   Qk E(0.01 + SCt,k) Let Score(t)   #Hits(E|t) The offline computation needs an initial trimming phase (step 1) for optimization purposes.
Also, once the co-occurrence levels are in place and the terms most correlated with the keywords extracted from the input Web page have been identi ed, one more operation is necessary, namely calculating the correlation of each proposed term with the entire set of extracted keywords (i.e., with E), rather than with each keyword individually.
Two approaches are possible: (1) using a product of the correlation between the term and all keywords in E (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire set of extracted keywords (step 5b).
Small scale tuning experiments performed before the actual empirical analysis indicated the latter approach to yield a slightly better outcome.
Finally, we considered the following Similarity Coef cients [24]:   Cosine Similarity, de ned as:   Mutual Information, de ned as:
 M I = log DFx,y pDFx   DFy N   DFx,y DFx   DFy (3) (4)   Likelihood Ratio, de ned below.
DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.
Dunning s Likelihood Ratio   [15] is a co-occurrence based metric similar to  2.
It starts from attempting to reject the null hypothesis, according to which two terms A and B would appear in P (A B) = P (A), where P (A B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.
Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.
We thus compare the two binomial processes by using likelihood ratios of their associated hypotheses, as follows:   = max 0 H( ; k) max  H( ; k) (5) where   is a point in the parameter space  ,  0 is the particular hypothesis being tested, and k is a point in the space of observations K. More details can be found in [15].
Thesaurus Based Extraction.
Large scale thesauri encapsulate global knowledge about term relationships.
Thus, after having extracted the set of keywords describing the input Web page (i.e., as with Step 1 of Algorithm 3.2.1), we generate an additional set containing all their related terms, as identi ed using an external thesauri.
Then, to achieve personalization, we trim this latter set by keeping only those terms that are frequently co-occurring over user s Desktop with the initially identi ed keywords.
The algorithm is as follows: Algorithm 3.2.3.
Thesaurus based keyword similarity search.
Select the following sets of related terms using WordNet:

 2a: 2b: 2c:

 Search the Desktop with (P|t), i.e.,
 Syn: All Synonyms Sub: All sub-concepts residing one level below k Super: All super-concepts residing one level above k For each term t of Si: the original set, as expanded with t Let H be the number of hits of the above search (i.e., the co-occurrence level of t with P ) Return Top-K terms as ordered by their H values.
We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).
As they represent quite different types of association, we investigated them separately.
Further, we limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid any noisy annotations, with T = DocsPerTopic , MinDocs).
We set DocsP erT opic = 2, 500, and min( M inDocs = 5, the latter one coping with small input Desktops.
The hybrid approach mixes the document oriented approach with the keyword oriented one.
The system  rst extracts some relevant keywords from an input Web page.
This set of keywords is the same as in Step 1 of Algorithm 3.2.1 and re ects a generic viewpoint on the document.
We personalize this viewpoint by retrieving the Desktop documents most relevant to it (i.e., using Lucene Desktop search) and then by extracting relevant keywords from them.
The generic algorithm is as follows: Algorithm 3.3.1.
Hybrid generation of Web annotations.
Extract the relevant keywords Pi from p






 Find the most relevant Desktop documents Di,j.
For each document Di,j: Extract its relevant keywords.
Steps 2 and 6 utilize the keyword extraction techniques described in Section 3.1.2.
Step 4 represents a regular Desktop search, in which the results are ordered by their relevance to the query, i.e., to each keyword Pi.
System Setup.
We have asked 16 users (PhD and Post-Doc students in various areas of computer science and education) to support us with the experiments.
Our Lucene3 based system was running on each personal Desktop.
Lucene suited our interests best, given its rapid search algorithms, its  exibility and adaptivity, and last but not least its cross-platform portability.
Thus, each user s corpus of personal documents has been indexed for later faster retrieval.
More speci cally, we indexed all Emails, Web Cache documents, and all Files within user selected paths.
The Web pages to be annotated have been selected randomly from each user s cache.
To ensure that the user did not arti cially collect a lot of data on the topic of the selected pages after having browsed them, we only picked pages browsed within the last week.
For the annotation, we chose two input pages per each category: small (below 4 KB), medium (between 4 KB and 32 KB), and large (more than 32 KB) [18].
In total, 96 pages were used as input over the entire experiment, and over 2,000 resulted annotations were graded.
Algorithms.
We applied our three approaches for keyword extraction and annotation to the input pages, i.e., Document oriented, Keyword oriented and Hybrid.
In all cases, in order to optimize the run-time computation speed, we chose to limit the number of output keywords extracted per Desktop document to the maximum number of annotations desired (i.e., four).
We applied the Document oriented approach with the following algorithms:


 Second, we investigated the automatic annotation by applying the Keyword oriented approach.
For step one of our generic approach (see Algorithm 3.2.1) we used the keyword extraction algorithms TF, DF, LC and SS.
For step two we investigated the following algorithms for  nding similar keywords:
 using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coef cients;
 with synonyms, sub-concepts, and super-concepts.
All in all, this gives us 24 possible combinations, i.e., TF+TC[CS], TF+TC[MI], .
.
.
, SS+WN[Sub], and SS+WN[Sup].
Finally, we conducted the study with the Hybrid approach.
We used the four keyword extraction algorithms for both the input Web 3http://lucene.apache.org






















 Algorithm P@1






















 Table 1: P@1-4 for document oriented extraction for small Web pages.
Table 2: P@1-4 for document oriented extraction for medium Web pages.
page and the identi ed Desktop documents.
These gave 16 combinations, i.e., TF+TF (applying TF for the Web page and TF for the Desktop documents), TF+DF, .
.
.
, SS+LC and SS+SS.
Rating the Personal Annotations.
For each input page, the annotations produced by all algorithms were shuf ed and blinded such that the user was not aware of either the algorithm which produced them, or of their ranking within the list of generated results.
Each proposed keyword was rated 0 (not relevant) or 1 (relevant).
It did not matter how much relevant each proposed annotation was, as it would have been a valid result anyway (i.e., if it has at least a reasonable degree of relevance to the input Web page, then it can be returned as annotation).
It can be easily shown that those annotations closer to the actual content of the input Web document are generated more often.
We measured the quality of the produced annotations using Precision, a standard Information Retrieval evaluation measure.
As our algorithms also generated a con dence score for each proposed annotation, we were able to order these suggestions and calculate the precision at different levels, namely P@1, P@2, P@3 and P@4.
The precision at level K (P@K) is the precision score when only considering the Top-K output4.
It represents the amount of relevant annotations proposed within the Top-K suggestions divided by K, i.e., the total number of annotations considered.
Four different levels of K were analyzed, as it was not clear which is the best amount of annotations to generate using our approaches.
First, the P@K scores were computed for each user, algorithm, and Web page in particular.
We averaged these values over the Web pages of the same type, obtaining user s opinion on the tuple <algorithm, type of Web page>.
We further averaged over all subjects, and the resulting values are listed in the tables to come.
When doing the experiments, the users were speci cally asked to rate the generated annotations taking into account both the target Web page and their personal interests.
The idea was to generate for each subject not only generic keywords extracted from the current Web page, but others that connect it to her Desktop, and therefore her interests.
Hence, the experiment is designed to evaluate our two main research questions: First, how well can automatic and personal annotation of Web pages be conducted with our approach?
Second, which algorithm is producing the best results?
We will split our discussion of the experimental results in two parts,  rst analyzing the output of each approach in particular, and then comparing them to  nd the best method for generating personalized annotations.
We compare our algorithms according to their P@3 scores (i.e., the average value over the ratings given to the Top-3 annotations, as ordered using their con dence levels), as most of them did not always produce more than 3 annotations above the strict minimal con dence thresholds we set.
minimal thresholds for the con dence scores of each algorithm.
Whenever they were not reached, the algorithm simply generated K  < K annotations for that input page.
Algorithm P@1






















 Table 3: P@1-4 for document oriented extraction for large Web pages.
Document Oriented Extraction.
The investigation of our document based approaches provided us with an interesting insight: The Term Frequency metric performs best overall, thus generating better annotations than more informed techniques such as Lexical Compounds or Sentence Selection.
After looking at each result in particular, we found that TF produces constantly good or very good output, whereas LC for example was much more unstable, yielding for some Web pages excellent annotations, but for some others rather poor ones.
Nevertheless, both LC and SS had very good ratings, even the best ones for those subjects with dense Desktops.
Thus, for future work one might consider designing an adaptive personalized annotation framework, which automatically selects the best algorithm as a function of different parameters, such as the amount of resources indexed within the personal collection.
DF was only mediocre (except for the case of small input Web pages), which is explainable, as its extractions are based on generic entire Desktop statistics, thus diverging too much from the context of the annotated Web page.
Averaging over all types of input  les, the best precisions were 0.79 when considering only the  rst output annotation, 0.81 when the top two results were analyzed, 0.79 for the top three, and 0.77 for all four.
As LC yielded the  rst value, and TF the latter three ones, we conclude that LC is the method of choice if only one annotation is desired.
However, its quality degrades fast, and TF should be used when looking for more results.
All document oriented results are summarized in Tables 1, 2, and 3, for the small, medium, and large pages respectively.
Keyword Oriented Extraction.
We will split the analysis of the keyword based algorithms based on their two steps.
For extracting keywords from the single Web page used as input, all methods perform rather similarly for small pages, with TF performing slightly better.
Again, we notice that the small input is the easiest one for our algorithms.
More interesting, Document Frequency yields the best basis for keyword similarity search when used with medium and large input pages.
It thus selects best those terms from the input Web page which are most representative to the user.
This is correct, as it is the only algorithm that relates the extraction also to the Desktop content by using DF values from user s personal collection.
In the case of keyword similarity search, the results are very clear.
WordNet based techniques seem to perform rather poorly, indicating external thesauri are not a good choice for annotations, as they cover a too general content.
Note that one might obtain a better outcome when targeting this scenario to application speci c thesauri and input Web pages.
On the other hand, all co-occurrence based algorithms produce very good results, with TC[CS] and TC[MI] being slightly better.
TF+WN[Syn] TF+WN[Sub] TF+WN[Sup]


 DF+WN[Syn] DF+WN[Sub] DF+WN[Sup]


 LC+WN[Syn] LC+WN[Sub] LC+WN[Sup]


 SS+WN[Syn] SS+WN[Sub] SS+WN[Sup]









 -























-























-















------------------




Algorithm


 TF+WN[Syn] TF+WN[Sub] TF+WN[Sup]


 DF+WN[Syn] DF+WN[Sub] DF+WN[Sup]


 LC+WN[Syn] LC+WN[Sub] LC+WN[Sup]


 SS+WN[Syn] SS+WN[Sub] SS+WN[Sup]


































 -























-













Table 4: P@1-4 for kewyord oriented extraction for small Web pages.
Table 5: P@1-4 for kewyord oriented extraction for medium Web pages.
----

----

--




-------------------




-----
The best average ratings overall were 0.81 at the  rst annotation (DF+TC[CS] and DF+TC[MI]), 0.82 at the second one (DF+TC[CS]) and  nally 0.80, when considering only the top third output (DF+TC[MI]).
No method managed to generally produce more than three highly qualitative annotations.
All keyword oriented results are summarized in Tables 4, 5, and 6, for the small, medium, and large pages respectively.
In order to ease the inspection of results, for each keyword extraction algorithm (i.e.,  rst step), the best method with respect to P@3 is in bold.
Hybrid Extraction.
Our hybrid algorithms are composed of two phases,  rst a single page keyword extraction, and then another one, at the Desktop level and over those documents matching the best formerly extracted keywords (i.e., as obtained by searching the local Desktop with Lucene).
All methods performed rather similar, with small differences between each other.
An inspection of the actual data showed that again DF performed particularly well at the  rst extraction step.
However, this single word output was not always discriminative enough for a regular Desktop search, i.e., for  nding Desktop documents highly similar to the input Web page.
In fact, this is true for all keyword extraction techniques, and this is why the keyword oriented methods managed to surpass the hybrid ones in the quality of their output.
For the second step, TF performed visibly better with medium and large pages, and all methods were close to each other for small input data.
This is in accordance with the document oriented approaches, which use a similar technique.
SS+TF is the best overall algorithm, with a precision of 0.80 at only the  rst result, and of 0.74 at the top two ones.
Then, TF+TF performs best, yielding a score of 0.73 at the top three annotations, and 0.74 when all results are included in the analysis.
All results are also depicted in Tables 7, 8, and 9, for the small, medium, and large pages respectively.
For each keyword extraction algorithm (i.e.,  rst step), the best method is in bold.
Comparison.
We now turn our attention to  nding global conclusions over all our proposed algorithms.
In order to better pursue this analysis, we depict the three best performing algorithms of each category (i.e., document oriented, keyword oriented, and hybrid) in Figure 1.
Algorithm


 TF+WN[Syn] TF+WN[Sub] TF+WN[Sup]


 DF+WN[Syn] DF+WN[Sub] DF+WN[Sup]


 LC+WN[Syn] LC+WN[Sub] LC+WN[Sup]


 SS+WN[Syn] SS+WN[Sub] SS+WN[Sup]









 -























-























-













Table 6: P@1-4 for kewyord oriented extraction for large Web pages.
For small Web pages (the leftmost bar), the keyword oriented approaches are by far the best ones, being placed on positions one, two and four.
They are followed by the document oriented ones, and  nally by the hybrid methods, all global differences being very clear.
In the case of medium sized pages, the proposed algorithms are harder to separate, performing similarly when analyzed over their best three representatives.
Interesting here is the strong drop of TF+TC[LR], indicating that term frequency is good at single document keyword extraction only with small Web pages.
Finally, we observe that the document oriented approaches are the best with large pages, which is reasonable, as they have the most amount of information available when searching the local Desktop for documents similar to the input Web page.
They are followed by the keyword oriented techniques, and then by the hybrid ones.
Algorithm P@1


















































































 Table 7: P@1-4 for hybrid extraction for small Web pages.
Table 9: P@1-4 for hybrid extraction for large Web pages.
Algorithm P@1


















































































 Table 8: P@1-4 for hybrid extraction for medium Web pages.
Figure 1 shows the quality of the output as a function of the input page size.
As small Web pages contain few terms, they yield a clearer output, either when searching for related documents (as with document oriented techniques), or when extracting their keywords (as in the keyword oriented approaches), etc.
As the content size increases, more noise appears, and processing becomes more dif cult.
Yet when Web pages have reached a reasonably large size, a number of informative terms tend to stand out, thus easing their processing to some extent.
We also averaged the results of all algorithms over all evaluated pages (in the rightmost column).
We observed an obvious  ranking  across our generic approaches: (1) Keyword based algorithms (especially DF+TC[MI]), (2) Document based techniques, and (3) Hybrid ones.
Though one would probably expect the latter ones to be the best, they suffered from the fact that the extracted keywords were insuf cient to enable the retrieval of highly similar Desktop documents.
On the contrary, the keyword based algorithms offer the optimal balance between the content of the input Web page and the personal  les, producing a good selection of keywords which are both contained, as well as missing from the input page.
Finally, in Table 10 we give several examples for the output produced by the best two algorithms per generic approach.
Let us inspect the  rst one in more detail.
While some of the generated annotations can also be identi ed with previous work methods (e.g.,  search , as it has a high term frequency in the input URL), many others would have not been located by them either because they are not named entities and have a low frequency in the input page (e.g.,  schema  or  proximity search ), or simply because they are not contained in the starting URL (e.g.,  retrieval  or  malleable  Figure 1: Precision at the  rst three output annotations for the best methods of each category.
Algorithm



 http://citeseer.ist.psu.edu/542246.html Search Information Proximity Search Relational Databases Schema Schema Search Database Web Search Engine Web Web Web Web Search Search Query Query Search Web Pages Information System Page Search http://en.wikipedia.org/wiki/PageRank

















 User Banks System Web Retrieval Malleable Probabilistic System Foreign Key Schema Networks Information Authority Transfer Pages Link Structure Conference Conference Links Semantic Semantic Information Computer http://www.l3s.de/ chirita/resume.htm Bucharest University Search Computer Science Information Retrieval Personalized Web Search Technical Report Retrieval Retrieval Search Search Search Search Retrieval Web System Information Web Pages Ranking Semantic Table 10: Examples of annotations produced by different types of algorithms.
  both major research interests of our subject, highly related to the annotated page; or  Banks system    a database search system very similar to the one presented in the given Web page; or  probabilistic , etc.
).
Practical Issues.
As we discussed earlier, the approach we propose is highly scalable: Our annotation algorithms are highly ef- cient, and utilize client processing power to annotate the Web pages.
Users don t need to produce annotations for all pages they visit, but only for a sample of them (possibly randomly selected).
The server collecting the data is free to limit the amount of incoming connections in order not to become overloaded.
SMLAverage0.600.700.800.90DF+TC[MI]TFDF+TC[CS]LCTF+TFTF+TC[LR]SSLC+TFSS+TFWeb page sizeP@3WWW 2007 / Track: Semantic WebSession: Semantic Web and Web 2.0852From an implementation perspective, the algorithms proposed here can be easily integrated into browser toolbars already distributed by the major search engines.
In fact, these toolbars already communicate with logging servers in order to send statistical browsing information5, utilized for enhancing the search results ranking function.
Thus, only a small additional communication cost is necessary.
The approach we presented here enables a range of applications, from the most obvious, such as personalized Web search or Web recommendations, to ontology learning and Web advertising.
Personalized Web Search.
An interesting application is Web search personalization [8].
One could exploit such keyword extraction algorithms to generate term based pro les from each user s collection of personal documents.
Upon searching some external collection (e.g., the Web), the output results could be biased towards those pages residing closer to the user pro le in the terms hyperspace.
Web Recommendations for Desktop Tasks.
It is quite common for people to search the Web for currently existing content to assist their present work tasks.
It is possible to use the approaches we proposed in this paper in order to automatically suggest such pages [9, 22].
More speci cally, upon editing a Desktop document, relevant keywords would be extracted from the already written content and utilized to search on the Web for additional, useful resources on the same topic.
Then, these automatically located pages could be displayed in a condensed format (e.g., title and URL) using a small discreet window placed for example in the bottom-right corner of the screen.
Ontology Learning.
In the scenario of ontology-driven annotations, where an underlying ontology (customizable for each user) provides the terminology for such annotations, it might be necessary to enrich the ontology with user-speci c concepts.
These can be provided from the user s context, represented by keywords extracted from her Desktop environment.
For instance, the initial Personal Information Management ontology might lack a concept relevant to a speci c user, for instance the class  Research Interests  as subclass of  Interests .
An analysis of keywords detected by the Lexical Compounds method is particularly valuable for an Ontology Learning approach, which we intend to investigate in future work.
Based on the metrics described in this paper, the term  Research Interests  could be suggested as a keyword, and adopted as a class in the user-speci c layer of the ontology.
Relevant multiword expressions detected in such a way, sharing the same head noun, may be proposed as classes and organized hierarchically, while a hypothesis analysis for collocation over the Desktop corpus will enable us to distinguish between a simple modi ed head noun and a multiword expression bearing more meaning than the sum of its parts.
Further knowledge may be extracted by considering sentences containing the keywords from the set determined by the Sentence Selection algorithm, which are often of descriptive nature.
Considering the example above, it would be straightforward to identify  information extraction ,  natural language processing  and  knowledge representation  as instances of the concept  Research Interests , given for example the sentence  His research interests include information extraction[..]  and given the assumption that variations of this sentence will be found in documents on the Desktop and on the Web.
Finally, this strategy will enable us to extract both instances and relevant relations between the proposed classes.
Other Applications.
If we move away from using personal Desktops, we can identify quite a lot of other applications of the same algorithms, some of them even already investigated.
Due to space limitations we note here only one very important example: Web advertising.
Keywords, as extracted from Web pages with the algorithms we presented, could be used to better match advertisements, as well as to propose better bidding expressions for the owner of each input Web site.
We have described a novel approach for scalable automatic personalized generation of annotation tags for Web pages.
To the best of our knowledge there is no algorithm that does the same.
Our technique overcomes the burden of manual tagging and it does not require any manual de nition of interest pro les.
It generates personalized annotation tags for Web pages by building upon the implicit background knowledge residing on each user s personal Desktop.
Also, in contrast to keyword extraction algorithms that can only propose terms which actually appear on the input Web page, our system proposes a more diverse range of tags which are closer to the personal viewpoint of the user.
The results produced provide a high user satisfaction (usually above 70%).
Thus, the greatest bene t of P-TAG is the high relevance of the tags for the user, and therefore the capacity of the tag to describe a Web page and to serve for a precise information retrieval.
The current implementation of P-TAG is intended to assist Web search engines.
For the near future we therefore plan to implement a shared server approach that supports social tagging, in which the system would know about personal annotations from other users and would be able to identify the most popular annotations, e.g., the ones with the highest score.
This would also enable the sharing of the automatic generated personal annotations in a collaborative environment, and would simply automatically create, apply and share tags dynamically.
For such a server based approach we envision the following ad-vantages:
 and thus cover various user interests.
machines to collect the annotations, as well as from how many machines.
One can easily mine the dominating interests of the persons browsing a given Web page or set of pages.
Similarly, the Web page could be made searchable for an extended set of keywords, including the annotations generated with our algorithm.
volatility of the Web; newly created Web pages get annotated automatically, as they are visited by users.
We believe that P-TAG provides rather intriguing possibilities which can lead to a considerable high amount of annotated Web pages by automatic personalized tagging, thus lowering the barrier for the application of more semantics on the Web.
