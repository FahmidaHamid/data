One criticism of search engines is that when queries are issued, most return the same results to users.
In fact, the vast  Work was done when the author was visiting Microsoft Research Asia.
Copyright is held by the International World Wide Web Conference Com(cid:173) mittee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
ACM 978(cid:173)1(cid:173)59593(cid:173)654(cid:173)7/07/0005.
majority of queries to search engines are short [27, 12] and ambiguous [16, 7], and di erent users may have completely di erent information needs and goals under the same query [12, 26, 23, 34].
For example, a biologist may use query  mouse  to get information about rodents, while programmers may use the same query to  nd information about computer peripherals.
When such a query is submitted to a search engine, it takes a moment for a user to choose which information he/she wishes to get.
On the query  free mp3 download , the users  selections can also vary though almost all of them are  nding some websites to download free mp3: one may select the website  www.yourmp3.net , while another may prefer the website  www.seekasong.com .
Personalized search is considered a solution to this problem since di erent search results based on preferences of users are provided.
Various personalization strategies including [21, 22, 26, 31, 14, 9, 35, 30, 19] have been proposed, and personalized web search systems have been developed, but they are far from optimal.
One problem of current personalized search is that most proposed methods are uniformly applied to all users and queries.
In fact, we think that queries should not be handled in the same manner because we  nd: (1) Personalization may lack e ectiveness on some queries, and there is no need for personalization on such queries.
This has also been found by [34].
For example on the query  mouse  mentioned above, using personalization based on user interest pro le, we could achieve greater relevance for individual users than common web search.
Beyond all doubt, the personalization brings signi cant bene t to users in this case.
Contrarily, for the query  Google , which is a typical navigational query as de ned in [3, 17], almost all of the users are consistently selecting results to redirect to Google s homepage, and therefore none of the personalized strategies could provide signi cant bene ts to users.
(2) Di erent strategies may have variant e ects on di er-ent queries.
For the query  free mp3 download , using the typical user interest pro le-based personalization such as the method proposed in [6], which led to better results for the query  mouse , we may achieve poor results because the results for query  free mp3 download  are mostly classi ed into one topic category and the pro le-based personaliza-tion is too coarse to  lter out the desired results.
In such a case, simply leveraging pages visited by this user in the past may achieve better performance.
Furthermore, simply applying one personalization strategy on some queries without any consideration may harm user experience.
For example, when a sports fan submits the query  o ce , he/she may not be seeking information on sports, but may be seeking help on Microsoft O ce Software or any other number of o ce-related inquiries.
In this situation, if interest-based personalization is done, many irrelevant results could erroneously be moved to the front and the user may become confused.
(3) Personalization strategies may provide di erent e ec-tiveness based on di erent search histories and under variant contexts.
For example, it could be di cult to learn interests of users who have done few searches.
Furthermore, as Shen et al. [25] noted, users often search for documents to satisfy short-term information needs, which may be inconsistent with general user interests.
In such cases, long-term user pro les may be useless and short-term query context may be more useful.
In short, the e ectiveness of a speci c personalized search strategy may show great improvement over that of non-personalized search on some queries for some users, and under some search contexts, but it can also be unnecessary and even harmful to search under some situations.
Until now, little investigation has been done on how personaliza-tion strategies perform under di erent situations.
In this paper, we get some conclusions on this problem and make the following contributions: (1) We develop a large-scale personalized search evaluation framework based on query logs.
In this framework, different personalized re-ranking strategies are simulated and the search accuracy is approximately evaluated by real user clicks recorded in query logs automatically.
The framework enables us to evaluate personalization on a large scale.
(2) We propose two click-based personalized search strategies and three pro le-based personalized search strategies.
We evaluate all  ve approaches in the evaluation framework using 12-day query logs from MSN search engine1 and provide an in-depth analysis on the results.
(3) We reveal that personalization has di erent e ective-ness on di erent queries, users, and search contexts.
Person-alization brings signi cant search accuracy improvements on the queries with large click entropy, and has little e ect on the queries with small click entropy.
Personalization strategies can even harm the search accuracy on some queries.
Therefore, we conclude that not all queries should be personalized equally.
(4) We show that click-based personalization strategies perform consistently and considerably well though they can only work on the repeated queries.
We  nd that our pro le-based strategies are unstable because of the straightforward implementation.
We also  nd the pro le-based methods become more unstable when users search history grows.
We reveal that both long-term and short-term contexts are very important in improving search performance for pro le-based personalization.
The remaining sections are organized as follows.
In Section 2, we discuss related works.
We present a re-ranking framework and introduce how to use this framework to evaluate the personalization strategies in Section 3.
In Section 4, we give several personalization strategies in both person and group levels.
In Section 5 we introduce the dataset used in our experiments and detailed data statistics.
We compare and analyze the results of these strategies in Section 6.
We conclude our work in Section 7.
There are several prior attempts on personalizing web search.
One approach is to ask users to specify general interests.
The user interests are then used to  lter search results by checking content similarity between returned web pages and user interests [22, 6].
For example, [6] used ODP2 entries to implement personalized search based on user pro les corresponding to topic vectors from the ODP hierarchy.
Unfortunately, studies have also shown that the vast majority of users are reluctant to provide any explicit feedback on search results and their interests [4].
Many later works on personalized web search focused on how to automatically learn user preferences without any user e orts [22, 19,
 categories or term lists/vectors.
In [19], user pro les were represented by a hierarchical category tree based on ODP and corresponding keywords associated with each category.
User pro les were automatically learned from search history.
In [29], user preferences were built as vectors of distinct terms and constructed by accumulating past preferences, including both long-term and short-term preferences.
Tan et al.
[31] used the methods of statistical language modeling to mine contextual information from long-term search history.
In this paper, user pro les are represented as weighted topic categories, similar with those given in [28, 6, 22], and these pro les are also automatically learned from users  past clicked web pages.
Many personalized web search strategies based on hyperlink structure of web have also been investigated.
Personalized PageRank, which is a modi cation of the global PageRank algorithm, was  rst proposed for personalized web search in [20].
In [10], multiple Personalized PageRank scores, one for each main topic of ODP, were used to enable  topic sensitive  web search.
Jeh and Widom [14] gave an approach that could scale well with the size of hub vectors to realize personalized search based on Topic-Sensitive PageRank.
The authors of [32] extended the well-known HITS algorithm by arti cially increasing the authority and hub scores of the pages marked relevant by the user in previous searches.
Most recently, [17] developed a method to automatically estimate user hidden interests based on Topic-Sensitive PageRank scores of the user s past clicked pages.
In most of above personalized search strategies, only the information provided by user himself/herself is used to create user pro les.
These are also some strategies which incorporate the preferences of a group of users to accomplish personalized search.
In these approaches, the search histories of users who have similar interest with test user are used to re ne the search.
Collaborative  ltering is a typical group-based personalization method and has been used in personalized search in [29] and [30].
In [29], users  pro les can be constructed based on the modi ed collaborative  l-tering algorithm [15].
In [30], the authors proposed a novel method CubeSVD to apply personalized web search by analyzing the correlation among users, queries, and web pages contained in click-through data.
In this paper, we also introduce a method which incorporates click histories of a group of users to personalize web search.
Some people have also found that personalization has variant e ectiveness on di erent queries.
For instance, Teevan et al. [34] suggested that not all queries should be handled

 in the same manner.
For less ambiguous queries, current web search ranking might be su cient and thus personaliza-tion is unnecessary.
In [6] and [5], test queries were divided into three types: clear queries, semi-ambiguous queries, and ambiguous queries.
The authors also concluded that per-sonalization signi cantly increased output quality for ambiguous and semi-ambiguous queries, but for clear queries, one should prefer common web search.
In [31], queries were divided into fresh queries and recurring queries.
The authors found that recent history tended to be much more useful than remote history especially for fresh queries while the entire history was helpful for improving the search accuracy of recurring queries.
This also gave us a sense that not all queries should be personalized in the same way.
These conclusions inspired our detailed analysis.
The typical evaluation method used in existing personalized search research is to conduct user studies [23, 26, 6,
 ple participate in the evaluated personalized search system over several days.
The user pro les are manually speci ed by participants themselves [6] or automatically learned from search histories.
To evaluate the performance of personalized search, each participant is required to issue a certain number of test queries and determine whether each result is relevant.
The advantage of this approach is that the relevance can be explicitly speci ed by participants.
Unfortunately, there are still some drawbacks in this method.
The constraint of the number of participants and test queries may bias the accuracy and reliability of the evaluation.
We propose a framework that enables large-scale evaluation of personalized search.
In this framework, we use click-through data recorded in query logs to simulate user experience in web search.
In general, when a user issues a query, he/she usually checks the documents in the result list from top to bottom.
He/she clicks one or more documents which look more relevant to him/her, and skip the documents which he/she is not interested in.
If a speci c per-sonalization method can re-rank the  relevant  documents fronter in the result list, the user will be more satis ed with the search.
Therefore, we utilize clicking decisions as relevance judgments to evaluate the search accuracy.
Since click-through data can be collected at low cost, it is possible to do large-scale evaluation using this framework.
Furthermore, since click-through data re ect real world distributions of query, user, and user selections, they could be more accurate to evaluate personalized search using click-through data than user surveys.
One potential concern about the evaluation framework is that the original user selections may be in uenced by initial result rankings[2], and thus it could be unfair to evaluate a reordering of the original search results using the original click data.
Our framework may fail to evaluate the ranking alternation of documents that are relevant but were not clicked by users, and this may bias the evaluation.
However, our framework is still e ective to evaluate approximate search accuracy.
It is the best method we could adopt to enable large-scale evaluation of personalized search.
We will investigate more stable methodology in future work.
In the evaluation framework, we use MSN query logs to simulate and evaluate the personalized re-ranking.
In MSN query logs, each user is identi ed by  Cookie GUID , which remains the same in a machine as long as a cookie is not cleared.
For each query, MSN search engine logs the query terms and records all click-through information including clicked web pages and their ranks.
A  Browser GUID , which remains the same before the browser is reopen, is also recorded for each query.
It is used as the simple identi er of a session, which includes a series of queries made by a single user within a small range of time and is usually meant to capture a single user s attempt to ful ll a single information need [27, 12].
In this evaluation framework, we  rst download search results from MSN search engine, then use one personaliza-tion strategy to re-rank the results.
The click-through data recorded in test set is then used to evaluate the re-ranking performance.
In more detail, query re-ranking and evaluation are completed in the following steps: (1) Download the top 50 search results from MSN search engine for the test query.
We denote the downloaded web pages with U and denote the rank list that contains the rankings of the web pages with  1.
(2) Compute a personalized score for each web page xi   U using personalization algorithm and then generate a new rank list  2 with respect to U sorted by descending personalized scores.
The personalized strategies are introduced in Section 4.
(3) Combine the rankings in  1 and  2 using Borda  ranking fusion method [13, 8] and sort the web pages with the combined rankings.
The  nal rank list is denoted with   .
  is the  nal personalized search result list for the query.
Notice that we use the rank-based ranking fusion method because we are unable to get the relevance scores from MSN search engine.
(4) Use the measurements introduced in Section 3.2 to evaluate the personalization performance on   .
In this paper, we assume the results downloaded from MSN search engine are consistent with those returned to the user when the query was submitted.
We use the most recent MSN query logs on August 2006 and download search results in the early days on September 2006 so that the changes of search results can be ignored (We also tried the approach of rebuilding the search results from query logs but it failed because of the sparseness of queries and user clicks).
We downloaded only the top 50 search results because most users never look beyond the top 50 entries in the test set.
We use two measurements to evaluate the personalized search accuracy of di erent strategies: rank scoring metric introduced in [30, 15] and average rank metric introduced in [23].
Rank scoring metric proposed by Breese [15] is used to evaluate the e ectiveness of the collaborative  ltering systems which return an ordered list of recommended items.
Sun et al.[30] used it to evaluate the personalized web search accuracy and we also use it in this paper.
The expected utility of a ranked list of web pages is de ned as Rs = X j  (s, j) 2(j 1)/( 1) where j is the rank of a page in the list,  (s, j) is 1 if page j is clicked in the test query s and 0 otherwise, and   is set to 5 as the authors did.
The  nal rank scoring re ects the utilities of all test queries: repeated by the same user and this approach will only bring bene ts to these queries.
This approach is denoted with P-Click.
R = 100 Ps Rs Ps RM ax s (1) ests s Here, RM ax is the obtained maximum possible utility when all pages which have been clicked appear at the top of the ranked list.
Larger rank scoring value indicates better performance of personalized search.
Average rank metric is used to measure the quality of personalized search in [23, 28].
The average rank of a query s is de ned as below.
AvgRanks =
 |Ps| X p Ps R(p) Here Ps denotes the set of clicked web pages on test query s, R(p) denotes the rank of page p. The  nal average rank on test query set S is computed as: AvgRank =

 s S AvgRanks (2) Smaller average rank value indicates better placements of relevant result, or better result quality.
In fact, rank scoring metric and average rank metric has similar e ectiveness on evaluating personalization performance, and our experimental results show that they are consistent.
As we described in Section 2, personalized search methods can be categorized into person level and group level.
In this paper, we propose several re-ranking methods in both levels to accomplish personalized search.
These strategies are used to re-rank search results by computing a personalized score S (q, p, u) for each page p in the results returned to user u on query q, as Section 3 introduced.
In the following sections, we will introduce the strategies.
Clicks We suppose that for a query q submitted by a user u, the web pages frequently clicked by u in the past are more relevant to u than those seldom clicked by u, and thus, the personalized score on page p can be computed by: SP Click (q, p, u) = |Clicks (q, p, u)| |Clicks (q,  , u)| +   (3) Here, |Clicks (q, p, u)| is the click number on web page p by user u on query q in the past, |Clicks (q,  , u)| is the total click number on query q by u, and   is a smoothing factor (  = 0.5 in this paper).
Notice that |Clicks (q, p, u)| actually decides the ranking of the page, while |Clicks (q,  , u)| and   are only used for normalization.
A disadvantage to this approach is that the re-ranking will fail when the user has never asked this query.
We  nd that in our dataset, about one-third of the test queries are As introduced in Section 2, many current researches use interest pro les to personalize search results [22, 19, 6].
In this paper, we also proposed a personalization method based on user interest pro le (we denote this method with L-Pro le).
User s pro le cl (u) is presented as a weighting vector of 67 pre-de ned topic categories provided by KDD Cup-2005 [18].
When a user submits a query, each of the returned web pages is also mapped to a weighting category vector.
The similarity between the user pro le vector and page category vector is then used to re-rank search results: SL-P rof ile (q, p, u) = cl (u)   c (p) kcl (u)k kc (p)k (4) Here c (p) is category vector of web page p. c (p) is generated by a tool using the query and web page classi cation method introduced in [24].
Given a web page p, the tool returns top 6 categories which p belongs to with corresponding con dences.
Each component c (p)i of c (p) is the classi cation con dence returned by the tool, which means the probability that page p should be classi ed into category i.
If category i is not in the 6 categories returned by the tool, then we set c (p)i = 0.
User s pro le cl (u) is automatically learned from his/her past clicked web pages as the following equation: cl(u) = X P (p|u)w(p)c(p) p P(u) Here P(u) is the collection of web pages visited by user u in the past.
P (p|u) can be thought of as the probability that user u clicks web page p, i.e., P (p|u) = |Clicks ( , p, u)| |Clicks ( ,  , u)| Here, |Clicks ( ,  , u)| is the total click times made by u and|Clicks ( , p, u)| is the click times on web page p made by u. w (p) is the impact weight for page p when generating user pro les.
We assume that the web pages submitted by many users are less important when building user pro les, thus, w (p) = log
 |U(p)| |U| is the number of total users; |U(p)| is the number of users who have ever visited web page p.
In method L-Pro le, user s pro le cl(u) is accumulated from user s visited web pages in the past.
This pro le is called long-term pro le in previous works [29, 26, 31].
In fact, as investigated by [26], short-term user pro le is more useful for improving search in current session.
In this paper, we use the clicks on the previous queries in current session to build user s short-term pro le.
A user s short-term pro le cs(u) is computed as below.
cs(u) =
 |Ps(q)| X p Ps(q) c(p) Ps(q) is the collection of visited pages on previous queries in current session.
The personalized score of page p using short-term pro le is computed as the following equation: SS-P rof ile(q, p, u) = cs (u)   c (p) kcs (u)k kc (p)k (5) This approach is denoted with S-Pro le.
We can also fuse the long-term personalized score and the short-term personalized score using a simple linear combination: SLS-P rof ile (q, p, u) =  SL-P rof ile (q, p, u) + (1    )SS-P rof ile (q, p, u) (6) We denote this approach with LS-Pro le.
Methods L-Pro le, S-Pro le, and LS-Pro le are generally called pro le-based methods for short in this paper.
We use the K-Nearest Neighbor Collaborative Filtering algorithm to test group-based personalization.
Due to the data sparsity in our dataset, using traditional CF methods on web search is inadequate.
Instead, we compute the user similarity based on long-term user pro les: Sim (u1, u2) = cl (u1)   cl (u2) kcl (u1)k kcl (u2)k The K-Nearest neighbors are obtained based on the user similarity computed as follows.
Su (ua) = {us|rank (Sim (ua, us))   K} Then we use the historical clicks made by similar users to re-rank the search results:
 Sim(us, u) |Clicks (q, p, us)| SG-Click (q, p, u) = us Su(u)   + P us  Su(u) |Clicks (q,  , us)| (7) We denote this approach with G-Click.
In this section, we introduce the dataset used in our experiments.
We collect a set of MSN query logs for 12 days in August
 large, we randomly sample 10,000 distinct users (identi ed by  Cookie GUID ) from the users in the United States on August 19, 2006.
These users and their click-through logs are extracted as our dataset.
In addition, the queries without any clicks (about 34.6% of all queries) are excluded from the dataset because they are useless in our experiments.
The entire dataset is split into two parts: a training set and a test set.
The training set contains the log data of the  rst 11 days and the log data of the last day is used for testing.
Table 1 summarizes the statistics.
Notice that all 10,000 users have search activities in the training set because users are sampled from the logs of training days.
Because users are randomly sampled, this dataset could re ect the characteristics of the entire logs.
It also has similar characteristics of those in existing reports [27, 37, 11, 36,
 sections.
Table 1: Basic statistics of dataset Item #days #users #queries #distinct queries #Clicks #Clicks/#queries #sessions
 Training Test
























 i s e m
 y r e u






 Query ID(ordered by query times in descending order) (a) Distribution of query frequency (by log scale).
s r e s u f o r e b m u






 Query ID(ordered by number of users in descending order) (b) Distribution of user number of queries (by log scale).
Figure 1: Query popularity distributions.
In our dataset, more than 80% distinct queries are only issued once in a 12-day period, and about 90% distinct queries string are issued only by one user.
The 3% most popular distinct queries are issued by more than 47% users.
The statistics is similar with that given in [27, 37, 11], and this indicates that information needs on the Web are quite diverse.
Furthermore, we  nd that query frequency can also be characterized by Zipf distributions, consistent with that found by [37].
Figure 1(a) plots the distributions of query frequency.
In this  gure, queries are sorted by query times in descending order: the  rst query is the most popular one, and the last is the most unpopular one.
Figure 1(b) plots the distribution of number of users on each query.
As Table 1 shows, 1,792 users have search activities on the test day.
Figure 2 plots the distribution of historical (i.e. in s r e s u f t o e g a n e c r e









 s r e s u f t o e g a n e c r e




















 User historical query days User historical query times (a) Distribution of user historical query days (b) Distribution of user historical query times Figure 2: Distributions of user search frequency in training days for test users i s n o s s e s f o e g a t n e c r e










 In this paper, we de ne click entropy of query as Equation 8.
ClickEntroy(q) = X  P (p|q) log2 P (p|q) (8) p P(q) Here ClickEntroy(q) is the click entropy of query q. P(q) is the collection of web pages clicked on query q. P (p|q) is the percentage of the clicks on web page p among all the clicks on q, i.e.,






 Queries per session P (p|q) = |Clicks(q, p,  )| |Clicks(q,  ,  )| Figure 3: Distribution of query number per session.
training days) query day and query times for users in the test set.
Because users are sampled on one of the training days, each user has at least a daylong query history.
We  nd about 30% users in the test set have more than 5 days  query history and about 50 % of them submit more than 10 queries in training days.
In our dataset, 2,130 (about 46%) of all 4,639 queries in the test set are repeated ones that have been submitted in the training days, either by the same user or by di erent users.
Furthermore, 1,535 queries (72% of repeated ones and 33% of test queries) are repeated by the same user.
These results are consistent with those given in [33] and are helpful for personalized search.
In our dataset, we use  Browser GUID  as a simple iden-ti er of session.
A user opens a browser and asks one or more queries and then closes the browser: the whole process is considered as a session in this paper.
Figure 3 shows the distribution of number of queries in a session.
About 30% sessions contain at least two queries.
This indicates that users sometimes submit several queries to ful ll an information need.
As found by [34], for queries which showed less variation among individuals, the personalization may be insu cient.
Click entropy is a direct indication of query click variation.
If all users click only one same page on query q, then we have ClickEntroy(q) = 0.
Smaller click entropy means that the majorities of users agree with each other on a small number of web pages.
In such cases, there is no need to do personalization.
Large click entropy indicates that many web pages were clicked for the query.
This may mean: (1) a user has to select several pages to satisfy this query, which means the query is an informational query [3, 17].
Person-alization can help to  lter the pages that are more relevant to users by making use of historical selections.
(2) Di erent users have di erent selections on this query, which means the query is an ambiguous query.
In such cases, personal-ization can be used to provide di erent web pages to each individual.
Figure 4(a) shows the click entropy distribution.
More than 65% queries have low click entropy between 0 and 0.5.
We  nd many of these queries are submitted only by one user and the user only clicks one web page.
Figure 4(b) shows the click entropy distribution for queries asked more than  ve times.
Figure 4(c) plots the click entropy distribution for queries submitted by at least three users.
From these  gures, we also  nd that the majorities of the more popular queries have low click entropies.
In this section, we will give detailed evaluation and analysis of the  ve personalized search strategies.
Notice that the original web search method without any personalization, which is used for comparing with the personalized methods, is denoted with  WEB .
We let K = 50 for method G-Click and   = 0.3 for method LS-Pro le, and both of the two settings are empirical.
In our experiments, we  nd 676 queries in total 4,639 test s e i r e u q f t o e g a n e c r e






 s e i r e u q f t o e g a n e c r e






 s e i r e u q f t o e g a n e c r e











 >5




 >5




 >5 Click entropy of queries (a) All queries Click entropy of queries Click entropy of queries (b) Queries with query times>5 (c) Queries with user number>2 Figure 4: Distribution of query click entropy.
queries lost the clicked web pages in downloaded search results.
This is because MSN search engine has changed for these queries.
We excluded these queries when reporting the experimental results in the following sections.
Furthermore, we  nd for 57% (2,256/3,963) of the left queries, users select only the top results.
In other words, original search method WEB has done the best on these queries and per-sonalization does not provide improvements.
We call the other 1,707 queries, on which users select not only the top results, not-optimal queries.
Table 2 shows the overall e ectiveness of the personaliza-tion strategies on the test queries.
We  nd: (1) Click-based personalization methods G-Click and P-Click outperform method WEB on the whole.
For instance, on the not-optimal queries, method P-Click has a signi cant (p < 0.01) 3.68% improvement over method WEB and method G-Click have a signi cant (p < 0.01) 3.62% improvement over WEB (using rank scoring metric).
P-Click and G-Click methods also have signi cant improvements (1.39% and 1.37%) over WEB on all test queries including both not-optimal and optimal queries.
These results show that click-based personalization methods can generally improve web search performance.
(2) Methods P-Click and G-Click have no signi cant different performances on the whole.
In our experiments, we sample 10,000 users and select the 50 most similar users for each test user in G-Click approach (we also try the methods to select 20 and 100 users, but they show no signi cant di erence).
By reason of high user query sparsity, selected similar users may have few search histories on the queries submitted by test user.
This makes group-level personaliza-tion perform no signi cant improvement over person-level personalization.
If more days  logs are given and more users are selected, method G-Click may perform better.
(3) Pro le-based methods L-Pro le, S-Pro le, and LS-Pro le perform less well on average.
We compute rank scor-ings of all the methods for each single test query and then plot the distributions of rank scoring increment over WEB method for each personalization strategy in Figure 5.
We  nd that though L-Pro le, S-Pro le, and LS-Pro le methods improve the search accuracy on many queries, they also harm the performance on more queries, which makes them perform worse on average.
This indicates that the straightforward implementation of pro le-based strategies we employ in this paper do not work well, at least not as stable as the click-based ones.
We will give some analysis on why our pro le-based methods are unstable in Section 6.5.
Table 2: Overall performance of personalization strategies.
R.S.
denotes the rank scoring metric and A.R.
denotes the average rank metric.
method
 P-Click L-Pro le S-Pro le LS-Pro le G-Click all not-optimal




























 We give the average search accuracy improvements of different personalization strategies on the test queries with different click entropy in Figure 6.
We use only the queries asked by at least three users to make the click entropy more reliable.
We see that the improvement of the personalized search performance increases when the click entropy of query becomes larger, especially when click entropy   1.5.
For the click-based methods P-Click and G-Click, the improvement of personalization is very limited on the queries with click entropies between 0 and 0.5.
The G-Click method, which gets the best performance for these queries, has only a non-signi cant 0.37% improvement over WEB methods in rank scoring metric.
This means users have small variance on these queries, and the search engine has done well for these queries, while on the queries with click entropy 2.5, the result is disparate: both P-Click and G-Click methods make exciting performance.
In the rank scoring metric, method G-Click has a signi cant (p < 0.01) 23.37% improvement over method WEB and P-Click method have a signi cant (p < 0.01) 23.68% improvement over method WEB.
Pro le-based methods L-Pro le, S-Pro le and LS-Pro le worsen search performance when click entropy < 1.5, while L-Pro le and LS-Pro le also achieve better performances on queries with click entropy   1.5 (we wonder why method L-Pro le also worsens search accuracy when click entropy 2.5 and will provide additional analysis on this in future work).
All these results indicate that on the queries with small click entropy (which means that these queries are less ambiguous or more navigational), the personalization is insuf- cient and thus personalization is unnecessary.
s e i r e u q t s e t f o r e b m u




 s e i r e u q t s e t f o r e b m u




 s e i r e u q t s e t f o r e b m u




 s e i r e u q t s e t f o r e b m u




 s e i r e u q t s e t f o r e b m u



 -100 -50


 -100 -50


 -100 -50


 -100 -50


 -100 -50


 P-Click - WEB (a) P-Click L-Profile - WEB (b) L-Pro le S-Profile - WEB (c) S-Pro le LS-Profile - WEB (d) LS-Pro le G-Click - WEB (e) G-Click Figure 5: Distributions of rank scoring increment over WEB method.
The count of the test queries with the same rank scoring increment range is plot in y-axis with log scale.
P-Click L-Profile S-Profile LS-Profile G-Click t n e m e v o r p m
 g n i r o c
 k n a






 -5% -10% t n e m e v o r p m
 k n a
 e g a r e v




 -20% -40% -60%
 P-Click L-Profile S-Profile LS-Profile G-Click




 >=2.5




 >=2.5 Entropy (a) Ranking scoring Entropy (b) Average rank Figure 6: Search accuracy improvements over WEB method on the queries with variant click entropies.
Only the queries asked by at least three users are included.
Notice that in  gure (b), smaller average rank means higher search accuracy.
In Subsection 5.4, we  nd about 46% test queries are repeated by the same user or di erent users and 33% queries are repeated by the same user.
It means that users often review the queries and results they ever referred.
Teevan et al.
[33] have also observed that re nding behavior is common, and have shown that repeat clicks can often be predicted based on a user s previous queries and clicks.
In this paper, methods P-Click and G-Click are based on historical clicks.
The high repetition ratio in real world makes these click-based personalization strategies work well.
Table 3(a) shows the personalization performance on the repeated non-optimal queries repeated by either the same user or di erent users and Table 3(b) gives the results on the queries repeated by the same user.
We  nd the per-sonalization methods P-Click and G-Click have signi cant improvements over WEB method on queries repeated by same user.
This means that when a user resubmit a query, his/her selections are also highly consistent with the past and the personalization based on his/her past clicks performs well.
These results tell us that we should record user query and click histories and use them to improve future search if no privacy problems exist.
We also should provide convenient ways for users to review their search histories, just like those provided by some current search engines.
Do users who frequently use search engine bene t more from personalized search?
Do pro le-based personalized search strategies perform better when the search history grows?
To answer these questions, we plot the improvements of rank scorings on queries given by users with di er-ent search frequencies in Figure 7.
We  nd: (1) Using click-based methods P-Click and G-Click, users who have greater search activities in training days do not consistently bene t more than users who do less searching.
This is because users who frequently use the search engine may have more varied information needs.
They may repeat old queries, but they may also submit lots of fresh queries, which makes our click-based methods P-Click and G-Click perform similar averages for users with di erent search frequencies (notice that the two series of methods P-Click and G-Click are very close to each other).
(2) Method L-Pro le when using a user s long-term interest pro le can perform better when a user has more queries, especially when the number of queries grows from 0 to 70.
This is because we can catch users  long-term interests more accurately when their search histories are long enough.
At the same time, we  nd that the performance of L-Pro le becomes more unstable when the user has more and more queries, especially when they have more than 80 queries.
This is because there is more noise in queries and furthermore the users have varied information needs.
This tell us that when the user s search histories increase, we should take more analysis on user s real information need and select only appropriate search histories to build up user pro les.
Tan et al. [31]  nd that the best performance of pro le-based personalized search methods they proposed is achieved when Table 3: Performance on repeated queries.
In Table(a), Y means that the query is repeated by either the same user or di erent users and N means not.
In Table(b), Y means that the query is repeated by the same user and N means that the query is  rst submitted by a user.
All the queries are not-optimal queries.
(a) Performance on repeated queries (b) Performance on user-repeated queries method
 P-Click L-Pro le S-Pro le LS-Pro le G-Click





























 method
 P-Click L-Pro le S-Pro le LS-Pro le G-Click






























 P-Click L-Profile S-Profile LS-Profile G-Click t n e m e v o r p m
 g n i r o c
 k n a



 -5% -10% -15%




 Historical queries of user Figure 7: Rank scoring increments over WEB method on all test queries submitted by users with di erent query frequencies.
using click-through data of past searches that are related to the current query.
We think this is because of the same reasons.
(3) Methods S-Pro le and LS-Pro le are less sensitive to historical query number.
Method LS-Pro le is more stable than method L-Pro le.
From Table 2, we surprisingly  nd that the pro le-based personalization strategies perform less optimally, which is inconsistent with existing investigations [28].
We think this is due to the rough implementation of our strategies.
The experimental results indicate that the straightforward implementation we employ does not work well.
From Subsection 6.4 we see it is di cult to build an appropriate user pro le even when the user history is rich.
The search history inevitably involves a lot of noisy information that is irrelevant to the current search and such noise can harm the performance of personalization, as indicated by [31].
In our experiments we simply use all the historical user searches to learn user pro les without distinguishing between relevant and irrelevant parts, which may make the personalization unstable.
We also do none normalization and smoothing when generating user pro les.
Since these strategies are far from optimal, we will do more investigation and try to improve their performance in future work.
Although pro le-based approaches perform badly in our experiments, we can still  nd an interesting thing.
Method LS-Pro le is more stable than methods L-Pro le and S-Pro le, as shown in Table 2, Figure 5, Figure 6 and Figure 7.
That means the incorporation of long-term interest and short-term context can gain better performance than solely using either of them.
In other words, both long-term and short-term search contexts are very important to personalize search results.
The combination of the two type of search context can make the prediction of real user information need more reliable.
In this paper, we try to investigate whether personaliza-tion is consistently e ective under di erent situations.
We develop a evaluation framework based on query logs to enable large-scale evaluation of personalized search.
We use 12 days of MSN query logs to evaluate  ve personalized search strategies.
We  nd all proposed methods have signi cant improvements over common web search on queries with large click entropy.
On the queries with small click entropy, they have similar or even worse performance than common web search.
These results tell us that personalized search has di erent e ectiveness on di erent queries and thus not all queries should be handled in the same manner.
Click entropy can be used as a simple measurement on whether the query should be personalized and we strongly encourage the investigation of more reliable ones.
Experimental results also show that click-based personal-ization strategies work well.
They are straightforward and stable though they can work only on repeated queries.
We suggest that search engine keeps the search histories and provides convenient and secure review ways to users.
The pro le-based personalized search strategies proposed in this paper are not as stable as the click-based ones.
They could improve the search accuracy on some queries, but they also harm many queries.
Since these strategies are far from optimal, we will continue our work to improve them in future.
We also  nd for pro le-based methods, both long-term and short-term contexts are important in improving search performance.
The appropriate combination of them can be more reliable than solely using either of them.
We are grateful to Dwight Daniels for edits and comments on writing the paper.
Comments from the four anonymous referees are invaluable for us to prepare the  nal version.
