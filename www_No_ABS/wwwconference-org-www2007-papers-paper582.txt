On July 29, 2006, AOL released over twenty million search queries from over 600K users, representing about 1.5% of AOL s search data from March, April, and May of 2006.
The data contained the query, session id, anonymized user id, and the rank and domain of the clicked result.
The media  eld day began almost immediately, with journalists competing to identify the most scandalous and revealing sessions in the data.
Nine days after the release, AOL issued an apology and called the release a  screw up,  removed the web site, and terminated a number of employees responsible for the decision, including the CTO.
There is great appetite to study query logs as a rich window into human intent, but as this vignette shows, the privacy concerns are broad and well-founded, and the pub-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
lic is rightly sensitive to potential breaches.
Academic researchers are enthusiastic about receiving anonymized data for research purposes, but to date, there is no satisfying framework for proving privacy properties of a query log anonymization scheme.
We do not have such a framework to propose.
Instead, we present a practical analysis of a natural anonymization scheme, and show that it may be broken to reveal information broadly considered to be highly sensitive.
The particular scheme we study is token-based hashing, in which each search string is tokenized, and each token is securely hashed into an identi er.
We show that serious leaks are possible in token-based hashing even when the order of the underlying tokens is hidden.
Our basic technique is the following.
We assume the attacker has access to a  reference  query log that has been released in its entirety, such as the AOL query log, or earlier logs released by Excite or Altavista.
We employ the reference query log to extract statistical properties of words in the log le.
We then process the anonymized log to invert the hash function based on co-occurrences of tokens within searches; interestingly, inverting cannot be done using just the token frequencies.
The technical matching algorithms we employ must provide good accuracy while being somewhat e cient to run on large query logs.
This turns out to be a nontrivial problem, and much of our time is spent describing and evaluating our approaches to address this e ciency issue.
Based on the mapping extracted between hashes in the anonymized query log and words in the reference query log, we perform a detailed evaluation of the potential for uncovering sensitive information from a log protected by token-based hashing.
Where possible, we incorporate publicly-available third-party information that would be available for an attacker.
We begin by focusing on person names, which are particularly sensitive due to the large number of  vanity queries  that occur in log les, in which a user searches for his or her own name.
We study extraction of these names using a hand-built name spotter seeded with a list of common  rst and last names, employing public data from the US census to help in the matching.
Surprisingly, we are aided in matching obscure names by the prevalence of queries for celebrities.
By matching the co-occurrence properties of  Tom Cruise  or  Jane Fonda,  we learn the hash values corresponding to the  rst names Tom and Jane.
From there, we will miss last names that are unique and unambiguous, but we will capture many other last names that occur in other contexts with characteristic co-occurrence properties.
city/state pairs), company names, adult searches, and revealing terms that would be highly sensitive if published.
Revealing terms include pornographic queries, as well as queries around such topics as murder, suicide, searches for employment, and the like.
We study the number of sessions containing a properly extracted relatively non-famous name and one of these other categories of terms.
We unearthed numerous sessions containing de-anonymized names of non-famous individuals along with queries for adult and revealing terms.
In the context of query logs released without session information, there are two primary risks.
First, there are many techniques to try to reestablish session links by analyzing query term and topic similarity.
Second, we also unearthed various de-anonymized queries containing both a reference to a non-famous person and a location.
There have been numerous approaches to de ning a framework capturing what is meant by  privacy.  In this work, we argue that attackers will naturally make use of signi cant amounts of domain knowledge, large external corpora, and highly tailored approaches.
In addition to work on frameworks and provable guarantees, usefully anonymized query logs will need to be scrutinized from the perspective of a sophisticated attacker before they can be released; at the very least, they should be proof against the attacks we describe.
That said, we should also note that our approach contains a key weakness.
Speci cally, it allows us to  nd only terms that exist in the reference query log, and that occur in the anonymized query log with su ciently rich co-occurrences.
Sequences of digits, such as street numbers, are very unlikely to be matched unless they occur in another context (such as a very famous address, the name of a song, common model number, or the like).
There is a large and thriving body of work on search log analysis, which has resulted in highly valuable insights in many di erent areas, including broad query characterization [17, 3, 14, 4], broad behavioral analysis of searches [4, 5, 19], deeper analysis of particular query formats [20, 21], query clustering [15], term caching [9], and query reformulation [6].
In every one of these cases, anonymization at the level of the query as provided by a hash of the entire search string would have made the analysis impossible.
In all cases but the query format analysis, token-based hashing would have allowed some interesting work, and in most cases, the entire research agenda would have been admissible.
Thus, there are many arguments in favor of token-based hashing as an approach to anonymization.
We present the  ip side of the coin, with an analysis of the dangers, and we conclude that signi cant privacy breaches would occur.
The best-studied framework for privacy preservation is k-anonymity, introduced by Samarati and Sweeney [16], and studied in a wide range of follow-on work (see for instance [2,
 of structured records.
A relation is mapped row-by-row to a new privacy-preserving relation, which is said to be k-anonymous if each set of potentially revealing values (for instance, the zip code, age, and gender of an individual) occurs at least k times.
The motivation behind the de nition is as follows: even if there are external sources that might allow mapping of such indirect data as zip code, age, and gender back to a particular individual, nonetheless, the new anonymized database will map back to at least k di erent individuals, providing some measure of privacy for su ciently large k. There are two concerns with this scheme in our world.
First, our setting is not naturally structured, so it is unclear how to extend k-anonymity; it is clearly not practical to make the entire session history of a user identical to that of k   1 other users.
In fact, it is not clear which parts of a query session should even be treated as values in a relation.
And second, revealing that somebody in a set of one hundred users is querying about techniques for suicide is already revealing too much information.
The related problems of text-based pseudonym discovery and stylometrics have been heavily studied; in these problems a body of text is available from a number of authors, and the goal is to determine which of these authors are identical.
See [11] and the references therein.
The problem of aligning hashes in one log  le with tokens in another also resembles previous work in statistical machine translation that automatically construct bilingual lexicon (dictionary) from parallel corpora (text in one language together with its translation in the other language).
If we look more closely, they are very di erent beyond the resemblance at the surface level.
Most notably, while work in bilingual lexicon construction in machine translation assumes sentence-level alignment in the parallel corpora, we do not have query-level alignment between the two log  les; furthermore, the two log  les are very far from being semantically equivalent.
There is a large body of work on log anonymization; see for instance [12, 18].
This problem is super cially related to ours, but the techniques used are very di erent.
The goal is to provide anonymity, but classical approaches focus on hiding the IP address, while later approaches propose developing application-dependent multiple levels of privacy for a much wider set of attributes.
Nonetheless, the problems that arise in our domain of mapping large numbers of words based on an enormous co-occurrence matrix do not arise in anonymization of network logs.
Our formulation of the problem is also somewhat related to the well-known problem of graph matching and graph isomorphism.
The di erence, however, is that our graphs are richer in terms of what they represent and so are more amenable to statistical techniques.
We begin with some notation, and a formal de nition of our problem.
We then give an overview of the dataset we will study.
With data in hand, we describe our family of algorithms and give performance results comparing them.
In the following section, we will turn to a discussion of the results themselves, and cover the privacy implications in more detail.
We begin with some notation.
Recall that we will employ an unhashed query log in order to generate statistics for our attack on the hashed query log.
Let QR be the raw (unhashed) query log and QA be the anonymized (hashed) query log.
For a query log Q (raw or anonymized), let term(Q) denote the set of all terms that occur in Q; in the case when Q in Q; let coocN (s, t, Q) = cooc(s, t, Q)/P is a raw query log, this will be the set of tokens and when Q is an anonymized query log, this will be the set of hashes.
Let freq(s, Q) denote the number of times the term s occurs in Q; t freq(t, Q) be its normalized version corresponding to the probability of s in log Q.
Let cooc(s, t, Q) denote the number of times s co-occurs with t t0 cooc(s, t0, Q) be its normalized version, representing the probability that a particular term co-occurring with s is in fact t. We drop Q whenever the query log is clear from the context.
Recall that our goal is to map the hashes of QA to the tokens of QR.
We will employ a bipartite graph to re ect the candidate mappings between hashes and tokens, as follows.
De ne a weighted bipartite graph G = (L, R, E) as a set of left nodes L, right nodes R, and edges e = ( , r)   E   L   R. By convention, we will always take L to be a set of hashes, and R to be a set of tokens.
Let w : E   R be a real-valued weight function on edges; w(e) will represent the quality of the map between the hash and the token connected by edge e.
Fix a vocabulary size n, and let Gn = (Ln, Rn, En) be a bipartite graph representing mappings between the most frequent n hashes in QA and the most frequent n tokens in QR.
Our goal is to map the hashes in Ln to tokens in Rn, taking into account freq( ) and cooc( ) information; in other words, we seek a bijective mapping   : Ln   Rn.
Accuracy and matchable sets.
We de ne the following performance metric of a mapping   for a vocabulary size n. Given L and R, let   : L   R   { } be the correct mapping of hashes to tokens, where the function takes   if the hash has no corresponding token on the right hand side.
Given a mapping   : L   R, the accuracy is de ned to be |{  |  ( ) =  ( )} |{  |  ( ) 6=  }| .
The denominator of this expression is the size of the match-able set, which is the set of hashes that can possibly be mapped to tokens.
This set imposes an upper bound on the performance of any mapping and therefore, accuracy measures the fraction of the matchable set obtained by  .
In our results, we specify the accuracy and wherever applicable, the size of matchable set.
High-level approach.
We use the following general framework to compute the mapping.
Our framework can be expressed in terms of how two generic functions, namely, InitialMapping and Up-dateMapping, are realized.
Algorithm ComputeMapping (QA, QR, n)     InitialMapping (QA, QR) While not done     UpdateMapping (QA, QR,  ) The function InitialMapping takes L, R along with the query logs and computes an initial candidate mapping   : L   R. The function UpdateMapping takes L, R, the query logs, and the current mapping, and outputs a new mapping.
Based on di erent realizations of these functions, we obtain di erent methods for computing the mapping.
Data.
We use log  les from Yahoo!
web search in our experiments.
For privacy reasons, these  les are carefully controlled and cannot be released for general study (especially under token-based hashing).
In general, we extract one set of queries to act as the raw log QR, and a distinct set of queries to act as the anonymized log QA.
We process the anonymized log  le by performing white-space tokenization, and applying a secure hash function to each token, producing hashes that we must now try to invert.
For all the experiments in this section, the query log  les consist of random samples of six-hour query logs from a week apart in May,
 Section 4 we will consider other log le pairs.
We study three approaches to selecting an initial mapping, as follows: Random.
Randomly assign each node in L to a unique node in R in a one-to-one fashion.
Frequency-based.
Order the hashes in L by freq( , QA) and the tokens in R by freq( , QR).
Then the i-th most frequent hash in L is mapped to the i-th most frequent token in R.
NodeStart.
This is a more complex technique that builds a simple  ve-element vector combining di erent types of information about a token or a hash.
All  ve elements of this vector can be computed on a completely hashed query log, and thus represent a  ngerprint of the style in which the token or hash appears in the log.
If a hash and a token have very di erent  ngerprints, then the hash is unlikely to have been computed from that token.
The  ve dimensions of the feature vector g(s) are:


 t cooc(s, t, Q).
in Q, divided by freq(s, Q).
t freqN (t, Q)   cooc(s, t, Q))/(P
 t cooc(s, t, Q)).
We compute this feature vector for each node in L and then normalize the values of each dimension to have mean
 malized feature vector for each node in R, where the nor-malizations are dependent on the other values of R. The distance between two nodes     L and r   R is simply the L1 distance between their vectors: |g( )   g(r)|.
The initial mapping is then computed by the score-based greedy, described in Section 3.3.1; for now, it su ces to assume that this method computes a mapping of hashes to tokens using the L1 distance we computed.
We evaluate all of these initial mappings in the context of a greedy UpdateMapping function described below in Section 3.3.
Figure 1 shows the results for each of the three InitialMap-ping functions just described, for various di erent iterations of the UpdateMapping function.
The  gure clearly shows (|matchable set| = 918) using di erent initial mappings.
that the accuracy is almost independent of the choice of initial mappings.
However, the sophisticated NodeStart mapping reaches the maximum accuracy quite quickly.
Moreover, the accuracy of the frequency-based mapping at iteration 0 hints that it is hopeless to just use the frequencies of the hashes and the tokens towards obtaining a mapping.
It is also interesting to observe the  S shaped curve corresponding to the random initial mapping; this suggests the presence of a threshold at which point the randomness in the initial mapping is slowly replaced by structure.
Note that the plateau at the end of the NodeStart curve does not re ect a stable mapping.
Although the accuracy stays the same starting from iteration two, portions of the mapping are still changing at each iteration.
All further experiments employ the NodeStart function.
This section describes various di erent approaches to updating the mapping.
However, to begin, we discuss the general problem of comparing various candidate tokens as mappings for a particular hash, based on information in the current mapping.
Figure 2 gives an example of the situation that may arise when computing the distance between a hash and a word.
We wish to evaluate the quality of the map between hash h and token w. The mapping has already mapped h1 to w2, and h3 to w3, so the distance computation should take this mapping into account: h and w share co-occurrences.
If later information becomes available about the other unmapped neighbors of h, the distance between h and w will need to be updated.
Distance measure.
The distance measure we adopt is the following.
We represent each node as a vector over |L| +|R| dimensions whose entries give the co-occurrence probabilities with the corresponding token or hash.
Tokens have nonzero weight only in dimensions corresponding to tokens.
Hashes begin with nonzero weight only in dimensions corresponding to hashes, but each time a hash h is mapped Figure 2: A candidate mapping between a hash and a token.
to a token w, all nonzero entries for h are migrated to w.
In Figure 2, for instance, hash h will have nonzero entries only for h2, w2, and w3.
Distance is then given by the L1 distance between the corresponding vectors.
Mapping-based distance.
Rather than actually perform this migration of nonzero entries, however, we simply de ne the distance in terms of the initial co-occurrences among hashes and among tokens, based on a mapping function  , as follows:

 d ( , r) = |coocN ( ,  
 , QA)   coocN (r,  ( 

 This idea falls within the general theme of identifying similar tokens through similar contexts.
For instance, based on this intuition, past work has explored word clustering [13] and paraphrase extraction [1] using natural language texts from a single language.
We di er from such previous work in that a mapping between hashes and tokens is involved in de ning the distributional similarity.
In addition, the kind of contexts at our disposal (co-occurring words within queries) can be very di erent from the kind of contexts available from proper, grammatical English sentences.1 We compare L1 measure against corresponding quantities for L2 and cosine measures, using the following method.
We pick n = 10, 000 and we choose a random sample of
 to either L1, L2, or cosine measures.
The fraction of times the closest token under the measure was indeed the correct token is shown in the table below.
Cosine
 This shows that L1 measure clearly dominates the other measures.
Note that this is in line with the result of Lee [8] who showed that L1 measure is preferred over L2 and cosine
 grained contexts via syntactic analysis of full-length sentences, we may be getting an approximation of the optimal context by using all other words appearing in the same query as the context for the target word.
After all, users are more likely to type in the  essential  words, which can be viewed as a  distilled  version of what the corresponding sentence would have been.
as our distance going forward.
We now turn to schemes for UpdateMapping.
We present four schemes, the  rst two based on a distance score, and the last two based on post-processing of the distance score to produce a ranked list of candidates for each hash and each token.
Table 1 shows the results.
The performance of score-based greedy is on par with the other three algorithms and since score-based greedy is simpler, we use this method going forward.
The methods presented in the previous section take quadratic
 We discuss two score-based methods   greedy and a method based on the minimum cost perfect matching.
Score-based greedy.
In the score-based greedy method, we consider all pairs     L, r   R and sort them by the distance d ( , r).
We then maintain the L R triples h , r, d ( , r)i on a heap.
At each step, we pick the triple h , r, di with the minimum d value from the heap, set the updated mapping  0( ) = r, and delete all elements in the heap of the form h , , i and h , r,  i.
The running time of this greedy method is O(n2 log n), where the running time is dominated by having to compute all the pairwise distances.
For the rest of the paper, greedy will always refer to the score-based greedy.
jective map  0 : L   R that minimizes P Minimum cost perfect matching.
Instead of constructing the mapping in a greedy way using scores, we can appeal to the minimum cost perfect matching formulation, applied to the bipartite graph with w( , r) = d ( , r).
Recall that in minimum cost perfect matching, the goal is to  nd a bi L,r R d ( , r).
Using standard algorithms, this problem can be solved in time O(n5/2) (see [7]).
The solution to this problem yields the updated mapping  0.
We discuss two rank-based method   greedy and a method based on the stable marriage problem.
Rank-based greedy.
In the rank-based greedy method, we use the rank information instead of the score information.
Formally, the function d ( , ) provides a ranking of all r   let the rank of r   R be rank (r).
R with respect to  ; Likewise, the function d ( , r) can be used to obtain the rank of     L with respect to r, denoted rankr( ).
Let d( , r) = rank (r) + rankr( ).
We now apply the score-based greedy method with the above distance function d( , ) to  nd the updated mapping  .
Stable marriage.
Recall the stable marriage problem.
We are given a bipartite graph consisting of men and women, where each man ranks all the women and each woman ranks all the men.
A marriage (bijective matching) of men and women is said to be unstable if there are two couples (m, w) and (m0, w0) such that m ranks w0 above w and w0 ranks m above m0.
Given the bipartite graph, the goal is to construct a marriage that is stable.
This problem can be solved in O(n2) time (see [7]).
In our case, the men correspond to L and the women correspond to R and as in the rank-based greedy case, the function d ( , ) provides a ranking of all r   R with respect to     L and the function d ( , r) provides a ranking of all     L with respect to r   R. Hence by applying the stable marriage algorithm, we can  nd the updated mapping  .
time to run.
For increasingly deep query logs, it is not possible to proceed without some modi cations for e ciency.
We describe a number of approaches here.
In this approach we use a  xed   : L   R to approximate the distance between a hash  0   L0   L and a token r0   R0   R. Let g0( ) be the NodeStart function for the larger vocabulary.
Let proximations
  (  ) =
 cooc






 be the mass of the co-occurrences covered by  .
Let

 , r  (  ) = min(cooc



 N (r ,  , QA), cooc
 ,  ( ), QR)) be the overlap between  0 and r0 within L. Let
 , r  ( 
 ) = 1    ( 0, r0)  ( 0) be an estimate of the distance between  0 and r0 as given by the terms in the size n vocabulary.
We then set the distance
  d ( 
 , r
 ) =  ( 

 ) + (1    ( 
 , r ))   |g0( 0)   g0(r0)
 , where C is set to the number of features (in our case, 5).
Note that if  ( 0) is bounded away from 0, then  ( 0, r0) is perhaps a good estimate of the actual distance and the  rst term dominates and if  ( 0) is close to 0, then g0( ) plays a heavier role.
We will report some experiments for this expansion after describing a pruning technique below.
We give two approaches to heuristic pruning that can dramatically reduce the number of candidate hash-token pairs that must be considered.
 pruning.
The  rst approach is to restrict the set of pairs ( , r)   L   R that are ever considered in all the Up-dateMapping methods.
For each  , we order the r s based on increasing values of |g( )   g(r)| and choose the top   fraction of r s in this ordering, for some   < 1.
Thus, the total number of pairs to be considered is now  n2.
 pruning.
In a similar spirit, for each s, we choose T 0   t0 T 0 coocN (s, t, Q)    , for some   < 1.
In other words, each s chooses the fewest t s such that these t s garner at least   mass of the co-occurrence distribution.
We do not explore the performance of  pruning further.
term(Q) such that |T 0| is minimal andP To postulate the e ect of pruning, we study how far the ranks of hashes and tokens migrate.
Let rank(s, Q) be the rank of s, when the terms in Q are ordered according to freq(s, Q).
Speci cally, for     L, we plot the distribution of (n)



 set



 score mincost greedy matching rank greedy marriage stable















 Table 1: Accuracy of score-based greedy, mincost matching, rank-based greedy, and stable-marriage algorithms.
|rank( , L)   rank( ( ), R)|, where   is the correct mapping.
Figure 3 plots this value for various buckets of values of rank( , L).
For example, an x-value of 200 corresponds to tokens with rank from 100 to 200.
And the y-axis shows the absolute distance between the token s rank in QR versus
 Thus,  pruning shows some impact on overall performance, but this cost may be acceptable at a 10X improvement in runtime.
Vocabulary expansion is capable of high accuracy, and is thus a promising technique for larger problems scales.
We employ this technique for larger runs in Section 4.
We now present two additional approaches to improving e ciency, each of which may be employed in either an exact setting or an approximate setting for greater e ciency.
The  rst is based on a heap structure for continuous update of the possible mappings, and the second is based on an inverted index.
We present these approaches, and have implemented them in our algorithms, but we leave a thorough performance evaluation for future work.
In the  rst proposal, we continuously enlarge the domain of   and use this to approximate the distance between a hash  0   L0 \ L and a token r0   R0 \ R. Initially we implicitly assume d0( 0, r0) = 1 for all the pairs.
We place the tuple h ,  ( ), d ( ,  ( ))i on a heap.
We then repeat the following until the heap is empty.
Let ( 0, r0, ) be the pair that has the smallest distance on the heap.
We set  ( 0) = r0.
Now, we go through all the  00   L0 \ L that co-occur with  0 and all the r00   R0 \ R that co-occur with r0 and update the estimated distance d0( 00, r00) using the new mapping information as
 d ( 
 , r )   min`cooc  cooc

 ,  





 ,  

 , QA)   cooc
 N (r

 , r
 If the h 00, r00, i exists in the heap, we update its distance by d0( 00, r00); otherwise, we insert the h 00, r00, d0( 00, r00)i into the heap.
In this section we propose a way to speed up the computations by using a reverse index.
We compute an index I on the tokens in R such that I(r) will return all the tokens that co-occur with r. Now, given current mapping   and an     L, we can quickly compute the distance to any r   R by using the following set: [ I( ( 
 )).
cooc( , 0,QA)>0 Figure 3: Rank migration.
We present a simple evaluation of the e ectiveness of  pruning and vocabulary expansion.
We study a 2K-word mapping problem using the most frequent terms of our query logs.
The size of the matchable set for this case is 1851, so we measure performance as number of correctly mapped hashes out of 1851.
We perform four experiments.
Exp1: Begin by mapping 1K nodes using NodeStart and
 ulary expansion with  pruning using   = 0.1 in order to map the remaining 1K nodes.
Exp2: Begin by mapping 1K nodes using NodeStart and
 ulary expansion with no  pruning in order to map the remaining 1K nodes.
Exp3: Begin by mapping 2K nodes using NodeStart, then perform a single iteration of greedy updates.
Exp4: Begin by mapping 2K nodes using NodeStart, then perform two iterations of greedy updates.
The success rates are as follows.
Experiment Accuracy







 If |S | (cid:28) |R|, then we gain.
Note however that if  0 is a high-frequent hash, then  ( 0) is a corresponding high-frequent token and so |S | could be large, rendering this whole method less attractive.
In this section we describe larger-scale experiments on our base query logs, then turn to an evaluation of the impact of varying the size of the query logs, and the distance in time between the capture of the raw and anonymized log  les.
In the following section, we move to a discussion of particular privacy breaches that are possible under token-based hashing.
In this section we employ matching algorithms that successively matches the most frequent 1K, 2K, 4K, 8K, and
 vocabulary expansion with a single greedy update at each expansion stage.
Table 2 shows basic data to characterize the information available to the mapping algorithm at these scales.
As the table shows, the 1000-th most frequent term appears around 1000 times; and for a sub-graph consisting of only the 1000 most frequent terms, the average degree is about 300 (i.e., on average each term co-occurs with 300 other terms in the top 1000).
As we move to 16K terms, the frequency of the least frequent term is 52 in the hashes and 63 in the tokens, so the total available information becomes sparser.
The results are shown in Figure 4, which shows for each depth the performance on the entire range, plus the performance on the top and bottom half of the range.
Table 3 gives the actual accuracy at each expansion increment.
We are able to perform the inversion with accuracy 99.5% for the  rst 1K hashes, dropping to 68% for all 16K hashes.
To give some insight into these results, it is possible to ask how many hashes, if the mapping   of all their co-occurring terms were perfect, would in fact have lowest distance to their correct match this may be seen as a di cult threshold for an algorithm to beat.
This is about 92% for 10K terms, compared with 83% for our algorithm at 8K terms, indicating that while there are still gains to be had, the matching is becoming quite di cult as the tail gets longer.
Figure 4: Accuracy of expanding the vocabulary with distance approximations.
n Accuracy












 Table 3: Accuracy for matching up to 16K terms/hashes.
We now turn to an examination of how variation in the raw and anonymized logs impacts the performance of the algorithms.
First a note on terminology.
For a query log Q, we use interval to denote the time di erence between the start and end time when the query log was collected.
For a raw query log and an anonymized query log, we use gap to denote the time di erence between the start time of the anonymized log and the start time of the raw log.
For some of the experiments presented in this section, we seeded the algorithm with a  bootstrap mapping  consisting of a small number of correct mappings in order to allow faster convergence; however, this mapping did not have a signi cant impact on overall accuracy.
Recall that gap refers to the time between the raw query log and the anonymized query log.
We took a random sample of 3 million queries from a six-hour interval of time for both the raw and anonymized query logs.
For e ciency, we use the heap-based continuous update method, with an initial bootstrap mapping of 100.
The vocabulary size was set to 1000.
We show results for matching 1K terms, for various values of the gap.
The results appear in Table 4, which shows non-monotonic behavior: we perform very well with a gap of one week compared to a gap of one day.
This might re ect some weekly periodicity in the query logs.
Gap Accuracy Matchable 1dy

 1wk

 1mo

 2mo

 Table 4: Accuracy with di erent gaps between the tokens (R) and the hashes (L).
The goal of this experiment is to measure the impact of the interval of a query log on accuracy; recall that by interval we mean the start and end times of the query logs.
We use the raw query log data starting May 17, 2006 and the anonymized query log data starting July 17, 2006.
We considered intervals of one hour, three hours, six hours, nine hours, one day, and one week intervals.
For each interval, we took a random sample of 3 million queries from the raw and anonymized query logs.
For e ciency, we use the heap-based continuous update method, with an initial bootstrap mapping of 100.
The vocabulary size was set to 1000.
The results are shown in Table 4.2.2.
The matchable set is quite high for di erent interval sizes implying a large overlap in the queries, irrespective of the interval size.
hash side statistics number of queries

 vocabulary size (n) average freq.
of the least degree freq.
term in graph average freq.
of the least degree freq.
term in graph
























 Table 2: Basic statistics of the data.
Dataset for vocabulary size n consists of the subsets of the query logs with only top-n (i.e., n most frequent) terms.
Interval Accuracy Matchable 1hr

 3hr

 6hr

 9hr

 12hr

 1dy

 1wk

 Table 5: Accuracy for di erent intervals between the start and end times for each query log.
In this section we perform an analysis of the breaches in privacy that may be revealed by the level of hash inversions we have shown to be possible in Section 3.
First we de- ne key categories of entities that (arguably) reveal privacy.
Next we consider the portion of the query log where the hash inversion makes almost no mistakes, and study occurrences of these privacy-relevant entities.
We selected key categories of privacy-relevant entity types that we spot in query strings: person names, company names, place names, adult terms, and revealing terms.
We de ne these  ve categories below, and describe how we performed the extraction.
i.
Person names.
We built a simple context-free name spotter suitable for use in short snippets of text based on  dictionary  lookup constrained by a number of handcrafted rules.
To begin with, we formed a set of potential names by pairing up all  rstnames and lastnames from the top 100K names published by the US census.
For a  rstname-lastname pair to be considered valid, it must satisfy at least one of the following three conditions: (1) The  rstname-lastname pair is present in a list of manually maintained true names.
(2) Either the  rstname or the lastname is absent from a small English word dictionary.
(3) The frequency of either the  rstname or the lastname in the census data is less than 0.006.
In addition, the  rstname-lastname pair must not be present in a manually maintained list of false names.
We performed manual evaluation over random samples to determine which query strings actually correspond to names to verify that names identi ed in query strings that satisfy the above conditions are indeed valid person names.
Not surprisingly, most occurrences of person names in query logs are famous people of one  avor or another.
We de- ne a subset of person names to be non-star names.
These are names that occur fewer than 10 times in the log; we chose the threshold by hand based on the point at which few famous names appeared.
ii.
Company names.
We employed the Forbes top 1000 public companies, and top 300 private companies, plus a number of abbreviations added by hand.
We perform case-insensitive matching of these company names to the log.
Any queries ending in  inc  or  corp  are also tagged as relevant to companies.
iii.
Places.
We gathered the names of all US states, and their abbreviations (with the exception of OR).
A word followed by a US state or followed by  city  or  county  is considered to be a place name if it occurs in the dictionary capitalized, or doesn t occur in the dictionary.
iv.
Adult terms.
These are gathered by scanning the top 2K most popular terms in the query log, and manually annotating those that are clearly adult searches.
We selected 14 adult terms.
In a log of 3.51M queries, adult terms occur 71418 times, covering about 2% of all queries.
v. Revealing terms.
These are terms that are not adult per se, but nonetheless have implications for privacy, if they were revealed for instance to an employer or a spouse.
We selected 12 revealing words for this study: career, surgery, cheats, lesbian, disease, hospital, jobs, pregnancy, medical, cheat, gay, and cancer.
In a log of 3.51M queries, revealing terms occur 37593 times, covering about 1% of all queries.
Our previous analysis, using query logs without session information, showed that the  rst 1K most frequent terms of a query log can be mapped with accuracy over 99% on the matchable set.
We assume this carries over to a di erent query log with session information.
In the top 1K most frequent terms in this query log, we  nd 1839 person names,
 within a session, we  nd the following within-session results.
The  rst column in Table 6 gives the number of sessions that contain an entity or a combination of entities speci ed in the second column.
From the table, it is evident that even the relevant information.
Session count Entity type







 person name company name place name non-star name non-star name and a place name non-star name and a company name non-star name and an adult term non-star name and a revealing term Table 6: Number of sessions with privacy-relevant entities in top 1K terms of the query log with session information.
If the anonymized log le does not include session information, there are still potentially privacy-revealing queries.
Using the mapping of the top 8K terms achieved by our algorithm, we  nd within correctly mapped individual queries the following occurrences of potential privacy breaches.
Table 7 shows the number of distinct entities.
Count Entity type




 name place company query with name and place query with non-star name and place Table 7: Number of distinct privacy-relevant entities using top 8K terms of the query log without session information.
Finally, we found a number of mistakes made by our algorithm, which give insights into the di cult cases, as well as the types of co-occurrences that are common in query logs.
Some example mismatches are shown below.
Not surprisingly, since we seek to map hashes into words with similar contexts, quite a number of hashes are (reasonably) mapped into synonyms or paraphrases of the original tokens, as well as related concepts that tend to appear in similar contexts.
Since these types of mismatches are semantically equivalent or related to the correct matches, they may still be very e ective in incurring privacy breaches.
Not all mismatches remain  helpful  in this way.
With limited amount of data and noise incurred by the non-overlapping part of the vocabulary, some of the hash-token pairs may never get correctly mapped and remain as misleading contexts for other pairs.
Thus, it is not surprising that we also have inexplicable mismatches where there are no obvious semantic relations between the two words.
Synonyms.
retreat getaway furnace   replace pill  supplement pics   photos Terms used in similar contexts.
celine  elvis may  april positive  negative heel  toe pilates  abdominal wants   millionaire avis   hertz Unexplained.
killer   crack origami   biodiesel suicide   geometry

 In this paper we studied the natural token-based hashing in which each search string is tokenized, and each token is securely hashed into an identi er to create an anonymous query log.
We show that serious leaks are possible whether the identi ers are presented in the same order as the underlying tokens, or whether the order is hidden.
We thus show that user concerns around privacy are very real at least in the case of token-based hashing.
Future work includes expanding the scope and applicability of our algorithms to make them work for large values of n.
