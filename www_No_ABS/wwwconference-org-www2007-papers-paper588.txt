The increased use of content-management systems to generate webpages has signi cantly enriched the browsing experience of end users; the multitude of site navigation links, sidebars, copyright notices, and timestamps provide easy-to-access and often useful information to the users.
From an objective standpoint, however, these  template  structures pollute the content by digressing from the main topic of discourse of the webpage.
Furthermore, they can cripple the performance of many modules of search engines, including the index, ranking function, summarization, duplicate detection, etc.
With templated content currently constituting more than half of all HTML on the web and growing steadily [3, 11], it is imperative that search engines develop scalable tools and techniques to reliably detect templates on a webpage.
Most existing methods for template detection operate on a per website basis by analyzing several webpages from the  Most of the work was done while the author was visiting Yahoo!
Research.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
site and identifying content and/or structure that repeats across many pages.
While these  site-level  template detection methods o er a lot of promise, they are of limited use because of the following two reasons.
First, site-level templates constitute only a small fraction of all templates on the web.
For instance, page and session-speci c navigation aids such as  Also bought  lists, ads, etc.
are not captured by the site-level notion of templates.
Second, these methods are error prone when the number of pages analyzed from a site is statistically insigni cant, either because the site is small, or because a large fraction of the site is yet to be crawled.
In particular, they are totally inapplicable when pages from a new website are encountered for the  rst time.
An alternative paradigm that avoids many of these pitfalls is to detect templates on per webpage basis, i.e.,  page-level  template detection.
This is especially attractive since it can be easily deployed as a drop-in module in existing crawler work ows.
A tempting approach to page-level template detection is to extract su ciently rich features from the DOM nodes and train a classi er to assign  templateness  scores to each node in a DOM tree.
While this approach is entirely plausible, it has several handicaps.
First, for the classi er to have a reasonable performance, both accurate and comprehensive training data is required; this can involve prohibitive human e ort.
Second, by classifying each DOM node in isolation this approach does not take a global view of the templateness of nodes in the DOM tree.
In this paper we develop a novel framework for page-level template detection.
Main contributions.
Our  rst contribution is a method to automatically build a page-level templateness classi er.
This method works as follows.
First, we generate training data by applying the site-level template detection method on several randomly selected sites.
Next, we de ne and extract appropriate features for these site-level templates.
Finally, we use this automatically generated data to train a classi- er that can assign a templateness score to every node in a DOM tree.
We show that our classi er generalizes beyond its site-level training data and can also discover templates that manifest only at the page-level.
Our second contribution is the formulation of a global property that relates templateness scores across nodes of the DOM tree.
We assert that templateness is a monotone property: a node in the DOM tree is a template if and only if all its children are templates.
An appropriate relaxation of this property leads to the following regularized isotonic regression problem: given a tree with classi er scores at each si er scores, but satisfy the relaxed monotonicity property.
We provide an e cient algorithm to optimally solve this problem; this algorithm may be of independent interest.
On the whole, our approach eliminates the aforementioned issues with pure classi er-based approaches.
An interesting byproduct of our framework is that we obtain a sectioning of a page into segments; this is useful in many applications.
We perform an extensive set of experiments to validate our framework and algorithm.
In terms of detecting content within templates, our algorithm achieves an f measure in excess of 0.65 for text and 0.75 for links on a human-labeled test set.
We highlight the applications of template detection by showing that removing templates as a pre-processing step boosts the accuracy of standard web mining tasks on our datasets, by as much as 140% on duplicate detection, and
 gains obtained by using our page-level template detection approach are substantially greater than those obtained by using the more expensive site-level approach.
Our work in this paper is related to two broad areas of research: template detection and isotonic regression.
Site-level template detection.
The problem of template detection and removal was  rst studied by Bar-Yossef and Rajagopalan [3], who proposed a technique based on segmentation of the DOM tree, followed by the selection of certain segments as candidate templates depending on their content.
Yi et al. [26] and Yi and Liu [25] used a data structure called the style tree to take into account the metadata for each node, instead of its content.
Vieira et al. [24] framed the template detection problem as a problem of mapping identical nodes and subtrees in the DOM trees of two different pages.
They proposed performing the expensive task of template detection on a small number of pages, and then removing all instances of these templates from the entire site by a much cheaper approach.
Gibson et al. [11] conducted a detailed study of templates on the web, demonstrating the prevalence of templated content and its steady growth.
All of these methods, however, need multiple pages from the same website to perform template detection, and thus su er from the problems mentioned in the introduction.
Our page-level algorithm can use the site-level templates detected by these methods as training data, generalizing the concept of a template beyond what is found by these algorithms.
Page-level template detection.
Some page-level algorithms have also been proposed recently.
Kao et al. [14] segment a given webpage using a greedy algorithm operating on features derived from the page.
However, their method is not completely page-level; they also use some site-level features such as the number of links between pages on a website.
Debnath et al. [10] also propose a page-level algorithm ( L-Extractor ) that applies a classi er to DOM nodes (as in our algorithm), but only certain nodes are chosen for clas-si cation, based on a prede ned set of tags.
Kao et al. [13] propose a scheme based on information entropy to focus on the links and pages that are most information-rich, reducing the weights of template material as a byproduct.
Song et al. [22] use visual layout features of the webpage to segment it into blocks which are then judged on their salience and quality.
Other local algorithms based on machine learning have been proposed to remove certain types of template material.
Davison [9] uses decision tree learning to detect and remove  nepotistic  links, and Kushmerick [16] develops a browsing assistant that learns to automatically removes banner advertisements from pages.
Another set of papers focus only on segmentation of webpages for the purpose of displaying them on small mobile device screens [2, 7,
 While similar in spirit to our page-level template detection system, these algorithms are signi cantly di erent in the details.
Only a subset of DOM nodes ( segments ) are operated upon, this subset having been chosen prior to any determination of the templateness of those segments.
As a result, should a segment itself be composed of several template and non-template nodes, this would not be detected.
Our algorithm operates on each node in the DOM tree, and  nds segments based on the results of a templateness clas-si er; thus, it avoids these problems.
Isotonic regression.
The problem of isotonic regression crops up in a range of disciplines, from microarray data analysis [1] to epidemiology [19] and statistics [21].
Stout [23] showed that the optimal solution for complete orders can be computed in O(n log n) for L1, and O(n) time for L2 distance metrics.
Pardalos and Xue [20] gave an O(n log n) algorithm for L2 isotonic regression on rooted trees.
An-gelov et al. [1] recently presented an O(n2 log n) algorithm for L1 on trees.
We propose a general version of the isotonic regression problem on trees that subsumes the problems mentioned above.
For the L1 distance metric we provide an exact algorithm to solve this general problem in O(n2 log n) time, matching the best results on the special cases studied in the results cited above.
In this section we describe the proposed framework for the page-level template detection problem.
We  rst  x some notation.
Recall the DOM tree representation of an HTML document where each node in the DOM tree corresponds to an HTML fragment; we identify the DOM node with the fragment it represents.
Let T be the rooted DOM tree corresponding to the document.
From here onwards, we use the tree T as a metaphor for the document.
Let templ(T ) denote the set of all nodes in T that are templates.
We use i   T to denote that node i belongs to tree T , parent(i) to denote the parent of i in T , child(i) to denote the set of children of i in T , and root(T ) to denote the root of T .
Let H denote the set of all possible DOM nodes.
In the page-level template detection problem, we seek a boolean function   : H   {0, 1} such that   (i) = 1 for all i   templ(T ), and   (i) = 0 otherwise.
In a relaxed version of the problem, we seek a function   : H   [0, 1] where if i   templ(T ) and j /  templ(T ), then   (i) >   (j); using an appropriate threshold, we can round   to make it boolean.
A  rst-cut approach to page-level template detection would be to extract su ciently rich features from the DOM nodes (in the context of a page) and train a classi er x : H   [0, 1] to score the  templateness  of each node in a given page.
While this appears plausible, it has several issues when scrutinized closely.
The  rst set of issues revolve around the con-si er to learn the notion of  templateness  of DOM nodes on the web in general, it must be trained comprehensively over all forms of templates that it is likely to encounter.
The heterogeneity and scale of the web imply that a huge corpus of accurate and diverse training data will be required.
These requirements present a daunting task that demands tremendous human e ort.
Secondly, this approach to clas-si cation ignores the global property of templateness in the DOM tree, crisply stated as follows.
Property 1 (Templateness Monotonicity).
A node in the tree is a template if and only if all its children are templates.
In other words, the function   ( ) is monotone on the tree.
As is apparent, by working on each node of T in isolation, a naive classi er misses this intuitive relationship among templateness of nodes in the tree.
Our three step framework is meant precisely to address these issues, and is described below.
step is the automatic generation of training data.
To this end, we use the site-level template detection paradigm of [11].
Note that even though site-level template detection is less feasible as a web-scale template detection mechanism, we show that we can still use it to generate training data for our approach.
The basic intuition behind the site-level template detection approach is the following.
One of the common properties of templates is that they occur repeatedly across many pages on a single site.
Therefore, if a DOM node occurs many times on di erent pages from a single site, then it lends credible evidence that this DOM node perhaps corresponds to a template.
We now describe a generic algorithm that we will call SiteLevel ( ).
This algorithm operates on a site by site basis.
For each site, it obtains a set T of random pages from the site.
Then, for each page T   T and for every DOM node i   T , it computes h(i), where h( ) is a random hash     H be the set of DOM nodes that occur function.
Let I + on at least   fraction of pages in T .
Note that using hashes, this set can be identi ed e ciently.
SiteLevel returns I +   as the set of DOM nodes deemed templates.
identi ed by SiteLevel as training data DOM nodes I +   for a classi er.
For this, we  rst identify appropriate features of DOM nodes in I +   , in the context of the pages they appear in.
We then train a classi er x : H   [0, 1] using these features of the DOM nodes, treating those in I +   as positive examples; the output of the classi er is a template-ness score for a given DOM node in a tree.
The hope in using a classi er is that it can distill features from site-level templates that can be generalized to all templates on the web.
This can help us identify templates that don t manifest themselves by repeatedly occurring across multiple pages on a website   templates that a pure site-level template detection approach cannot discover by itself.
As we empirically observe in Section 6.2, this is indeed what happens.
classi er to assign a templateness score x( ) to each DOM node in the given page T .
However, as we argued earlier, this does not fully capture the essence of the problem since the templateness scores assigned by the classi er to each DOM node in isolation may not satisfy the property that a node is a template if and only if all its children are templates (Property 1).
On the other hand, assuming the classi er has reasonable accuracy, the scores it assigns makes sense for most, if not for all, of the nodes.
The question now is how to reconcile the score assigned by the classi er with the monotonicity property of the templates.
To handle this question, we  rst consider a natural generalization of the monotonicity property for the case of real valued templateness scores.
Suppose y(i) is the templateness score of a node i in the tree.
Then, y( ) is said to satisfy generalized templateness monotonicity if for every internal node i, with children j1, .
.
.
, j , y(i) = min{y(j1), .
.
.
, y(j )}, i.e., the templateness of an internal node is the equal to the least of its children s templateness scores.
Note that generalized monotonicity ensures,  rst, that the templateness score of a node is at least the templateness score of its parent, and second, that the templateness score of the parent equals the templateness score of all its children, when the children all have same templateness score.
We also have an additional requirement that the templateness score y( ) be close to the x( ) scores assigned by the classi- er.
Generalized monotonicity together with this closeness requirement leads to the problem of generalized isotonic regression on trees, which we solve in this paper.
While we defer the detailed description of our solution to the next section, we now highlight the advantages of our framework.
Besides addressing the issues with using just the classi er scores, our framework o ers additional bene ts.
First, the overall framework in simple and modular.
Second, any o -the-shelf classi er can be used, instead of having to design one that works speci cally for the given DOM tree structure.
Third, as we will see later, a neat byproduct of isotonic smoothing is that we obtain a sectioning of a page into segments; this can be useful in many applications.
In this section we formulate and solve the generalized iso-tonic regression problem on trees.
Recall that we are given as input a DOM tree with each node labeled by a score assigned by the classi er.
The purpose of isotonic regression is to  x these scores so that they satisfy the monotonicity constraints, while remaining as faithful as possible to the original classi er scores.
Let x(i) be the classi er score for each node i   T and let y(i) be the smoothed score we wish to obtain.
The  rst step in our formulation is to alter the generalized monotonicity property in two ways.
First, we only ensure that the templateness score of a node is at most the least of its children s scores, instead of equal to it.
This relaxation is derived from the current domain in which the cost of mis-classifying a non-template as a template is much higher than vice versa.
Hence, if according to the classi er an internal node s template score is much lower than that of all of its children, then we would want to respect that.
Second, we introduce a regularization that penalizes if, for a node i, the templateness score y(i) is di erent from those of its children y(j1), .
.
.
, y(jk).
Clearly, if y(j1) =   = y(jk), then this regularization will try to ensure that y(i) = y(j1).
Thus, we have   min{y(j1), .
.
.
, y(j )}.
For purposes of regularization, we develop the notion of compressed score that embodies sectioning of the DOM tree into subtrees.
A compressed score is a function  y : T   [0, 1]   { } with the following properties:  y(root(T )) 6=  , and (2) if i is an ancestor of j and  y(i) 6=   6=  y(j), then (3)  y(i) <  y(j).
Let the size | y| of the compressed score be the number of places where  y is de ned; | y| = |{i | i   T,  y(i) 6=  }|.
For all i   T such that  y(i) =  , let anc(i) be the closest ancestor of i such that  y(anc(i)) 6=  ; note that such an ancestor always exists by (2).
We now interpolate  y to a unique y as follows.
   y(anc(i))  y(i) y(i) = if  y(i) =   otherwise It is clear that if  y satis es (2) and (3), then the corresponding interpolated y satis es (1).
Also, given a y satisfying (1), it is easy to construct the unique  y.
From now on, we use the smoothed score y and its compressed counterpart  y interchangeably.
Finally, the cost of a smoothed score y with respect to x is de ned as c(y) =     | y| + d(x, y), (4) where   is a penalty term that captures the cost of each new smoothed score and d( , ) is some distance function.
It is also possible to have a node-speci c penalty term  i for node i; for simplicity of exposition, we state the algorithm in terms of a node-independent term  .
This cost function and the tree structure lead to a regularized version of the isotonic regression problem.
Problem 2 (Regularized Tree Isotonic Regression).
Given a tree T and x : T   [0, 1],  nd y : T   [0, 1] that satis es (1) and minimizes c(y) as given by (4).
1 For the rest of the paper, we take d( , ) to be the L1 norm since it is robust against outliers.
Before presenting the algorithm we discuss a key property of the L1 distance measure that aids us in designing an e -cient algorithm for this problem.
We show that the optimal smoothed scores in y can only come from the classi er scores in x.
Lemma 3.
There exists an optimal solution,  y, where, for all i   T , if  y(i) 6=  , then there is a j   T such that  y(i) = x(j).
Proof.
Consider the maximal connected subtree T 0 of nodes in T such that (1) i   T 0, and (2) for all j   T 0, y(j) =  y(i).
If  y(i) is not the median of the set of scores {x(j) | j   T 0}, then we can push  y(i) closer to the median by a small amount and decrease the cost of the solution given by (4); this follows since the median is the minimizer for L1 distance.
ered before in statistics and computer science contexts; it is usually referred to as the isotonic regression problem: given ~x = x1, .
.
.
, xn,  nd ~y = (y1, .
.
.
, yn) such that y1       yn and d(~x, ~y) is minimized, where d( , ) is some distance function.
It is easy to extend this de nition to the case when the yi s have to respect a given partial order, say, imposed by a tree.
We build a dynamic program using the above result to obtain an algorithm for the regularized tree isotonic regression problem.
Algorithm BuildError builds up an index function val(i, j) and an error function err(i, j) for each node i   T .
The value err(i, j) represents the cost of the optimal smoothed scores in the subtree rooted at i if its parent node has the smoothed score y(parent(i)) = x(j).
In this situation, the index val(i, j) is such that the optimal smoothed score for node i is given by y(i) = x(val(i, j)).
If val(i, j) is the same as j, i.e, the optimal value for i and parent(i) are the same x(j), then the only cost is the L1 distance between x(i) and x(val(i, j)), otherwise there is an additional   cost as well.
The algorithm computes this error function by  rst computing errors as if the additional   cost must always be added; this intermediate result is stored in the err0 array, where err0(j) is the error if the node under consideration has the smoothed score y(i) = x(j).
Then, it chooses between (a) continuing with the parent s value and subtracting   from the corresponding cost err0, or (b) creating a new section with a new value and paying in full the corresponding cost in err0.
Once all error functions have been computed, the optimal smoothed scores are obtained using Algorithm IsotoneSmooth, which starts with the best index p(root(T )) at the root, and progressively  nds the best index p( ) for nodes lower down in the tree.
Algorithm BuildError (i, x,  ) if (i is a leaf) then
 /* all values node i can take */ if (x(i)   x(j) >  ) then err(i, j) =  ; val(i, j) = i else err(i, j) = |x(i)   x(j)|; val(i, j) = j else 2. for child u of node i
 BuildError(u, x,  ) err0(j) = |x(i)   x(j)| +P /* all values node i can take */ k child(i) err(k, j) +  
 /* all values node parent(i) can take */ val  = argmink T,x(k)>x(j) err0(k) err  = err0(val ) if (err0(j)       err  or i = root(T )) then err(i, j) = err ; val(i, j) = val  else err(i, j) = err0(j)    ; val(i, j) = j Algorithm IsotoneSmooth (err, val) val  = argmini T p(root(T )) = val ; y(root(T )) = x(val ) for i in a breadth rst search order of T err(root(T ), i) p(i) = val(i, p(parent(i))); y(i) = x(p(i)) To demonstrate the correctness of this algorithm, we show that the restriction of the optimal solution to a subtree is also the optimal solution for the subtree under the mono-tonicity constraint imposed by its parent.
Consider the subtree rooted at any non-root node i   T .
Now suppose the smoothed score y(parent(i)) is speci ed.
Then, let z( ) be the smoothed scores of the optimal solution to the regularized tree isotonic regression problem for this subtree, under the additional constraint that z(i)   y(parent(i)).
Proof.
Consider a smoothed solution w( ) where w(j) = z(j) for all nodes j in the subtree of i, and w(j) = y(j) otherwise.
Since z( ) obeys the monotonicity property and z(i)   y(parent(i)), the solution w( ) obeys the monotonic-ity property.
Now, the cost c(w) is the sum of the cost for the smoothed scores z(j) in the subtree of i and the cost for the scores y(k) for all other nodes.
Thus, the di erence between c(w) and c(y) is just the di erence in costs for z(j) and y(j) in the subtree of i, for which we know that z( ) is the optimal.
The lemma follows.
Theorem 5.
Algorithm IsotoneSmooth solves the regularized tree isotonic regression problem.
Proof.
The algorithm computes up the optimal smoothed scores for each subtree, i.e., the err( , ) arrays, while maintaining (1) for every possible smoothed score of the parent.
By Lemma 3, the parent can take only  nitely many smoothed scores in the optimal solution, and by Lemma 4, combining the optimal smoothed scores for subtrees yields the optimal smoothed scores for the entire tree.
Complexity.
Let |T| = n. The space required per node is O(n), and so the total space required is O(n2).
Next, we consider the running time of the algorithm.
In the dynamic program, step 1 takes O(n2) time, step 3 takes O(n2) time amortized over all calls, and step 4 can be done in O(n2 log n) time by storing err0 values in a heap and then running over the nodes j   T in ascending order of x(j).
Hence, the total running time is O(n2 log n).
This matches the time complexity of previously known algorithms for the non-regularized forms of tree isotonic regression [1].
In this section we describe the details of both the classi cation and smoothing aspects of our system.
As mentioned before, we used a site-level template detection algorithm to generate training data for our classi- er.
The construction of training data involved two distinct steps: collecting webpages and obtaining labeled templates.
We sampled 3, 700 websites from the Yahoo!
search engine index such that each website had at least 100 webpages.
We also biased the sampling process slightly towards picking good quality host domains, and avoided picking pornographic or spam websites.
Then, for each website we downloaded at most 200 randomly picked webpages.
All DOM nodes that occurred on more than 10% of the pages of any website were tagged as site-level templates.
Since we wanted to learn a classi er for all internal DOM nodes we wanted representative labeled data from all levels of the DOM trees.
Hence, for each internal node we computed how much of its HTML was part of a site-level template.
DOM nodes with more than 85% of their HTML content within site-level templates were also labeled as templates.
The rest of the DOM nodes were used as instances of the non-templates class.
Note that the condition required for tagging a node as template is very strong.
This is done intentionally for two reasons.
First, recall that a node is a non-template if any node in its subtree is a non-template.
And second, the cost of misclassifying a non-template as a template is much higher than that of the reverse error.
There are multiple steps involved in learning the classi er.
Each of these steps is described below.
Preprocessing.
Each webpage is preprocessed and parsed so that features can be extracted for its DOM nodes.
The preprocessing step involves cleaning the HTML code using Hypar2, annotating the DOM nodes with position and area information using Mozilla3, and parsing the HTML to obtain a DOM tree structure.
The text in the HTML page is also processed to remove stop words.
Feature extraction.
The training data that we employ for learning corresponds to site-level templates.
However, we want our classi er to generalize to the global de nition of templates.
This makes the process of feature extraction very critical.
From each DOM node, we extract features that we believe are indicative of whether or not that DOM node is a template.
For example, intuitively, if the text within a DOM node shares a lot of words with the title of the webpage, then perhaps it is not a template node.
Similarly, the distance of a DOM node from the center for the page indicates its importance to the main purpose of the page, and hence its templateness.
In a similar fashion, we constructed several other features from the position and area annotations of DOM nodes as well as from the text, links, anchortext they contain.
The most discriminative features turned our to be: closeness to the margins of the webpage, number of links per word, fraction of text within anchors, the size of the anchors, fraction of links that are intra-site, and the ratio of visible characters to HTML content.
Classi er training.
We trained Logistic regression classi- ers [18] over the set of features described above.
Apart from performing very well, these classi ers have the additional bene t that their classi cation output can be interpreted as the probability of belonging to the predicted class.
In our exploratory experiments we observed that distributions of feature values varied heavily depending on the area of the DOM node.
This is because template and non-template nodes have very di erent characteristics at di erent levels of the DOM trees; these levels can be approximated by the area of the node.
Hence, we trained four logistic regression models for DOM nodes of di erent sizes.
Now, given a webpage, the appropriate logistic model is applied to each node of the DOM tree, and the output probabilities are fed to our post-classi cation smoothing function.
The smoothing algorithm allows arbitrary choices of penalty values for each tree node.
However, in the domain of template detection, there are several desiderata that a good penalty function must try to achieve.
We list these below, along with the particular functions that we considered, and the one that we  nally settled upon.
Desiderata for penalties.
There are three main desiderata for a smoothing algorithm in the context of template 2www.cse.iitb.ac.in/~soumen/download 3www.mozilla.org not form segments of their own.
Such nodes have very little content, and their classi cation scores are unreliable.
Also, having such small segments impairs the applicability of webpage segmentation to page visualization and browsing.
Second, adding nodes as segments should be easier as we move up from leaves to the root.
The smoothed values assigned to nodes high up in the tree impose constraints on the possible values in their entire subtree.
If creation of new segments is hard, such nodes may merge with other nodes to form larger segments whose smoothed scores may be drawn too far away from the classi cation score of the node itself, thus hurting all nodes in their subtree.
Third, if a child node accounts for a large fraction of the area of its parent node, then it should be harder to set the child to a value di erent from that of its parent.
This encourages the smoothing algorithm to form large sections without too much nesting, which agrees with our intuitions about how webpage segments are created.
Handling very small nodes.
All nodes whose area is less than 2000 sq pixels are neither classi ed nor smoothed; they are  hidden,  and their e ect is rolled into their parent node.
Thus, a node with k hidden children acts as if it were (k + 1) nodes, all with the same classi cation value.
This reduces to multiplying the distance measure d( , ) with (k +1), and the smoothing algorithm can handle it trivially.
This heuristic goes some way in achieving the  rst desideratum.
Penalty functions.
We experimented with several penalty functions, attempting to achieve the aforementioned desiderata.
Starting with a user-de ned constant c, several transformations for  i (penalty for node i) were tried: (1)  i = c   N/Ni, where Ni is the number of nodes in the subtree rooted at i, and N is the total number of nodes in the DOM tree.
This penalty is high for nodes near the leaves and low for nodes near the root, satisfying the  rst and second desiderata.
(2)  i = c   A/Ai, where Ai is the area of node i, and A the area of the whole HTML page.
Again, this penalty satis es the  rst and second desiderata.
(3)  i = c   Aparent(i)/Ai, where Aparent(i) is the area of the parent of node i.
This tries to achieve the third desideratum.
We tried all combinations of these penalties, over the a large range of the constant c, and visually inspected the results of smoothing on a few webpages.
We  nally settled on setting  i = 0.01  A/Ai, which gave the best results.
For the rest of this paper, unless speci ed otherwise, the penalty is always set to this function.
We now present an empirical evaluation of our system, called PageLevel.
Using human-labeled data, we show in Section 6.1 that our approach is very e ective in detecting the template sections of webpages.
Then, in Sections 6.2 and 6.3 we show that applying template detection as a pre-processing step signi cantly improves accuracy on standard web mining tasks such as duplicate webpage detection and webpage classi cation.
We also show that template removal using PageLevel provides more bene ts than using the more expensive SiteLevel approach.
Dataset Common Random Text
 Links Text
 Links PageLevel PageLevel Basic





 Smooth





 Table 1: Accuracy of PageLevel on Common and Random datasets in terms of f measure.
The desiderata for a template detection system are as follows: (a) it must divide the webpage into segments separating template and non-template content; and (b) it must accurately identify the webpage segments as template or non-template.
In this section we show that our system, PageLevel, achieves both these objectives.
Datasets.
In order to evaluate the template detection performance of PageLevel we manually created two labeled datasets.
Common.
We selected and manually labeled 44 pages from websites that were commonly visited by the authors.
The selected pages come from a diverse set of domains, such as, news websites like NYTimes and CNN, university websites like UTexas-Austin, etc.
For each webpage, the manual labeling process identi ed the largest possible HTML fragments that were either entirely template or non-template.
These HTML fragments correspond to nodes in the webpage DOM tree.
Hence, for each webpage we labeled an antichain of nodes through the DOM tree, forming an exhaustive and disjoint cover of all leaf nodes.
Random.
In order to evaluate the algorithms on webpages more representative of the general Web, we manually labeled
 directory4.
The selected set of webpages is a mix of topically focused content or hub pages and entry points to larger websites.
As was done for the Common dataset, the labeling process identi ed for each webpage an antichain of DOM nodes and marked each node as either template or non-template.
Template detection accuracy.
As we demonstrate later in this section, text and links present within the template regions mislead standard web-mining algorithms for tasks such as duplicate detection and automated classi cation.
Here we measure the e cacy of PageLevel in identifying text and links that occur within templates.
We report accuracy in terms of f measure, which is the harmonic mean of precision (p) and recall (r); i.e. f = 2pr/(p + r).
In the current setting, precision is the fraction of words (links) identi ed by PageLevel as occurring within templates that are also manually placed within templates.
Recall is the fraction of all words (links) manually labeled as lying within template regions that PageLevel also correctly identi es as templates.
This evaluation setting has previously been used by Vieira et al. [24].
In Table 1, we present the accuracy numbers (in terms of f measure) achieved by PageLevel for the two datasets: 4www.dmoz.org Basic, PageLevel Basic+Merge, and PageLevel Smooth.
Figure 2: Variation in template detection accuracy on the Common dataset with changing values of penalty.
The x-axis represents the factor being multiplied into the penalty.
Common and Random.
We present accuracies for two variations of our approach: PageLevel Basic only applies the classi er to the DOM nodes individually, while PageLevel Smooth, in addition also performs Isotonic smoothing on the templateness scores.
The performance is measured along multiple dimensions: Text, Anchor-text (AT), and Links.
As is clear from the table, our approach is very e ective in identifying all types of page content that lies within template regions.
Furthermore, smoothing is shown to signi -cantly improve accuracy across all dimensions of evaluation, in some cases by almost as much as 10%.
An interesting observation is that the accuracy over the Common dataset is slightly lower than that over Random.
This is because the template structure in webpages in Common is more extensive than those in Random.
Further, note that the gains a orded by the isotonic smoothing are larger on the more di cult of the two datasets.
Segmentation accuracy.
As we mentioned in Section 4, a byproduct of the isotonic smoothing algorithm is a segmentation of the page into DOM nodes that act as the roots of the template and non-templates regions.
Here, we show that the segmentation found by PageLevel closely matches the manually labeled segments.
Notice that the manual segmentation, an antichain of nodes of the DOM tree, induces a grouping of the leaves in which each node in the segmentation de nes a group.
A leaf then belong to the group corresponding to the segment node that covers it.
Similarly, the segmentation output by PageLevel, even though it allows for nested segments, also induces a grouping of leaves.
Each leaf can be considered as belonging to the group corresponding to its closest ancestor in the segmentation.
Hence, we can evaluate the PageLevel segmentation against the manually labeled one by comparing the corresponding groupings using the adjusted RAND index [12].
The adjusted RAND index is a measure of how similar two groupings are, i.e., whether pairs of objects (leaves) are together in both groupings, or in di erent groups in both groupings.
It is used as a preferred measure of agreement between clusterings [17].
The value of the adjusted RAND index is upper bounded by 1, and its expected value for a random clustering is 0.
In Figure 1 we plot the accuracy of PageLevel segmentation in terms of adjusted RAND.
The PageLevel Basic and PageLevel Smooth approaches have been described above.
As we can see the PageLevel Basic algorithm achieves close to random results, but this is expected since it is not per-Figure 3: Variation in segmentation accuracy both datasets with changing values of penalty.
The x-axis represents the factor being multiplied into the penalty.
forming any smoothing of scores and hence almost every leaf is in a group of its own.
In contrast the segmentation discovered by the isotonic smoothing function (PageLevel Smooth) conforms very well to the manually labeled segments.
In order to put the accuracy of PageLevel Smooth in context, we also present numbers for a PageLevel Basic + Merge heuristic.
This approach does a  naive  smoothing of the classi er scores by grouping adjacent leaves together when their templateness scores di er by less than   (the best   was found by exhaustive search).
As we can see from the plot,  merging  improves the scores of the PageLevel Basic; however, the results are still far lower than those achieved by isotonic smoothing.
This shows that the smoothing operation is constructing highly nontrivial segmentations of webpages.
E ect of variations in penalty.
We have shown above that PageLevel successfully obtains and labels template segments within webpages.
Here we discuss the sensitivity of our approach to penalty parameters in the isotonic smoothing function.
Figure 2 plots the variation in template detection accuracy on the Common dataset with changing values of penalty.
In the plot, the x-axis represents the factor multiplied into the penalty in order to vary it.
As we can see, an increase or decrease in penalty results in an decrease in the template detection accuracy.
However, the decrease is larger with higher values of penalty as this results in very few segments and hence a mixing up of template and non-template struc-SiteLevel FullText Dup Non-Dup


 (76%)
 (91.6%)
 (42.7%)
 (83.2%)
 (30.9%)
 (86.5%) Table 2: Number of duplicate and non-duplicate pairs detected by the shingling approach after removing templates detected by PageLevel and SiteLevel .
FullText indicates no template detection and removal.
tures into the same segment.
Lower values of penalty do not give us the improvements inherent in smoothing, but they do not reduce the discriminative power of the Basic classi er.
The same behavior is seen for Random as well.
Variations in segmentation accuracy on both datasets with changing values of penalty are plotted in Figure 3.
Just as in the case of template detection accuracy, the segmentation accuracy also forms a unimodal curve, dropping with high and low values of penalty.
However, the drop in segmentation accuracy is larger for changes in penalty values as compared to drop in detection accuracy.
This is because the smoothing impacts the segmentation performance more directly, as compared to detection performance.
As we increase (decrease) penalty values the number of groups of leaves obtained are lesser (greater) than the manually labeled groups.
Both these changes negatively impact the segmentation performance.
Another interesting di erence is that template detection accuracy achieves high values even when segmentation performance is not at its peak.
The reason is that the manual labeling is binary (template or non-template), while the segments we  nd are labeled with real numbered scores.
Hence, we can still achieve a high template detection accuracy when the smoothing function places leaves into groups smaller than those in the manual la-bellings.
However, groups smaller than those in the manual labeling causes a decrease in segmentation accuracy.
This indicates that if achieving good segmentation is our primary objective, using a slightly higher value of penalty might be advantageous.
To summarize, in this section we showed that PageLevel accurately segments webpages, and also labels the segments appropriately as template or non-template.
Further, we showed that isotonic smoothing is critical to its success, contributing to increases in both segmentation and template detection accuracies.
We were unable to provide any comparisons with the site-level approach on the human labeled data, since SiteLevel needs many pages from each website in order to make template judgments for pages.
Next we show that webpage template detection is very useful as a pre-processing step in several applications, such as  nding webpages with duplicate content, and webpage classi cation.
Furthermore, since in these datasets we have several webpages from the same website available, we also present an evaluation comparing PageLevel with the site-level template detection approach.
Duplicate webpages and mirrored websites present challenging problems to web search engines that crawl and index them.
Duplicated pages use up valuable index space and duplicate results returned for search queries spoil the user experience.
Hence, detection of duplicates on the Web in a scalable fashion has been the subject of much research [4, 5,
 shingles.
For each webpage, shingles are extracted by moving a window of  xed length over the text, and the ones with the N smallest hash values are stored.
Two documents that share shingles are then considered to be near-duplicates.
Problems caused by templates.
The templates regions often contain text whose purpose is orthogonal to the main content of the webpage.
Hence, this templated content must not be used while making decisions about whether pages are duplicates.
For example, text present within navigation bars, copyright notices etc., must not be compared when two pages are being checked for duplicate material.
The presence of templated content of webpages can foil duplicate detection algorithms whenever the shingling process retains shingles from the templated regions.
Two pages that have absolutely the same content, say the exact same AP news story repeated across two di erent news websites, might be considered non-duplicates if the shingling process retains shingles from the template regions of the webpages as this portion of the webpages is di erent.
This can lead to false negatives and cause us to return duplicate results for queries.
Similarly, two webpages with the same templated content but di erent main content might be considered duplicates if all the shingles hit the templated region.
This can result in false positives and cause us to ignore valuable content on the web.
In this section we evaluate the e ect of templates on duplicate detection performance, and also also compare the template detection performance of PageLevel to the site-level approach.
The Lyrics dataset.
We constructed the Lyrics dataset by obtaining the webpages containing lyrics for the same song from three di erent websites.
This way we knew that the webpages from di erent websites containing lyrics to the same song should be considered duplicates5.
We also knew that webpages containing lyrics of di erent songs, irrespective of what website they come from, should be considered non-duplicates.
We were able to obtain 2359 webpages from the websites www.absolutelyrics.com, www.lyricsondemand.
com, and www.seeklyrics.com containing lyrics to songs by artists ABBA, BeeGees, Beatles, Rolling Stones, Madonna, and Bon Jovi.
We chose to get lyrics by a few diverse artists in order to minimize the possibility of cover songs.
The Lyrics dataset consists of 1711 duplicate pairs (webpages with lyrics of the same song from di erent websites) and 2058 non-duplicate pairs (webpages with lyrics of di erent songs from the same website).
Experimental setup.
SiteLevel was run on all the pages on each lyrics website and the threshold parameter was set to 10%.
This setting was seen to perform well in [11].
We used a standard shingling process.
Before the shingling was performed the text of the webpage is made lowercase and only alphanumeric characters are retained.
Shingles are computed over moving windows of 6 consecutive words each, and the 8 minimum hashes are stored for each webpage.
A pair of pages is tagged as a duplicate if there are at least 4 matching hashes out of the 8 for each webpage.
scription errors on the di erent pages.
However, this a ects all algorithms equally.
FullText SiteLevel PageLevel PageLevel camera camera camera camera mobile mobile mobile notebook notebook printer mobile notebook printer tv notebook printer tv printer tv tv Average





















 Basic










 Smooth










 Table 3: Averaged classi cation accuracies on 2-class problems.
The best accuracies for each class combination are in bold.
We run this experiment under di erent settings: (1) all segments (and words) are used (FullText), (2) only segments tagged as non-template by PageLevel are used, and (3) only segments tagged as non-template by SiteLevel are used for shingling.
the text content of webpages [18] form the mainstay of webpage classi cation.
In this section we perform experiments on the e ect of template content on the classi cation of textual content of webpages, and show that template removal using PageLevel gives a boost in accuracy.
Results.
The results of our duplicate detection experiments are presented in Table 2.
Not detecting and removing template content (FullText) performs very badly, especially in  agging duplicate pairs.
Using the PageLevel approach to clean the data before shingling recovers 76% of the duplicate pairs, and 92% of non-duplicate ones.
These represent an improvement of 140% and 6% respectively over the Full-Text approach.
Finally, PageLevel also outperforms the SiteLevel template detection approach by a large margin in both  agging duplicate and non-duplicate pairs.
The Lyrics dataset is not representative of the density of duplicates and non-duplicate pairs found on the web; we created it to highlight the problems posed by templates to duplicate detection algorithms.
Hence, while the numbers seen in these experiments will not apply exactly to the web in general, the results are indicative of the bene ts of template detection and removal, and the dataset serves as an appropriate test-bed for comparing the algorithms PageLevel and SiteLevel.
Discussion.
Why does PageLevel outperform SiteLevel even though it is trained on the output of the latter?
Comparing errors performed by the two on the LYRICS dataset o ers us an opportunity to investigate this.
As stated before, errors occur when shingles come from templated regions of the page.
Many of the errors committed by SiteLevel involved shingles from a segment on  Popular lyrics by this Artist  that seemed to change based on the artist whose song lyrics were being displayed.
Since this segment changed within webpages on the same site, SiteLevel was unable to identify it as a template.
However, thanks to the careful selection of DOM node features in the page-level classi er, PageLevel generalizes beyond the site-level training data.
Thus, it picked out such segments as templates, boosting its accuracy signi cantly.
Automated classi cation of webpages is a well-studied problem and numerous approaches have been proposed for it.
While many sources of information like hyperlinks [6], site structure [15], etc.
are often used, techniques for classifying Problems caused by templates.
Even though classi cation algorithms are very good at identifying and removing noisy features, in certain scenarios templates can present a challenging problem.
Consider a binary classi cation problem between classes Camera and Notebook.
If the template terms (noise) in both classes are the same, a classi er would be able to detect and remove it, say, using the correlation of features to the class labels.
However, the noisy features could di er across the two classes; say, the webpages in Camera class come from CNET, and those in Notebook class come from PCConnection, the classi er will not be able to remove the template content automatically, making template detection as a pre-processing step imperative.
Dataset and experimental setup.
For the classi cation experiments we used a subset of the dataset used by Vieira et al. [24].
The dataset consisted of webpages on 5 topics (Camera, Notebook, Mobile, Printer, TV) obtained from 4 websites, CNET, J&R, PCConnection, and ZDNET.
Details of this dataset can be obtained in [24].
From this data, we constructed binary classi cation problems in which training data for classes C1 and C2 were taken from di erent websites, and the rest of the data for these classes were used as test data.
For instance, in one binary problem, C1 is Camera and C2 is Notebook.
The training data for C1 and C2 comes from CNET and PCConnection respectively.
The test data for C1 then comprises J&R, PCConnection, and ZDNET, while that for C2 comprises CNET, J&R, and ZD-NET.
This evaluation setting has been used previously [24,
 classi cation problems.
The classi cation accuracy numbers we report are averaged over all possible binary classi cation problems of the type mentioned above.
We run this experimental setup with di erent amounts of webpage cleaning: (1) with all segments (and words) (Full-Text), (2) with only segments tagged as non-template by SiteLevel, (3) only segments tagged as non-template by the PageLevel Basic algorithm (4) only segments tagged as non-template by the PageLevel Smooth algorithm.
Recall 6www.cs.cmu.edu/~mccallum/bow/ is disabled, and only raw templateness classi er assigned scores are used.
Results.
The results of the classi cation experiments are presented in Table 3.
The best accuracies for each class combination are highlighted in bold.
As we can see, using template detection as a pre-processing step always improves the classi cation accuracy of the Naive Bayes classi er.
Furthermore, webpage cleaning via the PageLevel algorithm outperforms SiteLevel on a majority of class combinations.
Even among the PageLevel approaches, the use of isotonic smoothing of the templateness classi er s output results in better template detection and removal, as evidenced by an increase in the Naive Bayes classi er accuracy.
In the  nal analysis, webpage template detection and removal via our PageLevel system increases classi cation accuracy on this dataset by an average of 18%.
We presented a framework for classi er based page-level template detection that constructs the training data and learns the notion of  templateness  automatically using the site-level template detection approach.
We formulated the smoothing of classi er assigned templateness scores as a regularized isotonic regression problem on trees, and presented an e cient algorithm to solve it exactly; this may be of independent interest.
Using human-labeled data we empirically validated our system s performance, and showed that template detection at the page-level, when used as a pre-processing step to webmining applications, such as duplicate detection and webpage classi cation, can boost accuracy sig-ni cantly.
Acknowledgments.
We thank Rajat Ahuja, Rupesh Mehta, Arun Ramanujapuram, and Amit Sasturkar for their valuable help.
We also thank the reviewers for their helpful comments.
