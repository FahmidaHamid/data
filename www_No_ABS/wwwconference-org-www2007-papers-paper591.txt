Keyword search is a convenient and widely-used approach to retrieve information from both unstructured and structured data [1, 4, 7, 10, 11].
Its appeal stems from the fact that keyword queries can be easily posed without requiring to use a query language and knowing the schema or structure of the data being searched.
For XML data, where the data is viewed as a hierarchically-structured rooted tree, a natural keyword search semantics is to return all the nodes in XML tree that contain all the keywords in their subtrees.
However, this simple search semantics can result in returning too many data nodes, many of which are only remotely linked to the nodes containing the keywords.
A recent direction to improve the e ectiveness of keyword search in XML data is based on the notion of smallest lowest Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
x1 x2 a2 c2 x3 x4 e1 b1 d2 x5 a1 e2 b2 c1 d1 a1 b1 a2 b2 b3 ...
bn 1 bn (a) T1 (b) T2 Figure 1: Example XML Trees T1 and T2 common ancestor (SLCA) semantics [14].
A keyword search using the SLCA semantics returns nodes in the XML data that satisfy the following two conditions: (1) the subtrees rooted at the nodes contain all the keywords, and (2) the nodes do not have any proper descendant node that satis es condition (1).
The set of returned data nodes are referred to as the SLCAs of the keyword search query.
Another recent work on keyword search based on the meaningful LCA (MLCA) semantics also shares the similar principle as SLCA [12].
The following example illustrates the di erence between the SLCA-based keyword search and the conventional LCA-based keyword search.
Example 1.1 Consider the XML tree T1 shown in Figure 1(a), where the keyword nodes are annotated with subscripts for ease of reference.
Consider a keyword search using the keywords {a, b, c, d, e} on T1.
If the search is based on the conventional LCA semantics, then the result is given by {x1, x2, x3, x4} as x1 is the LCA of {a2, b2, c2, d2, e2}, x2 is the LCA of {a1, b1, c2, d1, e1}, x3 is the LCA of {a1, b1, c1, d2, e1}, and x4 is the LCA of {a1, b1, c1, d1, e1}.
However, if the search is based on the SLCA semantics, then the result is given by {x4}.
Observe that each of x1, x2, and x3 is not a SLCA because it has a descendant node x4 that is a

 The state-of-the-art algorithms for keyword search using SLCA semantics are the Scan Eager (SE) and Indexed Lookup Eager (ILE) algorithms [14], which were shown to be more e cient than stack-based algorithms [8, 12].
The search involves at least one low frequency keyword, while the SE algorithm performs better when the frequencies of the keywords in the query do not vary signi cantly.
We classify both these algorithms as binary-SLCA approach (BS) as they are both based on the same principle of computing the SLCAs for a query with k keywords in terms of a sequence of k   1 intermediate SLCA computations, where each SLCA computation takes a pair of data node lists as inputs and outputs another data node list.
Speci cally, consider a search query with k keywords w1,     , wk.
Let Si denote the list of XML data nodes that are labeled with keyword ki, i   [1, k]; and let Li denote the SLCAs for a query with the  rst i keywords, i   [1, k].
The binary-SLCA algorithms compute the SLCAs for w1,     , wk by computing the sequence L2, L3,  , Lk, where each Li is computed by  nd-ing the SLCAs of Li 1 and Si (with L1 = S1).
An important observation exploited in the binary-SLCA algorithms is that the result size is bounded by min{|S1|,    ,|Sk|}; therefore, by choosing the keyword with the lowest frequency as k1 (i.e., |S1|   |Si| for i   [1, k]), the algorithms can guarantee that each |Li|   |S1|, for i   [2, k].
x1 x2 r1   x10 b11     b1001 a1   a100 a101    a200 a901   a1000 b10 b2 b1 Figure 2: Example XML Tree T3 However, a drawback of the binary-SLCA approach is that by computing the SLCAs in terms of a series of intermediate SLCA computations, it can often incur many unnecessary SLCA intermediate computations even when the result size is small as the following example illustrates.
Example 1.2 Consider the XML tree T3 in Figure 2.
The SLCAs for the keywords {a, b} in T3 are {x1, x2,  , x10}.
Since |Sa| < |Sb|, the BS approach will enumerate each of the  a  nodes in Sa to compute a potential SLCA with it.
Clearly, this approach results in many redundant computations; for example, the SLCA of ai and b1 gives the same result x1 for i   [1, 100].
In fact, the BS approach will incur a total of 1000 SLCA computations to produce a result of
 size 10.
Our  rst contribution in this paper (Section 3) is the proposal of a novel approach for processing SLCA-based keyword search queries called multiway-SLCA approach (MS).
In contrast to the BS approach, our MS approach computes each potential SLCA by taking one data node from each keyword list Si in a single step instead of breaking the SLCA computation into a series of intermediate binary SLCA computations.
Conceptually, each potential SLCA computed by the BS approach can be thought of as being driven by some node from S1 (i.e., the keyword list with the lowest frequency); on the other hand, our MS approach picks an  anchor  node from among the k keyword data lists to drive the multiway SLCA computation.
By doing so, our approach is able to optimize the selection of the anchor node (not necessarily from S1) to maximize the skipping of redundant computations.
The following example provides an idea of the skipping optimization of our MS approach.
Example 1.3 Consider the processing of the SLCA-based keyword search with keywords {a, b} on T3 in Figure 2 using our MS approach.
MS will  rst consider the  rst data nodes in all the keyword lists and selects the node that occurs the latest in T3 as the anchor node.
Thus, between a1   Sa and b1   Sb, b1 will be selected as the anchor node (the property behind this optimization will be explained later in the paper).
Next, using b1 as anchor, our approach will select the closest data nodes from the other keyword lists to compute a potential SLCA.
Thus, the  rst SLCA is computed for the set {b1, a100}.
After the  rst potential SLCA is computed, our approach will consider the  rst nodes in the keyword lists that occur after b1 for the next computation (i.e., nodes a101 and b2).
The next anchor node selected is b2, and the next SLCA computation involves b2 and a200.
Clearly, the MS approach is able to skip many unnecessary
 computations.
In addition to introducing the notion of anchor nodes that we alluded to for minimizing redundant computations, we also develop several optimizations to further maximize the skipping of data nodes in the keyword list without compromising correctnesses of query results.
Our second contribution in this paper (Section 4) is the generalization of the SLCA-based approach to handle more general keyword search queries beyond the implicit AND-semantics to support any combinations of AND and OR semantics.
This enables more  exible and expressive keyword search queries such as  (a OR b) AND c AND (d or E)  to be speci ed.
We extend our MS approach to evaluate general keyword search queries involving both AND and OR operators.
Finally, our third contribution in this paper (Section 5) is a comprehensive experimental performance evaluation which demonstrates that our proposed multiway-SLCA approach outperforms the previous binary-SLCA approach for both traditional keyword search queries as well as generalized keyword search queries.
Let K = {w1,    , wk} denote an input set of k keywords, where each keyword wi is associated with a set Si of nodes in an XML document T (sorted in document order1).
A set of nodes S = {v1,  , vk} is de ned to be a match for K if |S| = |K| and each vi   Si for i   [1, k].
We use Si to denote the data node list (sorted in document order) associated with the keyword wi.
For simplicity and without loss of generality, we assume w1 to be the lowest frequency keyword among the keywords in K; i.e., |S1|   |Si| for i   [1, k].
A node v in T is a lowest common ancestor (or LCA) for K if v is the lowest common ancestor node of some match S. Moreover, v is also a smallest lowest common ancestor (or SLCA) for K if each descendant of v in T is not a LCA for K.
Given two nodes v and w in a document tree T , v  p w denotes that v precedes w (or w succeeds v) in document order in T ; and v (cid:5)p w denotes that v  p w or v = w.
More generally, given two matches V = {v1,  , vk} and
 ument nodes.
closest(v, S) =(cid:0)(cid:2) (cid:3) pred(v, S) if lca(v, next(v, S))  a lca(v, pred(v, S)), next(v, S) otherwise.
W = {w1,  , wk}, where vi, wi   Si,   i   [1, k], we say that V precedes W (or W succeeds V ), denoted by V  p W , if they satisfy both the following properties: (1) vi (cid:5)p wi for each i   [1, k]; and (2) V (cid:7)= W .
We use v  a w to denote that v is a proper ancestor of w in T , and v (cid:5)a w to denote that v = w or v  a w.
Consider a node v and a set of nodes S. The function f irst(S) returns the  rst  node v(cid:2)   S such that v(cid:2) (cid:5)p vi for each vi   S. Similarly, the function last(S) returns the  last  node v(cid:2)   S such that vi (cid:5)p v(cid:2) for each vi   S. Both functions return null if any of its input argument values is null.
The function out(v, S) returns the  rst  node v(cid:2)   S such that v  p v(cid:2) is not a descendant of v or equal to v; i.e., out(v, S) = f irst({v(cid:2)   S | v  p v(cid:2), v (cid:7)(cid:5)a v(cid:2)}).
The function returns null if no such node exists or if v is null.
and v(cid:2) The function next(v, S) returns the  rst node in S that succeeds v if it exists; otherwise, it returns null.
The function pred(v, S) returns the predecessor of v in S, that is, the last node in S that precedes v if it exists; otherwise, it returns null.
The function closest(v, S) computes the closest node in S However, closest(v, S) returns null if both pred(v, S) and next(v, S) are null; and it returns the non-null value if exactly one of pred(v, S) and next(v, S) is null.
The function lca(S) computes the lowest common ancestor (or LCA) of the set of nodes S and returns null if any of its arguments is null.
For notational convenience, we assume that the root node of the data tree T has a virtual parent node, denoted by droot, such that droot is a proper ancestor node of every node in T .
The following example illustrates our de nitions.
Example 2.1 Consider the XML document tree T1 shown in Figure 1(a) and the keyword search query K = {a, b, c}.
Note that {a1, b1, c2} is a match for K; but neither {a1, c2} nor {x1, b2, c1} is a match for K. We have e1  p b2 but e2 (cid:7) p x4.
Moreover, {a1, b1, c1}  p {a2, b2, c2}.
We have x2 (cid:5)a e1 and x3 (cid:7)(cid:5)a c2.
If S = {d1, c1, d2}, then f irst(S) = d1, last(S) = d2, out(x4, S) = d2, next(x4, S) = c1, next(c1, S) = d2, next(e2, S) = null, pred(a1, S) = d1, pred(x4, S) = null, closest(b1, S) = c1, closest(a2, S) = d2, and lca(S) = x3.
2

 In this section, we present our new approach of processing SLCA-based keyword search queries called the multiway-SLCA approach (MS).
As alluded in the introduction, the key motivation behind our MS approach is to avoid the unnecessary overhead of the BS approach where SLCAs are computed in terms of intermediate SLCA computations by enumerating each data node in the lowest-frequency keyword list.
As Example 1.2 demonstrates, by rigidly driving the SLCA computations from the lowest-frequency keyword list can result in many redundant computations particularly when the size of the results is small.
We  rst introduce the notion of anchor nodes in Section 3.1 which is a central idea in our MS approach.
Section 3.2 then presents several important properties about anchored matches.
We present our  rst MS-based algorithm called basic multiway-SLCA (BMS) in Section 3.3, followed by our second improved MS-based algorithm, called incremental multiway-SLCA (IMS), in Section 3.4.
A match S = {v1,   , vk} is said to be anchored by a node va   S if for each vi   S   {va}, vi = closest(va, Si).
We refer to va as the anchor node of S.
The concept of anchor nodes is important to our multiway-SLCA approach as it enables us to restrict matches to those that are anchored by some nodes as the following result demonstrates.
Lemma 3.1.
If lca(S) is an SLCA and v   S, then lca(S) = ), where S(cid:2) is the set of nodes anchored by v.
lca(S(cid:2) Proof.
The proof is established by contradiction.
Suppose that lca(S) (cid:7)= lca(S(cid:2) ).
Since lca(S) is an SLCA and v   S(cid:2)   S, this implies that lca(S(cid:2) ) is a proper ancestor of lca(S).
It follows that there must exists some Si such that lca(S) (cid:7)(cid:5)a closest(v, Si).
However, since S   Si (cid:7)=  , we have a contradiction.
Thus, our MS approach only considers anchored sets for computing potential SLCAs.
The optimization potential of the above result was illustrated earlier in Example 1.3.
Recall that b1 is the selected anchor node for the SLCA computation with a100 = closest(b1, Sa).
By choosing b1 as the anchor node (instead of using a1 as in the BS approach), for the  rst SLCA computation, it follows from Lemma 3.1 that it is unnecessary to compute SLCAs for matches that include any ai, i   [1, 99] because such matches would necessarily include b1 and Lemma 3.1 states that it is not possible to generate new SLCAs with such matches.
Note in our MS approach, an anchor node can be chosen from any of keyword data lists (i.e., S1,   , Sk).
For nota-tional convenience, when we denote an anchor node as vm, we also mean that vm is selected from Sm, m   [1, k].
In this section, we present several important properties that form the basis of the optimizations in our MS-based algorithms.
S and S(cid:2) must necessarily be disjoint.
The following result states that if the LCAs of two matches are both distinct SLCAs, then the two matches ) are distinct SLCAs, Lemma 3.2.
If lca(S) and lca(S(cid:2) then S   S(cid:2) =  .
Proof.
Suppose lca(S) and lca(S(cid:2) ) are two distinct SLCAs.
If both S and S(cid:2) contains some common node, then lca(S) and lca(S(cid:2) ) must be related in one of three possibilities: ); (b) lca(S) is an ancestor of lca(S(cid:2) (a) lca(S) = lca(S(cid:2) ); or (c) lca(S) is a descendant of lca(S(cid:2) ).
Case (a) contradicts the fact that lca(S) and lca(S(cid:2) ) are distinct SLCAs.
Cases (b) and (c) imply that either lca(S) or lca(S(cid:2) ) is not a SLCA, contradicting the fact that lca(S) and lca(S(cid:2) ) are two distinct SLCAs.
Thus, it follows by contradiction that S   S(cid:2) =  .
Lemma 3.3.
Let V and W be two matches such that V  p W .
If lca(W ) is not a descendant of lca(V ), then for any of lca(V ).
Proof.
Let V  p W and lca(W ) is not a descendant of lca(V ), then either (a) all the nodes in the subtree rooted at lca(V ) precede all the nodes in the subtree rooted at lca(W ), or (b) lca(W ) is an ancestor of lca(V ).
For case (a), if lca(X) is a descendant of lca(V ), then all the nodes in the subtree rooted at lca(X) must precede all the nodes in the subtree rooted at lca(W ) contradicting that X succeeds W .
Thus, lca(X) cannot be a descendant of lca(V ) for case (a).
For case (b), W must contain some node w such that w is not a descendant of lca(V ) and w succeeds V ; if not, lca(W ) cannot be an ancestor of lca(V ).
Therefore, if lca(X) is a descendant of lca(V ), then this implies that w   W succeeds X, contradicting the fact that X succeeds W .
Thus, lca(X) cannot be a descendant of lca(V ) for case (b) as well.
Lemma 3.3 is a useful generalization of Lemma 2 in [14] that is exploited in our algorithms to determine whether a computed LCA is guaranteed to be a SLCA.
Speci cally, if V  p W and lca(W ) is not a descendant of lca(V ), then one can conclude that lca(W ) is an SLCA.
Lemma 3.4.
Consider two matches S and S(cid:2) , and S is anchored by some node v. If S(cid:2) S(cid:2) node u where u (cid:5)p v, then lca(S(cid:2) or an ancestor of lca(S).
, where S  p contains some ) is either equal to lca(S) .
Since {u, v(cid:2)}   S(cid:2) , we must have v (cid:5)p v(cid:2) , v must be a descendant of lca(S(cid:2) Proof.
Let v   Si, i   [1, k].
Let Si   S(cid:2) = {v(cid:2)}.
Since S  p S(cid:2) and u (cid:5)p v (cid:5)p v(cid:2) ) which implies that lca(S(cid:2) ) is either equal to lca(S), a descendant of lca(S), or an ancestor of lca(S).
However, if lca(S(cid:2) ) is a descendant of lca(S), it would contradict the fact that S is anchored by v. Therefore, lca(S(cid:2) ) is either equal to lca(S) or an ancestor of lca(S).
Lemma 3.4 provides a useful property to optimize the selection of the next match to be considered.
Speci cally, if we have considered a match S that is anchored by a node va, then we can skip matches that contain any node v (cid:5)p va in S.
Lemma 3.5.
Let S and S(cid:2) contains two nodes, where one is a descendant of lca(S), while the other is not, then lca(S(cid:2) ) is either equal to lca(S) or an ancestor of lca(S).
be two matches.
If S(cid:2) Proof.
Let v, w   S(cid:2) , where v is a descendant of lca(S), and w is not a descendant of lca(S).
Since v is a descendant of both lca(S) and lca(S(cid:2) ) are equal or one is an ancestor of the other.
However, since w is not a descendant of lca(S), lca(S(cid:2) ) cannot be a descendant of lca(S); and the claim follows.
), either lca(S) and lca(S(cid:2) Lemma 3.5 provides another useful property to optimize the selection of the next match to be considered.
Speci -cally, if we have considered a match S and lca(S) has been con rmed to be an SLCA, then we can skip matches S(cid:2) that contains some node that is a descendant of lca(S) as well as another node that is a not a descendant of lca(S).
Lemma 3.6.
Let S be a set of nodes.
Then lca(S) = lca(f irst(S), last(S)).
Proof.
Since {f irst(S), last(S)}   S, therefore, lca(S) is either equal to or an ancestor of lca(f irst(S), last(S)).
Clearly, for each v   S where f irst(S)  p v  p last(S), v is a descendant of lca(f irst(S), last(S)).
It follows that lca(S) = lca(f irst(S), last(S)).
Lemma 3.6 states that the LCA of a set of nodes S is equivalent to the LCA of the two extreme nodes (i.e., the  rst and last nodes) in S. This property enables the LCA computation for a set of nodes S to be improved signi cantly as it su ces to compute the LCA of S in terms of only its  rst and last nodes.
In this section, we present our  rst Multiway-SLCA-based algorithm called Basic Multiway-SLCA (BMS) for computing SLCAs for a set of keywords {w1,    , wk}.
The details are given in Algorithm 1 which takes k keyword data lists S1,  , Sk as input and returns the SLCAs as a collection of nodes.
Each Si is the list of data nodes associated with keyword wi.
The algorithm computes the SLCAs iteratively.
At each iteration, an anchor node vm is judiciously selected to compute the match anchored by vm and its LCA (denoted by  ).
If   is potentially an SLCA, it is maintained in an intermediate SLCA result list given by  1,  ,  n, n   1, where all the LCAs  i in the list are de nite SLCAs except for the most recently computed candidate  n.
To minimize the computation of LCAs that are not SLCAs, it is important to optimize the anchor node selected at each iteration.
Initially, step 2 initializes the  rst candidate SLCA  1 to be droot (the virtual root node of data tree); if  1 remains as droot at the end of the algorithm (step 22), then it means that the SLCA result list is empty.
The  rst anchor node vm is selected in step 1.
Instead of choosing the  rst node v1   S1 as the anchor (as is done in the BS approach), BMS selects the  rst node vm   Sm, m   [1, k] that is the  furthest  node among all the  rst nodes in S1,  , Sk.
In doing so, all the nodes in S1 that precede u1 = closest(vm, S1) are skipped from consideration as anchor nodes.
The correctness of this optimization stems from the fact that for each v   S1 that precedes u1, closest(v, Sm) must be vm; therefore, by Lemma 3.1, no SLCAs will be missed out by using vm as the  rst anchor node.
Steps 4 to 9 further optimize the selection of the anchor node to ensure that the total number of candidate SLCAs computed is no more than |S1| (elaborated in Section 3.5).
Speci cally, if the selected anchor node vm precedes closest(vm, S1), then by Lemma 3.1, no SLCAs will be omitted by replacing the anchor node vm with closest(vm, S1).
The usefulness of this optimization is illustrated in Example 3.2.
After an anchor node vm has been chosen, step 10 computes the match anchored by vm, and step 11 computes the LCA   of this match in terms of only its  rst and last nodes (based on Lemma 3.6).
Steps 12 to 16 check whether the newly computed LCA   can be a candidate SLCA; and if so, whether   can be used to eliminate the previous candidate SLCA  n.
Steps 17 to 20 optimize the selection of the next anchor node by choosing the furthest possible node that maximizes the number of skipped nodes: step 17 is based on Lemma 3.4 while steps 18 to 20 are based on Lemma 3.5.
Example 3.1 Consider computing SLCAs for the set of if (m (cid:3)= 1) then v1 = closest(vm, S1) if (vm  p v1) then Algorithm 1 Basic Multiway-SLCA (S1,  , Sk)
 2: initialize n = 1;  1 = droot 3: while (vm (cid:3)= null) do
















 21: end while 22: if ( 1 = droot) then return   else return { 1,    ,  n} end if end if vi = closest(vm, Si) for each i   [1, k], i (cid:3)= m   = lca(f irst(v1,    , vk), last(v1,    , vk)) if ( n (cid:5)a  ) then else if (  (cid:3)(cid:5)a  n) then n = n + 1;  n =   end if vm = last({next(vm, Si) | i   [1, k], vi (cid:5)p vm}) if (vm (cid:3)= null) and ( n (cid:3)(cid:5)a vm) then vm = last({vm}   {out( n, Si) | i   [1, k], i (cid:3)= m})  n =   end if keywords {a, b, c, d, e} on the data tree T1 in Figure 1(a) using the BMS algorithm.
Since each keyword has the same frequency, let S1 be the list of data nodes for keyword  a .
The  rst anchor node selected is c1, and the  rst candidate SLCA computed is  1 = lca(d1, e1, b1, a1, c1) = x4.
The next anchor node selected is a2, and the second candidate SLCA computed is   = lca(d2, e2, b2, c2, a2) = x1.
However, since x1 (cid:5)a x4, x1 is not a SLCA.
The next anchor node selected has a null value (due to next(a2, S1) = null), and the algorithm terminates with x4 as the only SLCA.
The next example illustrates the importance of the optimization performed by steps 4 to 9.
Example 3.2 Consider computing SLCAs for the set of keywords {a, b} on the datatree T2 in Figure 1(b).
Here, S1 refers to the list of nodes for keyword  a .
Using a non-optimized BMS algorithm that excludes steps 4 to 9, the  rst anchor node is b1 and the candidate SLCA computed is lca(b1, a2) = b1.
Similarly, the subsequent sequence of anchor nodes selected is b2, b3,   , bn, and the candidate SLCA computed for each of these anchor nodes bi is lca(bi, a2) = b1.
Clearly, the non-optimized BMS incurs many redundant SLCA computations that involve the same a2 node.
In general, the non-optimized BMS performs poorly when many anchor nodes share the same closest node (w.r.t.
some keyword) that succeeds the anchor nodes.
The BMS algorithm avoids this problem by bounding the number of computed SLCAs to be no more than |S1| using steps 4 to 9.
In this case, the  rst anchor node selected is optimized to a2 and the  rst candidate SLCA computed is lca(a2, b1) = b1.
The next anchor node selected has a null value (since next(a2, S1) = null) and the algorithm terminates without any redundant SLCA computations.
Thus, the number of candidate SLCA computations is reduced from n to just one.
In this section, we present our second Multiway-SLCA-based algorithm called Incremental Multiway-SLCA (IMS), vm = v1 if (m (cid:3)= 1) then v1 = closest(vm, S1) if (vm  p v1) then end if end if P = {pred(vm, Si) | i   [1, k], i (cid:3)= m}   {vm} N = {next(vm, Si) | i   [1, k], next(vm, Si) (cid:3)= null} initialize rmax = last(N ); repeat Algorithm 2 Incremental Multiway-SLCA (S1,  , Sk)
 2: initialize n = 1;  1 = droot 3: while (vm (cid:3)= null) do























 28: end while 29: if ( 1 = droot) then return   else return { 1,     ,  n} until (r = null) or (  (cid:3)(cid:5)a r) or (r = rmax) if (r = null) or (  (cid:3)(cid:5)a r) then if ( n (cid:5)a  ) then else if (  (cid:3)(cid:5)a  n) then n = n + 1;  n =   remove (cid:3) from P , where (cid:3) = f irst(P )   = lca((cid:3), r) r = last(r, v) where v   N s.t.
v = next(vm, Sj ), (cid:3)   Sj end if vm = last(r, out( n, S1),   , out( n, Sk))  n =   r = vm else vm = r end if whose details are shown in Algorithm 2.
IMS is an optimized variant of BMS that reduces the number of LCA computations.
In BMS (Algorithm 1), each computation of   incurs at least 2k   1 LCA computations: each of the k   1 calls to closest function in step 10 requires two LCA computations, and step 11 adds another LCA computation.
To avoid the large number of LCA computations incurred by an explicit computation of M , the IMS algorithm determines f irst(M ) and last(M ) without actually computing M .
In the following, we analyze the properties of f irst(M ) and last(M ), and explain how this can be achieved.
By definition of the match M anchored by vm, M must satisfy the following three conditions:
 [1, k], i (cid:7)= m, pred(vm, Si) (cid:7)= null} and N = {next(vm, Si) | i   [1, k], i (cid:7)= m, next(vm, Si) (cid:7)= null};

 Since M must contain vm and every node in P precedes vm, it follows that f irst(M )   P   {vm}.
Furthermore, last(M ) can be determined once f irst(M ) is known.
Let P (cid:2)   P denote the subset of nodes in P that precedes f irst(M ) (i.e., = {v   P | v  p f irst(M )}); and let N(cid:2)   N denote the P (cid:2) subset of nodes in N that corresponds to P (cid:2) that succeeds vm = {next(vm, Si) | i   [1, k], pred(vm, Si)   P (cid:2)}).
(i.e., N(cid:2) Since P (cid:2)   M =  , in order for M to satisfy condition (2), it is necessary that M   N(cid:2) .
Moreover, since |P (cid:2)| = |N(cid:2)| and |M| = k, we must have M = (P   P (cid:2) )   {vm}   N(cid:2) and last(M ) = last(N(cid:2) Since there are |P| + 1 possible values for f irst(M ), let M1, M2,  , M|P|+1 denote the sequence of matches where for each i   [1, |P|+1], we have (1) vm   Mi; (2) f irst(Mi)   ).
f irst(M|P|+1).
In other words, f irst(Mi) = (cid:0)(cid:2) (cid:3) f irst(P   {vm}) f irst((P   {vm})  otherwise.
i 1 j=1 f irst(Mj)) (cid:4) Based on the preceding analysis of f irst(M ) and last(M ), last(Mi) can be computed incrementally as follows: if i = 1, (1) last(Mi) =  vm where f irst(Mi 1)   Sx.
It follows that last(Mi 1   {next(vm, Sx)}) if i = 1, otherwise.
(2) f irst(M1)  p    p f irst(M|P|+1)  p last(M1) (cid:5)p   (cid:5)p last(M|P|+1).
(3) Clearly, M = Mj for some j   [1, |P|+1], where lca(Mi) (cid:5)a lca(Mj ) for i   [1, |P| + 1].
We can characterize Mj by the following two properties: (P1) for i   [1, j), lca(Mi) (cid:5)a last(Mi+1); and (P2) if j < |P| + 1, then lca(Mj ) (cid:7)(cid:5)a last(Mj+1).
Property (P1) implies that lca(Mi) (cid:5)a lca(Mi+1) for i   [1, j).
Speci cally, since f irst(Mi)  p f irst(Mi+1)  p last(Mi) (by Equation (3)), it follows that lca(Mi) (cid:5)a f irst(Mi+1); combining this with property (P1), we have lca(Mi) (cid:5)a lca(Mi+1).
Property (P2) implies that lca(Mi) (cid:5)a lca(Mj) for i   (j, |P| + 1].
To see this, note that lca(Mj ) (cid:5)a f irst(Mi) for i   (j, |P| + 1] (by Equation (3)).
Furthermore, since lca(Mj ) (cid:7)(cid:5)a last(Mj+1) (property (P2)) and last(Mj+1) (cid:5)p last(Mi) for i   (j + 1,|P| + 1] (by Equation (3)), it follows that lca(Mj ) (cid:7)(cid:5)a last(Mi) for i   (j, |P| + 1].
Thus, for i   (j,|P| + 1], we have Mj  p Mi, lca(Mj ) (cid:5)a f irst(Mi) and lca(Mj) (cid:7)(cid:5)a last(Mi); it follows from Lemma 3.5 that lca(Mi) (cid:5)a lca(Mj).
The IMS algorithm (Algorithm 2) shares many similarities with the BMS algorithm in the previous section.
The key di erence lies in steps 10 to 17 which determines lca(M ) for a match M anchored by a node vm without actually computing M .
The repeat loop enumerates a sequence of matches M1,M2,    to compute f irst(M ) and last(M ) and hence lca(M ).
In the ith iteration of the repeat loop (steps 13 to
 puted in step 15 (with r representing last(Mi)).
Step 16 determines last(Mi+1) for the next iteration.
The search for M is terminated when any one of the three conditions in step 17 is met.
Firstly, if r = null, then it means that next(vm, Sj) = null and there are no further matches in the data; therefore,   = lca(M ) and the next anchor node is correctly set to null by step 24.
Secondly, if   (cid:7)(cid:5)a r, then   = lca(M ) and Lemma 3.5 is applied to optimize the selection of the next anchor node in step 24.
Finally, if r = last(N ), then it means that all the matches Mi subsequently enumerated within the repeat loop must have last(Mi) = last(N ) as well; therefore, M must correspond to the very last match in the enumeration.
To quickly skip to this last match without continuing with the enumeration, step 26 applies Lemma 3.1 to update the next anchor node to be r.
Note that the number of LCA computations incurred by IMS for each candidate SLCA computation is at least one (one iteration of repeat loop) and at most k + 1 (k   1 iterations of repeat loop and one call to closest function).
In contrast, BMS requires between 2k   1 and 2k + 1 LCA computations.
Example 3.3 Consider again computing SLCAs for the set of keywords {a, b, c, d, e} on the data tree T1 in Figure 1(a), where S1 is associated with keyword  a .
Using the IMS algorithm, the  rst anchor node selected is c1, P = {d1, e1, b1, a1, c1}, and N = {d2, e2, b2, c2, a2}.
In the  rst iteration of the repeat loop,   = lca(d1, c1) = x4.
r is then updated to d2 and the repeat loop terminates since x4 (cid:7)(cid:5)a d2.
Therefore, the  rst candidate SLCA computed is  1 = x4.
The next anchor node selected is a2, P = {d2, e2, b2, c2, a2}, and N = {}.
In the  rst iteration of the repeat loop,   = lca(d2, a2) = x1.
r is then updated to null and the repeat loop terminates.
Therefore, since   (cid:5)a  1,   is de nitely not a SLCA.
The next anchor node has a null value and the algorithm terminates with x4 as the only SLCA.
The number of LCA computations incurred by the IMS algorithm is only two.
In contrast, the BMS algorithm incurs 18 LCA computations
 (Example 3.1).
In this section, we analyze the time complexity of our new algorithms.
We begin by establishing an upper bound on the number of candidate SLCAs computed by each of the BMS and IMS algorithms.
Lemma 3.7.
The number of candidate SLCAs computed by each of the BMS and IMS algorithms is no more than
 Proof.
We prove the claim for BMS; the proof for IMS follows similarly.
The upper bound is established by showing that for any two candidate SLCAs computed by BMS, their corresponding matches do not contain the same node from S1.
By step 1, the  rst anchor node selected either succeeds or is equal to f irst(S1).
Let vm be the anchor node used to compute a candidate SLCA lca(M ).
By the optimization in steps 4 to 9, vm (cid:5)p closest(vm, S1).
Moreover, by step 17, if M   S1 = {v1}, then the next anchor node selected either succeeds or is equal to next(v1, S1).
Thus, since no two matches computed by BMS share the same node from S1, the claim is established for BMS.
We now consider the costs of the various functions.
Let d denote the height of the XML data tree, and let S denote the data node list with the highest frequency; i.e. |Si|   |S| for i   [1, k].
As in [14], we assume that each data node is stored together with its Dewey label which enables the lca function to be computed e ciently in O(d) time.
The cost of f irst(v1,  , vk) and last(v1,  , vk) is each O(k).
The cost of pred(v, Si), next(v, Si), out(v, Si), and closest(v, Si) is each O(d log(|Si|)) based on a binary search of Si and comparing nodes using their Dewey labels.
For the BMS algorithm, the cost to compute a candidate SLCA is O(kd log(|S|)) (due to step 10).
Since there are at most |S1| candidate SLCAs (by Lemma 3.7), the time complexity of BMS algorithm is O(kd|S1| log(|S|)).
For the IMS algorithm, the cost to compute a candidate SLCA is also O(kd log(|S|)) (due to steps 10 and 11); thus the time complexity of IMS algorithm is also O(kd|S1| log(|S|)).
However, for each node v   T do Algorithm 3 Simple AND-OR-SLCA (Q)
 Ci = wi,1   wi,2         wi,ni , ni   1 initialize toDeleteV = false for each node v(cid:2)   R do toDeleteV = true exit inner for-loop else if (v(cid:2) (cid:5)a v) then remove v(cid:2) from R ) then if (v (cid:5)a v(cid:2)
 3: for i = 1 to k do















 20: end for
 end if end for
 end if end for if (toDeleteV) then delete v from T our experimental results show that IMS outperforms BMS as the number of candidate SLCAs computed by IMS is less than that by BMS (by up to a factor of 30).
respectively, O(kd|S|) and O(|S1|kd log(|S|) + |S1|2) [14].
In comparison, the time complexities of SE and ILE are,

 In this section, we examine how to process more general SLCA-based keyword search queries that go beyond the AND-semantics in conventional queries to support any combination of AND and OR boolean operators.
We consider AND-OR keyword search queries of the form: Q = (Q) | Q and Q | Q or Q | w, where w denotes some keyword.
In the following, we consider two approaches to process SLCA-based AND-OR keyword search queries.
The  rst approach is a straightforward application of the algorithms presented in the preceding section for processing conventional SLCA-based AND-keyword search queries by expressing the general AND-OR queries in disjunctive normal form (DNF).
The second approach is an extension of our Multiway-SLCA approach (MS).
A straightforward approach to process a general AND-OR query Q is to rewrite Q in DNF and evaluate it in two stages:  rst, evaluate each disjunct in Q using an existing AND-query evaluation algorithm; next, the results of the individual evaluations are combined by eliminating intermediate SLCAs that are ancestor nodes of some other intermediate SLCAs.
The algorithm, referred to as Simple AND-OR (SA) is shown in Algorithm 3.
In this section, we show how our Multiway-SLCA approach for processing conventional AND-keyword search queries can be easily generalized to process AND-OR keyword search queries.
The extended algorithm, called AND-OR Multiway-SLCA (AOMS), is shown in Algorithm 4.
Our approach requires a general AND-OR query Q to be expressed in conjunctive normal form (CNF), C1   Cn, r = vm vm = v1 if (m (cid:3)= 1) then v1 = closest(vm, C1) if (vm  p v1) then Algorithm 4 AND-OR Multiway-SLCA 1: initialize x = 1;  1 = droot
 3: while (vm (cid:3)= null) do












 end if end if P = {pred(vm, Ci) | i   [1, n], i (cid:3)= m}   {vm} N = {next(vm, Ci) | i   [1, n], next(vm, Ci) (cid:3)= null} initialize rmax = last(N ); repeat remove (cid:3) from P , where (cid:3) = f irst(P )   = lca((cid:3), r) r = last(r, v) where v   N s.t.
v = next(vm, Ci), (cid:3)   Si,j until (r = null) or (  (cid:3)(cid:5)a r) or (r = rmax) if (r = null) or (  (cid:3)(cid:5)a r) then if ( x (cid:5)a  ) then else if (  (cid:3)(cid:5)a  x) then x = x + 1;  x =  










 28: end while 29: if ( 1 = droot) then return   else return { 1,     ,  x} end if vm = last(r, out( x, C1),   , out( x, Cn))  x =   vm = r end if else where each conjunct Ci = wi,1       wi,ni .
is a disjunction of ni keywords, ni   1.
We use Si,j to denote the list of data nodes associated with each keyword wi,j, i   [1, n], j   [1, ni].
Note that AOMS algorithm is almost equivalent to IMS algorithm except that the six functions f irst, last, pred, next, closest, and out now involve a conjunct Ci instead of a keyword list Si.
These mildly generalized versions of the de nitions from Section 2 are extended as follows:   f irst(Ci) = f irst({f irst(Si,j) | j   [1, ni], f irst(Si,j) (cid:7)=   last(Ci) = last({last(Si,j) | j   [1, ni], last(Si,j) (cid:7)= null}).
null}).
(cid:7)= null}) (cid:7)= null})   pred(v, Ci) = last({pred(v, Si,j) | j   [1, ni], pred(v, Si,j) (cid:7)= null})   next(v, Ci) = f irst({next(v, Si,j) | j   [1, ni], next(v, Si,j)   closest(v, Ci) = pred(v, Ci) if lca(v, next(v, Ci))  a lca(v, pred(v, Ci)); otherwise, closest(v, Ci) = next(v, Ci).
  out(v, Ci) = f irst({out(v, Si,j) | j   [1, ni], out(v, Si,j) For simplicity and without loss of generality, we assume that |C1|   |Ci| for i   [1, n] where |Ci| =

 ni j=1 |Si,j|.
To verify the e ectiveness of our proposed algorithms, we conducted extensive experiments to compare their performance against existing approaches for evaluating both AND as well as AND-OR keyword search queries.
s m i ( e m
 n o i t l a u a v
 ) s m i ( e m
 n o i t a u a v
 l ) s m i ( e m
 n o i t l a u a v

























 ) s m i ( e m
 n o i t l a u a v






























 Query
 (a) k2-1000-1000


 Query
 (b) k4-1000-1000

 ) s m i ( e m
 n o i t a u a v
 l



















 Query









 Query







 (c) k2-100-1000 (d) k4-100-1000 ) s m i ( e m
 n o i t l a u a v

































 Query
 (e) k2-10-10000


 Query
 (f) k4-10-10000

 Figure 3: Comparison for AND queries Similar to what was done in [14], where there is both a non-indexed version of the algorithm (i.e., SE) as well as an indexed version of the algorithm (i.e., ILE), we also created two versions for each of our proposed algorithms.
We use IBMS, IIMS, and IAOMS, respectively, to refer to the indexed versions of BMS, IMS, and AOMS.
As in [14], in the non-indexed algorithms, all the data nodes are organized using a B-tree where the B-tree keys are the keywords of the data nodes and the B-tree data associated with each B-tree key is a list of Dewey labels of the data nodes having that key as keyword.
In the indexed algorithms, all the data nodes are organized using a B-tree where each B-tree key is a composite key consisting of a keyword (as primary key) and a Dewey number (as secondary key).
No data values are associated with this composite-key organization.
For the simple approach of evaluating AND-OR queries (Section 4.1), we have six variants of the algorithm denoted by SASE, SA-BMS, SA-IMS, SA-ILE, SA-IBMS, and SA-IIMS; where SAX denotes the variant using algorithm X to evaluate the AND-subqueries of the AND-OR query.
The evaluation of the algorithms was carried out by using di erent classes of queries.
Each class of AND queries is denoted by kN-L-H, where N ,L, and H are three positive integer values: N represents the number of keywords, and L and H, with L   H, represent two keyword frequencies such that one of the N keywords has the low frequency L while each of the remaining N   1 keywords has the high frequency H; thus, L = |S1|.
Each class of AND-OR queries (in CNF) is denoted by cM-kN-L-H, where M represents the number of conjuncts in a query, N represents the number of keywords in each conjunct, L represents the frequency of each keyword in one conjunct, and H (with L   H) represents the frequency of each keyword in the remaining M   1 conjuncts.
For each class of queries, a set of 10 random queries were generated and each query was executed six times and its average execution time over the last  ve runs was computed.
All the experiments were conducted using a DBLP dataset [6] with two million data nodes.
Our implementation used BerkeleyDB (Java Edition) [3] to store the keyword data lists similar to what was done in [14].
The BerkeleyDB database was con gured using a page size of 8KB and a cache size of 1GB.
All the experiments were conducted on a 3GHz dual-core desktop PC with 1GB of RAM.
Figure 3 shows the comparison of the two binary-SLCA algorithms (SE and ILE) against our multiway-SLCA algorithms IMS and IIMS.
To avoid cluttering the graphs, we have omitted the BMS and IBMS algorithms as they were outperformed by the IMS and IIMS algorithms (by up to an order of magnitude), respectively.
Compared to the BMS variants, the IMS variants not only incur fewer number of candidate SLCA computations but they are also more e -cient in SLCA computations.
Each graph in Figure 3 shows the performance comparison for a di erent query class.
For each query class, the ten random queries shown (Q1 to Q10) are ordered in non-descending order of the number of candidate SLCA computations incurred by the IMS algorithm.
Figures 3(a) and 3(b) show the results for the case where the low and high frequencies are the same.
Comparing IMS and IIMS, we observe that IIMS generally performs better than IMS only when the number of candidate SLCA computations is small.
For the binary-SLCA algorithms, our results are consistent with [14] with SE outperforming ILE.
Overall, IMS generally performs the best for both k2-1000-1000 and k4-1000-1000 with IIMS performing better than IMS for some cases.
Figures 3(c) and 3(d) show the results for the case where the low and high frequencies di er by a factor of 10.
In this case, the non-indexed methods generally perform better than the indexed methods.
For k2-100-1000, IMS has an edge over SE.
For k4-100-1000, IIMS performs the best when the number of its candidate SLCA computations is small; otherwise, SE generally has the best performance.
Figures 3(e) and 3(f) show the results for the case where the low and high frequencies di er by a factor of 100.
In this case, the indexed methods perform better than the non-indexed methods.
IIMS and ILE are comparable when the number of candidate SLCA computations by IIMS is small; otherwise, ILE gives better performance.
Figure 4 compares the performance of algorithms SASE, SA-IMS, AOMS, SA-ILE, SA-IIMS, and IAOMS for AND-OR queries; algorithms SA-BMS and SA-IBMS have been omitted as they are outperformed by the IMS variants.
The evaluation times shown in Figure 4 are average evaluation times of ten queries.
Figures 4(a) and 4(b) show the results for the case where the low and high frequencies are the same, while Figures 4(c) and 4(d) show the results for the case where the low and high frequencies di er by a factor of 10 and 100, respectively.
In all these cases, the non-indexed methods outperform their indexed counterparts, with AOMS giving the best performance.
E cient algorithms for computing LCAs on trees have been carefully studied by a number of early work [2, 9].
However, the algorithms designed there are meant for main-memory resident data.
Schmidt et al.
[13] propose the meet operator for querying XML document by computing the LCAs of nodes in XML trees.
XRANK [8] proposes a stack-based keyword search algorithm and the results are ranked by a Page-Rank hyperlink metric extended to XML.
Their ranking techniques are orthogonal to the retrieval and could be easily incorporated into our work.
Another work XSEarch [5], which is an extension of information-retrieval techniques, is mainly focused on the semantics and ranking of query results.
The research work in [14, 12] is the most closely related to our current work, and both work adopt the idea of smallest LCA (SLCA) or Meaningful LCA (MLCA), which are similar ideas.
Li et al. [12] propose a novel schema-free way to incorporate keyword search in XQuery.
They also develop an e cient stack-based MLCA searching algorithm.
XKSearch [14] focuses on  nding the smallest LCA of keywords in XML documents, and it proposes several algorithms, which we compared against in this paper.
More recently, there has also been a lot of interest on keyword search in relational database systems [1, 4, 7, 10, 11] where the emphasis is mainly on optimizing join queries to generate tree tuples.
In this paper, we examined the problem of processing SLCA-based keyword search queries for XML data.
We have presented a novel approach called multiway-SLCA approach that is more e cient than the state-of-the-art binary-SLCA approach for evaluating SLCA-based keyword queries.
In addition, we have also extended our approach to process more general keyword search queries that go beyond the traditional AND semantics to support any combination of AND and OR boolean operators.
Our experimental performance evaluation using real XML datasets demonstrate the e ciency of our new algorithms compared to previous algorithms.
As part of our future work, we intend to extend our approach to handle complex keyword search queries with any combination of AND, OR, and NOT operators.
