In this paper we focus on methods for ex-Background.
ternal evaluation of search engines.
These methods interact only with the public interfaces of search engines and do not rely on privileged access to internal search engine data or on speci c knowledge of how the search engines work.
External evaluation provides the means for objective benchmarking of search engines.
Such benchmarks can be used by search engine users and clients to gauge the quality of the service they get and by researchers to compare search engines.
Even search engines themselves may bene t from external benchmarks, as they can help them reveal their strengths and weaknesses relative to their competitors.
Our study concentrates on measurement of global quality metrics of search engines, like corpus size, index fresh Supported by the European Commission Marie Curie International Reintegration Grant.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Maxim Gurevich Dept.
of Electrical Engineering Technion, Haifa 32000, Israel gmax@tx.technion.ac.il ness, and density of spam or duplicate pages in the corpus.
Such metrics are relevance neutral, and therefore no human judgment is required for computing them.
Still, as external access to search engine data is highly restricted, designing automatic methods for measuring these quality metrics is very challenging.
Our objective is to design measurement algorithms that are both accurate and e cient.
E ciency is particularly important for two reasons.
First, e cient algorithms can be executed even by parties whose resources are limited, like researchers.
Second, as search engines are highly dynamic, e cient algorithms are necessary for capturing instantaneous snapshots of the search engines.
Problem statement.
Let D denote the corpus of documents indexed by the search engine.
We focus on measurement of metrics that can be expressed as either sums or averages over D. Given a target function f : D   R, the sum metric and the average metric corresponding to f are: sum(f ) , Xx D f (x), avg(f ) , sum(f )
 .
(In fact, we address sums and averages w.r.t.
arbitrary measures.
See more details in Section 2.)
Almost all the global quality metrics we are aware of can be expressed as sum or average metrics.
For example, the corpus size, |D|, is the sum of the constant 1 function (f (x) = 1 for all x); the density of spam pages in the corpus is the average of the spam indicator function (f (x) = 1, if x is a spam page, and f (x) = 0, otherwise); the number of unique documents in the corpus is the sum of the inverse duplicate-count function (f (x) = 1/dx, where dx is the number of duplicates x has, including x itself).
Many other metrics, like search engine overlap, sizes of subsets of the corpus, or index freshness can be expressed as sums or averages as well.
We allow also sums and averages of vector-valued functions f : D   Rm, which capture metrics like the distribution of pages in the corpus by language, country domain, or topic.
A search engine estimator for sum(f ) (resp., avg(f )) is a probabilistic procedure, which submits queries to the search engine, fetches pages from the web, computes the target function f on documents of its choice, and eventually outputs an estimate of sum(f ) (resp., avg(f )).
The quality of an estimator is measured in terms of its bias and its variance.
The e ciency of an estimator is measured in terms of the number of queries it submits to the search engine, the number of web pages it fetches, and the number of documents on which it computes the target function f .
State of the art.
Brute-force computation of search quality metrics is infeasible, due to the huge size of the corpus a few thousand queries per day and only the top k matches are returned.
(k is the search engine s result set size limit; e.g., k = 1, 000 for Google and Yahoo!)
If a query  over ows , i.e., has more than k matches, the user has no way of accessing results beyond the top k. Thus, fetching all the pages in a search engine s corpus is practically impossible.
An alternative to brute-force computation is sampling.
One samples random pages from the corpus and uses them to estimate the quality metrics.
If the samples are unbiased, then a small number of them is su cient to obtain accurate estimations.
The main challenge is to design algorithms that can e ciently generate unbiased samples from the corpus using queries to the public interface.
Bharat and Broder [4] were the  rst to propose such an algorithm.
The samples produced by their algorithm, however, su ered from severe bias towards long, content-rich, documents.
In our previous paper [2], we were able to correct this bias by proposing a technique for simulating unbiased sampling by biased sampling.
To this end, we applied several stochastic simulation methods, like rejection sampling [24] and the Metropolis-Hastings algorithm [22, 13].
Stochastic simulation, however, incurs signi cant overhead: in order to generate each unbiased sample, numerous biased samples are used, and this translates into elevated query and fetch costs.
For instance, our most e cient sampler needed about 2,000 queries to generate each uniform sample.
In an attempt to address this lack of e ciency, we also experimented [2] with importance sampling estimation.
Importance sampling [21, 16] enables estimation of sums and averages directly from the biased samples, without  rst generating unbiased samples.
This technique can signi cantly reduce the stochastic simulation overhead.
Nevertheless, our estimators in [2] used stochastic simulation twice (once to select random queries and once to select random documents), and we were able to use importance sampling to eliminate only the latter of the two.
Furthermore, our importance sampling estimator was still wasteful, as it used only a single result of each submitted query and discarded all the rest.
Broder et al. [6] have recently made remarkable progress by proposing a new estimator for search engine corpus size.
Their estimator (implicitly) employs importance sampling and does not resort to stochastic simulation at all.
Moreover, the estimator somehow makes use of all query results in the estimation and is thus less wasteful than the estimators in [2].
Broder et al. claimed their method can be generalized to estimate other metrics, but have not provided any details.
The degree mismatch problem.
A prerequisite for applying importance sampling is the ability to compute for each biased sample an importance weight.
The importance weights are used to balance the contributions of the di erent biased samples to the  nal estimator.
In the estimators of [2, 6], computing the importance weight of a sample document translates into calculation of the document s  degree .
Given a large pool of queries (e.g.,  phrase queries of length 5 , or  8-digit string queries ), the degree of a document w.r.t.
the pool is the number of queries from the pool to whose results x belongs.
As the estimators of [2, 6] choose their sample documents from the results of random queries drawn from a query pool, these samples are biased towards high degree documents.
Document degrees, therefore, constitute the primary factor in determining the importance weights of sample documents.
As importance weights (and hence degrees) are computed for every sample document, degree computation should be extremely e cient.
Ideally, it should be done based on the content of x alone and without submitting queries to the search engine.
The above estimators do this by extracting all the terms/phrases from x and counting how many of them belong to the pool.
The resulting number is the document s predicted degree and is used as an approximation of the real degree.
In practice, the predicted degree may be quite di erent from the actual degree, since we do not exactly know how the search engine parses documents or how it selects the terms by which to index the document.
Moreover, we do not know a priori which of the queries that the document matches over ow; the document may fail to belong to the result sets of such queries if it is ranked too low.
These factors give rise to a degree mismatch a gap between the predicted degree and the actual degree.
The degree mismatch implies that the importance weights used by the estimators are not accurate, and this can signi cantly a ect the quality of the produced estimates.
In [2], we proved that if the density of over owing queries among all the queries that a document matches has low variance, then the bias incurred by degree mismatch is small.
Broder et al. [6] have not analyzed the e ect of degree mismatch on the quality of their estimations.
Several heuristic methods have been used by [2, 6] to overcome the degree mismatch problem.
In order to reduce the e ect of over owing queries, a pool of queries that are unlikely to over ow was chosen ([2] used a pool of phrases of length 5, while [6] used a pool of 8-digit strings).
However, pools that have low density of over owing queries are also more likely to have poor coverage, creating another bias.
[6] remove over owing queries from the pool by eliminating terms that occur frequently in a training corpus.
However, this heuristic can have many false positives or false negatives, depending on the choice of the frequency threshold.
In this paper we show how to over-Our contributions.
come the degree mismatch problem.
We present two search engine estimators that remain nearly unbiased and very ef- cient, even in the presence of highly mismatching degrees.
Our  rst contribution is a rigorous analysis of an  approximate importance sampling  procedure.
We prove that using importance sampling with approximate weights rather than the real weights incurs both a multiplicative bias and an additive bias.
The analysis immediately implies that the estimator of Broder et al. [6] su ers from signi cant bias in the presence of degree mismatch.
Our second contribution is the design of two new importance sampling estimators.
Our estimators use approximate weights, but are able to eliminate the more signi cant multiplicative bias, leading to nearly unbiased estimates.
These estimators can be viewed as generalizations of  ratio importance sampling  (cf.
[20]) and importance sampling with approximate trial weights [2].
The  rst estimator, the Accurate Estimator (AccEst), uses few search engine queries to proba-bilistically calculate exact document degrees, and is thereby able to achieve essentially unbiased estimations.
The second estimator, the E cient Estimator (E Est), predicts document degrees from the contents of documents alone, without submitting queries to the search engine, similarly to [2,
 degree mismatch, E Est estimates this factor by invoking the target function being measured, this costly computation can be done only once in a pre-processing step, and then be reused in multiple invocations of E Est.
Hence, the amortized cost of E Est is much lower than that of AccEst.
The estimations produced by E Est may be slightly less accurate than those of AccEst, because the additive bias cannot be always eliminated.
We note that our estimators are applicable to both the sum metric and the average metric w.r.t.
any target function.
This in contrast to the estimators in [2], which are e ciently applicable only to average metrics, and the estimator in [6], which is applicable only to sum metrics.
Our last contribution builds on the observation that the estimator of Broder et al. implicitly applies Rao Blackwelliza-tion [7], which is a well-known statistical tool for reducing estimation variance.
This technique is what makes their estimator so e cient.
We show that Rao Blackwellization can be applied to our estimators as well and prove that it is guaranteed to make them more e cient as long as the results of queries are su ciently variable.
Experimental results.
We evaluated the bias and the e ciency of our estimators as well as the estimators from [2, 6] on a local search engine that we built over a corpus of
 to estimate two di erent metrics: corpus size and density of sports pages.
The empirical study con rms our analytical  ndings: in the presence of many over owing queries, our estimators have essentially no bias, while the estimator of Broder et al. su ers from signi cant bias.
For example, the relative bias of AccEst in the corpus size estimation was
 al. was 75%.
The study also showed that our new estimators are up to 375 times more e cient than the rejection sampling estimator from [2].
Finally, the study demonstrated the e ectiveness of Rao-Blackwellization by reducing the query cost of estimators by up to 79%.
We used our estimators to measure the absolute sizes of three major search engines.
The results of this study gave gross underestimates of the true search engine sizes, largely due to the limited coverage of the pool of queries we used.
Even so, we showed that our estimates are up to 74 times higher than the estimates produced by (our implementation of) the Broder et al. estimator.
Other related work.
Apart from [4, 2, 6], several other studies estimated global quality metrics of search engines, like relative corpus size.
These studies are based on analyzing anecdotal queries [5], queries collected from user query logs [17, 10], or queries selected randomly from a pool a la Bharat and Broder [12, 8].
Using capture-recapture techniques (cf.
[19]) some of these studies infer measurements of the whole web.
Due to the bias in the samples, though, these estimates lack any statistical guarantees.
A di erent approach for evaluating search quality is by sampling pages from the whole web [18, 14, 15, 1, 23].
Sampling from the whole web, however, is a more di cult problem, and therefore all the known algorithms su er from severe bias.
In this section we introduce notations and de nitions used throughout the paper.
Search engines.
Fix a search engine S whose corpus D we wish to measure.
For each query q, the index of the search engine consists of a list of matching documents from D. Given q, the search engine returns these matches, ranked by relevance.
The search engine has a result set size limit k. If the number of matches for a query q exceeds k, only the top k matches are returned.
We denote the actual list of results returned on query q by results(q).
The cardinality of a query q, denoted card(q), is the total number of matches it has.
We say that q over ows, if card(q) > k, and that it under ows, if card(q) = 0.
Measures and integrals.
We restrict to measurements of metrics that can be written as discrete integrals over D: Int (f ) , Xx D f (x) (x).
Here, f : D   R is a target function and   : D   [0,  ) is a target measure.
  induces a corresponding probability distribution on D: , where Z  = Px D  (x) is the normalization  (x) =  (x) Z  constant of  .
We say that two di erent measures are the same up to normalization, if they induce the same probability distribution, but have di erent normalization constants.
When the target measure is a distribution, we call the integral Int (f ) an average metric.
Otherwise, it is a sum metric.
For example, corpus size is a sum metric, as the corresponding target measure is the uniform 1-measure ( (x) = 1 for all x   D), while the density of spam pages in the corpus is an average metric, because its corresponding measure is the uniform distribution on D ( (x) = 1/|D| for all x).
Everything we do in this paper can be generalized to deal with vector-valued functions f : D   Rm.
Yet, for simplicity of exposition, we focus on scalar functions.
Search engine estimators.
A search engine estimator for a metric Int (f ) is a randomized procedure P, which interacts only with the public interface of the search engine and/or with the web, and produces an estimate of Int (f ).
The procedure is assumed to have access to an  f oracle  and to a  oracle .
Given a document x   D, the f oracle returns f (x) and the  oracle returns  (x).
  is a measure on D, which is identical to   up to normalization.
We denote by Z  the normalization constant of  .
If Int (f ) is a sum metric, we require   =  .
If Int (f ) is an average metric,   can be any measure that is the same as   up to normalization and the normalization constant Z  need not be known in advance.
For example, when estimating the density of spam in the corpus, the  oracle can return for each document x the value  (x) = 1, which is identical to the target uniform distribution   up to normalization.
The quality of an estimator is measured in terms of two parameters: bias and variance.
The bias of P is the difference | E(P)   Int (f )|.
P is called unbiased, if E(P) = Int (f ).
The variance of P is var(P) = E((P   E(P))2).
The estimator s bias and variance can be used (via Cheby-shev s inequality) to determine estimation con dence intervals, i.e., parameters   > 0 and 0 <   < 1 for which Pr((1    ) Int (f )   P   (1 +  ) Int (f ))   1    .
The three expensive resources used by search engine estimators are: (1) queries submitted to the search engine; (2) web pages fetched; (3) calculations of the function f .
The expected query cost of an estimator P, denoted qcost(P), is the expected number of queries P submits to the search en-e ciency of di erent estimators, as they may have di erent variances.
The amortized query cost, de ned as qcost(P)   var(P), is a more robust measure of e ciency.
Expected and amortized fetch/function costs are de ned similarly.
Query pools and document degrees.
Let P be a pool of queries.
For a document x   D, we denote by queriesP (x) the set of queries to whose result sets x belongs: queriesP (x) , {q   P | x   results(q)}.
The degree of x w.r.t.
P, denoted degP (x), is |queriesP (x)|.
A basic task carried out again and again by our algorithms is the following: given a query pool P and a document x, compute degP (x).
Since we need to perform this task ultra-e ciently, we would like to do it based on the content of x alone and without submitting any queries to the search engine.
This may seem initially impossible to do.
However, if P consists of term/phrase queries, we can predict the queries in P that x matches, simply by extracting all the terms/phrases that occur in the text of x and that also belong to P. Let pqueriesP (x) denote this set of predicted queries and let pdegP (x) denote the predicted degree of x, i.e., |pqueriesP (x)|.
pdegP (x) will be our approximation for degP (x).
In practice, however, pqueriesP (x) may contain queries that do not belong to queriesP (x), and, conversely, queriesP (x) may contain queries that do not belong to pqueriesP (x).
We would like to somehow bridge the gap between the two.
Dealing with queries in queriesP (x) \ pqueriesP (x) is relatively easy.
We call a query q valid for x, if x belongs to the result set of q and we could have anticipated that by inspecting the content of x.
That is, q   queriesP (x)   pqueriesP (x).
The set of valid results for a query q is de ned as follows: vresults(q) , {x   results(q) | q is valid for x}.
Our algorithms will use only the valid results of queries, rather than all the results.
Therefore, for each document x, the set of valid queries for x is: vqueriesP (x) , {q   P | x   vresults(q)} = queriesP (x)   pqueriesP (x).
The valid degree of x, denoted vdegP (x), is |vqueriesP (x)|.
By using only valid results, we are guaranteed that vqueriesP (x)   pqueriesP (x), contributing to shrinking the di erence between the two.
Addressing queries in pqueriesP (x)\vqueriesP (x) is a more serious problem, because  guring out which of the queries in pqueriesP (x) do not belong to vqueriesP (x) requires submitting all these queries to the search engine a prohibitively expensive task.
We call the ratio vdegP (X) pdegP (X) the validity density of x and denote it by vdensityP (x).
From the above, vdensityP (x)   [0, 1] for all x.
The closer vdensityP (x) is to
 Usage of the k available results of over owing queries in our estimations is a potential source of bias, since such queries may favor documents with high static rank.
In order to eliminate any bias towards such documents, our algorithms simply ignore over owing queries.
Technically, we do this by de ning all the results of over owing queries to be  invalid .
Therefore, if q over ows, vresults(q) =  .
This in particular means that vqueriesP (x) cannot contain any over owing query.
We say that the pool P covers a document x, if there is at least one query q   P which is valid for x.
That is, vqueriesP (x) 6=  .
Note that documents that do not contain any of the terms/phrases in P or that match only over owing queries in P are not covered by P.
In this section we present a basic importance sampling search engine estimator.
It will be used as a basis for the more advanced estimators presented in subsequent sections.
Setup.
Like the pool-based estimators in [4, 2, 6], our estimator assumes knowledge of an explicit query pool P. For example, in our experiments, we used a pool of 1.75 billion English phrases of lengths 3-5.
Such a pool can be constructed in a pre-processing step, by crawling a representative corpus of web documents and extracting terms/phrases that occur therein (we used the ODP [9] directory for this purpose).
We can run the estimator with any such pool, yet the choice of the pool may a ect the bias and the e ciency of the estimator.
No matter how large P is, in practice, there will always be documents in D that P does not cover.
Since our estimator uses only queries from P, it can never reach such documents.
This means that our estimator can estimate integrals only over the set of documents that are covered by P and not over the entire corpus D. So regardless of the statistical bias of the estimator, there will be an additional  builtin  bias that depends on the coverage of the chosen query pool.
To simplify our discussion, from now on, we suppress this coverage-induced bias and use D to denote only the documents that are covered by P.
Importance sampling estimation.
Recall that we would like to estimate the integral Int (f ) for some given target function f and target measure   on D. The naive Monte Carlo estimator (cf.
[20]) for Int (f ) works as follows: (1) sample a random document X from the distribution   induced by  ; (2) compute the normalization constant Z  of  ; (3) output f (X)   Z .
It is easy to check that this estimator is unbiased.
Its variance can be reduced by averaging over multiple independent instances of the estimator.1 In our setting, however, this simple estimator is impractical, for the following reasons: (1) sampling from the distribution   may be hard or costly (e.g., when   is a uniform measure on D); (2) computation of the normalization constant Z  may be costly (e.g., in corpus size estimation, Z  = |D|, which is exactly the quantity we need to estimate); (3) the random variable f (X) may have high variance.
Importance sampling [21, 16, 20] can be used in these circumstances to obtain a more e cient estimator.
The basic idea of importance sampling is the following.
Instead of sampling a document X from  , the estimator samples a document Y from a di erent trial distribution p on D. p can be any distribution, as long as supp(p)   supp( ) (here, supp(p) = {x   D | p(x) > 0} is the support of p; supp( ) is de ned similarly).
In particular, we can choose it to be a distribution that is easy to sample from.
The importance sampling estimator is then de ned as follows: IS(Y) , f (Y)    (Y) p(Y) = f (Y)   w(Y).
The correction term w(Y) ,  (Y) p(Y) is called the  importance weight  and it guarantees that IS(Y) is an unbiased estima-
averages (cf.
[11, 6]), exist.
For simplicity of exposition, we will focus mainly on averaging in this paper.
Ep(IS(Y)) = = Xy supp( ) p(y)f (y)  (y) p(y) = Xy supp( ) f (y) (y) = Int (f ).
Implementation of an importance sampling estimator requires: (1) ability to sample e ciently from the trial distribution p; and (2) ability to compute the importance weight w(y) and the function value f (y), for any given element y   D. There is no need to know the normalization constant Z  or to be able to sample from  .
The sample space.
The sample space of the importance sampling estimator proposed in our previous paper [2] was the corpus of documents D. The trial distribution p was the  document degree distribution , in which documents are sampled proportionally to their degree.
In order to sample documents from this distribution, we had to sample queries from the pool P proportionally to their cardinality, and then to sample random documents from the result sets of these queries.
As cardinalities of queries are not known in advance, sampling queries from P required application of rejection sampling.
This step incurred signi cant overhead.
In this paper we propose a di erent sample space.
Rather than sampling queries and then documents in two separate steps, we sample them together.
The sample space is then   = P   D. Each sample is a query-document pair (q, x).
We extend the target measure   on D into a target measure   on   and the function f on D into a function F on  .
The extension is done in such a way that Int (F ) equals Int (f ).
We thus reduce the problem of estimating Int (f ) to the problem of estimating Int (F ).
For the latter, we can apply importance sampling directly on the two-dimensional sample space  , without having to resort to rejection sampling.
Let   = P   D. We extend   into a measure on   as follows:  (q, x) , I(x vresults(q)) (x) , where I is an indicator function: I(condition) = 1 if the condition is true, and 0 otherwise.
The connection between   and   is given by the following proposition: vdeg(x) Proposition 3.1.   is the marginal measure of   on D.
Furthermore, the normalization constants of   and   are the same.
It follows from the proposition that   is a distribution if and only if   is a distribution.
Similarly, we extend the function f on D into a function F on   as follows: F (q, x) , f (x).
It is easy to see that Int (F ) = Int (f ).
Our estimator therefore estimates the integral Int (F ).
The trial distribution.
We next describe the trial distribution for selecting query-document pairs from  .
Let P+ denote the collection of queries in P that have at least one valid result: P+ , {q   P | vresults(q) 6=  }.
Our trial distribution selects a pair (q, x) as follows: (1) pick a query q   P+ uniformly at random; (2) pick a document x   vresults(q) uniformly at random: p(q, x) , 1
   I(x   vresults(q)) vcard(q) .
one valid result.
We then select a document from the set of valid results of this query uniformly at random.
2: while (true) do








 Q := uniformly chosen query from P submit Q to S if Q over ows continue results(Q) := results returned by S download all pages in results(Q) vresults(Q) := valid results extracted from results(Q) if (vresults(Q) 6=  ) then X := uniformly chosen document from vresults(Q) return (Q,X) Figure 1: Trial distribution sampler The importance weights.
The importance weights corresponding to the target measure   and the trial distribution p are the following: w(q, x) =  (q, x) p(q, x) =  (x)   |P+|   vcard(q) vdeg(x) .
Thus, the importance sampling estimator for Int (f ) is: IS(Q, X) , f (X)    (X)   |P+|   vcard(Q) vdeg(X) , where (Q, X) is a sample from the trial distribution p. The caveat with this estimator is that computing the importance weights may be hard or costly to do for three reasons: (1) we cannot compute vdeg(x) without submitting queries to the search engine; (2) we do not know a priori which of the queries in P have at least one valid result and therefore cannot compute |P+|; and (3) if Int (f ) is an average metric, we may know  (x) only up to normalization.
The estimator of Broder et al. [6] resembles the above importance sampling estimator for the special case of corpus size estimation.
As they could not compute the exact importance weights, they used approximate importance weights, by substituting pdeg(x) for vdeg(x).
It is not clear, however, what is the e ect of the approximate importance weights on the bias of the estimator.
Also, it is unknown how to extend the estimator to work for average metrics.
We address these issues in the next section.
Suppose we come up with an approximate weight function u(q, x), which is  similar , but not identical, to w(q, x) (we will discuss possible alternatives for u(q, x) in the next section).
What is the e ect of using u(q, x) rather than w(q, x) in importance sampling?
In the following we analyze the quality and performance of this  approximate importance sampling  procedure.
Bias analysis.
Suppose supp(u)   supp(w).
The following lemma analyzes the bias of the approximate importance sampling estimator: AIS(Q, X) = f (X)   u(Q, X).
Lemma 4.1.
Ep(AIS(Q, X)) = Here, vcard(q) is the number of valid results q has.
Sampling from p can be done easily (see Figure 1): we repeatedly select queries from P uniformly at random, submit them to the search engine, and extract the valid results from each such query.
We stop when reaching a query that has at least = Int (f )   E  (cid:18) u(Q, X) w(Q, X)(cid:19) + Z    cov  (cid:18)f (X), u(Q, X) w(Q, X)(cid:19) , where   is the distribution induced by   and Z  is the normalization constant of  .
of all the other results in this paper, are postponed to the full version of the paper [3].
It follows from the lemma that there are two sources of bias in this estimator: (1) multiplicative bias, depending on the expectation of u/w under  ; and (2) additive bias, depending on the correlation between f and u/w and on the normalization constant Z .
Note that the multiplicative factor, even if small, may have a signi cant e ect on the estimator s bias, and thus must be eliminated.
The additive bias is typically less signi cant, as in many practical situations f and u/w are uncorrelated (e.g., when f is a constant function as is the case with corpus size estimation).
For a query-document pair (q, x), the ratio u(q, x)/w(q, x) is called the weight skew at (q, x).
The multiplicative bias factor is the expected weight skew under the target distribution  .
In order to eliminate this bias, we need to somehow estimate the expected weight skew.
For now, let us assume we have some unbiased estimator WSE for E  (cid:16) u(Q,X) w(Q,X)(cid:17) (WSE may depend on the same sample (Q, X) used by the importance sampling estimator).
It follows from Lemma 4.1 that: Ep(AIS(Q, X))
 = Int (f ) + w(Q,X)(cid:17) Z    cov  (cid:16)f (X), u(Q,X) E  (cid:16) u(Q,X) w(Q,X)(cid:17) .
Thus, the ratio of the expectations of the two estimators, AIS and WSE, gives us the desired result (Int (f )), modulo an additive bias factor.
Ignoring for the moment this additive bias, it would seem that a good estimator for Int (f ) is the ratio AIS WSE .
However, there is one problem: the expectation of a ratio is not the ratio of the expectations, i.e., E(cid:0) AIS WSE(cid:1) 6= E(AIS)
 To solve this problem, we resort to a well-known trick from statistics: if we replace the numerator and the denominator by averages of multiple independent instances of the numerator estimator and of the denominator estimator, the di erence between the expected ratio and the ratio of expectations can be diminished to 0.
This idea is formalized by the following theorem: Theorem 4.2.
Suppose A and B are two estimators such E(B) = I.
Let A1, .
.
.
, An and B1, .
.
.
, Bn be n indepen-that E(A) dent instances of A and B, respectively.
Then, | E(cid:18) 1
 i=1 Ai n Pn i=1 Bi(cid:19)   I|   n Pn  (cid:18)I   var(B)
 +
 n | cov(A, B)| E2(B) (cid:19) + o(cid:18) 1 n(cid:19) .
We can therefore de ne the approximate ratio importance sampling estimator for Int (f ) as follows:

 n Pn i=1 f (Xi)   u(Qi, Xi)
 , i=1 WSEi n Pn where (Q1, X1), .
.
.
, (Qn, Xn) are n independent samples from the trial distribution p and WSE1, .
.
.
, WSEn are n independent estimators of the weight skew (WSEi may depend on (Qi, Xi)).
Using Lemma 4.1 and Theorem 4.2, we can analyze the bias of this estimator: Lemma 4.3.
If E(WSE) = E  (cid:16) u(Q,X) w(Q,X)(cid:17), then w(Q,X)(cid:17) Z    cov  (cid:16)f (X), u(Q,X) | Ep(ARIS) Int (f )|   E  (cid:16) u(Q,X) w(Q,X)(cid:17) +O(cid:18) 1 n(cid:19) , where the O(1/n) term suppresses constant factors that depend on var(WSE) and on cov(f (X)   u(Q, X), WSE).
We conclude from the lemma that if we use su ciently many samples, then we are likely to get an estimate of Int (f ), which has only additive bias that depends on the correlation between f and u/w.
In this section we describe two variants of the approximate ratio importance sampling estimator (ARIS) discussed above.
The two estimators, the Accurate Estimator (Ac-cEst) and the E cient Estimator (E Est), o er di erent tradeo s between accuracy and e ciency.
The former has lower bias, while the latter is more e cient.
The estimators di er in the choice of the approximate importance weight function u(q, x) and in the expected weight skew estimator WSE.
Before we show how u and WSE are de ned in each of the estimators, let us rewrite the importance weights: w(q, x) =  (x)   |P+|   vcard(q) vdeg(x) =  (x)   |P+|   vcard(q) pdeg(x)   vdensity(x) .
Of the di erent terms that constitute the weight, the three we may not know a priori are  (x), |P+|, and vdensity(x).
vcard(q) is known, because we always obtain the pair (q, x) after having submitted q to the search engine and extracting its valid results.
pdeg(x) is known, because we can extract the predicted queries from the content of x.
The Accurate Estimator (AccEst) uses approximate weights uacc(q, x) that are equal to the exact weights w(q, x), up to a constant factor.
It follows that uacc(q, x)/w(q, x) is constant, and hence the correlation between f and uacc/w is 0.
Using Lemma 4.3, the bias of AccEst is then only O(1/n).
How do we come up with approximate weights that equal the exact weights up to a constant factor?
Well, we are unable to e ciently do this with deterministic approximate weights, but rather with probabilistic ones.
That is, Ac-cEst uses a probabilistic weight function uacc(q, x), for which E(uacc(q, x)) = const   w(q, x).
As our analysis for approximate importance sampling easily carries over to probabilistic weights as well, we can still apply Lemma 4.3 and obtain the desired bound on the bias of AccEst.
The approximate weights are de ned as follows: uacc(q, x) ,  (x)   |P|   vcard(q)   IVD(x) pdeg(x) .
That is,  (x) is approximated by  (x) (they are the same, if Int (f ) is a sum metric), |P+| is approximated by |P|, and the term 1/ vdensity(x) is estimated probabilistically by the  Inverse Validity Density Estimator  (IVD) described below.
Note that apart from the computation of IVD(x), computing uacc(q, x) requires no search engine queries.
Figure 2 shows a procedure for estimating 1/ vdensity(x) for a given document x, using a limited number of queries.
dom from the set of predicted queries pqueries(x).
It submits each query to the search engine and checks whether they are valid for x.
The procedure stops when reaching the  rst valid query and returns the number of queries sampled so far.
As this number is geometrically distributed with vdensity(x) as the success parameter, the expectation of this estimator is exactly 1/ vdensity(x).
Note that the procedure is always guaranteed to terminate, because we apply it only on documents x for which vdensity(x) > 0.
i := 1
 2: pqueries(x) := predicted queries for x
 4: while (true) do




 Q := uniformly chosen query from pqueries(x) submit Q to S results(Q) := results returned by S if (Q does not over ow and x   results(Q)) return i i := i + 1 Figure 2: Estimator for the inverse of the validity density The expectation of uacc(q, x) is analyzed as follows: E(uacc(q, x)) = E(cid:18)  (x)   |P|   vcard(q)   IVD(x) pdeg(x) (cid:19) = =



      (x)   |P+|   vcard(q)   E(IVD(x)) pdeg(x) Z  Z    w(q, x).
.
Z  Z 
 |P+|   Z  Hence, the expectation of uacc(q, x) equals the weight w(q, x), up to the unknown multiplicative constant It immediately follows that also the expected weight skew is
 |P+|   Z  and that cov  (f (X), uacc(Q, X)/w(Q, X)) = 0.
How we obtain an unbiased estimator WSE for the expected weight skew is di erent between the case Int (f ) is a sum a metric and the case it is an average metric.
Int (f ) is a sum metric.
Here, the  oracle Case 1: computes the target measure   explicitly.
That is,   =   and Z 
 Z  If we sample a query Q  uniformly at random from P, it has a probability of |P+| to have at least one valid result.
Therefore, we can estimate |P| |P+| as follows: repeatedly sample queries uniformly at random from P and submit them to the search engine; stop when reaching the  rst query that has at least one valid result; the number of queries submitted is an unbiased estimator of = 1, so the only term we need to estimate is |P|


 Case 2: Int (f ) is an average metric.
In this case   is a distribution, and thus Z  = 1.
Therefore, the expected |P+|   Z .
As Z  is not known in advance, weight skew is we cannot use the same expected weight skew estimator as above.
On the other hand, we observe that since   is a distribution, then the approximate weight uacc(Q, X) itself, where (Q, X)   p, is an unbiased estimator of the expected weight skew.
This follows from Lemma 4.1 with f   1: Proposition 5.1.
Ep(uacc(Q, X)) = Z    E  (cid:18) uacc(Q, X) w(Q, X) (cid:19) .
As Z  = 1, uacc(Q, X) is indeed an unbiased estimator of the expected weight skew.
Analysis.
By Lemma 4.3, the bias of the estimator is at most Z   cov    (cid:16) f (X), uacc (Q,X) w(Q,X) (cid:17)  (cid:16) uacc (Q,X) w(Q,X) (cid:17)
   + O(cid:0) 1 n(cid:1).
Recall that the covariance term is 0, and thus the bias is only O(1/n).
The cost of the computing uacc(q, x) is dominated by the cost of computing the inverse validity density.
This computation requires O(1/ vdensity(x)) queries to the search engine in expectation.
Complete analysis of the e ciency of the estimator is postponed to the full version of the paper.
The E cient Estimator (E Est) uses deterministic approximate weights, which require no queries to the search engine to compute, similarly to the estimators of [2, 6]: ue (q, x) ,  (x)   |P|   vcard(q) pdeg(x) .
That is,  (x) is approximated by  (x) (they are the same, if Int (f ) is a sum metric), |P+| is approximated by |P|, and the term 1/ vdensity(x) is ignored.
The weight skew in this case is characterized as follows: Proposition 5.2. ue (q, x)/w(q, x) = |P| |P+|  Z vdensity(x).
The estimator for the expected weight skew is again different between the case Int (f ) is a sum metric and the case Int (f ) is an average metric.
Case 1: Int (f ) is a sum metric.
By Proposition 5.1, ue (Q, X), where (Q, X)   p, is an unbiased estimator of the expected weight skew, modulo the constant factor Z : Ep(ue (Q, X)) = Z    E  (cid:18) ue (Q, X) w(Q, X) (cid:19) .
In our case   is not a distribution, so Z  is unknown.
Hence, ue (Q, X) is not su cient by itself to estimate the expected weight skew.
In order to obtain an estimator for the expected weight skew, we will divide ue (Q, X) by an unbiased estimator for Z .
We observe that Z  = Int (1), where 1 is the constant
 f   1, we can obtain an unbiased estimator of Z .
We thus have: Theorem 4.2, we can get a nearly unbiased estimator of the expected weight skew by averaging over multiple instances of ue (Q, X) and AccEst: E(AccEst) = E  (cid:16) ueff(Q,X) w(Q,X) (cid:17).
So, using again p(ueff(Q,X))

 i=1 ue (Qi, Xi) i=1 AccEsti .
n Pn n Pn Here, (Q1, X1), .
.
.
, (Qn, Xn) are n independent samples from p and AccEst1, .
.
.
, AccEstn are n independent Accurate Estimators for Int (1).
At this point the reader may be wonder why the E cient Estimator is more e cient than the Accurate Estimator.
After all, the E cient Estimator calls the Accurate Estimator!
The rationale behind this is the following.
Indeed, if we need to estimate only a single integral Int (f ), then the E cient Estimator is less e cient than the Accurate Estimator.
However, in practice, we usually need to compute multiple integrals Int (f1), .
.
.
, Int (ft) w.r.t.
the same target measure  .
Note that the constant Z , for whose on   and is independent of the target function f .
Therefore, we can reuse the estimation of Z  in the estimations of Int (f1), .
.
.
, Int (ft).
This implies that the amortized cost of the E cient Estimator is lower than that of the Accurate Estimator.
In this case   is Case 2: Int (f ) is an average metric.
a distribution and thus Z  = 1.
Therefore, by Proposition
 of the expected weight skew.
Analysis.
By Lemma 4.3, the bias of the estimator is at Z   cov   most ueff (Q,X) w(Q,X) (cid:17)  (cid:16) f (X),  (cid:16) ueff (Q,X) w(Q,X) (cid:17)
   + O(cid:0) 1 n(cid:1).
By the characterization of the weight skew using the validity density (Proposition 5.2) and recalling that   is the marginal distribution of   on D (Proposition 3.1), we can rewrite the bias as: Z    cov  (f (X), vdensity(X)) E  (vdensity(X)) + O(1/n).
That is, as long as the target function is not correlated with the validity density of documents, the bias is low.
The E cient Estimator is indeed e cient, because each approximate weight computation requires only fetching a single page from the web and no queries to the search engine.
Complete analysis of the e ciency of the estimator is postponed to the full version of this paper.
There is some inherent ine ciency in the importance sampling estimators: although each random query they submit to the search engine returns many results, they use at most a single result per query.
All other results are discarded.
The corpus size estimator of Broder et al. [6] uses all query results, and not just one.
We observe that what they did is an instance of the well-known Rao-Blackwellization technique for reducing estimation variance.
We next show how to apply Rao-Blackwellization on our importance sampling estimators in a similar fashion.
Recall that the basic approximate importance sampling estimator is AIS(Q, X) = f (X)   u(Q, X), where (Q, X) is a sample from the trial distribution p and u(Q, X) is an approximate weight.
Suppose now that instead of using only this single document in our basic estimator, we use all the query results: AISrb(Q) =
 vcard(Q) XX vresults(Q) f (X)   u(Q, X).
Each instance of AISrb is an average over several correlated instances of AIS.
The main point is that computing these correlated instances in bulk can be done with a single query.
The Rao-Blackwell theorem (cf.
[7]) implies that AISrb(Q) can be only better than AIS(Q, X) as an estimator of Int (f ): Theorem 6.1 (Rao-Blackwell Theorem).
AISrb has the same bias as AIS: E(AISrb(Q)) = E(AIS(Q, P)).
The variance of AISrb can only be lower: var(AISrb(Q)) = var(AIS(Q, X))   E(var(AIS(Q, X)|Q)).
By the above theorem, the expected reduction in variance is E(var(f (X)   u(Q, X))|Q), where Q is a uniformly chosen query from P+ and X is a uniformly chosen document from Q.
That is, the more variable are the results of queries w.r.t.
the target function f , the higher are the chances that Rao-Blackwellization will help.
In our empirical study we show that in practice Rao-Blackwellization can make a dramatic e ect.
See Section 7.
The variance reduction achieved by Rao-Blackwellization can lead to lower costs, as fewer instances of the estimator are needed in order to obtain a desired accuracy guarantee.
On the other hand, each instance of the estimator requires many more weight and function calculations (as many as the number of results of the sampled query), and if these are very costly (as is the case with the Accurate Estimator), then the increase in cost per instance may outweigh the reduction in the number of instances, eventually leading to higher amortized costs.
We conclude that Rao-Blackwellization should be used judiciously.
We conducted two sets of experiments.
In the  rst set we performed comparative evaluation of the bias and amortized cost of our new estimators, of the rejection sampling estimator from our previous paper [2], and of the Broder et al. estimator [6].
To this end, we ran all these estimators on a local search engine that we built over 2.4 million English documents fetched from ODP [9].
As we have ground truth for this search engine, we could compare the measurements produced by the estimators against the real values.
The second set of experiments was conducted over three major real search engines.
We used the Accurate Estimator to estimate the corpus size of each the search engines, with and without duplicate elimination.
(More accurately, we estimated sizes of large subsets of the search engine corpora.)
Experimental setup.
We used the same ODP search engine as in our previous paper [2].
The corpus of this search engine consists only of text, HTML, and pdf English-language documents from the ODP hierarchy.
Each document was given a serial id and indexed by single terms and phrases.
Only the  rst 10,000 terms in each document were considered.
Exact phrases were not allowed to cross boundaries, such as paragraph boundaries.
We used static ranking by serial id to rank query results.
In order to construct a query pool for the evaluation experiments, we split the ODP data set into two parts: a training set, consisting of every  fth page (when ordered by id), and a test set, consisting of the rest of the pages.
We used the training set to create a pool of phrases of length 4.
The measurements were done only on the test set.
The experiments on real search engines were conducted in February 2007.
The pool used by our estimators was a pool of 1.75 billion phrases of lengths 3-5 extracted from the ODP data set (the entire data set, not just the test set).
Evaluation experiments.
We compared the following
 wellization; (2) AccEst, with Rao Blackwellization; (3) E Est, without Rao Blackwellization; (4) E Est, with Rao Black-wellization; (5) the Broder et al. estimator; (6) the rejection sampling estimator from our previous paper.
We used the estimators to measure two metrics: (1) corpus size (i.e., the size of the test set); (2) density of pages (We used a simple keyword based classi er to determine whether a page is about sports or not.)
Note that the  rst metric is a sum metric, while the second is an average metric.
We did not use the rejection sampling estimator for estimating corpus size, as it can handle only average metrics.
We did not use the Broder et al. estimator for estimating the density of sports pages, because it can handle only sum metrics.
In order to have a common baseline, we allowed each estimator to use exactly 1 million queries.
Each estimator produced a di erent number of samples from these queries, depending on its amortized query cost.
We ran each experiment four times, with di erent values of the result set size limit k (k = 5, 20, 100, 200).
This was done in order to track the dependence of the estimators  bias and cost on the density of over owing queries (the lower k, the higher is the density).
Figure 3(a) compares the relative bias (bias divided by the estimated quantity) of our two estimators and the estimator of Broder et al. when measuring corpus size.
Figure 3(b) compares the relative bias of our two estimators and the rejection sampling estimator when measuring density of sports pages.
The results for the Rao-Blackwellized versions of these estimators are suppressed, since Rao-Blackwellization has no e ect on bias.
The results for the corpus size clearly show that our estimators have no bias at all, while the estimator of Broder et al su ers from signi cant bias, which grows with the density of over owing queries in the pool.
For example, for k = 5, the relative bias of the Broder et al. estimator was about 75%, while the relative bias of our estimators was 0.01%.
Note that since the target function is constant in this case, then its value has no correlation with the weight skew, which explains why also E Est has no bias.
The results for the density of sports pages show that AccEst is unbiased, as expected.
E Est has small bias, which emanates from a weak correlation between the function value and the validity density.
The rejection sampling method has a large observed bias, primarily because it produced a small number of samples and thus its variance is still high.
Figures 4(a) and 4(b) compare the amortized costs of the regular and the Rao-Blackwellized versions of our two estimators and of the rejection sampling estimator.
We used a square root scale in order to  t all the bars in a single graph.
The results clearly indicate that Rao-Blackwellization is effective in reducing estimation variance (and therefore also amortized costs) in both metrics and both estimators.
For example, in corpus size estimation, when k = 200, Rao-Blackwellization reduced the amortized query cost of AccEst by 79% and the query cost of E Est by 60%.
Furthermore, the amortized cost of the rejection sampling estimator is tremendously higher than the amortized cost of our new estimators (even the non-Rao-Blackwellized ones).
For example, when k = 200, Rao-Blackwellized E Est was 375 times more e cient than rejection sampling!
Experiments on real search engines.
We used our most accurate sampler, AccEst, to estimate the corpus sizes of three major search engines.
For reference, we also ran the Broder et al. estimator with the same query pool.
These measurements count duplicate pages as separate pages.
We also measured the duplicate-free size of the corpus, by estimating the average number of duplicates each document in the corpus has.
The results, together with con dence intervals, are plotted in Figure 5.
It can be seen that the estimations we got are far below the reported sizes of search engines.
The main reason for this is that our estimators e ectively measure the sizes of only subsets of the corpora the indexed pages that match at least one phrase from the pool.
As our pool consists of English-only phrases, pages that are not in English, not in HTML, pdf, or text format, or pages that are poor in text, are excluded from the measurement.
A second reason is that search engines may choose sometimes not to serve certain pages, even though these pages exist in their index and match the query, e.g., because these pages are spam or duplicates.
Another observation we can make from the results is that over owing queries really hurt the Broder et al. estimator also on live search engines.
We note that like Broder et al, we  ltered out from the query pool all phrases that occurred frequently (at least 10 times) in the ODP corpus.
Even after this  ltering, 3% of the queries over owed on the  rst search engine, 10% on the second, and 7% on the third.
The over owing queries probably incurred high bias that made the estimates produced by the Broder et al. estimator to be much lower than ours.
Finally, the experiments reveal search engines deal differently with duplicates.
In one of the search engines, the corpus size, after removing duplicates, shrunk in 28%, while in the other two it shrunk in 14% and 17%, respectively.
Broder et al, with dups AccEst, with dup AccEst, w/o dups ) s n o i l l i b ( e z i s s u p r o c e t u o s b
 l











 Figure 5: Corpus sizes of three major search engines, with and without elimination of duplicates.
In this paper we presented two new estimators for search engine quality metrics that can be expressed as discrete integrals.
Our estimators are able to overcome the  degree mismatch  problem and thereby be accurate and e cient at the same time.
We show both analytically and empirically that our estimators beat recently proposed estimators [2, 6].
In designing our estimators we employ a combination of statistical tools, like importance sampling and Rao Black-wellization.
By carefully analyzing the e ect of approximate weights on the bias of importance sampling, we were able to design procedures to mitigate the bias.
This bias-elimination technique for approximate importance sampling may be applicable in other scenarios as well.
a i b e v i t a l e

























 t s o c y r e u q d e z i t r o m a f o t o o r e r a u q
 Broder et al EffEst AccEst i s a b e v i t a e
 l Rejection sampling EffEst AccEst















 Result set size limit (k) (a) Corpus size.
Result set size limit (k) (b) Density of sports pages.
Figure 3: Relative bias of the estimators.
EffEst without RB EffEst with RB AccEst without RB AccEst with RB Rejection sampling EffEst without RB EffEst with RB AccEst without RB AccEst with RB t s o c y r e u q d e z i t r o m a f o t o o r e r a u q


















 Result set size limit (k) (a) Corpus size.
Result set size limit (k) (b) Density of sports pages.
Figure 4: Square root of amortized query cost of the estimators.
