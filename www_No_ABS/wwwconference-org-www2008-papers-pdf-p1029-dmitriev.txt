This paper discusses the problem of automatically identifying the boundaries of information resources on the Web, focusing on a specific type of resources called compound documents (cDocs) [1][4].
A compound document is a set web pages that in aggregate correspond to a semantically coherent information entity.
A typical example of a cDoc is a web news article consisting of several html pages1, or a set of web pages describing a digital camera, with different pages dedicated to specifications, reviews, photographic tests, etc2.
The ability to automatically identify cDocs would be useful in many applications, such as web and intranet search, improved 1 http://www.nytimes.com/2004/10/12/politics/campaign/12policy.html 2 http://www.dpreview.com/reviews/canoneos40d/ Copyright is held by the author/owner(s).
usability and functionality of web applications, automated collection generation, information extraction and content summarization.
For example, applied to web search it would let search engines index a document as a whole, instead of its individual pages, which could lead to improvement in the quality of search results [1], more accurate link analysis algorithms, and more accurate presentation of the results.
The problem of identifying cDocs was previously addressed in [1] and [5] using heuristic approaches.
However, these approaches fail to take into account the variety of goals with which web sites are created leading to different conventions being used in different user domains.
These approaches also do not account for the application-specific nature of cDocs3, enforcing a one-size-fits-all approach.
Our previous work [4] proposed a machine learning based approach to identifying cDocs, designed to address the above problems.
The algorithm used as training data several web sites with manually labeled cDocs, and was then applied to identify cDocs on new web sites.
Experiments on a set of educational web sites showed promising results, but also revealed several problems with this approach (see [4] for details).
This paper proposes a different scenario for identifying cDocs.
In this scenario, several example cDocs from a web site are used to train the algorithms that will then identify all other cDocs on the same web site.
This paper compares for this new scenario the algorithm of [4] to two other algorithms previously used in graph mining and natural language processing domains.
Baseline.
Several approaches based on the heuristics from [1] and [5] were used as a baseline.
One heuristic rule found to be very useful in [1] was to split the web site into cDocs according to the directory structure.
Another approach is to use a single feature, e.g.
content similarity, and compute its value for every pair of connected pages on the web site.
Then, any threshold value t defines a binary relation Rt on the set of all web pages, where for two pages p1 and p2, they are in the relation Rt(p1,p2) iff content similarity between p1 and p2 is greater than t. Transitive closure of this relation gives a set of cDocs.
We tried 5 such approaches based on different features, using the optimal value of t in the experiments.
Weighted Graph Clustering.
The Weighted Graph Clustering, or WGC [4], learns clustering of the web site based on a detailed analysis of the features of individual web pages and their immediate neighbors.
On the training phase, the user provides a few examples of cDocs on a web site.
Then, for every hyperlink at least one end of which is in a user-provided cDoc, a vector of feature values Xij is computed.
Given training data in the form of pairs (Xij, within/between), where the label indicates whether the hyperlink is within a user-provided cDoc, a logistic regression model is trained to estimates the probability of the hyperlink being within a cDoc.
On the inference phase, the learned model is applied to every hyperlink to generate for the web site a weighted graph.
Then, a variant of the shortest link

 criteria, is applied.
The resulting clusters are taken as cDocs.
Collective Clustering.
The Collective Clustering, or CCL, is a Conditional Markov Network model similar to the model of identity uncertainty proposed in [6] for the noun coreference task.
Let P be a set of random variables corresponding to the pages of a web site, and let L be a set of binary random variables corresponding to hyperlinks and specifying whether the hyperlink is within or between cDocs.
The CCL models the conditional P L P using potential functions fn over probability distribution ( | cliques in the web site graph:

 f p p l ,   n n ij
 ( exp(   )) = ) ) , i ( | j
 i , j n , The model is trained using the procedure described in [6].
The resulting labelling is taken as cDocs4.
Note that even though only potential functions over pairs of pages (cliques of size 2) are used, the labelling decision for a certain pair influences the values of potential functions of the overlapping pairs.
This collective nature of label assignment is the main difference of the CCL from the WGC, which makes labelling decisions independently.
Generalized Pattern Matching.
The Generalized Pattern Matching, or GPM [3], does not analyze content of web pages, but performs a global analysis of the web site graph using pattern matching techniques.
Based on the subgraphs corresponding to the user provided examples of cDocs the algorithm identifies their structural signatures (cores).
Intuitively, cores are subgraphs often encountered inside cDocs which do not cross cDoc boundaries.
Then, the GPM finds all occurrences of cores on the web site, and expands them to obtain complete cDocs according to a specified expansion rule.
(see [2][3] for details).
It turned out that the original algorithm from [3] applied to the problem of finding cDocs suffers from severe performance problems.
Therefore we used approximations on several steps of the algorithm, all based on sampling over search paths in the graph.
Details can be found in [2].
The dataset consisted of 60 web sites from 3 categories collected mostly from DMOZ directory: 20 educational, 20 news, and 20 commercial web sites.
On average, a web site had 169 pages and 1398 links.
For every web site, cDocs were manually identified by 3 labelers, resulting in 19, 12, and 20 cDocs per web site on average.
The average mutual agreements among labelers were rather low, between 0.4 and 0.6.
This confirms the point mentioned earlier that the aggregation criteria vary depending on the user or application.
It also suggests that one-size-fits-all heuristic approaches to cDoc identification will not be able to produce good results for all labelers.
For each web site, separate experiments were run for each labeler using 1, 2, and 3 cDocs as training examples, and all cDocs on the web site as a test set.
Recall was used as a primary evaluation measure5.
Tables 1 and 2 summarize the baseline performance.
As one can see, the relative order of the methods is similar for all three labelers.
There is also no single feature that
 Thus, a resulting labeling is always a non-overlapping set of clusters.
contain incomplete cDocs, making it difficult to use precision as an evaluation measure.
Note, however, that since cDocs do not overlap it is not possible to optimize recall at the expense of precision in our case.
does best for all three categories of web sites.
The results suggest that no single feature is sufficient for identifying cDocs.
Table 1.
Performance of baseline approaches on all sites.
content


 outlinks


 filename


 title


 directory


 Table 2.
Performance of baseline approaches on different categories of content


 sites for labeler L1.
filename


 outlinks


 title


 directory


 edu news com


 Table 3.
Performance of all algorithms on all sites.
Best baseline GPM









 Table 4.
Performance of all algorithms on different categories of sites for labeler L1 using 3 training cDocs.
Best baseline GPM


 edu news com Tables 3 and 4 present the results for all algorithms using 3 training cDocs.
The results are averages over 10 runs of the experiment, the numbers in parentheses are standard deviations.
For all labelers and all categoris the WGC outperforms all other approaches.
This indicates that, while no single feature can reliably identify cDocs, a machine learned combination of features can do that quite accurately.
Poor performance of the GPM is due to a very high density of news and commercial web sites (on more sparse educational web sites the GPM performs significantly better).
The reasons for poor performance of the CCL are not clear.
One hypothesis is that the presence of meaningless navigational links among cDocs leads to imprecise parameter estimation, another hypothesis is that the approximation algorithm used for parameter estimation of the model is not appropriate for our problem.
Developing a higher quality relational model for identifying cDocs is our primary direction for future work.
The author would like to thank Carl Lagoze, Thorsten Joachims, William Y.
Arms, and Paul Ginsparg for valuable comments on this work, and Ryan Workman and Stuart Tettemer for help with labeling the data.
This work was supported by the National Science Foundation grant number IIS-0430906.
