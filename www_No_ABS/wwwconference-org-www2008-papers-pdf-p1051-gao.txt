Sequential pattern mining has raised great interest in data mining research  eld in recent years.
Various mining methods have been proposed, including sequential pattern mining[1][5], and closed se-quentialpattern mining[7][6].
Sequential pattern mining has also shown its utility for Web data analysis, such as mining Web log data[2] and identifying comparative sentences from Web forum posting and product reviews[3].
However, there exists no existing work on mining frequent sequence generators, where a sequence generator is informally de ned as one of the minimal subsequences in an equivalence class.
Thus, generators have the same ability to describe an equivalence class as their corresponding subsequences of the same equivalence class, and according to the MDL princi-ple[4], generators are preferable to all sequential patterns in terms of Web page and product review classi cation.
In the rest of this paper, we  rst give a formal problem formulation and focus on our solution in Section 2, then present the performance study in Section 3.
We conclude the study in Section 4.
An input sequence database SDB contains a set of input sequences, where an input sequence is an ordered list of items (each item can appear multiple times in a sequence) and can be denoted by S=e1e2 .
.
.
en.
Given a pre x of sequence S, Spre=e1e2 .
.
.
ei, we de ne the projected sequence of Spre w.r.t.
S as ei+1e2 .
.
.
en.
The complete set of projected sequences of Spre w.r.t.
each sequence in SDB is called the projected database of Spre w.r.t.
Copyright is held by the author/owner(s).
SDB, denoted by SDBSpre .
Given a subsequence Sp = ep1 ep2 .
.
.
epm, its support sup(Sp) is de ned as the number of sequences in SDBSp each of which contains Sp, denoted by |SDBSp|.
Given a user speci ed minimum support threshold, min_sup, Sp is said to be frequent if sup(Sp)   min_sup holds.
Sp is called a sequence generator iff (cid:3)  S (cid:2) p < Sp (i.e., Sp con-(cid:2) p).
In addition, given a sequence tains S (cid:2) , we denote e1e2 .
.
.
ei 1ei+1 .
.
.
en S=e1e2 .
.
.
en and an item e by S(i), eiei+1 .
.
.
ej by S(i,j), and e1e2 .
.
.
ene (cid:2) p) and sup(Sp) = sup(S (cid:2) p such that S by <S,e (cid:2) (cid:2) >.
Given a minimum support threshold min_sup and an input sequence database SDB, the task of frequent sequence generator mining is to mine the complete set of sequence generators which are frequent in database SDB.
A na ve approach to mining the set of frequent sequence generators is to  rst apply a sequential pattern mining algorithm to  nd the set of frequent subsequences and check if each frequent subsequence is a generator.
However, it is inef cient as it cannot prune the unpromising parts of search space.
In this subsection we propose two novel pruning methods, Forward Prune and Backward Prune, which can be integrated with the pattern-growth enumeration framework [5] to speed up the mining process.
We  rst introduce Theorems 1 and 2 which form the basis of the pruning methods, but due to limited space we eliminate their proofs here.
THEOREM 1.
Given two sequences Sp1 and Sp2, if Sp1< Sp2 (i.e., Sp1 is a proper subsequence of Sp2) and SDBSp1 =SDBSp2, then any extension to Sp2 cannot be a generator.
1 (cid:2) THEOREM 2.
Given subsequence Sp=e1e2 .
.
.
en and an item (i = 1, 2,   , n), then we have that , if SDBSp =SDBS e SDB<Sp,e(cid:2)>= SDB<S p ,e(cid:2)>.
(i) p (i) LEMMA 1.
(Forward Prune).
Given subsequence Sp,and let     p) and for any local frequent p =<Sp, e
 item u of S p ,u>, then
 (cid:2) >, if sup(Sp)=sup(S   p we always have SDB<Sp,u>=SDB<S    p can be safely pruned.
PROOF.
Easily derived from Theorem 1.
LEMMA 2.
(Backward Prune).
Given Sp=e1e2 .
.
.
en, if there exists an index i(i = 1, 2,  , n   1) and a corresponding index j(j=i+1, i+2,   , n) such that SDB(Sp)(1,j) =SDB((Sp)(1,j) )(i) , then Sp can be safely pruned.
PROOF.
Easily derived from Theorem 2 and Theorem 1.
tial pattern mining algorithm, CloSpan [7].
Here we adapted the technique to the setting of sequence generator mining.
The preceding pruning techniques can be used to prune the unpromising parts of search space, but they cannot assure each mined frequent subsequence S=e1e2 .
.
.
en is a generator.
We devise a generator checking scheme as shown in Theorem 3 in order to perform this task, and it can be done ef ciently during pruning process by checking whether there exists such an index i(i=1, 2,    , n) that |SDBS|=|SDBS(i)|, as sup(S)=|SDBS| holds.
THEOREM 3.
A sequence S=e1e2 .
.
.
en is a generator if and only if (cid:3)  i that 1 i  n and sup(S)=sup(S(i)).
PROOF.
Easily derived from the de nition of generator and the well-known downward closure property of a sequence.
By integrating the preceding pruning methods and generator checking scheme with a traditional pattern growth framework [5], we can easily derive the FEAT algorithm as shown in Algorithm 1.
Given a pre x sequence SP , FEAT  rst  nds all its locally frequent items, uses each locally frequent item to grow SP , and builds the projected database for the new pre x (lines 2,3,4).
It adopts both the forward and backward pruning techniques to prune the unpromising parts of search space (lines 8,11), and uses the generator checking scheme to judge whether the new pre x is a generator (lines
 cursively calls itself with the new pre x as its input (lines 14,15).
Algorithm 1: F EAT (Sp, SDBSp , min_sup, F GS) Input : Pre x sequence SP , SP  s projected database SDBSp , minmum support min_sup, and result set F GS begin foreach i in localF requentItems(SDBSp , min_sup) do p, SDBSi p ); , canP rune, isGenerator);















   projectedDatabase(SDBSp , Si p); p  < Sp, i >; Si SDBSi p canP rune   f alse; isGenerator   true; if sup(SDBSp ) = sup(SDBSi p ) then canP rune   F orwardP rune(Sp, SDBSp , Si isGenerator   f alse; if not canP rune then BackwardP rune(Si if isGenerator then F GS   F GS   {Si if not canP rune then p, SDBSi p
 F EAT (Si P , SDBSi p , min_sup, F GS); end

 We conducted extensive performance study to evaluate FEAT algorithm on a computer with Intel Core Duo 2 E6550 CPU and 2GB memory installed.
Due to limited space, we only report the results for some real datasets.
The  rst dataset, Gazelle, is a Web click-stream data containing 29,369 sequences of Web page views.
The second dataset, ProgramTrace, is a program trace dataset.
The third dataset, Of ce07Review, contains 320 consumer reviews for Of ce
 are labeled as positive and negative, respectively.
Figure 1 shows the runtime ef ciency comparison between FEAT and Pre xSpan, a state-of-the-art algorithm for mining all sequential patterns.
Figure 1a) demonstrates that FEAT is slightly slower than P ref ixSpan when the minimum support threshold is high for sparse dataset Gazelle, however, with a minimum support threshold less than 0.026%, FEAT is signi cantly faster than P ref ixSpan.
PrefixSpan




 ) s d n o c e s n i ( e m i t n u
 PrefixSpan
 1e+008 1e+006



 ) s d n o c e s n i ( e m i t n u









 Minimum Support Threshold (in %) a) Gazelle Minimum Support Threshold (in %) b) P rogramT race Figure 1: Runtime Ef ciency Comparison This also validates that our pruning techniques are very effective, since without pruning FEAT needs to generate the same set of sequential patterns as Pre xSpan and perform generator checking to remove those non-generators, thus it should be no faster than Pre- xSpan if the pruning techniques are not applied.
Figure 1 b) shows that for dense dataset ProgramTrace, FEAT is signi cantly faster than Pre xSpan with any minimum support.
For example, Pre- xSpan used nearly 200,000 seconds to  nish even at a minimum support of 100% , while FEAT costs less then 0.02 seconds.
We used generators and sequential patterns as features to build SVM and Na ve Bayesian classi ers respectively.
The results for Of ce07Review dataset show that both generator-based and sequential pattern-based models achieve almost the same accuracy.
For example, with a minimum support of 2% and a minimum con -dence of 75%, both generator-based and sequential pattern-based Na ve Bayesian classi ers can achieve the same best accuracy of
 edge over sequential pattern-based approach in terms of ef ciency.
In this paper we study the problem of mining sequence generators, which has not been explored previously to our best knowledge.
We proposed two novel pruning methods and an ef cient generator checking scheme, and devised a frequent generator mining algorithm, FEAT.
An extensive performance study shows that FEAT is more ef cient than the state-of-the-art sequential pattern mining algorithm, Pre xSpan, and is very effective for classifying Web product reviews.
In future we will further explore its applications in Web page classi cation and click stream data analysis.
This work was partly supported by 973 Program under Grant No.
in University under Grant No.
NCET-07-0491, State Education Ministry of China.
