With the prevailing publishing activities over the Internet, the Web of nowadays covers almost every object and event in the real world.
This phenomenon has recently motivated the event detection community to discover knowledge such as topics, events and stories from large volumes of Web data [4][3].
Most of existing work detect events from either the content [4] or the structures [3] of Web data.
A recent research direction is to study the Web usage data.
For example, Zhao et al. [7] proposed to detect events from Web click-through data, which are the log data generated by Web search engines.
In this paper, we also focus on detecting events from Web click-through data.
Copyright is held by the author/owner(s).
Each entry of Web click-through data basically records the following four types of information: an anonymous user identity, the query issued by the user, the time at which the query was submitted for search, and the URL of clicked search result [5].
Hence, Web click-through data at least provide two aspects of useful knowledge for event detection: semantics of events (e.g., the knowledge indicated by the queries and the corresponding clicked pages) and time of events (e.g., the knowledge indicated by the timestamps at which the queries are issued).
Given a collection of Web click-through data, the overview of our approach is presented in Figure 1.
Basically, there are four steps involved: polar transformation, subspace estimation, subspace pruning, and cluster generation.
data to 2D polar space.
Each query session, containing a query and a set of corresponding pages clicked by a user, is mapped to a point in polar space such that the angle   and radius r of the point respectively re ect the semantics and the occurring time of the query session.
Particularly, given a set of query sessions {S1, S2,  , Sn}, the radius of the point ( i, ri) corresponding to the query session Si is given by T (Si)   minj(T (Sj)) ri = maxj(T (Sj))   minj (T (Sj)) where T (Si) is the occurring time of query sessions Sj.
ri takes value in the range of [0, 1].
We de ne the semantic similarity between two query sessions S1 = (Q1, P1) and S2 = (Q2, P2) as     |Q1   Q2| max{|Q1|,|Q2|} + Sim(S1, S2) = (1    )   |P1   P2| max{|P1|, |P2|} where Qi and Pi are a set of query keywords and a set of clicked pages of the session Si respectively.
We then compute a semantic similarity matrix for the set of query sessions and perform PCA on the matrix.
We use the  rst principle component to preserve the dominant variance in semantic similarities.
Let {f1, f2,  , fn} be the  rst principal component which corresponds to the set of query sessions {S1, S2,  , Sn}.
A query session Si can be mapped to a point ( i, ri) where  i is computed as fi   minj (fj)  i = maxj (fj)   minj(fj )    
 Obviously,  i is restricted to [0,  /2].
query sessions of similar semantics should be mapped to points of similar angles and lie on one and only one 1D subspace.
Therefore, in this step, we perform subspace estimation on the set of transformed data.
Our algorithm is based on Generalized Principal Component Analysis (GPCA) [6], an algebro-geometric approach
 90o 90o 90o e r: ti m :semantics Polar Transformation 0o Subspace Estimation 0o Subspace Pruning 0o Cluster Generation 0o Figure 1: Overview of DECK.
which simultaneously estimates subspace bases and assigns data points to subspaces.
We noticed that the performance of GPCA degrades in the presence of outliers and noises.
We then improve robustness of GPCA by assigning weight coef cients to data points and demoting the impact of noises and outliers.
As a true data point should locate in a cluster inside a subspace, its K nearest neighbors should have small variance in both the subspace direction and the orthogonal direction of the subspace.
Hence, given a data point xi, we assign a weight W (xi) as follows,

 ) ) n(NNxi (1) W (xi) = where s(N Nxi ) is the variance of xi s K nearest neighbors along the subspace direction and n(N Nxi ) is the variance of its neighbors along the orthogonal direction of the subspace.
When the data point xi lies in a cluster where data points spread along the di-) rection of the subspace, both the value of ) and the value of s(N Nxi ) + n(N Nxi ) are small.
Hence, the weight W (xi) is ) and/or s(N Nxi ) + n(N Nxi ) are close to 1.
Otherwise, large, which results in a small W (xi).
s(NNxi n(NNxi s(NNxi n(NNxi )
 that it contains clusters corresponding to real events.
Hence, we prune uninteresting subspaces in this step.
Based on our polar transformation schemes, the temporal  burst" and the semantic  burst" of query sessions should be re ected by the certainly distribution of data points along the subspace direction and the orthogonal direction of the subspace respectively.
In order to measure the certainty of the distribution of data points along the two directions, we project data points to the two directions respectively and calculate the respective histograms of the distributions.
Let (cid:3)h1, h2,    , hm(cid:4) and (cid:3)v1, v2,  , vn(cid:4), where hi and vi are individual bins, be the two corresponding histograms.
We employ the entropy measure to de ne the interestingness of a subspace si as follows.
I(si) = 1   [ p hi log hi   (1   p) i=1 vi log vi] (2) where p   [0, 1] is a weight which adjusts the importance of the entropy values in the two directions.
The interestingness measure takes values from 0 to 1.
The more certain the distributions in two directions, the smaller the entropies in the brackets of equation (2), the greater the value of interestingness.
Given some threshold  , subspace si will be pruned if I(si) <  .
i=1
 events can be detected from the remaining subspaces by clustering.
Particularly, we detect various events from interesting sub-spaces by employing a non-parametric clustering method called Mean Shift [2].
m(cid:2) n(cid:2)

 We conduct experiments on the real-life Web click-through data collected by AOL [5] from March 2006 through May 2006.
We manually labelled a set of events from the data set.
After  lter-ing events which are represented by less than 50 query sessions, a total of 35 events are used in our experiments.
The complete list of events is given in [1].
We then randomly select query sessions which do not represent any real events, together with the query sessions corresponding to real events, to generate  ve data sets, which respectively contain 5K, 10K, 20K, 50K and 100K query sessions.
y p o r t n

















 No.
of Query Sessions Figure 2: Entropy comparison between algorithms.
One of our performance evaluation using entropy measure is shown in Figure 2.
For each generated cluster i, we compute pij as the fraction of query sessions (or query-page pairs for the existing approach [7]) representing the true event j.
Then, the entropy the of cluster i is Ei =   (cid:3) j pij log pij.
The total entropy can be calculated as the sum of the entropies of each cluster weighted by , where m is the number the size of each cluster: E = of clusters, n is total number of query sessions (query-page pairs) and ni is the size of cluster i.
As shown by the  gure, our approach (denoted as DECK in the  gure) works better than the existing approach (denoted as 2PClustering in the  gure).
The  gure also reveals that our approach outperforms two of its alternative versions: DECK-GPCA (which does not improve the robustness of GPCA) and DECK-NP (which does not prune uninteresting subspaces).
ni Ei (cid:3)m n i In general, we proposed a novel approach for detecting events from Web click-through data.
Our approach based on robust subspace analysis considers the temporal feature and semantic feature of query sessions simultaneously.
Experiments on real-life Web click-through data [5] showed the effectiveness of the proposed approach.
