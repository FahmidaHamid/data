Internet Video sharing web sites such as YouTube [1] have attracted millions of users in a dazzling speed during the past few years.
Massive workload accompanies those web sites along with their business success.
In order to understand the nature of such unprecedented massive workload and the impact on online video data center design, we analyze Yahoo!
Video, the 2nd largest U.S. video sharing site in this paper.
The main contribution of our work is an extensive trace-driven analysis of Yahoo!
Video workload dynamics.
We crawled all 16 categories on the Yahoo!
Video site for
 collected every 30 minutes.
This measurement rate was chosen as a tradeo  between analysis requirement and resource Copyright is held by the author/owner(s).
constraint.
Due to the massive scale of Yahoo!
Video site, we limited the data collection to the  rst 10 pages of each category.
Since each page contains 10 video objects, each time the measurement collects dynamic workload information for 1600 video  les in total.
Throughout the whole collection period, we recorded 9,986 unique videos and a total of 32,064,496 video views.
This can be translated into a daily video request rate of 697064, and gave approximately
 2007, based on [2].
Video Duration: We recorded 9, 986 unique videos in total, and the video durations range from 2 to 7518 seconds.
Among them, 76.3% is less than 5 minutes, 91.82% is less than 10 minutes, and 97.66% is less than 25 minutes.
The mean video duration is 283.46 seconds, and the median duration is 159 seconds.
File Popularity: File popularity is de ned as the distribution of stream requests on video  les during a measurement interval.
We performed goodness-of- t test with several distribution models, and found Zipf with an exponential cuto   ts best and well on the  le popularity at four time scales - 30 minutes, 1 hour,1 day, and 1 week.
Job Size Stationarity: Job size distribution is de ned as the distribution of stream requests on video durations during a measurement interval.
We use histogram intersection distance [4] to measure the change between two job size distributions, and calculated the pairwise histogram intersection distance of two adjacent data points during the measuremnent.
Figure 1 shows the CDFs of histogram intersection distance distribution for 3 time scales.
We can see that within 30-minute and one-hour scale, the histogram distance is very small for most of the time.
For example, 90% of the time it is no more than 0.15.
But from day to day, the di erence of request size distributions is obvious.
This indicates that short-term dynamic provisioning only needs to focus on request arrival rate dynamics, while capacity planning at daily or longer basis has to take into account both arrival rate and job size dynamics.
Arrival Rate Predictability: We calculate the auto-correlation coe cient of the arrival rates at the time scale of 30 minutes, and from Figure 2 we can see that the workload is highly correlated in short term.
We also use Fourier









 y t i l i b a b o r p e v i t l a u m u




 30 minutes data one hour data one day data t i n e c i f f e o
 n o i t l a e r r o c o t u




 Histogram Intersection Score











 -0.1 x 1012






 r e w o p


 Lag (k)







 period (30 minutes X)

 s e t i u n m

 n h i t i w s t s e u q e r f o r e b m u







 stable period bursty period

 File rank
 Figure 1: Histogram intersection distance distribution Figure 2: Workload au-tocorrelation coe cient Figure 3: Workload periodicity Figure 4: Comparison of a bursty interval and its preceding interval analysis to discover the possible periodicity in the workload dynamics after removing a few workload spikes.
As shown in Figure 3, the maximum value on the  gure indicates that the period is one day.
With the strong periodicity components, well-known statistical prediction approaches can be applied to further improve the accuracy in capacity planning.
Burstiness: While we can not predict unexpected spikes in the workload, it is necessary to learn the nature of the burstiness and  nd out an e cient way to handle it once a busrty event happens.
The comparison of the request (popularity) distribution during one spike interval and that in the preceding interval is shown in Figure 4.
We can see that the workload can be seen as two parts: a base workload similar to the workload in the previous normal period, and an extra workload that is due to several very popular  les.
System model: We model a single video server as a group of virtual servers with First Come First Served (FCFS) queues.
The virtual server number corresponds to the physical server s capacity, which is de ned as the maximum number of concurrent streams delivered by the server without loosing a quality of stream.
In the analysis, the number 300 is chosed for the capacity of a video server based on the em-perical results in [3].
In this way, we model the video service center as a queueing system with multiple FCFS servers.
Workload Scheduling Schemes: We choose two well-known scheduling schemes to study: random dispatching, which doesn t make use of any information of the servers and just sends each incoming job to one of s server uniformly with probability 1/s; Least workload Left (LWL) scheme, which tries to achieve load balancing among servers by making use of the per-server workload information and assigns the job to the server with the least workload left at the arrival instant.
Service Level Agreements: We consider two QoS metrics for SLAs: the stream quality for an accepted connection, and the waiting time of a video request in the queue before accepted for streaming.
Assume enough network bandwidth, then QoS on stream quality within the data center side can be guaranteed through admission control based on server capacity.
For the waiting time W , we consider the bound on the tail of the waiting time distribution (called Wtail), de ned as P [W > x] < y with x > 0, y < 1.
For example, SLA could be that 90% of the requests experience no more than 5 seconds delays, i.e., x = 5 and y = 90%.
d e d e e n s r e v r e
 LWL scheme random scheme







 350 time Figure 5: Server Demands with di erent scheduling schemes Taking the one-week measurement data from Aug 13th to Aug 20th, we numerically calculated the server demands of random and LWL dispatching schemes with Poisson arrivals (based on results in [5]) and set the SLA requirement as Wtail: Pr[W > mean service time] < 0.3.
Figure 5 shows the results; on average 69.9% of servers could be saved with LWL scheme as compared to random dispatching scheme.
In this paper, we present the measurement study of a large Internet video sharing site - Yahoo!
Video.
With a clear goal to facilitate the data center design, this paper gives a comprehensive workload characterization and proposes a set of guidelines for workload and capacity management in a large-scale video distribution system.
The immediate work for next step is expanding the measurement to cover larger portion of the workload on Yahoo!
Video.
