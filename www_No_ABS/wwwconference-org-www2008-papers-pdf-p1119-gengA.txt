The practices of crafting web pages for the sole purpose of increasing the ranking of these or some affiliated pages, without improving the practicability to the surfers, are called web spam[4].
Web spam seriously deteriorates search engine ranking results.
Finding effective methods for spam detection has become increasingly urgent.
Analogous to the methods used in fighting email spam, [1] detected content spam via a number of statistical content based attributes.
[5] implemented a classifier to catch a large portion of spam, then several heuristics rules were designed to decide whether a node should be relabeled.
[2] summarized the existing content and link based method, detected web spam with machine learning algorithms, then gave some heuristic rules to improve the performance.
Both [2] and [5] achieved good results with the preliminary machine learning algorithms, but they optimized the detection result with some heuristic rules.
As we all know, effective spam detection is essentially an  arms race  between search engines and spamers.
Heuristic rules based detection system can be more Copyright is held by the author/owner(s).
easily manipulated by the spamers.
Compared with other optimization methods mentioned in [2][5], stack graph learning (Sgl) can mine topological dependencies for spam detection more reasonably[2], which is a simple two-stage learning method.
In this paper, we propose a feature re-extraction strategy to build a robust and high-performance detection system.
Based on the Web topology and preliminary detection results, a series of re-extracted features are computed for the second stage learning, which can be seen as the expansion of Sgl algorithm.
With this strategy, the complete detection process can be executed in the machine learning framework.
Experiments on the WEBSPAM-UK2006 benchmark shows that with both original and re-extracted features, the performance of spam detection can be improved evidently.
Figure.
1 is the flow chart of our proposed two-stage web spam detection strategy.
Preliminary detection is carried out based on the original extracted features, then the predicted spamicity will be used for the result optimization.
Instead of smoothing the result with heuristic rules, feature re-extraction strategy is adopted.
Detection algorithm on the expanded eigenspace will be implemented in the optimization stage.
Original feature extraction Preliminary detection Feature re-extraction Optimization Stage Second stage detection Detection result Figure 1: Flow chart of the proposed strategy.
Original features(of) are the features used for preliminary detection, which consist of hyperlink related features and content based features.
Similar with [2], most of the link related features are computed for the home page and page with the maximum PageRank in each host.
We don't use any linear combined features, since they are redundant from the perspective of feature selection.
The content-based features consist of the number of words in page, amount of anchor text, and fraction of visible content etc [1][2].
Based on the of, preliminary detection is
 features re-extraction.
Based on the host level link graph and the predicted spamicity, clustering features, propagation features and neighbor features are extracted.
The host level undirected graph G is defined as G = (V, E, w), where V is the set of hosts, w = f(n) is a weighting function, n is the number of links between any page in host u and any page in host v, and E is the set of edges with nonzero weight.
We cluster the graph G using the METIS graph partitioning algorithm.
With such algorithm, we can partition all the hosts into K clusters.
After partitioning, the clustering features(cf) can be computed as Hcf ( )  = spamicity HCh )  
 | ( ( h )( the summarized samples [4].
Both set1 and set2 are taken into account, where 4411 hosts are marked normal and 1803 hosts are marked spam.
Six times 5-fold cross-validation is run on the data set.
The precision, recall, true positive rate(TP), false positive rate(FP), area under ROC curve (AUC) and F-measure are used to measure the performance.
Table 1 shows the performance of web spam detection with different strategies.
The first line is the baseline, which is computed with the original features.
The second line reports the results computed with stack graph learning optimization, where both inlink and outlink relations are taken into consideration.
The last line gives the performance with both original and re-extracted features.
The figures in brackets are the improvement compared with the baseline.
From the table, we can see that when using all the features, a big improvement can be obtained on all of the measures.
The experimental results indicate that the proposed strategy can mine the topological dependencies more effectively.
Table 1.
Web Spam Detection Performace with Different Strategies Features

 Precision Recall F-measure
 of Sgl of + cf + pf + nf






 (6.41%)
 (8.12%)
 (-2.98%)
 (3.08%)
 (0.47%)
 (1.63%)
 (6.41%)
 (8.12%)
 (3.38%)
 (4.80%)
 (0.088%)
 (1.26%)

 In this paper, we detect web spam in the machine learning framework.
The proposed detection strategy includes two-stage feature extraction, which makes full use of the Web topological relation.
Experiment shows that the strategy is robust, and can detect web spam more effectively.
