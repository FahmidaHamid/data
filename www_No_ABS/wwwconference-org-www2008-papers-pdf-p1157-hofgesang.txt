Web personalization techniques allow companies to customize their web services according to their customer s needs.
However, changing customer behaviour and frequent structural changes and content updates of websites are likely to impair the underlying models.
Current personalization techniques require periodic updates and human interaction to cope with this problem and thus maintain up-to-date per-sonalization.
To automate this process, we need to identify points of change (see e.g.
[1]) in user behaviour automatically as the data  ows in.
Identifying such changes - whether they are seasonal or temporal, or a long-term behavioural trend shift - and promptly taking appropriate action in response leverages business advantage.
During our analysis of numerous web usage datasets, including data from online retail shops and an online bank, we identi ed three main problems that we summarize in the following three goals: Copyright is held by the author/owner(s).
Goal 1: Detect changes in individual behaviour A solution to this problem should include incremental maintenance of a compact user pro le for each individual and the output of changes.
Change signals can be used to update personalized content or for marketing actions while the base pro les can help selecting the proper personalized content.
Goal 2: Report  interesting  sessions From time to time online users may not be able to  nd some content or they may have technical di culties.
The idea behind the second problem is to identify such points,  interesting  sessions, in a stream of user actions.
Based on such alerts, an assistant may o er instant online help (e.g.
chat or voice call) or the system may take prompt automated action.
Goal 3: Detect changes in user activity Detecting changes in an individual s visitation frequency seems to relate to our  rst problem; however, it requires di erent treatment both technically and in the types of output alert.
For example, early identi cation of a decrease in an individual s visitation frequency may support actions for customer retention.
Based on the di erent types of change in visitation frequency, we may label changes as  runner up  or  slower down  based on the increase or decrease in frequency.
The contribution our work makes is the identi cation of three relevant problems in individual, on-the- y web per-sonalization and their proposed space and computationally e cient solutions.
We choose to use a treelike structure to represent user pro les.
Each node may contain more than one page id, a frequency counter, a reference to its parent and a vector of references to its children.
Adding a session (we keep visited pages as sets, discarding order and cardinality information) to a pro le tree can be broken down as a simpler recursive procedure.
The procedure is called, in each step, by a given tree node n and the remaining pages to be inserted, P .
As a  rst step, we select the children of n that have the highest overlap with P .
If there are more nodes with equal, nonzero overlap we If P has no overlap with any of the select the  rst one.
children nodes we insert P as a new node under n. Otherwise, we identify the type of overlap.
In case the children node is equivalent to P or is fully part of P , we increase the counter of the children and invoke another recursive step on the children node and the non-overlapping remainder of P .
In the other two cases, we need to split the children node by subtracting and raising the overlap directly above it as a new parent node, update the frequency counters and insert
 node under the newly created node of common pages.
In our methods, we maintain the pro les over a sliding window.
This can be easily done by maintaining references to leaf nodes in the tree in a  round  vector.
Here we de ne a distance metric over a pro le tree and an incoming session (S).
In fact, we de ne a similarity metric and we transform a distance by dist = 1   sim.
The similarity measure basically selects a branch with the highest overlap to the input session and calculates a score on it.
Once again, we can de ne this metric recursively as each node returns the number of its overlapping pages with the subset of pages it received plus the total overlap on the remainder of pages returned from its children nodes.
Each recursive step also returns a subscore and, among identical overlaps, we return the highest score as similarity.
We calculate the scores at each node by taking the minimum of the normalised node frequency ( n.f requency ) and the page probability (normP ).
Both norms are known upfront to the algorithm.
normT is simply the number of pages added to the tree and normP = 1/|S|.
normT

 Goal 1: Detect changes in individual behaviour A user pro le is maintained for each individual and compared, with a window of most recent sessions.
First, the tree is initialized with n sessions.
The score of an incoming session is calculated using the distance function de ned in Section If the session is not an outlier (score < toutlier), it is
 added to a bu er (ref ), excluding the last m scores, which are kept in a bu er W .
A change is detected when the distance (d) between the most recent scores (W ) and ref is larger than a threshold, Tchange.
The distance is de ned as wW ecdfref (w), where ecdf (x) is the empiri-d(ref, W ) = cal cumulative distribution function of ref .
If we maintain the ecdfref using a balanced tree algorithm, the complexity of the algorithm is O(log(|ref|)) and the distance function, dist, can be calculated e ciently as well.
(cid:2) Goal 2: Report  interesting  sessions The problem of  nding  interesting  sessions is rather subjective and cannot be clearly formulated.
We choose to rate pages as  highly interesting  when they are popular for a speci c individual but rarely visited by others.
Interesting sessions are those containing interesting pages.
We consider only out-lier sessions (calculated in Goal 1) as potential interesting sessions.
We calculate a new score over the outlier session based on an automatically maintained popularity matrix.
The components of this matrix are weights calculated by the T F   IDFi,j = ni,j(cid:3) |{dj :ti dj}| weighting applied to our data, where i refers to a term (web pages) and j to documents (individual sessions) and nij is the number of occurrences of term i in document j.
We label a session as interesting if its overall score is higher than a user-de ned threshold.
k nk,j
 log Goal 3: Detect changes in user activity While the  rst two problems, Goals 1 and 2, can be included in a common framework, we need another layer of data abstraction and a di erent system for the third problem.
The problem at hand is detecting change in a stream of binary data representing whether the user was active on a given day.
Our algorithm is based on the CUSUM (cumulative sum) algorithm [3] and we model the probability density func-c (cid:3) tion (pdf ) of the data by the poisson distribution.
For a given user we incrementally estimate the probability  p of visitation on a given day.
The estimate  p is compared to h alternatives palt via the Sc = ) sums, where t is the last change point and c is the current time.
Each Sc, for every h, is maintained incrementally.
The h alternatives are  xed upfront.
Given that  p is more likely than the alternatives, Sc has a downward trend.
An increasing Sc indicates a change in the trend and a change is detected if Sc   mint...c Sc > Talarm.
The complexity is linear in the number of alternatives, which is O(|H1|) per session.
t log( palt pdf  p

 Our experiment included results on server-side web access log data of an investment bank collected over a 1.5-year period.
To evaluate our change detection methods, we randomly selected two sets each of 200 individuals with their sessions, and labelled them together with  eld experts.
We looked at the individual sessions evolving over time and marked the points of change.
Goal 1: evaluation The accuracy of our method is 69.57% with 67 false alarms and the mean detection latency is 5.12 sessions.
Goal 2: evaluation While experimenting with the TF-IDF scoring, we found that the high popularity of a relatively few pages among all individuals resulted in extreme high term frequency (T Fi,j) components that overweighed the IDFi,j scores and, even though popular pages were present in most pro les, their  nal T F   IDFi,j scores were much higher for common pages than the much more interesting rare pages.
To avoid the e ect of the power law distribution of web pages, we ignored the T Fi,j weight and used only the IDFi,j component in  nal scoring.
Goal 3: evaluation The accuracy of our method is 77.36% with 117 false alarms, and the mean detection la tency is 6.37 days.
Figure 1 shows an example of score evolution and change detection both for Goal 1 and Goal 3.
For more experimental results we refer the reader to [2].
