Sponsored search auction is an e ective way of monetizing search activities where advertisers pay to place their ads on search results pages for speci c user keyword queries.
In this work we focus on the bidding optimization problem for an advertiser with budget constraints.
Formally, we address the following problem: for each keyword and each time period, how much should the advertiser bid (i.e., which position to obtain), so as to maximize ROI of the ads given a  xed budget and a  xed time horizon?
We can view each ad position as an item with associated weight (cost) and value (either revenue or pro t).
The advertiser has a budget constraint, and it naturally corresponds to the knapsack capacity.
Furthermore each advertiser can have at most one ad appear on each keyword results page.
This corresponds to the constraint that at most one item from each item-set can be taken in the multiple-choice knapsack problem (MCKP), a well-known variation of the knapsack problem (KP).
The following model of the bidding problem is proposed: given multiple keywords k   K, multiple time periods when the advertiser places bids t   {1, .
.
.
, T}, and multiple positions s   {1, .
.
.
, S}, the item-set N k ts) for all ad positions   t consists of items (wk Work was done while the author was an intern at HP Labs.
ts, vk Copyright is held by the author/owner(s).
Victor Naroditskiy Department of Computer Science Brown University Providence, RI 02912 victor@brown.edu s. Formally wk w ts and vk ts are de ned as follows: k k ts  k   p (t), k k ts)  k ts = p k ts = (V v (t),   s, t, k.
k k (s)X (s)X (1) Here V k denotes the expected value-per-click for keyword k, X k(t) denotes the number of user queries for keyword k at time period t,  k(s) denotes the click-through rate (CTR) of position s and keyword k (the ratio between total user clicks on the ad at s-th slot and the total number of impressions), and pk ts denotes the cost-per-click for ads in position s for keyword k at time t.
The bidding optimization problem has a strong stochastic  avor as bidding prices and click tra c change all the time.
The online and stochastic variant of MCKP (S-MCKP) where item-sets are received one at a time, allows us to explicitly incorporate this uncertainty in the distribution of future item-sets.
Although, decisions are made assuming that the item-sets are independent and identically distributed (iid), the algorithm performs well even when items in di erent time periods do not follow the same distribution as evidenced by experimental results for the  auto insurance  dataset.
In the last few years, a number of papers addressed the problem of revenue maximization or bidding optimization in sponsored search auctions [1, 4, 2, 3, 7].
None of the previous work proposed a solution that employs distributional information about prices and solves the bidding problem with multiple ad position, keywords, and time periods.
Zhou et al. [8] attack a very similar problem and model it as Online-MCKP; however their work focuses on competitive algorithms with worst-case performance guarantees while this work focuses on average-case performance with stochastic input.
The threshold function we develop for S-MCKP is based on the threshold function for the stochastic knapsack problem by Lueker [6].
Our algorithm for the S-MCKP is based on Lueker s Algorithm for Online-KP and an approximation for MCKP [5].
At a high level, we use a technique from the approximation for MCKP to convert each MCKP item-set into KP items and apply Lueker s threshold function to selected KP items.
We now describe the algorithm in detail.
We  rst apply a technique for converting items from a MCKP item-set into incremental items with the following property: taking multiple incremental items with decreasing e ciency (value/weight) is equivalent to taking exactly one original item.
The technique consists of two steps: (i) removing dominated and LP-dominated items from the item-
items.
For details, see Kellerer et al. [5], p.320.
from undominated After obtaining incremental items, we use a threshold function to decide which incremental items to take.
Lueker [6] proposed solving Online-KP using an adaptive threshold: only items that meet the threshold e ciency are put in the knapsack.
The threshold function (which we call g) maps the average remaining capacity per time period to the threshold e ciency e .
The threshold e ciency is such that the expected weight of the remaining items (all items are iid) with e ciency at least e is equal to the remaining capacity:     (cid:4) (cid:2) n(cid:3) C = Ew,v wi 1{ vi wi  e } i=1 = n Ew,v   f (e ) =
 n while f (e)   Ew,v (cid:5)  e } w 1{ v w (cid:6) w 1{ v w  e} (2) (cid:5) (cid:6) We need the distribution of incremental items to calculate the threshold function (Eq.
2), however such information may not be known or have a closed-form representation.
Alternatively, we can approximate the threshold function of incremental items using item-sets received in the past.
Formally, given a sample set of m incremental items, we can use  f to approximate f where m(cid:3)  f (e)   1 m wi 1{ vi wi  e} i=1 (3)  f is a piecewise constant function with at most m pieces, and it can be computed in time O(m log m) based on sorting of item e ciency.
The algorithm for S-MCKP is in Figure 1.
It consists of two phases: the  rst phase (optional) is to generate the threshold function if training item-sets are available.
In the second phase, at each time period the algorithm converts the current item-set to incremental items, checks which incremental items meet the threshold, and takes the corresponding item from the item-set.
The threshold function can be updated in step 2 by incorporating information from the current item-set.
In the case when no training item-sets are available in step 1, the  rst real item-set is used to generate the initial threshold function.
Input: item-set Nt at time t, for t = 1, .
.
.
, n; knapsack capacity C; (optional) training item-sets; Output: items to take at each time
 create incremental items from training item-sets r is the average number of incremental items per set generate  f using incremental items for t from 1 to n
 create incremental items from item-set Nt update  f and r based on incremental items   e /** r(n   t + 1) is the expected number of remaining incremental items **/ r(n t+1) )  1( =  f
 select incremental items with e ciency at least e (w, v) is the corresponding item of these selected incremental items if w   C take item (w, v), update C := C   w Figure 1: Algorithm for S-MCKP.
 

 We run two sets of experiments.
In the  rst set of experiments, we generate items with weights and values drawn independently from one of the following distributions: Uniform with support between 1 and 10, Normal with mean 10, and Exponential with mean 10.
The number of items per set received each time period is 5.
We express the budget (capacity) as a fraction of the mean weight of an item times the total number of time periods, i.e., C =   n  mean(w), where n is the number of time periods, mean(w) is the mean weight of a random item.
The value of   indicates whether the budget is large compared to the expected overall spending if you pick random items each time without any optimization.
We tested the algorithms on problem instances with the budget level     {0.05, 0.2, 0.5, 0.9, 1.1}.
We evaluate the performance of the algorithm based on the ratio of the value obtained by the algorithm and an upper bound on the optimal solution to o ine MCKP.
Figure 2 shows experimenal results with no training item-sets for generating the threshold function in step 1 of Alg 1.
The performance of the algorithm is almost always within 10% of the optimal when n   20 and approaches the optimal as n goes to  .
A graph for the performance of Alg 1 with the Uniform distribution is shown below.
Graphs with the other distributions look very similar, and are omitted due to space constraints.
o i t a r




   0.05   0.20   0.50   0.90   1.10




 number of periods Figure 2: Performance with U(1,10).
The second set of experiments uses a real dataset for the  auto insurance  keyword described in [8].
The performance of our algorithm is around 99% while the algorithm in [8] achieves around 90%-95%.
