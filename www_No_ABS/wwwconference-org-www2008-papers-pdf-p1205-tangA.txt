Name disambiguation is a very critical problem in many knowledge management applications, such as Digital Libraries and Semantic Web applications.
We have examined one hundred person names and found that the problem is very serious.
For example, there are 54 papers authored by 25  Jing Zhang .
Even, there are three  Yi Li  graduated from the author s lab.
In this paper, we propose a unified probabilistic framework to offer solutions to the above challenges.
We explore a dynamic approach for estimating the person number K. We propose a unified probabilistic model.
The model can achieve better performance in name disambiguation than the existing methods.
The problem can be described as: Given a person name a, let all publications containing the author named a as P={p1, p2,  , pn}.
Suppose there existing k actual researchers {y1, y2,  , yk} having the name a, our task is to assign these n publications to their real researcher yi.
Copyright is held by the author/owner(s).
We define five types of undirected relationships between papers.
Table 1 shows the relationships.
Relationship r1 represents that two papers are published at the same conference/journal.
Relationship r2 means two papers have a secondary author with the same name, and relationship r3 means one paper cites the other paper.
Table 1.
Relationships between papers Description R W Relation Name r1 w1 Co-Conference w2 r2 Co-Author w3 r3 Citation w4 r4 r5 w5 Constraints  -CoAuthor pi.pubvenue = pj.pubvenue   r, s>0, ai (s) pi cites pj or pj cites pi (r)=aj Feedbacks supplied by users  extension co-authorship ( >1) We use an example to explain relationship r5.
Suppose pi has authors  David Mitchell  and  Andrew Mark , and pj has authors  David Mitchell  and  Fernando Mulford .
(We are going to disambiguate  David Mitchell .)
If  Andrew Mark  and  Fernando Mulford  also coauthor one paper, then we say pi and pj have a 2-CoAuthor relationship.
The proposed framework is based on Hidden Markov Random Fields [1], which can be used to model dependencies between observations (here each paper can be viewed as an observation).
Then for each disambiguation task, we propose using the Bayesian Information Criterion (BIC) [2] as the criterion to estimate the person number K. We define an objective function for the disambiguation task.
Our goal is to optimize a parameter setting that maximizes the objective function with some given K.
Figure 1 shows the graphical structure of the HMRF model to the name disambiguation problem.
The edge between the hidden variables corresponds to the relationships between papers.
The value of each hidden variable indicates the assignment results.
By the fundamental theorem of random fields [3], the probability distribution of the label configuration Y has the form:
 ( ) =


 exp(     i ( y y , i j )   f y y ( , i j ))
 i (1) where potential function f(yi, yj) is a non-negative function defined based on the edge (yi, yj) and Ei represents all neighborhoods related to yi.
Z1 is a normalization factor.
Our goal
 the maximum a-posteriori configuration of the HMRF, i.e. maximize P(Y|X).
Suppose P(X) ) is a constant, then we get by the Bayes rule.
Therefore, our objective function is defined as:
 (
 ( find to is   ) ) ( | |
 max
 ( ) By integrating (1) into (2), we obtain:
 ( log( log( )) = = ( | | )) (2)
 max = log        


 exp(       i ( y y , i j )   f y y ( , i j ))     x X i   P x ( i | y i )
 i         (3) Then the problem is how to define the potential function f and how to estimate the generative probability P(xi|yi).
We define the potential function f(yi, yj) by the distance D(xi, xj) between paper pi and pj.
As for the probability P(xi|yi), we assume that publications is generated under the spherical Gaussian distribution and thus we have: P x ( i | y i ) = exp(   D x ( i ,   i ( ) )) (4)


 where  (i) is the cluster centroid that the paper xi is assigned.
Notation D(xi,  (i)) represents the distance between paper pi and its assigned cluster center  (i).
Thus putting equation (4) into (3), we obtain the objective function in minimizing form: =
 min     y y , i where Z=Z1Z2.
( i j )   f y y ( , i j ) +   x X i   D x ( i ,   i ( ) ) + log
 (5)
 i y1=1 co-conference y2=1 y3=1 cite y4=2 y9=3 t-coauthor y7=2 cite y5=2 y6=2 co-conference coauthor cite y8=1 coauthor y10=3 coauthor coauthor y11=3 x1 x2 x3 x4 x5 x9 x7 x6 x8 x10 x11 Figure 1.
The graphical structure of a HMRF
 Three tasks are executed by Expectation Maximization (EM): learning parameters of the distance measure, reassignment of each paper to a cluster, and update of cluster centroid u(i).
We define the distance function D(xi, xj) as follows [1]: x
 i || x
 j x || || || x i D x x , ( i ) 1 =   j , where || x i ||
 x
 i x
 i (6) j

 here A is a parameter matrix.
The EM process can be summarized as follows: in the E-step, given the current cluster centroid, every paper xi is reassigned to the cluster by maximizing p(yi|xi).
In the M-step, the cluster centers are re-estimated based on the assignments to minimize the objective function L; and the parameter matrix is updated to increase the objective function.
Our proposed strategy (see Algorithm 1) is to start by setting K as
 cluster.
The algorithm runs iteratively.
In each iteration, we try to split every cluster C into two sub-clusters.
We calculate a local BIC score of the new sub model M2.
Given BIC(M2) > BIC(M1), we split the cluster.
We calculate a global BIC score for the obtained new model.
The process continues if there exists split.
Finally, we use the global BIC score as the criterion to choose as output the model with the highest score.
BIC score is defined as
 ( v ) = log h (
 ( | h | ) )   |  
   n log( ) (7) For the parameter | |, we simply define it as the sum of the K cluster probabilities, weight of the relations, and cluster centroids.
To evaluate our method, we created two datasets, namely Abbreviated Name and Real Name.
The first dataset contains 10 abbreviated names (e.g.
 C.
Chang ) and the second data set has two real person names (e.g.
 Jing Zhang ).
We evaluated the performances of our method and the baseline methods (K-means) on the two data sets.
Results show that our method can significantly outperform the baseline method for name disambiguation (+46.54% on Abbreviate Name data set and +41.35% on Real Name data set in terms of the average F1-score).
We evaluated the effectiveness of estimation of the person number K. We have found that the estimated numbers by our approach are close to the results by human labeled.
We applied X-means to find the person number K. We found that X-means fails to find the actual number.
It always outputs only one cluster except  Yi Li  with 2.
See [4][5] for more evaluation results.
To further evaluate the effectiveness of our method, we applied it to expert finding and people association finding.
For expert finding, in terms of mean average precision (MAP), 2% improvements can be obtained.
For people association finding, we selected five pairs of person names and searched in ArnetMiner system, averagely 20% improvements can be obtained.
In this paper, we have investigated the problem of name disambiguation.
We have proposed a generalized probabilistic model to the problem.
We have explored a dynamic approach for estimating the person number K. Experiments show that the proposed method significantly outperforms the baseline methods.
