Template detection is an important technique since templates could heavily cripple the performance of other modules such as page classi cation modules or index builders.
Most previous approaches [1, 3, 4, 2] utilize content repetition as a hint for template detection.
A block which occurs in many pages is considered to be a template block.
For the use of content repetition, they all require a lot of web pages as input.
So in practice, these methods run in a batch manner.
They start working when enough web pages of a site have been gathered.
They detect all template blocks and then wait until another dataset is available.
However, caching the web pages often consume lots of storage.
And also, since a newly crawled page can not be processed until enough pages are collected, the delay of data refreshing is huge.
So the batch manner is not applicable to schemes in which the speed of data refreshing is concerned.
These schemes include news search engines, blog search engines, etc.
In this paper, we propose an incremental framework to detect templates in which a page is processed as soon as it has been crawled.
The basic idea of the framework is to maintain some information of past web pages, and detect templates with the guidance of past information.
Copyright is held by the author/owner(s).
Our template detection method are based on the repetition of text segments which are text nodes in DOM trees of web pages.
We use a data structure called the text segment table to maintain the repetition information, i.e. the contents and DFs of text segments.
We should note that when talking about the DFs of text segments, we must  rst clarify when two text segments are considered to be the same.
In our framework, two text segments are same only when their contents are literally equal and they have the same DOM path.
Whenever a new page is available, it will be passed through four steps: 1) page segmentation, 2) text segment table expansion, 3) template detection, and 4) text segment table shrinkage.
The segmentation process contains two steps: 1) A web page is divided into multiple blocks.
Currently, we choose some html tags that usually determine the page layout as separators, these html tags are <TABLE>, <DIV>, etc.
2) Then each block is further divided into text segments by html tags, process instructions, and html comments.
After page segmentation, text segments are used to update the text segment table.
If a text segment already exists in the table, the DF of it will be increased by one.
Otherwise, the text segment will be inserted into the table and its DF is initialized to one.
Template detection occurs in block level.
We search every text segment of a block in the text segment table, checking whether it is a template segment.
We de ne template segments as text segments whose DFs are larger than or equal to 5.
We can then calculate the template ratio of a block: template ratio = lengths of template segments lengths of all text segments If the template ratio of a block is larger than 0.7, we label the block as a template block.
The text segment table will consume more and more storage if only the expansion step is applied.
To control the storage use, we need to delete some text segments.
The cost of deleting a text segment is de ned as the times to classify a template segment as non-template segment be-(cid:2) (cid:2)
 pears after its deletion, it will be recognized as non-template segment.
The cost of deleting a text segment is related to the DF and the future occurring times of the text segment.
To minimize the cost of deletion, we allow text segments that have larger DFs to live longer than those with smaller DFs, because the former are more likely to occur in the future.
We should note that we don t use the publish times or crawling times as the timestamps of pages and blocks, but we assign every page a page number and use it as the timestamp.
The maximum living time of a text segment is then modelled by the logistic function: (cid:3) t = TbN
 (cid:4) Tb is the maximum living time of a text segment which appears only once.
df is the past DF of the text segment.
TbN de nes the upper bound of maximum living time of a text segment before a new occurrence comes, no matter what df is.
When a text segment doesn t appear for t, it will be removed from the text segment table.
As there is no standard test set, we build a new data set by hand.
We pick  ve popular sites and sample 400 pages from each site.
People are asked to label every block in pages as template or not depending on their understanding.
All sites are picked from the data set of Ma s work [3].
This guarantees the test set has no prejudice towards our method.
In this section, we compare our template detection method with shingle [3] and SST(Site Style Trees [4].
To our knowledge, SST is currently the best template detection method in the literature.
Our method runs in the incremental manner, while shingle and SST run in the batch manner.
Since the precision of template detection is often very high that the precision of all these methods is higher than 98% in our experiment, we don t concern the precision of methods.
We  rst compare the storage consumption of all template detection methods when their recall rates are equal.
The expected recall is 80%.
To achieve the recall rate, the number of web pages in each batch is adjusted for shingle method and SST method, and the parameters of logistic strategy is adjusted for our method.
In the incremental template detection process, the size of the text segment table changes when it is updated.
In the batch template detection process, the size of cached web pages also changes.
So we compares the average storage consumption of methods.
To be concrete, if the size of the text segment table when inserting the ith page is denoted as si, then the average size of the text segment table can be calculated as follow: (cid:5) (cid:5) AverageSize = si/
 all possible i all possible i In the batch template detection process, the average size of cached web pages is the average size of all batches.
The result is summarized in Table 1.
The second column contains the sizes of text segment tables of our method, the third column contains the sizes of web pages of shingle method, and the fourth column contains the sizes of web pages of SST method.
As shown in Table 1, our method with logistic strategy saves 93.81% storage compared with shingle method when Table 1: When the recall rates are equal, our method with logistic strategy saves 93.81% storage compared with shingle method and saves 93.22% storage compared with SST method.
amazon.com cnnfn.com ebay.com nba.com yahoo.com Average Size Saving Storage Our Method












 Shingle












 recalls of these two methods are equal.
In other words, our method only consumes 6.19% storage of that consumed by shingle method.
When compared with SST method, our strategy saves 93.22% storage.
On the other hand, we expect that our method achieves higher recall than other methods when using the same amount of storage.
In the experiment, the average number of pages in each batch is 24.
So in the batch manner, the template detection methods should wait for 24 pages to be collected before they can be run.
While in our framework, every page will be processed immediately.
In this paper, we propose a promising framework to detect template blocks incrementally.
Our approach uses the text segment table as a compact representation of past web pages, and uses an e ective updating strategy to maintain it.
Further more, a new template detection algorithm is proposed based on the text segment table.
Experiments on  ve popular web sites indicate that our framework consumes less than 7% storage than traditional methods.
And also the speed of data refreshing is accelerated.
