Before the advent of the World Wide Web, information retrieval algorithms were developed for relatively small and coherent document collections such as newspaper articles or book catalogs in a library.
In comparison to these collections, the Web is massive, much less coherent, changes more rapidly, and is spread over geographically distributed computers [1].
Scaling information retrieval algorithms to the World Wide Web is a challenging task.
Success to date is depicted by the ubiquitous use of search engines to access Internet content.
From the point of view of a search engine, the Web is a mix of two types of content: the  closed Web  and the  open Web  [2].
The closed web comprises a few high-quality controlled collections which a search engine can fully trust.
The  open Web,  on the other hand, includes the vast majority of Web pages, which lack an authority asserting their quality.
The openness of the Web has been the key to its rapid Copyright is held by the author/owner(s).
growth and success.
However, this openness is also a major source of new challenges for information retrieval methods.
Search engine spam is not a new problem; it has been an important issue for commercial providers for a number of years, and is not likely to be solved in the near future.
Web spam damages search engine reputation.
It exploits and as a result weakens the trust relationship between users and search engines [6].
According to Henzinger et al. [7],  Spamming has become so prevalent that every commercial search engine has had to take measures to identify and remove spam.
Without such measures, the quality of the rankings su ers severely.  On the  open web  a naive application of ranking methods in no longer an option.
For instance, PageRank [8] in its pure form is very susceptible to spam: the authors of [4] ranked 100 million pages using PageRank and found that 11 out of the top 20 were pornographic and achieved such high ranking through link manipulation.
Adversarial information retrieval is a research area in which several things remain to be discovered.
Sahami et al. [9] have noted that  Adversarial classi cation is an area in which precious little work has been done, but e ective methods can provide large gains.  Also, adversarial IR problems can be approached from many di erent perspectives, including information retrieval, machine learning and game theory.
Adversarial Information Retrieval addresses tasks such as gathering, indexing,  ltering, retrieving and ranking information from collections wherein a subset has been manipulated maliciously [5].
On the Web, the predominant form of such manipulation is  search engine spamming  or spamdexing, i.e.: malicious attempts to in uence the outcome of ranking algorithms, aimed at getting an undeserved high ranking for some items in the collection.
There is an economic incentive to rank higher in search engines, considering that a good ranking on them is strongly correlated with more tra c, which often translates to more revenue [10].
As in previous years, automatic detection of search engine spam is expected to be the dominant theme of this workshop.
Three basic forms of web spam are included:   Link spam   Content spam   Cloaking
 clude:   Blog spam  ltering   Click fraud detection   Reverse engineering of ranking algorithms   Web content  ltering   Advertisement blocking   Stealth crawling

 In 2007, we introduced a novel element: the Web spam challenge.
We released a reference collection for Web Spam Detection that comprises Web pages, a Web graph, and labels for a subset of the pages.
Web pages in this collection were labeled as  normal  or  spam  by humans [3].
Using this data set, the challenge was to predict which pages in the unlabeled part of the data are spam and which are normal.
For 2008, we released an updated reference collection covering a signi cantly increased number of hosts.
We also encouraged authors submitting papers on search engine spam to test their systems on the updated reference collection.
We ask that participating researchers submit predictions (normal/spam) for all unlabeled elements in the collection.
Predictions will be evaluated on a part of the collection for which human-provided labels will be held for testing.
Results will be announced at the AIRWeb 2008 workshop.
The Web spam challenge serves a dual purpose: it allows the comparison of di erent systems, which has not been possible in the past for lack of a reference collection; and it stimulates research on this area given its competitive nature.
