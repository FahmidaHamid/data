Recommender systems continue to play important and new roles in business on the World Wide Web [11, 12, 14,
 formation  ltering technique that seeks to identify a set of items that are likely of interest to users.
The most popular method adopted by contemporary rec-ommender systems is Collaborative Filtering (CF), where the core assumption is that similar users on similar items express similar interests.
The heart of memory-based CF methods is the measurement of similarity: either the similarity of users (a.k.a user-based CF) or the similarity of items (a.k.a items-based CF) or a hybrid of both.
The user-based CF computes the similarity among users, usually based on user pro les or past behavior [14, 10], and seeks consistency in the predictions among similar users.
But it is known that user-based CF often su ers from the data sparsity problem because most of the user-item ratings are missing in practice.
The item-based CF, on the other hand, allows input of additional item-wise information and is also capable of capturing the interactions among them [11, 12].
This is a major advantage of item-based CF when it comes to dealing with items that are networked, which are usually encountered on the Web.
For example, consider the problem of document recommendation in a digital library such as the CiteSeer (http://citeseer.ist.psu.edu).
As illustrated in Fig. 1, let documents be denoted as vertices on a directed graph where the edges indicate their citations.
The similarity among documents can be measured by their cocitations (cociting the same documents or being cocited by others) 1.
In this case, document B and C are similar because they are cocited by E.
Working with networked items for CF is of recent interest.
Recent work approaches this problem by leveraging the item similarities measured on an item graph [12], modeling item similarities by an undirected graph and, given several vertices labeled interesting, perform label propagation to rank the remaining vertices.
The key issue in label propagation
 two concepts in information sciences: bibliographic coupling and cocitation.
Figure 1: An example of citation graph.
on graphs is the measurement of vertex similarity, where related work simply borrows the recent results of the Lapla-cian on directed graphs [2] and semi-supervised learning of graphs [18].
Nevertheless, using a single graph Laplacian to measure the item similarity can over t in practice, especially for data on the Web, where the graphs tend to be noisy and sparse in nature.
For example, if we revisit Fig. 1 and consider two quite common scenarios, as illustrated in Fig. 2, it is easy to see why measuring item similarities based on a single graph can sometimes cause problems.
The  rst case is called missing citations, where for some reason a citation is missing (or equivalently is added) from the citation graph.
Then the similarity between A and B (or C) will not be encoded in the graph Laplacian.
The second case, called same authors, shows that if A and E are authored by the same researcher Z, using the citation graph only will not capture the similarity between D and B, which presumably should be similar because they are both cited by the author Z.
(a) Missing citations (b) Same authors Figure 2: Two common problematic scenarios for measuring item similarities on a single citation graph: missing citations and same authors.
Needless to say, the cases presented above are just two of the many problems caused by the noise and sparsity of the citation graph.
Noise in a citation graph is a result of a missing citation link or an incorrect one.
Fortunately, real world data can usually be described by di erent semantics or can be associated with other data.
In the focus of relational data in this paper, we work with several graphs regarding the same set of items.
For example, for document recommendation, in addition to the document citation graph, we also have a document-author bipartite graph that encodes the authorship, and a document-venue bipartite graph that indicates where the documents were published.
Such relationship between documents and other objects can be used to improve the measurement of document similarity.
The idea of this work is to combine multiple graphs to calculate the similarities among items.
The items can be the full vertex set of a graph (as in the citation graph) or can be a subset of a graph (as in document-author bipartite graph) 2.
work [16] where multiple graphs with the same set of vertices are combined.
By doing so, we let data from di erent semantics regarding the same item set complement each other.
In this paper, we implement a model of learning from multiple graphs by seeking a single low-dimensional embedding of items that captures the relative similarities among them.
Based on the obtained item embedding, we perform label propagation, giving rise to a new recommendation framework using semi-supervised learning on graphs.
In addition, we address the scalability issue and propose an incremental version of our new method, where an approximate embedding is calculated only for the new items.
The new methods are evaluated on two real world datasets prepared from Cite-Seer.
We compare the new batch method with a baseline modi ed from a recent semi-supervised learning algorithm on a directed graph and a basic user-based CF method using Singular Value Decomposition (SVD).
Also, we compare the new incremental method with the new batch method in terms of recommendation quality and e ciency.
We observe signi cant quality improvement in our batch method and signi cant e ciency improvement with tolerable quality loss for our incremental method.
The contributions of this work are: (1) We overcome the de ciency of a single graph (e.g.
noise, sparsity) by combining multiple information sources (or graphs) via a joint factorization to learn rich yet compact representation of the items in question; (2) To ensure e ectiveness and e ciency, we propose several novel factorization strategies tailored to the unique characteristics of each graph type, each becoming a sub-problem in the joint framework; (3) To handle the ever-growing volume of documents, we further develop an incremental updating algorithm that greatly improves the scalability, which is validated on two large real-world datasets.
The rest of this paper is organized as follows: Section 2 introduces how to realize recommendations using label propagation; Section 3 describes our method for learning item embedding from three general types of graphs; Section 4 further introduces the incremental version of our algorithm; Experiments are presented in Section 5; Section 6 discusses the related work; Conclusions are drawn in Section 7.
Label propagation is one typical kind of transductive learning in the semi-supervised learning category where the goal is to estimate the labels of unlabeled data using other partially labeled data and their similarities.
Label propagation on a network has many di erent applications.
For example, recent work shows that trust between individuals can be propagated on social networks [7] and user interests can be propagated on item graphs for recommendations [12].
In this work, we focus on using label propagation for document recommendation in digital libraries.
Let the document set be D, where |D| is the number of documents.
Suppose we are given the document citation graph GD = (VD, ED), which is an unweighted directed graph.
Suppose the pairwise similarities among the documents are described by the matrix S   R measured based on GD.
A few documents have been labeled  interesting  while the remaining are not, denoted by positive and zero values in the label vector y.
The goal is to  nd the score vector f   R where each element corresponds to the propagated interests.
Then


 documents by their interest scores.
A recent approach addressed the graph label propagation problem by minimizing the regularization loss below [18]:  (f )   f
 (I   S)f +  (cid:5)f   y(cid:5)2 , (1) where   > 0 is the regularization parameter.
The  rst term is the cost function for the smoothness constraint, which prefers small di erences in labels between nearby points; the second term is the  tting constraint that measures the di er-ence of f from given data label y.
Setting the  (y)/ f = 0, we can see that the solution f is essentially the solution to the linear equation:   (I    S)f   = (1    )y, (2) t   f   (3) t+1    Sf where   = 1/(1 +  ).
One solution to the above is given in a related work using a power method [18]: + (1    )y = f where f 0 is the random guess and f is the solution.
Here, notice that L = (I S) is essentially a variant Lapla-cian on this graph using S as the adjacency matrix; and  1 = L 1 is the graph di usion kernel.
Thus, K = (I    S) = (1   )Ky one essentially applies f )to rank documents for recommendation.
Now the interesting question is how to calculate S (or equivalently the kernel K) among the set D. However, there has been limited amount of work on obtaining S. For graph data, recent work borrows the results from spectral graph theory [1, 2], where the similarity measures on both undi-rected and directed graphs have been given.
For undirected graph, Su is simply the normalized adjacency matrix: = (1   )L 1y (or f     Su =    1/2  1/2
 (4) where   is a diagonal matrix such that W e =  e and e is an all-one column vector.
For directed graph, where the adjacency matrix is  rst normalized as a random walk transition  1W ), the similarity measure Sd is calculated matrix P (=   as:
 Sd =


 (5) where   is a diagonal matrix where each diagonal contains the stationary probability on the corresponding vertex 3.
Note that the similarity measures given above are derived from a single graph on D. However, many real world data can be described by multiple graphs, including those within D and between D and another set.
Such information is of more importance to combine especially when the a single view of the data is sparse or even incomplete.
In the following, we introduce a new way to integrate three general types of graphs.
Instead of estimating S directly, we seek to learn a low-dimensional latent linear space.
The immediate goal of this section is to determine the relative positions of all documents in a k-dimensional latent semantic space, say X   R |D| k, which will combine the social inferences in document citations, authorship and venues.
ing edges, we can incorporate certain randomness so that P denotes an ergodic Markov chain.
In the sequel, we assume k is a prescribed parameter which we do not seek to determine automatically.
Note a contribution of this work is the di erent strategies used for di erent graphs based on their characteristics, which are described in the following subsections.
We begin by a formulation of our problem.
Let D, A, V be the sets of documents, authors and venues and |D|, |A|, |V| be their sizes.
We have three graphs, one directed graph GD on D; one bipartite graph GDA between D and A; and one bipartite graph GDV between D and V, which describe the relationship among documents, between documents and authors, and between documents and venues.
Let the adjacency matrices of GD, GDA, GDV be D, A and V .
We assume all relationships in question are described by non-negative values.
For example, GD can be considered as to describe the citation relationship among D and Di,j = 1 if document di cites dj (Di,j = 0 if otherwise); GA can be considered as the authorship relationship (an author composes a document) or the citation relationship (an author cites a document) between D and A.
In this section, we relate the document embedding X to the citation matrix D, which is the adjacency matrix of the the directed graph GD.
The citation matrix D include two kinds of document co-occurrences: cociting and being cocited.
A cociting relationship among a set of documents means that they all cite a same document; A cocited relation refer to that several documents are cited together by an another document.
In many related work (e.g.
[18]) on directed graphs, these two kinds of document co-occurrences are used to infer the similarity among documents.
Probably the most well recognized way to represent the similarities among the nodes of a graph is associated with the graph Laplacian [2], say L   R
 , which is de ned as: L = I    Sd, (6) where Sd is the similarity matrix on directed graphs as measured in Eq.
5;     (0, 1) is a parameter for the Laplacian to be invertible; I is an identity matrix.
Note that Sd is symmetric and positive-semide nite.
Next we give the method to learn from GD.
Objective function: Suppose we have a document embedding X = [x1, ...xk] where xi contains the distribution of values of all documents on the i-th dimension of a k-dimensional latent space.
The overall  lack-of-smoothness  of the distribution of these vectors w.r.t.
to the Laplacian L can be measured as i Lxi = Tr(X
 x
 (7) (cid:2)
 1 i k where X = [x1, ...xk].
Here we seek to minimize the overall  lack-of-smoothness  so that the relative positions of documents in X will re ect the similarity in Sd.
Constraint: In addition to the objective function of X, we enforce a constraint on X so as to avoid getting a trivial solution (Note that X = 0 minimizes Eq.
7 if there is no constraint on X).
We choose to use the newly proposed log-determinant heuristic on X T X, a.k.a the log-det heuristic, denoted by log |X T X| [6].
It has been shown that the log |Y | is a smooth approximation for the rank of Y if Y is a positive semide nite matrix.
It is obvious the gram matrix X T X is
 positive semide nite.
Thus, when we maximize log |X T X|, we e ectively maximize the rank of X, which is at most k. Another way to understand log |X T X| is to note that
 i  i(X)2, where  i(Y ) is the i-th eigenvalue of Y and  i(X) is the i-th singular value of X.
Therefore, a full-ranked X is preferred when log |X T X| is maximized.
For more reasons on using the log-det heuristic, refer to the Comments below and [6].
i  i(X T X) = (cid:3) Using the log-det heuristic, we arrive at the combined optimization problem: (cid:4) min
 Tr(X TLX)   log |X

 (cid:5) (8) where Tr(A) is the trace function de ned as the sum of diagonal elements of A.
It has been shown that max{log |X T X|} (or equivalently min{  log |X T X|}) is a convex problem [6].
So Eq.
8 is still a convex problem.
Comments: First, it is interesting to notice that we did not use the traditional constraint on X (such as the or-thonormal constraint of the subspace used in PCA [15]).
The reason of choosing log-det heuristic in our case is because that (1) the orthonormal constraint is non-convex while the remaining of the problem is; (2) the orthonormal constraint cannot be solved by gradient-based methods and thus cannot be e ciently solved and cannot be easily combined with the other two factorizations in the following sections; (3) the log-det, log |X T X|, has a small problem scale (k   k) and can be solved e ectively by gradient-based methods.
Second, note a key di erence of this work from related work on link matrix factorization (e.g.
[20]) is that we seek to determine X to comply with the graph Laplacian (not to factorize the link matrix) which gives us a convex problem that is global optimal.
Here, we show how to learn from an author matrix, A, which is the adjacency matrix of the bipartite graph, GDA, that captures the relationship between D and A.
We can use GDA to encode two kinds of information between authors and documents, one being the authorship and the other being the author-citation-ship.
To encode authorship, we let (I   {0, 1}), where Ai,j indicates whether the
 i-th paper is authored by the j-th author; To encode author-citation-ship, we assume A   R , where Ai,j can be the number of times that document i is cited by author j (or the logarithm of the citation count for rescaling).
We consider both kinds of author-document relationship equivalently using matrix factorization, where authors in both cases are considered social features of documents, inferring similarities between documents.
The basic intuition is that the document related to a same set of authors should be relatively close in the latent space X.
The inference of this intuition to citation recommendation is that the other work of an author will be recommended given a reader is interested in several work by similar authors.
Given the authorship matrix A   R , we want to use X to approximate it.
Let the authors be described by an author pro le matrix W   R |A| k.
We can approximate A by XW T as: (cid:5)A   XW T(cid:5)2 F +  1(cid:5)W(cid:5)2
 min
 (9)
 where X and W are the minimizers.
To prevent over tting, the second term is used, where  1 is the parameter.
Note that later we will combine Eq.
8 and Eq.
9; So we do not show the constraint on (cid:5)X(cid:5)2 F here.
It is worth mentioning that the idea of using two latent semantic spaces to approximate a co-occurrence matrix is similar to that used in document content analysis (e.g.
the LSA [5]).
In the above, we have given the method for learning a representation of D from a directed citation graph GD and an undirected bipartite graph GDA.
In this section, we are given an additional piece of categorical information, which can be described by the bipartite venue graph GDV , where one set of nodes are the documents from D and the other set are the venues from V.
Similar to A, we have the venue matrix V   I , where Vi,j denotes whether document i is in venue j. However, a key di erence here is that each row in V has at most one nonzero element because one document can proceed in at most one venue.
Although we could as well employ XW T to approximate V (as in Sec.
3.2), we will show that the special property of V can help us cancel the variable matrix W , and thus reducing the optimization problem size for better e ciency.
Accordingly, we follow a similar but di erent ap-In particular, let us consider to use V to predict proach.
the X via linear combinations.
Suppose we have W2 as the coe cient, we seek to minimize the following: (cid:5)V W


 min
 (10) One can understand Eq.
10 in this way: Here each column of W2 can be considered as a cluster center of the corresponding class (i.e., the venues).
Then solving Eq.
10 in fact simultaneously (1) pushes the representation of documents close to their respective class centers; and (2) optimizes the centers to be close to their members.
 1
 (cid:5)V (V Next, we cancel W2 using the unique property of our venue matrix V .
Setting the derivative to be zero, we have 0 = F / W2 = 2(V T V W2   V T X), suggesting that
  (cid:5)V W T  1V T X.
Note that V T V is diagonal matrix
 and is thus invertible.
Plug in W2 back to Eq.
10.
We arrive at the optimization where W2 is canceled: X   X(cid:5)2
 (11) min
  1V T is the pseudo inverse of V .
Here since where (V T V ) V T V is |V|   |V| diagonal matrix, its inverse can be computed in |V|  ops.
Meanwhile, V (V T V )  1V T is block diagonal where each block denotes a complete graph among all documents within the same venue.
Note that Eq.
9 cannot  1 is a dense ma-be handled in the same way because (AT A) trix, resulting in a |D|   |D| dense matrix of A(AT A)
 which in practice raises scalability issues.
We have arrived at a combined optimization formulation given the above sub-problems.
We will combine Eq.
8, Eq.
9 and Eq.
10 in a uni ed optimization framework.
De ne the new objective J(X, W ) as a function of X, W .
We have an optimization below to learn the document embedding matrix
 (Tr(X TLX)   log |X T X| + (cid:5)A   XW T(cid:5)2 F +  (cid:5)W(cid:5)2 + (cid:5)V (V T V )  1V T X   X(cid:5)2

 (12)
 for learning from A;   is the weight for learning from V .
In this paper, we only empirically  nd the best values for   and   that yield the best F-scores for the current data set.
Future work on how to choose parameter values will be helpful to practitioners.
The optimization illustrated above can be solved using standard Conjugate Gradient (CG) method, where the key step is the evaluation of objective function and the gradient.
In Appendix .1, we show the gradients for the combined optimization.
After X is calculated, we can use linear model in the rec 1X T y.
We can obtain ommendation, i.e. f e ciency advantage over the power method as in Eq.
3.
 


 An incremental version of our new method will be proposed in this section.
The goal of incremental update of X is to avoid heavy computation of known documents when there is a small size of update.
The incentive for designing an incremental update algorithm is to delay (or avoid) re-computation in a batch approach.
The incremental update of X we will give is an e cient approximate solution.
In particular, suppose we have used document D0, V0, A0 and their relationship at time t0 to compute a document embedding X0 for the document set D0.
Now, at time t1, we have observed an additional set of new documents D1.
How can we use the pre-computed X0 to compute an embedding of D1 in X1   R |D1| k e ciently?
Note that typically |D1| is much smaller than |D0|.
We rewrite the objective function in Eq.
12.
Let X be the minimizer.
We assume that the embedding of old documents is in X0 and the X1   R |D1| k is the embedding for D1.
Here

     in the three new matrices below:


  

  


  
     ,

 where the A encodes the new document-author relationship; the V encodes the new the document-venue relationship (assuming no emergence of new venues); and the L denotes the new Laplacian calculated on the updated document citation graph.
By convention, the index 0 corresponds to the original part of the matrix and the index 1 indicates the new part.
For example, V0 is the venue matrix at time t0 and V1 is the venue matrix at time t1.
Consider the objective function in Eq.
12.
After several rewrites as entailed in Appendix .2, the objective function in Eq.
12 on the new set of matrices now becomes the following: J = Tr(X T

   log |X T + (cid:5)W0(cid:5)2 0 (cid:5)2 0 (cid:5)2 + (cid:5)A00   X0W T + (cid:5)A10   X1W T + (cid:5)(V0  + (cid:5)V1 









 F +  (cid:5)W1(cid:5)2 1 (cid:5)2 F +  (cid:5)A01   X0W T F +  (cid:5)A11   X1W T 1 (cid:5)2












 (13) where the coe cients are L, A, V , and   = (V T  

  ; The param- 
 The variables are X =

  

  

 eters are  ,  ,  .
We will make the Eq.
13 more e cient in this section, hoping to only calculate the incremental part of X for the new documents in D1.
First, let us assume that the incremental update of X only seek to update the embedding of D1 but does not change the original embedding of D0, i.e. that X0 is  xed.
Similarly, W0 is  xed for the authors observed before.
Second, we can see that V0 in V is  xed because documents will not change venues over time.
Third, we show that the segment in the new Laplacian L01 is approximately zero because no old documents can cite new documents which results in relatively small stationary probabilities on the new documents (we will show more details for this proposition in Appendix .3).
Given the above assumptions and observations, after discarding the constant terms, we have the following optimization for incremental update of X: Japp = Tr(X T
 + (cid:5)A01   X0W T 1 (cid:5)2

 F +  (cid:5)A10   X1W T 0 (cid:5)2


 + (cid:5)A11   X1W T 1 (cid:5)2
 F +  (cid:5)W1(cid:5)2








 + (cid:5)(V0 
 + (cid:5)V1 
 where   = (V T


 that has |D1|   k and |A1|   k elements respectively.
Since D1 and A1 are very small, the incremental calculation of X1 can be achieved very e ciently.
Again, this problem can be solved using conjugate gradient method where the gradients of Eq.
14 are presented in Appendix .1.
A real-world data set for experimentation was generated by sampling documents from CiteSeer using combined document meta-data from CiteSeer and another two sources (the ACM Guide, http://portal.acm.org/guide.cfm, and the DBLP, http://www.informatik.uni-trier.de/ ley/db) for enhanced data accuracy and coverage.
The meta-data was processed so that the ambiguous author names and noisy venue titles were canonicalized 4.
Since the data in CiteSeer are collected automatically by crawling the Web, we may not have enough information about certain authors.
Accordingly, we collected the documents by those top authors in CiteSeer ranked by their numbers of documents.
Then we collected the venues of these documents.
Similarly, we kept those venues with most documents in the prepared subset and discarded the venues that include fewer documents.
Following the same procedure, two datasets were prepared with di erent sizes.
The  rst dataset, referred to as DS1, has 400 authors, 9, 197 documents, 50 venues, and 19, 844 citations; The second dataset, referred to as DS2, which is larger in size, has 800 authors, 15, 073 documents, 100 venues, and 38, 614 citations.
ference proceedings from di erent years or the journals of di erent issues, were treated as the same venue.
The performance of recommendation can be measured by a wide range of metrics, including user experience studies and click-through monitoring.
For experimental purpose, this paper will evaluate the proposed method against citation records by cross-validation.
In particular, we randomly remove t documents, use the remaining documents as the seeds, perform recommendations, and judge the recommendation quality by examining how well these removed documents can be retrieved.
As suggested by real user usage patterns, we are only interested in the top recommended documents.
Quantitatively, we de ne the recommendation precision (p) as the percentage of the top recommended documents that are in fact from the true citation set.
The recall (r) is de ned as the percentage of true citations that are really recommended in the top m documents.
The F-score, which combines precision and recall is de ned as f = (1 +  2)rp/(r +  2p), where     [0, ) determines how relatively important we want the recall to be (Here we use   = 1, i.e. F-1 score, as in many related work.)
5 We have introduced a parameter in evaluation, m, which is the number of top documents we evaluate the f-score at.
This section introduces the experiments on recommendation quality.
We compare the recommendation by our algorithm with two other baselines: one based on Laplacian on directed graphs [2] and label propagation using graph Laplacian [18] (named as Lap) and the other based on Singular Vector Decomposition of the author matrix (named as SVD) 6.
We chose to compare with the Lap method to see whether the fusion of di erent graphs can e ectively produce additional information than the original graph citation graph; We chose the SVD on author matrix as another baseline because we would like compare our method against the traditional CF method on the additional graph information (as one can argue that the signi cant improvement of the new method is purely due to the use of the additional information).
f \ m m=t m=5 m=10
 f(lap)


 f(svd)


 f(new)


 f(lap)

 f(svd)




 f(new)


 Table 1: The f-score calculated on di erent numbers of top documents, m.
address, we cannot use the Mean Average Error (MAE), which is used for measuring the quality of a Collaborative Filtering algorithm, because we do not seek to approximate the ratings of documents but to preserve their preference orders in the recommendation ranking.
trix, the SVD of the rating is in fact a simple Collaborative Filtering (CF) method.
However, due to di erent objectives of our problem and the traditional CF, we will see later that our method outperforms SVD towards our goal signi cantly.
f \ t f(lap)
 f(svd) t=1 t=2



 t=3

 t=4

 f(new)



 f(lap)

 f(svd)






 f(new)



 Table 2: The f-score w.r.t.
di erent numbers of left-out documents, t.
Table 1 and Table 2 list the f-scores (de ned in Sec.
5.1) of three di erent methods (our new method with Lap and SVD) on two datasets (DS1 and DS2).
Table 1 for di erent number of top documents evaluated on (denoted by m).
We are able to see that the new method outperforms both Lap and SV D signi cantly on both datasets in di erent settings of parameters.
In general, the new method are 3   5 times better in f-score than Lap and 2.5 times better than SV D.
The Lap method under-performs SV D on the very top documents but beats it if evaluated on more top documents.
In addition, we notice that the f-scores get better in general as we look at more top documents.
Also, the f-scores on the smaller dataset DS1 are generally higher than those on the larger dataset DS2.
Here, we can see that the recommendation quality can be signi cantly improved by using the author matrix as the additional information.
Note that the di erent information, when used individually, such as the Lap on the citation graph or the SV D on the author graph, can be not as good.
However, if the multiple information are combined, the performance is greatly improved7.
The e ect of parameters for the new method is experimented in this section.
We experiment with di erent settings of dimensionality, or k, and weights on authors and venues, or   and  .
In Table 3, we show the f-scores for di erent k s.
It occurs that the f-scores become higher for greater k. We believe this is because the higher dimensional space can better captures the similarities in the original citation graphs.
However, on the other hand, we observe that it takes longer training time for greater k. Seeking k thus become a trade-o  between quality and e ciency.
In our experiments, we chose k = 100 as greater k do not seem to give much better results.
The CPU time for training at di erent k s are illustrated in Table 4.
f \ k

 k=50 k=100 k=150 k=200







 Table 3: The f-score w.r.t.
di erent setting of di-mensionality, k.
methods of formulating the author matrix, A, for example, using the number of citations from authors to documents in A.
The experiments show that using the citation-ship in A can be even better.
Due to space limit, here we present the experiments with authorship in A only.
t(new) time \ k k=50 k=100 k=150 k=200 The improvement is more signi cant on larger dataset (DS2) than small ones (DS1).
694s 940s 440s 638s 502s 743s 558s 820s 621s 910s Table 4: The CPU time for recommendations w.r.t.
di erent dimensionality, k.
) c e s ( e m i t Fig. 3 illustrates the f-scores for di erent settings of   and  , which are respectively the weights on authors and venues.
We determine which of the two components obtains greater improvement if incorporated, search for the best parameter for this component,  x it, and then search for the best parameter for the other component.
In our experiments, we observe that adding the author component tends to improve the recommendation quality better so we  rst tune  , which yields di erent f-scores, as shown by the blue curve in Fig. 3.
Then we  x the   = 0.1 and tune  , arriving at the best f-score at   = 0.05.
i g n n a r
 i ) c e s ( e m i t i g n n a r
 i

















 Batch training by percentage Incremental update







 Percentage of documents to update Batch training by percentage Incremental update







 Percentage of documents of update








 e r o c s   f



  : author weight  : venue weight



 Parameter value for  ,  


 Figure 3: f-scores for di erent settings of weights on the authors,  , and on the venues,  .
The   is tuned  rst for   = 0; Then   is tuned for the  xed best   = 0.1.
Here we present the experiments for another contribution of this work: incremental update.
The incremental update method we propose seeks to determine an approximate embedding of documents by working with the incremental data and the relationship between the new data and the old.
We evaluate this new method in its training time, recommendation quality, and propagation of errors.
Fig. 4 illustrates the comparison of training time for the incremental method and batch update by percentage on both datasets.
We try to use a fair baseline.
In particular, we compare with a percentage of batch update time, where the percentage re ects the relative amount of incremental data.
As illustrated in Fig. 4, the incremental method takes on average 1/2   1/5 of the training time of batch method.
Figure 4: Training time for incremental update and batch method w.r.t.
di erent percentage of incremental data on DS1 and DS2.
The training time for batch method is the corresponding percentage of the overall training time.
The next natural question to ask is how much quality has to be compromised for the improvement of e ciency.
Fig. 5 present the comparison of f-scores for di erent percentage of incremental using the incremental method with the batch method applied to the full data.
It turns out that the performance of incremental method deteriorates as the incremental data takes a large percentage.
Fortunately, the f-scores decrease at a slower ratio for the larger dataset (DS2).
This is because that more information is captured by the larger dataset with larger absolute size.
On average, the deterioration of recommendation quality can be signi cant if the incremental data takes more than 30% of the data.
So we would suggest rerun the batch process when the updated corpus exceeds the original size signi cantly.
Finally, we present the propagation of errors if the incremental update is applied to multiple times.
It has come to our attention that the performance deteriorates at a faster pace if one applies multiple steps of incremental updates.
Fig. 5.4 illustrates the f-scores w.r.t.
di erent numbers of steps in the incremental updates, for di erent overall percentage to update.
Notice that the f-scores deteriorate faster if the overall percentage of update is greater.
Also, the f-scores decrease slower at  rst 1   2 steps and faster from the 3rd step onwards.
It is then suggested that the new incremental method should be used with caution, preferring fewer number of uses or on a larger percentage of data for each use.
It seems that the error in the incremental updates is propagated more than linearly.
The incremental update methods presented in this section addresses the scalability issues in recommendation of large-scale dataset on the Web.
In practice, we recommend a combination of batch update and incremental update seeking a tradeo  between e ciency and scalability.
e r o c s   f






 e r o c s   f








 Percentage of update on DS1



 Percentage of update on DS2 Incremental update Batch update


 Incremental update Batch update


 Figure 5: f-scores for di erent percentage of incremental data in the incremental update, on DS1 and DS2, w.r.t.
the batch method applied to the full data.
e r o c s   f





 e r o c s   f


 p=0.1 p=0.2 p=0.3
 p=0.1 p=0.2 p=0.3



 Number of steps in incremental updates


 Number of steps in incremental updates Figure 6: f-scores for di erent numbers of steps in the incremental updates, for di erent overall percentage (p) to update, on DS1 and DS2.
This work is  rst related to a family of work on categorization of networked documents.
Categorization of networked documents is developed based on the link structure and the co-citation patterns (e.g.
[8] for Web document clustering).
In [8], all links are treated as undirected edge of the link graph and the content information is only used for weighing the links by the textual similarity between documents on both ends of the link.
Very recently, Chung[2] has proposed a regularization framework on directed graphs.
Soon after, Zhou et.al.
[18] used this regularization framework on directed graphs for semi-supervised learning, which also seek to explain ranking and categorization in the same semi-supervised learning framework.
Later, a work by Zhou et.al.
extended the regularization to multiple graphs with the same set of vertices [16], which, however, is di erent from this work where the item set can be either a full set or a subset of the graphs in question.
This work also relates to the category of work that approach document analysis via embedding documents onto a relatively low dimensional latent space [5, 17].
Latent Semantic Indexing (LSI) [5] is a representative work in this category that uses a latent semantic space to implicitly capture the information of documents.
Analysis tasks, such as classi cation, could be performed on the latent space.
Another commonly used method, Singular Value Decomposition (SVD), ensures that the data points in the latent space can optimally reconstruct the original documents.
Based on similar idea, Hofmann [9] proposed a probabilistic model, called Probabilistic Latent Semantic Indexing (pLSI).
This work is similar but di erent to pLSI in that we not only approximate one single document matrix but several matrices at the same time.
Finally, this work shares the idea of related work on combining multiple sources of information.
In this category, prior work by Cohn and Hofmann [4] extends the latent space model to construct the latent space from both content and link information, using content analysis based on pLSI and PHITS [3], which is a direct extension of pLSI on the links.
In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.
In that model, the outgoing links of the destination web page have no e ect on the source web page.
In other words, the overall link structure is not utilized in PHITS.
Communitiy discovery has also been done purely based on document content [19].
Recent work.
[20] utilizes the overall link structure by representing links using the latent information of their both end nodes.
In this way, the latent space truly uni es the content and the underlying link structure.
Our work is similar to that of [20] but we not only considers links but also co-link patterns by using the Laplacian on directed graphs.
We address the item-based collaborative  ltering problem for items that are networked.
We propose a new method for combining multiple graphs in order to measure item similarities.
In particular, the new method seeks a single low-dimensional embedding of items that captures the relative similarities among them in the latent space.
We formulate this as an optimization problem, where the learning of three general types of graphs are formulated as three sub-problems, each using a factorization strategy tailored to the unique characteristics of the graph type.
Based on the obtained item embedding, a new recommendation framework is developed using semi-supervised learning on graphs.
In addition, we address the scalability and propose an incremental version of the new method.
Approximate embeddings are calculated only for new items making it very e cient.
The new batch and incremental methods are evaluated on two real world datasets prepared from CiteSeer.
Experiments have demonstrated signi cant quality improvement for our batch method and signi cant e ciency improvement with tolerable quality loss for our incremental method.
For future work, we will pursue other applications of the new graph fusion technique, such as clustering or classi cation.
In addition, we want to extend our framework to graphs with hyperedges.
This work was funded in part by the National Science Foundations and the Raytheon Corporation.
as V =
 .1 The Gradients for Eq.
12 and Eq.
14 The gradients for Eq.
12 are:

 =

  1 +2 (XW T W   AW ) + +2 (V V
 = 2 (W X T X   AT X) + 2 W



   (16)  1V T is the pseudo inverse of V .
When where V searching for the solutions, we vectorize the gradients of X, W into a long vector.
In implementation, di erent calculation order of matrix product leads to very di erent ef- ciency.
For example, it is much more e cient to calculate


 because V and V     I)X as (V  
 are very sparse.
      The gradients for Eq.
14 are:

 + (X1W



  1  Japp






























 where P0 = (V0 
 The gradients of Eq.
14 w.r.t.
W1 are
 + (Q (17)





  Japp
 =  (W1X T





 + (W1X T
 .2 Rewriting the Objective Functions First, for the terms in Eq.
12 for learning from A, we introduce another set of variables in W1   R |A1| k, to describe the new authors in A1.
Then we have W T = [W T
 the document-author relationship be encoded in the author

 (18)    .
Then we have the following: matrix A: A =  

 (cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)  

        

     (cid:11)



 (cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)






 1 (cid:5)2 F + (cid:5)A01   X0W T 0 (cid:5)2 (cid:5)A00   X0W T F + (cid:5)A11   X1W T +(cid:5)A10   X1W T 0 (cid:5)2 1 (cid:5)2




 (cid:12) (cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)

 (cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10) (cid:5)A   XW T(cid:5)2
 = = and (cid:5)W(cid:5)2 F becomes (cid:5)W(cid:5)2
 (cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10) (cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)



 = (cid:5)W0(cid:5)2 F + (cid:5)W1(cid:5)2
 (20) Second, for the term in Eq.
12 regarding learning from venue matrix, V , we assume that there are no new venues showing up between t0 and t1.
So the new V takes the form  

     where V0 is the venue matrix at time t0.
 1V T =  .
We can see that Let the component V (V T V ) the learning objective becomes (cid:5)(    I)X(cid:5)2
 where (15)




      

  

    (cid:11)  
  

 = =          1 (cid:11) (cid:12)  

 (cid:11)  1



 (cid:12)

















     (21) (cid:12) (22)

 and   = (V T
 verse is very easy to compute.
Then we plug Eq.
22 into Eq.
21.
After several simple manipulations, we arrive at the following learning objective for venues: (cid:5)(V0   1
 (cid:5)V1   1 where   = (V T






  1





  1





 (23) Third, for the Laplacian terms in Eq.
12 for learning from the citation graph D, we have the following identities: =



     Tr(X TLX)    







     = Tr(X T



 (24) and log |X
 X| = log |X





 (25) where L00 is a |D0|   |D0| matrix for the graph on D0; L01 is a |D0|   |D1| matrix for interaction between D0 and D1; and L11 is a |D1|   |D1| matrix for the graph on D1.
.3 The Laplacian L is almost block diagonal: Here we will show that the Laplacian L on the new matrix   D at time t1 is near block diagonal.
Recall that the citation  .
matrix D at time t1 can be written as D =  

 Here, D00 is the same as the citation matrix used to compute the Laplacian at time t0.
Remember that L = I    S, where S is the similarity measured on the directed graph D in Eq.
5:
 (26)  1/2 and P is the stochastic matrix nor-where  S =  1/2P   malized from D and   is a diagonal matrix containing the )














   1

   1


     .
  1

 (28) stationary probabilities.
We rewrite  S as follows:  

    

  

      

   1
    
 (27) where P00, P01, P10, P11 are normalized from D00, D01, D10, D11 and the diagonal matrix  00 ( 11) contains the stationary probabilities on the old (new) documents.
We further know that P01 = 0 because new documents D1 cannot be cited by the old documents.
So we have:



   1










 And we also know that the new documents D1, with few citations among themselves, mainly cite the old documents in D0.
Thus, in the case when D0 is much larger than D1, the stationary probabilities on the new documents D1 are very   1 small, i.e.  11   0.
This gives us  

 have shown that  S is almost diagonal.
Let us rewrite  S as:   1
  S   diag(   
  
 which is almost block diagonal, i.e. L01   0.
However, note   1
 that L11 is not necessarily zero because   11 contains   1
 both  
 L00 in the new L is identical to the original Laplacian on D0.
Nevertheless, we discard the term X T
 because X0 is assumed to be unchanged.
