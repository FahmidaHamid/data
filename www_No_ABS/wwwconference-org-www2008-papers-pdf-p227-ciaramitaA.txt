Sponsored search is the task of placing ads that relate to the user s query on the same page as the search results returned by the search engine.
Typically sponsored search results resemble search result snippets in that they have a title, and a small amount of additional text, as in Figure 1.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
When the user clicks on the title he is taken to the landing page of the advertiser.
Search engine revenue is generated in large part by the sponsored results.
Of the $16.9 billion generated by online advertising in 2006, 40% was generated by search advertising [12].
Clearly it is an advantage to the search engine to provide ads users will click.
While the  nal ranking takes into account the amount bidded by the advertiser and, more generally, the micro-economic model adopted by the search engine to maximize revenue, relevance, or quality of the ads returned is a crucial factor.
Users are more likely to click ads that are relevant to their query.
In fact, there is evidence that the level of congruence between the ad and its context has a signi cant e ect even in the absence of a conscious response such as a click [34].
If we assume that congruency equates to topical similarity or  relevance , the task is to show the ads that are most similar to the user s query in the sponsored results.
With this in mind we would like to place ads in the sponsored results that are a good match for the user s query.
In this paper we abstract from the economic aspects of the problem and focus on the issue of improving the quality of the ads proposed as relevant to the query.
Information about the quality of match at the content level can be taken into account later, including the micro-economic model, to compile the  nal list of ads to be displayed.
While the publisher goal is maximizing pro t, we stress the fact that maximizing ads quality or relevance is a crucial factor in this process.
Aside from the di culties in assessing the similarity of an ad to a query that stem from the sparseness of the representation of both the query and the ad, the task of placing ads is complicated by the fact that users click on ads for a wide variety of reasons that are not re ected in the similarity of an ad to a query.
For example, there is a strong positional bias to user clicks.
Users are much more likely to click on items at the top of a ranked list of search results than items lower in the ranking [11, 14].
This makes using the click data to learn a ranking function over the ads and to evaluate the system more di cult.
Speci cally, user clicks are not an indication of absolute relevance.
Rather, as Joachims proposed [13], the user click mainly indicates the clicked item is more relevant than the those items which were ranked higher but were not clicked.
This observation implies that positive and negative examples can be extracted from query logs in a meaningful way, avoiding the complexities and noise of click-through rate estimation.
Building on Joachims  suggestion we create a training set from the query logs of a real sponsored search system.
We propose that this type of data can be also used directly
 Ads are shown on the right side under the label  SPONSOR RESULTS .
for evaluating a learning model.
A few studies have already shown that click feedback from users can be used to improve machine learned ranking [32, 1].
However, this type of information has not been used for both training and evaluation.
Previous work [32, 1] has ultimately relied on editorial data for evaluation.
We show that consistent results are obtained in this way across di erent ranking methods and di erent feature sets.
We formulate the problem of ranking a set of ads given a query as a learning task and investigate three learning methods of increasing complexity based on the perceptron algorithm: a binary linear classi er, a linear ranking model and a multilayer perceptron, or arti cial neural net.
Speci cally, we focus on online learning methods which suit the task of learning from large amounts of data, or from a stream of incoming feedback from query logs.
As hoped, we  nd that accuracy increases with the complexity of the model.
Retrieving ads is complex in part because the text is sparse.
In addition, features based on link structure that have been shown to bene t retrieval in a Web setting cannot be applied to ads because they lack an obvious link organization.
Exploiting our setup we investigate several classes of features which have been proposed recently for content match, the task of ranking ads with respect to the context of a Web page, rather than a query.
We start from the cosine similarity between query and ad.
We decompose the ad and use as features the similarity of individual components of the ad and the query, and also properties of the degree of overlapping between the query and the ad [25].
Next we investigate a class of language-independent, knowledge free, features based on the distributional similarity of pairs of words which have been used successfully in content match [6], and can be extracted from any text collection or query log.
These features measure the similarity between two texts independently of exact matches at the string level and are meant to capture indirect semantic associations.
In content match there are many words that can be extracted from a Web page to compute such features, while in sponsored search there are only the terms in the query.
We show that across all learning methods these features produce the best results.
Overall, with a compact set of just twelve features we improve signi cantly over all traditional models.
This paper presents the following primary contributions.
First, we show that click-data can be used directly for evaluation purposes which is a desirable property in the context of large scale systems that otherwise have to rely exclusively on editorial data, or carry out noisy estimations of click-through rates.
Second, we show empirically that di erent methods of increasing complexity can be applied to the task and generate consistent results.
This is important because it supports the hypothesis that the evaluation is consistent across di erent methods.
On the learning side, it also shows that taking into account pairwise information in training is bene cial in machine-learned ranking, even in noisy settings.
Finally, we provide empirical evidence on the utility of a class of simple features for ranking ads based on lexical similarity measures, which has possible applications to other query-based ranking tasks such as document retrieval and search in general.
The rest of the paper is organized as follows.
In Section 2 we introduce the method for unbiasing click logs.
Section 3 introduces several approaches to learning from click data for sponsored search.
Section 4 describes the features evaluated in our experiments, which are discussed in Sections 5 and 6.
Section 7 provides an overview of related work.
Employing user clicks to train and to evaluate a sponsored search system is a natural choice, since the goal in sponsored search is maximizing the number of clicks.
However, user clicks cannot be used in a straightforward manner because they have a strong positional bias [11, 14], and they only provide a relative indication of relevance [13].
There is a strong positional bias as highly ranked results or ads may be clicked because of their rank and not their relevance.
For example, a user may click on the top ranked ad and then click on the third ad in the ranking, even if the third ad may be more relevant to his query.
The reason for this bias is that users are likely to sequentially scan the ranked list of
 Blocks a1 a2 a3 a4 a5 a6 a2 1 a3 +1 a2 1 a4 1 1 a5 a6 +1 Figure 2.
Illustrative example of how blocks are generated from the clicked and non-clicked ads for a query.
items and may click on an item before scanning the whole list, or may never scan the whole list at all.
To investigate how to employ user clicks to train and evaluate a sponsored search system, we have collected a set of queries and the ads that had been shown with them on the right-hand side of the search engine results page, from the logs of the Yahoo!
Web search engine.
We sampled queries until we collected a su ciently large number of distinct clicked ads.
We sampled only queries with three or more query terms because longer queries are more likely to lead to higher conversion rates [24].
In other words, users issuing longer queries are more likely to visit a Web site and perform a transaction.
We also considered only one click for a query-ad pair from one user per day.
Following Joachims [13], we make a conservative assumption that a click can only serve as an indication that an ad is more relevant than the ads ranked higher but not clicked, but not as an absolute indication of the ad relevance.
In this setting, the clicks on the top ranked ad do not carry any information, because the top ranked ad cannot be ranked any higher.
In other words, there is no discriminative pairwise information associated with a click at position one.
Hence, we do not consider such clicks in the remainder of our experiments.
For each clicked ad, we create a block which consists of the clicked ad and the non-clicked ads that ranked higher, for a total of 123,798 blocks.
In each block, we assign a score of  +1  to the clicked ad and  1  to the ads that were ranked higher but were not clicked.
Figure 2 shows an example of the score assignment process.
On the left-hand side of the  gure, we show the ranking of six ads for a query.
The ellipses around ads a1, a3 and a5 denote that these ads were clicked by the user who submitted the query.
The  gold-standard  blocks of ads, shown on the right-hand side of the  gure, are generated in the following way.
First we ignore the click on ad a1 since this ad was already ranked  rst and it was clicked.
Then, we form one block of ads with a2 and a3, assigning scores of  1  and  +1 , respectively.
Next, we form a second block of ads consisting of a2, a4, a5 with scores  1  and a6 with score  +1 .
In Figure 2 blocks of ads are shown with shaded areas.
From the example above we obtain two gold standard blocks from one user session.
Alternatively, one could generate a single vector of binary values including all the positive and negative examples.
One potential problem is that in such a way one introduces e ectively an equivalence of relevance between clicked ads (positive points), while according to our conservative assumption we cannot really say anything about the relative relevance of two clicks in di erent blocks, because we do not have reliable means of interpreting clicks in an absolute way.
As a matter of fact, in some of the ranking models we propose (classi cation and multi-layer perceptron) the block structure is e ectively ignored in training.
However training on local pairwise preferences can be bene cial, as we show in the experimental section going from a global binary classi cation model to a ranking model which exploits the block structure.
The block-based encoding is compatible with both approaches and de nes a robust and intuitive learning task.
Learning with clicks can involve arbitrarily large amounts of data, or even learning from a continuous stream of data.
Online learning algorithms are the most natural choice for this type of task, since the data need not be considered (or stored in memory) all at once; each pattern is used for learning in isolation.
Notice how learning from blocks  ts perfectly online learning in that any feedback can be immediately used to update the current model, even when a query or ad is observed for the  rst time, without the need to accumulate evidence for reliable estimate of click-through rates.
As a general online learning framework we choose the per-ceptron algorithm.
The perceptron was invented by Frank Rosemblatt in 1958 [26], and was initially criticized because of its inability to solve nonlinear problems [22].
In fact, the perceptron, like support vector machines (SVM) and other methods, can learn nonlinear models by means of kernel functions in dual algorithms, or by means of higher-order feature mappings in the primal form, or again (see below Section 3.3) by means of multilayer architectures.
The perceptron has received much attention in recent years for its simplicity and  exibility.
Among other  elds, the per-ceptron is popular in natural language processing, where it has been successfully applied to several tasks such as syntactic parsing, tagging, information extraction and re-ranking [7, 29, 30, 31].
We preferred the perceptron over other popular methods, such as SVM, for which incremental formulations have been proposed [5, 17], because accurately-designed perceptrons (i.e., including regularization, margin functions, etc.)
often performs as well as more complex methods at a smaller computational cost.
Moreover, the simplicity of the algorithm allows easy customization, which is crucial in large scale settings.
In Section 3.4, we bench-marked one of our perceptron models on a ranking task, yielding results comparable to SVM and Boosting.
We investigate three approaches to learning to rank ads based on click data: classi cation, ranking, and nonlinear regression.
The general setting involves the following elements.
A pattern x   IRd is a vector of features extracted from an ad-query pair (a, q).
Each pattern xi is associated with a response value yi   { 1, +1}.
In classi cation we associate a vector for a pair which has not been clicked with 1, also referred to as class y0, and a vector for a pair which has been clicked with +1, also referred to as class y1.
The goal of learning is to  nd a set of parameters, or weights,   used to assign a score F (xi;  ) to patterns such that F (xi;  ) is close to the actual value yi.
In particular, we are interested in predicting the clicked ad in a block of ads.
In a classi cation framework the goal is to learn a function which is able to accurately assign a pattern to either the clicked or non-clicked class.
Patterns in the data are used independently of one another in training and the classi er simply  nds a weight vector which assigns each pattern to the correct class1.
After training, the classi er can be used to rank ads given a query; e.g., to identify the most likely clickable pattern in a block.
The basic classi er is a binary perceptron.
We extend the basic model by averaging and adding an uneven margin function.
Averaging is a method for regularizing the classi er by using   for prediction after training   the average weight vector of all perceptron models posited during training [10].
The uneven margin function is a method for learning a classi er with large margins for cases in which the distribution of classes is unbalanced [20].
Since non-clicked ads are more numerous than clicked ads we face an unbalanced learning task.
The uneven margin function pushes learning towards achieving a larger margin on the positive class.
The binary perceptron uses the sign function as a discriminant: F (x;  ) = Sgn(hx,  i) (1) We learn   from the training data.
The model has two adjustable parameters.
The  rst is the number of instances T to use in training, or the number of passes (epochs) over the training data.
The second concerns the uneven margin function, for which we de ne a constant  1 used in training to de ne a margin on the positive class.
While training, an error is made on a positive instance x if F (x;  )    1; the parameter on the negative class  0 = 0 and is e ectively ignored.
The learning rule is:  t+1 =  t + yixi (2) The ranking function de ned on the binary classi er is simply the inner product between the pattern and the weight vector: Sapm = hx,  i (3) In evaluation, we use this score to rank ads in each block.
Another way of modeling click feedback is directly as a ranking problem.
For this purpose we implement the ranking perceptron proposed by Shen and Joshi [30], which reduces the ranking problem to a binary classi cation problem.
The general objective is to exploit the pairwise preferences induced from the data by training on pairs of patterns, rather than independently on each pattern.
Let Rb be a set of pairs of patterns for a block b, such that (xi, xj)   Rb   r(yi) < r(yj), where r(yi) is the rank of xi in b; i.e., in our case, either yi = 1 and r(yi) = 1, or yi =  1 and r(yi) = 2.
Given a weight vector  , the score for a pattern x is again the inner product between the pattern and the weight vector: Srnk = hx,  i (4) However, the error function depends on pairwise scores.
In training, for each pair (xi, xj)   Rb, the score Srnk(xi  
 i.e., there exists an hyperplane which correctly classi es all data points.
Figure 3.
Examples of decision boundaries that can be learned by the linear, on the left, and nonlinear models, on the right, in a two-dimensional case.
X denotes positive (clicked) patterns while circles denote negative (non-clicked) patterns.
The linear classi er depicted on the left correctly classi es the positive examples, but misclassi es a negative one.
The nonlinear classi er can  nd complex decision boundaries to solve such non-linearly separable cases.
xj) is computed.
Given a margin function g and a positive learning margin   , if Srnk(xi   xj)   g(r(yi), r(yj))  , an update is made as follows:  t+1 =  t + (xi   xj )g(r(yi), r(yj))  (5) i   1 In particular, because the discriminant function is an inner product, Srnk(xi   xj) = Srnk(xi)   Srnk(xj).
By default we use g(i, j) = ( 1 j ) as a margin function as suggested in [30].
Although there are only two possible ranks in our setting, the hope is that training on pairs provides more information than training on patterns in isolation.
For regularization purposes, averaging is applied also to the ranking perceptron.
One possible drawback of the previous methods is that they are limited to learning linear solutions.
To improve the expressive power of our ranking functions, within the online perceptron approach, we experimented also with multilayer models.
The topology of multilayer perceptrons includes at least one nonlinear activation layer between the input and the output layers.
Multi-layer networks with sigmoid nonlinear layers can generate arbitrarily complex contiguous decision boundaries [8], as shown in Figure 3, and have been used successfully in several tasks, including learning to rank [3].
The multilayer perceptron is a fully connected three-layer network with the following structure:
 x0 = 1
 weight w0 = 1



 IRnH
 The score Smlp(x) of a pattern x is computed with a feed-forward pass: Smlp(x) = z = nH
 j=1  2 j wj +  2 0 = h 2, wi (6)
 netj = d
 i=1  1 ij xi +  1 0 = h 1 j , xi (7) The activation function f (.)
of the hidden unit is the sig-moid: f (net) =
 1 + exp a net .
(8) Supervised training begins with an untrained network with randomly initialized parameters.
Training is carried out with backpropagation [27].
An input pattern xi is selected, its score computed with a feedforward pass and compared to the true value yi.
Then the parameters are adjusted to bring the score closer to the actual value of the input pattern.
The error E on a pattern xi is the squared di erence between the guessed score Smlp(xi) and the actual value yi of xi, or for
 brevity (yi   si); thus E = 1 t,   is updated component-wise to  t+1 by taking a step in weight space which lowers the error function:  t+1 =  t +  t =  t    
  t (9) where   is the learning rate, which a ects the magnitude, or speed, of the changes in weight space.
The weight update for the hidden-to-output weights is:  2 i =  wi (10) where   = (yi zi).
The learning rule for the input-to-hidden weights is:  1 ij =  xjf  (netj) 1 ij  .
(11) where f   is the derivative of the nonlinear activation function.
A full comparison of the methods presented here with other methods on information retrieval tasks is beyond the scope of the paper.
However, to get an estimate for the accuracy of the methods we implemented, we evaluated the ranking perceptron (see Section 3), on the Letor dataset [21].
On all evaluation metrics the ranking perceptron achieves scores comparable to SVM on the OHSUMED and TD2003 datasets, and comparable to RankBoost on TD2004.
Thus in line with the best method in each dataset.
The mul-tilayer perceptron outperforms the ranking perceptron on exploratory runs, but we did not carry out extensive comparisons in this context.
A range of features, from simple word overlap and textual similarity features to statistical association between terms from the query and the ads, are used for learning a ranking model.
In particular, we are interested in  nding features which helps in the face of sparse data, as ads are characterized by small amounts of text.
We compute four features that assess the degree of overlap between the query and the ad materials.
The  rst feature has a value of one if all of the query terms are present in the ad: if ( t   q)t   a, F1 = 1, and 0 otherwise.
(12) The second feature has a value of one if some of the query terms are present in the ad: if  t   q such that t   a, F2 = 1, and 0 otherwise.
(13) The third feature has a value of one if none of the query terms are present in the ad: if  t   q such that t   a, F3 = 1, and 0 otherwise.
(14) The fourth feature is the percentage of the query terms that have an exact match in the ad materials.
Prior to computing the features, both the query and the ad were normalized for case.
For the purpose of word overlap, we chose to stem and stop less aggressively than we would do with functions that are smoothed.
For this reason we used a Krovetz stemmer [18], and only single characters were removed.
We compute the cosine similarity sim(q, a) between the query q and the ad a as follows: sim(q, a) = Pt q a wqtwat qPt q w2 qtqPt a w2 at (15) where the weight wt of a term in q or a corresponds to the tf   idf weight: wt = tf   log2
 nt + 0.5 (16) In Equation (16), tf is the frequency of a term in q or in a.
When considering queries q, tf is expected to be uniformly distributed with one being the most likely value, because terms are not likely to be repeated in queries.
In addition, N corresponds to the total number of available ads and nt corresponds to the number of ads in which term t occurs.
The tf   idf weight wat of term t in a is computed in the same way.
We have also computed the cosine similarity between q and each of the  elds of the ads, that is, the ad title at, the ad description ad and its bidded terms ab.
In all cases, we have applied Porter s stemming algorithm and we have removed stop words.
Cosine similarity has been used e ectively for ranking ads to place on Web pages in the setting of contextual advertising [25, 6].
A di erence with our setting is that in the case of contextual advertising, the cosine similarity is computed between the Web page and ad.
While there are more complex similarity functions that have been developed and applied for the case of computing the similarity between short snippets of text [28], we use cosine similarity because it is parameter free and inexpensive to compute.
Queries and ads are both short snippets of text, which tend not to have a high vocabulary overlap.
To address this issue, and exploit additional information from non-matching terms, we consider two features based on measuring the statistical association of terms from an external corpus.
NoKey SomeKey AllKey PercentKey Ad Title Description Bidterm AvePMI MaxPMI
 Word Overlap Features 1 if no query term is present in the ad materials; 0 otherwise 1 if at least one query term is present in the ad materials; 0 otherwise 1 if every query term is present in the ad materials; 0 otherwise The number of query terms present in the ad materials divided by the number of query terms Cosine Similarity Features The cosine similarity between the query and the ad materials (baseline) The cosine similarity between the query and the ad title The cosine similarity between the query and the ad description The cosine similarity between the query and the bidded terms Correlation Features The average pointwise mutual information between terms in the query and terms in the ad The maximum pointwise mutual information between terms in the query and terms in the ad Number of query-ad term pairs that have a  2 statistic in the top 5% of computed  2 values.
Table 1.
Summary of features.
The column  Abbrev.  provides an abbreviated name for one or more features, as they will be used in the experiments.
One measure of association between terms is pointwise mutual information (PMI).
We compute PMI between terms of a query q and the bidded terms of an ad a. PMI is based on co-occurrence information, which we obtain from a set of queries submitted to the Yahoo!
search engine: P M I(t1, t2) = log2 P (t1, t2) P (t1)P (t2) (17) where t1 is a term from q, and t2 is a bidded term from the ad a. P (t) is the probability that term t appears in the query log, and P (t1, t2) is the probability that terms t1 and t2 occur in the same query.
We form the pairs of t1 and t2 by extracting the query terms and the bidded terms of the ad.
We only consider pairs of terms consisting of distinct terms with at least one letter.
For each pair (q, a) we use two features: the average PMI and the maximum PMI, denoted by AvePMI and MaxPMI, respectively.
Another measure of association between terms is the  2 statistic, which is computed with respect to the occurrence in a query log of terms from a query, and the bidded terms of an ad:  2 = |L|(o11o22   o12o21)2 (o11 + o12)(o11 + o21)(o12 + o22)(o21 + o22) (18) where |L| is the number of queries in the query log, and oij are de ned in Table 2.
For example o11 stands for the number of queries in the log, which contain both terms t1 and t2.
Similarly, o12 stands for the number of queries in the log, in which term t2 occurs but term t1 does not.
We compute the  2 statistic for the same pairs of terms on which we compute the PMI features.
Then, for each query-ad pair, we count the number of term pairs that have a  2 higher than 95% of all the computed  2 values.
An overview of the features we use is shown in Table 1.
All feature values were normalized by column across the entire dataset with a z score, in order to have zero mean and unit standard deviation, thus each feature xi was standardized t1 o11 o21  t1 o12 o22 t2  t2 Table 2.
De nition of oij for the calculation of the  2 statistic in Equation 18.
Part Development size Test size














 Table 3.
The number of blocks in each of the development and test partitions of the data.
as: z = xi    i  i .
(19) In addition we augmented each data vector with a bias feature which has a value of one for every example, and serves as a prior on the response variable.
Before continuing, it is important to observe that all the features we described in this section can be computed e -ciently.
For example, the word overlap and cosine similarity features can be computed online, when a user query is received.
The correlation features can also be computed online e ciently by using a lookup table with word co-occurrence information for pairs of words.
We split the dataset into one training set, 5 development sets and 5 test sets, so that all the blocks for a given query are in the same set.
The exact number of blocks for each of the development and test sets is given in Table 3.
The training set consists of 109,560 blocks.
A ranking algorithm produces a score for each query-ad pair.
The ads are ranked according to this score.
Because of the way the data is constructed to account for the relative position of clicks, each block has only one click associated







 Classi cation Ranking Regression Prec at 1
 Prec at 1
 Prec at 1










































 Table 4.
The results for classi cation, ranking and regression, computed over all trials.
The best result is indicated in bold.
Results that are statistically signi cant with respect to the baseline are indicated with a star.
Results indicated with a dagger are statistically signi cant with respect to the features B + F + O.
The results for precision at one were not tested for statistical signi cance.
Classi cation Ranking Regression Feature set






 Prec at 1














 Prec at 1














 Prec at 1














 Table 5.
The average of  ve trials, for classi cation, ranking and regression.
The standard deviation refers to the 5 trials, not the standard deviation within a given trial.
The best result is shown in bold.
with it.
For this reason we evaluate the precision at rank one, which tells us how many clicked ads were placed in the  rst position by the ranker, and the mean reciprocal rank, which tells us the average rank of the clicked ad in the output of the ranker.
The mean reciprocal rank is computed as:

 n n
 i=1
 ranki (20) where ranki is the rank of the clicked ad according to the ranking function score and n is the total number of blocks.
The MRR score gives an indication of how far on average the ranker s guess is in the ranked list, thus providing a smoother measure than precision at one.
All adjustable parameters of the learning models were  xed on the development datasets.
The best values were selected by monitoring the average accuracy over the 5 development folds, the optimal values on development were used on the evaluation set.
We trained all models with a stochastic protocol, choosing a training instance at random without replacement: a block for the ranking case, a single pattern for the classi cation and multilayer models.
In the classi cation case we set the parameters T and   .
We tried three values for  1, 1, 10 and 100, and found 100 to give the best results.
As for the number of iterations, we found that all models (not only in classi cation) tended to converge quickly, rarely requiring more than 20 iterations to  nd the best results.
In the ranking model, in addition to the number of iterations T we optimize the positive learning margin   .
We obtained the best results with the value   = 1, which we used in all experiments with ranking perceptron.
The multilayer model has a number of adjustable parameters, some of the parameters were kept with default values; , the input-to-hidden weights were initialized ran-e.g., the sigmoid parameter a = 1.716 [8].
Following [8] we initialized the network weights for the hidden-to-output units uniformly at random in the interval   1 (nH) i <
 domly in the interval   1 (d) velopment data we found that hidden layers with 50 units, and   = 0.01, produced fast training and stable results, we kept these values  xed on all experiments involving the multilayer model.
The number of iterations was set on the development set, running a maximum of 50 iterations2.
.
On the de-<  2 <  2 ij < 1 (d)

 The baseline model has only one feature, the cosine similarity between the ad and the query with tf   idf weights.
Table 4 shows the results for classi cation, ranking, and mul-tilayer regression for each of the  ve test sets concatenated.
That is, for the  ve test folds evaluated as one dataset, in order to compute the signi cance of the mean reciprocal rank results.
For mean reciprocal rank we used a paired t-test.
Results indicated with a star are signi cant at the p < 0.05 level with respect to the baseline.
Most of the signi cant results are signi cant at the p < 0.01 level with respect to the baseline.
The precision at one results were not tested for statistical signi cance.
The signi cance for this metric is not computed because it is not appropriate for binary data.
We see that multilayer regression outperforms both clas-si cation and ranking, and further that the correlation features are a signi cant improvement over the other models.
For one third of the examples in the evaluation, the predictor correctly identi es that the  rst result was clicked, and an MRR of 0.60 indicates that on average the clicked result was between rank one and rank two.
on a desktop machine.
each model, this is to be expected because MRR can take values of 1, 0.5, 0.34, 0.25, 0.20, 0.18, etc.
An average MRR of 0.6 indicates that most clicked ads were at positions 1, 2 and 3.
If half of the results had an MRR of 1, and the other half had an MRR of 0.34, the mean would be 0.67, resulting in a standard deviation of 0.34.
Thus the high standard deviation in this case is an artifact of the metric.
We also compute the averages and standard deviation across the  ve test sets, shown in Table 5.
As indicated by the standard deviation for the trials, the method is robust to changes in the data set, even for precision at 1 which is in general a much less stable evaluation metric.
As already shown for content match [25, 19, 6] weighting the similarity of each component separately and adding features about the degree of overlapping between query and ad improve signi -cantly over the baseline.
The best result for each model are achieved adding the term correlation features.
Sponsored search click data is noisy, possibly more than search clicks.
People, and fraudulent software, might click on ads for reasons that have nothing to do with topical similarity or relevance.
While it is not obvious that we can learn to distinguish relevant from non-relevant ads based on a user click, we  nd that there is enough signal in the clicks that, with a simple method for unbiasing the rank of the click [13], it is possible to learn and carry out meaningful evaluation without the need for editorial judgments produced manually, or complex estimation of click-through rates.
Arguably, evaluating a classi er on the task of identifying the ad which will be clicked is more directly related to the task of successfully ranking ads then guessing indirectly the relevance assigned by humans.
However, since the two are strongly related it makes sense to investigate their interaction, in particular in learning.
An interesting question is if, in the presence of both types of information, it might be bene cial to solve both tasks at the same time.
This type of approach in fact  ts well the arti cial neural net learning methods [8] and would provide an interesting extension of the multilayer model.
The nonlinear multilayer perceptron outperforms both linear models.
Interestingly, both linear models when using also the correlation features seem to perform better when only use one (PMI or chi-squared) rather than both, (see Table 5).
This might depend on the fact that the features are strongly correlated and the linear classi er does not posses enough information to prefer one over the other in case of disagreements.
Thus it  nds a better solution by trusting always one over the other.
The nonlinear model instead has enough expressive power to capture subtler interactions between features and achieves the best results using both features.
Another interesting aspect is that, although there are only two possible rankings, and thus the problem boils down to a binary classi cation task, the linear ranking per-ceptron outperforms the simpler classi er.
The di erence seems to lie in the way training is performed, by considering pairwise of patterns.
Previous work on learning to rank [3] has proposed methods for training on pairs of patterns in multilayer architectures.
This is a natural direction for further investigation of this type of model.
In terms of the features, even the simple word overlap features produced statistically signi cant results over the baseline model.
Since we are e ectively re-ranking ad candidates retrieved by a retrieval system which we treat as a black box, candidates are biased by the initial ad placement algorithm, and it is possible that the initial retrieval system preferred ads with a high degree of lexical overlap with the query, and the word overlap features provided a  lter for those ads.
The correlation features, which capture related terms rather than matching terms, added a signi cant amount of discriminative information.
Such features are particularly promising because they are e ectively language-independent and knowledge free.
Similar statistics can be extracted from many resources simple to compile, or even generated by a search engine.
Overall these  ndings suggest both that relevant ads contain words related to the query, and that related terms can be captured e ciently with correlation measures such as pointwise mutual information and the chi-squared statistic.
There are several opportunities for further investigation of this type of features, for example by machine translation modeling [23].
One limitation of the current way of modeling click data is that  relevance  judgments induced by the logs are strictly binary.
We have seen that using pairwise information is useful in training and it may be desirable to generate more complex multi-valued feedback.
Joachims et al. [14] proposed manipulating the presentation of candidates to users by swapping the order of contiguous candidates to obtain likelihood ratios.
An interesting question is how such information might be automatically extracted from query logs.
Sponsored search can be thought of as a document retrieval problem, where the ads are the  documents  to be retrieved given a query.
As a retrieval problem, sponsored search is di cult because ad materials contain very few terms.
Because the language of the ads is so sparse, the vocabulary mismatch problem is even more critical.
Jones et al. [15] approach the problem of vocabulary mismatch by generating multiple rewrites of queries to incorporate related terms.
In their system, related terms are derived from user sessions in the query logs, where query rewrites have been identi ed.
The set of possible rewrites is constrained to contain only terms that are found in the database of advertising keywords.
They use a machine-learned ranking to determine the most relevant rewrite to match against the ads.
In a follow on to this work, Zhang et al. [35] use active learning to select the examples to use in training machine-learned ranking.
Both systems were evaluated on manual editorial judgments.
By contrast our system uses click data both for training and evaluating the system.
Furthermore, our models learn a ranking over the ads given a query directly, rather than learning a ranking over query rewrites.
Advertisements are represented in part by their keywords.
In one model of Online advertising, ads are matched to queries based on the keywords, and advertisers bid for the right to use the keywords to represent their product.
So a related task is keyword suggestion, which can be applied to sponsored search or to its sister technology, contextual advertising, which places an ad in a Web page based on the similarity between the ad and the Web page content.
Carrasco et al. [4] approach the problem of keyword suggestion by clustering bipartite advertiser-keyword graphs.
Yih et al. [33] extract keywords from Web pages using features such as the term frequency, the inverse document frequency,
 of using the keywords in an application.
Although we do not consider economic factors in our work, implicit in the task of ranking ads to improve the relevance at the top of the list, is the desire to increase user clicks.
Feng et al. [9] investigate the e ects of four di erent ranking mechanisms for advertisements in sponsored search on revenue.
Two of the mechanisms depend wholly on the bid price of the terms.
Of the remaining two, one proposes ranking ads by their expected clickthrough rate, and the other by a function of the bid price and the expected click-through rate.
The focus of this work is on the economic impact of the advertising placement mechanism, and uses a simulated environment which treats as a variable the degree of correlation between the advertiser s willingness to pay for advertising slots, and the amount of tra c they attract.
They  nd that ranking ads by a function of the bid price and the clickthrough rate outperforms other ranking mechanisms.
They further explore using the clickthrough rate to dynamically update the ranking mechanism.
They investigate two weighting schemes for user clicks.
In the unweighted scheme, an ad s clickscore is increased by 1 every time it is clicked.
In the weighted scheme the ad s clickscore is increased by  i where i is the rank of the clicked ad, which has the e ect of damping the scores of clicks in the top positions, and boosting the scores of ads in lower positions, which is one way of addressing the positional bias in user clicks.
There are several key di erences between this work and ours.
First, the evaluation metric in Feng et al.
is the expected revenue, whereas we evaluate the relevance of the ads.
The work by Feng et al.
is done in a simulated environment, whereas ours is performed on data from the search engine.
They use the clickthrough data as a measure of absolute relevance, rather than as a measure of relevance relative to other ads that were not clicked.
Finally, and most importantly, they are assessing the intrinsic relevance of an ad, independent of the context of the user s query.
Contextual advertising is a sister technology to sponsored search, and many of the techniques used to place ads in Web pages may be used to place ads in response to a user s query.
As with sponsored search, contextual advertising is usually a pay-per-click model, and the ad representations are similar in both sponsored search and contextual advertising.
The primary di erence is that rather than matching an ad to a query, the system matches the ad to a Web page.
Contextual advertising also su ers from the vocabulary mismatch problem.
To compensate for this, Ribeiro-Neto et al. [25] augment the representation of the target page using additional Web pages.
As a follow-on to this work, Lacerda et al. [19] select ranking functions using genetic programming, maximizing the average precision on the training data.
Broder et al. [2] use a semantic taxonomy to  nd topically similar terms, to facilitate the match between an ad and a Web page.
The current paper is a follow-on to work in contextual advertising, presented in [6] and [23].
Key di erences in the current work are the use of click data in place of human edited relevance judgments, both for learning a ranking function and for evaluation, the application to sponsored search rather than content match, and the use of several di erent types of classi ers.
Joachims [13] proposes that user clicks can be used to learn ranking functions for search engine results by considering that a user click is an indicator of relative relevance.
That is, a click at rank j indicates that the document at rank j is more relevant than unclicked documents that preceded it in the ranked list.
We adopt this ranking mechanism directly in our work, as described in Sections 2 and 5, however in addition to using it for training, we also use this ranking mechanism for evaluation.
Also, Joachims [13] is the  rst in a series of work investigating using implicit user feedback, such as user clicks, for learning ranking functions.
An overview of using implicit user feedback as surrogates for editorial relevance judgments can be found in [16].
In this paper we investigated an approach to learning and evaluating sponsored search ranking systems based exclusively on click-data and a simple conservative unbiasing method, which  ts together well with online learning protocols.
In particular, we focused on modeling the textual content which is a fundamental aspect of the ad selection problem.
We found empirically that our approach produces consistent results across di erent learning models, of varying complexity, and across di erent feature representations.
We found that learning on pairs of patterns is bene cial and that multilayer perceptrons provide a competitive platform for ranking from noisy data and compact feature representations.
We also showed how simple and e cient semantic correlation features provide a valuable source of discrimina-tive information in a complex task such as sponsored search, characterized by sparse textual descriptions.
Interesting avenues for further research might involve other ranking tasks where objects have small textual descriptions, such as image and multimedia retrieval, more sophisticated multi-layer ranking functions trained on pairwise preferences, and other variants of the simple unbiasing method used here.
