Learning to rank is a relatively new area of study in machine learning.
It aims to learn an assignment of scores to objects and rank the objects on the basis of the scores.
It has received much attention in recent years because of its important role in information retrieval.
Most research in learning to rank is conducted in the supervised fashion, in which a ranking function is learned from a given set of training instances.
The drawback with the supervised approach is that they tend to fail when the number of training instances is small.
In several real-world applications, in addition to the labeled training instances, a base ranker is available that can be used to rank the objects.
Then, the research question Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
is how to exploit the outputs from the base ranker when learning a ranking function from a small number of labeled instances.
We refer to this problem as Ranking Re ne-ment to distinguish it from supervised learning for ranking.
Below we show two examples for the application of ranking re nement: Relevance feedback In information retrieval, documents are often ordered by a prede ned relevance ranking function, such as BM25 [1] and Language Model for IR [2], that assesses the relevancy of documents to a given query.
Relevance feedback techniques are proposed to improve the retrieval accuracy by allowing users to provide relevance judgments for the  rst a few retrieved documents.
The research question here is how to enhance the accuracy of relevance feedback by combining the uses of the two types of information.
In this case, the base ranker is the relevance ranking function, and the training instances are the documents that are judged by the users.
Recommender system The goal of a recommender system is to rank the items according to the interest of an active user (i.e., the test user).
Usually, a few rated items are provided to indicate the preference of the active user.
Using the collaborative  ltering techniques [3], we can come up with a preliminary list of items ranked by using the preference information of other users.
The research question here is how to enhance the  nal ranking accuracy by leveraging the two types of information.
In this case, the base ranker is the collaborative  ltering algorithm, and the labeled instances are the items labeled by the active user.
Furthermore, any online learning of ranking functions can be viewed as a ranking re nement problem in that the ranking function is updated iteratively with new training instances collected on the  y.
A straightforward approach toward ranking re nement is to view the scores of the base ranker as an additional feature, and learn a ranking function over the augmented features.
As will be shown in the experiments, this is not the best approach for exploiting the information hidden in the base ranking function.
We believe that the most valuable information behind the base ranker is not its scores but the ranked list of objects it produces.
We therefore view the base ranker and the labeled instances as two complementary sources of information.
The key challenge in combining these two sources of information is that the ranked list
 the labeled instances tend to be noisy.
In this paper, we present a boosting algorithm for ranking re nement that can e ectively utilize the two types of information.
Our empirical study with relevance feedback and recommender system show that the proposed algorithm is e ective for ranking re nement, and it signi cantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature for the objects.
Most learning to rank algorithms are designed for the setting of supervised learning, in which a ranking function is learned from labeled instances.
The problem of learning to rank is often cast as a classi cation problem where the goal is to correctly classify the ordering relationship between any two instances.
Three well-known approaches in this category are Ranking-SVM [4, 5], RankBoost [6], and RankNet [7].
Ranking-SVM minimizes the number of incorrectly ordered pairs within the maximum margin framework.
Several variants [8, 9] are developed to further enhance the performance of Ranking-SVM.
RankBoost learns a ranking model based on the same consideration, but by means of Boosting.
RankNet [7] is a neural network based approach that uses cross entropy as its loss function.
Recently, Xu et al. [10] proposed another approach that is aimed at directly optimizing the performance measures in information retrieval, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG).
A special group of ranking problem is called ordinal regression [4], in which the output of the ranking function is restricted to a few ordinal categories.
Example algorithms for ordinal regression include the maximum margin based approach [11] and the Gaussian process based approach [12].
The ranking re nement problem di ers from the supervised ranking problem in that an imperfect base ranker is provided in addition to the labeled training instances.
The ranking problem is essential to information retrieval, whose goal is to rank a collection of documents by their relevance to a given query.
In particular, relevance feedback techniques [13] are developed to improve the accuracy of the existing retrieval algorithms.
There are two types of relevance feedback.
The  rst type, termed user relevance feedback, enhances the retrieval accuracy by collecting the user relevance judgments for the documents that are ranked on the top of the list.
As pointed out in the introduction section, the user relevance feedback problem can be treated as a problem of ranking re nement.
In the empirical study, we will show that the proposed algorithm for ranking re nement signi cantly outperforms the standard relevance feedback algorithm (i.e., the Rocchio algorithm) over several datasets.
The second type of relevance feedback, often termed pseudo relevance feedback, does not explicitly collect the user relevance judgments.
Instead, it treats the top ranked documents as relevant to the given query, and the documents ranked at the bottom as irrelevant.
These pseudo relevance judgments are used to improve the existing ranking function.
It is well known in information retrieval that pseudo relevance feedback may result in degradation of retrieval performance given the high probability of errors in pseudo relevance judgments [13].
This is similar to the noise of training instances in ranking re nement.
Let D = (x1, x2, .
.
.
, xn) denote the set of instances to be ordered, where each instance xi   Rd is a vector of d dimensions.
Let G : Rd   R denote the base ranking function (base ranker), and gi = G(xi) denote the ranking score assigned to xi by the base ranking function G. Instance xi is ranked before xj if gi > gj.
To make our problem general, we assume the label information collected from user feedback is presented as a set of ordered pairs, denoted by O = {(xik (cid:194) xjk )|k = 1, .
.
.
, m} where each pair xi (cid:194) xj
 indicates that instance xi is ranked before xj ranking re nement is to learn a ranking function F : Rd   R by exploiting both the labeled pairs in O and the ranking information given by G.
The  rst important question for ranking re nement is how to encode the ranking information provided by the base ranking function G. A straightforward approach is to use the ranking scores computed by G as an additional feature, and apply the existing algorithms, such as RankBoost and Ranking-SVM, to learn a ranking function from the labeled instances.
The drawback of this approach is twofold:   First, this approach only utilizes the ranking scores of the labeled instances.
The ranking information generated by the base ranking function for the unlabeled instances is completely ignored by this approach.
Since the number of labeled instances collected from users  feedbacks is considerably smaller than the number of unlabeled instances, this approach is not optimal in exploiting the information provided by the base ranking function.
  Second, we believe that the ranking orders generated by the base ranking function is substantially more reliable than the numerical values of the ranking scores.
Similar observation is found in the study of meta search whose goal is to combine the retrieval results of multiple search engines to create a better ranking list [14].
Empirical studies [14] showed that the meta search algorithms based on the document ranks often outperform the algorithms that directly use the relevance scores.
To address the above problems, we encode the order information generated by the base ranking function G with matrix W   [0, 1]n n.
Each Wi,j in the matrix represents the probability of ranking xi before xj and is de ned as follows Wi,j = exp( gi) exp( gi) + exp( gj) (1) In the above, Wi,j is de ned by a softmax function and the parameter     0 represents the con dence of the base ranking function.
To see the e ect of  , we consider two extreme cases:     = 0.
In this case, we have Wi,j = 0.5, which indicates that the ordering information generated by the base ranker is completely ignored.
ordered pairs while the converse is not true.
gi > gj 0.5 gi = gj
 gi < gj Wi,j = (2) Thus, W is almost a binary matrix, which implies that we completely trust ranked list generated by the base ranking function.
(cid:189) By varying the parameter  , we are able to alleviate the negative e ect from the base ranking function.
In our experiment, we set   to be inverse to the standard deviation of ranking scores of the  rst 10 retrieved documents.
set O with matrix T as follows: Similarly, we encode the ordering information inside the
  /2 otherwise Ti,j = (3) where parameter     [0, 1].
Ti,j represents the probability of ranking ranking xi before xj in the training data.
The parameter   re ects the error rate of training data, and is particularly useful when the labeled instances are derived from implicit user feedback that is usually noisier.
In our experiment, we set   = 1/2.
The goal of ranking re nement is to learn a ranking function F : Rd   R from matrix W and T that produces a more accurate ranked list than the base ranking function G. In particular, the optimal ranking function F should be consistent with the ranking information in W and T .
To this end, we measure the ranking errors of F with respect to both W and F , i.e., errw = errt = Wi,jI(Fj   Fi) Ti,jI(Fj   Fi) (4) (5) i,j=1 In the above, we introduce Fi = F (xi) and the indicator function I(x) that outputs 1 when the input boolean variable x is true and zero otherwise.
There are two problems with directly using the ranking errors errw and errt as the objective function:   First, both error functions are non-smooth functions since the indicator function I(x) is non-smooth.
It is well know that optimizing a non-smooth function is computationally more challenging than optimizing a smooth function because the derivative of a non-smooth function is not well de ned [15].
  Second, with two objectives at hand, the problem is essentially a multi-objective optimization problem [16].
Thus, another important question is how to combine multiple objectives into one single objective.
n(cid:88) n(cid:88) i,j=1     =  .
In this case, we have y).
The resulting new objective functions are: n(cid:88) n(cid:88) i,j=1 (cid:99)errw = (cid:99)errt = Wi,j exp(Fj   Fi) Ti,j exp(Fj   Fi) (6) (7) i,j=1 Note that since exp(x   y)   I(x   y), by minimizing the errors (cid:99)errw and (cid:99)errt, we are e ective in reducing the orig-using (cid:99)errw and (cid:99)errt comes from the theoretic result of Ad-inal ranking errors errw and errt.
Another advantage of Remark: It is interesting to examine the e ect of the aBoost [17], i.e., by minimizing the exponential loss function, the resulting classi er will not only reduce the training errors but also maximize the classi cation margin.
The enlarged classi cation margin is the key to guarantee a low generalization error for testing instances [17].
smoothing parameter   on the ranking error (cid:99)errt.
By substituting the expression (3) for Ti,j in (7), we have (cid:99)errt ex-(cid:99)errt = (1    ) n(cid:88) (xi(cid:194)xj ) O [exp(Fi   Fj) + exp(Fj   Fi)] exp(Fj   Fi) pressed as follows: (cid:88) +  
 i,j=1   (1    ) = (1    ) (cid:88)   (cid:88) (xi(cid:194)xj ) O (xi(cid:194)xj ) O n(cid:88) i,j=1 exp(Fj   Fi) +  
 (Fi   Fj)2 exp(Fj   Fi) +   2(1    ) (cid:107)F(cid:107)2
   (8) where (cid:107)F(cid:107)2 follows: S is a norm of vector F = (F1, .
.
.
, Fn) de ned as (cid:107)F(cid:107)2
 (cid:62) (nI   ee)F where I is the identity matrix and e is a vector of all ones.
The approximation of the second step in the above derivation follows the Taylor expansion of the exponential func-S/2(1    ), tion.
Clearly, the second term in (8), i.e.,  (cid:107)F(cid:107)2 is similar to the regularization term used by Support Vector Machines (SVM) [18].
Thus, the parameter   plays the role of coe cient in the regularized ranking error (cid:99)errt.
The problem of optimizing multiple objectives is usually called multi-objective optimization problem [16].
The most common approach is to linearly combine objectives, which in our case is to linearly combine the two error functions, i.e., La =  (cid:99)errw + (cid:99)errt = n(cid:88) i,j=1 ( Wi,j + Ti,j) exp(Fj   Fi) (9) In the following subsections, we will address these two questions separately.
To address the problem with non-smooth objective functions, we follow the idea of boosting by replacing the indicator function I(x   y) with an exponential function exp(x   where parameter   is used to combine two classi cation errors.
We refer to the approach based on the above objective function as  Linear Ranking Re nement , or LRR, for short.
The main problem with using the linearly combined objectives is how to decide an appropriate value for  .
In our experiments, we will show that di erent   could result in very di erent performance in information retrieval.
which makes combination of the two errors by their products, i.e., Lp = (cid:99)errw   (cid:99)errt (cid:195) n(cid:88) (cid:33)(cid:195) n(cid:88) = Ti,j exp(Fj   Fi) Wi,j exp(Fj   Fi) (10) (cid:33) Algorithm 1 Boosting algorithm for minimizing Lp
 set of labeled pairs
 3: repeat
 Compute  i,j for each instance pair as i,j=1 i,j=1  i,j = ai,j + bi,j (13) We refer to the approach as  Multiplicative Ranking Re nement , or MRR for short.
that is either The  rst concern on using the product is whether the resulting solution is Pareto e cient [16].
A solution F =
 n) (F1, .
.
.
Fn) is Pareto e cient for the objectives (cid:99)errw and (cid:99)errt if there does not exist any other solution F(cid:48) = (F (cid:48)

 In other words, if F is Pareto e cient, it guarantees that no solution is able to further reduce the two objectives simultaneously than F. It is well known that, according to multi-objective optimization theory [16], the solution found by minimizing La is guaranteed to be Pareto e cient.
Regarding the Pareto e ciency when minimizing Lp in (10), we have the following theorem: Theorem 1.
The optimal solution F = (F1, .
.
.
, Fn) found by minimizing the objective function Lp is Pareto e cient.
The proof of this theorem can be found in Appendix A.
The main advantage of using Lp rather than La is that it does not need a weight parameter.
This will be revealed in our empirical studies in that minimization of Lp usually signi cantly outperforms minimization of La even when the optimal combination weight   is used for La.
In order to compare the properties of the two di erent approaches for combination, we examine their  rst order derivatives.
Let   denote the parameters used by the ranking function F (x).
Then, the  rst order derivatives of La and Lp with respect to   are given as follows:  La = (Ti,j +  Wi,j) exp(Fj   Fi)( F (xj)    F (xi)) n(cid:88) (cid:195)(cid:88) i,j=1 Lp  Lp = (ai,j + bi,j) exp(Fj   Fi)( F (xj)    F (xi)) i,j=1 where ai,j = bi,j = (cid:80)n Wi,j exp(Fj   Fi) i,j=1 Wi,j exp(Fj   Fi) (cid:80)n Ti,j exp(Fj   Fi) i,j=1 Ti,j exp(Fj   Fi) (11) (12) Note that both derivative shares similar structures.
The key di erence between  La and  Lp is that in  Lp, ai,j and bi,j are used to weight the contribution from W and T for instance pair (xi, xj) when computing the derivative.
This is in contrast to  La where the weights for instance pair (xi, xj) are  Wi,j exp(Fj   Fi) and Ti,j exp(Fj   Fi).
The (cid:33) n(cid:88) j=1 n(cid:88) i=1 (cid:80)n (cid:80)n where ai,j and bi,j are de ned in (11) and (12).
Compute the weight for each instance as
 wi =  i,j    j,i (14)

 Train a classi er f (x) : Rd   {0, 1} that maximizes the following quantity   = |wi|f (xi)yi (15) (16)

 Predict fi for all instances in D Compute combination weight   as follows:   =

 log i,j=1  i,j (fi, 1) (fj, 0) i,j=1  i,j (fj, 1) (fi, 0) where fi = f (xi).
 (x, y) outputs 1 if x = y and zero otherwise.
Break the loop if     0
 F (x)   F (x) +  f (x) (17) 9: until reach the maximum number of iterations (cid:80)n (cid:80)n main advantage of using ai,j and bi,j comes from the fact i,j=1 bi,j = 1, that they are normalized, i.e., and therefore the contributions from W and T are naturally balanced when calculating the derivative.
i,j=1 ai,j = ment In this section, we will consider algorithms for learning the ranking function F (x) by respectively minimizing the objective function La and Lp.
The objective function La is similar to the objective function used by Rank-Boost except that a weight (Ti,j + Wi,j) is used for each instance pair.
We thus can simply modify the Rank-Boost algorithm to learn the optimal ranking function F (x).
Hence, in the sequel, we will focus on the boosting algorithm for minimizing Lp.
To learn the optimal ranking function F (x), we follow the greedy approach of boosting algorithms.
Since the information for training is a set of labeled instance pairs, a straightforward boosting approach is to iteratively update the weights of instance pairs and train a new ranking function for the given weighted pairs.
This is the strategy employed in the Rank-Boost algorithm [6].
However, since the number of instance pairs is O(n2), this approach could be computationally expensive when the number of instance n is large.
To address the above problem, we present a new boosting algorithm that converts the weights of instance pairs into weights for individual instances.
The key idea behind the new boosting algorithm is to derive an upper bound
 erations, denoted by LT p , is bounded as follows: Ti,j Wi,j exp   (  k      k)2 (18) (cid:33) (cid:195)   T(cid:88) (cid:33) (cid:195) n(cid:88) p  
 where i,j=1 n(cid:88) n(cid:88) n(cid:88) i,j=1 i,j=1  k =  k = k=1  k i,j (f k(xi), 1) (f k(xj), 0)  k i,j (f k(xi), 0) (f k(xj), 1) Figure 1: Reduction of the objective function Lp using the OHSUMED Data Set for the target objective that decouples functions for pairs of instances into functions for individual instances.
It is this decoupling that makes it possible to infer weights for individual instances from weights for instance pairs.
In addition, the new boosting algorithm is able to derive an appropriate binary class label for each instance using the computed weights.
Using both the weights and the class labels of instances, we can train a binary classi er f : Rd   {0, +1} and update the overall ranking function by F (cid:48)(x) = F (x)+ f (x) where   is the combination weight.
Note that by converting a ranking problem into a series of binary classi cation problems, the new boosting algorithm avoids the high computational cost arising from the large number of instance pairs.
Algorithm 1 summarizes the overall procedures for the proposed boosting algorithm minimizing Lp.
As the  rst step in the iteration, we compute  i,j for every pair of in-(cid:80)n stances that measures the uncertainty of ranking instance xi ahead of xj.
Next, we calculate the weight for instance xi as j=1  i,j    j,i.
It is important to note that wi can be wi = both positive and negative.
In particular, wi > 0 indicates that it is more likely to have xi ranked on the top of the ranked list than on the bottom of the list; wi   0 indicates the opposite.
Hence, we can derive the class label yi for xi based on the sign of wi: a positive class for placing instances on the top of the ranked list, and a negative class for placing instances on the bottom of the list.
Since |wi| indicates the overall con dence in deciding the ranking position of xi in the list, it is used to weight the importance of individual instances.
With this information, we will train a classi er that maximizes   in (15), which can be interpreted as a sort of classi cation accuracy.
Since most binary classi ers are unable to take weights into consideration, we will divide the training procedure into two steps: in the  rst step, we sample s instances according to the distribution that is proportional to the weights |wi|; we then train a binary classi er f : Rd   {0, +1} using the sampled instances.
We manually set s = max(20, n/5) in our empirical study.
A similar strategy is employed in the AdaBoost algorithm [6] and its e ectiveness has been veri ed in empirical studies.
In the remaining of this section, we will give justi cation to the proposed algorithm described in Table 1.
The main result is summarized in Theorem 2.
i,j=1 The above theorem essentially shows that by using the proposed algorithm, the objective function Lp will be reduced exponentially.
The key to proving Theorem 2 is to establish the relationship between the objective function Lp of two consecutive iterations.
This is because by upper bounding the log-ratio between Lp of two consecutive iterations, i.e., rt   log Lt p   log Lt 1 p we will have
 p = L0 p T(cid:89) t=1 Lt p Lt 1 p
 p exp , (cid:195) T(cid:88) (19) (cid:33) rt (20) t=1 For the convenience of presentation, in the following, we only consider two consecutive iterations without specifying the index of iteration.
Instead, we denote the quantities of the current iteration by symbol to di erentiate the quantities of the previous iteration.
In order to establish an upper bound for the log ratio, we  rst introduce the following lemma Lemma 1.
Assume  F (x) = F (x)+ f (x) where  F (x) and F (x) are the ranking functions of two consecutive iterations, respectively.
f : Rd   {0, 1} is a binary classi er and   is the combination weight.
We have the following inequality hold for any F , f , and  : log  Lp Lp    2 + (ai,j + bi,j) exp( (fj   fi)) (21) n(cid:88) i,j=1 where ai,j and bi,j are de ned (11) and (12), respectively.
The proof of Lemma 1 can be found in Appendix B. Using the Lemma 1, we present the proof of Theorem 2 in Appendix C.
Finally, we can show the relationship between the objective function Lp and the quantity   (in (15)) that is used to guide the training of binary classi ers in iterations.
This result is summarized in the following theorem: Theorem 3.
Let  k denote the value of the quantity   (in (15)) that is maximized by the binary classi er f k(x) learned in the kth iteration.
Assume that  k   0 for each iteration.
Then, the objective function after T iterations, denoted by LT p , is bounded as follows: (cid:195) n(cid:88) (cid:33)(cid:195) n(cid:88) (cid:33) (cid:33) (cid:195)   T(cid:88) Ti,j Wi,j exp  k (22) i,j=1 i,j=1 k=1 p  
 Theorem 2.
Let f k(x) denote the binary classi cation i,j denote  i,j function obtained in the kth iteration, and  k The proof of the above theorem can be found in Appendix D.
Theorem 3 provides a theoretical justi cation for Algorithm 1.
duces the objective function Lp.
This is further con rmed by our empirical study.
Figure 1 shows an example of reduction in the objective function Lp.
We clearly see that the objective function is reduced exponentially and receives the largest reduction during the  rst few iterations.
In this section, we evaluate the proposed algorithm for ranking re nement by two tasks, i.e., user relevance feedback and recommender system.
The objectives of our experiments are: (1) to compare the proposed algorithm for ranking re nement to the existing ranking algorithms, (2) to examine the performance of the proposed algorithm for ranking re nement with di erent numbers of training instances, (3) to examine the e ect of di erent base rankers on the performance of the proposed algorithm, and (4) to examine the time e ciency of the proposed algorithm for ranking re nement.
For the Relevance Feedback experiment, we used the LETOR testbed [20] that includes the OHSUMED dataset and the datasets from TREC 2003 and 2004.
The OHSUMED dataset consists of 106 queries.
For each query, a number of documents are retrieved and their relevance to the query is given at three levels: de nitely (2), possibly (1), or irrelevant (0).
There are a total of 16, 140 query-document relevance judgments.
For each query-document pair, a total of 25 ranking features are extracted.
There are 50 queries in the dataset of TREC 2003, and 75 queries in TREC
 which amounts to a total of 49, 171 query-document pairs for TREC 2003 and 74, 170 query-document pairs for TREC
 query-document pair.
There are 44 features extracted for each query-document pair.
The detailed information about OHSUMED and TREC data sets are available in [20].
For the Recommender System experiment, we used the MovieLens dataset, available at [19], contains 100, 000 ratings (from 1 to 5) for 1682 movies given by 943 users.
Each movie is represented by 51 binary features: 19 features are derived from the genres of movies and the rest 32 features are derived from the keywords that are used to describe the content of movies2.
To examine the e ectiveness of the proposed algorithm for ranking re nement, we compared the following ranking algorithms: Base Ranker: It is the base ranker used in the ranking re nement.
Rocchio: This algorithm extends the standard Rocchio algorithm for user relevance feedback and it creates a new query vector by linearly combining the query vector and vectors of feedback documents.
Given the initial query Q0, the relevant documents (R1, R2, ..., Rn1 )
 movie database IMBD.
The 32 most popular keywords used by the 1682 movies were selected.
and non-relevant documents (S1, S2, ..., Sn2 ), the new query according to Rocchio is: n1(cid:88) i=1 n2(cid:88) i=1 Q = Q0 +       Ri n1 Si n2 (23) Note, in our case, that each document is not represented by a vector of word frequency, but a vector of features that are computed based on its match to the query.
Hence, we don t have Q0, i.e., the representation vector for query itself.
We therefore set Q0 to be a vector of all zeros.
We used the inner product between the new query and documents as the scores to rank the documents.
To obtain the best performance, we vary   and   from 1 to 10 and choose the best setting.
SVM: This implements the Ranking-SVM algorithm using the SVM light package.
Note that it is commonly believed that Rank-Boost performs equally well as Ranking SVM.
The experimental results provided in the LETOR collection also con rm this.
Hence, we only compare the proposal algorithm with Ranking-SVM, but not Rank-Boost.
MRR: This is the Multiplicative Ranking Re nement algorithm that minimizes Lp in (10).
LRR: This is the Linear Ranking Re nement algorithm that minimizes La in (9).
Since the performance of LRR depends on the parameter  , we run LRR with 100 di erent values from 0.1 to +10 and choose the best and worst performance.
We referred them to as LRR-Worst and LRR-Best, respectively.
For a fair comparison, the output from the base ranker is used as an extra feature when using SVM (i.e., Ranking-SVM) and Rocchio.
To evaluate the performance of di erent algorithms, we used precision and normalized discounted cumulative gain (NDCG) at rank position k. Let (dR1 , dR2 , ..., dRn ) denote the top ranked documents according to the ranker R, and (rR1 , rR2 , .., rRn ) denote their binary judgments.
The precision at rank position k measures the relevancy of the  rst k documents and is de ned as follows k(cid:88) PR@k = rRi /k i=1 For the OHSUMED dataset, a document is deemed relevant when its score is two.
In the case of MovieLense data, a movie is deemed to be interesting to a test user when its rating is no less than 43.
Since the  rst top documents are more important than the other documents, we also employ the NDCG metric [21] that is de ned as follows: N DCGR@k = DCGR@k DCGT @k
 lar results in our empirical study.
We did not present results for the other thresholds due to the space limitation.
 ned as follows [21]: (cid:40) (cid:80)k rX1 DCGX @k = rX1 + if k = 1 if k > 1 rXi log2 i i=2
 Unless speci ed, for all the experiments with relevance feedback, we used the standard BM25 retrieval algorithm (i.e., the 21st feature in OHSUMED data set and 16th in TREC data sets) as the base ranker.
We followed the common practice of user relevance feedback by collecting the relevance judgments for the  rst 10 retrieved documents.
These user relevance judgments served as labeled instances in ranking re nement.
For the experiment with recommender system, the base ranker was created by applying a collaborative  ltering algorithm, more speci cally, the Personality Diagnosis algorithm [3], to the user rating data.
In particular, 20 users were randomly selected as the training users, and the remaining 923 users were used for testing.
For each test user, 10 rated movies were randomly selected and were used by the collaborative  ltering algorithm to identify the 20 training users who share the common interests with the test user.
Note that we did not compare the proposed algorithm to other information  ltering algorithms because the focus of this study is to examine the e ectiveness and the generality of the proposed approach for ranking re nement.
Figure 2 and 3 show the performance results of di erent algorithms in terms of precision and NDCG for the  rst 25 ranked documents.
First, by comparing the performance of the two variants of ranking re nement, we observed that the Multiplicative Ranking Re nement (MRR) algorithm is signi cantly more e ective than the Linear Ranking Re ne-ment (LLR) algorithm.
Indeed, MRR performs signi cantly better than the best case of LRR (i.e., LRR-best).
The key di erence between MRR and LRR is that MRR minimizes the product of the two error functions while LRR minimizes the weighted sum.
We believe it is the normalization scheme brought by MRR (see equations in (11) and (12)) that makes it performing better than LRR.
Second, comparing to the other three baseline algorithms, i.e., the base ranker, Rocchio, Ranking-SVM, we observed that MRR always signi cantly outperforms the baseline algorithms in all the cases.
More noticeable is the improvement made by the ranking re nement over the  rst a few ranking positions.
We thus conclude that Multiplicative Ranking Re nement is more e ective than the baseline algorithms for user relevance feedback in information retrieval.
To examine the robustness of the proposed algorithm with respect to the imperfectness of the base ranker, we tested the MRR algorithms with three di erent base rankers.
We plotted the results of all di erent features and selected three features which cover a good range of ranking quality.
The chosen base rankers for OHSUMED data set are the features

 performs using di erent base rankers (NDCG shows similar results).
The result indicates that the quality of base rankers has a direct impact on the performance of the MRR algorithm.
We also observed that the proposed algorithm is able to signi cantly improve the performance even with a poor base ranker.
More impressively, by comparing Figure 4 to Figure 2, we observed that even using the worst base ranker (i.e., feature 7 for OHSUMED, 21 for TREC 2003 ,and 36 for TREC 2004), the retrieval accuracy of MRR is comparable to the other methods using the best base ranker (i.e., the BM25 retrieval algorithm).
We thus conclude that the MRR algorithm is resilient to the imperfectness of base rankers.
To investigate the e ect of the number of feedback documents on the performance, we ran the MRR algorithm by varying the number of feedback documents from 5 to 20.
Figure 5 shows the result using varied number of feedback documents.
We clearly observed that the number of feedback documents have a direct e ect on the performance of ranking re nement.
However, even with a small amount of feedback, MRR is able to improve the retrieval performance considerably, particularly for the accuracy of the  rst few ranked documents.
We thus conclude that the proposed algorithm for ranking re nement is robust to the size of feedback data.
We evaluated the generality of the proposed algorithm by applying it to recommender system (movie recommendation).
Figure 6(a) and Figure 6(b) show the results of di er-ent algorithms when applied on the MovieLens dataset.
It is surprising to observe that the results of LRR, the linear ranking re nement algorithm, even with the tuned parameter  , is not even comparable to the the performance of the base ranker.
In contrast, the MRR algorithm is able to signi cantly improve the accuracy of the base ranker and outperform the other baseline algorithms considerably.
This result further indicates the importance of appropriately combining the two information sources, i.e., the ranking information behind the base ranker and the feedback information provided by users.
Figure 6(c) shows the sensitivity of MRR to the size of feedback data by varying the number of rated movies by the test user from 5 to 20.
Similar to the result for relevance feedback, we observed that the size of feedback data a ects the performance of MRR considerably.
However, even with
 ticeable improvement in the prediction accuracy compared to the base ranker.
This result further con rms the robustness of the proposed algorithm for ranking re nement with respect to the size of feedback data.
Figure 7 shows the e ciency of the MRR algorithm in terms of the running time for di erent numbers of rated movies for each test user.
We chose movies data set for the experiment because it provides a good range for the number of objects.
We partitioned the test users into groups where each group of users has a di erent number of rated movies.
The running time of MRR for each group is calculated by averaging it across all the users in the group.
As pointed in Section 3.4 and seen in Figure 7, the running time is linear in the number of instances.
Note that the relatively long running time is due to the MATLAB implementation.
(b) TREC 2003 (c) TREC 2004 Figure 2: Precision of relevance feedback for di erent algorithms (a) OHSUMED (b) TREC 2003 (c) TREC 2004 Figure 3: NDCG of relevance feedback for di erent algorithms of the proposed algorithm.
