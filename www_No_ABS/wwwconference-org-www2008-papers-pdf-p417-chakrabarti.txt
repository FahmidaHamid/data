Web advertising provides  nancial support for a large portion of today s Internet ecosystem, catering to a diverse set of sites like blogs, news, reviews etc.
Spurred by the tremendous growth in tra c in terms of volume, number of Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
users, user engagement, content diversity, the last few years have seen a tremendous growth in spending on web advertising.
According to eMarketer [9] the total internet advertiser spending in 2007 will reach almost 20 billion US dollars.
This establishes the Web as one of the top 3 advertisement mediums, along with TV and print media.
A major part of the advertising on the web falls into the category of textual ads: short textual messages usually marked as  sponsored links  or similar.
There are two main types of textual ads on the web today:
 ads on the result pages from a web search engine based on the search query.
All major current web search engines support such ads and act simultaneously as a search engine and an ad agency.
tising places ads within the content of a generic, third-party web page.
There usually is a commercial intermediary, called an ad-network, in charge of optimizing the ad selection with the twin goal of increasing revenue (shared between publisher and ad-network) and improving user experience.
Here also the main players are the major search engines; however, there are also many smaller players.
While the methods proposed in this paper could be adapted for both SS and CM advertising, we will focus in our analysis and experiments on the CM scenario.
Studies have shown that displaying ads that are closely related to the content of the page1 provide a better user experience and increase the probability of clicks [4, 21].
This intuition is analogous to that in conventional publishing, where there are very successful magazines (e.g., Vogue) where a majority of the content is topical advertising (fashion, in the case of Vogue).
Hence, estimating the relevance of an ad to a page is critical in serving ads at run-time.
Previously published approaches estimated the ad relevance based on co-occurrence of the same words or phrases within the ad and within the page (see [18, 13, 3] and the related work section for more details).
The model used in this body of work is to translate the ad search into a similarity search in a vector space.
Each ad is represented as a vector of features, as for example, unigrams, phrases and classes [3].
The page is also translated to a vector in the
 loss of generality, we focus on contextual features.
translated into  nding the ad vectors that are closest to the page vector.
To make the search e cient and scalable to hundreds of millions of ads and billions of requests per day, we can use an inverted index and an e cient similarity search algorithm as the one reported in [2].
A drawback of this method is that it relies on a-priori information and does not use the feedback (a posteriori) information that is collected in the form of ad impressions (displays) and clicks.
Another line of work uses click data to produce a CTR estimate for an ad, independent of the page (or query, in the Sponsored Search scenario [19, 17]).
The CTR is estimated based on features extracted from the ads that are then used in a learning framework to build models for estimation of the CTR of unseen ads.
In this approach the assumption is that the ads are selected by a deterministic method - by matching the bid phrase to a phrase from the page (or the query in Sponsored Search) and therefore to select the most clickable ads we only need to estimate the CTR on the ads with the matching bid phrase.
This simplifying assumption of the matching process is an obvious drawback of these approaches.
Another drawback is that these methods do not account for di erential click probabilities on di erent pages: If some pages in the corpus attract an audience that clicks on ads signi cantly more than average, then the learning of feature weights for ads will be biased towards ads that were (only by circumstance) shown on such pages.
Contributions.
In this work we combine the two di erent lines of work and propose a relevance based approach that is augmented to use the the click data to produce a CTR estimate.
We adapt the vector space similarity formulation and keep the format of the scoring formulas so that they are still suitable for inverted index evaluation that has low cost and high scalability.
To incorporate the feedback, we modify the scoring formulas with correction parameters that are learned from the click data.
We examine several scoring formulas in order of increasing complexity and propose learning the parameters by statistical learning methods.
We overcome the problem of learning from data with low click rates by a sampling method that equalizes the sizes of clicked and non-clicked portions of the data, and then solves the problem of the (page, ad) sparsity by explaining click propensities (scores) of ads on pages in terms of the words that occur in both.
In summary our main contributions are:   We propose a set of intuitive models of click propensities in terms of the words that occur in the webpage and ad.
The basic model takes into consideration the location of the words (title, main body, sidebar, etc.
), while the most complex model also considers word synonyms, the tf-idf values of the words and relevance measures, among others.
  The models were carefully chosen to extend scoring formulas in current use so that they could be easily implemented on top of existing systems.
Thus, they can be immediately used for e cient ad selection from a very large corpus of ads.
  We describe a fast method for  tting the parameters of these models, and prescriptions for picking the right model given the dataset size and runtime execution constraints.
  Extensive experiments on real-world datasets convincingly demonstrate the accuracy of our models.
Online advertising is an upcoming area of research and the published literature is sparse.
A study presented in [21] con rms the intuition that ads need to be relevant to the user s interest to avoid degrading the user s experience and increase the probability of reaction.
IR methods used in web search engines are one of the most prominent methods to ensure relevance of search results.
A pioneering report by Ribeiro-Neto et.
al on contextual matching [18] examines a number of strategies to match pages to ads based on extracted keywords.
Here also the ads and pages are represented as vectors in a vector space.
The work  rst presents  ve strategies that use cosine similarity to match page and the ad vectors.
The authors explore using di erent ad sections (bid phrase, title, body) as a basis for the ad vector.
The winning strategy out of the  rst  ve requires the bid phrase to appear on the page and then ranks all such ads by the cosine of the union of all the ad sections and the page vectors.
While both pages and ads are mapped to the same space, there is a discrepancy (impedance mismatch) between the vocabulary used in the ads and in the pages.
Furthermore, since in the vector model the dimensions are determined by the number of unique words, plain cosine similarity will not take into account synonyms.
To solve this problem, Ribeiro-Neto et al expand the page vocabulary with terms from other similar pages weighted based on the overall similarity of the origin page to the matched page, and show improved matching precision.
A followup work [13] proposes a method to learn impact of individual features using genetic programming to produce a matching function.
represented as a tree.
Arithmetic operators and the log function are internal nodes while di erent numerical features of the query and ad terms can be leafs of the function tree.
The results show that genetic programming  nds matching functions that signi cantly improve the matching compared to the best method (without page side expansion) reported in [18].
In our previous work [3] we expand on the search of the ads in a vector space by adding classes as additional dimensions.
This allows for having the ads topically match the content of the page.
We used tf-idf for weighting the unigram and phrase features and class con dence score returned by the classi er as weight of the class features.
While the class weights are trained over a labeled set of ads, there is no direct use of the click data to improve on the ad selection.
All of these approaches share the use of similarity in a feature space to relate ads to pages.
However none of these approaches uses statistical learning techniques to factor the implicit relevance feedback in a form of click data to learn how to better match pages to ads.
Another approach to contextual advertising is to reduce it to the problem of sponsored search advertising by extracting phrases from the page and matching them with the bid phrase of the ads.
In [22] a system for phrase extraction is described that used a variety of features to determine the importance of page phrases for advertising purposes.
The system is trained with pages that have been hand annotated with important phrases.
The learning algorithm takes into account features based on tf-idf, HTML meta data and
 evaluation, each page phrase up to length 5 is considered as potential result and evaluated against a trained classi er.
In our work we also use phrases as feature and we learn parameters to determine the relative importance of a particular phrase compared to other phrases and unigrams.
Another key di erence related to this work is that we use multiple features to select the ads.
Another line of research attempts to predict the click through rate of ads using similar tools (clustering, keyword matching and classi cation) for search advertising [17].
In this work, the ads are clustered by their bid phrases.
The click through rate is averaged over each cluster.
The CTR estimate for new ads is obtained by  nding the nearest cluster and assuming that cluster s CTR.
Results show that this improves over naive global CTR priors and CTR based on the bid phrase deciles.
Our proposed method to match relevant ads to pages is based on logistic regression, a popular technique in statistics and machine learning [14].
The regression enables us to combine click feedback and semantic information available from both pages and ads to determine relevancy.
This is more general than a pure relevance based approach that does not use click feedback in any form.
Indeed, our experiments convincingly demonstrate the usefulness of using click feedback to  nd more relevant ads.
There has been recent work on using regression models for determining relevant ads [19].
While it has the same  avor as our work, only ad-speci c features are learnt, which is only a subset of the features we consider.
In particular, in addition to page and ad speci c features, we learn features that capture interactions between pages and ads.
Furthermore, we combine word based features with traditional relevance measures to enhance matching relevant ads to pages.
Our models are more granular and can incorporate larger number of features, which reduces bias in CTR estimates and leads to better performance.
However, reduced bias comes at the price of increased variance, which can become a serious problem if the models become too granular and start over tting the training data.
To balance these two issues, we use a two-pronged strategy.
First, we use a relatively large but specially selected set of features, where the selection mechanism ensures that the features have reasonable support.
We also provide a mechanism based on prior probabilities to down-weight features that are too sparse.
The second strategy we use to prevent over tting is to train our models on an extremely large corpus (billions of records, several thousand features) which automatically increases the support of a large number of features.
Fortunately, data is plentiful especially for big ad-networks that serve a large number of publishers and advertisers.
However, increased training size poses a di cult computational challenge of scaling logistic regression to web scale data.
We overcome this by using an approximation based on a  divide and conquer strategy , i.e., we randomly split our training corpus into several pieces and  t a separate logistic regression to each piece.
The  nal result is obtained by combining estimates from all the pieces.
Our computation is carried out in the software framework called MapReduce[12] that supports large scale parallel computations using a cluster of commodity personal computers.
Roughly speaking, our method consists of three broad steps: (a) Feature extraction, (b) Feature selection, and (c) Coe cient estimation for features through a logistic regression.
We provide a detailed description of each below.
Pages and ads are treated as being composed of several regions.
For instance, a page is composed of page title, page metadata, page body, page URL etc.
Similarly, an ad is composed of ad title, ad body etc.
Within each region, we extract a set of words/phrases after stop word removal.
We associate a score (e.g.
region speci c tf, tf-idf) to each word that measures its importance in a given region.
For a given (page, ad) region combination, our model has three sets of features described below.
Page region speci c main e ects.
Web pages are usually composed of multiple regions with di erent visibility and prominence.
The impact of each region on the ad selection can thus vary.
We follow the intuition of our previous work [3] and we learn the e ect of each region separately.
For a word w in page region p(r) with score tp(r)w, the region-speci c main e ect is de ned as Mp(r)w = 1(w   p(r))   tp(r)w, that is, if the word is present in the page region p(r), the feature contributes its score else it does not contribute.
These features provide an estimate of word popularity.
They are not useful at the time of selecting relevant ads for a given page but help in getting better estimates of other terms in the model after adjusting for the e ect of popular words on a page.
For instance, if  camera  pages are popular in terms of click-through rates and 90% of our corpus consists of camera pages,  camera  ads that were the ones mostly shown on camera pages would tend to become popular even on  soccer  pages which constitute only 1% of the total corpus.
By incorporating page words in the model, we adjust for this e ect and get the correct matching ads for  soccer  pages.
Ad region speci c main e ects.
Ads are also composed of multiple regions, some visible to the user (title, abstract) and some used only in the ad selection (bid phrase, targeting attributes).
As with the page regions, the ad regions can have di erent impact on the ad selection.
For a word w in ad region a(r) with score ta(r)w, this is de ned as Ma(r)w = 1(w   a(r))   ta(r)w.
Unlike page speci c main e ects, it does play an important role when selecting relevant ads for a given page and provides more weight to popular ads.
Interaction e ects between page and ad regions.
For a word w1 in page region p(r1) and word w2 in ad region a(r2) with score f (tp(r1)w1 , ta(r2)w2 ) for some function f , this is given as Ip(r1)w1,a(r2)w2 = 1(w1   p(r1), w2   a(r2)) (1)   f (tp(r1)w1 , ta(r2)w2 ).
In this paper, we con ne ourselves to the case where w1 = w2 (i.e., the feature  res  only if the same word occurs in both the corresponding page and ad regions), but in general, one can consider co-occurrences of synonyms or related words.
Examples of f include the product function
 p tp(r1)w1   ta(r2)w2 and so on.
Interaction e ects are important components of our method and help in matching relevant ads to a given page.
For instance, occurrence of the word  camera  in the ad body is a strong indication of the ad being relevant for the page whose title contains the word  camera,  with the degree of relevance being determined by the regression.
For any given (page, ad) region combination, a large number of words occur in the training data.
Using them all as features might make the logistic regression ill-conditioned and in ate variance of the coe cient estimates.
Hence we take recourse to variable selection techniques which select a subset of important words to be used in our regression.
Variable selection in the context of regression is a well studied area with a rich literature.
Stepwise backward-forward automated variable selection algorithms are widely used for large scale applications but these methods have drawbacks, especially when features are correlated [8].
The general recommendation is to use as much domain knowledge as possible instead of using an automated procedure to select relevant variables.
However, in large scale settings as ours, some level of automation is necessary.
For reasons of scalability, we explore a two-stage approach.
In the  rst stage, we conservatively prune non-informative features using simple measures that can be computed using only a few passes over the training corpus.
In the second stage, we  t a regression to all the selected features from the  rst stage but down-weight them through a specially constructed prior that pools data from all the features, while putting more coe cient on those that are less sparse.
The latter approach will be described in more detail in the next subsection; we discuss our variable selection methods next.
We selected the variables using two methods, the  rst based on clicks and views, and the second based on relevance scores of words that are independent of any click feedback.
In the  rst approach (data-based), we rank words based on a measure that quanti es the interaction between words occurring in the page and ad regions.
For a word w, the interaction measure is de ned as iw = CT Rpage w CT Rboth w   CT Rad w (2) w where CT Rboth denotes the click-through rate (CTR) when w occurred both on page region and ad region of an ad displayed on a page, and CT Rpage w denote the marginal CTRs when w shown on the page and ad regions respectively.
Higher values of the ratio indicates stronger interaction being induced by the presence of the word which in turn should enhance the matching quality of ads to pages.
We also tried a variation of the measure above with a square root of the denominator with no signi cant impact.
and CT Rad w In the second approach (relevance-based), words are ranked by computing the average tf-idf scores across the entire page and ad corpus for the respective regions under consideration.
Here, we experiment with two measures: (a) create a single corpus by treating page and ad regions as documents and compute a single tf-idf average score for each word, and (b) Treat the page and ad regions as di erent corpora and use the geometric mean of tf-idf scores computed separately from page and ad regions for each word.
For both measures, we picked the top 1000 words and used Figure 1: Distribution of (page, ad) impressions: Plots are on log-log scale but ticks are on the original scale.
them in the logistic regression.
To avoid noisy estimates of CTRs in the ratio, we only consider words that were shown simultaneously on ad and page regions at least 10 times and had nonzero marginal probabilities.
As our experiments will show later, the data-based approach gives better results for the same number of words.
ij (1 pij)1 yij .
To Let yij denote the binary click outcome (1 for click, 0 for no click) when ad j is shown on page i.
We assume yij has a Bernoulli distribution with CTR pij , i.e., the probability distribution of yij is given by P (yij) = pyij determine relevant ads for a given page i, we need to estimate pij s, with higher values indicating more relevant ads.
For ads that are shown a large number of times on a page, the CTR can be estimated empirically by clicks per impression.
However, in our application a large fraction of page-ad pairs having a small number of impressions (Figure 1).
In fact, since the CTRs are typically low (0.1%   20% with a substantial right skewness in the distribution), number of impressions required to get precise empirical estimates are high.
For instance, to estimate a 5% CTR, we need 1K impressions to be even 85% con dent that our estimate is within 1% of the true CTR.
Thus, we take recourse to feature based models, i.e., pij is a function of features extracted from page and ad regions as discussed in section 3.1.
To allow for arbitrary real-valued coe cients for features, it is routine to map pij onto the real line via a monotonically increasing function.
The most widely used function is the logit which maps pij to logit(pij) = log [pij/(1   pij )].
We assume that logit(pij) is a linear function of features representing the main e ects and interaction e ects discussed in section 3.1.
For simplicity, consider a single (page, ad) region combination (p(r1), a(r2)).
The linear function in the logistic regression is given by:
 logit(pij) = logit(qij) +
  wMp(r1)w
 w (3) +  wMa(r2)w +  w,r1,r2 Ip(r1)w,a(r2)w w w where w = ( ,  ,  ) are unknown feature coe cients to be estimated by logistic regression, and logit(qij ) are known prior log-odds that could have been derived from a di erent
 where  p is the average CTR on the entire training corpus.
Another possibility we explore is to derive prior log-odds qij by combining relevance scores with click feedback.
To add new (page,ad) region combination, we only need to augment equation 3 with the appropriate linear terms for the page main and ad main e ects.
For the interaction e ects, we re-parametrize our model to facilitate indexing.
We explain our re-parametrization here and discuss the connection to indexing later in section 4.
For each (page,ad) combination (r1, r2), a word w that occurs in both r1 and r2 has a coe cient  w,r1 ,r2 which depends on the word, the page region and the ad region.
We assume the following parametrization.
 w,r1,r2 =  w    p(r1)    a(r2) (4) i.e., the interaction of a word for a given page and ad region combination is factored into word-speci c, page-speci c and ad-speci c components.
Thus, for M words, R1 page regions, R2 ad regions, the number of parameters equals M + R1 + R2 as opposed to M   R1   R2 in the original model.
The estimate of coe cients is obtained by maximizing the log-likelihood of the data as given by (yijlog(pij) + (1   yij )log(1   pij)) (5)
 ij where pij is given by equation 3.
The optimization problem described above may become ill-conditioned and lead to high variance estimates if features tend to be correlated or are sparse or both.
For exact technical details on necessary and su cient conditions that ensure convergence, we refer the reader to [15].
This is a drawback in our scenario where feature sparsity and correlations are routine.
To provide a robust solution, we put additional constraints on the coe cients in the form of priors.
A N (0,  2) prior would mean that the parameter estimates are pinned down in the range ( 3 , 3 ) with 99% probability a-priori.
In the absence of enough information about the coe cient from data, this ensures that the coe cient estimates do not diverge to the boundaries and cause numerical instability.
To put more stringent constraints on sparse features, we down-weight the prior variance  2 by a measure of relative sparsity which we de ne to be the variance of the feature occurrence process relative to average feature occurrence variance.
The feature occurrence variance is given by s(1   s), where s is the fraction of times the feature occurs.
In particular, we assume:        w   N  w   N  w   N       2   sp(w)(1   sp(w)) 2   sa(w)(1   sa(w))
 sp(1   sp) sa(1   sa) sI (1   sI ) 0,   0,   0,   Note that separate averages are used for the main page and ad e ects, and interaction e ects (indicated by the subscripts p, a, and I).
In all our experiments, we  x  2 = 9; experiments with several other values in the range of 3 to 20 did not yield much di erence.
Now, the optimization problem reduces to estimating the coe cients by maximizing the log-posterior which is the sum of the log-likelihood (Eq.
5) and the log-prior of the coe -cients, as discussed above.
Next, we discuss the optimization process itself.
Several approaches to optimize our objective function exist in the literature.
Among the ones that have been used in large-scale applications are iterative scaling [20], nonlinear conjugate gradient, quasi-Newton (in particular, limited memory BFGS) [7], iteratively-reweighted least squares [14], truncated Newton [16], and trust-region Newton [5].
All the methods are iterative and generate a sequence of estimates that converge to the optimal solution.
For all methods except iterative scaling, cost per iteration is high but the convergence is fast.
For iterative scaling which updates one component at a time, cost per iteration is low but convergence is slower.
For our application, the training corpus typically has several millions data points and several thousand features making it extremely slow to  t the model using these approaches on a single machine.
To scale our computations, we adopt a simple paralleliza-tion approach that randomly splits the data into several parts,  ts a logistic regression separately to each part and then combines the estimates obtained from each piece.
For convenience, we perform our computation in a MapReduce framework [12].
MapReduce is a programming model for processing large data sets.
It runs on a large cluster of commodity machines; it is highly scalable processing several gigabytes of data on thousands of machines and easy to use.
The run-time system automatically takes care of the details of partitioning the data, scheduling job across machines, handling failures and managing inter-machine communication.
To  t a logistic regression for a given piece, we use a simple iterative scaling (also known as conditional maximization) approach.
The algorithm is as follows: We initialize the co-e cients  s,  s, and  s to 0, and  p(.
) s and  a(.
) s to 1.
We update the value of each coe cient one at a time holding the others  xed at the current value by maximizing the likelihood through a Newton-Raphson method.
This completes a single iteration.
The procedure is continued through several iterations until convergence.
The method is guaranteed to converge since every step can only increase the likelihood.
Along with a coe cient estimate, the Newton-Raphson procedure provides an estimate of the negative Hessian, the inverse of which provides an estimate of variance of the co-e cient from maximum likelihood theory.
The results on the various data partitions are combined using a weighted average of the individual estimates, where the weight assigned to partition-speci c estimate is its relative precision obtained from the negative Hessian values.
This weighting scheme is the best way to combine estimates through a linear function [6].
Figure 2 describes our  tting procedure in detail.
A key feature of the model proposed in this paper is that it is suitable for e cient evaluation over an inverted indexes.
In this section we outline an implementation of a prototype ad search engine based on the WAND [2] algorithm and inverted indexing of the ads using the Hadoop distributed computing framework [10].
Our method allows for any kind of feature to be used in the ad search.
In our prototype we use unigrams, phrases and classes as features.
The inverted index is composed of one
 Algorithm CondMax Initialize coe i = 0 for all word features i and coe i = 1 for page-region and ad-region speci c parameters in the re-parametrized model of equation 4.
Iterate until convergence (cid:2) Iterate i over the set of features (i) = loglik = 0 (i) = f (cid:2)(cid:2) impressions, with the data
 i coe i   scorei (i) + (click or not   p)   scorei (cid:2) (i)   p(1   p)   score2 (cid:2)(cid:2) Set f Iterate over all providing scorei for feature i x = p = 1.0/(1.0 + exp( x)) (cid:2) (i) = f f (cid:2)(cid:2) f (i) = f loglik = loglik + log p   x   (1   click or not) (cid:2) (i) = f f (cid:2)(cid:2) f (i) = f coe i = coe i   f than previous (cid:2)(cid:2) Return (coe i, f (i)   coe i/ 2 (cid:2) (i)   1.0/ 2 (cid:2)(cid:2) (i)/f (i) if loglik is lower (i)) for all i i i (cid:2) i (cid:2)(cid:2) Algorithm CombineCoeffs Randomly split the data Call CondMax on each split Iterate over all features i (cid:2)(cid:2)(i)| splitj coe i |f |f(cid:2)(cid:2)(i)| C(i) =

 splitj Return C(i) as the  nal coe cient value for feature i Figure 2: Implementation of Logistic Regression.
postings list for each feature that has one entry (posting) for each ad that contains this feature.
The ads are represented by adIDs - unique numeric identi ers assigned to each ad.
Figure 3 shows the architecture of our ad search prototype.
We produce the inverted index over a grid of machines running the Hadoop framework [10].
The indexing starts by extracting features from the ads.
Each feature is represented by an unique numeric featureID.
The resulting data  le is sorted by < adID, f eatureID >.
Next we invert this  le by sorting the  le by < f eatureID, adID > as a key.
Finally we write the delta compressed posting lists used at runtime to evaluate the query.
There are a few important di erences in the ad search engine problem that require di erent approach compared to web search engines.
First, in web search, the queries are short and the documents are long.
In the ad search case, the number of features per ad is usually lower than the number of features extracted from a web page, which in our case represent the ad space query.
So it is almost never the case that an ad will contain all the features of the ad search query.
Therefore the ad search engine performs similarity search in the vector space with a long query and relatively short ad vectors.
In contrast for the majority of the web queries there are many pages that contain all the query words and one of the key issue is how to rank the pages containing the query.
Features are extracted from the input query.
The query is a bag of pairs < f eatureID, weight >.
For each query feature, WAND opens a cursor over the posting list of this feature.
During the evaluation the cursors are moved forward examining the documents as they are encountered.
WAND is a document at the time algorithm [1] that  nds the next cursor to be moved based on an upper bound of the score for the documents at which the cursors are currently positioned.
The algorithm keeps a heap of current candidates.
The invariant of the algorithm is that the heap contains the best
 IDs less than the document pointed by the current minimum cursor.
Cursors pointing on documents with upper bound smaller than the minimum score among the candidate docs are candidates for a move.
To  nd the upper bound for a document, the algorithm assumes that all cursors that are before the current will hit this document (i.e. the document contains all those terms represented by cursors before or at that document).
It has been shown that WAND can be used with any function that is monotonic with respect to the number of matching terms in the document.
It can also be easily shown that some non-monotonic scoring functions can also be used as long as we can  nd a mechanism to estimate the score upper bounds.
One family of such functions is a set of functions where a  xed subset of the features (known a priori) always decrease the score.
In such cases, the upper bound estimates just assume that these features do not appear in the ad.
An example of such function is a cosine similarity where some of the query coe cients are negative.
The scoring function proposed in this paper might have such coe cients and  ts well within the WAND framework.
Incorporating the logistic-regression based model in this framework is simple.
The scoring equation 3 is modi ed to exclude the page e ect and used as WAND scoring formula.
Figure 3 shows the stages in which the learned parameters are factored into the evaluation framework.
Ma(r) is used at indexing time to calculate a static score for each individual ad.
We use this score to assign an adID to the ads in decreasing ad score order.
This allows for estimating upper bounds of the ads that are skipped by using scores by using the score of the ad pointed by the preceding cursor in the sorted cursor list.
After the page is parsed and the features are extracted along with their TF-IDF scores, we apply the reweighing based on the table Iw.
The Mp(r) table is not used in the ad selection but just to adjust the  nal scores to calculate the probabilities according to equation 3.
We now provide a detailed discussion of all our experiments, conducted on a large-scale real-world dataset.
After a brief description of the data, we present results on the di erent word selection schemes, followed by comparisons of several variants of our model with a baseline model that does not utilize any click feedback and is based solely on relevance.
The results convincingly demonstrate the improved performance of models that use click feedback.
To demonstrate the e cacy of our approach, we conduct a series of experiments on retrospective data collected from some back-end servers of a large contextual advertising system.
We would like to emphasize that the servers were chosen subjectively and are not representative of the actual performance of the entire advertising system.
We conducted our experiments on 30 days of data.
The  rst 15 days of data were used as our training corpus while the results were evaluated on the remaining days.
Every display of an ad on a page is called an impression.
Thus, a single page view by a user generates multiple impressions and equals the number of ads shown on the page.
There are approximately .5B   1B page views on a day in the portion of the data that we examined.
De ning CTR as the number of clicks per impressions, overall CTRs for (page,ad) combinations are low and approximately in the range of .1%   20% with extreme right skew.
Labeling the clicks as 1 and non-clicks as 0, we have a learning problem with extreme class imbalance.
The logistic regression is known to provide biased estimates of weights in such scenarios [11].
A widely used strategy in such settings reduces imbalance by sampling from the majority class.
We also adopt a similar strategy but our sampling scheme is as follows: we retain all impressions associated with clicked page views and get a 1/2000 random sample from the pageviews associated with non-clicked page views.
All results reported in this paper are on data sampled in this fashion.
We conducted experiments with several (page,ad) region combinations but provide detailed analysis for experiments conducted with one region combination, viz, page title (call it pti) and ad title (call it oti).
In the training corpus, we obtained approximately 112K unique words after stop word removal.
To avoid introducing extremely sparse words in our regression, we truncated our word list to consider only those that had at least 100 impressions where the word occurred both on the page and ad.
Also, we further truncated our list by considering only words that had at least 5 clicks separately when shown on pages and ads.
The truncation left us with approximately 3.4K words to choose from.
We implemented one data based and two relevance-based feature selection schemes.
The data based feature selection criteria was described in section 3.1 and given by the measure de ned as iw in equation 2.
We conduct two experiments, one with the top 1K words and other with all 3.4K words.
Similar experiments were also conducted on words selected using two relevance-based measures described previously in Section 3.2.
Brie y, the  rst relevance-based measure assigns the average tf-idf score to a word.
It is computed from a single corpus which is the union of page title and ad title regions.
The second relevance-based measure is the geometric mean of average tf-idf scores of a word computed separately from page region and ad region corpora.
We also conducted a set of experiments by randomly selecting a set of 1K words from our list of words which performed very poorly and hence its results are not reported.
A pure relevance-based based model  nds relevance by using semantic information.
More speci cally, the relevance of an ad for a page is quanti ed by a score which is a function of page and ad content.
We tested two such scores for region combination (pti, oti), viz.
tf-idfw,pti   tf-idfw,oti Relevance1 = w and Relevance2 = qP Relevance1 w,pti  P w tf-idf2 w tf-idf2 w,oti Relevance2 is the cosine of the angle between tf-idf vectors for page and ad regions.
The distribution of Relevance1
 Relevance2.
was extremely right skewed with several outliers, hence we conduct all our experiments with Relevance2.
In addition to a model based solely on Relevance2, we explore a hybrid model which combines our word-based logistic regression with the relevance-based score model.
We explore two options: (a) use Relevance2 as an additional feature in the logistic regression but without adding substantial overhead to the indexing algorithm, and (b) use Relevance2 to derive the priors qij in equation 3.
Both these options require understanding the relationship between the log-odds of CTR and Relevance2; the relevance score needs to be transformed on to the scale of log-odds of probability.
We empirically studied this relationship by plotting empirically estimated log-odds against Relevance2 scores as follows: sort all positive Relevance2 values in the training data and create 100 bins.
For each bin, we compute the empirical log-odds and the average Relevance2 score.
Figure 4 shows the relationship.
Approximately 90% of impressions had a Relevance2 value of zero.
Excluding those, the curve clearly reveals a quadratic relationship.
Thus, for scheme (a) above, we add quadratic terms a + b   Relevance2 + c   Relevance2 2 to the logistic re gression and estimate parameters a, b and c. For scheme (b) above, we derive qij  s by using the approximate quadratic curve shown in  gure 4: logit(qij ) =  1.75 + 2.52   Relevance2   1.56   Relevance


 Our test data had a total of 14.48M impressions.
Of these,
 itive tf-idf score when considering only words in pti and oti.
We only report results on this pool.
We use the precision-recall metric to measure the performance of our algorithms.
In our scenario, recall for a given threshold on the model Figure 5: Precision-recall curves for the three word selection schemes with the best hybrid model that uses both words and relevance measure as features.
score is the fraction of all clicks that were present (recalled) in the test cases above that threshold.
Precision for a given recall value is the CTR on the test cases above the threshold.
Since our end goal is to rank ads for a given page, we are more interested in algorithms that recall the top fraction of clicks (e.g 2%, 5%, 10%) with greater precision.
Also note that, for a random classi er that classi es a test case as a click with a constant probability (overall CTR in training corpus), the precision equals that constant probability value for every recall value.
Our key results are as follows:   Figure 5 shows the precision-recall curves for the three word selection schemes for our best model, viz, the hybrid model that uses both words and relevance measure as features (results with other models were similar).
Of the three word selection schemes we tried, the data-based one using measure iw of equation 2 gave the best results, especially for low values of recall.
Of the two relevance-based word selection criteria, the one that uses the geometric mean of the average tf-idf score was slightly better than the one that used average tf-idf from the union of page and ad regions.
However, both the relevance schemes were inferior to the data-based one.
In addition, we found no signi cant di er-ence in going from the top 1000 words to the top 3400 words in the data-based scheme; hence, those results are not shown.
This does indicate, however, that the top few words according to our data-based measure iw are enough to capture most of the signal in the data.
  We conducted experiments using both tf and tf-idf as our scores for features in equation 3.
We consistently get better results with tf; all our subsequent results are reported with this measure.
  Figure 6 shows the precision-recall curve for the hybrid, word-only and relevance-based models.
Both our word-only logistic regression using tf as feature scores
 (b) All recall values Figure 6: Precision recall curves for three methods.
All curves are reported for the best word selection scheme.
The second curve was truncated at a recall value of 20% to provide a better summary of performance at small recall values.
Statistical error in all cases was less than 1% of the mean.
in equation 3 and the hybrid model that uses both word-based and Relevance2 based features outperformed the pure relevance model that ranks ads solely based on Relevance2.
In fact, the hybrid model is superior to the word only model; especially for low recall values like 2%, 5% and 10%.
The hybrid models that use Relevance2 as features and prior have approximately similar performance.
In fact, for a recall of 10%, the hybrid model provides 25% lift relative to a pure relevance-based model and 100% lift relative to random model in terms of precision.
We notice sharp spikes in the precision-recall curves, especially at low recall values.
This is indicative of high click density regions in the test data when ranked by model scores.
On further scrutiny we discovered some of these high density regions were caused by (page,ad) impressions that had extremely high click rates (e.g in the range of 30%   60%).
This can occur in our data due to a number of reasons: popular ads, popular pages, sudden burst of heavy clickers, etc.
The 95% con dence intervals obtained by producing precision-recall curves on several independent splits of test data was within 1% of the mean; hence, all our results are statistically signi cant.
  Figure 7 shows histograms of parameter estimates obtained from our best hybrid model.
The histograms show the distribution of coe cient estimate   average-tf for words for page main e ects, ad main e ects and page-ad interaction e ects.
Interestingly, the interaction e ects are small for a majority of words except for a small fraction that have large positive values.
This ties in with the observation that using the top 1000 words was as accurate as using the top 3400 words; if only a few words have signi cant interaction e ects, then adding more words as features into the logistic regression will not help.
Based on an extensive set of large scale experiments, we demonstrated that using click feedback signi cantly improves the accuracy of matching relevant ads to pages compared to traditional relevance scoring models that are solely based on semantic similarity.
We incorporate click feedback through a logistic regression by using the words on pages and ads as features.
We tested several variable selection schemes and found a simple data based scheme performs the best.
We also proposed a hybrid model which uses words and relevance scores as features and signi cantly outperforms the traditional relevance based models in our experiments.
Our solution is scalable and can process several gigabytes of data through a Hadoop parallelization framework.
In addition, it can be easily incorporated into existing ad-serving architectures.
We proposed a new class of models to combine relevance with click feedback for a contextual advertising system.
Our model is based on a logistic regression and allows for a large number of granular features.
The key feature of our modeling approach is the ability to model interactions that exist among words between page and ad regions in a way that is suitable for e cient evaluation over inverted indexes.
In fact, we employ a multiplicative factorization to model the interaction e ects for several (page, ad) regions in a parsimonious way that facilitates fast lookup of ads at run time.
Through large scale experiments, we convincingly demonstrate the advantage of combining relevance with click feedback.
In fact, we achieve a 25% lift in precision for a recall value of 10% relative to a pure relevance based model in our experiments.
In the future, we want to enrich our models to discover interactions that are caused due to word synonyms both using a supervised and unsupervised approach.
We are currently experimenting with a wide range of feature selection and model  tting scheme for the logistic regression proposed
 (b) Ad Main E ects (c) Page Ad Interaction E ects Figure 7: Average estimated weight for page main e ects, ad main e ects and interaction e ects.
The distribution is obtained as product of parameter estimate from regression and the average tf score associated with a word in the training corpus.
in the paper.
We are also exploring the application of our method to other systems where there is implicit feedback in the form of clicks, such as web search and search advertising.
and B. Ribeiro-Neto.
Learning to advertise.
In SIGIR  06: Proc.
of the 29th annual intl.
ACM SIGIR conf., pages 549 556, New York, NY, 2006.
ACM.
[14] P. McCullagh and J.A.Nelder.
Generalized Linear Models.
Chapman and Hall, 1989.
[15] M.J.Silvapulle.
On the existence of maximum likelihood estimates for the binomial response models.
Journal of the Royal Statistical Society, Series B,
 [16] P.Komarek and A.W.Moore.
Making logistic regression a core data mining tool with tr-irls.
In International Conference on Data Mining, pages
 [17] M. Regelson and D. Fain.
Predicting click-through rate using keyword clusters.
In In Proc.
of the Second Workshop on Sponsored Search Auctions, 2006.
[18] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S.
de Moura.
Impedance coupling in content-targeted advertising.
In SIGIR  05: Proc.
of the 28th annual intl.
ACM SIGIR conf., pages 496 503, New York,
 [19] M. Richardson, E. Dominowska, and R. Ragno.
Predicting clicks: estimating the click-through rate for new ads.
In WWW, pages 521 530, 2007.
[20] S.D.Pietra, V.D.Pietra, and J.La erty.
Inducing features of random  elds.
IEEE PAMI, 19:380 393,
 [21] C. Wang, P. Zhang, R. Choi, and M. D. Eredita.
Understanding consumers attitude toward advertising.
In Eighth Americas conf.
on Information System, pages 1143 1148, 2002.
[22] W. Yih, J. Goodman, and V. R. Carvalho.
Finding advertising keywords on web pages.
In WWW  06: Proc.
of the 15th intl.
conf.
on World Wide Web, pages 213 222, New York, NY, 2006.
ACM.
