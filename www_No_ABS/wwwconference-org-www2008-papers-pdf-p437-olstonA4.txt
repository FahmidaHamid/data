Modern search engines rely on incremental web crawlers [3] to feed content into various indexing and analysis layers, which in turn feed a ranking layer that handles user search queries.
The crawling layer has two responsibilities: downloading new pages, and keeping previously-downloaded pages fresh.
In this paper we study the latter responsibility, and consider what page revisitation policy a crawler should employ to achieve good freshness.
Good freshness can be guaranteed trivially by simply revisiting all pages very frequently.
However, doing so would place unacceptable burden on the crawler and leave few resources for discovering and downloading new pages.
Hence we seek revisitation policies that ensure adequate freshness while incurring as little overhead as possible.
Prior work on this problem has focused on two factors when determining how often to revisit each web page: Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
updated by its owner) [4, 6, 7].
tent has on search results) [12, 13].
A third important factor that has thus far been ignored is information longevity: the lifetimes of content fragments that appear and disappear from web pages over time.
Information longevity is not strongly correlated with change frequency, as we show in Section 3.2.
It is crucial for a crawler to focus on acquiring longevous (i.e., persistent) content, because ephemeral content such as advertisements or the  quote of the day  is invalid before it hits the index.
Besides, a search engine typically has little interest in tracking ephemeral content because it generally contributes little to understanding the main topic of a page.
Real web pages di er substantially in the longevity of their content.
Figure 1 shows the temporal evolution of two web pages, in terms of the presence or absence of individual content fragments.
(The manner in which we divide pages into fragments is not important for the present discussion.)
In each graph the horizontal axis plots time, and the vertical axis shows unique fragments, sorted by the order in which they  rst appear on the page.
Each horizontal stripe represents a fragment; the left endpoint is the time at which the fragment  rst appears on the page; the right endpoint is the time at which the fragment disappears.
Page A has a small amount of static content, and a large amount of highly volatile content that consists of dynamically generated text and links apparently used to promote other web pages owned by the same organization.
Page B contains a mixture of static content, volatile advertisements, and a third kind of content manifest as horizontal stripes roughly 30 to 60 days long.
Page B is part of a cooking site that displays a sliding window of recent recipes.
Contrasting Pages A and B, we appreciate the importance of considering the lifetime of information when crafting a page revisitation policy.
Pages A and B both undergo frequent updates.
However Page A is not worth revisiting often, as most of its updates simply replace old ephemeral information with new ephemeral information that would be pointless for the search engine to index.
Page B, on the other hand, is adding information that sticks around for one to two months (i.e., recipes) and might be worthwhile to index, making Page B worth revisiting often.
(We estimate the prevalence on the web of pages like A and B in Section 3.3.)
Information longevity has been measured in various ways in previous work [1, 8, 9].
However we are not aware of any work that considers its role in recrawl e ectiveness.
This paper makes the following contributions:   Identi cation of information longevity as a key factor in crawler performance.
  Longevity measurements of real web content, and a generative model that accounts for the observed characteristics (Section 3).
  New page revisitation policies that take into account information longevity in addition to the usual factors, and avoid wastefully downloading ephemeral content (Section 4).
  Empirical study of the online revisitation problem, where policies must sample page update behavior on the  y, in order to learn how pages change and use the learned information to schedule future revisitations (Section 5).
The revisitation policies we propose are highly practical.
They incur very little per-page space and time overhead.
Furthermore, unlike some previously-proposed policies, ours do not rely on global optimization methods, making them suitable for use in a large-scale parallel crawler.
Lastly, our policies automatically adapt to shifts in page change behavior.
Our revisitation policies are based on an underlying theory of optimal page revisitation, presented next.
The scenario we consider is as follows.
A crawler has acquired the content associated with a set of pages P. Each page P   P may undergo autonomous updates after the crawler s  rst visit, causing the web-resident copy of P to drift from the crawler s copy.
A page revisitation policy governs the schedule with which the crawler refreshes its local copy of P , by revisiting the web-resident copy and reacquiring its content, in order to bring the local copy in sync with the web copy, if only temporarily.
The following metrics are of interest:   Refresh cost: the total resources spent refreshing web pages.
Following previous work we make the approximation that every refresh event incurs equal resource utilization, and measure refresh cost as the total number of refresh events during in a given time period.
  Divergence: the degree to which the crawler s local cache of page content di ers from the true web-resident content, averaged across pages and across time.
Mathematically, we have a page divergence function D(P, P (cid:48)) of two copies of a page: the true copy P and cached copy P (cid:48).
Immediately following a refresh event, P = P (cid:48) and D(P, P (cid:48)) = 0.
Later, if P undergoes updates such that P (cid:54)= P (cid:48), D(P, P (cid:48)) > 0.
(We give the exact form of the D( ) function in Section 2.1.1.)
The overall divergence of the web cache at a given time is de ned as a weighted average across pages:



 (cid:48) ) (1)



 Z t2

 t=t1 where WP denotes the importance or relevance weight of page P , e.g., P  s pagerank or embarrassment coe cient [13].
Averaging across a given time interval (t1, t2), we have: D(P, t1, t2) = WP   D(P (t), P (cid:48) (t)) dt (2)
 and P (cid:48)(t) denotes the (possibly out-of-date) content cached for P as of time t.
Modern web pages tend to consist of multiple content regions stitched together, e.g., static logos and navigation bars, dynamic advertisements, and a central region containing the main content of the page, which is also dynamic but behaves quite di erently from advertisements.
Consequently, page divergence metrics that model a page as an atomic unit of content are no longer a good  t.
The page divergence metric we propose is called fragment staleness.
It measures the fraction of content fragments that di er between two versions of a page.
Mathematically, we treat each page version as a set of fragments and use the well-known Jaccard formula for comparing two sets:
 (cid:48) ) = 1   |F (P )   F (P (cid:48))| |F (P )   F (P (cid:48))| (3) where F (P ) denotes the set of fragments that comprise P .
As far as how we divide a page into fragments, we require a method that is amenable to e cient automated computation, because we will use it in our crawling algorithms.
Hence we rule out sophisticated semantic approaches to page segmentation, which are likely to be too heavyweight and noisy for this purpose.
A simple and robust alternative that is well aligned with the way search engines tend to treat documents is to treat sequences of consecutive words as coherent fragments.
We adopt the well-known shingling method [2], which emits a set of content fragments, one for each word-level k-gram (k   1 is a parameter).
Our rationale for adopting the fragment staleness metric is as follows.
Consider what would happen if the cached copy of a page contains some fragments that do not appear in the web-resident version: The search engine might display the page s URL in response to a user s search query string that matches the fragment; yet if the user clicks through she may  nd no relevant content.
Conversely, if a query matches some fragment in the web-resident copy that does not appear in the cached copy, the search engine would overlook a potentially relevant result.
Fragment staleness gives the likelihood of introducing false positives and false negatives into search results.1 Fragment staleness generalizes the  freshness  metric of [4, 6], which we refer to in this paper as holistic staleness (stale ness is the inverse of freshness).
Holistic staleness implicitly assumes one content fragment per page and yields a Boolean response: each page is either  fresh  or  stale. 
 A theory of optimal recrawling2 can be developed in a manner that is independent of any particular choice of per-page divergence metric.
The goal is a page revisitation policy that achieves minimal time-averaged divergence within a given refresh cost budget.
For the sake of the theory let
 ments, one can extend our formula to associate a numeric weight with each fragment, along the lines of [12].
The techniques presented in this paper are not tied to uniform weighting of fragments.
ferent context.
us assume that for each page P , divergence depends only on the time tP since the last refresh of P .
Assume furthermore that, other than in the case of a refresh event, divergence does not decrease.
Under these assumptions we P (t   tP ), can re-express divergence as D(P (t), P (cid:48)(t)) = D  for some monotonic function D 
 Let T be any nonnegative constant.
It can be shown using the method of Lagrange Multipliers that the following revis-itation policy achieves minimal divergence compared with all policies that perform an equal number of refreshes: At each point in time t, refresh exactly those pages that have UP (t   tP )   T , where Z t UP (t) = t   D P (t)       P (x) dx
 (4)
 Intuitively speaking, UP (t) gives the utility of refreshing page P at time t (relative to the last refresh time of P ).
The constant T represents a utility threshold; we are to refresh only those pages for which the utility of refreshing it is at least T .
The unit of utility is divergence   time.
Figure 2 shows divergence curves for the two web pages illustrated in Figure 1.
The horizontal axis of each graph plots time, assuming the crawler refreshes both pages at time
 staleness metric introduced in Section 2.1.1.3 If we expect the divergence curve of a page to behave similarly after the next refresh as it did after the t = 0 refresh, then we should favor refreshing Page B over refreshing Page A at the present time.
The reason is that refreshing Page B gives us more  bang for our buck,  so to speak, than Page A.
Utility at time t is given by the area of the shaded region.
Page B has higher utility than Page A, which matches our intuition of  bang for buck.  In addition to giving the intuition behind the utility formula derived in Section 2.2, this example also illustrates the appropriateness of fragment staleness as a divergence metric.
Although our generic theory of optimal revisitation of Section 2.2 can be applied to holistic staleness as well (see Appendix A; our result matches that of [4]), the nature of the holistic staleness metric can lead to undesirable behavior: Under a revisitation policy that optimizes holistic staleness, Pages A and B, which undergo equally frequent changes, are treated identically either both ignored or both refreshed with the same frequency.
One may wonder whether a simpler utility function such as Up(t) = D  P (t) (i.e., prioritize refreshing based on current divergence) may work nearly as well as the theoretically optimal formula given in Equation 4.
We have veri ed empirically via simulation over real web data (see Section 3.1) that refreshing according to Equation 4 yields signi cantly improved staleness for the same refresh cost, compared with using the naive form Up(t) = DP (t).
In fact, the naive form even performs worse than uniform refreshing, which is to be expected in light of the  ndings of [4].
stant over time, but occasionally undergoes a slight decrease.
These decreases are due to a few fragments reappearing after they had previously been removed from the page.
(The visualizations of Figure 1 do not show lapses in fragment presence.)
In this section we study the information longevity characteristics of real web pages.
After describing our data sets in Section 3.1 we measure the (lack of) correlation between information longevity and change frequency in Section 3.2.
Then in Section 3.3 we propose and validate a generative model for dynamic web content.
Lastly in Section 3.4 we measure the potential performance gain from use of a longevity-aware revisitation policy.
We use two real web data sets:   Random.
We obtained a random sample of 10,000 URLs from Yahoo s crawled collection, and con gured a crawler to download the content of each URL once every two days over the course of several months in
   High-quality.
Our second data set is based on a random sample of 10,000 URLs from the OpenDirec-tory [11].
We consider this data set to consist of much higher quality pages than random ones, on average.
For this data set we obtained 30 temporal snapshots, again by downloading each page every second day.
Within each data set we assign uniform importance weights (WP = 1 for every page).
The high-quality data set is of signi cantly more interest to study than the random data set, because crawlers typically avoid recrawling low-quality pages frequently.
The interesting question is how frequently to recrawl each high-quality page.
For most of our experiments we only report results on the high-quality data set, due to space constraints.
In general the two data sets yield similar results.
Unfortunately a few page snapshots are missing from the data, because in some cases the server hosting a given page was unreachable, even after several retries.
In these rare cases we substituted the content from the previous snapshot.
To evaluate page revisitation policies (Sections 3.4 and 5), we need a notion of  ground truth.  Since we are not the Figure 3: Change frequency versus information longevity.
originators of the web pages in our data sets, we do not have access to complete update histories.
The only information available to us comes from our bi-daily snapshots, and we need to interpolate.
Our method of interpolation is described in Appendix B.
Information Longevity Distribution Figure 3 plots change frequency versus information longevity for the high-quality data set.
Each point denotes a page.
Change frequency is computed as the number of snapshots that di er from the previous snapshot, normalized to [0, 1].
Information longevity is the average lifetime of fragments (shingles) on the page, in terms of the number of contiguous snapshots in which a given shingle occurred (shingles that were present in the initial or  nal snapshots are not included, since we are unable to determine their lifetimes).
The fact that the points in Figure 3 are substantially spread out indicates that information longevity is not strongly correlated with change frequency (the correlation coe cient is 0.67).
In other words, if we are told the change frequency
 content.
This observation, combined with the fact that information longevity is a major determinant of e ective page revisitation strategy (Section 1), motivates the study of information longevity on the web.
We model a page as a set of independent content regions, where the temporal behavior of each region falls into one of three categories:   Static behavior: content essentially remains unchanged throughout the page s lifetime, such as templates.
  Churn behavior: content is overwritten repeatedly, such as advertisements or  quote of the day.    Scroll behavior: content is appended periodically, such as blog postings or  what s new  items.
Typically, as new items are appended, very old items are removed.
Page A in Figure 1 consists of a small static region (templates) and a larger churn region (advertisements).
Page B consists of a large static region (templates), a churn region (advertisements) and a scroll region (recipe postings).
In our generative model, each churn or scroll region R has an associated Poisson update process with rate parameter  R (in our data sets updates closely follow a Poisson distribution, which is consistent with previous  ndings).
In a churn region, each update completely replaces the previous region content, yielding the fragment lifetime distribution: P(lifetime   t) = 1   e( R t) In a scroll region each update appends a new content item and evicts the item that was appended K updates previously, such that at any given time there are K items present.4 The fragment lifetime distribution is: P(lifetime   t) = 1   K 1X ( R   t)i   e R t i!
i=0 Figure 4 plots the lifetime distributions for a churn region and a scroll region with K = 10, where both regions have the same update rate  R = 0.25.
The two distributions are quite di erent.
Fragment lifetimes tend to be much longer in the scroll case.
In fact, in the scroll case it is unlikely for a fragment to have a short lifetime because it is unlikely for ten updates to occur in rapid succession, relative to  R.
To validate our generative model, we analyzed the real fragment lifetime distribution of pages from the high-quality data set.
We focused on a set of pages that have the same average change frequency  P = 0.25.
We assigned an estimated K value to every non-static fragment, based on the number of page update events the fragment  survives  (i.e., remains on the page).
For each page we found the most common K value among its fragments.
We obtained three groups of pages: those whose dominant K value is 1 (churn behavior), those dominated by K = 2 (short scroll behavior), and those dominated by
 churn model.
Nonetheless it is instructive to consider them as two distinct models.
Figure 4: Cumulative distribution of fragment lifetimes, under two di erent content evolution models.
Figure 5: Actual and modeled lifetime distributions.
scroll behavior with some K > 2 (the third category was not large enough to subdivide while still having enough data for smooth lifetime distributions).
Figure 5 plots the fragment lifetime distributions for the non-static fragments of the three groups of pages, along with corresponding lifetime distributions obtained from our generative model.
Each in-stantiation of the model re ects a distribution of K values that matches the distribution occurring in the data.
The actual lifetime curves closely match the ones predicted by the model.
One exception is that the K > 2 curve for the actual data diverges somewhat from the model at the end (top-right corner of Figure 5).
This discrepancy is an unfortunate artifact of the somewhat short time duration of our data set: Fragments present in the initial or  nal snapshot of a page were not included in our analysis because we cannot determine their full lifetimes.
Consequently the data is slightly biased against long-lifetime fragments.
Unlike the rightmost curve of Figure 4, none of the curves in Figure 5 exhibit the  at component at the beginning.
The reason is that in each of our page groups there is at least some K = 1 content that churns rapidly.
Implications
 In our data set, churn behavior accounts for 67% of the non-static fragments, with 33% exhibiting scroll behavior
 and 3 (Section 2.1).5 On both axes, lower is better.
Roughly half of the pages are completely static, so the largest interesting value of refresh cost is 0.5.
Even if we do refresh every non-static page at every opportunity (every two days, in our data set), staleness still does not go to zero due to divergence during the two-day period between refreshes.
(Recall from Section 3.1 that we interpolate between snapshots.)
The FS-S policy is roughly the analogue of HS-S.
Both
 assume stationary page change behavior (encoded as D  and  P , respectively).
Comparing these two, we see that the fragment-based policy (FS-S) performs signi cantly better, especially if we consider uniform refreshing as the baseline.
The fragment-based policy is geared toward refreshing content of high longevity, and avoids wasting resources on ephemeral content.
Turning to the dynamic policies FS-D and HS-D, we again see the fragment-based policy (FS-D) performing better.
Also, both dynamic policies vastly outperform their static counterparts, which points to adaptiveness as a big win.
We now turn our attention to online page revisitation policies.
An online policy is not given any a priori information about page change behavior, and must expend refreshes in order to learn how pages behave.
There is little previous work on this topic.
In this section we present two online revisitation policies.
We begin by introducing a data structure common to both policies, called a change pro le, in Section 4.1.
We then present our two online policies in Sections 4.2 and 4.3.
Both policies are based on our underlying theory of optimal refreshing (Section 2.2) and are governed by a utility threshold parameter T ; we discuss how to choose T in Section 4.4.
Lastly, in Section 4.5 we give a method of bounding the risk associated with over tting to past observations.
A change pro le of page P consists of a sequence of (time, divergence) pairs, starting with a base measurement (tB, 0) and followed by zero or more subsequent measurements in increasing order of time.
Time tB is called the base time, and represents the time at which the change pro le was initiated.
Each subsequent measurement corresponds to a refresh event performed subsequently to tB by the revisitation policy.
All divergence values in the pro le are computed relative to the base version P (tB).
An example change pro le is: (cid:104) (10, 0), (12, 0.2), (15,
 times for this page include 10, 12, 15 and 23, and that D(P (10), P (12)) = 0.2, D(P (10), P (15)) = 0.2, and D(P (10), P (23)) = 0.3.
P (10) is the base version; 10 is the base time.
Our online revisitation policies initiate and maintain change pro les in the following manner.
Each time a new refresh of P is performed, a new change pro le is created with base time tB set to the time of the refresh.
Each subsequent time
 staleness on the y-axis (omitted due to space constraints).
As expected the HS policies outperform the FS policies on that metric.
Figure 6: Performance of o ine revisitation policies.
(K > 1).
Based on these  ndings we conclude that about a third of the dynamic content on high-quality web sites behaves in a scrolling fashion.
The prevalence of scrolling content points to the inadequacy of models that only account for churning content, including the ones used in pervious work on recrawl scheduling.
Now that we have developed an understanding of information longevity as a distinct web evolution characteristic from change frequency, we study whether there is any advantage in adopting a longevity-aware web crawling policy.
For now we consider o ine policies, i.e., ones that rely on a-priori measurements of the data set to set the revisitation schedule (we consider online policies in Sections 4 and 5).
The o ine policies we consider are:   Uniform: refresh each non-static page with the same frequency.
Ignore static pages.
  Fragment staleness with static input (FS-S): the optimal policy for fragment staleness as derived in Section 2.2, given a-priori knowledge of the divergence P ( ) o ine: For each function D  t let D  P (t) equal the average divergence between pairs of snapshots separated by t time units.)
P ( ).
(We estimated D    Holistic staleness with static input (HS-S): from [4], the optimal policy for holistic staleness, given a-priori knowledge of each page P  s change frequency  P .
(We estimated  P o ine by dividing the total number of changes by the total time interval.)
  Fragment staleness with dynamic input (FS-D): the same as FS-S except instead of using a static, time-P ( ), at each point in averaged divergence function D  time use the divergence curve that occurred between the time of the previous refresh and the current time.
  Holistic staleness with dynamic input (HS-D): the same as FS-D, but substituting holistic staleness as the divergence metric.
Figure 6 shows how these policies perform on the high-quality data set.
The x-axis plots normalized refresh cost (1 corresponds to refreshing every snapshot of every page).
The
 (t, D(P (tB), P (t))).
Up to h change pro les are maintained simultaneously, each with a di erent base version.
(h is a positive integer parameter that governs the amount of history tracked by the policy.)
Our  rst policy is based on  tting divergence curves over the sampled divergence values found in the change pro les.
It works as follows.
Immediately after each refresh of page P , the policy updates its change pro les and then computes a new refresh period  P , i.e., the period of time to wait before refreshing P again.
The process for computing  P has three steps:
 tions recorded in the h di erent change pro les, into a single pro le.
In particular,  rst normalize each change pro le by subtracting the base version from each recorded timestamp, so that each pro le starts from t = 0.
Then take the union of the (t, D) pairs across all pro les, while combining entries that have the same t value by taking the average D value.
curve to the set of points in the combined change pro le, based on our generative model of Section 3.3.
In particular,  t one curve that corresponds to churn behavior, and a second curve for scroll behavior, and select the one that yields the closest  t.
the refresh period  P equal to the t value that causes UP (t) = T , where T is the utility threshold parameter.
To ensure that enough observations are collected at the outset, for each newly-discovered page P there is an initial learning phase in which the maximum refresh period is bounded by l   n where n denotes the total number of past refreshes of P and l > 0 is a learning parameter.
Hence initially the policy is required to perform refreshes in rapid succession, but the maximum refresh period grows linearly over time.
(Other functions to control the learning phase are also possible, and may be worth studying in future work.)
Our second policy does not attempt to  t a divergence curve based on a generative model.
Instead, it places conservative bounds on divergence, and uses the bounds to adjust the refresh period  P adaptively.
In addition to the h change pro les, for each page the bound-based policy maintains a reference time tR.
The policy attempts to  nd the optimal time for the next refresh, tR +  P , where  P denotes the optimal refresh period.
Once the policy believes it has reached or passed time tR +  P it resets the reference time tR to the current time, and repeats the process.
Each iteration is seeded with the refresh period used in the prior iteration, so that the process converges (assuming the page behavior is stationary).
The exact policy is as follows:
 pro le, determine upper and lower bounds on divergence as shown in Figure 7.
The upper bound curve represents the extreme situation where the jump in divergence between two successive observations occurs Figure 7: A change pro le, with lower and upper bounds on the area under the divergence curve.
immediately following the  rst observation.
The lower bound curve represents the opposite extreme, in which the jump in divergence occurs immediately prior to the second observation.
This manner of  tting divergence bounds is conservative: it assumes no model other than monotonic divergence.
bounds from the h change pro les into a single one by averaging.
Combine the lower bounds in the same fashion.
der the divergence upper bound curve, in the interval [0, t   tR], where t is the current time and tR is the reference time.
Do the same for the divergence lower bound curve.
Label these two areas Amax and Amin , respectively, as shown in Figure 7.
Then compute corresponding bounds on utility U as per Equation 4, yielding Umin and Umax respectively.
  If Umax < T , set  P := (t   tR)   2.
  If Umin   T , reset the reference time to the present time (tR := t), and set  P :=  P /2.
The rationale behind the above policy is that if the upper bound on utility is below the utility threshold T , the period (t   tR) was shorter than the optimal refresh period  P , so we should continue to explore larger refresh periods.
We do so by doubling the quantity (t   tR) and scheduling the next refresh that many time units in the future.
On the other hand, if the lower bound on utility is above the utility threshold, the period (t   tR) was longer than the optimal period  P , so we need to start over with a new reference time and explore shorter refresh periods.
To ensure that we use a small enough period to start with, we use half of the current period  P .
There is a third case in which the utility bounds straddle the utility threshold, i.e., Umin < T   Umax .
In this case we take the neutral action of leaving the refresh period as it is.
Overall, crawling resources must be shared between discovery and retrieval of new content, and refreshing of old content [7].
Hence there is an intrinsic tradeo  between freshness and coverage.
In view of this tradeo , the following overall crawling strategy seems appropriate: when there is an opportunity to boost freshness signi cantly by
 acquiring new content.
From Section 2.2 we know that basing refresh decisions on a  xed threshold of utility, measured according to Equation 4, is optimal in terms of freshness achieved per unit cost.
We leave the utility threshold T as a parameter to be set by a human administrator who can judge the relative importance of freshness and coverage in the appropriate business context.
T is set properly i  (1) it would be preferable to receive a freshness boost of magnitude T (in units of divergence   time) rather than download a new page, and (2) it would be preferable to download a new page than to receive a freshness boost of T   .6 In a parallel crawler, the value of T may be broadcast to all nodes at the outset of crawling (and during occasional global tuning).
Subsequently, all refresh scheduling decisions are local, because they depend only on T and a given page s change pro les.
Given that web servers are autonomous and pages can change arbitrarily at any time, it is important to mitigate the risk associated with waiting a long time between refreshes.
Our online revisitation policies (Sections 4.2 and 4.3) aim to refresh a page whenever the estimated utility penalty of not doing so exceeds T , in a best-e ort manner.
We wish also to guarantee that in the worst case, the utility penalty incurred without performing a refresh is at most   T , where     1 is a risk control parameter.
Let Dmax denote the maximum divergence value allowed under the chosen page divergence metric.
(Dmax = 1 under our fragment staleness metric.)
The maximum loss in utility incurred during t time units is t   Dmax .
To cap the utility loss between refreshes at   T , we restrict the refresh period  P to remain less than or equal to     T /Dmax at all times.
We evaluate our online page revisitation policies empirically, using simulation over our web data sets described in Section 3.1.
There are three sets of experiments:   Comparison between our two policies.
(Section 5.1)   Measurement of the space footprint.
(Section 5.2)   Comparison with previous work.
(Section 5.3)
 In our  rst experiment, we compare the performance of our two online revisitation policies: the one based on curve tting (Section 4.2) and the one based on divergence bounds (Section 4.3).
We perform separate measurements of warm-up behavior and steady-state behavior.
Warm-up behavior occurs during the time following discovery of a new page, while an online policy focuses on learning the page s change patterns.
Steady-state behavior follows, wherein the policy focuses primarily on exploiting its accumulated understanding of the page.
Figure 8 shows the policies  warm-up behavior their performance in the  rst 30 days of our high-quality data set.
loading various new pages, and weighs that against the utility of refreshing, is beyond the scope of this paper.
Figure 8: Warm-up performance of online revisita-tion policies, on high-quality data set.
Figure 9: Steady-state performance of online policies, on high-quality data set.
Figure 9 shows their steady-state behavior their performance in the  nal 30 days (there are 60 days total).
In both graphs refresh cost is plotted on the x-axis, and average fragment staleness is plotted on the y-axis.
The di erent points associated with a policy correspond to di erent utility threshold (T ) values.
For the bound-based policy we used h = 5 change pro les, and risk parameter   = 10.
For the curve tting policy we again used h = 5, and we tuned the learning factor l such that the investment in learning is proportional to the refresh cost incurred.
For reference, in both graphs we also show the performance of uniform refreshing and of the o ine FS-D policy (Section 3.4).
Ideally no policy should perform worse than uniform refreshing.
FS-D is a hypothetical policy that has full knowledge of the page s behavior even when it is not being refreshed.
It is e ectively impossible for an online policy to approach the performance of an o ine one.
From Figure 8 we see that during warm-up the curve tting and bound-based policies perform about the same amount of exploration (re ected as refresh cost).
For a given exploration cost, the staleness level during warm-up roughly coincides with uniform refreshing.
Turning to Figure 9 we see that in steady-state both of our policies perform sig-ni cantly better than uniform refreshing, with curve tting slightly ahead of bound-based.
formance.
Our online revisitation policies maintain per-page state.
In the presence of billions of pages, it is important to consider the total space footprint of this state.
For each page our policies maintain h change pro les, each of which consists of a short sequence of numeric (time, divergence) pairs.
Furthermore, for each change pro le a summary of the base version must be kept, to enable computation of divergence values relative to the base version.
A page summary consists of a list of page fragments.
To save space, each fragment is encoded as a hash value of the shingle string, using a collision-resistant hash function.
Furthermore, following [2], rather than storing the complete list of fragments, we retain only the m fragments of minimal hash value, for some constant m > 0.
A simple unbiased estimator of the Jaccard distance based on minimal hash value sets is given in [2].
The experiments reported in Section 5.1 used m =   (all fragments) and h = 5, where h is the number of historical change pro les and base version summaries maintained.
Figure 10 shows the performance of our curve tting and bound-based policies under di erent values of m and h. As we can see, there is little degradation in performance due to truncating history and using compact fragment summaries to estimate divergence.
With m = 8 and h = 1, we store just one base version summary containing just 8 shingle hashes, which requires 32 bytes.
In our experiments, change pro le sizes ranged from 4 to 12 observations, for a space footprint of between
 and 128 bytes per page.
If there are 10 billion pages, in the worst case the total space footprint amounts to a little over a terabyte.
Modern crawlers spread the per-page state across more than a thousand crawler nodes, for about a gigabyte of state per node, which easily  ts in memory on each node.
We compare our online curve tting policy (Section 4.2) against the policy proposed by Cho and Garcia-Molina for optimizing holistic staleness [4] coupled with a method of estimating change frequency proposed by the same authors [5], which we refer to as the CGM policy.
Since that work does not propose a way of scheduling exploratory refreshes for learning purposes, we use a simple linearly-growing warm-Figure 11: Comparison with previous work, on high-quality data set.
Figure 12: Comparison with previous work, on random data set.
up schedule for both CGM and our own policy.
After the warm-up period we allow each policy to set its own refresh schedule.
We measure post-warm-up performance.
Figure 11 shows the result of this comparison, on the high-quality data set.
Almost 90% of the content fragments are static, so it is trivial to achieve staleness below 0.1.
On the other extreme, 3% of the content is so dynamic that even under bi-daily refreshing it does not remain synchronized for long.
Hence the  playing  eld  on which the policies compete spans the 0.03 0.1 staleness range.
Figure 12 shows the same comparison, but on the random data set.
Observe that the minimum staleness level achievable on the random data set is much higher than on the high-quality data set.
This di erence is due to the fact that random pages tend to have more dynamic content than high-quality ones, perhaps aimed at attracting the attention of search engines and users.
As expected our policy, which is optimized for the fragment staleness metric, outperforms CGM in terms of fragment staleness per refresh cost, on both data sets.
Although the performance di erence is small in the low-cost/high-staleness part of the graphs (upper-left portion), the gap becomes substantial as we move to the moderate-cost/moderate-staleness part (middle portion) and beyond.
refresh cost 0.2 instead of 0.4 to achieve the same moderate staleness level, which translates to refreshing pages once every 10 days instead of every 5 days, on average.
If there are 1 billion high-quality pages, this di erence amounts to 100 million saved page downloads per day.
CGM assumes that every time a page changes, its old content is wiped out (i.e., churn behavior), and therefore ignores all frequently changing pages.
In contrast, our policy di erentiates between churn and scroll behavior, and revisits scroll content often to acquire its longevous content.
The only way to get CGM to visit that content is to have it visit all fast-changing content, thus incurring unnecessarily high refresh cost.
We identi ed information longevity as a distinct web evolution characteristic, and a key factor in crawler e ective-ness.
Previous web evolution models do not account for the information longevity characteristics we found on the web, so we proposed a new evolution model that  ts closely with actual observations.
We brought our  ndings to bear on the recrawl scheduling problem.
We began by formulating a general theory of optimal recrawling in which the optimization objective is to maximize correct information   time.
The theory led us to two simple online recrawl algorithms that target longevous content, and outperform previous approaches on real web data.
We thank Vladimir O tserov for his invaluable assistance with data gathering and analysis.
Also, we thank Ravi Ku-mar for suggesting a way to interpolate divergence values.
