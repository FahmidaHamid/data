Online social media content and associated services comprise one of the fastest growing segments on the Web.
Such content includes social bookmarking (Delicious1), social photo sharing (Flickr2), social video sharing (Youtube3), and many other forms of user-generated content.
This content is different from the traditional content on the web (web pages) in style, quality, authorship, and explicit support for social graphs.
The explicit support for social interactions between users, such as posting comments, rating content, and responding to questions and comments makes the social media unique and requires new techniques for analyzing and retrieving relevant content.
Question-Answering (henceforth QA) is a form of information retrieval where the users  information need is speci ed in the form of a natural language question, and the desired result is self-contained answer (not a list of documents).
QA has been particularly amenable to social media, as it allows a potentially more e ective alternative to web search by directly connecting users with the information needs to users willing to share the information directly.
Some very successful examples of community sites organized around Question Answering are Yahoo!
Answers4, and Naver5.
In these portals users can express speci c information needs by posting questions, and get direct responses authored by other web users, rather than browsing results of search engines.
Both questions and responses are stored for future use by allowing searchers to  rst attempt to locate an answer to their question, if the same or similar question has been answered in the 1http://del.icio.us/ 2http:// ickr.com/ 3http://youtube.com/ 4http://answers.yahoo.com/ 5http://www.naver.com/
 for existing answers for a given question becomes increasingly crucial to avoid duplication, and save time and e ort for the users.
For example, Yahoo!
Answers now has tens of millions of users, and stores hundreds of millions of answers to previously asked questions.
These databases of questions and respective answers is an invaluable resource for speci c information needs not well served by general-purpose search engines.
In particular, today s search engines are not yet generally capable to automatically generate brief but precise summaries that integrate information from multiple sources, or to answer queries that require deep semantic understanding of the query or the document.
For example, consider a query  When is the hurricane season in the Caribbean? , which we submit to both Yahoo!
Answers and Yahoo!
Web search engine.
Figure 1 and 2 show the best answer and top 5 search results respectively.
From those two  gures, it is easy to see that Yahoo!
Answers supply one brief answer but with high quality while for Yahoo!
Search results people still need to click into the webpage to  nd needed information.
Thus, QA sites and search services provides an alternative channel for obtaining information more quickly and more precisely on the web.
Figure 1: The question  When is hurricane season in Caribbean?  in Yahoo!
Answer and its best answer.
However,  nding relevant answers of a new query in QA archives is a di cult task that is distinct from web search and web question answering.
The main di erences are the availability of explicit user feedback and interactions, explicit authorship and attribution information, and organization of the content around topical categories and past ques-Figure 2: Top 5 results when searching  When is hurricane season in Caribbean?  on Yahoo!
Search.
tion threads.
Furthermore, the quality of the content varies even more than the quality of the traditional web content.
A large fraction of the content re ects often unsubstantiated opinions of users, which are not useful for factual information retrieval and question answering.
Hence, retrieving the correct factual answers to a question requires determining both relevance and quality of answer candidates.
As has been shown in recent work, user interactions, in particular explicit feedback from users, can provide a strong indication of the content quality [2].
Examples of such feedback include the selection of the  best  answer by the original question author, as well as the  thumbs up  and  thumbs down  ratings from other users.
What is not clear from previous work is how to integrate explicit user feedback information and relevance into a uni ed ranking framework to retrieve answers that are relevant, factual, and of high quality.
In this paper we present a ranking framework to take advantage of user interaction information to retrieve high quality relevant content in social media.
We focus on community Question Answering to explore how to integrate relevance, user interaction and community feedback information to  nd the right factual, well-formed content to answer a user s query.
Our contributions in this paper include:   An e ective framework for learning ranking functions for question answering that incorporates community-based features and user preferences (Sections 3).
  Analysis of user interactions in community QA with insights into the relationship between answer quality, relevance, and factual accuracy (Section 5).
In summary, retrieving correct, factual, and well-formed answers from community QA archives is a crucial and challenging task.
In this work we make signi cant progress towards this goal, allowing our system to  nd facts in the crowd with accuracy substantially higher than the current state-of-the-art.
Social media services provide popular web applications such as photo sharing (Flickr), social bookmarking (Delicious), and video sharing (Youtube), and, more recenlty,
 hoo!
Answers and Naver.
Question answering over community QA archives is di erent from traditional TREC QA [22], and applying QA techniques over the web [5].
The most signi cant di erence is that traditional QA operates over a large collection of documents (and/or web pages) whereas we are attempting to retrieve answers from a social media archive with a large amount of associated user-generated metadata [2].
This metadata (such as explicit user feedback on answer quality) is crucial due to the large disparity of the answer quality, as any user is free to contribute his or her answer for any question.
Because of the explosive rise in popularity of Yahoo!
Answers and Naver and other sites, community Question Answering has recently become an active area of research.
This area of QA can be traced to the research on answering questions using Usenet Frequently Asked Questions (FAQ) archives [7, 19, 17, 20].
Usenet FAQs could be viewed as precursors to today s community QA archives that ful lled a similar role but lacked intuitive support for explicit user feedback and other user interactions.
More recently, Jeon et al. [10] presented retrieval methods based on machine translation models to  nd similar questions from Naver, a community QA service, but did not take quality of answers into consideration.
Jurczyk and Agichtein [14] show an application of HITS algorithm to a QA portal.
Zhang et al. [24] analyze data from an online forum, seeking to identify users with high expertise.
Su et al. [21] analyzed the quality of answers in QA portals and found that the quality of each answers varies signi cantly.
Jeon et al. [11] built a model for answer quality based on features derived from the spe-ci c answer being analyzed.
Recently, Agichtein et al. [2] explored user interaction and content-based lexical features to identify high-quality content, but did not address how to retrieve relevant answers.
In contrast, we propose a framework to integrate the interaction features into the answer retrieval.
Our work is similar in spirit to integrating user interactions and feedback into web search [12, 13, 15, 1].
For example, implicit feedback in the form of result clickthrough was shown to be helpful for web search ranking.
The main difference of our current work is that we focus on question answering, which is a more precise form of search compared to general-purpose web search explored in the past.
As another departure from previous work, we do not assume the existence of large amounts of expertly labeled relevance judgments, but instead automatically generate relevance labels.
These di erences in setting provide an interesting challenge for learning ranking functions.
Many models and methods have been proposed for designing ranking functions, including vector space models [3], probabilistic models [4] and recently developed language models [18].
Some advanced machine learning methods are also incorporated to learn ranking such as SVM and gradient boosting [12, 23].
In recent years, once relevance judgments are extracted from clickthrough data, several new learning frameworks, e.g.
RankSVM [12], RankNet [6], Rank-Boost [8] have been created to utilize preference data to enhance ranking.
Our work is also related to the recent research of Ko et al. [16] that propose a probabilistic framework for answer selection for traditional question answer-ing.In this paper, we adapt a regression framework which is based on Gradient Boosting [25, 9].
We now present our ranking framework in more detail.
Ranking functions are at the core of an e ective QA retrieval system.
In this section, we will explore a learning-based approach to the design of the ranking functions.
We will focus on the speci c characteristics of Yahoo!
Answers and discuss how to employ user interactions and community-based features in Yahoo!
Answers to rank the answers.
We start with a more precise de nition of the problem of QA retrieval, and then describe the di erent interactions available in a state-of-the-art community QA portal.
Then we discuss how to represent textual elements and community-based elements in Yahoo!
Answers as a vector of features.
We then show how to extract preference data on answers from user interactions with Yahoo!
Answers.
Based on the extracted features and preference data, we apply the regression-based gradient boosting framework [25] to the problem of learning ranking function for QA retrieval.
In QA systems, there are a very large amount of questions and answers posted by a diverse community of users.
One posted question can solicitate several answers from a number of di erent users with varying degree of relevance to the posted question.
We abstract the social content in QA system as a set of question-answer pairs: (cid:104)qsti, ansj i(cid:105) where qsti is the ith question in the whole archive of the QA system and ansj i is the jth answer to this question.
Given a user query, our goal is to order the set of QA pairs according to their relevance to the query, and the ordering is done by learning a ranking function for triples of the form, (cid:104)qrk, qsti, ansj i(cid:105), where qrk is the k-query in a set of queries.
We will  rst discuss how to e ectively extract features and preference data for each triple of the above form and then discuss a regression-based gradient boosting methods for learning a ranking function for QA retrieval.
Community QA is a particularly popular form of social media, drawing millions of users to ask and answer each others  questions.
Unlike other social media services, community QA services such as Yahoo!
Answers provide distinct types of interactions among users that are speci c to the QA domain.
Users in Yahoo!
Answers do not only ask and answer questions, but also actively participate in regulating the system.
A user can vote for answers of other users, mark interesting questions and even report abusive behavior.
Therefore, a Yahoo!
Answers user has a threefold role: asker, answerer and evaluator.
And there are respectively three types of user interaction activities: asking, answering and evaluating.
We summarize elements and interactions in Yahoo!
Answers in Figure 3.
The rest of this paper will focus on how to utilize this information for QA retrieval.
In addition to facilitating the community question answering process, a community QA system must support e ec-tive search of the archives of past questions and answers.
In fact, one bene t of Yahoo!
Answers to users and web searchers is precisely the immense archive of hundreds of millions of answers to past questions, with the associated community feedback.
Searching this archive allows users
 features describing similarity between these three elements.
(We describe them as textual features in Table 1) User Interaction Features: As discussed before, there are three kinds of roles each user in QA system may play, namely Asker, Answerer and Evaluator.
Figure 3 shows the interactions between these three roles.
Askers post their questions on QA system in the hope that other users answer these questions.
Answerers submit their answers to the question in the QA system.
Some answers have high quality while the majority are not useful.
Evaluators give positive and negative votes for an answer after they read an answer and related question in the QA system.
In the community of QA system, each user can play all of these three roles at the same time.
For each user in the user community of a QA system, there are several features to describe his or her activities, such as the number of questions he or she asked, the number of answers he or she posted, the number of best answers he or she posted etc.
These features to certain extent can approximate the user s expertise in the QA community.
And user s expertise within certain topics can in turn indicate the quality of his or her answers to the questions about certain topics.
For example, in a query-question-answer triple, if answerer tend to post useful answers or even best answers in the past, he or she is more likely to give answers of high quality this time.
Similarly reputation of askers and eval-uators can also indicate quality of answers.
Therefore, in each query-question-answer triple, we also extract features indicating user s activities in the QA system.
As there is no information about evaluators in Yahoo!
Answer, we only consider features for askers and answerers, such as  number of questions the asker asked in the community ,  number of best answers the answerer posted in community .
These features are listed in Table 1.
Data One of the user interactions in a QA community, especially in Yahoo!
Answers, is their evaluations for the existing answers.
In Yahoo!
Answers, after reading existing answers for a question, user can give his or her judgment as the evaluation for the answers.
If he or she considers the answer as useful and of high quality, he or she can add a plus vote to this answer.
Otherwise, a minus votes may be added to the answer.
We examine users evaluation data and extracted a set of preference data which can be used for ranking the answers as follows.
For each query qr, under the same question qst, we consider two existing answers ans1 and ans2 from Yahoo!
Answers.
Assume that in the users evaluation data, ans1 has p1 plus votes and m1 minus votes out of n1 impressions while ans2 has p2 plus votes and m2 minus votes out of n2 impression.
We want to consider answer pairs ans1 and ans2 to see whether ans1 is preferred over ans2 in terms of their relevance to the question qst.
To this end, we assume that plus votes obey binomial distribution, showed as following: pk(1   p)n k n k (cid:181) (cid:182) B(k; n, p) = We use the approach in [25] and apply likelihood ratio test to examine whether a pair of answers is signi cant or not, i.e., whether there are enough votes to compare the pair.
In Figure 3: Elements and interactions in Yahoo!
Answers: Askers post questions on Yahoo!
Answers; Several users-answerers read questions and supply their answers to them; Users can also read these answers and give evaluations/votes; External users can submit queries to Yahoo!
Answers and receive relevant questions with answers.
the bene t of community feedback and precise and speci c answers for many information needs not supported by general web search.
However, because of the disparity in answer quality, and the di culty in mapping user information needs to past questions, the problem of retrieving both relevant and high quality factual answers requires the integration of both content features (to estimate relevance) as well as user interaction and community features (to estimate quality).
Having introduced our setting, we now describe our features for answer retrieval.
We follow the general practice in information retrieval and represent each query-question-answer triple (qr, qst, ans) as a combination of textual features (i.e., textual similarity between query, question and answers), statistical features (i.e., independent features for query, question and answers) and social features (i.e., user interaction activities and community-based elements).
In Yahoo!
Answers, there is an additional important type of user feedback   user evaluation in the form of votes (represented as the  thumbs up  and  thumbs down  metaphors).
We can use this information to infer preference relevance judgments for the set of answers.
In the following, we discuss features and preference data extraction in more details.
actions as Features Textual Elements Features: As we mentioned before, there are three textual elements in Yahoo!
Answers: questions, answers and queries (showed in Figure 3).
We  rst design features from each of these three elements independently, such as  number of tokens for a query  for query,  how long has the question been posted  for question,  number of received votes for an answer  for answer etc.
(We describe them as statistical features in Table 1) Then, we also extract textual features from relationship between questions, answers and queries.
For example, the number of overlapping terms and token number ratio be-
Overlapping terms between query and question Overlapping terms between query and answer Overlapping terms between question and answer Ratio between number of tokens in query and question Ratio between number of tokens in query and answer Ratio between number of tokens in question and answer Textual Features of Questions, Answers and Queries Query-Question Overlap Query-Answer Overlap Question-Answer Overlap Query-Question length ratio Query-Answer length ratio Question-Answer length ratio Statistical Features of Questions, Answers and Queries Query Length Question Length Answer Length Question Lifetime Answer Lifetime Yahoo!
Question Rank Yahoo!
Answer Rank Question Popularity Votes Number User Interaction/Social Elements Features Asker Total Points Asker Total Answers Asker Best Answer Number of Questions Asked by Asker Number of Questions Resolved by Asker Asker stars received Answerer Total Points Answerer Total Answers Answerer Best Answer Number of Questions Asked by Answerer Number of Questions Resolved by Answerer How many questions are resolved by the Answerer in Yahoo!
Answers Answerer stars received Points calculated by Yahoo!
based on Asker s activity history How many answers does the asker submit in Yahoo!
Answers How many best answers does the asker propose in Yahoo!
Answers How many questions does the asker post in Yahoo!
Answers How many questions are resolved by the asker in Yahoo!
Answers How many stars does the asker receive in Yahoo!
Answers Points calculated by Yahoo!
based on Answerer s activity history How many answers does the Answerer submit in Yahoo!
Answers How many best answers does the Answerer propose in Yahoo!
Answers How many questions does the Answerer post in Yahoo!
Answers Number of tokens in query Number of tokens in question Number of tokens in answer How long has this question been posted How long has this answer been posted The rank of question in Yahoo!
Answers The rank of answer in Yahoo!
Answers How many answers are received under the question Number of votes for answer How many stars does the Answerer receive in Yahoo!
Answers particular we compute the following statistic,   = B(p1 + p2; n1 + n2, (p1 + p2)/(n1 + n2)) B(p1; n1, p1/n1)B(p2; n2, p2/n2)    2 p1 p1+m1+s with For a pair of answers ans1 and ans2, when the above value is greater than a threshold, we say the pair is signi cant.
If ans1 and ans2 form a signi cant pair, we then extract a preference for the pair by comparing p2+m2+s , where s is positive constant, i.e., if the former value is bigger than the later one, then we say ans1 is preferred over ans2 which is denoted by ans1 (cid:194) ans2, and vice versa.
Besides users evaluation information, we can also extract preference data from labeled data.
For two query-question-answer items with the same query, p2 (qr, qst1, ans1) (qr, qst2, ans2), let their feature vectors be x and y, respectively.
If ans1 has a higher labeled grade than ans2, we include the preference x (cid:194) y while if ans2 has a higher labeled grade than ans1, we include the preference y (cid:194) x.
We will discuss how to obtain labeled data in details in section 4.1.
ence Data Once the features and preference data are extracted, the next question is how to use them for the purpose of learning a ranking function for QA retrieval.
We apply a framework for solving ranking problems from preference data developed in [25].
This framework proposes a squared hinge loss function for learning ranking functions from preference data; it also presents an algorithm that adapts functional gradient descent for minimizing the loss function.
We now brie y describe the basic idea of the algorithm in [25].
Suppose the set of available preferences is S = {(cid:104)xi, yi(cid:105) | xi (cid:194) yi, i = 1, ..., N}.
Here each (cid:104)x, y(cid:105)   S, x, y denote the feature vectors for two query-question-answer triples with the same query.
x (cid:194) y means that x is preferred over y, i.e. x should be ranked higher than y.
In other words, the answer in x is considered more relevant than that in y with respect to the same query in both triples.
In [25], the problem of learning ranking functions is cast as the problem of computing a function h, such that h match the given set of preferences, i.e., h(xi)   h(yi), if xi (cid:194) yi, i = 1, ..., N , as much as possible.
The following objective function (squared hinge loss function) is used to measure the risk of a given ranking function h,6
 of the total number of contradicting pairs in the given preference data with respect to the function h. We say x (cid:194) y is a contradicting pair with respect to h if h(x) < h(y).
i=1 R(h) =

 (max{0, h(yi)   h(xi) +  })2, and we need to solve the following minimization problem h HR(h), min where H is a function class, chosen to be linear combinations of regression trees in our case.
The above minimization problem is solved by using functional gradient descent discussed in [9].
We summarize the algorithm for learning ranking function h using gradient boosting (GBrank) as follows: Algorithm GBrank: Start with an initial guess h0, for k = 1, 2, ...
arate S into two disjoint sets, S + = {(cid:104)xi, yi(cid:105)   S|hk 1(xi)   hk 1(yi) +  } and
 = {(cid:104)xi, yi(cid:105)   S|hk 1(xi) < hk 1(yi) +  }
 ing Tree [9] and the following training data {(xi, hk 1(yi) +   ), (yi, hk 1(xi)     )|(cid:104)xi, yi(cid:105)   S }
 hk(x) = khk 1(x) +  gk(x) k + 1 where   is a shrinkage factor.
Two parameters need to be determined: the shrinkage factor and the number of iterations, this is usually done by cross-validation [25].
This section presents our evaluation setup.
First we describe our dataset including the queries and the corresponding corpus of questions and answers.
Then we describe our evaluation metrics (Section 4.2) and the ranking methods to compare (Section 4.3) for the experimental results reported in Section 5.
Factoid questions from the TREC QA benchmarks We use factoid questions from seven years of the TREC QA track evaluations (years 1999 2006)7 for the experiments reported in Section 6.
It is worth noting that TREC questions from the years 1999 to 2003 are independent of each other: each question is self-contained and we submit directly as the query.
Starting from 2004, however, the questions are organized in groups with a  target .
For those questions, we submit their  target  as well as the questions themselves.
In total, approximately 3,000 factoid TREC questions were compiled as the initial set of queries.
Since we need some candidate answers from Yahoo!
Answers to estimate how well di erent ranking functions perform, we select the 1250 TREC factoid questions that have at least one similar question in the Yahoo!
Answers archive.
7http://trec.nist.gov/data/qa.html Question-answer collection dataset Our dataset was collected in order to simulate a user s experience with a community QA site.
We submit each TREC query to the Yahoo!
Answers web service8 and retrieve up to 10 top-ranked related questions according to the Yahoo!
Answers ranking.
For each of these Yahoo!
questions, we retrieve as many answers as there are available for each question thread.
There are, in total, 89642 (cid:104)query, question, answer(cid:105) tuples.
17711 tuples (19.8%) are labeled as  relevant  while 71931 (81.2%) are labeled as non-relevant.
Relevance Judgments In our experiment, the data are labeled in two ways: by using the TREC factoid answer patterns, and, independently, manually in order to validate the pattern-based automatic labels.
For automatic relevance labels we use the available regular expression answer patterns for the TREC factoid questions.
We check every answer s text body, and if the text matches one of the answer patterns, we consider the answer text to be relevant, and non-relevant otherwise.
In order to validate the accuracy of our automatically-assigned relevance labels, we independently labeled a number of answers by hand.
The manually labeled answers were compared with the automatically generated labels, resulting in over 90% agreement between the automatic and manual methods.
In the cases of disagreements were due to the excessive strictness of the answer patterns, and to the world changing (e.g., with a di erent correct answer for a question  Who is the prime minister of Japan. ).
This is not surprising, as some of the answer patterns were created years ago around the time of the TREC QA evaluation.
In summary, automatically generated labels, even though with some small degree of noise, nevertheless exhibit high agreement with manual relevance judgments, and serve as a good proxy for comparing rankings.
We adapt the following information retrieval metrics to evaluate the performance of the ranking function.
  Mean Reciprocal Rank(MRR): The MRR of each individual query is the reciprocal of the rank at which the  rst relevant answer was returned, or 0 if none of the top N results contained a relevant answer.The score for a sequence of queries is the mean of the individual query s reciprocal ranks.
Thus, MRR is calculated as (cid:88) q Qr

 |Qr|
 rq where Qr is a set of test queries, rq is the rank of the  rst relevant document for q.
  Precision at K: for a given query, P (K) reports the fraction of answers ranked in the top K results that are labeled as relevant.
In our setting, we require a relevant answer to be labeled  matched  for TREC pattern.
For this metric, the position of relevant answers within the top K is irrelevant, while it measures overall user potential satisfaction with the top K results.
  Mean Average of Precision(MAP): Average precision for each query is de ned as the mean of the precision at K values calculated after each relevant answers 8http://developer.yahoo.com/answers/
 mean of average precisions of all queries in the test set.
This metrics is the most commonly used single-value summary of a run over a set of queries.
Thus, MAP is calculated as (cid:80)N r=1(P (r)   rel(r)) (cid:88) |Rq|

 |Qr| q Qr where Qr is a set of test queries, Rq is the set of relevant document for q, r is the rank, N is the number retrieved, rel() is a binary function on the relevance of a given rank, and P () is precision at a given cut-o  rank.
To evaluate the Q&A retrieval quality, we compare the quality of following methods:   Baseline Yahoo(baseline1): In this method, we adapt default ranking in Yahoo!
Answers.
Answers to a particular question are ordered by posting date.
The older one is ranked higher except that best answers always come  rst.
  Baseline Votes(baseline2): In this method, similar to our baseline1, we let best answers always be on top of the answer list.
However, following answers are ranked in decreasing order by number of (positive votes - negative votes) received.
If there is no vote for some answers, we order them by Yahoo!
default ranking.
  GBRanking: Ranking function with community/social features: this is our method presented in Section 3.4 For Yahoo!
Answers, since we  rst get a list of Yahoo!
questions for one TREC query, and each of these Yahoo!
questions has its own answers, there are multiple alternatives for calculating MRR, Precision and MAP values for Baseline Yahoo and Baseline Votes.
First, we need to introduce some nomenclature: for each TREC query T Q, we retrieve a list of related questions from Yahoo!
Answers Y Qa, Y Qb... (as shown in Figure 4).
After clicking on one of these questions, we get the answers to each question, e.g., Y Qa, as Y Q1 a , as shown in Figure 1: a, Y Q2 a...Y Qn   MRR MAX: Calculate MRR value for each Y Qa, Y Qb... and use the highest value as this T Q s MRR.
This baseline simulates an  intelligent  user who always selects the most relevant retrieved Yahoo!
question thread  rst (as measured by the corresponding MRR for the thread).
  MRR STRICT: Calculate MRR value for each Y Qa, Y Qb... and use the their average value as this T Q s MRR.
This baseline simulates a user who blindly follows the Yahoo!
Answer s ranking and examines retrieved question threads and corresponding answers in the order they were originally ranked.
  MRR RR(round robin): Use Y Qa s  rst answer as T Q s  rst answer, Y Qb   rst answer as T Q s second answer and so on, then calculate this T Q s MRR.
This baseline simulates a  jumpy  user who believes that answers that come  rst, no matter to which questions, are always better, and jumps between question threads looking at the top-ranked answers for each tread in order of the original ranking.
Figure 4: Top 5 results when searching  When is hurricane season in Caribbean?  on Yahoo!
Answers.
The variants for the other two metrics, Precision and MAP (namely, PREC MAX, PREC STRICT,
 MAP RR), are calculated similarly.
In summary, MRR MAX (and PREC MAX and MAP MAX) represent the upper bound on Yahoo!
Answers  accuracy (with the current retrieval and ranking algorithms) from the perspective of an intelligent searcher.
This is an extremely strong family of baseline metrics, as it assumes an  oracle  that always makes the right decision to click on the question threads that contain the correct answer in the highest ranking position.
To learn the ranking function, we generate the training and testing data as follows: we randomly select 400 TREC queries from total 1250 TREC queries and collect all the related QA for these 400 queries.
We use tenfold cross validation to perform the training of the proposed ranking function using the algorithm introduced above.
Tenfold cross validation involves dividing the judged data set randomly into 10 equally-sized partitions, and performing 10 training/testing steps, where each step uses 9 partitions to train the ranking function and the remaining partition to test its e ectiveness.
Note that the following results were done on this smaller set train the ranking function - the main evaluation will be performed in the next section, over the remaining 850 TREC questions that were not used in training in any way.
Figure 5 reports the Precision at K for the holdout validation data against each iteration of our learning algorithm.
It can be clearly seen that Precision increases for the  rst
 tern labels respectively.
While the accuracy when evaluating against manual labels is slightly lower than automatic labels, the di erences are not substantial, which implies that our algorithm still generates a nearly-optimal model even when trained on noisy relevance labels.
Furthermore, the high correspondence of automatic and manual label accuracy results validates our decision to use only automatic labels for the remaining experiments, to enable experiments on the larger scale.
To gain a better understanding of the important features for this domain we perform an ablation study on our feature set to explore which features are signi cant to answers ranking.
Figure 7: Precision at K for feature ablation study Figure 5: Precision at 1, 3, 5 for testing queries against GBrank iterations 60 iterations, after which the algorithm converges and addi- tional iterations are not helpful.
Figure 6: Precision at K for testing queries with manual labels and labels generated by TREC pattern As mentioned in Section 4.1, the relevance judgments was obtained by matching an answer with TREC answer patterns.
We have also found that 90% of items were given the same labels under both manual labeling and TREC pattern labeling and the remaining 10% of automatic labels were erroneous.
To show that our learning framework is robust, we experiment with training on the noisy automatically generated labels, and testing on the smaller set of  gold standard  manually assigned relevance labels.
For this experiment we used 50 queries (testing data) which have been labeled manually.
Then, we randomly select the other 350 TREC queries (training data) and related questions and answers to train the ranking function.
Figure 6 shows the Precision at K Figure 8: Precision at K without incorporating user evaluations as preference data As shown in Table 1, there are three major categories of our feature set: textual features, community features and statistical features.
Figure 7 reports the Precision at K
 respectively.
It is easy to see that removing both textual features and community features cause a signi cant decreasing on precision.
While there is a slight reduction on precision when removing statistical features, it is clear that these features are not as important as the textual and community features.
Interestingly, textual features are less important for Precision at 1.
We hypothesize that for the top result it is more important for an answer to be chosen as  best  by the asker (one of the community features), than to have appropriate textual characteristics.
In addition, we also test the e ectiveness of preference data from users evaluations.
In order to test its e ectiveness, we learn a ranking function without incorporating preference data from users evaluations.
Figure 8 shows the Precision at K of this new ranking function.
From this  gure, we can see that users evaluations play a very important role in learning ranking function.
In this experiment, we train our ranking function on the whole training data (i.e., the 400 TREC queries from the previous experiments) and test on the remainder holdout data of 850 TREC queries and associated community QA pairs.
Figure 10: Precision at K for GBrank, base-line2 MAX, baseline2 RR and baseline2 STRICT for vary K Table 2: Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) for GBrank and other metrics for baseline1
 MRR Gain MAP Gain













 GBrank
 Table 3: Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) for GBrank and other metrics for baseline2
 MRR Gain MAP Gain













 GBrank
 Figure 9: Precision at K for GBrank, base-line1 MAX, baseline1 RR and baseline1 STRICT for vary K Figures 9 and 10 illustrate the Precision at K of GBrank compared with the two baseline methods.
These  gures show that the precision of two baseline methods are nearly the same, while GBrank outperform both of them.
In particular, the Precision at 1 of GBrank is 76%, compared to
 also see that PREC MAX performs better than PREC RR and PREC STRICT, and GBrank has the similar performance with PREC MAX the values of K larger than 3.
In Table 2 and 3, we illustrate the MAP and MRR scores for two baseline methods as well as GBrank.
From these two tables, it is clear that MAX can perform better than the other two metrics for baseline, but GBrank reaches much better performance than all the metrics for two baseline methods.
In particular, GBrank achieves a gain of about To understand how GBRank can outperform an  oracle  baseline, consider that the ordering of answers within a question thread remains  xed (either by date   as the default, or by decreasing votes).
In contrast, GBrank obtains a better ranking of answers within each question thread, as well as a global ranking of all answers.
Then, improved ranking within each Yahoo questions thread contributes to the higher score than MRR MAX.
Overall, applied on Yahoo!
Answers, our proposed framework achieves a signi cant improvement on the performance of QA retrieval over the Yahoo!
Answers  default ranking and the supported optional votes-based ranking.
In addition, from the experiment, we can  nd that our method is able to retrieve relevant answers at the top of results.
In summary, we have shown that GBRank signi cantly outperforms extremely strong baselines, achieving precision at 1 of over 76% and MRR of over

 Community question answering is transforming the way people search for information.
We have presented a robust, e ective method for retrieving factual answers from community QA archives, and have demonstrated our method to be signi cantly more e ective than the best possible accuracy a user can achieve when interacting with the current state-of-the-art question search on a major community QA site.
Furthermore, our large scale experiments demonstrate that our method is robust to noise in the automatically generated training preference judgments.
We have complemented our study with an analysis of the results to gain insight into the signi cant dimensions of fact retrieval from social media.
In particular, we found that textual features and community features are crucial, and that user feedback, while noisy, provides su cient relevance judgment to improve the learning of the ranking functions.
By signi cantly improving the accuracy of retrieving well-formed, factual answers, our work has the potential to transform how users interact with community QA sites; to improve the experience by reducing duplicate questions; and to better integrate question answering and search over QA archive with the mainstream web search results.
In the future, we plan to extend this work beyond factoid question answering to complex questions and information needs.
We also plan to extend our techniques to gracefully blend the results of social media search with organic web search results for the appropriate information needs, such as question answering.
In summary, our work is a crucial component for factual information seeking in the increasingly important social media environment.
port of the Yahoo!
Answers team to allow extensive usage of the Answers search API.
We also thank Ke Zhou from Shanghai Jiaotong University for letting us use his gradient boosting code in implementing our ranking method.
