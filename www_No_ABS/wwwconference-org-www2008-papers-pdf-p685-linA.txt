Data from many social network datasets, including paper co-authorship networks and the blogosphere, is a graph where nodes represent individuals (e.g., club members, authors, and bloggers) and edges represent the relationship and interactions among individuals (e.g., interactions in a club, co-authorship, and hyperlinks in blogs).
In such social networks, individuals form communities by building relationships and interactions with each other.
The analysis of these communities (membership, structure and temporal dynamics) is an important research issue.
Traditional analysis of social networks treats the network as as a static graph, where the static graph is either derived from aggregation of data over all time or taken as a snapshot of data at a particular time.
These studies range from well-established social network analysis [22] to recent successful applications such as HITS and PageRank [7, 15].
However, this research omits one important feature of communities in networked data  the temporal evolution of communities.
By ignoring community evolution, prior works have overlooked a key aspect of online communities.
More recently, there has been a growing body of work on the analysis of communities and their temporal evolution in dynamic networks [1, 8, 9, 10, 11, 16, 19, 21].
However, a common weakness in these studies, as we will discuss in detail in related work, is that communities and their evolutions are studied separately usually community structures are independently extracted at consecutive timesteps and then in retrospect, evolutionary characteristics are introduced to explain the di erence between these community structures over time.
Such a two-stage approach may make sense when the community structure is unambiguous (e.g., when the community a liation is available).
However, more often than not, data from real-world networks are ambiguous and subject to noise.
Under such scenarios, if an algorithm extracts community structure for each timestep independently of other timesteps, it often results in community structures with high temporal variation.
Consequently, undesirable evolutionary characteristics may have to be introduced in order to explain the high variation in the community structures.
Therefore, we argue that a more appropriate approach is to analyze communities and their evo-lutions in a uni ed framework where the community structure provides evidence about community evolutions and at the same time, the evolutionary history o ers hints on what community structure is more appropriate.
For example, a community structure that introduces dramatic evolutions in a very short period of time is less desirable.
analysis techniques is that an individual is usually assigned to only one community at a time.
On the contrary, an individual may be engaged in multiple communities at the same time.
For example, a blogger who is a dance guru may also be an amateur photographer at the same time.
Because of this, an individual who usually participates in multiple communities should be assigned to multiple communities at the same time.
Therefore, instead of a hard community partition, we argue that a soft community membership is more informative, as it provides more details about how an individual participates in each of the communities.
In this paper, we propose a systematic framework for analyzing communities and their evolutions in dynamic networks, and we term our framework FacetNet 1.
Our main contributions are threefold:
 munities and their evolutions in a uni ed process.
In our framework, the community structure at a given timestep t is determined both by the networked data at t and by the historic community evolution patterns.
As a result, the discovered communities and their evo-lutions are more robust to noise and more reasonable (e.g., dramatic change in a short time is unlikely).
from static graphs to dynamic networks.
In contrast to a hard community partition, in our framework an individual can participate in multiple communities at the same time and with di erent participation levels.
Similarly, an observed relationship is generated due to a combined e ect from various communities.
Based on the soft community membership, we further de- ne two novel concepts Community Net and Evolution Net to represent community structures and their evolutions, respectively.
to converge to (local) optimal solutions to the proposed formulation.
We prove the correctness and convergence of our algorithm and show that this algorithm has low time complexity.
We also provide principled solutions to some practical issues, such as how to determine the number of communities and how to handle adding and removing of individuals in a dynamic network.
We use synthetic and real datasets (including a blog dataset and a paper co-authorship dataset) to demonstrate that compared to traditional methods, our framework provides more reasonable results on communities and their evolu-tions.
We also show that our framework is able to discover interesting insights in dynamic networks that are not directly obtainable from existing methods.
The rest of the paper is organized at follows.
First, we discuss related work.
In Section 2, we describe our basic framework in detail.
In Section 3, we introduce extensions of our framework to handle some practical issues.
In Section 4, we provide experimental studies.
Finally in Section 5, we give the conclusion.
Communities and EvoluTions in dynamic NETworks .
for  a Framework for Analyzing stands Related Work Community formation has been extensively studied in various research areas such as social network analysis, Web community analysis, computer vision, etc.
In social network analysis, an important research topic is to identify cohesive subgroups of individuals within a network where cohesive subgroups are de ned as  subsets of actors among whom there are relatively strong, direct, intense, frequent, or positive ties  ([22]).
Many approaches, such as clique-based, degree-based, and matrix-perturbation-based, have been proposed to extract cohesive subgroups from social network [22].
Communities also play an important role in Web analysis.
For example, Flake et al. [6] de ned Web communities as  a set of sites that have more links to members of the community than to nonmembers , and proposed algorithms to identify Web communities based on a maximum  ow/minmum cut framework.
Newman et al. [13] de ned a metric called modularity measure to quantify the strength of community structure which we will discuss in detail in a later section.
In computer vision, community extraction is closely related to image segmentation problem.
One e ective method in this area is the spectral clustering algorithm [4, 5, 18, 25] where the eigenvectors of certain normalized sim ilarity matrices are used for the clustering purpose.
Later, White et al. [23] pointed out the close relationship between Newman s modularity and the spectral clustering and proposed several algorithms to combine the two approaches.
Yu et al. [24] proposed a novel clustering framework on graphs where the cluster memberships are assigned in a probabilistic way.
In Yu s framework, cluster memberships can be extracted in di erent resolutions, representing local or global cluster structures.
A common issue in all the above studies is that they only analyzed static networks where no temporal analysis is used for evolution study.
Another issue in these studies is that they treat community extraction as a graph partition problem and therefore always result in hard community memberships, which disallows an individual to participate multiple communities at the same time.
Recently, there exists a growing body of literature on analyzing communities and their evolutions in dynamic networks.
Kumar et al. [8] studied the evolution of the blogo-sphere as a graph in terms of the change of characteristics, (such as in-degree, out-degree, strongly connected components), the change of communities, as well as the burstiness in blog community.
Leskovec et al. [10] studied the patterns of growth for graphs in various  elds and proposed generators that produce graphs exhibiting the discovered patterns.
Palla et al [16] analyzed a co-authorship network and a mobile phone network, where both networks are dynamic, by using the clique percolation method (CPM).
Toyoda et al. [21] studied the evolution of Web communities from a series of Web achieves by de ning di erent types of community changes, such as emerge, dissolve, grow, and shrink, as well as a set of metrics to quantify such changes for community evolution analysis.
Spiliopoulou et al. [19] proposed a framework, MONIC, to model and monitor cluster transitions over time.
They de ned a set of external transitions such as survive, split, disappear, to model transactions among di erent clusters and a set of internal transitions, such as size and location transitions to model changes within a community.
Asur et al. [1] introduced a family of events on both communities and individuals to characterize evolution of communities.
They also de ned a set of metrics to measure the
 ties and individuals.
Sun et al. [20] proposed a parameter-free algorithm, GraphScope, to mine time-evolving graphs where the Minimum Description Length (MDL) principle is employed to extract communities and to detect community changes.
Mei et al. [12] extracted latent themes from text and used the evolution graph of themes for temporal text mining.
All these studies, however, have a common weak point community extraction and community evolution are analyzed in two separated stages.
That is, when communities are extracted at a given timestep, historic community structure, which contains valuable information related to current community structure, is not taken into account.
There are some recent studies on evolutionary embedding and clustering that are closely related to our work.
In [17], Sarkar et al. proposed a dynamic method that embeds nodes into latent spaces where the locations of the nodes at consecutive timesteps are regularized so that dramatic change is unlikely.
In [2], Chakrabarti et al. proposed the  rst evolutionary clustering methods where the cluster membership at time t is in uenced by the clusters at time t-1.
Chi et al. [3] extended similar ideas and proposed the  rst evolutionary spectral clustering algorithms.
They used graph cut as a metric for measuring community structures and community evolutions.
All these studies di er from our work in that they regularize the current community membership at time t by using historic community membership indirectly.
In Chakrabarti et al. s evolutionary hierarchical clustering algorithm, historic community structure a ects the tree-node merging step in the current time.
In their evolutionary k-means clustering algorithm, historic centroids a ect the k-mean process at the current time.
In Chi et al. s algorithms, certain eigenvectors, instead of the community structure, are regularized over time.
In the work of Sarkar et al., although the relationship among nodes in latent spaces is preserved over time, the issue of communities are not directly addressed.
In contrast, in our proposed framework, the community membership itself is directly regularized over time.
First, a note on notations.
In this paper, we use lowercase letters, e.g., x, to represent scalars, vector-formed letters, e.g., ~v, to represent vectors, and uppercase letters, e.g., W , to represent matrices.
Both wij and (W )ij represent the element at the i-th row and j-th column of W .
We use vec (W ) to denote the vectorization of W , i.e., stacking the columns of W into a column vector.
A subscript t on a variable, e.g., Wt or wt;ij, denotes the value of that variable at time t. However, to avoid notation clutter, we try not to use the subscript t unless it is needed for clarity.
We assume that edges in the networked data are associated with discrete timesteps.
We use a snapshot graph Gt(Vt, Et) to model interactions at time t where in Gt, each node vi   Vt represents an individual and each edge eij   Et denotes the presence of interactions between vi and vj .
Assuming Gt has n nodes, we use a matrix W   Rn n (which is short for Wt) to represent the similarity between nodes in Gt, where wij > 0 if eij   Et and otherwise wij = 0.
With-+ out loss of generality, we assume that Pi,j wij = 1.
Over time, the interaction history is captured by a sequence of snapshot graphs hG1,       , Gt,       i indexed by time.
As mentioned in the introduction, we want to analyze communities and their evolutions in a uni ed process.
That is, at time t, we prefer a community structure so that the community evolution from t-1 to t is not unreasonably dramatic.
To achieve this goal, we propose to use the community structure at time t-1 (already extracted) to regularize the community structure at current time t (to be extracted).
To incorporate such a regulation, we introduce a cost function to measure the quality of community structure at time t, where the cost consists of two parts a snapshot cost and a temporal cost: cost =     CS + (1    )   CT (1) This cost function is  rst proposed by Chakrabarti et al. [2] in the context of evolutionary clustering.
In this cost function, the snapshot cost CS measures how well a community structure  ts W , the observed interactions at time t. The temporal cost CT measures how consistent the community structure is with respect to historic community structure (at time t-1 ).
The parameter   is set by the user to control the level of emphasis on each part of the total cost.
A community structure at time t should  t W well, where W is the observed interaction (similarity) matrix at time t.
This requirement is re ected in the snapshot cost CS in the cost function Eq.
(1).
We  rst describe how we model the community structure and how we de ne the snapshot cost.
Assume there exist m communities at time t. We further assume that the interaction (similarity) wij is a combined e ect due to all the m communities.
That is, we approximate k=1 pk  pk i pk j, where pk is the prior probability that the interaction wij is due to the k-th community, pk i and pk j are the probabilities that an interaction in community k involves node vi and vj, respectively.
Written in a matrix form, we have W   X X T where X   Rn m is a non-negative matrix with xik = pk i wij using a mixture model wij  Pm + and Pi xik = 1.
In addition,   is an m   m non-negative diagonal matrix with  k = pk, where  k is short for  kk.
Matrices X and   (or equivalently, their product X ) fully characterize the community structure in the mixture model.
This model was  rst proposed by Yu et al. in [24].
v2 v1 v2 v1 v2 v1 v3 v6 v3 c1 c2 v4 v5 v4 v5 (a) (b) v6 v6 v3 x31 x32 x41 v4 w34    1x31x41 + 2x32x42 c1  1  2 c2 x42 v5 (c) Figure 1: Schematic illustration of soft communities: (a) the original graph, (b) the bipartite graph with two communities, and (c) how to approximate an edge (w34) In Fig. 1, we use a toy example with 6 nodes and 2 communities to illustrate this model of community structure.
For a general graph (a), we use a special bipartite graph (b) to approximate (a).
Note that (b) has two more nodes, i.e., c1 and c2, corresponding to the two communities.
However,
 between a node v and a community c), it has less degree of freedom and so it is a more parsimonious explanation of (a).
In (c), we show how an edge w34 is generated in the mixture model as the sum of  1x31x41 and  2x32x42.
Equivalently, we are approximating W , which has rank n, by a product in the form of X X T , which has rank m. Based on this model, we de ne the snapshot cost CS as the error introduced by such an approximation, i.e., CS = D(W kX X T ) where D(AkB) = Pi,j(aij log aij   aij + bij) is the KL-divergence between A and B.
So the snapshot cost is high when the approximate community structure X X T fails to  t the observed data W well.
bij
 In the cost function Eq.
(1), the temporal cost CT is used to regularize the community structure so that it is less probable for unreasonably dramatic community evolution from time t-1 to t. We propose to achieve this regularization by de ning CT as the di erence between the community structure at time t and that at time t-1.
Recall that the community structure is captured by X .
Therefore, with
 .
= Xt 1 t 1, the temporal cost is de ned as CT = D(Y kX ) where D is the KL-divergence as de ned before.
So the temporal cost CT is high when there is a dramatic change of community structure from time t-1 to t.
Putting the snapshot cost CS and the temporal cost CT together, we have an optimization problem as to  nd the best community structure at time t, expressed by X and  , that minimizes the following total cost cost =     D(W kX X T ) + (1    )   D(Y kX ) (2) subject to X   Rn m m non-negative diagonal matrix.
Solving this optimization problem is the core of our FacetNet framework.
, Pi xik = 1, and   being an m-by-+
 We now provide two interpretations to the cost function Eq.
(2), one from the point of view of information theory and the other from that of probabilistic generative model.
In information theory, the KL-divergence D(P kQ) is also known as the relative entropy, and it represents the information gain if we use the precise distribution P instead of the approximate model Q (where Q tries to model P ).
In our community structure, X X T is the marginal distribution induced from the bipartite model and it tries to approximate W .
As a result, D(W kX X T ) gives us the information gain (or the error introduced) from our community structure X X T to the true distribution W .
A higher information gain suggests a larger error introduced by X X T and therefore implies a higher snapshot cost CS.
Similarly, in D(Y kX ), Y represents the community structure at time t-1.
When we try to use the current community structure X  to explain Y , if the information gain from X  to Y is larger, then the change of community structure from time t-1 to t will be more dramatic, and therefore the temporal cost CT will be higher.
Next we provide a probabilistic interpretation of our framework by using a  rst-order Markov generative model.
The basic ideas are that (1) the currently observed data Wt is generated from the current community structure following a certain distribution and (2) the current community structure at time t is generated by using the community structure at time t-1 as the prior distribution.
Let Ut be the community parameters at time t and Ut 1 be those at time t-1.
The goal is then to estimate the unseen parameters Ut, given Wt and Ut 1, i.e., U   = arg max log P (Wt, Ut|Ut 1) Ut We further assume a  rst-order Markov model, as illustrated in Fig. 2, and we have P (Wt, Ut|Ut 1) = P (Wt|Ut)P (Ut|Ut 1).
Therefore the log-likelihood function can be written as L(Ut) = log P (Wt|Ut) + log P (Ut|Ut 1) (3)

 Dirichlet Ut 1 Ut multinomial Wt 1 Wt Figure 2: Schematic illustration of the probabilistic model, where U = (X,  ) In our model, Ut 1 = (Xt 1,  t 1) and Ut = (Xt,  t).
Under the above probabilistic model, we have the following theorem, whose proof is given in the Appendix.
Theorem 1.
Under the assumptions that (1) vec (Wt) follows a multinomial distribution with parameter  t, where  t = vec(cid:0)Xt tX T t (cid:1), and (2) vec (Xt t) follows a Dirichlet distribution with parameter  t, where  t =  vec (Xt 1 t 1) + 1 and   = (1    )/ ; the parameter estimation of Xt and  t by maximum a posterior (MAP) in Eq.
(3) is equivalent to that by minimizing the cost function in Eq.
(2).
In this subsection, we  rst provide an iterative algorithm to solve the optimization problem de ned by Eq.
(2) and then show the time complexity of our algorithm.
In our algorithm, we use the following update rules and as stated in the following theorem, in each iteration, the algorithm updates the values of X and   in such a way that the cost function de ned in Eq.
(2) is monotonically decreased.
cally decrease the cost function de ned in Eq.
(2) and therefore converge to an optimal solution to the objective function: + (1    )   yik (4) xik = 1,  k yik (5) + (1    )  Xi  k = 1.
wij    k   xjk (X X T )ij xik   xik   2   Xj then normalize such that Xi  k    k      Xij then normalize such that Xk wij   xik   xjk (X X T )ij The proof for the correctness and the convergence of the above update rules is skipped due to space limit.
We now show the time complexity for each iteration of the updates in Theorem 2.
The most time-consuming part is to compute (X X T )ij for all i, j   {1, .
.
.
, n}.
However, it turns out that we do not have to compute (X X T )ij for each pair of (i, j), thanks to the sparseness of W .
In W , the number of nonzero elements is the number of edges in the snapshot graph, which we denote by  .
Then for each nonzero wij , we compute the corresponding (X X T )ij, which takes O(m) time with m being the number of communities.
As a result, the total complexity is O( m).
If we consider m, the number of communities, to be a constant and if the degree of nodes in the snapshot graph is bounded by another constant, then the complexity is reduced to O(n), i.e., linear in the number of nodes in the snapshot graph.
After obtaining the solution to Eq.
(2) by using our algorithm in Theorem 2, here are the ways we utilize the solution to analyze communities and their evolutions.
i.e., Assume we have computed the result at time t-1, (Xt 1,  t 1), and the result at time t, i.e., (Xt,  t).
In addition, we de ne a diagonal matrix Dt, whose diagonal elements are the row sums of Xt t, i.e., dt;ii =Pj (Xt t)ij .
Then we claim that the i-th row of D 1 t Xt t indicates the soft community memberships of vi at time t. We illustrate this by using an example shown in Fig. 3.
Recall that in the bipartite graph at time t (the right side of Fig. 3(a)), the weights of edges connecting vi to c1, c2, and c3 represent the joint probability P (vi, c1), P (vi, c2), and P (vi, c3).
The
 part normalizes this joint probability to get P (c1|vi), P (c2|vi), and P (c3|vi), i.e., the conditional probability that vi belongs to c1, c2, and c3, respectively.
And this conditional probability is exactly the soft community membership we are looking for.
Furthermore, we can see that the i-th diagonal element of Dt provides information about the level of activity of vi at time t.
t
 t D 1 The community structure itself, on the other hand, is expressed by  tX T t Xt t.
For this we again look at the bipartite graph at time t (the right side of Fig. 3(a)).
Induced from this bipartite graph, Xt X T t gives a marginal distribution on the subgraph with nodes {v1, .
.
.
, v6} in order to approximate Wt.
In a dual fashion, also induced from c1 c2 v1 v2 v3 v4 v5 v6 c1 c3 c2 t 1 t (a) c1 c2 c3 t (b) c1 c2 t 1 c1 c3 c2 t (c) Figure 3: Schematic illustration of communities and their evolutions: (a) two bipartite graphs at time t-1 and time t (merged by vi s), (b) the Community Net at time t induced by the bipartite graph at time t, and (c) the Evolution Net from t-1 to t induced by the two bipartite graphs t D 1 this bipartite graph,  tX T t Xt t gives a marginal distribution on the subgraph with nodes {c1, c2, c3} (Fig. 3(b)) and this is exactly the community structure we are looking for.
We call this induced subgraph on the community nodes (i.e.,{c1, c2, c3}) a Community Net.
Note that to induce the community net, each node vi participates in all the communities, with di erent levels.
This is more reasonable than traditional methods in which each node can only contribute to a single community.
To derive the community evolutions, we align the two bipartite graphs, that at time t-1 and that at time t, side by side by merging the corresponding network nodes vi s, as illustrated in Fig. 3(a).
Then a natural de nition of community evolution (from ct 1;i at time t-1 to ct;j at time t) is the probability of starting from ct 1;i, walking through the merged bipartite graphs, and reaching ct;j.
Such a walking process produces what we call the Evolution Net to represent community evolutions, as illustrated in Fig. 3(c).
A simple derivation shows that P (ct 1;i, ct;j) = ( t 1X T t Xt t)ij and P (ct;j|ct 1;i) = (X T t Xt t)ij .
Again, each node and each edge contribute to the evolution from ct 1;i to ct;j.
That is, all individuals and all interactions are related to all the community evolutions, with di erent levels.
We believe this is more reasonable than how community evolutions are derived in traditional methods.
In tradition methods, usually the intersection and union of community members at di erent time are used alone to compute community evolu-tions, with a questionable assumption that all members in a community should be treated with identical importance.
t 1D 1 t 1D 1

 In this section we introduce two extensions to our basic framework in order to handle inserting and removing of individuals and to determine the number of communities in a dynamic network over time.
In real applications, it occurs very often that some new individuals join a dynamic network (e.g., a new author in a
 a blogger who stops blogging).
We provide the following heuristic techniques in our algorithm to handle such inserting and removing of nodes.
Assume that at time t, out of the n nodes in the network, n1 existing nodes are removed from and n2 new nodes are inserted into the network.
We  rst handle the n1 removed nodes by removing the corresponding n1 rows from Y in Equations (4) and (5) to get Y  .
Next, we scale Y   to get Y   so that Y   is a valid joint distribution, i.e., Y   = ij .
The basic idea behind this heuristic is that we assume the n1 nodes are randomly selected, independent of their community membership.
Under such an assumption, Y   is a conditional distribution, conditioning on the remaining n   n1 nodes in the network.
To add the n2 nodes, we pad n2 rows of zeros to Y   to get  Y .
This heuristic is actually equivalent to assuming that these n2 nodes have already existed at time t-1 but as isolated nodes.
Y  /Pij y 
 So far we have assumed that the number of communities, m, is given beforehand by the user.
However, such an assumption will limit the scope of application of our framework.
In this subsection we try to answer two questions: how to automatically determine the number of communities at a given time t and how to revise our framework to allow the number of communities to change in di erent timesteps.
In [13], Newman et al.
introduces an elegant concept, the modularity Q, to measure the goodness of a community partition Pm where Q is de ned as Q(Pm) = m Xk=1" A(Vk, Vk)
  (cid:18) A(Vk, V ) A(V, V ) (cid:19)2#
 If we allow di erent community numbers at time t and t-1, then we have to revise Eq.
(2) accordingly because in Eq.
(2), the term D(Y kX ) requires Y (the community structure at t-1 ) and X  (the community structure at t) to have the same number of columns and therefore the same number .
of communities.
To solve this issue, we  rst de ne Z = Xt 1 t 1X T t 1 and then revise the cost function to be cost =     D(W kX X T ) + (1    )   D(ZkX X T ) (8) The basic idea is that when the community numbers are different at time t and t-1, instead of regularizing the community structure itself, we regularize the marginal distribution induced by the community structure at time t (i.e., X X T , which approximates Wt) so that it is not too far away from that at time t-1 (Xt 1 t 1X T t 1).
For the cost function given in Eq.
(8), the following update rules are used.
Theorem 4.
The following update rules will monotonically decrease the cost function de ned in Eq.
(8) and therefore converge to an optimal solution to the objective function.
(    wij + (1    )   zij)    k   xjk (X X T )ij xik = 1,  k xik   xik  Xj then normalize such that Xi  k    k  Xij then normalize such that Xk (X X T )ij  k = 1.
(    wij + (1    )   zij)   xik   xjk (9) (10) (6) The proof for the correctness and the convergence of the above update rules is skipped due to space limit.
with A(Vp, Vq) =Pi Vp,j Vq wij .
Basically, Q measures the deviation between the chance for edges among communities to be generated due to the community structure and the chance for the edges to be generated randomly.
Extensive experimental results [13, 23] have demonstrated that Q is an e ective measure for the community quality, where a maximal Q is a good indicator of the best community structure and therefore the best community number m.
Here we extend the concept of modularity to handle soft membership by de ning a Soft Modularity Qs: Qs =T rh(D 1X )T W (D 1X )i
 (7) where ~1 is a vector whose elements are all ones.
Qs has the following nice property, whose proof is given in the Appendix.
Theorem 3.
The Qs de ned in Eq.
(7) has the same probabilistic interpretation as the Q de ned in Eq.
(6), but in the context of soft community membership.
In addition, Qs is a generalized modularity in that Qs is identical to Q when D 1X  becomes a hard community membership (i.e., each row of D 1X  has one 1 and m-1 zeros).
So to detect the best community number m at time t, we run our algorithm for a range of candidates for m and pick the best one determined by Qs.
In this section, we use several synthetic datasets, a blog dataset, and a paper co-authorship dataset to study the performance of our FacetNet framework.
We start with the  rst synthetic dataset, which is a static network, to illustrate some good properties of our framework.
This dataset was  rst studied by White et al. [23] and is shown in Fig. 4(a).
The network contains 15 nodes which roughly form 3 communities C1, C2, and C3  where edges tend to occur between nodes in the same community.
We  rst check our soft modularity measure.
We apply our algorithm to the network with various community numbers m and the resulting Qs values are plotted in Fig. 4(b).
In addition, in Fig. 4(b) we also show the modularity values Q that are reported by White et al.
in [23].
As can be seen from the plot, both Qs and Q show distinct peaks when m = 3, which corresponds to the correct community number.
Next, after our algorithm correctly partitions the 15 nodes into three communities, we illustrate the soft community membership by studying two communities among the three  C1 = {6, 7, 8, 9, 10} and C2 = {11, 12, 13, 14, 15}.
In Fig. 4(a), we use the same circle shape to represent these 10 nodes but use di erent gray levels to indicate their community

















 (a)






 Modularity Q Soft Modularity Qs



 Community Number
 (b) Figure 4: (a) The synthetic dataset (b) Modular-ity and soft modularity under di erent community numbers membership we use white color to illustrate the level that a node belongs to C1 and dark color to show the level that a node belongs to C2.
As can be seen, while node 7, node 14, and node 15 have very clear community memberships, node 10 and node 13, who are on the boundary between C1 and C2, have rather fuzzy membership.
That is, our algorithm is capable of assigning meaningful soft membership to a node to indicate to which level the node belongs to a certain community.
The second dataset is generated according to the description by Newman et al.
in [13].
This dataset contains 128 nodes, which are divided 4 communities of 32 nodes each.
We generate data for 10 consecutive timesteps.
In each timestep from 2 to 10, dynamics are introduced in the following way: from each community we randomly select 3 members to leave their original community and to join randomly the other three communities.
Edges are added randomly with a higher probability pin for within-community edges and a lower probability pout for between-community edges.
However, the average degree for the nodes is set to 16.
As a result, a single parameter z, which represents the mean number of edges from a node to nodes in other communities, is enough to describe the data.
Because we have the ground truth for the community membership at each timestep, we directly study the accuracy of the community structure obtained by our framework.
We compare our FacetNet framework with 3 baseline algorithms.
The  rst baseline, which we call EvolSpec, is the evolutionary spectral clustering algorithm proposed by Chi et al. [3].
Because EvolSpec is an evolutionary version of the Normalized Cut (NCut) algorithm by Shi et al. [18], we take NCut as our second baseline.
Similarly, FacetNet is essentially an evolutionary version of the soft clustering method (SNMF ) by Yu et al. [24], we take SNMF as our third baseline.
Notice that FacetNet and EvolSpec are evolutionary algorithms whereas NCut and SNMF are not NCut and SNMF work on each snapshot graph independently of other snapshot graphs.
In addition, to make the results comparable, for FacetNet and SNMF we convert the soft membership into 0/1 indicators by assigning each node to the community it most likely belongs to.
Furthermore, in all the experiments, for FacetNet and EvolSpec we set   to be 0.9.
Fig. 5(a) and 5(b) show the accuracy and standard error of the community membership obtained by the four algorithms for two datasets generated with z = 3 and z = 5, respectively.
The accuracy is computed by the mutual information between the derived community membership and the ground n o i t a m r o f n
 l a u t u







 (a) z = 3 (b) z = 5 EvolSpec FacetNet NCut
 EvolSpec FacetNet NCut



 n o i t a m r o f n
 l a u t u



 Timestep





 Timestep

 Figure 5: Mutual information with respect to the ground truth over 10 timesteps when (a) z = 3 and (b) z = 5 truth, where a higher mutual information indicates better accuracy.
From the  gures we can see that when z = 3, i.e., when there is less noise and hence the community structure is easy to detect, both FacetNet and EvolSpec have accuracy improved starting from the second timestep, which suggests that an evolution framework is bene cial in this dynamic network.
In comparison, NCut and SNMF have relatively  at accuracy over all timesteps, with NCut slightly outperforms SNMF.
For the data where z = 5, there are more edges going between communities and therefore the community structure is more di cult to detect.
From Fig. 5(b) we can see that although at the  rst few timesteps FacetNet does not perform as good as EvolSpec, as time going further, FacetNet starts to outperform EvolSpec.
This suggests that the bene ts of FacetNet accumulates over time more than EvolSpec.
In addition, as for the case with z = 3, when z = 5, both FacetNet and EvolSpec clearly outperform their non-evolutionary version, NCut and SNMF.
(a) Time per Iteration (sec) (b) Iteration Numbers






 Size of Network








 Size of Network

 Figure 6: Running time for networks of di erent sizes (a) time per iteration (sec) and (b) number of iterations until converge Next, we study the time performance of FacetNet.
We repeat the above experiment over a family of networks of various sizes (node numbers).
In Fig. 6(a) we show the average running time per iteration of our algorithm on networks with di erent sizes.
In 6(b) we showed the number of iteration needed for convergence when the convergence criterion is that the change between two consecutive iterations is below a threshold of 1e-5.
As can be seen,  rst, the running time per iteration scales linearly with the size of network, which validates our theoretical analysis in Section 2; second, the number of iteration needed for convergence is insensitive to the network size, which implies that the overall running time of our algorithm scales linearly to the size of network.
The blog data was collected by an NEC in-house blog crawler.
Given seeds of manually picked highly ranked blogs, the crawler discovered blogs that are densely connected with the seeds, resulting in an expanded set of blogs that communicate with each other.
The crawler then continued monitoring for new entries over a long time period.
This NEC blog data set contains 148,681 entry-to-entry links among 407 blogs crawled during 12 consecutive month (a timestep is one month), between August 2005 and September 2006.
Following [14], we de ne W by wij =  wij /Pp,q  wpq where  wii = 1,  wij = exp( 1/(    lij)) if eij   Et, and otherwise  wij = 0.
In the formula, lij is the edge weight (e.g., # of links) of eij and  , which is set to 0.2, is a parameter to control marginal e ect when lij is increased.
(a) Soft Modularity Qs (b) Mutual Information








 Community Number





   = 0.1   = 0.5   = 0.9





 Timestep Figure 7: (a) Soft modularity and (b) mutual information under di erent   for the NEC dataset We start with analyzing the overall picture of the dataset.
We  rst aggregate all the edges over all timesteps into a single network and apply our algorithm to compute the soft modularity score Qs under di erent community numbers.
As can be seen in Fig. 7(a), a clear peak shows when the community number is 4.
We draw the aggregated graph in Fig. 8, according to the main community each blog most likely belongs to.
In addition, in Table 1 we list the top keywords, measured by the tf-idf score, that occur in the posts of these four communities.
It seems that C1 focuses on technology, C2 on politics, C3 on international entertainment, and C4 on digital libraries.
  

   

 Figure 8: Four communities in the NEC dataset Next, we analyze the blog data as a dynamic network.
After studying the content of the blogs, we  nd that the above Table 1: Top keywords among the four communities in the NEC dataset, sorted by the tf-idf score



 iraqi, roberts, bush, clinton, adsense, beta, skype,  refox, msn, rss, aol, yahoo, google, ebay, desktop, wordpress, voip, feeds, myspace, podcasting, technorati, search, engine, browser, ads, gmail, windows, os, developer, venture, marketing, apple, podcasts, developers, engines, mac, publishers, ceo, linux gop, uranium, hezbollah, democrats, rove, cia, republicans, saddam, qaeda, tax, republican, iraq, senate, troops, terrorists, administration, terrorist, wilson, conservative, taxes, liberal, intelligence, israel, terror, iran, weapons, war, soldiers shanghai, robots, installation, japan, japanese, architecture, art, chi-nese, china, saudi, phones,  led, mobile, games, korea, r d, sex, green, camera, sound, cell, body, africa, phone, entertainment,  lm, gay, in-dia, fuel, archive, design, elections,  ash, device, water, wireless, south library, learning, digital, resources, collection, conference, sta , communities, students, session, books, database, access, survey, university, science, canada, myspace, articles, education, technologies, knowledge,  led, virtual, tools, research, david, learn, services,  ickr, computers four communities stay rather stable over all the timesteps.
This e ect is partially due to the way these blogs are selected by our focused crawler our crawler chose to crawl a densely connected subgraph of the blogosphere where each node in the subgraph has large number of links and high level of interaction intensities.
Therefore, most of the selected blogs belong to some well-known bloggers and they seldom move around between communities.
We apply our FacetNet algorithm on the data with di erent  .
And we compute the mutual information between the extracted communities at each timestep and the four communities shown in Fig. 8.
Fig. 7(b) shows the results under  =0.1, 0.5, and 0.9.
As can be seen, as   increases, our algorithm emphasizes less on the temporal smoothness and as a result, the community structure has higher variation over time.
In addition, as   increases, the communities at each timestep deviate further from the communities obtained from the aggregated data.
These results on one hand justify our arguments in the introduction section and on the other hand demonstrate that our FacetNet framework is capable of controlling the trade-o  between the snapshot cost and the temporal cost in the cost function Eq.
(1).
Aug 05 c3 c1 c4 c2 Aug 05 c3 c1 c4 c2











 Sep 05 c3 c1 c4 c2





 Oct 05 c3 c1 c4 c2





 Nov 05 c3 c1 c4 c2





 Dec 05 c3 c1 c4 c2 Sep 05 c3 c1 c4 c2





 Oct 05 c3 c1 c4 c2






 Nov 05 c3 c1 c4 c2






 Dec 05 c3 c1 c4 c2












 Jan 06 c3 c1 c4 c2 Jan 06 c3 c1 c4 c2











 Feb 06 c3 c1 c4 c2 Feb 06 c3 c1 c4 c2 (a)   = 1 (b)   = 0.5 Figure 9: The Evolution Net of the NEC dataset when (a)   = 1 and (b)   = 0.5 Fig. 9 shows the Evolution Net derived from our framework when  =1 and 0.5 (  = 1 means no temporal smoothness is considered).
In the Evolution Net, the size of a node is proportional to  k and it represents the size of the corresponding community.
The edge label indicates the probability of a transition from the source community at t-1 to the



 Figure 10: The Community Net for the NEC dataset in September 2005 target community at t. (To avoid clutter, we did not show edges with probabilities less than 0.2).
From Fig. 9(a) we see that when no temporal smoothness is considered, C4 disappeared at the second timestep (Sep 05) and reappeared in the third timestep (Oct 05).
However, by carefully examining the original data, we did not  nd any signi cant events so support such changes.
Therefore, we conjecture that these changes are due to the data noises at the second timestep, which triggered the algorithm to split C1 into two communities and merge C4 to one of them.
In comparison, as can be seen from Fig. 9(b), when there is a temporal smoothness term, the four communities remain relatively stable.
That is, although there exist transitions among di erent types of communities, the majority of transitions are between communities of the same type.
These results demonstrate that the FacetNet framework is more robust to data noise.
From the Evolution Net, we can also obtain some other observations.
For example, the political community C2 is rather isolated from the rest communities over all the time.
In comparison, both C3 and C4 interacts with C1 heavily.
In addition, in Fig. 10 we show the Community Net at an arbitrary timestep (Sep 05).
In the Community Net, the node sizes are proportional to  t;k and the edge weights are proportional to the corresponding entries in  tX T t Xt t (self-loops are not shown).
We can see that this Community Net is a good synopsis of the aggregated network in Fig. 8.
t D 1
 The DBLP co-authorship dataset is a subset of that used by Asur et al. [1].
It contains papers in 28 conferences over 10 years (1997 2006), which span three areas database, data mining, and arti cial intelligence.
We selected a dense subgraph of 2950 authors from the original dataset and partition the time into three periods with overlap: 1997 2000,
 Due to the space limit, we skip the detailed performance study and only point out two interesting issues about this dataset.
First, in the  rst two periods, the soft modular-ity shows (local) peaks at m = 4 while in the third period, m = 3 is optimal.
As a result, for this experiment we used the update algorithm described in Theorem 4.
Second, in Table 2 we list the top authors in the three communities detected by FacetNet in the third period (2003 2006).
Here the rank is determined by the value xik, i.e., pk i.
Recall that pk i indicates to what level the k-th community involves the i-th node.
So from our framework, we can directly infer who are the important members in each community.
However, notice that by important, we are not judging the quality or quantity of papers by an author.
Instead, in our framework the importance of a node in a community is determined by Table 2: Top members in the three communities during 2003 2006, sorted by xik, i.e., pk i Data Mining Database Arti cial Intelligence Philip S. Yu Jiawei Han Wei Wang Jian Pei Divesh Srivastava Surajit Chaudhuri Nick Koudas Elke A. Rundensteiner Divyakant Agrawal Jennifer Widom Kian-Lee Tan Beng Chin Ooi Stanley B. Zdonik Nikos Mamoulis Walid G. Aref Raghu Ramakrishnan Je rey F. Naughton David J. DeWitt Rajeev Motwani H. V. Jagadish Hans-Peter Kriegel William C. Regli Maxim Peysakhov Vincent A. Cicirello Evan Sultanik Gustave Anderson Andrew Burnheimer David Dorsey Moshe Kam Joseph Kopena its contribution to the community structure.
For example, in Table 2, some of the top authors in the AI area are ranked high partially because they participated in a series of papers with up to 20 coauthors and these large coauthor cliques heavily in uenced the community structure of the AI community in our dataset.
The analysis of communities and their evolutions in dynamic temporal networks is a challenging research problem with broad applications.
In this paper, we proposed a framework, FacetNet, to solve this problem.
Unlike traditional two-stage techniques that separate the task of community extraction and the task of evolution extraction, the FacetNet framework combines these two tasks in a uni ed process.
Therefore, not only community structures determine the evolutions, but also the historic evolution patterns regularize current community structure.
As a result, it is less likely for the current community structure to deviate too dramatically from the most recent history.
In addition to the basic framework, we also proposed to use soft community membership and we introduced several novel concepts such as Community Net, Evolution Net, and Soft Modular-ity, to measure and to visualize the resulting communities and their evolutions.
Extensive experimental studies demonstrated that our framework provide communities and evolu-tions that are more accurate and more robust to data noise.
Acknowledgments We thank Kai Yu for providing us with the SNMF source code and for the helpful discussion; thank Junichi Tatemura and Koji Hino for helping prepare the blog dataset; thank Sitaram Asur and Srinivasan Parthasarathy for providing us with the DBLP dataset.
