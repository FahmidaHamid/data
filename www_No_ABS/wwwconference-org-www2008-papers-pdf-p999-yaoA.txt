With the rapid expansion of the Web, more and more entity instances, such as persons, books and movies, are appearing on the Web.
Most of them exist in structured web Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Figure 1: Two person instances pages where their contents are organized in table-like format.
Each page has a set of data regions, and each region stands for an entity instance.
It s a main research line to identify data regions and extract corresponding entity instances from structured pages [15, 22, 20, 24] in Web mining.
The entity instances, de ned as data records [15, 22] or web objects [20, 24], often include names, attribute values, and labels which describe the attribute types.
For example, both instances in Figure 1 contain several attributes regarding the same person.
For the attribute type  Birthday , the  rst instance uses Born as its label, which is di erent from Birthday as used in the second.
Therefore, we should make sure they are of the same attribute type if we want to integrate these two instances.
In fact, it s common for entities of one entity type to have many types of attributes with each of them described by di erent labels.
Consequently, to integrate the web entity instances of the same entity type, we have to establish a map between labels and their types.
To establish this map, a global attribute schema, which re ects the main attribute types and their descriptive labels, must be constructed in advance.
We focus on automatically learning a global entity attribute schema for all the web entities of a speci c entity type.
For example, for person-type entities, a basic description that a person entity should contain attributes of name, gender, birthday, birthplace and weight is provided by the users.
Then, based on this description, we can locate related web pages to extract person entity instances, mine other latent attributes, and then use the new attributes to locate more pages for more instances.
This process runs iteratively until necessary attribute labels for the main attribute types have been discovered.
Based on these attribute labels, entity instances and related pages, we discover the relations between di erent attribute labels, and,  nally, construct the global entity attribute schema for person-type web entities.
The global attribute schema for web entities of a speci c entity type is essential to integrate web entity instances.
Moreover, from the global schema, the types of the impor-
type can be automatically identi ed, and then used as the input to web object extraction systems to extract web objects with these important attributes.
And this makes the results of these systems more valuable and reasonable.
However, for this aim, there are obvious di culties.
On the one hand, enough frequent attribute labels must be extracted from the Web, and they should also be able to represent the main attribute types which constitute the global schema.
On the other hand, the number of frequent labels in learning a global attribute schema is usually large and their qualities are not uniformly high, due to the variety and complexity of web entity instances.
In this paper, we propose the problem of learning a global attribute schema for all the web entities of a speci c entity type for the  rst time, and present a novel framework to address this problem.
Under this framework,  rst, we propose an iterative web entity instances extraction approach to extract enough web entity instances as well as frequent attribute labels to learn a global attribute schema.
Next, we present a maximum entropy-based method to automatically learn a global attribute schema based on the frequent labels, entity instances and related pages.
Finally, we demonstrate our technique on person-type and movie-type entities on the Chinese Web, and experimental results show that our technique is general, e cient and e ective.
This article is further structured as follows.
Section 2 discusses related works.
Section 3 formulates the problem of learning a global attribute schema.
Section 4 presents the iterative method of entity instances extraction.
Section 5 proposes the maximum entropy-based approach of schema discovery.
Section 6 illustrates the experiments and analyzes the experimental results.
Section 7 summarizes this paper.
To the best of our knowledge, no research has yet adequately addressed the problem of learning a global attribute schema from the Web for entities of a given entity type.
The previous study in [8] seeks to discover hidden schema model for query interfaces on deep Web.
This study focuses on  nding a hidden schema model to match schemas of query interfaces in the same domain.
Its goal is the same as ours in  nding a global schema.
However, its method is e ective and e cient only when the number of attribute labels in query interfaces is relatively small, because it creates the global schema based on all possible label partitions, and its time complexity is exponential to the number of labels.
Moreover, it assumes that attribute labels in each query interface s schema are of di erent types, which is natural in deep Web.
However, in our research, the attribute labels and the possible label partitions are both far more numerous than the aforementioned study.
Further, the attribute labels in one entity instance are not always di erent types.
Besides, the other related researches can be summarized in the following aspects:
 Schema extraction is the task of automatically discovering implicit structural information from semi-structured data.
Web data belongs to semi-structured data class, i.e., data with self-contained schema [1].
Previous studies [19, 11, 3] discover schema for web data by utilizing the structure of web pages.
In these studies, schema for web data can be constructed at di erent levels: giving a schema for a set of logically related sites by viewing them as a whole [2]; examining a single site [3]; or, structuring single page.
Similar to these studies, our research aims at learning a schema for web entities.
However, our study is di erent from them in three aspects.
First, we target learning a schema from large numbers of entity instances and related web pages, instead of from a single page, a single site or a set of logically related sites.
Second, our study mines the attribute labels, entity instances and related pages, instead of analyzing the structure of HTML documents.
And lastly, our study seeks to learn a global schema for all the web entities of a speci c entity type, while previous related studies aimed to extract the schema of web data appearing in some sites or pages.
Schema matching aims at discovering semantic correspondences of attributes of schemas across heterogeneous sources.
Previous studies can be classi ed into schema-level matching [17, 5, 9, 16] and instance-level matching [14, 6, 12, 4].
A recent research [21] proposes a uni ed solution to address the two corresponding schema matching problems of intra-site and inter-site.
[8] assumes that a hidden generative distribution exists in related schemas, and presents a statistical schema matching method to reconstruct the hidden generative distribution based on the input schemas.
[10] addresses the problem of complex schema matching, which matches a set of m attributes to another set of n attributes from two individual schemas.
In our research, we  rst look for frequent attribute labels in entity instances from di erent pages, and then combine labels which belong to the same attribute type.
Naturally, the attribute labels correspond to elements in schemas.
However, our research is unlike schema matching.
First, schema matching integrates di erent but related schemas, which are derived from di erent data resources.
Meanwhile, our research combines attribute labels extracted from web entity instances to learn a global attribute schema.
Second, the schemas used in schema matching are extracted from regular data sources, such as deep web databases, whose quality is high and their vocabularies are relatively small, while our attribute labels are collected from surface web pages, whose quality is not uniformly high and their quantity is relatively large.
The general problem of learning a global attribute schema can be formulated as follows: Formally, assume we are given a basic description for entities of a speci c entity type in Minit, which is provided by users under the framework of a prede ned entity model M .
Minit presents users  knowledge about the entities of the target entity type.
Then, our goal is to get a global attribute schema for all the web entities of the target entity type, which is denoted as S and is learned based on Minit and related web pages.
To describe the problem clearly, we  rst de ne a concept  entity  as follows: De nition 1 An entity is something that has a distinct, separate existence, and owns an entity type and all the attributes which are owned by all the entites with the same entity type.
A web entity is an entity whose instances are
 Web page set A global attribute schema Entity instances extraction Attribute label set Global schema discovery Entity instance set Search engines Figure 2: The framework of learning a global attribute schema presented in web pages.
Formally, an entity can be represented as e = {et, (t1, v1), .
.
.
, (ti, vi), .
.
., (tn, vn)}, where et is the entity type, ti and vi is the type and value of ith attribute of e, respectively.
De nition 2 An entity instance is a partial container of a speci c entity.
An entity instance contains some attribute labels and their values of the entity.
Formally, an entity instance can be represented as inst = {(l1, v1), .
.
.
, (li, vi), .
.
., (ln, vn)}, where li and vi is the label and value of ith attribute of inst.
For example, the two person instances in Figure 1 are both entity instances of the same entity.
De nition 3 An entity model is a model which characterizes the features of all the entities of a speci c entity type, which include their entity type, the types of their attributes, and the labels describing their attributes.
Formally, an entity model is represented as M = {et, L1, .
.
.
, Li, .
.
.
, Ln}, where et is the entity type, and Li is a set of labels describing one attribute type.
De nition 4 An initial entity model is a special entity model provided by users to present the basic description of all the entities of the target entity type.
Due to the limitation of users  knowledge, its content only covers a small fraction of the overall knowledge of all the target entities, but it is enough to characterize them.
As a result, an initial entity model must contain the entity type and some sets of attribute labels.
An initial entity model can be formally represented as Minit = {et, L1, .
.
.
, Lj , .
.
.
, Ln}, where for  Minit,  M , |M |   |Minit|, and for  Lj   Minit ,  Li   M , Lj   Li.
To learn a global attribute schema, an attribute schema for all the entities of a target entity type is de ned as follows: De nition 5 An attribute schema for all the entities of a target entity type is a schema about attribute types owned by these entities.
It re ects the types of attributes owned by these entities and the labels describing each attribute type.
Similarly, a global attribute schema for all the entities of a speci c entity type is the schema which de nes all the main attribute types and labels describing each attribute type.
An attribute schema can be represented as S = {(t1, L1), .
.
., (ti, Li), .
.
.
, (tn, Ln)}, where ti is the ith attribute type and can be initialized by one attribute label in Li, and Li is the label set of the ith attribute.
Based on the above de ni-tions, our problem can be formulated as follows: Given an initial entity model Minit provided by users under an entity model framework M , automatically extract enough entity instances {insti}n i=1 from the Web, mine attributes labels Algorithm 1 The iterative entity instances extraction algorithm Input: an initial entity model Minit.
Output: an attribute label set L, an entity instance set I, and a related page set P .
i=1 based on Minit; issue qi to a search engine SE, get the retrieved page set Ri; for each page rij in Ri do if rij is a structured page then extract entity instances from rij, put them into I, and put rij into P ;


 end if end for

 10: end for 11: if iteration termination criteria is ful lled then
 put all attribute labels in I into L, and terminate the algorithm; 13: else
 create one-element label set for each attribute label in I, put all the label sets into Minit, and go to 2 to continue; 15: end if from {insti}n ing attributes labels through the mining of {insti}n related web pages.
i=1 and learn an global attribute schema S us-i=1 and

 The framework of learning a global attribute schema is illustrated in Figure 2, in which the  rst step is to iteratively extract enough entity instances that contain su cient frequent labels to discover the global schema.
To achieve this goal,  rst, we automatically locate related structured pages which contain entity instances.
Then, for each page, we extract entity instances.
In this step, the emphasis is to determine whether the extracted entity instances are enough to obtain su cient frequent labels to terminate the extraction process.
However, we could not know how many frequent labels are enough in advance.
To determine that number, we need to observe whether the number of frequent labels converges after the creation of a certain number of new instances or new iterations.
We propose an iterative algorithm as in Algorithm 1 to automatically locate related structured pages and extract enough entity instances inside.
There are three important issues in this algorithm as follows:
 Each query consists of several attribute labels in Minit, and the query set created in each iteration does not include the queries created in previous iterations.
Moreover, in order to locate structured pages containing entity instances of the target entity type, each query should contain enough labels in Minit.
If the labels in a query is not enough, unrelated pages will be retrieved.
For example, the query { Name  AND  Age  AND  Weight } will locate many target structured pages which contain person instances, while the query { Name  AND  Weight } will locate many other unrelated pages.
Furthermore, to retrieve
 query should not be excessive because the query with excessive labels will match only a few related pages.
For instance, the query { Name  AND  Gender  AND  Age  AND  Birthday  AND  Birthplace  AND  Height  AND  Weight  AND  Race } will match fewer pages than the query { Name  AND  Gender  AND  Age  AND  Birthday }.
Our experimental result indicates that the proper number of labels in one query is 4.
After retrieving related pages, we attempt to extract their containing entity instances.
For each page, we  rst identify its data regions with the help of related mining techniques such as [15].
If one or more regions are discovered, which implies that this page might be a structured page with entity instances then we scan each region to count the label number corresponding to the query in that region.
If the ratio between this number and the number of labels in the query reaches a certain threshold (0.7 in our experiments), this region is regarded as a candidate data region containing one entity instance.
We then extract this region, mine the pattern of the labels in the query appearing in this region, and utilize the pattern to locate each label with its value.
To get enough frequent labels, the termination criteria of the iterative extraction process is a key point.
We assume that the frequency of a frequent label is above a certain threshold, and such labels are su cient if no new frequent label is discovered after a prede ned number (  ) of new instances have been extracted, or a prede ned number ( ) of continuous iterations have been performed.
Then, after each iteration, if at least   (300 in our experiments) new instances are extracted without new labels, we can conclude that the labels are su cient and iterations can be terminated.
And if   (7 in our experiments) continuous iterations have been performed while the number of new instances are below   , we consider that the labels are su cient and iterations should also be terminated.
Experimental results in section 6 a rm the e ectiveness of our iteration termination criteria.
Assume the set of frequent labels is L = {li}n i=1, the set of web entity instances is I = {instj }m j=1, and the set of related pages is P = {pk}t k=1, our goal is to mine the content of L, I and P to combine labels of the same attribute type together, and,  nally, to create the attribute schema S.
To achieve this goal, we  rst preprocess L using the constraints embedded in I to generate candidate attribute types, and get two sets of label sets C and L .
Each element of C represents a unique candidate attribute type and consists of attribute labels belonging to this attribute type.
Each element of L  also consists of attribute labels belonging to the same attribute type, but it does not represent a unique attribute type.
In other words, the attribute type represented by each L s element is independent of the attribute types represented by another elements.
Then, based on C and L , we propose a maximum entropy-based method to discover the attribute types hidden in L, and learn the global attribute schema.
The aim of candidate attribute types generation is to create two sets C and L  based on L by utilizing the constraints on labels embedded in I.
The constraints can be represented as two factors: labels concurrence factor and same entity-value factor.
Labels concurrence factor: This factor is to measure the probability that two labels are of di erent attribute types.
Its main idea is that, it is possible for two labels, which often appear together in entity instances, to be of different attribute types.
This is based on the observation that two attribute labels appearing in one entity instances are often used to describe di erent attribute types.
Strictly speaking, when two labels appear in one instance, they should not describe the same attribute type.
However, due to the diversity of web entity instances, there are some instances in which two labels are actually of the same attribute type.
To measure the probability that two labels have di erent attribute types, we de ne the labels concurrence factor as follows : C(li, lj) = |Ii   Ij | min(|Ii|, |Ij|) (1) where li and lj are two labels in L, Ii and Ij are two sets of instances which contain li and lj , respectively.
Same entity-value factor: This factor measures the probability of two labels being of the same attribute type.
The rationale behind this factor is that, when two labels often appear with the same value in di erent instances of the same entity, it is possible for them to be of the same attribute type.
This inference is derived from the observation that di erent web pages often describe the same attribute type in the same entity s instances by using di erent labels.
Then, if two instances are found to belong to the same entity (which will be discussed later), each pair of their labels appearing in them with the same value could be considered to be of the same attribute type.
However, this inference may not stand for all situations, because some labels from different instances of the same entity may often contain some meaningless values such as  unknown ,  none , and  secret , when these labels are actually not of the same attribute type.
Consequently, we de ne the same entity-value factor as follows to measure the probability of two labels li and lj being of the same attribute type: S(li, lj) =
 ij| |IPij| (2) ij = {(instmi , instmj )|SameEntity(instmi, instmj ) = true, where
  vij , (li, vij )   instmi , (lj, vij )   instmj } and IPij = {(instmi , instmj )|SameEntity(instmi, instmj ) = true,  vi, (li, vi)   instmi ,  vj , (lj, vj )   instmj } SameEntity(instmi, instmj ) = true means instmi and instmj are actually of the same entity, which is determined by the names and the overlap of attribute values of two instances.
The overlap of attribute values can be calculated as O(instmi , instmj ) = |Vmi   Vmj | min(|Vmi |, |Vmj |) where Vmi and Vmj are the attribute value sets of instmi and instmj , respectively.
Two instances instmi and instmj are considered as the same entity s instances if their names are
 generation Input: one label set L.
Output: two set of label sets C and L .
type into one group, and put each group into L ; 3: repeat

 for each element Li in L  do if Li represents an attribute type di erent from types in C then put Li into C, and remove it from L  ; terminate the f or loop;




 end if end for the same and the overlap of attribute values O(instmi , instmj ) is above a certain threshold.
This assessment is simple and e ective, as illustrated in section 6.
Based on the two factors de ned above, we can generate candidate attribute types from the label set L using Algorithm 2.
In this algorithm, two issues need to be explained:   Two labels li and lj have the same attribute type if their same entity-value factor S(li, lj) is above a certain threshold  .
And this relation can be transferred to more labels.
As a result, we can group labels which are of the same attribute type into one set, as illustrated in 2nd line in Algorithm 2.
  Two labels li and lj have di erent attribute types if their labels concurrence factor C(li, lj) is above a certain threshold  .
Furthermore, for two attribute label sets L1 and L2 where labels in each of them have the same attribute type, if one label in L1 and another in L2 have di erent attribute types, all the labels in L1 have a di erent attribute type from those of L2.
By this means, we can determine whether Li represents a new attribute type di erent from types in C, as in 5th line in Algorithm 2.
After generating candidate attribute labels, we get C and L  based on the original label set L. To learn a global attribute schema,  rst, we initialize the schema S using C, since each element of C represents a unique attribute type.
Next, by utilizing our proposed maximum entropy-based method, we  nd new attribute types represented by some elements of L  and group them with their labels into S, or  nd the attribute types in S to which some elements of L  belong and group labels in these elements into the corresponding elements of S. The algorithm of global schema discovery is illustrated in Algorithm 3.
The frequency of a label in the above algorithm is the percentage of the extracted entity instances owning the label in instance set I in all the extracted instances.
Moreover, the key point of this algorithm is to use the maximum entropy-based method to determine whether a set of labels represents a new attribute type, which will be explained in detail.
select label lij with maximum frequency from Ci ; ti   lij, and put (ti, Ci) into S; Algorithm 3 The algorithm of global schema discovery Input: two sets of label set C and L .
Output: an attribute schema S.
5: end for





 select label lkm with maximum frequency from Lk; tk   lkm, and put (tk, Lk) into S; if Lk represents a new attribute type then discover the attribute type tl in S which Lk belongs to, and put all labels in Lk into tl s label set Ll ; else
 end if 13: end for
 Maximum entropy model is a general-purpose statistical model that can freely incorporate various problem-speci c knowledge in terms of features which are not required to be strongly independent.
As a result, one can choose arbitrary features to re ect the characteristics of the problem domain as faithfully as possible.
Suppose Lk = {lh}|Lk | As in our problem, given an attribute schema S, and a label set Lk whose elements are of the same attribute type, our goal is to determine whether labels in Lk are of a new attribute type, or of an existing attribute type in S. And this determination is related to various dependent features, and is proper to be solved using maximum entropy model.
h=1 , for each label lh   Lk, there are some entity instances which contain lh in the instances set I.
For each instance instj which appears in web page wj and contains lh, we can calculate a probability of lh being of an attribute type ti in S as p(ti|chj ), where chj is a context of lh and is composed of some features created based on lh, instj and wj .
Then, for Lk, we can calculate its average probability being of the attribute type ti as AverP (Lk, ti) = Ph
 |Il| j=1 p(ti|chj ) |Il| |Lk| (3) where Il is a subset of I and all instances in Il contain the label lh.
If AverP (Lk, ti)   P, where P is a certain threshold, and Lk can not be determined by labels concurrence factor to be of di erent attribute type from ti, then labels in Lk are of the attribute type of ti; Else, labels in Lk are not of the type ti.
Moreover, if labels in Lk are determined to be not of all the types in S, they should be of a new attribute type.
Given the instance instj and the web page wj, the probability of lh being of the attribute type ti is p(ti|chj ) = (4) p(ti, chj ) Pi p(ti, chj ) Under the maximum entropy framework, let ti be t, chj be c, p(ti, chj ) can be replaced as p(t, c), which is the joint probability of t and c, and maximizes the entropy H(p).
H(p) =  X p(t, c) log p(t, c)
 sch Table 2: Four initial entity models for person-type and movie-type entities Features  templates  lh+1 = X lh 1 = X vh = X th+1 = X th+1 = X w 1 = X w1 = X &th = T &th = T &th = T &th = T &th = T &th = T &th = T under the following constraints X p(t, c)fj (p, c) = X  p(t, c)fj (t, c), 1   j   k where  p(t, c) is the observed distribution of attribute types and the contexts of attribute labels in training data, and fj (t, c) is a feature created on the basis of attribute type t and attribute label context c.
The distribution p(t, c) in above constaints is given by p(t, c) =   k Yj=1 fj (t,c)   j where   is a normalization factor, the  j s are the unknown parameters of the model, and each  j corresponds to a fj (t, c) and can be seen as the weight of fj(t, c).
There are several algorithms designed to estimate these unknown parameters, and we select Limited-Memory Variable Metric because it has been proved to be especially e ective[18].
In our problem, the context of an attribute label is based on the set of labels, values, types, and words surrounding the label in the web page, which can be seen as the  scene  of the label.
The  scene  of label lh in an instance instj which appears in a web page wj is sch = {lh, lh+1, lh 1, vh, th+1, th 1, w 1, w1} where lh 1 and lh+1 are the labels before and behind lh in instj , vh is the value of lh, th 1 and th+1 are the type of lh 1 and lh+1, and w 1 and w1 are the words before and behind lh in wj.
If lh is the  rst label in instj , lh 1 and th 1 are de ned to be special characters  H  and  TH  ; if lh is the last label in instj , lh+1 and th+1 are de ned to be characters  E   and  TE  .
Similary, if lh is the  rst word in wj , w 1 is set to  FW  , and if it s the last word, w1 is set to  LW   .
Based on the  scene  of lh, the features can be created by scanning each pair (sch, th) in the training data with the feature  templates  given in Table 1, where th is the attribute type of the label with the  scene  sch, and the training data is created using the attribute types in the current schema S.
Given sch as the current  scene , a feature always asks some yes/no question about sch, and constrains th to a certain attribute type.
The instantiations for the variables X and T in Table 1 are obtained by automatically scanning training data.
For example, for a label  Birth time  named lh, one feature with scene sch might be fj (sch, lh) =   1 if lh+1= Gender & th=  Birthday  ; 0 otherwise.
Model Minit0 Minit1 Minit2 Minit3 Details of model {person, {N ame}, {Age}, {Gender, Sex}, {Birthplace}, {Birthday, Birthtime}, {Occupation}} {person, {N ame}, {Age}, {Gender}, {W eight}} {movie, {T itle}, {Director}, {Language}, {Region}} {movie, {T itle}, {Director}, {Actor}, {Genre, T ype}, {Running time}} For this feature, each entity instance in training set is automatically scanned to initialize it.
If lh is found in one instance with attribute type  Birthday , and  Gender  is the label behind it, fj(sch, lh) is to 1 in this instance; else, it s set to 0.
The proposed iterative entity instances extraction method and global attribute schema learning approach are fully implemented and evaluated on person-type and movie-type entities.
The goal of the experimental study are:(i) to check the performance of the iterative entity instances extraction algorithm; (ii) to discover the e ectiveness of candidate attribute types generation method, (iii) to evaluate the performance of our global attribute schema learning method.
We select Google as the search engine to extract entity instances on the Chinese Web.
Due to the limitation of Google s Web interface, we expand each query for several times using Recursive Query Expansion (RQE) [7], based on a set of high-frequency words in a standard Chinese Web collection named CWT100G (http://www.cwirf.org).
We select person-type and movie-type entities to evaluate the performance of our technique.
For entities of each entity type, we specify two di erent initial models and guarantee each model is able to characterize them.
We list these four initial models in Table 2, where Minit0 and Minit1 are models for person-type entities, and the others are for movie-type entities.
To the best of our knowledge, our research is the  rst study aimed at learning a global attribute schema for all the web entities of a speci c entity type.
The only relevant study is [8], which also aims at mining a global schema from an attribute label set.
While, this study requires high-quality schemas as input and is sensitive to errors in schemas, and its e ciency decreases sharply with the increase of the unique labels  number in schemas.
We select the algorithm M GSsd in [8] as the baseline, and implement it to compare the result s quality and the algorithm s e ciency with our method.
To evaluate the performance of the iterative entity instances extraction algorithm, we utilize three metrics: precision, entity-type error ratio, and attribute-label error ratio.
Precision is de ned as the percentage of the correct instances, which are truly of the speci ed entity type and all their elements (pairs of label and value) belong to the corresponding entities, in all the extracted instances.
Entity-type error ratio is the ratio of instances which are not of the
 Model # iterations # time (hour) # instances # pages # frequent labels # unfrequent labels Minit0 Minit1 Minit2 Minit3























 l s e b a l t n e u q e r f f o r e b m u










 Minit0 Minit1







 Number of instances (*104) l s e b a l t n e u q e r f f o r e b m u










 Minit2 Minit3






 Number of instances (*104) Figure 3: Number of frequent labels for di erent person-type entity instances Figure 4: Number of frequent labels for di erent movie-type entity instances speci ed entity type, to all the extracted instances.
And attribute-label error ratio is the ratio of the extracted instances in which some elements actually do not belong to the corresponding entities, to all the extracted instances.
The reason for the entity-type error is that the iterative instances extraction process might locate some pages containing other information which is wrongly extracted as the desired entity instances.
And the reason for the attribute-label error is that the structures of some pages are too  exible to accurately mine the data regions from them.
Actually, the learning of an attribute schema can be seen as the clustering of labels with the aim of grouping labels of the same attribute type into the same cluster.
Assume the gold schema is Sgold = {(t1, L1), .
.
., (ti, Li), .
.
.
, (tn, Ln)}, and the learned schema is Slearned = {(t1, L1), .
.
.
, (tj, Lj ), .
.
.
, (tm, Lm)}.
Then, the labels clustering result corresponding to Sgold is Cgold = {L1, .
.
.
, Li, .
.
.
, Ln}, and labels clustering result corresponding to Slearned is Clearned = {L1, .
.
.
, Li, .
.
.
, Lm}.
We utilize weighted average Fscore (waFs-core), which is often used in clustering evaluation[13, 23], as the evaluation metric to measure the quality of the learned schema Slearned, using the waFscore of Clearned compared with Cgold.
The results using di erent initial models are illustrated in Table 3, in which frequent labels are those whose frequencies are above 0.005.
For person-type entities, both models (Minit0 and Minit1) are capable of extracting enough instances.
Figure 3 shows the change of the frequent labels  number with the increase of the number of extracted person instances.
It shows that for both models, the number of frequent labels increases sharply at the beginning of extraction process, and begin to converge with the progress of extraction process.
Moreover, in the 839 frequent labels created by Minit0, 826 labels also appear in the 863 labels created by Minit1, which indicates that most of the frequent labels have been discovered.
In the following experimental results, Table 4: Evaluation of the extracted entity instances Entity type Precision Entity-type Attribute-label error ratio error ratio person-type movie-type





 we select Minit0 s result as the working basis for person-type entities.
For movie-type instances, the two models are also capable of extracting enough instances, and the change of the frequent labels  number with the increase of the extracted movie instances  number is illustrated in Figure 4.
The same conclusion can be drawn from this  gure.
Furthermore, it is observed that 376 frequent labels created by Minit3 also appear in the 406 labels created by Minit2, and we select Minit3 s result as the working basis for movie-type entities.
To evaluate the quality of the extracted instances, we randomly select 200 instances from the results of Minit0 and Minit3, respectively, and do the evaluation by manually checking them.
The result is illustrated in Table 4.
As we can see our algorithm could extract instances of speci ed entity type precisely, at the same time some errors exist for the errors in the mining of data regions.
Moreover, in order to  nd out why the attribute-label error ratio of movie-type entities (12%) is higher than that of person-type entities (6%), we manually check the pages including these instances, and  nd that movie entity instances often appear in some arbitrary created pages with other movie instances, and the borders between them are not clearly identi ed by HTML tags, which makes the data region mining technique invalid when processing these pages.
While person entity instances often appear in well-structured pages which include the resumes of persons, and data regions can be accurately mined from these pages.
In this section, we  rst evaluate the performance of labels concurrence factor and same entity-value factor, and then report the generated candidate attribute types.
i n o s c e r









 Movie-type Person-type
 Recall i i n o s c e r








 Movie-type Person-type



 Threshold

 Figure 5: Precision and recall of labels concurrence factor for di erent thresholds Figure 6: Precision of judgment of same entity for di erent thresholds Labels concurrence factor: For person-type entities, we randomly select 110 frequent attribute labels, compute labels concurrence factor for each pair of them, and get 2672 pairs with nonzero labels concurrence factors.
Then, we manually evaluate whether labels in these pairs are actually of di erent types and  nd 2606 pairs of labels which are of di erent types.
Finally, we utilize our algorithm to automatically judge whether two labels are of di erent types, by determining whether their labels concurrence factors is above a certain threshold.
Similarly, we randomly select 55 frequent labels for movie-type entities, and do the same experiment.
We get 1286 pairs with nonzero labels concurrence factors and  nd 1201 pairs of labels which are of di erent types.
Figure 5 shows the relation between the precision and recall of the judgment when using di erent thresholds.
We can see that the precision of judgment for person-type entities is always higher than that of movie-type entities.
It is because the attribute-label error ratios of person-type entities are lower than those of movie-type entities as illustrated in section 6.3.1, and more pairs of movie-type entities  labels which are not of di erent types appear together in some instances due to higher attribute-label error ratios.
To get precise judgment, we select 0.15 as the best threshold for person-type entities, and 0.18 as the best threshold for movie-type entities.
Same entity-value factor: The basis of this factor is the judgment of whether two instances are of the same entity.
For person-type instances, we randomly select 50 person names from extracted instances and make sure each person name has at least 3 di erent instances, and totally get
 ues for all pairs of instances which own the same name.
After that, for each pair of instances which own the same name, we manually make the judgment on whether they are of the same entity.
We also select 50 movies from 215 movie instances and run the same experiment.
Figure 6 shows the precisions of the proposed method in section 5.1.1 for different thresholds.
As we can see that this simple method is e ective to make right judgments, and we select 0.75 as the best threshold to be used in experiments.
Based on the judgment about two instances being of the same entity, we can calculate same entity-value factor for each label pair.
If the factor is above a certain threshold, our algorithm can judge that they are of the same type.
Figure 7 shows the relation between the precision and recall of the judgment with di erent thresholds for 110 frequent labels of person instances and 55 frequent labels of movie instances.
Movie-type Person-type






 i i n o s c e r



 Recall Figure 7: Precision and recall of same entity-value factor for di erent thresholds As we can see that the precision of judgment for movie-type entities is always higher than that of person-type entities.
It is because movie-type entity instances include less meaningless attribute values than person-type entity instances.
In order to get precise result, we select 0.84 as the best threshold for person-type entities, and 0.82 as the best threshold for movie-type entities.
Candidate attribute types: For person-type entities, we select the top 400 frequent attribute labels to generate candidate attribute types, because these labels account for 96.9 percent of all the labels  occurrences in the extracted instances.
We get 3941 pairs of labels which are of di erent attribute types and 55 pairs of labels which are of the same attribute types, and generate 19 candidate attribute types.
For movie-type entities, we select the top 150 frequent labels since they account for 97.1 percent of all the labels  occurrences, and get 1947 pairs of labels which are of di erent types and 21 pairs of labels which are of the same types, and generate 7 candidate attribute types.
We select top 400 frequent labels of person-type entities and top 150 frequent labels of movie-type entities as the basic label sets (Lperson and Lmovie) to evaluate the performance of our global attribute schema learning method.
First, for each label set, three annotators independently create two schemas by discovering the meaning of attribute labels with the help of related entity instances and pages.
Then, a  nal schema for each label set is created by another annotator who is familiar with the meaning of attributes labels based on three schemas.
Finally, we get two gold schemas, SglodP erson and SgoldM ovie, and create the gold attribute labels clustering results, CgoldP erson and CgoldM ovie.
Comparison with the baseline As mentioned in section 6.1, we select M GSsd in [8] as the baseline, in which
 Table 7: Details of four label sets created on Lmovie Label set Occurrence ratio # Created di erent schemas Label set Occurrence ratio # Created di erent schemas Top10 Top20 Top30 Top40







 Top10 Top20 Top30 Top40







 Table 6: Comparison between our method and baseline for person-type entities Table 8: Comparison between our method and baseline for movie-type entities Label set Method WaFscore Running time(s) Label set Method WaFscore Running time(s) Top10 Top20 Top30 Top40 Our method Baseline Our method Baseline Our method Baseline Our method Baseline













   about 1553586 Top10 Top20 Top30 Top40 Our method Baseline Our method Baseline Our method Baseline Our method Baseline













   about 51372504 the input is a set of schemas consisting of attribute labels.
Moreover, due to the time complexity of the baseline, for Lperson and Lmovie, we only select the top 10, 20, 30 and
 created label set Lc, we create schemas by automatically scanning each extracted instance insti to create one set of labels which are both in Lc and insti, and regarding this label set as the schema.
Finally, we run M GSsd using these schemas to create the desired attribute schemas, and compare the result s quality and the algorithm s e ciency with our method.
For the baseline, since the hypothesis space is too big (360600 for person-type entities, and 2826021 for movie-type entities) when the number of labels is 40, we only estimate its running time.
From the analysis of the baseline s running log, we  nd that running time is mainly spent on  2 estimation.
From the theoretical analysis of the algorithm, we discover that the time spent on  2 estimation of each hypothesis is the same.
As a result, by recording the running time spent on  2 estimation for a small number of hypothesizes, we estimate the running time of all the hypothesizes for the baseline.
For person-type entities, the details of the four label sets created on Lperson are illustrated in Table 5, in which the second column, occurrence ratio, is the ratio of the occurrences of labels of the corresponding label set to the occurrences of all the labels in extracted instances, and the third column is the number of di erent schemas created by this label set.
Table 6 shows the comparison of result quality and e ciency between our method and the baseline.
As we can see the result s quality of our method is better than the baseline, which is because the baseline is too sensitive to the errors of schemas, while our method utilizes probabilistic method to eliminate errors.
Furthermore, when the label set is small, the e ciency of the baseline is higher than our method, while with the increase of the label set s size, the e ciency begins to decline sharply for its exponential time complexity.
However, the decline of our method s e ciency is far more smoother than the baseline.
Moreover, the baseline is only e ective when the number of labels is less than 40, and become impractical even when processing the Top40 label set, which only accounts for 75.72% of the labels  occurrences and is inadequate to create the global attribute schema.
Therefore, the baseline can not create the global attribute schema e ectively and e ciently.
On the contrary, as the evaluation result illustrated, our method can learn the global attribute schema e ectively and e ciently.
We run the same experiment on movie-type entities, and the details of the label sets created on Lmovie are illustrated in Table 7.
The comparison result is illustrated in Table 8, and we can see that our method also outperforms the baseline.
e r o c s
 e g a r e v a d e h g e
 t i







 Movie-type Person-type

 Value of N Figure 8: WaFscores on TopN label sets Performances on di erent label sets We create top 50, 100, 150, 200, 250, 300, 350 and 400 label sets from Lperson, and top 25, 50, 75, 100, 125 and 150 label sets from Lmovie, and compare the results  quality on these di erent label sets.
The comparison is illustrated in Figure 8.
As we can see that our method perform well when the size of label set is relatively small and the performance declines with the growth of the label set.
This is because that the labels with lower frequency own less features to characterize their attribute types, while the labels with higher frequency possess more features.
However, even for the Top300 label set of person-type entities, which accounts for 95.35% of the labels  occurrences, the wavFscore of the created global attribute schema is still acceptable (0.7122), and the time spent on
 the Top100 label set for movie-type entities, which account for 94.90% of the labels  occurrences, the waFscore of the created global attribute schema is also acceptable (0.7123), and the time used to create the schema is 996.862 seconds (about 17 minutes).
This paper explores the problem of learning a global attribute schema for web entities of a given entity type.
This problem is essential to facilitate the integration of entity instances and perform valuable and reasonable web object extraction, and is di cult due to the complexity and variety of web entity instances.
We propose a general framework to automatically learn a global attribute schema, and further specialize it to develop an iterative instances extraction algorithm to extract su cient instances and attribute labels, as well as a maximum entropy-based approach to construct the global attribute schema.
We demonstrate our technique on person-type and movie-type entities on the Chinese Web, and create two global attribute schemas for the entities of these two types, and the weighted average Fscores for the two created schemas are 0.7122 and 0.7123, respectively.
These results validate the e ciency and e ectiveness of our technique in learning the global attribute schema for web entities of the speci c type.
We thank Dr. Tao Meng, Lijiang Chen, Jing He, Shengliang Gao, Mengcheng Duan, Nan Di, Yuan Liu and Bo Peng for their comments.
The work in this paper was supported by NSFC Grant 60773162 and 863 Project Grant 2006AA01Z196.
