Web page ranking has been traditionally based on hand designed ranking functions such as BM25 [18].
With the inclusion of thousands of features for ranking, hand-tuning of ranking function becomes intractable.
Several machine learning algorithms have been applied to automatically optimize ranking functions [4, 5].
Machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each query-document pair.
The cost of the editorial labeling is usually quite expensive.
Moreover, the relevance labels of the training examples could change over time.
For example, if the query is time sensitive or recurrent (e.g.
 www  or  presidential election ), a search engine is expected to return the Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
most up-to-date documents/sites to the users.
However, it would be prohibitive to keep all the relevance labels up to date.
Click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information.
Compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance.
Clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions [7, 13, 14, 15], or as signals to directly in uence ranking [1, 13].
However, clicks are known to be biased, by the presentation order, the appearance (e.g.
title and abstract) of the documents, and the reputation of individual sites.
Many studies [8, 10] have attempted to account the position-bias of click.
Carterette and Jones [7] proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment.
Other research [10, 21, 16] attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks.
Two di erent types of the click models are position models [8, 10, 17] and the cascade model [8].
A position model assumes that a click depends on both relevance and examination.
Each rank has a certain probability of being examined, which decays by rank and depends only on rank.
A click on a url1 indicates that the url is examined and considered relevant by the user.
However this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability.
Take for example two equally relevant urls for a query: a user may only click on the top one, feel satis- ed, and then leave the search result page.
In this case, the positional bias cannot fully explain the lack of clicks for the second url.
The cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked.
Here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls.
The cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks.
Even though the cascade model is quite restrictive, the authors of that paper showed that
 for the entire display block consisting of the title, abstract and url of the corresponding result.
than the position models described above.
None of the above models distinguish perceived relevance and actual relevance3.
Because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance.
While there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er.
In this paper, a dynamic bayesian network (DBN) model is proposed to model the users  browsing behavior.
As in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant.
Similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document.
The user chooses to examine the next url if he/she is unsatis ed with the clicked url (based on actual relevance).
Our model di ers from the cascade model in two aspects: 1. because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance.
during a search.
We compare the proposed model with pervious models and show that the dynamic bayesian network based model outperforms the others.
The predicted relevance for each url are then used in two ways: either as a feature in a ranking function or used as supplementary data to learn a ranking function.
We show that the function learned with these predicted relevance is not far from being as good as a function trained with a large amount of editorial data.
We further show that combining both type of data can lead to an even more accurate ranking function.
As explained above, the presentation bias refers to the fact that users are more likely to click on documents at the top of the ranking.
A popular class of methods for dealing with this presentation bias problem are the position based models [8, 10,
 clicks on a link if the following two conditions are met: the user examined the url and found it relevant; in addition, the probability of examination depends only on the position.
More precisely, given a url u at position p, the probability of a click is modeled through a hidden variable E denoting if u was examined or not: P (C = 1|u, p)
 = e {0,1} | P (C = 1|u, p, E = e)P (E = e|u, p) = P (C = 1|u, E = 1) P (E = 1|p) .
{z := u } | {z := p }
 tween the number of times this url was clicked and the number of times it was shown.
a search engine.
Actual relevance means the relevance of the landing page.
The last equation made use of the following assumptions: there is no click if the user did not examine the url; if the url is examined, the probability of click depends only on its relevance; the probability of examination depends only the position.
As a result the probability of a click is the product between two probabilities  u and  p: the  rst one models the relevance of the url to the query while the second one captures the position e ect.
Remember that our goal is to infer the relevance of an url based on the click logs.
That is exactly what  u represents: the perceived relevance of an url to the user, independent of the position.
If we make the additional assumption that  1 = 1   that is the user always examine the  rst result   then  u can be interpreted as an equivalent CTR at position 1, i.e. the CTR of that url had it been placed in the  rst position.
Note that the query q is implicit here; more formally, we should write  uq to stress the dependence to the query, but in the rest of paper we assume that the query is  xed.
A cheap and straightforward approach is to estimate  p as the aggregated CTR (over all queries and sessions) in position p. Suppose there are N sessions in which u appeared and for the i-th session ci   {0, 1} indicates if there was a click and pi is the position in which the url u appeared.
Then  u is computed as [19]:

 i=1 ci i=1  pi  u = .
(1) As in [19] we refer to this method as clicks over expected clicks (COEC), because the denominator can be seen as the number of  expected  clicks given the positions that the url appeared in.
The problem with the COEC model is that the estimation of   is biased.
It would be valid if the search engine gives results in a random order.
But since more relevant documents tend to appear higher in the ranking, the observed CTR at a given position captures not only the position bias, but also the typical relevance at this position.
Another approach is to  nd  u and  p by maximum likelihood.
Note that the urls need to have been shown in di erent positions for this approach (and other below) to be meaningful.
Otherwise the solution is ill-de ned.
This makes sense because in order to capture the position e ect, one needs to observe the CTR of the same url at di erent positions.
That is usually the case because of the constant variations in a search engine.
Given the vector  p, the maximum likelihood solution for  u is:  u = arg max   ci log( pi ) + (1   ci) log(1    pi ).
(2)
 i=1 The vector  p is estimated by an alternate (or joint) maximization of the likelihood between  u and  p.
A drawback of the above approach is that it can lead to  u > 1.
This is not desirable since  u is supposed to represent a probability.
Instead of maximizing the likelihood directly, one can use the Expectation-Maximization (EM) algorithm where the hidden variables are the examination variables E [10].
This ensures that  u   1.
We used the model.
Another alternative is to use a slightly di erent model related to logistic regression [8]: P (C = 1|u, p) :=
 1 + exp(   u    p) .
(3) The click probability is not a product of probabilities any longer, but it is still a function of the url and of the position.
The main advantage is that it ensures that the resulting probability is always between 0 and 1; also the optimization is much easier since it is an unconstrained and jointly convex problem.
Ei 1 Ei Ei+1 Ci Ai au Si su i 1Y Cascade model [8] di ers from the above position models in that it considers the dependency among urls in a same search results page and model all clicks and skips simultaneously in a session.
It assumes that the user views search results from top to bottom and decides whether to click each url.
Once a click is issued, documents below the clicked result are not examined regardless of the position.
With the cascade model, each document d, is either clicked with probability rd (i.e. probability that the document is relevant) or skipped with probability (1-rd).
The cascade model assumes that a user who clicks never comes back, and a user who skips always continues.
A click on the i-th document indicates: 1. the user must have decided to skip the ranks above; 2. the user deem the i-th document relevant.
The probability of click on i-th document can thus be expressed as: P (Ci = 1) = ri (1   rj).
(4) j=1

 We now introduce another model which considers the results set as a whole and takes into account the in uence of the other urls while estimating the relevance of a given url from click logs.
The reason to consider the relevance of other urls is the following: take for instance a relevant document in position 3; if both documents in position 1 and 2 are very relevant, it is likely that this document will have very few clicks; on the other hand, if the two top documents are irrelevant, it will have a lot of clicks.
A click model depending only on the position will not be able to make the distinction between these two cases.
We extend the idea of cascade model and propose a Dynamic Bayesian Network (DBN) [11] to model simultaneously the relevance of all documents.
The Dynamic Bayesian Network that we propose is illustrated in  gure 1.
The sequence is over the documents in the search result list.
For simplicity, we keep only the top 10 documents appearing in the  rst page of results, which means that the sequence goes from 1 to 10.
The variables inside the box are de ned at the session level, while those out of the box are de ned at the query level.
As before, we assume that the query is  xed.
For a given position i, in addition to the observed variable Ci indicating whether there was a click or not at this Figure 1: The DBN used for clicks modeling.
Ci is the the only observed variable.
position, the following hidden binary variables are de ned to model examination, perceived relevance, and actual relevance, respectively:   Ei: did the user examine the url?
  Ai: was the user attracted by the url?
  Si: was the user satis ed by the landing page?
The following equations describe the model: Ai = 1, Ei = 1   Ci = 1 P (Ai = 1) = au P (Si = 1|Ci = 1) = su P (Ei+1 = 1|Ei = 1, Si = 0) =   Ci = 0   Si = 0 Si = 1   Ei+1 = 0 Ei = 0   Ei+1 = 0 (5a) (5b) (5c) (5d) (5e) (5f) (5g) As in the examination model, we assume that there is a click if and only if the user looked at the url and was attracted by it (5a).
The probability of being attracted depends only on the url (5b).
Similar to the cascade model, the user scans the urls linearly from top to bottom until he decides to stop.
After the user clicks and visits the url, there is a certain probability that he will be satis ed by this url (5c).
On the other hand, if he does not click, he will not be satis ed (5d).
Once the user is satis ed by the url he has visited, he stops his search (5e).
If the user is not satis ed by the current result, there is a probability 1     that the user abandons his search (5f) and a probability   that the user examines the next url.
In other words,   measures the perseverance of the user4.
If the user did not examine the position i, he will not examine the subsequent positions (5g).
In addition, au and su have a beta prior.
The choice of this prior is natural because the beta distribution is conjugate to the binomial distribution.
It is clear that some of the assumptions are not realistic and we discuss in section 8 how to extend them.
However, as shown in the experimental section, this model can already explain accurately the observed clicks.
4it would be better to de ne the perseverance   at the user level, but we simply take the same value for all users.
ables au and su related to the relevance of the document.
The  rst one models the perceived relevance since it measures the probability of a click based on the url.
The second one is the probability that the user is satis ed given that he has clicked on the link; so it can been understood as a  ratio  between actual and perceived relevance.
Indeed, if we de ne the relevance of the url as the probability that the user is satis ed given that he has seen the url, we have: ru :=P (Si = 1|Ei = 1) =P (Si = 1|Ci = 1)P (Ci = 1|Ei = 1) =ausu (6) As far we know, this is the  rst click model which attempts to model the actual relevance rather than the perceived relevance only.
The examination model can be seen as a special case of our model where the Ei are independent and have a distribution that only depends on the position.
In that case the Si are meaningless because they cannot be inferred.
The cascade model of [8] is a special case of our model with   = 1 and su = 1.
That is, the user keeps examining until he  nds a document that appears relevant.
He then clicks and stops.
In [7] the relevance of the documents is predicted from click logs and the CTRs of the other documents is used during the prediction.
Their motivation is di erent (which is to evaluate the quality of search engine), but they address the same problem: the in uence of other documents on CTRs.
However the de nition of CTRs in other positions is problematic if the set of documents changed between sessions.
Joachims [13] introduced the so-called skip-above pairs: when the user has not clicked at position i but has clicked at position j > i, it is an indication that the document in position j is preferred to the document in position i.
We recover this type of pair with our model because in that case 1 = Aj > Ai = 0.
However, the problem with learning a ranking function with skip-above pairs is that one tends to learn the reverse function than the one in production (there are only  negative  instances in some sense).
For the position models, it has been observed that it is better to use di erent vectors   for di erent type of queries such as navigational vs informational5.
The reason is that for navigational queries, the CTRs decay much faster with the position.
But we argue that this decay is not a function of the query type, but of the quality of the top urls.
For a navigational query, the top result is usually excellent and there are very few clicks in lower positions.
Our click model captures this e ect directly and does not need di er-ent browsing models for di erent type of queries.
The Dynamic Bayesian Network of  gure 1 is just slightly more complex than a standard Hidden Markov Model (HMM) because of the conditional dependence between the hidden state at position i + 1 and the observation at position i given the hidden state at position i.
The Expectaction-Maximization (EM) algorithm [9] is used to  nd the maximum likelihood estimate of the variables au and su and the forward-backward algorithm is used to to compute the posterior probabilities of the hidden variables.
Please refer to the Appendix of the paper for a detailed derivation of the E-step and M-step, as well as a con dence computation for the latent variables.
Even though we could have also estimated   with EM, we simply treated   as a con gurable parameter for the model.
Three types of experiments are performed to validate our model.
First we evaluate the click model in terms of predicted CTR at position 1.
Then we use the predicted relevance as signals in ranking.
In a third type of experiments, we use the predicted relevance as targets to train a ranking function.
The click log is obtained from a commercial search engine.
A session can be de ned in various ways, but for all our experiments, it is de ned as as follows.
A session always has a unique user and unique query.
It starts when a user issues a query and ends with 60 minutes idle time on the user side.
For each session, we get the query, list of urls in result sets, and list of clicked urls.
Simple normalization is applied to queries and urls.
As explained before, we restrict ourselves to the top 10 urls on the  rst result page.
Sessions for which the clicks are not in the same order as the ranking (for instance the user clicks  rst on the 4th link then on the 2nd link) have zero probability under our model.
On average, roughly 3% of the sessions contain one or more out-of-the-order clicks.
We could have decided to swap the click order for these sessions, but we simply discarded them.
We also discarded all the queries for which we have less than 10 sessions.
We  rst assess how accurate is our proposed click model.
For this task, we get 58M sessions and 682k unique queries from the click log of the UK market.
A natural way to evaluate a click model would be to proceed as in [10]: for a given query, take some sessions for training and evaluate the likelihood of clicks on the other sessions.
But our goal is di erent: we are not so much interested in click prediction, but more on how accurate the latent variables au and su are: they are indeed the variables that will be used later for ranking.
It is di cult to assess the quality of su, but it is easy for au.
Indeed au (like  u for the examination model) is a prediction of the CTR in position 1.
The following leave-one-out experimental protocol is used:

 some other positions;
 url appeared in position 1;
 au;
 sessions;

 by the number of test sessions.
to reach a particular site [3].
When computing the error between the actual CTR in position 1 and the predicted one, two types of error metrics squared error (left) and KL divergence (right).
The x-axis shows the minimum number of training sessions that have been used for estimating this CTR.
are used: the Mean-Square-Error (MSE) and the KL divergence.
Note that the KL divergence is, up to a constant value, the same as the likelihood of the clicks on the test sessions.
The DBN model requires the input of a parameter  .
We  rst perform the above leave-one-out procedure to empirically determine the optimal value of  .
The result is shown in  gure 3.
The DBN model reaches the smallest MSE when   = 0.9, which indicates that users are persistent in  nding relevant documents.
For the rest of the experiments, we set   to 0.9.
N +ap D+ap+bp D by their smoothed version the entire dataset.
We then replace the raw CTRs of the form N .
The COEC model predicts sometimes CTR larger than 1: in that case, we clamp the prediction to 1 when computing the MSE error.
However, even after clamping to 1, the KL divergence is in nite when the test CTR is smaller than 1.
That is the reason why COEC does not appear in the right hand side of  gure 2.
As for the cascade model described by equation (4), it can only handle sessions which have exactly one click.
When testing this method we thus discarded all the sessions which have zero or more than one click.
While the cascade model is rather restricted by assuming that users are very persistent in examining until they  nd a relevant document, we see that it performs better than the COEC and examination model and comparably to the logistic model, indicating it is useful to consider the interaction between urls in the same search result page.
These results are consistent with the ones reported in [8].
In  gure 2, we broke down the errors as a function of the minimum number of sessions used for training.
For instance
 (query,urls) pairs such that the number of sessions in which this url does not appear in position 1 is at least 1000.
When the number of sessions is large, the choice of the prior distributions on au and su does not play an important role.
One would typically expect that the accuracy improves with the number of sessions.
That is roughly the case for the cascade and our model, but surprisingly, the accuracy of the other models deteriorates when the number of sessions is very large.
We investigated this issue by looking at the (query,url) pairs with a large error.
An example was the query  myspace  and the url www.myspace.com in the UK market.
The url in postion 1 should be uk.myspace.com, but because of variations in the ranking due to tests for instance, the url www.myspace.com appeared several times in position 1.
For these sessions, the CTR was very high, 0.97, as expected.
In other sessions, it appeared in position 2 and had a low CTR of 0.11.
This low CTR is expected because the url in  rst position is then uk.myspace.com and most users do not even look at www.myspace.com.
However, the logistic model predicted a CTR of 0.21.
The predicted CTR Figure 3: Mean squared error of the DBN model in predicting the CTR at position 1 as a function of  .
We compare the DBN model with other click models described in section 2.
The results are shown in  gure 2.
The COEC, examination, and logistic models are respectively described by equations (1), (2), and (3).
For these methods we use the empirical Bayes method [6] with a beta prior to smooth the observed probabilities of clicks at di erent positions.
This yields more stable and accurate results.
More precisely, the CTRs at a given position p are assumed to be drawn from a beta distribution with parameters ap and bp.
These parameters are found by maximum likelihood on
 there is roughly a factor 2 di erence between the CTR of an url in position 1 and in position 2.
The reason why there is a factor 9 for the query myspace is because the url shown in position 1 was already excellent and the users hardly looked at position 2.
The examination and logistic model do not take this information into account but the DBN does: it predicted a CTR of 0.95 for www.myspace.com.
Perfect urls in position 1 tend to happen more often for navigational queries.
For this type of query, we have a lot of sessions; that explains why the bad performance of the position models is exacerbated when a large number of sessions is considered.
We have not done a formal comparison with the model of [10], but preliminary experiments show that it su ers from the same problem as the other position model described above.
Figure 4: The   vector from COEC (1) and the examination (2) models.
The former corresponds to the observed CTRs, while the latter aims at isolating the position e ect in CTR modeling.
Of peripheral interest, but still noteworthy is  gure 4 which compares the vector   for the two models of the form P (C = 1|u, p) =  u p.
As explained in section 2, the values of  p decay too fast for the COEC model, because this model does not make the distinction between the position e ect and the fact that more relevant documents tend to appear at the top of the ranking.
The logistic and examination models are able to make this distinction, intuitively by taking advantage of the position change of some of the urls in the click logs.
The accuracy of CTR prediction may not directly translate to relevance.
In the second set of experiments, we use the predicted relevance directly to rank urls.
In this case, the relative order of click prediction is more important than its absolute values.
We compare the DBN model with the cascade model and the logistic model.
We also include in the comparison a baseline ranking function   that uses many other ranking signals (e.g.
BM25 scores).
This function is typical of ranking functions used by web search engines.
The data is di erent from above: we considered only the queries for which we have editorial judgments and for which we have at least 10 sessions over a period of several months.
This resulted in 3153 queries and 44.5M sessions.
The following experimental protocol is used:


 for the urls;
 given by the DBN model;
 (NDCG) [12] at rank 5;
 The results are shown in  gure 5 where we broke down the NDCG5 as a function of the minimum number of sessions required for a url.
For instance, the minimum number of sessions of 1000 means that the NDCG5 has been computed on all the (query,urls) pairs such that the number of sessions for this url is at least 1000.
With the increasing number of sessions for each url, we expect the click prediction to be more accurate and more con dent, leading to improved ranking of the urls.
This is exactly what we have observed in  gure 5   for all the click models, NDCG5 improves with the number of sessions.
However, when we restrict the experiment to higher number of sessions for the urls, fewer urls are left for each query.
In the extreme case, a query could contain only one url and the NDCG5 would always be one.
Indeed the average number of urls per query is 10.5 overall, but if we restrict ourselves to urls with more than 10,000 sessions, this number goes down to 8.
The NDCG5 is thus less discriminative when the number of sessions for each urls is very high and as a result, the performance of the baseline function is not constant.
That is the reason why we also plot the NDCG5 relative to the baseline function (right of  gure 5) to remove the e ect of varying number of urls per query.
In general, the DBN model is able to rank the urls better than the logistic model and the cascade model.
As expected, with the increasing number of sessions for the urls (i.e. predictions are more con dent), both the NDCG5 and the relative NDCG5 increases.
As a special case of the DBN model (  = 1 and su = 1), the cascade model behaves very similarly as the DBN model but with a lower NDCG5, which con rms the necessity of introducing the notion of satisfaction su and perseverance  .
The cascade model su ers indeed from being able to only consider sessions with exactly one click.
On the other hand, the logistic model behaves very di erently than the cascade model and the DBN model.
The di erence of relative NDCG5 between the DBN model and logistic model is large when the number of sessions is small.
When the number of sessions becomes larger, the di erence between DBN and logistic models gets smaller, mainly because there are less number of urls for each query.
Given the above observations, we then  x the minimum number of sessions to 10 and the minimum number of urls per query to 10.
As a result 392 queries pass the criteria and on average each query contains 13.4 urls.
We then compute NDCG5 on this data set.
As shown in table 1, the NDCG5 for the DBN model is 5.8% and 2.4% better than that of the logistic model and the cascade model respectively.
All the di erence are statistically signi cant according to a Wilcoxon sign-rank test (p   0.001).
In order to quantify the in uence of the satisfaction variable su, we also ranked according to au only instead of (6).
The di erence is only
 7 an extension of our model resulting in a better modeling of the satisfaction.
minimum number of sessions for each url (left).
The NDCG5 is also plotted relative to the baseline function (right).
Table 1: NDCG5 computed when requiring at least 10 sessions per url and at least 10 urls for each query.
Right column shows the relative di erence with respect to the DBN model.
Logistic Cascade
 DBN (au only) Baseline   -5.8% -2.4%



 -0.5%
   Compared to the baseline ranking function  , the NDCG5 for the DBN model is only 6.3% worse.
Given the baseline function   uses more than hundreds of ranking signals and is trained with rather large set of editorially labeled data, this indicates the predicted relevance are very accurate in terms of ranking.
We then try to improve the baseline function   by using the predicted relevance from the DBN model as an additional ranking signal.
About 0.8% NDCG5 gain was achieved.
Furthermore, this signal is observed to be one of the top 10 important ranking signals among hundreds of ranking signal, indicating the high correlation between the predicted relevance and relevance.
relevance Machine learning for web search ranking has  rst been introduced in [4]; we follow here the gradient boosted decision trees framework applied to pairwise preferences [20].
We have two sets of pairwise preferences:
 and 126k urls, resulting in about 1M preference pairs;
 urls (corresponding to 420k unique queries) by  lter-ing based on a threshold on the con dence (equation (8)); converting the relevance scores (equation (6)) into preferences yields about 2M pairs.
For each (query,url) pair we extract a feature vector x.
A pair (xi, xj) in the preference set indicates that xi is preferred to xj, which should ideally translates to f (xi) > f (xj), where f is the ranking function.
The boosting algorithm optimizes the following objective function (see [20] for details on the boosting procedure): max(0, 1   (f (xi)   f (xj)))2 + max(0, 1   (f (xi)   f (xj)))2.
(7)

 (xi,xj ) PE (xi,xj ) PC 1    
  
 Figure 6: Relative DCG5 for various values of   (see equation (7)).
The relative DCG5 is normalized to be 1 for   = 0, which corresponds to learning only based on editorial judgments.
On the other hand when   = 1 only click data are used for learning.
The objective function is thus a combination of editorial based and click based preference.
The test set is a held
 mulative Gain (DCG) at rank 5 is computed.
The relative performance as a function of   is plotted in  gure 6.
We can draw two interesting conclusions from this plot:
 than a standard model learned with editorial judgments; this is remarkable because in this experiment the set of editorial judgments is relatively large.
This is an indication that learning from clicks can be very valuable for markets where there are few or no editorial judgments.
DCG, which is considered substantial in the web search ranking community.
So even in markets where a lot of editorial judgments are available, we can still leverage clicks to reach higher DCG.
Finally note that the evaluation has been done with an editorial metric.
But because of the discrepancies between clicks and editorial judgments discussed below, we expect that our model trained on clicks would perform even better had we evaluated it on a click based metric.
As we have shown in  gure 3, the best prediction of the CTR at position 1 was obtained for   = 0.9.
But   = 1 produces only slightly worse prediction.
And this particular setting is interesting because in this case, the inference is much simpler.
Indeed the user then keeps examining until he is satis ed, which means that the last click provided a satisfying result and the results below it were not examined.
The forward-backwards algorithm and EM are thus not needed because there is no ambiguity on the examination variables: E1 =   = E(cid:96) = 1 and E(cid:96)+1 =   = E10 = 0 with (cid:96) being the position of the last click.
The latent variables au and su are estimated using simple counting as described in algorithm 1.
Algorithm 1 Simpli ed model estimation for   = 1.
u , sN u , aD Initialize aN current query.
for all sessions do u , sD u to 0 for all urls u associated with for all u above or at the last clicked url do u   aD aD u + 1 end for for all u that got clicked do u + 1 u + 1 u   aN aN u   sD sD end for u   sN sN u + 1, where u is the last clicked url.
end for//  a,  a,  s,  s are prior Beta parameters for au and su.
for all urls u do au = (aN su = (sN u +  a)/(aD u +  s)/(sD u +  a +  a).
u +  s +  s).
end for

 In the last two sets of experiments, clicks are used as a proxy for editorial judgments.
It is important to evaluate the correlation between relevance estimated by our click model and actual relevance given by editors.
A natural measure is the number of contradicting pairs (related to Kendall s tau test).
We converted all the editorial judgments in pairwise preferences and did the same from the relevance scores extracted from our model.
On the intersection of both sets of pairs, there is a disagreement in the preference for 20% of the pairs.
We investigated the reasons for these discrepancies and found that excluding errors in editorial judgments, theses reasons can be summarized into two main categories: 1: popularity is not necessary aligned with relevance; 2: clicks measure mostly perceived relevance, while editors judge the relevance of the landing page; An example for the  rst category is the query  adobe : the home page www.adobe.com seems to be the most relevant url but most users click on the acrobat reader link.
Another example query is  bank of america .
Most users prefer to click on the online banking page http://www.bankofamerica.
com/onlinebanking/ while editors tend to consider the home page http://www.bankofamerica.com as the destination page for this query.
Given this inherent discrepancy between relevance and clicks, we may never be able to close the gap completely.
On the other hand, it may be useful to leverage the predicted relevance to re ne the de nition of relevance and hence the guideline for editorial relevance judgment.
The second type of inconsistency can be further divided in two subcategories: cases where relevance of the search result snippets are very di erent from that of the landing page; and cases where users click based on the trustworthiness of the page rather than the relevance of the page.
The  rst subcategory most often is related to the presentation of the title and summary of the url.
An example query for the second subcategory is  travel insurance .
While there are many small insurance companies focus on selling travel insurance (more relevance in terms of relevance judgment), the users still tend to click more often on the sites of branded insurance companies, where travel insurance is only a small fraction of their business.
In summary this study made clear that de ning relevance is a complex matter and that clicks and editorial judgments are two related but distinct ways of answering the user needs.
So far all the above experiments only considered web search results.
In fact, a wealth of information has been blended into the search result page: the most noticeable one is sponsored search results.In addition, search engines nowadays tend to contain many links to help users to quickly navigate to their search destination, including related search, query misspelling suggestion and short cuts.
We discovered that in many cases, lack of clicks on search results is due to the fact that users have chosen to click on urls from one or more of the above sections.
We thus would like to model the clicks on the entire search result page.
We modify the DBN model slightly to consider the whole page clicks.
Previously we only considered the top
 urls: the leading url de ned as the urls at the top of search result page (e.g.
sponsored search, spelling suggestion); the trailing url de ned as the urls at the bottom of the search result page (e.g.
pagination).
examined the urls in the search result section; while clicks on the trailing url indicates the users most likely are unsat-is ed with previous urls.
Again, we use the predicted relevance from the re ned DBN model to rank urls and compute NDCG5.
The results are summarized in table 2.
As for table 1, we only keep urls with at least 10 sessions and the queries with at least 10 urls.
The re ned DBN model outperforms the original DBN model by 2.2% and the di erence is statistically signi cant (p   0.001).
Table 2: NDCG5 for DBN when considering whole page relevance.
Setting is the same as table 1.
Right column shows the relative di erence with respect to the improved DBN model.
DBN   10 nodes DBN   12 nodes DBN   12 nodes (au only) Baseline   -2.2%


 -1.2%
   In addition, for this improved DBN model, the satisfaction variables su seem to be better estimated   there is a 1.2% drop (p   0.001) in accuracy if we rank according to au only (as compared to 0.5% for the original model, see table 1).
This is probably because we now model clicks on the bottom of search page such as  next : when a user click on the next button, it is likely that he is not satis ed with the last url he visited.
Our original DBN ignored this fact and incorrectly attributed high satisfaction for this kind of url.
Extracting relevance information from click logs is a challenging but valuable task for web search ranking.
In this paper we have proposed a novel click model based on dynamic bayesian network.
The major contribution of the work is to introduce the notion of satisfaction to separately model the relevance of the landing page and perceived relevance at the search result page.
We have demonstrated in this paper that the DBN model outperforms other click models.
There are several extensions which can improve the accuracy of our model.
Other than the preliminary experiments we have done to consider urls in the entire search result page for click modeling, another extension is to incorporate the time users spent on a page, which is expected to be very helpful in predicting the user satisfaction.
We can also allow the user to be satis ed even if he does not click (e.g.
he might have ful lled his request just by reading the abstract).
In addition, the satisfaction variable can be continuous instead of binary: for informational queries, the user typically  nds bits of information on each page and stops when his overall information need is ful lled.
This can be done by introducing a dependency between the Si variables.
Finally, a more challenging extension is to consider a nonlinear examination model: this would require to model both forward and backward jumps.
Most of existing click modeling methods are biased by the search engine used to collect the clicks, and they mostly serve as  positive feedback : if a document was never presented to the user, then the document would not be clicked.
Another direction to extend the work would be to utilize the query smoothing to infer relevance for extra documents.
The authors would like to thank Ralf Gutsche for valuable assistance on click data processing.
The authors also wish to thank Georges Dupret, Narayanan Sadagopan and Belle Tseng for insightful discussions.
