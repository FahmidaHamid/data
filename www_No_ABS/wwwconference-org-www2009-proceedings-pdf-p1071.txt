Almost two-thirds of American households, irrespective of age, background, and gender (40% women) play computer or video games [3].
The insight behind  human computation  is to use  games with a purpose  (GWAPs) to channel this time and energy toward solving computational problems that are easy for humans but signi cantly hard for computers (i.e., automation) [5].
A very successful example of such games is the ESP game [5], also released as the Google Image La-beler, where players provide accurate labels for images while playing the game.
We propose Thumbs-Up as another example of a GWAP to address the (document) ranking problem for search engines.
In Thumbs-Up, players are shown the same input (a query and images of two relevant search results) and must agree which search result is more relevant to the query.
Thumbs-Up is based on two fundamental hypotheses.
The  rst states that given two documents, humans are better at ranking their (perceived) relevance than a computer.
This Copyright is held by the author/owner(s).
Figure 1: Screenshot of Thumbs-Up.
is especially true if ranking involves personalization.
Moreover, although computers (and search engines) employ sophisticated machine learning methods to rank search results, humans are still needed.
They are directly employed to provide absolute or preference judgments, or are indirectly utilized as users to provide preference judgments through their clicks.
The second hypothesis states that preference judgments are easier for humans to make than absolute judgments, which is well supported by some recent work, e.g., see [1] and the references therein.
As for prior work, we are unaware of any application of GWAPs to search engine ranking [5].
In [1] and the references therein, methods are proposed to extract preference judgments from users but without using a game or GWAP framework.
The analysis of the collected results is also performed di erently.
Game rules.
A player logs in and is randomly matched with another player.
Both players are shown the same input query and images of two web pages deemed relevant to the query.
To increase their scores, the players must agree on the same page as more relevant.
Game features.
We incorporated several well-known game features to make Thumbs-Up challenging and fun, which include: a time limit (60 s), score keeping (60 per successful match), daily and all time high score lists, and randomness in selecting partners, queries, and images.
Features unique to Thumbs-Up include: image magni cation on mouse-over and single-click selection.
We selected queries randomly from web search user query logs.
The top  ve search result (URLs) for each query were then scraped from major search engines.
Each URL was judged by professional judges, and a digital image of the page was generated.
We implemented Thumbs-Up using a Java/Tomcat front-end with MySQL on the back-end, and released it internally to engineers at Yahoo!
Web Search.
Within one week we had collected the following statistics: 52 users, 20 queries, 1223 games, 1349 games (including skipped games), and 102 unique URLs viewed.
Moreover, about 1/3 of the users played more than 20 games and four of them played more than 100.
More than half of the games ended up with a score (agreement) for almost all users.
The time spent per game varied from 5 s to 40 s, with an average of slightly less than 15 s. We require 10 games to rank  ve URLs for a given query, but collected 6 times more games on the average.
These statistics seem to indicate that the game was fun and challenging to play.
We ranked the Thumbs-Up results using the following methods: (1) gain: ranking by judgments from professional judges, resulting in an upper bound on DCG; (2) C1, C2, and C3: rankings using the three cost functions C1, C2, and C3, respectively; (3) kemeny: ranking using the Kemeny rule (C in Eq.
2); and (4) scrape: ranking of a major search engine.
Fig. 2 shows the DCG moving averages as each query is added.
The DCG values for the last query give the  nal DCG values per method.
From this  gure, we see that  gain  performs the best, as expected, and  kemeny  performs the worst, which is surprising as it optimally minimizes the number of disagreements.
We also see that our cost functions perform remarkably well: C1 almost matches the major search engine (within 1%), and the other two are identical and close to  gain  (within 1%), beating the major search engine by up to 8% and the Kemeny rank aggregation by up to 13%.
We believe this result is signi cant considering the magnitude of the investment in search engine ranking technology.
We developed a GWAP called Thumbs-Up for humans to play to rank search results.
Our limited experience suggests that Thumbs-Up is not only fun and challenging to play, but also performs surprisingly well.
