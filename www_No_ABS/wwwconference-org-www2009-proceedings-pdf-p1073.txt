The main objective of a search engine is to help the user ful ll his information need e ciently.
Major search engines usually provide suggestions in the form of queries that are somehow related to the user information need, with the goal to point the user in the right direction.
Authors proposed di erent techniques to address this problem [1, 3].
The design of e ective and e cient algorithms for such suggestions is a complex and challenging task, as well as the evaluation of them.
In fact, human-based evaluation has been Copyright is held by the author/owner(s).
traditionally used.
Although very precise, its main inconvenience is the non repeatability of the experiments, which makes di cult an extensive comparison of such techniques.
In this work we introduce the Search Shortcut Discovery Problem as a problem related with the use of query suggestions in search engines, and the potential reductions obtained in the user session length.
We de ne an evaluation methodology for this problem based on query logs that will allow a straightforward, yet interesting, comparison of the techniques that could be applied to this problem.
We have focused on the application of Collaborative Filtering algorithms [2] to this problem, a technique based on user preferences, that has been successful in several domains, such as electronic commerce.
Collaborative Filtering algorithms can be divided in memory-based and model-based approaches.
In order to apply these techniques to the query shortcuts problem, we have to extract information from the query log data.
As the goal in the shortcuts problem is to recommend queries for a given session, it seems reasonable to treat each Query Session as a user and each Query as an item.
Then, the query ratings must be inferred from the information in the query log.
As a preliminary approach, in this work we have given to the last query of a session a possitive rating if the user has clicked on, at least, one result, or a negative rating otherwise.
All remaining queries are considered neutral.
The idea of Search Shortcuts can be easily explained with a simple example.
Suppose a (su ciently) high number of users has queried the engine for q1, q2, q3, and  nally, after asking for q4, they clicked on a result presented in the Search Engine Result Page (SERP).
We can assume that q4 is a good query, since it was followed by a click.
Therefore, we can also consider q4 as relevant for users interested in topics related to q1, q2, and q3.
Whenever another user starts to search for topics related to q1, q2, or q3, q4 will be proposed as a shortcut.
Obviously, the earlier a relevant shortcut is shown during the user session, the more e ective it has to be considered.
Following this idea, a given algorithm can be easily evaluated using the following function:   =
 n tP q h( t|) m=1  q =` |t   m   f (m) |h( t|)| (1) Where:    u =< q1 .
.
.
qn > is a query session for a given user.
   t| is the head of   up to t   n is the sequence of the  rst t queries in  , i.e.  t| =< q1, .
.
.
, qt >    |t is the tail of   from t   n is the sequence of the last n   t queries in  , i.e.  |t =< qt+1, .
.
.
, qn >   h : S   2Q is the k-way shortcut function, taking as argument a session and returning a set of queries of cardinality less then k, i.e. |h ( )|   k.
  f (m) is a monotonic increasing function.
  The function [q =  m] = 1 if and only if the query q is equal to the query  m.
As data set for our experiments we have used a subset of the AOL query log, consisting of 3,558,412 records.
To perform the evaluation, we have divided our query log data in two subsets: training and evaluation, by randomly chosing a percentage of the sessions as the data to be used to train the algorithms.
Then, we fed the algorithms with the  rst two queries from each session in the evaluation set.
The evaluation is performed using both traditional metrics and the similarity measure proposed in section 2.
The results, as can be seen in Fig. 1, clearly show the di erences between Memory and Model-Based approaches when applied to this problem.
Memory-Based algorithms showed pretty accurate results, but they cover a low fraction of the data, thus resulting useless for the search shortcut problem.
Results on other algorithms are a bit surprising, but we can easily relate such results with both the limitations of traditional metrics, and the way we are implicitly assigning ratings to queries.
Regarding this last issue, the simple three-rating schema we have proposed leads to a rating matrix where most queries have a neutral rating, thus biasing the evaluation, specially on the MAE metric.
On the other hand, limitations on evaluation methodology are clearly observed on classi cation and rank accuracy metrics, where all algorithms have obtained modest results.
This is a problem related with both the sparsity and high volume of the dataset, and the way these metrics are measured in an o ine dataset.
Anyway, by taking into account these limitations, we can note the pretty good results of the Item-Mean, Trends-Based and Personality Diagnosis.
This shows that both techniques can obtain enough information from sparse data, so further experiments on these algorithms seem valuable.
In this work we have introduced the Search Shortcuts problem, directly related with recommender systems and query suggestion, and we have presented a well-de ned model and evaluation framework, that allows the comparison and Figure 1: Results in the predictive accuracy metrics for several Collaborative Filtering algorithms.
evaluation of algorithms using an o ine dataset.
Speci -cally, we have studied the application of Collaborative Filtering techniques to the search shortcuts problem, evaluating several algorithms on a real query log data.
Our experiments have shown the limitations of traditional algorithms, as the high volume and sparsity of the query log data lead to poor results in most cases.
Speci cally, traditional memory-based approaches present very low coverage, due to the fact that they are only based on a small part of the information available.
Traditional metrics and evaluation methodologies have also shown some important limitations.
Classi cation accuracy metrics obtain valuable results, but only if the details of the evaluation methodology are taken into account.
In particular, the usage of o ine and sparse datasets impose several limitations in the evaluation, as items considered relevant by the algorithm cannot be compared in many cases with real data.
Several limitations have also been observed in the way used to extract information from the query log data.
The three level rating (positive, negative, neutral) does not perform as expected, especially because most queries are in fact neutral.
The way we map queries to items can also be improved, by considering the terms that compose a query.
Techniques such as stemming or stopwords removal, can effectively reduce the data sparsity, and thus reduce the main reason of the poor performance of most algorithms.
