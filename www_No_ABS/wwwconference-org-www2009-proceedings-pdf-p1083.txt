Text classi cation is widely used in information retrieval area and text mining area, especially in the web environment.
Web-scale taxonomy classi cation entails high e -ciency of the classi er, and Naive Bayes Classi er (NBC) naturally lends itself to such tasks because it is simple, fast, easy to implement and relatively e ective.
NBC has been studied for a long time in small datasets[2].
Unfortunately, when employing NBC on tasks with thousands classes, we found that it achieves extremely bad performance which we will discuss in detail later.
One may think this poor performance must come from some intrinsic limitations of NBC (say, the independent assumption).
And this gives the motivation in some works to try to use extra information (e.g., the hierarchy structure of the taxonomy)[3] or other complicated classi cation algorithms to handle the case of a large number of classes [1].
In this paper, we aim to maintain the advantage of NBC (e.g.
simple algorithm, easy implementation and fast computation), and to achieve good performance at the same time.
We discover that two largely ignored problems of NBC can severely hurt its classi cation performance.
We call them contradiction pair problem and discriminative evidence cancelation problem, which will be discussed in Section
 Copyright is held by the author/owner(s).
Hongyuan Zha College of Computing Georgia Institute of Technology Atlanta, GA - 30332 zha@cc.gatech.edu rather benign for small number of classes but manifest themselves notably in the presence of a large number of classes.
To deal with the two problems, we propose two di erent modi cations on NBC: Weight Manipulation Naive Bayes and Parametric Smoothing Naive Bayes.
Our modi ed NBC can improve the accuracy from 9% to about 50% for a 1000-class case, and from 20% to about 70% for a 200-class case, while maintaining all the advantages of NBC.
Moreover, each of them is designed to  x the two problems without making NBC much slower or signi cantly more di cult to implement.
For word wi and class c, standard NBC based on multino-mial model and Laplacian smoothing yields maximum likelihood estimation of the class conditional probability pml(wi|c) = c/Nc, and smoothed estimate ps(wi|c) = N i N i Nc+V .
To classify a test document d, NBC assigns d by the label (cid:2)(d) = arg maxc{log p(c) + c is the frequency count of the word wi appearing in training set of class c, Nc = c, fi is the occurrence of wi in the test document d.
wi d fi log p(wi|c)}.
Here N i (cid:2) (cid:2) i N i c+1 The  rst problem is what we call the contradiction pair problem.
It means, for a word and the classes, the smoothed estimates do not preserve the order of the maximum likelihood estimates (MLE), i.e. for a word w and two classes c1, c2, we may have MLEs with pml(w|c1) > pml(w|c2) but the smoothed estimates with ps(w|c2) < ps(w|c2).
Figure
 on smoothed estimates to make class predictions, the consequence of contradiction pair problem is that NBC loses some evidences carried by the maximum likelihood estimates.
Maximum Likelihood Estimation pml(w|c5) pml(w|c2) pml(w|c1) pml(w|c9) e g r a
 l l a m
 Smoothed Estimation ps(w|c3) ps(w|c1) ps(w|c2) ps(w|c7) e g r a
 l l a m
 Smoothing Contradition Pair Figure 1: Illustration of contradiction pair problem.
We call the second problem discriminative evidence can-celation problem which can be illustrated using a simple example in Figure 2.
Assume a document with three words d = {w1, w2, w3} and class c is one of the candidate classes Training Na ve Bayes Testing ps(w1|c(cid:397))=0.1 ps(w2|c(cid:397))=0.1 ps(w3|c(cid:397))=0.1 and d={w1,w2,w3} p(d|c(cid:397)) =1.0e-3 p(d|c) =0.24e-3 ps(w1|c)=0.49 ps(w2|c)=0.49 ps(w3|c)=0.001 ( pml(w3|c)=0 ) Discriminative Evidence p(c|d) < p(c(cid:397)|d) c(cid:397) beats c (cid:3) Figure 2: Illustration of discriminative evidence can-celation problem where we assumed p(c) = p(c ).
Besides, there is p(c|d) = p(d|c)p(c)/p(d) to label d, suppose {(w1, c), (w2, c)} are discriminative evidence.
The smoothed estimation ps(wi|c) is discriminative evidence when ps(w1|c) is far larger than the average of ps(w1|cj), i.e. ps(w1|c) >> Avgj{ps(w1|cj)}.
We assume the same is true for ps(w2|c).
In this case, for d, it is natu-(cid:3) ral to prefer c over c ) is discriminative evi-(cid:3) dence for class c .
Unfortunately, standard NBC does not do this: when p(w3|c) is very small, class c will not be able to (cid:3) compete against c ) are only moderately large.
That is to say, the e ects of discrimina-tive evidences are overwhelmed by low probability estimates of other words in NBC.
unless ps(w3|c (cid:3) if ps(w1|c (cid:3) ), ps(w2|c (cid:3) ), ps(w3|c (cid:3) Contradiction pair problem and discriminative evidence cancelation problem manifest themselves notably in the presence of a large number of classes.
It is because: (i)More classes mean more chances of contradiction pairs.
Average to every prediction, the number of possible contradiction pairs is linear to the number of class.
(ii) there is a large amount of zero-frequency in word-class frequency table.
They may get small and unreliable values in smoothing and then easily overwhelm those discriminative evidence.
(iii)the values of Nc may vary more greatly, which largely randomize the order of ps(w|cj).
The above claims can be veri ed by some careful calculations.
We propose two di erent modi cations in this section.
The  rst is Weight Manipulation Naive Bayes (WMNB).
We regard estimations p(wi|c) as the weight instead of probabil-i p(wi|c) = ity, which means they are free from the constraint
 u to denote the weight.
Our idea is that we could directly use maximum likelihood estimations as the weight for nonzero frequencies of wi in cu, and use moderate small weight for zero frequencies.
The modi ed training process of Naive Bayes is presented as: (cid:2) (cid:3) i u = z (cid:2) log pml(wi|cu) = log N i  / j:N j u=0 1 u   log Nu (cid:2) u (cid:2)= 0 if N i otherwise (1) j:N j (cid:2) u}.
wi d fi   zi WMNB can alleviate contradiction pair problem: where   < 0 is a constant value, u=0 1 counts the number of zero-freqency word for class c. To classify d = {f1, f2 .
.
.
fV }, the modi ed testing process of Naive Bayes is: (cid:2)(d) = arg maxu{log p(cu) + if we have pml(wi|cu)   pml(wi|cv) > 0, there are two possibilities: (i) pml(wi|cu) > pml(wi|cv) > 0: then we have zi u   v = log pml(wi|cu)   log pml(wi|cv) > 0 (ii) pml(wi|cu) > zi pml(wi|cv) = 0: then since   is the constant value to turn, we can always take one small enough   that meets zi v. For discriminative evidence cancelation problem, WMNB can alleviate it by controlling the sum of those small weights.
u > zi We call the second modi cation Parametric Smoothing Naive Bayes.
It tries to modify traditional smoothing methods to alleviate the two problems in this paper.
Besides Laplacian method employed by standard Naive Bayes, we can also borrow other smoothing methods from language model.
We modify four well-known smoothing methods and lead to four variants: PNBC-Laplace, PNBC-Absolute, PNBC-Linear and PNBC-WittenBell.
The basic idea is that NBC should be able to control the extent of reducing the probability of non-zero-frequency words.
For example, in PNBC-Laplace, we take ps(wi|c) = w   , with a moderate samll   decided by cross validation, the smoothed estimations are close to maximum likelihood estimations and avoid discriminative evidence cancelation problem as well.
Table 1: Performance Comparison on three datasets (cid:2) N i c+  Nc+ Standard NB
 PNBC-Laplace PNBC-Absolute PNBC-Linear PNBC-Wittenbell




















 Table 1 shows the overall performance of WMNB and the variants of PNBC on a 1,048 classes and 185,728 documents Open Directory Project(ODP) dataset; 505 classes and 1,910,741 documents Yahoo Question Answer (YahooQA) dataset; and 20 Newgroups as well.
We use standard NBC as the comparisons.
It can be seen clearly that our mod-i ed Naive Bayes results in remarkable improvements on ODP and YahooQA by alleviating the contradiction pair problem and discriminative evidence cancelation problem.
Moreover, the performances of our modi ed algorithms are similar to each other.
In other words, not a speci c kind of variants, but their shared underlying principles proposed before count.
Figure 3 plots the performance curve for standard NBC, WMNB and PNBC-Laplace on YahooQA and ODP datasets with di erent number of classes.
We can see that WMNB and PNBC-Laplace beat Standard NBC at every points.
These evaluations proves our modi ed Naive Bayes algorithms are e ective on web-scale taxonomies.
