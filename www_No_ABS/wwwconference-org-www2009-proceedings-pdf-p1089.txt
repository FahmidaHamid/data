Crawling is one of the most important tasks of a search engine, and is also a critical part of many other web applications.
The breadth, depth, and freshness of the search results depend crucially on the quality of crawling.
As the number of pages and sites on the web increases rapidly, deploying an e ective and e cient crawling strategy becomes critical for a search engine.
A typical crawler [2] maintains a list of unvisited URLs called frontier, which is initialized with seed URLs.
In each crawling loop, the crawler picks a URL from the frontier, fetches the corresponding page, parses the retrieved page to extract URLs, and adds unvisited URLs to the frontier.
Typically, the crawler needs to keep revisiting some or all the URLs to check if they are updated since the last crawl.
Due to the in nite nature of the web and the competition between getting new URLs and refreshing the old ones, even web-scale crawlers can never crawl  all  the URLs from the web.
Di erent crawling strategies resulting from di erent ways of ordering the URLs in the frontier can explore the web in di erent ways.
However, all of them start from the seed pages and proceed by exploring the neighborhoods of the Copyright is held by the author/owner(s).
seed pages in one way or another.
Thus, to a large extent, selecting good quality seed determines the quality of the crawl.
Since there exists a wide variety of crawling strategies, in this paper, we make two assumptions.
One, a crawler crawls pages only within h hops from the seed, and two, the number k of seeds is given.
Both of these assumptions are not crucial for the algorithms we will describe later, but will simplify the explanation.
Intuitively, the problem of seed selection is, given a currently known portion of the web and the desired number of seeds k, to select seeds so that as many as possible new good pages will get crawled, and as many as possible currently crawled good pages will be retained.
There are several naive strategies which can achieve this goal.
The simplest strategy, used as a baseline in our experiments, is to select the seeds at random.
The second strategy is to select k pages with the highest PageRank.
This makes sense since one can expect to  nd high quality pages around other high quality pages.
Another intuitive strategy is to select k pages with most outlinks.
A heuristic, which can improve the above strategies, is to  rst split the known portion of the web into smaller units, such as web sites, and then select seeds independently for each web site.
Such strategy allows distributing seeds  evenly  across the web, allocating to each web site a fraction of the total number of seeds, which is proportional to the site s importance.
It also allows performing seed computation in parallel and on a smaller data set.
However, naive seed selection strategies cannot directly optimize the criteria we are interested in.
To formally de ne the problem of seed selection, we assume that every crawled web page has an associated value.
Higher value indicates higher quality or higher potential to discover new pages calculated, for example, as the number of uncrawled URLs the page links to.
In addition, the value can be negative if the page is undesirable, such as a spam page.
We can now de ne the seed selection problem as maximizing the value of the portion of the web graph  covered  by the seeds.
Definition 1 (Seed Selection Problem).
Given a directed graph G = (V, E), a function w : V   R, assigning a weight w(v) to every v   V , and edges unweighted, given the number of seeds k, and the number of hops a seed covers h, select the seeds so that w( k i=1Ai) is maximized, where Ai = {v|v   V, v within h hops of seed i}, w(A) =(cid:80) v A w(v).
instance of the Maximum K-Coverage Problem, which is known to be NP-hard [1].
However, a greedy iterative approximation exists for this problem, which achieves 1   1 approximation [1].
The algorithm is shown below.
e Algorithm 1 Seed Selection Algorithm Input: Weighted Graph G of (a portion of) the web, Maximal Seed Number k, Number of hops allowed h Output: Selected Seeds S Algorithm:





 Remove s and all covered vertices; Update G Add s to S IF G is empty THEN break Find s which covers maximal value within h hops Figure 1: Coverage of Pages Unfortunately, performing even a single iteration of the algorithm is computationally expensive for large h, due to the exponential in h complexity of step 2.
Therefore we resort to an approximation again.
We propose and evaluate four approximation procedures for performing step 2: L1 : Choose a page p with the highest numOutlinks(p) L2 : Choose a page p with the highest sum of values of all pages within 1 hop of p L3 : Choose a page p with the highest sum of values of all pages within 2 hops of p SCC : Choose a strongly connected component in the graph with the highest sum of page values, select a page p within that component according to the L2 algorithm.
Below we describe our experimental setup and presents results evaluating the quality of the above algorithms.
To evaluate the performance of the algorithms, we selected a random sample of 1000 web sites from Malaysian web, each containing at least 100 pages and having at least 1 external link into the site.
We used an experimental web-scale crawl from the summer 2008 to obtain all pages crawled from these web sites.
We also obtained other page attributes, such as whether the page was determined to be spam, whether the page was clicked on in search engine results, and how many uncrawled pages the page links to.
We assigned a value to each page according to the following rules: For every page p set w(p) = 1.
If p is spam, set w(p) =  10.
If p was clicked, set w(p) = 10.
Let n be the number of uncrawled pages the page links to.
Set w(p) = w(p) + n   Puniq, where Puniq is the probability of uniqueness of a newly crawled page on the site.
This probability is calculated based on past crawl statistics.
We implemented the three naive algorithms and the four graph-based algorithms described above.
By measuring the coverage of pages and of value independently on each web site, we evaluated the performance of the algorithms in terms of the percentage of improvement over the random seed selection strategy.
In the experiment, we varied the number of seeds generated for each site from 1 to 10, keeping the number of hops  xed at 5.
The  gures shown below present the average performance over 1000 hosts.
Figure 1 shows the results for the page coverage and Figure 2 shows the results for the value coverage (error bars Figure 2: Coverage of Value indicate standard errors).
The following conclusions can be drawn from these results: First, random strategy performs poorly, while all other strategies outperform it signi cantly.
As one may expect, the improvement over the random strategy is larger when the number of seeds is small, and it decreases as the number of seeds grows.
Second, all four of the graph based algorithms outperform the naive seed selection strategies.
Third, L1 algorithm, which uses outdegree instead of value, produces best results in terms of the coverage of pages, but performs much worse in terms the coverage of value.
Fourth, L2 and L3 algorithms perform similarly (L3 slightly outperforms L2), suggesting that using 1 hop approximation is enough to identify a good seed.
Finally, the SCC algorithm does not perform as well as L2 and L3.
In this paper we discuss the problem of seed selection for a web-scale crawler.
We formalize this problem as a graph theoretic problem, analyze its complexity and propose several approximate algorithms.
Experimental results on a dataset of 1000 real web sites demonstrates that our algorithms sig-ni cantly outperform the naive seed selection strategies.
