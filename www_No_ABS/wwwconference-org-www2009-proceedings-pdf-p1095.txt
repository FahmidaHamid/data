Sponsored search is a successful form of online advertising, with annual revenue exceeding billions of dollars.
With the goal of branding or/and marketing, advertisers create ads and bid on relevant keywords.
The auction winners have their ads displayed as sponsored links alongside the organic search results when bidded terms are queried.
Keyword generation/suggestion methods are employed to assist advertisers in  nding all the terms relevant to their products or services.
The term relevance is crucial to capture valid queries and clicks from their potential customers.
Provided an initial keyword which represents a concept such as  shoes  (i.e. a seed term), some typical keyword generation methods make use of a set of elements (such as URLs, frequent queries and meta-tags) that enrich the meaning of the seed to select high co-occurrence or similar terms as suggestions.
To be e ective, keyword generation requires even hundreds of relevant terms to be suggested for a seed.
Some popular keyword generation tools (e.g.
Google s Adwords Tool) mainly rely on query log mining and have the drawbacks of failing to suggest terms that don t contain the seed and ignoring the semantic similarity between terms.
Recent related work tends to exploit semantic relation  of China (NO.2008BAH26B02).
Supported by the National Key Technology R&D Program Copyright is held by the author/owner(s).
ships between terms.
For keyword generation, Joshi and Motwani [3] present TermsNet, which captures semantic similarity between terms as a directed graph; Abhishek and Hosanagar [1] use a web based kernel function to establish semantic similarity between terms; Chen and et al. [2] exploit the semantic knowledge among concept hierarchy.
However, there is no e ort taking user relevance feedback information, especially that from advertisers, into account.
In this paper, we propose an interactive model to explore relevance feedback for keyword generation.
Our approach also di ers from previous work in formulating the ranking of relevant terms as a supervised learning problem, in which the term relevance is learned from several representative features rather than determined using a speci c measure.
We establish semantic relevance between terms by leveraging relevance feedback, which conveys direct semantic information.
For a seed term, our system  rst employs search engines to match it with a large set of ranked candidate terms that are determined using TFIDF measure.
The candidates achieve a high coverage of relevant terms, but a poor precision.
Users are then required to provide relevant/irrelevant labeling on just a few candidates selected by the system.
Given these labeled candidates, a regression model is trained to predict the relevance scores of unlabeled candidates to the seed, from some representative features about them.
Based on the relevance scores, we re-rank all the candidate terms to improve the precision.
Finally, top ranked terms are selected as suggestions for the seed and returned to users.
Due to the consideration of the performance and user satisfaction, there are two requirements: (1) The samples to be selected for labeling should be as few as possible since labeling is labor-intensive.
(2) The quality of training samples a ects the performance directly.
A natural strategy is to select the most informative samples using active learning.
To be e cient, we represent each seed term using a characteristic document which contains retrieved snippets from top search-hits.
It is similar to [3].
Given a set of seed terms, the corresponding characteristic documents form a corpus.
We remove stop words from these documents and stem the terms using Porter s stemmer as preprocessing.
The TFIDF of all terms in the corpus is used as the initial weight of relevance.
Top weighted terms in a characteristic document are selected as the candidates for the corresponding seed term.
For each (seed, candidate) term pair, the following features characterizing the relevance between them are used as predictor variables: (1) TF and TFIDF of the candidate in the search snippets document of the seed; (2) Inverse TF: the frequency of the seed in the search snippets document of the candidate; (3) Search Snippets Similarity: the frequency with which the snippets of top search results for the seed and the candidate contain the same words.
(4) Common Search URLs: the frequency with which the seed and the candidate share the same search URLs.
i i n o s c e r
 e g a r e v










 Random Baseline1 Baseline0








 Top N Suggested Terms We apply an active learning approach called Transductive Experimental Design (TED [4]), which tends to select candidates (for labeling and training) that are hard-to-predict and representative for unlabeled candidates.
Let V denote both the matrix [v1, ..., vn]T   R n d and the data set {vi}, and X denote both the matrix [x1, ..., xm]T   R m d and the data subset {xi}, where X   V (m < n).
In our case, vi is the feature vector of a (candidate, seed) term pair.
De ne f (x) = wT x as the output function learned from measurements yi = wT xi + i, i = 1, ..., m, where w   R d is weight vector and i   N (0,  2) is measurement error.
Let yi (label) be the relevance score (1 or 0) associated with the feature vector xi.
Consider a regularized linear regression problem, the maximum likelihood estimate of w is given by (cid:4)
 xi   yi)
 +  ||w||2 J(w) = w (w  w = arg min (1) where   > 0 and || || is the vector 2-norm.
It is known that the estimation e = w    w has a covariance matrix given by  2Cw, where Cw is the inverted Hessian of J(w) i=1 m(cid:3) (cid:6) 1 (cid:2) (cid:5) Cw =  2J(w)  w2

 X +  I)  1 (2) (cid:8) n(cid:3) (cid:7) The introduced regularization improves numerical stability since XT X +  I is full-rank.
Let f = [f (v1), ..., f (vn)]T be the function values on all the available data V, then the predictive error f    f has the covariance matrix  2Cf with (3) Cf = VCwV X +  I)
  1



 In contrast to Cw, Cf directly characterizes the quality of predictions on the target data V. Therefore, the total predictive variance on the complete data set V is given by (f (vi)   f (vi))

 =  2T r(Cf ) (4) i=1 One should  nd a subset X which can minimize the total predictive variance.
In our case, we apply sequential optimization of Kernel Transductive Experimental Design [4].
We select 100 category names widely spread over di erent topics from eBay and Amazon to form the set of seed terms.
They re popular among advertisers and customers.
For each characteristic document generation, top 400 Google search-hits are acquired.
Top 400 weighted terms in each seed s characteristic document are selected as candidate terms.
Experimental results are averaged over 20 benchmark seed terms.
Ground truth labeling of relevant/irrelevant was provided by 5 human evaluators who are familiar with related Figure 1: Average Precision Curves materials, and it was performed beforehand after we generated candidate terms, whose initial ranking is regarded as Baseline0.
For the sake of user satisfaction, we select just 10 samples from the set of each seed s 400 candidates for user labeling.
Given these 10 labeled samples, we equivalently apply kernel regression with Gaussian kernels to predict the relevance scores of unlabeled candidates.
The following methods are compared: (1) Pseudo-Relevance Feedback: we select 5 samples as positive ones from the top of the initial ranking and 5 samples as negative ones from the end.
It is regarded as Baseline1 since no user labeling is required; (2) Random Sampling: we randomly select samples and repeat 100 rounds to get an average result.
(3) TED is employed to select samples.
  is  xed as 0.0001.
Figure 1 shows the average precision curves for the relevance of top 200 suggested terms after re-ranking, where precision is de ned as the ratio of relevant terms generated to the total number of terms generated.
As illustrated, both random sampling and TED signi cantly outperform pseudo-relevance feedback since additional user relevance feedback is used.
In this case, TED consistently performs the best, with 88.3% average precision achieved for top 100 suggestions, and 65.1% for top 200 suggestions.
TED achieves 2.2% performance improvement for top 100 suggestions, and 2.7% for top 200 suggestions, comparing to random sampling.
This is because active learning selects the training samples which can minimize the total predictive variance.
We have shown the e ectiveness of our relevance feedback based interactive model in generating relevant terms.
Experimental results highlight the advantage of the active learning algorithm in selecting the most informative samples for user labeling.
Furthermore, our approach achieves a good tradeo  between the performance and user satisfaction.
In practice, our keyword generation framework can be extended to other applications such as term clustering.
Currently, only single word terms are considered.
We are going to explore the use of phrases in the future work.
