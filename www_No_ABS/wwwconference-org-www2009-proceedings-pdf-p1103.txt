Discussion threads have long been a popular option for web users to exchange opinions and share knowledge, e.g.
thousands of web forum sites, mailing lists, chat rooms, and so on.
A discussion thread usually originated from a root post by the thread starter.
Fig. 1 gives an intuitive description of a thread1.
It contains 7 posts.
The  rst post is a piece of news about the release of  SilverLight 2.0 .
Some users comment on this post, i.e., the 2nd and 3rd posts are about the  update time ; some users have further questions and initiate sub-discussions, i.e., the 5th, 6th, and 7th posts are about  Javascript communication ; others troll or complain, i.e., the 4th post.
As more users joining in and making comments, the thread grows, forming a nested dialogue structure as shown in the left part of Fig. 1.
Furthermore, discussion threads show rich complexity in the semantics.
Since users always response to others, previous posts a ect later posts and cause the topic to drift in a thread.
This is shown in the right part of Fig. 1.
The goal of this paper is to model both the structure and semantics of a discussion thread in a simultaneous way.
 This work was done when the  rst author visiting Microsoft Research, Asia.
1http://developers.slashdot.org/comments.pl?sid=1000769 Copyright is held by the author/owner(s).
Figure 1: An example of the structure and semantics of a discussion thread from Slashdot.
expressed as a mixture of topics, as (cid:126)d(i) (cid:39)(cid:80)T A discussion thread has the following four characteristics.
A discussion thread has several topics.
Suppose there are T topics and V words, the jth topic is described as a distribution over the word space RV , as R is real numbers and (cid:126)x(j)   RV , 1   j   T .
Then, each post (cid:126)d(i) is   (cid:126)x(j), where  (i) is the coe cient of d(i) on topic (cid:126)x(j).
To estimate the topic space X = {(cid:126)x(1), .
.
.
, (cid:126)x(T )}, in SMSS we minimize the loss function (cid:107)D   X (cid:107)2 F .
Here the thread contains L posts as D = { (cid:126)d(1), .
.
.
, (cid:126)d(L)}; and the coe cient matrix   = {(cid:126) (1) .
.
.
(cid:126) (L)}.
j=1  (i) j j An individual post is related to a few topics.
Although one thread may contain several semantic topics, each individual post usually concentrates on a limited number of topics.
Therefore, we assume (cid:126) (i) of each post is sparse and introduce a regularizer (cid:107)(cid:126) (i)(cid:107)1 in SMSS.
larizer (cid:107)(cid:126) (i)  (cid:80)i 1 A post is related to its previous posts.
Users usually read current posts in a thread before they reply.
Thus the semantics of a reply post is related to its previous posts.
In SMSS we formally describe such reply structure as a regu-k is the structural coe cient between the ith and kth posts.
In other words, (cid:126) (i) can be expressed as a linear combination of (cid:126) (k).
k   (cid:126) (k)(cid:107)2 F , where b(i) k=1 b(i) The reply relations are sparse.
In most situations, users only intend to comment on one or two previous posts.
Again, in SMSS we introduce a regularizer to favor such sparse structural coe cients (cid:107)(cid:126)b(i)(cid:107)1.
Based on the above observations, the SMSS model is to estimate the value of topic matrix X, the coe cient matrix  , and the structural coe cients b for each post, by mini-junkSilverlight 1.0JavascriptSilverlight 2.0 ReleasedAbout timeAs I still haven't installed Silverlight 1.0 or seen a site that requires it.Re:About timeSilverlight 1.0 should never have come out.
Silverlight 1.0 vs Silverlight 2.0 is like comparing Flash to Flex...And nothing of valueJavascript communicationwas gained.We're looking for a replacement for canvas in IE.
excanvas sucks.
We could use flash, but the Javascriptflash interface is very slow...Re:Javascript communicationuse SVG, it IS XMLRe:Javascript communicationNo.
SVG is no good for what we need.
Also, its cross-browser support is actually poorer, and performance is abysmal.StructureSemanticsWWW 2009 MADRID!Poster Sessions: Wednesday, April 22, 20091103L(cid:88) mizing the following loss function f : (cid:107)(cid:126) (i)(cid:107)1 f = (cid:107)D   X (cid:107)2 L(cid:88) F +  1 (cid:107)(cid:126) (i)   i 1(cid:88) i=1 k=1 + 2 i=1 k   (cid:126) (k)(cid:107)2 b(i) F +  3 L(cid:88) i=1 (cid:107)(cid:126)b(i)(cid:107)1 (1) M(cid:88) Here, the optimization objective balances the four terms by parameter  1,  2, and  3.
In this way, both the semantics and the structure information are estimated simultaneously.
Furthermore, for a collection of M threads which shared the same topics matrix X, we can optimize them together by minimizing: minimizeX,{ (t)},{(cid:126)bi} f (n)( ) (2) The optimization problem is not jointly convex but can be solved by iteratively minimizing the convex sub-problems.
n=1

 To demonstrate the e ciency of the proposed SMSS model, we reconstruct the reply relationships among posts using the semantics and structures estimated by SMSS.
Intuitively, posts with reply relations should have similar terms.
However, the term similarity is unreliable as posts in discussion threads are usually very short.
Our idea is to integrate the revealed semantic topics and structure as additional information in the similarity measure.
Formally the similarity of a given post j and a previous post i is the combination of all the features, as: sim(i, j) = sim( (cid:126)d(i), (cid:126)d(j)) + w1   sim((cid:126)b(i),(cid:126)b(j)) +w2   sim((cid:126) (i), (cid:126) (j)) (3) Based on the similarity, we propose an approach to analyze a thread with L posts.
That is, for a new post we compute the similarity between itself and all previous posts, rank the similarity, choose the post with highest score as a candidate parent.
In case that the candidate parent is not similar enough to the new post, we assume this post initializes a new discussion branch of the thread.
In experiment, we adopt two forums, Apple Discussion2 and Slashdot3, as our data sources.
These two forums are carefully selected because they have provided clear reply relations for evaluation.
Since threads in Apple Discussion are much shorter, we sample 2000 threads including 20000 posts.
For Slashdot, we sample 100 threads which also contains about 20000 posts.
We manually write a wrapper to parse these pages and extract the exact reply relations as the ground truth.
The evaluation metric is precision.
For comparison, we also adopt some naive methods such as Nearest-Previous (NP), Reply-Root (RR), and Only Document Similarity (DS).
NP assigns each post to the nearest previous post as the reply target; RR assigns each post to the root post as the reply target; and DS assigns each post to the post has most similar terms.
Moreover, we also compared our SMSS model with some state-of-the-art models which can provide semantic topic analysis, such as latent dirichlet allocation (LDA) [1] and 2http://discussions.apple.com/ 3http://www.slashdot.org/ Table 1: Performance of reply reconstruction in all posts v.s.
high-quality posts Method Slashdot Apple All Posts Good Posts All posts Good Posts





























 LDA, (cid:126) (j) the special words with background model (SWB) [2].
Similar to Eq.3, we compute the post similarity by sim(i, j) = sim( (cid:126)d(i), (cid:126)d(j))+w1 sim((cid:126) (i) LDA) from LDA.
While SWB is an extension of LDA and allows words in documents to be modeled as either originating from general topics, or from post-speci c  special  word distributions, or from a thread-wide background distribution.
We leverage both its topic distribution (cid:126) (i) SW B for similarity computing, as sim(i, j) = sim( (cid:126)d(i), (cid:126)d(j)) + w1   sim((cid:126) (i) SW B and special-words distribution (cid:126) (i) SW B) + w2   sim( (cid:126) (i) SW B, (cid:126) (j) SW B, (cid:126) (j)
 All the three methods (LDA, SWB, and SMSS) achieve best average performance at w1 = 0.9 while the w2 is tuned for SWB and SMSS respectively.
The experiment results are shown in Table 1.
From Table 1, we have four observations: (I) in Slashdot, a certain number of posts reply to the thread root, few to the nearest previous post; while in Apple Discussion, there are almost equal number of posts replying to the nearest previous post and the root.
This is because: discussion threads in Apple Discussion follow a Question-Answering style.
New solutions and fresh questions in replies invoke a serial of discussions.
However, threads in Slashdot are usually initialized by a piece of news.
Interesting aspects of the news and brilliant replies arise branches of discussions.
(II) SWB and LDA show slight improvements to the baseline DS.
This has veri ed our assumption that topics are robust in modeling the semantics; but topics are not capable enough to extract reply relations.
(III) In our experiment SWB achieves best performance when w2 is very small.
This is because posts in threaded discussions are usually short.
It is very di cult to estimate a sound coe cient for document speci c word distribution.
(IV) SMSS demonstrates signi cant improvement.
The major di erence between SMSS and former approaches is that SMSS resolves the structure representation b(i) for post pi in each discussion thread.
The best parameter of structural similarity is w2 = 0.9.
This indicates that, besides of semantic similarities, structure similarities are more distinguishing in identifying reply relations.
Furthermore, we also analyze the performance for the posts of di erent quality.
We de ne posts whose score is larger than
 in Apple Discussion as good posts.
The similarity based methods have better performance for these posts with high quality.
It makes sense since the posts with high quality may cause more signi cative replies.
